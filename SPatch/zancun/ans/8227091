
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv2,2/3] arm64: Add support for ARCH_SUPPORTS_DEBUG_PAGEALLOC - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv2,2/3] arm64: Add support for ARCH_SUPPORTS_DEBUG_PAGEALLOC</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=130331">Laura Abbott</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 4, 2016, 7:43 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1454615017-24672-3-git-send-email-labbott@fedoraproject.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8227091/mbox/"
   >mbox</a>
|
   <a href="/patch/8227091/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8227091/">/patch/8227091/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 932CFBEEE5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  4 Feb 2016 19:44:17 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id CC28720383
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  4 Feb 2016 19:44:16 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id EBF7720389
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  4 Feb 2016 19:44:15 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S966260AbcBDToM (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 4 Feb 2016 14:44:12 -0500
Received: from mx1.redhat.com ([209.132.183.28]:40761 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S934131AbcBDTnm (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 4 Feb 2016 14:43:42 -0500
Received: from int-mx11.intmail.prod.int.phx2.redhat.com
	(int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	by mx1.redhat.com (Postfix) with ESMTPS id E7C363A1169;
	Thu,  4 Feb 2016 19:43:41 +0000 (UTC)
Received: from labbott-redhat-machine.redhat.com
	(ovpn-112-42.phx2.redhat.com [10.3.112.42])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id u14Jhcr8017868; Thu, 4 Feb 2016 14:43:40 -0500
From: Laura Abbott &lt;labbott@fedoraproject.org&gt;
To: Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;, Mark Rutland &lt;mark.rutland@arm.com&gt;,
	Ard Biesheuvel &lt;ard.biesheuvel@linaro.org&gt;
Cc: Laura Abbott &lt;labbott@fedoraproject.org&gt;,
	linux-arm-kernel@lists.infradead.org, linux-kernel@vger.kernel.org
Subject: [PATCHv2 2/3] arm64: Add support for ARCH_SUPPORTS_DEBUG_PAGEALLOC
Date: Thu,  4 Feb 2016 11:43:36 -0800
Message-Id: &lt;1454615017-24672-3-git-send-email-labbott@fedoraproject.org&gt;
In-Reply-To: &lt;1454615017-24672-1-git-send-email-labbott@fedoraproject.org&gt;
References: &lt;1454615017-24672-1-git-send-email-labbott@fedoraproject.org&gt;
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.4 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130331">Laura Abbott</a> - Feb. 4, 2016, 7:43 p.m.</div>
<pre class="content">
ARCH_SUPPORTS_DEBUG_PAGEALLOC provides a hook to map and unmap
pages for debugging purposes. This requires memory be mapped
with PAGE_SIZE mappings since breaking down larger mappings
at runtime will lead to TLB conflicts. Check if debug_pagealloc
is enabled at runtime and if so, map everyting with PAGE_SIZE
pages. Implement the functions to actually map/unmap the
pages at runtime.
<span class="signed-off-by">
Signed-off-by: Laura Abbott &lt;labbott@fedoraproject.org&gt;</span>
---
 arch/arm64/Kconfig       |  3 +++
 arch/arm64/mm/mmu.c      | 25 +++++++++++++++++++++----
 arch/arm64/mm/pageattr.c | 46 ++++++++++++++++++++++++++++++++++++----------
 3 files changed, 60 insertions(+), 14 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=66681">Ard Biesheuvel</a> - Feb. 5, 2016, 12:05 p.m.</div>
<pre class="content">
Hi Laura,

On 4 February 2016 at 20:43, Laura Abbott &lt;labbott@fedoraproject.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; ARCH_SUPPORTS_DEBUG_PAGEALLOC provides a hook to map and unmap</span>
<span class="quote">&gt; pages for debugging purposes. This requires memory be mapped</span>
<span class="quote">&gt; with PAGE_SIZE mappings since breaking down larger mappings</span>
<span class="quote">&gt; at runtime will lead to TLB conflicts. Check if debug_pagealloc</span>
<span class="quote">&gt; is enabled at runtime and if so, map everyting with PAGE_SIZE</span>
<span class="quote">&gt; pages. Implement the functions to actually map/unmap the</span>
<span class="quote">&gt; pages at runtime.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Laura Abbott &lt;labbott@fedoraproject.org&gt;</span>

This looks correct to me, but I still have a concern below.
<span class="quote">

&gt; ---</span>
<span class="quote">&gt;  arch/arm64/Kconfig       |  3 +++</span>
<span class="quote">&gt;  arch/arm64/mm/mmu.c      | 25 +++++++++++++++++++++----</span>
<span class="quote">&gt;  arch/arm64/mm/pageattr.c | 46 ++++++++++++++++++++++++++++++++++++----------</span>
<span class="quote">&gt;  3 files changed, 60 insertions(+), 14 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="quote">&gt; index 8cc6228..0f33218 100644</span>
<span class="quote">&gt; --- a/arch/arm64/Kconfig</span>
<span class="quote">&gt; +++ b/arch/arm64/Kconfig</span>
<span class="quote">&gt; @@ -537,6 +537,9 @@ config HOTPLUG_CPU</span>
<span class="quote">&gt;  source kernel/Kconfig.preempt</span>
<span class="quote">&gt;  source kernel/Kconfig.hz</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +config ARCH_SUPPORTS_DEBUG_PAGEALLOC</span>
<span class="quote">&gt; +       def_bool y</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config ARCH_HAS_HOLES_MEMORYMODEL</span>
<span class="quote">&gt;         def_bool y if SPARSEMEM</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; index ef0d66c..be81a59 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; @@ -180,8 +180,14 @@ static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;         pmd = pmd_set_fixmap_offset(pud, addr);</span>
<span class="quote">&gt;         do {</span>
<span class="quote">&gt;                 next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -               /* try section mapping first */</span>
<span class="quote">&gt; -               if (((addr | next | phys) &amp; ~SECTION_MASK) == 0) {</span>
<span class="quote">&gt; +               /*</span>
<span class="quote">&gt; +                * try a section mapping first</span>
<span class="quote">&gt; +                *</span>
<span class="quote">&gt; +                * See comment in use_1G_block for why we need the check</span>
<span class="quote">&gt; +                * for !pgtable_alloc with !debug_pagealloc</span>
<span class="quote">&gt; +                */</span>
<span class="quote">&gt; +               if (((addr | next | phys) &amp; ~SECTION_MASK) == 0 &amp;&amp;</span>
<span class="quote">&gt; +                     (!debug_pagealloc_enabled() || !pgtable_alloc)) {</span>
<span class="quote">&gt;                         pmd_t old_pmd =*pmd;</span>
<span class="quote">&gt;                         set_pmd(pmd, __pmd(phys |</span>
<span class="quote">&gt;                                            pgprot_val(mk_sect_prot(prot))));</span>
<span class="quote">&gt; @@ -208,8 +214,19 @@ static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static inline bool use_1G_block(unsigned long addr, unsigned long next,</span>
<span class="quote">&gt; -                       unsigned long phys)</span>
<span class="quote">&gt; +                       unsigned long phys, phys_addr_t (*pgtable_alloc)(void))</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * If debug_page_alloc is enabled we don&#39;t want to be using sections</span>
<span class="quote">&gt; +        * since everything needs to be mapped with pages. The catch is</span>
<span class="quote">&gt; +        * that we only want to force pages if we can allocate the next</span>
<span class="quote">&gt; +        * layer of page tables. If there is no pgtable_alloc function,</span>
<span class="quote">&gt; +        * it&#39;s too early to allocate another layer and we should use</span>
<span class="quote">&gt; +        * section mappings.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       if (pgtable_alloc &amp;&amp; debug_pagealloc_enabled())</span>
<span class="quote">&gt; +               return false;</span>
<span class="quote">&gt; +</span>

I would suggest you stick this comment and test in a separate function
&#39;bool block_mappings_allowed(phys_addr_t (*pgtable_alloc)(void))&#39; (or
a better name if you can think of one, by all means) and call it from
both sites where you need to perform the check. This keeps the
symmetry, and removes the need to change the prototype of
use_1G_block() to pass pgtable_alloc only to test it for NULL-ness

With that change:
<span class="reviewed-by">Reviewed-by: Ard Biesheuvel &lt;ard.biesheuvel@linaro,.org&gt;</span>
<span class="quote">

&gt;         if (PAGE_SHIFT != 12)</span>
<span class="quote">&gt;                 return false;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -241,7 +258,7 @@ static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;                 /*</span>
<span class="quote">&gt;                  * For 4K granule only, attempt to put down a 1GB block</span>
<span class="quote">&gt;                  */</span>
<span class="quote">&gt; -               if (use_1G_block(addr, next, phys)) {</span>
<span class="quote">&gt; +               if (use_1G_block(addr, next, phys, pgtable_alloc)) {</span>
<span class="quote">&gt;                         pud_t old_pud = *pud;</span>
<span class="quote">&gt;                         set_pud(pud, __pud(phys |</span>
<span class="quote">&gt;                                            pgprot_val(mk_sect_prot(prot))));</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; index 1360a02..57877af 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; @@ -37,14 +37,31 @@ static int change_page_range(pte_t *ptep, pgtable_t token, unsigned long addr,</span>
<span class="quote">&gt;         return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * This function assumes that the range is mapped with PAGE_SIZE pages.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static int __change_memory_common(unsigned long start, unsigned long size,</span>
<span class="quote">&gt; +                               pgprot_t set_mask, pgprot_t clear_mask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct page_change_data data;</span>
<span class="quote">&gt; +       int ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       data.set_mask = set_mask;</span>
<span class="quote">&gt; +       data.clear_mask = clear_mask;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       ret = apply_to_page_range(&amp;init_mm, start, size, change_page_range,</span>
<span class="quote">&gt; +                                       &amp;data);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       flush_tlb_kernel_range(start, start+size);</span>
<span class="quote">&gt; +       return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;                                 pgprot_t set_mask, pgprot_t clear_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         unsigned long start = addr;</span>
<span class="quote">&gt;         unsigned long size = PAGE_SIZE*numpages;</span>
<span class="quote">&gt;         unsigned long end = start + size;</span>
<span class="quote">&gt; -       int ret;</span>
<span class="quote">&gt; -       struct page_change_data data;</span>
<span class="quote">&gt;         struct vm_struct *area;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (!PAGE_ALIGNED(addr)) {</span>
<span class="quote">&gt; @@ -72,14 +89,7 @@ static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;             !(area-&gt;flags &amp; VM_ALLOC))</span>
<span class="quote">&gt;                 return -EINVAL;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       data.set_mask = set_mask;</span>
<span class="quote">&gt; -       data.clear_mask = clear_mask;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -       ret = apply_to_page_range(&amp;init_mm, start, size, change_page_range,</span>
<span class="quote">&gt; -                                       &amp;data);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -       flush_tlb_kernel_range(start, end);</span>
<span class="quote">&gt; -       return ret;</span>
<span class="quote">&gt; +       return __change_memory_common(start, size, set_mask, clear_mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  int set_memory_ro(unsigned long addr, int numpages)</span>
<span class="quote">&gt; @@ -111,3 +121,19 @@ int set_memory_x(unsigned long addr, int numpages)</span>
<span class="quote">&gt;                                         __pgprot(PTE_PXN));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(set_memory_x);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_DEBUG_PAGEALLOC</span>
<span class="quote">&gt; +void __kernel_map_pages(struct page *page, int numpages, int enable)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       unsigned long addr = (unsigned long) page_address(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (enable)</span>
<span class="quote">&gt; +               __change_memory_common(addr, PAGE_SIZE*numpages,</span>
<span class="quote">&gt; +                                       __pgprot(PTE_VALID),</span>
<span class="quote">&gt; +                                       __pgprot(0));</span>
<span class="quote">&gt; +       else</span>
<span class="quote">&gt; +               __change_memory_common(addr, PAGE_SIZE*numpages,</span>
<span class="quote">&gt; +                                       __pgprot(0),</span>
<span class="quote">&gt; +                                       __pgprot(PTE_VALID));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; 2.5.0</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - Feb. 5, 2016, 2:20 p.m.</div>
<pre class="content">
On Thu, Feb 04, 2016 at 11:43:36AM -0800, Laura Abbott wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; ARCH_SUPPORTS_DEBUG_PAGEALLOC provides a hook to map and unmap</span>
<span class="quote">&gt; pages for debugging purposes. This requires memory be mapped</span>
<span class="quote">&gt; with PAGE_SIZE mappings since breaking down larger mappings</span>
<span class="quote">&gt; at runtime will lead to TLB conflicts. Check if debug_pagealloc</span>
<span class="quote">&gt; is enabled at runtime and if so, map everyting with PAGE_SIZE</span>
<span class="quote">&gt; pages. Implement the functions to actually map/unmap the</span>
<span class="quote">&gt; pages at runtime.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Laura Abbott &lt;labbott@fedoraproject.org&gt;</span>

I&#39;ve given this a spin on Juno, with and without the config option
selected, and with and without the command line option. I&#39;ve also given
it a spin on Seattle with inline KASAN also enabled.

I wasn&#39;t sure how to deliberately trigger a failure, but those all
booted fine, and the dumepd page tables looks right, so FWIW:
<span class="tested-by">
Tested-by: Mark Rutland &lt;mark.rutland@arm.com&gt;</span>

I have a few minor comments below, and with those fixed up:
<span class="reviewed-by">
Reviewed-by: Mark Rutland &lt;mark.rutland@arm.com&gt;</span>
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  arch/arm64/Kconfig       |  3 +++</span>
<span class="quote">&gt;  arch/arm64/mm/mmu.c      | 25 +++++++++++++++++++++----</span>
<span class="quote">&gt;  arch/arm64/mm/pageattr.c | 46 ++++++++++++++++++++++++++++++++++++----------</span>
<span class="quote">&gt;  3 files changed, 60 insertions(+), 14 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="quote">&gt; index 8cc6228..0f33218 100644</span>
<span class="quote">&gt; --- a/arch/arm64/Kconfig</span>
<span class="quote">&gt; +++ b/arch/arm64/Kconfig</span>
<span class="quote">&gt; @@ -537,6 +537,9 @@ config HOTPLUG_CPU</span>
<span class="quote">&gt;  source kernel/Kconfig.preempt</span>
<span class="quote">&gt;  source kernel/Kconfig.hz</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config ARCH_SUPPORTS_DEBUG_PAGEALLOC</span>
<span class="quote">&gt; +	def_bool y</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config ARCH_HAS_HOLES_MEMORYMODEL</span>
<span class="quote">&gt;  	def_bool y if SPARSEMEM</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; index ef0d66c..be81a59 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; @@ -180,8 +180,14 @@ static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  	pmd = pmd_set_fixmap_offset(pud, addr);</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		/* try section mapping first */</span>
<span class="quote">&gt; -		if (((addr | next | phys) &amp; ~SECTION_MASK) == 0) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * try a section mapping first</span>
<span class="quote">&gt; +		 *</span>
<span class="quote">&gt; +		 * See comment in use_1G_block for why we need the check</span>
<span class="quote">&gt; +		 * for !pgtable_alloc with !debug_pagealloc</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (((addr | next | phys) &amp; ~SECTION_MASK) == 0 &amp;&amp;</span>
<span class="quote">&gt; +		      (!debug_pagealloc_enabled() || !pgtable_alloc)) {</span>
<span class="quote">&gt;  			pmd_t old_pmd =*pmd;</span>
<span class="quote">&gt;  			set_pmd(pmd, __pmd(phys |</span>
<span class="quote">&gt;  					   pgprot_val(mk_sect_prot(prot))));</span>
<span class="quote">&gt; @@ -208,8 +214,19 @@ static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline bool use_1G_block(unsigned long addr, unsigned long next,</span>
<span class="quote">&gt; -			unsigned long phys)</span>
<span class="quote">&gt; +			unsigned long phys, phys_addr_t (*pgtable_alloc)(void))</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If debug_page_alloc is enabled we don&#39;t want to be using sections</span>
<span class="quote">&gt; +	 * since everything needs to be mapped with pages. The catch is</span>
<span class="quote">&gt; +	 * that we only want to force pages if we can allocate the next</span>
<span class="quote">&gt; +	 * layer of page tables. If there is no pgtable_alloc function,</span>
<span class="quote">&gt; +	 * it&#39;s too early to allocate another layer and we should use</span>
<span class="quote">&gt; +	 * section mappings.</span>
<span class="quote">&gt; +	 */</span>

I&#39;m not sure this quite captures the rationale, as we only care about
the linear map using pages (AFAIK), and the earliness only matters
w.r.t. the DTB mapping. How about:

	/*
	 * If debug_page_alloc is enabled we must map the linear map
	 * using pages. However, other mappings created by
	 * create_mapping_noalloc must use sections in some cases. Allow
	 * sections to be used in those cases, where no pgtable_alloc
	 * function is provided.
	 */

Does that sound ok to you?

As a future optimisation, I think we can allow sections when mapping
permanent kernel chunks (.e.g .rodata and .text), as these shouldn&#39;t
contain pages available for dynamic allocation. That would require using
something other than the presence of pgtable_alloc to determine when we
should force page usage.
<span class="quote">
&gt; +	if (pgtable_alloc &amp;&amp; debug_pagealloc_enabled())</span>
<span class="quote">&gt; +		return false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (PAGE_SHIFT != 12)</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -241,7 +258,7 @@ static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * For 4K granule only, attempt to put down a 1GB block</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (use_1G_block(addr, next, phys)) {</span>
<span class="quote">&gt; +		if (use_1G_block(addr, next, phys, pgtable_alloc)) {</span>
<span class="quote">&gt;  			pud_t old_pud = *pud;</span>
<span class="quote">&gt;  			set_pud(pud, __pud(phys |</span>
<span class="quote">&gt;  					   pgprot_val(mk_sect_prot(prot))));</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; index 1360a02..57877af 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; @@ -37,14 +37,31 @@ static int change_page_range(pte_t *ptep, pgtable_t token, unsigned long addr,</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * This function assumes that the range is mapped with PAGE_SIZE pages.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static int __change_memory_common(unsigned long start, unsigned long size,</span>
<span class="quote">&gt; +				pgprot_t set_mask, pgprot_t clear_mask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page_change_data data;</span>
<span class="quote">&gt; +	int ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	data.set_mask = set_mask;</span>
<span class="quote">&gt; +	data.clear_mask = clear_mask;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ret = apply_to_page_range(&amp;init_mm, start, size, change_page_range,</span>
<span class="quote">&gt; +					&amp;data);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	flush_tlb_kernel_range(start, start+size);</span>

Nit: spaces around &#39;+&#39; please.
<span class="quote">
&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;  				pgprot_t set_mask, pgprot_t clear_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long start = addr;</span>
<span class="quote">&gt;  	unsigned long size = PAGE_SIZE*numpages;</span>
<span class="quote">&gt;  	unsigned long end = start + size;</span>
<span class="quote">&gt; -	int ret;</span>
<span class="quote">&gt; -	struct page_change_data data;</span>
<span class="quote">&gt;  	struct vm_struct *area;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!PAGE_ALIGNED(addr)) {</span>
<span class="quote">&gt; @@ -72,14 +89,7 @@ static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;  	    !(area-&gt;flags &amp; VM_ALLOC))</span>
<span class="quote">&gt;  		return -EINVAL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	data.set_mask = set_mask;</span>
<span class="quote">&gt; -	data.clear_mask = clear_mask;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	ret = apply_to_page_range(&amp;init_mm, start, size, change_page_range,</span>
<span class="quote">&gt; -					&amp;data);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	flush_tlb_kernel_range(start, end);</span>
<span class="quote">&gt; -	return ret;</span>
<span class="quote">&gt; +	return __change_memory_common(start, size, set_mask, clear_mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  int set_memory_ro(unsigned long addr, int numpages)</span>
<span class="quote">&gt; @@ -111,3 +121,19 @@ int set_memory_x(unsigned long addr, int numpages)</span>
<span class="quote">&gt;  					__pgprot(PTE_PXN));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(set_memory_x);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_DEBUG_PAGEALLOC</span>
<span class="quote">&gt; +void __kernel_map_pages(struct page *page, int numpages, int enable)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long addr = (unsigned long) page_address(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (enable)</span>
<span class="quote">&gt; +		__change_memory_common(addr, PAGE_SIZE*numpages,</span>
<span class="quote">&gt; +					__pgprot(PTE_VALID),</span>
<span class="quote">&gt; +					__pgprot(0));</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		__change_memory_common(addr, PAGE_SIZE*numpages,</span>
<span class="quote">&gt; +					__pgprot(0),</span>
<span class="quote">&gt; +					__pgprot(PTE_VALID));</span>

Nit: spaces around each &#39;*&#39; here, please.

Thanks,
Mark.
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.5.0</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - Feb. 6, 2016, 12:08 a.m.</div>
<pre class="content">
On 02/05/2016 06:20 AM, Mark Rutland wrote:
<span class="quote">&gt; On Thu, Feb 04, 2016 at 11:43:36AM -0800, Laura Abbott wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ARCH_SUPPORTS_DEBUG_PAGEALLOC provides a hook to map and unmap</span>
<span class="quote">&gt;&gt; pages for debugging purposes. This requires memory be mapped</span>
<span class="quote">&gt;&gt; with PAGE_SIZE mappings since breaking down larger mappings</span>
<span class="quote">&gt;&gt; at runtime will lead to TLB conflicts. Check if debug_pagealloc</span>
<span class="quote">&gt;&gt; is enabled at runtime and if so, map everyting with PAGE_SIZE</span>
<span class="quote">&gt;&gt; pages. Implement the functions to actually map/unmap the</span>
<span class="quote">&gt;&gt; pages at runtime.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Laura Abbott &lt;labbott@fedoraproject.org&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;ve given this a spin on Juno, with and without the config option</span>
<span class="quote">&gt; selected, and with and without the command line option. I&#39;ve also given</span>
<span class="quote">&gt; it a spin on Seattle with inline KASAN also enabled.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I wasn&#39;t sure how to deliberately trigger a failure, but those all</span>
<span class="quote">&gt; booted fine, and the dumepd page tables looks right, so FWIW:</span>
<span class="quote">&gt;</span>

I wrote a test that does a write after free on an allocated page
for my testing. Might be worth it to look into adding a test to
the lkdtm module.

I also did testing with 64K pages but couldn&#39;t test on 16K due
to lack of hardware and QEMU running off into the weeds.
<span class="quote">  
&gt; Tested-by: Mark Rutland &lt;mark.rutland@arm.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I have a few minor comments below, and with those fixed up:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Reviewed-by: Mark Rutland &lt;mark.rutland@arm.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;   arch/arm64/Kconfig       |  3 +++</span>
<span class="quote">&gt;&gt;   arch/arm64/mm/mmu.c      | 25 +++++++++++++++++++++----</span>
<span class="quote">&gt;&gt;   arch/arm64/mm/pageattr.c | 46 ++++++++++++++++++++++++++++++++++++----------</span>
<span class="quote">&gt;&gt;   3 files changed, 60 insertions(+), 14 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="quote">&gt;&gt; index 8cc6228..0f33218 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/Kconfig</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/Kconfig</span>
<span class="quote">&gt;&gt; @@ -537,6 +537,9 @@ config HOTPLUG_CPU</span>
<span class="quote">&gt;&gt;   source kernel/Kconfig.preempt</span>
<span class="quote">&gt;&gt;   source kernel/Kconfig.hz</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +config ARCH_SUPPORTS_DEBUG_PAGEALLOC</span>
<span class="quote">&gt;&gt; +	def_bool y</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;   config ARCH_HAS_HOLES_MEMORYMODEL</span>
<span class="quote">&gt;&gt;   	def_bool y if SPARSEMEM</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt;&gt; index ef0d66c..be81a59 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt;&gt; @@ -180,8 +180,14 @@ static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;&gt;   	pmd = pmd_set_fixmap_offset(pud, addr);</span>
<span class="quote">&gt;&gt;   	do {</span>
<span class="quote">&gt;&gt;   		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt;&gt; -		/* try section mapping first */</span>
<span class="quote">&gt;&gt; -		if (((addr | next | phys) &amp; ~SECTION_MASK) == 0) {</span>
<span class="quote">&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * try a section mapping first</span>
<span class="quote">&gt;&gt; +		 *</span>
<span class="quote">&gt;&gt; +		 * See comment in use_1G_block for why we need the check</span>
<span class="quote">&gt;&gt; +		 * for !pgtable_alloc with !debug_pagealloc</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		if (((addr | next | phys) &amp; ~SECTION_MASK) == 0 &amp;&amp;</span>
<span class="quote">&gt;&gt; +		      (!debug_pagealloc_enabled() || !pgtable_alloc)) {</span>
<span class="quote">&gt;&gt;   			pmd_t old_pmd =*pmd;</span>
<span class="quote">&gt;&gt;   			set_pmd(pmd, __pmd(phys |</span>
<span class="quote">&gt;&gt;   					   pgprot_val(mk_sect_prot(prot))));</span>
<span class="quote">&gt;&gt; @@ -208,8 +214,19 @@ static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   static inline bool use_1G_block(unsigned long addr, unsigned long next,</span>
<span class="quote">&gt;&gt; -			unsigned long phys)</span>
<span class="quote">&gt;&gt; +			unsigned long phys, phys_addr_t (*pgtable_alloc)(void))</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * If debug_page_alloc is enabled we don&#39;t want to be using sections</span>
<span class="quote">&gt;&gt; +	 * since everything needs to be mapped with pages. The catch is</span>
<span class="quote">&gt;&gt; +	 * that we only want to force pages if we can allocate the next</span>
<span class="quote">&gt;&gt; +	 * layer of page tables. If there is no pgtable_alloc function,</span>
<span class="quote">&gt;&gt; +	 * it&#39;s too early to allocate another layer and we should use</span>
<span class="quote">&gt;&gt; +	 * section mappings.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m not sure this quite captures the rationale, as we only care about</span>
<span class="quote">&gt; the linear map using pages (AFAIK), and the earliness only matters</span>
<span class="quote">&gt; w.r.t. the DTB mapping. How about:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	/*</span>
<span class="quote">&gt; 	 * If debug_page_alloc is enabled we must map the linear map</span>
<span class="quote">&gt; 	 * using pages. However, other mappings created by</span>
<span class="quote">&gt; 	 * create_mapping_noalloc must use sections in some cases. Allow</span>
<span class="quote">&gt; 	 * sections to be used in those cases, where no pgtable_alloc</span>
<span class="quote">&gt; 	 * function is provided.</span>
<span class="quote">&gt; 	 */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Does that sound ok to you?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; As a future optimisation, I think we can allow sections when mapping</span>
<span class="quote">&gt; permanent kernel chunks (.e.g .rodata and .text), as these shouldn&#39;t</span>
<span class="quote">&gt; contain pages available for dynamic allocation. That would require using</span>
<span class="quote">&gt; something other than the presence of pgtable_alloc to determine when we</span>
<span class="quote">&gt; should force page usage.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +	if (pgtable_alloc &amp;&amp; debug_pagealloc_enabled())</span>
<span class="quote">&gt;&gt; +		return false;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;   	if (PAGE_SHIFT != 12)</span>
<span class="quote">&gt;&gt;   		return false;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -241,7 +258,7 @@ static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;&gt;   		/*</span>
<span class="quote">&gt;&gt;   		 * For 4K granule only, attempt to put down a 1GB block</span>
<span class="quote">&gt;&gt;   		 */</span>
<span class="quote">&gt;&gt; -		if (use_1G_block(addr, next, phys)) {</span>
<span class="quote">&gt;&gt; +		if (use_1G_block(addr, next, phys, pgtable_alloc)) {</span>
<span class="quote">&gt;&gt;   			pud_t old_pud = *pud;</span>
<span class="quote">&gt;&gt;   			set_pud(pud, __pud(phys |</span>
<span class="quote">&gt;&gt;   					   pgprot_val(mk_sect_prot(prot))));</span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt;&gt; index 1360a02..57877af 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt;&gt; @@ -37,14 +37,31 @@ static int change_page_range(pte_t *ptep, pgtable_t token, unsigned long addr,</span>
<span class="quote">&gt;&gt;   	return 0;</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * This function assumes that the range is mapped with PAGE_SIZE pages.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static int __change_memory_common(unsigned long start, unsigned long size,</span>
<span class="quote">&gt;&gt; +				pgprot_t set_mask, pgprot_t clear_mask)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct page_change_data data;</span>
<span class="quote">&gt;&gt; +	int ret;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	data.set_mask = set_mask;</span>
<span class="quote">&gt;&gt; +	data.clear_mask = clear_mask;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	ret = apply_to_page_range(&amp;init_mm, start, size, change_page_range,</span>
<span class="quote">&gt;&gt; +					&amp;data);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	flush_tlb_kernel_range(start, start+size);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Nit: spaces around &#39;+&#39; please.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +	return ret;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;   static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;&gt;   				pgprot_t set_mask, pgprot_t clear_mask)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt;   	unsigned long start = addr;</span>
<span class="quote">&gt;&gt;   	unsigned long size = PAGE_SIZE*numpages;</span>
<span class="quote">&gt;&gt;   	unsigned long end = start + size;</span>
<span class="quote">&gt;&gt; -	int ret;</span>
<span class="quote">&gt;&gt; -	struct page_change_data data;</span>
<span class="quote">&gt;&gt;   	struct vm_struct *area;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   	if (!PAGE_ALIGNED(addr)) {</span>
<span class="quote">&gt;&gt; @@ -72,14 +89,7 @@ static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;&gt;   	    !(area-&gt;flags &amp; VM_ALLOC))</span>
<span class="quote">&gt;&gt;   		return -EINVAL;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -	data.set_mask = set_mask;</span>
<span class="quote">&gt;&gt; -	data.clear_mask = clear_mask;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	ret = apply_to_page_range(&amp;init_mm, start, size, change_page_range,</span>
<span class="quote">&gt;&gt; -					&amp;data);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	flush_tlb_kernel_range(start, end);</span>
<span class="quote">&gt;&gt; -	return ret;</span>
<span class="quote">&gt;&gt; +	return __change_memory_common(start, size, set_mask, clear_mask);</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   int set_memory_ro(unsigned long addr, int numpages)</span>
<span class="quote">&gt;&gt; @@ -111,3 +121,19 @@ int set_memory_x(unsigned long addr, int numpages)</span>
<span class="quote">&gt;&gt;   					__pgprot(PTE_PXN));</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;   EXPORT_SYMBOL_GPL(set_memory_x);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_DEBUG_PAGEALLOC</span>
<span class="quote">&gt;&gt; +void __kernel_map_pages(struct page *page, int numpages, int enable)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	unsigned long addr = (unsigned long) page_address(page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	if (enable)</span>
<span class="quote">&gt;&gt; +		__change_memory_common(addr, PAGE_SIZE*numpages,</span>
<span class="quote">&gt;&gt; +					__pgprot(PTE_VALID),</span>
<span class="quote">&gt;&gt; +					__pgprot(0));</span>
<span class="quote">&gt;&gt; +	else</span>
<span class="quote">&gt;&gt; +		__change_memory_common(addr, PAGE_SIZE*numpages,</span>
<span class="quote">&gt;&gt; +					__pgprot(0),</span>
<span class="quote">&gt;&gt; +					__pgprot(PTE_VALID));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Nit: spaces around each &#39;*&#39; here, please.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Mark.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; --</span>
<span class="quote">&gt;&gt; 2.5.0</span>
<span class="quote">&gt;&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/Kconfig b/arch/arm64/Kconfig</span>
<span class="p_header">index 8cc6228..0f33218 100644</span>
<span class="p_header">--- a/arch/arm64/Kconfig</span>
<span class="p_header">+++ b/arch/arm64/Kconfig</span>
<span class="p_chunk">@@ -537,6 +537,9 @@</span> <span class="p_context"> config HOTPLUG_CPU</span>
 source kernel/Kconfig.preempt
 source kernel/Kconfig.hz
 
<span class="p_add">+config ARCH_SUPPORTS_DEBUG_PAGEALLOC</span>
<span class="p_add">+	def_bool y</span>
<span class="p_add">+</span>
 config ARCH_HAS_HOLES_MEMORYMODEL
 	def_bool y if SPARSEMEM
 
<span class="p_header">diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="p_header">index ef0d66c..be81a59 100644</span>
<span class="p_header">--- a/arch/arm64/mm/mmu.c</span>
<span class="p_header">+++ b/arch/arm64/mm/mmu.c</span>
<span class="p_chunk">@@ -180,8 +180,14 @@</span> <span class="p_context"> static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,</span>
 	pmd = pmd_set_fixmap_offset(pud, addr);
 	do {
 		next = pmd_addr_end(addr, end);
<span class="p_del">-		/* try section mapping first */</span>
<span class="p_del">-		if (((addr | next | phys) &amp; ~SECTION_MASK) == 0) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * try a section mapping first</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * See comment in use_1G_block for why we need the check</span>
<span class="p_add">+		 * for !pgtable_alloc with !debug_pagealloc</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (((addr | next | phys) &amp; ~SECTION_MASK) == 0 &amp;&amp;</span>
<span class="p_add">+		      (!debug_pagealloc_enabled() || !pgtable_alloc)) {</span>
 			pmd_t old_pmd =*pmd;
 			set_pmd(pmd, __pmd(phys |
 					   pgprot_val(mk_sect_prot(prot))));
<span class="p_chunk">@@ -208,8 +214,19 @@</span> <span class="p_context"> static void alloc_init_pmd(pud_t *pud, unsigned long addr, unsigned long end,</span>
 }
 
 static inline bool use_1G_block(unsigned long addr, unsigned long next,
<span class="p_del">-			unsigned long phys)</span>
<span class="p_add">+			unsigned long phys, phys_addr_t (*pgtable_alloc)(void))</span>
 {
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If debug_page_alloc is enabled we don&#39;t want to be using sections</span>
<span class="p_add">+	 * since everything needs to be mapped with pages. The catch is</span>
<span class="p_add">+	 * that we only want to force pages if we can allocate the next</span>
<span class="p_add">+	 * layer of page tables. If there is no pgtable_alloc function,</span>
<span class="p_add">+	 * it&#39;s too early to allocate another layer and we should use</span>
<span class="p_add">+	 * section mappings.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (pgtable_alloc &amp;&amp; debug_pagealloc_enabled())</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
 	if (PAGE_SHIFT != 12)
 		return false;
 
<span class="p_chunk">@@ -241,7 +258,7 @@</span> <span class="p_context"> static void alloc_init_pud(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
 		/*
 		 * For 4K granule only, attempt to put down a 1GB block
 		 */
<span class="p_del">-		if (use_1G_block(addr, next, phys)) {</span>
<span class="p_add">+		if (use_1G_block(addr, next, phys, pgtable_alloc)) {</span>
 			pud_t old_pud = *pud;
 			set_pud(pud, __pud(phys |
 					   pgprot_val(mk_sect_prot(prot))));
<span class="p_header">diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c</span>
<span class="p_header">index 1360a02..57877af 100644</span>
<span class="p_header">--- a/arch/arm64/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/arm64/mm/pageattr.c</span>
<span class="p_chunk">@@ -37,14 +37,31 @@</span> <span class="p_context"> static int change_page_range(pte_t *ptep, pgtable_t token, unsigned long addr,</span>
 	return 0;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * This function assumes that the range is mapped with PAGE_SIZE pages.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int __change_memory_common(unsigned long start, unsigned long size,</span>
<span class="p_add">+				pgprot_t set_mask, pgprot_t clear_mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page_change_data data;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	data.set_mask = set_mask;</span>
<span class="p_add">+	data.clear_mask = clear_mask;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = apply_to_page_range(&amp;init_mm, start, size, change_page_range,</span>
<span class="p_add">+					&amp;data);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(start, start+size);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int change_memory_common(unsigned long addr, int numpages,
 				pgprot_t set_mask, pgprot_t clear_mask)
 {
 	unsigned long start = addr;
 	unsigned long size = PAGE_SIZE*numpages;
 	unsigned long end = start + size;
<span class="p_del">-	int ret;</span>
<span class="p_del">-	struct page_change_data data;</span>
 	struct vm_struct *area;
 
 	if (!PAGE_ALIGNED(addr)) {
<span class="p_chunk">@@ -72,14 +89,7 @@</span> <span class="p_context"> static int change_memory_common(unsigned long addr, int numpages,</span>
 	    !(area-&gt;flags &amp; VM_ALLOC))
 		return -EINVAL;
 
<span class="p_del">-	data.set_mask = set_mask;</span>
<span class="p_del">-	data.clear_mask = clear_mask;</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = apply_to_page_range(&amp;init_mm, start, size, change_page_range,</span>
<span class="p_del">-					&amp;data);</span>
<span class="p_del">-</span>
<span class="p_del">-	flush_tlb_kernel_range(start, end);</span>
<span class="p_del">-	return ret;</span>
<span class="p_add">+	return __change_memory_common(start, size, set_mask, clear_mask);</span>
 }
 
 int set_memory_ro(unsigned long addr, int numpages)
<span class="p_chunk">@@ -111,3 +121,19 @@</span> <span class="p_context"> int set_memory_x(unsigned long addr, int numpages)</span>
 					__pgprot(PTE_PXN));
 }
 EXPORT_SYMBOL_GPL(set_memory_x);
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_DEBUG_PAGEALLOC</span>
<span class="p_add">+void __kernel_map_pages(struct page *page, int numpages, int enable)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = (unsigned long) page_address(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (enable)</span>
<span class="p_add">+		__change_memory_common(addr, PAGE_SIZE*numpages,</span>
<span class="p_add">+					__pgprot(PTE_VALID),</span>
<span class="p_add">+					__pgprot(0));</span>
<span class="p_add">+	else</span>
<span class="p_add">+		__change_memory_common(addr, PAGE_SIZE*numpages,</span>
<span class="p_add">+					__pgprot(0),</span>
<span class="p_add">+					__pgprot(PTE_VALID));</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



