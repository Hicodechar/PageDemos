
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3] vfio/type1: Adopt fast IOTLB flush interface when unmap IOVAs - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3] vfio/type1: Adopt fast IOTLB flush interface when unmap IOVAs</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2908">Suthikulpanit, Suravee</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 22, 2018, 4:29 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1516595377-4681-1-git-send-email-suravee.suthikulpanit@amd.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10177323/mbox/"
   >mbox</a>
|
   <a href="/patch/10177323/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10177323/">/patch/10177323/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	B689F60353 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 22 Jan 2018 04:30:47 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A23152580E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 22 Jan 2018 04:30:47 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 96A38274A3; Mon, 22 Jan 2018 04:30:47 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 70B5626E97
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 22 Jan 2018 04:30:45 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751139AbeAVEaU (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 21 Jan 2018 23:30:20 -0500
Received: from mail-by2nam01on0074.outbound.protection.outlook.com
	([104.47.34.74]:27968
	&quot;EHLO NAM01-BY2-obe.outbound.protection.outlook.com&quot;
	rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
	id S1751004AbeAVEaR (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 21 Jan 2018 23:30:17 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=amdcloud.onmicrosoft.com; s=selector1-amd-com;
	h=From:Date:Subject:Message-ID:Content-Type:MIME-Version;
	bh=clxWlBeUGZv05UWBVEajkL39Oekf4ldue91ZZSPiFBE=;
	b=VJeYDyugFusjVCk1Hhpz12hGs7zU614+tTWsJCUD3A39S52cv5a+UqytoQBbfd/aFLHUcbkONbaOAsE5i0LTRlOmLAl4hgtptTtbl05MBSN5O33y1RjoZo4hB1nLFuv6heLNxqNLL/P1lwyahC88Ho3Q4JBTJQvKx1kx0vY32AA=
Authentication-Results: spf=none (sender IP is )
	smtp.mailfrom=Suravee.Suthikulpanit@amd.com; 
Received: from ssuthiku-rhel74.localdomain (114.109.128.54) by
	CY4PR12MB1733.namprd12.prod.outlook.com (10.175.62.143) with
	Microsoft SMTP Server (version=TLS1_2,
	cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P256) id
	15.20.428.17; Mon, 22 Jan 2018 04:30:12 +0000
From: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;
To: kvm@vger.kernel.org, iommu@lists.linux-foundation.org,
	linux-kernel@vger.kernel.org
Cc: alex.williamson@redhat.com, joro@8bytes.org, jroedel@suse.de,
	Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;
Subject: [PATCH v3] vfio/type1: Adopt fast IOTLB flush interface when unmap
	IOVAs
Date: Sun, 21 Jan 2018 23:29:37 -0500
Message-Id: &lt;1516595377-4681-1-git-send-email-suravee.suthikulpanit@amd.com&gt;
X-Mailer: git-send-email 1.8.3.1
MIME-Version: 1.0
Content-Type: text/plain
X-Originating-IP: [114.109.128.54]
X-ClientProxiedBy: HK2PR02CA0181.apcprd02.prod.outlook.com (10.171.31.17) To
	CY4PR12MB1733.namprd12.prod.outlook.com (10.175.62.143)
X-MS-PublicTrafficType: Email
X-MS-Office365-Filtering-Correlation-Id: 9e902bdf-aac8-495e-5ac2-08d56150d57c
X-MS-Office365-Filtering-HT: Tenant
X-Microsoft-Antispam: UriScan:; BCL:0; PCL:0;
	RULEID:(7020095)(4652020)(5600026)(4604075)(48565401081)(2017052603307)(7153060)(7193020);
	SRVR:CY4PR12MB1733; 
X-Microsoft-Exchange-Diagnostics: 1; CY4PR12MB1733;
	3:5vJnvGhy58XmmhiiBLVTvzVcHPbvc+pHQ8fovVaOu8Z30DgVxYXnEHP3iulGNr3tL/CeDVqF6B0txq0YjtHB/uwzwQeqzg5ssuNYP055mTLuI40NW9GZcxYaQ0MhnwV2PgqWjItxRSTyA6Ouq+UWv6viBdkeLTGImFnK7M3+2/5WTOX9RiloSYcBQX1UrKQUFws8bU1CdpzYAttoCRqNU7O/RNl5tTzHFxyXNoy4Mj5EY9A/eerlDJ8a3lYaNpOa;
	25:67+GtTFIe6aCb1vmiVy7Yzp9E9Nh5QRzkAA//acOpsPPy5/LgKp35wDJt+PpAHU72suxXZCvc0XM42unX0JAvFYNmRTl5q34pQXPFjnmFqaTxaOOX85xPWZXgWACgJ82bfXYXhP3sdKh8af+wzAbogG4RLlFZOv28N4lqXeGzr3REWNChKQCv7rgqIuzi4BoNLKGpxLx7Ac2rKpG0ycCmsCM+66MgRUvX/C1W6NhWAX+h0lsZSKpJqhpv8sspxagVjVbF5QotY2OskczlrXN+zN2vcjxzWDj8Q8qhKir0qw017eJNyOuDfhOLt/KV4ry9b5HsrhqBGjiKkUd/v0mHg==;
	31:gY7q+3f/BDOQ4/oqB4HivzCzLdAK+9tLelmOKKPjI1RUNg0JSBPNchw68vlgfprhdZwBZk9yNBselecmMHHkvNIbMhFA/JjrUwO3tJzBh5HHnCqnRzhGPWT/kSl6JTeVYRaMO7lKTwwgeqOfxzbchNuheG9sj27ljJ5R74F2JDdmwjc185spy5hvr5GPToZxhwHfahxMPpStmN8tOIJLQMfxfxd/mOeooF+8S+OYdPk=
X-MS-TrafficTypeDiagnostic: CY4PR12MB1733:
X-Microsoft-Exchange-Diagnostics: 1; CY4PR12MB1733;
	20:newHd4r5KbzbZAiJ4mD7bNaoQ50m4rWAbjs6JvLqquQfCrqcHBYeXEkjD03G9rLv0wj1e+PKHdVim31qDj0Cfgukr+XzRhk2yGw56NPFfgVuAlXKwGvKp01JtPpYTX6EBLc1WmLsSOmbEKZMDPFpzM9g0InOSDLotiK15wH8KasDEwjP8ZdWIOHflxigBaIUcKM8j94cu6xKx15iCwat3rkVaC78fmcgn4fDL2BboUbO99IGCsYN19n0+KVkAtA6q6B6rYsbrIOY7Tuol/4E4a4TMWf06Ay7w1T044mr0iIMH9B6py4JW2PRGnVVc9q+Q1Kd84HkNhA6w1SS3nnzKSxDRdBPBhJ8PJ+DCwgzqu6jQgM3TTPGw9vcuaqw79651dXJj61omyQRx9z5BHxsmwteDBc/xKEi94qGB0twuTzRTPZW7hlH9aulDVti684YBWjmtnqtHlgYH6kqQIPJcg5OM16vQgwLGUFyw7dz/xdvnz7jurGdU1XT9Z3G27qI;
	4:TLtsZuYg1xeJgyEPAUJrijUBJimarUngnO+K/k7JWVNVhkL0cXCJSIvpwxyWvddJ3QY4m9afTZHvyZHo8RT85iT9XQUB7RRD4b8uGwRbJ39lxtc5UijfUhlVINNGavWBluY/sG9sZQouJovlNkm1zSmZzdUBd95tDapSHBbPJIeF7519DpD3PYlggQSNUPmPPargXAWTvj2ir9g6H65AdQXC1VvibS/DdOrq+rlL73A6x+D6NbJoG8xSG5bi/IId00RRsQ3pvZr+cwGv1JAanb12T0FTcLt94duK4jP/MCp45f4rVHZ+gL0kQGtLhn1a
X-Microsoft-Antispam-PRVS: &lt;CY4PR12MB1733DAECA58D98E0F932E3F5F3EC0@CY4PR12MB1733.namprd12.prod.outlook.com&gt;
X-Exchange-Antispam-Report-Test: UriScan:(767451399110);
X-Exchange-Antispam-Report-CFA-Test: BCL:0; PCL:0;
	RULEID:(6040501)(2401047)(8121501046)(5005006)(3002001)(10201501046)(93006095)(93001095)(3231023)(2400081)(944501161)(6055026)(6041288)(20161123564045)(201703131423095)(201702281528075)(20161123555045)(201703061421075)(201703061406153)(20161123560045)(20161123562045)(20161123558120)(6072148)(201708071742011);
	SRVR:CY4PR12MB1733; BCL:0; PCL:0;
	RULEID:(100000803101)(100110400095); SRVR:CY4PR12MB1733; 
X-Forefront-PRVS: 0560A2214D
X-Forefront-Antispam-Report: SFV:NSPM;
	SFS:(10009020)(346002)(376002)(366004)(39380400002)(39860400002)(396003)(199004)(189003)(6486002)(2906002)(48376002)(50466002)(68736007)(50226002)(3846002)(6116002)(5660300001)(16526018)(16586007)(52116002)(51416003)(316002)(26005)(6506007)(386003)(59450400001)(305945005)(7736002)(66066001)(47776003)(478600001)(6306002)(6512007)(6666003)(36756003)(4720700003)(86362001)(53936002)(105586002)(8676002)(81156014)(81166006)(97736004)(8936002)(4326008)(72206003)(106356001)(25786009);
	DIR:OUT; SFP:1101; SCL:1; SRVR:CY4PR12MB1733;
	H:ssuthiku-rhel74.localdomain; FPR:; SPF:None;
	PTR:InfoNoRecords; A:1; MX:1; LANG:en; 
Received-SPF: None (protection.outlook.com: amd.com does not designate
	permitted sender hosts)
X-Microsoft-Exchange-Diagnostics: =?us-ascii?Q?1; CY4PR12MB1733;
	23:YIIZM1FUUSAhlj8LpLvlKICp7VOXDVO2f1mJcZiBG?=
	=?us-ascii?Q?FuZVZBCiv2usvNgixgyNMyamG0XZ2rcw4duaJozPiYoM9LPbVa+FLVhDrSXb?=
	=?us-ascii?Q?rAdpgXgYUjetGTlHCOwYR3s/ke3whQWRTeh9aYiKXhsRJWk9h5t8tCo1i6dH?=
	=?us-ascii?Q?t9HOElAjEP50l/oIMi42mqu1Se3l734q6D5HxLlW2DJ1bCywbH5rvo46Zo0G?=
	=?us-ascii?Q?ohJ6mb7ywIthihKPMywFp7yRh6sg6VFWsbrzphhEQgvLfHf1YY7Jab4ZCiyc?=
	=?us-ascii?Q?w0Do/qHaH7ndN9xiu1kCyhHYb1cX4rjEQbKpHfRDpYhgkd/+iI4Heu9E8geq?=
	=?us-ascii?Q?ldSoF0zqanowz6xs7ofUfg+hxeC3ZZVEFfGHlkP7BF3CQugXoZJ/oj5lcy3L?=
	=?us-ascii?Q?T1pH7l+eaoWW/V/jYlw29Xn9AQcZPX+6Qmph7DkWND10PzWahL1XHOkw2Hn0?=
	=?us-ascii?Q?PGHq4DIvu3XDorIt02u0LhTylFwjNvYPwHi4VbFi1jdBt+M1uBKWJoBCmUo0?=
	=?us-ascii?Q?lXEHwmLCw2WYzd2WkuLIJCcu0oFz4d/Q6NQqoNzc4VHQrWurCH2p8/ZCPt2i?=
	=?us-ascii?Q?OUfTYYDG4+XO13hh8muI6kuBqcGt94eQ/wmyoIBC1wwH0NdeqdMJcz2flZcw?=
	=?us-ascii?Q?KllT9R0xaKIkLTITVjj2ybXCPbZpiP7XQWQxjrDuO7CmFpUEGnnW4b++mQDW?=
	=?us-ascii?Q?8yjL5MPFD6cEBMHEIarmAB0p5d/EL1kCeSXiLQxFEr7FUaeiboOG+JL1tDyY?=
	=?us-ascii?Q?RHcYU2Nhhx5gxW0r8PoiMzQxqylGSg7XM6iILlxARpZmqm7qApuzT4yPO1ha?=
	=?us-ascii?Q?R0RJzmqaPJq9knuFXG4r+hjbf6pMUuLeDo+vN8+udt6BJcoszlV4Pb/eWKKB?=
	=?us-ascii?Q?v9sCV0hGDnhwu3+RX1EHSguLrBIOROilF0HA5JIwCIpHll7DcVUKH9H/0Exj?=
	=?us-ascii?Q?M3uxUhof5QMCo7zsQDm6t2U8eVYNDwRZitiFm3Xit1l5KrW+98eMuc5ua6q4?=
	=?us-ascii?Q?zps6i+tTZiR4dg7Hf+6iVpk81dQ9HTlGWAj/NI/WMWZOHR8JTSZhXISoNLPU?=
	=?us-ascii?Q?sJwiXh0En2gUXkWGbhGY8bt0cucwF/dqq9ReI+2aOMTFANllD8uhv/uLlyTE?=
	=?us-ascii?Q?nI9nvVrjFM=3D?=
X-Microsoft-Exchange-Diagnostics: 1; CY4PR12MB1733;
	6:KvxFdRuz23fXdKaOmmf53gtukdP98OaOIM7tNsQ0PGQqvxRUHPVYXFc+FjW/PrtWS0qpp/PdtjQWNHN1zV+JKnmaDDAUv4jF1JKwGsIG3YBF4stjhQvwBeHU1ojMpqvE0RlxQx8GQut5wtipD6rVZ7yCnsil4e7kK1WsV6fUkwoNUz1mEdUd4+IDaab6vcMD5jqE7KoL/8fyC8dlo8vZOvLx6J3TdNTpsHkXn+TfrthtFuJUAv4Sv8Z0rOyFYJxQrExImG6OaoLptYPdrfRx11EYGbphLScDM0od+kWfCu1RUqEoxMieMprk1yrXxfazA82SRXlSqX8d88Kljkc7TuWPdgj7Y1jBdszl8aZ9GCA=;
	5:iBZIuCoNAGJDf3eExW9rE7GcU8aHSScoAjmM1zMJFux+s5cGYEsfWvcDBNXnRSpqX3Hh0OdZ0kP+w3/NnsqlkobHK4VcSP76mQ25M2voQPyaTwSXpnzG952CPHhHlBkZrSB5T66j6Fk2ZVf/BHLQBmgqlhRth4cShGkVjePMgBY=;
	24:jbXZuqpW5P5unJbMxpIM2vIasOMX5NnCmqFU/V+xInzJrhk32OTbECOullpA/7pzdpLNXLWS+brK9fgnCTBX/tKfAa1mAHt7xi4vlxAqEj8=;
	7:3vAbHxe5n2TJyjcpxAFfFi+0Zfv1AMS9CDl2Ebp/uegGX0GyQVnV3YJjN5lTFsGJJz99Am2OmmKVjByjlEHoAB/CnjaHImQpsxCMyrSRuMOOgQ5CtvpHnTXlJswb2OZFDKDyERrTzz9+0Onvg1vNWcVWIsFCstgv9PX1mWYh0kuEqlT3AYE7vYKbkgeX0NzC8nPLuvCngsz0Efm6ZsvS+ja1gOz/n9/4nJLwj8QToI6l0IYgURVa7cS3vbw/q0ZD
SpamDiagnosticOutput: 1:99
SpamDiagnosticMetadata: NSPM
X-Microsoft-Exchange-Diagnostics: 1; CY4PR12MB1733;
	20:sbYGRqAfhrCFlFpKT9IHfPyPXpyp2CMUqOjkZbm7V2dKkYAAYYIT04oYWE7Z3ugZTTW3fbPYfpL7Ay9jrGvbAWYeYQE04blWwss+zcd89rf1Z3AcnyrKSlRLvjwHFUzC/R/XauFbi7BAR+FYwO2oaYe/3itdYRR144L3yotv4Sf81jOusiQ9XVLcyHOJjmbKuMloTXnoLZwf0ayKM+gzDujgTHvqE5PiHNvuei2nf/+30erX+kzo9dSt1COyi2jB
X-OriginatorOrg: amd.com
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 22 Jan 2018 04:30:12.5548
	(UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: 9e902bdf-aac8-495e-5ac2-08d56150d57c
X-MS-Exchange-CrossTenant-FromEntityHeader: Hosted
X-MS-Exchange-CrossTenant-Id: 3dd8961f-e488-4e60-8e11-a82d994e183d
X-MS-Exchange-Transport-CrossTenantHeadersStamped: CY4PR12MB1733
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2908">Suthikulpanit, Suravee</a> - Jan. 22, 2018, 4:29 a.m.</div>
<pre class="content">
VFIO IOMMU type1 currently upmaps IOVA pages synchronously, which requires
IOTLB flushing for every unmapping. This results in large IOTLB flushing
overhead when handling pass-through devices has a large number of mapped
IOVAs (e.g. GPUs). This could also cause IOTLB invalidate time-out issue
on AMD IOMMU with certain dGPUs.

This can be avoided by using the new IOTLB flushing interface.

Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;
Cc: Joerg Roedel &lt;jroedel@suse.de&gt;
<span class="signed-off-by">Signed-off-by: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;</span>
---
Changes from V2: (https://lkml.org/lkml/2017/12/27/43)

  * In vfio_unmap_unpin(), fallback to use slow IOTLB flush
    when fast IOTLB flush fails (per Alex).

  * Do not adopt fast IOTLB flush in map_try_harder().

  * Submit VFIO and AMD IOMMU patches separately.

 drivers/vfio/vfio_iommu_type1.c | 98 +++++++++++++++++++++++++++++++++++++++++
 1 file changed, 98 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7781">Alex Williamson</a> - Jan. 23, 2018, 10:04 p.m.</div>
<pre class="content">
Hi Suravee,

On Sun, 21 Jan 2018 23:29:37 -0500
Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt; wrote:
<span class="quote">
&gt; VFIO IOMMU type1 currently upmaps IOVA pages synchronously, which requires</span>
<span class="quote">&gt; IOTLB flushing for every unmapping. This results in large IOTLB flushing</span>
<span class="quote">&gt; overhead when handling pass-through devices has a large number of mapped</span>
<span class="quote">&gt; IOVAs (e.g. GPUs). This could also cause IOTLB invalidate time-out issue</span>
<span class="quote">&gt; on AMD IOMMU with certain dGPUs.</span>

Nit, GPUs don&#39;t imply a larger number of mapped IOVAs than any
other type of device, QEMU statically maps the entire VM address space
regardless of the device type.  I am curious though which dGPUs and what
size guests experience this problem, I&#39;d also like to have those
details in the commit log.  Is there any performance data to go along
with this?
<span class="quote">
&gt; This can be avoided by using the new IOTLB flushing interface.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Joerg Roedel &lt;jroedel@suse.de&gt;</span>
<span class="quote">&gt; Signed-off-by: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; Changes from V2: (https://lkml.org/lkml/2017/12/27/43)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   * In vfio_unmap_unpin(), fallback to use slow IOTLB flush</span>
<span class="quote">&gt;     when fast IOTLB flush fails (per Alex).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   * Do not adopt fast IOTLB flush in map_try_harder().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   * Submit VFIO and AMD IOMMU patches separately.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  drivers/vfio/vfio_iommu_type1.c | 98 +++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  1 file changed, 98 insertions(+)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; index e30e29a..5c40b00 100644</span>
<span class="quote">&gt; --- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; +++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; @@ -102,6 +102,13 @@ struct vfio_pfn {</span>
<span class="quote">&gt;  	atomic_t		ref_count;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +struct vfio_regions {</span>
<span class="quote">&gt; +	struct list_head list;</span>
<span class="quote">&gt; +	dma_addr_t iova;</span>
<span class="quote">&gt; +	phys_addr_t phys;</span>
<span class="quote">&gt; +	size_t len;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)	\</span>
<span class="quote">&gt;  					(!list_empty(&amp;iommu-&gt;domain_list))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -479,6 +486,36 @@ static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,</span>
<span class="quote">&gt;  	return unlocked;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="quote">&gt; + * Therefore, when using IOTLB flush sync interface, VFIO need to keep track</span>
<span class="quote">&gt; + * of these regions (currently using a list).</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This value specifies maximum number of regions for each IOTLB flush sync.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define VFIO_IOMMU_TLB_SYNC_MAX		512</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static long vfio_sync_unpin(struct vfio_dma *dma, struct vfio_domain *domain,</span>
<span class="quote">&gt; +				struct list_head *regions)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	long unlocked = 0;</span>
<span class="quote">&gt; +	struct vfio_regions *entry, *next;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	iommu_tlb_sync(domain-&gt;domain);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	list_for_each_entry_safe(entry, next, regions, list) {</span>
<span class="quote">&gt; +		unlocked += vfio_unpin_pages_remote(dma,</span>
<span class="quote">&gt; +						    entry-&gt;iova,</span>
<span class="quote">&gt; +						    entry-&gt;phys &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						    entry-&gt;len &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						    false);</span>
<span class="quote">&gt; +		list_del(&amp;entry-&gt;list);</span>
<span class="quote">&gt; +		kfree(entry);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	cond_resched();</span>
<span class="quote">&gt; +	return unlocked;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static int vfio_pin_page_external(struct vfio_dma *dma, unsigned long vaddr,</span>
<span class="quote">&gt;  				  unsigned long *pfn_base, bool do_accounting)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -648,12 +685,40 @@ static int vfio_iommu_type1_unpin_pages(void *iommu_data,</span>
<span class="quote">&gt;  	return i &gt; npage ? npage : (i &gt; 0 ? i : -EINVAL);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static size_t try_unmap_unpin_fast(struct vfio_domain *domain, dma_addr_t iova,</span>
<span class="quote">&gt; +				   size_t len, phys_addr_t phys,</span>
<span class="quote">&gt; +				   struct list_head *unmapped_regions)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vfio_regions *entry;</span>
<span class="quote">&gt; +	size_t unmapped;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="quote">&gt; +	if (!entry)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>

size_t is unsigned, so pushing -ENOMEM out though this unsigned
function and the callee interpreting it as unsigned, means we&#39;re going
to see this as a very large unmap, not an error condition.  Looks like
the IOMMU API has problems in this space too, ex. iommu_unmap(), Joerg?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	unmapped = iommu_unmap_fast(domain-&gt;domain, iova, len);</span>
<span class="quote">&gt; +	if (WARN_ON(!unmapped)) {</span>
<span class="quote">&gt; +		kfree(entry);</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	}</span>

Not sure about the handling of this, the zero check is just a
consistency validation.  If there&#39;s nothing mapped where we think there
should be something mapped, we warn and throw out the whole vfio_dma.
After this patch, such an error gets warned twice, which doesn&#39;t
really seem to be an improvement.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="quote">&gt; +	entry-&gt;iova = iova;</span>
<span class="quote">&gt; +	entry-&gt;phys = phys;</span>
<span class="quote">&gt; +	entry-&gt;len  = unmapped;</span>
<span class="quote">&gt; +	list_add_tail(&amp;entry-&gt;list, unmapped_regions);</span>
<span class="quote">&gt; +	return unmapped;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  			     bool do_accounting)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	dma_addr_t iova = dma-&gt;iova, end = dma-&gt;iova + dma-&gt;size;</span>
<span class="quote">&gt;  	struct vfio_domain *domain, *d;</span>
<span class="quote">&gt; +	struct list_head unmapped_regions;</span>
<span class="quote">&gt; +	bool use_fastpath = true;</span>
<span class="quote">&gt;  	long unlocked = 0;</span>
<span class="quote">&gt; +	int cnt = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!dma-&gt;size)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; @@ -661,6 +726,8 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  	if (!IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	INIT_LIST_HEAD(&amp;unmapped_regions);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * We use the IOMMU to track the physical addresses, otherwise we&#39;d</span>
<span class="quote">&gt;  	 * need a much more complicated tracking system.  Unfortunately that</span>
<span class="quote">&gt; @@ -698,6 +765,33 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * First, try to use fast unmap/unpin. In case of failure,</span>
<span class="quote">&gt; +		 * sync upto the current point, and continue the slow</span>
<span class="quote">&gt; +		 * unmap/unpin path.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (use_fastpath) {</span>
<span class="quote">&gt; +			unmapped = try_unmap_unpin_fast(domain, iova,</span>
<span class="quote">&gt; +							len, phys,</span>
<span class="quote">&gt; +							&amp;unmapped_regions);</span>
<span class="quote">&gt; +			if (unmapped &gt; 0) {</span>
<span class="quote">&gt; +				iova += unmapped;</span>
<span class="quote">&gt; +				cnt++;</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				use_fastpath = false;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (cnt &gt;= VFIO_IOMMU_TLB_SYNC_MAX || !use_fastpath) {</span>
<span class="quote">&gt; +				unlocked += vfio_sync_unpin(dma, domain,</span>
<span class="quote">&gt; +							    &amp;unmapped_regions);</span>
<span class="quote">&gt; +				cnt = 0;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (use_fastpath)</span>
<span class="quote">&gt; +				continue;</span>
<span class="quote">&gt; +		}</span>

It seems sufficient on a fast path failure to sync and unpin the list
and continue.  It would probably be wise to single (slow) step the
failing entry to guarantee forward progress, but I don&#39;t see an
advantage to switching to only slow mode for the full remainder of the
vfio_dma.  Thanks,

Alex
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		/* Slow unmap/unpin path */</span>
<span class="quote">&gt;  		unmapped = iommu_unmap(domain-&gt;domain, iova, len);</span>
<span class="quote">&gt;  		if (WARN_ON(!unmapped))</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; @@ -712,6 +806,10 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	dma-&gt;iommu_mapped = false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (cnt)</span>
<span class="quote">&gt; +		unlocked += vfio_sync_unpin(dma, domain, &amp;unmapped_regions);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (do_accounting) {</span>
<span class="quote">&gt;  		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);</span>
<span class="quote">&gt;  		return 0;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2908">Suthikulpanit, Suravee</a> - Jan. 24, 2018, 4:38 a.m.</div>
<pre class="content">
Alex/Joerg,

On 1/24/18 5:04 AM, Alex Williamson wrote:
<span class="quote">&gt;&gt; +static size_t try_unmap_unpin_fast(struct vfio_domain *domain, dma_addr_t iova,</span>
<span class="quote">&gt;&gt; +				   size_t len, phys_addr_t phys,</span>
<span class="quote">&gt;&gt; +				   struct list_head *unmapped_regions)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct vfio_regions *entry;</span>
<span class="quote">&gt;&gt; +	size_t unmapped;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="quote">&gt;&gt; +	if (!entry)</span>
<span class="quote">&gt;&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; size_t is unsigned, so pushing -ENOMEM out though this unsigned</span>
<span class="quote">&gt; function and the callee interpreting it as unsigned, means we&#39;re going</span>
<span class="quote">&gt; to see this as a very large unmap, not an error condition.  Looks like</span>
<span class="quote">&gt; the IOMMU API has problems in this space too, ex. iommu_unmap(), Joerg?</span>
<span class="quote">&gt; </span>


I can clean up the APIs to use ssize_t, where it can return errors.

Suravee
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2908">Suthikulpanit, Suravee</a> - Jan. 24, 2018, 10 a.m.</div>
<pre class="content">
Alex / Joerg,

On 1/24/18 5:04 AM, Alex Williamson wrote:
<span class="quote">&gt;&gt; @@ -648,12 +685,40 @@ static int vfio_iommu_type1_unpin_pages(void *iommu_data,</span>
<span class="quote">&gt;&gt;   	return i &gt; npage ? npage : (i &gt; 0 ? i : -EINVAL);</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;   </span>
<span class="quote">&gt;&gt; +static size_t try_unmap_unpin_fast(struct vfio_domain *domain, dma_addr_t iova,</span>
<span class="quote">&gt;&gt; +				   size_t len, phys_addr_t phys,</span>
<span class="quote">&gt;&gt; +				   struct list_head *unmapped_regions)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct vfio_regions *entry;</span>
<span class="quote">&gt;&gt; +	size_t unmapped;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="quote">&gt;&gt; +	if (!entry)</span>
<span class="quote">&gt;&gt; +		return -ENOMEM;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	unmapped = iommu_unmap_fast(domain-&gt;domain, iova, len);</span>
<span class="quote">&gt;&gt; +	if (WARN_ON(!unmapped)) {</span>
<span class="quote">&gt;&gt; +		kfree(entry);</span>
<span class="quote">&gt;&gt; +		return -EINVAL;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt; Not sure about the handling of this, the zero check is just a</span>
<span class="quote">&gt; consistency validation.  If there&#39;s nothing mapped where we think there</span>
<span class="quote">&gt; should be something mapped, we warn and throw out the whole vfio_dma.</span>
<span class="quote">&gt; After this patch, such an error gets warned twice, which doesn&#39;t</span>
<span class="quote">&gt; really seem to be an improvement.</span>
<span class="quote">&gt; </span>

Since iommu_unmap() and iommu_unmap_fast() can return errors, instead of just zero check,
we should also check for errors, warn, and bail out the whole vfio_dma.

Thanks,
Suravee
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">index e30e29a..5c40b00 100644</span>
<span class="p_header">--- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">+++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_chunk">@@ -102,6 +102,13 @@</span> <span class="p_context"> struct vfio_pfn {</span>
 	atomic_t		ref_count;
 };
 
<span class="p_add">+struct vfio_regions {</span>
<span class="p_add">+	struct list_head list;</span>
<span class="p_add">+	dma_addr_t iova;</span>
<span class="p_add">+	phys_addr_t phys;</span>
<span class="p_add">+	size_t len;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #define IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)	\
 					(!list_empty(&amp;iommu-&gt;domain_list))
 
<span class="p_chunk">@@ -479,6 +486,36 @@</span> <span class="p_context"> static long vfio_unpin_pages_remote(struct vfio_dma *dma, dma_addr_t iova,</span>
 	return unlocked;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="p_add">+ * Therefore, when using IOTLB flush sync interface, VFIO need to keep track</span>
<span class="p_add">+ * of these regions (currently using a list).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This value specifies maximum number of regions for each IOTLB flush sync.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define VFIO_IOMMU_TLB_SYNC_MAX		512</span>
<span class="p_add">+</span>
<span class="p_add">+static long vfio_sync_unpin(struct vfio_dma *dma, struct vfio_domain *domain,</span>
<span class="p_add">+				struct list_head *regions)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long unlocked = 0;</span>
<span class="p_add">+	struct vfio_regions *entry, *next;</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu_tlb_sync(domain-&gt;domain);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry_safe(entry, next, regions, list) {</span>
<span class="p_add">+		unlocked += vfio_unpin_pages_remote(dma,</span>
<span class="p_add">+						    entry-&gt;iova,</span>
<span class="p_add">+						    entry-&gt;phys &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						    entry-&gt;len &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						    false);</span>
<span class="p_add">+		list_del(&amp;entry-&gt;list);</span>
<span class="p_add">+		kfree(entry);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	cond_resched();</span>
<span class="p_add">+	return unlocked;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int vfio_pin_page_external(struct vfio_dma *dma, unsigned long vaddr,
 				  unsigned long *pfn_base, bool do_accounting)
 {
<span class="p_chunk">@@ -648,12 +685,40 @@</span> <span class="p_context"> static int vfio_iommu_type1_unpin_pages(void *iommu_data,</span>
 	return i &gt; npage ? npage : (i &gt; 0 ? i : -EINVAL);
 }
 
<span class="p_add">+static size_t try_unmap_unpin_fast(struct vfio_domain *domain, dma_addr_t iova,</span>
<span class="p_add">+				   size_t len, phys_addr_t phys,</span>
<span class="p_add">+				   struct list_head *unmapped_regions)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vfio_regions *entry;</span>
<span class="p_add">+	size_t unmapped;</span>
<span class="p_add">+</span>
<span class="p_add">+	entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="p_add">+	if (!entry)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	unmapped = iommu_unmap_fast(domain-&gt;domain, iova, len);</span>
<span class="p_add">+	if (WARN_ON(!unmapped)) {</span>
<span class="p_add">+		kfree(entry);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu_tlb_range_add(domain-&gt;domain, iova, unmapped);</span>
<span class="p_add">+	entry-&gt;iova = iova;</span>
<span class="p_add">+	entry-&gt;phys = phys;</span>
<span class="p_add">+	entry-&gt;len  = unmapped;</span>
<span class="p_add">+	list_add_tail(&amp;entry-&gt;list, unmapped_regions);</span>
<span class="p_add">+	return unmapped;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,
 			     bool do_accounting)
 {
 	dma_addr_t iova = dma-&gt;iova, end = dma-&gt;iova + dma-&gt;size;
 	struct vfio_domain *domain, *d;
<span class="p_add">+	struct list_head unmapped_regions;</span>
<span class="p_add">+	bool use_fastpath = true;</span>
 	long unlocked = 0;
<span class="p_add">+	int cnt = 0;</span>
 
 	if (!dma-&gt;size)
 		return 0;
<span class="p_chunk">@@ -661,6 +726,8 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 	if (!IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu))
 		return 0;
 
<span class="p_add">+	INIT_LIST_HEAD(&amp;unmapped_regions);</span>
<span class="p_add">+</span>
 	/*
 	 * We use the IOMMU to track the physical addresses, otherwise we&#39;d
 	 * need a much more complicated tracking system.  Unfortunately that
<span class="p_chunk">@@ -698,6 +765,33 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 				break;
 		}
 
<span class="p_add">+		/*</span>
<span class="p_add">+		 * First, try to use fast unmap/unpin. In case of failure,</span>
<span class="p_add">+		 * sync upto the current point, and continue the slow</span>
<span class="p_add">+		 * unmap/unpin path.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (use_fastpath) {</span>
<span class="p_add">+			unmapped = try_unmap_unpin_fast(domain, iova,</span>
<span class="p_add">+							len, phys,</span>
<span class="p_add">+							&amp;unmapped_regions);</span>
<span class="p_add">+			if (unmapped &gt; 0) {</span>
<span class="p_add">+				iova += unmapped;</span>
<span class="p_add">+				cnt++;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				use_fastpath = false;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (cnt &gt;= VFIO_IOMMU_TLB_SYNC_MAX || !use_fastpath) {</span>
<span class="p_add">+				unlocked += vfio_sync_unpin(dma, domain,</span>
<span class="p_add">+							    &amp;unmapped_regions);</span>
<span class="p_add">+				cnt = 0;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (use_fastpath)</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Slow unmap/unpin path */</span>
 		unmapped = iommu_unmap(domain-&gt;domain, iova, len);
 		if (WARN_ON(!unmapped))
 			break;
<span class="p_chunk">@@ -712,6 +806,10 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 	}
 
 	dma-&gt;iommu_mapped = false;
<span class="p_add">+</span>
<span class="p_add">+	if (cnt)</span>
<span class="p_add">+		unlocked += vfio_sync_unpin(dma, domain, &amp;unmapped_regions);</span>
<span class="p_add">+</span>
 	if (do_accounting) {
 		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);
 		return 0;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



