
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] Locking changes for v4.2 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] Locking changes for v4.2</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 22, 2015, 7:30 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20150622073015.GA1986@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6653671/mbox/"
   >mbox</a>
|
   <a href="/patch/6653671/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6653671/">/patch/6653671/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 868BA9F399
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 22 Jun 2015 07:31:10 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 8DB262062D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 22 Jun 2015 07:31:05 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 90BE320513
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 22 Jun 2015 07:31:00 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756131AbbFVHao (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 22 Jun 2015 03:30:44 -0400
Received: from mail-wg0-f51.google.com ([74.125.82.51]:36376 &quot;EHLO
	mail-wg0-f51.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1754498AbbFVHaX (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 22 Jun 2015 03:30:23 -0400
Received: by wguu7 with SMTP id u7so61136772wgu.3
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 22 Jun 2015 00:30:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version:content-type
	:content-disposition:user-agent;
	bh=+Bm+ImA7dn9XyguQxJEv63MD99vCZBTBgBUoRl7k5DM=;
	b=dr+11UACgVqcL0u6NAc1wRurPPHuL4vwOxHmBOicedpgO7SgPrWv9iLuq9Pc64E1BL
	7x8N3qCQaMoQC6PrSiJTeM4sECIZH72IpR7ulLsIJVbv+A/i/KrB+AE0wZ74HeSvM3RN
	KGksnCuvjLoPBaUo/EkToc7EbRvihGd2rxuOuxalKF3sxZbZjN8o/W1nKOWJOYtFqYRl
	+8IyFGoF4VaTd6dcsdRUnJcWnj55+MRdcBwcJ3ZV7Dck23ma5ard08711acviglisury
	bDAodTkJzSU4cjCh4LbWdbUCQOA4312pRZ1UdqljJ+u7SiL3iaXwiMSG+jFBDbNOl4Le
	Wqfw==
X-Received: by 10.194.77.211 with SMTP id u19mr46512434wjw.19.1434958219330; 
	Mon, 22 Jun 2015 00:30:19 -0700 (PDT)
Received: from gmail.com (54033495.catv.pool.telekom.hu. [84.3.52.149])
	by mx.google.com with ESMTPSA id
	tl3sm29096569wjc.20.2015.06.22.00.30.17
	(version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 22 Jun 2015 00:30:17 -0700 (PDT)
Date: Mon, 22 Jun 2015 09:30:15 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;Paul E. McKenney&quot; &lt;paulmck@us.ibm.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] Locking changes for v4.2
Message-ID: &lt;20150622073015.GA1986@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.23 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.2 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,UNPARSEABLE_RELAY
	autolearn=ham version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - June 22, 2015, 7:30 a.m.</div>
<pre class="content">
Linus,

Please pull the latest locking-core-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git locking-core-for-linus

   # HEAD: 68722101ec3a0e179408a13708dd020e04f54aab locking/lockdep: Remove hard coded array size dependency

The main changes are:

 - &#39;qspinlock&#39; support, enabled on x86: queued spinlocks - these are now the 
   spinlock variant used by x86 as they outperform ticket spinlocks in every 
   category. (Waiman Long)

 - &#39;pvqspinlock&#39; support on x86: paravirtualized variant of queued spinlocks. 
   (Waiman Long, Peter Zijlstra)

 - &#39;qrwlock&#39; support, enabled on x86: queued rwlocks. Similar to queued spinlocks, 
   they are now the variant used by x86:

     CONFIG_ARCH_USE_QUEUED_SPINLOCKS=y
     CONFIG_QUEUED_SPINLOCKS=y
     CONFIG_ARCH_USE_QUEUED_RWLOCKS=y
     CONFIG_QUEUED_RWLOCKS=y

 - various lockdep fixlets

 - various locking primitives cleanups, further WRITE_ONCE() propagation

 Thanks,

	Ingo

------------------&gt;
Borislav Petkov (1):
      lockdep: Do not break user-visible string

David Vrabel (1):
      locking/pvqspinlock, x86: Enable PV qspinlock for Xen

George Beshers (1):
      locking/lockdep: Remove hard coded array size dependency

Ingo Molnar (1):
      locking/pvqspinlock: Rename QUEUED_SPINLOCK to QUEUED_SPINLOCKS

Peter Zijlstra (2):
      locking/arch: Add WRITE_ONCE() to set_mb()
      locking/arch: Rename set_mb() to smp_store_mb()

Peter Zijlstra (Intel) (4):
      locking/qspinlock: Add pending bit
      locking/qspinlock: Optimize for smaller NR_CPUS
      locking/qspinlock: Revert to test-and-set on hypervisors
      locking/pvqspinlock, x86: Implement the paravirt qspinlock call patching

Preeti U Murthy (1):
      kernel: Replace reference to ASSIGN_ONCE() with WRITE_ONCE() in comment

Sebastian Andrzej Siewior (1):
      locking/rtmutex: Drop usage of __HAVE_ARCH_CMPXCHG

Thomas Gleixner (2):
      arch: Remove __ARCH_HAVE_CMPXCHG
      rtmutex: Warn if trylock is called from hard/softirq context

Waiman Long (10):
      locking/rwsem: Reduce spinlock contention in wakeup after up_read()/up_write()
      locking/qspinlock: Introduce a simple generic 4-byte queued spinlock
      locking/qspinlock, x86: Enable x86-64 to use queued spinlocks
      locking/qspinlock: Extract out code snippets for the next patch
      locking/qspinlock: Use a simple write to grab the lock
      locking/pvqspinlock: Implement simple paravirt support for the qspinlock
      locking/pvqspinlock, x86: Enable PV qspinlock for KVM
      locking/pvqspinlock: Replace xchg() by the more descriptive set_mb()
      locking/qrwlock: Rename QUEUE_RWLOCK to QUEUED_RWLOCKS
      locking/qrwlock: Don&#39;t contend with readers when setting _QW_WAITING


 Documentation/memory-barriers.txt         |   6 +-
 arch/alpha/include/asm/cmpxchg.h          |   2 -
 arch/arm/include/asm/barrier.h            |   2 +-
 arch/arm64/include/asm/barrier.h          |   2 +-
 arch/avr32/include/asm/cmpxchg.h          |   2 -
 arch/hexagon/include/asm/cmpxchg.h        |   1 -
 arch/ia64/include/asm/barrier.h           |   7 +-
 arch/ia64/include/uapi/asm/cmpxchg.h      |   2 -
 arch/m32r/include/asm/cmpxchg.h           |   2 -
 arch/m68k/include/asm/cmpxchg.h           |   1 -
 arch/metag/include/asm/barrier.h          |   2 +-
 arch/metag/include/asm/cmpxchg.h          |   2 -
 arch/mips/include/asm/barrier.h           |   4 +-
 arch/mips/include/asm/cmpxchg.h           |   2 -
 arch/parisc/include/asm/cmpxchg.h         |   2 -
 arch/powerpc/include/asm/barrier.h        |   2 +-
 arch/powerpc/include/asm/cmpxchg.h        |   1 -
 arch/s390/include/asm/barrier.h           |   2 +-
 arch/s390/include/asm/cmpxchg.h           |   2 -
 arch/score/include/asm/cmpxchg.h          |   2 -
 arch/sh/include/asm/barrier.h             |   2 +-
 arch/sh/include/asm/cmpxchg.h             |   2 -
 arch/sparc/include/asm/barrier_64.h       |   4 +-
 arch/sparc/include/asm/cmpxchg_32.h       |   1 -
 arch/sparc/include/asm/cmpxchg_64.h       |   2 -
 arch/tile/include/asm/atomic_64.h         |   3 -
 arch/x86/Kconfig                          |   5 +-
 arch/x86/include/asm/barrier.h            |   4 +-
 arch/x86/include/asm/cmpxchg.h            |   2 -
 arch/x86/include/asm/paravirt.h           |  29 +-
 arch/x86/include/asm/paravirt_types.h     |  10 +
 arch/x86/include/asm/qspinlock.h          |  57 ++++
 arch/x86/include/asm/qspinlock_paravirt.h |   6 +
 arch/x86/include/asm/spinlock.h           |   5 +
 arch/x86/include/asm/spinlock_types.h     |   4 +
 arch/x86/kernel/kvm.c                     |  43 +++
 arch/x86/kernel/paravirt-spinlocks.c      |  24 +-
 arch/x86/kernel/paravirt_patch_32.c       |  22 +-
 arch/x86/kernel/paravirt_patch_64.c       |  22 +-
 arch/x86/um/asm/barrier.h                 |   3 +-
 arch/x86/xen/spinlock.c                   |  64 +++-
 fs/select.c                               |   6 +-
 include/asm-generic/barrier.h             |   4 +-
 include/asm-generic/cmpxchg.h             |   3 -
 include/asm-generic/qspinlock.h           | 139 +++++++++
 include/asm-generic/qspinlock_types.h     |  79 +++++
 include/linux/compiler.h                  |   4 +-
 include/linux/lockdep.h                   |   4 +-
 include/linux/osq_lock.h                  |   5 +
 include/linux/sched.h                     |   8 +-
 kernel/Kconfig.locks                      |  13 +-
 kernel/futex.c                            |   2 +-
 kernel/locking/Makefile                   |   3 +-
 kernel/locking/lockdep.c                  |   3 +-
 kernel/locking/mcs_spinlock.h             |   1 +
 kernel/locking/qrwlock.c                  |  30 +-
 kernel/locking/qspinlock.c                | 473 ++++++++++++++++++++++++++++++
 kernel/locking/qspinlock_paravirt.h       | 325 ++++++++++++++++++++
 kernel/locking/rtmutex.c                  |  13 +-
 kernel/locking/rwsem-xadd.c               |  44 +++
 kernel/sched/wait.c                       |   4 +-
 61 files changed, 1423 insertions(+), 102 deletions(-)
 create mode 100644 arch/x86/include/asm/qspinlock.h
 create mode 100644 arch/x86/include/asm/qspinlock_paravirt.h
 create mode 100644 include/asm-generic/qspinlock.h
 create mode 100644 include/asm-generic/qspinlock_types.h
 create mode 100644 kernel/locking/qspinlock.c
 create mode 100644 kernel/locking/qspinlock_paravirt.h

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2423">Eric Dumazet</a> - June 26, 2015, 9:11 a.m.</div>
<pre class="content">
On Mon, 2015-06-22 at 09:30 +0200, Ingo Molnar wrote:
<span class="quote">&gt; Linus,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please pull the latest locking-core-for-linus git tree from:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git locking-core-for-linus</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;    # HEAD: 68722101ec3a0e179408a13708dd020e04f54aab locking/lockdep: Remove hard coded array size dependency</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The main changes are:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - &#39;qspinlock&#39; support, enabled on x86: queued spinlocks - these are now the </span>
<span class="quote">&gt;    spinlock variant used by x86 as they outperform ticket spinlocks in every </span>
<span class="quote">&gt;    category. (Waiman Long)</span>

Very interesting.

While rebasing http://www.kernelhub.org/?msg=737214&amp;p=2

I found that indeed qspinlock improved the scalability issue by a big
factor.

linux-4.1

lpaa23:~# pipebench -n 16
threads:16 sleep:30 pinned:1
total operations: 6977349, ops/sec 232578
6977349

lpaa23:~# pipebench -n 16
threads:16 sleep:30 pinned:1
total operations: 7326280, ops/sec 244209
7326280


Current Linus tree :

lpaa23:~# pipebench -n 16
threads:16 sleep:30 pinned:1
total operations: 15640802, ops/sec 521360
15640802

lpaa23:~# pipebench -n 16
threads:16 sleep:30 pinned:1
total operations: 15045022, ops/sec 501500
15045022


Adding fd_install() patch then :

lpaa23:~# pipebench -n 16
threads:16 sleep:30 pinned:1
total operations: 21471043, ops/sec 715701
21471043

lpaa23:~# pipebench -n 16
threads:16 sleep:30 pinned:1
total operations: 21068501, ops/sec 702283
21068501





--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60141">Waiman Long</a> - June 26, 2015, 5:27 p.m.</div>
<pre class="content">
On 06/26/2015 05:11 AM, Eric Dumazet wrote:
<span class="quote">&gt; On Mon, 2015-06-22 at 09:30 +0200, Ingo Molnar wrote:</span>
<span class="quote">&gt;&gt; Linus,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Please pull the latest locking-core-for-linus git tree from:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;     git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git locking-core-for-linus</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;     # HEAD: 68722101ec3a0e179408a13708dd020e04f54aab locking/lockdep: Remove hard coded array size dependency</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The main changes are:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   - &#39;qspinlock&#39; support, enabled on x86: queued spinlocks - these are now the</span>
<span class="quote">&gt;&gt;     spinlock variant used by x86 as they outperform ticket spinlocks in every</span>
<span class="quote">&gt;&gt;     category. (Waiman Long)</span>
<span class="quote">&gt; Very interesting.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; While rebasing http://www.kernelhub.org/?msg=737214&amp;p=2</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I found that indeed qspinlock improved the scalability issue by a big</span>
<span class="quote">&gt; factor.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; linux-4.1</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; lpaa23:~# pipebench -n 16</span>
<span class="quote">&gt; threads:16 sleep:30 pinned:1</span>
<span class="quote">&gt; total operations: 6977349, ops/sec 232578</span>
<span class="quote">&gt; 6977349</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; lpaa23:~# pipebench -n 16</span>
<span class="quote">&gt; threads:16 sleep:30 pinned:1</span>
<span class="quote">&gt; total operations: 7326280, ops/sec 244209</span>
<span class="quote">&gt; 7326280</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Current Linus tree :</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; lpaa23:~# pipebench -n 16</span>
<span class="quote">&gt; threads:16 sleep:30 pinned:1</span>
<span class="quote">&gt; total operations: 15640802, ops/sec 521360</span>
<span class="quote">&gt; 15640802</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; lpaa23:~# pipebench -n 16</span>
<span class="quote">&gt; threads:16 sleep:30 pinned:1</span>
<span class="quote">&gt; total operations: 15045022, ops/sec 501500</span>
<span class="quote">&gt; 15045022</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Adding fd_install() patch then :</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; lpaa23:~# pipebench -n 16</span>
<span class="quote">&gt; threads:16 sleep:30 pinned:1</span>
<span class="quote">&gt; total operations: 21471043, ops/sec 715701</span>
<span class="quote">&gt; 21471043</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; lpaa23:~# pipebench -n 16</span>
<span class="quote">&gt; threads:16 sleep:30 pinned:1</span>
<span class="quote">&gt; total operations: 21068501, ops/sec 702283</span>
<span class="quote">&gt; 21068501</span>

I am glad that the qspinlock patch helps. May I know what kind of 
machine (cpu type, # of sockets, etc) that you are running the test on? 
Usually the more sockets the machine has, the bigger the improvement you 
will see.

Cheers,
Longman
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2423">Eric Dumazet</a> - June 26, 2015, 7:57 p.m.</div>
<pre class="content">
On Fri, 2015-06-26 at 13:27 -0400, Waiman Long wrote:
<span class="quote">
&gt; I am glad that the qspinlock patch helps. May I know what kind of </span>
<span class="quote">&gt; machine (cpu type, # of sockets, etc) that you are running the test on? </span>
<span class="quote">&gt; Usually the more sockets the machine has, the bigger the improvement you </span>
<span class="quote">&gt; will see.</span>

This particular host had two Intel(R) Xeon(R) CPU E5-2696 v2 @ 2.50GHz

(12 cores per socket, total of 48 hyper threads)

Note that if pipebench uses 48 threads (instead of 16 as my previous
mail did), there is almost no degradation.

lpaa23:~# ./pipebench -n 16
threads:16 sleep:30 pinned:1
total operations: 20228142, ops/sec 674271
20228142

lpaa23:~# ./pipebench -n 48
threads:48 sleep:30 pinned:1
total operations: 19763616, ops/sec 658787
19763616


Thanks

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/memory-barriers.txt b/Documentation/memory-barriers.txt</span>
<span class="p_header">index f95746189b5d..fe4020e4b468 100644</span>
<span class="p_header">--- a/Documentation/memory-barriers.txt</span>
<span class="p_header">+++ b/Documentation/memory-barriers.txt</span>
<span class="p_chunk">@@ -1662,7 +1662,7 @@</span> <span class="p_context"> CPU from reordering them.</span>
 
 There are some more advanced barrier functions:
 
<span class="p_del">- (*) set_mb(var, value)</span>
<span class="p_add">+ (*) smp_store_mb(var, value)</span>
 
      This assigns the value to the variable and then inserts a full memory
      barrier after it, depending on the function.  It isn&#39;t guaranteed to
<span class="p_chunk">@@ -1975,7 +1975,7 @@</span> <span class="p_context"> A general memory barrier is interpolated automatically by set_current_state()</span>
 	CPU 1
 	===============================
 	set_current_state();
<span class="p_del">-	  set_mb();</span>
<span class="p_add">+	  smp_store_mb();</span>
 	    STORE current-&gt;state
 	    &lt;general barrier&gt;
 	LOAD event_indicated
<span class="p_chunk">@@ -2016,7 +2016,7 @@</span> <span class="p_context"> something up.  The barrier occurs before the task state is cleared, and so sits</span>
 	CPU 1				CPU 2
 	===============================	===============================
 	set_current_state();		STORE event_indicated
<span class="p_del">-	  set_mb();			wake_up();</span>
<span class="p_add">+	  smp_store_mb();		wake_up();</span>
 	    STORE current-&gt;state	  &lt;write barrier&gt;
 	    &lt;general barrier&gt;		  STORE current-&gt;state
 	LOAD event_indicated
<span class="p_header">diff --git a/arch/alpha/include/asm/cmpxchg.h b/arch/alpha/include/asm/cmpxchg.h</span>
<span class="p_header">index 429e8cd0d78e..e5117766529e 100644</span>
<span class="p_header">--- a/arch/alpha/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/alpha/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -66,6 +66,4 @@</span> <span class="p_context"></span>
 #undef __ASM__MB
 #undef ____cmpxchg
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG 1</span>
<span class="p_del">-</span>
 #endif /* _ALPHA_CMPXCHG_H */
<span class="p_header">diff --git a/arch/arm/include/asm/barrier.h b/arch/arm/include/asm/barrier.h</span>
<span class="p_header">index d2f81e6b8c1c..6c2327e1c732 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/barrier.h</span>
<span class="p_chunk">@@ -81,7 +81,7 @@</span> <span class="p_context"> do {									\</span>
 #define read_barrier_depends()		do { } while(0)
 #define smp_read_barrier_depends()	do { } while(0)
 
<span class="p_del">-#define set_mb(var, value)	do { var = value; smp_mb(); } while (0)</span>
<span class="p_add">+#define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); smp_mb(); } while (0)</span>
 
 #define smp_mb__before_atomic()	smp_mb()
 #define smp_mb__after_atomic()	smp_mb()
<span class="p_header">diff --git a/arch/arm64/include/asm/barrier.h b/arch/arm64/include/asm/barrier.h</span>
<span class="p_header">index 71f19c4dc0de..0fa47c4275cb 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/barrier.h</span>
<span class="p_chunk">@@ -114,7 +114,7 @@</span> <span class="p_context"> do {									\</span>
 #define read_barrier_depends()		do { } while(0)
 #define smp_read_barrier_depends()	do { } while(0)
 
<span class="p_del">-#define set_mb(var, value)	do { var = value; smp_mb(); } while (0)</span>
<span class="p_add">+#define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); smp_mb(); } while (0)</span>
 #define nop()		asm volatile(&quot;nop&quot;);
 
 #define smp_mb__before_atomic()	smp_mb()
<span class="p_header">diff --git a/arch/avr32/include/asm/cmpxchg.h b/arch/avr32/include/asm/cmpxchg.h</span>
<span class="p_header">index 962a6aeab787..366bbeaeb405 100644</span>
<span class="p_header">--- a/arch/avr32/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/avr32/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -70,8 +70,6 @@</span> <span class="p_context"> extern unsigned long __cmpxchg_u64_unsupported_on_32bit_kernels(</span>
    if something tries to do an invalid cmpxchg().  */
 extern void __cmpxchg_called_with_bad_pointer(void);
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG 1</span>
<span class="p_del">-</span>
 static inline unsigned long __cmpxchg(volatile void *ptr, unsigned long old,
 				      unsigned long new, int size)
 {
<span class="p_header">diff --git a/arch/hexagon/include/asm/cmpxchg.h b/arch/hexagon/include/asm/cmpxchg.h</span>
<span class="p_header">index 9e7802911a57..a6e34e2acbba 100644</span>
<span class="p_header">--- a/arch/hexagon/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/hexagon/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -64,7 +64,6 @@</span> <span class="p_context"> static inline unsigned long __xchg(unsigned long x, volatile void *ptr,</span>
  *  looks just like atomic_cmpxchg on our arch currently with a bunch of
  *  variable casting.
  */
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG 1</span>
 
 #define cmpxchg(ptr, old, new)					\
 ({								\
<span class="p_header">diff --git a/arch/ia64/include/asm/barrier.h b/arch/ia64/include/asm/barrier.h</span>
<span class="p_header">index f6769eb2bbf9..843ba435e43b 100644</span>
<span class="p_header">--- a/arch/ia64/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/ia64/include/asm/barrier.h</span>
<span class="p_chunk">@@ -77,12 +77,7 @@</span> <span class="p_context"> do {									\</span>
 	___p1;								\
 })
 
<span class="p_del">-/*</span>
<span class="p_del">- * XXX check on this ---I suspect what Linus really wants here is</span>
<span class="p_del">- * acquire vs release semantics but we can&#39;t discuss this stuff with</span>
<span class="p_del">- * Linus just yet.  Grrr...</span>
<span class="p_del">- */</span>
<span class="p_del">-#define set_mb(var, value)	do { (var) = (value); mb(); } while (0)</span>
<span class="p_add">+#define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); mb(); } while (0)</span>
 
 /*
  * The group barrier in front of the rsm &amp; ssm are necessary to ensure
<span class="p_header">diff --git a/arch/ia64/include/uapi/asm/cmpxchg.h b/arch/ia64/include/uapi/asm/cmpxchg.h</span>
<span class="p_header">index f35109b1d907..a0e3620f8f13 100644</span>
<span class="p_header">--- a/arch/ia64/include/uapi/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/ia64/include/uapi/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -61,8 +61,6 @@</span> <span class="p_context"> extern void ia64_xchg_called_with_bad_pointer(void);</span>
  * indicated by comparing RETURN with OLD.
  */
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG 1</span>
<span class="p_del">-</span>
 /*
  * This function doesn&#39;t exist, so you&#39;ll get a linker error
  * if something tries to do an invalid cmpxchg().
<span class="p_header">diff --git a/arch/m32r/include/asm/cmpxchg.h b/arch/m32r/include/asm/cmpxchg.h</span>
<span class="p_header">index de651db20b43..14bf9b739dd2 100644</span>
<span class="p_header">--- a/arch/m32r/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/m32r/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -107,8 +107,6 @@</span> <span class="p_context"> __xchg_local(unsigned long x, volatile void *ptr, int size)</span>
 	((__typeof__(*(ptr)))__xchg_local((unsigned long)(x), (ptr),	\
 			sizeof(*(ptr))))
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG	1</span>
<span class="p_del">-</span>
 static inline unsigned long
 __cmpxchg_u32(volatile unsigned int *p, unsigned int old, unsigned int new)
 {
<span class="p_header">diff --git a/arch/m68k/include/asm/cmpxchg.h b/arch/m68k/include/asm/cmpxchg.h</span>
<span class="p_header">index bc755bc620ad..83b1df80f0ac 100644</span>
<span class="p_header">--- a/arch/m68k/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/m68k/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -90,7 +90,6 @@</span> <span class="p_context"> extern unsigned long __invalid_cmpxchg_size(volatile void *,</span>
  * indicated by comparing RETURN with OLD.
  */
 #ifdef CONFIG_RMW_INSNS
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG	1</span>
 
 static inline unsigned long __cmpxchg(volatile void *p, unsigned long old,
 				      unsigned long new, int size)
<span class="p_header">diff --git a/arch/metag/include/asm/barrier.h b/arch/metag/include/asm/barrier.h</span>
<span class="p_header">index d703d8e26a65..5a696e507930 100644</span>
<span class="p_header">--- a/arch/metag/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/metag/include/asm/barrier.h</span>
<span class="p_chunk">@@ -84,7 +84,7 @@</span> <span class="p_context"> static inline void fence(void)</span>
 #define read_barrier_depends()		do { } while (0)
 #define smp_read_barrier_depends()	do { } while (0)
 
<span class="p_del">-#define set_mb(var, value) do { var = value; smp_mb(); } while (0)</span>
<span class="p_add">+#define smp_store_mb(var, value) do { WRITE_ONCE(var, value); smp_mb(); } while (0)</span>
 
 #define smp_store_release(p, v)						\
 do {									\
<span class="p_header">diff --git a/arch/metag/include/asm/cmpxchg.h b/arch/metag/include/asm/cmpxchg.h</span>
<span class="p_header">index b1bc1be8540f..be29e3e44321 100644</span>
<span class="p_header">--- a/arch/metag/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/metag/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -51,8 +51,6 @@</span> <span class="p_context"> static inline unsigned long __cmpxchg(volatile void *ptr, unsigned long old,</span>
 	return old;
 }
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG 1</span>
<span class="p_del">-</span>
 #define cmpxchg(ptr, o, n)						\
 	({								\
 		__typeof__(*(ptr)) _o_ = (o);				\
<span class="p_header">diff --git a/arch/mips/include/asm/barrier.h b/arch/mips/include/asm/barrier.h</span>
<span class="p_header">index 2b8bbbcb9be0..7ecba84656d4 100644</span>
<span class="p_header">--- a/arch/mips/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/mips/include/asm/barrier.h</span>
<span class="p_chunk">@@ -112,8 +112,8 @@</span> <span class="p_context"></span>
 #define __WEAK_LLSC_MB		&quot;		\n&quot;
 #endif
 
<span class="p_del">-#define set_mb(var, value) \</span>
<span class="p_del">-	do { var = value; smp_mb(); } while (0)</span>
<span class="p_add">+#define smp_store_mb(var, value) \</span>
<span class="p_add">+	do { WRITE_ONCE(var, value); smp_mb(); } while (0)</span>
 
 #define smp_llsc_mb()	__asm__ __volatile__(__WEAK_LLSC_MB : : :&quot;memory&quot;)
 
<span class="p_header">diff --git a/arch/mips/include/asm/cmpxchg.h b/arch/mips/include/asm/cmpxchg.h</span>
<span class="p_header">index 412f945f1f5e..b71ab4a5fd50 100644</span>
<span class="p_header">--- a/arch/mips/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/mips/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -138,8 +138,6 @@</span> <span class="p_context"> static inline unsigned long __xchg(unsigned long x, volatile void * ptr, int siz</span>
 		__xchg((unsigned long)(x), (ptr), sizeof(*(ptr))));	\
 })
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG 1</span>
<span class="p_del">-</span>
 #define __cmpxchg_asm(ld, st, m, old, new)				\
 ({									\
 	__typeof(*(m)) __ret;						\
<span class="p_header">diff --git a/arch/parisc/include/asm/cmpxchg.h b/arch/parisc/include/asm/cmpxchg.h</span>
<span class="p_header">index dbd13354ec41..0a90b965cccb 100644</span>
<span class="p_header">--- a/arch/parisc/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/parisc/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -46,8 +46,6 @@</span> <span class="p_context"> __xchg(unsigned long x, __volatile__ void *ptr, int size)</span>
 #define xchg(ptr, x) \
 	((__typeof__(*(ptr)))__xchg((unsigned long)(x), (ptr), sizeof(*(ptr))))
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG	1</span>
<span class="p_del">-</span>
 /* bug catcher for when unsupported size is used - won&#39;t link */
 extern void __cmpxchg_called_with_bad_pointer(void);
 
<span class="p_header">diff --git a/arch/powerpc/include/asm/barrier.h b/arch/powerpc/include/asm/barrier.h</span>
<span class="p_header">index a3bf5be111ff..39505d660a70 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/barrier.h</span>
<span class="p_chunk">@@ -34,7 +34,7 @@</span> <span class="p_context"></span>
 #define rmb()  __asm__ __volatile__ (&quot;sync&quot; : : : &quot;memory&quot;)
 #define wmb()  __asm__ __volatile__ (&quot;sync&quot; : : : &quot;memory&quot;)
 
<span class="p_del">-#define set_mb(var, value)	do { var = value; mb(); } while (0)</span>
<span class="p_add">+#define smp_store_mb(var, value)	do { WRITE_ONCE(var, value); mb(); } while (0)</span>
 
 #ifdef __SUBARCH_HAS_LWSYNC
 #    define SMPWMB      LWSYNC
<span class="p_header">diff --git a/arch/powerpc/include/asm/cmpxchg.h b/arch/powerpc/include/asm/cmpxchg.h</span>
<span class="p_header">index d463c68fe7f0..ad6263cffb0f 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -144,7 +144,6 @@</span> <span class="p_context"> __xchg_local(volatile void *ptr, unsigned long x, unsigned int size)</span>
  * Compare and exchange - if *p == old, set it to new,
  * and return the old value of *p.
  */
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG	1</span>
 
 static __always_inline unsigned long
 __cmpxchg_u32(volatile unsigned int *p, unsigned long old, unsigned long new)
<span class="p_header">diff --git a/arch/s390/include/asm/barrier.h b/arch/s390/include/asm/barrier.h</span>
<span class="p_header">index 8d724718ec21..e6f8615a11eb 100644</span>
<span class="p_header">--- a/arch/s390/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/s390/include/asm/barrier.h</span>
<span class="p_chunk">@@ -36,7 +36,7 @@</span> <span class="p_context"></span>
 #define smp_mb__before_atomic()		smp_mb()
 #define smp_mb__after_atomic()		smp_mb()
 
<span class="p_del">-#define set_mb(var, value)		do { var = value; mb(); } while (0)</span>
<span class="p_add">+#define smp_store_mb(var, value)		do { WRITE_ONCE(var, value); mb(); } while (0)</span>
 
 #define smp_store_release(p, v)						\
 do {									\
<span class="p_header">diff --git a/arch/s390/include/asm/cmpxchg.h b/arch/s390/include/asm/cmpxchg.h</span>
<span class="p_header">index 4eadec466b8c..411464f4c97a 100644</span>
<span class="p_header">--- a/arch/s390/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/s390/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -32,8 +32,6 @@</span> <span class="p_context"></span>
 	__old;								\
 })
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG</span>
<span class="p_del">-</span>
 #define __cmpxchg_double_op(p1, p2, o1, o2, n1, n2, insn)		\
 ({									\
 	register __typeof__(*(p1)) __old1 asm(&quot;2&quot;) = (o1);		\
<span class="p_header">diff --git a/arch/score/include/asm/cmpxchg.h b/arch/score/include/asm/cmpxchg.h</span>
<span class="p_header">index f384839c3ee5..cc3f6420b71c 100644</span>
<span class="p_header">--- a/arch/score/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/score/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -42,8 +42,6 @@</span> <span class="p_context"> static inline unsigned long __cmpxchg(volatile unsigned long *m,</span>
 					(unsigned long)(o),	\
 					(unsigned long)(n)))
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG	1</span>
<span class="p_del">-</span>
 #include &lt;asm-generic/cmpxchg-local.h&gt;
 
 #endif /* _ASM_SCORE_CMPXCHG_H */
<span class="p_header">diff --git a/arch/sh/include/asm/barrier.h b/arch/sh/include/asm/barrier.h</span>
<span class="p_header">index 43715308b068..bf91037db4e0 100644</span>
<span class="p_header">--- a/arch/sh/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/sh/include/asm/barrier.h</span>
<span class="p_chunk">@@ -32,7 +32,7 @@</span> <span class="p_context"></span>
 #define ctrl_barrier()	__asm__ __volatile__ (&quot;nop;nop;nop;nop;nop;nop;nop;nop&quot;)
 #endif
 
<span class="p_del">-#define set_mb(var, value) do { (void)xchg(&amp;var, value); } while (0)</span>
<span class="p_add">+#define smp_store_mb(var, value) do { (void)xchg(&amp;var, value); } while (0)</span>
 
 #include &lt;asm-generic/barrier.h&gt;
 
<span class="p_header">diff --git a/arch/sh/include/asm/cmpxchg.h b/arch/sh/include/asm/cmpxchg.h</span>
<span class="p_header">index f6bd1406b897..85c97b188d71 100644</span>
<span class="p_header">--- a/arch/sh/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/sh/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -46,8 +46,6 @@</span> <span class="p_context"> extern void __xchg_called_with_bad_pointer(void);</span>
  * if something tries to do an invalid cmpxchg(). */
 extern void __cmpxchg_called_with_bad_pointer(void);
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG 1</span>
<span class="p_del">-</span>
 static inline unsigned long __cmpxchg(volatile void * ptr, unsigned long old,
 		unsigned long new, int size)
 {
<span class="p_header">diff --git a/arch/sparc/include/asm/barrier_64.h b/arch/sparc/include/asm/barrier_64.h</span>
<span class="p_header">index 76648941fea7..809941e33e12 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/barrier_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/barrier_64.h</span>
<span class="p_chunk">@@ -40,8 +40,8 @@</span> <span class="p_context"> do {	__asm__ __volatile__(&quot;ba,pt	%%xcc, 1f\n\t&quot; \</span>
 #define dma_rmb()	rmb()
 #define dma_wmb()	wmb()
 
<span class="p_del">-#define set_mb(__var, __value) \</span>
<span class="p_del">-	do { __var = __value; membar_safe(&quot;#StoreLoad&quot;); } while(0)</span>
<span class="p_add">+#define smp_store_mb(__var, __value) \</span>
<span class="p_add">+	do { WRITE_ONCE(__var, __value); membar_safe(&quot;#StoreLoad&quot;); } while(0)</span>
 
 #ifdef CONFIG_SMP
 #define smp_mb()	mb()
<span class="p_header">diff --git a/arch/sparc/include/asm/cmpxchg_32.h b/arch/sparc/include/asm/cmpxchg_32.h</span>
<span class="p_header">index d38b52dca216..83ffb83c5397 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/cmpxchg_32.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/cmpxchg_32.h</span>
<span class="p_chunk">@@ -34,7 +34,6 @@</span> <span class="p_context"> static inline unsigned long __xchg(unsigned long x, __volatile__ void * ptr, int</span>
  *
  * Cribbed from &lt;asm-parisc/atomic.h&gt;
  */
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG	1</span>
 
 /* bug catcher for when unsupported size is used - won&#39;t link */
 void __cmpxchg_called_with_bad_pointer(void);
<span class="p_header">diff --git a/arch/sparc/include/asm/cmpxchg_64.h b/arch/sparc/include/asm/cmpxchg_64.h</span>
<span class="p_header">index 0e1ed6cfbf68..faa2f61058c2 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/cmpxchg_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/cmpxchg_64.h</span>
<span class="p_chunk">@@ -65,8 +65,6 @@</span> <span class="p_context"> static inline unsigned long __xchg(unsigned long x, __volatile__ void * ptr,</span>
 
 #include &lt;asm-generic/cmpxchg-local.h&gt;
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG 1</span>
<span class="p_del">-</span>
 static inline unsigned long
 __cmpxchg_u32(volatile int *m, int old, int new)
 {
<span class="p_header">diff --git a/arch/tile/include/asm/atomic_64.h b/arch/tile/include/asm/atomic_64.h</span>
<span class="p_header">index 7b11c5fadd42..0496970cef82 100644</span>
<span class="p_header">--- a/arch/tile/include/asm/atomic_64.h</span>
<span class="p_header">+++ b/arch/tile/include/asm/atomic_64.h</span>
<span class="p_chunk">@@ -105,9 +105,6 @@</span> <span class="p_context"> static inline long atomic64_add_unless(atomic64_t *v, long a, long u)</span>
 
 #define atomic64_inc_not_zero(v)	atomic64_add_unless((v), 1, 0)
 
<span class="p_del">-/* Define this to indicate that cmpxchg is an efficient operation. */</span>
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG</span>
<span class="p_del">-</span>
 #endif /* !__ASSEMBLY__ */
 
 #endif /* _ASM_TILE_ATOMIC_64_H */
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 226d5696e1d1..4e986e809861 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -127,7 +127,8 @@</span> <span class="p_context"> config X86</span>
 	select MODULES_USE_ELF_RELA if X86_64
 	select CLONE_BACKWARDS if X86_32
 	select ARCH_USE_BUILTIN_BSWAP
<span class="p_del">-	select ARCH_USE_QUEUE_RWLOCK</span>
<span class="p_add">+	select ARCH_USE_QUEUED_SPINLOCKS</span>
<span class="p_add">+	select ARCH_USE_QUEUED_RWLOCKS</span>
 	select OLD_SIGSUSPEND3 if X86_32 || IA32_EMULATION
 	select OLD_SIGACTION if X86_32
 	select COMPAT_OLD_SIGACTION if IA32_EMULATION
<span class="p_chunk">@@ -666,7 +667,7 @@</span> <span class="p_context"> config PARAVIRT_DEBUG</span>
 config PARAVIRT_SPINLOCKS
 	bool &quot;Paravirtualization layer for spinlocks&quot;
 	depends on PARAVIRT &amp;&amp; SMP
<span class="p_del">-	select UNINLINE_SPIN_UNLOCK</span>
<span class="p_add">+	select UNINLINE_SPIN_UNLOCK if !QUEUED_SPINLOCKS</span>
 	---help---
 	  Paravirtualized spinlocks allow a pvops backend to replace the
 	  spinlock implementation with something virtualization-friendly
<span class="p_header">diff --git a/arch/x86/include/asm/barrier.h b/arch/x86/include/asm/barrier.h</span>
<span class="p_header">index 959e45b81fe2..e51a8f803f55 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/barrier.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/barrier.h</span>
<span class="p_chunk">@@ -35,12 +35,12 @@</span> <span class="p_context"></span>
 #define smp_mb()	mb()
 #define smp_rmb()	dma_rmb()
 #define smp_wmb()	barrier()
<span class="p_del">-#define set_mb(var, value) do { (void)xchg(&amp;var, value); } while (0)</span>
<span class="p_add">+#define smp_store_mb(var, value) do { (void)xchg(&amp;var, value); } while (0)</span>
 #else /* !SMP */
 #define smp_mb()	barrier()
 #define smp_rmb()	barrier()
 #define smp_wmb()	barrier()
<span class="p_del">-#define set_mb(var, value) do { var = value; barrier(); } while (0)</span>
<span class="p_add">+#define smp_store_mb(var, value) do { WRITE_ONCE(var, value); barrier(); } while (0)</span>
 #endif /* SMP */
 
 #define read_barrier_depends()		do { } while (0)
<span class="p_header">diff --git a/arch/x86/include/asm/cmpxchg.h b/arch/x86/include/asm/cmpxchg.h</span>
<span class="p_header">index 99c105d78b7e..ad19841eddfe 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -4,8 +4,6 @@</span> <span class="p_context"></span>
 #include &lt;linux/compiler.h&gt;
 #include &lt;asm/alternative.h&gt; /* Provides LOCK_PREFIX */
 
<span class="p_del">-#define __HAVE_ARCH_CMPXCHG 1</span>
<span class="p_del">-</span>
 /*
  * Non-existant functions to indicate usage errors at link time
  * (or compile-time if the compiler implements __compiletime_error().
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">index 8957810ad7d1..d143bfad45d7 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt.h</span>
<span class="p_chunk">@@ -712,6 +712,31 @@</span> <span class="p_context"> static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,</span>
 
 #if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_PARAVIRT_SPINLOCKS)
 
<span class="p_add">+#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,</span>
<span class="p_add">+							u32 val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	PVOP_VCALL2(pv_lock_ops.queued_spin_lock_slowpath, lock, val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void pv_queued_spin_unlock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	PVOP_VCALLEE1(pv_lock_ops.queued_spin_unlock, lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void pv_wait(u8 *ptr, u8 val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	PVOP_VCALL2(pv_lock_ops.wait, ptr, val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void pv_kick(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	PVOP_VCALL1(pv_lock_ops.kick, cpu);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_add">+</span>
 static __always_inline void __ticket_lock_spinning(struct arch_spinlock *lock,
 							__ticket_t ticket)
 {
<span class="p_chunk">@@ -724,7 +749,9 @@</span> <span class="p_context"> static __always_inline void __ticket_unlock_kick(struct arch_spinlock *lock,</span>
 	PVOP_VCALL2(pv_lock_ops.unlock_kick, lock, ticket);
 }
 
<span class="p_del">-#endif</span>
<span class="p_add">+#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* SMP &amp;&amp; PARAVIRT_SPINLOCKS */</span>
 
 #ifdef CONFIG_X86_32
 #define PV_SAVE_REGS &quot;pushl %ecx; pushl %edx;&quot;
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">index f7b0b5c112f2..8766c7c395c2 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_chunk">@@ -333,9 +333,19 @@</span> <span class="p_context"> struct arch_spinlock;</span>
 typedef u16 __ticket_t;
 #endif
 
<span class="p_add">+struct qspinlock;</span>
<span class="p_add">+</span>
 struct pv_lock_ops {
<span class="p_add">+#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+	void (*queued_spin_lock_slowpath)(struct qspinlock *lock, u32 val);</span>
<span class="p_add">+	struct paravirt_callee_save queued_spin_unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	void (*wait)(u8 *ptr, u8 val);</span>
<span class="p_add">+	void (*kick)(int cpu);</span>
<span class="p_add">+#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
 	struct paravirt_callee_save lock_spinning;
 	void (*unlock_kick)(struct arch_spinlock *lock, __ticket_t ticket);
<span class="p_add">+#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
 };
 
 /* This contains all the paravirt structures: we get a convenient
<span class="p_header">diff --git a/arch/x86/include/asm/qspinlock.h b/arch/x86/include/asm/qspinlock.h</span>
new file mode 100644
<span class="p_header">index 000000000000..9d51fae1cba3</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/qspinlock.h</span>
<span class="p_chunk">@@ -0,0 +1,57 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_X86_QSPINLOCK_H</span>
<span class="p_add">+#define _ASM_X86_QSPINLOCK_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cpufeature.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/qspinlock_types.h&gt;</span>
<span class="p_add">+#include &lt;asm/paravirt.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define	queued_spin_unlock queued_spin_unlock</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * queued_spin_unlock - release a queued spinlock</span>
<span class="p_add">+ * @lock : Pointer to queued spinlock structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * A smp_store_release() on the least-significant byte.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void native_queued_spin_unlock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	smp_store_release((u8 *)lock, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PARAVIRT_SPINLOCKS</span>
<span class="p_add">+extern void native_queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);</span>
<span class="p_add">+extern void __pv_init_lock_hash(void);</span>
<span class="p_add">+extern void __pv_queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);</span>
<span class="p_add">+extern void __raw_callee_save___pv_queued_spin_unlock(struct qspinlock *lock);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pv_queued_spin_lock_slowpath(lock, val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void queued_spin_unlock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pv_queued_spin_unlock(lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void queued_spin_unlock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	native_queued_spin_unlock(lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define virt_queued_spin_lock virt_queued_spin_lock</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool virt_queued_spin_lock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_HYPERVISOR))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (atomic_cmpxchg(&amp;lock-&gt;val, 0, _Q_LOCKED_VAL) != 0)</span>
<span class="p_add">+		cpu_relax();</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/qspinlock.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_QSPINLOCK_H */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/qspinlock_paravirt.h b/arch/x86/include/asm/qspinlock_paravirt.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b002e711ba88</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/qspinlock_paravirt.h</span>
<span class="p_chunk">@@ -0,0 +1,6 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef __ASM_QSPINLOCK_PARAVIRT_H</span>
<span class="p_add">+#define __ASM_QSPINLOCK_PARAVIRT_H</span>
<span class="p_add">+</span>
<span class="p_add">+PV_CALLEE_SAVE_REGS_THUNK(__pv_queued_spin_unlock);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/x86/include/asm/spinlock.h b/arch/x86/include/asm/spinlock.h</span>
<span class="p_header">index 64b611782ef0..be0a05913b91 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/spinlock.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/spinlock.h</span>
<span class="p_chunk">@@ -42,6 +42,10 @@</span> <span class="p_context"></span>
 extern struct static_key paravirt_ticketlocks_enabled;
 static __always_inline bool static_key_false(struct static_key *key);
 
<span class="p_add">+#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+#include &lt;asm/qspinlock.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+</span>
 #ifdef CONFIG_PARAVIRT_SPINLOCKS
 
 static inline void __ticket_enter_slowpath(arch_spinlock_t *lock)
<span class="p_chunk">@@ -196,6 +200,7 @@</span> <span class="p_context"> static inline void arch_spin_unlock_wait(arch_spinlock_t *lock)</span>
 		cpu_relax();
 	}
 }
<span class="p_add">+#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
 
 /*
  * Read-write spinlocks, allowing multiple readers
<span class="p_header">diff --git a/arch/x86/include/asm/spinlock_types.h b/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_header">index 5f9d7572d82b..65c3e37f879a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_chunk">@@ -23,6 +23,9 @@</span> <span class="p_context"> typedef u32 __ticketpair_t;</span>
 
 #define TICKET_SHIFT	(sizeof(__ticket_t) * 8)
 
<span class="p_add">+#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+#include &lt;asm-generic/qspinlock_types.h&gt;</span>
<span class="p_add">+#else</span>
 typedef struct arch_spinlock {
 	union {
 		__ticketpair_t head_tail;
<span class="p_chunk">@@ -33,6 +36,7 @@</span> <span class="p_context"> typedef struct arch_spinlock {</span>
 } arch_spinlock_t;
 
 #define __ARCH_SPIN_LOCK_UNLOCKED	{ { 0 } }
<span class="p_add">+#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
 
 #include &lt;asm-generic/qrwlock_types.h&gt;
 
<span class="p_header">diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c</span>
<span class="p_header">index 9435620062df..1681504e44a4 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvm.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvm.c</span>
<span class="p_chunk">@@ -584,6 +584,39 @@</span> <span class="p_context"> static void kvm_kick_cpu(int cpu)</span>
 	kvm_hypercall2(KVM_HC_KICK_CPU, flags, apicid);
 }
 
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/qspinlock.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static void kvm_wait(u8 *ptr, u8 val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (in_nmi())</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (READ_ONCE(*ptr) != val)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * halt until it&#39;s our turn and kicked. Note that we do safe halt</span>
<span class="p_add">+	 * for irq enabled case to avoid hang when lock info is overwritten</span>
<span class="p_add">+	 * in irq spinlock slowpath and no spurious interrupt occur to save us.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (arch_irqs_disabled_flags(flags))</span>
<span class="p_add">+		halt();</span>
<span class="p_add">+	else</span>
<span class="p_add">+		safe_halt();</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_add">+</span>
 enum kvm_contention_stat {
 	TAKEN_SLOW,
 	TAKEN_SLOW_PICKUP,
<span class="p_chunk">@@ -817,6 +850,8 @@</span> <span class="p_context"> static void kvm_unlock_kick(struct arch_spinlock *lock, __ticket_t ticket)</span>
 	}
 }
 
<span class="p_add">+#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_add">+</span>
 /*
  * Setup pv_lock_ops to exploit KVM_FEATURE_PV_UNHALT if present.
  */
<span class="p_chunk">@@ -828,8 +863,16 @@</span> <span class="p_context"> void __init kvm_spinlock_init(void)</span>
 	if (!kvm_para_has_feature(KVM_FEATURE_PV_UNHALT))
 		return;
 
<span class="p_add">+#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+	__pv_init_lock_hash();</span>
<span class="p_add">+	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;</span>
<span class="p_add">+	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);</span>
<span class="p_add">+	pv_lock_ops.wait = kvm_wait;</span>
<span class="p_add">+	pv_lock_ops.kick = kvm_kick_cpu;</span>
<span class="p_add">+#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
 	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(kvm_lock_spinning);
 	pv_lock_ops.unlock_kick = kvm_unlock_kick;
<span class="p_add">+#endif</span>
 }
 
 static __init int kvm_spinlock_init_jump(void)
<span class="p_header">diff --git a/arch/x86/kernel/paravirt-spinlocks.c b/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_header">index bbb6c7316341..33ee3e0efd65 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_chunk">@@ -8,11 +8,33 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/paravirt.h&gt;
 
<span class="p_add">+#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+__visible void __native_queued_spin_unlock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	native_queued_spin_unlock(lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+PV_CALLEE_SAVE_REGS_THUNK(__native_queued_spin_unlock);</span>
<span class="p_add">+</span>
<span class="p_add">+bool pv_is_native_spin_unlock(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pv_lock_ops.queued_spin_unlock.func ==</span>
<span class="p_add">+		__raw_callee_save___native_queued_spin_unlock;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 struct pv_lock_ops pv_lock_ops = {
 #ifdef CONFIG_SMP
<span class="p_add">+#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+	.queued_spin_lock_slowpath = native_queued_spin_lock_slowpath,</span>
<span class="p_add">+	.queued_spin_unlock = PV_CALLEE_SAVE(__native_queued_spin_unlock),</span>
<span class="p_add">+	.wait = paravirt_nop,</span>
<span class="p_add">+	.kick = paravirt_nop,</span>
<span class="p_add">+#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
 	.lock_spinning = __PV_IS_CALLEE_SAVE(paravirt_nop),
 	.unlock_kick = paravirt_nop,
<span class="p_del">-#endif</span>
<span class="p_add">+#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_add">+#endif /* SMP */</span>
 };
 EXPORT_SYMBOL(pv_lock_ops);
 
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_32.c b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">index d9f32e6d6ab6..e1b013696dde 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_chunk">@@ -12,6 +12,10 @@</span> <span class="p_context"> DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;mov %cr3, %eax&quot;);</span>
 DEF_NATIVE(pv_cpu_ops, clts, &quot;clts&quot;);
 DEF_NATIVE(pv_cpu_ops, read_tsc, &quot;rdtsc&quot;);
 
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%eax)&quot;);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 unsigned paravirt_patch_ident_32(void *insnbuf, unsigned len)
 {
 	/* arg in %eax, return in %eax */
<span class="p_chunk">@@ -24,6 +28,8 @@</span> <span class="p_context"> unsigned paravirt_patch_ident_64(void *insnbuf, unsigned len)</span>
 	return 0;
 }
 
<span class="p_add">+extern bool pv_is_native_spin_unlock(void);</span>
<span class="p_add">+</span>
 unsigned native_patch(u8 type, u16 clobbers, void *ibuf,
 		      unsigned long addr, unsigned len)
 {
<span class="p_chunk">@@ -47,14 +53,22 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_mmu_ops, write_cr3);
 		PATCH_SITE(pv_cpu_ops, clts);
 		PATCH_SITE(pv_cpu_ops, read_tsc);
<span class="p_del">-</span>
<span class="p_del">-	patch_site:</span>
<span class="p_del">-		ret = paravirt_patch_insns(ibuf, len, start, end);</span>
<span class="p_del">-		break;</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):</span>
<span class="p_add">+			if (pv_is_native_spin_unlock()) {</span>
<span class="p_add">+				start = start_pv_lock_ops_queued_spin_unlock;</span>
<span class="p_add">+				end   = end_pv_lock_ops_queued_spin_unlock;</span>
<span class="p_add">+				goto patch_site;</span>
<span class="p_add">+			}</span>
<span class="p_add">+#endif</span>
 
 	default:
 		ret = paravirt_patch_default(type, clobbers, ibuf, addr, len);
 		break;
<span class="p_add">+</span>
<span class="p_add">+patch_site:</span>
<span class="p_add">+		ret = paravirt_patch_insns(ibuf, len, start, end);</span>
<span class="p_add">+		break;</span>
 	}
 #undef PATCH_SITE
 	return ret;
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_64.c b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">index a1da6737ba5b..a1fa86782186 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_chunk">@@ -21,6 +21,10 @@</span> <span class="p_context"> DEF_NATIVE(pv_cpu_ops, swapgs, &quot;swapgs&quot;);</span>
 DEF_NATIVE(, mov32, &quot;mov %edi, %eax&quot;);
 DEF_NATIVE(, mov64, &quot;mov %rdi, %rax&quot;);
 
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%rdi)&quot;);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 unsigned paravirt_patch_ident_32(void *insnbuf, unsigned len)
 {
 	return paravirt_patch_insns(insnbuf, len,
<span class="p_chunk">@@ -33,6 +37,8 @@</span> <span class="p_context"> unsigned paravirt_patch_ident_64(void *insnbuf, unsigned len)</span>
 				    start__mov64, end__mov64);
 }
 
<span class="p_add">+extern bool pv_is_native_spin_unlock(void);</span>
<span class="p_add">+</span>
 unsigned native_patch(u8 type, u16 clobbers, void *ibuf,
 		      unsigned long addr, unsigned len)
 {
<span class="p_chunk">@@ -59,14 +65,22 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_cpu_ops, clts);
 		PATCH_SITE(pv_mmu_ops, flush_tlb_single);
 		PATCH_SITE(pv_cpu_ops, wbinvd);
<span class="p_del">-</span>
<span class="p_del">-	patch_site:</span>
<span class="p_del">-		ret = paravirt_patch_insns(ibuf, len, start, end);</span>
<span class="p_del">-		break;</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):</span>
<span class="p_add">+			if (pv_is_native_spin_unlock()) {</span>
<span class="p_add">+				start = start_pv_lock_ops_queued_spin_unlock;</span>
<span class="p_add">+				end   = end_pv_lock_ops_queued_spin_unlock;</span>
<span class="p_add">+				goto patch_site;</span>
<span class="p_add">+			}</span>
<span class="p_add">+#endif</span>
 
 	default:
 		ret = paravirt_patch_default(type, clobbers, ibuf, addr, len);
 		break;
<span class="p_add">+</span>
<span class="p_add">+patch_site:</span>
<span class="p_add">+		ret = paravirt_patch_insns(ibuf, len, start, end);</span>
<span class="p_add">+		break;</span>
 	}
 #undef PATCH_SITE
 	return ret;
<span class="p_header">diff --git a/arch/x86/um/asm/barrier.h b/arch/x86/um/asm/barrier.h</span>
<span class="p_header">index 7e8a1a650435..b9531d343134 100644</span>
<span class="p_header">--- a/arch/x86/um/asm/barrier.h</span>
<span class="p_header">+++ b/arch/x86/um/asm/barrier.h</span>
<span class="p_chunk">@@ -39,7 +39,8 @@</span> <span class="p_context"></span>
 #define smp_mb()	barrier()
 #define smp_rmb()	barrier()
 #define smp_wmb()	barrier()
<span class="p_del">-#define set_mb(var, value) do { var = value; barrier(); } while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define smp_store_mb(var, value) do { WRITE_ONCE(var, value); barrier(); } while (0)</span>
 
 #define read_barrier_depends()		do { } while (0)
 #define smp_read_barrier_depends()	do { } while (0)
<span class="p_header">diff --git a/arch/x86/xen/spinlock.c b/arch/x86/xen/spinlock.c</span>
<span class="p_header">index 956374c1edbc..9e2ba5c6e1dd 100644</span>
<span class="p_header">--- a/arch/x86/xen/spinlock.c</span>
<span class="p_header">+++ b/arch/x86/xen/spinlock.c</span>
<span class="p_chunk">@@ -17,6 +17,56 @@</span> <span class="p_context"></span>
 #include &quot;xen-ops.h&quot;
 #include &quot;debugfs.h&quot;
 
<span class="p_add">+static DEFINE_PER_CPU(int, lock_kicker_irq) = -1;</span>
<span class="p_add">+static DEFINE_PER_CPU(char *, irq_name);</span>
<span class="p_add">+static bool xen_pvspin = true;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/qspinlock.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static void xen_qlock_kick(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	xen_send_IPI_one(cpu, XEN_SPIN_UNLOCK_VECTOR);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Halt the current CPU &amp; release it back to the host</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void xen_qlock_wait(u8 *byte, u8 val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int irq = __this_cpu_read(lock_kicker_irq);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If kicker interrupts not initialized yet, just spin */</span>
<span class="p_add">+	if (irq == -1)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* clear pending */</span>
<span class="p_add">+	xen_clear_irq_pending(irq);</span>
<span class="p_add">+	barrier();</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We check the byte value after clearing pending IRQ to make sure</span>
<span class="p_add">+	 * that we won&#39;t miss a wakeup event because of the clearing.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The sync_clear_bit() call in xen_clear_irq_pending() is atomic.</span>
<span class="p_add">+	 * So it is effectively a memory barrier for x86.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (READ_ONCE(*byte) != val)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If an interrupt happens here, it will leave the wakeup irq</span>
<span class="p_add">+	 * pending, which will cause xen_poll_irq() to return</span>
<span class="p_add">+	 * immediately.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Block until irq becomes pending (or perhaps a spurious wakeup) */</span>
<span class="p_add">+	xen_poll_irq(irq);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_add">+</span>
 enum xen_contention_stat {
 	TAKEN_SLOW,
 	TAKEN_SLOW_PICKUP,
<span class="p_chunk">@@ -100,12 +150,9 @@</span> <span class="p_context"> struct xen_lock_waiting {</span>
 	__ticket_t want;
 };
 
<span class="p_del">-static DEFINE_PER_CPU(int, lock_kicker_irq) = -1;</span>
<span class="p_del">-static DEFINE_PER_CPU(char *, irq_name);</span>
 static DEFINE_PER_CPU(struct xen_lock_waiting, lock_waiting);
 static cpumask_t waiting_cpus;
 
<span class="p_del">-static bool xen_pvspin = true;</span>
 __visible void xen_lock_spinning(struct arch_spinlock *lock, __ticket_t want)
 {
 	int irq = __this_cpu_read(lock_kicker_irq);
<span class="p_chunk">@@ -217,6 +264,7 @@</span> <span class="p_context"> static void xen_unlock_kick(struct arch_spinlock *lock, __ticket_t next)</span>
 		}
 	}
 }
<span class="p_add">+#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
 
 static irqreturn_t dummy_handler(int irq, void *dev_id)
 {
<span class="p_chunk">@@ -280,8 +328,16 @@</span> <span class="p_context"> void __init xen_init_spinlocks(void)</span>
 		return;
 	}
 	printk(KERN_DEBUG &quot;xen: PV spinlocks enabled\n&quot;);
<span class="p_add">+#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+	__pv_init_lock_hash();</span>
<span class="p_add">+	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;</span>
<span class="p_add">+	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);</span>
<span class="p_add">+	pv_lock_ops.wait = xen_qlock_wait;</span>
<span class="p_add">+	pv_lock_ops.kick = xen_qlock_kick;</span>
<span class="p_add">+#else</span>
 	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(xen_lock_spinning);
 	pv_lock_ops.unlock_kick = xen_unlock_kick;
<span class="p_add">+#endif</span>
 }
 
 /*
<span class="p_chunk">@@ -310,7 +366,7 @@</span> <span class="p_context"> static __init int xen_parse_nopvspin(char *arg)</span>
 }
 early_param(&quot;xen_nopvspin&quot;, xen_parse_nopvspin);
 
<span class="p_del">-#ifdef CONFIG_XEN_DEBUG_FS</span>
<span class="p_add">+#if defined(CONFIG_XEN_DEBUG_FS) &amp;&amp; !defined(CONFIG_QUEUED_SPINLOCKS)</span>
 
 static struct dentry *d_spin_debug;
 
<span class="p_header">diff --git a/fs/select.c b/fs/select.c</span>
<span class="p_header">index f684c750e08a..015547330e88 100644</span>
<span class="p_header">--- a/fs/select.c</span>
<span class="p_header">+++ b/fs/select.c</span>
<span class="p_chunk">@@ -189,7 +189,7 @@</span> <span class="p_context"> static int __pollwake(wait_queue_t *wait, unsigned mode, int sync, void *key)</span>
 	 * doesn&#39;t imply write barrier and the users expect write
 	 * barrier semantics on wakeup functions.  The following
 	 * smp_wmb() is equivalent to smp_wmb() in try_to_wake_up()
<span class="p_del">-	 * and is paired with set_mb() in poll_schedule_timeout.</span>
<span class="p_add">+	 * and is paired with smp_store_mb() in poll_schedule_timeout.</span>
 	 */
 	smp_wmb();
 	pwq-&gt;triggered = 1;
<span class="p_chunk">@@ -244,7 +244,7 @@</span> <span class="p_context"> int poll_schedule_timeout(struct poll_wqueues *pwq, int state,</span>
 	/*
 	 * Prepare for the next iteration.
 	 *
<span class="p_del">-	 * The following set_mb() serves two purposes.  First, it&#39;s</span>
<span class="p_add">+	 * The following smp_store_mb() serves two purposes.  First, it&#39;s</span>
 	 * the counterpart rmb of the wmb in pollwake() such that data
 	 * written before wake up is always visible after wake up.
 	 * Second, the full barrier guarantees that triggered clearing
<span class="p_chunk">@@ -252,7 +252,7 @@</span> <span class="p_context"> int poll_schedule_timeout(struct poll_wqueues *pwq, int state,</span>
 	 * this problem doesn&#39;t exist for the first iteration as
 	 * add_wait_queue() has full barrier semantics.
 	 */
<span class="p_del">-	set_mb(pwq-&gt;triggered, 0);</span>
<span class="p_add">+	smp_store_mb(pwq-&gt;triggered, 0);</span>
 
 	return rc;
 }
<span class="p_header">diff --git a/include/asm-generic/barrier.h b/include/asm-generic/barrier.h</span>
<span class="p_header">index f5c40b0fadc2..e6a83d712ef6 100644</span>
<span class="p_header">--- a/include/asm-generic/barrier.h</span>
<span class="p_header">+++ b/include/asm-generic/barrier.h</span>
<span class="p_chunk">@@ -66,8 +66,8 @@</span> <span class="p_context"></span>
 #define smp_read_barrier_depends()	do { } while (0)
 #endif
 
<span class="p_del">-#ifndef set_mb</span>
<span class="p_del">-#define set_mb(var, value)  do { (var) = (value); mb(); } while (0)</span>
<span class="p_add">+#ifndef smp_store_mb</span>
<span class="p_add">+#define smp_store_mb(var, value)  do { WRITE_ONCE(var, value); mb(); } while (0)</span>
 #endif
 
 #ifndef smp_mb__before_atomic
<span class="p_header">diff --git a/include/asm-generic/cmpxchg.h b/include/asm-generic/cmpxchg.h</span>
<span class="p_header">index 811fb1e9b061..3766ab34aa45 100644</span>
<span class="p_header">--- a/include/asm-generic/cmpxchg.h</span>
<span class="p_header">+++ b/include/asm-generic/cmpxchg.h</span>
<span class="p_chunk">@@ -86,9 +86,6 @@</span> <span class="p_context"> unsigned long __xchg(unsigned long x, volatile void *ptr, int size)</span>
 
 /*
  * Atomic compare and exchange.
<span class="p_del">- *</span>
<span class="p_del">- * Do not define __HAVE_ARCH_CMPXCHG because we want to use it to check whether</span>
<span class="p_del">- * a cmpxchg primitive faster than repeated local irq save/restore exists.</span>
  */
 #include &lt;asm-generic/cmpxchg-local.h&gt;
 
<span class="p_header">diff --git a/include/asm-generic/qspinlock.h b/include/asm-generic/qspinlock.h</span>
new file mode 100644
<span class="p_header">index 000000000000..83bfb87f5bf1</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/asm-generic/qspinlock.h</span>
<span class="p_chunk">@@ -0,0 +1,139 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Queued spinlock</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Waiman Long &lt;waiman.long@hp.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __ASM_GENERIC_QSPINLOCK_H</span>
<span class="p_add">+#define __ASM_GENERIC_QSPINLOCK_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/qspinlock_types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * queued_spin_is_locked - is the spinlock locked?</span>
<span class="p_add">+ * @lock: Pointer to queued spinlock structure</span>
<span class="p_add">+ * Return: 1 if it is locked, 0 otherwise</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline int queued_spin_is_locked(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic_read(&amp;lock-&gt;val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * queued_spin_value_unlocked - is the spinlock structure unlocked?</span>
<span class="p_add">+ * @lock: queued spinlock structure</span>
<span class="p_add">+ * Return: 1 if it is unlocked, 0 otherwise</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * N.B. Whenever there are tasks waiting for the lock, it is considered</span>
<span class="p_add">+ *      locked wrt the lockref code to avoid lock stealing by the lockref</span>
<span class="p_add">+ *      code and change things underneath the lock. This also allows some</span>
<span class="p_add">+ *      optimizations to be applied without conflict with lockref.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline int queued_spin_value_unlocked(struct qspinlock lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !atomic_read(&amp;lock.val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * queued_spin_is_contended - check if the lock is contended</span>
<span class="p_add">+ * @lock : Pointer to queued spinlock structure</span>
<span class="p_add">+ * Return: 1 if lock contended, 0 otherwise</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline int queued_spin_is_contended(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic_read(&amp;lock-&gt;val) &amp; ~_Q_LOCKED_MASK;</span>
<span class="p_add">+}</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * queued_spin_trylock - try to acquire the queued spinlock</span>
<span class="p_add">+ * @lock : Pointer to queued spinlock structure</span>
<span class="p_add">+ * Return: 1 if lock acquired, 0 if failed</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline int queued_spin_trylock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!atomic_read(&amp;lock-&gt;val) &amp;&amp;</span>
<span class="p_add">+	   (atomic_cmpxchg(&amp;lock-&gt;val, 0, _Q_LOCKED_VAL) == 0))</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * queued_spin_lock - acquire a queued spinlock</span>
<span class="p_add">+ * @lock: Pointer to queued spinlock structure</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline void queued_spin_lock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 val;</span>
<span class="p_add">+</span>
<span class="p_add">+	val = atomic_cmpxchg(&amp;lock-&gt;val, 0, _Q_LOCKED_VAL);</span>
<span class="p_add">+	if (likely(val == 0))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	queued_spin_lock_slowpath(lock, val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef queued_spin_unlock</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * queued_spin_unlock - release a queued spinlock</span>
<span class="p_add">+ * @lock : Pointer to queued spinlock structure</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline void queued_spin_unlock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * smp_mb__before_atomic() in order to guarantee release semantics</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	smp_mb__before_atomic_dec();</span>
<span class="p_add">+	atomic_sub(_Q_LOCKED_VAL, &amp;lock-&gt;val);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * queued_spin_unlock_wait - wait until current lock holder releases the lock</span>
<span class="p_add">+ * @lock : Pointer to queued spinlock structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * There is a very slight possibility of live-lock if the lockers keep coming</span>
<span class="p_add">+ * and the waiter is just unfortunate enough to not see any unlock state.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void queued_spin_unlock_wait(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (atomic_read(&amp;lock-&gt;val) &amp; _Q_LOCKED_MASK)</span>
<span class="p_add">+		cpu_relax();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef virt_queued_spin_lock</span>
<span class="p_add">+static __always_inline bool virt_queued_spin_lock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Initializier</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define	__ARCH_SPIN_LOCK_UNLOCKED	{ ATOMIC_INIT(0) }</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Remapping spinlock architecture specific functions to the corresponding</span>
<span class="p_add">+ * queued spinlock functions.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define arch_spin_is_locked(l)		queued_spin_is_locked(l)</span>
<span class="p_add">+#define arch_spin_is_contended(l)	queued_spin_is_contended(l)</span>
<span class="p_add">+#define arch_spin_value_unlocked(l)	queued_spin_value_unlocked(l)</span>
<span class="p_add">+#define arch_spin_lock(l)		queued_spin_lock(l)</span>
<span class="p_add">+#define arch_spin_trylock(l)		queued_spin_trylock(l)</span>
<span class="p_add">+#define arch_spin_unlock(l)		queued_spin_unlock(l)</span>
<span class="p_add">+#define arch_spin_lock_flags(l, f)	queued_spin_lock(l)</span>
<span class="p_add">+#define arch_spin_unlock_wait(l)	queued_spin_unlock_wait(l)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASM_GENERIC_QSPINLOCK_H */</span>
<span class="p_header">diff --git a/include/asm-generic/qspinlock_types.h b/include/asm-generic/qspinlock_types.h</span>
new file mode 100644
<span class="p_header">index 000000000000..85f888e86761</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/asm-generic/qspinlock_types.h</span>
<span class="p_chunk">@@ -0,0 +1,79 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Queued spinlock</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Waiman Long &lt;waiman.long@hp.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __ASM_GENERIC_QSPINLOCK_TYPES_H</span>
<span class="p_add">+#define __ASM_GENERIC_QSPINLOCK_TYPES_H</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Including atomic.h with PARAVIRT on will cause compilation errors because</span>
<span class="p_add">+ * of recursive header file incluson via paravirt_types.h. So don&#39;t include</span>
<span class="p_add">+ * it if PARAVIRT is on.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef CONFIG_PARAVIRT</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/atomic.h&gt;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct qspinlock {</span>
<span class="p_add">+	atomic_t	val;</span>
<span class="p_add">+} arch_spinlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Bitfields in the atomic value:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * When NR_CPUS &lt; 16K</span>
<span class="p_add">+ *  0- 7: locked byte</span>
<span class="p_add">+ *     8: pending</span>
<span class="p_add">+ *  9-15: not used</span>
<span class="p_add">+ * 16-17: tail index</span>
<span class="p_add">+ * 18-31: tail cpu (+1)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * When NR_CPUS &gt;= 16K</span>
<span class="p_add">+ *  0- 7: locked byte</span>
<span class="p_add">+ *     8: pending</span>
<span class="p_add">+ *  9-10: tail index</span>
<span class="p_add">+ * 11-31: tail cpu (+1)</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define	_Q_SET_MASK(type)	(((1U &lt;&lt; _Q_ ## type ## _BITS) - 1)\</span>
<span class="p_add">+				      &lt;&lt; _Q_ ## type ## _OFFSET)</span>
<span class="p_add">+#define _Q_LOCKED_OFFSET	0</span>
<span class="p_add">+#define _Q_LOCKED_BITS		8</span>
<span class="p_add">+#define _Q_LOCKED_MASK		_Q_SET_MASK(LOCKED)</span>
<span class="p_add">+</span>
<span class="p_add">+#define _Q_PENDING_OFFSET	(_Q_LOCKED_OFFSET + _Q_LOCKED_BITS)</span>
<span class="p_add">+#if CONFIG_NR_CPUS &lt; (1U &lt;&lt; 14)</span>
<span class="p_add">+#define _Q_PENDING_BITS		8</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define _Q_PENDING_BITS		1</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#define _Q_PENDING_MASK		_Q_SET_MASK(PENDING)</span>
<span class="p_add">+</span>
<span class="p_add">+#define _Q_TAIL_IDX_OFFSET	(_Q_PENDING_OFFSET + _Q_PENDING_BITS)</span>
<span class="p_add">+#define _Q_TAIL_IDX_BITS	2</span>
<span class="p_add">+#define _Q_TAIL_IDX_MASK	_Q_SET_MASK(TAIL_IDX)</span>
<span class="p_add">+</span>
<span class="p_add">+#define _Q_TAIL_CPU_OFFSET	(_Q_TAIL_IDX_OFFSET + _Q_TAIL_IDX_BITS)</span>
<span class="p_add">+#define _Q_TAIL_CPU_BITS	(32 - _Q_TAIL_CPU_OFFSET)</span>
<span class="p_add">+#define _Q_TAIL_CPU_MASK	_Q_SET_MASK(TAIL_CPU)</span>
<span class="p_add">+</span>
<span class="p_add">+#define _Q_TAIL_OFFSET		_Q_TAIL_IDX_OFFSET</span>
<span class="p_add">+#define _Q_TAIL_MASK		(_Q_TAIL_IDX_MASK | _Q_TAIL_CPU_MASK)</span>
<span class="p_add">+</span>
<span class="p_add">+#define _Q_LOCKED_VAL		(1U &lt;&lt; _Q_LOCKED_OFFSET)</span>
<span class="p_add">+#define _Q_PENDING_VAL		(1U &lt;&lt; _Q_PENDING_OFFSET)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASM_GENERIC_QSPINLOCK_TYPES_H */</span>
<span class="p_header">diff --git a/include/linux/compiler.h b/include/linux/compiler.h</span>
<span class="p_header">index 867722591be2..03e227ba481c 100644</span>
<span class="p_header">--- a/include/linux/compiler.h</span>
<span class="p_header">+++ b/include/linux/compiler.h</span>
<span class="p_chunk">@@ -250,7 +250,7 @@</span> <span class="p_context"> static __always_inline void __write_once_size(volatile void *p, void *res, int s</span>
 	({ union { typeof(x) __val; char __c[1]; } __u; __read_once_size(&amp;(x), __u.__c, sizeof(x)); __u.__val; })
 
 #define WRITE_ONCE(x, val) \
<span class="p_del">-	({ typeof(x) __val = (val); __write_once_size(&amp;(x), &amp;__val, sizeof(__val)); __val; })</span>
<span class="p_add">+	({ union { typeof(x) __val; char __c[1]; } __u = { .__val = (val) }; __write_once_size(&amp;(x), __u.__c, sizeof(x)); __u.__val; })</span>
 
 #endif /* __KERNEL__ */
 
<span class="p_chunk">@@ -450,7 +450,7 @@</span> <span class="p_context"> static __always_inline void __write_once_size(volatile void *p, void *res, int s</span>
  * with an explicit memory barrier or atomic instruction that provides the
  * required ordering.
  *
<span class="p_del">- * If possible use READ_ONCE/ASSIGN_ONCE instead.</span>
<span class="p_add">+ * If possible use READ_ONCE()/WRITE_ONCE() instead.</span>
  */
 #define __ACCESS_ONCE(x) ({ \
 	 __maybe_unused typeof(x) __var = (__force typeof(x)) 0; \
<span class="p_header">diff --git a/include/linux/lockdep.h b/include/linux/lockdep.h</span>
<span class="p_header">index 066ba4157541..2722111591a3 100644</span>
<span class="p_header">--- a/include/linux/lockdep.h</span>
<span class="p_header">+++ b/include/linux/lockdep.h</span>
<span class="p_chunk">@@ -130,8 +130,8 @@</span> <span class="p_context"> enum bounce_type {</span>
 };
 
 struct lock_class_stats {
<span class="p_del">-	unsigned long			contention_point[4];</span>
<span class="p_del">-	unsigned long			contending_point[4];</span>
<span class="p_add">+	unsigned long			contention_point[LOCKSTAT_POINTS];</span>
<span class="p_add">+	unsigned long			contending_point[LOCKSTAT_POINTS];</span>
 	struct lock_time		read_waittime;
 	struct lock_time		write_waittime;
 	struct lock_time		read_holdtime;
<span class="p_header">diff --git a/include/linux/osq_lock.h b/include/linux/osq_lock.h</span>
<span class="p_header">index 3a6490e81b28..703ea5c30a33 100644</span>
<span class="p_header">--- a/include/linux/osq_lock.h</span>
<span class="p_header">+++ b/include/linux/osq_lock.h</span>
<span class="p_chunk">@@ -32,4 +32,9 @@</span> <span class="p_context"> static inline void osq_lock_init(struct optimistic_spin_queue *lock)</span>
 extern bool osq_lock(struct optimistic_spin_queue *lock);
 extern void osq_unlock(struct optimistic_spin_queue *lock);
 
<span class="p_add">+static inline bool osq_is_locked(struct optimistic_spin_queue *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic_read(&amp;lock-&gt;tail) != OSQ_UNLOCKED_VAL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 26a2e6122734..18f197223ebd 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -252,7 +252,7 @@</span> <span class="p_context"> extern char ___assert_task_state[1 - 2*!!(</span>
 #define set_task_state(tsk, state_value)			\
 	do {							\
 		(tsk)-&gt;task_state_change = _THIS_IP_;		\
<span class="p_del">-		set_mb((tsk)-&gt;state, (state_value));		\</span>
<span class="p_add">+		smp_store_mb((tsk)-&gt;state, (state_value));		\</span>
 	} while (0)
 
 /*
<span class="p_chunk">@@ -274,7 +274,7 @@</span> <span class="p_context"> extern char ___assert_task_state[1 - 2*!!(</span>
 #define set_current_state(state_value)				\
 	do {							\
 		current-&gt;task_state_change = _THIS_IP_;		\
<span class="p_del">-		set_mb(current-&gt;state, (state_value));		\</span>
<span class="p_add">+		smp_store_mb(current-&gt;state, (state_value));		\</span>
 	} while (0)
 
 #else
<span class="p_chunk">@@ -282,7 +282,7 @@</span> <span class="p_context"> extern char ___assert_task_state[1 - 2*!!(</span>
 #define __set_task_state(tsk, state_value)		\
 	do { (tsk)-&gt;state = (state_value); } while (0)
 #define set_task_state(tsk, state_value)		\
<span class="p_del">-	set_mb((tsk)-&gt;state, (state_value))</span>
<span class="p_add">+	smp_store_mb((tsk)-&gt;state, (state_value))</span>
 
 /*
  * set_current_state() includes a barrier so that the write of current-&gt;state
<span class="p_chunk">@@ -298,7 +298,7 @@</span> <span class="p_context"> extern char ___assert_task_state[1 - 2*!!(</span>
 #define __set_current_state(state_value)		\
 	do { current-&gt;state = (state_value); } while (0)
 #define set_current_state(state_value)			\
<span class="p_del">-	set_mb(current-&gt;state, (state_value))</span>
<span class="p_add">+	smp_store_mb(current-&gt;state, (state_value))</span>
 
 #endif
 
<span class="p_header">diff --git a/kernel/Kconfig.locks b/kernel/Kconfig.locks</span>
<span class="p_header">index 08561f1acd13..ebdb0043203a 100644</span>
<span class="p_header">--- a/kernel/Kconfig.locks</span>
<span class="p_header">+++ b/kernel/Kconfig.locks</span>
<span class="p_chunk">@@ -235,9 +235,16 @@</span> <span class="p_context"> config LOCK_SPIN_ON_OWNER</span>
        def_bool y
        depends on MUTEX_SPIN_ON_OWNER || RWSEM_SPIN_ON_OWNER
 
<span class="p_del">-config ARCH_USE_QUEUE_RWLOCK</span>
<span class="p_add">+config ARCH_USE_QUEUED_SPINLOCKS</span>
 	bool
 
<span class="p_del">-config QUEUE_RWLOCK</span>
<span class="p_del">-	def_bool y if ARCH_USE_QUEUE_RWLOCK</span>
<span class="p_add">+config QUEUED_SPINLOCKS</span>
<span class="p_add">+	def_bool y if ARCH_USE_QUEUED_SPINLOCKS</span>
<span class="p_add">+	depends on SMP</span>
<span class="p_add">+</span>
<span class="p_add">+config ARCH_USE_QUEUED_RWLOCKS</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
<span class="p_add">+config QUEUED_RWLOCKS</span>
<span class="p_add">+	def_bool y if ARCH_USE_QUEUED_RWLOCKS</span>
 	depends on SMP
<span class="p_header">diff --git a/kernel/futex.c b/kernel/futex.c</span>
<span class="p_header">index 2579e407ff67..55ca63ad9622 100644</span>
<span class="p_header">--- a/kernel/futex.c</span>
<span class="p_header">+++ b/kernel/futex.c</span>
<span class="p_chunk">@@ -2055,7 +2055,7 @@</span> <span class="p_context"> static void futex_wait_queue_me(struct futex_hash_bucket *hb, struct futex_q *q,</span>
 {
 	/*
 	 * The task state is guaranteed to be set before another task can
<span class="p_del">-	 * wake it. set_current_state() is implemented using set_mb() and</span>
<span class="p_add">+	 * wake it. set_current_state() is implemented using smp_store_mb() and</span>
 	 * queue_me() calls spin_unlock() upon completion, both serializing
 	 * access to the hash list and forcing another memory barrier.
 	 */
<span class="p_header">diff --git a/kernel/locking/Makefile b/kernel/locking/Makefile</span>
<span class="p_header">index de7a416cca2a..7dd5c9918e4c 100644</span>
<span class="p_header">--- a/kernel/locking/Makefile</span>
<span class="p_header">+++ b/kernel/locking/Makefile</span>
<span class="p_chunk">@@ -17,6 +17,7 @@</span> <span class="p_context"> obj-$(CONFIG_SMP) += spinlock.o</span>
 obj-$(CONFIG_LOCK_SPIN_ON_OWNER) += osq_lock.o
 obj-$(CONFIG_SMP) += lglock.o
 obj-$(CONFIG_PROVE_LOCKING) += spinlock.o
<span class="p_add">+obj-$(CONFIG_QUEUED_SPINLOCKS) += qspinlock.o</span>
 obj-$(CONFIG_RT_MUTEXES) += rtmutex.o
 obj-$(CONFIG_DEBUG_RT_MUTEXES) += rtmutex-debug.o
 obj-$(CONFIG_RT_MUTEX_TESTER) += rtmutex-tester.o
<span class="p_chunk">@@ -25,5 +26,5 @@</span> <span class="p_context"> obj-$(CONFIG_DEBUG_SPINLOCK) += spinlock_debug.o</span>
 obj-$(CONFIG_RWSEM_GENERIC_SPINLOCK) += rwsem-spinlock.o
 obj-$(CONFIG_RWSEM_XCHGADD_ALGORITHM) += rwsem-xadd.o
 obj-$(CONFIG_PERCPU_RWSEM) += percpu-rwsem.o
<span class="p_del">-obj-$(CONFIG_QUEUE_RWLOCK) += qrwlock.o</span>
<span class="p_add">+obj-$(CONFIG_QUEUED_RWLOCKS) += qrwlock.o</span>
 obj-$(CONFIG_LOCK_TORTURE_TEST) += locktorture.o
<span class="p_header">diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c</span>
<span class="p_header">index a0831e1b99f4..a61bb1d37a52 100644</span>
<span class="p_header">--- a/kernel/locking/lockdep.c</span>
<span class="p_header">+++ b/kernel/locking/lockdep.c</span>
<span class="p_chunk">@@ -4066,8 +4066,7 @@</span> <span class="p_context"> void __init lockdep_info(void)</span>
 
 #ifdef CONFIG_DEBUG_LOCKDEP
 	if (lockdep_init_error) {
<span class="p_del">-		printk(&quot;WARNING: lockdep init error! lock-%s was acquired&quot;</span>
<span class="p_del">-			&quot;before lockdep_init\n&quot;, lock_init_error);</span>
<span class="p_add">+		printk(&quot;WARNING: lockdep init error: lock &#39;%s&#39; was acquired before lockdep_init().\n&quot;, lock_init_error);</span>
 		printk(&quot;Call stack leading to lockdep invocation was:\n&quot;);
 		print_stack_trace(&amp;lockdep_init_trace, 0);
 	}
<span class="p_header">diff --git a/kernel/locking/mcs_spinlock.h b/kernel/locking/mcs_spinlock.h</span>
<span class="p_header">index 75e114bdf3f2..fd91aaa4554c 100644</span>
<span class="p_header">--- a/kernel/locking/mcs_spinlock.h</span>
<span class="p_header">+++ b/kernel/locking/mcs_spinlock.h</span>
<span class="p_chunk">@@ -17,6 +17,7 @@</span> <span class="p_context"></span>
 struct mcs_spinlock {
 	struct mcs_spinlock *next;
 	int locked; /* 1 if lock acquired */
<span class="p_add">+	int count;  /* nesting count, see qspinlock.c */</span>
 };
 
 #ifndef arch_mcs_spin_lock_contended
<span class="p_header">diff --git a/kernel/locking/qrwlock.c b/kernel/locking/qrwlock.c</span>
<span class="p_header">index f956ede7f90d..6c5da483966b 100644</span>
<span class="p_header">--- a/kernel/locking/qrwlock.c</span>
<span class="p_header">+++ b/kernel/locking/qrwlock.c</span>
<span class="p_chunk">@@ -1,5 +1,5 @@</span> <span class="p_context"></span>
 /*
<span class="p_del">- * Queue read/write lock</span>
<span class="p_add">+ * Queued read/write locks</span>
  *
  * This program is free software; you can redistribute it and/or modify
  * it under the terms of the GNU General Public License as published by
<span class="p_chunk">@@ -22,6 +22,26 @@</span> <span class="p_context"></span>
 #include &lt;linux/hardirq.h&gt;
 #include &lt;asm/qrwlock.h&gt;
 
<span class="p_add">+/*</span>
<span class="p_add">+ * This internal data structure is used for optimizing access to some of</span>
<span class="p_add">+ * the subfields within the atomic_t cnts.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct __qrwlock {</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		atomic_t cnts;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+#ifdef __LITTLE_ENDIAN</span>
<span class="p_add">+			u8 wmode;	/* Writer mode   */</span>
<span class="p_add">+			u8 rcnts[3];	/* Reader counts */</span>
<span class="p_add">+#else</span>
<span class="p_add">+			u8 rcnts[3];	/* Reader counts */</span>
<span class="p_add">+			u8 wmode;	/* Writer mode   */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		};</span>
<span class="p_add">+	};</span>
<span class="p_add">+	arch_spinlock_t	lock;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 /**
  * rspin_until_writer_unlock - inc reader count &amp; spin until writer is gone
  * @lock  : Pointer to queue rwlock structure
<span class="p_chunk">@@ -107,10 +127,10 @@</span> <span class="p_context"> void queue_write_lock_slowpath(struct qrwlock *lock)</span>
 	 * or wait for a previous writer to go away.
 	 */
 	for (;;) {
<span class="p_del">-		cnts = atomic_read(&amp;lock-&gt;cnts);</span>
<span class="p_del">-		if (!(cnts &amp; _QW_WMASK) &amp;&amp;</span>
<span class="p_del">-		    (atomic_cmpxchg(&amp;lock-&gt;cnts, cnts,</span>
<span class="p_del">-				    cnts | _QW_WAITING) == cnts))</span>
<span class="p_add">+		struct __qrwlock *l = (struct __qrwlock *)lock;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!READ_ONCE(l-&gt;wmode) &amp;&amp;</span>
<span class="p_add">+		   (cmpxchg(&amp;l-&gt;wmode, 0, _QW_WAITING) == 0))</span>
 			break;
 
 		cpu_relax_lowlatency();
<span class="p_header">diff --git a/kernel/locking/qspinlock.c b/kernel/locking/qspinlock.c</span>
new file mode 100644
<span class="p_header">index 000000000000..38c49202d532</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/kernel/locking/qspinlock.c</span>
<span class="p_chunk">@@ -0,0 +1,473 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Queued spinlock</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * (C) Copyright 2013-2015 Hewlett-Packard Development Company, L.P.</span>
<span class="p_add">+ * (C) Copyright 2013-2014 Red Hat, Inc.</span>
<span class="p_add">+ * (C) Copyright 2015 Intel Corp.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Waiman Long &lt;waiman.long@hp.com&gt;</span>
<span class="p_add">+ *          Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _GEN_PV_LOCK_SLOWPATH</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/smp.h&gt;</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+#include &lt;linux/cpumask.h&gt;</span>
<span class="p_add">+#include &lt;linux/percpu.h&gt;</span>
<span class="p_add">+#include &lt;linux/hardirq.h&gt;</span>
<span class="p_add">+#include &lt;linux/mutex.h&gt;</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+#include &lt;asm/qspinlock.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The basic principle of a queue-based spinlock can best be understood</span>
<span class="p_add">+ * by studying a classic queue-based spinlock implementation called the</span>
<span class="p_add">+ * MCS lock. The paper below provides a good description for this kind</span>
<span class="p_add">+ * of lock.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * http://www.cise.ufl.edu/tr/DOC/REP-1992-71.pdf</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This queued spinlock implementation is based on the MCS lock, however to make</span>
<span class="p_add">+ * it fit the 4 bytes we assume spinlock_t to be, and preserve its existing</span>
<span class="p_add">+ * API, we must modify it somehow.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * In particular; where the traditional MCS lock consists of a tail pointer</span>
<span class="p_add">+ * (8 bytes) and needs the next pointer (another 8 bytes) of its own node to</span>
<span class="p_add">+ * unlock the next pending (next-&gt;locked), we compress both these: {tail,</span>
<span class="p_add">+ * next-&gt;locked} into a single u32 value.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Since a spinlock disables recursion of its own context and there is a limit</span>
<span class="p_add">+ * to the contexts that can nest; namely: task, softirq, hardirq, nmi. As there</span>
<span class="p_add">+ * are at most 4 nesting levels, it can be encoded by a 2-bit number. Now</span>
<span class="p_add">+ * we can encode the tail by combining the 2-bit nesting level with the cpu</span>
<span class="p_add">+ * number. With one byte for the lock value and 3 bytes for the tail, only a</span>
<span class="p_add">+ * 32-bit word is now needed. Even though we only need 1 bit for the lock,</span>
<span class="p_add">+ * we extend it to a full byte to achieve better performance for architectures</span>
<span class="p_add">+ * that support atomic byte write.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We also change the first spinner to spin on the lock bit instead of its</span>
<span class="p_add">+ * node; whereby avoiding the need to carry a node from lock to unlock, and</span>
<span class="p_add">+ * preserving existing lock API. This also makes the unlock code simpler and</span>
<span class="p_add">+ * faster.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * N.B. The current implementation only supports architectures that allow</span>
<span class="p_add">+ *      atomic operations on smaller 8-bit and 16-bit data types.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;mcs_spinlock.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PARAVIRT_SPINLOCKS</span>
<span class="p_add">+#define MAX_NODES	8</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define MAX_NODES	4</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Per-CPU queue node structures; we can never have more than 4 nested</span>
<span class="p_add">+ * contexts: task, softirq, hardirq, nmi.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Exactly fits one 64-byte cacheline on a 64-bit architecture.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * PV doubles the storage and uses the second cacheline for PV state.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static DEFINE_PER_CPU_ALIGNED(struct mcs_spinlock, mcs_nodes[MAX_NODES]);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * We must be able to distinguish between no-tail and the tail at 0:0,</span>
<span class="p_add">+ * therefore increment the cpu number by one.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline u32 encode_tail(int cpu, int idx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 tail;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_DEBUG_SPINLOCK</span>
<span class="p_add">+	BUG_ON(idx &gt; 3);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	tail  = (cpu + 1) &lt;&lt; _Q_TAIL_CPU_OFFSET;</span>
<span class="p_add">+	tail |= idx &lt;&lt; _Q_TAIL_IDX_OFFSET; /* assume &lt; 4 */</span>
<span class="p_add">+</span>
<span class="p_add">+	return tail;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct mcs_spinlock *decode_tail(u32 tail)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int cpu = (tail &gt;&gt; _Q_TAIL_CPU_OFFSET) - 1;</span>
<span class="p_add">+	int idx = (tail &amp;  _Q_TAIL_IDX_MASK) &gt;&gt; _Q_TAIL_IDX_OFFSET;</span>
<span class="p_add">+</span>
<span class="p_add">+	return per_cpu_ptr(&amp;mcs_nodes[idx], cpu);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define _Q_LOCKED_PENDING_MASK (_Q_LOCKED_MASK | _Q_PENDING_MASK)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * By using the whole 2nd least significant byte for the pending bit, we</span>
<span class="p_add">+ * can allow better optimization of the lock acquisition for the pending</span>
<span class="p_add">+ * bit holder.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This internal structure is also used by the set_locked function which</span>
<span class="p_add">+ * is not restricted to _Q_PENDING_BITS == 8.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct __qspinlock {</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		atomic_t val;</span>
<span class="p_add">+#ifdef __LITTLE_ENDIAN</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			u8	locked;</span>
<span class="p_add">+			u8	pending;</span>
<span class="p_add">+		};</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			u16	locked_pending;</span>
<span class="p_add">+			u16	tail;</span>
<span class="p_add">+		};</span>
<span class="p_add">+#else</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			u16	tail;</span>
<span class="p_add">+			u16	locked_pending;</span>
<span class="p_add">+		};</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			u8	reserved[2];</span>
<span class="p_add">+			u8	pending;</span>
<span class="p_add">+			u8	locked;</span>
<span class="p_add">+		};</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	};</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#if _Q_PENDING_BITS == 8</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_pending_set_locked - take ownership and clear the pending bit.</span>
<span class="p_add">+ * @lock: Pointer to queued spinlock structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * *,1,0 -&gt; *,0,1</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Lock stealing is not allowed if this function is used.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline void clear_pending_set_locked(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct __qspinlock *l = (void *)lock;</span>
<span class="p_add">+</span>
<span class="p_add">+	WRITE_ONCE(l-&gt;locked_pending, _Q_LOCKED_VAL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * xchg_tail - Put in the new queue tail code word &amp; retrieve previous one</span>
<span class="p_add">+ * @lock : Pointer to queued spinlock structure</span>
<span class="p_add">+ * @tail : The new queue tail code word</span>
<span class="p_add">+ * Return: The previous queue tail code word</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * xchg(lock, tail)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * p,*,* -&gt; n,*,* ; prev = xchg(lock, node)</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct __qspinlock *l = (void *)lock;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (u32)xchg(&amp;l-&gt;tail, tail &gt;&gt; _Q_TAIL_OFFSET) &lt;&lt; _Q_TAIL_OFFSET;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* _Q_PENDING_BITS == 8 */</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_pending_set_locked - take ownership and clear the pending bit.</span>
<span class="p_add">+ * @lock: Pointer to queued spinlock structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * *,1,0 -&gt; *,0,1</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline void clear_pending_set_locked(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_add(-_Q_PENDING_VAL + _Q_LOCKED_VAL, &amp;lock-&gt;val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * xchg_tail - Put in the new queue tail code word &amp; retrieve previous one</span>
<span class="p_add">+ * @lock : Pointer to queued spinlock structure</span>
<span class="p_add">+ * @tail : The new queue tail code word</span>
<span class="p_add">+ * Return: The previous queue tail code word</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * xchg(lock, tail)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * p,*,* -&gt; n,*,* ; prev = xchg(lock, node)</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline u32 xchg_tail(struct qspinlock *lock, u32 tail)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 old, new, val = atomic_read(&amp;lock-&gt;val);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (;;) {</span>
<span class="p_add">+		new = (val &amp; _Q_LOCKED_PENDING_MASK) | tail;</span>
<span class="p_add">+		old = atomic_cmpxchg(&amp;lock-&gt;val, val, new);</span>
<span class="p_add">+		if (old == val)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		val = old;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return old;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* _Q_PENDING_BITS == 8 */</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * set_locked - Set the lock bit and own the lock</span>
<span class="p_add">+ * @lock: Pointer to queued spinlock structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * *,*,0 -&gt; *,0,1</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __always_inline void set_locked(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct __qspinlock *l = (void *)lock;</span>
<span class="p_add">+</span>
<span class="p_add">+	WRITE_ONCE(l-&gt;locked, _Q_LOCKED_VAL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Generate the native code for queued_spin_unlock_slowpath(); provide NOPs for</span>
<span class="p_add">+ * all the PV callbacks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void __pv_init_node(struct mcs_spinlock *node) { }</span>
<span class="p_add">+static __always_inline void __pv_wait_node(struct mcs_spinlock *node) { }</span>
<span class="p_add">+static __always_inline void __pv_kick_node(struct mcs_spinlock *node) { }</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void __pv_wait_head(struct qspinlock *lock,</span>
<span class="p_add">+					   struct mcs_spinlock *node) { }</span>
<span class="p_add">+</span>
<span class="p_add">+#define pv_enabled()		false</span>
<span class="p_add">+</span>
<span class="p_add">+#define pv_init_node		__pv_init_node</span>
<span class="p_add">+#define pv_wait_node		__pv_wait_node</span>
<span class="p_add">+#define pv_kick_node		__pv_kick_node</span>
<span class="p_add">+#define pv_wait_head		__pv_wait_head</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PARAVIRT_SPINLOCKS</span>
<span class="p_add">+#define queued_spin_lock_slowpath	native_queued_spin_lock_slowpath</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _GEN_PV_LOCK_SLOWPATH */</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * queued_spin_lock_slowpath - acquire the queued spinlock</span>
<span class="p_add">+ * @lock: Pointer to queued spinlock structure</span>
<span class="p_add">+ * @val: Current value of the queued spinlock 32-bit word</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * (queue tail, pending bit, lock value)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *              fast     :    slow                                  :    unlock</span>
<span class="p_add">+ *                       :                                          :</span>
<span class="p_add">+ * uncontended  (0,0,0) -:--&gt; (0,0,1) ------------------------------:--&gt; (*,*,0)</span>
<span class="p_add">+ *                       :       | ^--------.------.             /  :</span>
<span class="p_add">+ *                       :       v           \      \            |  :</span>
<span class="p_add">+ * pending               :    (0,1,1) +--&gt; (0,1,0)   \           |  :</span>
<span class="p_add">+ *                       :       | ^--&#39;              |           |  :</span>
<span class="p_add">+ *                       :       v                   |           |  :</span>
<span class="p_add">+ * uncontended           :    (n,x,y) +--&gt; (n,0,0) --&#39;           |  :</span>
<span class="p_add">+ *   queue               :       | ^--&#39;                          |  :</span>
<span class="p_add">+ *                       :       v                               |  :</span>
<span class="p_add">+ * contended             :    (*,x,y) +--&gt; (*,0,0) ---&gt; (*,0,1) -&#39;  :</span>
<span class="p_add">+ *   queue               :         ^--&#39;                             :</span>
<span class="p_add">+ */</span>
<span class="p_add">+void queued_spin_lock_slowpath(struct qspinlock *lock, u32 val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mcs_spinlock *prev, *next, *node;</span>
<span class="p_add">+	u32 new, old, tail;</span>
<span class="p_add">+	int idx;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(CONFIG_NR_CPUS &gt;= (1U &lt;&lt; _Q_TAIL_CPU_BITS));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pv_enabled())</span>
<span class="p_add">+		goto queue;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (virt_queued_spin_lock(lock))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * wait for in-progress pending-&gt;locked hand-overs</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * 0,1,0 -&gt; 0,0,1</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (val == _Q_PENDING_VAL) {</span>
<span class="p_add">+		while ((val = atomic_read(&amp;lock-&gt;val)) == _Q_PENDING_VAL)</span>
<span class="p_add">+			cpu_relax();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * trylock || pending</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * 0,0,0 -&gt; 0,0,1 ; trylock</span>
<span class="p_add">+	 * 0,0,1 -&gt; 0,1,1 ; pending</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (;;) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If we observe any contention; queue.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (val &amp; ~_Q_LOCKED_MASK)</span>
<span class="p_add">+			goto queue;</span>
<span class="p_add">+</span>
<span class="p_add">+		new = _Q_LOCKED_VAL;</span>
<span class="p_add">+		if (val == new)</span>
<span class="p_add">+			new |= _Q_PENDING_VAL;</span>
<span class="p_add">+</span>
<span class="p_add">+		old = atomic_cmpxchg(&amp;lock-&gt;val, val, new);</span>
<span class="p_add">+		if (old == val)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		val = old;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * we won the trylock</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (new == _Q_LOCKED_VAL)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * we&#39;re pending, wait for the owner to go away.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * *,1,1 -&gt; *,1,0</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * this wait loop must be a load-acquire such that we match the</span>
<span class="p_add">+	 * store-release that clears the locked bit and create lock</span>
<span class="p_add">+	 * sequentiality; this is because not all clear_pending_set_locked()</span>
<span class="p_add">+	 * implementations imply full barriers.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	while ((val = smp_load_acquire(&amp;lock-&gt;val.counter)) &amp; _Q_LOCKED_MASK)</span>
<span class="p_add">+		cpu_relax();</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * take ownership and clear the pending bit.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * *,1,0 -&gt; *,0,1</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	clear_pending_set_locked(lock);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * End of pending bit optimistic spinning and beginning of MCS</span>
<span class="p_add">+	 * queuing.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+queue:</span>
<span class="p_add">+	node = this_cpu_ptr(&amp;mcs_nodes[0]);</span>
<span class="p_add">+	idx = node-&gt;count++;</span>
<span class="p_add">+	tail = encode_tail(smp_processor_id(), idx);</span>
<span class="p_add">+</span>
<span class="p_add">+	node += idx;</span>
<span class="p_add">+	node-&gt;locked = 0;</span>
<span class="p_add">+	node-&gt;next = NULL;</span>
<span class="p_add">+	pv_init_node(node);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We touched a (possibly) cold cacheline in the per-cpu queue node;</span>
<span class="p_add">+	 * attempt the trylock once more in the hope someone let go while we</span>
<span class="p_add">+	 * weren&#39;t watching.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (queued_spin_trylock(lock))</span>
<span class="p_add">+		goto release;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We have already touched the queueing cacheline; don&#39;t bother with</span>
<span class="p_add">+	 * pending stuff.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * p,*,* -&gt; n,*,*</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	old = xchg_tail(lock, tail);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * if there was a previous node; link it and wait until reaching the</span>
<span class="p_add">+	 * head of the waitqueue.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (old &amp; _Q_TAIL_MASK) {</span>
<span class="p_add">+		prev = decode_tail(old);</span>
<span class="p_add">+		WRITE_ONCE(prev-&gt;next, node);</span>
<span class="p_add">+</span>
<span class="p_add">+		pv_wait_node(node);</span>
<span class="p_add">+		arch_mcs_spin_lock_contended(&amp;node-&gt;locked);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * we&#39;re at the head of the waitqueue, wait for the owner &amp; pending to</span>
<span class="p_add">+	 * go away.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * *,x,y -&gt; *,0,0</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * this wait loop must use a load-acquire such that we match the</span>
<span class="p_add">+	 * store-release that clears the locked bit and create lock</span>
<span class="p_add">+	 * sequentiality; this is because the set_locked() function below</span>
<span class="p_add">+	 * does not imply a full barrier.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pv_wait_head(lock, node);</span>
<span class="p_add">+	while ((val = smp_load_acquire(&amp;lock-&gt;val.counter)) &amp; _Q_LOCKED_PENDING_MASK)</span>
<span class="p_add">+		cpu_relax();</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * claim the lock:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * n,0,0 -&gt; 0,0,1 : lock, uncontended</span>
<span class="p_add">+	 * *,0,0 -&gt; *,0,1 : lock, contended</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * If the queue head is the only one in the queue (lock value == tail),</span>
<span class="p_add">+	 * clear the tail code and grab the lock. Otherwise, we only need</span>
<span class="p_add">+	 * to grab the lock.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (;;) {</span>
<span class="p_add">+		if (val != tail) {</span>
<span class="p_add">+			set_locked(lock);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		old = atomic_cmpxchg(&amp;lock-&gt;val, val, _Q_LOCKED_VAL);</span>
<span class="p_add">+		if (old == val)</span>
<span class="p_add">+			goto release;	/* No contention */</span>
<span class="p_add">+</span>
<span class="p_add">+		val = old;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * contended path; wait for next, release.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	while (!(next = READ_ONCE(node-&gt;next)))</span>
<span class="p_add">+		cpu_relax();</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_mcs_spin_unlock_contended(&amp;next-&gt;locked);</span>
<span class="p_add">+	pv_kick_node(next);</span>
<span class="p_add">+</span>
<span class="p_add">+release:</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * release the node</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	this_cpu_dec(mcs_nodes[0].count);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(queued_spin_lock_slowpath);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Generate the paravirt code for queued_spin_unlock_slowpath().</span>
<span class="p_add">+ */</span>
<span class="p_add">+#if !defined(_GEN_PV_LOCK_SLOWPATH) &amp;&amp; defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
<span class="p_add">+#define _GEN_PV_LOCK_SLOWPATH</span>
<span class="p_add">+</span>
<span class="p_add">+#undef  pv_enabled</span>
<span class="p_add">+#define pv_enabled()	true</span>
<span class="p_add">+</span>
<span class="p_add">+#undef pv_init_node</span>
<span class="p_add">+#undef pv_wait_node</span>
<span class="p_add">+#undef pv_kick_node</span>
<span class="p_add">+#undef pv_wait_head</span>
<span class="p_add">+</span>
<span class="p_add">+#undef  queued_spin_lock_slowpath</span>
<span class="p_add">+#define queued_spin_lock_slowpath	__pv_queued_spin_lock_slowpath</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;qspinlock_paravirt.h&quot;</span>
<span class="p_add">+#include &quot;qspinlock.c&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/kernel/locking/qspinlock_paravirt.h b/kernel/locking/qspinlock_paravirt.h</span>
new file mode 100644
<span class="p_header">index 000000000000..04ab18151cc8</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/kernel/locking/qspinlock_paravirt.h</span>
<span class="p_chunk">@@ -0,0 +1,325 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _GEN_PV_LOCK_SLOWPATH</span>
<span class="p_add">+#error &quot;do not include this file&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/hash.h&gt;</span>
<span class="p_add">+#include &lt;linux/bootmem.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Implement paravirt qspinlocks; the general idea is to halt the vcpus instead</span>
<span class="p_add">+ * of spinning them.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This relies on the architecture to provide two paravirt hypercalls:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   pv_wait(u8 *ptr, u8 val) -- suspends the vcpu if *ptr == val</span>
<span class="p_add">+ *   pv_kick(cpu)             -- wakes a suspended vcpu</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Using these we implement __pv_queued_spin_lock_slowpath() and</span>
<span class="p_add">+ * __pv_queued_spin_unlock() to replace native_queued_spin_lock_slowpath() and</span>
<span class="p_add">+ * native_queued_spin_unlock().</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _Q_SLOW_VAL	(3U &lt;&lt; _Q_LOCKED_OFFSET)</span>
<span class="p_add">+</span>
<span class="p_add">+enum vcpu_state {</span>
<span class="p_add">+	vcpu_running = 0,</span>
<span class="p_add">+	vcpu_halted,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct pv_node {</span>
<span class="p_add">+	struct mcs_spinlock	mcs;</span>
<span class="p_add">+	struct mcs_spinlock	__res[3];</span>
<span class="p_add">+</span>
<span class="p_add">+	int			cpu;</span>
<span class="p_add">+	u8			state;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Lock and MCS node addresses hash table for fast lookup</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Hashing is done on a per-cacheline basis to minimize the need to access</span>
<span class="p_add">+ * more than one cacheline.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Dynamically allocate a hash table big enough to hold at least 4X the</span>
<span class="p_add">+ * number of possible cpus in the system. Allocation is done on page</span>
<span class="p_add">+ * granularity. So the minimum number of hash buckets should be at least</span>
<span class="p_add">+ * 256 (64-bit) or 512 (32-bit) to fully utilize a 4k page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Since we should not be holding locks from NMI context (very rare indeed) the</span>
<span class="p_add">+ * max load factor is 0.75, which is around the point where open addressing</span>
<span class="p_add">+ * breaks down.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct pv_hash_entry {</span>
<span class="p_add">+	struct qspinlock *lock;</span>
<span class="p_add">+	struct pv_node   *node;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define PV_HE_PER_LINE	(SMP_CACHE_BYTES / sizeof(struct pv_hash_entry))</span>
<span class="p_add">+#define PV_HE_MIN	(PAGE_SIZE / sizeof(struct pv_hash_entry))</span>
<span class="p_add">+</span>
<span class="p_add">+static struct pv_hash_entry *pv_lock_hash;</span>
<span class="p_add">+static unsigned int pv_lock_hash_bits __read_mostly;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Allocate memory for the PV qspinlock hash buckets</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function should be called from the paravirt spinlock initialization</span>
<span class="p_add">+ * routine.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init __pv_init_lock_hash(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int pv_hash_size = ALIGN(4 * num_possible_cpus(), PV_HE_PER_LINE);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pv_hash_size &lt; PV_HE_MIN)</span>
<span class="p_add">+		pv_hash_size = PV_HE_MIN;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Allocate space from bootmem which should be page-size aligned</span>
<span class="p_add">+	 * and hence cacheline aligned.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pv_lock_hash = alloc_large_system_hash(&quot;PV qspinlock&quot;,</span>
<span class="p_add">+					       sizeof(struct pv_hash_entry),</span>
<span class="p_add">+					       pv_hash_size, 0, HASH_EARLY,</span>
<span class="p_add">+					       &amp;pv_lock_hash_bits, NULL,</span>
<span class="p_add">+					       pv_hash_size, pv_hash_size);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define for_each_hash_entry(he, offset, hash)						\</span>
<span class="p_add">+	for (hash &amp;= ~(PV_HE_PER_LINE - 1), he = &amp;pv_lock_hash[hash], offset = 0;	\</span>
<span class="p_add">+	     offset &lt; (1 &lt;&lt; pv_lock_hash_bits);						\</span>
<span class="p_add">+	     offset++, he = &amp;pv_lock_hash[(hash + offset) &amp; ((1 &lt;&lt; pv_lock_hash_bits) - 1)])</span>
<span class="p_add">+</span>
<span class="p_add">+static struct qspinlock **pv_hash(struct qspinlock *lock, struct pv_node *node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);</span>
<span class="p_add">+	struct pv_hash_entry *he;</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_hash_entry(he, offset, hash) {</span>
<span class="p_add">+		if (!cmpxchg(&amp;he-&gt;lock, NULL, lock)) {</span>
<span class="p_add">+			WRITE_ONCE(he-&gt;node, node);</span>
<span class="p_add">+			return &amp;he-&gt;lock;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Hard assume there is a free entry for us.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This is guaranteed by ensuring every blocked lock only ever consumes</span>
<span class="p_add">+	 * a single entry, and since we only have 4 nesting levels per CPU</span>
<span class="p_add">+	 * and allocated 4*nr_possible_cpus(), this must be so.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The single entry is guaranteed by having the lock owner unhash</span>
<span class="p_add">+	 * before it releases.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct pv_node *pv_unhash(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long offset, hash = hash_ptr(lock, pv_lock_hash_bits);</span>
<span class="p_add">+	struct pv_hash_entry *he;</span>
<span class="p_add">+	struct pv_node *node;</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_hash_entry(he, offset, hash) {</span>
<span class="p_add">+		if (READ_ONCE(he-&gt;lock) == lock) {</span>
<span class="p_add">+			node = READ_ONCE(he-&gt;node);</span>
<span class="p_add">+			WRITE_ONCE(he-&gt;lock, NULL);</span>
<span class="p_add">+			return node;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Hard assume we&#39;ll find an entry.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This guarantees a limited lookup time and is itself guaranteed by</span>
<span class="p_add">+	 * having the lock owner do the unhash -- IFF the unlock sees the</span>
<span class="p_add">+	 * SLOW flag, there MUST be a hash entry.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Initialize the PV part of the mcs_spinlock node.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void pv_init_node(struct mcs_spinlock *node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pv_node *pn = (struct pv_node *)node;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(struct pv_node) &gt; 5*sizeof(struct mcs_spinlock));</span>
<span class="p_add">+</span>
<span class="p_add">+	pn-&gt;cpu = smp_processor_id();</span>
<span class="p_add">+	pn-&gt;state = vcpu_running;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Wait for node-&gt;locked to become true, halt the vcpu after a short spin.</span>
<span class="p_add">+ * pv_kick_node() is used to wake the vcpu again.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void pv_wait_node(struct mcs_spinlock *node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pv_node *pn = (struct pv_node *)node;</span>
<span class="p_add">+	int loop;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (;;) {</span>
<span class="p_add">+		for (loop = SPIN_THRESHOLD; loop; loop--) {</span>
<span class="p_add">+			if (READ_ONCE(node-&gt;locked))</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			cpu_relax();</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Order pn-&gt;state vs pn-&gt;locked thusly:</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * [S] pn-&gt;state = vcpu_halted	  [S] next-&gt;locked = 1</span>
<span class="p_add">+		 *     MB			      MB</span>
<span class="p_add">+		 * [L] pn-&gt;locked		[RmW] pn-&gt;state = vcpu_running</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Matches the xchg() from pv_kick_node().</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		smp_store_mb(pn-&gt;state, vcpu_halted);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!READ_ONCE(node-&gt;locked))</span>
<span class="p_add">+			pv_wait(&amp;pn-&gt;state, vcpu_halted);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Reset the vCPU state to avoid unncessary CPU kicking</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WRITE_ONCE(pn-&gt;state, vcpu_running);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If the locked flag is still not set after wakeup, it is a</span>
<span class="p_add">+		 * spurious wakeup and the vCPU should wait again. However,</span>
<span class="p_add">+		 * there is a pretty high overhead for CPU halting and kicking.</span>
<span class="p_add">+		 * So it is better to spin for a while in the hope that the</span>
<span class="p_add">+		 * MCS lock will be released soon.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * By now our node-&gt;locked should be 1 and our caller will not actually</span>
<span class="p_add">+	 * spin-wait for it. We do however rely on our caller to do a</span>
<span class="p_add">+	 * load-acquire for us.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Called after setting next-&gt;locked = 1, used to wake those stuck in</span>
<span class="p_add">+ * pv_wait_node().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void pv_kick_node(struct mcs_spinlock *node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pv_node *pn = (struct pv_node *)node;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note that because node-&gt;locked is already set, this actual</span>
<span class="p_add">+	 * mcs_spinlock entry could be re-used already.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This should be fine however, kicking people for no reason is</span>
<span class="p_add">+	 * harmless.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * See the comment in pv_wait_node().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (xchg(&amp;pn-&gt;state, vcpu_running) == vcpu_halted)</span>
<span class="p_add">+		pv_kick(pn-&gt;cpu);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Wait for l-&gt;locked to become clear; halt the vcpu after a short spin.</span>
<span class="p_add">+ * __pv_queued_spin_unlock() will wake us.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void pv_wait_head(struct qspinlock *lock, struct mcs_spinlock *node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pv_node *pn = (struct pv_node *)node;</span>
<span class="p_add">+	struct __qspinlock *l = (void *)lock;</span>
<span class="p_add">+	struct qspinlock **lp = NULL;</span>
<span class="p_add">+	int loop;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (;;) {</span>
<span class="p_add">+		for (loop = SPIN_THRESHOLD; loop; loop--) {</span>
<span class="p_add">+			if (!READ_ONCE(l-&gt;locked))</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			cpu_relax();</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		WRITE_ONCE(pn-&gt;state, vcpu_halted);</span>
<span class="p_add">+		if (!lp) { /* ONCE */</span>
<span class="p_add">+			lp = pv_hash(lock, pn);</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * lp must be set before setting _Q_SLOW_VAL</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * [S] lp = lock                [RmW] l = l-&gt;locked = 0</span>
<span class="p_add">+			 *     MB                             MB</span>
<span class="p_add">+			 * [S] l-&gt;locked = _Q_SLOW_VAL  [L]   lp</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * Matches the cmpxchg() in __pv_queued_spin_unlock().</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (!cmpxchg(&amp;l-&gt;locked, _Q_LOCKED_VAL, _Q_SLOW_VAL)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * The lock is free and _Q_SLOW_VAL has never</span>
<span class="p_add">+				 * been set. Therefore we need to unhash before</span>
<span class="p_add">+				 * getting the lock.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				WRITE_ONCE(*lp, NULL);</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+		pv_wait(&amp;l-&gt;locked, _Q_SLOW_VAL);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * The unlocker should have freed the lock before kicking the</span>
<span class="p_add">+		 * CPU. So if the lock is still not free, it is a spurious</span>
<span class="p_add">+		 * wakeup and so the vCPU should wait again after spinning for</span>
<span class="p_add">+		 * a while.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Lock is unlocked now; the caller will acquire it without waiting.</span>
<span class="p_add">+	 * As with pv_wait_node() we rely on the caller to do a load-acquire</span>
<span class="p_add">+	 * for us.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PV version of the unlock function to be used in stead of</span>
<span class="p_add">+ * queued_spin_unlock().</span>
<span class="p_add">+ */</span>
<span class="p_add">+__visible void __pv_queued_spin_unlock(struct qspinlock *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct __qspinlock *l = (void *)lock;</span>
<span class="p_add">+	struct pv_node *node;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We must not unlock if SLOW, because in that case we must first</span>
<span class="p_add">+	 * unhash. Otherwise it would be possible to have multiple @lock</span>
<span class="p_add">+	 * entries, which would be BAD.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (likely(cmpxchg(&amp;l-&gt;locked, _Q_LOCKED_VAL, 0) == _Q_LOCKED_VAL))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Since the above failed to release, this must be the SLOW path.</span>
<span class="p_add">+	 * Therefore start by looking up the blocked node and unhashing it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	node = pv_unhash(lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Now that we have a reference to the (likely) blocked pv_node,</span>
<span class="p_add">+	 * release the lock.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	smp_store_release(&amp;l-&gt;locked, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At this point the memory pointed at by lock can be freed/reused,</span>
<span class="p_add">+	 * however we can still use the pv_node to kick the CPU.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (READ_ONCE(node-&gt;state) == vcpu_halted)</span>
<span class="p_add">+		pv_kick(node-&gt;cpu);</span>
<span class="p_add">+}</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Include the architecture specific callee-save thunk of the</span>
<span class="p_add">+ * __pv_queued_spin_unlock(). This thunk is put together with</span>
<span class="p_add">+ * __pv_queued_spin_unlock() near the top of the file to make sure</span>
<span class="p_add">+ * that the callee-save thunk and the real unlock function are close</span>
<span class="p_add">+ * to each other sharing consecutive instruction cachelines.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;asm/qspinlock_paravirt.h&gt;</span>
<span class="p_add">+</span>
<span class="p_header">diff --git a/kernel/locking/rtmutex.c b/kernel/locking/rtmutex.c</span>
<span class="p_header">index b73279367087..5fa042be94d5 100644</span>
<span class="p_header">--- a/kernel/locking/rtmutex.c</span>
<span class="p_header">+++ b/kernel/locking/rtmutex.c</span>
<span class="p_chunk">@@ -70,10 +70,10 @@</span> <span class="p_context"> static void fixup_rt_mutex_waiters(struct rt_mutex *lock)</span>
 }
 
 /*
<span class="p_del">- * We can speed up the acquire/release, if the architecture</span>
<span class="p_del">- * supports cmpxchg and if there&#39;s no debugging state to be set up</span>
<span class="p_add">+ * We can speed up the acquire/release, if there&#39;s no debugging state to be</span>
<span class="p_add">+ * set up.</span>
  */
<span class="p_del">-#if defined(__HAVE_ARCH_CMPXCHG) &amp;&amp; !defined(CONFIG_DEBUG_RT_MUTEXES)</span>
<span class="p_add">+#ifndef CONFIG_DEBUG_RT_MUTEXES</span>
 # define rt_mutex_cmpxchg(l,c,n)	(cmpxchg(&amp;l-&gt;owner, c, n) == c)
 static inline void mark_rt_mutex_waiters(struct rt_mutex *lock)
 {
<span class="p_chunk">@@ -1441,10 +1441,17 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(rt_mutex_timed_lock);</span>
  *
  * @lock:	the rt_mutex to be locked
  *
<span class="p_add">+ * This function can only be called in thread context. It&#39;s safe to</span>
<span class="p_add">+ * call it from atomic regions, but not from hard interrupt or soft</span>
<span class="p_add">+ * interrupt context.</span>
<span class="p_add">+ *</span>
  * Returns 1 on success and 0 on contention
  */
 int __sched rt_mutex_trylock(struct rt_mutex *lock)
 {
<span class="p_add">+	if (WARN_ON(in_irq() || in_nmi() || in_serving_softirq()))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	return rt_mutex_fasttrylock(lock, rt_mutex_slowtrylock);
 }
 EXPORT_SYMBOL_GPL(rt_mutex_trylock);
<span class="p_header">diff --git a/kernel/locking/rwsem-xadd.c b/kernel/locking/rwsem-xadd.c</span>
<span class="p_header">index 3417d0172a5d..0f189714e457 100644</span>
<span class="p_header">--- a/kernel/locking/rwsem-xadd.c</span>
<span class="p_header">+++ b/kernel/locking/rwsem-xadd.c</span>
<span class="p_chunk">@@ -409,11 +409,24 @@</span> <span class="p_context"> static bool rwsem_optimistic_spin(struct rw_semaphore *sem)</span>
 	return taken;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Return true if the rwsem has active spinner</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool rwsem_has_spinner(struct rw_semaphore *sem)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return osq_is_locked(&amp;sem-&gt;osq);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #else
 static bool rwsem_optimistic_spin(struct rw_semaphore *sem)
 {
 	return false;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline bool rwsem_has_spinner(struct rw_semaphore *sem)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
 #endif
 
 /*
<span class="p_chunk">@@ -496,7 +509,38 @@</span> <span class="p_context"> struct rw_semaphore *rwsem_wake(struct rw_semaphore *sem)</span>
 {
 	unsigned long flags;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If a spinner is present, it is not necessary to do the wakeup.</span>
<span class="p_add">+	 * Try to do wakeup only if the trylock succeeds to minimize</span>
<span class="p_add">+	 * spinlock contention which may introduce too much delay in the</span>
<span class="p_add">+	 * unlock operation.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *    spinning writer		up_write/up_read caller</span>
<span class="p_add">+	 *    ---------------		-----------------------</span>
<span class="p_add">+	 * [S]   osq_unlock()		[L]   osq</span>
<span class="p_add">+	 *	 MB			      RMB</span>
<span class="p_add">+	 * [RmW] rwsem_try_write_lock() [RmW] spin_trylock(wait_lock)</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Here, it is important to make sure that there won&#39;t be a missed</span>
<span class="p_add">+	 * wakeup while the rwsem is free and the only spinning writer goes</span>
<span class="p_add">+	 * to sleep without taking the rwsem. Even when the spinning writer</span>
<span class="p_add">+	 * is just going to break out of the waiting loop, it will still do</span>
<span class="p_add">+	 * a trylock in rwsem_down_write_failed() before sleeping. IOW, if</span>
<span class="p_add">+	 * rwsem_has_spinner() is true, it will guarantee at least one</span>
<span class="p_add">+	 * trylock attempt on the rwsem later on.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (rwsem_has_spinner(sem)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * The smp_rmb() here is to make sure that the spinner</span>
<span class="p_add">+		 * state is consulted before reading the wait_lock.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		smp_rmb();</span>
<span class="p_add">+		if (!raw_spin_trylock_irqsave(&amp;sem-&gt;wait_lock, flags))</span>
<span class="p_add">+			return sem;</span>
<span class="p_add">+		goto locked;</span>
<span class="p_add">+	}</span>
 	raw_spin_lock_irqsave(&amp;sem-&gt;wait_lock, flags);
<span class="p_add">+locked:</span>
 
 	/* do nothing if list empty */
 	if (!list_empty(&amp;sem-&gt;wait_list))
<span class="p_header">diff --git a/kernel/sched/wait.c b/kernel/sched/wait.c</span>
<span class="p_header">index 852143a79f36..9bc82329eaad 100644</span>
<span class="p_header">--- a/kernel/sched/wait.c</span>
<span class="p_header">+++ b/kernel/sched/wait.c</span>
<span class="p_chunk">@@ -341,7 +341,7 @@</span> <span class="p_context"> long wait_woken(wait_queue_t *wait, unsigned mode, long timeout)</span>
 	 * condition being true _OR_ WQ_FLAG_WOKEN such that we will not miss
 	 * an event.
 	 */
<span class="p_del">-	set_mb(wait-&gt;flags, wait-&gt;flags &amp; ~WQ_FLAG_WOKEN); /* B */</span>
<span class="p_add">+	smp_store_mb(wait-&gt;flags, wait-&gt;flags &amp; ~WQ_FLAG_WOKEN); /* B */</span>
 
 	return timeout;
 }
<span class="p_chunk">@@ -354,7 +354,7 @@</span> <span class="p_context"> int woken_wake_function(wait_queue_t *wait, unsigned mode, int sync, void *key)</span>
 	 * doesn&#39;t imply write barrier and the users expects write
 	 * barrier semantics on wakeup functions.  The following
 	 * smp_wmb() is equivalent to smp_wmb() in try_to_wake_up()
<span class="p_del">-	 * and is paired with set_mb() in wait_woken().</span>
<span class="p_add">+	 * and is paired with smp_store_mb() in wait_woken().</span>
 	 */
 	smp_wmb(); /* C */
 	wait-&gt;flags |= WQ_FLAG_WOKEN;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



