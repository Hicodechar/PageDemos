
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/3] mm: Send one IPI per CPU to TLB flush multiple pages that were recently unmapped - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/3] mm: Send one IPI per CPU to TLB flush multiple pages that were recently unmapped</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 8, 2015, 12:50 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1433767854-24408-3-git-send-email-mgorman@suse.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6564751/mbox/"
   >mbox</a>
|
   <a href="/patch/6564751/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6564751/">/patch/6564751/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 5F1E2C0020
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  8 Jun 2015 12:51:50 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id E468520398
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  8 Jun 2015 12:51:48 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 5556D20524
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  8 Jun 2015 12:51:47 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753122AbbFHMvi (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 8 Jun 2015 08:51:38 -0400
Received: from cantor2.suse.de ([195.135.220.15]:46120 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752100AbbFHMvA (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 8 Jun 2015 08:51:00 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 08884AC27;
	Mon,  8 Jun 2015 12:50:59 +0000 (UTC)
From: Mel Gorman &lt;mgorman@suse.de&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Minchan Kim &lt;minchan@kernel.org&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andi Kleen &lt;andi@firstfloor.org&gt;, H Peter Anvin &lt;hpa@zytor.com&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;, Linux-MM &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;
Subject: [PATCH 2/3] mm: Send one IPI per CPU to TLB flush multiple pages
	that were recently unmapped
Date: Mon,  8 Jun 2015 13:50:53 +0100
Message-Id: &lt;1433767854-24408-3-git-send-email-mgorman@suse.de&gt;
X-Mailer: git-send-email 2.3.5
In-Reply-To: &lt;1433767854-24408-1-git-send-email-mgorman@suse.de&gt;
References: &lt;1433767854-24408-1-git-send-email-mgorman@suse.de&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 8, 2015, 12:50 p.m.</div>
<pre class="content">
An IPI is sent to flush remote TLBs when a page is unmapped that was
recently accessed by other CPUs. There are many circumstances where this
happens but the obvious one is kswapd reclaiming pages belonging to a
running process as kswapd and the task are likely running on separate CPUs.

On small machines, this is not a significant problem but as machine
gets larger with more cores and more memory, the cost of these IPIs can
be high. This patch uses a structure similar in principle to a pagevec
to collect a list of PFNs and CPUs that require flushing. It then sends
one IPI per CPU that was mapping any of those pages to flush the list of
PFNs. A new TLB flush helper is required for this and one is added for
x86. Other architectures will need to decide if batching like this is both
safe and worth the memory overhead. Specifically the requirement is;

	If a clean page is unmapped and not immediately flushed, the
	architecture must guarantee that a write to that page from a CPU
	with a cached TLB entry will trap a page fault.

This is essentially what the kernel already depends on but the window is
much larger with this patch applied and is worth highlighting.

The impact of this patch depends on the workload as measuring any benefit
requires both mapped pages co-located on the LRU and memory pressure. The
case with the biggest impact is multiple processes reading mapped pages
taken from the vm-scalability test suite. The test case uses NR_CPU readers
of mapped files that consume 10*RAM.

vmscale on a 4-node machine with 64G RAM and 48 CPUs
           4.1.0-rc6     4.1.0-rc6
             vanilla batchunmap-v5
User          577.35        618.60
System       5927.06       4195.03
Elapsed       162.21        121.31

The workload completed 25% faster with 29% less CPU time.

This is showing that the readers completed 25% with 30% less CPU time. From
vmstats, it is known that the vanilla kernel was interrupted roughly 900K
times per second during the steady phase of the test and the patched kernel
was interrupts 180K times per second.

The impact is much lower on a small machine

vmscale on a 1-node machine with 8G RAM and 1 CPU
           4.1.0-rc6     4.1.0-rc6
             vanilla batchunmap-v5
User           59.14         58.86
System        109.15         83.78
Elapsed        27.32         23.14

It&#39;s still a noticeable improvement with vmstat showing interrupts went
from roughly 500K per second to 45K per second.

The patch will have no impact on workloads with no memory pressure or
have relatively few mapped pages.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@suse.de&gt;</span>
---
 arch/x86/Kconfig                |   1 +
 arch/x86/include/asm/tlbflush.h |   2 +
 include/linux/init_task.h       |   8 +++
 include/linux/rmap.h            |   3 ++
 include/linux/sched.h           |  14 ++++++
 init/Kconfig                    |   8 +++
 kernel/fork.c                   |   5 ++
 kernel/sched/core.c             |   3 ++
 mm/internal.h                   |  11 +++++
 mm/rmap.c                       | 105 +++++++++++++++++++++++++++++++++++++++-
 mm/vmscan.c                     |  23 ++++++++-
 11 files changed, 181 insertions(+), 2 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - June 8, 2015, 10:38 p.m.</div>
<pre class="content">
On Mon,  8 Jun 2015 13:50:53 +0100 Mel Gorman &lt;mgorman@suse.de&gt; wrote:
<span class="quote">
&gt; An IPI is sent to flush remote TLBs when a page is unmapped that was</span>
<span class="quote">&gt; recently accessed by other CPUs. There are many circumstances where this</span>
<span class="quote">&gt; happens but the obvious one is kswapd reclaiming pages belonging to a</span>
<span class="quote">&gt; running process as kswapd and the task are likely running on separate CPUs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On small machines, this is not a significant problem but as machine</span>
<span class="quote">&gt; gets larger with more cores and more memory, the cost of these IPIs can</span>
<span class="quote">&gt; be high. This patch uses a structure similar in principle to a pagevec</span>
<span class="quote">&gt; to collect a list of PFNs and CPUs that require flushing. It then sends</span>
<span class="quote">&gt; one IPI per CPU that was mapping any of those pages to flush the list of</span>
<span class="quote">&gt; PFNs. A new TLB flush helper is required for this and one is added for</span>
<span class="quote">&gt; x86. Other architectures will need to decide if batching like this is both</span>
<span class="quote">&gt; safe and worth the memory overhead. Specifically the requirement is;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	If a clean page is unmapped and not immediately flushed, the</span>
<span class="quote">&gt; 	architecture must guarantee that a write to that page from a CPU</span>
<span class="quote">&gt; 	with a cached TLB entry will trap a page fault.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is essentially what the kernel already depends on but the window is</span>
<span class="quote">&gt; much larger with this patch applied and is worth highlighting.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The impact of this patch depends on the workload as measuring any benefit</span>
<span class="quote">&gt; requires both mapped pages co-located on the LRU and memory pressure. The</span>
<span class="quote">&gt; case with the biggest impact is multiple processes reading mapped pages</span>
<span class="quote">&gt; taken from the vm-scalability test suite. The test case uses NR_CPU readers</span>
<span class="quote">&gt; of mapped files that consume 10*RAM.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; vmscale on a 4-node machine with 64G RAM and 48 CPUs</span>
<span class="quote">&gt;            4.1.0-rc6     4.1.0-rc6</span>
<span class="quote">&gt;              vanilla batchunmap-v5</span>
<span class="quote">&gt; User          577.35        618.60</span>
<span class="quote">&gt; System       5927.06       4195.03</span>
<span class="quote">&gt; Elapsed       162.21        121.31</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The workload completed 25% faster with 29% less CPU time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is showing that the readers completed 25% with 30% less CPU time. From</span>
<span class="quote">&gt; vmstats, it is known that the vanilla kernel was interrupted roughly 900K</span>
<span class="quote">&gt; times per second during the steady phase of the test and the patched kernel</span>
<span class="quote">&gt; was interrupts 180K times per second.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The impact is much lower on a small machine</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; vmscale on a 1-node machine with 8G RAM and 1 CPU</span>
<span class="quote">&gt;            4.1.0-rc6     4.1.0-rc6</span>
<span class="quote">&gt;              vanilla batchunmap-v5</span>
<span class="quote">&gt; User           59.14         58.86</span>
<span class="quote">&gt; System        109.15         83.78</span>
<span class="quote">&gt; Elapsed        27.32         23.14</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s still a noticeable improvement with vmstat showing interrupts went</span>
<span class="quote">&gt; from roughly 500K per second to 45K per second.</span>

Looks nice.
<span class="quote">
&gt; The patch will have no impact on workloads with no memory pressure or</span>
<span class="quote">&gt; have relatively few mapped pages.</span>

What benefit can we expect to see to any real-world userspace?
<span class="quote">
&gt; --- a/include/linux/init_task.h</span>
<span class="quote">&gt; +++ b/include/linux/init_task.h</span>
<span class="quote">&gt; @@ -175,6 +175,13 @@ extern struct task_group root_task_group;</span>
<span class="quote">&gt;  # define INIT_NUMA_BALANCING(tsk)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="quote">&gt; +# define INIT_TLBFLUSH_UNMAP_BATCH_CONTROL(tsk)				\</span>
<span class="quote">&gt; +	.tlb_ubc = NULL,</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +# define INIT_TLBFLUSH_UNMAP_BATCH_CONTROL(tsk)</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_KASAN</span>
<span class="quote">&gt;  # define INIT_KASAN(tsk)						\</span>
<span class="quote">&gt;  	.kasan_depth = 1,</span>
<span class="quote">&gt; @@ -257,6 +264,7 @@ extern struct task_group root_task_group;</span>
<span class="quote">&gt;  	INIT_RT_MUTEXES(tsk)						\</span>
<span class="quote">&gt;  	INIT_VTIME(tsk)							\</span>
<span class="quote">&gt;  	INIT_NUMA_BALANCING(tsk)					\</span>
<span class="quote">&gt; +	INIT_TLBFLUSH_UNMAP_BATCH_CONTROL(tsk)				\</span>
<span class="quote">&gt;  	INIT_KASAN(tsk)							\</span>
<span class="quote">&gt;  }</span>

We don&#39;t really need any of this - init_task starts up all-zero anyway.
Maybe it&#39;s useful for documentation reasons (dubious), but I bet we&#39;ve
already missed fields.
<span class="quote">
&gt;</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; --- a/include/linux/sched.h</span>
<span class="quote">&gt; +++ b/include/linux/sched.h</span>
<span class="quote">&gt; @@ -1289,6 +1289,16 @@ enum perf_event_task_context {</span>
<span class="quote">&gt;  	perf_nr_task_contexts,</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/* Matches SWAP_CLUSTER_MAX but refined to limit header dependencies */</span>
<span class="quote">&gt; +#define BATCH_TLBFLUSH_SIZE 32UL</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Track pages that require TLB flushes */</span>
<span class="quote">&gt; +struct tlbflush_unmap_batch {</span>
<span class="quote">&gt; +	struct cpumask cpumask;</span>
<span class="quote">&gt; +	unsigned long nr_pages;</span>
<span class="quote">&gt; +	unsigned long pfns[BATCH_TLBFLUSH_SIZE];</span>

Why are we storing pfn&#39;s rather than page*&#39;s?

I&#39;m trying to get my head around what&#39;s actually in this structure.

Each thread has one of these, lazily allocated &lt;under circumstances&gt;. 
The cpumask field contains a mask of all the CPUs which have done
&lt;something&gt;.  The careful reader will find mm_struct.cpu_vm_mask_var
and will wonder why it didn&#39;t need documenting, sigh.

Wanna fill in the blanks?  As usual, understanding the data structure
is the key to understanding the design, so it&#39;s worth a couple of
paragraphs.  With this knowledge, the reader may understand why
try_to_unmap_flush() has preempt_disable() protection but
set_tlb_ubc_flush_pending() doesn&#39;t need it!
<span class="quote">
&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  struct task_struct {</span>
<span class="quote">&gt;  	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */</span>
<span class="quote">&gt;  	void *stack;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -581,6 +583,90 @@ vma_address(struct page *page, struct vm_area_struct *vma)</span>
<span class="quote">&gt;  	return address;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="quote">&gt; +static void percpu_flush_tlb_batch_pages(void *data)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct tlbflush_unmap_batch *tlb_ubc = data;</span>
<span class="quote">&gt; +	int i;</span>

Anally speaking, this should be unsigned long (in which case it
shouldn&#39;t be called `i&#39;!).  Or make tlbflush_unmap_batch.nr_pages int. 
But int is signed, which is silly.  Whatever ;)
<span class="quote">

&gt; +	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);</span>
<span class="quote">&gt; +	for (i = 0; i &lt; tlb_ubc-&gt;nr_pages; i++)</span>
<span class="quote">&gt; +		flush_local_tlb_addr(tlb_ubc-&gt;pfns[i] &lt;&lt; PAGE_SHIFT);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Flush TLB entries for recently unmapped pages from remote CPUs. It is</span>
<span class="quote">&gt; + * important that if a PTE was dirty when it was unmapped that it&#39;s flushed</span>

s/important that/important /
<span class="quote">
&gt; + * before any IO is initiated on the page to prevent lost writes. Similarly,</span>
<span class="quote">&gt; + * it must be flushed before freeing to prevent data leakage.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +void try_to_unmap_flush(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;</span>
<span class="quote">&gt; +	int cpu;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!tlb_ubc || !tlb_ubc-&gt;nr_pages)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	trace_tlb_flush(TLB_REMOTE_SHOOTDOWN, tlb_ubc-&gt;nr_pages);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	preempt_disable();</span>
<span class="quote">&gt; +	cpu = smp_processor_id();</span>

get_cpu()
<span class="quote">
&gt; +	if (cpumask_test_cpu(cpu, &amp;tlb_ubc-&gt;cpumask))</span>
<span class="quote">&gt; +		percpu_flush_tlb_batch_pages(&amp;tlb_ubc-&gt;cpumask);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (cpumask_any_but(&amp;tlb_ubc-&gt;cpumask, cpu) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt; +		smp_call_function_many(&amp;tlb_ubc-&gt;cpumask,</span>
<span class="quote">&gt; +			percpu_flush_tlb_batch_pages, (void *)tlb_ubc, true);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	cpumask_clear(&amp;tlb_ubc-&gt;cpumask);</span>
<span class="quote">&gt; +	tlb_ubc-&gt;nr_pages = 0;</span>
<span class="quote">&gt; +	preempt_enable();</span>

put_cpu()
<span class="quote">
&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Returns true if the TLB flush should be deferred to the end of a batch of</span>
<span class="quote">&gt; + * unmap operations to reduce IPIs.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	bool should_defer = false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!current-&gt;tlb_ubc || !(flags &amp; TTU_BATCH_FLUSH))</span>
<span class="quote">&gt; +		return false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* If remote CPUs need to be flushed then defer batch the flush */</span>
<span class="quote">&gt; +	if (cpumask_any_but(mm_cpumask(mm), get_cpu()) &lt; nr_cpu_ids)</span>
<span class="quote">&gt; +		should_defer = true;</span>
<span class="quote">&gt; +	put_cpu();</span>

What did the get_cpu() protect?
<span class="quote">
&gt; +	return should_defer;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
<span class="quote">&gt; +		struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Allocate the control structure for batch TLB flushing. An allocation</span>
<span class="quote">&gt; + * failure is harmless as the reclaimer will send IPIs where necessary.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static void alloc_tlb_ubc(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!current-&gt;tlb_ubc)</span>
<span class="quote">&gt; +		current-&gt;tlb_ubc = kzalloc(sizeof(struct tlbflush_unmap_batch),</span>
<span class="quote">&gt; +						GFP_KERNEL | __GFP_NOWARN);</span>

A GFP_KERNEL allocation from deep within page reclaim?  Seems imprudent
if only from a stack-usage POV.
<span class="quote">
&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static inline void alloc_tlb_ubc(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif /* CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 9, 2015, 11:07 a.m.</div>
<pre class="content">
On Mon, Jun 08, 2015 at 03:38:13PM -0700, Andrew Morton wrote:
<span class="quote">&gt; On Mon,  8 Jun 2015 13:50:53 +0100 Mel Gorman &lt;mgorman@suse.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; An IPI is sent to flush remote TLBs when a page is unmapped that was</span>
<span class="quote">&gt; &gt; recently accessed by other CPUs. There are many circumstances where this</span>
<span class="quote">&gt; &gt; happens but the obvious one is kswapd reclaiming pages belonging to a</span>
<span class="quote">&gt; &gt; running process as kswapd and the task are likely running on separate CPUs.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On small machines, this is not a significant problem but as machine</span>
<span class="quote">&gt; &gt; gets larger with more cores and more memory, the cost of these IPIs can</span>
<span class="quote">&gt; &gt; be high. This patch uses a structure similar in principle to a pagevec</span>
<span class="quote">&gt; &gt; to collect a list of PFNs and CPUs that require flushing. It then sends</span>
<span class="quote">&gt; &gt; one IPI per CPU that was mapping any of those pages to flush the list of</span>
<span class="quote">&gt; &gt; PFNs. A new TLB flush helper is required for this and one is added for</span>
<span class="quote">&gt; &gt; x86. Other architectures will need to decide if batching like this is both</span>
<span class="quote">&gt; &gt; safe and worth the memory overhead. Specifically the requirement is;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	If a clean page is unmapped and not immediately flushed, the</span>
<span class="quote">&gt; &gt; 	architecture must guarantee that a write to that page from a CPU</span>
<span class="quote">&gt; &gt; 	with a cached TLB entry will trap a page fault.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This is essentially what the kernel already depends on but the window is</span>
<span class="quote">&gt; &gt; much larger with this patch applied and is worth highlighting.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The impact of this patch depends on the workload as measuring any benefit</span>
<span class="quote">&gt; &gt; requires both mapped pages co-located on the LRU and memory pressure. The</span>
<span class="quote">&gt; &gt; case with the biggest impact is multiple processes reading mapped pages</span>
<span class="quote">&gt; &gt; taken from the vm-scalability test suite. The test case uses NR_CPU readers</span>
<span class="quote">&gt; &gt; of mapped files that consume 10*RAM.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; vmscale on a 4-node machine with 64G RAM and 48 CPUs</span>
<span class="quote">&gt; &gt;            4.1.0-rc6     4.1.0-rc6</span>
<span class="quote">&gt; &gt;              vanilla batchunmap-v5</span>
<span class="quote">&gt; &gt; User          577.35        618.60</span>
<span class="quote">&gt; &gt; System       5927.06       4195.03</span>
<span class="quote">&gt; &gt; Elapsed       162.21        121.31</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The workload completed 25% faster with 29% less CPU time.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This is showing that the readers completed 25% with 30% less CPU time. From</span>
<span class="quote">&gt; &gt; vmstats, it is known that the vanilla kernel was interrupted roughly 900K</span>
<span class="quote">&gt; &gt; times per second during the steady phase of the test and the patched kernel</span>
<span class="quote">&gt; &gt; was interrupts 180K times per second.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The impact is much lower on a small machine</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; vmscale on a 1-node machine with 8G RAM and 1 CPU</span>
<span class="quote">&gt; &gt;            4.1.0-rc6     4.1.0-rc6</span>
<span class="quote">&gt; &gt;              vanilla batchunmap-v5</span>
<span class="quote">&gt; &gt; User           59.14         58.86</span>
<span class="quote">&gt; &gt; System        109.15         83.78</span>
<span class="quote">&gt; &gt; Elapsed        27.32         23.14</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It&#39;s still a noticeable improvement with vmstat showing interrupts went</span>
<span class="quote">&gt; &gt; from roughly 500K per second to 45K per second.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Looks nice.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; The patch will have no impact on workloads with no memory pressure or</span>
<span class="quote">&gt; &gt; have relatively few mapped pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What benefit can we expect to see to any real-world userspace?</span>
<span class="quote">&gt; </span>

Only a small subset of workloads will see a benefit -- ones that mmap a
lot of data with working sets larger than the size of a node. Some
streaming media servers allegedly do this.

Some numerical processing applications may hit this. Those that use glibc
for large buffers use mmap and if the application is larger than a NUMA
node, it&#39;ll need to be unmapped and flushed. Python/NumPY uses large
maps for large buffers (based on the paper &quot;Doubling the Performance of
Python/Numpy with less than 100 SLOC&quot;). Whether users of NumPY hit this
issue or not depends on whether kswapd is active.

Anecdotally, I&#39;m aware from IRC of one user that is experimenting with
a large HTTP cache and a load generator that spent a lot of time sending
IPIs that was obvious from the profile. I asked weeks ago that they post
the results here which they promised they would but didn&#39;t. Unfortunately,
I don&#39;t know the persons real name to cc them. Rik might.

Anecdotally, I also believe that Intel hit this internally with some
internal workload but I&#39;m basing this on idle comments at LSF/MM. However
they were unwilling or unable to describe exactly what the test does and
against what software.

I know this is more vague than you&#39;d like. By and large, I&#39;m relying on
the assumption that if we are reclaiming mapped pages from kswapd context
then sending one IPI per page is stupid.
<span class="quote">
&gt; &gt; --- a/include/linux/init_task.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/init_task.h</span>
<span class="quote">&gt; &gt; @@ -175,6 +175,13 @@ extern struct task_group root_task_group;</span>
<span class="quote">&gt; &gt;  # define INIT_NUMA_BALANCING(tsk)</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="quote">&gt; &gt; +# define INIT_TLBFLUSH_UNMAP_BATCH_CONTROL(tsk)				\</span>
<span class="quote">&gt; &gt; +	.tlb_ubc = NULL,</span>
<span class="quote">&gt; &gt; +#else</span>
<span class="quote">&gt; &gt; +# define INIT_TLBFLUSH_UNMAP_BATCH_CONTROL(tsk)</span>
<span class="quote">&gt; &gt; +#endif</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  #ifdef CONFIG_KASAN</span>
<span class="quote">&gt; &gt;  # define INIT_KASAN(tsk)						\</span>
<span class="quote">&gt; &gt;  	.kasan_depth = 1,</span>
<span class="quote">&gt; &gt; @@ -257,6 +264,7 @@ extern struct task_group root_task_group;</span>
<span class="quote">&gt; &gt;  	INIT_RT_MUTEXES(tsk)						\</span>
<span class="quote">&gt; &gt;  	INIT_VTIME(tsk)							\</span>
<span class="quote">&gt; &gt;  	INIT_NUMA_BALANCING(tsk)					\</span>
<span class="quote">&gt; &gt; +	INIT_TLBFLUSH_UNMAP_BATCH_CONTROL(tsk)				\</span>
<span class="quote">&gt; &gt;  	INIT_KASAN(tsk)							\</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We don&#39;t really need any of this - init_task starts up all-zero anyway.</span>
<span class="quote">&gt; Maybe it&#39;s useful for documentation reasons (dubious), but I bet we&#39;ve</span>
<span class="quote">&gt; already missed fields.</span>
<span class="quote">&gt; </span>

True. I&#39;ll look into removing it and make sure there is no adverse
impact.
<span class="quote">
&gt; &gt;</span>
<span class="quote">&gt; &gt; ...</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; --- a/include/linux/sched.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/sched.h</span>
<span class="quote">&gt; &gt; @@ -1289,6 +1289,16 @@ enum perf_event_task_context {</span>
<span class="quote">&gt; &gt;  	perf_nr_task_contexts,</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +/* Matches SWAP_CLUSTER_MAX but refined to limit header dependencies */</span>
<span class="quote">&gt; &gt; +#define BATCH_TLBFLUSH_SIZE 32UL</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/* Track pages that require TLB flushes */</span>
<span class="quote">&gt; &gt; +struct tlbflush_unmap_batch {</span>
<span class="quote">&gt; &gt; +	struct cpumask cpumask;</span>
<span class="quote">&gt; &gt; +	unsigned long nr_pages;</span>
<span class="quote">&gt; &gt; +	unsigned long pfns[BATCH_TLBFLUSH_SIZE];</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why are we storing pfn&#39;s rather than page*&#39;s?</span>
<span class="quote">&gt; </span>

Because a page would require a page-&gt;pfn lookup within the IPI handler.
That will work but it&#39;s unnecessarily wasteful.
<span class="quote">
&gt; I&#39;m trying to get my head around what&#39;s actually in this structure.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Each thread has one of these, lazily allocated &lt;under circumstances&gt;. </span>
<span class="quote">&gt; The cpumask field contains a mask of all the CPUs which have done</span>
<span class="quote">&gt; &lt;something&gt;.  The careful reader will find mm_struct.cpu_vm_mask_var</span>
<span class="quote">&gt; and will wonder why it didn&#39;t need documenting, sigh.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Wanna fill in the blanks?  As usual, understanding the data structure</span>
<span class="quote">&gt; is the key to understanding the design, so it&#39;s worth a couple of</span>
<span class="quote">&gt; paragraphs.  With this knowledge, the reader may understand why</span>
<span class="quote">&gt; try_to_unmap_flush() has preempt_disable() protection but</span>
<span class="quote">&gt; set_tlb_ubc_flush_pending() doesn&#39;t need it!</span>
<span class="quote">&gt; </span>

Is this any help?

struct tlbflush_unmap_batch {
        /*
         * Each bit set is a CPU that potentially has a TLB entry for one of
         * the PFNs being flushed. See set_tlb_ubc_flush_pending().
         */
        struct cpumask cpumask;

        /*
         * The number and list of pfns to be flushed. PFNs are tracked instead
         * of struct pages to avoid multiple page-&gt;pfn lookups by each CPU that
         * receives an IPI in percpu_flush_tlb_batch_pages
         */
        unsigned long nr_pages;
        unsigned long pfns[BATCH_TLBFLUSH_SIZE];

        /*
         * If true then the PTE was dirty when unmapped. The entry must be
         * flushed before IO is initiated or a stale TLB entry potentially
         * allows an update without redirtying the page.
         */
        bool writable;
};
<span class="quote">
&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  struct task_struct {</span>
<span class="quote">&gt; &gt;  	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */</span>
<span class="quote">&gt; &gt;  	void *stack;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; ...</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; @@ -581,6 +583,90 @@ vma_address(struct page *page, struct vm_area_struct *vma)</span>
<span class="quote">&gt; &gt;  	return address;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="quote">&gt; &gt; +static void percpu_flush_tlb_batch_pages(void *data)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct tlbflush_unmap_batch *tlb_ubc = data;</span>
<span class="quote">&gt; &gt; +	int i;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Anally speaking, this should be unsigned long (in which case it</span>
<span class="quote">&gt; shouldn&#39;t be called `i&#39;!).  Or make tlbflush_unmap_batch.nr_pages int. </span>
<span class="quote">&gt; But int is signed, which is silly.  Whatever ;)</span>
<span class="quote">&gt; </span>

I can make it unsigned int which is a micro-optimisation for loop iterators
anyway.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);</span>
<span class="quote">&gt; &gt; +	for (i = 0; i &lt; tlb_ubc-&gt;nr_pages; i++)</span>
<span class="quote">&gt; &gt; +		flush_local_tlb_addr(tlb_ubc-&gt;pfns[i] &lt;&lt; PAGE_SHIFT);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Flush TLB entries for recently unmapped pages from remote CPUs. It is</span>
<span class="quote">&gt; &gt; + * important that if a PTE was dirty when it was unmapped that it&#39;s flushed</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; s/important that/important /</span>
<span class="quote">&gt; </span>

Fixed.
<span class="quote">
&gt; &gt; + * before any IO is initiated on the page to prevent lost writes. Similarly,</span>
<span class="quote">&gt; &gt; + * it must be flushed before freeing to prevent data leakage.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +void try_to_unmap_flush(void)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;</span>
<span class="quote">&gt; &gt; +	int cpu;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!tlb_ubc || !tlb_ubc-&gt;nr_pages)</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	trace_tlb_flush(TLB_REMOTE_SHOOTDOWN, tlb_ubc-&gt;nr_pages);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	preempt_disable();</span>
<span class="quote">&gt; &gt; +	cpu = smp_processor_id();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; get_cpu()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	if (cpumask_test_cpu(cpu, &amp;tlb_ubc-&gt;cpumask))</span>
<span class="quote">&gt; &gt; +		percpu_flush_tlb_batch_pages(&amp;tlb_ubc-&gt;cpumask);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (cpumask_any_but(&amp;tlb_ubc-&gt;cpumask, cpu) &lt; nr_cpu_ids) {</span>
<span class="quote">&gt; &gt; +		smp_call_function_many(&amp;tlb_ubc-&gt;cpumask,</span>
<span class="quote">&gt; &gt; +			percpu_flush_tlb_batch_pages, (void *)tlb_ubc, true);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +	cpumask_clear(&amp;tlb_ubc-&gt;cpumask);</span>
<span class="quote">&gt; &gt; +	tlb_ubc-&gt;nr_pages = 0;</span>
<span class="quote">&gt; &gt; +	preempt_enable();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; put_cpu()</span>
<span class="quote">&gt; </span>

Gack. Fixed.
<span class="quote">
&gt; &gt; +}</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; ...</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Returns true if the TLB flush should be deferred to the end of a batch of</span>
<span class="quote">&gt; &gt; + * unmap operations to reduce IPIs.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	bool should_defer = false;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!current-&gt;tlb_ubc || !(flags &amp; TTU_BATCH_FLUSH))</span>
<span class="quote">&gt; &gt; +		return false;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/* If remote CPUs need to be flushed then defer batch the flush */</span>
<span class="quote">&gt; &gt; +	if (cpumask_any_but(mm_cpumask(mm), get_cpu()) &lt; nr_cpu_ids)</span>
<span class="quote">&gt; &gt; +		should_defer = true;</span>
<span class="quote">&gt; &gt; +	put_cpu();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What did the get_cpu() protect?</span>
<span class="quote">&gt; </span>

To safely lookup the current running CPU number. smp_processor_id() is
potentially safe except in specific circumstances and I did not think
raw_smp_processor_id() was justified.
<span class="quote">
&gt; &gt; +	return should_defer;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +#else</span>
<span class="quote">&gt; &gt; +static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
<span class="quote">&gt; &gt; +		struct page *page)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; ...</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; +#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Allocate the control structure for batch TLB flushing. An allocation</span>
<span class="quote">&gt; &gt; + * failure is harmless as the reclaimer will send IPIs where necessary.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +static void alloc_tlb_ubc(void)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	if (!current-&gt;tlb_ubc)</span>
<span class="quote">&gt; &gt; +		current-&gt;tlb_ubc = kzalloc(sizeof(struct tlbflush_unmap_batch),</span>
<span class="quote">&gt; &gt; +						GFP_KERNEL | __GFP_NOWARN);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A GFP_KERNEL allocation from deep within page reclaim?  Seems imprudent</span>
<span class="quote">&gt; if only from a stack-usage POV.</span>
<span class="quote">&gt; </span>

When we call it, we are in PF_MEMALLOC context either set in the page
allocator from direct reclaim or because kswapd always sets it. This limits
the stack depth. It would be comparable to the depth we reach during normal
reclaim calling into shrink_page_list and everything below it so we should
be safe. Granted, the dependency on PF_MEMALLOC is not obvious at all.
<span class="quote">
&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +#else</span>
<span class="quote">&gt; &gt; +static inline void alloc_tlb_ubc(void)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +#endif /* CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH */</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; ...</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 226d5696e1d1..4e8bd86735af 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -31,6 +31,7 @@</span> <span class="p_context"> config X86</span>
 	select ARCH_MIGHT_HAVE_PC_SERIO
 	select HAVE_AOUT if X86_32
 	select HAVE_UNSTABLE_SCHED_CLOCK
<span class="p_add">+	select ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
 	select ARCH_SUPPORTS_NUMA_BALANCING if X86_64
 	select ARCH_SUPPORTS_INT128 if X86_64
 	select HAVE_IDE
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index cd791948b286..10c197a649f5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -152,6 +152,8 @@</span> <span class="p_context"> static inline void __flush_tlb_one(unsigned long addr)</span>
  * and page-granular flushes are available only on i486 and up.
  */
 
<span class="p_add">+#define flush_local_tlb_addr(addr) __flush_tlb_single(addr)</span>
<span class="p_add">+</span>
 #ifndef CONFIG_SMP
 
 /* &quot;_up&quot; is for UniProcessor.
<span class="p_header">diff --git a/include/linux/init_task.h b/include/linux/init_task.h</span>
<span class="p_header">index 696d22312b31..0771937b47e1 100644</span>
<span class="p_header">--- a/include/linux/init_task.h</span>
<span class="p_header">+++ b/include/linux/init_task.h</span>
<span class="p_chunk">@@ -175,6 +175,13 @@</span> <span class="p_context"> extern struct task_group root_task_group;</span>
 # define INIT_NUMA_BALANCING(tsk)
 #endif
 
<span class="p_add">+#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="p_add">+# define INIT_TLBFLUSH_UNMAP_BATCH_CONTROL(tsk)				\</span>
<span class="p_add">+	.tlb_ubc = NULL,</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define INIT_TLBFLUSH_UNMAP_BATCH_CONTROL(tsk)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_KASAN
 # define INIT_KASAN(tsk)						\
 	.kasan_depth = 1,
<span class="p_chunk">@@ -257,6 +264,7 @@</span> <span class="p_context"> extern struct task_group root_task_group;</span>
 	INIT_RT_MUTEXES(tsk)						\
 	INIT_VTIME(tsk)							\
 	INIT_NUMA_BALANCING(tsk)					\
<span class="p_add">+	INIT_TLBFLUSH_UNMAP_BATCH_CONTROL(tsk)				\</span>
 	INIT_KASAN(tsk)							\
 }
 
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index c89c53a113a8..29446aeef36e 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -89,6 +89,9 @@</span> <span class="p_context"> enum ttu_flags {</span>
 	TTU_IGNORE_MLOCK = (1 &lt;&lt; 8),	/* ignore mlock */
 	TTU_IGNORE_ACCESS = (1 &lt;&lt; 9),	/* don&#39;t age */
 	TTU_IGNORE_HWPOISON = (1 &lt;&lt; 10),/* corrupted page is recoverable */
<span class="p_add">+	TTU_BATCH_FLUSH = (1 &lt;&lt; 11),	/* Batch TLB flushes where possible</span>
<span class="p_add">+					 * and caller guarantees they will</span>
<span class="p_add">+					 * do a final flush if necessary */</span>
 };
 
 #ifdef CONFIG_MMU
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 26a2e6122734..57ff61b16565 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -1289,6 +1289,16 @@</span> <span class="p_context"> enum perf_event_task_context {</span>
 	perf_nr_task_contexts,
 };
 
<span class="p_add">+/* Matches SWAP_CLUSTER_MAX but refined to limit header dependencies */</span>
<span class="p_add">+#define BATCH_TLBFLUSH_SIZE 32UL</span>
<span class="p_add">+</span>
<span class="p_add">+/* Track pages that require TLB flushes */</span>
<span class="p_add">+struct tlbflush_unmap_batch {</span>
<span class="p_add">+	struct cpumask cpumask;</span>
<span class="p_add">+	unsigned long nr_pages;</span>
<span class="p_add">+	unsigned long pfns[BATCH_TLBFLUSH_SIZE];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 struct task_struct {
 	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */
 	void *stack;
<span class="p_chunk">@@ -1648,6 +1658,10 @@</span> <span class="p_context"> struct task_struct {</span>
 	unsigned long numa_pages_migrated;
 #endif /* CONFIG_NUMA_BALANCING */
 
<span class="p_add">+#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	struct rcu_head rcu;
 
 	/*
<span class="p_header">diff --git a/init/Kconfig b/init/Kconfig</span>
<span class="p_header">index dc24dec60232..1ff5d17e518a 100644</span>
<span class="p_header">--- a/init/Kconfig</span>
<span class="p_header">+++ b/init/Kconfig</span>
<span class="p_chunk">@@ -904,6 +904,14 @@</span> <span class="p_context"> config ARCH_SUPPORTS_NUMA_BALANCING</span>
 	bool
 
 #
<span class="p_add">+# For architectures that have a local TLB flush for a PFN without knowledge</span>
<span class="p_add">+# of the VMA. The architecture must provide guarantees on what happens if</span>
<span class="p_add">+# a clean TLB cache entry is written after the unmap. Details are in mm/rmap.c</span>
<span class="p_add">+# near the check for should_defer_flush.</span>
<span class="p_add">+config ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
<span class="p_add">+#</span>
 # For architectures that know their GCC __int128 support is sound
 #
 config ARCH_SUPPORTS_INT128
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 03c1eaaa6ef5..8ea9e8730ada 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -257,6 +257,11 @@</span> <span class="p_context"> void __put_task_struct(struct task_struct *tsk)</span>
 	delayacct_tsk_free(tsk);
 	put_signal_struct(tsk-&gt;signal);
 
<span class="p_add">+#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="p_add">+	kfree(tsk-&gt;tlb_ubc);</span>
<span class="p_add">+	tsk-&gt;tlb_ubc = NULL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	if (!profile_handoff_task(tsk))
 		free_task(tsk);
 }
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index 123673291ffb..936ce13aeb97 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -1843,6 +1843,9 @@</span> <span class="p_context"> static void __sched_fork(unsigned long clone_flags, struct task_struct *p)</span>
 
 	p-&gt;numa_group = NULL;
 #endif /* CONFIG_NUMA_BALANCING */
<span class="p_add">+#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="p_add">+	p-&gt;tlb_ubc = NULL;</span>
<span class="p_add">+#endif /* CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH */</span>
 }
 
 #ifdef CONFIG_NUMA_BALANCING
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index a25e359a4039..8cbb68ccc731 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -433,4 +433,15 @@</span> <span class="p_context"> unsigned long reclaim_clean_pages_from_list(struct zone *zone,</span>
 #define ALLOC_CMA		0x80 /* allow allocations from CMA areas */
 #define ALLOC_FAIR		0x100 /* fair zone allocation */
 
<span class="p_add">+enum ttu_flags;</span>
<span class="p_add">+struct tlbflush_unmap_batch;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="p_add">+void try_to_unmap_flush(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void try_to_unmap_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH */</span>
 #endif	/* __MM_INTERNAL_H */
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 24dd3f9fee27..a8dbba62398a 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -60,6 +60,8 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/tlbflush.h&gt;
 
<span class="p_add">+#include &lt;trace/events/tlb.h&gt;</span>
<span class="p_add">+</span>
 #include &quot;internal.h&quot;
 
 static struct kmem_cache *anon_vma_cachep;
<span class="p_chunk">@@ -581,6 +583,90 @@</span> <span class="p_context"> vma_address(struct page *page, struct vm_area_struct *vma)</span>
 	return address;
 }
 
<span class="p_add">+#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="p_add">+static void percpu_flush_tlb_batch_pages(void *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = data;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);</span>
<span class="p_add">+	for (i = 0; i &lt; tlb_ubc-&gt;nr_pages; i++)</span>
<span class="p_add">+		flush_local_tlb_addr(tlb_ubc-&gt;pfns[i] &lt;&lt; PAGE_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Flush TLB entries for recently unmapped pages from remote CPUs. It is</span>
<span class="p_add">+ * important that if a PTE was dirty when it was unmapped that it&#39;s flushed</span>
<span class="p_add">+ * before any IO is initiated on the page to prevent lost writes. Similarly,</span>
<span class="p_add">+ * it must be flushed before freeing to prevent data leakage.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void try_to_unmap_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!tlb_ubc || !tlb_ubc-&gt;nr_pages)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	trace_tlb_flush(TLB_REMOTE_SHOOTDOWN, tlb_ubc-&gt;nr_pages);</span>
<span class="p_add">+</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	cpu = smp_processor_id();</span>
<span class="p_add">+	if (cpumask_test_cpu(cpu, &amp;tlb_ubc-&gt;cpumask))</span>
<span class="p_add">+		percpu_flush_tlb_batch_pages(&amp;tlb_ubc-&gt;cpumask);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpumask_any_but(&amp;tlb_ubc-&gt;cpumask, cpu) &lt; nr_cpu_ids) {</span>
<span class="p_add">+		smp_call_function_many(&amp;tlb_ubc-&gt;cpumask,</span>
<span class="p_add">+			percpu_flush_tlb_batch_pages, (void *)tlb_ubc, true);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	cpumask_clear(&amp;tlb_ubc-&gt;cpumask);</span>
<span class="p_add">+	tlb_ubc-&gt;nr_pages = 0;</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;</span>
<span class="p_add">+</span>
<span class="p_add">+	cpumask_or(&amp;tlb_ubc-&gt;cpumask, &amp;tlb_ubc-&gt;cpumask, mm_cpumask(mm));</span>
<span class="p_add">+	tlb_ubc-&gt;pfns[tlb_ubc-&gt;nr_pages] = page_to_pfn(page);</span>
<span class="p_add">+	tlb_ubc-&gt;nr_pages++;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tlb_ubc-&gt;nr_pages == BATCH_TLBFLUSH_SIZE)</span>
<span class="p_add">+		try_to_unmap_flush();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Returns true if the TLB flush should be deferred to the end of a batch of</span>
<span class="p_add">+ * unmap operations to reduce IPIs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool should_defer = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!current-&gt;tlb_ubc || !(flags &amp; TTU_BATCH_FLUSH))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If remote CPUs need to be flushed then defer batch the flush */</span>
<span class="p_add">+	if (cpumask_any_but(mm_cpumask(mm), get_cpu()) &lt; nr_cpu_ids)</span>
<span class="p_add">+		should_defer = true;</span>
<span class="p_add">+	put_cpu();</span>
<span class="p_add">+</span>
<span class="p_add">+	return should_defer;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH */</span>
<span class="p_add">+</span>
 /*
  * At what user virtual address is page expected in vma?
  * Caller should check the page is actually part of the vma.
<span class="p_chunk">@@ -1213,7 +1299,24 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 
 	/* Nuke the page table entry. */
 	flush_cache_page(vma, address, page_to_pfn(page));
<span class="p_del">-	pteval = ptep_clear_flush(vma, address, pte);</span>
<span class="p_add">+	if (should_defer_flush(mm, flags)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We clear the PTE but do not flush so potentially a remote</span>
<span class="p_add">+		 * CPU could still be writing to the page. If the entry was</span>
<span class="p_add">+		 * previously clean then the architecture must guarantee that</span>
<span class="p_add">+		 * a clear-&gt;dirty transition on a cached TLB entry is written</span>
<span class="p_add">+		 * through and traps if the PTE is unmapped.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pteval = ptep_get_and_clear(mm, address, pte);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Potentially writable TLBs must be flushed before IO */</span>
<span class="p_add">+		if (pte_dirty(pteval))</span>
<span class="p_add">+			flush_tlb_page(vma, address);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			set_tlb_ubc_flush_pending(mm, page);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pteval = ptep_clear_flush(vma, address, pte);</span>
<span class="p_add">+	}</span>
 
 	/* Move the dirty bit to the physical page now the pte is gone. */
 	if (pte_dirty(pteval))
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 5e8eadd71bac..5121742ccb87 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -1024,7 +1024,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 * processes. Try to unmap it here.
 		 */
 		if (page_mapped(page) &amp;&amp; mapping) {
<span class="p_del">-			switch (try_to_unmap(page, ttu_flags)) {</span>
<span class="p_add">+			switch (try_to_unmap(page,</span>
<span class="p_add">+					ttu_flags|TTU_BATCH_FLUSH)) {</span>
 			case SWAP_FAIL:
 				goto activate_locked;
 			case SWAP_AGAIN:
<span class="p_chunk">@@ -1175,6 +1176,7 @@</span> <span class="p_context"> keep:</span>
 	}
 
 	mem_cgroup_uncharge_list(&amp;free_pages);
<span class="p_add">+	try_to_unmap_flush();</span>
 	free_hot_cold_page_list(&amp;free_pages, true);
 
 	list_splice(&amp;ret_pages, page_list);
<span class="p_chunk">@@ -2118,6 +2120,23 @@</span> <span class="p_context"> out:</span>
 	}
 }
 
<span class="p_add">+#ifdef CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Allocate the control structure for batch TLB flushing. An allocation</span>
<span class="p_add">+ * failure is harmless as the reclaimer will send IPIs where necessary.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void alloc_tlb_ubc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!current-&gt;tlb_ubc)</span>
<span class="p_add">+		current-&gt;tlb_ubc = kzalloc(sizeof(struct tlbflush_unmap_batch),</span>
<span class="p_add">+						GFP_KERNEL | __GFP_NOWARN);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void alloc_tlb_ubc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_ARCH_SUPPORTS_LOCAL_TLB_PFN_FLUSH */</span>
<span class="p_add">+</span>
 /*
  * This is a basic per-zone page freer.  Used by both kswapd and direct reclaim.
  */
<span class="p_chunk">@@ -2152,6 +2171,8 @@</span> <span class="p_context"> static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
 	scan_adjusted = (global_reclaim(sc) &amp;&amp; !current_is_kswapd() &amp;&amp;
 			 sc-&gt;priority == DEF_PRIORITY);
 
<span class="p_add">+	alloc_tlb_ubc();</span>
<span class="p_add">+</span>
 	blk_start_plug(&amp;plug);
 	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
 					nr[LRU_INACTIVE_FILE]) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



