
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[05/36] HMM: introduce heterogeneous memory management v3. - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [05/36] HMM: introduce heterogeneous memory management v3.</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 21, 2015, 7:31 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1432236705-4209-6-git-send-email-j.glisse@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6458471/mbox/"
   >mbox</a>
|
   <a href="/patch/6458471/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6458471/">/patch/6458471/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id DFD7C9F1C1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 21 May 2015 19:34:45 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 0C8F820513
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 21 May 2015 19:34:44 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id EB0942052D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 21 May 2015 19:34:41 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756547AbbEUTdw (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 21 May 2015 15:33:52 -0400
Received: from mail-qk0-f182.google.com ([209.85.220.182]:33381 &quot;EHLO
	mail-qk0-f182.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1756395AbbEUTdc (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 21 May 2015 15:33:32 -0400
Received: by qkgv12 with SMTP id v12so64232938qkg.0;
	Thu, 21 May 2015 12:33:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=from:to:cc:subject:date:message-id:in-reply-to:references
	:mime-version:content-type:content-transfer-encoding;
	bh=0fwdvEqq4zS+osUlB18IcTq2EXGqalnFsGBmU5Z4a6Y=;
	b=GBcEv6iHj6rtGwFScqnBjd1ObH5s1tXBYhfINQNX+HXiSJfHD0IvF5xGx9Vwt7Znpw
	xPw7K7qtGnj25x8/QZ43t6MzEog+nXtKwlK+TmuHxWs6zJ0jb/rR7liWMETupeQ02Txt
	sPmwYBXWKY6le+QtHuSjuSBpIXCA9T4i51CgoDKQt2jI+DH6Rc/W4KYFF+FanL4NBYos
	ptAdGB/PKarcoRJuOgkdS85vxkSuPrJw4ox7ZZuP55hoZkWPcnnu/mKSfF5Cgyo5ck2g
	qEgJnnGTjBzNd5D5fq4ocmHCbvva6wvSLrarFVTkmL9G2iYq5G4HKZUClTTY4Pbc2mh9
	CmLw==
X-Received: by 10.140.83.116 with SMTP id i107mr5897429qgd.97.1432236811823; 
	Thu, 21 May 2015 12:33:31 -0700 (PDT)
Received: from localhost.localdomain.com (nat-pool-bos-t.redhat.com.
	[66.187.233.206]) by mx.google.com with ESMTPSA id
	6sm13922601qks.37.2015.05.21.12.33.28
	(version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 21 May 2015 12:33:31 -0700 (PDT)
From: j.glisse@gmail.com
To: akpm@linux-foundation.org
Cc: &lt;linux-kernel@vger.kernel.org&gt;, linux-mm@kvack.org,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	&lt;joro@8bytes.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Johannes Weiner &lt;jweiner@redhat.com&gt;,
	Larry Woodman &lt;lwoodman@redhat.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Airlie &lt;airlied@redhat.com&gt;, Brendan Conoboy &lt;blc@redhat.com&gt;,
	Joe Donohue &lt;jdonohue@redhat.com&gt;, Duncan Poole &lt;dpoole@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;,
	John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Lucien Dunning &lt;ldunning@nvidia.com&gt;,
	Cameron Buschardt &lt;cabuschardt@nvidia.com&gt;,
	Arvind Gopalakrishnan &lt;arvindg@nvidia.com&gt;,
	Haggai Eran &lt;haggaie@mellanox.com&gt;,
	Shachar Raindel &lt;raindel@mellanox.com&gt;, Liran Liss &lt;liranl@mellanox.com&gt;,
	Roland Dreier &lt;roland@purestorage.com&gt;, Ben Sander &lt;ben.sander@amd.com&gt;,
	Greg Stoner &lt;Greg.Stoner@amd.com&gt;, John Bridgman &lt;John.Bridgman@amd.com&gt;,
	Michael Mantor &lt;Michael.Mantor@amd.com&gt;,
	Paul Blinzer &lt;Paul.Blinzer@amd.com&gt;,
	Laurent Morichetti &lt;Laurent.Morichetti@amd.com&gt;,
	Alexander Deucher &lt;Alexander.Deucher@amd.com&gt;,
	Oded Gabbay &lt;Oded.Gabbay@amd.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Jatin Kumar &lt;jakumar@nvidia.com&gt;, &lt;linux-rdma@vger.kernel.org&gt;
Subject: [PATCH 05/36] HMM: introduce heterogeneous memory management v3.
Date: Thu, 21 May 2015 15:31:14 -0400
Message-Id: &lt;1432236705-4209-6-git-send-email-j.glisse@gmail.com&gt;
X-Mailer: git-send-email 1.8.3.1
In-Reply-To: &lt;1432236705-4209-1-git-send-email-j.glisse@gmail.com&gt;
References: &lt;1432236705-4209-1-git-send-email-j.glisse@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.8 required=5.0 tests=BAYES_00,
	DKIM_ADSP_CUSTOM_MED, 
	DKIM_SIGNED, FREEMAIL_FROM, RCVD_IN_DNSWL_HI, T_DKIM_INVALID,
	T_RP_MATCHES_RCVD, 
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - May 21, 2015, 7:31 p.m.</div>
<pre class="content">
<span class="from">From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>

This patch only introduce core HMM functions for registering a new
mirror and stopping a mirror as well as HMM device registering and
unregistering.

The lifecycle of HMM object is handled differently then the one of
mmu_notifier because unlike mmu_notifier there can be concurrent
call from both mm code to HMM code and/or from device driver code
to HMM code. Moreover lifetime of HMM can be uncorrelated from the
lifetime of the process that is being mirror (GPU might take longer
time to cleanup).

Changed since v1:
  - Updated comment of hmm_device_register().

Changed since v2:
  - Expose struct hmm for easy access to mm struct.
  - Simplify hmm_mirror_register() arguments.
  - Removed the device name.
  - Refcount the mirror struct internaly to HMM allowing to get
    rid of the srcu and making the device driver callback error
    handling simpler.
  - Safe to call several time hmm_mirror_unregister().
  - Rework the mmu_notifier unregistration and release callback.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Jatin Kumar &lt;jakumar@nvidia.com&gt;</span>
cc: &lt;linux-rdma@vger.kernel.org&gt;
---
 MAINTAINERS              |   7 +
 include/linux/hmm.h      | 164 +++++++++++++++++++++
 include/linux/mm.h       |  11 ++
 include/linux/mm_types.h |  14 ++
 kernel/fork.c            |   2 +
 mm/Kconfig               |  15 ++
 mm/Makefile              |   1 +
 mm/hmm.c                 | 370 +++++++++++++++++++++++++++++++++++++++++++++++
 8 files changed, 584 insertions(+)
 create mode 100644 include/linux/hmm.h
 create mode 100644 mm/hmm.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - May 27, 2015, 5:50 a.m.</div>
<pre class="content">
j.glisse@gmail.com writes:
<span class="quote">
&gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch only introduce core HMM functions for registering a new</span>
<span class="quote">&gt; mirror and stopping a mirror as well as HMM device registering and</span>
<span class="quote">&gt; unregistering.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The lifecycle of HMM object is handled differently then the one of</span>
<span class="quote">&gt; mmu_notifier because unlike mmu_notifier there can be concurrent</span>
<span class="quote">&gt; call from both mm code to HMM code and/or from device driver code</span>
<span class="quote">&gt; to HMM code. Moreover lifetime of HMM can be uncorrelated from the</span>
<span class="quote">&gt; lifetime of the process that is being mirror (GPU might take longer</span>
<span class="quote">&gt; time to cleanup).</span>
<span class="quote">&gt;</span>

......
<span class="quote">
&gt; +struct hmm_device_ops {</span>
<span class="quote">&gt; +	/* release() - mirror must stop using the address space.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * @mirror: The mirror that link process address space with the device.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * When this is call, device driver must kill all device thread using</span>

s/call/called, ?
<span class="quote">
&gt; +	 * this mirror. Also, this callback is the last thing call by HMM and</span>
<span class="quote">&gt; +	 * HMM will not access the mirror struct after this call (ie no more</span>
<span class="quote">&gt; +	 * dereference of it so it is safe for the device driver to free it).</span>
<span class="quote">&gt; +	 * It is call either from :</span>
<span class="quote">&gt; +	 *   - mm dying (all process using this mm exiting).</span>
<span class="quote">&gt; +	 *   - hmm_mirror_unregister() (if no other thread holds a reference)</span>
<span class="quote">&gt; +	 *   - outcome of some device error reported by any of the device</span>
<span class="quote">&gt; +	 *     callback against that mirror.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	void (*release)(struct hmm_mirror *mirror);</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* struct hmm - per mm_struct HMM states.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @mm: The mm struct this hmm is associated with.</span>
<span class="quote">&gt; + * @mirrors: List of all mirror for this mm (one per device).</span>
<span class="quote">&gt; + * @vm_end: Last valid address for this mm (exclusive).</span>
<span class="quote">&gt; + * @kref: Reference counter.</span>
<span class="quote">&gt; + * @rwsem: Serialize the mirror list modifications.</span>
<span class="quote">&gt; + * @mmu_notifier: The mmu_notifier of this mm.</span>
<span class="quote">&gt; + * @rcu: For delayed cleanup call from mmu_notifier.release() callback.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * For each process address space (mm_struct) there is one and only one hmm</span>
<span class="quote">&gt; + * struct. hmm functions will redispatch to each devices the change made to</span>
<span class="quote">&gt; + * the process address space.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Device driver must not access this structure other than for getting the</span>
<span class="quote">&gt; + * mm pointer.</span>
<span class="quote">&gt; + */</span>

.....
<span class="quote">
&gt;  #ifndef AT_VECTOR_SIZE_ARCH</span>
<span class="quote">&gt;  #define AT_VECTOR_SIZE_ARCH 0</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -451,6 +455,16 @@ struct mm_struct {</span>
<span class="quote">&gt;  #ifdef CONFIG_MMU_NOTIFIER</span>
<span class="quote">&gt;  	struct mmu_notifier_mm *mmu_notifier_mm;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +#ifdef CONFIG_HMM</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * hmm always register an mmu_notifier we rely on mmu notifier to keep</span>
<span class="quote">&gt; +	 * refcount on mm struct as well as forbiding registering hmm on a</span>
<span class="quote">&gt; +	 * dying mm</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * This field is set with mmap_sem old in write mode.</span>

s/old/held/ ?
<span class="quote">

&gt; +	 */</span>
<span class="quote">&gt; +	struct hmm *hmm;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS</span>
<span class="quote">&gt;  	pgtable_t pmd_huge_pte; /* protected by page_table_lock */</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="quote">&gt; index 0e0ae9a..4083be7 100644</span>
<span class="quote">&gt; --- a/kernel/fork.c</span>
<span class="quote">&gt; +++ b/kernel/fork.c</span>
<span class="quote">&gt; @@ -27,6 +27,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/binfmts.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mman.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mmu_notifier.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/hmm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/fs.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/vmacache.h&gt;</span>
<span class="quote">&gt; @@ -597,6 +598,7 @@ static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)</span>
<span class="quote">&gt;  	mm_init_aio(mm);</span>
<span class="quote">&gt;  	mm_init_owner(mm, p);</span>
<span class="quote">&gt;  	mmu_notifier_mm_init(mm);</span>
<span class="quote">&gt; +	hmm_mm_init(mm);</span>
<span class="quote">&gt;  	clear_tlb_flush_pending(mm);</span>
<span class="quote">&gt;  #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS</span>
<span class="quote">&gt;  	mm-&gt;pmd_huge_pte = NULL;</span>
<span class="quote">&gt; diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="quote">&gt; index 52ffb86..189e48f 100644</span>
<span class="quote">&gt; --- a/mm/Kconfig</span>
<span class="quote">&gt; +++ b/mm/Kconfig</span>
<span class="quote">&gt; @@ -653,3 +653,18 @@ config DEFERRED_STRUCT_PAGE_INIT</span>
<span class="quote">&gt;  	  when kswapd starts. This has a potential performance impact on</span>
<span class="quote">&gt;  	  processes running early in the lifetime of the systemm until kswapd</span>
<span class="quote">&gt;  	  finishes the initialisation.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +if STAGING</span>
<span class="quote">&gt; +config HMM</span>
<span class="quote">&gt; +	bool &quot;Enable heterogeneous memory management (HMM)&quot;</span>
<span class="quote">&gt; +	depends on MMU</span>
<span class="quote">&gt; +	select MMU_NOTIFIER</span>
<span class="quote">&gt; +	select GENERIC_PAGE_TABLE</span>

What is GENERIC_PAGE_TABLE ?
<span class="quote">
&gt; +	default n</span>
<span class="quote">&gt; +	help</span>
<span class="quote">&gt; +	  Heterogeneous memory management provide infrastructure for a device</span>
<span class="quote">&gt; +	  to mirror a process address space into an hardware mmu or into any</span>
<span class="quote">&gt; +	  things supporting pagefault like event.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  If unsure, say N to disable hmm.</span>

-aneesh

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - May 27, 2015, 2:38 p.m.</div>
<pre class="content">
On Wed, May 27, 2015 at 11:20:05AM +0530, Aneesh Kumar K.V wrote:
<span class="quote">&gt; j.glisse@gmail.com writes:</span>

Noted your grammar fixes.
<span class="quote">
&gt; &gt; diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="quote">&gt; &gt; index 52ffb86..189e48f 100644</span>
<span class="quote">&gt; &gt; --- a/mm/Kconfig</span>
<span class="quote">&gt; &gt; +++ b/mm/Kconfig</span>
<span class="quote">&gt; &gt; @@ -653,3 +653,18 @@ config DEFERRED_STRUCT_PAGE_INIT</span>
<span class="quote">&gt; &gt;  	  when kswapd starts. This has a potential performance impact on</span>
<span class="quote">&gt; &gt;  	  processes running early in the lifetime of the systemm until kswapd</span>
<span class="quote">&gt; &gt;  	  finishes the initialisation.</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +if STAGING</span>
<span class="quote">&gt; &gt; +config HMM</span>
<span class="quote">&gt; &gt; +	bool &quot;Enable heterogeneous memory management (HMM)&quot;</span>
<span class="quote">&gt; &gt; +	depends on MMU</span>
<span class="quote">&gt; &gt; +	select MMU_NOTIFIER</span>
<span class="quote">&gt; &gt; +	select GENERIC_PAGE_TABLE</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What is GENERIC_PAGE_TABLE ?</span>

Let over of when patch 0006 what a seperate feature that was introduced
before this patch. I failed to remove that chunk. Just ignore it.

Cheers,
Jérôme
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=134341">Mark Hairgrove</a> - June 8, 2015, 7:40 p.m.</div>
<pre class="content">
On Thu, 21 May 2015, j.glisse@gmail.com wrote:
<span class="quote">
&gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch only introduce core HMM functions for registering a new</span>
<span class="quote">&gt; mirror and stopping a mirror as well as HMM device registering and</span>
<span class="quote">&gt; unregistering.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +/* struct hmm_device_operations - HMM device operation callback</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct hmm_device_ops {</span>
<span class="quote">&gt; +	/* release() - mirror must stop using the address space.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * @mirror: The mirror that link process address space with the device.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * When this is call, device driver must kill all device thread using</span>
<span class="quote">&gt; +	 * this mirror. Also, this callback is the last thing call by HMM and</span>
<span class="quote">&gt; +	 * HMM will not access the mirror struct after this call (ie no more</span>
<span class="quote">&gt; +	 * dereference of it so it is safe for the device driver to free it).</span>
<span class="quote">&gt; +	 * It is call either from :</span>
<span class="quote">&gt; +	 *   - mm dying (all process using this mm exiting).</span>
<span class="quote">&gt; +	 *   - hmm_mirror_unregister() (if no other thread holds a reference)</span>
<span class="quote">&gt; +	 *   - outcome of some device error reported by any of the device</span>
<span class="quote">&gt; +	 *     callback against that mirror.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	void (*release)(struct hmm_mirror *mirror);</span>
<span class="quote">&gt; +};</span>

The comment that -&gt;release is called when the mm dies doesn&#39;t match the
implementation. -&gt;release is only called when the mirror is destroyed, and
that can only happen after the mirror has been unregistered. This may not
happen until after the mm dies.

Is the intent for the driver to get the callback when the mm goes down?
That seems beneficial so the driver can kill whatever&#39;s happening on the
device. Otherwise the device may continue operating in a dead address
space until the driver&#39;s file gets closed and it unregisters the mirror.
<span class="quote">

&gt; +static void hmm_mirror_destroy(struct kref *kref)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hmm_device *device;</span>
<span class="quote">&gt; +	struct hmm_mirror *mirror;</span>
<span class="quote">&gt; +	struct hmm *hmm;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mirror = container_of(kref, struct hmm_mirror, kref);</span>
<span class="quote">&gt; +	device = mirror-&gt;device;</span>
<span class="quote">&gt; +	hmm = mirror-&gt;hmm;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mutex_lock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; +	list_del_init(&amp;mirror-&gt;dlist);</span>
<span class="quote">&gt; +	device-&gt;ops-&gt;release(mirror);</span>
<span class="quote">&gt; +	mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; +}</span>

The hmm variable is unused. It also probably isn&#39;t safe to access at this
point.
<span class="quote">

&gt; +static void hmm_mirror_kill(struct hmm_mirror *mirror)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	down_write(&amp;mirror-&gt;hmm-&gt;rwsem);</span>
<span class="quote">&gt; +	if (!hlist_unhashed(&amp;mirror-&gt;mlist)) {</span>
<span class="quote">&gt; +		hlist_del_init(&amp;mirror-&gt;mlist);</span>
<span class="quote">&gt; +		up_write(&amp;mirror-&gt;hmm-&gt;rwsem);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		hmm_mirror_unref(&amp;mirror);</span>
<span class="quote">&gt; +	} else</span>
<span class="quote">&gt; +		up_write(&amp;mirror-&gt;hmm-&gt;rwsem);</span>
<span class="quote">&gt; +}</span>

Shouldn&#39;t this call hmm_unref? hmm_mirror_register calls hmm_ref but
there&#39;s no corresponding hmm_unref when the mirror goes away. As a result
the hmm struct gets leaked and thus so does the entire mm since
mmu_notifier_unregister is never called.

It might also be a good idea to set mirror-&gt;hmm = NULL here to prevent
accidental use in say hmm_mirror_destroy.
<span class="quote">

&gt; +/* hmm_device_unregister() - unregister a device with HMM.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @device: The hmm_device struct.</span>
<span class="quote">&gt; + * Returns: 0 on success or -EBUSY otherwise.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Call when device driver want to unregister itself with HMM. This will check</span>
<span class="quote">&gt; + * that there is no any active mirror and returns -EBUSY if so.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +int hmm_device_unregister(struct hmm_device *device)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	mutex_lock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; +	if (!list_empty(&amp;device-&gt;mirrors)) {</span>
<span class="quote">&gt; +		mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; +		return -EBUSY;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>

I assume that the intention is for the caller to spin on
hmm_device_unregister until -EBUSY is no longer returned?

If so, I think there&#39;s a race here in the case of mm teardown happening
concurrently with hmm_mirror_unregister. This can happen if the parent
process was forked and exits while the child closes the file, or if the
file is passed to another process and closed last there while the original
process exits.

The upshot is that the hmm_device may still be referenced by another
thread even after hmm_device_unregister returns 0.

The below sequence shows how this might happen. Coming into this, the
mirror&#39;s ref count is 2:

Thread A (file close)               Thread B (process exit)
----------------------              ----------------------
                                    hmm_notifier_release
                                      down_write(&amp;hmm-&gt;rwsem);
hmm_mirror_unregister
  hmm_mirror_kill
    down_write(&amp;hmm-&gt;rwsem);
    // Blocked on thread B
                                      hlist_del_init(&amp;mirror-&gt;mlist);
                                      up_write(&amp;hmm-&gt;rwsem);

                                      // Thread A unblocked
                                      // Thread B is preempted
    // hlist_unhashed returns 1
    up_write(&amp;hmm-&gt;rwsem);

  // Mirror ref goes 2 -&gt; 1
  hmm_mirror_unref(&amp;mirror);

  // hmm_mirror_unregister returns

At this point hmm_mirror_unregister has returned to the caller but the
mirror still is in use by thread B. Since all mirrors have been
unregistered, the driver in thread A is now free to call
hmm_device_unregister.

                                      // Thread B is scheduled

                                      // Mirror ref goes 1 -&gt; 0
                                      hmm_mirror_unref(&amp;mirror);
                                        hmm_mirror_destroy(&amp;mirror)
                                          mutex_lock(&amp;device-&gt;mutex);
                                          list_del_init(&amp;mirror-&gt;dlist);
                                          device-&gt;ops-&gt;release(mirror);
                                          mutex_unlock(&amp;device-&gt;mutex);

hmm_device_unregister
  mutex_lock(&amp;device-&gt;mutex);
  // Device list empty
  mutex_unlock(&amp;device-&gt;mutex);
  return 0;
// Caller frees device

Do you agree that this sequence can happen, or am I missing something
which prevents it?

If this can happen, the problem is that the only thing preventing thread A
from freeing the device is that thread B has device-&gt;mutex locked. That&#39;s
bad, because a lock within a structure cannot be used to control freeing
that structure. The mutex_unlock in thread B may internally still access
the mutex memory even after the atomic operation which unlocks the mutex
and unblocks thread A.

This can&#39;t be solved by having the driver wait for the -&gt;release mirror
callback before it calls hmm_device_unregister, because the race happens
after that point.

A kref on the device itself might solve this, but the core issue IMO is
that hmm_mirror_unregister doesn&#39;t wait for hmm_notifier_release to
complete before returning. It feels like hmm_mirror_unregister wants to do
a synchronize_srcu on the mmu_notifier srcu. Is that possible?

Whatever the resolution, it would be useful for the block comments of
hmm_mirror_unregister and hmm_device_unregister to describe the
expectations on the caller and what the caller is guaranteed as far as
mirror and device lifetimes go.

Thanks,
Mark
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - June 8, 2015, 9:17 p.m.</div>
<pre class="content">
On Mon, Jun 08, 2015 at 12:40:18PM -0700, Mark Hairgrove wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu, 21 May 2015, j.glisse@gmail.com wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This patch only introduce core HMM functions for registering a new</span>
<span class="quote">&gt; &gt; mirror and stopping a mirror as well as HMM device registering and</span>
<span class="quote">&gt; &gt; unregistering.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; [...]</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; +/* struct hmm_device_operations - HMM device operation callback</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +struct hmm_device_ops {</span>
<span class="quote">&gt; &gt; +	/* release() - mirror must stop using the address space.</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * @mirror: The mirror that link process address space with the device.</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * When this is call, device driver must kill all device thread using</span>
<span class="quote">&gt; &gt; +	 * this mirror. Also, this callback is the last thing call by HMM and</span>
<span class="quote">&gt; &gt; +	 * HMM will not access the mirror struct after this call (ie no more</span>
<span class="quote">&gt; &gt; +	 * dereference of it so it is safe for the device driver to free it).</span>
<span class="quote">&gt; &gt; +	 * It is call either from :</span>
<span class="quote">&gt; &gt; +	 *   - mm dying (all process using this mm exiting).</span>
<span class="quote">&gt; &gt; +	 *   - hmm_mirror_unregister() (if no other thread holds a reference)</span>
<span class="quote">&gt; &gt; +	 *   - outcome of some device error reported by any of the device</span>
<span class="quote">&gt; &gt; +	 *     callback against that mirror.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	void (*release)(struct hmm_mirror *mirror);</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The comment that -&gt;release is called when the mm dies doesn&#39;t match the</span>
<span class="quote">&gt; implementation. -&gt;release is only called when the mirror is destroyed, and</span>
<span class="quote">&gt; that can only happen after the mirror has been unregistered. This may not</span>
<span class="quote">&gt; happen until after the mm dies.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is the intent for the driver to get the callback when the mm goes down?</span>
<span class="quote">&gt; That seems beneficial so the driver can kill whatever&#39;s happening on the</span>
<span class="quote">&gt; device. Otherwise the device may continue operating in a dead address</span>
<span class="quote">&gt; space until the driver&#39;s file gets closed and it unregisters the mirror.</span>

This was the intent before merging free &amp; release. I guess i need to
reinstate the free versus release callback. Sadly the lifetime for HMM
is more complex than mmu_notifier as we intend the mirror struct to
be embedded into a driver private struct.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +static void hmm_mirror_destroy(struct kref *kref)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hmm_device *device;</span>
<span class="quote">&gt; &gt; +	struct hmm_mirror *mirror;</span>
<span class="quote">&gt; &gt; +	struct hmm *hmm;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	mirror = container_of(kref, struct hmm_mirror, kref);</span>
<span class="quote">&gt; &gt; +	device = mirror-&gt;device;</span>
<span class="quote">&gt; &gt; +	hmm = mirror-&gt;hmm;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	mutex_lock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; &gt; +	list_del_init(&amp;mirror-&gt;dlist);</span>
<span class="quote">&gt; &gt; +	device-&gt;ops-&gt;release(mirror);</span>
<span class="quote">&gt; &gt; +	mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The hmm variable is unused. It also probably isn&#39;t safe to access at this</span>
<span class="quote">&gt; point.</span>

hmm_unref(hmm); was lost somewhere probably in a rebase and it is safe to
access hmm here.
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +static void hmm_mirror_kill(struct hmm_mirror *mirror)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	down_write(&amp;mirror-&gt;hmm-&gt;rwsem);</span>
<span class="quote">&gt; &gt; +	if (!hlist_unhashed(&amp;mirror-&gt;mlist)) {</span>
<span class="quote">&gt; &gt; +		hlist_del_init(&amp;mirror-&gt;mlist);</span>
<span class="quote">&gt; &gt; +		up_write(&amp;mirror-&gt;hmm-&gt;rwsem);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		hmm_mirror_unref(&amp;mirror);</span>
<span class="quote">&gt; &gt; +	} else</span>
<span class="quote">&gt; &gt; +		up_write(&amp;mirror-&gt;hmm-&gt;rwsem);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Shouldn&#39;t this call hmm_unref? hmm_mirror_register calls hmm_ref but</span>
<span class="quote">&gt; there&#39;s no corresponding hmm_unref when the mirror goes away. As a result</span>
<span class="quote">&gt; the hmm struct gets leaked and thus so does the entire mm since</span>
<span class="quote">&gt; mmu_notifier_unregister is never called.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It might also be a good idea to set mirror-&gt;hmm = NULL here to prevent</span>
<span class="quote">&gt; accidental use in say hmm_mirror_destroy.</span>

No, hmm_mirror_destroy must be the one doing the hmm_unref(hmm)
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +/* hmm_device_unregister() - unregister a device with HMM.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * @device: The hmm_device struct.</span>
<span class="quote">&gt; &gt; + * Returns: 0 on success or -EBUSY otherwise.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Call when device driver want to unregister itself with HMM. This will check</span>
<span class="quote">&gt; &gt; + * that there is no any active mirror and returns -EBUSY if so.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +int hmm_device_unregister(struct hmm_device *device)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	mutex_lock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; &gt; +	if (!list_empty(&amp;device-&gt;mirrors)) {</span>
<span class="quote">&gt; &gt; +		mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; &gt; +		return -EBUSY;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +	mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I assume that the intention is for the caller to spin on</span>
<span class="quote">&gt; hmm_device_unregister until -EBUSY is no longer returned?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If so, I think there&#39;s a race here in the case of mm teardown happening</span>
<span class="quote">&gt; concurrently with hmm_mirror_unregister. This can happen if the parent</span>
<span class="quote">&gt; process was forked and exits while the child closes the file, or if the</span>
<span class="quote">&gt; file is passed to another process and closed last there while the original</span>
<span class="quote">&gt; process exits.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The upshot is that the hmm_device may still be referenced by another</span>
<span class="quote">&gt; thread even after hmm_device_unregister returns 0.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The below sequence shows how this might happen. Coming into this, the</span>
<span class="quote">&gt; mirror&#39;s ref count is 2:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thread A (file close)               Thread B (process exit)</span>
<span class="quote">&gt; ----------------------              ----------------------</span>
<span class="quote">&gt;                                     hmm_notifier_release</span>
<span class="quote">&gt;                                       down_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt; hmm_mirror_unregister</span>
<span class="quote">&gt;   hmm_mirror_kill</span>
<span class="quote">&gt;     down_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt;     // Blocked on thread B</span>
<span class="quote">&gt;                                       hlist_del_init(&amp;mirror-&gt;mlist);</span>
<span class="quote">&gt;                                       up_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                       // Thread A unblocked</span>
<span class="quote">&gt;                                       // Thread B is preempted</span>
<span class="quote">&gt;     // hlist_unhashed returns 1</span>
<span class="quote">&gt;     up_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   // Mirror ref goes 2 -&gt; 1</span>
<span class="quote">&gt;   hmm_mirror_unref(&amp;mirror);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   // hmm_mirror_unregister returns</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; At this point hmm_mirror_unregister has returned to the caller but the</span>
<span class="quote">&gt; mirror still is in use by thread B. Since all mirrors have been</span>
<span class="quote">&gt; unregistered, the driver in thread A is now free to call</span>
<span class="quote">&gt; hmm_device_unregister.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                       // Thread B is scheduled</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                       // Mirror ref goes 1 -&gt; 0</span>
<span class="quote">&gt;                                       hmm_mirror_unref(&amp;mirror);</span>
<span class="quote">&gt;                                         hmm_mirror_destroy(&amp;mirror)</span>
<span class="quote">&gt;                                           mutex_lock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt;                                           list_del_init(&amp;mirror-&gt;dlist);</span>
<span class="quote">&gt;                                           device-&gt;ops-&gt;release(mirror);</span>
<span class="quote">&gt;                                           mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; hmm_device_unregister</span>
<span class="quote">&gt;   mutex_lock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt;   // Device list empty</span>
<span class="quote">&gt;   mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt;   return 0;</span>
<span class="quote">&gt; // Caller frees device</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Do you agree that this sequence can happen, or am I missing something</span>
<span class="quote">&gt; which prevents it?</span>

Can&#39;t happen because child have mm-&gt;hmm = NULL ie only one hmm per mm
and hmm is tie to only one mm. It is the responsability of the device
driver to make sure same apply to private reference to the hmm mirror
struct ie hmm_mirror should never be tie to a private file struct.
<span class="quote">
&gt; </span>
<span class="quote">&gt; If this can happen, the problem is that the only thing preventing thread A</span>
<span class="quote">&gt; from freeing the device is that thread B has device-&gt;mutex locked. That&#39;s</span>
<span class="quote">&gt; bad, because a lock within a structure cannot be used to control freeing</span>
<span class="quote">&gt; that structure. The mutex_unlock in thread B may internally still access</span>
<span class="quote">&gt; the mutex memory even after the atomic operation which unlocks the mutex</span>
<span class="quote">&gt; and unblocks thread A.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This can&#39;t be solved by having the driver wait for the -&gt;release mirror</span>
<span class="quote">&gt; callback before it calls hmm_device_unregister, because the race happens</span>
<span class="quote">&gt; after that point.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A kref on the device itself might solve this, but the core issue IMO is</span>
<span class="quote">&gt; that hmm_mirror_unregister doesn&#39;t wait for hmm_notifier_release to</span>
<span class="quote">&gt; complete before returning. It feels like hmm_mirror_unregister wants to do</span>
<span class="quote">&gt; a synchronize_srcu on the mmu_notifier srcu. Is that possible?</span>

I guess i need to revisit once again the whole lifetime issue.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Whatever the resolution, it would be useful for the block comments of</span>
<span class="quote">&gt; hmm_mirror_unregister and hmm_device_unregister to describe the</span>
<span class="quote">&gt; expectations on the caller and what the caller is guaranteed as far as</span>
<span class="quote">&gt; mirror and device lifetimes go.</span>

Yes i need to fix comment and add again spend time on lifetime. I
obviously completely screw that up in that version of the patchset.

Cheers,
Jérôme
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=134341">Mark Hairgrove</a> - June 9, 2015, 1:54 a.m.</div>
<pre class="content">
On Mon, 8 Jun 2015, Jerome Glisse wrote:
<span class="quote">
&gt; On Mon, Jun 08, 2015 at 12:40:18PM -0700, Mark Hairgrove wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; On Thu, 21 May 2015, j.glisse@gmail.com wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; This patch only introduce core HMM functions for registering a new</span>
<span class="quote">&gt; &gt; &gt; mirror and stopping a mirror as well as HMM device registering and</span>
<span class="quote">&gt; &gt; &gt; unregistering.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; +/* struct hmm_device_operations - HMM device operation callback</span>
<span class="quote">&gt; &gt; &gt; + */</span>
<span class="quote">&gt; &gt; &gt; +struct hmm_device_ops {</span>
<span class="quote">&gt; &gt; &gt; +	/* release() - mirror must stop using the address space.</span>
<span class="quote">&gt; &gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; &gt; +	 * @mirror: The mirror that link process address space with the device.</span>
<span class="quote">&gt; &gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; &gt; +	 * When this is call, device driver must kill all device thread using</span>
<span class="quote">&gt; &gt; &gt; +	 * this mirror. Also, this callback is the last thing call by HMM and</span>
<span class="quote">&gt; &gt; &gt; +	 * HMM will not access the mirror struct after this call (ie no more</span>
<span class="quote">&gt; &gt; &gt; +	 * dereference of it so it is safe for the device driver to free it).</span>
<span class="quote">&gt; &gt; &gt; +	 * It is call either from :</span>
<span class="quote">&gt; &gt; &gt; +	 *   - mm dying (all process using this mm exiting).</span>
<span class="quote">&gt; &gt; &gt; +	 *   - hmm_mirror_unregister() (if no other thread holds a reference)</span>
<span class="quote">&gt; &gt; &gt; +	 *   - outcome of some device error reported by any of the device</span>
<span class="quote">&gt; &gt; &gt; +	 *     callback against that mirror.</span>
<span class="quote">&gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; +	void (*release)(struct hmm_mirror *mirror);</span>
<span class="quote">&gt; &gt; &gt; +};</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The comment that -&gt;release is called when the mm dies doesn&#39;t match the</span>
<span class="quote">&gt; &gt; implementation. -&gt;release is only called when the mirror is destroyed, and</span>
<span class="quote">&gt; &gt; that can only happen after the mirror has been unregistered. This may not</span>
<span class="quote">&gt; &gt; happen until after the mm dies.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Is the intent for the driver to get the callback when the mm goes down?</span>
<span class="quote">&gt; &gt; That seems beneficial so the driver can kill whatever&#39;s happening on the</span>
<span class="quote">&gt; &gt; device. Otherwise the device may continue operating in a dead address</span>
<span class="quote">&gt; &gt; space until the driver&#39;s file gets closed and it unregisters the mirror.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This was the intent before merging free &amp; release. I guess i need to</span>
<span class="quote">&gt; reinstate the free versus release callback. Sadly the lifetime for HMM</span>
<span class="quote">&gt; is more complex than mmu_notifier as we intend the mirror struct to</span>
<span class="quote">&gt; be embedded into a driver private struct.</span>

Can you clarify how that&#39;s different from mmu_notifiers? Those are also
embedded into a driver-owned struct.

Is the goal to allow calling hmm_mirror_unregister from within the &quot;mm is
dying&quot; HMM callback? I don&#39;t know whether that&#39;s really necessary as long
as there&#39;s some association between the driver files and the mirrors.
<span class="quote">

&gt; &gt; If so, I think there&#39;s a race here in the case of mm teardown happening</span>
<span class="quote">&gt; &gt; concurrently with hmm_mirror_unregister.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; [...]</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Do you agree that this sequence can happen, or am I missing something</span>
<span class="quote">&gt; &gt; which prevents it?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Can&#39;t happen because child have mm-&gt;hmm = NULL ie only one hmm per mm</span>
<span class="quote">&gt; and hmm is tie to only one mm. It is the responsability of the device</span>
<span class="quote">&gt; driver to make sure same apply to private reference to the hmm mirror</span>
<span class="quote">&gt; struct ie hmm_mirror should never be tie to a private file struct.</span>

It&#39;s useful for the driver to have some association between files and
mirrors. If the file is closed prior to process exit we would like to
unregister the mirror, otherwise it will persist until process teardown.
The association doesn&#39;t have to be 1:1 but having the files ref count the
mirror or something would be useful.

But even if we assume no association at all between files and mirrors, are
you sure that prevents the race? The driver may choose to unregister the
hmm_device at any point once its files are closed. In the case of module
unload the device unregister can&#39;t be prevented. If mm teardown hasn&#39;t
happened yet mirrors may still be active and registered on that
hmm_device. The driver thus has to first call hmm_mirror_unregister on all
active mirrors, then call hmm_device_unregister. mm teardown of those
mirrors may trigger at any point in this sequence, so we&#39;re right back to
that race.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - June 9, 2015, 3:56 p.m.</div>
<pre class="content">
On Mon, Jun 08, 2015 at 06:54:29PM -0700, Mark Hairgrove wrote:
<span class="quote">&gt; On Mon, 8 Jun 2015, Jerome Glisse wrote:</span>
<span class="quote">&gt; &gt; On Mon, Jun 08, 2015 at 12:40:18PM -0700, Mark Hairgrove wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu, 21 May 2015, j.glisse@gmail.com wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; This patch only introduce core HMM functions for registering a new</span>
<span class="quote">&gt; &gt; &gt; &gt; mirror and stopping a mirror as well as HMM device registering and</span>
<span class="quote">&gt; &gt; &gt; &gt; unregistering.</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; +/* struct hmm_device_operations - HMM device operation callback</span>
<span class="quote">&gt; &gt; &gt; &gt; + */</span>
<span class="quote">&gt; &gt; &gt; &gt; +struct hmm_device_ops {</span>
<span class="quote">&gt; &gt; &gt; &gt; +	/* release() - mirror must stop using the address space.</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * @mirror: The mirror that link process address space with the device.</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * When this is call, device driver must kill all device thread using</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * this mirror. Also, this callback is the last thing call by HMM and</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * HMM will not access the mirror struct after this call (ie no more</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * dereference of it so it is safe for the device driver to free it).</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * It is call either from :</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 *   - mm dying (all process using this mm exiting).</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 *   - hmm_mirror_unregister() (if no other thread holds a reference)</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 *   - outcome of some device error reported by any of the device</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 *     callback against that mirror.</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; &gt; +	void (*release)(struct hmm_mirror *mirror);</span>
<span class="quote">&gt; &gt; &gt; &gt; +};</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; The comment that -&gt;release is called when the mm dies doesn&#39;t match the</span>
<span class="quote">&gt; &gt; &gt; implementation. -&gt;release is only called when the mirror is destroyed, and</span>
<span class="quote">&gt; &gt; &gt; that can only happen after the mirror has been unregistered. This may not</span>
<span class="quote">&gt; &gt; &gt; happen until after the mm dies.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Is the intent for the driver to get the callback when the mm goes down?</span>
<span class="quote">&gt; &gt; &gt; That seems beneficial so the driver can kill whatever&#39;s happening on the</span>
<span class="quote">&gt; &gt; &gt; device. Otherwise the device may continue operating in a dead address</span>
<span class="quote">&gt; &gt; &gt; space until the driver&#39;s file gets closed and it unregisters the mirror.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This was the intent before merging free &amp; release. I guess i need to</span>
<span class="quote">&gt; &gt; reinstate the free versus release callback. Sadly the lifetime for HMM</span>
<span class="quote">&gt; &gt; is more complex than mmu_notifier as we intend the mirror struct to</span>
<span class="quote">&gt; &gt; be embedded into a driver private struct.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can you clarify how that&#39;s different from mmu_notifiers? Those are also</span>
<span class="quote">&gt; embedded into a driver-owned struct.</span>

For HMM you want to be able to kill a mirror from HMM, you might have kernel
thread call inside HMM with a mirror (outside any device file lifetime) ...
The mirror is not only use at register &amp; unregister, there is a lot more thing
you can call using the HMM mirror struct.

So the HMM mirror lifetime as a result is more complex, it can not simply be
free from the mmu_notifier_release callback or randomly. It needs to be
refcounted. The mmu_notifier code assume that the mmu_notifier struct is
embedded inside a struct that has a lifetime properly synchronize with the
mm. For HMM mirror this is not something that sounds like a good idea as there
is too many way to get it wrong.

So idea of HMM mirror is that it can out last the mm lifetime but the HMM
struct can not. So you have hmm_mirror &lt;~&gt; hmm &lt;-&gt; mm and the mirror can be
&quot;unlink&quot; and have different lifetime from the hmm that itself has same life
time as mm.
<span class="quote">
&gt; Is the goal to allow calling hmm_mirror_unregister from within the &quot;mm is</span>
<span class="quote">&gt; dying&quot; HMM callback? I don&#39;t know whether that&#39;s really necessary as long</span>
<span class="quote">&gt; as there&#39;s some association between the driver files and the mirrors.</span>

No this is not a goal and i actualy forbid that.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; &gt; If so, I think there&#39;s a race here in the case of mm teardown happening</span>
<span class="quote">&gt; &gt; &gt; concurrently with hmm_mirror_unregister.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Do you agree that this sequence can happen, or am I missing something</span>
<span class="quote">&gt; &gt; &gt; which prevents it?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Can&#39;t happen because child have mm-&gt;hmm = NULL ie only one hmm per mm</span>
<span class="quote">&gt; &gt; and hmm is tie to only one mm. It is the responsability of the device</span>
<span class="quote">&gt; &gt; driver to make sure same apply to private reference to the hmm mirror</span>
<span class="quote">&gt; &gt; struct ie hmm_mirror should never be tie to a private file struct.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s useful for the driver to have some association between files and</span>
<span class="quote">&gt; mirrors. If the file is closed prior to process exit we would like to</span>
<span class="quote">&gt; unregister the mirror, otherwise it will persist until process teardown.</span>
<span class="quote">&gt; The association doesn&#39;t have to be 1:1 but having the files ref count the</span>
<span class="quote">&gt; mirror or something would be useful.</span>

This is allowed, i might have put strong word here, but you can associate
with a file struct. What you can not do is use the mirror from a different
process ie one with a different mm struct as mirror is linked to a single
mm. So on fork there is no callback to update the private file struct, when
the device file is duplicated (well just refcount inc) against a different
process. This is something you need to be carefull in your driver. Inside
the dummy driver i abuse that to actually test proper behavior of HMM but
it should not be use as an example.
<span class="quote">
&gt; </span>
<span class="quote">&gt; But even if we assume no association at all between files and mirrors, are</span>
<span class="quote">&gt; you sure that prevents the race? The driver may choose to unregister the</span>
<span class="quote">&gt; hmm_device at any point once its files are closed. In the case of module</span>
<span class="quote">&gt; unload the device unregister can&#39;t be prevented. If mm teardown hasn&#39;t</span>
<span class="quote">&gt; happened yet mirrors may still be active and registered on that</span>
<span class="quote">&gt; hmm_device. The driver thus has to first call hmm_mirror_unregister on all</span>
<span class="quote">&gt; active mirrors, then call hmm_device_unregister. mm teardown of those</span>
<span class="quote">&gt; mirrors may trigger at any point in this sequence, so we&#39;re right back to</span>
<span class="quote">&gt; that race.</span>

So when device driver unload the first thing it needs to do is kill all of
its context ie all of its HMM mirror (unregister them) by doing so it will
make sure that there can be no more call to any of its functions.

The race with mm teardown does not exist as what matter for mm teardown is
the fact that the mirror is on the struct hmm mirrors list or not. Either
the device driver is first to remove the mirror from the list or it is the
mm teardown but this is lock protected so only one thread can do it.

The issue you pointed is really about decoupling the lifetime of the mirror
context (ie hardware thread that use the mirror) and the lifetime of the
structure that embedded the hmm_mirror struct. The device driver will care
about the second while everything else will only really care about the
first. The second tells you when you know for sure that there will be no
more callback to your device driver code. The first only tells you that
there should be no more activity associated with that mirror but some thread
might still hold a reference on the underlying struct.


Hope this clarify design and motivation behind the hmm_mirror vs hmm struct
lifetime.


Cheers,
Jérôme
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=134341">Mark Hairgrove</a> - June 10, 2015, 3:33 a.m.</div>
<pre class="content">
On Tue, 9 Jun 2015, Jerome Glisse wrote:
<span class="quote">
&gt; On Mon, Jun 08, 2015 at 06:54:29PM -0700, Mark Hairgrove wrote:</span>
<span class="quote">&gt; &gt; Can you clarify how that&#39;s different from mmu_notifiers? Those are also</span>
<span class="quote">&gt; &gt; embedded into a driver-owned struct.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For HMM you want to be able to kill a mirror from HMM, you might have kernel</span>
<span class="quote">&gt; thread call inside HMM with a mirror (outside any device file lifetime) ...</span>
<span class="quote">&gt; The mirror is not only use at register &amp; unregister, there is a lot more thing</span>
<span class="quote">&gt; you can call using the HMM mirror struct.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So the HMM mirror lifetime as a result is more complex, it can not simply be</span>
<span class="quote">&gt; free from the mmu_notifier_release callback or randomly. It needs to be</span>
<span class="quote">&gt; refcounted.</span>

Sure, there are driver -&gt; HMM calls like hmm_mirror_fault that 
mmu_notifiers don&#39;t have, but I don&#39;t understand why that fundamentally 
makes HMM mirror lifetimes more complex. Decoupling hmm_mirror lifetime 
from mm lifetime adds complexity too.
<span class="quote">
&gt; The mmu_notifier code assume that the mmu_notifier struct is</span>
<span class="quote">&gt; embedded inside a struct that has a lifetime properly synchronize with the</span>
<span class="quote">&gt; mm. For HMM mirror this is not something that sounds like a good idea as there</span>
<span class="quote">&gt; is too many way to get it wrong.</span>

What kind of synchronization with the mm are you referring to here? 
Clients of mmu_notifiers don&#39;t have to do anything as far as I know. 
They&#39;re guaranteed that the mm won&#39;t go away because each registered 
notifier bumps mm_count.
<span class="quote">
&gt; So idea of HMM mirror is that it can out last the mm lifetime but the HMM</span>
<span class="quote">&gt; struct can not. So you have hmm_mirror &lt;~&gt; hmm &lt;-&gt; mm and the mirror can be</span>
<span class="quote">&gt; &quot;unlink&quot; and have different lifetime from the hmm that itself has same life</span>
<span class="quote">&gt; time as mm.</span>

Per the earlier discussion hmm_mirror_destroy is missing a call to 
hmm_unref. If that&#39;s added back I don&#39;t understand how the mirror can 
persist past the hmm struct. The mirror can be unlinked from hmm&#39;s list, 
yes, but that doesn&#39;t mean that hmm/mm can be torn down. The hmm/mm 
structs will stick around until hmm_destroy since that does the 
mmu_notifier_unregister. hmm_destroy can&#39;t be called until the last 
hmm_mirror_destroy.

Doesn&#39;t that mean that hmm/mm are guaranteed to be allocated until the 
last hmm_mirror_unregister? That sounds like a good guarantee to make.
<span class="quote">

&gt; </span>
<span class="quote">&gt; &gt; Is the goal to allow calling hmm_mirror_unregister from within the &quot;mm is</span>
<span class="quote">&gt; &gt; dying&quot; HMM callback? I don&#39;t know whether that&#39;s really necessary as long</span>
<span class="quote">&gt; &gt; as there&#39;s some association between the driver files and the mirrors.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No this is not a goal and i actualy forbid that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; If so, I think there&#39;s a race here in the case of mm teardown happening</span>
<span class="quote">&gt; &gt; &gt; &gt; concurrently with hmm_mirror_unregister.</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; Do you agree that this sequence can happen, or am I missing something</span>
<span class="quote">&gt; &gt; &gt; &gt; which prevents it?</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Can&#39;t happen because child have mm-&gt;hmm = NULL ie only one hmm per mm</span>
<span class="quote">&gt; &gt; &gt; and hmm is tie to only one mm. It is the responsability of the device</span>
<span class="quote">&gt; &gt; &gt; driver to make sure same apply to private reference to the hmm mirror</span>
<span class="quote">&gt; &gt; &gt; struct ie hmm_mirror should never be tie to a private file struct.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It&#39;s useful for the driver to have some association between files and</span>
<span class="quote">&gt; &gt; mirrors. If the file is closed prior to process exit we would like to</span>
<span class="quote">&gt; &gt; unregister the mirror, otherwise it will persist until process teardown.</span>
<span class="quote">&gt; &gt; The association doesn&#39;t have to be 1:1 but having the files ref count the</span>
<span class="quote">&gt; &gt; mirror or something would be useful.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is allowed, i might have put strong word here, but you can associate</span>
<span class="quote">&gt; with a file struct. What you can not do is use the mirror from a different</span>
<span class="quote">&gt; process ie one with a different mm struct as mirror is linked to a single</span>
<span class="quote">&gt; mm. So on fork there is no callback to update the private file struct, when</span>
<span class="quote">&gt; the device file is duplicated (well just refcount inc) against a different</span>
<span class="quote">&gt; process. This is something you need to be carefull in your driver. Inside</span>
<span class="quote">&gt; the dummy driver i abuse that to actually test proper behavior of HMM but</span>
<span class="quote">&gt; it should not be use as an example.</span>

So to confirm, on all file operations from user space the driver is 
expected to check that current-&gt;mm matches the mm associated with the 
struct file&#39;s hmm_mirror?

On file-&gt;release the driver still ought to call hmm_mirror_unregister 
regardless of whether the mms match, otherwise we&#39;ll never tear down the 
mirror. That means we&#39;re not saved from the race condition because 
hmm_mirror_unregister can happen in one thread while hmm_notifier_release 
might be happening in another thread.
<span class="quote">

&gt; &gt; </span>
<span class="quote">&gt; &gt; But even if we assume no association at all between files and mirrors, are</span>
<span class="quote">&gt; &gt; you sure that prevents the race? The driver may choose to unregister the</span>
<span class="quote">&gt; &gt; hmm_device at any point once its files are closed. In the case of module</span>
<span class="quote">&gt; &gt; unload the device unregister can&#39;t be prevented. If mm teardown hasn&#39;t</span>
<span class="quote">&gt; &gt; happened yet mirrors may still be active and registered on that</span>
<span class="quote">&gt; &gt; hmm_device. The driver thus has to first call hmm_mirror_unregister on all</span>
<span class="quote">&gt; &gt; active mirrors, then call hmm_device_unregister. mm teardown of those</span>
<span class="quote">&gt; &gt; mirrors may trigger at any point in this sequence, so we&#39;re right back to</span>
<span class="quote">&gt; &gt; that race.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So when device driver unload the first thing it needs to do is kill all of</span>
<span class="quote">&gt; its context ie all of its HMM mirror (unregister them) by doing so it will</span>
<span class="quote">&gt; make sure that there can be no more call to any of its functions.</span>

When is the driver expected to call hmm_mirror_unregister? Is it file 
close, module unload, or some other time?

If it&#39;s file close, there&#39;s no need to unregister anything on module 
unload because the files were all closed already.

If it&#39;s module unload, then the mirrors and mms all get leaked until that 
point.

We&#39;re exposed to the race in both cases.
<span class="quote">
&gt; </span>
<span class="quote">&gt; The race with mm teardown does not exist as what matter for mm teardown is</span>
<span class="quote">&gt; the fact that the mirror is on the struct hmm mirrors list or not. Either</span>
<span class="quote">&gt; the device driver is first to remove the mirror from the list or it is the</span>
<span class="quote">&gt; mm teardown but this is lock protected so only one thread can do it.</span>
<span class="quote">&gt; </span>

Agreed, removing the mirror from the list is not a &quot;race&quot; in the classical 
sense. The true race is between hmm_notifier_release&#39;s device mutex_unlock 
(process exit) and post-hmm_device_unregister device mutex free (driver 
close/unload). What I meant is that in order to expose that race you first 
need one thread to call hmm_mirror_unregister while another thread is in 
hmm_notifier_release.

Regardless of where hmm_mirror_unregister is called (file close, module 
unload, etc) it can happen concurrently with hmm_notifier_release so we&#39;re 
exposed to this race.
<span class="quote">

&gt; The issue you pointed is really about decoupling the lifetime of the mirror</span>
<span class="quote">&gt; context (ie hardware thread that use the mirror) and the lifetime of the</span>
<span class="quote">&gt; structure that embedded the hmm_mirror struct. The device driver will care</span>
<span class="quote">&gt; about the second while everything else will only really care about the</span>
<span class="quote">&gt; first. The second tells you when you know for sure that there will be no</span>
<span class="quote">&gt; more callback to your device driver code. The first only tells you that</span>
<span class="quote">&gt; there should be no more activity associated with that mirror but some thread</span>
<span class="quote">&gt; might still hold a reference on the underlying struct.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hope this clarify design and motivation behind the hmm_mirror vs hmm struct</span>
<span class="quote">&gt; lifetime.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cheers,</span>
<span class="quote">&gt; Jérôme</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - June 10, 2015, 3:42 p.m.</div>
<pre class="content">
On Tue, Jun 09, 2015 at 08:33:12PM -0700, Mark Hairgrove wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Tue, 9 Jun 2015, Jerome Glisse wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Mon, Jun 08, 2015 at 06:54:29PM -0700, Mark Hairgrove wrote:</span>
<span class="quote">&gt; &gt; &gt; Can you clarify how that&#39;s different from mmu_notifiers? Those are also</span>
<span class="quote">&gt; &gt; &gt; embedded into a driver-owned struct.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; For HMM you want to be able to kill a mirror from HMM, you might have kernel</span>
<span class="quote">&gt; &gt; thread call inside HMM with a mirror (outside any device file lifetime) ...</span>
<span class="quote">&gt; &gt; The mirror is not only use at register &amp; unregister, there is a lot more thing</span>
<span class="quote">&gt; &gt; you can call using the HMM mirror struct.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So the HMM mirror lifetime as a result is more complex, it can not simply be</span>
<span class="quote">&gt; &gt; free from the mmu_notifier_release callback or randomly. It needs to be</span>
<span class="quote">&gt; &gt; refcounted.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure, there are driver -&gt; HMM calls like hmm_mirror_fault that </span>
<span class="quote">&gt; mmu_notifiers don&#39;t have, but I don&#39;t understand why that fundamentally </span>
<span class="quote">&gt; makes HMM mirror lifetimes more complex. Decoupling hmm_mirror lifetime </span>
<span class="quote">&gt; from mm lifetime adds complexity too.</span>

Driver-&gt;HMM calls can happen from random kernel thread thus you need to
garanty that hmm_mirror can not go away. More over you can have CPU MM
call into HMM outside of mmu_notifier. Basicly you can get to HMM code
by many different code path, unlike any of the current mmu_notifier.

So refcounting is necessary as otherwise the device driver might decide
to unregister and free the mirror while some other kernel thread is
about to dereference the exact same mirror. Synchronization with mmu
notifier srcu will not be enough in the case of page fault on remote
memory for instance. Other case too.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; The mmu_notifier code assume that the mmu_notifier struct is</span>
<span class="quote">&gt; &gt; embedded inside a struct that has a lifetime properly synchronize with the</span>
<span class="quote">&gt; &gt; mm. For HMM mirror this is not something that sounds like a good idea as there</span>
<span class="quote">&gt; &gt; is too many way to get it wrong.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What kind of synchronization with the mm are you referring to here? </span>
<span class="quote">&gt; Clients of mmu_notifiers don&#39;t have to do anything as far as I know. </span>
<span class="quote">&gt; They&#39;re guaranteed that the mm won&#39;t go away because each registered </span>
<span class="quote">&gt; notifier bumps mm_count.</span>

So for all current user afaict (kvm, xen, intel, radeon) tie to a file,
(sgi gru) tie to vma, (iommu) tie to mm. Which means this is all properly
synchronize with lifetime of mm (ignoring the fork case).

The struct that is tie to mmu_notifier for all of them can be accessed
only through one code path (ioctl for most of them).
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; So idea of HMM mirror is that it can out last the mm lifetime but the HMM</span>
<span class="quote">&gt; &gt; struct can not. So you have hmm_mirror &lt;~&gt; hmm &lt;-&gt; mm and the mirror can be</span>
<span class="quote">&gt; &gt; &quot;unlink&quot; and have different lifetime from the hmm that itself has same life</span>
<span class="quote">&gt; &gt; time as mm.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Per the earlier discussion hmm_mirror_destroy is missing a call to </span>
<span class="quote">&gt; hmm_unref. If that&#39;s added back I don&#39;t understand how the mirror can </span>
<span class="quote">&gt; persist past the hmm struct. The mirror can be unlinked from hmm&#39;s list, </span>
<span class="quote">&gt; yes, but that doesn&#39;t mean that hmm/mm can be torn down. The hmm/mm </span>
<span class="quote">&gt; structs will stick around until hmm_destroy since that does the </span>
<span class="quote">&gt; mmu_notifier_unregister. hmm_destroy can&#39;t be called until the last </span>
<span class="quote">&gt; hmm_mirror_destroy.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Doesn&#39;t that mean that hmm/mm are guaranteed to be allocated until the </span>
<span class="quote">&gt; last hmm_mirror_unregister? That sounds like a good guarantee to make.</span>

Like said, just ignore current code it is utterly broken in so many way
when it comes to lifetime. I screw that part badly when reworking the
patchset, i was focusing on other part.

I fixed that in my tree, i am waiting for more review on other part as
anyway the lifetime thing is easy to rework/fix.

http://cgit.freedesktop.org/~glisse/linux/log/?h=hmm

[...]
<span class="quote">&gt; &gt; &gt; &gt; &gt; If so, I think there&#39;s a race here in the case of mm teardown happening</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; concurrently with hmm_mirror_unregister.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Do you agree that this sequence can happen, or am I missing something</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; which prevents it?</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; Can&#39;t happen because child have mm-&gt;hmm = NULL ie only one hmm per mm</span>
<span class="quote">&gt; &gt; &gt; &gt; and hmm is tie to only one mm. It is the responsability of the device</span>
<span class="quote">&gt; &gt; &gt; &gt; driver to make sure same apply to private reference to the hmm mirror</span>
<span class="quote">&gt; &gt; &gt; &gt; struct ie hmm_mirror should never be tie to a private file struct.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; It&#39;s useful for the driver to have some association between files and</span>
<span class="quote">&gt; &gt; &gt; mirrors. If the file is closed prior to process exit we would like to</span>
<span class="quote">&gt; &gt; &gt; unregister the mirror, otherwise it will persist until process teardown.</span>
<span class="quote">&gt; &gt; &gt; The association doesn&#39;t have to be 1:1 but having the files ref count the</span>
<span class="quote">&gt; &gt; &gt; mirror or something would be useful.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This is allowed, i might have put strong word here, but you can associate</span>
<span class="quote">&gt; &gt; with a file struct. What you can not do is use the mirror from a different</span>
<span class="quote">&gt; &gt; process ie one with a different mm struct as mirror is linked to a single</span>
<span class="quote">&gt; &gt; mm. So on fork there is no callback to update the private file struct, when</span>
<span class="quote">&gt; &gt; the device file is duplicated (well just refcount inc) against a different</span>
<span class="quote">&gt; &gt; process. This is something you need to be carefull in your driver. Inside</span>
<span class="quote">&gt; &gt; the dummy driver i abuse that to actually test proper behavior of HMM but</span>
<span class="quote">&gt; &gt; it should not be use as an example.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So to confirm, on all file operations from user space the driver is </span>
<span class="quote">&gt; expected to check that current-&gt;mm matches the mm associated with the </span>
<span class="quote">&gt; struct file&#39;s hmm_mirror?</span>

Well you might have a valid usecase for that, just be aware that
anything your driver do with the hmm_mirror will actually impact
the mm of the parent. Which i assume is not what you want.

I would actualy thought that what you want is having a way to find
hmm_mirror using both device file &amp; mm as a key. Otherwise you can
not really use HMM with process that like to fork themself. Which
is a valid usecase to me. For instance process start using HMM
through your driver, decide to fork itself and to also use HMM
through your driver inside its child.
<span class="quote">
&gt; </span>
<span class="quote">&gt; On file-&gt;release the driver still ought to call hmm_mirror_unregister </span>
<span class="quote">&gt; regardless of whether the mms match, otherwise we&#39;ll never tear down the </span>
<span class="quote">&gt; mirror. That means we&#39;re not saved from the race condition because </span>
<span class="quote">&gt; hmm_mirror_unregister can happen in one thread while hmm_notifier_release </span>
<span class="quote">&gt; might be happening in another thread.</span>

Again there is no race the mirror list is the synchronization point and
it is protected by a lock. So either hmm_mirror_unregister() wins or the
other thread hmm_notifier_release()
<span class="quote">
&gt; &gt; &gt; But even if we assume no association at all between files and mirrors, are</span>
<span class="quote">&gt; &gt; &gt; you sure that prevents the race? The driver may choose to unregister the</span>
<span class="quote">&gt; &gt; &gt; hmm_device at any point once its files are closed. In the case of module</span>
<span class="quote">&gt; &gt; &gt; unload the device unregister can&#39;t be prevented. If mm teardown hasn&#39;t</span>
<span class="quote">&gt; &gt; &gt; happened yet mirrors may still be active and registered on that</span>
<span class="quote">&gt; &gt; &gt; hmm_device. The driver thus has to first call hmm_mirror_unregister on all</span>
<span class="quote">&gt; &gt; &gt; active mirrors, then call hmm_device_unregister. mm teardown of those</span>
<span class="quote">&gt; &gt; &gt; mirrors may trigger at any point in this sequence, so we&#39;re right back to</span>
<span class="quote">&gt; &gt; &gt; that race.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So when device driver unload the first thing it needs to do is kill all of</span>
<span class="quote">&gt; &gt; its context ie all of its HMM mirror (unregister them) by doing so it will</span>
<span class="quote">&gt; &gt; make sure that there can be no more call to any of its functions.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When is the driver expected to call hmm_mirror_unregister? Is it file </span>
<span class="quote">&gt; close, module unload, or some other time?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If it&#39;s file close, there&#39;s no need to unregister anything on module </span>
<span class="quote">&gt; unload because the files were all closed already.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If it&#39;s module unload, then the mirrors and mms all get leaked until that </span>
<span class="quote">&gt; point.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We&#39;re exposed to the race in both cases.</span>

You unregister as soon as you want, it is up to your driver to do it,
i do not enforce anything. The only thing i enforce is that you can
not unregister the hmm device driver before all mirror are unregistered
and free.

So yes for device driver you want to unregister when device file is
close (which happens when the process exit).

In both cases there is no race as explain above.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The race with mm teardown does not exist as what matter for mm teardown is</span>
<span class="quote">&gt; &gt; the fact that the mirror is on the struct hmm mirrors list or not. Either</span>
<span class="quote">&gt; &gt; the device driver is first to remove the mirror from the list or it is the</span>
<span class="quote">&gt; &gt; mm teardown but this is lock protected so only one thread can do it.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Agreed, removing the mirror from the list is not a &quot;race&quot; in the classical </span>
<span class="quote">&gt; sense. The true race is between hmm_notifier_release&#39;s device mutex_unlock </span>
<span class="quote">&gt; (process exit) and post-hmm_device_unregister device mutex free (driver </span>
<span class="quote">&gt; close/unload). What I meant is that in order to expose that race you first </span>
<span class="quote">&gt; need one thread to call hmm_mirror_unregister while another thread is in </span>
<span class="quote">&gt; hmm_notifier_release.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Regardless of where hmm_mirror_unregister is called (file close, module </span>
<span class="quote">&gt; unload, etc) it can happen concurrently with hmm_notifier_release so we&#39;re </span>
<span class="quote">&gt; exposed to this race.</span>

There is no race here, the mirror struct will only be freed once as again
the list is a synchronization point. Whoever remove the mirror from the
list is responsible to drop the list reference.

In the fixed code the only thing that will happen twice is the -&gt;release()
callback. Even that can be work around to garanty it is call only once.

Anyway i do not see anyrace here.

Cheers,
Jérôme
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=134341">Mark Hairgrove</a> - June 11, 2015, 1:15 a.m.</div>
<pre class="content">
On Wed, 10 Jun 2015, Jerome Glisse wrote:
<span class="quote">
&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Like said, just ignore current code it is utterly broken in so many way</span>
<span class="quote">&gt; when it comes to lifetime. I screw that part badly when reworking the</span>
<span class="quote">&gt; patchset, i was focusing on other part.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I fixed that in my tree, i am waiting for more review on other part as</span>
<span class="quote">&gt; anyway the lifetime thing is easy to rework/fix.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; http://cgit.freedesktop.org/~glisse/linux/log/?h=hmm</span>
<span class="quote">&gt; </span>

Ok, I&#39;m working through the other patches so I&#39;ll check the updates out 
once I&#39;ve made it through. My primary interest in this discussion is 
making sure we know the plan for mirror and device lifetimes.
<span class="quote">

&gt; &gt; So to confirm, on all file operations from user space the driver is </span>
<span class="quote">&gt; &gt; expected to check that current-&gt;mm matches the mm associated with the </span>
<span class="quote">&gt; &gt; struct file&#39;s hmm_mirror?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well you might have a valid usecase for that, just be aware that</span>
<span class="quote">&gt; anything your driver do with the hmm_mirror will actually impact</span>
<span class="quote">&gt; the mm of the parent. Which i assume is not what you want.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would actualy thought that what you want is having a way to find</span>
<span class="quote">&gt; hmm_mirror using both device file &amp; mm as a key. Otherwise you can</span>
<span class="quote">&gt; not really use HMM with process that like to fork themself. Which</span>
<span class="quote">&gt; is a valid usecase to me. For instance process start using HMM</span>
<span class="quote">&gt; through your driver, decide to fork itself and to also use HMM</span>
<span class="quote">&gt; through your driver inside its child.</span>

Agreed, that sounds reasonable, and the use case is valid. I was digging 
into this to make sure we don&#39;t prevent that.
<span class="quote">

&gt; &gt; </span>
<span class="quote">&gt; &gt; On file-&gt;release the driver still ought to call hmm_mirror_unregister </span>
<span class="quote">&gt; &gt; regardless of whether the mms match, otherwise we&#39;ll never tear down the </span>
<span class="quote">&gt; &gt; mirror. That means we&#39;re not saved from the race condition because </span>
<span class="quote">&gt; &gt; hmm_mirror_unregister can happen in one thread while hmm_notifier_release </span>
<span class="quote">&gt; &gt; might be happening in another thread.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Again there is no race the mirror list is the synchronization point and</span>
<span class="quote">&gt; it is protected by a lock. So either hmm_mirror_unregister() wins or the</span>
<span class="quote">&gt; other thread hmm_notifier_release()</span>

Yes, I agree. That&#39;s not the race I&#39;m worried about. I&#39;m worried about a 
race on the device lifetime, but in order to hit that one first 
hmm_notifier_release must take the lock and remove the mirror from the 
list before hmm_mirror_unregister does it. That&#39;s why I brought it up.
<span class="quote">

&gt; </span>
<span class="quote">&gt; You unregister as soon as you want, it is up to your driver to do it,</span>
<span class="quote">&gt; i do not enforce anything. The only thing i enforce is that you can</span>
<span class="quote">&gt; not unregister the hmm device driver before all mirror are unregistered</span>
<span class="quote">&gt; and free.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So yes for device driver you want to unregister when device file is</span>
<span class="quote">&gt; close (which happens when the process exit).</span>

Sounds good.
<span class="quote">

&gt; </span>
<span class="quote">&gt; There is no race here, the mirror struct will only be freed once as again</span>
<span class="quote">&gt; the list is a synchronization point. Whoever remove the mirror from the</span>
<span class="quote">&gt; list is responsible to drop the list reference.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In the fixed code the only thing that will happen twice is the -&gt;release()</span>
<span class="quote">&gt; callback. Even that can be work around to garanty it is call only once.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Anyway i do not see anyrace here.</span>
<span class="quote">&gt; </span>

The mirror lifetime is fine. The problem I see is with the device lifetime 
on a multi-core system. Imagine this sequence:

- On CPU1 the mm associated with the mirror is going down
- On CPU2 the driver unregisters the mirror then the device

When this happens, the last device mutex_unlock on CPU1 is the only thing 
preventing the free of the device in CPU2. That doesn&#39;t work, as described 
in this thread: https://lkml.org/lkml/2013/12/2/997

Here&#39;s the full sequence again with mutex_unlock split apart. Hopefully 
this shows the device_unregister problem more clearly:

CPU1 (mm release)                   CPU2 (driver)
----------------------              ----------------------
hmm_notifier_release
  down_write(&amp;hmm-&gt;rwsem);
  hlist_del_init(&amp;mirror-&gt;mlist);
  up_write(&amp;hmm-&gt;rwsem);

  // CPU1 thread is preempted or 
  // something
                                    hmm_mirror_unregister
                                      hmm_mirror_kill
                                        down_write(&amp;hmm-&gt;rwsem);
                                        // mirror removed by CPU1 already
                                        // so hlist_unhashed returns 1
                                        up_write(&amp;hmm-&gt;rwsem);

                                      hmm_mirror_unref(&amp;mirror);
                                      // Mirror ref now 1

                                      // CPU2 thread is preempted or
                                      // something
// CPU1 thread is scheduled

hmm_mirror_unref(&amp;mirror);
  // Mirror ref now 0, cleanup
  hmm_mirror_destroy(&amp;mirror)
    mutex_lock(&amp;device-&gt;mutex);
    list_del_init(&amp;mirror-&gt;dlist);
    device-&gt;ops-&gt;release(mirror);
      kfree(mirror);
                                      // CPU2 thread is scheduled, now
                                      // both CPU1 and CPU2 are running

                                    hmm_device_unregister
                                      mutex_lock(&amp;device-&gt;mutex);
                                        mutex_optimistic_spin()
    mutex_unlock(&amp;device-&gt;mutex);
      [...]
      __mutex_unlock_common_slowpath
        // CPU2 releases lock
        atomic_set(&amp;lock-&gt;count, 1);
                                          // Spinning CPU2 acquires now-
                                          // free lock
                                      // mutex_lock returns
                                      // Device list empty
                                      mutex_unlock(&amp;device-&gt;mutex);
                                      return 0;
                                    kfree(hmm_device);
        // CPU1 still accessing 
        // hmm_device-&gt;mutex in 
        //__mutex_unlock_common_slowpath
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - June 11, 2015, 2:23 p.m.</div>
<pre class="content">
On Wed, Jun 10, 2015 at 06:15:08PM -0700, Mark Hairgrove wrote:

[...]
<span class="quote">&gt; &gt; There is no race here, the mirror struct will only be freed once as again</span>
<span class="quote">&gt; &gt; the list is a synchronization point. Whoever remove the mirror from the</span>
<span class="quote">&gt; &gt; list is responsible to drop the list reference.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In the fixed code the only thing that will happen twice is the -&gt;release()</span>
<span class="quote">&gt; &gt; callback. Even that can be work around to garanty it is call only once.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Anyway i do not see anyrace here.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The mirror lifetime is fine. The problem I see is with the device lifetime </span>
<span class="quote">&gt; on a multi-core system. Imagine this sequence:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - On CPU1 the mm associated with the mirror is going down</span>
<span class="quote">&gt; - On CPU2 the driver unregisters the mirror then the device</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When this happens, the last device mutex_unlock on CPU1 is the only thing </span>
<span class="quote">&gt; preventing the free of the device in CPU2. That doesn&#39;t work, as described </span>
<span class="quote">&gt; in this thread: https://lkml.org/lkml/2013/12/2/997</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Here&#39;s the full sequence again with mutex_unlock split apart. Hopefully </span>
<span class="quote">&gt; this shows the device_unregister problem more clearly:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CPU1 (mm release)                   CPU2 (driver)</span>
<span class="quote">&gt; ----------------------              ----------------------</span>
<span class="quote">&gt; hmm_notifier_release</span>
<span class="quote">&gt;   down_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt;   hlist_del_init(&amp;mirror-&gt;mlist);</span>
<span class="quote">&gt;   up_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   // CPU1 thread is preempted or </span>
<span class="quote">&gt;   // something</span>
<span class="quote">&gt;                                     hmm_mirror_unregister</span>
<span class="quote">&gt;                                       hmm_mirror_kill</span>
<span class="quote">&gt;                                         down_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt;                                         // mirror removed by CPU1 already</span>
<span class="quote">&gt;                                         // so hlist_unhashed returns 1</span>
<span class="quote">&gt;                                         up_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                       hmm_mirror_unref(&amp;mirror);</span>
<span class="quote">&gt;                                       // Mirror ref now 1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                       // CPU2 thread is preempted or</span>
<span class="quote">&gt;                                       // something</span>
<span class="quote">&gt; // CPU1 thread is scheduled</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; hmm_mirror_unref(&amp;mirror);</span>
<span class="quote">&gt;   // Mirror ref now 0, cleanup</span>
<span class="quote">&gt;   hmm_mirror_destroy(&amp;mirror)</span>
<span class="quote">&gt;     mutex_lock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt;     list_del_init(&amp;mirror-&gt;dlist);</span>
<span class="quote">&gt;     device-&gt;ops-&gt;release(mirror);</span>
<span class="quote">&gt;       kfree(mirror);</span>
<span class="quote">&gt;                                       // CPU2 thread is scheduled, now</span>
<span class="quote">&gt;                                       // both CPU1 and CPU2 are running</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;                                     hmm_device_unregister</span>
<span class="quote">&gt;                                       mutex_lock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt;                                         mutex_optimistic_spin()</span>
<span class="quote">&gt;     mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt;       [...]</span>
<span class="quote">&gt;       __mutex_unlock_common_slowpath</span>
<span class="quote">&gt;         // CPU2 releases lock</span>
<span class="quote">&gt;         atomic_set(&amp;lock-&gt;count, 1);</span>
<span class="quote">&gt;                                           // Spinning CPU2 acquires now-</span>
<span class="quote">&gt;                                           // free lock</span>
<span class="quote">&gt;                                       // mutex_lock returns</span>
<span class="quote">&gt;                                       // Device list empty</span>
<span class="quote">&gt;                                       mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt;                                       return 0;</span>
<span class="quote">&gt;                                     kfree(hmm_device);</span>
<span class="quote">&gt;         // CPU1 still accessing </span>
<span class="quote">&gt;         // hmm_device-&gt;mutex in </span>
<span class="quote">&gt;         //__mutex_unlock_common_slowpath</span>

Ok i see the race you are afraid of and really it is an unlikely one
__mutex_unlock_common_slowpath() take a spinlock right after allowing
other to take the mutex, when we are in your scenario there is no
contention on that spinlock so it is taken right away and as there
is no one in the mutex wait list then it goes directly to unlock the
spinlock and return. You can ignore the debug function as if debugging
is enabled than the mutex_lock() would need to also take the spinlock
and thus you would have proper synchronization btw 2 thread thanks to
the mutex.wait_lock.

So basicly while CPU1 is going :
spin_lock(mutex.wait_lock)
if (!list_empty(mutex.wait_list)) {
  // wait_list is empty so branch not taken
}
spin_unlock(mutex.wait_lock)

CPU2 would have to test the mirror list and mutex_unlock and return
before the spin_unlock() of CPU1. This is a tight race, i can add a
synchronize_rcu() to device_unregister after the mutex_unlock() so
that we also add a grace period before the device is potentialy freed
which should make that race completely unlikely.

Moreover for something really bad to happen it would need that the
freed memory to be reallocated right away by some other thread. Which
really sound unlikely unless CPU1 is the slowest of all :)

Cheers,
Jérôme
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=134341">Mark Hairgrove</a> - June 11, 2015, 10:26 p.m.</div>
<pre class="content">
On Thu, 11 Jun 2015, Jerome Glisse wrote:
<span class="quote">
&gt; On Wed, Jun 10, 2015 at 06:15:08PM -0700, Mark Hairgrove wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; There is no race here, the mirror struct will only be freed once as again</span>
<span class="quote">&gt; &gt; &gt; the list is a synchronization point. Whoever remove the mirror from the</span>
<span class="quote">&gt; &gt; &gt; list is responsible to drop the list reference.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; In the fixed code the only thing that will happen twice is the -&gt;release()</span>
<span class="quote">&gt; &gt; &gt; callback. Even that can be work around to garanty it is call only once.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Anyway i do not see anyrace here.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The mirror lifetime is fine. The problem I see is with the device lifetime </span>
<span class="quote">&gt; &gt; on a multi-core system. Imagine this sequence:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; - On CPU1 the mm associated with the mirror is going down</span>
<span class="quote">&gt; &gt; - On CPU2 the driver unregisters the mirror then the device</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; When this happens, the last device mutex_unlock on CPU1 is the only thing </span>
<span class="quote">&gt; &gt; preventing the free of the device in CPU2. That doesn&#39;t work, as described </span>
<span class="quote">&gt; &gt; in this thread: https://lkml.org/lkml/2013/12/2/997</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Here&#39;s the full sequence again with mutex_unlock split apart. Hopefully </span>
<span class="quote">&gt; &gt; this shows the device_unregister problem more clearly:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; CPU1 (mm release)                   CPU2 (driver)</span>
<span class="quote">&gt; &gt; ----------------------              ----------------------</span>
<span class="quote">&gt; &gt; hmm_notifier_release</span>
<span class="quote">&gt; &gt;   down_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt; &gt;   hlist_del_init(&amp;mirror-&gt;mlist);</span>
<span class="quote">&gt; &gt;   up_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;   // CPU1 thread is preempted or </span>
<span class="quote">&gt; &gt;   // something</span>
<span class="quote">&gt; &gt;                                     hmm_mirror_unregister</span>
<span class="quote">&gt; &gt;                                       hmm_mirror_kill</span>
<span class="quote">&gt; &gt;                                         down_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt; &gt;                                         // mirror removed by CPU1 already</span>
<span class="quote">&gt; &gt;                                         // so hlist_unhashed returns 1</span>
<span class="quote">&gt; &gt;                                         up_write(&amp;hmm-&gt;rwsem);</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;                                       hmm_mirror_unref(&amp;mirror);</span>
<span class="quote">&gt; &gt;                                       // Mirror ref now 1</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;                                       // CPU2 thread is preempted or</span>
<span class="quote">&gt; &gt;                                       // something</span>
<span class="quote">&gt; &gt; // CPU1 thread is scheduled</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; hmm_mirror_unref(&amp;mirror);</span>
<span class="quote">&gt; &gt;   // Mirror ref now 0, cleanup</span>
<span class="quote">&gt; &gt;   hmm_mirror_destroy(&amp;mirror)</span>
<span class="quote">&gt; &gt;     mutex_lock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; &gt;     list_del_init(&amp;mirror-&gt;dlist);</span>
<span class="quote">&gt; &gt;     device-&gt;ops-&gt;release(mirror);</span>
<span class="quote">&gt; &gt;       kfree(mirror);</span>
<span class="quote">&gt; &gt;                                       // CPU2 thread is scheduled, now</span>
<span class="quote">&gt; &gt;                                       // both CPU1 and CPU2 are running</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;                                     hmm_device_unregister</span>
<span class="quote">&gt; &gt;                                       mutex_lock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; &gt;                                         mutex_optimistic_spin()</span>
<span class="quote">&gt; &gt;     mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; &gt;       [...]</span>
<span class="quote">&gt; &gt;       __mutex_unlock_common_slowpath</span>
<span class="quote">&gt; &gt;         // CPU2 releases lock</span>
<span class="quote">&gt; &gt;         atomic_set(&amp;lock-&gt;count, 1);</span>
<span class="quote">&gt; &gt;                                           // Spinning CPU2 acquires now-</span>
<span class="quote">&gt; &gt;                                           // free lock</span>
<span class="quote">&gt; &gt;                                       // mutex_lock returns</span>
<span class="quote">&gt; &gt;                                       // Device list empty</span>
<span class="quote">&gt; &gt;                                       mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="quote">&gt; &gt;                                       return 0;</span>
<span class="quote">&gt; &gt;                                     kfree(hmm_device);</span>
<span class="quote">&gt; &gt;         // CPU1 still accessing </span>
<span class="quote">&gt; &gt;         // hmm_device-&gt;mutex in </span>
<span class="quote">&gt; &gt;         //__mutex_unlock_common_slowpath</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok i see the race you are afraid of and really it is an unlikely one</span>
<span class="quote">&gt; __mutex_unlock_common_slowpath() take a spinlock right after allowing</span>
<span class="quote">&gt; other to take the mutex, when we are in your scenario there is no</span>
<span class="quote">&gt; contention on that spinlock so it is taken right away and as there</span>
<span class="quote">&gt; is no one in the mutex wait list then it goes directly to unlock the</span>
<span class="quote">&gt; spinlock and return. You can ignore the debug function as if debugging</span>
<span class="quote">&gt; is enabled than the mutex_lock() would need to also take the spinlock</span>
<span class="quote">&gt; and thus you would have proper synchronization btw 2 thread thanks to</span>
<span class="quote">&gt; the mutex.wait_lock.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So basicly while CPU1 is going :</span>
<span class="quote">&gt; spin_lock(mutex.wait_lock)</span>
<span class="quote">&gt; if (!list_empty(mutex.wait_list)) {</span>
<span class="quote">&gt;   // wait_list is empty so branch not taken</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; spin_unlock(mutex.wait_lock)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CPU2 would have to test the mirror list and mutex_unlock and return</span>
<span class="quote">&gt; before the spin_unlock() of CPU1. This is a tight race, i can add a</span>
<span class="quote">&gt; synchronize_rcu() to device_unregister after the mutex_unlock() so</span>
<span class="quote">&gt; that we also add a grace period before the device is potentialy freed</span>
<span class="quote">&gt; which should make that race completely unlikely.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Moreover for something really bad to happen it would need that the</span>
<span class="quote">&gt; freed memory to be reallocated right away by some other thread. Which</span>
<span class="quote">&gt; really sound unlikely unless CPU1 is the slowest of all :)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cheers,</span>
<span class="quote">&gt; Jérôme</span>
<span class="quote">&gt; </span>

But CPU1 could get preempted between the atomic_set and the 
spin_lock_mutex, and then it doesn&#39;t matter whether or not a grace period 
has elapsed before CPU2 proceeds.

Making race conditions less likely just makes them harder to pinpoint when 
they inevitably appear in the wild. I don&#39;t think it makes sense to spend 
any effort in making a race condition less likely, and that thread I 
referenced (https://lkml.org/lkml/2013/12/2/997) is fairly strong evidence 
that fixing this race actually matters. So, I think this race condition 
really needs to be fixed.

One fix is for hmm_mirror_unregister to wait for hmm_notifier_release 
completion between hmm_mirror_kill and hmm_mirror_unref. It can do this by 
calling synchronize_srcu() on the mmu_notifier&#39;s srcu. This has the 
benefit that the driver is guaranteed not to get the &quot;mm is dead&quot; callback 
after hmm_mirror_unregister returns.

In fact, are there any callbacks on the mirror that can arrive after 
hmm_mirror_unregister? If so, how will hmm_device_unregister solve them?

From a general standpoint, hmm_device_unregister must perform some kind of 
synchronization to be sure that all mirrors are completely released and 
done and no new callbacks will trigger. Since that has to be true, can&#39;t 
that synchronization be moved into hmm_mirror_unregister instead?

If that happens there&#39;s no need for a &quot;mirror can be freed&quot; -&gt;release 
callback at all because the driver is guaranteed that a mirror is done 
after hmm_mirror_unregister.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - June 15, 2015, 2:32 p.m.</div>
<pre class="content">
On Thu, Jun 11, 2015 at 03:26:46PM -0700, Mark Hairgrove wrote:
<span class="quote">&gt; On Thu, 11 Jun 2015, Jerome Glisse wrote:</span>
<span class="quote">&gt; &gt; On Wed, Jun 10, 2015 at 06:15:08PM -0700, Mark Hairgrove wrote:</span>

[...]
<span class="quote">&gt; &gt; Ok i see the race you are afraid of and really it is an unlikely one</span>
<span class="quote">&gt; &gt; __mutex_unlock_common_slowpath() take a spinlock right after allowing</span>
<span class="quote">&gt; &gt; other to take the mutex, when we are in your scenario there is no</span>
<span class="quote">&gt; &gt; contention on that spinlock so it is taken right away and as there</span>
<span class="quote">&gt; &gt; is no one in the mutex wait list then it goes directly to unlock the</span>
<span class="quote">&gt; &gt; spinlock and return. You can ignore the debug function as if debugging</span>
<span class="quote">&gt; &gt; is enabled than the mutex_lock() would need to also take the spinlock</span>
<span class="quote">&gt; &gt; and thus you would have proper synchronization btw 2 thread thanks to</span>
<span class="quote">&gt; &gt; the mutex.wait_lock.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So basicly while CPU1 is going :</span>
<span class="quote">&gt; &gt; spin_lock(mutex.wait_lock)</span>
<span class="quote">&gt; &gt; if (!list_empty(mutex.wait_list)) {</span>
<span class="quote">&gt; &gt;   // wait_list is empty so branch not taken</span>
<span class="quote">&gt; &gt; }</span>
<span class="quote">&gt; &gt; spin_unlock(mutex.wait_lock)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; CPU2 would have to test the mirror list and mutex_unlock and return</span>
<span class="quote">&gt; &gt; before the spin_unlock() of CPU1. This is a tight race, i can add a</span>
<span class="quote">&gt; &gt; synchronize_rcu() to device_unregister after the mutex_unlock() so</span>
<span class="quote">&gt; &gt; that we also add a grace period before the device is potentialy freed</span>
<span class="quote">&gt; &gt; which should make that race completely unlikely.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Moreover for something really bad to happen it would need that the</span>
<span class="quote">&gt; &gt; freed memory to be reallocated right away by some other thread. Which</span>
<span class="quote">&gt; &gt; really sound unlikely unless CPU1 is the slowest of all :)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Cheers,</span>
<span class="quote">&gt; &gt; Jérôme</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But CPU1 could get preempted between the atomic_set and the </span>
<span class="quote">&gt; spin_lock_mutex, and then it doesn&#39;t matter whether or not a grace period </span>
<span class="quote">&gt; has elapsed before CPU2 proceeds.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Making race conditions less likely just makes them harder to pinpoint when </span>
<span class="quote">&gt; they inevitably appear in the wild. I don&#39;t think it makes sense to spend </span>
<span class="quote">&gt; any effort in making a race condition less likely, and that thread I </span>
<span class="quote">&gt; referenced (https://lkml.org/lkml/2013/12/2/997) is fairly strong evidence </span>
<span class="quote">&gt; that fixing this race actually matters. So, I think this race condition </span>
<span class="quote">&gt; really needs to be fixed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; One fix is for hmm_mirror_unregister to wait for hmm_notifier_release </span>
<span class="quote">&gt; completion between hmm_mirror_kill and hmm_mirror_unref. It can do this by </span>
<span class="quote">&gt; calling synchronize_srcu() on the mmu_notifier&#39;s srcu. This has the </span>
<span class="quote">&gt; benefit that the driver is guaranteed not to get the &quot;mm is dead&quot; callback </span>
<span class="quote">&gt; after hmm_mirror_unregister returns.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In fact, are there any callbacks on the mirror that can arrive after </span>
<span class="quote">&gt; hmm_mirror_unregister? If so, how will hmm_device_unregister solve them?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; From a general standpoint, hmm_device_unregister must perform some kind of </span>
<span class="quote">&gt; synchronization to be sure that all mirrors are completely released and </span>
<span class="quote">&gt; done and no new callbacks will trigger. Since that has to be true, can&#39;t </span>
<span class="quote">&gt; that synchronization be moved into hmm_mirror_unregister instead?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If that happens there&#39;s no need for a &quot;mirror can be freed&quot; -&gt;release </span>
<span class="quote">&gt; callback at all because the driver is guaranteed that a mirror is done </span>
<span class="quote">&gt; after hmm_mirror_unregister.</span>

Well there is no need or 2 callback (relase|stop , free) just one, the
release|stop that is needed. I kind of went halfway last week on this.
I will probably rework that a little to keep just one call and rely on
driver to call hmm_mirror_unregister()

Cheers,
Jérôme
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="p_header">index 78ea7b6..2f2a2be 100644</span>
<span class="p_header">--- a/MAINTAINERS</span>
<span class="p_header">+++ b/MAINTAINERS</span>
<span class="p_chunk">@@ -4730,6 +4730,13 @@</span> <span class="p_context"> F:	include/uapi/linux/if_hippi.h</span>
 F:	net/802/hippi.c
 F:	drivers/net/hippi/
 
<span class="p_add">+HMM - Heterogeneous Memory Management</span>
<span class="p_add">+M:	Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+L:	linux-mm@kvack.org</span>
<span class="p_add">+S:	Maintained</span>
<span class="p_add">+F:	mm/hmm.c</span>
<span class="p_add">+F:	include/linux/hmm.h</span>
<span class="p_add">+</span>
 HOST AP DRIVER
 M:	Jouni Malinen &lt;j@w1.fi&gt;
 L:	hostap@shmoo.com (subscribers-only)
<span class="p_header">diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
new file mode 100644
<span class="p_header">index 0000000..175a757</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/hmm.h</span>
<span class="p_chunk">@@ -0,0 +1,164 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright 2013 Red Hat Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+/* This is a heterogeneous memory management (hmm). In a nutshell this provide</span>
<span class="p_add">+ * an API to mirror a process address on a device which has its own mmu using</span>
<span class="p_add">+ * its own page table for the process. It supports everything except special</span>
<span class="p_add">+ * vma.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Mandatory hardware features :</span>
<span class="p_add">+ *   - An mmu with pagetable.</span>
<span class="p_add">+ *   - Read only flag per cpu page.</span>
<span class="p_add">+ *   - Page fault ie hardware must stop and wait for kernel to service fault.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Optional hardware features :</span>
<span class="p_add">+ *   - Dirty bit per cpu page.</span>
<span class="p_add">+ *   - Access bit per cpu page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The hmm code handle all the interfacing with the core kernel mm code and</span>
<span class="p_add">+ * provide a simple API. It does support migrating system memory to device</span>
<span class="p_add">+ * memory and handle migration back to system memory on cpu page fault.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Migrated memory is considered as swaped from cpu and core mm code point of</span>
<span class="p_add">+ * view.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef _HMM_H</span>
<span class="p_add">+#define _HMM_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_HMM</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/list.h&gt;</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;linux/atomic.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm_types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mmu_notifier.h&gt;</span>
<span class="p_add">+#include &lt;linux/workqueue.h&gt;</span>
<span class="p_add">+#include &lt;linux/mman.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+struct hmm_device;</span>
<span class="p_add">+struct hmm_mirror;</span>
<span class="p_add">+struct hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_device - Each device must register one and only one hmm_device.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The hmm_device is the link btw HMM and each device driver.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* struct hmm_device_operations - HMM device operation callback</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm_device_ops {</span>
<span class="p_add">+	/* release() - mirror must stop using the address space.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * @mirror: The mirror that link process address space with the device.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * When this is call, device driver must kill all device thread using</span>
<span class="p_add">+	 * this mirror. Also, this callback is the last thing call by HMM and</span>
<span class="p_add">+	 * HMM will not access the mirror struct after this call (ie no more</span>
<span class="p_add">+	 * dereference of it so it is safe for the device driver to free it).</span>
<span class="p_add">+	 * It is call either from :</span>
<span class="p_add">+	 *   - mm dying (all process using this mm exiting).</span>
<span class="p_add">+	 *   - hmm_mirror_unregister() (if no other thread holds a reference)</span>
<span class="p_add">+	 *   - outcome of some device error reported by any of the device</span>
<span class="p_add">+	 *     callback against that mirror.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	void (*release)(struct hmm_mirror *mirror);</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* struct hmm - per mm_struct HMM states.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mm: The mm struct this hmm is associated with.</span>
<span class="p_add">+ * @mirrors: List of all mirror for this mm (one per device).</span>
<span class="p_add">+ * @vm_end: Last valid address for this mm (exclusive).</span>
<span class="p_add">+ * @kref: Reference counter.</span>
<span class="p_add">+ * @rwsem: Serialize the mirror list modifications.</span>
<span class="p_add">+ * @mmu_notifier: The mmu_notifier of this mm.</span>
<span class="p_add">+ * @rcu: For delayed cleanup call from mmu_notifier.release() callback.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For each process address space (mm_struct) there is one and only one hmm</span>
<span class="p_add">+ * struct. hmm functions will redispatch to each devices the change made to</span>
<span class="p_add">+ * the process address space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Device driver must not access this structure other than for getting the</span>
<span class="p_add">+ * mm pointer.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm {</span>
<span class="p_add">+	struct mm_struct	*mm;</span>
<span class="p_add">+	struct hlist_head	mirrors;</span>
<span class="p_add">+	unsigned long		vm_end;</span>
<span class="p_add">+	struct kref		kref;</span>
<span class="p_add">+	struct rw_semaphore	rwsem;</span>
<span class="p_add">+	struct mmu_notifier	mmu_notifier;</span>
<span class="p_add">+	struct rcu_head		rcu;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* struct hmm_device - per device HMM structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @dev: Linux device structure pointer.</span>
<span class="p_add">+ * @ops: The hmm operations callback.</span>
<span class="p_add">+ * @mirrors: List of all active mirrors for the device.</span>
<span class="p_add">+ * @mutex: Mutex protecting mirrors list.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each device that want to mirror an address space must register one of this</span>
<span class="p_add">+ * struct (only once per linux device).</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm_device {</span>
<span class="p_add">+	struct device			*dev;</span>
<span class="p_add">+	const struct hmm_device_ops	*ops;</span>
<span class="p_add">+	struct list_head		mirrors;</span>
<span class="p_add">+	struct mutex			mutex;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+int hmm_device_register(struct hmm_device *device);</span>
<span class="p_add">+int hmm_device_unregister(struct hmm_device *device);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_mirror - device specific mirroring functions.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each device that mirror a process has a uniq hmm_mirror struct associating</span>
<span class="p_add">+ * the process address space with the device. Same process can be mirrored by</span>
<span class="p_add">+ * several different devices at the same time.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* struct hmm_mirror - per device and per mm HMM structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @device: The hmm_device struct this hmm_mirror is associated to.</span>
<span class="p_add">+ * @hmm: The hmm struct this hmm_mirror is associated to.</span>
<span class="p_add">+ * @kref: Reference counter (private to HMM do not use).</span>
<span class="p_add">+ * @dlist: List of all hmm_mirror for same device.</span>
<span class="p_add">+ * @mlist: List of all hmm_mirror for same process.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each device that want to mirror an address space must register one of this</span>
<span class="p_add">+ * struct for each of the address space it wants to mirror. Same device can</span>
<span class="p_add">+ * mirror several different address space. As well same address space can be</span>
<span class="p_add">+ * mirror by different devices.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm_mirror {</span>
<span class="p_add">+	struct hmm_device	*device;</span>
<span class="p_add">+	struct hmm		*hmm;</span>
<span class="p_add">+	struct kref		kref;</span>
<span class="p_add">+	struct list_head	dlist;</span>
<span class="p_add">+	struct hlist_node	mlist;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+int hmm_mirror_register(struct hmm_mirror *mirror);</span>
<span class="p_add">+void hmm_mirror_unregister(struct hmm_mirror *mirror);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_HMM */</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 2923a51..cf642d9 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -2199,5 +2199,16 @@</span> <span class="p_context"> void __init setup_nr_node_ids(void);</span>
 static inline void setup_nr_node_ids(void) {}
 #endif
 
<span class="p_add">+#ifdef CONFIG_HMM</span>
<span class="p_add">+static inline void hmm_mm_init(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mm-&gt;hmm = NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else /* !CONFIG_HMM */</span>
<span class="p_add">+static inline void hmm_mm_init(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* !CONFIG_HMM */</span>
<span class="p_add">+</span>
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MM_H */
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index 0038ac7..4494f7f 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -15,6 +15,10 @@</span> <span class="p_context"></span>
 #include &lt;asm/page.h&gt;
 #include &lt;asm/mmu.h&gt;
 
<span class="p_add">+#ifdef CONFIG_HMM</span>
<span class="p_add">+struct hmm;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifndef AT_VECTOR_SIZE_ARCH
 #define AT_VECTOR_SIZE_ARCH 0
 #endif
<span class="p_chunk">@@ -451,6 +455,16 @@</span> <span class="p_context"> struct mm_struct {</span>
 #ifdef CONFIG_MMU_NOTIFIER
 	struct mmu_notifier_mm *mmu_notifier_mm;
 #endif
<span class="p_add">+#ifdef CONFIG_HMM</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * hmm always register an mmu_notifier we rely on mmu notifier to keep</span>
<span class="p_add">+	 * refcount on mm struct as well as forbiding registering hmm on a</span>
<span class="p_add">+	 * dying mm</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This field is set with mmap_sem old in write mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+#endif</span>
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS
 	pgtable_t pmd_huge_pte; /* protected by page_table_lock */
 #endif
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 0e0ae9a..4083be7 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -27,6 +27,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/binfmts.h&gt;
 #include &lt;linux/mman.h&gt;
 #include &lt;linux/mmu_notifier.h&gt;
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
 #include &lt;linux/fs.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/vmacache.h&gt;
<span class="p_chunk">@@ -597,6 +598,7 @@</span> <span class="p_context"> static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)</span>
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
 	mmu_notifier_mm_init(mm);
<span class="p_add">+	hmm_mm_init(mm);</span>
 	clear_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS
 	mm-&gt;pmd_huge_pte = NULL;
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index 52ffb86..189e48f 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -653,3 +653,18 @@</span> <span class="p_context"> config DEFERRED_STRUCT_PAGE_INIT</span>
 	  when kswapd starts. This has a potential performance impact on
 	  processes running early in the lifetime of the systemm until kswapd
 	  finishes the initialisation.
<span class="p_add">+</span>
<span class="p_add">+if STAGING</span>
<span class="p_add">+config HMM</span>
<span class="p_add">+	bool &quot;Enable heterogeneous memory management (HMM)&quot;</span>
<span class="p_add">+	depends on MMU</span>
<span class="p_add">+	select MMU_NOTIFIER</span>
<span class="p_add">+	select GENERIC_PAGE_TABLE</span>
<span class="p_add">+	default n</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Heterogeneous memory management provide infrastructure for a device</span>
<span class="p_add">+	  to mirror a process address space into an hardware mmu or into any</span>
<span class="p_add">+	  things supporting pagefault like event.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If unsure, say N to disable hmm.</span>
<span class="p_add">+endif # STAGING</span>
<span class="p_header">diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="p_header">index 98c4eae..90ca9c4 100644</span>
<span class="p_header">--- a/mm/Makefile</span>
<span class="p_header">+++ b/mm/Makefile</span>
<span class="p_chunk">@@ -78,3 +78,4 @@</span> <span class="p_context"> obj-$(CONFIG_CMA)	+= cma.o</span>
 obj-$(CONFIG_MEMORY_BALLOON) += balloon_compaction.o
 obj-$(CONFIG_PAGE_EXTENSION) += page_ext.o
 obj-$(CONFIG_CMA_DEBUGFS) += cma_debug.o
<span class="p_add">+obj-$(CONFIG_HMM) += hmm.o</span>
<span class="p_header">diff --git a/mm/hmm.c b/mm/hmm.c</span>
new file mode 100644
<span class="p_header">index 0000000..e684dd0</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/hmm.c</span>
<span class="p_chunk">@@ -0,0 +1,370 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright 2013 Red Hat Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+/* This is the core code for heterogeneous memory management (HMM). HMM intend</span>
<span class="p_add">+ * to provide helper for mirroring a process address space on a device as well</span>
<span class="p_add">+ * as allowing migration of data between system memory and device memory refer</span>
<span class="p_add">+ * as remote memory from here on out.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Refer to include/linux/hmm.h for further information on general design.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/export.h&gt;</span>
<span class="p_add">+#include &lt;linux/bitmap.h&gt;</span>
<span class="p_add">+#include &lt;linux/list.h&gt;</span>
<span class="p_add">+#include &lt;linux/rculist.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/mmu_notifier.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/hugetlb.h&gt;</span>
<span class="p_add">+#include &lt;linux/fs.h&gt;</span>
<span class="p_add">+#include &lt;linux/file.h&gt;</span>
<span class="p_add">+#include &lt;linux/ksm.h&gt;</span>
<span class="p_add">+#include &lt;linux/rmap.h&gt;</span>
<span class="p_add">+#include &lt;linux/swap.h&gt;</span>
<span class="p_add">+#include &lt;linux/swapops.h&gt;</span>
<span class="p_add">+#include &lt;linux/mmu_context.h&gt;</span>
<span class="p_add">+#include &lt;linux/memcontrol.h&gt;</span>
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
<span class="p_add">+#include &lt;linux/wait.h&gt;</span>
<span class="p_add">+#include &lt;linux/mman.h&gt;</span>
<span class="p_add">+#include &lt;linux/delay.h&gt;</span>
<span class="p_add">+#include &lt;linux/workqueue.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;internal.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+static struct mmu_notifier_ops hmm_notifier_ops;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct hmm_mirror *hmm_mirror_ref(struct hmm_mirror *mirror);</span>
<span class="p_add">+static inline void hmm_mirror_unref(struct hmm_mirror **mirror);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm - core HMM functions.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Core HMM functions that deal with all the process mm activities.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static int hmm_init(struct hmm *hmm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	hmm-&gt;mm = current-&gt;mm;</span>
<span class="p_add">+	hmm-&gt;vm_end = TASK_SIZE;</span>
<span class="p_add">+	kref_init(&amp;hmm-&gt;kref);</span>
<span class="p_add">+	INIT_HLIST_HEAD(&amp;hmm-&gt;mirrors);</span>
<span class="p_add">+	init_rwsem(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* register notifier */</span>
<span class="p_add">+	hmm-&gt;mmu_notifier.ops = &amp;hmm_notifier_ops;</span>
<span class="p_add">+	return __mmu_notifier_register(&amp;hmm-&gt;mmu_notifier, current-&gt;mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int hmm_add_mirror(struct hmm *hmm, struct hmm_mirror *mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_mirror *tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	down_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+	hlist_for_each_entry(tmp, &amp;hmm-&gt;mirrors, mlist)</span>
<span class="p_add">+		if (tmp-&gt;device == mirror-&gt;device) {</span>
<span class="p_add">+			/* Same device can mirror only once. */</span>
<span class="p_add">+			up_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	hlist_add_head(&amp;mirror-&gt;mlist, &amp;hmm-&gt;mirrors);</span>
<span class="p_add">+	hmm_mirror_ref(mirror);</span>
<span class="p_add">+	up_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct hmm *hmm_ref(struct hmm *hmm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!hmm || !kref_get_unless_zero(&amp;hmm-&gt;kref))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	return hmm;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_destroy_delayed(struct rcu_head *rcu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm = container_of(rcu, struct hmm, rcu);</span>
<span class="p_add">+	kfree(hmm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_destroy(struct kref *kref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm = container_of(kref, struct hmm, kref);</span>
<span class="p_add">+	BUG_ON(!hlist_empty(&amp;hmm-&gt;mirrors));</span>
<span class="p_add">+</span>
<span class="p_add">+	down_write(&amp;hmm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	/* A new hmm might have been register before reaching that point. */</span>
<span class="p_add">+	if (hmm-&gt;mm-&gt;hmm == hmm)</span>
<span class="p_add">+		hmm-&gt;mm-&gt;hmm = NULL;</span>
<span class="p_add">+	up_write(&amp;hmm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_unregister_no_release(&amp;hmm-&gt;mmu_notifier, hmm-&gt;mm);</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_call_srcu(&amp;hmm-&gt;rcu, &amp;hmm_destroy_delayed);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct hmm *hmm_unref(struct hmm *hmm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (hmm)</span>
<span class="p_add">+		kref_put(&amp;hmm-&gt;kref, hmm_destroy);</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_notifier - HMM callback for mmu_notifier tracking change to process mm.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM use use mmu notifier to track change made to process address space.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void hmm_notifier_release(struct mmu_notifier *mn, struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm = hmm_ref(container_of(mn, struct hmm, mmu_notifier));</span>
<span class="p_add">+	if (!hmm)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	down_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+	while (hmm-&gt;mirrors.first) {</span>
<span class="p_add">+		struct hmm_mirror *mirror;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Here we are holding the mirror reference from the mirror</span>
<span class="p_add">+		 * list. As list removal is synchronized through spinlock, no</span>
<span class="p_add">+		 * other thread can assume it holds that reference.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		mirror = hlist_entry(hmm-&gt;mirrors.first,</span>
<span class="p_add">+				     struct hmm_mirror,</span>
<span class="p_add">+				     mlist);</span>
<span class="p_add">+		hlist_del_init(&amp;mirror-&gt;mlist);</span>
<span class="p_add">+		up_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+		hmm_mirror_unref(&amp;mirror);</span>
<span class="p_add">+</span>
<span class="p_add">+		down_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	up_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm_unref(hmm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct mmu_notifier_ops hmm_notifier_ops = {</span>
<span class="p_add">+	.release		= hmm_notifier_release,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_mirror - per device mirroring functions.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each device that mirror a process has a uniq hmm_mirror struct. A process</span>
<span class="p_add">+ * can be mirror by several devices at the same time.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Below are all the functions and their helpers use by device driver to mirror</span>
<span class="p_add">+ * the process address space. Those functions either deals with updating the</span>
<span class="p_add">+ * device page table (through hmm callback). Or provide helper functions use by</span>
<span class="p_add">+ * the device driver to fault in range of memory in the device page table.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline struct hmm_mirror *hmm_mirror_ref(struct hmm_mirror *mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!mirror || !kref_get_unless_zero(&amp;mirror-&gt;kref))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	return mirror;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_mirror_destroy(struct kref *kref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_device *device;</span>
<span class="p_add">+	struct hmm_mirror *mirror;</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+	mirror = container_of(kref, struct hmm_mirror, kref);</span>
<span class="p_add">+	device = mirror-&gt;device;</span>
<span class="p_add">+	hmm = mirror-&gt;hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;device-&gt;mutex);</span>
<span class="p_add">+	list_del_init(&amp;mirror-&gt;dlist);</span>
<span class="p_add">+	device-&gt;ops-&gt;release(mirror);</span>
<span class="p_add">+	mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void hmm_mirror_unref(struct hmm_mirror **mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_mirror *tmp = mirror ? *mirror : NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tmp) {</span>
<span class="p_add">+		*mirror = NULL;</span>
<span class="p_add">+		kref_put(&amp;tmp-&gt;kref, hmm_mirror_destroy);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_mirror_register() - register mirror against current process for a device.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mirror: The mirror struct being registered.</span>
<span class="p_add">+ * Returns: 0 on success or -ENOMEM, -EINVAL on error.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Call when device driver want to start mirroring a process address space. The</span>
<span class="p_add">+ * HMM shim will register mmu_notifier and start monitoring process address</span>
<span class="p_add">+ * space changes. Hence callback to device driver might happen even before this</span>
<span class="p_add">+ * function return.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The task device driver want to mirror must be current !</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Only one mirror per mm and hmm_device can be created, it will return NULL if</span>
<span class="p_add">+ * the hmm_device already has an hmm_mirror for the the mm.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int hmm_mirror_register(struct hmm_mirror *mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	struct hmm *hmm = NULL;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Sanity checks. */</span>
<span class="p_add">+	BUG_ON(!mirror);</span>
<span class="p_add">+	BUG_ON(!mirror-&gt;device);</span>
<span class="p_add">+	BUG_ON(!mm);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Initialize the mirror struct fields, the mlist init and del dance is</span>
<span class="p_add">+	 * necessary to make the error path easier for driver and for hmm.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kref_init(&amp;mirror-&gt;kref);</span>
<span class="p_add">+	INIT_HLIST_NODE(&amp;mirror-&gt;mlist);</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;mirror-&gt;dlist);</span>
<span class="p_add">+	mutex_lock(&amp;mirror-&gt;device-&gt;mutex);</span>
<span class="p_add">+	list_add(&amp;mirror-&gt;dlist, &amp;mirror-&gt;device-&gt;mirrors);</span>
<span class="p_add">+	mutex_unlock(&amp;mirror-&gt;device-&gt;mutex);</span>
<span class="p_add">+</span>
<span class="p_add">+	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm = mm-&gt;hmm ? hmm_ref(hmm) : NULL;</span>
<span class="p_add">+	if (hmm == NULL) {</span>
<span class="p_add">+		/* no hmm registered yet so register one */</span>
<span class="p_add">+		hmm = kzalloc(sizeof(*mm-&gt;hmm), GFP_KERNEL);</span>
<span class="p_add">+		if (hmm == NULL) {</span>
<span class="p_add">+			up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			goto error;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = hmm_init(hmm);</span>
<span class="p_add">+		if (ret) {</span>
<span class="p_add">+			up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			kfree(hmm);</span>
<span class="p_add">+			goto error;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		mm-&gt;hmm = hmm;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	mirror-&gt;hmm = hmm;</span>
<span class="p_add">+	ret = hmm_add_mirror(hmm, mirror);</span>
<span class="p_add">+	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		mirror-&gt;hmm = NULL;</span>
<span class="p_add">+		hmm_unref(hmm);</span>
<span class="p_add">+		goto error;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+error:</span>
<span class="p_add">+	mutex_lock(&amp;mirror-&gt;device-&gt;mutex);</span>
<span class="p_add">+	list_del_init(&amp;mirror-&gt;dlist);</span>
<span class="p_add">+	mutex_unlock(&amp;mirror-&gt;device-&gt;mutex);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_mirror_register);</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_mirror_kill(struct hmm_mirror *mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	down_write(&amp;mirror-&gt;hmm-&gt;rwsem);</span>
<span class="p_add">+	if (!hlist_unhashed(&amp;mirror-&gt;mlist)) {</span>
<span class="p_add">+		hlist_del_init(&amp;mirror-&gt;mlist);</span>
<span class="p_add">+		up_write(&amp;mirror-&gt;hmm-&gt;rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+		hmm_mirror_unref(&amp;mirror);</span>
<span class="p_add">+	} else</span>
<span class="p_add">+		up_write(&amp;mirror-&gt;hmm-&gt;rwsem);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_mirror_unregister() - unregister a mirror.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mirror: The mirror that link process address space with the device.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Driver can call this function when it wants to stop mirroring a process.</span>
<span class="p_add">+ * This will trigger a call to the -&gt;release() callback if it did not aleady</span>
<span class="p_add">+ * happen.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that caller must hold a reference on the mirror.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void hmm_mirror_unregister(struct hmm_mirror *mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (mirror == NULL)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm_mirror_kill(mirror);</span>
<span class="p_add">+	hmm_mirror_unref(&amp;mirror);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_mirror_unregister);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_device - Each device driver must register one and only one hmm_device</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The hmm_device is the link btw HMM and each device driver.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_device_register() - register a device with HMM.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @device: The hmm_device struct.</span>
<span class="p_add">+ * Returns: 0 on success or -EINVAL otherwise.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Call when device driver want to register itself with HMM. Device driver must</span>
<span class="p_add">+ * only register once.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int hmm_device_register(struct hmm_device *device)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* sanity check */</span>
<span class="p_add">+	BUG_ON(!device);</span>
<span class="p_add">+	BUG_ON(!device-&gt;ops);</span>
<span class="p_add">+	BUG_ON(!device-&gt;ops-&gt;release);</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_init(&amp;device-&gt;mutex);</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;device-&gt;mirrors);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_device_register);</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_device_unregister() - unregister a device with HMM.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @device: The hmm_device struct.</span>
<span class="p_add">+ * Returns: 0 on success or -EBUSY otherwise.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Call when device driver want to unregister itself with HMM. This will check</span>
<span class="p_add">+ * that there is no any active mirror and returns -EBUSY if so.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int hmm_device_unregister(struct hmm_device *device)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mutex_lock(&amp;device-&gt;mutex);</span>
<span class="p_add">+	if (!list_empty(&amp;device-&gt;mirrors)) {</span>
<span class="p_add">+		mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="p_add">+		return -EBUSY;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	mutex_unlock(&amp;device-&gt;mutex);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_device_unregister);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



