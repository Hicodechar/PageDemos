
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[3/3] mm: migrate: Stabilise page count when migrating transparent hugepages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [3/3] mm: migrate: Stabilise page count when migrating transparent hugepages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=7096">Will Deacon</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 6, 2017, 5:58 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1496771916-28203-4-git-send-email-will.deacon@arm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9769447/mbox/"
   >mbox</a>
|
   <a href="/patch/9769447/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9769447/">/patch/9769447/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	4F29D6035D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 17:59:05 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4CF3223201
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 17:59:05 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 419C9284EE; Tue,  6 Jun 2017 17:59:05 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B782123201
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 17:59:04 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751652AbdFFR6c (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 6 Jun 2017 13:58:32 -0400
Received: from foss.arm.com ([217.140.101.70]:50640 &quot;EHLO foss.arm.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751405AbdFFR6a (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 6 Jun 2017 13:58:30 -0400
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
	by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id 8F59615BE;
	Tue,  6 Jun 2017 10:58:29 -0700 (PDT)
Received: from edgewater-inn.cambridge.arm.com
	(usa-sjc-imap-foss1.foss.arm.com [10.72.51.249])
	by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPA id
	604793F911; Tue,  6 Jun 2017 10:58:29 -0700 (PDT)
Received: by edgewater-inn.cambridge.arm.com (Postfix, from userid 1000)
	id 04C811AE17A1; Tue,  6 Jun 2017 18:58:36 +0100 (BST)
From: Will Deacon &lt;will.deacon@arm.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc: mark.rutland@arm.com, akpm@linux-foundation.org,
	kirill.shutemov@linux.intel.com, Punit.Agrawal@arm.com,
	mgorman@suse.de, steve.capper@arm.com, Will Deacon &lt;will.deacon@arm.com&gt;
Subject: [PATCH 3/3] mm: migrate: Stabilise page count when migrating
	transparent hugepages
Date: Tue,  6 Jun 2017 18:58:36 +0100
Message-Id: &lt;1496771916-28203-4-git-send-email-will.deacon@arm.com&gt;
X-Mailer: git-send-email 2.1.4
In-Reply-To: &lt;1496771916-28203-1-git-send-email-will.deacon@arm.com&gt;
References: &lt;1496771916-28203-1-git-send-email-will.deacon@arm.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - June 6, 2017, 5:58 p.m.</div>
<pre class="content">
When migrating a transparent hugepage, migrate_misplaced_transhuge_page
guards itself against a concurrent fastgup of the page by checking that
the page count is equal to 2 before and after installing the new pmd.

If the page count changes, then the pmd is reverted back to the original
entry, however there is a small window where the new (possibly writable)
pmd is installed and the underlying page could be written by userspace.
Restoring the old pmd could therefore result in loss of data.

This patch fixes the problem by freezing the page count whilst updating
the page tables, which protects against a concurrent fastgup without the
need to restore the old pmd in the failure case (since the page count can
no longer change under our feet).

Cc: Mel Gorman &lt;mgorman@suse.de&gt;
<span class="signed-off-by">Signed-off-by: Will Deacon &lt;will.deacon@arm.com&gt;</span>
---
 mm/migrate.c | 15 ++-------------
 1 file changed, 2 insertions(+), 13 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - June 8, 2017, 10:47 a.m.</div>
<pre class="content">
On Tue, Jun 06, 2017 at 06:58:36PM +0100, Will Deacon wrote:
<span class="quote">&gt; When migrating a transparent hugepage, migrate_misplaced_transhuge_page</span>
<span class="quote">&gt; guards itself against a concurrent fastgup of the page by checking that</span>
<span class="quote">&gt; the page count is equal to 2 before and after installing the new pmd.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If the page count changes, then the pmd is reverted back to the original</span>
<span class="quote">&gt; entry, however there is a small window where the new (possibly writable)</span>
<span class="quote">&gt; pmd is installed and the underlying page could be written by userspace.</span>
<span class="quote">&gt; Restoring the old pmd could therefore result in loss of data.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch fixes the problem by freezing the page count whilst updating</span>
<span class="quote">&gt; the page tables, which protects against a concurrent fastgup without the</span>
<span class="quote">&gt; need to restore the old pmd in the failure case (since the page count can</span>
<span class="quote">&gt; no longer change under our feet).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="quote">&gt; Signed-off-by: Will Deacon &lt;will.deacon@arm.com&gt;</span>

Looks correct to me.
<span class="acked-by">
Acked-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - June 8, 2017, 10:52 a.m.</div>
<pre class="content">
On 06/06/2017 07:58 PM, Will Deacon wrote:
<span class="quote">&gt; When migrating a transparent hugepage, migrate_misplaced_transhuge_page</span>
<span class="quote">&gt; guards itself against a concurrent fastgup of the page by checking that</span>
<span class="quote">&gt; the page count is equal to 2 before and after installing the new pmd.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If the page count changes, then the pmd is reverted back to the original</span>
<span class="quote">&gt; entry, however there is a small window where the new (possibly writable)</span>
<span class="quote">&gt; pmd is installed and the underlying page could be written by userspace.</span>
<span class="quote">&gt; Restoring the old pmd could therefore result in loss of data.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch fixes the problem by freezing the page count whilst updating</span>
<span class="quote">&gt; the page tables, which protects against a concurrent fastgup without the</span>
<span class="quote">&gt; need to restore the old pmd in the failure case (since the page count can</span>
<span class="quote">&gt; no longer change under our feet).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="quote">&gt; Signed-off-by: Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  mm/migrate.c | 15 ++-------------</span>
<span class="quote">&gt;  1 file changed, 2 insertions(+), 13 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index 89a0a1707f4c..8b21f1b1ec6e 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -1913,7 +1913,6 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;  	int page_lru = page_is_file_cache(page);</span>
<span class="quote">&gt;  	unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt;  	unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt; -	pmd_t orig_entry;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Rate-limit the amount of data that is being migrated to a node.</span>
<span class="quote">&gt; @@ -1956,8 +1955,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;  	/* Recheck the target PMD */</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; -	if (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {</span>
<span class="quote">&gt; -fail_putback:</span>
<span class="quote">&gt; +	if (unlikely(!pmd_same(*pmd, entry) || !page_ref_freeze(page, 2))) {</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1979,7 +1977,6 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	orig_entry = *pmd;</span>
<span class="quote">&gt;  	entry = mk_huge_pmd(new_page, vma-&gt;vm_page_prot);</span>
<span class="quote">&gt;  	entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1996,15 +1993,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>

There&#39;s a comment above this:

       /*
         * Clear the old entry under pagetable lock and establish the new PTE.
         * Any parallel GUP will either observe the old page blocking on the
         * page lock, block on the page table lock or observe the new page.
         * The SetPageUptodate on the new page and page_add_new_anon_rmap
         * guarantee the copy is visible before the pagetable update.
         */

Is it still correct? Didn&#39;t the freezing prevent some of the cases above?
<span class="quote">
&gt;  	set_pmd_at(mm, mmun_start, pmd, entry);</span>
<span class="quote">&gt;  	update_mmu_cache_pmd(vma, address, &amp;entry);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (page_count(page) != 2) {</span>

BTW, how did the old code recognize that page count would increase and then
decrease back?
<span class="quote">
&gt; -		set_pmd_at(mm, mmun_start, pmd, orig_entry);</span>
<span class="quote">&gt; -		flush_pmd_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; -		update_mmu_cache_pmd(vma, address, &amp;entry);</span>
<span class="quote">&gt; -		page_remove_rmap(new_page, true);</span>
<span class="quote">&gt; -		goto fail_putback;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +	page_ref_unfreeze(page, 2);</span>
<span class="quote">&gt;  	mlock_migrate_page(new_page, page);</span>
<span class="quote">&gt;  	page_remove_rmap(page, true);</span>
<span class="quote">&gt;  	set_page_owner_migrate_reason(new_page, MR_NUMA_MISPLACED);</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - June 8, 2017, 12:07 p.m.</div>
<pre class="content">
On Thu, Jun 08, 2017 at 12:52:07PM +0200, Vlastimil Babka wrote:
<span class="quote">&gt; On 06/06/2017 07:58 PM, Will Deacon wrote:</span>
<span class="quote">&gt; &gt; When migrating a transparent hugepage, migrate_misplaced_transhuge_page</span>
<span class="quote">&gt; &gt; guards itself against a concurrent fastgup of the page by checking that</span>
<span class="quote">&gt; &gt; the page count is equal to 2 before and after installing the new pmd.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If the page count changes, then the pmd is reverted back to the original</span>
<span class="quote">&gt; &gt; entry, however there is a small window where the new (possibly writable)</span>
<span class="quote">&gt; &gt; pmd is installed and the underlying page could be written by userspace.</span>
<span class="quote">&gt; &gt; Restoring the old pmd could therefore result in loss of data.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This patch fixes the problem by freezing the page count whilst updating</span>
<span class="quote">&gt; &gt; the page tables, which protects against a concurrent fastgup without the</span>
<span class="quote">&gt; &gt; need to restore the old pmd in the failure case (since the page count can</span>
<span class="quote">&gt; &gt; no longer change under our feet).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Cc: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  mm/migrate.c | 15 ++-------------</span>
<span class="quote">&gt; &gt;  1 file changed, 2 insertions(+), 13 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; &gt; index 89a0a1707f4c..8b21f1b1ec6e 100644</span>
<span class="quote">&gt; &gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; &gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; &gt; @@ -1913,7 +1913,6 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt; &gt;  	int page_lru = page_is_file_cache(page);</span>
<span class="quote">&gt; &gt;  	unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt; &gt;  	unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt; &gt; -	pmd_t orig_entry;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	/*</span>
<span class="quote">&gt; &gt;  	 * Rate-limit the amount of data that is being migrated to a node.</span>
<span class="quote">&gt; &gt; @@ -1956,8 +1955,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt; &gt;  	/* Recheck the target PMD */</span>
<span class="quote">&gt; &gt;  	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; &gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; &gt; -	if (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {</span>
<span class="quote">&gt; &gt; -fail_putback:</span>
<span class="quote">&gt; &gt; +	if (unlikely(!pmd_same(*pmd, entry) || !page_ref_freeze(page, 2))) {</span>
<span class="quote">&gt; &gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; &gt;  		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -1979,7 +1977,6 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt; &gt;  		goto out_unlock;</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	orig_entry = *pmd;</span>
<span class="quote">&gt; &gt;  	entry = mk_huge_pmd(new_page, vma-&gt;vm_page_prot);</span>
<span class="quote">&gt; &gt;  	entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -1996,15 +1993,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There&#39;s a comment above this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;        /*</span>
<span class="quote">&gt;          * Clear the old entry under pagetable lock and establish the new PTE.</span>
<span class="quote">&gt;          * Any parallel GUP will either observe the old page blocking on the</span>
<span class="quote">&gt;          * page lock, block on the page table lock or observe the new page.</span>
<span class="quote">&gt;          * The SetPageUptodate on the new page and page_add_new_anon_rmap</span>
<span class="quote">&gt;          * guarantee the copy is visible before the pagetable update.</span>
<span class="quote">&gt;          */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is it still correct? Didn&#39;t the freezing prevent some of the cases above?</span>

I don&#39;t think the comment needs to change, the freezing is just doing
correctly what the code tried to do before. Granted, the blocking might come
about because of the count momentarily being set to 0 (and
page_cache_add_speculative bailing), but that&#39;s just fastGUP implementation
details, I think.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;  	set_pmd_at(mm, mmun_start, pmd, entry);</span>
<span class="quote">&gt; &gt;  	update_mmu_cache_pmd(vma, address, &amp;entry);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	if (page_count(page) != 2) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; BTW, how did the old code recognize that page count would increase and then</span>
<span class="quote">&gt; decrease back?</span>

I&#39;m not sure that case matters because the inc/dec would happen before the
new PMD is put in place (otherwise it wouldn&#39;t be reachable via the
fastGUP).

Will
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=146341">zhong jiang</a> - June 9, 2017, 8:25 a.m.</div>
<pre class="content">
On 2017/6/8 20:07, Will Deacon wrote:
<span class="quote">&gt; On Thu, Jun 08, 2017 at 12:52:07PM +0200, Vlastimil Babka wrote:</span>
<span class="quote">&gt;&gt; On 06/06/2017 07:58 PM, Will Deacon wrote:</span>
<span class="quote">&gt;&gt;&gt; When migrating a transparent hugepage, migrate_misplaced_transhuge_page</span>
<span class="quote">&gt;&gt;&gt; guards itself against a concurrent fastgup of the page by checking that</span>
<span class="quote">&gt;&gt;&gt; the page count is equal to 2 before and after installing the new pmd.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; If the page count changes, then the pmd is reverted back to the original</span>
<span class="quote">&gt;&gt;&gt; entry, however there is a small window where the new (possibly writable)</span>
<span class="quote">&gt;&gt;&gt; pmd is installed and the underlying page could be written by userspace.</span>
<span class="quote">&gt;&gt;&gt; Restoring the old pmd could therefore result in loss of data.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This patch fixes the problem by freezing the page count whilst updating</span>
<span class="quote">&gt;&gt;&gt; the page tables, which protects against a concurrent fastgup without the</span>
<span class="quote">&gt;&gt;&gt; need to restore the old pmd in the failure case (since the page count can</span>
<span class="quote">&gt;&gt;&gt; no longer change under our feet).</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Cc: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="quote">&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt;  mm/migrate.c | 15 ++-------------</span>
<span class="quote">&gt;&gt;&gt;  1 file changed, 2 insertions(+), 13 deletions(-)</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt;&gt;&gt; index 89a0a1707f4c..8b21f1b1ec6e 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt;&gt;&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt;&gt;&gt; @@ -1913,7 +1913,6 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;&gt;  	int page_lru = page_is_file_cache(page);</span>
<span class="quote">&gt;&gt;&gt;  	unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt;&gt;&gt;  	unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt;&gt;&gt; -	pmd_t orig_entry;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;&gt;  	 * Rate-limit the amount of data that is being migrated to a node.</span>
<span class="quote">&gt;&gt;&gt; @@ -1956,8 +1955,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;&gt;  	/* Recheck the target PMD */</span>
<span class="quote">&gt;&gt;&gt;  	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;&gt;&gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt;&gt;&gt; -	if (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {</span>
<span class="quote">&gt;&gt;&gt; -fail_putback:</span>
<span class="quote">&gt;&gt;&gt; +	if (unlikely(!pmd_same(*pmd, entry) || !page_ref_freeze(page, 2))) {</span>
<span class="quote">&gt;&gt;&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;&gt;&gt;  		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; @@ -1979,7 +1977,6 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;&gt;  		goto out_unlock;</span>
<span class="quote">&gt;&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	orig_entry = *pmd;</span>
<span class="quote">&gt;&gt;&gt;  	entry = mk_huge_pmd(new_page, vma-&gt;vm_page_prot);</span>
<span class="quote">&gt;&gt;&gt;  	entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; @@ -1996,15 +1993,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt; There&#39;s a comment above this:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;        /*</span>
<span class="quote">&gt;&gt;          * Clear the old entry under pagetable lock and establish the new PTE.</span>
<span class="quote">&gt;&gt;          * Any parallel GUP will either observe the old page blocking on the</span>
<span class="quote">&gt;&gt;          * page lock, block on the page table lock or observe the new page.</span>
<span class="quote">&gt;&gt;          * The SetPageUptodate on the new page and page_add_new_anon_rmap</span>
<span class="quote">&gt;&gt;          * guarantee the copy is visible before the pagetable update.</span>
<span class="quote">&gt;&gt;          */</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Is it still correct? Didn&#39;t the freezing prevent some of the cases above?</span>
<span class="quote">&gt; I don&#39;t think the comment needs to change, the freezing is just doing</span>
<span class="quote">&gt; correctly what the code tried to do before. Granted, the blocking might come</span>
<span class="quote">&gt; about because of the count momentarily being set to 0 (and</span>
<span class="quote">&gt; page_cache_add_speculative bailing), but that&#39;s just fastGUP implementation</span>
<span class="quote">&gt; details, I think.</span>
  The pagetable lock will prevent the fastgup by userspace.  why the race will come across
  I do not unaderstand , can you explain it please?

 Thanks
 zhongjiang
<span class="quote">&gt;&gt;&gt;  	set_pmd_at(mm, mmun_start, pmd, entry);</span>
<span class="quote">&gt;&gt;&gt;  	update_mmu_cache_pmd(vma, address, &amp;entry);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	if (page_count(page) != 2) {</span>
<span class="quote">&gt;&gt; BTW, how did the old code recognize that page count would increase and then</span>
<span class="quote">&gt;&gt; decrease back?</span>
<span class="quote">&gt; I&#39;m not sure that case matters because the inc/dec would happen before the</span>
<span class="quote">&gt; new PMD is put in place (otherwise it wouldn&#39;t be reachable via the</span>
<span class="quote">&gt; fastGUP).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Will</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; .</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=146341">zhong jiang</a> - June 9, 2017, 9:16 a.m.</div>
<pre class="content">
On 2017/6/8 20:07, Will Deacon wrote:
<span class="quote">&gt; On Thu, Jun 08, 2017 at 12:52:07PM +0200, Vlastimil Babka wrote:</span>
<span class="quote">&gt;&gt; On 06/06/2017 07:58 PM, Will Deacon wrote:</span>
<span class="quote">&gt;&gt;&gt; When migrating a transparent hugepage, migrate_misplaced_transhuge_page</span>
<span class="quote">&gt;&gt;&gt; guards itself against a concurrent fastgup of the page by checking that</span>
<span class="quote">&gt;&gt;&gt; the page count is equal to 2 before and after installing the new pmd.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; If the page count changes, then the pmd is reverted back to the original</span>
<span class="quote">&gt;&gt;&gt; entry, however there is a small window where the new (possibly writable)</span>
<span class="quote">&gt;&gt;&gt; pmd is installed and the underlying page could be written by userspace.</span>
<span class="quote">&gt;&gt;&gt; Restoring the old pmd could therefore result in loss of data.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This patch fixes the problem by freezing the page count whilst updating</span>
<span class="quote">&gt;&gt;&gt; the page tables, which protects against a concurrent fastgup without the</span>
<span class="quote">&gt;&gt;&gt; need to restore the old pmd in the failure case (since the page count can</span>
<span class="quote">&gt;&gt;&gt; no longer change under our feet).</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Cc: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="quote">&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt;  mm/migrate.c | 15 ++-------------</span>
<span class="quote">&gt;&gt;&gt;  1 file changed, 2 insertions(+), 13 deletions(-)</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt;&gt;&gt; index 89a0a1707f4c..8b21f1b1ec6e 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt;&gt;&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt;&gt;&gt; @@ -1913,7 +1913,6 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;&gt;  	int page_lru = page_is_file_cache(page);</span>
<span class="quote">&gt;&gt;&gt;  	unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt;&gt;&gt;  	unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt;&gt;&gt; -	pmd_t orig_entry;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  	/*</span>
<span class="quote">&gt;&gt;&gt;  	 * Rate-limit the amount of data that is being migrated to a node.</span>
<span class="quote">&gt;&gt;&gt; @@ -1956,8 +1955,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;&gt;  	/* Recheck the target PMD */</span>
<span class="quote">&gt;&gt;&gt;  	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;&gt;&gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt;&gt;&gt; -	if (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {</span>
<span class="quote">&gt;&gt;&gt; -fail_putback:</span>
<span class="quote">&gt;&gt;&gt; +	if (unlikely(!pmd_same(*pmd, entry) || !page_ref_freeze(page, 2))) {</span>
<span class="quote">&gt;&gt;&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;&gt;&gt;  		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; @@ -1979,7 +1977,6 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;&gt;  		goto out_unlock;</span>
<span class="quote">&gt;&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	orig_entry = *pmd;</span>
<span class="quote">&gt;&gt;&gt;  	entry = mk_huge_pmd(new_page, vma-&gt;vm_page_prot);</span>
<span class="quote">&gt;&gt;&gt;  	entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; @@ -1996,15 +1993,7 @@ int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt; There&#39;s a comment above this:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;        /*</span>
<span class="quote">&gt;&gt;          * Clear the old entry under pagetable lock and establish the new PTE.</span>
<span class="quote">&gt;&gt;          * Any parallel GUP will either observe the old page blocking on the</span>
<span class="quote">&gt;&gt;          * page lock, block on the page table lock or observe the new page.</span>
<span class="quote">&gt;&gt;          * The SetPageUptodate on the new page and page_add_new_anon_rmap</span>
<span class="quote">&gt;&gt;          * guarantee the copy is visible before the pagetable update.</span>
<span class="quote">&gt;&gt;          */</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Is it still correct? Didn&#39;t the freezing prevent some of the cases above?</span>
<span class="quote">&gt; I don&#39;t think the comment needs to change, the freezing is just doing</span>
<span class="quote">&gt; correctly what the code tried to do before. Granted, the blocking might come</span>
<span class="quote">&gt; about because of the count momentarily being set to 0 (and</span>
<span class="quote">&gt; page_cache_add_speculative bailing), but that&#39;s just fastGUP implementation</span>
<span class="quote">&gt; details, I think.</span>
 I think it should be hold off the speculative , but the fastgup by userspace.
<span class="quote">&gt;&gt;&gt;  	set_pmd_at(mm, mmun_start, pmd, entry);</span>
<span class="quote">&gt;&gt;&gt;  	update_mmu_cache_pmd(vma, address, &amp;entry);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; -	if (page_count(page) != 2) {</span>
<span class="quote">&gt;&gt; BTW, how did the old code recognize that page count would increase and then</span>
<span class="quote">&gt;&gt; decrease back?</span>
<span class="quote">&gt; I&#39;m not sure that case matters because the inc/dec would happen before the</span>
<span class="quote">&gt; new PMD is put in place (otherwise it wouldn&#39;t be reachable via the</span>
<span class="quote">&gt; fastGUP).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Will</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; .</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 89a0a1707f4c..8b21f1b1ec6e 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1913,7 +1913,6 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 	int page_lru = page_is_file_cache(page);
 	unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;
 	unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;
<span class="p_del">-	pmd_t orig_entry;</span>
 
 	/*
 	 * Rate-limit the amount of data that is being migrated to a node.
<span class="p_chunk">@@ -1956,8 +1955,7 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 	/* Recheck the target PMD */
 	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
 	ptl = pmd_lock(mm, pmd);
<span class="p_del">-	if (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {</span>
<span class="p_del">-fail_putback:</span>
<span class="p_add">+	if (unlikely(!pmd_same(*pmd, entry) || !page_ref_freeze(page, 2))) {</span>
 		spin_unlock(ptl);
 		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 
<span class="p_chunk">@@ -1979,7 +1977,6 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 		goto out_unlock;
 	}
 
<span class="p_del">-	orig_entry = *pmd;</span>
 	entry = mk_huge_pmd(new_page, vma-&gt;vm_page_prot);
 	entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
 
<span class="p_chunk">@@ -1996,15 +1993,7 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 	set_pmd_at(mm, mmun_start, pmd, entry);
 	update_mmu_cache_pmd(vma, address, &amp;entry);
 
<span class="p_del">-	if (page_count(page) != 2) {</span>
<span class="p_del">-		set_pmd_at(mm, mmun_start, pmd, orig_entry);</span>
<span class="p_del">-		flush_pmd_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="p_del">-		mmu_notifier_invalidate_range(mm, mmun_start, mmun_end);</span>
<span class="p_del">-		update_mmu_cache_pmd(vma, address, &amp;entry);</span>
<span class="p_del">-		page_remove_rmap(new_page, true);</span>
<span class="p_del">-		goto fail_putback;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_add">+	page_ref_unfreeze(page, 2);</span>
 	mlock_migrate_page(new_page, page);
 	page_remove_rmap(page, true);
 	set_page_owner_migrate_reason(new_page, MR_NUMA_MISPLACED);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



