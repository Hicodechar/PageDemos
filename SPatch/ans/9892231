
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v7,9/9] sparc64: Add support for ADI (Application Data Integrity) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v7,9/9] sparc64: Add support for ADI (Application Data Integrity)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=63231">Khalid Aziz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 9, 2017, 9:26 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;3a687666c2e7972fb6d2379848f31006ac1dd59a.1502219353.git.khalid.aziz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9892231/mbox/"
   >mbox</a>
|
   <a href="/patch/9892231/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9892231/">/patch/9892231/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	AD17160363 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  9 Aug 2017 21:29:17 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9CD78286F8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  9 Aug 2017 21:29:17 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 909F5288A7; Wed,  9 Aug 2017 21:29:17 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 25015286F8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  9 Aug 2017 21:29:15 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752576AbdHIV3M (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 9 Aug 2017 17:29:12 -0400
Received: from userp1040.oracle.com ([156.151.31.81]:19846 &quot;EHLO
	userp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752095AbdHIV3I (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 9 Aug 2017 17:29:08 -0400
Received: from aserv0021.oracle.com (aserv0021.oracle.com [141.146.126.233])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id v79LS1UP001619
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Wed, 9 Aug 2017 21:28:02 GMT
Received: from aserv0121.oracle.com (aserv0121.oracle.com [141.146.126.235])
	by aserv0021.oracle.com (8.14.4/8.14.4) with ESMTP id
	v79LS1U6027732
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Wed, 9 Aug 2017 21:28:01 GMT
Received: from abhmp0013.oracle.com (abhmp0013.oracle.com [141.146.116.19])
	by aserv0121.oracle.com (8.14.4/8.13.8) with ESMTP id
	v79LS1pJ018752; Wed, 9 Aug 2017 21:28:01 GMT
Received: from concerto.us.oracle.com (/24.9.64.241)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Wed, 09 Aug 2017 14:28:00 -0700
From: Khalid Aziz &lt;khalid.aziz@oracle.com&gt;
To: davem@davemloft.net, dave.hansen@linux.intel.com
Cc: Khalid Aziz &lt;khalid.aziz@oracle.com&gt;, corbet@lwn.net,
	bob.picco@oracle.com, steven.sistare@oracle.com,
	pasha.tatashin@oracle.com, mike.kravetz@oracle.com,
	mingo@kernel.org, nitin.m.gupta@oracle.com,
	kirill.shutemov@linux.intel.com, tom.hromatka@oracle.com,
	eric.saint.etienne@oracle.com, allen.pais@oracle.com,
	cmetcalf@mellanox.com, akpm@linux-foundation.org,
	geert@linux-m68k.org, tklauser@distanz.ch, atish.patra@oracle.com,
	vijay.ac.kumar@oracle.com, peterz@infradead.org, mhocko@suse.com,
	jack@suse.cz, lstoakes@gmail.com, hughd@google.com,
	thomas.tai@oracle.com, paul.gortmaker@windriver.com,
	ross.zwisler@linux.intel.com, dave.jiang@intel.com,
	willy@infradead.org, ying.huang@intel.com, zhongjiang@huawei.com,
	minchan@kernel.org, vegard.nossum@oracle.com,
	imbrenda@linux.vnet.ibm.com, aneesh.kumar@linux.vnet.ibm.com,
	aarcange@redhat.com, linux-doc@vger.kernel.org,
	linux-kernel@vger.kernel.org, sparclinux@vger.kernel.org,
	linux-mm@kvack.org, Khalid Aziz &lt;khalid@gonehiking.org&gt;
Subject: [PATCH v7 9/9] sparc64: Add support for ADI (Application Data
	Integrity)
Date: Wed,  9 Aug 2017 15:26:02 -0600
Message-Id: &lt;3a687666c2e7972fb6d2379848f31006ac1dd59a.1502219353.git.khalid.aziz@oracle.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;cover.1502219353.git.khalid.aziz@oracle.com&gt;
References: &lt;cover.1502219353.git.khalid.aziz@oracle.com&gt;
In-Reply-To: &lt;cover.1502219353.git.khalid.aziz@oracle.com&gt;
References: &lt;cover.1502219353.git.khalid.aziz@oracle.com&gt;
X-Source-IP: aserv0021.oracle.com [141.146.126.233]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=63231">Khalid Aziz</a> - Aug. 9, 2017, 9:26 p.m.</div>
<pre class="content">
ADI is a new feature supported on SPARC M7 and newer processors to allow
hardware to catch rogue accesses to memory. ADI is supported for data
fetches only and not instruction fetches. An app can enable ADI on its
data pages, set version tags on them and use versioned addresses to
access the data pages. Upper bits of the address contain the version
tag. On M7 processors, upper four bits (bits 63-60) contain the version
tag. If a rogue app attempts to access ADI enabled data pages, its
access is blocked and processor generates an exception. Please see
Documentation/sparc/adi.txt for further details.

This patch extends mprotect to enable ADI (TSTATE.mcde), enable/disable
MCD (Memory Corruption Detection) on selected memory ranges, enable
TTE.mcd in PTEs, return ADI parameters to userspace and save/restore ADI
version tags on page swap out/in or migration. ADI is not enabled by
default for any task. A task must explicitly enable ADI on a memory
range and set version tag for ADI to be effective for the task.
<span class="signed-off-by">
Signed-off-by: Khalid Aziz &lt;khalid.aziz@oracle.com&gt;</span>
Cc: Khalid Aziz &lt;khalid@gonehiking.org&gt;
---
v7:
	- Enhanced arch_validate_prot() to enable ADI only on writable
	  addresses backed by physical RAM
	- Added support for saving/restoring ADI tags for each ADI
	  block size address range on a page on swap in/out
	- Added code to copy ADI tags on COW
	- Updated values for auxiliary vectors to not conflict with
	  values on other architectures to avoid conflict in glibc. glibc
	  consolidates all auxiliary vectors into its headers and
	  duplicate values in consolidated header are problematic
	- Disable same page merging on ADI enabled pages since ADI tags
	  may not match on pages with identical data
	- Broke the patch up further into smaller patches

v6:
	- Eliminated instructions to read and write PSTATE as well as
	  MCDPER and PMCDPER on every access to userspace addresses
	  by setting PSTATE and PMCDPER correctly upon entry into
	  kernel. PSTATE.mcde and PMCDPER are set upon entry into
	  kernel when running on an M7 processor. PSTATE.mcde being
	  set only affects memory accesses that have TTE.mcd set.
	  PMCDPER being set only affects writes to memory addresses
	  that have TTE.mcd set. This ensures any faults caused by
	  ADI tag mismatch on a write are exposed before kernel returns
	  to userspace.

v5:
	- Fixed indentation issues and instrcuctions in assembly code
	- Removed CONFIG_SPARC64 from mdesc.c
	- Changed to maintain state of MCDPER register in thread info
	  flags as opposed to in mm context. MCDPER is a per-thread
	  state and belongs in thread info flag as opposed to mm context
	  which is shared across threads. Added comments to clarify this
	  is a lazily maintained state and must be updated on context
	  switch and copy_process()
	- Updated code to use the new arch_do_swap_page() and
	  arch_unmap_one() functions

v4:
	- Broke patch up into smaller patches

v3:
	- Removed CONFIG_SPARC_ADI
	- Replaced prctl commands with mprotect
	- Added auxiliary vectors for ADI parameters
	- Enabled ADI for swappable pages

v2:
	- Fixed a build error

 Documentation/sparc/adi.txt             | 272 +++++++++++++++++++++++++++++++
 arch/sparc/include/asm/mman.h           |  72 ++++++++-
 arch/sparc/include/asm/mmu_64.h         |  17 ++
 arch/sparc/include/asm/mmu_context_64.h |  43 +++++
 arch/sparc/include/asm/page_64.h        |   4 +
 arch/sparc/include/asm/pgtable_64.h     |  46 ++++++
 arch/sparc/include/asm/thread_info_64.h |   2 +-
 arch/sparc/include/asm/trap_block.h     |   2 +
 arch/sparc/include/uapi/asm/mman.h      |   2 +
 arch/sparc/kernel/adi_64.c              | 277 ++++++++++++++++++++++++++++++++
 arch/sparc/kernel/etrap_64.S            |  28 +++-
 arch/sparc/kernel/process_64.c          |  25 +++
 arch/sparc/kernel/setup_64.c            |  11 +-
 arch/sparc/kernel/vmlinux.lds.S         |   5 +
 arch/sparc/mm/gup.c                     |  37 +++++
 arch/sparc/mm/hugetlbpage.c             |  14 +-
 arch/sparc/mm/init_64.c                 |  33 ++++
 arch/sparc/mm/tsb.c                     |  21 +++
 include/linux/mm.h                      |   3 +
 mm/ksm.c                                |   4 +
 20 files changed, 913 insertions(+), 5 deletions(-)
 create mode 100644 Documentation/sparc/adi.txt
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=129">David Miller</a> - Aug. 16, 2017, 4:58 a.m.</div>
<pre class="content">
<span class="from">From: Khalid Aziz &lt;khalid.aziz@oracle.com&gt;</span>
Date: Wed,  9 Aug 2017 15:26:02 -0600
<span class="quote">
&gt; +void adi_restore_tags(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		      unsigned long addr, pte_t pte)</span>
<span class="quote">&gt; +{</span>
 ...
<span class="quote">&gt; +	tag = tag_start(addr, tag_desc);</span>
<span class="quote">&gt; +	paddr = pte_val(pte) &amp; _PAGE_PADDR_4V;</span>
<span class="quote">&gt; +	for (tmp = paddr; tmp &lt; (paddr+PAGE_SIZE); tmp += adi_blksize()) {</span>
<span class="quote">&gt; +		version1 = (*tag) &gt;&gt; 4;</span>
<span class="quote">&gt; +		version2 = (*tag) &amp; 0x0f;</span>
<span class="quote">&gt; +		*tag++ = 0;</span>
<span class="quote">&gt; +		asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="quote">&gt; +			:</span>
<span class="quote">&gt; +			: &quot;r&quot; (version1), &quot;r&quot; (tmp),</span>
<span class="quote">&gt; +			  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt; +		tmp += adi_blksize();</span>
<span class="quote">&gt; +		asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="quote">&gt; +			:</span>
<span class="quote">&gt; +			: &quot;r&quot; (version2), &quot;r&quot; (tmp),</span>
<span class="quote">&gt; +			  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	asm volatile(&quot;membar #Sync\n\t&quot;);</span>

You do a membar here.
<span class="quote">
&gt; +		for (i = pfrom; i &lt; (pfrom + PAGE_SIZE); i += adi_blksize()) {</span>
<span class="quote">&gt; +			asm volatile(&quot;ldxa [%1] %2, %0\n\t&quot;</span>
<span class="quote">&gt; +					: &quot;=r&quot; (adi_tag)</span>
<span class="quote">&gt; +					:  &quot;r&quot; (i), &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt; +			asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="quote">&gt; +					:</span>
<span class="quote">&gt; +					: &quot;r&quot; (adi_tag), &quot;r&quot; (pto),</span>
<span class="quote">&gt; +					  &quot;i&quot; (ASI_MCD_REAL));</span>

But not here.

Is this OK?  I suspect you need to add a membar this this second piece
of MCD tag storing code.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=63231">Khalid Aziz</a> - Aug. 16, 2017, 2:44 p.m.</div>
<pre class="content">
On 08/15/2017 10:58 PM, David Miller wrote:
<span class="quote">&gt; From: Khalid Aziz &lt;khalid.aziz@oracle.com&gt;</span>
<span class="quote">&gt; Date: Wed,  9 Aug 2017 15:26:02 -0600</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +void adi_restore_tags(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; +		      unsigned long addr, pte_t pte)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;   ...</span>
<span class="quote">&gt;&gt; +	tag = tag_start(addr, tag_desc);</span>
<span class="quote">&gt;&gt; +	paddr = pte_val(pte) &amp; _PAGE_PADDR_4V;</span>
<span class="quote">&gt;&gt; +	for (tmp = paddr; tmp &lt; (paddr+PAGE_SIZE); tmp += adi_blksize()) {</span>
<span class="quote">&gt;&gt; +		version1 = (*tag) &gt;&gt; 4;</span>
<span class="quote">&gt;&gt; +		version2 = (*tag) &amp; 0x0f;</span>
<span class="quote">&gt;&gt; +		*tag++ = 0;</span>
<span class="quote">&gt;&gt; +		asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="quote">&gt;&gt; +			:</span>
<span class="quote">&gt;&gt; +			: &quot;r&quot; (version1), &quot;r&quot; (tmp),</span>
<span class="quote">&gt;&gt; +			  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt;&gt; +		tmp += adi_blksize();</span>
<span class="quote">&gt;&gt; +		asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="quote">&gt;&gt; +			:</span>
<span class="quote">&gt;&gt; +			: &quot;r&quot; (version2), &quot;r&quot; (tmp),</span>
<span class="quote">&gt;&gt; +			  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	asm volatile(&quot;membar #Sync\n\t&quot;);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You do a membar here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +		for (i = pfrom; i &lt; (pfrom + PAGE_SIZE); i += adi_blksize()) {</span>
<span class="quote">&gt;&gt; +			asm volatile(&quot;ldxa [%1] %2, %0\n\t&quot;</span>
<span class="quote">&gt;&gt; +					: &quot;=r&quot; (adi_tag)</span>
<span class="quote">&gt;&gt; +					:  &quot;r&quot; (i), &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt;&gt; +			asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="quote">&gt;&gt; +					:</span>
<span class="quote">&gt;&gt; +					: &quot;r&quot; (adi_tag), &quot;r&quot; (pto),</span>
<span class="quote">&gt;&gt; +					  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But not here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this OK?  I suspect you need to add a membar this this second piece</span>
<span class="quote">&gt; of MCD tag storing code.</span>

Hi Dave,

You are right. This tag storing code needs membar as well. I will add that.

Thanks,
Khalid
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173107">Anthony Yznaga</a> - Aug. 25, 2017, 10:31 p.m.</div>
<pre class="content">
<span class="quote">&gt; On Aug 9, 2017, at 2:26 PM, Khalid Aziz &lt;khalid.aziz@oracle.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ADI is a new feature supported on SPARC M7 and newer processors to allow</span>
<span class="quote">&gt; hardware to catch rogue accesses to memory. ADI is supported for data</span>
<span class="quote">&gt; fetches only and not instruction fetches. An app can enable ADI on its</span>
<span class="quote">&gt; data pages, set version tags on them and use versioned addresses to</span>
<span class="quote">&gt; access the data pages. Upper bits of the address contain the version</span>
<span class="quote">&gt; tag. On M7 processors, upper four bits (bits 63-60) contain the version</span>
<span class="quote">&gt; tag. If a rogue app attempts to access ADI enabled data pages, its</span>
<span class="quote">&gt; access is blocked and processor generates an exception. Please see</span>
<span class="quote">&gt; Documentation/sparc/adi.txt for further details.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch extends mprotect to enable ADI (TSTATE.mcde), enable/disable</span>
<span class="quote">&gt; MCD (Memory Corruption Detection) on selected memory ranges, enable</span>
<span class="quote">&gt; TTE.mcd in PTEs, return ADI parameters to userspace and save/restore ADI</span>
<span class="quote">&gt; version tags on page swap out/in or migration. ADI is not enabled by</span>
<span class="quote">&gt; default for any task. A task must explicitly enable ADI on a memory</span>
<span class="quote">&gt; range and set version tag for ADI to be effective for the task.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Khalid Aziz &lt;khalid.aziz@oracle.com&gt;</span>
<span class="quote">&gt; Cc: Khalid Aziz &lt;khalid@gonehiking.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; v7:</span>
<span class="quote">&gt; 	- Enhanced arch_validate_prot() to enable ADI only on writable</span>
<span class="quote">&gt; 	  addresses backed by physical RAM</span>
<span class="quote">&gt; 	- Added support for saving/restoring ADI tags for each ADI</span>
<span class="quote">&gt; 	  block size address range on a page on swap in/out</span>
<span class="quote">&gt; 	- Added code to copy ADI tags on COW</span>
<span class="quote">&gt; 	- Updated values for auxiliary vectors to not conflict with</span>
<span class="quote">&gt; 	  values on other architectures to avoid conflict in glibc. glibc</span>
<span class="quote">&gt; 	  consolidates all auxiliary vectors into its headers and</span>
<span class="quote">&gt; 	  duplicate values in consolidated header are problematic</span>
<span class="quote">&gt; 	- Disable same page merging on ADI enabled pages since ADI tags</span>
<span class="quote">&gt; 	  may not match on pages with identical data</span>
<span class="quote">&gt; 	- Broke the patch up further into smaller patches</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; v6:</span>
<span class="quote">&gt; 	- Eliminated instructions to read and write PSTATE as well as</span>
<span class="quote">&gt; 	  MCDPER and PMCDPER on every access to userspace addresses</span>
<span class="quote">&gt; 	  by setting PSTATE and PMCDPER correctly upon entry into</span>
<span class="quote">&gt; 	  kernel. PSTATE.mcde and PMCDPER are set upon entry into</span>
<span class="quote">&gt; 	  kernel when running on an M7 processor. PSTATE.mcde being</span>
<span class="quote">&gt; 	  set only affects memory accesses that have TTE.mcd set.</span>
<span class="quote">&gt; 	  PMCDPER being set only affects writes to memory addresses</span>
<span class="quote">&gt; 	  that have TTE.mcd set. This ensures any faults caused by</span>
<span class="quote">&gt; 	  ADI tag mismatch on a write are exposed before kernel returns</span>
<span class="quote">&gt; 	  to userspace.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; v5:</span>
<span class="quote">&gt; 	- Fixed indentation issues and instrcuctions in assembly code</span>
<span class="quote">&gt; 	- Removed CONFIG_SPARC64 from mdesc.c</span>
<span class="quote">&gt; 	- Changed to maintain state of MCDPER register in thread info</span>
<span class="quote">&gt; 	  flags as opposed to in mm context. MCDPER is a per-thread</span>
<span class="quote">&gt; 	  state and belongs in thread info flag as opposed to mm context</span>
<span class="quote">&gt; 	  which is shared across threads. Added comments to clarify this</span>
<span class="quote">&gt; 	  is a lazily maintained state and must be updated on context</span>
<span class="quote">&gt; 	  switch and copy_process()</span>
<span class="quote">&gt; 	- Updated code to use the new arch_do_swap_page() and</span>
<span class="quote">&gt; 	  arch_unmap_one() functions</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; v4:</span>
<span class="quote">&gt; 	- Broke patch up into smaller patches</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; v3:</span>
<span class="quote">&gt; 	- Removed CONFIG_SPARC_ADI</span>
<span class="quote">&gt; 	- Replaced prctl commands with mprotect</span>
<span class="quote">&gt; 	- Added auxiliary vectors for ADI parameters</span>
<span class="quote">&gt; 	- Enabled ADI for swappable pages</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; v2:</span>
<span class="quote">&gt; 	- Fixed a build error</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Documentation/sparc/adi.txt             | 272 +++++++++++++++++++++++++++++++</span>
<span class="quote">&gt; arch/sparc/include/asm/mman.h           |  72 ++++++++-</span>
<span class="quote">&gt; arch/sparc/include/asm/mmu_64.h         |  17 ++</span>
<span class="quote">&gt; arch/sparc/include/asm/mmu_context_64.h |  43 +++++</span>
<span class="quote">&gt; arch/sparc/include/asm/page_64.h        |   4 +</span>
<span class="quote">&gt; arch/sparc/include/asm/pgtable_64.h     |  46 ++++++</span>
<span class="quote">&gt; arch/sparc/include/asm/thread_info_64.h |   2 +-</span>
<span class="quote">&gt; arch/sparc/include/asm/trap_block.h     |   2 +</span>
<span class="quote">&gt; arch/sparc/include/uapi/asm/mman.h      |   2 +</span>
<span class="quote">&gt; arch/sparc/kernel/adi_64.c              | 277 ++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt; arch/sparc/kernel/etrap_64.S            |  28 +++-</span>
<span class="quote">&gt; arch/sparc/kernel/process_64.c          |  25 +++</span>
<span class="quote">&gt; arch/sparc/kernel/setup_64.c            |  11 +-</span>
<span class="quote">&gt; arch/sparc/kernel/vmlinux.lds.S         |   5 +</span>
<span class="quote">&gt; arch/sparc/mm/gup.c                     |  37 +++++</span>
<span class="quote">&gt; arch/sparc/mm/hugetlbpage.c             |  14 +-</span>
<span class="quote">&gt; arch/sparc/mm/init_64.c                 |  33 ++++</span>
<span class="quote">&gt; arch/sparc/mm/tsb.c                     |  21 +++</span>
<span class="quote">&gt; include/linux/mm.h                      |   3 +</span>
<span class="quote">&gt; mm/ksm.c                                |   4 +</span>
<span class="quote">&gt; 20 files changed, 913 insertions(+), 5 deletions(-)</span>
<span class="quote">&gt; create mode 100644 Documentation/sparc/adi.txt</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/Documentation/sparc/adi.txt b/Documentation/sparc/adi.txt</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..383bc65fec1e</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/Documentation/sparc/adi.txt</span>
<span class="quote">&gt; @@ -0,0 +1,272 @@</span>
<span class="quote">&gt; +Application Data Integrity (ADI)</span>
<span class="quote">&gt; +================================</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +SPARC M7 processor adds the Application Data Integrity (ADI) feature.</span>
<span class="quote">&gt; +ADI allows a task to set version tags on any subset of its address</span>
<span class="quote">&gt; +space. Once ADI is enabled and version tags are set for ranges of</span>
<span class="quote">&gt; +address space of a task, the processor will compare the tag in pointers</span>
<span class="quote">&gt; +to memory in these ranges to the version set by the application</span>
<span class="quote">&gt; +previously. Access to memory is granted only if the tag in given pointer</span>
<span class="quote">&gt; +matches the tag set by the application. In case of mismatch, processor</span>
<span class="quote">&gt; +raises an exception.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +Following steps must be taken by a task to enable ADI fully:</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +1. Set the user mode PSTATE.mcde bit. This acts as master switch for</span>
<span class="quote">&gt; +   the task&#39;s entire address space to enable/disable ADI for the task.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +2. Set TTE.mcd bit on any TLB entries that correspond to the range of</span>
<span class="quote">&gt; +   addresses ADI is being enabled on. MMU checks the version tag only</span>
<span class="quote">&gt; +   on the pages that have TTE.mcd bit set.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +3. Set the version tag for virtual addresses using stxa instruction</span>
<span class="quote">&gt; +   and one of the MCD specific ASIs. Each stxa instruction sets the</span>
<span class="quote">&gt; +   given tag for one ADI block size number of bytes. This step must</span>
<span class="quote">&gt; +   be repeated for entire page to set tags for entire page.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +ADI block size for the platform is provided by the hypervisor to kernel</span>
<span class="quote">&gt; +in machine description tables. Hypervisor also provides the number of</span>
<span class="quote">&gt; +top bits in the virtual address that specify the version tag.  Once</span>
<span class="quote">&gt; +version tag has been set for a memory location, the tag is stored in the</span>
<span class="quote">&gt; +physical memory and the same tag must be present in the ADI version tag</span>
<span class="quote">&gt; +bits of the virtual address being presented to the MMU. For example on</span>
<span class="quote">&gt; +SPARC M7 processor, MMU uses bits 63-60 for version tags and ADI block</span>
<span class="quote">&gt; +size is same as cacheline size which is 64 bytes. A task that sets ADI</span>
<span class="quote">&gt; +version to, say 10, on a range of memory, must access that memory using</span>
<span class="quote">&gt; +virtual addresses that contain 0xa in bits 63-60.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +ADI is enabled on a set of pages using mprotect() with PROT_ADI flag.</span>
<span class="quote">&gt; +When ADI is enabled on a set of pages by a task for the first time,</span>
<span class="quote">&gt; +kernel sets the PSTATE.mcde bit fot the task. Version tags for memory</span>
<span class="quote">&gt; +addresses are set with an stxa instruction on the addresses using</span>
<span class="quote">&gt; +ASI_MCD_PRIMARY or ASI_MCD_ST_BLKINIT_PRIMARY. ADI block size is</span>
<span class="quote">&gt; +provided by the hypervisor to the kernel.  Kernel returns the value of</span>
<span class="quote">&gt; +ADI block size to userspace using auxiliary vector along with other ADI</span>
<span class="quote">&gt; +info. Following auxiliary vectors are provided by the kernel:</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	AT_ADI_BLKSZ	ADI block size. This is the granularity and</span>
<span class="quote">&gt; +			alignment, in bytes, of ADI versioning.</span>
<span class="quote">&gt; +	AT_ADI_NBITS	Number of ADI version bits in the VA</span>

The previous patch series also defined AT_ADI_UEONADI.  Why was that
removed?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +IMPORTANT NOTES:</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +- Version tag values of 0x0 and 0xf are reserved.</span>

The documentation should probably state more specifically that an
in-memory tag value of 0x0 or 0xf is treated as &quot;match all&quot; by the HW
meaning that a mismatch exception will never be generated regardless
of the tag bits set in the VA accessing the memory.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +- Version tags are set on virtual addresses from userspace even though</span>
<span class="quote">&gt; +  tags are stored in physical memory. Tags are set on a physical page</span>
<span class="quote">&gt; +  after it has been allocated to a task and a pte has been created for</span>
<span class="quote">&gt; +  it.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +- When a task frees a memory page it had set version tags on, the page</span>
<span class="quote">&gt; +  goes back to free page pool. When this page is re-allocated to a task,</span>
<span class="quote">&gt; +  kernel clears the page using block initialization ASI which clears the</span>
<span class="quote">&gt; +  version tags as well for the page. If a page allocated to a task is</span>
<span class="quote">&gt; +  freed and allocated back to the same task, old version tags set by the</span>
<span class="quote">&gt; +  task on that page will no longer be present.</span>

The specifics should be included here, too, so someone doesn&#39;t have
to guess what&#39;s going on if they make changes and the tags are no longer
cleared.  The HW clears the tag for a cacheline for block initializing
stores to 64-byte aligned addresses if PSTATE.mcde=0 or TTE.mcd=0.
PSTATE.mce is set when executing in the kernel, but pages are cleared
using kernel physical mapping VAs which are mapped with TTE.mcd=0.

Another HW behavior that should be mentioned is that tag mismatches
are not detected for non-faulting loads.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +- Kernel does not set any tags for user pages and it is entirely a</span>
<span class="quote">&gt; +  task&#39;s responsibility to set any version tags. Kernel does ensure the</span>
<span class="quote">&gt; +  version tags are preserved if a page is swapped out to the disk and</span>
<span class="quote">&gt; +  swapped back in. It also preserves that version tags if a page is</span>
<span class="quote">&gt; +  migrated.</span>

I only have a cursory understanding of how page migration works, but
I could not see how the tags would be preserved if a page were migrated.
I figured the place to copy the tags would be migrate_page_copy(), but
I don&#39;t see changes there.
<span class="quote">

&gt; +</span>
<span class="quote">&gt; +- ADI works for any size pages. A userspace task need not be aware of</span>
<span class="quote">&gt; +  page size when using ADI. It can simply select a virtual address</span>
<span class="quote">&gt; +  range, enable ADI on the range using mprotect() and set version tags</span>
<span class="quote">&gt; +  for the entire range. mprotect() ensures range is aligned to page size</span>
<span class="quote">&gt; +  and is a multiple of page size.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +ADI related traps</span>
<span class="quote">&gt; +-----------------</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +With ADI enabled, following new traps may occur:</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +Disrupting memory corruption</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	When a store accesses a memory localtion that has TTE.mcd=1,</span>
<span class="quote">&gt; +	the task is running with ADI enabled (PSTATE.mcde=1), and the ADI</span>
<span class="quote">&gt; +	tag in the address used (bits 63:60) does not match the tag set on</span>
<span class="quote">&gt; +	the corresponding cacheline, a memory corruption trap occurs. By</span>
<span class="quote">&gt; +	default, it is a disrupting trap and is sent to the hypervisor</span>
<span class="quote">&gt; +	first. Hypervisor creates a sun4v error report and sends a</span>
<span class="quote">&gt; +	resumable error (TT=0x7e) trap to the kernel. The kernel sends</span>
<span class="quote">&gt; +	a SIGSEGV to the task that resulted in this trap with the following</span>
<span class="quote">&gt; +	info:</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		siginfo.si_signo = SIGSEGV;</span>
<span class="quote">&gt; +		siginfo.errno = 0;</span>
<span class="quote">&gt; +		siginfo.si_code = SEGV_ADIDERR;</span>
<span class="quote">&gt; +		siginfo.si_addr = addr; /* PC where first mismatch occurred */</span>
<span class="quote">&gt; +		siginfo.si_trapno = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +Precise memory corruption</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	When a store accesses a memory location that has TTE.mcd=1,</span>
<span class="quote">&gt; +	the task is running with ADI enabled (PSTATE.mcde=1), and the ADI</span>
<span class="quote">&gt; +	tag in the address used (bits 63:60) does not match the tag set on</span>
<span class="quote">&gt; +	the corresponding cacheline, a memory corruption trap occurs. If</span>
<span class="quote">&gt; +	MCD precise exception is enabled (MCDPERR=1), a precise</span>
<span class="quote">&gt; +	exception is sent to the kernel with TT=0x1a. The kernel sends</span>
<span class="quote">&gt; +	a SIGSEGV to the task that resulted in this trap with the following</span>
<span class="quote">&gt; +	info:</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		siginfo.si_signo = SIGSEGV;</span>
<span class="quote">&gt; +		siginfo.errno = 0;</span>
<span class="quote">&gt; +		siginfo.si_code = SEGV_ADIPERR;</span>
<span class="quote">&gt; +		siginfo.si_addr = addr;	/* address that caused trap */</span>
<span class="quote">&gt; +		siginfo.si_trapno = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	NOTE: ADI tag mismatch on a load always results in precise trap.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +MCD disabled</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	When a task has not enabled ADI and attempts to set ADI version</span>
<span class="quote">&gt; +	on a memory address, processor sends an MCD disabled trap. This</span>
<span class="quote">&gt; +	trap is handled by hypervisor first and the hypervisor vectors this</span>
<span class="quote">&gt; +	trap through to the kernel as Data Access Exception trap with</span>
<span class="quote">&gt; +	fault type set to 0xa (invalid ASI). When this occurs, the kernel</span>
<span class="quote">&gt; +	sends the task SIGSEGV signal with following info:</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		siginfo.si_signo = SIGSEGV;</span>
<span class="quote">&gt; +		siginfo.errno = 0;</span>
<span class="quote">&gt; +		siginfo.si_code = SEGV_ACCADI;</span>
<span class="quote">&gt; +		siginfo.si_addr = addr;	/* address that caused trap */</span>
<span class="quote">&gt; +		siginfo.si_trapno = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +Sample program to use ADI</span>
<span class="quote">&gt; +-------------------------</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +Following sample program is meant to illustrate how to use the ADI</span>
<span class="quote">&gt; +functionality.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;unistd.h&gt;</span>
<span class="quote">&gt; +#include &lt;stdio.h&gt;</span>
<span class="quote">&gt; +#include &lt;stdlib.h&gt;</span>
<span class="quote">&gt; +#include &lt;elf.h&gt;</span>
<span class="quote">&gt; +#include &lt;sys/ipc.h&gt;</span>
<span class="quote">&gt; +#include &lt;sys/shm.h&gt;</span>
<span class="quote">&gt; +#include &lt;sys/mman.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/asi.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef AT_ADI_BLKSZ</span>
<span class="quote">&gt; +#define AT_ADI_BLKSZ	48</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +#ifndef AT_ADI_NBITS</span>
<span class="quote">&gt; +#define AT_ADI_NBITS	49</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef PROT_ADI</span>
<span class="quote">&gt; +#define PROT_ADI	0x10</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define BUFFER_SIZE     32*1024*1024UL</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +main(int argc, char* argv[], char* envp[])</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +        unsigned long i, mcde, adi_blksz, adi_nbits;</span>
<span class="quote">&gt; +        char *shmaddr, *tmp_addr, *end, *veraddr, *clraddr;</span>
<span class="quote">&gt; +        int shmid, version;</span>
<span class="quote">&gt; +	Elf64_auxv_t *auxv;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	adi_blksz = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	while(*envp++ != NULL);</span>
<span class="quote">&gt; +	for (auxv = (Elf64_auxv_t *)envp; auxv-&gt;a_type != AT_NULL; auxv++) {</span>
<span class="quote">&gt; +		switch (auxv-&gt;a_type) {</span>
<span class="quote">&gt; +		case AT_ADI_BLKSZ:</span>
<span class="quote">&gt; +			adi_blksz = auxv-&gt;a_un.a_val;</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		case AT_ADI_NBITS:</span>
<span class="quote">&gt; +			adi_nbits = auxv-&gt;a_un.a_val;</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	if (adi_blksz == 0) {</span>
<span class="quote">&gt; +		fprintf(stderr, &quot;Oops! ADI is not supported\n&quot;);</span>
<span class="quote">&gt; +		exit(1);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	printf(&quot;ADI capabilities:\n&quot;);</span>
<span class="quote">&gt; +	printf(&quot;\tBlock size = %ld\n&quot;, adi_blksz);</span>
<span class="quote">&gt; +	printf(&quot;\tNumber of bits = %ld\n&quot;, adi_nbits);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +        if ((shmid = shmget(2, BUFFER_SIZE,</span>
<span class="quote">&gt; +                                IPC_CREAT | SHM_R | SHM_W)) &lt; 0) {</span>
<span class="quote">&gt; +                perror(&quot;shmget failed&quot;);</span>
<span class="quote">&gt; +                exit(1);</span>
<span class="quote">&gt; +        }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +        shmaddr = shmat(shmid, NULL, 0);</span>
<span class="quote">&gt; +        if (shmaddr == (char *)-1) {</span>
<span class="quote">&gt; +                perror(&quot;shm attach failed&quot;);</span>
<span class="quote">&gt; +                shmctl(shmid, IPC_RMID, NULL);</span>
<span class="quote">&gt; +                exit(1);</span>
<span class="quote">&gt; +        }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (mprotect(shmaddr, BUFFER_SIZE, PROT_READ|PROT_WRITE|PROT_ADI)) {</span>
<span class="quote">&gt; +		perror(&quot;mprotect failed&quot;);</span>
<span class="quote">&gt; +		goto err_out;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +        /* Set the ADI version tag on the shm segment</span>
<span class="quote">&gt; +         */</span>
<span class="quote">&gt; +        version = 10;</span>
<span class="quote">&gt; +        tmp_addr = shmaddr;</span>
<span class="quote">&gt; +        end = shmaddr + BUFFER_SIZE;</span>
<span class="quote">&gt; +        while (tmp_addr &lt; end) {</span>
<span class="quote">&gt; +                asm volatile(</span>
<span class="quote">&gt; +                        &quot;stxa %1, [%0]0x90\n\t&quot;</span>
<span class="quote">&gt; +                        :</span>
<span class="quote">&gt; +                        : &quot;r&quot; (tmp_addr), &quot;r&quot; (version));</span>
<span class="quote">&gt; +                tmp_addr += adi_blksz;</span>
<span class="quote">&gt; +        }</span>
<span class="quote">&gt; +	asm volatile(&quot;membar #Sync\n\t&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +        /* Create a versioned address from the normal address by placing</span>
<span class="quote">&gt; +	 * version tag in the upper adi_nbits bits</span>
<span class="quote">&gt; +         */</span>
<span class="quote">&gt; +        tmp_addr = (void *) ((unsigned long)shmaddr &lt;&lt; adi_nbits);</span>
<span class="quote">&gt; +        tmp_addr = (void *) ((unsigned long)tmp_addr &gt;&gt; adi_nbits);</span>
<span class="quote">&gt; +        veraddr = (void *) (((unsigned long)version &lt;&lt; (64-adi_nbits))</span>
<span class="quote">&gt; +                        | (unsigned long)tmp_addr);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +        printf(&quot;Starting the writes:\n&quot;);</span>
<span class="quote">&gt; +        for (i = 0; i &lt; BUFFER_SIZE; i++) {</span>
<span class="quote">&gt; +                veraddr[i] = (char)(i);</span>
<span class="quote">&gt; +                if (!(i % (1024 * 1024)))</span>
<span class="quote">&gt; +                        printf(&quot;.&quot;);</span>
<span class="quote">&gt; +        }</span>
<span class="quote">&gt; +        printf(&quot;\n&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +        printf(&quot;Verifying data...&quot;);</span>
<span class="quote">&gt; +	fflush(stdout);</span>
<span class="quote">&gt; +        for (i = 0; i &lt; BUFFER_SIZE; i++)</span>
<span class="quote">&gt; +                if (veraddr[i] != (char)i)</span>
<span class="quote">&gt; +                        printf(&quot;\nIndex %lu mismatched\n&quot;, i);</span>
<span class="quote">&gt; +        printf(&quot;Done.\n&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +        /* Disable ADI and clean up</span>
<span class="quote">&gt; +         */</span>
<span class="quote">&gt; +	if (mprotect(shmaddr, BUFFER_SIZE, PROT_READ|PROT_WRITE)) {</span>
<span class="quote">&gt; +		perror(&quot;mprotect failed&quot;);</span>
<span class="quote">&gt; +		goto err_out;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +        if (shmdt((const void *)shmaddr) != 0)</span>
<span class="quote">&gt; +                perror(&quot;Detach failure&quot;);</span>
<span class="quote">&gt; +        shmctl(shmid, IPC_RMID, NULL);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +        exit(0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +err_out:</span>
<span class="quote">&gt; +        if (shmdt((const void *)shmaddr) != 0)</span>
<span class="quote">&gt; +                perror(&quot;Detach failure&quot;);</span>
<span class="quote">&gt; +        shmctl(shmid, IPC_RMID, NULL);</span>
<span class="quote">&gt; +        exit(1);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; diff --git a/arch/sparc/include/asm/mman.h b/arch/sparc/include/asm/mman.h</span>
<span class="quote">&gt; index 59bb5938d852..b799796ad963 100644</span>
<span class="quote">&gt; --- a/arch/sparc/include/asm/mman.h</span>
<span class="quote">&gt; +++ b/arch/sparc/include/asm/mman.h</span>
<span class="quote">&gt; @@ -6,5 +6,75 @@</span>
<span class="quote">&gt; #ifndef __ASSEMBLY__</span>
<span class="quote">&gt; #define arch_mmap_check(addr,len,flags)	sparc_mmap_check(addr,len)</span>
<span class="quote">&gt; int sparc_mmap_check(unsigned long addr, unsigned long len);</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_SPARC64</span>
<span class="quote">&gt; +#include &lt;asm/adi_64.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define arch_calc_vm_prot_bits(prot, pkey) sparc_calc_vm_prot_bits(prot)</span>
<span class="quote">&gt; +static inline unsigned long sparc_calc_vm_prot_bits(unsigned long prot)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (prot &amp; PROT_ADI) {</span>
<span class="quote">&gt; +		struct pt_regs *regs;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!current-&gt;mm-&gt;context.adi) {</span>
<span class="quote">&gt; +			regs = task_pt_regs(current);</span>
<span class="quote">&gt; +			regs-&gt;tstate |= TSTATE_MCDE;</span>
<span class="quote">&gt; +			current-&gt;mm-&gt;context.adi = true;</span>

If a process is multi-threaded when it enables ADI on some memory for
the first time, TSTATE_MCDE will only be set for the calling thread
and it will not be possible to enable it for the other threads.
One possible way to handle this is to enable TSTATE_MCDE for all user
threads when they are initialized if adi_capable() returns true.
<span class="quote">

&gt; +		}</span>
<span class="quote">&gt; +		return VM_SPARC_ADI;</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define arch_vm_get_page_prot(vm_flags) sparc_vm_get_page_prot(vm_flags)</span>
<span class="quote">&gt; +static inline pgprot_t sparc_vm_get_page_prot(unsigned long vm_flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (vm_flags &amp; VM_SPARC_ADI) ? __pgprot(_PAGE_MCD_4V) : __pgprot(0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define arch_validate_prot(prot, addr) sparc_validate_prot(prot, addr)</span>
<span class="quote">&gt; +static inline int sparc_validate_prot(unsigned long prot, unsigned long addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (prot &amp; ~(PROT_READ | PROT_WRITE | PROT_EXEC | PROT_SEM | PROT_ADI))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +	if (prot &amp; PROT_ADI) {</span>
<span class="quote">&gt; +		if (!adi_capable())</span>
<span class="quote">&gt; +			return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* ADI tags can not be set on read-only memory, so it makes</span>
<span class="quote">&gt; +		 * sense to enable ADI on writable memory only.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (!(prot &amp; PROT_WRITE))</span>
<span class="quote">&gt; +			return 0;</span>

This prevents the use of ADI for the legitimate case where shared memory
is mapped read/write for a master process but mapped read-only for a
client process.  The master process could set the tags and communicate
the expected tag values to the client.
<span class="quote">

&gt; +</span>
<span class="quote">&gt; +		if (addr) {</span>
<span class="quote">&gt; +			struct vm_area_struct *vma;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			vma = find_vma(current-&gt;mm, addr);</span>
<span class="quote">&gt; +			if (vma) {</span>
<span class="quote">&gt; +				/* ADI can not be enabled on PFN</span>
<span class="quote">&gt; +				 * mapped pages</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +				if (vma-&gt;vm_flags &amp; (VM_PFNMAP | VM_MIXEDMAP))</span>
<span class="quote">&gt; +					return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				/* Mergeable pages can become unmergeable</span>
<span class="quote">&gt; +				 * if ADI is enabled on them even if they</span>
<span class="quote">&gt; +				 * have identical data on them. This can be</span>
<span class="quote">&gt; +				 * because ADI enabled pages with identical</span>
<span class="quote">&gt; +				 * data may still not have identical ADI</span>
<span class="quote">&gt; +				 * tags on them. Disallow ADI on mergeable</span>
<span class="quote">&gt; +				 * pages.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +				if (vma-&gt;vm_flags &amp; VM_MERGEABLE)</span>
<span class="quote">&gt; +					return 0;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	return 1;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif /* CONFIG_SPARC64 */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* __ASSEMBLY__ */</span>
<span class="quote">&gt; #endif /* __SPARC_MMAN_H__ */</span>
<span class="quote">&gt; diff --git a/arch/sparc/include/asm/mmu_64.h b/arch/sparc/include/asm/mmu_64.h</span>
<span class="quote">&gt; index 83b36a5371ff..a65d51ebe00b 100644</span>
<span class="quote">&gt; --- a/arch/sparc/include/asm/mmu_64.h</span>
<span class="quote">&gt; +++ b/arch/sparc/include/asm/mmu_64.h</span>
<span class="quote">&gt; @@ -89,6 +89,20 @@ struct tsb_config {</span>
<span class="quote">&gt; #define MM_NUM_TSBS	1</span>
<span class="quote">&gt; #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +/* ADI tags are stored when a page is swapped out and the storage for</span>
<span class="quote">&gt; + * tags is allocated dynamically. There is a tag storage descriptor</span>
<span class="quote">&gt; + * associated with each set of tag storage pages. Tag storage descriptors</span>
<span class="quote">&gt; + * are allocated dynamically. Since kernel will allocate a full page for</span>
<span class="quote">&gt; + * each tag storage descriptor, we can store up to</span>
<span class="quote">&gt; + * PAGE_SIZE/sizeof(tag storage descriptor) descriptors on that page.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +typedef struct {</span>
<span class="quote">&gt; +	unsigned long	start;		/* Start address for this tag storage */</span>
<span class="quote">&gt; +	unsigned long	end;		/* Last address for tag storage */</span>
<span class="quote">&gt; +	unsigned char	*tags;		/* Where the tags are */</span>
<span class="quote">&gt; +	unsigned long	tag_users;	/* number of references to descriptor */</span>
<span class="quote">&gt; +} tag_storage_desc_t;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; typedef struct {</span>
<span class="quote">&gt; 	spinlock_t		lock;</span>
<span class="quote">&gt; 	unsigned long		sparc64_ctx_val;</span>
<span class="quote">&gt; @@ -96,6 +110,9 @@ typedef struct {</span>
<span class="quote">&gt; 	unsigned long		thp_pte_count;</span>
<span class="quote">&gt; 	struct tsb_config	tsb_block[MM_NUM_TSBS];</span>
<span class="quote">&gt; 	struct hv_tsb_descr	tsb_descr[MM_NUM_TSBS];</span>
<span class="quote">&gt; +	bool			adi;</span>
<span class="quote">&gt; +	tag_storage_desc_t	*tag_store;</span>
<span class="quote">&gt; +	spinlock_t		tag_lock;</span>
<span class="quote">&gt; } mm_context_t;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #endif /* !__ASSEMBLY__ */</span>
<span class="quote">&gt; diff --git a/arch/sparc/include/asm/mmu_context_64.h b/arch/sparc/include/asm/mmu_context_64.h</span>
<span class="quote">&gt; index 2cddcda4f85f..68de059551f9 100644</span>
<span class="quote">&gt; --- a/arch/sparc/include/asm/mmu_context_64.h</span>
<span class="quote">&gt; +++ b/arch/sparc/include/asm/mmu_context_64.h</span>
<span class="quote">&gt; @@ -9,6 +9,7 @@</span>
<span class="quote">&gt; #include &lt;linux/mm_types.h&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #include &lt;asm/spitfire.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/adi_64.h&gt;</span>
<span class="quote">&gt; #include &lt;asm-generic/mm_hooks.h&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)</span>
<span class="quote">&gt; @@ -129,6 +130,48 @@ static inline void switch_mm(struct mm_struct *old_mm, struct mm_struct *mm, str</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #define deactivate_mm(tsk,mm)	do { } while (0)</span>
<span class="quote">&gt; #define activate_mm(active_mm, mm) switch_mm(active_mm, mm, NULL)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define  __HAVE_ARCH_START_CONTEXT_SWITCH</span>
<span class="quote">&gt; +static inline void arch_start_context_switch(struct task_struct *prev)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/* Save the current state of MCDPER register for the process</span>
<span class="quote">&gt; +	 * we are switching from</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (adi_capable()) {</span>
<span class="quote">&gt; +		register unsigned long tmp_mcdper;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		__asm__ __volatile__(</span>
<span class="quote">&gt; +			&quot;.word 0x83438000\n\t&quot;	/* rd  %mcdper, %g1 */</span>
<span class="quote">&gt; +			&quot;mov %%g1, %0\n\t&quot;</span>
<span class="quote">&gt; +			: &quot;=r&quot; (tmp_mcdper)</span>
<span class="quote">&gt; +			:</span>
<span class="quote">&gt; +			: &quot;g1&quot;);</span>
<span class="quote">&gt; +		if (tmp_mcdper)</span>
<span class="quote">&gt; +			set_tsk_thread_flag(prev, TIF_MCDPER);</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +			clear_tsk_thread_flag(prev, TIF_MCDPER);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define finish_arch_post_lock_switch	finish_arch_post_lock_switch</span>
<span class="quote">&gt; +static inline void finish_arch_post_lock_switch(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/* Restore the state of MCDPER register for the new process</span>
<span class="quote">&gt; +	 * just switched to.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (adi_capable()) {</span>
<span class="quote">&gt; +		register unsigned long tmp_mcdper;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		tmp_mcdper = test_thread_flag(TIF_MCDPER);</span>
<span class="quote">&gt; +		__asm__ __volatile__(</span>
<span class="quote">&gt; +			&quot;mov %0, %%g1\n\t&quot;</span>
<span class="quote">&gt; +			&quot;.word 0x9d800001\n\t&quot;	/* wr %g0, %g1, %mcdper&quot; */</span>
<span class="quote">&gt; +			:</span>
<span class="quote">&gt; +			: &quot;ir&quot; (tmp_mcdper)</span>
<span class="quote">&gt; +			: &quot;g1&quot;);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; #endif /* !(__ASSEMBLY__) */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #endif /* !(__SPARC64_MMU_CONTEXT_H) */</span>
<span class="quote">&gt; diff --git a/arch/sparc/include/asm/page_64.h b/arch/sparc/include/asm/page_64.h</span>
<span class="quote">&gt; index 5961b2d8398a..dc582c5611f8 100644</span>
<span class="quote">&gt; --- a/arch/sparc/include/asm/page_64.h</span>
<span class="quote">&gt; +++ b/arch/sparc/include/asm/page_64.h</span>
<span class="quote">&gt; @@ -46,6 +46,10 @@ struct page;</span>
<span class="quote">&gt; void clear_user_page(void *addr, unsigned long vaddr, struct page *page);</span>
<span class="quote">&gt; #define copy_page(X,Y)	memcpy((void *)(X), (void *)(Y), PAGE_SIZE)</span>
<span class="quote">&gt; void copy_user_page(void *to, void *from, unsigned long vaddr, struct page *topage);</span>
<span class="quote">&gt; +#define __HAVE_ARCH_COPY_USER_HIGHPAGE</span>
<span class="quote">&gt; +struct vm_area_struct;</span>
<span class="quote">&gt; +void copy_user_highpage(struct page *to, struct page *from,</span>
<span class="quote">&gt; +			unsigned long vaddr, struct vm_area_struct *vma);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; /* Unlike sparc32, sparc64&#39;s parameter passing API is more</span>
<span class="quote">&gt;  * sane in that structures which as small enough are passed</span>
<span class="quote">&gt; diff --git a/arch/sparc/include/asm/pgtable_64.h b/arch/sparc/include/asm/pgtable_64.h</span>
<span class="quote">&gt; index af045061f41e..51da342c392d 100644</span>
<span class="quote">&gt; --- a/arch/sparc/include/asm/pgtable_64.h</span>
<span class="quote">&gt; +++ b/arch/sparc/include/asm/pgtable_64.h</span>
<span class="quote">&gt; @@ -18,6 +18,7 @@</span>
<span class="quote">&gt; #include &lt;asm/types.h&gt;</span>
<span class="quote">&gt; #include &lt;asm/spitfire.h&gt;</span>
<span class="quote">&gt; #include &lt;asm/asi.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/adi.h&gt;</span>
<span class="quote">&gt; #include &lt;asm/page.h&gt;</span>
<span class="quote">&gt; #include &lt;asm/processor.h&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -570,6 +571,18 @@ static inline pte_t pte_mkspecial(pte_t pte)</span>
<span class="quote">&gt; 	return pte;</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +static inline pte_t pte_mkmcd(pte_t pte)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pte_val(pte) |= _PAGE_MCD_4V;</span>
<span class="quote">&gt; +	return pte;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline pte_t pte_mknotmcd(pte_t pte)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pte_val(pte) &amp;= ~_PAGE_MCD_4V;</span>
<span class="quote">&gt; +	return pte;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; static inline unsigned long pte_young(pte_t pte)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	unsigned long mask;</span>
<span class="quote">&gt; @@ -1001,6 +1014,39 @@ int page_in_phys_avail(unsigned long paddr);</span>
<span class="quote">&gt; int remap_pfn_range(struct vm_area_struct *, unsigned long, unsigned long,</span>
<span class="quote">&gt; 		    unsigned long, pgprot_t);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +void adi_restore_tags(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		      unsigned long addr, pte_t pte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +int adi_save_tags(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		  unsigned long addr, pte_t oldpte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __HAVE_ARCH_DO_SWAP_PAGE</span>
<span class="quote">&gt; +static inline void arch_do_swap_page(struct mm_struct *mm,</span>
<span class="quote">&gt; +				     struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				     unsigned long addr,</span>
<span class="quote">&gt; +				     pte_t pte, pte_t oldpte)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/* If this is a new page being mapped in, there can be no</span>
<span class="quote">&gt; +	 * ADI tags stored away for this page. Skip looking for</span>
<span class="quote">&gt; +	 * stored tags</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (pte_none(oldpte))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (adi_state.enabled &amp;&amp; (pte_val(pte) &amp; _PAGE_MCD_4V))</span>
<span class="quote">&gt; +		adi_restore_tags(mm, vma, addr, pte);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __HAVE_ARCH_UNMAP_ONE</span>
<span class="quote">&gt; +static inline int arch_unmap_one(struct mm_struct *mm,</span>
<span class="quote">&gt; +				 struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				 unsigned long addr, pte_t oldpte)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (adi_state.enabled &amp;&amp; (pte_val(oldpte) &amp; _PAGE_MCD_4V))</span>
<span class="quote">&gt; +		return adi_save_tags(mm, vma, addr, oldpte);</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; static inline int io_remap_pfn_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt; 				     unsigned long from, unsigned long pfn,</span>
<span class="quote">&gt; 				     unsigned long size, pgprot_t prot)</span>
<span class="quote">&gt; diff --git a/arch/sparc/include/asm/thread_info_64.h b/arch/sparc/include/asm/thread_info_64.h</span>
<span class="quote">&gt; index 38a24f257b85..9c04acb1f9af 100644</span>
<span class="quote">&gt; --- a/arch/sparc/include/asm/thread_info_64.h</span>
<span class="quote">&gt; +++ b/arch/sparc/include/asm/thread_info_64.h</span>
<span class="quote">&gt; @@ -190,7 +190,7 @@ register struct thread_info *current_thread_info_reg asm(&quot;g6&quot;);</span>
<span class="quote">&gt;  *       in using in assembly, else we can&#39;t use the mask as</span>
<span class="quote">&gt;  *       an immediate value in instructions such as andcc.</span>
<span class="quote">&gt;  */</span>
<span class="quote">&gt; -/* flag bit 12 is available */</span>
<span class="quote">&gt; +#define TIF_MCDPER		12	/* Precise MCD exception */</span>
<span class="quote">&gt; #define TIF_MEMDIE		13	/* is terminating due to OOM killer */</span>
<span class="quote">&gt; #define TIF_POLLING_NRFLAG	14</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/sparc/include/asm/trap_block.h b/arch/sparc/include/asm/trap_block.h</span>
<span class="quote">&gt; index ec9c04de3664..b283e940671a 100644</span>
<span class="quote">&gt; --- a/arch/sparc/include/asm/trap_block.h</span>
<span class="quote">&gt; +++ b/arch/sparc/include/asm/trap_block.h</span>
<span class="quote">&gt; @@ -72,6 +72,8 @@ struct sun4v_1insn_patch_entry {</span>
<span class="quote">&gt; };</span>
<span class="quote">&gt; extern struct sun4v_1insn_patch_entry __sun4v_1insn_patch,</span>
<span class="quote">&gt; 	__sun4v_1insn_patch_end;</span>
<span class="quote">&gt; +extern struct sun4v_1insn_patch_entry __sun_m7_1insn_patch,</span>
<span class="quote">&gt; +	__sun_m7_1insn_patch_end;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; struct sun4v_2insn_patch_entry {</span>
<span class="quote">&gt; 	unsigned int	addr;</span>
<span class="quote">&gt; diff --git a/arch/sparc/include/uapi/asm/mman.h b/arch/sparc/include/uapi/asm/mman.h</span>
<span class="quote">&gt; index 9765896ecb2c..a72c03397345 100644</span>
<span class="quote">&gt; --- a/arch/sparc/include/uapi/asm/mman.h</span>
<span class="quote">&gt; +++ b/arch/sparc/include/uapi/asm/mman.h</span>
<span class="quote">&gt; @@ -5,6 +5,8 @@</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; /* SunOS&#39;ified... */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +#define PROT_ADI	0x10		/* ADI enabled */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; #define MAP_RENAME      MAP_ANONYMOUS   /* In SunOS terminology */</span>
<span class="quote">&gt; #define MAP_NORESERVE   0x40            /* don&#39;t reserve swap pages */</span>
<span class="quote">&gt; #define MAP_INHERIT     0x80            /* SunOS doesn&#39;t do this, but... */</span>
<span class="quote">&gt; diff --git a/arch/sparc/kernel/adi_64.c b/arch/sparc/kernel/adi_64.c</span>
<span class="quote">&gt; index 9fbb5dd4a7bf..83c1e36ae5fa 100644</span>
<span class="quote">&gt; --- a/arch/sparc/kernel/adi_64.c</span>
<span class="quote">&gt; +++ b/arch/sparc/kernel/adi_64.c</span>
<span class="quote">&gt; @@ -7,10 +7,24 @@</span>
<span class="quote">&gt;  * This work is licensed under the terms of the GNU GPL, version 2.</span>
<span class="quote">&gt;  */</span>
<span class="quote">&gt; #include &lt;linux/init.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/mm_types.h&gt;</span>
<span class="quote">&gt; #include &lt;asm/mdesc.h&gt;</span>
<span class="quote">&gt; #include &lt;asm/adi_64.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mmu_64.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/pgtable_64.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Each page of storage for ADI tags can accommodate tags for 128</span>
<span class="quote">&gt; + * pages. When ADI enabled pages are being swapped out, it would be</span>
<span class="quote">&gt; + * prudent to allocate at least enough tag storage space to accommodate</span>
<span class="quote">&gt; + * SWAPFILE_CLUSTER number of pages. Allocate enough tag storage to</span>
<span class="quote">&gt; + * store tags for four SWAPFILE_CLUSTER pages to reduce need for</span>
<span class="quote">&gt; + * further allocations for same vma.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define TAG_STORAGE_PAGES	8</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; struct adi_config adi_state;</span>
<span class="quote">&gt; +EXPORT_SYMBOL(adi_state);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; /* mdesc_adi_init() : Parse machine description provided by the</span>
<span class="quote">&gt;  *	hypervisor to detect ADI capabilities</span>
<span class="quote">&gt; @@ -78,6 +92,19 @@ void __init mdesc_adi_init(void)</span>
<span class="quote">&gt; 		goto adi_not_found;</span>
<span class="quote">&gt; 	adi_state.caps.nbits = *val;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +	/* Some of the code to support swapping ADI tags is written</span>
<span class="quote">&gt; +	 * assumption that two ADI tags can fit inside one byte. If</span>
<span class="quote">&gt; +	 * this assumption is broken by a future architecture change,</span>
<span class="quote">&gt; +	 * that code will have to be revisited. If that were to happen,</span>
<span class="quote">&gt; +	 * disable ADI support so we do not get unpredictable results</span>
<span class="quote">&gt; +	 * with programs trying to use ADI and their pages getting</span>
<span class="quote">&gt; +	 * swapped out</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (adi_state.caps.nbits &gt; 4) {</span>
<span class="quote">&gt; +		pr_warn(&quot;WARNING: ADI tag size &gt;4 on this platform. Disabling AADI support\n&quot;);</span>
<span class="quote">&gt; +		adi_state.enabled = false;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; 	mdesc_release(hp);</span>
<span class="quote">&gt; 	return;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -88,3 +115,253 @@ void __init mdesc_adi_init(void)</span>
<span class="quote">&gt; 	if (hp)</span>
<span class="quote">&gt; 		mdesc_release(hp);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +tag_storage_desc_t *find_tag_store(struct mm_struct *mm,</span>
<span class="quote">&gt; +				   struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				   unsigned long addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	tag_storage_desc_t *tag_desc = NULL;</span>
<span class="quote">&gt; +	unsigned long i, max_desc, flags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Check if this vma already has tag storage descriptor</span>
<span class="quote">&gt; +	 * allocated for it.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	max_desc = PAGE_SIZE/sizeof(tag_storage_desc_t);</span>
<span class="quote">&gt; +	if (mm-&gt;context.tag_store) {</span>
<span class="quote">&gt; +		tag_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt; +		spin_lock_irqsave(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="quote">&gt; +		for (i = 0; i &lt; max_desc; i++) {</span>
<span class="quote">&gt; +			if ((addr &gt;= tag_desc-&gt;start) &amp;&amp;</span>
<span class="quote">&gt; +			    ((addr + PAGE_SIZE - 1) &lt;= tag_desc-&gt;end))</span>
<span class="quote">&gt; +				break;</span>
<span class="quote">&gt; +			tag_desc++;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		spin_unlock_irqrestore(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* If no matching entries were found, this must be a</span>
<span class="quote">&gt; +		 * freshly allocated page</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (i &gt;= max_desc)</span>
<span class="quote">&gt; +			tag_desc = NULL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return tag_desc;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +tag_storage_desc_t *alloc_tag_store(struct mm_struct *mm,</span>
<span class="quote">&gt; +				    struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				    unsigned long addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned char *tags;</span>
<span class="quote">&gt; +	unsigned long i, size, max_desc, flags;</span>
<span class="quote">&gt; +	tag_storage_desc_t *tag_desc, *open_desc;</span>
<span class="quote">&gt; +	unsigned long end_addr, hole_start, hole_end;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	max_desc = PAGE_SIZE/sizeof(tag_storage_desc_t);</span>
<span class="quote">&gt; +	open_desc = NULL;</span>
<span class="quote">&gt; +	hole_start = 0;</span>
<span class="quote">&gt; +	hole_end = ULONG_MAX;</span>
<span class="quote">&gt; +	end_addr = addr + PAGE_SIZE - 1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Check if this vma already has tag storage descriptor</span>
<span class="quote">&gt; +	 * allocated for it.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="quote">&gt; +	if (mm-&gt;context.tag_store) {</span>
<span class="quote">&gt; +		tag_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* Look for a matching entry for this address. While doing</span>
<span class="quote">&gt; +		 * that, look for the first open slot as well and find</span>
<span class="quote">&gt; +		 * the hole in already allocated range where this request</span>
<span class="quote">&gt; +		 * will fit in.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		for (i = 0; i &lt; max_desc; i++) {</span>
<span class="quote">&gt; +			if (tag_desc-&gt;tag_users == 0) {</span>
<span class="quote">&gt; +				if (open_desc == NULL)</span>
<span class="quote">&gt; +					open_desc = tag_desc;</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				if ((addr &gt;= tag_desc-&gt;start) &amp;&amp;</span>
<span class="quote">&gt; +				    (tag_desc-&gt;end &gt;= (addr + PAGE_SIZE - 1))) {</span>
<span class="quote">&gt; +					tag_desc-&gt;tag_users++;</span>
<span class="quote">&gt; +					goto out;</span>
<span class="quote">&gt; +				}</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			if ((tag_desc-&gt;start &gt; end_addr) &amp;&amp;</span>
<span class="quote">&gt; +			    (tag_desc-&gt;start &lt; hole_end))</span>
<span class="quote">&gt; +				hole_end = tag_desc-&gt;start;</span>
<span class="quote">&gt; +			if ((tag_desc-&gt;end &lt; addr) &amp;&amp;</span>
<span class="quote">&gt; +			    (tag_desc-&gt;end &gt; hole_start))</span>
<span class="quote">&gt; +				hole_start = tag_desc-&gt;end;</span>
<span class="quote">&gt; +			tag_desc++;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		size = sizeof(tag_storage_desc_t)*max_desc;</span>
<span class="quote">&gt; +		mm-&gt;context.tag_store = kzalloc(size, GFP_NOIO|__GFP_NOWARN);</span>

The spin_lock_irqsave() above means that all but level 15 interrupts
will be disabled when kzalloc() is called.  If kzalloc() can sleep
there&#39;s a risk of deadlock.
<span class="quote">

&gt; +		if (mm-&gt;context.tag_store == NULL) {</span>
<span class="quote">&gt; +			tag_desc = NULL;</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		tag_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt; +		for (i = 0; i &lt; max_desc; i++, tag_desc++)</span>
<span class="quote">&gt; +			tag_desc-&gt;tag_users = 0;</span>
<span class="quote">&gt; +		open_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt; +		i = 0;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Check if we ran out of tag storage descriptors */</span>
<span class="quote">&gt; +	if (open_desc == NULL) {</span>
<span class="quote">&gt; +		tag_desc = NULL;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Mark this tag descriptor slot in use and then initialize it */</span>
<span class="quote">&gt; +	tag_desc = open_desc;</span>
<span class="quote">&gt; +	tag_desc-&gt;tag_users = 1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Tag storage has not been allocated for this vma and space</span>
<span class="quote">&gt; +	 * is available in tag storage descriptor. Since this page is</span>
<span class="quote">&gt; +	 * being swapped out, there is high probability subsequent pages</span>
<span class="quote">&gt; +	 * in the VMA will be swapped out as well. Allocates pages to</span>
<span class="quote">&gt; +	 * store tags for as many pages in this vma as possible but not</span>
<span class="quote">&gt; +	 * more than TAG_STORAGE_PAGES. Each byte in tag space holds</span>
<span class="quote">&gt; +	 * two ADI tags since each ADI tag is 4 bits. Each ADI tag</span>
<span class="quote">&gt; +	 * covers adi_blksize() worth of addresses. Check if the hole is</span>
<span class="quote">&gt; +	 * big enough to accommodate full address range for using</span>
<span class="quote">&gt; +	 * TAG_STORAGE_PAGES number of tag pages.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	size = TAG_STORAGE_PAGES * PAGE_SIZE;</span>
<span class="quote">&gt; +	end_addr = addr + (size*2*adi_blksize()) - 1;</span>

Since size &gt; PAGE_SIZE, end_addr could theoretically overflow.
<span class="quote">

&gt; +	if (hole_end &lt; end_addr) {</span>
<span class="quote">&gt; +		/* Available hole is too small on the upper end of</span>
<span class="quote">&gt; +		 * address. Can we expand the range towards the lower</span>
<span class="quote">&gt; +		 * address and maximize use of this slot?</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		unsigned long tmp_addr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		end_addr = hole_end - 1;</span>
<span class="quote">&gt; +		tmp_addr = end_addr - (size*2*adi_blksize()) + 1;</span>

Similarily, tmp_addr may underflow.
<span class="quote">
&gt; +		if (tmp_addr &lt; hole_start) {</span>
<span class="quote">&gt; +			/* Available hole is restricted on lower address</span>
<span class="quote">&gt; +			 * end as well</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			tmp_addr = hole_start + 1;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		addr = tmp_addr;</span>
<span class="quote">&gt; +		size = (end_addr + 1 - addr)/(2*adi_blksize());</span>
<span class="quote">&gt; +		size = (size + (PAGE_SIZE-adi_blksize()))/PAGE_SIZE;</span>
<span class="quote">&gt; +		size = size * PAGE_SIZE;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	tags = kzalloc(size, GFP_NOIO|__GFP_NOWARN);</span>

Potential deadlock due to PIL=14?
<span class="quote">

&gt; +	if (tags == NULL) {</span>
<span class="quote">&gt; +		tag_desc-&gt;tag_users = 0;</span>
<span class="quote">&gt; +		tag_desc = NULL;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	tag_desc-&gt;start = addr;</span>
<span class="quote">&gt; +	tag_desc-&gt;tags = tags;</span>
<span class="quote">&gt; +	tag_desc-&gt;end = end_addr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="quote">&gt; +	return tag_desc;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void del_tag_store(tag_storage_desc_t *tag_desc, struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +	unsigned char *tags = NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="quote">&gt; +	tag_desc-&gt;tag_users--;</span>
<span class="quote">&gt; +	if (tag_desc-&gt;tag_users == 0) {</span>
<span class="quote">&gt; +		tag_desc-&gt;start = tag_desc-&gt;end = 0;</span>
<span class="quote">&gt; +		/* Do not free up the tag storage space allocated</span>
<span class="quote">&gt; +		 * by the first descriptor. This is persistent</span>
<span class="quote">&gt; +		 * emergency tag storage space for the task.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (tag_desc != mm-&gt;context.tag_store) {</span>
<span class="quote">&gt; +			tags = tag_desc-&gt;tags;</span>
<span class="quote">&gt; +			tag_desc-&gt;tags = NULL;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="quote">&gt; +	kfree(tags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define tag_start(addr, tag_desc)		\</span>
<span class="quote">&gt; +	((tag_desc)-&gt;tags + ((addr - (tag_desc)-&gt;start)/(2*adi_blksize())))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Retrieve any saved ADI tags for the page being swapped back in and</span>
<span class="quote">&gt; + * restore these tags to the newly allocated physical page.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +void adi_restore_tags(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		      unsigned long addr, pte_t pte)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned char *tag;</span>
<span class="quote">&gt; +	tag_storage_desc_t *tag_desc;</span>
<span class="quote">&gt; +	unsigned long paddr, tmp, version1, version2;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Check if the swapped out page has an ADI version</span>
<span class="quote">&gt; +	 * saved. If yes, restore version tag to the newly</span>
<span class="quote">&gt; +	 * allocated page.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	tag_desc = find_tag_store(mm, vma, addr);</span>
<span class="quote">&gt; +	if (tag_desc == NULL)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	tag = tag_start(addr, tag_desc);</span>
<span class="quote">&gt; +	paddr = pte_val(pte) &amp; _PAGE_PADDR_4V;</span>
<span class="quote">&gt; +	for (tmp = paddr; tmp &lt; (paddr+PAGE_SIZE); tmp += adi_blksize()) {</span>
<span class="quote">&gt; +		version1 = (*tag) &gt;&gt; 4;</span>
<span class="quote">&gt; +		version2 = (*tag) &amp; 0x0f;</span>
<span class="quote">&gt; +		*tag++ = 0;</span>
<span class="quote">&gt; +		asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="quote">&gt; +			:</span>
<span class="quote">&gt; +			: &quot;r&quot; (version1), &quot;r&quot; (tmp),</span>
<span class="quote">&gt; +			  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt; +		tmp += adi_blksize();</span>
<span class="quote">&gt; +		asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="quote">&gt; +			:</span>
<span class="quote">&gt; +			: &quot;r&quot; (version2), &quot;r&quot; (tmp),</span>
<span class="quote">&gt; +			  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	asm volatile(&quot;membar #Sync\n\t&quot;);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Check and mark this tag space for release later if</span>
<span class="quote">&gt; +	 * the swapped in page was the last user of tag space</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	del_tag_store(tag_desc, mm);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* A page is about to be swapped out. Save any ADI tags associated with</span>
<span class="quote">&gt; + * this physical page so they can be restored later when the page is swapped</span>
<span class="quote">&gt; + * back in.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +int adi_save_tags(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		  unsigned long addr, pte_t oldpte)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned char *tag;</span>
<span class="quote">&gt; +	tag_storage_desc_t *tag_desc;</span>
<span class="quote">&gt; +	unsigned long version1, version2, paddr, tmp;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	tag_desc = alloc_tag_store(mm, vma, addr);</span>
<span class="quote">&gt; +	if (tag_desc == NULL)</span>
<span class="quote">&gt; +		return -1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	tag = tag_start(addr, tag_desc);</span>
<span class="quote">&gt; +	paddr = pte_val(oldpte) &amp; _PAGE_PADDR_4V;</span>
<span class="quote">&gt; +	for (tmp = paddr; tmp &lt; (paddr+PAGE_SIZE); tmp += adi_blksize()) {</span>
<span class="quote">&gt; +		asm volatile(&quot;ldxa [%1] %2, %0\n\t&quot;</span>
<span class="quote">&gt; +				: &quot;=r&quot; (version1)</span>
<span class="quote">&gt; +				: &quot;r&quot; (tmp), &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt; +		tmp += adi_blksize();</span>
<span class="quote">&gt; +		asm volatile(&quot;ldxa [%1] %2, %0\n\t&quot;</span>
<span class="quote">&gt; +				: &quot;=r&quot; (version2)</span>
<span class="quote">&gt; +				: &quot;r&quot; (tmp), &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt; +		*tag = (version1 &lt;&lt; 4) | version2;</span>
<span class="quote">&gt; +		tag++;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; diff --git a/arch/sparc/kernel/etrap_64.S b/arch/sparc/kernel/etrap_64.S</span>
<span class="quote">&gt; index 1276ca2567ba..7be33bf45cff 100644</span>
<span class="quote">&gt; --- a/arch/sparc/kernel/etrap_64.S</span>
<span class="quote">&gt; +++ b/arch/sparc/kernel/etrap_64.S</span>
<span class="quote">&gt; @@ -132,7 +132,33 @@ etrap_save:	save	%g2, -STACK_BIAS, %sp</span>
<span class="quote">&gt; 		stx	%g6, [%sp + PTREGS_OFF + PT_V9_G6]</span>
<span class="quote">&gt; 		stx	%g7, [%sp + PTREGS_OFF + PT_V9_G7]</span>
<span class="quote">&gt; 		or	%l7, %l0, %l7</span>
<span class="quote">&gt; -		sethi	%hi(TSTATE_TSO | TSTATE_PEF), %l0</span>
<span class="quote">&gt; +661:		sethi	%hi(TSTATE_TSO | TSTATE_PEF), %l0</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * If userspace is using ADI, it could potentially pass</span>
<span class="quote">&gt; +		 * a pointer with version tag embedded in it. To maintain</span>
<span class="quote">&gt; +		 * the ADI security, we must enable PSTATE.mcde. Userspace</span>
<span class="quote">&gt; +		 * would have already set TTE.mcd in an earlier call to</span>
<span class="quote">&gt; +		 * kernel and set the version tag for the address being</span>
<span class="quote">&gt; +		 * dereferenced. Setting PSTATE.mcde would ensure any</span>
<span class="quote">&gt; +		 * access to userspace data through a system call honors</span>
<span class="quote">&gt; +		 * ADI and does not allow a rogue app to bypass ADI by</span>
<span class="quote">&gt; +		 * using system calls. Setting PSTATE.mcde only affects</span>
<span class="quote">&gt; +		 * accesses to virtual addresses that have TTE.mcd set.</span>
<span class="quote">&gt; +		 * Set PMCDPER to ensure any exceptions caused by ADI</span>
<span class="quote">&gt; +		 * version tag mismatch are exposed before system call</span>
<span class="quote">&gt; +		 * returns to userspace. Setting PMCDPER affects only</span>
<span class="quote">&gt; +		 * writes to virtual addresses that have TTE.mcd set and</span>
<span class="quote">&gt; +		 * have a version tag set as well.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		.section .sun_m7_1insn_patch, &quot;ax&quot;</span>
<span class="quote">&gt; +		.word	661b</span>
<span class="quote">&gt; +		sethi	%hi(TSTATE_TSO | TSTATE_PEF | TSTATE_MCDE), %l0</span>
<span class="quote">&gt; +		.previous</span>
<span class="quote">&gt; +661:		nop</span>
<span class="quote">&gt; +		.section .sun_m7_1insn_patch, &quot;ax&quot;</span>
<span class="quote">&gt; +		.word	661b</span>
<span class="quote">&gt; +		.word 0xaf902001	/* wrpr %g0, 1, %pmcdper */</span>

I commented on this on the last patch series revision.  PMCDPER could be
set once when each CPU is configured rather than every time the kernel
is entered.  Since it&#39;s never cleared, setting it repeatedly unnecessarily
impacts the performance of etrap.

Also, there are places in rtrap where PSTATE is set before continuing
execution in the kernel.  These should also be patched to set TSTATE_MCDE.
<span class="quote">

&gt; +		.previous</span>
<span class="quote">&gt; 		or	%l7, %l0, %l7</span>
<span class="quote">&gt; 		wrpr	%l2, %tnpc</span>
<span class="quote">&gt; 		wrpr	%l7, (TSTATE_PRIV | TSTATE_IE), %tstate</span>
<span class="quote">&gt; diff --git a/arch/sparc/kernel/process_64.c b/arch/sparc/kernel/process_64.c</span>
<span class="quote">&gt; index b96104da5bd6..defa5723dfa6 100644</span>
<span class="quote">&gt; --- a/arch/sparc/kernel/process_64.c</span>
<span class="quote">&gt; +++ b/arch/sparc/kernel/process_64.c</span>
<span class="quote">&gt; @@ -664,6 +664,31 @@ int copy_thread(unsigned long clone_flags, unsigned long sp,</span>
<span class="quote">&gt; 	return 0;</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +/* TIF_MCDPER in thread info flags for current task is updated lazily upon</span>
<span class="quote">&gt; + * a context switch. Update the this flag in current task&#39;s thread flags</span>
<span class="quote">&gt; + * before dup so the dup&#39;d task will inherit the current TIF_MCDPER flag.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (adi_capable()) {</span>
<span class="quote">&gt; +		register unsigned long tmp_mcdper;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		__asm__ __volatile__(</span>
<span class="quote">&gt; +			&quot;.word 0x83438000\n\t&quot;	/* rd  %mcdper, %g1 */</span>
<span class="quote">&gt; +			&quot;mov %%g1, %0\n\t&quot;</span>
<span class="quote">&gt; +			: &quot;=r&quot; (tmp_mcdper)</span>
<span class="quote">&gt; +			:</span>
<span class="quote">&gt; +			: &quot;g1&quot;);</span>
<span class="quote">&gt; +		if (tmp_mcdper)</span>
<span class="quote">&gt; +			set_thread_flag(TIF_MCDPER);</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +			clear_thread_flag(TIF_MCDPER);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	*dst = *src;</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; typedef struct {</span>
<span class="quote">&gt; 	union {</span>
<span class="quote">&gt; 		unsigned int	pr_regs[32];</span>
<span class="quote">&gt; diff --git a/arch/sparc/kernel/setup_64.c b/arch/sparc/kernel/setup_64.c</span>
<span class="quote">&gt; index 422b17880955..a9da205da394 100644</span>
<span class="quote">&gt; --- a/arch/sparc/kernel/setup_64.c</span>
<span class="quote">&gt; +++ b/arch/sparc/kernel/setup_64.c</span>
<span class="quote">&gt; @@ -240,6 +240,12 @@ void sun4v_patch_1insn_range(struct sun4v_1insn_patch_entry *start,</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +void sun_m7_patch_1insn_range(struct sun4v_1insn_patch_entry *start,</span>
<span class="quote">&gt; +			     struct sun4v_1insn_patch_entry *end)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	sun4v_patch_1insn_range(start, end);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; void sun4v_patch_2insn_range(struct sun4v_2insn_patch_entry *start,</span>
<span class="quote">&gt; 			     struct sun4v_2insn_patch_entry *end)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; @@ -289,9 +295,12 @@ static void __init sun4v_patch(void)</span>
<span class="quote">&gt; 	sun4v_patch_2insn_range(&amp;__sun4v_2insn_patch,</span>
<span class="quote">&gt; 				&amp;__sun4v_2insn_patch_end);</span>
<span class="quote">&gt; 	if (sun4v_chip_type == SUN4V_CHIP_SPARC_M7 ||</span>
<span class="quote">&gt; -	    sun4v_chip_type == SUN4V_CHIP_SPARC_SN)</span>
<span class="quote">&gt; +	    sun4v_chip_type == SUN4V_CHIP_SPARC_SN) {</span>
<span class="quote">&gt; +		sun_m7_patch_1insn_range(&amp;__sun_m7_1insn_patch,</span>
<span class="quote">&gt; +					 &amp;__sun_m7_1insn_patch_end);</span>
<span class="quote">&gt; 		sun_m7_patch_2insn_range(&amp;__sun_m7_2insn_patch,</span>
<span class="quote">&gt; 					 &amp;__sun_m7_2insn_patch_end);</span>

Why not call sun4v_patch_1insn_range() and sun4v_patch_2insn_range()
here instead of adding new functions that just call these functions?

Anthony
<span class="quote">
&gt; +		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	sun4v_hvapi_init();</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; diff --git a/arch/sparc/kernel/vmlinux.lds.S b/arch/sparc/kernel/vmlinux.lds.S</span>
<span class="quote">&gt; index 572db686f845..20a70682cce7 100644</span>
<span class="quote">&gt; --- a/arch/sparc/kernel/vmlinux.lds.S</span>
<span class="quote">&gt; +++ b/arch/sparc/kernel/vmlinux.lds.S</span>
<span class="quote">&gt; @@ -144,6 +144,11 @@ SECTIONS</span>
<span class="quote">&gt; 		*(.pause_3insn_patch)</span>
<span class="quote">&gt; 		__pause_3insn_patch_end = .;</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt; +	.sun_m7_1insn_patch : {</span>
<span class="quote">&gt; +		__sun_m7_1insn_patch = .;</span>
<span class="quote">&gt; +		*(.sun_m7_1insn_patch)</span>
<span class="quote">&gt; +		__sun_m7_1insn_patch_end = .;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; 	.sun_m7_2insn_patch : {</span>
<span class="quote">&gt; 		__sun_m7_2insn_patch = .;</span>
<span class="quote">&gt; 		*(.sun_m7_2insn_patch)</span>
<span class="quote">&gt; diff --git a/arch/sparc/mm/gup.c b/arch/sparc/mm/gup.c</span>
<span class="quote">&gt; index cd0e32bbcb1d..579f7ae75b35 100644</span>
<span class="quote">&gt; --- a/arch/sparc/mm/gup.c</span>
<span class="quote">&gt; +++ b/arch/sparc/mm/gup.c</span>
<span class="quote">&gt; @@ -11,6 +11,7 @@</span>
<span class="quote">&gt; #include &lt;linux/pagemap.h&gt;</span>
<span class="quote">&gt; #include &lt;linux/rwsem.h&gt;</span>
<span class="quote">&gt; #include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/adi.h&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; /*</span>
<span class="quote">&gt;  * The performance critical leaf functions are made noinline otherwise gcc</span>
<span class="quote">&gt; @@ -157,6 +158,24 @@ int __get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
<span class="quote">&gt; 	pgd_t *pgdp;</span>
<span class="quote">&gt; 	int nr = 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +#ifdef CONFIG_SPARC64</span>
<span class="quote">&gt; +	if (adi_capable()) {</span>
<span class="quote">&gt; +		long addr = start;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* If userspace has passed a versioned address, kernel</span>
<span class="quote">&gt; +		 * will not find it in the VMAs since it does not store</span>
<span class="quote">&gt; +		 * the version tags in the list of VMAs. Storing version</span>
<span class="quote">&gt; +		 * tags in list of VMAs is impractical since they can be</span>
<span class="quote">&gt; +		 * changed any time from userspace without dropping into</span>
<span class="quote">&gt; +		 * kernel. Any address search in VMAs will be done with</span>
<span class="quote">&gt; +		 * non-versioned addresses. Ensure the ADI version bits</span>
<span class="quote">&gt; +		 * are dropped here by sign extending the last bit before</span>
<span class="quote">&gt; +		 * ADI bits. IOMMU does not implement version tags.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		addr = (addr &lt;&lt; (long)adi_nbits()) &gt;&gt; (long)adi_nbits();</span>
<span class="quote">&gt; +		start = addr;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; 	start &amp;= PAGE_MASK;</span>
<span class="quote">&gt; 	addr = start;</span>
<span class="quote">&gt; 	len = (unsigned long) nr_pages &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; @@ -187,6 +206,24 @@ int get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
<span class="quote">&gt; 	pgd_t *pgdp;</span>
<span class="quote">&gt; 	int nr = 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +#ifdef CONFIG_SPARC64</span>
<span class="quote">&gt; +	if (adi_capable()) {</span>
<span class="quote">&gt; +		long addr = start;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/* If userspace has passed a versioned address, kernel</span>
<span class="quote">&gt; +		 * will not find it in the VMAs since it does not store</span>
<span class="quote">&gt; +		 * the version tags in the list of VMAs. Storing version</span>
<span class="quote">&gt; +		 * tags in list of VMAs is impractical since they can be</span>
<span class="quote">&gt; +		 * changed any time from userspace without dropping into</span>
<span class="quote">&gt; +		 * kernel. Any address search in VMAs will be done with</span>
<span class="quote">&gt; +		 * non-versioned addresses. Ensure the ADI version bits</span>
<span class="quote">&gt; +		 * are dropped here by sign extending the last bit before</span>
<span class="quote">&gt; +		 * ADI bits. IOMMU does not implements version tags,</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		addr = (addr &lt;&lt; (long)adi_nbits()) &gt;&gt; (long)adi_nbits();</span>
<span class="quote">&gt; +		start = addr;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; 	start &amp;= PAGE_MASK;</span>
<span class="quote">&gt; 	addr = start;</span>
<span class="quote">&gt; 	len = (unsigned long) nr_pages &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; diff --git a/arch/sparc/mm/hugetlbpage.c b/arch/sparc/mm/hugetlbpage.c</span>
<span class="quote">&gt; index 88855e383b34..487ed1f1ce86 100644</span>
<span class="quote">&gt; --- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="quote">&gt; +++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="quote">&gt; @@ -177,8 +177,20 @@ pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,</span>
<span class="quote">&gt; 			 struct page *page, int writeable)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	unsigned int shift = huge_page_shift(hstate_vma(vma));</span>
<span class="quote">&gt; +	pte_t pte;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	return hugepage_shift_to_tte(entry, shift);</span>
<span class="quote">&gt; +	pte = hugepage_shift_to_tte(entry, shift);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_SPARC64</span>
<span class="quote">&gt; +	/* If this vma has ADI enabled on it, turn on TTE.mcd</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (vma-&gt;vm_flags &amp; VM_SPARC_ADI)</span>
<span class="quote">&gt; +		return pte_mkmcd(pte);</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		return pte_mknotmcd(pte);</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +	return pte;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; static unsigned int sun4v_huge_tte_to_shift(pte_t entry)</span>
<span class="quote">&gt; diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c</span>
<span class="quote">&gt; index 3c40ebd50f92..94854e7e833e 100644</span>
<span class="quote">&gt; --- a/arch/sparc/mm/init_64.c</span>
<span class="quote">&gt; +++ b/arch/sparc/mm/init_64.c</span>
<span class="quote">&gt; @@ -3087,3 +3087,36 @@ void flush_tlb_kernel_range(unsigned long start, unsigned long end)</span>
<span class="quote">&gt; 		do_flush_tlb_kernel_range(start, end);</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void copy_user_highpage(struct page *to, struct page *from,</span>
<span class="quote">&gt; +	unsigned long vaddr, struct vm_area_struct *vma)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	char *vfrom, *vto;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	vfrom = kmap_atomic(from);</span>
<span class="quote">&gt; +	vto = kmap_atomic(to);</span>
<span class="quote">&gt; +	copy_user_page(vto, vfrom, vaddr, to);</span>
<span class="quote">&gt; +	kunmap_atomic(vto);</span>
<span class="quote">&gt; +	kunmap_atomic(vfrom);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* If this page has ADI enabled, copy over any ADI tags</span>
<span class="quote">&gt; +	 * as well</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (vma-&gt;vm_flags &amp; VM_SPARC_ADI) {</span>
<span class="quote">&gt; +		unsigned long pfrom, pto, i, adi_tag;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pfrom = page_to_phys(from);</span>
<span class="quote">&gt; +		pto = page_to_phys(to);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		for (i = pfrom; i &lt; (pfrom + PAGE_SIZE); i += adi_blksize()) {</span>
<span class="quote">&gt; +			asm volatile(&quot;ldxa [%1] %2, %0\n\t&quot;</span>
<span class="quote">&gt; +					: &quot;=r&quot; (adi_tag)</span>
<span class="quote">&gt; +					:  &quot;r&quot; (i), &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt; +			asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="quote">&gt; +					:</span>
<span class="quote">&gt; +					: &quot;r&quot; (adi_tag), &quot;r&quot; (pto),</span>
<span class="quote">&gt; +					  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="quote">&gt; +			pto += adi_blksize();</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; diff --git a/arch/sparc/mm/tsb.c b/arch/sparc/mm/tsb.c</span>
<span class="quote">&gt; index 0d4b998c7d7b..6518cc42056b 100644</span>
<span class="quote">&gt; --- a/arch/sparc/mm/tsb.c</span>
<span class="quote">&gt; +++ b/arch/sparc/mm/tsb.c</span>
<span class="quote">&gt; @@ -545,6 +545,9 @@ int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	mm-&gt;context.sparc64_ctx_val = 0UL;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +	mm-&gt;context.tag_store = NULL;</span>
<span class="quote">&gt; +	spin_lock_init(&amp;mm-&gt;context.tag_lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)</span>
<span class="quote">&gt; 	/* We reset them to zero because the fork() page copying</span>
<span class="quote">&gt; 	 * will re-increment the counters as the parent PTEs are</span>
<span class="quote">&gt; @@ -610,4 +613,22 @@ void destroy_context(struct mm_struct *mm)</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	spin_unlock_irqrestore(&amp;ctx_alloc_lock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* If ADI tag storage was allocated for this task, free it */</span>
<span class="quote">&gt; +	if (mm-&gt;context.tag_store) {</span>
<span class="quote">&gt; +		tag_storage_desc_t *tag_desc;</span>
<span class="quote">&gt; +		unsigned long max_desc;</span>
<span class="quote">&gt; +		unsigned char *tags;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		tag_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt; +		max_desc = PAGE_SIZE/sizeof(tag_storage_desc_t);</span>
<span class="quote">&gt; +		for (i = 0; i &lt; max_desc; i++) {</span>
<span class="quote">&gt; +			tags = tag_desc-&gt;tags;</span>
<span class="quote">&gt; +			tag_desc-&gt;tags = NULL;</span>
<span class="quote">&gt; +			kfree(tags);</span>
<span class="quote">&gt; +			tag_desc++;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		kfree(mm-&gt;context.tag_store);</span>
<span class="quote">&gt; +		mm-&gt;context.tag_store = NULL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="quote">&gt; index b7aa3932e6d4..c0972114036f 100644</span>
<span class="quote">&gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt; @@ -231,6 +231,9 @@ extern unsigned int kobjsize(const void *objp);</span>
<span class="quote">&gt; # define VM_GROWSUP	VM_ARCH_1</span>
<span class="quote">&gt; #elif defined(CONFIG_IA64)</span>
<span class="quote">&gt; # define VM_GROWSUP	VM_ARCH_1</span>
<span class="quote">&gt; +#elif defined(CONFIG_SPARC64)</span>
<span class="quote">&gt; +# define VM_SPARC_ADI	VM_ARCH_1	/* Uses ADI tag for access control */</span>
<span class="quote">&gt; +# define VM_ARCH_CLEAR	VM_SPARC_ADI</span>
<span class="quote">&gt; #elif !defined(CONFIG_MMU)</span>
<span class="quote">&gt; # define VM_MAPPED_COPY	VM_ARCH_1	/* T if mapped copy of data (nommu mmap) */</span>
<span class="quote">&gt; #endif</span>
<span class="quote">&gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt; index 216184af0e19..bb82399816ef 100644</span>
<span class="quote">&gt; --- a/mm/ksm.c</span>
<span class="quote">&gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt; @@ -1797,6 +1797,10 @@ int ksm_madvise(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt; 		if (*vm_flags &amp; VM_SAO)</span>
<span class="quote">&gt; 			return 0;</span>
<span class="quote">&gt; #endif</span>
<span class="quote">&gt; +#ifdef VM_SPARC_ADI</span>
<span class="quote">&gt; +		if (*vm_flags &amp; VM_SPARC_ADI)</span>
<span class="quote">&gt; +			return 0;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		if (!test_bit(MMF_VM_MERGEABLE, &amp;mm-&gt;flags)) {</span>
<span class="quote">&gt; 			err = __ksm_enter(mm);</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.11.0</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe from this list: send the line &quot;unsubscribe sparclinux&quot; in</span>
<span class="quote">&gt; the body of a message to majordomo@vger.kernel.org</span>
<span class="quote">&gt; More majordomo info at  http://vger.kernel.org/majordomo-info.html</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=63231">Khalid Aziz</a> - Aug. 30, 2017, 10:27 p.m.</div>
<pre class="content">
Hi Anthony,

Thanks for taking the time to provide feedback. My comments inline below.

On 08/25/2017 04:31 PM, Anthony Yznaga wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; On Aug 9, 2017, at 2:26 PM, Khalid Aziz &lt;khalid.aziz@oracle.com&gt; wrote:</span>
<span class="quote">&gt;&gt; ......deleted......</span>
<span class="quote">&gt;&gt; +provided by the hypervisor to the kernel.  Kernel returns the value of</span>
<span class="quote">&gt;&gt; +ADI block size to userspace using auxiliary vector along with other ADI</span>
<span class="quote">&gt;&gt; +info. Following auxiliary vectors are provided by the kernel:</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	AT_ADI_BLKSZ	ADI block size. This is the granularity and</span>
<span class="quote">&gt;&gt; +			alignment, in bytes, of ADI versioning.</span>
<span class="quote">&gt;&gt; +	AT_ADI_NBITS	Number of ADI version bits in the VA</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The previous patch series also defined AT_ADI_UEONADI.  Why was that</span>
<span class="quote">&gt; removed?</span>

This was based upon a conversation we had when you mentioned future 
processors may not implement this or change the way this is interpreted 
and any applications depending upon this value would break at that 
point. I removed it to eliminate building an unreliable dependency. If I 
misunderstood what you said, please let me know.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +IMPORTANT NOTES:</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +- Version tag values of 0x0 and 0xf are reserved.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The documentation should probably state more specifically that an</span>
<span class="quote">&gt; in-memory tag value of 0x0 or 0xf is treated as &quot;match all&quot; by the HW</span>
<span class="quote">&gt; meaning that a mismatch exception will never be generated regardless</span>
<span class="quote">&gt; of the tag bits set in the VA accessing the memory.</span>

Will do.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +- Version tags are set on virtual addresses from userspace even though</span>
<span class="quote">&gt;&gt; +  tags are stored in physical memory. Tags are set on a physical page</span>
<span class="quote">&gt;&gt; +  after it has been allocated to a task and a pte has been created for</span>
<span class="quote">&gt;&gt; +  it.</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +- When a task frees a memory page it had set version tags on, the page</span>
<span class="quote">&gt;&gt; +  goes back to free page pool. When this page is re-allocated to a task,</span>
<span class="quote">&gt;&gt; +  kernel clears the page using block initialization ASI which clears the</span>
<span class="quote">&gt;&gt; +  version tags as well for the page. If a page allocated to a task is</span>
<span class="quote">&gt;&gt; +  freed and allocated back to the same task, old version tags set by the</span>
<span class="quote">&gt;&gt; +  task on that page will no longer be present.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The specifics should be included here, too, so someone doesn&#39;t have</span>
<span class="quote">&gt; to guess what&#39;s going on if they make changes and the tags are no longer</span>
<span class="quote">&gt; cleared.  The HW clears the tag for a cacheline for block initializing</span>
<span class="quote">&gt; stores to 64-byte aligned addresses if PSTATE.mcde=0 or TTE.mcd=0.</span>
<span class="quote">&gt; PSTATE.mce is set when executing in the kernel, but pages are cleared</span>
<span class="quote">&gt; using kernel physical mapping VAs which are mapped with TTE.mcd=0.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Another HW behavior that should be mentioned is that tag mismatches</span>
<span class="quote">&gt; are not detected for non-faulting loads.</span>

Sure, I can add that.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +- Kernel does not set any tags for user pages and it is entirely a</span>
<span class="quote">&gt;&gt; +  task&#39;s responsibility to set any version tags. Kernel does ensure the</span>
<span class="quote">&gt;&gt; +  version tags are preserved if a page is swapped out to the disk and</span>
<span class="quote">&gt;&gt; +  swapped back in. It also preserves that version tags if a page is</span>
<span class="quote">&gt;&gt; +  migrated.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I only have a cursory understanding of how page migration works, but</span>
<span class="quote">&gt; I could not see how the tags would be preserved if a page were migrated.</span>
<span class="quote">&gt; I figured the place to copy the tags would be migrate_page_copy(), but</span>
<span class="quote">&gt; I don&#39;t see changes there.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>

For migrating user pages, the way I understand the code works is if the 
page is mapped (which is the only time ADI tags are even in place), 
try_to_unmap() is called with TTU_MIGRATION flag set. try_to_unmap() 
will call arch_unmap_one() which saves the tags from currently mapped 
page. When the new page has been allocated, contents of the old page are 
faulted in through do_swap_page() which will call arch_do_swap_page(). 
arch_do_swap_page() then restores the ADI tags.
<span class="quote">

&gt;&gt; diff --git a/arch/sparc/include/asm/mman.h b/arch/sparc/include/asm/mman.h</span>
<span class="quote">&gt;&gt; index 59bb5938d852..b799796ad963 100644</span>
<span class="quote">&gt;&gt; --- a/arch/sparc/include/asm/mman.h</span>
<span class="quote">&gt;&gt; +++ b/arch/sparc/include/asm/mman.h</span>
<span class="quote">&gt;&gt; @@ -6,5 +6,75 @@</span>
<span class="quote">&gt;&gt; #ifndef __ASSEMBLY__</span>
<span class="quote">&gt;&gt; #define arch_mmap_check(addr,len,flags)	sparc_mmap_check(addr,len)</span>
<span class="quote">&gt;&gt; int sparc_mmap_check(unsigned long addr, unsigned long len);</span>
<span class="quote">&gt;&gt; -#endif</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_SPARC64</span>
<span class="quote">&gt;&gt; +#include &lt;asm/adi_64.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define arch_calc_vm_prot_bits(prot, pkey) sparc_calc_vm_prot_bits(prot)</span>
<span class="quote">&gt;&gt; +static inline unsigned long sparc_calc_vm_prot_bits(unsigned long prot)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	if (prot &amp; PROT_ADI) {</span>
<span class="quote">&gt;&gt; +		struct pt_regs *regs;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		if (!current-&gt;mm-&gt;context.adi) {</span>
<span class="quote">&gt;&gt; +			regs = task_pt_regs(current);</span>
<span class="quote">&gt;&gt; +			regs-&gt;tstate |= TSTATE_MCDE;</span>
<span class="quote">&gt;&gt; +			current-&gt;mm-&gt;context.adi = true;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If a process is multi-threaded when it enables ADI on some memory for</span>
<span class="quote">&gt; the first time, TSTATE_MCDE will only be set for the calling thread</span>
<span class="quote">&gt; and it will not be possible to enable it for the other threads.</span>
<span class="quote">&gt; One possible way to handle this is to enable TSTATE_MCDE for all user</span>
<span class="quote">&gt; threads when they are initialized if adi_capable() returns true.</span>
<span class="quote">&gt; </span>

Or set TSTATE_MCDE unconditionally here by removing &quot;if 
(!current-&gt;mm-&gt;context.adi)&quot;?
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +		return VM_SPARC_ADI;</span>
<span class="quote">&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt; +		return 0;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define arch_vm_get_page_prot(vm_flags) sparc_vm_get_page_prot(vm_flags)</span>
<span class="quote">&gt;&gt; +static inline pgprot_t sparc_vm_get_page_prot(unsigned long vm_flags)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	return (vm_flags &amp; VM_SPARC_ADI) ? __pgprot(_PAGE_MCD_4V) : __pgprot(0);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define arch_validate_prot(prot, addr) sparc_validate_prot(prot, addr)</span>
<span class="quote">&gt;&gt; +static inline int sparc_validate_prot(unsigned long prot, unsigned long addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	if (prot &amp; ~(PROT_READ | PROT_WRITE | PROT_EXEC | PROT_SEM | PROT_ADI))</span>
<span class="quote">&gt;&gt; +		return 0;</span>
<span class="quote">&gt;&gt; +	if (prot &amp; PROT_ADI) {</span>
<span class="quote">&gt;&gt; +		if (!adi_capable())</span>
<span class="quote">&gt;&gt; +			return 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		/* ADI tags can not be set on read-only memory, so it makes</span>
<span class="quote">&gt;&gt; +		 * sense to enable ADI on writable memory only.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		if (!(prot &amp; PROT_WRITE))</span>
<span class="quote">&gt;&gt; +			return 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This prevents the use of ADI for the legitimate case where shared memory</span>
<span class="quote">&gt; is mapped read/write for a master process but mapped read-only for a</span>
<span class="quote">&gt; client process.  The master process could set the tags and communicate</span>
<span class="quote">&gt; the expected tag values to the client.</span>

A non-writable mapping can access the shared memory using non-ADI tagged 
addresses if it does not enable ADI on its mappings, so it is 
superfluous to even allow enabling ADI. I can remove this if that helps 
any use cases that wouldn&#39;t work with above condition.
<span class="quote">
&gt;&gt; +tag_storage_desc_t *alloc_tag_store(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt; +				    struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; +				    unsigned long addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	unsigned char *tags;</span>
<span class="quote">&gt;&gt; +	unsigned long i, size, max_desc, flags;</span>
<span class="quote">&gt;&gt; +	tag_storage_desc_t *tag_desc, *open_desc;</span>
<span class="quote">&gt;&gt; +	unsigned long end_addr, hole_start, hole_end;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	max_desc = PAGE_SIZE/sizeof(tag_storage_desc_t);</span>
<span class="quote">&gt;&gt; +	open_desc = NULL;</span>
<span class="quote">&gt;&gt; +	hole_start = 0;</span>
<span class="quote">&gt;&gt; +	hole_end = ULONG_MAX;</span>
<span class="quote">&gt;&gt; +	end_addr = addr + PAGE_SIZE - 1;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* Check if this vma already has tag storage descriptor</span>
<span class="quote">&gt;&gt; +	 * allocated for it.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	spin_lock_irqsave(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="quote">&gt;&gt; +	if (mm-&gt;context.tag_store) {</span>
<span class="quote">&gt;&gt; +		tag_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		/* Look for a matching entry for this address. While doing</span>
<span class="quote">&gt;&gt; +		 * that, look for the first open slot as well and find</span>
<span class="quote">&gt;&gt; +		 * the hole in already allocated range where this request</span>
<span class="quote">&gt;&gt; +		 * will fit in.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		for (i = 0; i &lt; max_desc; i++) {</span>
<span class="quote">&gt;&gt; +			if (tag_desc-&gt;tag_users == 0) {</span>
<span class="quote">&gt;&gt; +				if (open_desc == NULL)</span>
<span class="quote">&gt;&gt; +					open_desc = tag_desc;</span>
<span class="quote">&gt;&gt; +			} else {</span>
<span class="quote">&gt;&gt; +				if ((addr &gt;= tag_desc-&gt;start) &amp;&amp;</span>
<span class="quote">&gt;&gt; +				    (tag_desc-&gt;end &gt;= (addr + PAGE_SIZE - 1))) {</span>
<span class="quote">&gt;&gt; +					tag_desc-&gt;tag_users++;</span>
<span class="quote">&gt;&gt; +					goto out;</span>
<span class="quote">&gt;&gt; +				}</span>
<span class="quote">&gt;&gt; +			}</span>
<span class="quote">&gt;&gt; +			if ((tag_desc-&gt;start &gt; end_addr) &amp;&amp;</span>
<span class="quote">&gt;&gt; +			    (tag_desc-&gt;start &lt; hole_end))</span>
<span class="quote">&gt;&gt; +				hole_end = tag_desc-&gt;start;</span>
<span class="quote">&gt;&gt; +			if ((tag_desc-&gt;end &lt; addr) &amp;&amp;</span>
<span class="quote">&gt;&gt; +			    (tag_desc-&gt;end &gt; hole_start))</span>
<span class="quote">&gt;&gt; +				hole_start = tag_desc-&gt;end;</span>
<span class="quote">&gt;&gt; +			tag_desc++;</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt; +		size = sizeof(tag_storage_desc_t)*max_desc;</span>
<span class="quote">&gt;&gt; +		mm-&gt;context.tag_store = kzalloc(size, GFP_NOIO|__GFP_NOWARN);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The spin_lock_irqsave() above means that all but level 15 interrupts</span>
<span class="quote">&gt; will be disabled when kzalloc() is called.  If kzalloc() can sleep</span>
<span class="quote">&gt; there&#39;s a risk of deadlock.</span>

I could call kzalloc() with GFP_NOWAIT instead of GFP_NOIO. Would that 
address the risk of deadlock?
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +		if (mm-&gt;context.tag_store == NULL) {</span>
<span class="quote">&gt;&gt; +			tag_desc = NULL;</span>
<span class="quote">&gt;&gt; +			goto out;</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +		tag_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt;&gt; +		for (i = 0; i &lt; max_desc; i++, tag_desc++)</span>
<span class="quote">&gt;&gt; +			tag_desc-&gt;tag_users = 0;</span>
<span class="quote">&gt;&gt; +		open_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt;&gt; +		i = 0;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* Check if we ran out of tag storage descriptors */</span>
<span class="quote">&gt;&gt; +	if (open_desc == NULL) {</span>
<span class="quote">&gt;&gt; +		tag_desc = NULL;</span>
<span class="quote">&gt;&gt; +		goto out;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* Mark this tag descriptor slot in use and then initialize it */</span>
<span class="quote">&gt;&gt; +	tag_desc = open_desc;</span>
<span class="quote">&gt;&gt; +	tag_desc-&gt;tag_users = 1;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* Tag storage has not been allocated for this vma and space</span>
<span class="quote">&gt;&gt; +	 * is available in tag storage descriptor. Since this page is</span>
<span class="quote">&gt;&gt; +	 * being swapped out, there is high probability subsequent pages</span>
<span class="quote">&gt;&gt; +	 * in the VMA will be swapped out as well. Allocates pages to</span>
<span class="quote">&gt;&gt; +	 * store tags for as many pages in this vma as possible but not</span>
<span class="quote">&gt;&gt; +	 * more than TAG_STORAGE_PAGES. Each byte in tag space holds</span>
<span class="quote">&gt;&gt; +	 * two ADI tags since each ADI tag is 4 bits. Each ADI tag</span>
<span class="quote">&gt;&gt; +	 * covers adi_blksize() worth of addresses. Check if the hole is</span>
<span class="quote">&gt;&gt; +	 * big enough to accommodate full address range for using</span>
<span class="quote">&gt;&gt; +	 * TAG_STORAGE_PAGES number of tag pages.</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	size = TAG_STORAGE_PAGES * PAGE_SIZE;</span>
<span class="quote">&gt;&gt; +	end_addr = addr + (size*2*adi_blksize()) - 1;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Since size &gt; PAGE_SIZE, end_addr could theoretically overflow &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +	if (hole_end &lt; end_addr) {</span>
<span class="quote">&gt;&gt; +		/* Available hole is too small on the upper end of</span>
<span class="quote">&gt;&gt; +		 * address. Can we expand the range towards the lower</span>
<span class="quote">&gt;&gt; +		 * address and maximize use of this slot?</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		unsigned long tmp_addr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		end_addr = hole_end - 1;</span>
<span class="quote">&gt;&gt; +		tmp_addr = end_addr - (size*2*adi_blksize()) + 1;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Similarily, tmp_addr may underflow.</span>

I will add checks for these two.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +		if (tmp_addr &lt; hole_start) {</span>
<span class="quote">&gt;&gt; +			/* Available hole is restricted on lower address</span>
<span class="quote">&gt;&gt; +			 * end as well</span>
<span class="quote">&gt;&gt; +			 */</span>
<span class="quote">&gt;&gt; +			tmp_addr = hole_start + 1;</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +		addr = tmp_addr;</span>
<span class="quote">&gt;&gt; +		size = (end_addr + 1 - addr)/(2*adi_blksize());</span>
<span class="quote">&gt;&gt; +		size = (size + (PAGE_SIZE-adi_blksize()))/PAGE_SIZE;</span>
<span class="quote">&gt;&gt; +		size = size * PAGE_SIZE;</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt; +	tags = kzalloc(size, GFP_NOIO|__GFP_NOWARN);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Potential deadlock due to PIL=14?</span>

Same as above - call kzalloc() with GFP_NOWAIT?
<span class="quote">
&gt;&gt; diff --git a/arch/sparc/kernel/etrap_64.S b/arch/sparc/kernel/etrap_64.S</span>
<span class="quote">&gt;&gt; index 1276ca2567ba..7be33bf45cff 100644</span>
<span class="quote">&gt;&gt; --- a/arch/sparc/kernel/etrap_64.S</span>
<span class="quote">&gt;&gt; +++ b/arch/sparc/kernel/etrap_64.S</span>
<span class="quote">&gt;&gt; @@ -132,7 +132,33 @@ etrap_save:	save	%g2, -STACK_BIAS, %sp</span>
<span class="quote">&gt;&gt; 		stx	%g6, [%sp + PTREGS_OFF + PT_V9_G6]</span>
<span class="quote">&gt;&gt; 		stx	%g7, [%sp + PTREGS_OFF + PT_V9_G7]</span>
<span class="quote">&gt;&gt; 		or	%l7, %l0, %l7</span>
<span class="quote">&gt;&gt; -		sethi	%hi(TSTATE_TSO | TSTATE_PEF), %l0</span>
<span class="quote">&gt;&gt; +661:		sethi	%hi(TSTATE_TSO | TSTATE_PEF), %l0</span>
<span class="quote">&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * If userspace is using ADI, it could potentially pass</span>
<span class="quote">&gt;&gt; +		 * a pointer with version tag embedded in it. To maintain</span>
<span class="quote">&gt;&gt; +		 * the ADI security, we must enable PSTATE.mcde. Userspace</span>
<span class="quote">&gt;&gt; +		 * would have already set TTE.mcd in an earlier call to</span>
<span class="quote">&gt;&gt; +		 * kernel and set the version tag for the address being</span>
<span class="quote">&gt;&gt; +		 * dereferenced. Setting PSTATE.mcde would ensure any</span>
<span class="quote">&gt;&gt; +		 * access to userspace data through a system call honors</span>
<span class="quote">&gt;&gt; +		 * ADI and does not allow a rogue app to bypass ADI by</span>
<span class="quote">&gt;&gt; +		 * using system calls. Setting PSTATE.mcde only affects</span>
<span class="quote">&gt;&gt; +		 * accesses to virtual addresses that have TTE.mcd set.</span>
<span class="quote">&gt;&gt; +		 * Set PMCDPER to ensure any exceptions caused by ADI</span>
<span class="quote">&gt;&gt; +		 * version tag mismatch are exposed before system call</span>
<span class="quote">&gt;&gt; +		 * returns to userspace. Setting PMCDPER affects only</span>
<span class="quote">&gt;&gt; +		 * writes to virtual addresses that have TTE.mcd set and</span>
<span class="quote">&gt;&gt; +		 * have a version tag set as well.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		.section .sun_m7_1insn_patch, &quot;ax&quot;</span>
<span class="quote">&gt;&gt; +		.word	661b</span>
<span class="quote">&gt;&gt; +		sethi	%hi(TSTATE_TSO | TSTATE_PEF | TSTATE_MCDE), %l0</span>
<span class="quote">&gt;&gt; +		.previous</span>
<span class="quote">&gt;&gt; +661:		nop</span>
<span class="quote">&gt;&gt; +		.section .sun_m7_1insn_patch, &quot;ax&quot;</span>
<span class="quote">&gt;&gt; +		.word	661b</span>
<span class="quote">&gt;&gt; +		.word 0xaf902001	/* wrpr %g0, 1, %pmcdper */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I commented on this on the last patch series revision.  PMCDPER could be</span>
<span class="quote">&gt; set once when each CPU is configured rather than every time the kernel</span>
<span class="quote">&gt; is entered.  Since it&#39;s never cleared, setting it repeatedly unnecessarily</span>
<span class="quote">&gt; impacts the performance of etrap.</span>

Yes, you did and I thought I had addressed it in that thread:

&quot;&gt;&gt; I considered that possibility. What made me uncomfortable with that 
is there is no way to prevent a driver/module or future code elsewhere 
in kernel from clearing PMCDPER with possibly good reason. If that were 
to happen, setting PMCDPER here ensures kernel will always see 
consistent behavior with system calls. It does come at a cost. Is that 
cost unacceptable to ensure consistent behavior?
<span class="quote">&gt; </span>
<span class="quote">&gt; Aren&#39;t you still at risk if the thread relinquishes the CPU while in the kernel and is then rescheduled on a CPU where PMCDPER has erroneously been left cleared?  You may need to save and restore PMCDPER as well as MCDPER on context switch, but I don&#39;t know if that will cover you completely.</span>
<span class="quote">&gt; &quot;</span>

I should add setting PMCDPER to 1 in finish_arch_post_lock_switch() to 
address the possibility you had mentioned.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Also, there are places in rtrap where PSTATE is set before continuing</span>
<span class="quote">&gt; execution in the kernel.  These should also be patched to set TSTATE_MCDE.</span>
<span class="quote">&gt; </span>

I will find and fix those.
<span class="quote">
&gt;&gt; diff --git a/arch/sparc/kernel/setup_64.c b/arch/sparc/kernel/setup_64.c</span>
<span class="quote">&gt;&gt; index 422b17880955..a9da205da394 100644</span>
<span class="quote">&gt;&gt; --- a/arch/sparc/kernel/setup_64.c</span>
<span class="quote">&gt;&gt; +++ b/arch/sparc/kernel/setup_64.c</span>
<span class="quote">&gt;&gt; @@ -240,6 +240,12 @@ void sun4v_patch_1insn_range(struct sun4v_1insn_patch_entry *start,</span>
<span class="quote">&gt;&gt; 	}</span>
<span class="quote">&gt;&gt; }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +void sun_m7_patch_1insn_range(struct sun4v_1insn_patch_entry *start,</span>
<span class="quote">&gt;&gt; +			     struct sun4v_1insn_patch_entry *end)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	sun4v_patch_1insn_range(start, end);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; void sun4v_patch_2insn_range(struct sun4v_2insn_patch_entry *start,</span>
<span class="quote">&gt;&gt; 			     struct sun4v_2insn_patch_entry *end)</span>
<span class="quote">&gt;&gt; {</span>
<span class="quote">&gt;&gt; @@ -289,9 +295,12 @@ static void __init sun4v_patch(void)</span>
<span class="quote">&gt;&gt; 	sun4v_patch_2insn_range(&amp;__sun4v_2insn_patch,</span>
<span class="quote">&gt;&gt; 				&amp;__sun4v_2insn_patch_end);</span>
<span class="quote">&gt;&gt; 	if (sun4v_chip_type == SUN4V_CHIP_SPARC_M7 ||</span>
<span class="quote">&gt;&gt; -	    sun4v_chip_type == SUN4V_CHIP_SPARC_SN)</span>
<span class="quote">&gt;&gt; +	    sun4v_chip_type == SUN4V_CHIP_SPARC_SN) {</span>
<span class="quote">&gt;&gt; +		sun_m7_patch_1insn_range(&amp;__sun_m7_1insn_patch,</span>
<span class="quote">&gt;&gt; +					 &amp;__sun_m7_1insn_patch_end);</span>
<span class="quote">&gt;&gt; 		sun_m7_patch_2insn_range(&amp;__sun_m7_2insn_patch,</span>
<span class="quote">&gt;&gt; 					 &amp;__sun_m7_2insn_patch_end);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why not call sun4v_patch_1insn_range() and sun4v_patch_2insn_range()</span>
<span class="quote">&gt; here instead of adding new functions that just call these functions?</span>

Sounds reasonable, I can change that.

Thanks,
Khalid
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=129">David Miller</a> - Aug. 30, 2017, 10:38 p.m.</div>
<pre class="content">
<span class="from">From: Khalid Aziz &lt;khalid.aziz@oracle.com&gt;</span>
Date: Wed, 30 Aug 2017 16:27:54 -0600
<span class="quote">
&gt;&gt;&gt; +#define arch_calc_vm_prot_bits(prot, pkey)</span>
<span class="quote">&gt;&gt;&gt; sparc_calc_vm_prot_bits(prot)</span>
<span class="quote">&gt;&gt;&gt; +static inline unsigned long sparc_calc_vm_prot_bits(unsigned long</span>
<span class="quote">&gt;&gt;&gt; prot)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	if (prot &amp; PROT_ADI) {</span>
<span class="quote">&gt;&gt;&gt; +		struct pt_regs *regs;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +		if (!current-&gt;mm-&gt;context.adi) {</span>
<span class="quote">&gt;&gt;&gt; +			regs = task_pt_regs(current);</span>
<span class="quote">&gt;&gt;&gt; +			regs-&gt;tstate |= TSTATE_MCDE;</span>
<span class="quote">&gt;&gt;&gt; +			current-&gt;mm-&gt;context.adi = true;</span>
<span class="quote">&gt;&gt; If a process is multi-threaded when it enables ADI on some memory for</span>
<span class="quote">&gt;&gt; the first time, TSTATE_MCDE will only be set for the calling thread</span>
<span class="quote">&gt;&gt; and it will not be possible to enable it for the other threads.</span>
<span class="quote">&gt;&gt; One possible way to handle this is to enable TSTATE_MCDE for all user</span>
<span class="quote">&gt;&gt; threads when they are initialized if adi_capable() returns true.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or set TSTATE_MCDE unconditionally here by removing &quot;if</span>
<span class="quote">&gt; (!current-&gt;mm-&gt;context.adi)&quot;?</span>

I think you have to make &quot;ADI enabled&quot; a property of the mm_struct.

Then you can broadcast to mm-&gt;cpu_vm_mask a per-cpu interrupt that
updates regs-&gt;tstate of a thread using &#39;mm&#39; is currently executing.

And in the context switch code you set TSTATE_MCDE if it&#39;s not set
already.

That should cover all threaded case.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=63231">Khalid Aziz</a> - Aug. 30, 2017, 11:23 p.m.</div>
<pre class="content">
On 08/30/2017 04:38 PM, David Miller wrote:
<span class="quote">&gt; From: Khalid Aziz &lt;khalid.aziz@oracle.com&gt;</span>
<span class="quote">&gt; Date: Wed, 30 Aug 2017 16:27:54 -0600</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; +#define arch_calc_vm_prot_bits(prot, pkey)</span>
<span class="quote">&gt;&gt;&gt;&gt; sparc_calc_vm_prot_bits(prot)</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline unsigned long sparc_calc_vm_prot_bits(unsigned long</span>
<span class="quote">&gt;&gt;&gt;&gt; prot)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +	if (prot &amp; PROT_ADI) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +		struct pt_regs *regs;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +		if (!current-&gt;mm-&gt;context.adi) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +			regs = task_pt_regs(current);</span>
<span class="quote">&gt;&gt;&gt;&gt; +			regs-&gt;tstate |= TSTATE_MCDE;</span>
<span class="quote">&gt;&gt;&gt;&gt; +			current-&gt;mm-&gt;context.adi = true;</span>
<span class="quote">&gt;&gt;&gt; If a process is multi-threaded when it enables ADI on some memory for</span>
<span class="quote">&gt;&gt;&gt; the first time, TSTATE_MCDE will only be set for the calling thread</span>
<span class="quote">&gt;&gt;&gt; and it will not be possible to enable it for the other threads.</span>
<span class="quote">&gt;&gt;&gt; One possible way to handle this is to enable TSTATE_MCDE for all user</span>
<span class="quote">&gt;&gt;&gt; threads when they are initialized if adi_capable() returns true.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Or set TSTATE_MCDE unconditionally here by removing &quot;if</span>
<span class="quote">&gt;&gt; (!current-&gt;mm-&gt;context.adi)&quot;?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think you have to make &quot;ADI enabled&quot; a property of the mm_struct.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Then you can broadcast to mm-&gt;cpu_vm_mask a per-cpu interrupt that</span>
<span class="quote">&gt; updates regs-&gt;tstate of a thread using &#39;mm&#39; is currently executing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And in the context switch code you set TSTATE_MCDE if it&#39;s not set</span>
<span class="quote">&gt; already.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That should cover all threaded case.</span>

That is an interesting idea. This would enable TSTATE_MCDE on all 
threads of a process as soon as one thread enables it. If we consider 
the case where the parent creates a shared memory area and spawns a 
bunch of threads. These threads access the shared memory without ADI 
enabled. Now one of the threads decides to enable ADI on the shared 
memory. As soon as it does that, we enable TSTATE_MCDE across all 
threads and since threads are all using the same TTE for the shared 
memory, every thread becomes subject to ADI verification. If one of the 
other threads was in the middle of accessing the shared memory, it will 
get a sigsegv. If we did not enable TSTATE_MCDE across all threads, it 
could have continued execution without fault. In other words, updating 
TSTATE_MCDE across all threads will eliminate the option of running some 
threads with ADI enabled and some not while accessing the same shared 
memory. This could be necessary at least for short periods of time 
before threads can communicate with each other and all switch to 
accessing shared memory with ADI enabled using same tag. Does that sound 
like a valid use case or am I off in the weeds here?

Thanks,
Khalid
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=129">David Miller</a> - Aug. 31, 2017, 12:09 a.m.</div>
<pre class="content">
<span class="from">From: Khalid Aziz &lt;khalid.aziz@oracle.com&gt;</span>
Date: Wed, 30 Aug 2017 17:23:37 -0600
<span class="quote">
&gt; That is an interesting idea. This would enable TSTATE_MCDE on all</span>
<span class="quote">&gt; threads of a process as soon as one thread enables it. If we consider</span>
<span class="quote">&gt; the case where the parent creates a shared memory area and spawns a</span>
<span class="quote">&gt; bunch of threads. These threads access the shared memory without ADI</span>
<span class="quote">&gt; enabled. Now one of the threads decides to enable ADI on the shared</span>
<span class="quote">&gt; memory. As soon as it does that, we enable TSTATE_MCDE across all</span>
<span class="quote">&gt; threads and since threads are all using the same TTE for the shared</span>
<span class="quote">&gt; memory, every thread becomes subject to ADI verification. If one of</span>
<span class="quote">&gt; the other threads was in the middle of accessing the shared memory, it</span>
<span class="quote">&gt; will get a sigsegv. If we did not enable TSTATE_MCDE across all</span>
<span class="quote">&gt; threads, it could have continued execution without fault. In other</span>
<span class="quote">&gt; words, updating TSTATE_MCDE across all threads will eliminate the</span>
<span class="quote">&gt; option of running some threads with ADI enabled and some not while</span>
<span class="quote">&gt; accessing the same shared memory. This could be necessary at least for</span>
<span class="quote">&gt; short periods of time before threads can communicate with each other</span>
<span class="quote">&gt; and all switch to accessing shared memory with ADI enabled using same</span>
<span class="quote">&gt; tag. Does that sound like a valid use case or am I off in the weeds</span>
<span class="quote">&gt; here?</span>

A threaded application needs to synchronize and properly orchestrate
access to shared memory.

When a change is made to a mappping, in this case setting ADI
attributes, it&#39;s being done for the address space not the thread.

And the address space is shared amongst threads.

Therefore ADI is not really a per-thread property but rather
a per-address-space property.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=63231">Khalid Aziz</a> - Aug. 31, 2017, 4:38 p.m.</div>
<pre class="content">
On 08/30/2017 06:09 PM, David Miller wrote:
<span class="quote">&gt; From: Khalid Aziz &lt;khalid.aziz@oracle.com&gt;</span>
<span class="quote">&gt; Date: Wed, 30 Aug 2017 17:23:37 -0600</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; That is an interesting idea. This would enable TSTATE_MCDE on all</span>
<span class="quote">&gt;&gt; threads of a process as soon as one thread enables it. If we consider</span>
<span class="quote">&gt;&gt; the case where the parent creates a shared memory area and spawns a</span>
<span class="quote">&gt;&gt; bunch of threads. These threads access the shared memory without ADI</span>
<span class="quote">&gt;&gt; enabled. Now one of the threads decides to enable ADI on the shared</span>
<span class="quote">&gt;&gt; memory. As soon as it does that, we enable TSTATE_MCDE across all</span>
<span class="quote">&gt;&gt; threads and since threads are all using the same TTE for the shared</span>
<span class="quote">&gt;&gt; memory, every thread becomes subject to ADI verification. If one of</span>
<span class="quote">&gt;&gt; the other threads was in the middle of accessing the shared memory, it</span>
<span class="quote">&gt;&gt; will get a sigsegv. If we did not enable TSTATE_MCDE across all</span>
<span class="quote">&gt;&gt; threads, it could have continued execution without fault. In other</span>
<span class="quote">&gt;&gt; words, updating TSTATE_MCDE across all threads will eliminate the</span>
<span class="quote">&gt;&gt; option of running some threads with ADI enabled and some not while</span>
<span class="quote">&gt;&gt; accessing the same shared memory. This could be necessary at least for</span>
<span class="quote">&gt;&gt; short periods of time before threads can communicate with each other</span>
<span class="quote">&gt;&gt; and all switch to accessing shared memory with ADI enabled using same</span>
<span class="quote">&gt;&gt; tag. Does that sound like a valid use case or am I off in the weeds</span>
<span class="quote">&gt;&gt; here?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A threaded application needs to synchronize and properly orchestrate</span>
<span class="quote">&gt; access to shared memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When a change is made to a mappping, in this case setting ADI</span>
<span class="quote">&gt; attributes, it&#39;s being done for the address space not the thread.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And the address space is shared amongst threads.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Therefore ADI is not really a per-thread property but rather</span>
<span class="quote">&gt; a per-address-space property.</span>
<span class="quote">&gt; </span>

That does make sense.

Thanks,
Khalid
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173107">Anthony Yznaga</a> - Sept. 1, 2017, 5:38 a.m.</div>
<pre class="content">
Hi Khalid,
<span class="quote">
&gt; On Aug 30, 2017, at 3:27 PM, Khalid Aziz &lt;khalid.aziz@Oracle.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi Anthony,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for taking the time to provide feedback. My comments inline below.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 08/25/2017 04:31 PM, Anthony Yznaga wrote:</span>
<span class="quote">&gt;&gt;&gt; On Aug 9, 2017, at 2:26 PM, Khalid Aziz &lt;khalid.aziz@oracle.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; ......deleted......</span>
<span class="quote">&gt;&gt;&gt; +provided by the hypervisor to the kernel.  Kernel returns the value of</span>
<span class="quote">&gt;&gt;&gt; +ADI block size to userspace using auxiliary vector along with other ADI</span>
<span class="quote">&gt;&gt;&gt; +info. Following auxiliary vectors are provided by the kernel:</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	AT_ADI_BLKSZ	ADI block size. This is the granularity and</span>
<span class="quote">&gt;&gt;&gt; +			alignment, in bytes, of ADI versioning.</span>
<span class="quote">&gt;&gt;&gt; +	AT_ADI_NBITS	Number of ADI version bits in the VA</span>
<span class="quote">&gt;&gt; The previous patch series also defined AT_ADI_UEONADI.  Why was that</span>
<span class="quote">&gt;&gt; removed?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This was based upon a conversation we had when you mentioned future processors may not implement this or change the way this is interpreted and any applications depending upon this value would break at that point. I removed it to eliminate building an unreliable dependency. If I misunderstood what you said, please let me know.</span>

On M7 there is an array of versions maintained for cachelines in the L2
cache. If a UE is detected in this array it results in the flush of all
eight ways of the array.  Clean lines go away, but dirty lines are
written back to memory with the version forced to 0xE.  The ue-on-adp MD
property communicates this tag value that may result from a UE in order
to give the guest the opportunity to avoid using the tag value.  An
application that intentionally used ADI in a way that relied on ADI
exceptions for its functionality may not want to have to consider
whether the mismatch was legitimate or due to a UE.

On M8 the HW implementation is changed and a tag value will never be
forced to another value.  That said, I think the ue-on-adp property
value was unfortunately inadvertently carried forward to M8.

It could probably be argued that the likelihood of seeing the UE is so
low that SW can ignore the possibility, but including the information
in an auxvec shouldn&#39;t break anything.
<span class="quote">

&gt; </span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +IMPORTANT NOTES:</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +- Version tag values of 0x0 and 0xf are reserved.</span>
<span class="quote">&gt;&gt; The documentation should probably state more specifically that an</span>
<span class="quote">&gt;&gt; in-memory tag value of 0x0 or 0xf is treated as &quot;match all&quot; by the HW</span>
<span class="quote">&gt;&gt; meaning that a mismatch exception will never be generated regardless</span>
<span class="quote">&gt;&gt; of the tag bits set in the VA accessing the memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Will do.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +- Version tags are set on virtual addresses from userspace even though</span>
<span class="quote">&gt;&gt;&gt; +  tags are stored in physical memory. Tags are set on a physical page</span>
<span class="quote">&gt;&gt;&gt; +  after it has been allocated to a task and a pte has been created for</span>
<span class="quote">&gt;&gt;&gt; +  it.</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +- When a task frees a memory page it had set version tags on, the page</span>
<span class="quote">&gt;&gt;&gt; +  goes back to free page pool. When this page is re-allocated to a task,</span>
<span class="quote">&gt;&gt;&gt; +  kernel clears the page using block initialization ASI which clears the</span>
<span class="quote">&gt;&gt;&gt; +  version tags as well for the page. If a page allocated to a task is</span>
<span class="quote">&gt;&gt;&gt; +  freed and allocated back to the same task, old version tags set by the</span>
<span class="quote">&gt;&gt;&gt; +  task on that page will no longer be present.</span>
<span class="quote">&gt;&gt; The specifics should be included here, too, so someone doesn&#39;t have</span>
<span class="quote">&gt;&gt; to guess what&#39;s going on if they make changes and the tags are no longer</span>
<span class="quote">&gt;&gt; cleared.  The HW clears the tag for a cacheline for block initializing</span>
<span class="quote">&gt;&gt; stores to 64-byte aligned addresses if PSTATE.mcde=0 or TTE.mcd=0.</span>
<span class="quote">&gt;&gt; PSTATE.mce is set when executing in the kernel, but pages are cleared</span>
<span class="quote">&gt;&gt; using kernel physical mapping VAs which are mapped with TTE.mcd=0.</span>
<span class="quote">&gt;&gt; Another HW behavior that should be mentioned is that tag mismatches</span>
<span class="quote">&gt;&gt; are not detected for non-faulting loads.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure, I can add that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +- Kernel does not set any tags for user pages and it is entirely a</span>
<span class="quote">&gt;&gt;&gt; +  task&#39;s responsibility to set any version tags. Kernel does ensure the</span>
<span class="quote">&gt;&gt;&gt; +  version tags are preserved if a page is swapped out to the disk and</span>
<span class="quote">&gt;&gt;&gt; +  swapped back in. It also preserves that version tags if a page is</span>
<span class="quote">&gt;&gt;&gt; +  migrated.</span>
<span class="quote">&gt;&gt; I only have a cursory understanding of how page migration works, but</span>
<span class="quote">&gt;&gt; I could not see how the tags would be preserved if a page were migrated.</span>
<span class="quote">&gt;&gt; I figured the place to copy the tags would be migrate_page_copy(), but</span>
<span class="quote">&gt;&gt; I don&#39;t see changes there.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For migrating user pages, the way I understand the code works is if the page is mapped (which is the only time ADI tags are even in place), try_to_unmap() is called with TTU_MIGRATION flag set. try_to_unmap() will call arch_unmap_one() which saves the tags from currently mapped page. When the new page has been allocated, contents of the old page are faulted in through do_swap_page() which will call arch_do_swap_page(). arch_do_swap_page() then restores the ADI tags.</span>

My understanding from reading the code is that __unmap_and_move() calls
try_to_unmap() which unmaps the page and installs a migration pte.
move_to_new_page() is then called which copies the data.  Finally,
remove_migration_ptes() is called which removes the migration pte and
installs an updated regular pte.  If a fault on the page happens while
the migration pte is installed, do_swap_page() is called and the
faulting thread waits for the migration to complete before proceeding. 
However, if no fault happens before the migration completes, a regular
pte will be found by the next fault and do_swap_page() will not be
called.
<span class="quote">

&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/sparc/include/asm/mman.h b/arch/sparc/include/asm/mman.h</span>
<span class="quote">&gt;&gt;&gt; index 59bb5938d852..b799796ad963 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/sparc/include/asm/mman.h</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/sparc/include/asm/mman.h</span>
<span class="quote">&gt;&gt;&gt; @@ -6,5 +6,75 @@</span>
<span class="quote">&gt;&gt;&gt; #ifndef __ASSEMBLY__</span>
<span class="quote">&gt;&gt;&gt; #define arch_mmap_check(addr,len,flags)	sparc_mmap_check(addr,len)</span>
<span class="quote">&gt;&gt;&gt; int sparc_mmap_check(unsigned long addr, unsigned long len);</span>
<span class="quote">&gt;&gt;&gt; -#endif</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#ifdef CONFIG_SPARC64</span>
<span class="quote">&gt;&gt;&gt; +#include &lt;asm/adi_64.h&gt;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#define arch_calc_vm_prot_bits(prot, pkey) sparc_calc_vm_prot_bits(prot)</span>
<span class="quote">&gt;&gt;&gt; +static inline unsigned long sparc_calc_vm_prot_bits(unsigned long prot)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	if (prot &amp; PROT_ADI) {</span>
<span class="quote">&gt;&gt;&gt; +		struct pt_regs *regs;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +		if (!current-&gt;mm-&gt;context.adi) {</span>
<span class="quote">&gt;&gt;&gt; +			regs = task_pt_regs(current);</span>
<span class="quote">&gt;&gt;&gt; +			regs-&gt;tstate |= TSTATE_MCDE;</span>
<span class="quote">&gt;&gt;&gt; +			current-&gt;mm-&gt;context.adi = true;</span>
<span class="quote">&gt;&gt; If a process is multi-threaded when it enables ADI on some memory for</span>
<span class="quote">&gt;&gt; the first time, TSTATE_MCDE will only be set for the calling thread</span>
<span class="quote">&gt;&gt; and it will not be possible to enable it for the other threads.</span>
<span class="quote">&gt;&gt; One possible way to handle this is to enable TSTATE_MCDE for all user</span>
<span class="quote">&gt;&gt; threads when they are initialized if adi_capable() returns true.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or set TSTATE_MCDE unconditionally here by removing &quot;if (!current-&gt;mm-&gt;context.adi)&quot;?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; +		}</span>
<span class="quote">&gt;&gt;&gt; +		return VM_SPARC_ADI;</span>
<span class="quote">&gt;&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt;&gt; +		return 0;</span>
<span class="quote">&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#define arch_vm_get_page_prot(vm_flags) sparc_vm_get_page_prot(vm_flags)</span>
<span class="quote">&gt;&gt;&gt; +static inline pgprot_t sparc_vm_get_page_prot(unsigned long vm_flags)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	return (vm_flags &amp; VM_SPARC_ADI) ? __pgprot(_PAGE_MCD_4V) : __pgprot(0);</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#define arch_validate_prot(prot, addr) sparc_validate_prot(prot, addr)</span>
<span class="quote">&gt;&gt;&gt; +static inline int sparc_validate_prot(unsigned long prot, unsigned long addr)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	if (prot &amp; ~(PROT_READ | PROT_WRITE | PROT_EXEC | PROT_SEM | PROT_ADI))</span>
<span class="quote">&gt;&gt;&gt; +		return 0;</span>
<span class="quote">&gt;&gt;&gt; +	if (prot &amp; PROT_ADI) {</span>
<span class="quote">&gt;&gt;&gt; +		if (!adi_capable())</span>
<span class="quote">&gt;&gt;&gt; +			return 0;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +		/* ADI tags can not be set on read-only memory, so it makes</span>
<span class="quote">&gt;&gt;&gt; +		 * sense to enable ADI on writable memory only.</span>
<span class="quote">&gt;&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt;&gt; +		if (!(prot &amp; PROT_WRITE))</span>
<span class="quote">&gt;&gt;&gt; +			return 0;</span>
<span class="quote">&gt;&gt; This prevents the use of ADI for the legitimate case where shared memory</span>
<span class="quote">&gt;&gt; is mapped read/write for a master process but mapped read-only for a</span>
<span class="quote">&gt;&gt; client process.  The master process could set the tags and communicate</span>
<span class="quote">&gt;&gt; the expected tag values to the client.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A non-writable mapping can access the shared memory using non-ADI tagged addresses if it does not enable ADI on its mappings, so it is superfluous to even allow enabling ADI. I can remove this if that helps any use cases that wouldn&#39;t work with above condition.</span>

Allowing ADI to be enabled on read-only shared memory leaves the option
open to set up ADI in a way to detect unintended accesses that might
otherwise be missed.
<span class="quote">

&gt; </span>
<span class="quote">&gt;&gt;&gt; +tag_storage_desc_t *alloc_tag_store(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;&gt; +				    struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;&gt; +				    unsigned long addr)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	unsigned char *tags;</span>
<span class="quote">&gt;&gt;&gt; +	unsigned long i, size, max_desc, flags;</span>
<span class="quote">&gt;&gt;&gt; +	tag_storage_desc_t *tag_desc, *open_desc;</span>
<span class="quote">&gt;&gt;&gt; +	unsigned long end_addr, hole_start, hole_end;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	max_desc = PAGE_SIZE/sizeof(tag_storage_desc_t);</span>
<span class="quote">&gt;&gt;&gt; +	open_desc = NULL;</span>
<span class="quote">&gt;&gt;&gt; +	hole_start = 0;</span>
<span class="quote">&gt;&gt;&gt; +	hole_end = ULONG_MAX;</span>
<span class="quote">&gt;&gt;&gt; +	end_addr = addr + PAGE_SIZE - 1;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	/* Check if this vma already has tag storage descriptor</span>
<span class="quote">&gt;&gt;&gt; +	 * allocated for it.</span>
<span class="quote">&gt;&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt;&gt; +	spin_lock_irqsave(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="quote">&gt;&gt;&gt; +	if (mm-&gt;context.tag_store) {</span>
<span class="quote">&gt;&gt;&gt; +		tag_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +		/* Look for a matching entry for this address. While doing</span>
<span class="quote">&gt;&gt;&gt; +		 * that, look for the first open slot as well and find</span>
<span class="quote">&gt;&gt;&gt; +		 * the hole in already allocated range where this request</span>
<span class="quote">&gt;&gt;&gt; +		 * will fit in.</span>
<span class="quote">&gt;&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt;&gt; +		for (i = 0; i &lt; max_desc; i++) {</span>
<span class="quote">&gt;&gt;&gt; +			if (tag_desc-&gt;tag_users == 0) {</span>
<span class="quote">&gt;&gt;&gt; +				if (open_desc == NULL)</span>
<span class="quote">&gt;&gt;&gt; +					open_desc = tag_desc;</span>
<span class="quote">&gt;&gt;&gt; +			} else {</span>
<span class="quote">&gt;&gt;&gt; +				if ((addr &gt;= tag_desc-&gt;start) &amp;&amp;</span>
<span class="quote">&gt;&gt;&gt; +				    (tag_desc-&gt;end &gt;= (addr + PAGE_SIZE - 1))) {</span>
<span class="quote">&gt;&gt;&gt; +					tag_desc-&gt;tag_users++;</span>
<span class="quote">&gt;&gt;&gt; +					goto out;</span>
<span class="quote">&gt;&gt;&gt; +				}</span>
<span class="quote">&gt;&gt;&gt; +			}</span>
<span class="quote">&gt;&gt;&gt; +			if ((tag_desc-&gt;start &gt; end_addr) &amp;&amp;</span>
<span class="quote">&gt;&gt;&gt; +			    (tag_desc-&gt;start &lt; hole_end))</span>
<span class="quote">&gt;&gt;&gt; +				hole_end = tag_desc-&gt;start;</span>
<span class="quote">&gt;&gt;&gt; +			if ((tag_desc-&gt;end &lt; addr) &amp;&amp;</span>
<span class="quote">&gt;&gt;&gt; +			    (tag_desc-&gt;end &gt; hole_start))</span>
<span class="quote">&gt;&gt;&gt; +				hole_start = tag_desc-&gt;end;</span>
<span class="quote">&gt;&gt;&gt; +			tag_desc++;</span>
<span class="quote">&gt;&gt;&gt; +		}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	} else {</span>
<span class="quote">&gt;&gt;&gt; +		size = sizeof(tag_storage_desc_t)*max_desc;</span>
<span class="quote">&gt;&gt;&gt; +		mm-&gt;context.tag_store = kzalloc(size, GFP_NOIO|__GFP_NOWARN);</span>
<span class="quote">&gt;&gt; The spin_lock_irqsave() above means that all but level 15 interrupts</span>
<span class="quote">&gt;&gt; will be disabled when kzalloc() is called.  If kzalloc() can sleep</span>
<span class="quote">&gt;&gt; there&#39;s a risk of deadlock.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I could call kzalloc() with GFP_NOWAIT instead of GFP_NOIO. Would that address the risk of deadlock?</span>

I think so.  It may also mean that allocation failures are likely to be
seen since available memory is low enough to cause swapping in the first
place.
<span class="quote">

&gt; </span>
<span class="quote">&gt;&gt;&gt; +		if (mm-&gt;context.tag_store == NULL) {</span>
<span class="quote">&gt;&gt;&gt; +			tag_desc = NULL;</span>
<span class="quote">&gt;&gt;&gt; +			goto out;</span>
<span class="quote">&gt;&gt;&gt; +		}</span>
<span class="quote">&gt;&gt;&gt; +		tag_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt;&gt;&gt; +		for (i = 0; i &lt; max_desc; i++, tag_desc++)</span>
<span class="quote">&gt;&gt;&gt; +			tag_desc-&gt;tag_users = 0;</span>
<span class="quote">&gt;&gt;&gt; +		open_desc = mm-&gt;context.tag_store;</span>
<span class="quote">&gt;&gt;&gt; +		i = 0;</span>
<span class="quote">&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	/* Check if we ran out of tag storage descriptors */</span>
<span class="quote">&gt;&gt;&gt; +	if (open_desc == NULL) {</span>
<span class="quote">&gt;&gt;&gt; +		tag_desc = NULL;</span>
<span class="quote">&gt;&gt;&gt; +		goto out;</span>
<span class="quote">&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	/* Mark this tag descriptor slot in use and then initialize it */</span>
<span class="quote">&gt;&gt;&gt; +	tag_desc = open_desc;</span>
<span class="quote">&gt;&gt;&gt; +	tag_desc-&gt;tag_users = 1;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	/* Tag storage has not been allocated for this vma and space</span>
<span class="quote">&gt;&gt;&gt; +	 * is available in tag storage descriptor. Since this page is</span>
<span class="quote">&gt;&gt;&gt; +	 * being swapped out, there is high probability subsequent pages</span>
<span class="quote">&gt;&gt;&gt; +	 * in the VMA will be swapped out as well. Allocates pages to</span>
<span class="quote">&gt;&gt;&gt; +	 * store tags for as many pages in this vma as possible but not</span>
<span class="quote">&gt;&gt;&gt; +	 * more than TAG_STORAGE_PAGES. Each byte in tag space holds</span>
<span class="quote">&gt;&gt;&gt; +	 * two ADI tags since each ADI tag is 4 bits. Each ADI tag</span>
<span class="quote">&gt;&gt;&gt; +	 * covers adi_blksize() worth of addresses. Check if the hole is</span>
<span class="quote">&gt;&gt;&gt; +	 * big enough to accommodate full address range for using</span>
<span class="quote">&gt;&gt;&gt; +	 * TAG_STORAGE_PAGES number of tag pages.</span>
<span class="quote">&gt;&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt;&gt; +	size = TAG_STORAGE_PAGES * PAGE_SIZE;</span>
<span class="quote">&gt;&gt;&gt; +	end_addr = addr + (size*2*adi_blksize()) - 1;</span>
<span class="quote">&gt;&gt; Since size &gt; PAGE_SIZE, end_addr could theoretically overflow &gt;</span>
<span class="quote">&gt;&gt;&gt; +	if (hole_end &lt; end_addr) {</span>
<span class="quote">&gt;&gt;&gt; +		/* Available hole is too small on the upper end of</span>
<span class="quote">&gt;&gt;&gt; +		 * address. Can we expand the range towards the lower</span>
<span class="quote">&gt;&gt;&gt; +		 * address and maximize use of this slot?</span>
<span class="quote">&gt;&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt;&gt; +		unsigned long tmp_addr;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +		end_addr = hole_end - 1;</span>
<span class="quote">&gt;&gt;&gt; +		tmp_addr = end_addr - (size*2*adi_blksize()) + 1;</span>
<span class="quote">&gt;&gt; Similarily, tmp_addr may underflow.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I will add checks for these two.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; +		if (tmp_addr &lt; hole_start) {</span>
<span class="quote">&gt;&gt;&gt; +			/* Available hole is restricted on lower address</span>
<span class="quote">&gt;&gt;&gt; +			 * end as well</span>
<span class="quote">&gt;&gt;&gt; +			 */</span>
<span class="quote">&gt;&gt;&gt; +			tmp_addr = hole_start + 1;</span>
<span class="quote">&gt;&gt;&gt; +		}</span>
<span class="quote">&gt;&gt;&gt; +		addr = tmp_addr;</span>
<span class="quote">&gt;&gt;&gt; +		size = (end_addr + 1 - addr)/(2*adi_blksize());</span>
<span class="quote">&gt;&gt;&gt; +		size = (size + (PAGE_SIZE-adi_blksize()))/PAGE_SIZE;</span>
<span class="quote">&gt;&gt;&gt; +		size = size * PAGE_SIZE;</span>
<span class="quote">&gt;&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;&gt; +	tags = kzalloc(size, GFP_NOIO|__GFP_NOWARN);</span>
<span class="quote">&gt;&gt; Potential deadlock due to PIL=14?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Same as above - call kzalloc() with GFP_NOWAIT?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/sparc/kernel/etrap_64.S b/arch/sparc/kernel/etrap_64.S</span>
<span class="quote">&gt;&gt;&gt; index 1276ca2567ba..7be33bf45cff 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/sparc/kernel/etrap_64.S</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/sparc/kernel/etrap_64.S</span>
<span class="quote">&gt;&gt;&gt; @@ -132,7 +132,33 @@ etrap_save:	save	%g2, -STACK_BIAS, %sp</span>
<span class="quote">&gt;&gt;&gt; 		stx	%g6, [%sp + PTREGS_OFF + PT_V9_G6]</span>
<span class="quote">&gt;&gt;&gt; 		stx	%g7, [%sp + PTREGS_OFF + PT_V9_G7]</span>
<span class="quote">&gt;&gt;&gt; 		or	%l7, %l0, %l7</span>
<span class="quote">&gt;&gt;&gt; -		sethi	%hi(TSTATE_TSO | TSTATE_PEF), %l0</span>
<span class="quote">&gt;&gt;&gt; +661:		sethi	%hi(TSTATE_TSO | TSTATE_PEF), %l0</span>
<span class="quote">&gt;&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt;&gt; +		 * If userspace is using ADI, it could potentially pass</span>
<span class="quote">&gt;&gt;&gt; +		 * a pointer with version tag embedded in it. To maintain</span>
<span class="quote">&gt;&gt;&gt; +		 * the ADI security, we must enable PSTATE.mcde. Userspace</span>
<span class="quote">&gt;&gt;&gt; +		 * would have already set TTE.mcd in an earlier call to</span>
<span class="quote">&gt;&gt;&gt; +		 * kernel and set the version tag for the address being</span>
<span class="quote">&gt;&gt;&gt; +		 * dereferenced. Setting PSTATE.mcde would ensure any</span>
<span class="quote">&gt;&gt;&gt; +		 * access to userspace data through a system call honors</span>
<span class="quote">&gt;&gt;&gt; +		 * ADI and does not allow a rogue app to bypass ADI by</span>
<span class="quote">&gt;&gt;&gt; +		 * using system calls. Setting PSTATE.mcde only affects</span>
<span class="quote">&gt;&gt;&gt; +		 * accesses to virtual addresses that have TTE.mcd set.</span>
<span class="quote">&gt;&gt;&gt; +		 * Set PMCDPER to ensure any exceptions caused by ADI</span>
<span class="quote">&gt;&gt;&gt; +		 * version tag mismatch are exposed before system call</span>
<span class="quote">&gt;&gt;&gt; +		 * returns to userspace. Setting PMCDPER affects only</span>
<span class="quote">&gt;&gt;&gt; +		 * writes to virtual addresses that have TTE.mcd set and</span>
<span class="quote">&gt;&gt;&gt; +		 * have a version tag set as well.</span>
<span class="quote">&gt;&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt;&gt; +		.section .sun_m7_1insn_patch, &quot;ax&quot;</span>
<span class="quote">&gt;&gt;&gt; +		.word	661b</span>
<span class="quote">&gt;&gt;&gt; +		sethi	%hi(TSTATE_TSO | TSTATE_PEF | TSTATE_MCDE), %l0</span>
<span class="quote">&gt;&gt;&gt; +		.previous</span>
<span class="quote">&gt;&gt;&gt; +661:		nop</span>
<span class="quote">&gt;&gt;&gt; +		.section .sun_m7_1insn_patch, &quot;ax&quot;</span>
<span class="quote">&gt;&gt;&gt; +		.word	661b</span>
<span class="quote">&gt;&gt;&gt; +		.word 0xaf902001	/* wrpr %g0, 1, %pmcdper */</span>
<span class="quote">&gt;&gt; I commented on this on the last patch series revision.  PMCDPER could be</span>
<span class="quote">&gt;&gt; set once when each CPU is configured rather than every time the kernel</span>
<span class="quote">&gt;&gt; is entered.  Since it&#39;s never cleared, setting it repeatedly unnecessarily</span>
<span class="quote">&gt;&gt; impacts the performance of etrap.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, you did and I thought I had addressed it in that thread:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &quot;&gt;&gt; I considered that possibility. What made me uncomfortable with that is there is no way to prevent a driver/module or future code elsewhere in kernel from clearing PMCDPER with possibly good reason. If that were to happen, setting PMCDPER here ensures kernel will always see consistent behavior with system calls. It does come at a cost. Is that cost unacceptable to ensure consistent behavior?</span>

Any driver/module has the ability to cause problems by writing any
privileged register of its choice.  It would be a bug to clear PMCDPER
and not restore it, and the consequence is that a mismatch detected in
privileged mode would result in a disrupting exception instead of a
precise exception.  Perhaps a warning could be logged if this unexpected
case occurs.

Anthony
<span class="quote">

&gt;&gt; Aren&#39;t you still at risk if the thread relinquishes the CPU while in the kernel and is then rescheduled on a CPU where PMCDPER has erroneously been left cleared?  You may need to save and restore PMCDPER as well as MCDPER on context switch, but I don&#39;t know if that will cover you completely.</span>
<span class="quote">&gt;&gt; &quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I should add setting PMCDPER to 1 in finish_arch_post_lock_switch() to address the possibility you had mentioned.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Also, there are places in rtrap where PSTATE is set before continuing</span>
<span class="quote">&gt;&gt; execution in the kernel.  These should also be patched to set TSTATE_MCDE.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I will find and fix those.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/sparc/kernel/setup_64.c b/arch/sparc/kernel/setup_64.c</span>
<span class="quote">&gt;&gt;&gt; index 422b17880955..a9da205da394 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/sparc/kernel/setup_64.c</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/sparc/kernel/setup_64.c</span>
<span class="quote">&gt;&gt;&gt; @@ -240,6 +240,12 @@ void sun4v_patch_1insn_range(struct sun4v_1insn_patch_entry *start,</span>
<span class="quote">&gt;&gt;&gt; 	}</span>
<span class="quote">&gt;&gt;&gt; }</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; +void sun_m7_patch_1insn_range(struct sun4v_1insn_patch_entry *start,</span>
<span class="quote">&gt;&gt;&gt; +			     struct sun4v_1insn_patch_entry *end)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	sun4v_patch_1insn_range(start, end);</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; void sun4v_patch_2insn_range(struct sun4v_2insn_patch_entry *start,</span>
<span class="quote">&gt;&gt;&gt; 			     struct sun4v_2insn_patch_entry *end)</span>
<span class="quote">&gt;&gt;&gt; {</span>
<span class="quote">&gt;&gt;&gt; @@ -289,9 +295,12 @@ static void __init sun4v_patch(void)</span>
<span class="quote">&gt;&gt;&gt; 	sun4v_patch_2insn_range(&amp;__sun4v_2insn_patch,</span>
<span class="quote">&gt;&gt;&gt; 				&amp;__sun4v_2insn_patch_end);</span>
<span class="quote">&gt;&gt;&gt; 	if (sun4v_chip_type == SUN4V_CHIP_SPARC_M7 ||</span>
<span class="quote">&gt;&gt;&gt; -	    sun4v_chip_type == SUN4V_CHIP_SPARC_SN)</span>
<span class="quote">&gt;&gt;&gt; +	    sun4v_chip_type == SUN4V_CHIP_SPARC_SN) {</span>
<span class="quote">&gt;&gt;&gt; +		sun_m7_patch_1insn_range(&amp;__sun_m7_1insn_patch,</span>
<span class="quote">&gt;&gt;&gt; +					 &amp;__sun_m7_1insn_patch_end);</span>
<span class="quote">&gt;&gt;&gt; 		sun_m7_patch_2insn_range(&amp;__sun_m7_2insn_patch,</span>
<span class="quote">&gt;&gt;&gt; 					 &amp;__sun_m7_2insn_patch_end);</span>
<span class="quote">&gt;&gt; Why not call sun4v_patch_1insn_range() and sun4v_patch_2insn_range()</span>
<span class="quote">&gt;&gt; here instead of adding new functions that just call these functions?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sounds reasonable, I can change that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Khalid</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1167">Pavel Machek</a> - Sept. 4, 2017, 4:25 p.m.</div>
<pre class="content">
Hi!
<span class="quote">
&gt; ADI is a new feature supported on SPARC M7 and newer processors to allow</span>
<span class="quote">&gt; hardware to catch rogue accesses to memory. ADI is supported for data</span>
<span class="quote">&gt; fetches only and not instruction fetches. An app can enable ADI on its</span>
<span class="quote">&gt; data pages, set version tags on them and use versioned addresses to</span>
<span class="quote">&gt; access the data pages. Upper bits of the address contain the version</span>
<span class="quote">&gt; tag. On M7 processors, upper four bits (bits 63-60) contain the version</span>
<span class="quote">&gt; tag. If a rogue app attempts to access ADI enabled data pages, its</span>
<span class="quote">&gt; access is blocked and processor generates an exception. Please see</span>
<span class="quote">&gt; Documentation/sparc/adi.txt for further details.</span>

I&#39;m afraid I still don&#39;t understand what this is meant to prevent.

IOMMU ignores these, so this is not to prevent rogue DMA from doing
bad stuff.

Will gcc be able to compile code that uses these automatically? That
does not sound easy to me. Can libc automatically use this in malloc()
to prevent accessing freed data when buffers are overrun?

Is this for benefit of JITs?

Thanks,

									Pavel
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=129">David Miller</a> - Sept. 5, 2017, 9:44 p.m.</div>
<pre class="content">
<span class="from">From: Pavel Machek &lt;pavel@ucw.cz&gt;</span>
Date: Mon, 4 Sep 2017 18:25:30 +0200
<span class="quote">
&gt; Will gcc be able to compile code that uses these automatically? That</span>
<span class="quote">&gt; does not sound easy to me. Can libc automatically use this in malloc()</span>
<span class="quote">&gt; to prevent accessing freed data when buffers are overrun?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this for benefit of JITs?</span>

Anything that can control mappings and the virtual address used to
access memory can use ADI.

malloc() is of course one such case.  It can map memory with ADI
enabled, and return buffer addresses to malloc() callers with the
proper virtual address bits set to satisfy the ADI key checks.

And by induction anything using malloc() for it&#39;s memory allocation
gets ADI protection as well.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=63231">Khalid Aziz</a> - Sept. 6, 2017, 2:10 p.m.</div>
<pre class="content">
On 09/04/2017 10:25 AM, Pavel Machek wrote:
<span class="quote">&gt; Hi!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; ADI is a new feature supported on SPARC M7 and newer processors to allow</span>
<span class="quote">&gt;&gt; hardware to catch rogue accesses to memory. ADI is supported for data</span>
<span class="quote">&gt;&gt; fetches only and not instruction fetches. An app can enable ADI on its</span>
<span class="quote">&gt;&gt; data pages, set version tags on them and use versioned addresses to</span>
<span class="quote">&gt;&gt; access the data pages. Upper bits of the address contain the version</span>
<span class="quote">&gt;&gt; tag. On M7 processors, upper four bits (bits 63-60) contain the version</span>
<span class="quote">&gt;&gt; tag. If a rogue app attempts to access ADI enabled data pages, its</span>
<span class="quote">&gt;&gt; access is blocked and processor generates an exception. Please see</span>
<span class="quote">&gt;&gt; Documentation/sparc/adi.txt for further details.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m afraid I still don&#39;t understand what this is meant to prevent.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IOMMU ignores these, so this is not to prevent rogue DMA from doing</span>
<span class="quote">&gt; bad stuff.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Will gcc be able to compile code that uses these automatically? That</span>
<span class="quote">&gt; does not sound easy to me. Can libc automatically use this in malloc()</span>
<span class="quote">&gt; to prevent accessing freed data when buffers are overrun?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this for benefit of JITs?</span>
<span class="quote">&gt; </span>

David explained it well. Yes, preventing buffer overflow is one of the 
uses of ADI. Protecting critical data from wild writes caused by 
programming errors is another use. ADI can be used for debugging as well 
during development.

Thanks,
Khalid
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1167">Pavel Machek</a> - Sept. 6, 2017, 10:32 p.m.</div>
<pre class="content">
On Tue 2017-09-05 14:44:56, David Miller wrote:
<span class="quote">&gt; From: Pavel Machek &lt;pavel@ucw.cz&gt;</span>
<span class="quote">&gt; Date: Mon, 4 Sep 2017 18:25:30 +0200</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Will gcc be able to compile code that uses these automatically? That</span>
<span class="quote">&gt; &gt; does not sound easy to me. Can libc automatically use this in malloc()</span>
<span class="quote">&gt; &gt; to prevent accessing freed data when buffers are overrun?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Is this for benefit of JITs?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Anything that can control mappings and the virtual address used to</span>
<span class="quote">&gt; access memory can use ADI.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; malloc() is of course one such case.  It can map memory with ADI</span>
<span class="quote">&gt; enabled, and return buffer addresses to malloc() callers with the</span>
<span class="quote">&gt; proper virtual address bits set to satisfy the ADI key checks.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And by induction anything using malloc() for it&#39;s memory allocation</span>
<span class="quote">&gt; gets ADI protection as well.</span>

I see; that&#39;s actually quite a nice trick.

I guess it does not protect against stack-based overflows, but should
help against heap-based overflows, so it improves security a bit, too.

Nice, thanks for explanation.
									Pavel
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176301">Steven Sistare</a> - Sept. 8, 2017, 12:18 p.m.</div>
<pre class="content">
On 9/6/2017 6:32 PM, Pavel Machek wrote:
<span class="quote">&gt; On Tue 2017-09-05 14:44:56, David Miller wrote:</span>
<span class="quote">&gt;&gt; From: Pavel Machek &lt;pavel@ucw.cz&gt;</span>
<span class="quote">&gt;&gt; Date: Mon, 4 Sep 2017 18:25:30 +0200</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Will gcc be able to compile code that uses these automatically? That</span>
<span class="quote">&gt;&gt;&gt; does not sound easy to me. Can libc automatically use this in malloc()</span>
<span class="quote">&gt;&gt;&gt; to prevent accessing freed data when buffers are overrun?</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Is this for benefit of JITs?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Anything that can control mappings and the virtual address used to</span>
<span class="quote">&gt;&gt; access memory can use ADI.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; malloc() is of course one such case.  It can map memory with ADI</span>
<span class="quote">&gt;&gt; enabled, and return buffer addresses to malloc() callers with the</span>
<span class="quote">&gt;&gt; proper virtual address bits set to satisfy the ADI key checks.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; And by induction anything using malloc() for it&#39;s memory allocation</span>
<span class="quote">&gt;&gt; gets ADI protection as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I see; that&#39;s actually quite a nice trick.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess it does not protect against stack-based overflows, but should</span>
<span class="quote">&gt; help against heap-based overflows, so it improves security a bit, too.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Nice, thanks for explanation.</span>

ADI can also be used to protect the stack.  Modify ADI versions for
a 64B aligned portion of the register save area in the kernel spill
and fill handlers,  and accidental or malicious access to the area 
from userland will trap.  Other data on the stack can be corrupted, 
but one cannot linearly overflow into the next stack frame without 
tripping over the ADI canary.  There are a few other details to handle,
such as setjmp/longjmp and JITs that modify the stack, but that is the gist.  
This is not part of the current patch, but has been implemented on
Solaris.

ADI could protect other data on the stack, but that requires 
compiler code generation changes.

- Steve
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/sparc/adi.txt b/Documentation/sparc/adi.txt</span>
new file mode 100644
<span class="p_header">index 000000000000..383bc65fec1e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/Documentation/sparc/adi.txt</span>
<span class="p_chunk">@@ -0,0 +1,272 @@</span> <span class="p_context"></span>
<span class="p_add">+Application Data Integrity (ADI)</span>
<span class="p_add">+================================</span>
<span class="p_add">+</span>
<span class="p_add">+SPARC M7 processor adds the Application Data Integrity (ADI) feature.</span>
<span class="p_add">+ADI allows a task to set version tags on any subset of its address</span>
<span class="p_add">+space. Once ADI is enabled and version tags are set for ranges of</span>
<span class="p_add">+address space of a task, the processor will compare the tag in pointers</span>
<span class="p_add">+to memory in these ranges to the version set by the application</span>
<span class="p_add">+previously. Access to memory is granted only if the tag in given pointer</span>
<span class="p_add">+matches the tag set by the application. In case of mismatch, processor</span>
<span class="p_add">+raises an exception.</span>
<span class="p_add">+</span>
<span class="p_add">+Following steps must be taken by a task to enable ADI fully:</span>
<span class="p_add">+</span>
<span class="p_add">+1. Set the user mode PSTATE.mcde bit. This acts as master switch for</span>
<span class="p_add">+   the task&#39;s entire address space to enable/disable ADI for the task.</span>
<span class="p_add">+</span>
<span class="p_add">+2. Set TTE.mcd bit on any TLB entries that correspond to the range of</span>
<span class="p_add">+   addresses ADI is being enabled on. MMU checks the version tag only</span>
<span class="p_add">+   on the pages that have TTE.mcd bit set.</span>
<span class="p_add">+</span>
<span class="p_add">+3. Set the version tag for virtual addresses using stxa instruction</span>
<span class="p_add">+   and one of the MCD specific ASIs. Each stxa instruction sets the</span>
<span class="p_add">+   given tag for one ADI block size number of bytes. This step must</span>
<span class="p_add">+   be repeated for entire page to set tags for entire page.</span>
<span class="p_add">+</span>
<span class="p_add">+ADI block size for the platform is provided by the hypervisor to kernel</span>
<span class="p_add">+in machine description tables. Hypervisor also provides the number of</span>
<span class="p_add">+top bits in the virtual address that specify the version tag.  Once</span>
<span class="p_add">+version tag has been set for a memory location, the tag is stored in the</span>
<span class="p_add">+physical memory and the same tag must be present in the ADI version tag</span>
<span class="p_add">+bits of the virtual address being presented to the MMU. For example on</span>
<span class="p_add">+SPARC M7 processor, MMU uses bits 63-60 for version tags and ADI block</span>
<span class="p_add">+size is same as cacheline size which is 64 bytes. A task that sets ADI</span>
<span class="p_add">+version to, say 10, on a range of memory, must access that memory using</span>
<span class="p_add">+virtual addresses that contain 0xa in bits 63-60.</span>
<span class="p_add">+</span>
<span class="p_add">+ADI is enabled on a set of pages using mprotect() with PROT_ADI flag.</span>
<span class="p_add">+When ADI is enabled on a set of pages by a task for the first time,</span>
<span class="p_add">+kernel sets the PSTATE.mcde bit fot the task. Version tags for memory</span>
<span class="p_add">+addresses are set with an stxa instruction on the addresses using</span>
<span class="p_add">+ASI_MCD_PRIMARY or ASI_MCD_ST_BLKINIT_PRIMARY. ADI block size is</span>
<span class="p_add">+provided by the hypervisor to the kernel.  Kernel returns the value of</span>
<span class="p_add">+ADI block size to userspace using auxiliary vector along with other ADI</span>
<span class="p_add">+info. Following auxiliary vectors are provided by the kernel:</span>
<span class="p_add">+</span>
<span class="p_add">+	AT_ADI_BLKSZ	ADI block size. This is the granularity and</span>
<span class="p_add">+			alignment, in bytes, of ADI versioning.</span>
<span class="p_add">+	AT_ADI_NBITS	Number of ADI version bits in the VA</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+IMPORTANT NOTES:</span>
<span class="p_add">+</span>
<span class="p_add">+- Version tag values of 0x0 and 0xf are reserved.</span>
<span class="p_add">+</span>
<span class="p_add">+- Version tags are set on virtual addresses from userspace even though</span>
<span class="p_add">+  tags are stored in physical memory. Tags are set on a physical page</span>
<span class="p_add">+  after it has been allocated to a task and a pte has been created for</span>
<span class="p_add">+  it.</span>
<span class="p_add">+</span>
<span class="p_add">+- When a task frees a memory page it had set version tags on, the page</span>
<span class="p_add">+  goes back to free page pool. When this page is re-allocated to a task,</span>
<span class="p_add">+  kernel clears the page using block initialization ASI which clears the</span>
<span class="p_add">+  version tags as well for the page. If a page allocated to a task is</span>
<span class="p_add">+  freed and allocated back to the same task, old version tags set by the</span>
<span class="p_add">+  task on that page will no longer be present.</span>
<span class="p_add">+</span>
<span class="p_add">+- Kernel does not set any tags for user pages and it is entirely a</span>
<span class="p_add">+  task&#39;s responsibility to set any version tags. Kernel does ensure the</span>
<span class="p_add">+  version tags are preserved if a page is swapped out to the disk and</span>
<span class="p_add">+  swapped back in. It also preserves that version tags if a page is</span>
<span class="p_add">+  migrated.</span>
<span class="p_add">+</span>
<span class="p_add">+- ADI works for any size pages. A userspace task need not be aware of</span>
<span class="p_add">+  page size when using ADI. It can simply select a virtual address</span>
<span class="p_add">+  range, enable ADI on the range using mprotect() and set version tags</span>
<span class="p_add">+  for the entire range. mprotect() ensures range is aligned to page size</span>
<span class="p_add">+  and is a multiple of page size.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+ADI related traps</span>
<span class="p_add">+-----------------</span>
<span class="p_add">+</span>
<span class="p_add">+With ADI enabled, following new traps may occur:</span>
<span class="p_add">+</span>
<span class="p_add">+Disrupting memory corruption</span>
<span class="p_add">+</span>
<span class="p_add">+	When a store accesses a memory localtion that has TTE.mcd=1,</span>
<span class="p_add">+	the task is running with ADI enabled (PSTATE.mcde=1), and the ADI</span>
<span class="p_add">+	tag in the address used (bits 63:60) does not match the tag set on</span>
<span class="p_add">+	the corresponding cacheline, a memory corruption trap occurs. By</span>
<span class="p_add">+	default, it is a disrupting trap and is sent to the hypervisor</span>
<span class="p_add">+	first. Hypervisor creates a sun4v error report and sends a</span>
<span class="p_add">+	resumable error (TT=0x7e) trap to the kernel. The kernel sends</span>
<span class="p_add">+	a SIGSEGV to the task that resulted in this trap with the following</span>
<span class="p_add">+	info:</span>
<span class="p_add">+</span>
<span class="p_add">+		siginfo.si_signo = SIGSEGV;</span>
<span class="p_add">+		siginfo.errno = 0;</span>
<span class="p_add">+		siginfo.si_code = SEGV_ADIDERR;</span>
<span class="p_add">+		siginfo.si_addr = addr; /* PC where first mismatch occurred */</span>
<span class="p_add">+		siginfo.si_trapno = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Precise memory corruption</span>
<span class="p_add">+</span>
<span class="p_add">+	When a store accesses a memory location that has TTE.mcd=1,</span>
<span class="p_add">+	the task is running with ADI enabled (PSTATE.mcde=1), and the ADI</span>
<span class="p_add">+	tag in the address used (bits 63:60) does not match the tag set on</span>
<span class="p_add">+	the corresponding cacheline, a memory corruption trap occurs. If</span>
<span class="p_add">+	MCD precise exception is enabled (MCDPERR=1), a precise</span>
<span class="p_add">+	exception is sent to the kernel with TT=0x1a. The kernel sends</span>
<span class="p_add">+	a SIGSEGV to the task that resulted in this trap with the following</span>
<span class="p_add">+	info:</span>
<span class="p_add">+</span>
<span class="p_add">+		siginfo.si_signo = SIGSEGV;</span>
<span class="p_add">+		siginfo.errno = 0;</span>
<span class="p_add">+		siginfo.si_code = SEGV_ADIPERR;</span>
<span class="p_add">+		siginfo.si_addr = addr;	/* address that caused trap */</span>
<span class="p_add">+		siginfo.si_trapno = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	NOTE: ADI tag mismatch on a load always results in precise trap.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+MCD disabled</span>
<span class="p_add">+</span>
<span class="p_add">+	When a task has not enabled ADI and attempts to set ADI version</span>
<span class="p_add">+	on a memory address, processor sends an MCD disabled trap. This</span>
<span class="p_add">+	trap is handled by hypervisor first and the hypervisor vectors this</span>
<span class="p_add">+	trap through to the kernel as Data Access Exception trap with</span>
<span class="p_add">+	fault type set to 0xa (invalid ASI). When this occurs, the kernel</span>
<span class="p_add">+	sends the task SIGSEGV signal with following info:</span>
<span class="p_add">+</span>
<span class="p_add">+		siginfo.si_signo = SIGSEGV;</span>
<span class="p_add">+		siginfo.errno = 0;</span>
<span class="p_add">+		siginfo.si_code = SEGV_ACCADI;</span>
<span class="p_add">+		siginfo.si_addr = addr;	/* address that caused trap */</span>
<span class="p_add">+		siginfo.si_trapno = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+Sample program to use ADI</span>
<span class="p_add">+-------------------------</span>
<span class="p_add">+</span>
<span class="p_add">+Following sample program is meant to illustrate how to use the ADI</span>
<span class="p_add">+functionality.</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;unistd.h&gt;</span>
<span class="p_add">+#include &lt;stdio.h&gt;</span>
<span class="p_add">+#include &lt;stdlib.h&gt;</span>
<span class="p_add">+#include &lt;elf.h&gt;</span>
<span class="p_add">+#include &lt;sys/ipc.h&gt;</span>
<span class="p_add">+#include &lt;sys/shm.h&gt;</span>
<span class="p_add">+#include &lt;sys/mman.h&gt;</span>
<span class="p_add">+#include &lt;asm/asi.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef AT_ADI_BLKSZ</span>
<span class="p_add">+#define AT_ADI_BLKSZ	48</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#ifndef AT_ADI_NBITS</span>
<span class="p_add">+#define AT_ADI_NBITS	49</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef PROT_ADI</span>
<span class="p_add">+#define PROT_ADI	0x10</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define BUFFER_SIZE     32*1024*1024UL</span>
<span class="p_add">+</span>
<span class="p_add">+main(int argc, char* argv[], char* envp[])</span>
<span class="p_add">+{</span>
<span class="p_add">+        unsigned long i, mcde, adi_blksz, adi_nbits;</span>
<span class="p_add">+        char *shmaddr, *tmp_addr, *end, *veraddr, *clraddr;</span>
<span class="p_add">+        int shmid, version;</span>
<span class="p_add">+	Elf64_auxv_t *auxv;</span>
<span class="p_add">+</span>
<span class="p_add">+	adi_blksz = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while(*envp++ != NULL);</span>
<span class="p_add">+	for (auxv = (Elf64_auxv_t *)envp; auxv-&gt;a_type != AT_NULL; auxv++) {</span>
<span class="p_add">+		switch (auxv-&gt;a_type) {</span>
<span class="p_add">+		case AT_ADI_BLKSZ:</span>
<span class="p_add">+			adi_blksz = auxv-&gt;a_un.a_val;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case AT_ADI_NBITS:</span>
<span class="p_add">+			adi_nbits = auxv-&gt;a_un.a_val;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (adi_blksz == 0) {</span>
<span class="p_add">+		fprintf(stderr, &quot;Oops! ADI is not supported\n&quot;);</span>
<span class="p_add">+		exit(1);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;ADI capabilities:\n&quot;);</span>
<span class="p_add">+	printf(&quot;\tBlock size = %ld\n&quot;, adi_blksz);</span>
<span class="p_add">+	printf(&quot;\tNumber of bits = %ld\n&quot;, adi_nbits);</span>
<span class="p_add">+</span>
<span class="p_add">+        if ((shmid = shmget(2, BUFFER_SIZE,</span>
<span class="p_add">+                                IPC_CREAT | SHM_R | SHM_W)) &lt; 0) {</span>
<span class="p_add">+                perror(&quot;shmget failed&quot;);</span>
<span class="p_add">+                exit(1);</span>
<span class="p_add">+        }</span>
<span class="p_add">+</span>
<span class="p_add">+        shmaddr = shmat(shmid, NULL, 0);</span>
<span class="p_add">+        if (shmaddr == (char *)-1) {</span>
<span class="p_add">+                perror(&quot;shm attach failed&quot;);</span>
<span class="p_add">+                shmctl(shmid, IPC_RMID, NULL);</span>
<span class="p_add">+                exit(1);</span>
<span class="p_add">+        }</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mprotect(shmaddr, BUFFER_SIZE, PROT_READ|PROT_WRITE|PROT_ADI)) {</span>
<span class="p_add">+		perror(&quot;mprotect failed&quot;);</span>
<span class="p_add">+		goto err_out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+        /* Set the ADI version tag on the shm segment</span>
<span class="p_add">+         */</span>
<span class="p_add">+        version = 10;</span>
<span class="p_add">+        tmp_addr = shmaddr;</span>
<span class="p_add">+        end = shmaddr + BUFFER_SIZE;</span>
<span class="p_add">+        while (tmp_addr &lt; end) {</span>
<span class="p_add">+                asm volatile(</span>
<span class="p_add">+                        &quot;stxa %1, [%0]0x90\n\t&quot;</span>
<span class="p_add">+                        :</span>
<span class="p_add">+                        : &quot;r&quot; (tmp_addr), &quot;r&quot; (version));</span>
<span class="p_add">+                tmp_addr += adi_blksz;</span>
<span class="p_add">+        }</span>
<span class="p_add">+	asm volatile(&quot;membar #Sync\n\t&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+        /* Create a versioned address from the normal address by placing</span>
<span class="p_add">+	 * version tag in the upper adi_nbits bits</span>
<span class="p_add">+         */</span>
<span class="p_add">+        tmp_addr = (void *) ((unsigned long)shmaddr &lt;&lt; adi_nbits);</span>
<span class="p_add">+        tmp_addr = (void *) ((unsigned long)tmp_addr &gt;&gt; adi_nbits);</span>
<span class="p_add">+        veraddr = (void *) (((unsigned long)version &lt;&lt; (64-adi_nbits))</span>
<span class="p_add">+                        | (unsigned long)tmp_addr);</span>
<span class="p_add">+</span>
<span class="p_add">+        printf(&quot;Starting the writes:\n&quot;);</span>
<span class="p_add">+        for (i = 0; i &lt; BUFFER_SIZE; i++) {</span>
<span class="p_add">+                veraddr[i] = (char)(i);</span>
<span class="p_add">+                if (!(i % (1024 * 1024)))</span>
<span class="p_add">+                        printf(&quot;.&quot;);</span>
<span class="p_add">+        }</span>
<span class="p_add">+        printf(&quot;\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+        printf(&quot;Verifying data...&quot;);</span>
<span class="p_add">+	fflush(stdout);</span>
<span class="p_add">+        for (i = 0; i &lt; BUFFER_SIZE; i++)</span>
<span class="p_add">+                if (veraddr[i] != (char)i)</span>
<span class="p_add">+                        printf(&quot;\nIndex %lu mismatched\n&quot;, i);</span>
<span class="p_add">+        printf(&quot;Done.\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+        /* Disable ADI and clean up</span>
<span class="p_add">+         */</span>
<span class="p_add">+	if (mprotect(shmaddr, BUFFER_SIZE, PROT_READ|PROT_WRITE)) {</span>
<span class="p_add">+		perror(&quot;mprotect failed&quot;);</span>
<span class="p_add">+		goto err_out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+        if (shmdt((const void *)shmaddr) != 0)</span>
<span class="p_add">+                perror(&quot;Detach failure&quot;);</span>
<span class="p_add">+        shmctl(shmid, IPC_RMID, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+        exit(0);</span>
<span class="p_add">+</span>
<span class="p_add">+err_out:</span>
<span class="p_add">+        if (shmdt((const void *)shmaddr) != 0)</span>
<span class="p_add">+                perror(&quot;Detach failure&quot;);</span>
<span class="p_add">+        shmctl(shmid, IPC_RMID, NULL);</span>
<span class="p_add">+        exit(1);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/sparc/include/asm/mman.h b/arch/sparc/include/asm/mman.h</span>
<span class="p_header">index 59bb5938d852..b799796ad963 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/mman.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/mman.h</span>
<span class="p_chunk">@@ -6,5 +6,75 @@</span> <span class="p_context"></span>
 #ifndef __ASSEMBLY__
 #define arch_mmap_check(addr,len,flags)	sparc_mmap_check(addr,len)
 int sparc_mmap_check(unsigned long addr, unsigned long len);
<span class="p_del">-#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_SPARC64</span>
<span class="p_add">+#include &lt;asm/adi_64.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_calc_vm_prot_bits(prot, pkey) sparc_calc_vm_prot_bits(prot)</span>
<span class="p_add">+static inline unsigned long sparc_calc_vm_prot_bits(unsigned long prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (prot &amp; PROT_ADI) {</span>
<span class="p_add">+		struct pt_regs *regs;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!current-&gt;mm-&gt;context.adi) {</span>
<span class="p_add">+			regs = task_pt_regs(current);</span>
<span class="p_add">+			regs-&gt;tstate |= TSTATE_MCDE;</span>
<span class="p_add">+			current-&gt;mm-&gt;context.adi = true;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		return VM_SPARC_ADI;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_vm_get_page_prot(vm_flags) sparc_vm_get_page_prot(vm_flags)</span>
<span class="p_add">+static inline pgprot_t sparc_vm_get_page_prot(unsigned long vm_flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (vm_flags &amp; VM_SPARC_ADI) ? __pgprot(_PAGE_MCD_4V) : __pgprot(0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_validate_prot(prot, addr) sparc_validate_prot(prot, addr)</span>
<span class="p_add">+static inline int sparc_validate_prot(unsigned long prot, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (prot &amp; ~(PROT_READ | PROT_WRITE | PROT_EXEC | PROT_SEM | PROT_ADI))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	if (prot &amp; PROT_ADI) {</span>
<span class="p_add">+		if (!adi_capable())</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* ADI tags can not be set on read-only memory, so it makes</span>
<span class="p_add">+		 * sense to enable ADI on writable memory only.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!(prot &amp; PROT_WRITE))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (addr) {</span>
<span class="p_add">+			struct vm_area_struct *vma;</span>
<span class="p_add">+</span>
<span class="p_add">+			vma = find_vma(current-&gt;mm, addr);</span>
<span class="p_add">+			if (vma) {</span>
<span class="p_add">+				/* ADI can not be enabled on PFN</span>
<span class="p_add">+				 * mapped pages</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (vma-&gt;vm_flags &amp; (VM_PFNMAP | VM_MIXEDMAP))</span>
<span class="p_add">+					return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+				/* Mergeable pages can become unmergeable</span>
<span class="p_add">+				 * if ADI is enabled on them even if they</span>
<span class="p_add">+				 * have identical data on them. This can be</span>
<span class="p_add">+				 * because ADI enabled pages with identical</span>
<span class="p_add">+				 * data may still not have identical ADI</span>
<span class="p_add">+				 * tags on them. Disallow ADI on mergeable</span>
<span class="p_add">+				 * pages.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (vma-&gt;vm_flags &amp; VM_MERGEABLE)</span>
<span class="p_add">+					return 0;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_SPARC64 */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
 #endif /* __SPARC_MMAN_H__ */
<span class="p_header">diff --git a/arch/sparc/include/asm/mmu_64.h b/arch/sparc/include/asm/mmu_64.h</span>
<span class="p_header">index 83b36a5371ff..a65d51ebe00b 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/mmu_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/mmu_64.h</span>
<span class="p_chunk">@@ -89,6 +89,20 @@</span> <span class="p_context"> struct tsb_config {</span>
 #define MM_NUM_TSBS	1
 #endif
 
<span class="p_add">+/* ADI tags are stored when a page is swapped out and the storage for</span>
<span class="p_add">+ * tags is allocated dynamically. There is a tag storage descriptor</span>
<span class="p_add">+ * associated with each set of tag storage pages. Tag storage descriptors</span>
<span class="p_add">+ * are allocated dynamically. Since kernel will allocate a full page for</span>
<span class="p_add">+ * each tag storage descriptor, we can store up to</span>
<span class="p_add">+ * PAGE_SIZE/sizeof(tag storage descriptor) descriptors on that page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	unsigned long	start;		/* Start address for this tag storage */</span>
<span class="p_add">+	unsigned long	end;		/* Last address for tag storage */</span>
<span class="p_add">+	unsigned char	*tags;		/* Where the tags are */</span>
<span class="p_add">+	unsigned long	tag_users;	/* number of references to descriptor */</span>
<span class="p_add">+} tag_storage_desc_t;</span>
<span class="p_add">+</span>
 typedef struct {
 	spinlock_t		lock;
 	unsigned long		sparc64_ctx_val;
<span class="p_chunk">@@ -96,6 +110,9 @@</span> <span class="p_context"> typedef struct {</span>
 	unsigned long		thp_pte_count;
 	struct tsb_config	tsb_block[MM_NUM_TSBS];
 	struct hv_tsb_descr	tsb_descr[MM_NUM_TSBS];
<span class="p_add">+	bool			adi;</span>
<span class="p_add">+	tag_storage_desc_t	*tag_store;</span>
<span class="p_add">+	spinlock_t		tag_lock;</span>
 } mm_context_t;
 
 #endif /* !__ASSEMBLY__ */
<span class="p_header">diff --git a/arch/sparc/include/asm/mmu_context_64.h b/arch/sparc/include/asm/mmu_context_64.h</span>
<span class="p_header">index 2cddcda4f85f..68de059551f9 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/mmu_context_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/mmu_context_64.h</span>
<span class="p_chunk">@@ -9,6 +9,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm_types.h&gt;
 
 #include &lt;asm/spitfire.h&gt;
<span class="p_add">+#include &lt;asm/adi_64.h&gt;</span>
 #include &lt;asm-generic/mm_hooks.h&gt;
 
 static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)
<span class="p_chunk">@@ -129,6 +130,48 @@</span> <span class="p_context"> static inline void switch_mm(struct mm_struct *old_mm, struct mm_struct *mm, str</span>
 
 #define deactivate_mm(tsk,mm)	do { } while (0)
 #define activate_mm(active_mm, mm) switch_mm(active_mm, mm, NULL)
<span class="p_add">+</span>
<span class="p_add">+#define  __HAVE_ARCH_START_CONTEXT_SWITCH</span>
<span class="p_add">+static inline void arch_start_context_switch(struct task_struct *prev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* Save the current state of MCDPER register for the process</span>
<span class="p_add">+	 * we are switching from</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (adi_capable()) {</span>
<span class="p_add">+		register unsigned long tmp_mcdper;</span>
<span class="p_add">+</span>
<span class="p_add">+		__asm__ __volatile__(</span>
<span class="p_add">+			&quot;.word 0x83438000\n\t&quot;	/* rd  %mcdper, %g1 */</span>
<span class="p_add">+			&quot;mov %%g1, %0\n\t&quot;</span>
<span class="p_add">+			: &quot;=r&quot; (tmp_mcdper)</span>
<span class="p_add">+			:</span>
<span class="p_add">+			: &quot;g1&quot;);</span>
<span class="p_add">+		if (tmp_mcdper)</span>
<span class="p_add">+			set_tsk_thread_flag(prev, TIF_MCDPER);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			clear_tsk_thread_flag(prev, TIF_MCDPER);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define finish_arch_post_lock_switch	finish_arch_post_lock_switch</span>
<span class="p_add">+static inline void finish_arch_post_lock_switch(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* Restore the state of MCDPER register for the new process</span>
<span class="p_add">+	 * just switched to.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (adi_capable()) {</span>
<span class="p_add">+		register unsigned long tmp_mcdper;</span>
<span class="p_add">+</span>
<span class="p_add">+		tmp_mcdper = test_thread_flag(TIF_MCDPER);</span>
<span class="p_add">+		__asm__ __volatile__(</span>
<span class="p_add">+			&quot;mov %0, %%g1\n\t&quot;</span>
<span class="p_add">+			&quot;.word 0x9d800001\n\t&quot;	/* wr %g0, %g1, %mcdper&quot; */</span>
<span class="p_add">+			:</span>
<span class="p_add">+			: &quot;ir&quot; (tmp_mcdper)</span>
<span class="p_add">+			: &quot;g1&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* !(__ASSEMBLY__) */
 
 #endif /* !(__SPARC64_MMU_CONTEXT_H) */
<span class="p_header">diff --git a/arch/sparc/include/asm/page_64.h b/arch/sparc/include/asm/page_64.h</span>
<span class="p_header">index 5961b2d8398a..dc582c5611f8 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/page_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/page_64.h</span>
<span class="p_chunk">@@ -46,6 +46,10 @@</span> <span class="p_context"> struct page;</span>
 void clear_user_page(void *addr, unsigned long vaddr, struct page *page);
 #define copy_page(X,Y)	memcpy((void *)(X), (void *)(Y), PAGE_SIZE)
 void copy_user_page(void *to, void *from, unsigned long vaddr, struct page *topage);
<span class="p_add">+#define __HAVE_ARCH_COPY_USER_HIGHPAGE</span>
<span class="p_add">+struct vm_area_struct;</span>
<span class="p_add">+void copy_user_highpage(struct page *to, struct page *from,</span>
<span class="p_add">+			unsigned long vaddr, struct vm_area_struct *vma);</span>
 
 /* Unlike sparc32, sparc64&#39;s parameter passing API is more
  * sane in that structures which as small enough are passed
<span class="p_header">diff --git a/arch/sparc/include/asm/pgtable_64.h b/arch/sparc/include/asm/pgtable_64.h</span>
<span class="p_header">index af045061f41e..51da342c392d 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -18,6 +18,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/types.h&gt;
 #include &lt;asm/spitfire.h&gt;
 #include &lt;asm/asi.h&gt;
<span class="p_add">+#include &lt;asm/adi.h&gt;</span>
 #include &lt;asm/page.h&gt;
 #include &lt;asm/processor.h&gt;
 
<span class="p_chunk">@@ -570,6 +571,18 @@</span> <span class="p_context"> static inline pte_t pte_mkspecial(pte_t pte)</span>
 	return pte;
 }
 
<span class="p_add">+static inline pte_t pte_mkmcd(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_val(pte) |= _PAGE_MCD_4V;</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mknotmcd(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_val(pte) &amp;= ~_PAGE_MCD_4V;</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline unsigned long pte_young(pte_t pte)
 {
 	unsigned long mask;
<span class="p_chunk">@@ -1001,6 +1014,39 @@</span> <span class="p_context"> int page_in_phys_avail(unsigned long paddr);</span>
 int remap_pfn_range(struct vm_area_struct *, unsigned long, unsigned long,
 		    unsigned long, pgprot_t);
 
<span class="p_add">+void adi_restore_tags(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_add">+		      unsigned long addr, pte_t pte);</span>
<span class="p_add">+</span>
<span class="p_add">+int adi_save_tags(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_add">+		  unsigned long addr, pte_t oldpte);</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_DO_SWAP_PAGE</span>
<span class="p_add">+static inline void arch_do_swap_page(struct mm_struct *mm,</span>
<span class="p_add">+				     struct vm_area_struct *vma,</span>
<span class="p_add">+				     unsigned long addr,</span>
<span class="p_add">+				     pte_t pte, pte_t oldpte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* If this is a new page being mapped in, there can be no</span>
<span class="p_add">+	 * ADI tags stored away for this page. Skip looking for</span>
<span class="p_add">+	 * stored tags</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (pte_none(oldpte))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (adi_state.enabled &amp;&amp; (pte_val(pte) &amp; _PAGE_MCD_4V))</span>
<span class="p_add">+		adi_restore_tags(mm, vma, addr, pte);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_UNMAP_ONE</span>
<span class="p_add">+static inline int arch_unmap_one(struct mm_struct *mm,</span>
<span class="p_add">+				 struct vm_area_struct *vma,</span>
<span class="p_add">+				 unsigned long addr, pte_t oldpte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (adi_state.enabled &amp;&amp; (pte_val(oldpte) &amp; _PAGE_MCD_4V))</span>
<span class="p_add">+		return adi_save_tags(mm, vma, addr, oldpte);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline int io_remap_pfn_range(struct vm_area_struct *vma,
 				     unsigned long from, unsigned long pfn,
 				     unsigned long size, pgprot_t prot)
<span class="p_header">diff --git a/arch/sparc/include/asm/thread_info_64.h b/arch/sparc/include/asm/thread_info_64.h</span>
<span class="p_header">index 38a24f257b85..9c04acb1f9af 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/thread_info_64.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/thread_info_64.h</span>
<span class="p_chunk">@@ -190,7 +190,7 @@</span> <span class="p_context"> register struct thread_info *current_thread_info_reg asm(&quot;g6&quot;);</span>
  *       in using in assembly, else we can&#39;t use the mask as
  *       an immediate value in instructions such as andcc.
  */
<span class="p_del">-/* flag bit 12 is available */</span>
<span class="p_add">+#define TIF_MCDPER		12	/* Precise MCD exception */</span>
 #define TIF_MEMDIE		13	/* is terminating due to OOM killer */
 #define TIF_POLLING_NRFLAG	14
 
<span class="p_header">diff --git a/arch/sparc/include/asm/trap_block.h b/arch/sparc/include/asm/trap_block.h</span>
<span class="p_header">index ec9c04de3664..b283e940671a 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/trap_block.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/trap_block.h</span>
<span class="p_chunk">@@ -72,6 +72,8 @@</span> <span class="p_context"> struct sun4v_1insn_patch_entry {</span>
 };
 extern struct sun4v_1insn_patch_entry __sun4v_1insn_patch,
 	__sun4v_1insn_patch_end;
<span class="p_add">+extern struct sun4v_1insn_patch_entry __sun_m7_1insn_patch,</span>
<span class="p_add">+	__sun_m7_1insn_patch_end;</span>
 
 struct sun4v_2insn_patch_entry {
 	unsigned int	addr;
<span class="p_header">diff --git a/arch/sparc/include/uapi/asm/mman.h b/arch/sparc/include/uapi/asm/mman.h</span>
<span class="p_header">index 9765896ecb2c..a72c03397345 100644</span>
<span class="p_header">--- a/arch/sparc/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/sparc/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -5,6 +5,8 @@</span> <span class="p_context"></span>
 
 /* SunOS&#39;ified... */
 
<span class="p_add">+#define PROT_ADI	0x10		/* ADI enabled */</span>
<span class="p_add">+</span>
 #define MAP_RENAME      MAP_ANONYMOUS   /* In SunOS terminology */
 #define MAP_NORESERVE   0x40            /* don&#39;t reserve swap pages */
 #define MAP_INHERIT     0x80            /* SunOS doesn&#39;t do this, but... */
<span class="p_header">diff --git a/arch/sparc/kernel/adi_64.c b/arch/sparc/kernel/adi_64.c</span>
<span class="p_header">index 9fbb5dd4a7bf..83c1e36ae5fa 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/adi_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/adi_64.c</span>
<span class="p_chunk">@@ -7,10 +7,24 @@</span> <span class="p_context"></span>
  * This work is licensed under the terms of the GNU GPL, version 2.
  */
 #include &lt;linux/init.h&gt;
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm_types.h&gt;</span>
 #include &lt;asm/mdesc.h&gt;
 #include &lt;asm/adi_64.h&gt;
<span class="p_add">+#include &lt;asm/mmu_64.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable_64.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Each page of storage for ADI tags can accommodate tags for 128</span>
<span class="p_add">+ * pages. When ADI enabled pages are being swapped out, it would be</span>
<span class="p_add">+ * prudent to allocate at least enough tag storage space to accommodate</span>
<span class="p_add">+ * SWAPFILE_CLUSTER number of pages. Allocate enough tag storage to</span>
<span class="p_add">+ * store tags for four SWAPFILE_CLUSTER pages to reduce need for</span>
<span class="p_add">+ * further allocations for same vma.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define TAG_STORAGE_PAGES	8</span>
 
 struct adi_config adi_state;
<span class="p_add">+EXPORT_SYMBOL(adi_state);</span>
 
 /* mdesc_adi_init() : Parse machine description provided by the
  *	hypervisor to detect ADI capabilities
<span class="p_chunk">@@ -78,6 +92,19 @@</span> <span class="p_context"> void __init mdesc_adi_init(void)</span>
 		goto adi_not_found;
 	adi_state.caps.nbits = *val;
 
<span class="p_add">+	/* Some of the code to support swapping ADI tags is written</span>
<span class="p_add">+	 * assumption that two ADI tags can fit inside one byte. If</span>
<span class="p_add">+	 * this assumption is broken by a future architecture change,</span>
<span class="p_add">+	 * that code will have to be revisited. If that were to happen,</span>
<span class="p_add">+	 * disable ADI support so we do not get unpredictable results</span>
<span class="p_add">+	 * with programs trying to use ADI and their pages getting</span>
<span class="p_add">+	 * swapped out</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (adi_state.caps.nbits &gt; 4) {</span>
<span class="p_add">+		pr_warn(&quot;WARNING: ADI tag size &gt;4 on this platform. Disabling AADI support\n&quot;);</span>
<span class="p_add">+		adi_state.enabled = false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	mdesc_release(hp);
 	return;
 
<span class="p_chunk">@@ -88,3 +115,253 @@</span> <span class="p_context"> void __init mdesc_adi_init(void)</span>
 	if (hp)
 		mdesc_release(hp);
 }
<span class="p_add">+</span>
<span class="p_add">+tag_storage_desc_t *find_tag_store(struct mm_struct *mm,</span>
<span class="p_add">+				   struct vm_area_struct *vma,</span>
<span class="p_add">+				   unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	tag_storage_desc_t *tag_desc = NULL;</span>
<span class="p_add">+	unsigned long i, max_desc, flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check if this vma already has tag storage descriptor</span>
<span class="p_add">+	 * allocated for it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	max_desc = PAGE_SIZE/sizeof(tag_storage_desc_t);</span>
<span class="p_add">+	if (mm-&gt;context.tag_store) {</span>
<span class="p_add">+		tag_desc = mm-&gt;context.tag_store;</span>
<span class="p_add">+		spin_lock_irqsave(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="p_add">+		for (i = 0; i &lt; max_desc; i++) {</span>
<span class="p_add">+			if ((addr &gt;= tag_desc-&gt;start) &amp;&amp;</span>
<span class="p_add">+			    ((addr + PAGE_SIZE - 1) &lt;= tag_desc-&gt;end))</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			tag_desc++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		spin_unlock_irqrestore(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* If no matching entries were found, this must be a</span>
<span class="p_add">+		 * freshly allocated page</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (i &gt;= max_desc)</span>
<span class="p_add">+			tag_desc = NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return tag_desc;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+tag_storage_desc_t *alloc_tag_store(struct mm_struct *mm,</span>
<span class="p_add">+				    struct vm_area_struct *vma,</span>
<span class="p_add">+				    unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned char *tags;</span>
<span class="p_add">+	unsigned long i, size, max_desc, flags;</span>
<span class="p_add">+	tag_storage_desc_t *tag_desc, *open_desc;</span>
<span class="p_add">+	unsigned long end_addr, hole_start, hole_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	max_desc = PAGE_SIZE/sizeof(tag_storage_desc_t);</span>
<span class="p_add">+	open_desc = NULL;</span>
<span class="p_add">+	hole_start = 0;</span>
<span class="p_add">+	hole_end = ULONG_MAX;</span>
<span class="p_add">+	end_addr = addr + PAGE_SIZE - 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check if this vma already has tag storage descriptor</span>
<span class="p_add">+	 * allocated for it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock_irqsave(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="p_add">+	if (mm-&gt;context.tag_store) {</span>
<span class="p_add">+		tag_desc = mm-&gt;context.tag_store;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Look for a matching entry for this address. While doing</span>
<span class="p_add">+		 * that, look for the first open slot as well and find</span>
<span class="p_add">+		 * the hole in already allocated range where this request</span>
<span class="p_add">+		 * will fit in.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		for (i = 0; i &lt; max_desc; i++) {</span>
<span class="p_add">+			if (tag_desc-&gt;tag_users == 0) {</span>
<span class="p_add">+				if (open_desc == NULL)</span>
<span class="p_add">+					open_desc = tag_desc;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				if ((addr &gt;= tag_desc-&gt;start) &amp;&amp;</span>
<span class="p_add">+				    (tag_desc-&gt;end &gt;= (addr + PAGE_SIZE - 1))) {</span>
<span class="p_add">+					tag_desc-&gt;tag_users++;</span>
<span class="p_add">+					goto out;</span>
<span class="p_add">+				}</span>
<span class="p_add">+			}</span>
<span class="p_add">+			if ((tag_desc-&gt;start &gt; end_addr) &amp;&amp;</span>
<span class="p_add">+			    (tag_desc-&gt;start &lt; hole_end))</span>
<span class="p_add">+				hole_end = tag_desc-&gt;start;</span>
<span class="p_add">+			if ((tag_desc-&gt;end &lt; addr) &amp;&amp;</span>
<span class="p_add">+			    (tag_desc-&gt;end &gt; hole_start))</span>
<span class="p_add">+				hole_start = tag_desc-&gt;end;</span>
<span class="p_add">+			tag_desc++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		size = sizeof(tag_storage_desc_t)*max_desc;</span>
<span class="p_add">+		mm-&gt;context.tag_store = kzalloc(size, GFP_NOIO|__GFP_NOWARN);</span>
<span class="p_add">+		if (mm-&gt;context.tag_store == NULL) {</span>
<span class="p_add">+			tag_desc = NULL;</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		tag_desc = mm-&gt;context.tag_store;</span>
<span class="p_add">+		for (i = 0; i &lt; max_desc; i++, tag_desc++)</span>
<span class="p_add">+			tag_desc-&gt;tag_users = 0;</span>
<span class="p_add">+		open_desc = mm-&gt;context.tag_store;</span>
<span class="p_add">+		i = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check if we ran out of tag storage descriptors */</span>
<span class="p_add">+	if (open_desc == NULL) {</span>
<span class="p_add">+		tag_desc = NULL;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Mark this tag descriptor slot in use and then initialize it */</span>
<span class="p_add">+	tag_desc = open_desc;</span>
<span class="p_add">+	tag_desc-&gt;tag_users = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Tag storage has not been allocated for this vma and space</span>
<span class="p_add">+	 * is available in tag storage descriptor. Since this page is</span>
<span class="p_add">+	 * being swapped out, there is high probability subsequent pages</span>
<span class="p_add">+	 * in the VMA will be swapped out as well. Allocates pages to</span>
<span class="p_add">+	 * store tags for as many pages in this vma as possible but not</span>
<span class="p_add">+	 * more than TAG_STORAGE_PAGES. Each byte in tag space holds</span>
<span class="p_add">+	 * two ADI tags since each ADI tag is 4 bits. Each ADI tag</span>
<span class="p_add">+	 * covers adi_blksize() worth of addresses. Check if the hole is</span>
<span class="p_add">+	 * big enough to accommodate full address range for using</span>
<span class="p_add">+	 * TAG_STORAGE_PAGES number of tag pages.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	size = TAG_STORAGE_PAGES * PAGE_SIZE;</span>
<span class="p_add">+	end_addr = addr + (size*2*adi_blksize()) - 1;</span>
<span class="p_add">+	if (hole_end &lt; end_addr) {</span>
<span class="p_add">+		/* Available hole is too small on the upper end of</span>
<span class="p_add">+		 * address. Can we expand the range towards the lower</span>
<span class="p_add">+		 * address and maximize use of this slot?</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		unsigned long tmp_addr;</span>
<span class="p_add">+</span>
<span class="p_add">+		end_addr = hole_end - 1;</span>
<span class="p_add">+		tmp_addr = end_addr - (size*2*adi_blksize()) + 1;</span>
<span class="p_add">+		if (tmp_addr &lt; hole_start) {</span>
<span class="p_add">+			/* Available hole is restricted on lower address</span>
<span class="p_add">+			 * end as well</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			tmp_addr = hole_start + 1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		addr = tmp_addr;</span>
<span class="p_add">+		size = (end_addr + 1 - addr)/(2*adi_blksize());</span>
<span class="p_add">+		size = (size + (PAGE_SIZE-adi_blksize()))/PAGE_SIZE;</span>
<span class="p_add">+		size = size * PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	tags = kzalloc(size, GFP_NOIO|__GFP_NOWARN);</span>
<span class="p_add">+	if (tags == NULL) {</span>
<span class="p_add">+		tag_desc-&gt;tag_users = 0;</span>
<span class="p_add">+		tag_desc = NULL;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	tag_desc-&gt;start = addr;</span>
<span class="p_add">+	tag_desc-&gt;tags = tags;</span>
<span class="p_add">+	tag_desc-&gt;end = end_addr;</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="p_add">+	return tag_desc;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void del_tag_store(tag_storage_desc_t *tag_desc, struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	unsigned char *tags = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="p_add">+	tag_desc-&gt;tag_users--;</span>
<span class="p_add">+	if (tag_desc-&gt;tag_users == 0) {</span>
<span class="p_add">+		tag_desc-&gt;start = tag_desc-&gt;end = 0;</span>
<span class="p_add">+		/* Do not free up the tag storage space allocated</span>
<span class="p_add">+		 * by the first descriptor. This is persistent</span>
<span class="p_add">+		 * emergency tag storage space for the task.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (tag_desc != mm-&gt;context.tag_store) {</span>
<span class="p_add">+			tags = tag_desc-&gt;tags;</span>
<span class="p_add">+			tag_desc-&gt;tags = NULL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;mm-&gt;context.tag_lock, flags);</span>
<span class="p_add">+	kfree(tags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define tag_start(addr, tag_desc)		\</span>
<span class="p_add">+	((tag_desc)-&gt;tags + ((addr - (tag_desc)-&gt;start)/(2*adi_blksize())))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Retrieve any saved ADI tags for the page being swapped back in and</span>
<span class="p_add">+ * restore these tags to the newly allocated physical page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void adi_restore_tags(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_add">+		      unsigned long addr, pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned char *tag;</span>
<span class="p_add">+	tag_storage_desc_t *tag_desc;</span>
<span class="p_add">+	unsigned long paddr, tmp, version1, version2;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check if the swapped out page has an ADI version</span>
<span class="p_add">+	 * saved. If yes, restore version tag to the newly</span>
<span class="p_add">+	 * allocated page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	tag_desc = find_tag_store(mm, vma, addr);</span>
<span class="p_add">+	if (tag_desc == NULL)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	tag = tag_start(addr, tag_desc);</span>
<span class="p_add">+	paddr = pte_val(pte) &amp; _PAGE_PADDR_4V;</span>
<span class="p_add">+	for (tmp = paddr; tmp &lt; (paddr+PAGE_SIZE); tmp += adi_blksize()) {</span>
<span class="p_add">+		version1 = (*tag) &gt;&gt; 4;</span>
<span class="p_add">+		version2 = (*tag) &amp; 0x0f;</span>
<span class="p_add">+		*tag++ = 0;</span>
<span class="p_add">+		asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="p_add">+			:</span>
<span class="p_add">+			: &quot;r&quot; (version1), &quot;r&quot; (tmp),</span>
<span class="p_add">+			  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="p_add">+		tmp += adi_blksize();</span>
<span class="p_add">+		asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="p_add">+			:</span>
<span class="p_add">+			: &quot;r&quot; (version2), &quot;r&quot; (tmp),</span>
<span class="p_add">+			  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="p_add">+	}</span>
<span class="p_add">+	asm volatile(&quot;membar #Sync\n\t&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check and mark this tag space for release later if</span>
<span class="p_add">+	 * the swapped in page was the last user of tag space</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	del_tag_store(tag_desc, mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* A page is about to be swapped out. Save any ADI tags associated with</span>
<span class="p_add">+ * this physical page so they can be restored later when the page is swapped</span>
<span class="p_add">+ * back in.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int adi_save_tags(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="p_add">+		  unsigned long addr, pte_t oldpte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned char *tag;</span>
<span class="p_add">+	tag_storage_desc_t *tag_desc;</span>
<span class="p_add">+	unsigned long version1, version2, paddr, tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	tag_desc = alloc_tag_store(mm, vma, addr);</span>
<span class="p_add">+	if (tag_desc == NULL)</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	tag = tag_start(addr, tag_desc);</span>
<span class="p_add">+	paddr = pte_val(oldpte) &amp; _PAGE_PADDR_4V;</span>
<span class="p_add">+	for (tmp = paddr; tmp &lt; (paddr+PAGE_SIZE); tmp += adi_blksize()) {</span>
<span class="p_add">+		asm volatile(&quot;ldxa [%1] %2, %0\n\t&quot;</span>
<span class="p_add">+				: &quot;=r&quot; (version1)</span>
<span class="p_add">+				: &quot;r&quot; (tmp), &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="p_add">+		tmp += adi_blksize();</span>
<span class="p_add">+		asm volatile(&quot;ldxa [%1] %2, %0\n\t&quot;</span>
<span class="p_add">+				: &quot;=r&quot; (version2)</span>
<span class="p_add">+				: &quot;r&quot; (tmp), &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="p_add">+		*tag = (version1 &lt;&lt; 4) | version2;</span>
<span class="p_add">+		tag++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/sparc/kernel/etrap_64.S b/arch/sparc/kernel/etrap_64.S</span>
<span class="p_header">index 1276ca2567ba..7be33bf45cff 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/etrap_64.S</span>
<span class="p_header">+++ b/arch/sparc/kernel/etrap_64.S</span>
<span class="p_chunk">@@ -132,7 +132,33 @@</span> <span class="p_context"> etrap_save:	save	%g2, -STACK_BIAS, %sp</span>
 		stx	%g6, [%sp + PTREGS_OFF + PT_V9_G6]
 		stx	%g7, [%sp + PTREGS_OFF + PT_V9_G7]
 		or	%l7, %l0, %l7
<span class="p_del">-		sethi	%hi(TSTATE_TSO | TSTATE_PEF), %l0</span>
<span class="p_add">+661:		sethi	%hi(TSTATE_TSO | TSTATE_PEF), %l0</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If userspace is using ADI, it could potentially pass</span>
<span class="p_add">+		 * a pointer with version tag embedded in it. To maintain</span>
<span class="p_add">+		 * the ADI security, we must enable PSTATE.mcde. Userspace</span>
<span class="p_add">+		 * would have already set TTE.mcd in an earlier call to</span>
<span class="p_add">+		 * kernel and set the version tag for the address being</span>
<span class="p_add">+		 * dereferenced. Setting PSTATE.mcde would ensure any</span>
<span class="p_add">+		 * access to userspace data through a system call honors</span>
<span class="p_add">+		 * ADI and does not allow a rogue app to bypass ADI by</span>
<span class="p_add">+		 * using system calls. Setting PSTATE.mcde only affects</span>
<span class="p_add">+		 * accesses to virtual addresses that have TTE.mcd set.</span>
<span class="p_add">+		 * Set PMCDPER to ensure any exceptions caused by ADI</span>
<span class="p_add">+		 * version tag mismatch are exposed before system call</span>
<span class="p_add">+		 * returns to userspace. Setting PMCDPER affects only</span>
<span class="p_add">+		 * writes to virtual addresses that have TTE.mcd set and</span>
<span class="p_add">+		 * have a version tag set as well.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		.section .sun_m7_1insn_patch, &quot;ax&quot;</span>
<span class="p_add">+		.word	661b</span>
<span class="p_add">+		sethi	%hi(TSTATE_TSO | TSTATE_PEF | TSTATE_MCDE), %l0</span>
<span class="p_add">+		.previous</span>
<span class="p_add">+661:		nop</span>
<span class="p_add">+		.section .sun_m7_1insn_patch, &quot;ax&quot;</span>
<span class="p_add">+		.word	661b</span>
<span class="p_add">+		.word 0xaf902001	/* wrpr %g0, 1, %pmcdper */</span>
<span class="p_add">+		.previous</span>
 		or	%l7, %l0, %l7
 		wrpr	%l2, %tnpc
 		wrpr	%l7, (TSTATE_PRIV | TSTATE_IE), %tstate
<span class="p_header">diff --git a/arch/sparc/kernel/process_64.c b/arch/sparc/kernel/process_64.c</span>
<span class="p_header">index b96104da5bd6..defa5723dfa6 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/process_64.c</span>
<span class="p_chunk">@@ -664,6 +664,31 @@</span> <span class="p_context"> int copy_thread(unsigned long clone_flags, unsigned long sp,</span>
 	return 0;
 }
 
<span class="p_add">+/* TIF_MCDPER in thread info flags for current task is updated lazily upon</span>
<span class="p_add">+ * a context switch. Update the this flag in current task&#39;s thread flags</span>
<span class="p_add">+ * before dup so the dup&#39;d task will inherit the current TIF_MCDPER flag.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int arch_dup_task_struct(struct task_struct *dst, struct task_struct *src)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (adi_capable()) {</span>
<span class="p_add">+		register unsigned long tmp_mcdper;</span>
<span class="p_add">+</span>
<span class="p_add">+		__asm__ __volatile__(</span>
<span class="p_add">+			&quot;.word 0x83438000\n\t&quot;	/* rd  %mcdper, %g1 */</span>
<span class="p_add">+			&quot;mov %%g1, %0\n\t&quot;</span>
<span class="p_add">+			: &quot;=r&quot; (tmp_mcdper)</span>
<span class="p_add">+			:</span>
<span class="p_add">+			: &quot;g1&quot;);</span>
<span class="p_add">+		if (tmp_mcdper)</span>
<span class="p_add">+			set_thread_flag(TIF_MCDPER);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			clear_thread_flag(TIF_MCDPER);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	*dst = *src;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 typedef struct {
 	union {
 		unsigned int	pr_regs[32];
<span class="p_header">diff --git a/arch/sparc/kernel/setup_64.c b/arch/sparc/kernel/setup_64.c</span>
<span class="p_header">index 422b17880955..a9da205da394 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/setup_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/setup_64.c</span>
<span class="p_chunk">@@ -240,6 +240,12 @@</span> <span class="p_context"> void sun4v_patch_1insn_range(struct sun4v_1insn_patch_entry *start,</span>
 	}
 }
 
<span class="p_add">+void sun_m7_patch_1insn_range(struct sun4v_1insn_patch_entry *start,</span>
<span class="p_add">+			     struct sun4v_1insn_patch_entry *end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	sun4v_patch_1insn_range(start, end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void sun4v_patch_2insn_range(struct sun4v_2insn_patch_entry *start,
 			     struct sun4v_2insn_patch_entry *end)
 {
<span class="p_chunk">@@ -289,9 +295,12 @@</span> <span class="p_context"> static void __init sun4v_patch(void)</span>
 	sun4v_patch_2insn_range(&amp;__sun4v_2insn_patch,
 				&amp;__sun4v_2insn_patch_end);
 	if (sun4v_chip_type == SUN4V_CHIP_SPARC_M7 ||
<span class="p_del">-	    sun4v_chip_type == SUN4V_CHIP_SPARC_SN)</span>
<span class="p_add">+	    sun4v_chip_type == SUN4V_CHIP_SPARC_SN) {</span>
<span class="p_add">+		sun_m7_patch_1insn_range(&amp;__sun_m7_1insn_patch,</span>
<span class="p_add">+					 &amp;__sun_m7_1insn_patch_end);</span>
 		sun_m7_patch_2insn_range(&amp;__sun_m7_2insn_patch,
 					 &amp;__sun_m7_2insn_patch_end);
<span class="p_add">+		}</span>
 
 	sun4v_hvapi_init();
 }
<span class="p_header">diff --git a/arch/sparc/kernel/vmlinux.lds.S b/arch/sparc/kernel/vmlinux.lds.S</span>
<span class="p_header">index 572db686f845..20a70682cce7 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/vmlinux.lds.S</span>
<span class="p_header">+++ b/arch/sparc/kernel/vmlinux.lds.S</span>
<span class="p_chunk">@@ -144,6 +144,11 @@</span> <span class="p_context"> SECTIONS</span>
 		*(.pause_3insn_patch)
 		__pause_3insn_patch_end = .;
 	}
<span class="p_add">+	.sun_m7_1insn_patch : {</span>
<span class="p_add">+		__sun_m7_1insn_patch = .;</span>
<span class="p_add">+		*(.sun_m7_1insn_patch)</span>
<span class="p_add">+		__sun_m7_1insn_patch_end = .;</span>
<span class="p_add">+	}</span>
 	.sun_m7_2insn_patch : {
 		__sun_m7_2insn_patch = .;
 		*(.sun_m7_2insn_patch)
<span class="p_header">diff --git a/arch/sparc/mm/gup.c b/arch/sparc/mm/gup.c</span>
<span class="p_header">index cd0e32bbcb1d..579f7ae75b35 100644</span>
<span class="p_header">--- a/arch/sparc/mm/gup.c</span>
<span class="p_header">+++ b/arch/sparc/mm/gup.c</span>
<span class="p_chunk">@@ -11,6 +11,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/pagemap.h&gt;
 #include &lt;linux/rwsem.h&gt;
 #include &lt;asm/pgtable.h&gt;
<span class="p_add">+#include &lt;asm/adi.h&gt;</span>
 
 /*
  * The performance critical leaf functions are made noinline otherwise gcc
<span class="p_chunk">@@ -157,6 +158,24 @@</span> <span class="p_context"> int __get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
 	pgd_t *pgdp;
 	int nr = 0;
 
<span class="p_add">+#ifdef CONFIG_SPARC64</span>
<span class="p_add">+	if (adi_capable()) {</span>
<span class="p_add">+		long addr = start;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* If userspace has passed a versioned address, kernel</span>
<span class="p_add">+		 * will not find it in the VMAs since it does not store</span>
<span class="p_add">+		 * the version tags in the list of VMAs. Storing version</span>
<span class="p_add">+		 * tags in list of VMAs is impractical since they can be</span>
<span class="p_add">+		 * changed any time from userspace without dropping into</span>
<span class="p_add">+		 * kernel. Any address search in VMAs will be done with</span>
<span class="p_add">+		 * non-versioned addresses. Ensure the ADI version bits</span>
<span class="p_add">+		 * are dropped here by sign extending the last bit before</span>
<span class="p_add">+		 * ADI bits. IOMMU does not implement version tags.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		addr = (addr &lt;&lt; (long)adi_nbits()) &gt;&gt; (long)adi_nbits();</span>
<span class="p_add">+		start = addr;</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
 	start &amp;= PAGE_MASK;
 	addr = start;
 	len = (unsigned long) nr_pages &lt;&lt; PAGE_SHIFT;
<span class="p_chunk">@@ -187,6 +206,24 @@</span> <span class="p_context"> int get_user_pages_fast(unsigned long start, int nr_pages, int write,</span>
 	pgd_t *pgdp;
 	int nr = 0;
 
<span class="p_add">+#ifdef CONFIG_SPARC64</span>
<span class="p_add">+	if (adi_capable()) {</span>
<span class="p_add">+		long addr = start;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* If userspace has passed a versioned address, kernel</span>
<span class="p_add">+		 * will not find it in the VMAs since it does not store</span>
<span class="p_add">+		 * the version tags in the list of VMAs. Storing version</span>
<span class="p_add">+		 * tags in list of VMAs is impractical since they can be</span>
<span class="p_add">+		 * changed any time from userspace without dropping into</span>
<span class="p_add">+		 * kernel. Any address search in VMAs will be done with</span>
<span class="p_add">+		 * non-versioned addresses. Ensure the ADI version bits</span>
<span class="p_add">+		 * are dropped here by sign extending the last bit before</span>
<span class="p_add">+		 * ADI bits. IOMMU does not implements version tags,</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		addr = (addr &lt;&lt; (long)adi_nbits()) &gt;&gt; (long)adi_nbits();</span>
<span class="p_add">+		start = addr;</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
 	start &amp;= PAGE_MASK;
 	addr = start;
 	len = (unsigned long) nr_pages &lt;&lt; PAGE_SHIFT;
<span class="p_header">diff --git a/arch/sparc/mm/hugetlbpage.c b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">index 88855e383b34..487ed1f1ce86 100644</span>
<span class="p_header">--- a/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/sparc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -177,8 +177,20 @@</span> <span class="p_context"> pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,</span>
 			 struct page *page, int writeable)
 {
 	unsigned int shift = huge_page_shift(hstate_vma(vma));
<span class="p_add">+	pte_t pte;</span>
 
<span class="p_del">-	return hugepage_shift_to_tte(entry, shift);</span>
<span class="p_add">+	pte = hugepage_shift_to_tte(entry, shift);</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_SPARC64</span>
<span class="p_add">+	/* If this vma has ADI enabled on it, turn on TTE.mcd</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_SPARC_ADI)</span>
<span class="p_add">+		return pte_mkmcd(pte);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return pte_mknotmcd(pte);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+#endif</span>
 }
 
 static unsigned int sun4v_huge_tte_to_shift(pte_t entry)
<span class="p_header">diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c</span>
<span class="p_header">index 3c40ebd50f92..94854e7e833e 100644</span>
<span class="p_header">--- a/arch/sparc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/init_64.c</span>
<span class="p_chunk">@@ -3087,3 +3087,36 @@</span> <span class="p_context"> void flush_tlb_kernel_range(unsigned long start, unsigned long end)</span>
 		do_flush_tlb_kernel_range(start, end);
 	}
 }
<span class="p_add">+</span>
<span class="p_add">+void copy_user_highpage(struct page *to, struct page *from,</span>
<span class="p_add">+	unsigned long vaddr, struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char *vfrom, *vto;</span>
<span class="p_add">+</span>
<span class="p_add">+	vfrom = kmap_atomic(from);</span>
<span class="p_add">+	vto = kmap_atomic(to);</span>
<span class="p_add">+	copy_user_page(vto, vfrom, vaddr, to);</span>
<span class="p_add">+	kunmap_atomic(vto);</span>
<span class="p_add">+	kunmap_atomic(vfrom);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If this page has ADI enabled, copy over any ADI tags</span>
<span class="p_add">+	 * as well</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_SPARC_ADI) {</span>
<span class="p_add">+		unsigned long pfrom, pto, i, adi_tag;</span>
<span class="p_add">+</span>
<span class="p_add">+		pfrom = page_to_phys(from);</span>
<span class="p_add">+		pto = page_to_phys(to);</span>
<span class="p_add">+</span>
<span class="p_add">+		for (i = pfrom; i &lt; (pfrom + PAGE_SIZE); i += adi_blksize()) {</span>
<span class="p_add">+			asm volatile(&quot;ldxa [%1] %2, %0\n\t&quot;</span>
<span class="p_add">+					: &quot;=r&quot; (adi_tag)</span>
<span class="p_add">+					:  &quot;r&quot; (i), &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="p_add">+			asm volatile(&quot;stxa %0, [%1] %2\n\t&quot;</span>
<span class="p_add">+					:</span>
<span class="p_add">+					: &quot;r&quot; (adi_tag), &quot;r&quot; (pto),</span>
<span class="p_add">+					  &quot;i&quot; (ASI_MCD_REAL));</span>
<span class="p_add">+			pto += adi_blksize();</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/sparc/mm/tsb.c b/arch/sparc/mm/tsb.c</span>
<span class="p_header">index 0d4b998c7d7b..6518cc42056b 100644</span>
<span class="p_header">--- a/arch/sparc/mm/tsb.c</span>
<span class="p_header">+++ b/arch/sparc/mm/tsb.c</span>
<span class="p_chunk">@@ -545,6 +545,9 @@</span> <span class="p_context"> int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
 
 	mm-&gt;context.sparc64_ctx_val = 0UL;
 
<span class="p_add">+	mm-&gt;context.tag_store = NULL;</span>
<span class="p_add">+	spin_lock_init(&amp;mm-&gt;context.tag_lock);</span>
<span class="p_add">+</span>
 #if defined(CONFIG_HUGETLB_PAGE) || defined(CONFIG_TRANSPARENT_HUGEPAGE)
 	/* We reset them to zero because the fork() page copying
 	 * will re-increment the counters as the parent PTEs are
<span class="p_chunk">@@ -610,4 +613,22 @@</span> <span class="p_context"> void destroy_context(struct mm_struct *mm)</span>
 	}
 
 	spin_unlock_irqrestore(&amp;ctx_alloc_lock, flags);
<span class="p_add">+</span>
<span class="p_add">+	/* If ADI tag storage was allocated for this task, free it */</span>
<span class="p_add">+	if (mm-&gt;context.tag_store) {</span>
<span class="p_add">+		tag_storage_desc_t *tag_desc;</span>
<span class="p_add">+		unsigned long max_desc;</span>
<span class="p_add">+		unsigned char *tags;</span>
<span class="p_add">+</span>
<span class="p_add">+		tag_desc = mm-&gt;context.tag_store;</span>
<span class="p_add">+		max_desc = PAGE_SIZE/sizeof(tag_storage_desc_t);</span>
<span class="p_add">+		for (i = 0; i &lt; max_desc; i++) {</span>
<span class="p_add">+			tags = tag_desc-&gt;tags;</span>
<span class="p_add">+			tag_desc-&gt;tags = NULL;</span>
<span class="p_add">+			kfree(tags);</span>
<span class="p_add">+			tag_desc++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		kfree(mm-&gt;context.tag_store);</span>
<span class="p_add">+		mm-&gt;context.tag_store = NULL;</span>
<span class="p_add">+	}</span>
 }
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index b7aa3932e6d4..c0972114036f 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -231,6 +231,9 @@</span> <span class="p_context"> extern unsigned int kobjsize(const void *objp);</span>
 # define VM_GROWSUP	VM_ARCH_1
 #elif defined(CONFIG_IA64)
 # define VM_GROWSUP	VM_ARCH_1
<span class="p_add">+#elif defined(CONFIG_SPARC64)</span>
<span class="p_add">+# define VM_SPARC_ADI	VM_ARCH_1	/* Uses ADI tag for access control */</span>
<span class="p_add">+# define VM_ARCH_CLEAR	VM_SPARC_ADI</span>
 #elif !defined(CONFIG_MMU)
 # define VM_MAPPED_COPY	VM_ARCH_1	/* T if mapped copy of data (nommu mmap) */
 #endif
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index 216184af0e19..bb82399816ef 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -1797,6 +1797,10 @@</span> <span class="p_context"> int ksm_madvise(struct vm_area_struct *vma, unsigned long start,</span>
 		if (*vm_flags &amp; VM_SAO)
 			return 0;
 #endif
<span class="p_add">+#ifdef VM_SPARC_ADI</span>
<span class="p_add">+		if (*vm_flags &amp; VM_SPARC_ADI)</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+#endif</span>
 
 		if (!test_bit(MMF_VM_MERGEABLE, &amp;mm-&gt;flags)) {
 			err = __ksm_enter(mm);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



