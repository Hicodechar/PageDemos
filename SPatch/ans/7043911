
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/3] mm: rename alloc_pages_exact_node to __alloc_pages_node - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/3] mm: rename alloc_pages_exact_node to __alloc_pages_node</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 20, 2015, 11:43 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1440071002-19085-1-git-send-email-vbabka@suse.cz&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7043911/mbox/"
   >mbox</a>
|
   <a href="/patch/7043911/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7043911/">/patch/7043911/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 25D72C05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Aug 2015 11:44:46 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 413D72057F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Aug 2015 11:44:41 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id CDCEB20574
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Aug 2015 11:44:35 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752137AbbHTLoG (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 20 Aug 2015 07:44:06 -0400
Received: from mx2.suse.de ([195.135.220.15]:59961 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751400AbbHTLoD (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 20 Aug 2015 07:44:03 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay2.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id E1490AC4E;
	Thu, 20 Aug 2015 11:43:58 +0000 (UTC)
From: Vlastimil Babka &lt;vbabka@suse.cz&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	Mel Gorman &lt;mgorman@suse.de&gt;, David Rientjes &lt;rientjes@google.com&gt;,
	Greg Thelen &lt;gthelen@google.com&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Christoph Lameter &lt;cl@linux.com&gt;, Pekka Enberg &lt;penberg@kernel.org&gt;,
	Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	linux-ia64@vger.kernel.org, linuxppc-dev@lists.ozlabs.org,
	cbe-oss-dev@lists.ozlabs.org, kvm@vger.kernel.org,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;, Tony Luck &lt;tony.luck@intel.com&gt;,
	Fenghua Yu &lt;fenghua.yu@intel.com&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;,
	Paul Mackerras &lt;paulus@samba.org&gt;, Gleb Natapov &lt;gleb@kernel.org&gt;,
	Paolo Bonzini &lt;pbonzini@redhat.com&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Cliff Whickman &lt;cpw@sgi.com&gt;,
	Michael Ellerman &lt;mpe@ellerman.id.au&gt;, Robin Holt &lt;robinmholt@gmail.com&gt;
Subject: [PATCH 1/3] mm: rename alloc_pages_exact_node to __alloc_pages_node
Date: Thu, 20 Aug 2015 13:43:20 +0200
Message-Id: &lt;1440071002-19085-1-git-send-email-vbabka@suse.cz&gt;
X-Mailer: git-send-email 2.5.0
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-2.5 required=5.0 tests=BAYES_00,RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Aug. 20, 2015, 11:43 a.m.</div>
<pre class="content">
The function alloc_pages_exact_node() was introduced in 6484eb3e2a81 (&quot;page
allocator: do not check NUMA node ID when the caller knows the node is valid&quot;)
as an optimized variant of alloc_pages_node(), that doesn&#39;t fallback to current
node for nid == NUMA_NO_NODE. Unfortunately the name of the function can easily
suggest that the allocation is restricted to the given node and fails
otherwise. In truth, the node is only preferred, unless __GFP_THISNODE is
passed among the gfp flags.

The misleading name has lead to mistakes in the past, see 5265047ac301 (&quot;mm,
thp: really limit transparent hugepage allocation to local node&quot;) and
b360edb43f8e (&quot;mm, mempolicy: migrate_to_node should only migrate to node&quot;).

Another issue with the name is that there&#39;s a family of alloc_pages_exact*()
functions where &#39;exact&#39; means exact size (instead of page order), which leads
to more confusion.

To prevent further mistakes, this patch effectively renames
alloc_pages_exact_node() to __alloc_pages_node() to better convey that it&#39;s
an optimized variant of alloc_pages_node() not intended for general usage.
Both functions get described in comments.

It has been also considered to really provide a convenience function for
allocations restricted to a node, but the major opinion seems to be that
__GFP_THISNODE already provides that functionality and we shouldn&#39;t duplicate
the API needlessly. The number of users would be small anyway.

Existing callers of alloc_pages_exact_node() are simply converted to call
__alloc_pages_node(), with the exception of sba_alloc_coherent() which
open-codes the check for NUMA_NO_NODE, so it is converted to use
alloc_pages_node() instead. This means it no longer performs some VM_BUG_ON
checks, and since the current check for nid in alloc_pages_node() uses a &#39;nid &lt;
0&#39; comparison (which includes NUMA_NO_NODE), it may hide wrong values which
would be previously exposed. Both differences will be rectified by the next
patch.

To sum up, this patch makes no functional changes, except temporarily hiding
potentially buggy callers. Restricting the checks in alloc_pages_node() is
left for the next patch which can in turn expose more existing buggy callers.
<span class="signed-off-by">
Signed-off-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="acked-by">Acked-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
Cc: Mel Gorman &lt;mgorman@suse.de&gt;
Cc: David Rientjes &lt;rientjes@google.com&gt;
Cc: Greg Thelen &lt;gthelen@google.com&gt;
Cc: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
<span class="acked-by">Acked-by: Christoph Lameter &lt;cl@linux.com&gt;</span>
Cc: Pekka Enberg &lt;penberg@kernel.org&gt;
Cc: Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;
Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;
Cc: Tony Luck &lt;tony.luck@intel.com&gt;
Cc: Fenghua Yu &lt;fenghua.yu@intel.com&gt;
Cc: Arnd Bergmann &lt;arnd@arndb.de&gt;
Cc: Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;
Cc: Paul Mackerras &lt;paulus@samba.org&gt;
<span class="acked-by">Acked-by: Michael Ellerman &lt;mpe@ellerman.id.au&gt;</span>
Cc: Gleb Natapov &lt;gleb@kernel.org&gt;
Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: Ingo Molnar &lt;mingo@redhat.com&gt;
Cc: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Cliff Whickman &lt;cpw@sgi.com&gt;
<span class="acked-by">Acked-by: Robin Holt &lt;robinmholt@gmail.com&gt;</span>
---
 v3: fixed the slob part (Christoph Lameter), removed forgotten hunk from
     huge_memory.c (DavidR)

 arch/ia64/hp/common/sba_iommu.c   |  6 +-----
 arch/ia64/kernel/uncached.c       |  2 +-
 arch/ia64/sn/pci/pci_dma.c        |  2 +-
 arch/powerpc/platforms/cell/ras.c |  2 +-
 arch/x86/kvm/vmx.c                |  2 +-
 drivers/misc/sgi-xp/xpc_uv.c      |  2 +-
 include/linux/gfp.h               | 23 +++++++++++++++--------
 kernel/profile.c                  |  8 ++++----
 mm/filemap.c                      |  2 +-
 mm/huge_memory.c                  |  2 +-
 mm/hugetlb.c                      |  4 ++--
 mm/memory-failure.c               |  2 +-
 mm/mempolicy.c                    |  4 ++--
 mm/migrate.c                      |  4 ++--
 mm/page_alloc.c                   |  2 --
 mm/slab.c                         |  2 +-
 mm/slob.c                         |  4 ++--
 mm/slub.c                         |  2 +-
 18 files changed, 38 insertions(+), 37 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 20, 2015, 2:03 p.m.</div>
<pre class="content">
On Thu 20-08-15 13:43:20, Vlastimil Babka wrote:
<span class="quote">&gt; The function alloc_pages_exact_node() was introduced in 6484eb3e2a81 (&quot;page</span>
<span class="quote">&gt; allocator: do not check NUMA node ID when the caller knows the node is valid&quot;)</span>
<span class="quote">&gt; as an optimized variant of alloc_pages_node(), that doesn&#39;t fallback to current</span>
<span class="quote">&gt; node for nid == NUMA_NO_NODE. Unfortunately the name of the function can easily</span>
<span class="quote">&gt; suggest that the allocation is restricted to the given node and fails</span>
<span class="quote">&gt; otherwise. In truth, the node is only preferred, unless __GFP_THISNODE is</span>
<span class="quote">&gt; passed among the gfp flags.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The misleading name has lead to mistakes in the past, see 5265047ac301 (&quot;mm,</span>
<span class="quote">&gt; thp: really limit transparent hugepage allocation to local node&quot;) and</span>
<span class="quote">&gt; b360edb43f8e (&quot;mm, mempolicy: migrate_to_node should only migrate to node&quot;).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Another issue with the name is that there&#39;s a family of alloc_pages_exact*()</span>
<span class="quote">&gt; functions where &#39;exact&#39; means exact size (instead of page order), which leads</span>
<span class="quote">&gt; to more confusion.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To prevent further mistakes, this patch effectively renames</span>
<span class="quote">&gt; alloc_pages_exact_node() to __alloc_pages_node() to better convey that it&#39;s</span>
<span class="quote">&gt; an optimized variant of alloc_pages_node() not intended for general usage.</span>
<span class="quote">&gt; Both functions get described in comments.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It has been also considered to really provide a convenience function for</span>
<span class="quote">&gt; allocations restricted to a node, but the major opinion seems to be that</span>
<span class="quote">&gt; __GFP_THISNODE already provides that functionality and we shouldn&#39;t duplicate</span>
<span class="quote">&gt; the API needlessly. The number of users would be small anyway.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Existing callers of alloc_pages_exact_node() are simply converted to call</span>
<span class="quote">&gt; __alloc_pages_node(), with the exception of sba_alloc_coherent() which</span>
<span class="quote">&gt; open-codes the check for NUMA_NO_NODE, so it is converted to use</span>
<span class="quote">&gt; alloc_pages_node() instead. This means it no longer performs some VM_BUG_ON</span>
<span class="quote">&gt; checks, and since the current check for nid in alloc_pages_node() uses a &#39;nid &lt;</span>
<span class="quote">&gt; 0&#39; comparison (which includes NUMA_NO_NODE), it may hide wrong values which</span>
<span class="quote">&gt; would be previously exposed. Both differences will be rectified by the next</span>
<span class="quote">&gt; patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To sum up, this patch makes no functional changes, except temporarily hiding</span>
<span class="quote">&gt; potentially buggy callers. Restricting the checks in alloc_pages_node() is</span>
<span class="quote">&gt; left for the next patch which can in turn expose more existing buggy callers.</span>

Nice cleanup!
<span class="quote">
&gt; Signed-off-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="quote">&gt; Acked-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="quote">&gt; Cc: David Rientjes &lt;rientjes@google.com&gt;</span>
<span class="quote">&gt; Cc: Greg Thelen &lt;gthelen@google.com&gt;</span>
<span class="quote">&gt; Cc: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; Acked-by: Christoph Lameter &lt;cl@linux.com&gt;</span>
<span class="quote">&gt; Cc: Pekka Enberg &lt;penberg@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;</span>
<span class="quote">&gt; Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; Cc: Tony Luck &lt;tony.luck@intel.com&gt;</span>
<span class="quote">&gt; Cc: Fenghua Yu &lt;fenghua.yu@intel.com&gt;</span>
<span class="quote">&gt; Cc: Arnd Bergmann &lt;arnd@arndb.de&gt;</span>
<span class="quote">&gt; Cc: Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;</span>
<span class="quote">&gt; Cc: Paul Mackerras &lt;paulus@samba.org&gt;</span>
<span class="quote">&gt; Acked-by: Michael Ellerman &lt;mpe@ellerman.id.au&gt;</span>
<span class="quote">&gt; Cc: Gleb Natapov &lt;gleb@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Paolo Bonzini &lt;pbonzini@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;</span>
<span class="quote">&gt; Cc: Ingo Molnar &lt;mingo@redhat.com&gt;</span>
<span class="quote">&gt; Cc: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;</span>
<span class="quote">&gt; Cc: Cliff Whickman &lt;cpw@sgi.com&gt;</span>
<span class="quote">&gt; Acked-by: Robin Holt &lt;robinmholt@gmail.com&gt;</span>
<span class="acked-by">
Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  v3: fixed the slob part (Christoph Lameter), removed forgotten hunk from</span>
<span class="quote">&gt;      huge_memory.c (DavidR)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  arch/ia64/hp/common/sba_iommu.c   |  6 +-----</span>
<span class="quote">&gt;  arch/ia64/kernel/uncached.c       |  2 +-</span>
<span class="quote">&gt;  arch/ia64/sn/pci/pci_dma.c        |  2 +-</span>
<span class="quote">&gt;  arch/powerpc/platforms/cell/ras.c |  2 +-</span>
<span class="quote">&gt;  arch/x86/kvm/vmx.c                |  2 +-</span>
<span class="quote">&gt;  drivers/misc/sgi-xp/xpc_uv.c      |  2 +-</span>
<span class="quote">&gt;  include/linux/gfp.h               | 23 +++++++++++++++--------</span>
<span class="quote">&gt;  kernel/profile.c                  |  8 ++++----</span>
<span class="quote">&gt;  mm/filemap.c                      |  2 +-</span>
<span class="quote">&gt;  mm/huge_memory.c                  |  2 +-</span>
<span class="quote">&gt;  mm/hugetlb.c                      |  4 ++--</span>
<span class="quote">&gt;  mm/memory-failure.c               |  2 +-</span>
<span class="quote">&gt;  mm/mempolicy.c                    |  4 ++--</span>
<span class="quote">&gt;  mm/migrate.c                      |  4 ++--</span>
<span class="quote">&gt;  mm/page_alloc.c                   |  2 --</span>
<span class="quote">&gt;  mm/slab.c                         |  2 +-</span>
<span class="quote">&gt;  mm/slob.c                         |  4 ++--</span>
<span class="quote">&gt;  mm/slub.c                         |  2 +-</span>
<span class="quote">&gt;  18 files changed, 38 insertions(+), 37 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/ia64/hp/common/sba_iommu.c b/arch/ia64/hp/common/sba_iommu.c</span>
<span class="quote">&gt; index 344387a..a6d6190 100644</span>
<span class="quote">&gt; --- a/arch/ia64/hp/common/sba_iommu.c</span>
<span class="quote">&gt; +++ b/arch/ia64/hp/common/sba_iommu.c</span>
<span class="quote">&gt; @@ -1140,13 +1140,9 @@ sba_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_NUMA</span>
<span class="quote">&gt;  	{</span>
<span class="quote">&gt; -		int node = ioc-&gt;node;</span>
<span class="quote">&gt;  		struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		if (node == NUMA_NO_NODE)</span>
<span class="quote">&gt; -			node = numa_node_id();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		page = alloc_pages_exact_node(node, flags, get_order(size));</span>
<span class="quote">&gt; +		page = alloc_pages_node(ioc-&gt;node, flags, get_order(size));</span>
<span class="quote">&gt;  		if (unlikely(!page))</span>
<span class="quote">&gt;  			return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/ia64/kernel/uncached.c b/arch/ia64/kernel/uncached.c</span>
<span class="quote">&gt; index 20e8a9b..f3976da 100644</span>
<span class="quote">&gt; --- a/arch/ia64/kernel/uncached.c</span>
<span class="quote">&gt; +++ b/arch/ia64/kernel/uncached.c</span>
<span class="quote">&gt; @@ -97,7 +97,7 @@ static int uncached_add_chunk(struct uncached_pool *uc_pool, int nid)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* attempt to allocate a granule&#39;s worth of cached memory pages */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page = alloc_pages_exact_node(nid,</span>
<span class="quote">&gt; +	page = __alloc_pages_node(nid,</span>
<span class="quote">&gt;  				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,</span>
<span class="quote">&gt;  				IA64_GRANULE_SHIFT-PAGE_SHIFT);</span>
<span class="quote">&gt;  	if (!page) {</span>
<span class="quote">&gt; diff --git a/arch/ia64/sn/pci/pci_dma.c b/arch/ia64/sn/pci/pci_dma.c</span>
<span class="quote">&gt; index d0853e8..8f59907 100644</span>
<span class="quote">&gt; --- a/arch/ia64/sn/pci/pci_dma.c</span>
<span class="quote">&gt; +++ b/arch/ia64/sn/pci/pci_dma.c</span>
<span class="quote">&gt; @@ -92,7 +92,7 @@ static void *sn_dma_alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	node = pcibus_to_node(pdev-&gt;bus);</span>
<span class="quote">&gt;  	if (likely(node &gt;=0)) {</span>
<span class="quote">&gt; -		struct page *p = alloc_pages_exact_node(node,</span>
<span class="quote">&gt; +		struct page *p = __alloc_pages_node(node,</span>
<span class="quote">&gt;  						flags, get_order(size));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (likely(p))</span>
<span class="quote">&gt; diff --git a/arch/powerpc/platforms/cell/ras.c b/arch/powerpc/platforms/cell/ras.c</span>
<span class="quote">&gt; index e865d74..2d4f60c 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/platforms/cell/ras.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/platforms/cell/ras.c</span>
<span class="quote">&gt; @@ -123,7 +123,7 @@ static int __init cbe_ptcal_enable_on_node(int nid, int order)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	area-&gt;nid = nid;</span>
<span class="quote">&gt;  	area-&gt;order = order;</span>
<span class="quote">&gt; -	area-&gt;pages = alloc_pages_exact_node(area-&gt;nid,</span>
<span class="quote">&gt; +	area-&gt;pages = __alloc_pages_node(area-&gt;nid,</span>
<span class="quote">&gt;  						GFP_KERNEL|__GFP_THISNODE,</span>
<span class="quote">&gt;  						area-&gt;order);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; index 5477ab8..d019868 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/vmx.c</span>
<span class="quote">&gt; @@ -3150,7 +3150,7 @@ static struct vmcs *alloc_vmcs_cpu(int cpu)</span>
<span class="quote">&gt;  	struct page *pages;</span>
<span class="quote">&gt;  	struct vmcs *vmcs;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	pages = alloc_pages_exact_node(node, GFP_KERNEL, vmcs_config.order);</span>
<span class="quote">&gt; +	pages = __alloc_pages_node(node, GFP_KERNEL, vmcs_config.order);</span>
<span class="quote">&gt;  	if (!pages)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  	vmcs = page_address(pages);</span>
<span class="quote">&gt; diff --git a/drivers/misc/sgi-xp/xpc_uv.c b/drivers/misc/sgi-xp/xpc_uv.c</span>
<span class="quote">&gt; index 95c8944..340b44d 100644</span>
<span class="quote">&gt; --- a/drivers/misc/sgi-xp/xpc_uv.c</span>
<span class="quote">&gt; +++ b/drivers/misc/sgi-xp/xpc_uv.c</span>
<span class="quote">&gt; @@ -239,7 +239,7 @@ xpc_create_gru_mq_uv(unsigned int mq_size, int cpu, char *irq_name,</span>
<span class="quote">&gt;  	mq-&gt;mmr_blade = uv_cpu_to_blade_id(cpu);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	nid = cpu_to_node(cpu);</span>
<span class="quote">&gt; -	page = alloc_pages_exact_node(nid,</span>
<span class="quote">&gt; +	page = __alloc_pages_node(nid,</span>
<span class="quote">&gt;  				      GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,</span>
<span class="quote">&gt;  				      pg_order);</span>
<span class="quote">&gt;  	if (page == NULL) {</span>
<span class="quote">&gt; diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="quote">&gt; index 3bd64b1..d2c142b 100644</span>
<span class="quote">&gt; --- a/include/linux/gfp.h</span>
<span class="quote">&gt; +++ b/include/linux/gfp.h</span>
<span class="quote">&gt; @@ -303,20 +303,28 @@ __alloc_pages(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  	return __alloc_pages_nodemask(gfp_mask, order, zonelist, NULL);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,</span>
<span class="quote">&gt; -						unsigned int order)</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Allocate pages, preferring the node given as nid. The node must be valid and</span>
<span class="quote">&gt; + * online. For more general interface, see alloc_pages_node().</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline struct page *</span>
<span class="quote">&gt; +__alloc_pages_node(int nid, gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	/* Unknown node is current node */</span>
<span class="quote">&gt; -	if (nid &lt; 0)</span>
<span class="quote">&gt; -		nid = numa_node_id();</span>
<span class="quote">&gt; +	VM_BUG_ON(nid &lt; 0 || nid &gt;= MAX_NUMNODES || !node_online(nid));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline struct page *alloc_pages_exact_node(int nid, gfp_t gfp_mask,</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Allocate pages, preferring the node given as nid. When nid == NUMA_NO_NODE,</span>
<span class="quote">&gt; + * prefer the current CPU&#39;s node.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,</span>
<span class="quote">&gt;  						unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	VM_BUG_ON(nid &lt; 0 || nid &gt;= MAX_NUMNODES || !node_online(nid));</span>
<span class="quote">&gt; +	/* Unknown node is current node */</span>
<span class="quote">&gt; +	if (nid &lt; 0)</span>
<span class="quote">&gt; +		nid = numa_node_id();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -357,7 +365,6 @@ extern unsigned long get_zeroed_page(gfp_t gfp_mask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void *alloc_pages_exact(size_t size, gfp_t gfp_mask);</span>
<span class="quote">&gt;  void free_pages_exact(void *virt, size_t size);</span>
<span class="quote">&gt; -/* This is different from alloc_pages_exact_node !!! */</span>
<span class="quote">&gt;  void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define __get_free_page(gfp_mask) \</span>
<span class="quote">&gt; diff --git a/kernel/profile.c b/kernel/profile.c</span>
<span class="quote">&gt; index a7bcd28..99513e1 100644</span>
<span class="quote">&gt; --- a/kernel/profile.c</span>
<span class="quote">&gt; +++ b/kernel/profile.c</span>
<span class="quote">&gt; @@ -339,7 +339,7 @@ static int profile_cpu_callback(struct notifier_block *info,</span>
<span class="quote">&gt;  		node = cpu_to_mem(cpu);</span>
<span class="quote">&gt;  		per_cpu(cpu_profile_flip, cpu) = 0;</span>
<span class="quote">&gt;  		if (!per_cpu(cpu_profile_hits, cpu)[1]) {</span>
<span class="quote">&gt; -			page = alloc_pages_exact_node(node,</span>
<span class="quote">&gt; +			page = __alloc_pages_node(node,</span>
<span class="quote">&gt;  					GFP_KERNEL | __GFP_ZERO,</span>
<span class="quote">&gt;  					0);</span>
<span class="quote">&gt;  			if (!page)</span>
<span class="quote">&gt; @@ -347,7 +347,7 @@ static int profile_cpu_callback(struct notifier_block *info,</span>
<span class="quote">&gt;  			per_cpu(cpu_profile_hits, cpu)[1] = page_address(page);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (!per_cpu(cpu_profile_hits, cpu)[0]) {</span>
<span class="quote">&gt; -			page = alloc_pages_exact_node(node,</span>
<span class="quote">&gt; +			page = __alloc_pages_node(node,</span>
<span class="quote">&gt;  					GFP_KERNEL | __GFP_ZERO,</span>
<span class="quote">&gt;  					0);</span>
<span class="quote">&gt;  			if (!page)</span>
<span class="quote">&gt; @@ -543,14 +543,14 @@ static int create_hash_tables(void)</span>
<span class="quote">&gt;  		int node = cpu_to_mem(cpu);</span>
<span class="quote">&gt;  		struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		page = alloc_pages_exact_node(node,</span>
<span class="quote">&gt; +		page = __alloc_pages_node(node,</span>
<span class="quote">&gt;  				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,</span>
<span class="quote">&gt;  				0);</span>
<span class="quote">&gt;  		if (!page)</span>
<span class="quote">&gt;  			goto out_cleanup;</span>
<span class="quote">&gt;  		per_cpu(cpu_profile_hits, cpu)[1]</span>
<span class="quote">&gt;  				= (struct profile_hit *)page_address(page);</span>
<span class="quote">&gt; -		page = alloc_pages_exact_node(node,</span>
<span class="quote">&gt; +		page = __alloc_pages_node(node,</span>
<span class="quote">&gt;  				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,</span>
<span class="quote">&gt;  				0);</span>
<span class="quote">&gt;  		if (!page)</span>
<span class="quote">&gt; diff --git a/mm/filemap.c b/mm/filemap.c</span>
<span class="quote">&gt; index 204fd1c..b510a0d 100644</span>
<span class="quote">&gt; --- a/mm/filemap.c</span>
<span class="quote">&gt; +++ b/mm/filemap.c</span>
<span class="quote">&gt; @@ -674,7 +674,7 @@ struct page *__page_cache_alloc(gfp_t gfp)</span>
<span class="quote">&gt;  		do {</span>
<span class="quote">&gt;  			cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="quote">&gt;  			n = cpuset_mem_spread_node();</span>
<span class="quote">&gt; -			page = alloc_pages_exact_node(n, gfp, 0);</span>
<span class="quote">&gt; +			page = __alloc_pages_node(n, gfp, 0);</span>
<span class="quote">&gt;  		} while (!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		return page;</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index 7109330..15abd31 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -2456,7 +2456,7 @@ khugepaged_alloc_page(struct page **hpage, gfp_t gfp, struct mm_struct *mm,</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	*hpage = alloc_pages_exact_node(node, gfp, HPAGE_PMD_ORDER);</span>
<span class="quote">&gt; +	*hpage = __alloc_pages_node(node, gfp, HPAGE_PMD_ORDER);</span>
<span class="quote">&gt;  	if (unlikely(!*hpage)) {</span>
<span class="quote">&gt;  		count_vm_event(THP_COLLAPSE_ALLOC_FAILED);</span>
<span class="quote">&gt;  		*hpage = ERR_PTR(-ENOMEM);</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 586aa69..b4c23bb 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -1331,7 +1331,7 @@ static struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page = alloc_pages_exact_node(nid,</span>
<span class="quote">&gt; +	page = __alloc_pages_node(nid,</span>
<span class="quote">&gt;  		htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|</span>
<span class="quote">&gt;  						__GFP_REPEAT|__GFP_NOWARN,</span>
<span class="quote">&gt;  		huge_page_order(h));</span>
<span class="quote">&gt; @@ -1483,7 +1483,7 @@ static struct page *alloc_buddy_huge_page(struct hstate *h, int nid)</span>
<span class="quote">&gt;  				   __GFP_REPEAT|__GFP_NOWARN,</span>
<span class="quote">&gt;  				   huge_page_order(h));</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		page = alloc_pages_exact_node(nid,</span>
<span class="quote">&gt; +		page = __alloc_pages_node(nid,</span>
<span class="quote">&gt;  			htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|</span>
<span class="quote">&gt;  			__GFP_REPEAT|__GFP_NOWARN, huge_page_order(h));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="quote">&gt; index 613389e..023f92b 100644</span>
<span class="quote">&gt; --- a/mm/memory-failure.c</span>
<span class="quote">&gt; +++ b/mm/memory-failure.c</span>
<span class="quote">&gt; @@ -1509,7 +1509,7 @@ static struct page *new_page(struct page *p, unsigned long private, int **x)</span>
<span class="quote">&gt;  		return alloc_huge_page_node(page_hstate(compound_head(p)),</span>
<span class="quote">&gt;  						   nid);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		return alloc_pages_exact_node(nid, GFP_HIGHUSER_MOVABLE, 0);</span>
<span class="quote">&gt; +		return __alloc_pages_node(nid, GFP_HIGHUSER_MOVABLE, 0);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="quote">&gt; index d6f2cae..87a1779 100644</span>
<span class="quote">&gt; --- a/mm/mempolicy.c</span>
<span class="quote">&gt; +++ b/mm/mempolicy.c</span>
<span class="quote">&gt; @@ -942,7 +942,7 @@ static struct page *new_node_page(struct page *page, unsigned long node, int **x</span>
<span class="quote">&gt;  		return alloc_huge_page_node(page_hstate(compound_head(page)),</span>
<span class="quote">&gt;  					node);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		return alloc_pages_exact_node(node, GFP_HIGHUSER_MOVABLE |</span>
<span class="quote">&gt; +		return __alloc_pages_node(node, GFP_HIGHUSER_MOVABLE |</span>
<span class="quote">&gt;  						    __GFP_THISNODE, 0);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1998,7 +1998,7 @@ alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		nmask = policy_nodemask(gfp, pol);</span>
<span class="quote">&gt;  		if (!nmask || node_isset(hpage_node, *nmask)) {</span>
<span class="quote">&gt;  			mpol_cond_put(pol);</span>
<span class="quote">&gt; -			page = alloc_pages_exact_node(hpage_node,</span>
<span class="quote">&gt; +			page = __alloc_pages_node(hpage_node,</span>
<span class="quote">&gt;  						gfp | __GFP_THISNODE, order);</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index fbf1798..b7da48f 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -1201,7 +1201,7 @@ static struct page *new_page_node(struct page *p, unsigned long private,</span>
<span class="quote">&gt;  		return alloc_huge_page_node(page_hstate(compound_head(p)),</span>
<span class="quote">&gt;  					pm-&gt;node);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		return alloc_pages_exact_node(pm-&gt;node,</span>
<span class="quote">&gt; +		return __alloc_pages_node(pm-&gt;node,</span>
<span class="quote">&gt;  				GFP_HIGHUSER_MOVABLE | __GFP_THISNODE, 0);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1561,7 +1561,7 @@ static struct page *alloc_misplaced_dst_page(struct page *page,</span>
<span class="quote">&gt;  	int nid = (int) data;</span>
<span class="quote">&gt;  	struct page *newpage;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	newpage = alloc_pages_exact_node(nid,</span>
<span class="quote">&gt; +	newpage = __alloc_pages_node(nid,</span>
<span class="quote">&gt;  					 (GFP_HIGHUSER_MOVABLE |</span>
<span class="quote">&gt;  					  __GFP_THISNODE | __GFP_NOMEMALLOC |</span>
<span class="quote">&gt;  					  __GFP_NORETRY | __GFP_NOWARN) &amp;</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index c1024db..35f1d20 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -3518,8 +3518,6 @@ EXPORT_SYMBOL(alloc_pages_exact);</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Like alloc_pages_exact(), but try to allocate on node nid first before falling</span>
<span class="quote">&gt;   * back.</span>
<span class="quote">&gt; - * Note this is not alloc_pages_exact_node() which allocates on a specific node,</span>
<span class="quote">&gt; - * but is not exact.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/mm/slab.c b/mm/slab.c</span>
<span class="quote">&gt; index bf7169c..d890750 100644</span>
<span class="quote">&gt; --- a/mm/slab.c</span>
<span class="quote">&gt; +++ b/mm/slab.c</span>
<span class="quote">&gt; @@ -1595,7 +1595,7 @@ static struct page *kmem_getpages(struct kmem_cache *cachep, gfp_t flags,</span>
<span class="quote">&gt;  	if (memcg_charge_slab(cachep, flags, cachep-&gt;gfporder))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page = alloc_pages_exact_node(nodeid, flags | __GFP_NOTRACK, cachep-&gt;gfporder);</span>
<span class="quote">&gt; +	page = __alloc_pages_node(nodeid, flags | __GFP_NOTRACK, cachep-&gt;gfporder);</span>
<span class="quote">&gt;  	if (!page) {</span>
<span class="quote">&gt;  		memcg_uncharge_slab(cachep, cachep-&gt;gfporder);</span>
<span class="quote">&gt;  		slab_out_of_memory(cachep, flags, nodeid);</span>
<span class="quote">&gt; diff --git a/mm/slob.c b/mm/slob.c</span>
<span class="quote">&gt; index 165bbd3..0d7e5df 100644</span>
<span class="quote">&gt; --- a/mm/slob.c</span>
<span class="quote">&gt; +++ b/mm/slob.c</span>
<span class="quote">&gt; @@ -45,7 +45,7 @@</span>
<span class="quote">&gt;   * NUMA support in SLOB is fairly simplistic, pushing most of the real</span>
<span class="quote">&gt;   * logic down to the page allocator, and simply doing the node accounting</span>
<span class="quote">&gt;   * on the upper levels. In the event that a node id is explicitly</span>
<span class="quote">&gt; - * provided, alloc_pages_exact_node() with the specified node id is used</span>
<span class="quote">&gt; + * provided, __alloc_pages_node() with the specified node id is used</span>
<span class="quote">&gt;   * instead. The common case (or when the node id isn&#39;t explicitly provided)</span>
<span class="quote">&gt;   * will default to the current node, as per numa_node_id().</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; @@ -193,7 +193,7 @@ static void *slob_new_pages(gfp_t gfp, int order, int node)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_NUMA</span>
<span class="quote">&gt;  	if (node != NUMA_NO_NODE)</span>
<span class="quote">&gt; -		page = alloc_pages_exact_node(node, gfp, order);</span>
<span class="quote">&gt; +		page = __alloc_pages_node(node, gfp, order);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  		page = alloc_pages(gfp, order);</span>
<span class="quote">&gt; diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="quote">&gt; index 8987bd5..e180f8d 100644</span>
<span class="quote">&gt; --- a/mm/slub.c</span>
<span class="quote">&gt; +++ b/mm/slub.c</span>
<span class="quote">&gt; @@ -1336,7 +1336,7 @@ static inline struct page *alloc_slab_page(struct kmem_cache *s,</span>
<span class="quote">&gt;  	if (node == NUMA_NO_NODE)</span>
<span class="quote">&gt;  		page = alloc_pages(flags, order);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		page = alloc_pages_exact_node(node, flags, order);</span>
<span class="quote">&gt; +		page = __alloc_pages_node(node, flags, order);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!page)</span>
<span class="quote">&gt;  		memcg_uncharge_slab(s, order);</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.5.0</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in</span>
<span class="quote">&gt; the body of a message to majordomo@vger.kernel.org</span>
<span class="quote">&gt; More majordomo info at  http://vger.kernel.org/majordomo-info.html</span>
<span class="quote">&gt; Please read the FAQ at  http://www.tux.org/lkml/</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/ia64/hp/common/sba_iommu.c b/arch/ia64/hp/common/sba_iommu.c</span>
<span class="p_header">index 344387a..a6d6190 100644</span>
<span class="p_header">--- a/arch/ia64/hp/common/sba_iommu.c</span>
<span class="p_header">+++ b/arch/ia64/hp/common/sba_iommu.c</span>
<span class="p_chunk">@@ -1140,13 +1140,9 @@</span> <span class="p_context"> sba_alloc_coherent(struct device *dev, size_t size, dma_addr_t *dma_handle,</span>
 
 #ifdef CONFIG_NUMA
 	{
<span class="p_del">-		int node = ioc-&gt;node;</span>
 		struct page *page;
 
<span class="p_del">-		if (node == NUMA_NO_NODE)</span>
<span class="p_del">-			node = numa_node_id();</span>
<span class="p_del">-</span>
<span class="p_del">-		page = alloc_pages_exact_node(node, flags, get_order(size));</span>
<span class="p_add">+		page = alloc_pages_node(ioc-&gt;node, flags, get_order(size));</span>
 		if (unlikely(!page))
 			return NULL;
 
<span class="p_header">diff --git a/arch/ia64/kernel/uncached.c b/arch/ia64/kernel/uncached.c</span>
<span class="p_header">index 20e8a9b..f3976da 100644</span>
<span class="p_header">--- a/arch/ia64/kernel/uncached.c</span>
<span class="p_header">+++ b/arch/ia64/kernel/uncached.c</span>
<span class="p_chunk">@@ -97,7 +97,7 @@</span> <span class="p_context"> static int uncached_add_chunk(struct uncached_pool *uc_pool, int nid)</span>
 
 	/* attempt to allocate a granule&#39;s worth of cached memory pages */
 
<span class="p_del">-	page = alloc_pages_exact_node(nid,</span>
<span class="p_add">+	page = __alloc_pages_node(nid,</span>
 				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				IA64_GRANULE_SHIFT-PAGE_SHIFT);
 	if (!page) {
<span class="p_header">diff --git a/arch/ia64/sn/pci/pci_dma.c b/arch/ia64/sn/pci/pci_dma.c</span>
<span class="p_header">index d0853e8..8f59907 100644</span>
<span class="p_header">--- a/arch/ia64/sn/pci/pci_dma.c</span>
<span class="p_header">+++ b/arch/ia64/sn/pci/pci_dma.c</span>
<span class="p_chunk">@@ -92,7 +92,7 @@</span> <span class="p_context"> static void *sn_dma_alloc_coherent(struct device *dev, size_t size,</span>
 	 */
 	node = pcibus_to_node(pdev-&gt;bus);
 	if (likely(node &gt;=0)) {
<span class="p_del">-		struct page *p = alloc_pages_exact_node(node,</span>
<span class="p_add">+		struct page *p = __alloc_pages_node(node,</span>
 						flags, get_order(size));
 
 		if (likely(p))
<span class="p_header">diff --git a/arch/powerpc/platforms/cell/ras.c b/arch/powerpc/platforms/cell/ras.c</span>
<span class="p_header">index e865d74..2d4f60c 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/cell/ras.c</span>
<span class="p_header">+++ b/arch/powerpc/platforms/cell/ras.c</span>
<span class="p_chunk">@@ -123,7 +123,7 @@</span> <span class="p_context"> static int __init cbe_ptcal_enable_on_node(int nid, int order)</span>
 
 	area-&gt;nid = nid;
 	area-&gt;order = order;
<span class="p_del">-	area-&gt;pages = alloc_pages_exact_node(area-&gt;nid,</span>
<span class="p_add">+	area-&gt;pages = __alloc_pages_node(area-&gt;nid,</span>
 						GFP_KERNEL|__GFP_THISNODE,
 						area-&gt;order);
 
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index 5477ab8..d019868 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -3150,7 +3150,7 @@</span> <span class="p_context"> static struct vmcs *alloc_vmcs_cpu(int cpu)</span>
 	struct page *pages;
 	struct vmcs *vmcs;
 
<span class="p_del">-	pages = alloc_pages_exact_node(node, GFP_KERNEL, vmcs_config.order);</span>
<span class="p_add">+	pages = __alloc_pages_node(node, GFP_KERNEL, vmcs_config.order);</span>
 	if (!pages)
 		return NULL;
 	vmcs = page_address(pages);
<span class="p_header">diff --git a/drivers/misc/sgi-xp/xpc_uv.c b/drivers/misc/sgi-xp/xpc_uv.c</span>
<span class="p_header">index 95c8944..340b44d 100644</span>
<span class="p_header">--- a/drivers/misc/sgi-xp/xpc_uv.c</span>
<span class="p_header">+++ b/drivers/misc/sgi-xp/xpc_uv.c</span>
<span class="p_chunk">@@ -239,7 +239,7 @@</span> <span class="p_context"> xpc_create_gru_mq_uv(unsigned int mq_size, int cpu, char *irq_name,</span>
 	mq-&gt;mmr_blade = uv_cpu_to_blade_id(cpu);
 
 	nid = cpu_to_node(cpu);
<span class="p_del">-	page = alloc_pages_exact_node(nid,</span>
<span class="p_add">+	page = __alloc_pages_node(nid,</span>
 				      GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				      pg_order);
 	if (page == NULL) {
<span class="p_header">diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="p_header">index 3bd64b1..d2c142b 100644</span>
<span class="p_header">--- a/include/linux/gfp.h</span>
<span class="p_header">+++ b/include/linux/gfp.h</span>
<span class="p_chunk">@@ -303,20 +303,28 @@</span> <span class="p_context"> __alloc_pages(gfp_t gfp_mask, unsigned int order,</span>
 	return __alloc_pages_nodemask(gfp_mask, order, zonelist, NULL);
 }
 
<span class="p_del">-static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,</span>
<span class="p_del">-						unsigned int order)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Allocate pages, preferring the node given as nid. The node must be valid and</span>
<span class="p_add">+ * online. For more general interface, see alloc_pages_node().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline struct page *</span>
<span class="p_add">+__alloc_pages_node(int nid, gfp_t gfp_mask, unsigned int order)</span>
 {
<span class="p_del">-	/* Unknown node is current node */</span>
<span class="p_del">-	if (nid &lt; 0)</span>
<span class="p_del">-		nid = numa_node_id();</span>
<span class="p_add">+	VM_BUG_ON(nid &lt; 0 || nid &gt;= MAX_NUMNODES || !node_online(nid));</span>
 
 	return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));
 }
 
<span class="p_del">-static inline struct page *alloc_pages_exact_node(int nid, gfp_t gfp_mask,</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Allocate pages, preferring the node given as nid. When nid == NUMA_NO_NODE,</span>
<span class="p_add">+ * prefer the current CPU&#39;s node.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline struct page *alloc_pages_node(int nid, gfp_t gfp_mask,</span>
 						unsigned int order)
 {
<span class="p_del">-	VM_BUG_ON(nid &lt; 0 || nid &gt;= MAX_NUMNODES || !node_online(nid));</span>
<span class="p_add">+	/* Unknown node is current node */</span>
<span class="p_add">+	if (nid &lt; 0)</span>
<span class="p_add">+		nid = numa_node_id();</span>
 
 	return __alloc_pages(gfp_mask, order, node_zonelist(nid, gfp_mask));
 }
<span class="p_chunk">@@ -357,7 +365,6 @@</span> <span class="p_context"> extern unsigned long get_zeroed_page(gfp_t gfp_mask);</span>
 
 void *alloc_pages_exact(size_t size, gfp_t gfp_mask);
 void free_pages_exact(void *virt, size_t size);
<span class="p_del">-/* This is different from alloc_pages_exact_node !!! */</span>
 void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask);
 
 #define __get_free_page(gfp_mask) \
<span class="p_header">diff --git a/kernel/profile.c b/kernel/profile.c</span>
<span class="p_header">index a7bcd28..99513e1 100644</span>
<span class="p_header">--- a/kernel/profile.c</span>
<span class="p_header">+++ b/kernel/profile.c</span>
<span class="p_chunk">@@ -339,7 +339,7 @@</span> <span class="p_context"> static int profile_cpu_callback(struct notifier_block *info,</span>
 		node = cpu_to_mem(cpu);
 		per_cpu(cpu_profile_flip, cpu) = 0;
 		if (!per_cpu(cpu_profile_hits, cpu)[1]) {
<span class="p_del">-			page = alloc_pages_exact_node(node,</span>
<span class="p_add">+			page = __alloc_pages_node(node,</span>
 					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
<span class="p_chunk">@@ -347,7 +347,7 @@</span> <span class="p_context"> static int profile_cpu_callback(struct notifier_block *info,</span>
 			per_cpu(cpu_profile_hits, cpu)[1] = page_address(page);
 		}
 		if (!per_cpu(cpu_profile_hits, cpu)[0]) {
<span class="p_del">-			page = alloc_pages_exact_node(node,</span>
<span class="p_add">+			page = __alloc_pages_node(node,</span>
 					GFP_KERNEL | __GFP_ZERO,
 					0);
 			if (!page)
<span class="p_chunk">@@ -543,14 +543,14 @@</span> <span class="p_context"> static int create_hash_tables(void)</span>
 		int node = cpu_to_mem(cpu);
 		struct page *page;
 
<span class="p_del">-		page = alloc_pages_exact_node(node,</span>
<span class="p_add">+		page = __alloc_pages_node(node,</span>
 				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				0);
 		if (!page)
 			goto out_cleanup;
 		per_cpu(cpu_profile_hits, cpu)[1]
 				= (struct profile_hit *)page_address(page);
<span class="p_del">-		page = alloc_pages_exact_node(node,</span>
<span class="p_add">+		page = __alloc_pages_node(node,</span>
 				GFP_KERNEL | __GFP_ZERO | __GFP_THISNODE,
 				0);
 		if (!page)
<span class="p_header">diff --git a/mm/filemap.c b/mm/filemap.c</span>
<span class="p_header">index 204fd1c..b510a0d 100644</span>
<span class="p_header">--- a/mm/filemap.c</span>
<span class="p_header">+++ b/mm/filemap.c</span>
<span class="p_chunk">@@ -674,7 +674,7 @@</span> <span class="p_context"> struct page *__page_cache_alloc(gfp_t gfp)</span>
 		do {
 			cpuset_mems_cookie = read_mems_allowed_begin();
 			n = cpuset_mem_spread_node();
<span class="p_del">-			page = alloc_pages_exact_node(n, gfp, 0);</span>
<span class="p_add">+			page = __alloc_pages_node(n, gfp, 0);</span>
 		} while (!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie));
 
 		return page;
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 7109330..15abd31 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -2456,7 +2456,7 @@</span> <span class="p_context"> khugepaged_alloc_page(struct page **hpage, gfp_t gfp, struct mm_struct *mm,</span>
 	 */
 	up_read(&amp;mm-&gt;mmap_sem);
 
<span class="p_del">-	*hpage = alloc_pages_exact_node(node, gfp, HPAGE_PMD_ORDER);</span>
<span class="p_add">+	*hpage = __alloc_pages_node(node, gfp, HPAGE_PMD_ORDER);</span>
 	if (unlikely(!*hpage)) {
 		count_vm_event(THP_COLLAPSE_ALLOC_FAILED);
 		*hpage = ERR_PTR(-ENOMEM);
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 586aa69..b4c23bb 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1331,7 +1331,7 @@</span> <span class="p_context"> static struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)</span>
 {
 	struct page *page;
 
<span class="p_del">-	page = alloc_pages_exact_node(nid,</span>
<span class="p_add">+	page = __alloc_pages_node(nid,</span>
 		htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|
 						__GFP_REPEAT|__GFP_NOWARN,
 		huge_page_order(h));
<span class="p_chunk">@@ -1483,7 +1483,7 @@</span> <span class="p_context"> static struct page *alloc_buddy_huge_page(struct hstate *h, int nid)</span>
 				   __GFP_REPEAT|__GFP_NOWARN,
 				   huge_page_order(h));
 	else
<span class="p_del">-		page = alloc_pages_exact_node(nid,</span>
<span class="p_add">+		page = __alloc_pages_node(nid,</span>
 			htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|
 			__GFP_REPEAT|__GFP_NOWARN, huge_page_order(h));
 
<span class="p_header">diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="p_header">index 613389e..023f92b 100644</span>
<span class="p_header">--- a/mm/memory-failure.c</span>
<span class="p_header">+++ b/mm/memory-failure.c</span>
<span class="p_chunk">@@ -1509,7 +1509,7 @@</span> <span class="p_context"> static struct page *new_page(struct page *p, unsigned long private, int **x)</span>
 		return alloc_huge_page_node(page_hstate(compound_head(p)),
 						   nid);
 	else
<span class="p_del">-		return alloc_pages_exact_node(nid, GFP_HIGHUSER_MOVABLE, 0);</span>
<span class="p_add">+		return __alloc_pages_node(nid, GFP_HIGHUSER_MOVABLE, 0);</span>
 }
 
 /*
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index d6f2cae..87a1779 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -942,7 +942,7 @@</span> <span class="p_context"> static struct page *new_node_page(struct page *page, unsigned long node, int **x</span>
 		return alloc_huge_page_node(page_hstate(compound_head(page)),
 					node);
 	else
<span class="p_del">-		return alloc_pages_exact_node(node, GFP_HIGHUSER_MOVABLE |</span>
<span class="p_add">+		return __alloc_pages_node(node, GFP_HIGHUSER_MOVABLE |</span>
 						    __GFP_THISNODE, 0);
 }
 
<span class="p_chunk">@@ -1998,7 +1998,7 @@</span> <span class="p_context"> alloc_pages_vma(gfp_t gfp, int order, struct vm_area_struct *vma,</span>
 		nmask = policy_nodemask(gfp, pol);
 		if (!nmask || node_isset(hpage_node, *nmask)) {
 			mpol_cond_put(pol);
<span class="p_del">-			page = alloc_pages_exact_node(hpage_node,</span>
<span class="p_add">+			page = __alloc_pages_node(hpage_node,</span>
 						gfp | __GFP_THISNODE, order);
 			goto out;
 		}
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index fbf1798..b7da48f 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1201,7 +1201,7 @@</span> <span class="p_context"> static struct page *new_page_node(struct page *p, unsigned long private,</span>
 		return alloc_huge_page_node(page_hstate(compound_head(p)),
 					pm-&gt;node);
 	else
<span class="p_del">-		return alloc_pages_exact_node(pm-&gt;node,</span>
<span class="p_add">+		return __alloc_pages_node(pm-&gt;node,</span>
 				GFP_HIGHUSER_MOVABLE | __GFP_THISNODE, 0);
 }
 
<span class="p_chunk">@@ -1561,7 +1561,7 @@</span> <span class="p_context"> static struct page *alloc_misplaced_dst_page(struct page *page,</span>
 	int nid = (int) data;
 	struct page *newpage;
 
<span class="p_del">-	newpage = alloc_pages_exact_node(nid,</span>
<span class="p_add">+	newpage = __alloc_pages_node(nid,</span>
 					 (GFP_HIGHUSER_MOVABLE |
 					  __GFP_THISNODE | __GFP_NOMEMALLOC |
 					  __GFP_NORETRY | __GFP_NOWARN) &amp;
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index c1024db..35f1d20 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -3518,8 +3518,6 @@</span> <span class="p_context"> EXPORT_SYMBOL(alloc_pages_exact);</span>
  *
  * Like alloc_pages_exact(), but try to allocate on node nid first before falling
  * back.
<span class="p_del">- * Note this is not alloc_pages_exact_node() which allocates on a specific node,</span>
<span class="p_del">- * but is not exact.</span>
  */
 void * __meminit alloc_pages_exact_nid(int nid, size_t size, gfp_t gfp_mask)
 {
<span class="p_header">diff --git a/mm/slab.c b/mm/slab.c</span>
<span class="p_header">index bf7169c..d890750 100644</span>
<span class="p_header">--- a/mm/slab.c</span>
<span class="p_header">+++ b/mm/slab.c</span>
<span class="p_chunk">@@ -1595,7 +1595,7 @@</span> <span class="p_context"> static struct page *kmem_getpages(struct kmem_cache *cachep, gfp_t flags,</span>
 	if (memcg_charge_slab(cachep, flags, cachep-&gt;gfporder))
 		return NULL;
 
<span class="p_del">-	page = alloc_pages_exact_node(nodeid, flags | __GFP_NOTRACK, cachep-&gt;gfporder);</span>
<span class="p_add">+	page = __alloc_pages_node(nodeid, flags | __GFP_NOTRACK, cachep-&gt;gfporder);</span>
 	if (!page) {
 		memcg_uncharge_slab(cachep, cachep-&gt;gfporder);
 		slab_out_of_memory(cachep, flags, nodeid);
<span class="p_header">diff --git a/mm/slob.c b/mm/slob.c</span>
<span class="p_header">index 165bbd3..0d7e5df 100644</span>
<span class="p_header">--- a/mm/slob.c</span>
<span class="p_header">+++ b/mm/slob.c</span>
<span class="p_chunk">@@ -45,7 +45,7 @@</span> <span class="p_context"></span>
  * NUMA support in SLOB is fairly simplistic, pushing most of the real
  * logic down to the page allocator, and simply doing the node accounting
  * on the upper levels. In the event that a node id is explicitly
<span class="p_del">- * provided, alloc_pages_exact_node() with the specified node id is used</span>
<span class="p_add">+ * provided, __alloc_pages_node() with the specified node id is used</span>
  * instead. The common case (or when the node id isn&#39;t explicitly provided)
  * will default to the current node, as per numa_node_id().
  *
<span class="p_chunk">@@ -193,7 +193,7 @@</span> <span class="p_context"> static void *slob_new_pages(gfp_t gfp, int order, int node)</span>
 
 #ifdef CONFIG_NUMA
 	if (node != NUMA_NO_NODE)
<span class="p_del">-		page = alloc_pages_exact_node(node, gfp, order);</span>
<span class="p_add">+		page = __alloc_pages_node(node, gfp, order);</span>
 	else
 #endif
 		page = alloc_pages(gfp, order);
<span class="p_header">diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="p_header">index 8987bd5..e180f8d 100644</span>
<span class="p_header">--- a/mm/slub.c</span>
<span class="p_header">+++ b/mm/slub.c</span>
<span class="p_chunk">@@ -1336,7 +1336,7 @@</span> <span class="p_context"> static inline struct page *alloc_slab_page(struct kmem_cache *s,</span>
 	if (node == NUMA_NO_NODE)
 		page = alloc_pages(flags, order);
 	else
<span class="p_del">-		page = alloc_pages_exact_node(node, flags, order);</span>
<span class="p_add">+		page = __alloc_pages_node(node, flags, order);</span>
 
 	if (!page)
 		memcg_uncharge_slab(s, order);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



