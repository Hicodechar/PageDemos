
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[05/10] mm, page_alloc: Distinguish between being unable to sleep, unwilling to sleep and avoiding waking kswapd - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [05/10] mm, page_alloc: Distinguish between being unable to sleep, unwilling to sleep and avoiding waking kswapd</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 21, 2015, 10:52 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1442832762-7247-6-git-send-email-mgorman@techsingularity.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7229221/mbox/"
   >mbox</a>
|
   <a href="/patch/7229221/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7229221/">/patch/7229221/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 35C2E9F380
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 21 Sep 2015 10:53:47 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 6A5F620721
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 21 Sep 2015 10:53:43 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 5120320727
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 21 Sep 2015 10:53:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932343AbbIUKxh (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 21 Sep 2015 06:53:37 -0400
Received: from outbound-smtp05.blacknight.com ([81.17.249.38]:41991 &quot;EHLO
	outbound-smtp05.blacknight.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S932214AbbIUKwq (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 21 Sep 2015 06:52:46 -0400
Received: from mail.blacknight.com (pemlinmail05.blacknight.ie
	[81.17.254.26])
	by outbound-smtp05.blacknight.com (Postfix) with ESMTPS id 865C69901C
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 21 Sep 2015 10:52:44 +0000 (UTC)
Received: (qmail 12347 invoked from network); 21 Sep 2015 10:52:44 -0000
Received: from unknown (HELO stampy.163woodhaven.lan)
	(mgorman@techsingularity.net@[37.228.245.0])
	by 81.17.254.9 with ESMTPA; 21 Sep 2015 10:52:44 -0000
From: Mel Gorman &lt;mgorman@techsingularity.net&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;, David Rientjes &lt;rientjes@google.com&gt;,
	Joonsoo Kim &lt;iamjoonsoo.kim@lge.com&gt;, Michal Hocko &lt;mhocko@kernel.org&gt;,
	Linux-MM &lt;linux-mm@kvack.org&gt;, LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;
Subject: [PATCH 05/10] mm,
	page_alloc: Distinguish between being unable to sleep,
	unwilling to sleep and avoiding waking kswapd
Date: Mon, 21 Sep 2015 11:52:37 +0100
Message-Id: &lt;1442832762-7247-6-git-send-email-mgorman@techsingularity.net&gt;
X-Mailer: git-send-email 2.4.6
In-Reply-To: &lt;1442832762-7247-1-git-send-email-mgorman@techsingularity.net&gt;
References: &lt;1442832762-7247-1-git-send-email-mgorman@techsingularity.net&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - Sept. 21, 2015, 10:52 a.m.</div>
<pre class="content">
__GFP_WAIT has been used to identify atomic context in callers that hold
spinlocks or are in interrupts. They are expected to be high priority and
have access one of two watermarks lower than &quot;min&quot; which can be referred
to as the &quot;atomic reserve&quot;. __GFP_HIGH users get access to the first lower
watermark and can be called the &quot;high priority reserve&quot;.

Over time, callers had a requirement to not block when fallback options
were available. Some have abused __GFP_WAIT leading to a situation where
an optimisitic allocation with a fallback option can access atomic reserves.

This patch uses __GFP_ATOMIC to identify callers that are truely atomic,
cannot sleep and have no alternative. High priority users continue to use
__GFP_HIGH. __GFP_DIRECT_RECLAIM identifies callers that can sleep and are
willing to enter direct reclaim. __GFP_KSWAPD_RECLAIM to identify callers
that want to wake kswapd for background reclaim. __GFP_WAIT is redefined
as a caller that is willing to enter direct reclaim and wake kswapd for
background reclaim.

This patch then converts a number of sites

o __GFP_ATOMIC is used by callers that are high priority and have memory
  pools for those requests. GFP_ATOMIC uses this flag.

o Callers that have a limited mempool to guarantee forward progress clear
  __GFP_DIRECT_RECLAIM but keep __GFP_KSWAPD_RECLAIM. bio allocations fall
  into this category where kswapd will still be woken but atomic reserves
  are not used as there is a one-entry mempool to guarantee progress.

o Callers that are checking if they are non-blocking should use the
  helper gfpflags_allow_blocking() where possible. This is because
  checking for __GFP_WAIT as was done historically now can trigger false
  positives. Some exceptions like dm-crypt.c exist where the code intent
  is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to
  flag manipulations.

o Callers that built their own GFP flags instead of starting with GFP_KERNEL
  and friends now also need to specify __GFP_KSWAPD_RECLAIM.

The first key hazard to watch out for is callers that removed __GFP_WAIT
and was depending on access to atomic reserves for inconspicuous reasons.
In some cases it may be appropriate for them to use __GFP_HIGH.

The second key hazard is callers that assembled their own combination of
GFP flags instead of starting with something like GFP_KERNEL. They may
now wish to specify __GFP_KSWAPD_RECLAIM. It&#39;s almost certainly harmless
if it&#39;s missed in most cases as other activity will wake kswapd.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="acked-by">Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
---
 Documentation/vm/balance                           | 14 ++++---
 arch/arm/mm/dma-mapping.c                          |  6 +--
 arch/arm/xen/mm.c                                  |  2 +-
 arch/arm64/mm/dma-mapping.c                        |  4 +-
 arch/x86/kernel/pci-dma.c                          |  2 +-
 block/bio.c                                        | 26 ++++++------
 block/blk-core.c                                   | 16 ++++----
 block/blk-ioc.c                                    |  2 +-
 block/blk-mq-tag.c                                 |  2 +-
 block/blk-mq.c                                     |  8 ++--
 drivers/block/drbd/drbd_receiver.c                 |  3 +-
 drivers/block/osdblk.c                             |  2 +-
 drivers/connector/connector.c                      |  3 +-
 drivers/firewire/core-cdev.c                       |  2 +-
 drivers/gpu/drm/i915/i915_gem.c                    |  2 +-
 drivers/infiniband/core/sa_query.c                 |  2 +-
 drivers/iommu/amd_iommu.c                          |  2 +-
 drivers/iommu/intel-iommu.c                        |  2 +-
 drivers/md/dm-crypt.c                              |  6 +--
 drivers/md/dm-kcopyd.c                             |  2 +-
 drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c     |  2 +-
 drivers/media/pci/solo6x10/solo6x10-v4l2.c         |  2 +-
 drivers/media/pci/tw68/tw68-video.c                |  2 +-
 drivers/mtd/mtdcore.c                              |  3 +-
 drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c    |  2 +-
 drivers/staging/android/ion/ion_system_heap.c      |  2 +-
 .../lustre/include/linux/libcfs/libcfs_private.h   |  2 +-
 drivers/usb/host/u132-hcd.c                        |  2 +-
 drivers/video/fbdev/vermilion/vermilion.c          |  2 +-
 fs/btrfs/disk-io.c                                 |  2 +-
 fs/btrfs/extent_io.c                               | 14 +++----
 fs/btrfs/volumes.c                                 |  4 +-
 fs/ext4/super.c                                    |  2 +-
 fs/fscache/cookie.c                                |  2 +-
 fs/fscache/page.c                                  |  6 +--
 fs/jbd2/transaction.c                              |  4 +-
 fs/nfs/file.c                                      |  6 +--
 fs/xfs/xfs_qm.c                                    |  2 +-
 include/linux/gfp.h                                | 46 ++++++++++++++++------
 include/linux/skbuff.h                             |  6 +--
 include/net/sock.h                                 |  2 +-
 include/trace/events/gfpflags.h                    |  5 ++-
 kernel/audit.c                                     |  6 +--
 kernel/cgroup.c                                    |  2 +-
 kernel/locking/lockdep.c                           |  2 +-
 kernel/power/snapshot.c                            |  2 +-
 kernel/smp.c                                       |  2 +-
 lib/idr.c                                          |  4 +-
 lib/radix-tree.c                                   | 10 ++---
 mm/backing-dev.c                                   |  2 +-
 mm/dmapool.c                                       |  2 +-
 mm/memcontrol.c                                    |  8 ++--
 mm/mempool.c                                       | 10 ++---
 mm/migrate.c                                       |  2 +-
 mm/page_alloc.c                                    | 43 ++++++++++++--------
 mm/slab.c                                          | 18 ++++-----
 mm/slub.c                                          | 10 ++---
 mm/vmalloc.c                                       |  2 +-
 mm/vmscan.c                                        |  4 +-
 mm/zswap.c                                         |  5 ++-
 net/core/skbuff.c                                  |  8 ++--
 net/core/sock.c                                    |  6 ++-
 net/netlink/af_netlink.c                           |  2 +-
 net/rds/ib_recv.c                                  |  4 +-
 net/rxrpc/ar-connection.c                          |  2 +-
 net/sctp/associola.c                               |  2 +-
 66 files changed, 212 insertions(+), 174 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Sept. 24, 2015, 1:51 p.m.</div>
<pre class="content">
On Mon 21-09-15 11:52:37, Mel Gorman wrote:
<span class="quote">&gt; __GFP_WAIT has been used to identify atomic context in callers that hold</span>
<span class="quote">&gt; spinlocks or are in interrupts. They are expected to be high priority and</span>
<span class="quote">&gt; have access one of two watermarks lower than &quot;min&quot; which can be referred</span>
<span class="quote">&gt; to as the &quot;atomic reserve&quot;. __GFP_HIGH users get access to the first lower</span>
<span class="quote">&gt; watermark and can be called the &quot;high priority reserve&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Over time, callers had a requirement to not block when fallback options</span>
<span class="quote">&gt; were available. Some have abused __GFP_WAIT leading to a situation where</span>
<span class="quote">&gt; an optimisitic allocation with a fallback option can access atomic reserves.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch uses __GFP_ATOMIC to identify callers that are truely atomic,</span>
<span class="quote">&gt; cannot sleep and have no alternative. High priority users continue to use</span>
<span class="quote">&gt; __GFP_HIGH. __GFP_DIRECT_RECLAIM identifies callers that can sleep and are</span>
<span class="quote">&gt; willing to enter direct reclaim. __GFP_KSWAPD_RECLAIM to identify callers</span>
<span class="quote">&gt; that want to wake kswapd for background reclaim. __GFP_WAIT is redefined</span>
<span class="quote">&gt; as a caller that is willing to enter direct reclaim and wake kswapd for</span>
<span class="quote">&gt; background reclaim.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch then converts a number of sites</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; o __GFP_ATOMIC is used by callers that are high priority and have memory</span>
<span class="quote">&gt;   pools for those requests. GFP_ATOMIC uses this flag.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; o Callers that have a limited mempool to guarantee forward progress clear</span>
<span class="quote">&gt;   __GFP_DIRECT_RECLAIM but keep __GFP_KSWAPD_RECLAIM. bio allocations fall</span>
<span class="quote">&gt;   into this category where kswapd will still be woken but atomic reserves</span>
<span class="quote">&gt;   are not used as there is a one-entry mempool to guarantee progress.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; o Callers that are checking if they are non-blocking should use the</span>
<span class="quote">&gt;   helper gfpflags_allow_blocking() where possible. This is because</span>
<span class="quote">&gt;   checking for __GFP_WAIT as was done historically now can trigger false</span>
<span class="quote">&gt;   positives. Some exceptions like dm-crypt.c exist where the code intent</span>
<span class="quote">&gt;   is clearer if __GFP_DIRECT_RECLAIM is used instead of the helper due to</span>
<span class="quote">&gt;   flag manipulations.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; o Callers that built their own GFP flags instead of starting with GFP_KERNEL</span>
<span class="quote">&gt;   and friends now also need to specify __GFP_KSWAPD_RECLAIM.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The first key hazard to watch out for is callers that removed __GFP_WAIT</span>
<span class="quote">&gt; and was depending on access to atomic reserves for inconspicuous reasons.</span>
<span class="quote">&gt; In some cases it may be appropriate for them to use __GFP_HIGH.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The second key hazard is callers that assembled their own combination of</span>
<span class="quote">&gt; GFP flags instead of starting with something like GFP_KERNEL. They may</span>
<span class="quote">&gt; now wish to specify __GFP_KSWAPD_RECLAIM. It&#39;s almost certainly harmless</span>
<span class="quote">&gt; if it&#39;s missed in most cases as other activity will wake kswapd.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>

I belive I&#39;ve checked this one and acked it already. Anyway
<span class="acked-by">Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  Documentation/vm/balance                           | 14 ++++---</span>
<span class="quote">&gt;  arch/arm/mm/dma-mapping.c                          |  6 +--</span>
<span class="quote">&gt;  arch/arm/xen/mm.c                                  |  2 +-</span>
<span class="quote">&gt;  arch/arm64/mm/dma-mapping.c                        |  4 +-</span>
<span class="quote">&gt;  arch/x86/kernel/pci-dma.c                          |  2 +-</span>
<span class="quote">&gt;  block/bio.c                                        | 26 ++++++------</span>
<span class="quote">&gt;  block/blk-core.c                                   | 16 ++++----</span>
<span class="quote">&gt;  block/blk-ioc.c                                    |  2 +-</span>
<span class="quote">&gt;  block/blk-mq-tag.c                                 |  2 +-</span>
<span class="quote">&gt;  block/blk-mq.c                                     |  8 ++--</span>
<span class="quote">&gt;  drivers/block/drbd/drbd_receiver.c                 |  3 +-</span>
<span class="quote">&gt;  drivers/block/osdblk.c                             |  2 +-</span>
<span class="quote">&gt;  drivers/connector/connector.c                      |  3 +-</span>
<span class="quote">&gt;  drivers/firewire/core-cdev.c                       |  2 +-</span>
<span class="quote">&gt;  drivers/gpu/drm/i915/i915_gem.c                    |  2 +-</span>
<span class="quote">&gt;  drivers/infiniband/core/sa_query.c                 |  2 +-</span>
<span class="quote">&gt;  drivers/iommu/amd_iommu.c                          |  2 +-</span>
<span class="quote">&gt;  drivers/iommu/intel-iommu.c                        |  2 +-</span>
<span class="quote">&gt;  drivers/md/dm-crypt.c                              |  6 +--</span>
<span class="quote">&gt;  drivers/md/dm-kcopyd.c                             |  2 +-</span>
<span class="quote">&gt;  drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c     |  2 +-</span>
<span class="quote">&gt;  drivers/media/pci/solo6x10/solo6x10-v4l2.c         |  2 +-</span>
<span class="quote">&gt;  drivers/media/pci/tw68/tw68-video.c                |  2 +-</span>
<span class="quote">&gt;  drivers/mtd/mtdcore.c                              |  3 +-</span>
<span class="quote">&gt;  drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c    |  2 +-</span>
<span class="quote">&gt;  drivers/staging/android/ion/ion_system_heap.c      |  2 +-</span>
<span class="quote">&gt;  .../lustre/include/linux/libcfs/libcfs_private.h   |  2 +-</span>
<span class="quote">&gt;  drivers/usb/host/u132-hcd.c                        |  2 +-</span>
<span class="quote">&gt;  drivers/video/fbdev/vermilion/vermilion.c          |  2 +-</span>
<span class="quote">&gt;  fs/btrfs/disk-io.c                                 |  2 +-</span>
<span class="quote">&gt;  fs/btrfs/extent_io.c                               | 14 +++----</span>
<span class="quote">&gt;  fs/btrfs/volumes.c                                 |  4 +-</span>
<span class="quote">&gt;  fs/ext4/super.c                                    |  2 +-</span>
<span class="quote">&gt;  fs/fscache/cookie.c                                |  2 +-</span>
<span class="quote">&gt;  fs/fscache/page.c                                  |  6 +--</span>
<span class="quote">&gt;  fs/jbd2/transaction.c                              |  4 +-</span>
<span class="quote">&gt;  fs/nfs/file.c                                      |  6 +--</span>
<span class="quote">&gt;  fs/xfs/xfs_qm.c                                    |  2 +-</span>
<span class="quote">&gt;  include/linux/gfp.h                                | 46 ++++++++++++++++------</span>
<span class="quote">&gt;  include/linux/skbuff.h                             |  6 +--</span>
<span class="quote">&gt;  include/net/sock.h                                 |  2 +-</span>
<span class="quote">&gt;  include/trace/events/gfpflags.h                    |  5 ++-</span>
<span class="quote">&gt;  kernel/audit.c                                     |  6 +--</span>
<span class="quote">&gt;  kernel/cgroup.c                                    |  2 +-</span>
<span class="quote">&gt;  kernel/locking/lockdep.c                           |  2 +-</span>
<span class="quote">&gt;  kernel/power/snapshot.c                            |  2 +-</span>
<span class="quote">&gt;  kernel/smp.c                                       |  2 +-</span>
<span class="quote">&gt;  lib/idr.c                                          |  4 +-</span>
<span class="quote">&gt;  lib/radix-tree.c                                   | 10 ++---</span>
<span class="quote">&gt;  mm/backing-dev.c                                   |  2 +-</span>
<span class="quote">&gt;  mm/dmapool.c                                       |  2 +-</span>
<span class="quote">&gt;  mm/memcontrol.c                                    |  8 ++--</span>
<span class="quote">&gt;  mm/mempool.c                                       | 10 ++---</span>
<span class="quote">&gt;  mm/migrate.c                                       |  2 +-</span>
<span class="quote">&gt;  mm/page_alloc.c                                    | 43 ++++++++++++--------</span>
<span class="quote">&gt;  mm/slab.c                                          | 18 ++++-----</span>
<span class="quote">&gt;  mm/slub.c                                          | 10 ++---</span>
<span class="quote">&gt;  mm/vmalloc.c                                       |  2 +-</span>
<span class="quote">&gt;  mm/vmscan.c                                        |  4 +-</span>
<span class="quote">&gt;  mm/zswap.c                                         |  5 ++-</span>
<span class="quote">&gt;  net/core/skbuff.c                                  |  8 ++--</span>
<span class="quote">&gt;  net/core/sock.c                                    |  6 ++-</span>
<span class="quote">&gt;  net/netlink/af_netlink.c                           |  2 +-</span>
<span class="quote">&gt;  net/rds/ib_recv.c                                  |  4 +-</span>
<span class="quote">&gt;  net/rxrpc/ar-connection.c                          |  2 +-</span>
<span class="quote">&gt;  net/sctp/associola.c                               |  2 +-</span>
<span class="quote">&gt;  66 files changed, 212 insertions(+), 174 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/Documentation/vm/balance b/Documentation/vm/balance</span>
<span class="quote">&gt; index c46e68cf9344..964595481af6 100644</span>
<span class="quote">&gt; --- a/Documentation/vm/balance</span>
<span class="quote">&gt; +++ b/Documentation/vm/balance</span>
<span class="quote">&gt; @@ -1,12 +1,14 @@</span>
<span class="quote">&gt;  Started Jan 2000 by Kanoj Sarcar &lt;kanoj@sgi.com&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -Memory balancing is needed for non __GFP_WAIT as well as for non</span>
<span class="quote">&gt; -__GFP_IO allocations.</span>
<span class="quote">&gt; +Memory balancing is needed for !__GFP_ATOMIC and !__GFP_KSWAPD_RECLAIM as</span>
<span class="quote">&gt; +well as for non __GFP_IO allocations.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -There are two reasons to be requesting non __GFP_WAIT allocations:</span>
<span class="quote">&gt; -the caller can not sleep (typically intr context), or does not want</span>
<span class="quote">&gt; -to incur cost overheads of page stealing and possible swap io for</span>
<span class="quote">&gt; -whatever reasons.</span>
<span class="quote">&gt; +The first reason why a caller may avoid reclaim is that the caller can not</span>
<span class="quote">&gt; +sleep due to holding a spinlock or is in interrupt context. The second may</span>
<span class="quote">&gt; +be that the caller is willing to fail the allocation without incurring the</span>
<span class="quote">&gt; +overhead of page reclaim. This may happen for opportunistic high-order</span>
<span class="quote">&gt; +allocation requests that have order-0 fallback options. In such cases,</span>
<span class="quote">&gt; +the caller may also wish to avoid waking kswapd.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  __GFP_IO allocation requests are made to prevent file system deadlocks.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; index 1a7815e5421b..38307d8312ac 100644</span>
<span class="quote">&gt; --- a/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -651,12 +651,12 @@ static void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (nommu())</span>
<span class="quote">&gt;  		addr = __alloc_simple_buffer(dev, size, gfp, &amp;page);</span>
<span class="quote">&gt; -	else if (dev_get_cma_area(dev) &amp;&amp; (gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	else if (dev_get_cma_area(dev) &amp;&amp; (gfp &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		addr = __alloc_from_contiguous(dev, size, prot, &amp;page,</span>
<span class="quote">&gt;  					       caller, want_vaddr);</span>
<span class="quote">&gt;  	else if (is_coherent)</span>
<span class="quote">&gt;  		addr = __alloc_simple_buffer(dev, size, gfp, &amp;page);</span>
<span class="quote">&gt; -	else if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	else if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt;  		addr = __alloc_from_pool(size, &amp;page);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  		addr = __alloc_remap_buffer(dev, size, gfp, prot, &amp;page,</span>
<span class="quote">&gt; @@ -1363,7 +1363,7 @@ static void *arm_iommu_alloc_attrs(struct device *dev, size_t size,</span>
<span class="quote">&gt;  	*handle = DMA_ERROR_CODE;</span>
<span class="quote">&gt;  	size = PAGE_ALIGN(size);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt;  		return __iommu_alloc_atomic(dev, size, handle);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; index 6dd911d1f0ac..99eec9063f68 100644</span>
<span class="quote">&gt; --- a/arch/arm/xen/mm.c</span>
<span class="quote">&gt; +++ b/arch/arm/xen/mm.c</span>
<span class="quote">&gt; @@ -25,7 +25,7 @@</span>
<span class="quote">&gt;  unsigned long xen_get_swiotlb_free_pages(unsigned int order)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct memblock_region *reg;</span>
<span class="quote">&gt; -	gfp_t flags = __GFP_NOWARN;</span>
<span class="quote">&gt; +	gfp_t flags = __GFP_NOWARN|__GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_memblock(memory, reg) {</span>
<span class="quote">&gt;  		if (reg-&gt;base &lt; (phys_addr_t)0xffffffff) {</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; index 99224dcebdc5..478234383c2c 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -100,7 +100,7 @@ static void *__dma_alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;  	if (IS_ENABLED(CONFIG_ZONE_DMA) &amp;&amp;</span>
<span class="quote">&gt;  	    dev-&gt;coherent_dma_mask &lt;= DMA_BIT_MASK(32))</span>
<span class="quote">&gt;  		flags |= GFP_DMA;</span>
<span class="quote">&gt; -	if (dev_get_cma_area(dev) &amp;&amp; (flags &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (dev_get_cma_area(dev) &amp;&amp; gfpflags_allow_blocking(flags)) {</span>
<span class="quote">&gt;  		struct page *page;</span>
<span class="quote">&gt;  		void *addr;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -148,7 +148,7 @@ static void *__dma_alloc(struct device *dev, size_t size,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	size = PAGE_ALIGN(size);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!coherent &amp;&amp; !(flags &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (!coherent &amp;&amp; !gfpflags_allow_blocking(flags)) {</span>
<span class="quote">&gt;  		struct page *page = NULL;</span>
<span class="quote">&gt;  		void *addr = __alloc_from_pool(size, &amp;page, flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; index 1b55de1267cf..a8e618b16a66 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/pci-dma.c</span>
<span class="quote">&gt; @@ -90,7 +90,7 @@ void *dma_generic_alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt;  	page = NULL;</span>
<span class="quote">&gt;  	/* CMA can be used only in the context which permits sleeping */</span>
<span class="quote">&gt; -	if (flag &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(flag)) {</span>
<span class="quote">&gt;  		page = dma_alloc_from_contiguous(dev, count, get_order(size));</span>
<span class="quote">&gt;  		if (page &amp;&amp; page_to_phys(page) + size &gt; dma_mask) {</span>
<span class="quote">&gt;  			dma_release_from_contiguous(dev, page, count);</span>
<span class="quote">&gt; diff --git a/block/bio.c b/block/bio.c</span>
<span class="quote">&gt; index ad3f276d74bc..4f184d938942 100644</span>
<span class="quote">&gt; --- a/block/bio.c</span>
<span class="quote">&gt; +++ b/block/bio.c</span>
<span class="quote">&gt; @@ -211,7 +211,7 @@ struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,</span>
<span class="quote">&gt;  		bvl = mempool_alloc(pool, gfp_mask);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt;  		struct biovec_slab *bvs = bvec_slabs + *idx;</span>
<span class="quote">&gt; -		gfp_t __gfp_mask = gfp_mask &amp; ~(__GFP_WAIT | __GFP_IO);</span>
<span class="quote">&gt; +		gfp_t __gfp_mask = gfp_mask &amp; ~(__GFP_DIRECT_RECLAIM | __GFP_IO);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Make this allocation restricted and don&#39;t dump info on</span>
<span class="quote">&gt; @@ -221,11 +221,11 @@ struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,</span>
<span class="quote">&gt;  		__gfp_mask |= __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; -		 * Try a slab allocation. If this fails and __GFP_WAIT</span>
<span class="quote">&gt; +		 * Try a slab allocation. If this fails and __GFP_DIRECT_RECLAIM</span>
<span class="quote">&gt;  		 * is set, retry with the 1-entry mempool</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		bvl = kmem_cache_alloc(bvs-&gt;slab, __gfp_mask);</span>
<span class="quote">&gt; -		if (unlikely(!bvl &amp;&amp; (gfp_mask &amp; __GFP_WAIT))) {</span>
<span class="quote">&gt; +		if (unlikely(!bvl &amp;&amp; (gfp_mask &amp; __GFP_DIRECT_RECLAIM))) {</span>
<span class="quote">&gt;  			*idx = BIOVEC_MAX_IDX;</span>
<span class="quote">&gt;  			goto fallback;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -395,12 +395,12 @@ static void punt_bios_to_rescuer(struct bio_set *bs)</span>
<span class="quote">&gt;   *   If @bs is NULL, uses kmalloc() to allocate the bio; else the allocation is</span>
<span class="quote">&gt;   *   backed by the @bs&#39;s mempool.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - *   When @bs is not NULL, if %__GFP_WAIT is set then bio_alloc will always be</span>
<span class="quote">&gt; - *   able to allocate a bio. This is due to the mempool guarantees. To make this</span>
<span class="quote">&gt; - *   work, callers must never allocate more than 1 bio at a time from this pool.</span>
<span class="quote">&gt; - *   Callers that need to allocate more than 1 bio must always submit the</span>
<span class="quote">&gt; - *   previously allocated bio for IO before attempting to allocate a new one.</span>
<span class="quote">&gt; - *   Failure to do so can cause deadlocks under memory pressure.</span>
<span class="quote">&gt; + *   When @bs is not NULL, if %__GFP_DIRECT_RECLAIM is set then bio_alloc will</span>
<span class="quote">&gt; + *   always be able to allocate a bio. This is due to the mempool guarantees.</span>
<span class="quote">&gt; + *   To make this work, callers must never allocate more than 1 bio at a time</span>
<span class="quote">&gt; + *   from this pool. Callers that need to allocate more than 1 bio must always</span>
<span class="quote">&gt; + *   submit the previously allocated bio for IO before attempting to allocate</span>
<span class="quote">&gt; + *   a new one. Failure to do so can cause deadlocks under memory pressure.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   *   Note that when running under generic_make_request() (i.e. any block</span>
<span class="quote">&gt;   *   driver), bios are not submitted until after you return - see the code in</span>
<span class="quote">&gt; @@ -459,13 +459,13 @@ struct bio *bio_alloc_bioset(gfp_t gfp_mask, int nr_iovecs, struct bio_set *bs)</span>
<span class="quote">&gt;  		 * We solve this, and guarantee forward progress, with a rescuer</span>
<span class="quote">&gt;  		 * workqueue per bio_set. If we go to allocate and there are</span>
<span class="quote">&gt;  		 * bios on current-&gt;bio_list, we first try the allocation</span>
<span class="quote">&gt; -		 * without __GFP_WAIT; if that fails, we punt those bios we</span>
<span class="quote">&gt; -		 * would be blocking to the rescuer workqueue before we retry</span>
<span class="quote">&gt; -		 * with the original gfp_flags.</span>
<span class="quote">&gt; +		 * without __GFP_DIRECT_RECLAIM; if that fails, we punt those</span>
<span class="quote">&gt; +		 * bios we would be blocking to the rescuer workqueue before</span>
<span class="quote">&gt; +		 * we retry with the original gfp_flags.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (current-&gt;bio_list &amp;&amp; !bio_list_empty(current-&gt;bio_list))</span>
<span class="quote">&gt; -			gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +			gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		p = mempool_alloc(bs-&gt;bio_pool, gfp_mask);</span>
<span class="quote">&gt;  		if (!p &amp;&amp; gfp_mask != saved_gfp) {</span>
<span class="quote">&gt; diff --git a/block/blk-core.c b/block/blk-core.c</span>
<span class="quote">&gt; index 2eb722d48773..0391206868e9 100644</span>
<span class="quote">&gt; --- a/block/blk-core.c</span>
<span class="quote">&gt; +++ b/block/blk-core.c</span>
<span class="quote">&gt; @@ -1160,8 +1160,8 @@ static struct request *__get_request(struct request_list *rl, int rw_flags,</span>
<span class="quote">&gt;   * @bio: bio to allocate request for (can be %NULL)</span>
<span class="quote">&gt;   * @gfp_mask: allocation mask</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * Get a free request from @q.  If %__GFP_WAIT is set in @gfp_mask, this</span>
<span class="quote">&gt; - * function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="quote">&gt; + * Get a free request from @q.  If %__GFP_DIRECT_RECLAIM is set in @gfp_mask,</span>
<span class="quote">&gt; + * this function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Must be called with @q-&gt;queue_lock held and,</span>
<span class="quote">&gt;   * Returns ERR_PTR on failure, with @q-&gt;queue_lock held.</span>
<span class="quote">&gt; @@ -1181,7 +1181,7 @@ static struct request *get_request(struct request_queue *q, int rw_flags,</span>
<span class="quote">&gt;  	if (!IS_ERR(rq))</span>
<span class="quote">&gt;  		return rq;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT) || unlikely(blk_queue_dying(q))) {</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp_mask) || unlikely(blk_queue_dying(q))) {</span>
<span class="quote">&gt;  		blk_put_rl(rl);</span>
<span class="quote">&gt;  		return rq;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1259,11 +1259,11 @@ EXPORT_SYMBOL(blk_get_request);</span>
<span class="quote">&gt;   * BUG.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * WARNING: When allocating/cloning a bio-chain, careful consideration should be</span>
<span class="quote">&gt; - * given to how you allocate bios. In particular, you cannot use __GFP_WAIT for</span>
<span class="quote">&gt; - * anything but the first bio in the chain. Otherwise you risk waiting for IO</span>
<span class="quote">&gt; - * completion of a bio that hasn&#39;t been submitted yet, thus resulting in a</span>
<span class="quote">&gt; - * deadlock. Alternatively bios should be allocated using bio_kmalloc() instead</span>
<span class="quote">&gt; - * of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="quote">&gt; + * given to how you allocate bios. In particular, you cannot use</span>
<span class="quote">&gt; + * __GFP_DIRECT_RECLAIM for anything but the first bio in the chain. Otherwise</span>
<span class="quote">&gt; + * you risk waiting for IO completion of a bio that hasn&#39;t been submitted yet,</span>
<span class="quote">&gt; + * thus resulting in a deadlock. Alternatively bios should be allocated using</span>
<span class="quote">&gt; + * bio_kmalloc() instead of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="quote">&gt;   * If possible a big IO should be split into smaller parts when allocation</span>
<span class="quote">&gt;   * fails. Partial allocation should not be an error, or you risk a live-lock.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; diff --git a/block/blk-ioc.c b/block/blk-ioc.c</span>
<span class="quote">&gt; index 1a27f45ec776..381cb50a673c 100644</span>
<span class="quote">&gt; --- a/block/blk-ioc.c</span>
<span class="quote">&gt; +++ b/block/blk-ioc.c</span>
<span class="quote">&gt; @@ -289,7 +289,7 @@ struct io_context *get_task_io_context(struct task_struct *task,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct io_context *ioc;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt;  		task_lock(task);</span>
<span class="quote">&gt; diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c</span>
<span class="quote">&gt; index 9115c6d59948..f6020c624967 100644</span>
<span class="quote">&gt; --- a/block/blk-mq-tag.c</span>
<span class="quote">&gt; +++ b/block/blk-mq-tag.c</span>
<span class="quote">&gt; @@ -264,7 +264,7 @@ static int bt_get(struct blk_mq_alloc_data *data,</span>
<span class="quote">&gt;  	if (tag != -1)</span>
<span class="quote">&gt;  		return tag;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!(data-&gt;gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(data-&gt;gfp))</span>
<span class="quote">&gt;  		return -1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	bs = bt_wait_ptr(bt, hctx);</span>
<span class="quote">&gt; diff --git a/block/blk-mq.c b/block/blk-mq.c</span>
<span class="quote">&gt; index f2d67b4047a0..7c322cea838f 100644</span>
<span class="quote">&gt; --- a/block/blk-mq.c</span>
<span class="quote">&gt; +++ b/block/blk-mq.c</span>
<span class="quote">&gt; @@ -85,7 +85,7 @@ static int blk_mq_queue_enter(struct request_queue *q, gfp_t gfp)</span>
<span class="quote">&gt;  		if (percpu_ref_tryget_live(&amp;q-&gt;mq_usage_counter))</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		if (!(gfp &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +		if (!gfpflags_allow_blocking(gfp))</span>
<span class="quote">&gt;  			return -EBUSY;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		ret = wait_event_interruptible(q-&gt;mq_freeze_wq,</span>
<span class="quote">&gt; @@ -261,11 +261,11 @@ struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ctx = blk_mq_get_ctx(q);</span>
<span class="quote">&gt;  	hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);</span>
<span class="quote">&gt; -	blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_WAIT,</span>
<span class="quote">&gt; +	blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_DIRECT_RECLAIM,</span>
<span class="quote">&gt;  			reserved, ctx, hctx);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	rq = __blk_mq_alloc_request(&amp;alloc_data, rw);</span>
<span class="quote">&gt; -	if (!rq &amp;&amp; (gfp &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (!rq &amp;&amp; (gfp &amp; __GFP_DIRECT_RECLAIM)) {</span>
<span class="quote">&gt;  		__blk_mq_run_hw_queue(hctx);</span>
<span class="quote">&gt;  		blk_mq_put_ctx(ctx);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1207,7 +1207,7 @@ static struct request *blk_mq_map_request(struct request_queue *q,</span>
<span class="quote">&gt;  		ctx = blk_mq_get_ctx(q);</span>
<span class="quote">&gt;  		hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);</span>
<span class="quote">&gt;  		blk_mq_set_alloc_data(&amp;alloc_data, q,</span>
<span class="quote">&gt; -				__GFP_WAIT|GFP_ATOMIC, false, ctx, hctx);</span>
<span class="quote">&gt; +				__GFP_WAIT|__GFP_HIGH, false, ctx, hctx);</span>
<span class="quote">&gt;  		rq = __blk_mq_alloc_request(&amp;alloc_data, rw);</span>
<span class="quote">&gt;  		ctx = alloc_data.ctx;</span>
<span class="quote">&gt;  		hctx = alloc_data.hctx;</span>
<span class="quote">&gt; diff --git a/drivers/block/drbd/drbd_receiver.c b/drivers/block/drbd/drbd_receiver.c</span>
<span class="quote">&gt; index c097909c589c..b4b5680ac6ad 100644</span>
<span class="quote">&gt; --- a/drivers/block/drbd/drbd_receiver.c</span>
<span class="quote">&gt; +++ b/drivers/block/drbd/drbd_receiver.c</span>
<span class="quote">&gt; @@ -357,7 +357,8 @@ drbd_alloc_peer_req(struct drbd_peer_device *peer_device, u64 id, sector_t secto</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (has_payload &amp;&amp; data_size) {</span>
<span class="quote">&gt; -		page = drbd_alloc_pages(peer_device, nr_pages, (gfp_mask &amp; __GFP_WAIT));</span>
<span class="quote">&gt; +		page = drbd_alloc_pages(peer_device, nr_pages,</span>
<span class="quote">&gt; +					gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  		if (!page)</span>
<span class="quote">&gt;  			goto fail;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/drivers/block/osdblk.c b/drivers/block/osdblk.c</span>
<span class="quote">&gt; index e22942596207..1b709a4e3b5e 100644</span>
<span class="quote">&gt; --- a/drivers/block/osdblk.c</span>
<span class="quote">&gt; +++ b/drivers/block/osdblk.c</span>
<span class="quote">&gt; @@ -271,7 +271,7 @@ static struct bio *bio_chain_clone(struct bio *old_chain, gfp_t gfpmask)</span>
<span class="quote">&gt;  			goto err_out;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		tmp-&gt;bi_bdev = NULL;</span>
<span class="quote">&gt; -		gfpmask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +		gfpmask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  		tmp-&gt;bi_next = NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (!new_chain)</span>
<span class="quote">&gt; diff --git a/drivers/connector/connector.c b/drivers/connector/connector.c</span>
<span class="quote">&gt; index 30f522848c73..d7373ca69c99 100644</span>
<span class="quote">&gt; --- a/drivers/connector/connector.c</span>
<span class="quote">&gt; +++ b/drivers/connector/connector.c</span>
<span class="quote">&gt; @@ -124,7 +124,8 @@ int cn_netlink_send_mult(struct cn_msg *msg, u16 len, u32 portid, u32 __group,</span>
<span class="quote">&gt;  	if (group)</span>
<span class="quote">&gt;  		return netlink_broadcast(dev-&gt;nls, skb, portid, group,</span>
<span class="quote">&gt;  					 gfp_mask);</span>
<span class="quote">&gt; -	return netlink_unicast(dev-&gt;nls, skb, portid, !(gfp_mask&amp;__GFP_WAIT));</span>
<span class="quote">&gt; +	return netlink_unicast(dev-&gt;nls, skb, portid,</span>
<span class="quote">&gt; +			!gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(cn_netlink_send_mult);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/drivers/firewire/core-cdev.c b/drivers/firewire/core-cdev.c</span>
<span class="quote">&gt; index 2a3973a7c441..36a7c2d89a01 100644</span>
<span class="quote">&gt; --- a/drivers/firewire/core-cdev.c</span>
<span class="quote">&gt; +++ b/drivers/firewire/core-cdev.c</span>
<span class="quote">&gt; @@ -486,7 +486,7 @@ static int ioctl_get_info(struct client *client, union ioctl_arg *arg)</span>
<span class="quote">&gt;  static int add_client_resource(struct client *client,</span>
<span class="quote">&gt;  			       struct client_resource *resource, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	bool preload = !!(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	bool preload = gfpflags_allow_blocking(gfp_mask);</span>
<span class="quote">&gt;  	unsigned long flags;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="quote">&gt; index 4d631a946481..d58cb9e034fe 100644</span>
<span class="quote">&gt; --- a/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="quote">&gt; +++ b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="quote">&gt; @@ -2215,7 +2215,7 @@ i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	mapping = file_inode(obj-&gt;base.filp)-&gt;i_mapping;</span>
<span class="quote">&gt;  	gfp = mapping_gfp_mask(mapping);</span>
<span class="quote">&gt; -	gfp |= __GFP_NORETRY | __GFP_NOWARN | __GFP_NO_KSWAPD;</span>
<span class="quote">&gt; +	gfp |= __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="quote">&gt;  	gfp &amp;= ~(__GFP_IO | __GFP_WAIT);</span>
<span class="quote">&gt;  	sg = st-&gt;sgl;</span>
<span class="quote">&gt;  	st-&gt;nents = 0;</span>
<span class="quote">&gt; diff --git a/drivers/infiniband/core/sa_query.c b/drivers/infiniband/core/sa_query.c</span>
<span class="quote">&gt; index 8c014b33d8e0..59ab264c99c4 100644</span>
<span class="quote">&gt; --- a/drivers/infiniband/core/sa_query.c</span>
<span class="quote">&gt; +++ b/drivers/infiniband/core/sa_query.c</span>
<span class="quote">&gt; @@ -1083,7 +1083,7 @@ static void init_mad(struct ib_sa_mad *mad, struct ib_mad_agent *agent)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int send_mad(struct ib_sa_query *query, int timeout_ms, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	bool preload = !!(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	bool preload = gfpflags_allow_blocking(gfp_mask);</span>
<span class="quote">&gt;  	unsigned long flags;</span>
<span class="quote">&gt;  	int ret, id;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/drivers/iommu/amd_iommu.c b/drivers/iommu/amd_iommu.c</span>
<span class="quote">&gt; index f82060e778a2..1c0006e1ba4a 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/amd_iommu.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/amd_iommu.c</span>
<span class="quote">&gt; @@ -2755,7 +2755,7 @@ static void *alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	page = alloc_pages(flag | __GFP_NOWARN,  get_order(size));</span>
<span class="quote">&gt;  	if (!page) {</span>
<span class="quote">&gt; -		if (!(flag &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +		if (!gfpflags_allow_blocking(flag))</span>
<span class="quote">&gt;  			return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		page = dma_alloc_from_contiguous(dev, size &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; index 2d7349a3ee14..ecdafbe81a5e 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/intel-iommu.c</span>
<span class="quote">&gt; @@ -3533,7 +3533,7 @@ static void *intel_alloc_coherent(struct device *dev, size_t size,</span>
<span class="quote">&gt;  			flags |= GFP_DMA32;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (flags &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(flags)) {</span>
<span class="quote">&gt;  		unsigned int count = size &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		page = dma_alloc_from_contiguous(dev, count, order);</span>
<span class="quote">&gt; diff --git a/drivers/md/dm-crypt.c b/drivers/md/dm-crypt.c</span>
<span class="quote">&gt; index d60c88df5234..55ec935de2b4 100644</span>
<span class="quote">&gt; --- a/drivers/md/dm-crypt.c</span>
<span class="quote">&gt; +++ b/drivers/md/dm-crypt.c</span>
<span class="quote">&gt; @@ -993,7 +993,7 @@ static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
<span class="quote">&gt;  	struct bio_vec *bvec;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt; -	if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (unlikely(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		mutex_lock(&amp;cc-&gt;bio_alloc_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	clone = bio_alloc_bioset(GFP_NOIO, nr_iovecs, cc-&gt;bs);</span>
<span class="quote">&gt; @@ -1009,7 +1009,7 @@ static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
<span class="quote">&gt;  		if (!page) {</span>
<span class="quote">&gt;  			crypt_free_buffer_pages(cc, clone);</span>
<span class="quote">&gt;  			bio_put(clone);</span>
<span class="quote">&gt; -			gfp_mask |= __GFP_WAIT;</span>
<span class="quote">&gt; +			gfp_mask |= __GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  			goto retry;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1026,7 +1026,7 @@ static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  return_clone:</span>
<span class="quote">&gt; -	if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (unlikely(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		mutex_unlock(&amp;cc-&gt;bio_alloc_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return clone;</span>
<span class="quote">&gt; diff --git a/drivers/md/dm-kcopyd.c b/drivers/md/dm-kcopyd.c</span>
<span class="quote">&gt; index 3a7cade5e27d..1452ed9aacb4 100644</span>
<span class="quote">&gt; --- a/drivers/md/dm-kcopyd.c</span>
<span class="quote">&gt; +++ b/drivers/md/dm-kcopyd.c</span>
<span class="quote">&gt; @@ -244,7 +244,7 @@ static int kcopyd_get_pages(struct dm_kcopyd_client *kc,</span>
<span class="quote">&gt;  	*pages = NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt; -		pl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY);</span>
<span class="quote">&gt; +		pl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY | __GFP_KSWAPD_RECLAIM);</span>
<span class="quote">&gt;  		if (unlikely(!pl)) {</span>
<span class="quote">&gt;  			/* Use reserved pages */</span>
<span class="quote">&gt;  			pl = kc-&gt;pages;</span>
<span class="quote">&gt; diff --git a/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c b/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="quote">&gt; index 53fff5425c13..fb2cb4bdc0c1 100644</span>
<span class="quote">&gt; --- a/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="quote">&gt; +++ b/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="quote">&gt; @@ -1291,7 +1291,7 @@ static struct solo_enc_dev *solo_enc_alloc(struct solo_dev *solo_dev,</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.ops = &amp;solo_enc_video_qops;</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.mem_ops = &amp;vb2_dma_sg_memops;</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.drv_priv = solo_enc;</span>
<span class="quote">&gt; -	solo_enc-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="quote">&gt; +	solo_enc-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.buf_struct_size = sizeof(struct solo_vb2_buf);</span>
<span class="quote">&gt;  	solo_enc-&gt;vidq.lock = &amp;solo_enc-&gt;lock;</span>
<span class="quote">&gt; diff --git a/drivers/media/pci/solo6x10/solo6x10-v4l2.c b/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="quote">&gt; index 63ae8a61f603..bde77b22340c 100644</span>
<span class="quote">&gt; --- a/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="quote">&gt; +++ b/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="quote">&gt; @@ -675,7 +675,7 @@ int solo_v4l2_init(struct solo_dev *solo_dev, unsigned nr)</span>
<span class="quote">&gt;  	solo_dev-&gt;vidq.mem_ops = &amp;vb2_dma_contig_memops;</span>
<span class="quote">&gt;  	solo_dev-&gt;vidq.drv_priv = solo_dev;</span>
<span class="quote">&gt;  	solo_dev-&gt;vidq.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;</span>
<span class="quote">&gt; -	solo_dev-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="quote">&gt; +	solo_dev-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  	solo_dev-&gt;vidq.buf_struct_size = sizeof(struct solo_vb2_buf);</span>
<span class="quote">&gt;  	solo_dev-&gt;vidq.lock = &amp;solo_dev-&gt;lock;</span>
<span class="quote">&gt;  	ret = vb2_queue_init(&amp;solo_dev-&gt;vidq);</span>
<span class="quote">&gt; diff --git a/drivers/media/pci/tw68/tw68-video.c b/drivers/media/pci/tw68/tw68-video.c</span>
<span class="quote">&gt; index 8355e55b4e8e..e556f989aaab 100644</span>
<span class="quote">&gt; --- a/drivers/media/pci/tw68/tw68-video.c</span>
<span class="quote">&gt; +++ b/drivers/media/pci/tw68/tw68-video.c</span>
<span class="quote">&gt; @@ -975,7 +975,7 @@ int tw68_video_init2(struct tw68_dev *dev, int video_nr)</span>
<span class="quote">&gt;  	dev-&gt;vidq.ops = &amp;tw68_video_qops;</span>
<span class="quote">&gt;  	dev-&gt;vidq.mem_ops = &amp;vb2_dma_sg_memops;</span>
<span class="quote">&gt;  	dev-&gt;vidq.drv_priv = dev;</span>
<span class="quote">&gt; -	dev-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="quote">&gt; +	dev-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  	dev-&gt;vidq.buf_struct_size = sizeof(struct tw68_buf);</span>
<span class="quote">&gt;  	dev-&gt;vidq.lock = &amp;dev-&gt;lock;</span>
<span class="quote">&gt;  	dev-&gt;vidq.min_buffers_needed = 2;</span>
<span class="quote">&gt; diff --git a/drivers/mtd/mtdcore.c b/drivers/mtd/mtdcore.c</span>
<span class="quote">&gt; index 8bbbb751bf45..2dfb291a47c6 100644</span>
<span class="quote">&gt; --- a/drivers/mtd/mtdcore.c</span>
<span class="quote">&gt; +++ b/drivers/mtd/mtdcore.c</span>
<span class="quote">&gt; @@ -1188,8 +1188,7 @@ EXPORT_SYMBOL_GPL(mtd_writev);</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  void *mtd_kmalloc_up_to(const struct mtd_info *mtd, size_t *size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	gfp_t flags = __GFP_NOWARN | __GFP_WAIT |</span>
<span class="quote">&gt; -		       __GFP_NORETRY | __GFP_NO_KSWAPD;</span>
<span class="quote">&gt; +	gfp_t flags = __GFP_NOWARN | __GFP_DIRECT_RECLAIM | __GFP_NORETRY;</span>
<span class="quote">&gt;  	size_t min_alloc = max_t(size_t, mtd-&gt;writesize, PAGE_SIZE);</span>
<span class="quote">&gt;  	void *kbuf;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="quote">&gt; index 44173be5cbf0..f8d7a2f06950 100644</span>
<span class="quote">&gt; --- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="quote">&gt; +++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="quote">&gt; @@ -691,7 +691,7 @@ static void *bnx2x_frag_alloc(const struct bnx2x_fastpath *fp, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (fp-&gt;rx_frag_size) {</span>
<span class="quote">&gt;  		/* GFP_KERNEL allocations are used only during initialization */</span>
<span class="quote">&gt; -		if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +		if (unlikely(gfpflags_allow_blocking(gfp_mask)))</span>
<span class="quote">&gt;  			return (void *)__get_free_page(gfp_mask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		return netdev_alloc_frag(fp-&gt;rx_frag_size);</span>
<span class="quote">&gt; diff --git a/drivers/staging/android/ion/ion_system_heap.c b/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="quote">&gt; index 7a7a9a047230..d4cdbf28dbb6 100644</span>
<span class="quote">&gt; --- a/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="quote">&gt; +++ b/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="quote">&gt; @@ -27,7 +27,7 @@</span>
<span class="quote">&gt;  #include &quot;ion_priv.h&quot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static gfp_t high_order_gfp_flags = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN |</span>
<span class="quote">&gt; -				     __GFP_NORETRY) &amp; ~__GFP_WAIT;</span>
<span class="quote">&gt; +				     __GFP_NORETRY) &amp; ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  static gfp_t low_order_gfp_flags  = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN);</span>
<span class="quote">&gt;  static const unsigned int orders[] = {8, 4, 0};</span>
<span class="quote">&gt;  static const int num_orders = ARRAY_SIZE(orders);</span>
<span class="quote">&gt; diff --git a/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h b/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="quote">&gt; index 9544860e3292..78bde2c11b50 100644</span>
<span class="quote">&gt; --- a/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="quote">&gt; +++ b/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="quote">&gt; @@ -95,7 +95,7 @@ do {								    \</span>
<span class="quote">&gt;  do {									    \</span>
<span class="quote">&gt;  	LASSERT(!in_interrupt() ||					    \</span>
<span class="quote">&gt;  		((size) &lt;= LIBCFS_VMALLOC_SIZE &amp;&amp;			    \</span>
<span class="quote">&gt; -		 ((mask) &amp; __GFP_WAIT) == 0));				    \</span>
<span class="quote">&gt; +		 !gfpflags_allow_blocking(mask)));			    \</span>
<span class="quote">&gt;  } while (0)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define LIBCFS_ALLOC_POST(ptr, size)					    \</span>
<span class="quote">&gt; diff --git a/drivers/usb/host/u132-hcd.c b/drivers/usb/host/u132-hcd.c</span>
<span class="quote">&gt; index a67bd5090330..67b3b9d9dfd1 100644</span>
<span class="quote">&gt; --- a/drivers/usb/host/u132-hcd.c</span>
<span class="quote">&gt; +++ b/drivers/usb/host/u132-hcd.c</span>
<span class="quote">&gt; @@ -2244,7 +2244,7 @@ static int u132_urb_enqueue(struct usb_hcd *hcd, struct urb *urb,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct u132 *u132 = hcd_to_u132(hcd);</span>
<span class="quote">&gt;  	if (irqs_disabled()) {</span>
<span class="quote">&gt; -		if (__GFP_WAIT &amp; mem_flags) {</span>
<span class="quote">&gt; +		if (gfpflags_allow_blocking(mem_flags)) {</span>
<span class="quote">&gt;  			printk(KERN_ERR &quot;invalid context for function that migh&quot;</span>
<span class="quote">&gt;  				&quot;t sleep\n&quot;);</span>
<span class="quote">&gt;  			return -EINVAL;</span>
<span class="quote">&gt; diff --git a/drivers/video/fbdev/vermilion/vermilion.c b/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="quote">&gt; index 6b70d7f62b2f..1c1e95a0b8fa 100644</span>
<span class="quote">&gt; --- a/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="quote">&gt; +++ b/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="quote">&gt; @@ -99,7 +99,7 @@ static int vmlfb_alloc_vram_area(struct vram_area *va, unsigned max_order,</span>
<span class="quote">&gt;  		 * below the first 16MB.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		flags = __GFP_DMA | __GFP_HIGH;</span>
<span class="quote">&gt; +		flags = __GFP_DMA | __GFP_HIGH | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  		va-&gt;logical =</span>
<span class="quote">&gt;  			 __get_free_pages(flags, --max_order);</span>
<span class="quote">&gt;  	} while (va-&gt;logical == 0 &amp;&amp; max_order &gt; min_order);</span>
<span class="quote">&gt; diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c</span>
<span class="quote">&gt; index 0d98aee34fee..5632ba60c8f5 100644</span>
<span class="quote">&gt; --- a/fs/btrfs/disk-io.c</span>
<span class="quote">&gt; +++ b/fs/btrfs/disk-io.c</span>
<span class="quote">&gt; @@ -2572,7 +2572,7 @@ int open_ctree(struct super_block *sb,</span>
<span class="quote">&gt;  	fs_info-&gt;commit_interval = BTRFS_DEFAULT_COMMIT_INTERVAL;</span>
<span class="quote">&gt;  	fs_info-&gt;avg_delayed_ref_runtime = NSEC_PER_SEC &gt;&gt; 6; /* div by 64 */</span>
<span class="quote">&gt;  	/* readahead state */</span>
<span class="quote">&gt; -	INIT_RADIX_TREE(&amp;fs_info-&gt;reada_tree, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +	INIT_RADIX_TREE(&amp;fs_info-&gt;reada_tree, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  	spin_lock_init(&amp;fs_info-&gt;reada_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	fs_info-&gt;thread_pool_size = min_t(unsigned long,</span>
<span class="quote">&gt; diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c</span>
<span class="quote">&gt; index f1018cfbfefa..7956b310c194 100644</span>
<span class="quote">&gt; --- a/fs/btrfs/extent_io.c</span>
<span class="quote">&gt; +++ b/fs/btrfs/extent_io.c</span>
<span class="quote">&gt; @@ -594,7 +594,7 @@ int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  	if (bits &amp; (EXTENT_IOBITS | EXTENT_BOUNDARY))</span>
<span class="quote">&gt;  		clear = 1;</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt; -	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Don&#39;t care for allocation failure here because we might end</span>
<span class="quote">&gt;  		 * up not needing the pre-allocated extent state at all, which</span>
<span class="quote">&gt; @@ -718,7 +718,7 @@ int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  	if (start &gt; end)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	spin_unlock(&amp;tree-&gt;lock);</span>
<span class="quote">&gt; -	if (mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(mask))</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt;  	goto again;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -850,7 +850,7 @@ __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	bits |= EXTENT_FIRST_DELALLOC;</span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt; -	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
<span class="quote">&gt;  		prealloc = alloc_extent_state(mask);</span>
<span class="quote">&gt;  		BUG_ON(!prealloc);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1028,7 +1028,7 @@ __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  	if (start &gt; end)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	spin_unlock(&amp;tree-&gt;lock);</span>
<span class="quote">&gt; -	if (mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(mask))</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt;  	goto again;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1076,7 +1076,7 @@ int convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  	btrfs_debug_check_extent_io_range(tree, start, end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt; -	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Best effort, don&#39;t worry if extent state allocation fails</span>
<span class="quote">&gt;  		 * here for the first iteration. We might have a cached state</span>
<span class="quote">&gt; @@ -1253,7 +1253,7 @@ int convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
<span class="quote">&gt;  	if (start &gt; end)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	spin_unlock(&amp;tree-&gt;lock);</span>
<span class="quote">&gt; -	if (mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(mask))</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt;  	first_iteration = false;</span>
<span class="quote">&gt;  	goto again;</span>
<span class="quote">&gt; @@ -4267,7 +4267,7 @@ int try_release_extent_mapping(struct extent_map_tree *map,</span>
<span class="quote">&gt;  	u64 start = page_offset(page);</span>
<span class="quote">&gt;  	u64 end = start + PAGE_CACHE_SIZE - 1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if ((mask &amp; __GFP_WAIT) &amp;&amp;</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(mask) &amp;&amp;</span>
<span class="quote">&gt;  	    page-&gt;mapping-&gt;host-&gt;i_size &gt; 16 * 1024 * 1024) {</span>
<span class="quote">&gt;  		u64 len;</span>
<span class="quote">&gt;  		while (start &lt;= end) {</span>
<span class="quote">&gt; diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c</span>
<span class="quote">&gt; index 6fc735869c18..e023919b4470 100644</span>
<span class="quote">&gt; --- a/fs/btrfs/volumes.c</span>
<span class="quote">&gt; +++ b/fs/btrfs/volumes.c</span>
<span class="quote">&gt; @@ -156,8 +156,8 @@ static struct btrfs_device *__alloc_device(void)</span>
<span class="quote">&gt;  	spin_lock_init(&amp;dev-&gt;reada_lock);</span>
<span class="quote">&gt;  	atomic_set(&amp;dev-&gt;reada_in_flight, 0);</span>
<span class="quote">&gt;  	atomic_set(&amp;dev-&gt;dev_stats_ccnt, 0);</span>
<span class="quote">&gt; -	INIT_RADIX_TREE(&amp;dev-&gt;reada_zones, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; -	INIT_RADIX_TREE(&amp;dev-&gt;reada_extents, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +	INIT_RADIX_TREE(&amp;dev-&gt;reada_zones, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt; +	INIT_RADIX_TREE(&amp;dev-&gt;reada_extents, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return dev;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/fs/ext4/super.c b/fs/ext4/super.c</span>
<span class="quote">&gt; index a63c7b0a10cf..49f6c78ee3af 100644</span>
<span class="quote">&gt; --- a/fs/ext4/super.c</span>
<span class="quote">&gt; +++ b/fs/ext4/super.c</span>
<span class="quote">&gt; @@ -1058,7 +1058,7 @@ static int bdev_try_to_free_page(struct super_block *sb, struct page *page,</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	if (journal)</span>
<span class="quote">&gt;  		return jbd2_journal_try_to_free_buffers(journal, page,</span>
<span class="quote">&gt; -							wait &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +						wait &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  	return try_to_free_buffers(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/fs/fscache/cookie.c b/fs/fscache/cookie.c</span>
<span class="quote">&gt; index d403c69bee08..4304072161aa 100644</span>
<span class="quote">&gt; --- a/fs/fscache/cookie.c</span>
<span class="quote">&gt; +++ b/fs/fscache/cookie.c</span>
<span class="quote">&gt; @@ -111,7 +111,7 @@ struct fscache_cookie *__fscache_acquire_cookie(</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* radix tree insertion won&#39;t use the preallocation pool unless it&#39;s</span>
<span class="quote">&gt;  	 * told it may not wait */</span>
<span class="quote">&gt; -	INIT_RADIX_TREE(&amp;cookie-&gt;stores, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +	INIT_RADIX_TREE(&amp;cookie-&gt;stores, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	switch (cookie-&gt;def-&gt;type) {</span>
<span class="quote">&gt;  	case FSCACHE_COOKIE_TYPE_INDEX:</span>
<span class="quote">&gt; diff --git a/fs/fscache/page.c b/fs/fscache/page.c</span>
<span class="quote">&gt; index 483bbc613bf0..79483b3d8c6f 100644</span>
<span class="quote">&gt; --- a/fs/fscache/page.c</span>
<span class="quote">&gt; +++ b/fs/fscache/page.c</span>
<span class="quote">&gt; @@ -58,7 +58,7 @@ bool release_page_wait_timeout(struct fscache_cookie *cookie, struct page *page)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * decide whether a page can be released, possibly by cancelling a store to it</span>
<span class="quote">&gt; - * - we&#39;re allowed to sleep if __GFP_WAIT is flagged</span>
<span class="quote">&gt; + * - we&#39;re allowed to sleep if __GFP_DIRECT_RECLAIM is flagged</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
<span class="quote">&gt;  				  struct page *page,</span>
<span class="quote">&gt; @@ -122,7 +122,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
<span class="quote">&gt;  	 * allocator as the work threads writing to the cache may all end up</span>
<span class="quote">&gt;  	 * sleeping on memory allocation, so we may need to impose a timeout</span>
<span class="quote">&gt;  	 * too. */</span>
<span class="quote">&gt; -	if (!(gfp &amp; __GFP_WAIT) || !(gfp &amp; __GFP_FS)) {</span>
<span class="quote">&gt; +	if (!(gfp &amp; __GFP_DIRECT_RECLAIM) || !(gfp &amp; __GFP_FS)) {</span>
<span class="quote">&gt;  		fscache_stat(&amp;fscache_n_store_vmscan_busy);</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -132,7 +132,7 @@ bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
<span class="quote">&gt;  		_debug(&quot;fscache writeout timeout page: %p{%lx}&quot;,</span>
<span class="quote">&gt;  			page, page-&gt;index);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	gfp &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +	gfp &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  	goto try_again;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(__fscache_maybe_release_page);</span>
<span class="quote">&gt; diff --git a/fs/jbd2/transaction.c b/fs/jbd2/transaction.c</span>
<span class="quote">&gt; index 6b8338ec2464..89463eee6791 100644</span>
<span class="quote">&gt; --- a/fs/jbd2/transaction.c</span>
<span class="quote">&gt; +++ b/fs/jbd2/transaction.c</span>
<span class="quote">&gt; @@ -1937,8 +1937,8 @@ __journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)</span>
<span class="quote">&gt;   * @journal: journal for operation</span>
<span class="quote">&gt;   * @page: to try and free</span>
<span class="quote">&gt;   * @gfp_mask: we use the mask to detect how hard should we try to release</span>
<span class="quote">&gt; - * buffers. If __GFP_WAIT and __GFP_FS is set, we wait for commit code to</span>
<span class="quote">&gt; - * release the buffers.</span>
<span class="quote">&gt; + * buffers. If __GFP_DIRECT_RECLAIM and __GFP_FS is set, we wait for commit</span>
<span class="quote">&gt; + * code to release the buffers.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * For all the buffers on this page,</span>
<span class="quote">&gt; diff --git a/fs/nfs/file.c b/fs/nfs/file.c</span>
<span class="quote">&gt; index c0f9b1ed12b9..17d3417c8a74 100644</span>
<span class="quote">&gt; --- a/fs/nfs/file.c</span>
<span class="quote">&gt; +++ b/fs/nfs/file.c</span>
<span class="quote">&gt; @@ -473,8 +473,8 @@ static int nfs_release_page(struct page *page, gfp_t gfp)</span>
<span class="quote">&gt;  	dfprintk(PAGECACHE, &quot;NFS: release_page(%p)\n&quot;, page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Always try to initiate a &#39;commit&#39; if relevant, but only</span>
<span class="quote">&gt; -	 * wait for it if __GFP_WAIT is set.  Even then, only wait 1</span>
<span class="quote">&gt; -	 * second and only if the &#39;bdi&#39; is not congested.</span>
<span class="quote">&gt; +	 * wait for it if the caller allows blocking.  Even then,</span>
<span class="quote">&gt; +	 * only wait 1 second and only if the &#39;bdi&#39; is not congested.</span>
<span class="quote">&gt;  	 * Waiting indefinitely can cause deadlocks when the NFS</span>
<span class="quote">&gt;  	 * server is on this machine, when a new TCP connection is</span>
<span class="quote">&gt;  	 * needed and in other rare cases.  There is no particular</span>
<span class="quote">&gt; @@ -484,7 +484,7 @@ static int nfs_release_page(struct page *page, gfp_t gfp)</span>
<span class="quote">&gt;  	if (mapping) {</span>
<span class="quote">&gt;  		struct nfs_server *nfss = NFS_SERVER(mapping-&gt;host);</span>
<span class="quote">&gt;  		nfs_commit_inode(mapping-&gt;host, 0);</span>
<span class="quote">&gt; -		if ((gfp &amp; __GFP_WAIT) &amp;&amp;</span>
<span class="quote">&gt; +		if (gfpflags_allow_blocking(gfp) &amp;&amp;</span>
<span class="quote">&gt;  		    !bdi_write_congested(&amp;nfss-&gt;backing_dev_info)) {</span>
<span class="quote">&gt;  			wait_on_page_bit_killable_timeout(page, PG_private,</span>
<span class="quote">&gt;  							  HZ);</span>
<span class="quote">&gt; diff --git a/fs/xfs/xfs_qm.c b/fs/xfs/xfs_qm.c</span>
<span class="quote">&gt; index eac9549efd52..587174fd4f2c 100644</span>
<span class="quote">&gt; --- a/fs/xfs/xfs_qm.c</span>
<span class="quote">&gt; +++ b/fs/xfs/xfs_qm.c</span>
<span class="quote">&gt; @@ -525,7 +525,7 @@ xfs_qm_shrink_scan(</span>
<span class="quote">&gt;  	unsigned long		freed;</span>
<span class="quote">&gt;  	int			error;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if ((sc-&gt;gfp_mask &amp; (__GFP_FS|__GFP_WAIT)) != (__GFP_FS|__GFP_WAIT))</span>
<span class="quote">&gt; +	if ((sc-&gt;gfp_mask &amp; (__GFP_FS|__GFP_DIRECT_RECLAIM)) != (__GFP_FS|__GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	INIT_LIST_HEAD(&amp;isol.buffers);</span>
<span class="quote">&gt; diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="quote">&gt; index 440fca3e7e5d..b56e811b6f7c 100644</span>
<span class="quote">&gt; --- a/include/linux/gfp.h</span>
<span class="quote">&gt; +++ b/include/linux/gfp.h</span>
<span class="quote">&gt; @@ -29,12 +29,13 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define ___GFP_NOMEMALLOC	0x10000u</span>
<span class="quote">&gt;  #define ___GFP_HARDWALL		0x20000u</span>
<span class="quote">&gt;  #define ___GFP_THISNODE		0x40000u</span>
<span class="quote">&gt; -#define ___GFP_WAIT		0x80000u</span>
<span class="quote">&gt; +#define ___GFP_ATOMIC		0x80000u</span>
<span class="quote">&gt;  #define ___GFP_NOACCOUNT	0x100000u</span>
<span class="quote">&gt;  #define ___GFP_NOTRACK		0x200000u</span>
<span class="quote">&gt; -#define ___GFP_NO_KSWAPD	0x400000u</span>
<span class="quote">&gt; +#define ___GFP_DIRECT_RECLAIM	0x400000u</span>
<span class="quote">&gt;  #define ___GFP_OTHER_NODE	0x800000u</span>
<span class="quote">&gt;  #define ___GFP_WRITE		0x1000000u</span>
<span class="quote">&gt; +#define ___GFP_KSWAPD_RECLAIM	0x2000000u</span>
<span class="quote">&gt;  /* If the above are modified, __GFP_BITS_SHIFT may need updating */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -71,7 +72,7 @@ struct vm_area_struct;</span>
<span class="quote">&gt;   * __GFP_MOVABLE: Flag that this page will be movable by the page migration</span>
<span class="quote">&gt;   * mechanism or reclaimed</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -#define __GFP_WAIT	((__force gfp_t)___GFP_WAIT)	/* Can wait and reschedule? */</span>
<span class="quote">&gt; +#define __GFP_ATOMIC	((__force gfp_t)___GFP_ATOMIC)  /* Caller cannot wait or reschedule */</span>
<span class="quote">&gt;  #define __GFP_HIGH	((__force gfp_t)___GFP_HIGH)	/* Should access emergency pools? */</span>
<span class="quote">&gt;  #define __GFP_IO	((__force gfp_t)___GFP_IO)	/* Can start physical IO? */</span>
<span class="quote">&gt;  #define __GFP_FS	((__force gfp_t)___GFP_FS)	/* Can call down to low-level FS? */</span>
<span class="quote">&gt; @@ -94,23 +95,37 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define __GFP_NOACCOUNT	((__force gfp_t)___GFP_NOACCOUNT) /* Don&#39;t account to kmemcg */</span>
<span class="quote">&gt;  #define __GFP_NOTRACK	((__force gfp_t)___GFP_NOTRACK)  /* Don&#39;t track with kmemcheck */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define __GFP_NO_KSWAPD	((__force gfp_t)___GFP_NO_KSWAPD)</span>
<span class="quote">&gt;  #define __GFP_OTHER_NODE ((__force gfp_t)___GFP_OTHER_NODE) /* On behalf of other node */</span>
<span class="quote">&gt;  #define __GFP_WRITE	((__force gfp_t)___GFP_WRITE)	/* Allocator intends to dirty page */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * A caller that is willing to wait may enter direct reclaim and will</span>
<span class="quote">&gt; + * wake kswapd to reclaim pages in the background until the high</span>
<span class="quote">&gt; + * watermark is met. A caller may wish to clear __GFP_DIRECT_RECLAIM to</span>
<span class="quote">&gt; + * avoid unnecessary delays when a fallback option is available but</span>
<span class="quote">&gt; + * still allow kswapd to reclaim in the background. The kswapd flag</span>
<span class="quote">&gt; + * can be cleared when the reclaiming of pages would cause unnecessary</span>
<span class="quote">&gt; + * disruption.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define __GFP_WAIT ((__force gfp_t)(___GFP_DIRECT_RECLAIM|___GFP_KSWAPD_RECLAIM))</span>
<span class="quote">&gt; +#define __GFP_DIRECT_RECLAIM	((__force gfp_t)___GFP_DIRECT_RECLAIM) /* Caller can reclaim */</span>
<span class="quote">&gt; +#define __GFP_KSWAPD_RECLAIM	((__force gfp_t)___GFP_KSWAPD_RECLAIM) /* kswapd can wake */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;   * This may seem redundant, but it&#39;s a way of annotating false positives vs.</span>
<span class="quote">&gt;   * allocations that simply cannot be supported (e.g. page tables).</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #define __GFP_NOTRACK_FALSE_POSITIVE (__GFP_NOTRACK)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define __GFP_BITS_SHIFT 25	/* Room for N __GFP_FOO bits */</span>
<span class="quote">&gt; +#define __GFP_BITS_SHIFT 26	/* Room for N __GFP_FOO bits */</span>
<span class="quote">&gt;  #define __GFP_BITS_MASK ((__force gfp_t)((1 &lt;&lt; __GFP_BITS_SHIFT) - 1))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/* This equals 0, but use constants in case they ever change */</span>
<span class="quote">&gt; -#define GFP_NOWAIT	(GFP_ATOMIC &amp; ~__GFP_HIGH)</span>
<span class="quote">&gt; -/* GFP_ATOMIC means both !wait (__GFP_WAIT not set) and use emergency pool */</span>
<span class="quote">&gt; -#define GFP_ATOMIC	(__GFP_HIGH)</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * GFP_ATOMIC callers can not sleep, need the allocation to succeed.</span>
<span class="quote">&gt; + * A lower watermark is applied to allow access to &quot;atomic reserves&quot;</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define GFP_ATOMIC	(__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; +#define GFP_NOWAIT	(__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt;  #define GFP_NOIO	(__GFP_WAIT)</span>
<span class="quote">&gt;  #define GFP_NOFS	(__GFP_WAIT | __GFP_IO)</span>
<span class="quote">&gt;  #define GFP_KERNEL	(__GFP_WAIT | __GFP_IO | __GFP_FS)</span>
<span class="quote">&gt; @@ -119,10 +134,10 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define GFP_USER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)</span>
<span class="quote">&gt;  #define GFP_HIGHUSER	(GFP_USER | __GFP_HIGHMEM)</span>
<span class="quote">&gt;  #define GFP_HIGHUSER_MOVABLE	(GFP_HIGHUSER | __GFP_MOVABLE)</span>
<span class="quote">&gt; -#define GFP_IOFS	(__GFP_IO | __GFP_FS)</span>
<span class="quote">&gt; -#define GFP_TRANSHUGE	(GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="quote">&gt; -			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | \</span>
<span class="quote">&gt; -			 __GFP_NO_KSWAPD)</span>
<span class="quote">&gt; +#define GFP_IOFS	(__GFP_IO | __GFP_FS | __GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; +#define GFP_TRANSHUGE	((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="quote">&gt; +			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN) &amp; \</span>
<span class="quote">&gt; +			 ~__GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* This mask makes up all the page movable related flags */</span>
<span class="quote">&gt;  #define GFP_MOVABLE_MASK (__GFP_RECLAIMABLE|__GFP_MOVABLE)</span>
<span class="quote">&gt; @@ -164,6 +179,11 @@ static inline int gfpflags_to_migratetype(const gfp_t gfp_flags)</span>
<span class="quote">&gt;  	return (gfp_flags &amp; GFP_MOVABLE_MASK) &gt;&gt; GFP_MOVABLE_SHIFT;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline bool gfpflags_allow_blocking(const gfp_t gfp_flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return gfp_flags &amp; __GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_HIGHMEM</span>
<span class="quote">&gt;  #define OPT_ZONE_HIGHMEM ZONE_HIGHMEM</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h</span>
<span class="quote">&gt; index 2738d355cdf9..6f1f5a813554 100644</span>
<span class="quote">&gt; --- a/include/linux/skbuff.h</span>
<span class="quote">&gt; +++ b/include/linux/skbuff.h</span>
<span class="quote">&gt; @@ -1215,7 +1215,7 @@ static inline int skb_cloned(const struct sk_buff *skb)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline int skb_unclone(struct sk_buff *skb, gfp_t pri)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(pri));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (skb_cloned(skb))</span>
<span class="quote">&gt;  		return pskb_expand_head(skb, 0, 0, pri);</span>
<span class="quote">&gt; @@ -1299,7 +1299,7 @@ static inline int skb_shared(const struct sk_buff *skb)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(pri));</span>
<span class="quote">&gt;  	if (skb_shared(skb)) {</span>
<span class="quote">&gt;  		struct sk_buff *nskb = skb_clone(skb, pri);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1335,7 +1335,7 @@ static inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)</span>
<span class="quote">&gt;  static inline struct sk_buff *skb_unshare(struct sk_buff *skb,</span>
<span class="quote">&gt;  					  gfp_t pri)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(pri));</span>
<span class="quote">&gt;  	if (skb_cloned(skb)) {</span>
<span class="quote">&gt;  		struct sk_buff *nskb = skb_copy(skb, pri);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/net/sock.h b/include/net/sock.h</span>
<span class="quote">&gt; index 7aa78440559a..e822cdf8b855 100644</span>
<span class="quote">&gt; --- a/include/net/sock.h</span>
<span class="quote">&gt; +++ b/include/net/sock.h</span>
<span class="quote">&gt; @@ -2020,7 +2020,7 @@ struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static inline struct page_frag *sk_page_frag(struct sock *sk)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (sk-&gt;sk_allocation &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(sk-&gt;sk_allocation))</span>
<span class="quote">&gt;  		return &amp;current-&gt;task_frag;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return &amp;sk-&gt;sk_frag;</span>
<span class="quote">&gt; diff --git a/include/trace/events/gfpflags.h b/include/trace/events/gfpflags.h</span>
<span class="quote">&gt; index d6fd8e5b14b7..dde6bf092c8a 100644</span>
<span class="quote">&gt; --- a/include/trace/events/gfpflags.h</span>
<span class="quote">&gt; +++ b/include/trace/events/gfpflags.h</span>
<span class="quote">&gt; @@ -20,7 +20,7 @@</span>
<span class="quote">&gt;  	{(unsigned long)GFP_ATOMIC,		&quot;GFP_ATOMIC&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)GFP_NOIO,		&quot;GFP_NOIO&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_HIGH,		&quot;GFP_HIGH&quot;},		\</span>
<span class="quote">&gt; -	{(unsigned long)__GFP_WAIT,		&quot;GFP_WAIT&quot;},		\</span>
<span class="quote">&gt; +	{(unsigned long)__GFP_ATOMIC,		&quot;GFP_ATOMIC&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_IO,		&quot;GFP_IO&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_COLD,		&quot;GFP_COLD&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_NOWARN,		&quot;GFP_NOWARN&quot;},		\</span>
<span class="quote">&gt; @@ -36,7 +36,8 @@</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_RECLAIMABLE,	&quot;GFP_RECLAIMABLE&quot;},	\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_MOVABLE,		&quot;GFP_MOVABLE&quot;},		\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_NOTRACK,		&quot;GFP_NOTRACK&quot;},		\</span>
<span class="quote">&gt; -	{(unsigned long)__GFP_NO_KSWAPD,	&quot;GFP_NO_KSWAPD&quot;},	\</span>
<span class="quote">&gt; +	{(unsigned long)__GFP_DIRECT_RECLAIM,	&quot;GFP_DIRECT_RECLAIM&quot;},	\</span>
<span class="quote">&gt; +	{(unsigned long)__GFP_KSWAPD_RECLAIM,	&quot;GFP_KSWAPD_RECLAIM&quot;},	\</span>
<span class="quote">&gt;  	{(unsigned long)__GFP_OTHER_NODE,	&quot;GFP_OTHER_NODE&quot;}	\</span>
<span class="quote">&gt;  	) : &quot;GFP_NOWAIT&quot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/kernel/audit.c b/kernel/audit.c</span>
<span class="quote">&gt; index 662c007635fb..6ae6e2b62e3e 100644</span>
<span class="quote">&gt; --- a/kernel/audit.c</span>
<span class="quote">&gt; +++ b/kernel/audit.c</span>
<span class="quote">&gt; @@ -1357,16 +1357,16 @@ struct audit_buffer *audit_log_start(struct audit_context *ctx, gfp_t gfp_mask,</span>
<span class="quote">&gt;  	if (unlikely(audit_filter_type(type)))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (gfp_mask &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +	if (gfp_mask &amp; __GFP_DIRECT_RECLAIM) {</span>
<span class="quote">&gt;  		if (audit_pid &amp;&amp; audit_pid == current-&gt;pid)</span>
<span class="quote">&gt; -			gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="quote">&gt; +			gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  		else</span>
<span class="quote">&gt;  			reserve = 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	while (audit_backlog_limit</span>
<span class="quote">&gt;  	       &amp;&amp; skb_queue_len(&amp;audit_skb_queue) &gt; audit_backlog_limit + reserve) {</span>
<span class="quote">&gt; -		if (gfp_mask &amp; __GFP_WAIT &amp;&amp; audit_backlog_wait_time) {</span>
<span class="quote">&gt; +		if (gfp_mask &amp; __GFP_DIRECT_RECLAIM &amp;&amp; audit_backlog_wait_time) {</span>
<span class="quote">&gt;  			long sleep_time;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			sleep_time = timeout_start + audit_backlog_wait_time - jiffies;</span>
<span class="quote">&gt; diff --git a/kernel/cgroup.c b/kernel/cgroup.c</span>
<span class="quote">&gt; index 2cf0f79f1fc9..e843dffa7b87 100644</span>
<span class="quote">&gt; --- a/kernel/cgroup.c</span>
<span class="quote">&gt; +++ b/kernel/cgroup.c</span>
<span class="quote">&gt; @@ -211,7 +211,7 @@ static int cgroup_idr_alloc(struct idr *idr, void *ptr, int start, int end,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	idr_preload(gfp_mask);</span>
<span class="quote">&gt;  	spin_lock_bh(&amp;cgroup_idr_lock);</span>
<span class="quote">&gt; -	ret = idr_alloc(idr, ptr, start, end, gfp_mask &amp; ~__GFP_WAIT);</span>
<span class="quote">&gt; +	ret = idr_alloc(idr, ptr, start, end, gfp_mask &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  	spin_unlock_bh(&amp;cgroup_idr_lock);</span>
<span class="quote">&gt;  	idr_preload_end();</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt; diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c</span>
<span class="quote">&gt; index 8acfbf773e06..9aa39f20f593 100644</span>
<span class="quote">&gt; --- a/kernel/locking/lockdep.c</span>
<span class="quote">&gt; +++ b/kernel/locking/lockdep.c</span>
<span class="quote">&gt; @@ -2738,7 +2738,7 @@ static void __lockdep_trace_alloc(gfp_t gfp_mask, unsigned long flags)</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* no reclaim without waiting on it */</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (!(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* this guy won&#39;t enter reclaim */</span>
<span class="quote">&gt; diff --git a/kernel/power/snapshot.c b/kernel/power/snapshot.c</span>
<span class="quote">&gt; index 5235dd4e1e2f..3a970604308f 100644</span>
<span class="quote">&gt; --- a/kernel/power/snapshot.c</span>
<span class="quote">&gt; +++ b/kernel/power/snapshot.c</span>
<span class="quote">&gt; @@ -1779,7 +1779,7 @@ alloc_highmem_pages(struct memory_bitmap *bm, unsigned int nr_highmem)</span>
<span class="quote">&gt;  	while (to_alloc-- &gt; 0) {</span>
<span class="quote">&gt;  		struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		page = alloc_image_page(__GFP_HIGHMEM);</span>
<span class="quote">&gt; +		page = alloc_image_page(__GFP_HIGHMEM|__GFP_KSWAPD_RECLAIM);</span>
<span class="quote">&gt;  		memory_bm_set_bit(bm, page_to_pfn(page));</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return nr_highmem;</span>
<span class="quote">&gt; diff --git a/kernel/smp.c b/kernel/smp.c</span>
<span class="quote">&gt; index 07854477c164..d903c02223af 100644</span>
<span class="quote">&gt; --- a/kernel/smp.c</span>
<span class="quote">&gt; +++ b/kernel/smp.c</span>
<span class="quote">&gt; @@ -669,7 +669,7 @@ void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),</span>
<span class="quote">&gt;  	cpumask_var_t cpus;</span>
<span class="quote">&gt;  	int cpu, ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (likely(zalloc_cpumask_var(&amp;cpus, (gfp_flags|__GFP_NOWARN)))) {</span>
<span class="quote">&gt;  		preempt_disable();</span>
<span class="quote">&gt; diff --git a/lib/idr.c b/lib/idr.c</span>
<span class="quote">&gt; index 5335c43adf46..6098336df267 100644</span>
<span class="quote">&gt; --- a/lib/idr.c</span>
<span class="quote">&gt; +++ b/lib/idr.c</span>
<span class="quote">&gt; @@ -399,7 +399,7 @@ void idr_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  	 * allocation guarantee.  Disallow usage from those contexts.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	WARN_ON_ONCE(in_interrupt());</span>
<span class="quote">&gt; -	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	preempt_disable();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -453,7 +453,7 @@ int idr_alloc(struct idr *idr, void *ptr, int start, int end, gfp_t gfp_mask)</span>
<span class="quote">&gt;  	struct idr_layer *pa[MAX_IDR_LEVEL + 1];</span>
<span class="quote">&gt;  	int id;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* sanity checks */</span>
<span class="quote">&gt;  	if (WARN_ON_ONCE(start &lt; 0))</span>
<span class="quote">&gt; diff --git a/lib/radix-tree.c b/lib/radix-tree.c</span>
<span class="quote">&gt; index f9ebe1c82060..fcf5d98574ce 100644</span>
<span class="quote">&gt; --- a/lib/radix-tree.c</span>
<span class="quote">&gt; +++ b/lib/radix-tree.c</span>
<span class="quote">&gt; @@ -188,7 +188,7 @@ radix_tree_node_alloc(struct radix_tree_root *root)</span>
<span class="quote">&gt;  	 * preloading in the interrupt anyway as all the allocations have to</span>
<span class="quote">&gt;  	 * be atomic. So just do normal allocation when in interrupt.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT) &amp;&amp; !in_interrupt()) {</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp_mask) &amp;&amp; !in_interrupt()) {</span>
<span class="quote">&gt;  		struct radix_tree_preload *rtp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; @@ -249,7 +249,7 @@ radix_tree_node_free(struct radix_tree_node *node)</span>
<span class="quote">&gt;   * with preemption not disabled.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * To make use of this facility, the radix tree must be initialised without</span>
<span class="quote">&gt; - * __GFP_WAIT being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt; + * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int __radix_tree_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -286,12 +286,12 @@ static int __radix_tree_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;   * with preemption not disabled.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * To make use of this facility, the radix tree must be initialised without</span>
<span class="quote">&gt; - * __GFP_WAIT being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt; + * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int radix_tree_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	/* Warn on non-sensical use... */</span>
<span class="quote">&gt; -	WARN_ON_ONCE(!(gfp_mask &amp; __GFP_WAIT));</span>
<span class="quote">&gt; +	WARN_ON_ONCE(!gfpflags_allow_blocking(gfp_mask));</span>
<span class="quote">&gt;  	return __radix_tree_preload(gfp_mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(radix_tree_preload);</span>
<span class="quote">&gt; @@ -303,7 +303,7 @@ EXPORT_SYMBOL(radix_tree_preload);</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int radix_tree_maybe_preload(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (gfp_mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;  		return __radix_tree_preload(gfp_mask);</span>
<span class="quote">&gt;  	/* Preloading doesn&#39;t help anything with this gfp mask, skip it */</span>
<span class="quote">&gt;  	preempt_disable();</span>
<span class="quote">&gt; diff --git a/mm/backing-dev.c b/mm/backing-dev.c</span>
<span class="quote">&gt; index 2df8ddcb0ca0..e7781eb35fd1 100644</span>
<span class="quote">&gt; --- a/mm/backing-dev.c</span>
<span class="quote">&gt; +++ b/mm/backing-dev.c</span>
<span class="quote">&gt; @@ -632,7 +632,7 @@ struct bdi_writeback *wb_get_create(struct backing_dev_info *bdi,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct bdi_writeback *wb;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(gfp));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!memcg_css-&gt;parent)</span>
<span class="quote">&gt;  		return &amp;bdi-&gt;wb;</span>
<span class="quote">&gt; diff --git a/mm/dmapool.c b/mm/dmapool.c</span>
<span class="quote">&gt; index 71a8998cd03a..55b53cffd9f6 100644</span>
<span class="quote">&gt; --- a/mm/dmapool.c</span>
<span class="quote">&gt; +++ b/mm/dmapool.c</span>
<span class="quote">&gt; @@ -326,7 +326,7 @@ void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,</span>
<span class="quote">&gt;  	size_t offset;</span>
<span class="quote">&gt;  	void *retval;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(mem_flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(mem_flags));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock_irqsave(&amp;pool-&gt;lock, flags);</span>
<span class="quote">&gt;  	list_for_each_entry(page, &amp;pool-&gt;page_list, page_list) {</span>
<span class="quote">&gt; diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="quote">&gt; index 6ddaeba34e09..2c65980c0a00 100644</span>
<span class="quote">&gt; --- a/mm/memcontrol.c</span>
<span class="quote">&gt; +++ b/mm/memcontrol.c</span>
<span class="quote">&gt; @@ -2012,7 +2012,7 @@ static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
<span class="quote">&gt;  	if (unlikely(task_in_memcg_oom(current)))</span>
<span class="quote">&gt;  		goto nomem;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;  		goto nomem;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mem_cgroup_events(mem_over_limit, MEMCG_MAX, 1);</span>
<span class="quote">&gt; @@ -2071,7 +2071,7 @@ static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
<span class="quote">&gt;  	css_get_many(&amp;memcg-&gt;css, batch);</span>
<span class="quote">&gt;  	if (batch &gt; nr_pages)</span>
<span class="quote">&gt;  		refill_stock(memcg, batch - nr_pages);</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;  		goto done;</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * If the hierarchy is above the normal consumption range,</span>
<span class="quote">&gt; @@ -4396,8 +4396,8 @@ static int mem_cgroup_do_precharge(unsigned long count)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	/* Try a single bulk charge without reclaim first */</span>
<span class="quote">&gt; -	ret = try_charge(mc.to, GFP_KERNEL &amp; ~__GFP_WAIT, count);</span>
<span class="quote">&gt; +	/* Try a single bulk charge without reclaim first, kswapd may wake */</span>
<span class="quote">&gt; +	ret = try_charge(mc.to, GFP_KERNEL &amp; ~__GFP_DIRECT_RECLAIM, count);</span>
<span class="quote">&gt;  	if (!ret) {</span>
<span class="quote">&gt;  		mc.precharge += count;</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt; diff --git a/mm/mempool.c b/mm/mempool.c</span>
<span class="quote">&gt; index 4c533bc51d73..004d42b1dfaf 100644</span>
<span class="quote">&gt; --- a/mm/mempool.c</span>
<span class="quote">&gt; +++ b/mm/mempool.c</span>
<span class="quote">&gt; @@ -320,13 +320,13 @@ void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
<span class="quote">&gt;  	gfp_t gfp_temp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	VM_WARN_ON_ONCE(gfp_mask &amp; __GFP_ZERO);</span>
<span class="quote">&gt; -	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfp_mask &amp; __GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	gfp_mask |= __GFP_NOMEMALLOC;	/* don&#39;t allocate emergency reserves */</span>
<span class="quote">&gt;  	gfp_mask |= __GFP_NORETRY;	/* don&#39;t loop in __alloc_pages */</span>
<span class="quote">&gt;  	gfp_mask |= __GFP_NOWARN;	/* failures are OK */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	gfp_temp = gfp_mask &amp; ~(__GFP_WAIT|__GFP_IO);</span>
<span class="quote">&gt; +	gfp_temp = gfp_mask &amp; ~(__GFP_DIRECT_RECLAIM|__GFP_IO);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  repeat_alloc:</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -349,7 +349,7 @@ void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; -	 * We use gfp mask w/o __GFP_WAIT or IO for the first round.  If</span>
<span class="quote">&gt; +	 * We use gfp mask w/o direct reclaim or IO for the first round.  If</span>
<span class="quote">&gt;  	 * alloc failed with that and @pool was empty, retry immediately.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (gfp_temp != gfp_mask) {</span>
<span class="quote">&gt; @@ -358,8 +358,8 @@ void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
<span class="quote">&gt;  		goto repeat_alloc;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	/* We must not sleep if !__GFP_WAIT */</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +	/* We must not sleep if !__GFP_DIRECT_RECLAIM */</span>
<span class="quote">&gt; +	if (!(gfp_mask &amp; __GFP_DIRECT_RECLAIM)) {</span>
<span class="quote">&gt;  		spin_unlock_irqrestore(&amp;pool-&gt;lock, flags);</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index c3cb566af3e2..a1c82b65dcad 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -1565,7 +1565,7 @@ static struct page *alloc_misplaced_dst_page(struct page *page,</span>
<span class="quote">&gt;  					 (GFP_HIGHUSER_MOVABLE |</span>
<span class="quote">&gt;  					  __GFP_THISNODE | __GFP_NOMEMALLOC |</span>
<span class="quote">&gt;  					  __GFP_NORETRY | __GFP_NOWARN) &amp;</span>
<span class="quote">&gt; -					 ~GFP_IOFS, 0);</span>
<span class="quote">&gt; +					 ~(__GFP_IO | __GFP_FS), 0);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return newpage;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index 4793bddb6b2a..b32081b02c49 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -169,12 +169,12 @@ void pm_restrict_gfp_mask(void)</span>
<span class="quote">&gt;  	WARN_ON(!mutex_is_locked(&amp;pm_mutex));</span>
<span class="quote">&gt;  	WARN_ON(saved_gfp_mask);</span>
<span class="quote">&gt;  	saved_gfp_mask = gfp_allowed_mask;</span>
<span class="quote">&gt; -	gfp_allowed_mask &amp;= ~GFP_IOFS;</span>
<span class="quote">&gt; +	gfp_allowed_mask &amp;= ~(__GFP_IO | __GFP_FS);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  bool pm_suspended_storage(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if ((gfp_allowed_mask &amp; GFP_IOFS) == GFP_IOFS)</span>
<span class="quote">&gt; +	if ((gfp_allowed_mask &amp; (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	return true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2183,7 +2183,7 @@ static bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	if (fail_page_alloc.ignore_gfp_highmem &amp;&amp; (gfp_mask &amp; __GFP_HIGHMEM))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt; -	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return should_fail(&amp;fail_page_alloc.attr, 1 &lt;&lt; order);</span>
<span class="quote">&gt; @@ -2685,7 +2685,7 @@ void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
<span class="quote">&gt;  		if (test_thread_flag(TIF_MEMDIE) ||</span>
<span class="quote">&gt;  		    (current-&gt;flags &amp; (PF_MEMALLOC | PF_EXITING)))</span>
<span class="quote">&gt;  			filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt; -	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +	if (in_interrupt() || !(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
<span class="quote">&gt;  		filter &amp;= ~SHOW_MEM_FILTER_NODES;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (fmt) {</span>
<span class="quote">&gt; @@ -2945,7 +2945,6 @@ static inline int</span>
<span class="quote">&gt;  gfp_to_alloc_flags(gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;</span>
<span class="quote">&gt; -	const bool atomic = !(gfp_mask &amp; (__GFP_WAIT | __GFP_NO_KSWAPD));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* __GFP_HIGH is assumed to be the same as ALLOC_HIGH to save a branch. */</span>
<span class="quote">&gt;  	BUILD_BUG_ON(__GFP_HIGH != (__force gfp_t) ALLOC_HIGH);</span>
<span class="quote">&gt; @@ -2954,11 +2953,11 @@ gfp_to_alloc_flags(gfp_t gfp_mask)</span>
<span class="quote">&gt;  	 * The caller may dip into page reserves a bit more if the caller</span>
<span class="quote">&gt;  	 * cannot run direct reclaim, or if the caller has realtime scheduling</span>
<span class="quote">&gt;  	 * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will</span>
<span class="quote">&gt; -	 * set both ALLOC_HARDER (atomic == true) and ALLOC_HIGH (__GFP_HIGH).</span>
<span class="quote">&gt; +	 * set both ALLOC_HARDER (__GFP_ATOMIC) and ALLOC_HIGH (__GFP_HIGH).</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	alloc_flags |= (__force int) (gfp_mask &amp; __GFP_HIGH);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (atomic) {</span>
<span class="quote">&gt; +	if (gfp_mask &amp; __GFP_ATOMIC) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Not worth trying to allocate harder for __GFP_NOMEMALLOC even</span>
<span class="quote">&gt;  		 * if it can&#39;t schedule.</span>
<span class="quote">&gt; @@ -2995,11 +2994,16 @@ bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)</span>
<span class="quote">&gt;  	return !!(gfp_to_alloc_flags(gfp_mask) &amp; ALLOC_NO_WATERMARKS);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline bool is_thp_gfp_mask(gfp_t gfp_mask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (gfp_mask &amp; (GFP_TRANSHUGE | __GFP_KSWAPD_RECLAIM)) == GFP_TRANSHUGE;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline struct page *</span>
<span class="quote">&gt;  __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  						struct alloc_context *ac)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	const gfp_t wait = gfp_mask &amp; __GFP_WAIT;</span>
<span class="quote">&gt; +	bool can_direct_reclaim = gfp_mask &amp; __GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt;  	int alloc_flags;</span>
<span class="quote">&gt;  	unsigned long pages_reclaimed = 0;</span>
<span class="quote">&gt; @@ -3020,15 +3024,23 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; +	 * We also sanity check to catch abuse of atomic reserves being used by</span>
<span class="quote">&gt; +	 * callers that are not in atomic context.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (WARN_ON_ONCE((gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)) ==</span>
<span class="quote">&gt; +				(__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="quote">&gt; +		gfp_mask &amp;= ~__GFP_ATOMIC;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt;  	 * If this allocation cannot block and it is for a specific node, then</span>
<span class="quote">&gt;  	 * fail early.  There&#39;s no need to wakeup kswapd or retry for a</span>
<span class="quote">&gt;  	 * speculative node-specific allocation.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; (gfp_mask &amp; __GFP_THISNODE) &amp;&amp; !wait)</span>
<span class="quote">&gt; +	if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; (gfp_mask &amp; __GFP_THISNODE) &amp;&amp; !can_direct_reclaim)</span>
<span class="quote">&gt;  		goto nopage;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_NO_KSWAPD))</span>
<span class="quote">&gt; +	if (gfp_mask &amp; __GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt;  		wake_all_kswapds(order, ac);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -3071,8 +3083,8 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	/* Atomic allocations - we can&#39;t balance anything */</span>
<span class="quote">&gt; -	if (!wait) {</span>
<span class="quote">&gt; +	/* Caller is not willing to reclaim, we can&#39;t balance anything */</span>
<span class="quote">&gt; +	if (!can_direct_reclaim) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * All existing users of the deprecated __GFP_NOFAIL are</span>
<span class="quote">&gt;  		 * blockable, so warn of any new users that actually allow this</span>
<span class="quote">&gt; @@ -3102,7 +3114,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  		goto got_pg;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Checks for THP-specific high-order allocations */</span>
<span class="quote">&gt; -	if ((gfp_mask &amp; GFP_TRANSHUGE) == GFP_TRANSHUGE) {</span>
<span class="quote">&gt; +	if (is_thp_gfp_mask(gfp_mask)) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * If compaction is deferred for high-order allocations, it is</span>
<span class="quote">&gt;  		 * because sync compaction recently failed. If this is the case</span>
<span class="quote">&gt; @@ -3137,8 +3149,7 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  	 * fault, so use asynchronous memory compaction for THP unless it is</span>
<span class="quote">&gt;  	 * khugepaged trying to collapse.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if ((gfp_mask &amp; GFP_TRANSHUGE) != GFP_TRANSHUGE ||</span>
<span class="quote">&gt; -						(current-&gt;flags &amp; PF_KTHREAD))</span>
<span class="quote">&gt; +	if (!is_thp_gfp_mask(gfp_mask) || (current-&gt;flags &amp; PF_KTHREAD))</span>
<span class="quote">&gt;  		migration_mode = MIGRATE_SYNC_LIGHT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* Try direct reclaim and then allocating */</span>
<span class="quote">&gt; @@ -3209,7 +3220,7 @@ __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	lockdep_trace_alloc(gfp_mask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfp_mask &amp; __GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (should_fail_alloc_page(gfp_mask, order))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; diff --git a/mm/slab.c b/mm/slab.c</span>
<span class="quote">&gt; index c77ebe6cc87c..3ff59926bf19 100644</span>
<span class="quote">&gt; --- a/mm/slab.c</span>
<span class="quote">&gt; +++ b/mm/slab.c</span>
<span class="quote">&gt; @@ -1030,12 +1030,12 @@ static inline int cache_free_alien(struct kmem_cache *cachep, void *objp)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; - * Construct gfp mask to allocate from a specific node but do not invoke reclaim</span>
<span class="quote">&gt; - * or warn about failures.</span>
<span class="quote">&gt; + * Construct gfp mask to allocate from a specific node but do not direct reclaim</span>
<span class="quote">&gt; + * or warn about failures. kswapd may still wake to reclaim in the background.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static inline gfp_t gfp_exact_node(gfp_t flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return (flags | __GFP_THISNODE | __GFP_NOWARN) &amp; ~__GFP_WAIT;</span>
<span class="quote">&gt; +	return (flags | __GFP_THISNODE | __GFP_NOWARN) &amp; ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -2625,7 +2625,7 @@ static int cache_grow(struct kmem_cache *cachep,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	offset *= cachep-&gt;colour_off;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;  		local_irq_enable();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -2655,7 +2655,7 @@ static int cache_grow(struct kmem_cache *cachep,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	cache_init_objs(cachep, page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;  		local_irq_disable();</span>
<span class="quote">&gt;  	check_irq_off();</span>
<span class="quote">&gt;  	spin_lock(&amp;n-&gt;list_lock);</span>
<span class="quote">&gt; @@ -2669,7 +2669,7 @@ static int cache_grow(struct kmem_cache *cachep,</span>
<span class="quote">&gt;  opps1:</span>
<span class="quote">&gt;  	kmem_freepages(cachep, page);</span>
<span class="quote">&gt;  failed:</span>
<span class="quote">&gt; -	if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;  		local_irq_disable();</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2861,7 +2861,7 @@ static void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags,</span>
<span class="quote">&gt;  static inline void cache_alloc_debugcheck_before(struct kmem_cache *cachep,</span>
<span class="quote">&gt;  						gfp_t flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	might_sleep_if(flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(flags));</span>
<span class="quote">&gt;  #if DEBUG</span>
<span class="quote">&gt;  	kmem_flagcheck(cachep, flags);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -3049,11 +3049,11 @@ static void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +		if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;  			local_irq_enable();</span>
<span class="quote">&gt;  		kmem_flagcheck(cache, flags);</span>
<span class="quote">&gt;  		page = kmem_getpages(cache, local_flags, numa_mem_id());</span>
<span class="quote">&gt; -		if (local_flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +		if (gfpflags_allow_blocking(local_flags))</span>
<span class="quote">&gt;  			local_irq_disable();</span>
<span class="quote">&gt;  		if (page) {</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt; diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="quote">&gt; index f614b5dc396b..2cdbf5db348e 100644</span>
<span class="quote">&gt; --- a/mm/slub.c</span>
<span class="quote">&gt; +++ b/mm/slub.c</span>
<span class="quote">&gt; @@ -1263,7 +1263,7 @@ static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	flags &amp;= gfp_allowed_mask;</span>
<span class="quote">&gt;  	lockdep_trace_alloc(flags);</span>
<span class="quote">&gt; -	might_sleep_if(flags &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	might_sleep_if(gfpflags_allow_blocking(flags));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (should_failslab(s-&gt;object_size, flags, s-&gt;flags))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; @@ -1352,7 +1352,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	flags &amp;= gfp_allowed_mask;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(flags))</span>
<span class="quote">&gt;  		local_irq_enable();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	flags |= s-&gt;allocflags;</span>
<span class="quote">&gt; @@ -1362,8 +1362,8 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
<span class="quote">&gt;  	 * so we fall-back to the minimum order allocation.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	alloc_gfp = (flags | __GFP_NOWARN | __GFP_NORETRY) &amp; ~__GFP_NOFAIL;</span>
<span class="quote">&gt; -	if ((alloc_gfp &amp; __GFP_WAIT) &amp;&amp; oo_order(oo) &gt; oo_order(s-&gt;min))</span>
<span class="quote">&gt; -		alloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) &amp; ~__GFP_WAIT;</span>
<span class="quote">&gt; +	if ((alloc_gfp &amp; __GFP_DIRECT_RECLAIM) &amp;&amp; oo_order(oo) &gt; oo_order(s-&gt;min))</span>
<span class="quote">&gt; +		alloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) &amp; ~__GFP_DIRECT_RECLAIM;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	page = alloc_slab_page(s, alloc_gfp, node, oo);</span>
<span class="quote">&gt;  	if (unlikely(!page)) {</span>
<span class="quote">&gt; @@ -1423,7 +1423,7 @@ static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
<span class="quote">&gt;  	page-&gt;frozen = 1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt; -	if (flags &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfpflags_allow_blocking(flags))</span>
<span class="quote">&gt;  		local_irq_disable();</span>
<span class="quote">&gt;  	if (!page)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; diff --git a/mm/vmalloc.c b/mm/vmalloc.c</span>
<span class="quote">&gt; index 2faaa2976447..9ad4dcb0631c 100644</span>
<span class="quote">&gt; --- a/mm/vmalloc.c</span>
<span class="quote">&gt; +++ b/mm/vmalloc.c</span>
<span class="quote">&gt; @@ -1617,7 +1617,7 @@ static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,</span>
<span class="quote">&gt;  			goto fail;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		area-&gt;pages[i] = page;</span>
<span class="quote">&gt; -		if (gfp_mask &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +		if (gfpflags_allow_blocking(gfp_mask))</span>
<span class="quote">&gt;  			cond_resched();</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt; index 8b2786fd42b5..30a87ac1af80 100644</span>
<span class="quote">&gt; --- a/mm/vmscan.c</span>
<span class="quote">&gt; +++ b/mm/vmscan.c</span>
<span class="quote">&gt; @@ -1476,7 +1476,7 @@ static int too_many_isolated(struct zone *zone, int file,</span>
<span class="quote">&gt;  	 * won&#39;t get blocked by normal direct-reclaimers, forming a circular</span>
<span class="quote">&gt;  	 * deadlock.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if ((sc-&gt;gfp_mask &amp; GFP_IOFS) == GFP_IOFS)</span>
<span class="quote">&gt; +	if ((sc-&gt;gfp_mask &amp; (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))</span>
<span class="quote">&gt;  		inactive &gt;&gt;= 3;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return isolated &gt; inactive;</span>
<span class="quote">&gt; @@ -3794,7 +3794,7 @@ int zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Do not scan if the allocation should not be delayed.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (!(gfp_mask &amp; __GFP_WAIT) || (current-&gt;flags &amp; PF_MEMALLOC))</span>
<span class="quote">&gt; +	if (!gfpflags_allow_blocking(gfp_mask) || (current-&gt;flags &amp; PF_MEMALLOC))</span>
<span class="quote">&gt;  		return ZONE_RECLAIM_NOSCAN;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; diff --git a/mm/zswap.c b/mm/zswap.c</span>
<span class="quote">&gt; index 4043df7c672f..e54166d3732e 100644</span>
<span class="quote">&gt; --- a/mm/zswap.c</span>
<span class="quote">&gt; +++ b/mm/zswap.c</span>
<span class="quote">&gt; @@ -571,7 +571,7 @@ static struct zswap_pool *zswap_pool_find_get(char *type, char *compressor)</span>
<span class="quote">&gt;  static struct zswap_pool *zswap_pool_create(char *type, char *compressor)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct zswap_pool *pool;</span>
<span class="quote">&gt; -	gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="quote">&gt; +	gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pool = kzalloc(sizeof(*pool), GFP_KERNEL);</span>
<span class="quote">&gt;  	if (!pool) {</span>
<span class="quote">&gt; @@ -1011,7 +1011,8 @@ static int zswap_frontswap_store(unsigned type, pgoff_t offset,</span>
<span class="quote">&gt;  	/* store */</span>
<span class="quote">&gt;  	len = dlen + sizeof(struct zswap_header);</span>
<span class="quote">&gt;  	ret = zpool_malloc(entry-&gt;pool-&gt;zpool, len,</span>
<span class="quote">&gt; -			   __GFP_NORETRY | __GFP_NOWARN, &amp;handle);</span>
<span class="quote">&gt; +			   __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM,</span>
<span class="quote">&gt; +			   &amp;handle);</span>
<span class="quote">&gt;  	if (ret == -ENOSPC) {</span>
<span class="quote">&gt;  		zswap_reject_compress_poor++;</span>
<span class="quote">&gt;  		goto put_dstmem;</span>
<span class="quote">&gt; diff --git a/net/core/skbuff.c b/net/core/skbuff.c</span>
<span class="quote">&gt; index dad4dd37e2aa..905bae96a742 100644</span>
<span class="quote">&gt; --- a/net/core/skbuff.c</span>
<span class="quote">&gt; +++ b/net/core/skbuff.c</span>
<span class="quote">&gt; @@ -414,7 +414,7 @@ struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,</span>
<span class="quote">&gt;  	len += NET_SKB_PAD;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if ((len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE)) ||</span>
<span class="quote">&gt; -	    (gfp_mask &amp; (__GFP_WAIT | GFP_DMA))) {</span>
<span class="quote">&gt; +	    (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {</span>
<span class="quote">&gt;  		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);</span>
<span class="quote">&gt;  		if (!skb)</span>
<span class="quote">&gt;  			goto skb_fail;</span>
<span class="quote">&gt; @@ -481,7 +481,7 @@ struct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,</span>
<span class="quote">&gt;  	len += NET_SKB_PAD + NET_IP_ALIGN;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if ((len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE)) ||</span>
<span class="quote">&gt; -	    (gfp_mask &amp; (__GFP_WAIT | GFP_DMA))) {</span>
<span class="quote">&gt; +	    (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {</span>
<span class="quote">&gt;  		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);</span>
<span class="quote">&gt;  		if (!skb)</span>
<span class="quote">&gt;  			goto skb_fail;</span>
<span class="quote">&gt; @@ -4451,7 +4451,7 @@ struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	gfp_head = gfp_mask;</span>
<span class="quote">&gt; -	if (gfp_head &amp; __GFP_WAIT)</span>
<span class="quote">&gt; +	if (gfp_head &amp; __GFP_DIRECT_RECLAIM)</span>
<span class="quote">&gt;  		gfp_head |= __GFP_REPEAT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	*errcode = -ENOBUFS;</span>
<span class="quote">&gt; @@ -4466,7 +4466,7 @@ struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		while (order) {</span>
<span class="quote">&gt;  			if (npages &gt;= 1 &lt;&lt; order) {</span>
<span class="quote">&gt; -				page = alloc_pages((gfp_mask &amp; ~__GFP_WAIT) |</span>
<span class="quote">&gt; +				page = alloc_pages((gfp_mask &amp; ~__GFP_DIRECT_RECLAIM) |</span>
<span class="quote">&gt;  						   __GFP_COMP |</span>
<span class="quote">&gt;  						   __GFP_NOWARN |</span>
<span class="quote">&gt;  						   __GFP_NORETRY,</span>
<span class="quote">&gt; diff --git a/net/core/sock.c b/net/core/sock.c</span>
<span class="quote">&gt; index ca2984afe16e..4a61a0add949 100644</span>
<span class="quote">&gt; --- a/net/core/sock.c</span>
<span class="quote">&gt; +++ b/net/core/sock.c</span>
<span class="quote">&gt; @@ -1879,8 +1879,10 @@ bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pfrag-&gt;offset = 0;</span>
<span class="quote">&gt;  	if (SKB_FRAG_PAGE_ORDER) {</span>
<span class="quote">&gt; -		pfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_WAIT) | __GFP_COMP |</span>
<span class="quote">&gt; -					  __GFP_NOWARN | __GFP_NORETRY,</span>
<span class="quote">&gt; +		/* Avoid direct reclaim but allow kswapd to wake */</span>
<span class="quote">&gt; +		pfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_DIRECT_RECLAIM) |</span>
<span class="quote">&gt; +					  __GFP_COMP | __GFP_NOWARN |</span>
<span class="quote">&gt; +					  __GFP_NORETRY,</span>
<span class="quote">&gt;  					  SKB_FRAG_PAGE_ORDER);</span>
<span class="quote">&gt;  		if (likely(pfrag-&gt;page)) {</span>
<span class="quote">&gt;  			pfrag-&gt;size = PAGE_SIZE &lt;&lt; SKB_FRAG_PAGE_ORDER;</span>
<span class="quote">&gt; diff --git a/net/netlink/af_netlink.c b/net/netlink/af_netlink.c</span>
<span class="quote">&gt; index 7f86d3b55060..173c0abe4094 100644</span>
<span class="quote">&gt; --- a/net/netlink/af_netlink.c</span>
<span class="quote">&gt; +++ b/net/netlink/af_netlink.c</span>
<span class="quote">&gt; @@ -2084,7 +2084,7 @@ int netlink_broadcast_filtered(struct sock *ssk, struct sk_buff *skb, u32 portid</span>
<span class="quote">&gt;  	consume_skb(info.skb2);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (info.delivered) {</span>
<span class="quote">&gt; -		if (info.congested &amp;&amp; (allocation &amp; __GFP_WAIT))</span>
<span class="quote">&gt; +		if (info.congested &amp;&amp; gfpflags_allow_blocking(allocation))</span>
<span class="quote">&gt;  			yield();</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/net/rds/ib_recv.c b/net/rds/ib_recv.c</span>
<span class="quote">&gt; index f43831e4186a..dcfb59775acc 100644</span>
<span class="quote">&gt; --- a/net/rds/ib_recv.c</span>
<span class="quote">&gt; +++ b/net/rds/ib_recv.c</span>
<span class="quote">&gt; @@ -305,7 +305,7 @@ static int rds_ib_recv_refill_one(struct rds_connection *conn,</span>
<span class="quote">&gt;  	gfp_t slab_mask = GFP_NOWAIT;</span>
<span class="quote">&gt;  	gfp_t page_mask = GFP_NOWAIT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (gfp &amp; __GFP_WAIT) {</span>
<span class="quote">&gt; +	if (gfp &amp; __GFP_DIRECT_RECLAIM) {</span>
<span class="quote">&gt;  		slab_mask = GFP_KERNEL;</span>
<span class="quote">&gt;  		page_mask = GFP_HIGHUSER;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -379,7 +379,7 @@ void rds_ib_recv_refill(struct rds_connection *conn, int prefill, gfp_t gfp)</span>
<span class="quote">&gt;  	struct ib_recv_wr *failed_wr;</span>
<span class="quote">&gt;  	unsigned int posted = 0;</span>
<span class="quote">&gt;  	int ret = 0;</span>
<span class="quote">&gt; -	bool can_wait = !!(gfp &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	bool can_wait = !!(gfp &amp; __GFP_DIRECT_RECLAIM);</span>
<span class="quote">&gt;  	u32 pos;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* the goal here is to just make sure that someone, somewhere</span>
<span class="quote">&gt; diff --git a/net/rxrpc/ar-connection.c b/net/rxrpc/ar-connection.c</span>
<span class="quote">&gt; index 6631f4f1e39b..3b5de4b86058 100644</span>
<span class="quote">&gt; --- a/net/rxrpc/ar-connection.c</span>
<span class="quote">&gt; +++ b/net/rxrpc/ar-connection.c</span>
<span class="quote">&gt; @@ -500,7 +500,7 @@ int rxrpc_connect_call(struct rxrpc_sock *rx,</span>
<span class="quote">&gt;  		if (bundle-&gt;num_conns &gt;= 20) {</span>
<span class="quote">&gt;  			_debug(&quot;too many conns&quot;);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -			if (!(gfp &amp; __GFP_WAIT)) {</span>
<span class="quote">&gt; +			if (!gfpflags_allow_blocking(gfp)) {</span>
<span class="quote">&gt;  				_leave(&quot; = -EAGAIN&quot;);</span>
<span class="quote">&gt;  				return -EAGAIN;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; diff --git a/net/sctp/associola.c b/net/sctp/associola.c</span>
<span class="quote">&gt; index 197c3f59ecbf..75369ae8de1e 100644</span>
<span class="quote">&gt; --- a/net/sctp/associola.c</span>
<span class="quote">&gt; +++ b/net/sctp/associola.c</span>
<span class="quote">&gt; @@ -1588,7 +1588,7 @@ int sctp_assoc_lookup_laddr(struct sctp_association *asoc,</span>
<span class="quote">&gt;  /* Set an association id for a given association */</span>
<span class="quote">&gt;  int sctp_assoc_set_id(struct sctp_association *asoc, gfp_t gfp)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	bool preload = !!(gfp &amp; __GFP_WAIT);</span>
<span class="quote">&gt; +	bool preload = gfpflags_allow_blocking(gfp);</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* If the id is already assigned, keep it. */</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.4.6</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Sept. 24, 2015, 8:55 p.m.</div>
<pre class="content">
On Mon, Sep 21, 2015 at 11:52:37AM +0100, Mel Gorman wrote:
<span class="quote">&gt; @@ -119,10 +134,10 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define GFP_USER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)</span>
<span class="quote">&gt;  #define GFP_HIGHUSER	(GFP_USER | __GFP_HIGHMEM)</span>
<span class="quote">&gt;  #define GFP_HIGHUSER_MOVABLE	(GFP_HIGHUSER | __GFP_MOVABLE)</span>
<span class="quote">&gt; -#define GFP_IOFS	(__GFP_IO | __GFP_FS)</span>
<span class="quote">&gt; -#define GFP_TRANSHUGE	(GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="quote">&gt; -			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | \</span>
<span class="quote">&gt; -			 __GFP_NO_KSWAPD)</span>
<span class="quote">&gt; +#define GFP_IOFS	(__GFP_IO | __GFP_FS | __GFP_KSWAPD_RECLAIM)</span>

These are some really odd semantics to be given a name like that.

GFP_IOFS was introduced as a short-hand for testing/setting/clearing
these two bits at the same time, not to be used for allocations. In
fact, the only user for allocations is lustre, and it&#39;s not at all
obious why those sites shouldn&#39;t include __GFP_WAIT as well.

Removing this definition altogether would probably be best.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - Sept. 25, 2015, 12:51 p.m.</div>
<pre class="content">
On Thu, Sep 24, 2015 at 04:55:09PM -0400, Johannes Weiner wrote:
<span class="quote">&gt; On Mon, Sep 21, 2015 at 11:52:37AM +0100, Mel Gorman wrote:</span>
<span class="quote">&gt; &gt; @@ -119,10 +134,10 @@ struct vm_area_struct;</span>
<span class="quote">&gt; &gt;  #define GFP_USER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)</span>
<span class="quote">&gt; &gt;  #define GFP_HIGHUSER	(GFP_USER | __GFP_HIGHMEM)</span>
<span class="quote">&gt; &gt;  #define GFP_HIGHUSER_MOVABLE	(GFP_HIGHUSER | __GFP_MOVABLE)</span>
<span class="quote">&gt; &gt; -#define GFP_IOFS	(__GFP_IO | __GFP_FS)</span>
<span class="quote">&gt; &gt; -#define GFP_TRANSHUGE	(GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="quote">&gt; &gt; -			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | \</span>
<span class="quote">&gt; &gt; -			 __GFP_NO_KSWAPD)</span>
<span class="quote">&gt; &gt; +#define GFP_IOFS	(__GFP_IO | __GFP_FS | __GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; These are some really odd semantics to be given a name like that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; GFP_IOFS was introduced as a short-hand for testing/setting/clearing</span>
<span class="quote">&gt; these two bits at the same time, not to be used for allocations. In</span>
<span class="quote">&gt; fact, the only user for allocations is lustre, and it&#39;s not at all</span>
<span class="quote">&gt; obious why those sites shouldn&#39;t include __GFP_WAIT as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Removing this definition altogether would probably be best.</span>

Ok, I&#39;ll add a TODO to create a patch that removes GFP_IOFS entirely. It
can be tacked on to the end of the series.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Sept. 25, 2015, 7:01 p.m.</div>
<pre class="content">
On Fri, Sep 25, 2015 at 01:51:06PM +0100, Mel Gorman wrote:
<span class="quote">&gt; On Thu, Sep 24, 2015 at 04:55:09PM -0400, Johannes Weiner wrote:</span>
<span class="quote">&gt; &gt; On Mon, Sep 21, 2015 at 11:52:37AM +0100, Mel Gorman wrote:</span>
<span class="quote">&gt; &gt; &gt; @@ -119,10 +134,10 @@ struct vm_area_struct;</span>
<span class="quote">&gt; &gt; &gt;  #define GFP_USER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)</span>
<span class="quote">&gt; &gt; &gt;  #define GFP_HIGHUSER	(GFP_USER | __GFP_HIGHMEM)</span>
<span class="quote">&gt; &gt; &gt;  #define GFP_HIGHUSER_MOVABLE	(GFP_HIGHUSER | __GFP_MOVABLE)</span>
<span class="quote">&gt; &gt; &gt; -#define GFP_IOFS	(__GFP_IO | __GFP_FS)</span>
<span class="quote">&gt; &gt; &gt; -#define GFP_TRANSHUGE	(GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="quote">&gt; &gt; &gt; -			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | \</span>
<span class="quote">&gt; &gt; &gt; -			 __GFP_NO_KSWAPD)</span>
<span class="quote">&gt; &gt; &gt; +#define GFP_IOFS	(__GFP_IO | __GFP_FS | __GFP_KSWAPD_RECLAIM)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; These are some really odd semantics to be given a name like that.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; GFP_IOFS was introduced as a short-hand for testing/setting/clearing</span>
<span class="quote">&gt; &gt; these two bits at the same time, not to be used for allocations. In</span>
<span class="quote">&gt; &gt; fact, the only user for allocations is lustre, and it&#39;s not at all</span>
<span class="quote">&gt; &gt; obious why those sites shouldn&#39;t include __GFP_WAIT as well.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Removing this definition altogether would probably be best.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, I&#39;ll add a TODO to create a patch that removes GFP_IOFS entirely. It</span>
<span class="quote">&gt; can be tacked on to the end of the series.</span>

Okay, that makes sense to me. Thanks!
<span class="acked-by">
Acked-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/vm/balance b/Documentation/vm/balance</span>
<span class="p_header">index c46e68cf9344..964595481af6 100644</span>
<span class="p_header">--- a/Documentation/vm/balance</span>
<span class="p_header">+++ b/Documentation/vm/balance</span>
<span class="p_chunk">@@ -1,12 +1,14 @@</span> <span class="p_context"></span>
 Started Jan 2000 by Kanoj Sarcar &lt;kanoj@sgi.com&gt;
 
<span class="p_del">-Memory balancing is needed for non __GFP_WAIT as well as for non</span>
<span class="p_del">-__GFP_IO allocations.</span>
<span class="p_add">+Memory balancing is needed for !__GFP_ATOMIC and !__GFP_KSWAPD_RECLAIM as</span>
<span class="p_add">+well as for non __GFP_IO allocations.</span>
 
<span class="p_del">-There are two reasons to be requesting non __GFP_WAIT allocations:</span>
<span class="p_del">-the caller can not sleep (typically intr context), or does not want</span>
<span class="p_del">-to incur cost overheads of page stealing and possible swap io for</span>
<span class="p_del">-whatever reasons.</span>
<span class="p_add">+The first reason why a caller may avoid reclaim is that the caller can not</span>
<span class="p_add">+sleep due to holding a spinlock or is in interrupt context. The second may</span>
<span class="p_add">+be that the caller is willing to fail the allocation without incurring the</span>
<span class="p_add">+overhead of page reclaim. This may happen for opportunistic high-order</span>
<span class="p_add">+allocation requests that have order-0 fallback options. In such cases,</span>
<span class="p_add">+the caller may also wish to avoid waking kswapd.</span>
 
 __GFP_IO allocation requests are made to prevent file system deadlocks.
 
<span class="p_header">diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c</span>
<span class="p_header">index 1a7815e5421b..38307d8312ac 100644</span>
<span class="p_header">--- a/arch/arm/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -651,12 +651,12 @@</span> <span class="p_context"> static void *__dma_alloc(struct device *dev, size_t size, dma_addr_t *handle,</span>
 
 	if (nommu())
 		addr = __alloc_simple_buffer(dev, size, gfp, &amp;page);
<span class="p_del">-	else if (dev_get_cma_area(dev) &amp;&amp; (gfp &amp; __GFP_WAIT))</span>
<span class="p_add">+	else if (dev_get_cma_area(dev) &amp;&amp; (gfp &amp; __GFP_DIRECT_RECLAIM))</span>
 		addr = __alloc_from_contiguous(dev, size, prot, &amp;page,
 					       caller, want_vaddr);
 	else if (is_coherent)
 		addr = __alloc_simple_buffer(dev, size, gfp, &amp;page);
<span class="p_del">-	else if (!(gfp &amp; __GFP_WAIT))</span>
<span class="p_add">+	else if (!gfpflags_allow_blocking(gfp))</span>
 		addr = __alloc_from_pool(size, &amp;page);
 	else
 		addr = __alloc_remap_buffer(dev, size, gfp, prot, &amp;page,
<span class="p_chunk">@@ -1363,7 +1363,7 @@</span> <span class="p_context"> static void *arm_iommu_alloc_attrs(struct device *dev, size_t size,</span>
 	*handle = DMA_ERROR_CODE;
 	size = PAGE_ALIGN(size);
 
<span class="p_del">-	if (!(gfp &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp))</span>
 		return __iommu_alloc_atomic(dev, size, handle);
 
 	/*
<span class="p_header">diff --git a/arch/arm/xen/mm.c b/arch/arm/xen/mm.c</span>
<span class="p_header">index 6dd911d1f0ac..99eec9063f68 100644</span>
<span class="p_header">--- a/arch/arm/xen/mm.c</span>
<span class="p_header">+++ b/arch/arm/xen/mm.c</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"></span>
 unsigned long xen_get_swiotlb_free_pages(unsigned int order)
 {
 	struct memblock_region *reg;
<span class="p_del">-	gfp_t flags = __GFP_NOWARN;</span>
<span class="p_add">+	gfp_t flags = __GFP_NOWARN|__GFP_KSWAPD_RECLAIM;</span>
 
 	for_each_memblock(memory, reg) {
 		if (reg-&gt;base &lt; (phys_addr_t)0xffffffff) {
<span class="p_header">diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">index 99224dcebdc5..478234383c2c 100644</span>
<span class="p_header">--- a/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -100,7 +100,7 @@</span> <span class="p_context"> static void *__dma_alloc_coherent(struct device *dev, size_t size,</span>
 	if (IS_ENABLED(CONFIG_ZONE_DMA) &amp;&amp;
 	    dev-&gt;coherent_dma_mask &lt;= DMA_BIT_MASK(32))
 		flags |= GFP_DMA;
<span class="p_del">-	if (dev_get_cma_area(dev) &amp;&amp; (flags &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (dev_get_cma_area(dev) &amp;&amp; gfpflags_allow_blocking(flags)) {</span>
 		struct page *page;
 		void *addr;
 
<span class="p_chunk">@@ -148,7 +148,7 @@</span> <span class="p_context"> static void *__dma_alloc(struct device *dev, size_t size,</span>
 
 	size = PAGE_ALIGN(size);
 
<span class="p_del">-	if (!coherent &amp;&amp; !(flags &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (!coherent &amp;&amp; !gfpflags_allow_blocking(flags)) {</span>
 		struct page *page = NULL;
 		void *addr = __alloc_from_pool(size, &amp;page, flags);
 
<span class="p_header">diff --git a/arch/x86/kernel/pci-dma.c b/arch/x86/kernel/pci-dma.c</span>
<span class="p_header">index 1b55de1267cf..a8e618b16a66 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pci-dma.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pci-dma.c</span>
<span class="p_chunk">@@ -90,7 +90,7 @@</span> <span class="p_context"> void *dma_generic_alloc_coherent(struct device *dev, size_t size,</span>
 again:
 	page = NULL;
 	/* CMA can be used only in the context which permits sleeping */
<span class="p_del">-	if (flag &amp; __GFP_WAIT) {</span>
<span class="p_add">+	if (gfpflags_allow_blocking(flag)) {</span>
 		page = dma_alloc_from_contiguous(dev, count, get_order(size));
 		if (page &amp;&amp; page_to_phys(page) + size &gt; dma_mask) {
 			dma_release_from_contiguous(dev, page, count);
<span class="p_header">diff --git a/block/bio.c b/block/bio.c</span>
<span class="p_header">index ad3f276d74bc..4f184d938942 100644</span>
<span class="p_header">--- a/block/bio.c</span>
<span class="p_header">+++ b/block/bio.c</span>
<span class="p_chunk">@@ -211,7 +211,7 @@</span> <span class="p_context"> struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,</span>
 		bvl = mempool_alloc(pool, gfp_mask);
 	} else {
 		struct biovec_slab *bvs = bvec_slabs + *idx;
<span class="p_del">-		gfp_t __gfp_mask = gfp_mask &amp; ~(__GFP_WAIT | __GFP_IO);</span>
<span class="p_add">+		gfp_t __gfp_mask = gfp_mask &amp; ~(__GFP_DIRECT_RECLAIM | __GFP_IO);</span>
 
 		/*
 		 * Make this allocation restricted and don&#39;t dump info on
<span class="p_chunk">@@ -221,11 +221,11 @@</span> <span class="p_context"> struct bio_vec *bvec_alloc(gfp_t gfp_mask, int nr, unsigned long *idx,</span>
 		__gfp_mask |= __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN;
 
 		/*
<span class="p_del">-		 * Try a slab allocation. If this fails and __GFP_WAIT</span>
<span class="p_add">+		 * Try a slab allocation. If this fails and __GFP_DIRECT_RECLAIM</span>
 		 * is set, retry with the 1-entry mempool
 		 */
 		bvl = kmem_cache_alloc(bvs-&gt;slab, __gfp_mask);
<span class="p_del">-		if (unlikely(!bvl &amp;&amp; (gfp_mask &amp; __GFP_WAIT))) {</span>
<span class="p_add">+		if (unlikely(!bvl &amp;&amp; (gfp_mask &amp; __GFP_DIRECT_RECLAIM))) {</span>
 			*idx = BIOVEC_MAX_IDX;
 			goto fallback;
 		}
<span class="p_chunk">@@ -395,12 +395,12 @@</span> <span class="p_context"> static void punt_bios_to_rescuer(struct bio_set *bs)</span>
  *   If @bs is NULL, uses kmalloc() to allocate the bio; else the allocation is
  *   backed by the @bs&#39;s mempool.
  *
<span class="p_del">- *   When @bs is not NULL, if %__GFP_WAIT is set then bio_alloc will always be</span>
<span class="p_del">- *   able to allocate a bio. This is due to the mempool guarantees. To make this</span>
<span class="p_del">- *   work, callers must never allocate more than 1 bio at a time from this pool.</span>
<span class="p_del">- *   Callers that need to allocate more than 1 bio must always submit the</span>
<span class="p_del">- *   previously allocated bio for IO before attempting to allocate a new one.</span>
<span class="p_del">- *   Failure to do so can cause deadlocks under memory pressure.</span>
<span class="p_add">+ *   When @bs is not NULL, if %__GFP_DIRECT_RECLAIM is set then bio_alloc will</span>
<span class="p_add">+ *   always be able to allocate a bio. This is due to the mempool guarantees.</span>
<span class="p_add">+ *   To make this work, callers must never allocate more than 1 bio at a time</span>
<span class="p_add">+ *   from this pool. Callers that need to allocate more than 1 bio must always</span>
<span class="p_add">+ *   submit the previously allocated bio for IO before attempting to allocate</span>
<span class="p_add">+ *   a new one. Failure to do so can cause deadlocks under memory pressure.</span>
  *
  *   Note that when running under generic_make_request() (i.e. any block
  *   driver), bios are not submitted until after you return - see the code in
<span class="p_chunk">@@ -459,13 +459,13 @@</span> <span class="p_context"> struct bio *bio_alloc_bioset(gfp_t gfp_mask, int nr_iovecs, struct bio_set *bs)</span>
 		 * We solve this, and guarantee forward progress, with a rescuer
 		 * workqueue per bio_set. If we go to allocate and there are
 		 * bios on current-&gt;bio_list, we first try the allocation
<span class="p_del">-		 * without __GFP_WAIT; if that fails, we punt those bios we</span>
<span class="p_del">-		 * would be blocking to the rescuer workqueue before we retry</span>
<span class="p_del">-		 * with the original gfp_flags.</span>
<span class="p_add">+		 * without __GFP_DIRECT_RECLAIM; if that fails, we punt those</span>
<span class="p_add">+		 * bios we would be blocking to the rescuer workqueue before</span>
<span class="p_add">+		 * we retry with the original gfp_flags.</span>
 		 */
 
 		if (current-&gt;bio_list &amp;&amp; !bio_list_empty(current-&gt;bio_list))
<span class="p_del">-			gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="p_add">+			gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
 
 		p = mempool_alloc(bs-&gt;bio_pool, gfp_mask);
 		if (!p &amp;&amp; gfp_mask != saved_gfp) {
<span class="p_header">diff --git a/block/blk-core.c b/block/blk-core.c</span>
<span class="p_header">index 2eb722d48773..0391206868e9 100644</span>
<span class="p_header">--- a/block/blk-core.c</span>
<span class="p_header">+++ b/block/blk-core.c</span>
<span class="p_chunk">@@ -1160,8 +1160,8 @@</span> <span class="p_context"> static struct request *__get_request(struct request_list *rl, int rw_flags,</span>
  * @bio: bio to allocate request for (can be %NULL)
  * @gfp_mask: allocation mask
  *
<span class="p_del">- * Get a free request from @q.  If %__GFP_WAIT is set in @gfp_mask, this</span>
<span class="p_del">- * function keeps retrying under memory pressure and fails iff @q is dead.</span>
<span class="p_add">+ * Get a free request from @q.  If %__GFP_DIRECT_RECLAIM is set in @gfp_mask,</span>
<span class="p_add">+ * this function keeps retrying under memory pressure and fails iff @q is dead.</span>
  *
  * Must be called with @q-&gt;queue_lock held and,
  * Returns ERR_PTR on failure, with @q-&gt;queue_lock held.
<span class="p_chunk">@@ -1181,7 +1181,7 @@</span> <span class="p_context"> static struct request *get_request(struct request_queue *q, int rw_flags,</span>
 	if (!IS_ERR(rq))
 		return rq;
 
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT) || unlikely(blk_queue_dying(q))) {</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp_mask) || unlikely(blk_queue_dying(q))) {</span>
 		blk_put_rl(rl);
 		return rq;
 	}
<span class="p_chunk">@@ -1259,11 +1259,11 @@</span> <span class="p_context"> EXPORT_SYMBOL(blk_get_request);</span>
  * BUG.
  *
  * WARNING: When allocating/cloning a bio-chain, careful consideration should be
<span class="p_del">- * given to how you allocate bios. In particular, you cannot use __GFP_WAIT for</span>
<span class="p_del">- * anything but the first bio in the chain. Otherwise you risk waiting for IO</span>
<span class="p_del">- * completion of a bio that hasn&#39;t been submitted yet, thus resulting in a</span>
<span class="p_del">- * deadlock. Alternatively bios should be allocated using bio_kmalloc() instead</span>
<span class="p_del">- * of bio_alloc(), as that avoids the mempool deadlock.</span>
<span class="p_add">+ * given to how you allocate bios. In particular, you cannot use</span>
<span class="p_add">+ * __GFP_DIRECT_RECLAIM for anything but the first bio in the chain. Otherwise</span>
<span class="p_add">+ * you risk waiting for IO completion of a bio that hasn&#39;t been submitted yet,</span>
<span class="p_add">+ * thus resulting in a deadlock. Alternatively bios should be allocated using</span>
<span class="p_add">+ * bio_kmalloc() instead of bio_alloc(), as that avoids the mempool deadlock.</span>
  * If possible a big IO should be split into smaller parts when allocation
  * fails. Partial allocation should not be an error, or you risk a live-lock.
  */
<span class="p_header">diff --git a/block/blk-ioc.c b/block/blk-ioc.c</span>
<span class="p_header">index 1a27f45ec776..381cb50a673c 100644</span>
<span class="p_header">--- a/block/blk-ioc.c</span>
<span class="p_header">+++ b/block/blk-ioc.c</span>
<span class="p_chunk">@@ -289,7 +289,7 @@</span> <span class="p_context"> struct io_context *get_task_io_context(struct task_struct *task,</span>
 {
 	struct io_context *ioc;
 
<span class="p_del">-	might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
 
 	do {
 		task_lock(task);
<span class="p_header">diff --git a/block/blk-mq-tag.c b/block/blk-mq-tag.c</span>
<span class="p_header">index 9115c6d59948..f6020c624967 100644</span>
<span class="p_header">--- a/block/blk-mq-tag.c</span>
<span class="p_header">+++ b/block/blk-mq-tag.c</span>
<span class="p_chunk">@@ -264,7 +264,7 @@</span> <span class="p_context"> static int bt_get(struct blk_mq_alloc_data *data,</span>
 	if (tag != -1)
 		return tag;
 
<span class="p_del">-	if (!(data-&gt;gfp &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(data-&gt;gfp))</span>
 		return -1;
 
 	bs = bt_wait_ptr(bt, hctx);
<span class="p_header">diff --git a/block/blk-mq.c b/block/blk-mq.c</span>
<span class="p_header">index f2d67b4047a0..7c322cea838f 100644</span>
<span class="p_header">--- a/block/blk-mq.c</span>
<span class="p_header">+++ b/block/blk-mq.c</span>
<span class="p_chunk">@@ -85,7 +85,7 @@</span> <span class="p_context"> static int blk_mq_queue_enter(struct request_queue *q, gfp_t gfp)</span>
 		if (percpu_ref_tryget_live(&amp;q-&gt;mq_usage_counter))
 			return 0;
 
<span class="p_del">-		if (!(gfp &amp; __GFP_WAIT))</span>
<span class="p_add">+		if (!gfpflags_allow_blocking(gfp))</span>
 			return -EBUSY;
 
 		ret = wait_event_interruptible(q-&gt;mq_freeze_wq,
<span class="p_chunk">@@ -261,11 +261,11 @@</span> <span class="p_context"> struct request *blk_mq_alloc_request(struct request_queue *q, int rw, gfp_t gfp,</span>
 
 	ctx = blk_mq_get_ctx(q);
 	hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);
<span class="p_del">-	blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_WAIT,</span>
<span class="p_add">+	blk_mq_set_alloc_data(&amp;alloc_data, q, gfp &amp; ~__GFP_DIRECT_RECLAIM,</span>
 			reserved, ctx, hctx);
 
 	rq = __blk_mq_alloc_request(&amp;alloc_data, rw);
<span class="p_del">-	if (!rq &amp;&amp; (gfp &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (!rq &amp;&amp; (gfp &amp; __GFP_DIRECT_RECLAIM)) {</span>
 		__blk_mq_run_hw_queue(hctx);
 		blk_mq_put_ctx(ctx);
 
<span class="p_chunk">@@ -1207,7 +1207,7 @@</span> <span class="p_context"> static struct request *blk_mq_map_request(struct request_queue *q,</span>
 		ctx = blk_mq_get_ctx(q);
 		hctx = q-&gt;mq_ops-&gt;map_queue(q, ctx-&gt;cpu);
 		blk_mq_set_alloc_data(&amp;alloc_data, q,
<span class="p_del">-				__GFP_WAIT|GFP_ATOMIC, false, ctx, hctx);</span>
<span class="p_add">+				__GFP_WAIT|__GFP_HIGH, false, ctx, hctx);</span>
 		rq = __blk_mq_alloc_request(&amp;alloc_data, rw);
 		ctx = alloc_data.ctx;
 		hctx = alloc_data.hctx;
<span class="p_header">diff --git a/drivers/block/drbd/drbd_receiver.c b/drivers/block/drbd/drbd_receiver.c</span>
<span class="p_header">index c097909c589c..b4b5680ac6ad 100644</span>
<span class="p_header">--- a/drivers/block/drbd/drbd_receiver.c</span>
<span class="p_header">+++ b/drivers/block/drbd/drbd_receiver.c</span>
<span class="p_chunk">@@ -357,7 +357,8 @@</span> <span class="p_context"> drbd_alloc_peer_req(struct drbd_peer_device *peer_device, u64 id, sector_t secto</span>
 	}
 
 	if (has_payload &amp;&amp; data_size) {
<span class="p_del">-		page = drbd_alloc_pages(peer_device, nr_pages, (gfp_mask &amp; __GFP_WAIT));</span>
<span class="p_add">+		page = drbd_alloc_pages(peer_device, nr_pages,</span>
<span class="p_add">+					gfpflags_allow_blocking(gfp_mask));</span>
 		if (!page)
 			goto fail;
 	}
<span class="p_header">diff --git a/drivers/block/osdblk.c b/drivers/block/osdblk.c</span>
<span class="p_header">index e22942596207..1b709a4e3b5e 100644</span>
<span class="p_header">--- a/drivers/block/osdblk.c</span>
<span class="p_header">+++ b/drivers/block/osdblk.c</span>
<span class="p_chunk">@@ -271,7 +271,7 @@</span> <span class="p_context"> static struct bio *bio_chain_clone(struct bio *old_chain, gfp_t gfpmask)</span>
 			goto err_out;
 
 		tmp-&gt;bi_bdev = NULL;
<span class="p_del">-		gfpmask &amp;= ~__GFP_WAIT;</span>
<span class="p_add">+		gfpmask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
 		tmp-&gt;bi_next = NULL;
 
 		if (!new_chain)
<span class="p_header">diff --git a/drivers/connector/connector.c b/drivers/connector/connector.c</span>
<span class="p_header">index 30f522848c73..d7373ca69c99 100644</span>
<span class="p_header">--- a/drivers/connector/connector.c</span>
<span class="p_header">+++ b/drivers/connector/connector.c</span>
<span class="p_chunk">@@ -124,7 +124,8 @@</span> <span class="p_context"> int cn_netlink_send_mult(struct cn_msg *msg, u16 len, u32 portid, u32 __group,</span>
 	if (group)
 		return netlink_broadcast(dev-&gt;nls, skb, portid, group,
 					 gfp_mask);
<span class="p_del">-	return netlink_unicast(dev-&gt;nls, skb, portid, !(gfp_mask&amp;__GFP_WAIT));</span>
<span class="p_add">+	return netlink_unicast(dev-&gt;nls, skb, portid,</span>
<span class="p_add">+			!gfpflags_allow_blocking(gfp_mask));</span>
 }
 EXPORT_SYMBOL_GPL(cn_netlink_send_mult);
 
<span class="p_header">diff --git a/drivers/firewire/core-cdev.c b/drivers/firewire/core-cdev.c</span>
<span class="p_header">index 2a3973a7c441..36a7c2d89a01 100644</span>
<span class="p_header">--- a/drivers/firewire/core-cdev.c</span>
<span class="p_header">+++ b/drivers/firewire/core-cdev.c</span>
<span class="p_chunk">@@ -486,7 +486,7 @@</span> <span class="p_context"> static int ioctl_get_info(struct client *client, union ioctl_arg *arg)</span>
 static int add_client_resource(struct client *client,
 			       struct client_resource *resource, gfp_t gfp_mask)
 {
<span class="p_del">-	bool preload = !!(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	bool preload = gfpflags_allow_blocking(gfp_mask);</span>
 	unsigned long flags;
 	int ret;
 
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">index 4d631a946481..d58cb9e034fe 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_chunk">@@ -2215,7 +2215,7 @@</span> <span class="p_context"> i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
 	 */
 	mapping = file_inode(obj-&gt;base.filp)-&gt;i_mapping;
 	gfp = mapping_gfp_mask(mapping);
<span class="p_del">-	gfp |= __GFP_NORETRY | __GFP_NOWARN | __GFP_NO_KSWAPD;</span>
<span class="p_add">+	gfp |= __GFP_NORETRY | __GFP_NOWARN;</span>
 	gfp &amp;= ~(__GFP_IO | __GFP_WAIT);
 	sg = st-&gt;sgl;
 	st-&gt;nents = 0;
<span class="p_header">diff --git a/drivers/infiniband/core/sa_query.c b/drivers/infiniband/core/sa_query.c</span>
<span class="p_header">index 8c014b33d8e0..59ab264c99c4 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/sa_query.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/sa_query.c</span>
<span class="p_chunk">@@ -1083,7 +1083,7 @@</span> <span class="p_context"> static void init_mad(struct ib_sa_mad *mad, struct ib_mad_agent *agent)</span>
 
 static int send_mad(struct ib_sa_query *query, int timeout_ms, gfp_t gfp_mask)
 {
<span class="p_del">-	bool preload = !!(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	bool preload = gfpflags_allow_blocking(gfp_mask);</span>
 	unsigned long flags;
 	int ret, id;
 
<span class="p_header">diff --git a/drivers/iommu/amd_iommu.c b/drivers/iommu/amd_iommu.c</span>
<span class="p_header">index f82060e778a2..1c0006e1ba4a 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu.c</span>
<span class="p_chunk">@@ -2755,7 +2755,7 @@</span> <span class="p_context"> static void *alloc_coherent(struct device *dev, size_t size,</span>
 
 	page = alloc_pages(flag | __GFP_NOWARN,  get_order(size));
 	if (!page) {
<span class="p_del">-		if (!(flag &amp; __GFP_WAIT))</span>
<span class="p_add">+		if (!gfpflags_allow_blocking(flag))</span>
 			return NULL;
 
 		page = dma_alloc_from_contiguous(dev, size &gt;&gt; PAGE_SHIFT,
<span class="p_header">diff --git a/drivers/iommu/intel-iommu.c b/drivers/iommu/intel-iommu.c</span>
<span class="p_header">index 2d7349a3ee14..ecdafbe81a5e 100644</span>
<span class="p_header">--- a/drivers/iommu/intel-iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/intel-iommu.c</span>
<span class="p_chunk">@@ -3533,7 +3533,7 @@</span> <span class="p_context"> static void *intel_alloc_coherent(struct device *dev, size_t size,</span>
 			flags |= GFP_DMA32;
 	}
 
<span class="p_del">-	if (flags &amp; __GFP_WAIT) {</span>
<span class="p_add">+	if (gfpflags_allow_blocking(flags)) {</span>
 		unsigned int count = size &gt;&gt; PAGE_SHIFT;
 
 		page = dma_alloc_from_contiguous(dev, count, order);
<span class="p_header">diff --git a/drivers/md/dm-crypt.c b/drivers/md/dm-crypt.c</span>
<span class="p_header">index d60c88df5234..55ec935de2b4 100644</span>
<span class="p_header">--- a/drivers/md/dm-crypt.c</span>
<span class="p_header">+++ b/drivers/md/dm-crypt.c</span>
<span class="p_chunk">@@ -993,7 +993,7 @@</span> <span class="p_context"> static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
 	struct bio_vec *bvec;
 
 retry:
<span class="p_del">-	if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (unlikely(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
 		mutex_lock(&amp;cc-&gt;bio_alloc_lock);
 
 	clone = bio_alloc_bioset(GFP_NOIO, nr_iovecs, cc-&gt;bs);
<span class="p_chunk">@@ -1009,7 +1009,7 @@</span> <span class="p_context"> static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
 		if (!page) {
 			crypt_free_buffer_pages(cc, clone);
 			bio_put(clone);
<span class="p_del">-			gfp_mask |= __GFP_WAIT;</span>
<span class="p_add">+			gfp_mask |= __GFP_DIRECT_RECLAIM;</span>
 			goto retry;
 		}
 
<span class="p_chunk">@@ -1026,7 +1026,7 @@</span> <span class="p_context"> static struct bio *crypt_alloc_buffer(struct dm_crypt_io *io, unsigned size)</span>
 	}
 
 return_clone:
<span class="p_del">-	if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (unlikely(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
 		mutex_unlock(&amp;cc-&gt;bio_alloc_lock);
 
 	return clone;
<span class="p_header">diff --git a/drivers/md/dm-kcopyd.c b/drivers/md/dm-kcopyd.c</span>
<span class="p_header">index 3a7cade5e27d..1452ed9aacb4 100644</span>
<span class="p_header">--- a/drivers/md/dm-kcopyd.c</span>
<span class="p_header">+++ b/drivers/md/dm-kcopyd.c</span>
<span class="p_chunk">@@ -244,7 +244,7 @@</span> <span class="p_context"> static int kcopyd_get_pages(struct dm_kcopyd_client *kc,</span>
 	*pages = NULL;
 
 	do {
<span class="p_del">-		pl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY);</span>
<span class="p_add">+		pl = alloc_pl(__GFP_NOWARN | __GFP_NORETRY | __GFP_KSWAPD_RECLAIM);</span>
 		if (unlikely(!pl)) {
 			/* Use reserved pages */
 			pl = kc-&gt;pages;
<span class="p_header">diff --git a/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c b/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="p_header">index 53fff5425c13..fb2cb4bdc0c1 100644</span>
<span class="p_header">--- a/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="p_header">+++ b/drivers/media/pci/solo6x10/solo6x10-v4l2-enc.c</span>
<span class="p_chunk">@@ -1291,7 +1291,7 @@</span> <span class="p_context"> static struct solo_enc_dev *solo_enc_alloc(struct solo_dev *solo_dev,</span>
 	solo_enc-&gt;vidq.ops = &amp;solo_enc_video_qops;
 	solo_enc-&gt;vidq.mem_ops = &amp;vb2_dma_sg_memops;
 	solo_enc-&gt;vidq.drv_priv = solo_enc;
<span class="p_del">-	solo_enc-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="p_add">+	solo_enc-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
 	solo_enc-&gt;vidq.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;
 	solo_enc-&gt;vidq.buf_struct_size = sizeof(struct solo_vb2_buf);
 	solo_enc-&gt;vidq.lock = &amp;solo_enc-&gt;lock;
<span class="p_header">diff --git a/drivers/media/pci/solo6x10/solo6x10-v4l2.c b/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="p_header">index 63ae8a61f603..bde77b22340c 100644</span>
<span class="p_header">--- a/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="p_header">+++ b/drivers/media/pci/solo6x10/solo6x10-v4l2.c</span>
<span class="p_chunk">@@ -675,7 +675,7 @@</span> <span class="p_context"> int solo_v4l2_init(struct solo_dev *solo_dev, unsigned nr)</span>
 	solo_dev-&gt;vidq.mem_ops = &amp;vb2_dma_contig_memops;
 	solo_dev-&gt;vidq.drv_priv = solo_dev;
 	solo_dev-&gt;vidq.timestamp_flags = V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC;
<span class="p_del">-	solo_dev-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="p_add">+	solo_dev-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
 	solo_dev-&gt;vidq.buf_struct_size = sizeof(struct solo_vb2_buf);
 	solo_dev-&gt;vidq.lock = &amp;solo_dev-&gt;lock;
 	ret = vb2_queue_init(&amp;solo_dev-&gt;vidq);
<span class="p_header">diff --git a/drivers/media/pci/tw68/tw68-video.c b/drivers/media/pci/tw68/tw68-video.c</span>
<span class="p_header">index 8355e55b4e8e..e556f989aaab 100644</span>
<span class="p_header">--- a/drivers/media/pci/tw68/tw68-video.c</span>
<span class="p_header">+++ b/drivers/media/pci/tw68/tw68-video.c</span>
<span class="p_chunk">@@ -975,7 +975,7 @@</span> <span class="p_context"> int tw68_video_init2(struct tw68_dev *dev, int video_nr)</span>
 	dev-&gt;vidq.ops = &amp;tw68_video_qops;
 	dev-&gt;vidq.mem_ops = &amp;vb2_dma_sg_memops;
 	dev-&gt;vidq.drv_priv = dev;
<span class="p_del">-	dev-&gt;vidq.gfp_flags = __GFP_DMA32;</span>
<span class="p_add">+	dev-&gt;vidq.gfp_flags = __GFP_DMA32 | __GFP_KSWAPD_RECLAIM;</span>
 	dev-&gt;vidq.buf_struct_size = sizeof(struct tw68_buf);
 	dev-&gt;vidq.lock = &amp;dev-&gt;lock;
 	dev-&gt;vidq.min_buffers_needed = 2;
<span class="p_header">diff --git a/drivers/mtd/mtdcore.c b/drivers/mtd/mtdcore.c</span>
<span class="p_header">index 8bbbb751bf45..2dfb291a47c6 100644</span>
<span class="p_header">--- a/drivers/mtd/mtdcore.c</span>
<span class="p_header">+++ b/drivers/mtd/mtdcore.c</span>
<span class="p_chunk">@@ -1188,8 +1188,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(mtd_writev);</span>
  */
 void *mtd_kmalloc_up_to(const struct mtd_info *mtd, size_t *size)
 {
<span class="p_del">-	gfp_t flags = __GFP_NOWARN | __GFP_WAIT |</span>
<span class="p_del">-		       __GFP_NORETRY | __GFP_NO_KSWAPD;</span>
<span class="p_add">+	gfp_t flags = __GFP_NOWARN | __GFP_DIRECT_RECLAIM | __GFP_NORETRY;</span>
 	size_t min_alloc = max_t(size_t, mtd-&gt;writesize, PAGE_SIZE);
 	void *kbuf;
 
<span class="p_header">diff --git a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="p_header">index 44173be5cbf0..f8d7a2f06950 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/broadcom/bnx2x/bnx2x_cmn.c</span>
<span class="p_chunk">@@ -691,7 +691,7 @@</span> <span class="p_context"> static void *bnx2x_frag_alloc(const struct bnx2x_fastpath *fp, gfp_t gfp_mask)</span>
 {
 	if (fp-&gt;rx_frag_size) {
 		/* GFP_KERNEL allocations are used only during initialization */
<span class="p_del">-		if (unlikely(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+		if (unlikely(gfpflags_allow_blocking(gfp_mask)))</span>
 			return (void *)__get_free_page(gfp_mask);
 
 		return netdev_alloc_frag(fp-&gt;rx_frag_size);
<span class="p_header">diff --git a/drivers/staging/android/ion/ion_system_heap.c b/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="p_header">index 7a7a9a047230..d4cdbf28dbb6 100644</span>
<span class="p_header">--- a/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="p_header">+++ b/drivers/staging/android/ion/ion_system_heap.c</span>
<span class="p_chunk">@@ -27,7 +27,7 @@</span> <span class="p_context"></span>
 #include &quot;ion_priv.h&quot;
 
 static gfp_t high_order_gfp_flags = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN |
<span class="p_del">-				     __GFP_NORETRY) &amp; ~__GFP_WAIT;</span>
<span class="p_add">+				     __GFP_NORETRY) &amp; ~__GFP_DIRECT_RECLAIM;</span>
 static gfp_t low_order_gfp_flags  = (GFP_HIGHUSER | __GFP_ZERO | __GFP_NOWARN);
 static const unsigned int orders[] = {8, 4, 0};
 static const int num_orders = ARRAY_SIZE(orders);
<span class="p_header">diff --git a/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h b/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="p_header">index 9544860e3292..78bde2c11b50 100644</span>
<span class="p_header">--- a/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="p_header">+++ b/drivers/staging/lustre/include/linux/libcfs/libcfs_private.h</span>
<span class="p_chunk">@@ -95,7 +95,7 @@</span> <span class="p_context"> do {								    \</span>
 do {									    \
 	LASSERT(!in_interrupt() ||					    \
 		((size) &lt;= LIBCFS_VMALLOC_SIZE &amp;&amp;			    \
<span class="p_del">-		 ((mask) &amp; __GFP_WAIT) == 0));				    \</span>
<span class="p_add">+		 !gfpflags_allow_blocking(mask)));			    \</span>
 } while (0)
 
 #define LIBCFS_ALLOC_POST(ptr, size)					    \
<span class="p_header">diff --git a/drivers/usb/host/u132-hcd.c b/drivers/usb/host/u132-hcd.c</span>
<span class="p_header">index a67bd5090330..67b3b9d9dfd1 100644</span>
<span class="p_header">--- a/drivers/usb/host/u132-hcd.c</span>
<span class="p_header">+++ b/drivers/usb/host/u132-hcd.c</span>
<span class="p_chunk">@@ -2244,7 +2244,7 @@</span> <span class="p_context"> static int u132_urb_enqueue(struct usb_hcd *hcd, struct urb *urb,</span>
 {
 	struct u132 *u132 = hcd_to_u132(hcd);
 	if (irqs_disabled()) {
<span class="p_del">-		if (__GFP_WAIT &amp; mem_flags) {</span>
<span class="p_add">+		if (gfpflags_allow_blocking(mem_flags)) {</span>
 			printk(KERN_ERR &quot;invalid context for function that migh&quot;
 				&quot;t sleep\n&quot;);
 			return -EINVAL;
<span class="p_header">diff --git a/drivers/video/fbdev/vermilion/vermilion.c b/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="p_header">index 6b70d7f62b2f..1c1e95a0b8fa 100644</span>
<span class="p_header">--- a/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="p_header">+++ b/drivers/video/fbdev/vermilion/vermilion.c</span>
<span class="p_chunk">@@ -99,7 +99,7 @@</span> <span class="p_context"> static int vmlfb_alloc_vram_area(struct vram_area *va, unsigned max_order,</span>
 		 * below the first 16MB.
 		 */
 
<span class="p_del">-		flags = __GFP_DMA | __GFP_HIGH;</span>
<span class="p_add">+		flags = __GFP_DMA | __GFP_HIGH | __GFP_KSWAPD_RECLAIM;</span>
 		va-&gt;logical =
 			 __get_free_pages(flags, --max_order);
 	} while (va-&gt;logical == 0 &amp;&amp; max_order &gt; min_order);
<span class="p_header">diff --git a/fs/btrfs/disk-io.c b/fs/btrfs/disk-io.c</span>
<span class="p_header">index 0d98aee34fee..5632ba60c8f5 100644</span>
<span class="p_header">--- a/fs/btrfs/disk-io.c</span>
<span class="p_header">+++ b/fs/btrfs/disk-io.c</span>
<span class="p_chunk">@@ -2572,7 +2572,7 @@</span> <span class="p_context"> int open_ctree(struct super_block *sb,</span>
 	fs_info-&gt;commit_interval = BTRFS_DEFAULT_COMMIT_INTERVAL;
 	fs_info-&gt;avg_delayed_ref_runtime = NSEC_PER_SEC &gt;&gt; 6; /* div by 64 */
 	/* readahead state */
<span class="p_del">-	INIT_RADIX_TREE(&amp;fs_info-&gt;reada_tree, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="p_add">+	INIT_RADIX_TREE(&amp;fs_info-&gt;reada_tree, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
 	spin_lock_init(&amp;fs_info-&gt;reada_lock);
 
 	fs_info-&gt;thread_pool_size = min_t(unsigned long,
<span class="p_header">diff --git a/fs/btrfs/extent_io.c b/fs/btrfs/extent_io.c</span>
<span class="p_header">index f1018cfbfefa..7956b310c194 100644</span>
<span class="p_header">--- a/fs/btrfs/extent_io.c</span>
<span class="p_header">+++ b/fs/btrfs/extent_io.c</span>
<span class="p_chunk">@@ -594,7 +594,7 @@</span> <span class="p_context"> int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 	if (bits &amp; (EXTENT_IOBITS | EXTENT_BOUNDARY))
 		clear = 1;
 again:
<span class="p_del">-	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
 		/*
 		 * Don&#39;t care for allocation failure here because we might end
 		 * up not needing the pre-allocated extent state at all, which
<span class="p_chunk">@@ -718,7 +718,7 @@</span> <span class="p_context"> int clear_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 	if (start &gt; end)
 		goto out;
 	spin_unlock(&amp;tree-&gt;lock);
<span class="p_del">-	if (mask &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(mask))</span>
 		cond_resched();
 	goto again;
 }
<span class="p_chunk">@@ -850,7 +850,7 @@</span> <span class="p_context"> __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 
 	bits |= EXTENT_FIRST_DELALLOC;
 again:
<span class="p_del">-	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
 		prealloc = alloc_extent_state(mask);
 		BUG_ON(!prealloc);
 	}
<span class="p_chunk">@@ -1028,7 +1028,7 @@</span> <span class="p_context"> __set_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 	if (start &gt; end)
 		goto out;
 	spin_unlock(&amp;tree-&gt;lock);
<span class="p_del">-	if (mask &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(mask))</span>
 		cond_resched();
 	goto again;
 }
<span class="p_chunk">@@ -1076,7 +1076,7 @@</span> <span class="p_context"> int convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 	btrfs_debug_check_extent_io_range(tree, start, end);
 
 again:
<span class="p_del">-	if (!prealloc &amp;&amp; (mask &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	if (!prealloc &amp;&amp; gfpflags_allow_blocking(mask)) {</span>
 		/*
 		 * Best effort, don&#39;t worry if extent state allocation fails
 		 * here for the first iteration. We might have a cached state
<span class="p_chunk">@@ -1253,7 +1253,7 @@</span> <span class="p_context"> int convert_extent_bit(struct extent_io_tree *tree, u64 start, u64 end,</span>
 	if (start &gt; end)
 		goto out;
 	spin_unlock(&amp;tree-&gt;lock);
<span class="p_del">-	if (mask &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(mask))</span>
 		cond_resched();
 	first_iteration = false;
 	goto again;
<span class="p_chunk">@@ -4267,7 +4267,7 @@</span> <span class="p_context"> int try_release_extent_mapping(struct extent_map_tree *map,</span>
 	u64 start = page_offset(page);
 	u64 end = start + PAGE_CACHE_SIZE - 1;
 
<span class="p_del">-	if ((mask &amp; __GFP_WAIT) &amp;&amp;</span>
<span class="p_add">+	if (gfpflags_allow_blocking(mask) &amp;&amp;</span>
 	    page-&gt;mapping-&gt;host-&gt;i_size &gt; 16 * 1024 * 1024) {
 		u64 len;
 		while (start &lt;= end) {
<span class="p_header">diff --git a/fs/btrfs/volumes.c b/fs/btrfs/volumes.c</span>
<span class="p_header">index 6fc735869c18..e023919b4470 100644</span>
<span class="p_header">--- a/fs/btrfs/volumes.c</span>
<span class="p_header">+++ b/fs/btrfs/volumes.c</span>
<span class="p_chunk">@@ -156,8 +156,8 @@</span> <span class="p_context"> static struct btrfs_device *__alloc_device(void)</span>
 	spin_lock_init(&amp;dev-&gt;reada_lock);
 	atomic_set(&amp;dev-&gt;reada_in_flight, 0);
 	atomic_set(&amp;dev-&gt;dev_stats_ccnt, 0);
<span class="p_del">-	INIT_RADIX_TREE(&amp;dev-&gt;reada_zones, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="p_del">-	INIT_RADIX_TREE(&amp;dev-&gt;reada_extents, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="p_add">+	INIT_RADIX_TREE(&amp;dev-&gt;reada_zones, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
<span class="p_add">+	INIT_RADIX_TREE(&amp;dev-&gt;reada_extents, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
 
 	return dev;
 }
<span class="p_header">diff --git a/fs/ext4/super.c b/fs/ext4/super.c</span>
<span class="p_header">index a63c7b0a10cf..49f6c78ee3af 100644</span>
<span class="p_header">--- a/fs/ext4/super.c</span>
<span class="p_header">+++ b/fs/ext4/super.c</span>
<span class="p_chunk">@@ -1058,7 +1058,7 @@</span> <span class="p_context"> static int bdev_try_to_free_page(struct super_block *sb, struct page *page,</span>
 		return 0;
 	if (journal)
 		return jbd2_journal_try_to_free_buffers(journal, page,
<span class="p_del">-							wait &amp; ~__GFP_WAIT);</span>
<span class="p_add">+						wait &amp; ~__GFP_DIRECT_RECLAIM);</span>
 	return try_to_free_buffers(page);
 }
 
<span class="p_header">diff --git a/fs/fscache/cookie.c b/fs/fscache/cookie.c</span>
<span class="p_header">index d403c69bee08..4304072161aa 100644</span>
<span class="p_header">--- a/fs/fscache/cookie.c</span>
<span class="p_header">+++ b/fs/fscache/cookie.c</span>
<span class="p_chunk">@@ -111,7 +111,7 @@</span> <span class="p_context"> struct fscache_cookie *__fscache_acquire_cookie(</span>
 
 	/* radix tree insertion won&#39;t use the preallocation pool unless it&#39;s
 	 * told it may not wait */
<span class="p_del">-	INIT_RADIX_TREE(&amp;cookie-&gt;stores, GFP_NOFS &amp; ~__GFP_WAIT);</span>
<span class="p_add">+	INIT_RADIX_TREE(&amp;cookie-&gt;stores, GFP_NOFS &amp; ~__GFP_DIRECT_RECLAIM);</span>
 
 	switch (cookie-&gt;def-&gt;type) {
 	case FSCACHE_COOKIE_TYPE_INDEX:
<span class="p_header">diff --git a/fs/fscache/page.c b/fs/fscache/page.c</span>
<span class="p_header">index 483bbc613bf0..79483b3d8c6f 100644</span>
<span class="p_header">--- a/fs/fscache/page.c</span>
<span class="p_header">+++ b/fs/fscache/page.c</span>
<span class="p_chunk">@@ -58,7 +58,7 @@</span> <span class="p_context"> bool release_page_wait_timeout(struct fscache_cookie *cookie, struct page *page)</span>
 
 /*
  * decide whether a page can be released, possibly by cancelling a store to it
<span class="p_del">- * - we&#39;re allowed to sleep if __GFP_WAIT is flagged</span>
<span class="p_add">+ * - we&#39;re allowed to sleep if __GFP_DIRECT_RECLAIM is flagged</span>
  */
 bool __fscache_maybe_release_page(struct fscache_cookie *cookie,
 				  struct page *page,
<span class="p_chunk">@@ -122,7 +122,7 @@</span> <span class="p_context"> bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
 	 * allocator as the work threads writing to the cache may all end up
 	 * sleeping on memory allocation, so we may need to impose a timeout
 	 * too. */
<span class="p_del">-	if (!(gfp &amp; __GFP_WAIT) || !(gfp &amp; __GFP_FS)) {</span>
<span class="p_add">+	if (!(gfp &amp; __GFP_DIRECT_RECLAIM) || !(gfp &amp; __GFP_FS)) {</span>
 		fscache_stat(&amp;fscache_n_store_vmscan_busy);
 		return false;
 	}
<span class="p_chunk">@@ -132,7 +132,7 @@</span> <span class="p_context"> bool __fscache_maybe_release_page(struct fscache_cookie *cookie,</span>
 		_debug(&quot;fscache writeout timeout page: %p{%lx}&quot;,
 			page, page-&gt;index);
 
<span class="p_del">-	gfp &amp;= ~__GFP_WAIT;</span>
<span class="p_add">+	gfp &amp;= ~__GFP_DIRECT_RECLAIM;</span>
 	goto try_again;
 }
 EXPORT_SYMBOL(__fscache_maybe_release_page);
<span class="p_header">diff --git a/fs/jbd2/transaction.c b/fs/jbd2/transaction.c</span>
<span class="p_header">index 6b8338ec2464..89463eee6791 100644</span>
<span class="p_header">--- a/fs/jbd2/transaction.c</span>
<span class="p_header">+++ b/fs/jbd2/transaction.c</span>
<span class="p_chunk">@@ -1937,8 +1937,8 @@</span> <span class="p_context"> __journal_try_to_free_buffer(journal_t *journal, struct buffer_head *bh)</span>
  * @journal: journal for operation
  * @page: to try and free
  * @gfp_mask: we use the mask to detect how hard should we try to release
<span class="p_del">- * buffers. If __GFP_WAIT and __GFP_FS is set, we wait for commit code to</span>
<span class="p_del">- * release the buffers.</span>
<span class="p_add">+ * buffers. If __GFP_DIRECT_RECLAIM and __GFP_FS is set, we wait for commit</span>
<span class="p_add">+ * code to release the buffers.</span>
  *
  *
  * For all the buffers on this page,
<span class="p_header">diff --git a/fs/nfs/file.c b/fs/nfs/file.c</span>
<span class="p_header">index c0f9b1ed12b9..17d3417c8a74 100644</span>
<span class="p_header">--- a/fs/nfs/file.c</span>
<span class="p_header">+++ b/fs/nfs/file.c</span>
<span class="p_chunk">@@ -473,8 +473,8 @@</span> <span class="p_context"> static int nfs_release_page(struct page *page, gfp_t gfp)</span>
 	dfprintk(PAGECACHE, &quot;NFS: release_page(%p)\n&quot;, page);
 
 	/* Always try to initiate a &#39;commit&#39; if relevant, but only
<span class="p_del">-	 * wait for it if __GFP_WAIT is set.  Even then, only wait 1</span>
<span class="p_del">-	 * second and only if the &#39;bdi&#39; is not congested.</span>
<span class="p_add">+	 * wait for it if the caller allows blocking.  Even then,</span>
<span class="p_add">+	 * only wait 1 second and only if the &#39;bdi&#39; is not congested.</span>
 	 * Waiting indefinitely can cause deadlocks when the NFS
 	 * server is on this machine, when a new TCP connection is
 	 * needed and in other rare cases.  There is no particular
<span class="p_chunk">@@ -484,7 +484,7 @@</span> <span class="p_context"> static int nfs_release_page(struct page *page, gfp_t gfp)</span>
 	if (mapping) {
 		struct nfs_server *nfss = NFS_SERVER(mapping-&gt;host);
 		nfs_commit_inode(mapping-&gt;host, 0);
<span class="p_del">-		if ((gfp &amp; __GFP_WAIT) &amp;&amp;</span>
<span class="p_add">+		if (gfpflags_allow_blocking(gfp) &amp;&amp;</span>
 		    !bdi_write_congested(&amp;nfss-&gt;backing_dev_info)) {
 			wait_on_page_bit_killable_timeout(page, PG_private,
 							  HZ);
<span class="p_header">diff --git a/fs/xfs/xfs_qm.c b/fs/xfs/xfs_qm.c</span>
<span class="p_header">index eac9549efd52..587174fd4f2c 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_qm.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_qm.c</span>
<span class="p_chunk">@@ -525,7 +525,7 @@</span> <span class="p_context"> xfs_qm_shrink_scan(</span>
 	unsigned long		freed;
 	int			error;
 
<span class="p_del">-	if ((sc-&gt;gfp_mask &amp; (__GFP_FS|__GFP_WAIT)) != (__GFP_FS|__GFP_WAIT))</span>
<span class="p_add">+	if ((sc-&gt;gfp_mask &amp; (__GFP_FS|__GFP_DIRECT_RECLAIM)) != (__GFP_FS|__GFP_DIRECT_RECLAIM))</span>
 		return 0;
 
 	INIT_LIST_HEAD(&amp;isol.buffers);
<span class="p_header">diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="p_header">index 440fca3e7e5d..b56e811b6f7c 100644</span>
<span class="p_header">--- a/include/linux/gfp.h</span>
<span class="p_header">+++ b/include/linux/gfp.h</span>
<span class="p_chunk">@@ -29,12 +29,13 @@</span> <span class="p_context"> struct vm_area_struct;</span>
 #define ___GFP_NOMEMALLOC	0x10000u
 #define ___GFP_HARDWALL		0x20000u
 #define ___GFP_THISNODE		0x40000u
<span class="p_del">-#define ___GFP_WAIT		0x80000u</span>
<span class="p_add">+#define ___GFP_ATOMIC		0x80000u</span>
 #define ___GFP_NOACCOUNT	0x100000u
 #define ___GFP_NOTRACK		0x200000u
<span class="p_del">-#define ___GFP_NO_KSWAPD	0x400000u</span>
<span class="p_add">+#define ___GFP_DIRECT_RECLAIM	0x400000u</span>
 #define ___GFP_OTHER_NODE	0x800000u
 #define ___GFP_WRITE		0x1000000u
<span class="p_add">+#define ___GFP_KSWAPD_RECLAIM	0x2000000u</span>
 /* If the above are modified, __GFP_BITS_SHIFT may need updating */
 
 /*
<span class="p_chunk">@@ -71,7 +72,7 @@</span> <span class="p_context"> struct vm_area_struct;</span>
  * __GFP_MOVABLE: Flag that this page will be movable by the page migration
  * mechanism or reclaimed
  */
<span class="p_del">-#define __GFP_WAIT	((__force gfp_t)___GFP_WAIT)	/* Can wait and reschedule? */</span>
<span class="p_add">+#define __GFP_ATOMIC	((__force gfp_t)___GFP_ATOMIC)  /* Caller cannot wait or reschedule */</span>
 #define __GFP_HIGH	((__force gfp_t)___GFP_HIGH)	/* Should access emergency pools? */
 #define __GFP_IO	((__force gfp_t)___GFP_IO)	/* Can start physical IO? */
 #define __GFP_FS	((__force gfp_t)___GFP_FS)	/* Can call down to low-level FS? */
<span class="p_chunk">@@ -94,23 +95,37 @@</span> <span class="p_context"> struct vm_area_struct;</span>
 #define __GFP_NOACCOUNT	((__force gfp_t)___GFP_NOACCOUNT) /* Don&#39;t account to kmemcg */
 #define __GFP_NOTRACK	((__force gfp_t)___GFP_NOTRACK)  /* Don&#39;t track with kmemcheck */
 
<span class="p_del">-#define __GFP_NO_KSWAPD	((__force gfp_t)___GFP_NO_KSWAPD)</span>
 #define __GFP_OTHER_NODE ((__force gfp_t)___GFP_OTHER_NODE) /* On behalf of other node */
 #define __GFP_WRITE	((__force gfp_t)___GFP_WRITE)	/* Allocator intends to dirty page */
 
 /*
<span class="p_add">+ * A caller that is willing to wait may enter direct reclaim and will</span>
<span class="p_add">+ * wake kswapd to reclaim pages in the background until the high</span>
<span class="p_add">+ * watermark is met. A caller may wish to clear __GFP_DIRECT_RECLAIM to</span>
<span class="p_add">+ * avoid unnecessary delays when a fallback option is available but</span>
<span class="p_add">+ * still allow kswapd to reclaim in the background. The kswapd flag</span>
<span class="p_add">+ * can be cleared when the reclaiming of pages would cause unnecessary</span>
<span class="p_add">+ * disruption.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __GFP_WAIT ((__force gfp_t)(___GFP_DIRECT_RECLAIM|___GFP_KSWAPD_RECLAIM))</span>
<span class="p_add">+#define __GFP_DIRECT_RECLAIM	((__force gfp_t)___GFP_DIRECT_RECLAIM) /* Caller can reclaim */</span>
<span class="p_add">+#define __GFP_KSWAPD_RECLAIM	((__force gfp_t)___GFP_KSWAPD_RECLAIM) /* kswapd can wake */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * This may seem redundant, but it&#39;s a way of annotating false positives vs.
  * allocations that simply cannot be supported (e.g. page tables).
  */
 #define __GFP_NOTRACK_FALSE_POSITIVE (__GFP_NOTRACK)
 
<span class="p_del">-#define __GFP_BITS_SHIFT 25	/* Room for N __GFP_FOO bits */</span>
<span class="p_add">+#define __GFP_BITS_SHIFT 26	/* Room for N __GFP_FOO bits */</span>
 #define __GFP_BITS_MASK ((__force gfp_t)((1 &lt;&lt; __GFP_BITS_SHIFT) - 1))
 
<span class="p_del">-/* This equals 0, but use constants in case they ever change */</span>
<span class="p_del">-#define GFP_NOWAIT	(GFP_ATOMIC &amp; ~__GFP_HIGH)</span>
<span class="p_del">-/* GFP_ATOMIC means both !wait (__GFP_WAIT not set) and use emergency pool */</span>
<span class="p_del">-#define GFP_ATOMIC	(__GFP_HIGH)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * GFP_ATOMIC callers can not sleep, need the allocation to succeed.</span>
<span class="p_add">+ * A lower watermark is applied to allow access to &quot;atomic reserves&quot;</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define GFP_ATOMIC	(__GFP_HIGH|__GFP_ATOMIC|__GFP_KSWAPD_RECLAIM)</span>
<span class="p_add">+#define GFP_NOWAIT	(__GFP_KSWAPD_RECLAIM)</span>
 #define GFP_NOIO	(__GFP_WAIT)
 #define GFP_NOFS	(__GFP_WAIT | __GFP_IO)
 #define GFP_KERNEL	(__GFP_WAIT | __GFP_IO | __GFP_FS)
<span class="p_chunk">@@ -119,10 +134,10 @@</span> <span class="p_context"> struct vm_area_struct;</span>
 #define GFP_USER	(__GFP_WAIT | __GFP_IO | __GFP_FS | __GFP_HARDWALL)
 #define GFP_HIGHUSER	(GFP_USER | __GFP_HIGHMEM)
 #define GFP_HIGHUSER_MOVABLE	(GFP_HIGHUSER | __GFP_MOVABLE)
<span class="p_del">-#define GFP_IOFS	(__GFP_IO | __GFP_FS)</span>
<span class="p_del">-#define GFP_TRANSHUGE	(GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="p_del">-			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN | \</span>
<span class="p_del">-			 __GFP_NO_KSWAPD)</span>
<span class="p_add">+#define GFP_IOFS	(__GFP_IO | __GFP_FS | __GFP_KSWAPD_RECLAIM)</span>
<span class="p_add">+#define GFP_TRANSHUGE	((GFP_HIGHUSER_MOVABLE | __GFP_COMP | \</span>
<span class="p_add">+			 __GFP_NOMEMALLOC | __GFP_NORETRY | __GFP_NOWARN) &amp; \</span>
<span class="p_add">+			 ~__GFP_KSWAPD_RECLAIM)</span>
 
 /* This mask makes up all the page movable related flags */
 #define GFP_MOVABLE_MASK (__GFP_RECLAIMABLE|__GFP_MOVABLE)
<span class="p_chunk">@@ -164,6 +179,11 @@</span> <span class="p_context"> static inline int gfpflags_to_migratetype(const gfp_t gfp_flags)</span>
 	return (gfp_flags &amp; GFP_MOVABLE_MASK) &gt;&gt; GFP_MOVABLE_SHIFT;
 }
 
<span class="p_add">+static inline bool gfpflags_allow_blocking(const gfp_t gfp_flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return gfp_flags &amp; __GFP_DIRECT_RECLAIM;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_HIGHMEM
 #define OPT_ZONE_HIGHMEM ZONE_HIGHMEM
 #else
<span class="p_header">diff --git a/include/linux/skbuff.h b/include/linux/skbuff.h</span>
<span class="p_header">index 2738d355cdf9..6f1f5a813554 100644</span>
<span class="p_header">--- a/include/linux/skbuff.h</span>
<span class="p_header">+++ b/include/linux/skbuff.h</span>
<span class="p_chunk">@@ -1215,7 +1215,7 @@</span> <span class="p_context"> static inline int skb_cloned(const struct sk_buff *skb)</span>
 
 static inline int skb_unclone(struct sk_buff *skb, gfp_t pri)
 {
<span class="p_del">-	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(pri));</span>
 
 	if (skb_cloned(skb))
 		return pskb_expand_head(skb, 0, 0, pri);
<span class="p_chunk">@@ -1299,7 +1299,7 @@</span> <span class="p_context"> static inline int skb_shared(const struct sk_buff *skb)</span>
  */
 static inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)
 {
<span class="p_del">-	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(pri));</span>
 	if (skb_shared(skb)) {
 		struct sk_buff *nskb = skb_clone(skb, pri);
 
<span class="p_chunk">@@ -1335,7 +1335,7 @@</span> <span class="p_context"> static inline struct sk_buff *skb_share_check(struct sk_buff *skb, gfp_t pri)</span>
 static inline struct sk_buff *skb_unshare(struct sk_buff *skb,
 					  gfp_t pri)
 {
<span class="p_del">-	might_sleep_if(pri &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(pri));</span>
 	if (skb_cloned(skb)) {
 		struct sk_buff *nskb = skb_copy(skb, pri);
 
<span class="p_header">diff --git a/include/net/sock.h b/include/net/sock.h</span>
<span class="p_header">index 7aa78440559a..e822cdf8b855 100644</span>
<span class="p_header">--- a/include/net/sock.h</span>
<span class="p_header">+++ b/include/net/sock.h</span>
<span class="p_chunk">@@ -2020,7 +2020,7 @@</span> <span class="p_context"> struct sk_buff *sk_stream_alloc_skb(struct sock *sk, int size, gfp_t gfp,</span>
  */
 static inline struct page_frag *sk_page_frag(struct sock *sk)
 {
<span class="p_del">-	if (sk-&gt;sk_allocation &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(sk-&gt;sk_allocation))</span>
 		return &amp;current-&gt;task_frag;
 
 	return &amp;sk-&gt;sk_frag;
<span class="p_header">diff --git a/include/trace/events/gfpflags.h b/include/trace/events/gfpflags.h</span>
<span class="p_header">index d6fd8e5b14b7..dde6bf092c8a 100644</span>
<span class="p_header">--- a/include/trace/events/gfpflags.h</span>
<span class="p_header">+++ b/include/trace/events/gfpflags.h</span>
<span class="p_chunk">@@ -20,7 +20,7 @@</span> <span class="p_context"></span>
 	{(unsigned long)GFP_ATOMIC,		&quot;GFP_ATOMIC&quot;},		\
 	{(unsigned long)GFP_NOIO,		&quot;GFP_NOIO&quot;},		\
 	{(unsigned long)__GFP_HIGH,		&quot;GFP_HIGH&quot;},		\
<span class="p_del">-	{(unsigned long)__GFP_WAIT,		&quot;GFP_WAIT&quot;},		\</span>
<span class="p_add">+	{(unsigned long)__GFP_ATOMIC,		&quot;GFP_ATOMIC&quot;},		\</span>
 	{(unsigned long)__GFP_IO,		&quot;GFP_IO&quot;},		\
 	{(unsigned long)__GFP_COLD,		&quot;GFP_COLD&quot;},		\
 	{(unsigned long)__GFP_NOWARN,		&quot;GFP_NOWARN&quot;},		\
<span class="p_chunk">@@ -36,7 +36,8 @@</span> <span class="p_context"></span>
 	{(unsigned long)__GFP_RECLAIMABLE,	&quot;GFP_RECLAIMABLE&quot;},	\
 	{(unsigned long)__GFP_MOVABLE,		&quot;GFP_MOVABLE&quot;},		\
 	{(unsigned long)__GFP_NOTRACK,		&quot;GFP_NOTRACK&quot;},		\
<span class="p_del">-	{(unsigned long)__GFP_NO_KSWAPD,	&quot;GFP_NO_KSWAPD&quot;},	\</span>
<span class="p_add">+	{(unsigned long)__GFP_DIRECT_RECLAIM,	&quot;GFP_DIRECT_RECLAIM&quot;},	\</span>
<span class="p_add">+	{(unsigned long)__GFP_KSWAPD_RECLAIM,	&quot;GFP_KSWAPD_RECLAIM&quot;},	\</span>
 	{(unsigned long)__GFP_OTHER_NODE,	&quot;GFP_OTHER_NODE&quot;}	\
 	) : &quot;GFP_NOWAIT&quot;
 
<span class="p_header">diff --git a/kernel/audit.c b/kernel/audit.c</span>
<span class="p_header">index 662c007635fb..6ae6e2b62e3e 100644</span>
<span class="p_header">--- a/kernel/audit.c</span>
<span class="p_header">+++ b/kernel/audit.c</span>
<span class="p_chunk">@@ -1357,16 +1357,16 @@</span> <span class="p_context"> struct audit_buffer *audit_log_start(struct audit_context *ctx, gfp_t gfp_mask,</span>
 	if (unlikely(audit_filter_type(type)))
 		return NULL;
 
<span class="p_del">-	if (gfp_mask &amp; __GFP_WAIT) {</span>
<span class="p_add">+	if (gfp_mask &amp; __GFP_DIRECT_RECLAIM) {</span>
 		if (audit_pid &amp;&amp; audit_pid == current-&gt;pid)
<span class="p_del">-			gfp_mask &amp;= ~__GFP_WAIT;</span>
<span class="p_add">+			gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;</span>
 		else
 			reserve = 0;
 	}
 
 	while (audit_backlog_limit
 	       &amp;&amp; skb_queue_len(&amp;audit_skb_queue) &gt; audit_backlog_limit + reserve) {
<span class="p_del">-		if (gfp_mask &amp; __GFP_WAIT &amp;&amp; audit_backlog_wait_time) {</span>
<span class="p_add">+		if (gfp_mask &amp; __GFP_DIRECT_RECLAIM &amp;&amp; audit_backlog_wait_time) {</span>
 			long sleep_time;
 
 			sleep_time = timeout_start + audit_backlog_wait_time - jiffies;
<span class="p_header">diff --git a/kernel/cgroup.c b/kernel/cgroup.c</span>
<span class="p_header">index 2cf0f79f1fc9..e843dffa7b87 100644</span>
<span class="p_header">--- a/kernel/cgroup.c</span>
<span class="p_header">+++ b/kernel/cgroup.c</span>
<span class="p_chunk">@@ -211,7 +211,7 @@</span> <span class="p_context"> static int cgroup_idr_alloc(struct idr *idr, void *ptr, int start, int end,</span>
 
 	idr_preload(gfp_mask);
 	spin_lock_bh(&amp;cgroup_idr_lock);
<span class="p_del">-	ret = idr_alloc(idr, ptr, start, end, gfp_mask &amp; ~__GFP_WAIT);</span>
<span class="p_add">+	ret = idr_alloc(idr, ptr, start, end, gfp_mask &amp; ~__GFP_DIRECT_RECLAIM);</span>
 	spin_unlock_bh(&amp;cgroup_idr_lock);
 	idr_preload_end();
 	return ret;
<span class="p_header">diff --git a/kernel/locking/lockdep.c b/kernel/locking/lockdep.c</span>
<span class="p_header">index 8acfbf773e06..9aa39f20f593 100644</span>
<span class="p_header">--- a/kernel/locking/lockdep.c</span>
<span class="p_header">+++ b/kernel/locking/lockdep.c</span>
<span class="p_chunk">@@ -2738,7 +2738,7 @@</span> <span class="p_context"> static void __lockdep_trace_alloc(gfp_t gfp_mask, unsigned long flags)</span>
 		return;
 
 	/* no reclaim without waiting on it */
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (!(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
 		return;
 
 	/* this guy won&#39;t enter reclaim */
<span class="p_header">diff --git a/kernel/power/snapshot.c b/kernel/power/snapshot.c</span>
<span class="p_header">index 5235dd4e1e2f..3a970604308f 100644</span>
<span class="p_header">--- a/kernel/power/snapshot.c</span>
<span class="p_header">+++ b/kernel/power/snapshot.c</span>
<span class="p_chunk">@@ -1779,7 +1779,7 @@</span> <span class="p_context"> alloc_highmem_pages(struct memory_bitmap *bm, unsigned int nr_highmem)</span>
 	while (to_alloc-- &gt; 0) {
 		struct page *page;
 
<span class="p_del">-		page = alloc_image_page(__GFP_HIGHMEM);</span>
<span class="p_add">+		page = alloc_image_page(__GFP_HIGHMEM|__GFP_KSWAPD_RECLAIM);</span>
 		memory_bm_set_bit(bm, page_to_pfn(page));
 	}
 	return nr_highmem;
<span class="p_header">diff --git a/kernel/smp.c b/kernel/smp.c</span>
<span class="p_header">index 07854477c164..d903c02223af 100644</span>
<span class="p_header">--- a/kernel/smp.c</span>
<span class="p_header">+++ b/kernel/smp.c</span>
<span class="p_chunk">@@ -669,7 +669,7 @@</span> <span class="p_context"> void on_each_cpu_cond(bool (*cond_func)(int cpu, void *info),</span>
 	cpumask_var_t cpus;
 	int cpu, ret;
 
<span class="p_del">-	might_sleep_if(gfp_flags &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp_flags));</span>
 
 	if (likely(zalloc_cpumask_var(&amp;cpus, (gfp_flags|__GFP_NOWARN)))) {
 		preempt_disable();
<span class="p_header">diff --git a/lib/idr.c b/lib/idr.c</span>
<span class="p_header">index 5335c43adf46..6098336df267 100644</span>
<span class="p_header">--- a/lib/idr.c</span>
<span class="p_header">+++ b/lib/idr.c</span>
<span class="p_chunk">@@ -399,7 +399,7 @@</span> <span class="p_context"> void idr_preload(gfp_t gfp_mask)</span>
 	 * allocation guarantee.  Disallow usage from those contexts.
 	 */
 	WARN_ON_ONCE(in_interrupt());
<span class="p_del">-	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
 
 	preempt_disable();
 
<span class="p_chunk">@@ -453,7 +453,7 @@</span> <span class="p_context"> int idr_alloc(struct idr *idr, void *ptr, int start, int end, gfp_t gfp_mask)</span>
 	struct idr_layer *pa[MAX_IDR_LEVEL + 1];
 	int id;
 
<span class="p_del">-	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp_mask));</span>
 
 	/* sanity checks */
 	if (WARN_ON_ONCE(start &lt; 0))
<span class="p_header">diff --git a/lib/radix-tree.c b/lib/radix-tree.c</span>
<span class="p_header">index f9ebe1c82060..fcf5d98574ce 100644</span>
<span class="p_header">--- a/lib/radix-tree.c</span>
<span class="p_header">+++ b/lib/radix-tree.c</span>
<span class="p_chunk">@@ -188,7 +188,7 @@</span> <span class="p_context"> radix_tree_node_alloc(struct radix_tree_root *root)</span>
 	 * preloading in the interrupt anyway as all the allocations have to
 	 * be atomic. So just do normal allocation when in interrupt.
 	 */
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT) &amp;&amp; !in_interrupt()) {</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp_mask) &amp;&amp; !in_interrupt()) {</span>
 		struct radix_tree_preload *rtp;
 
 		/*
<span class="p_chunk">@@ -249,7 +249,7 @@</span> <span class="p_context"> radix_tree_node_free(struct radix_tree_node *node)</span>
  * with preemption not disabled.
  *
  * To make use of this facility, the radix tree must be initialised without
<span class="p_del">- * __GFP_WAIT being passed to INIT_RADIX_TREE().</span>
<span class="p_add">+ * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().</span>
  */
 static int __radix_tree_preload(gfp_t gfp_mask)
 {
<span class="p_chunk">@@ -286,12 +286,12 @@</span> <span class="p_context"> static int __radix_tree_preload(gfp_t gfp_mask)</span>
  * with preemption not disabled.
  *
  * To make use of this facility, the radix tree must be initialised without
<span class="p_del">- * __GFP_WAIT being passed to INIT_RADIX_TREE().</span>
<span class="p_add">+ * __GFP_DIRECT_RECLAIM being passed to INIT_RADIX_TREE().</span>
  */
 int radix_tree_preload(gfp_t gfp_mask)
 {
 	/* Warn on non-sensical use... */
<span class="p_del">-	WARN_ON_ONCE(!(gfp_mask &amp; __GFP_WAIT));</span>
<span class="p_add">+	WARN_ON_ONCE(!gfpflags_allow_blocking(gfp_mask));</span>
 	return __radix_tree_preload(gfp_mask);
 }
 EXPORT_SYMBOL(radix_tree_preload);
<span class="p_chunk">@@ -303,7 +303,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(radix_tree_preload);</span>
  */
 int radix_tree_maybe_preload(gfp_t gfp_mask)
 {
<span class="p_del">-	if (gfp_mask &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(gfp_mask))</span>
 		return __radix_tree_preload(gfp_mask);
 	/* Preloading doesn&#39;t help anything with this gfp mask, skip it */
 	preempt_disable();
<span class="p_header">diff --git a/mm/backing-dev.c b/mm/backing-dev.c</span>
<span class="p_header">index 2df8ddcb0ca0..e7781eb35fd1 100644</span>
<span class="p_header">--- a/mm/backing-dev.c</span>
<span class="p_header">+++ b/mm/backing-dev.c</span>
<span class="p_chunk">@@ -632,7 +632,7 @@</span> <span class="p_context"> struct bdi_writeback *wb_get_create(struct backing_dev_info *bdi,</span>
 {
 	struct bdi_writeback *wb;
 
<span class="p_del">-	might_sleep_if(gfp &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(gfp));</span>
 
 	if (!memcg_css-&gt;parent)
 		return &amp;bdi-&gt;wb;
<span class="p_header">diff --git a/mm/dmapool.c b/mm/dmapool.c</span>
<span class="p_header">index 71a8998cd03a..55b53cffd9f6 100644</span>
<span class="p_header">--- a/mm/dmapool.c</span>
<span class="p_header">+++ b/mm/dmapool.c</span>
<span class="p_chunk">@@ -326,7 +326,7 @@</span> <span class="p_context"> void *dma_pool_alloc(struct dma_pool *pool, gfp_t mem_flags,</span>
 	size_t offset;
 	void *retval;
 
<span class="p_del">-	might_sleep_if(mem_flags &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(mem_flags));</span>
 
 	spin_lock_irqsave(&amp;pool-&gt;lock, flags);
 	list_for_each_entry(page, &amp;pool-&gt;page_list, page_list) {
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 6ddaeba34e09..2c65980c0a00 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -2012,7 +2012,7 @@</span> <span class="p_context"> static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
 	if (unlikely(task_in_memcg_oom(current)))
 		goto nomem;
 
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp_mask))</span>
 		goto nomem;
 
 	mem_cgroup_events(mem_over_limit, MEMCG_MAX, 1);
<span class="p_chunk">@@ -2071,7 +2071,7 @@</span> <span class="p_context"> static int try_charge(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
 	css_get_many(&amp;memcg-&gt;css, batch);
 	if (batch &gt; nr_pages)
 		refill_stock(memcg, batch - nr_pages);
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp_mask))</span>
 		goto done;
 	/*
 	 * If the hierarchy is above the normal consumption range,
<span class="p_chunk">@@ -4396,8 +4396,8 @@</span> <span class="p_context"> static int mem_cgroup_do_precharge(unsigned long count)</span>
 {
 	int ret;
 
<span class="p_del">-	/* Try a single bulk charge without reclaim first */</span>
<span class="p_del">-	ret = try_charge(mc.to, GFP_KERNEL &amp; ~__GFP_WAIT, count);</span>
<span class="p_add">+	/* Try a single bulk charge without reclaim first, kswapd may wake */</span>
<span class="p_add">+	ret = try_charge(mc.to, GFP_KERNEL &amp; ~__GFP_DIRECT_RECLAIM, count);</span>
 	if (!ret) {
 		mc.precharge += count;
 		return ret;
<span class="p_header">diff --git a/mm/mempool.c b/mm/mempool.c</span>
<span class="p_header">index 4c533bc51d73..004d42b1dfaf 100644</span>
<span class="p_header">--- a/mm/mempool.c</span>
<span class="p_header">+++ b/mm/mempool.c</span>
<span class="p_chunk">@@ -320,13 +320,13 @@</span> <span class="p_context"> void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
 	gfp_t gfp_temp;
 
 	VM_WARN_ON_ONCE(gfp_mask &amp; __GFP_ZERO);
<span class="p_del">-	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfp_mask &amp; __GFP_DIRECT_RECLAIM);</span>
 
 	gfp_mask |= __GFP_NOMEMALLOC;	/* don&#39;t allocate emergency reserves */
 	gfp_mask |= __GFP_NORETRY;	/* don&#39;t loop in __alloc_pages */
 	gfp_mask |= __GFP_NOWARN;	/* failures are OK */
 
<span class="p_del">-	gfp_temp = gfp_mask &amp; ~(__GFP_WAIT|__GFP_IO);</span>
<span class="p_add">+	gfp_temp = gfp_mask &amp; ~(__GFP_DIRECT_RECLAIM|__GFP_IO);</span>
 
 repeat_alloc:
 
<span class="p_chunk">@@ -349,7 +349,7 @@</span> <span class="p_context"> void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
 	}
 
 	/*
<span class="p_del">-	 * We use gfp mask w/o __GFP_WAIT or IO for the first round.  If</span>
<span class="p_add">+	 * We use gfp mask w/o direct reclaim or IO for the first round.  If</span>
 	 * alloc failed with that and @pool was empty, retry immediately.
 	 */
 	if (gfp_temp != gfp_mask) {
<span class="p_chunk">@@ -358,8 +358,8 @@</span> <span class="p_context"> void * mempool_alloc(mempool_t *pool, gfp_t gfp_mask)</span>
 		goto repeat_alloc;
 	}
 
<span class="p_del">-	/* We must not sleep if !__GFP_WAIT */</span>
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT)) {</span>
<span class="p_add">+	/* We must not sleep if !__GFP_DIRECT_RECLAIM */</span>
<span class="p_add">+	if (!(gfp_mask &amp; __GFP_DIRECT_RECLAIM)) {</span>
 		spin_unlock_irqrestore(&amp;pool-&gt;lock, flags);
 		return NULL;
 	}
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index c3cb566af3e2..a1c82b65dcad 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1565,7 +1565,7 @@</span> <span class="p_context"> static struct page *alloc_misplaced_dst_page(struct page *page,</span>
 					 (GFP_HIGHUSER_MOVABLE |
 					  __GFP_THISNODE | __GFP_NOMEMALLOC |
 					  __GFP_NORETRY | __GFP_NOWARN) &amp;
<span class="p_del">-					 ~GFP_IOFS, 0);</span>
<span class="p_add">+					 ~(__GFP_IO | __GFP_FS), 0);</span>
 
 	return newpage;
 }
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 4793bddb6b2a..b32081b02c49 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -169,12 +169,12 @@</span> <span class="p_context"> void pm_restrict_gfp_mask(void)</span>
 	WARN_ON(!mutex_is_locked(&amp;pm_mutex));
 	WARN_ON(saved_gfp_mask);
 	saved_gfp_mask = gfp_allowed_mask;
<span class="p_del">-	gfp_allowed_mask &amp;= ~GFP_IOFS;</span>
<span class="p_add">+	gfp_allowed_mask &amp;= ~(__GFP_IO | __GFP_FS);</span>
 }
 
 bool pm_suspended_storage(void)
 {
<span class="p_del">-	if ((gfp_allowed_mask &amp; GFP_IOFS) == GFP_IOFS)</span>
<span class="p_add">+	if ((gfp_allowed_mask &amp; (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))</span>
 		return false;
 	return true;
 }
<span class="p_chunk">@@ -2183,7 +2183,7 @@</span> <span class="p_context"> static bool should_fail_alloc_page(gfp_t gfp_mask, unsigned int order)</span>
 		return false;
 	if (fail_page_alloc.ignore_gfp_highmem &amp;&amp; (gfp_mask &amp; __GFP_HIGHMEM))
 		return false;
<span class="p_del">-	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (fail_page_alloc.ignore_gfp_wait &amp;&amp; (gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
 		return false;
 
 	return should_fail(&amp;fail_page_alloc.attr, 1 &lt;&lt; order);
<span class="p_chunk">@@ -2685,7 +2685,7 @@</span> <span class="p_context"> void warn_alloc_failed(gfp_t gfp_mask, int order, const char *fmt, ...)</span>
 		if (test_thread_flag(TIF_MEMDIE) ||
 		    (current-&gt;flags &amp; (PF_MEMALLOC | PF_EXITING)))
 			filter &amp;= ~SHOW_MEM_FILTER_NODES;
<span class="p_del">-	if (in_interrupt() || !(gfp_mask &amp; __GFP_WAIT))</span>
<span class="p_add">+	if (in_interrupt() || !(gfp_mask &amp; __GFP_DIRECT_RECLAIM))</span>
 		filter &amp;= ~SHOW_MEM_FILTER_NODES;
 
 	if (fmt) {
<span class="p_chunk">@@ -2945,7 +2945,6 @@</span> <span class="p_context"> static inline int</span>
 gfp_to_alloc_flags(gfp_t gfp_mask)
 {
 	int alloc_flags = ALLOC_WMARK_MIN | ALLOC_CPUSET;
<span class="p_del">-	const bool atomic = !(gfp_mask &amp; (__GFP_WAIT | __GFP_NO_KSWAPD));</span>
 
 	/* __GFP_HIGH is assumed to be the same as ALLOC_HIGH to save a branch. */
 	BUILD_BUG_ON(__GFP_HIGH != (__force gfp_t) ALLOC_HIGH);
<span class="p_chunk">@@ -2954,11 +2953,11 @@</span> <span class="p_context"> gfp_to_alloc_flags(gfp_t gfp_mask)</span>
 	 * The caller may dip into page reserves a bit more if the caller
 	 * cannot run direct reclaim, or if the caller has realtime scheduling
 	 * policy or is asking for __GFP_HIGH memory.  GFP_ATOMIC requests will
<span class="p_del">-	 * set both ALLOC_HARDER (atomic == true) and ALLOC_HIGH (__GFP_HIGH).</span>
<span class="p_add">+	 * set both ALLOC_HARDER (__GFP_ATOMIC) and ALLOC_HIGH (__GFP_HIGH).</span>
 	 */
 	alloc_flags |= (__force int) (gfp_mask &amp; __GFP_HIGH);
 
<span class="p_del">-	if (atomic) {</span>
<span class="p_add">+	if (gfp_mask &amp; __GFP_ATOMIC) {</span>
 		/*
 		 * Not worth trying to allocate harder for __GFP_NOMEMALLOC even
 		 * if it can&#39;t schedule.
<span class="p_chunk">@@ -2995,11 +2994,16 @@</span> <span class="p_context"> bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)</span>
 	return !!(gfp_to_alloc_flags(gfp_mask) &amp; ALLOC_NO_WATERMARKS);
 }
 
<span class="p_add">+static inline bool is_thp_gfp_mask(gfp_t gfp_mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (gfp_mask &amp; (GFP_TRANSHUGE | __GFP_KSWAPD_RECLAIM)) == GFP_TRANSHUGE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline struct page *
 __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,
 						struct alloc_context *ac)
 {
<span class="p_del">-	const gfp_t wait = gfp_mask &amp; __GFP_WAIT;</span>
<span class="p_add">+	bool can_direct_reclaim = gfp_mask &amp; __GFP_DIRECT_RECLAIM;</span>
 	struct page *page = NULL;
 	int alloc_flags;
 	unsigned long pages_reclaimed = 0;
<span class="p_chunk">@@ -3020,15 +3024,23 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 	}
 
 	/*
<span class="p_add">+	 * We also sanity check to catch abuse of atomic reserves being used by</span>
<span class="p_add">+	 * callers that are not in atomic context.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (WARN_ON_ONCE((gfp_mask &amp; (__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)) ==</span>
<span class="p_add">+				(__GFP_ATOMIC|__GFP_DIRECT_RECLAIM)))</span>
<span class="p_add">+		gfp_mask &amp;= ~__GFP_ATOMIC;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
 	 * If this allocation cannot block and it is for a specific node, then
 	 * fail early.  There&#39;s no need to wakeup kswapd or retry for a
 	 * speculative node-specific allocation.
 	 */
<span class="p_del">-	if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; (gfp_mask &amp; __GFP_THISNODE) &amp;&amp; !wait)</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_NUMA) &amp;&amp; (gfp_mask &amp; __GFP_THISNODE) &amp;&amp; !can_direct_reclaim)</span>
 		goto nopage;
 
 retry:
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_NO_KSWAPD))</span>
<span class="p_add">+	if (gfp_mask &amp; __GFP_KSWAPD_RECLAIM)</span>
 		wake_all_kswapds(order, ac);
 
 	/*
<span class="p_chunk">@@ -3071,8 +3083,8 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 		}
 	}
 
<span class="p_del">-	/* Atomic allocations - we can&#39;t balance anything */</span>
<span class="p_del">-	if (!wait) {</span>
<span class="p_add">+	/* Caller is not willing to reclaim, we can&#39;t balance anything */</span>
<span class="p_add">+	if (!can_direct_reclaim) {</span>
 		/*
 		 * All existing users of the deprecated __GFP_NOFAIL are
 		 * blockable, so warn of any new users that actually allow this
<span class="p_chunk">@@ -3102,7 +3114,7 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 		goto got_pg;
 
 	/* Checks for THP-specific high-order allocations */
<span class="p_del">-	if ((gfp_mask &amp; GFP_TRANSHUGE) == GFP_TRANSHUGE) {</span>
<span class="p_add">+	if (is_thp_gfp_mask(gfp_mask)) {</span>
 		/*
 		 * If compaction is deferred for high-order allocations, it is
 		 * because sync compaction recently failed. If this is the case
<span class="p_chunk">@@ -3137,8 +3149,7 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 	 * fault, so use asynchronous memory compaction for THP unless it is
 	 * khugepaged trying to collapse.
 	 */
<span class="p_del">-	if ((gfp_mask &amp; GFP_TRANSHUGE) != GFP_TRANSHUGE ||</span>
<span class="p_del">-						(current-&gt;flags &amp; PF_KTHREAD))</span>
<span class="p_add">+	if (!is_thp_gfp_mask(gfp_mask) || (current-&gt;flags &amp; PF_KTHREAD))</span>
 		migration_mode = MIGRATE_SYNC_LIGHT;
 
 	/* Try direct reclaim and then allocating */
<span class="p_chunk">@@ -3209,7 +3220,7 @@</span> <span class="p_context"> __alloc_pages_nodemask(gfp_t gfp_mask, unsigned int order,</span>
 
 	lockdep_trace_alloc(gfp_mask);
 
<span class="p_del">-	might_sleep_if(gfp_mask &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfp_mask &amp; __GFP_DIRECT_RECLAIM);</span>
 
 	if (should_fail_alloc_page(gfp_mask, order))
 		return NULL;
<span class="p_header">diff --git a/mm/slab.c b/mm/slab.c</span>
<span class="p_header">index c77ebe6cc87c..3ff59926bf19 100644</span>
<span class="p_header">--- a/mm/slab.c</span>
<span class="p_header">+++ b/mm/slab.c</span>
<span class="p_chunk">@@ -1030,12 +1030,12 @@</span> <span class="p_context"> static inline int cache_free_alien(struct kmem_cache *cachep, void *objp)</span>
 }
 
 /*
<span class="p_del">- * Construct gfp mask to allocate from a specific node but do not invoke reclaim</span>
<span class="p_del">- * or warn about failures.</span>
<span class="p_add">+ * Construct gfp mask to allocate from a specific node but do not direct reclaim</span>
<span class="p_add">+ * or warn about failures. kswapd may still wake to reclaim in the background.</span>
  */
 static inline gfp_t gfp_exact_node(gfp_t flags)
 {
<span class="p_del">-	return (flags | __GFP_THISNODE | __GFP_NOWARN) &amp; ~__GFP_WAIT;</span>
<span class="p_add">+	return (flags | __GFP_THISNODE | __GFP_NOWARN) &amp; ~__GFP_DIRECT_RECLAIM;</span>
 }
 #endif
 
<span class="p_chunk">@@ -2625,7 +2625,7 @@</span> <span class="p_context"> static int cache_grow(struct kmem_cache *cachep,</span>
 
 	offset *= cachep-&gt;colour_off;
 
<span class="p_del">-	if (local_flags &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(local_flags))</span>
 		local_irq_enable();
 
 	/*
<span class="p_chunk">@@ -2655,7 +2655,7 @@</span> <span class="p_context"> static int cache_grow(struct kmem_cache *cachep,</span>
 
 	cache_init_objs(cachep, page);
 
<span class="p_del">-	if (local_flags &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(local_flags))</span>
 		local_irq_disable();
 	check_irq_off();
 	spin_lock(&amp;n-&gt;list_lock);
<span class="p_chunk">@@ -2669,7 +2669,7 @@</span> <span class="p_context"> static int cache_grow(struct kmem_cache *cachep,</span>
 opps1:
 	kmem_freepages(cachep, page);
 failed:
<span class="p_del">-	if (local_flags &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(local_flags))</span>
 		local_irq_disable();
 	return 0;
 }
<span class="p_chunk">@@ -2861,7 +2861,7 @@</span> <span class="p_context"> static void *cache_alloc_refill(struct kmem_cache *cachep, gfp_t flags,</span>
 static inline void cache_alloc_debugcheck_before(struct kmem_cache *cachep,
 						gfp_t flags)
 {
<span class="p_del">-	might_sleep_if(flags &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(flags));</span>
 #if DEBUG
 	kmem_flagcheck(cachep, flags);
 #endif
<span class="p_chunk">@@ -3049,11 +3049,11 @@</span> <span class="p_context"> static void *fallback_alloc(struct kmem_cache *cache, gfp_t flags)</span>
 		 */
 		struct page *page;
 
<span class="p_del">-		if (local_flags &amp; __GFP_WAIT)</span>
<span class="p_add">+		if (gfpflags_allow_blocking(local_flags))</span>
 			local_irq_enable();
 		kmem_flagcheck(cache, flags);
 		page = kmem_getpages(cache, local_flags, numa_mem_id());
<span class="p_del">-		if (local_flags &amp; __GFP_WAIT)</span>
<span class="p_add">+		if (gfpflags_allow_blocking(local_flags))</span>
 			local_irq_disable();
 		if (page) {
 			/*
<span class="p_header">diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="p_header">index f614b5dc396b..2cdbf5db348e 100644</span>
<span class="p_header">--- a/mm/slub.c</span>
<span class="p_header">+++ b/mm/slub.c</span>
<span class="p_chunk">@@ -1263,7 +1263,7 @@</span> <span class="p_context"> static inline struct kmem_cache *slab_pre_alloc_hook(struct kmem_cache *s,</span>
 {
 	flags &amp;= gfp_allowed_mask;
 	lockdep_trace_alloc(flags);
<span class="p_del">-	might_sleep_if(flags &amp; __GFP_WAIT);</span>
<span class="p_add">+	might_sleep_if(gfpflags_allow_blocking(flags));</span>
 
 	if (should_failslab(s-&gt;object_size, flags, s-&gt;flags))
 		return NULL;
<span class="p_chunk">@@ -1352,7 +1352,7 @@</span> <span class="p_context"> static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
 
 	flags &amp;= gfp_allowed_mask;
 
<span class="p_del">-	if (flags &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(flags))</span>
 		local_irq_enable();
 
 	flags |= s-&gt;allocflags;
<span class="p_chunk">@@ -1362,8 +1362,8 @@</span> <span class="p_context"> static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
 	 * so we fall-back to the minimum order allocation.
 	 */
 	alloc_gfp = (flags | __GFP_NOWARN | __GFP_NORETRY) &amp; ~__GFP_NOFAIL;
<span class="p_del">-	if ((alloc_gfp &amp; __GFP_WAIT) &amp;&amp; oo_order(oo) &gt; oo_order(s-&gt;min))</span>
<span class="p_del">-		alloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) &amp; ~__GFP_WAIT;</span>
<span class="p_add">+	if ((alloc_gfp &amp; __GFP_DIRECT_RECLAIM) &amp;&amp; oo_order(oo) &gt; oo_order(s-&gt;min))</span>
<span class="p_add">+		alloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) &amp; ~__GFP_DIRECT_RECLAIM;</span>
 
 	page = alloc_slab_page(s, alloc_gfp, node, oo);
 	if (unlikely(!page)) {
<span class="p_chunk">@@ -1423,7 +1423,7 @@</span> <span class="p_context"> static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)</span>
 	page-&gt;frozen = 1;
 
 out:
<span class="p_del">-	if (flags &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfpflags_allow_blocking(flags))</span>
 		local_irq_disable();
 	if (!page)
 		return NULL;
<span class="p_header">diff --git a/mm/vmalloc.c b/mm/vmalloc.c</span>
<span class="p_header">index 2faaa2976447..9ad4dcb0631c 100644</span>
<span class="p_header">--- a/mm/vmalloc.c</span>
<span class="p_header">+++ b/mm/vmalloc.c</span>
<span class="p_chunk">@@ -1617,7 +1617,7 @@</span> <span class="p_context"> static void *__vmalloc_area_node(struct vm_struct *area, gfp_t gfp_mask,</span>
 			goto fail;
 		}
 		area-&gt;pages[i] = page;
<span class="p_del">-		if (gfp_mask &amp; __GFP_WAIT)</span>
<span class="p_add">+		if (gfpflags_allow_blocking(gfp_mask))</span>
 			cond_resched();
 	}
 
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 8b2786fd42b5..30a87ac1af80 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -1476,7 +1476,7 @@</span> <span class="p_context"> static int too_many_isolated(struct zone *zone, int file,</span>
 	 * won&#39;t get blocked by normal direct-reclaimers, forming a circular
 	 * deadlock.
 	 */
<span class="p_del">-	if ((sc-&gt;gfp_mask &amp; GFP_IOFS) == GFP_IOFS)</span>
<span class="p_add">+	if ((sc-&gt;gfp_mask &amp; (__GFP_IO | __GFP_FS)) == (__GFP_IO | __GFP_FS))</span>
 		inactive &gt;&gt;= 3;
 
 	return isolated &gt; inactive;
<span class="p_chunk">@@ -3794,7 +3794,7 @@</span> <span class="p_context"> int zone_reclaim(struct zone *zone, gfp_t gfp_mask, unsigned int order)</span>
 	/*
 	 * Do not scan if the allocation should not be delayed.
 	 */
<span class="p_del">-	if (!(gfp_mask &amp; __GFP_WAIT) || (current-&gt;flags &amp; PF_MEMALLOC))</span>
<span class="p_add">+	if (!gfpflags_allow_blocking(gfp_mask) || (current-&gt;flags &amp; PF_MEMALLOC))</span>
 		return ZONE_RECLAIM_NOSCAN;
 
 	/*
<span class="p_header">diff --git a/mm/zswap.c b/mm/zswap.c</span>
<span class="p_header">index 4043df7c672f..e54166d3732e 100644</span>
<span class="p_header">--- a/mm/zswap.c</span>
<span class="p_header">+++ b/mm/zswap.c</span>
<span class="p_chunk">@@ -571,7 +571,7 @@</span> <span class="p_context"> static struct zswap_pool *zswap_pool_find_get(char *type, char *compressor)</span>
 static struct zswap_pool *zswap_pool_create(char *type, char *compressor)
 {
 	struct zswap_pool *pool;
<span class="p_del">-	gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN;</span>
<span class="p_add">+	gfp_t gfp = __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM;</span>
 
 	pool = kzalloc(sizeof(*pool), GFP_KERNEL);
 	if (!pool) {
<span class="p_chunk">@@ -1011,7 +1011,8 @@</span> <span class="p_context"> static int zswap_frontswap_store(unsigned type, pgoff_t offset,</span>
 	/* store */
 	len = dlen + sizeof(struct zswap_header);
 	ret = zpool_malloc(entry-&gt;pool-&gt;zpool, len,
<span class="p_del">-			   __GFP_NORETRY | __GFP_NOWARN, &amp;handle);</span>
<span class="p_add">+			   __GFP_NORETRY | __GFP_NOWARN | __GFP_KSWAPD_RECLAIM,</span>
<span class="p_add">+			   &amp;handle);</span>
 	if (ret == -ENOSPC) {
 		zswap_reject_compress_poor++;
 		goto put_dstmem;
<span class="p_header">diff --git a/net/core/skbuff.c b/net/core/skbuff.c</span>
<span class="p_header">index dad4dd37e2aa..905bae96a742 100644</span>
<span class="p_header">--- a/net/core/skbuff.c</span>
<span class="p_header">+++ b/net/core/skbuff.c</span>
<span class="p_chunk">@@ -414,7 +414,7 @@</span> <span class="p_context"> struct sk_buff *__netdev_alloc_skb(struct net_device *dev, unsigned int len,</span>
 	len += NET_SKB_PAD;
 
 	if ((len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE)) ||
<span class="p_del">-	    (gfp_mask &amp; (__GFP_WAIT | GFP_DMA))) {</span>
<span class="p_add">+	    (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {</span>
 		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);
 		if (!skb)
 			goto skb_fail;
<span class="p_chunk">@@ -481,7 +481,7 @@</span> <span class="p_context"> struct sk_buff *__napi_alloc_skb(struct napi_struct *napi, unsigned int len,</span>
 	len += NET_SKB_PAD + NET_IP_ALIGN;
 
 	if ((len &gt; SKB_WITH_OVERHEAD(PAGE_SIZE)) ||
<span class="p_del">-	    (gfp_mask &amp; (__GFP_WAIT | GFP_DMA))) {</span>
<span class="p_add">+	    (gfp_mask &amp; (__GFP_DIRECT_RECLAIM | GFP_DMA))) {</span>
 		skb = __alloc_skb(len, gfp_mask, SKB_ALLOC_RX, NUMA_NO_NODE);
 		if (!skb)
 			goto skb_fail;
<span class="p_chunk">@@ -4451,7 +4451,7 @@</span> <span class="p_context"> struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
 		return NULL;
 
 	gfp_head = gfp_mask;
<span class="p_del">-	if (gfp_head &amp; __GFP_WAIT)</span>
<span class="p_add">+	if (gfp_head &amp; __GFP_DIRECT_RECLAIM)</span>
 		gfp_head |= __GFP_REPEAT;
 
 	*errcode = -ENOBUFS;
<span class="p_chunk">@@ -4466,7 +4466,7 @@</span> <span class="p_context"> struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
 
 		while (order) {
 			if (npages &gt;= 1 &lt;&lt; order) {
<span class="p_del">-				page = alloc_pages((gfp_mask &amp; ~__GFP_WAIT) |</span>
<span class="p_add">+				page = alloc_pages((gfp_mask &amp; ~__GFP_DIRECT_RECLAIM) |</span>
 						   __GFP_COMP |
 						   __GFP_NOWARN |
 						   __GFP_NORETRY,
<span class="p_header">diff --git a/net/core/sock.c b/net/core/sock.c</span>
<span class="p_header">index ca2984afe16e..4a61a0add949 100644</span>
<span class="p_header">--- a/net/core/sock.c</span>
<span class="p_header">+++ b/net/core/sock.c</span>
<span class="p_chunk">@@ -1879,8 +1879,10 @@</span> <span class="p_context"> bool skb_page_frag_refill(unsigned int sz, struct page_frag *pfrag, gfp_t gfp)</span>
 
 	pfrag-&gt;offset = 0;
 	if (SKB_FRAG_PAGE_ORDER) {
<span class="p_del">-		pfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_WAIT) | __GFP_COMP |</span>
<span class="p_del">-					  __GFP_NOWARN | __GFP_NORETRY,</span>
<span class="p_add">+		/* Avoid direct reclaim but allow kswapd to wake */</span>
<span class="p_add">+		pfrag-&gt;page = alloc_pages((gfp &amp; ~__GFP_DIRECT_RECLAIM) |</span>
<span class="p_add">+					  __GFP_COMP | __GFP_NOWARN |</span>
<span class="p_add">+					  __GFP_NORETRY,</span>
 					  SKB_FRAG_PAGE_ORDER);
 		if (likely(pfrag-&gt;page)) {
 			pfrag-&gt;size = PAGE_SIZE &lt;&lt; SKB_FRAG_PAGE_ORDER;
<span class="p_header">diff --git a/net/netlink/af_netlink.c b/net/netlink/af_netlink.c</span>
<span class="p_header">index 7f86d3b55060..173c0abe4094 100644</span>
<span class="p_header">--- a/net/netlink/af_netlink.c</span>
<span class="p_header">+++ b/net/netlink/af_netlink.c</span>
<span class="p_chunk">@@ -2084,7 +2084,7 @@</span> <span class="p_context"> int netlink_broadcast_filtered(struct sock *ssk, struct sk_buff *skb, u32 portid</span>
 	consume_skb(info.skb2);
 
 	if (info.delivered) {
<span class="p_del">-		if (info.congested &amp;&amp; (allocation &amp; __GFP_WAIT))</span>
<span class="p_add">+		if (info.congested &amp;&amp; gfpflags_allow_blocking(allocation))</span>
 			yield();
 		return 0;
 	}
<span class="p_header">diff --git a/net/rds/ib_recv.c b/net/rds/ib_recv.c</span>
<span class="p_header">index f43831e4186a..dcfb59775acc 100644</span>
<span class="p_header">--- a/net/rds/ib_recv.c</span>
<span class="p_header">+++ b/net/rds/ib_recv.c</span>
<span class="p_chunk">@@ -305,7 +305,7 @@</span> <span class="p_context"> static int rds_ib_recv_refill_one(struct rds_connection *conn,</span>
 	gfp_t slab_mask = GFP_NOWAIT;
 	gfp_t page_mask = GFP_NOWAIT;
 
<span class="p_del">-	if (gfp &amp; __GFP_WAIT) {</span>
<span class="p_add">+	if (gfp &amp; __GFP_DIRECT_RECLAIM) {</span>
 		slab_mask = GFP_KERNEL;
 		page_mask = GFP_HIGHUSER;
 	}
<span class="p_chunk">@@ -379,7 +379,7 @@</span> <span class="p_context"> void rds_ib_recv_refill(struct rds_connection *conn, int prefill, gfp_t gfp)</span>
 	struct ib_recv_wr *failed_wr;
 	unsigned int posted = 0;
 	int ret = 0;
<span class="p_del">-	bool can_wait = !!(gfp &amp; __GFP_WAIT);</span>
<span class="p_add">+	bool can_wait = !!(gfp &amp; __GFP_DIRECT_RECLAIM);</span>
 	u32 pos;
 
 	/* the goal here is to just make sure that someone, somewhere
<span class="p_header">diff --git a/net/rxrpc/ar-connection.c b/net/rxrpc/ar-connection.c</span>
<span class="p_header">index 6631f4f1e39b..3b5de4b86058 100644</span>
<span class="p_header">--- a/net/rxrpc/ar-connection.c</span>
<span class="p_header">+++ b/net/rxrpc/ar-connection.c</span>
<span class="p_chunk">@@ -500,7 +500,7 @@</span> <span class="p_context"> int rxrpc_connect_call(struct rxrpc_sock *rx,</span>
 		if (bundle-&gt;num_conns &gt;= 20) {
 			_debug(&quot;too many conns&quot;);
 
<span class="p_del">-			if (!(gfp &amp; __GFP_WAIT)) {</span>
<span class="p_add">+			if (!gfpflags_allow_blocking(gfp)) {</span>
 				_leave(&quot; = -EAGAIN&quot;);
 				return -EAGAIN;
 			}
<span class="p_header">diff --git a/net/sctp/associola.c b/net/sctp/associola.c</span>
<span class="p_header">index 197c3f59ecbf..75369ae8de1e 100644</span>
<span class="p_header">--- a/net/sctp/associola.c</span>
<span class="p_header">+++ b/net/sctp/associola.c</span>
<span class="p_chunk">@@ -1588,7 +1588,7 @@</span> <span class="p_context"> int sctp_assoc_lookup_laddr(struct sctp_association *asoc,</span>
 /* Set an association id for a given association */
 int sctp_assoc_set_id(struct sctp_association *asoc, gfp_t gfp)
 {
<span class="p_del">-	bool preload = !!(gfp &amp; __GFP_WAIT);</span>
<span class="p_add">+	bool preload = gfpflags_allow_blocking(gfp);</span>
 	int ret;
 
 	/* If the id is already assigned, keep it. */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



