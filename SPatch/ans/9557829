
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[89/89] sched/headers: Clean up &lt;linux/sched.h&gt; - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [89/89] sched/headers: Clean up &lt;linux/sched.h&gt;</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 6, 2017, 1:36 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170206133614.GB22208@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9557829/mbox/"
   >mbox</a>
|
   <a href="/patch/9557829/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9557829/">/patch/9557829/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	9661B60413 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Feb 2017 13:36:34 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7DAC726247
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Feb 2017 13:36:34 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 71ED227F88; Mon,  6 Feb 2017 13:36:34 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.3 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI, RCVD_IN_SORBS_SPAM,
	T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A53F726247
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  6 Feb 2017 13:36:31 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752731AbdBFNg3 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 6 Feb 2017 08:36:29 -0500
Received: from mail-wj0-f196.google.com ([209.85.210.196]:35759 &quot;EHLO
	mail-wj0-f196.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752830AbdBFNgY (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 6 Feb 2017 08:36:24 -0500
Received: by mail-wj0-f196.google.com with SMTP id i7so2944282wjf.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 06 Feb 2017 05:36:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=sender:date:from:to:cc:subject:message-id:references:mime-version
	:content-disposition:in-reply-to:user-agent;
	bh=UlhODW2Ufl5XxvL/peVoPCpnq1SLjOFDycVYhfCwtbM=;
	b=jV33uAcmy3eD4HKgXdqYZw1iBjzpmEeF9giML7d3B+uWu7zkonurveXHDXzi6cNhPi
	vboTKqTt3O55VGSrK8RH+w+qfDFHw9X4tLRI79Cl1EIA07JaL27HpTe2+PyYiIP1s22x
	A5DjVRd1hZcm8RXMK5gU9U/hy4UZ1oy0FjuqMm8MV+6evQnUpw1dA1W2QfmdmCAuIApC
	TpxkFM7RQsmwzZbRlsL0ojlZyEYELI/2b2NF3TZPCE8NN8rfBFiafSuitQ3fx9P/babn
	UkTIB5CudDG2tWmnUtfVPmXdTSLFE+vMZ4CQTk76bPleXjZ5Jv6q339GU1adFFHeyKYJ
	5H9g==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:references:mime-version:content-disposition:in-reply-to:user-agent; 
	bh=UlhODW2Ufl5XxvL/peVoPCpnq1SLjOFDycVYhfCwtbM=;
	b=OuJFfUQCfwba1YOjQPzOtqBsjM/SXI3RYpFDQ1hquyd08sEdo6SoccQIWXsDc33Swg
	56EU6fyqaKqSuesaRxv7cmi/zF0MkH/n2dI54I8laKpVVIK4g7RTBXyCh6LK9a1EP/Qo
	ZbZ3FOsnLc47qeYnfjwQWG0ww+5+t4EYDEzzFrwH4IKoqyXCubxiSm/oFQey8nxQD4Pz
	JqlY9bKV0U0cGdRQKjREe3wZ+yfhwrH4V1bO6x6i+gvVqyu00MjQPgRMKF5+OcfExt4a
	bs7Jeh9/mixpsyrmY+4LEvM8k0Zik5B19WU8sVUR3F35wjUYvn3geyY0XvUbEO3HjP/W
	SqhA==
X-Gm-Message-State: AMke39kz8VLfVJZ5VGeIyrFZk44V+xVnB85vdoU1PHwgGzV8mnHv4WlH3rRFZIYDSwlyzQ==
X-Received: by 10.28.35.65 with SMTP id j62mr7995052wmj.31.1486388177233;
	Mon, 06 Feb 2017 05:36:17 -0800 (PST)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	10sm1545906wrw.13.2017.02.06.05.36.15
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 06 Feb 2017 05:36:16 -0800 (PST)
Date: Mon, 6 Feb 2017 14:36:14 +0100
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: linux-kernel@vger.kernel.org
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Mike Galbraith &lt;efault@gmx.de&gt;, Oleg Nesterov &lt;oleg@redhat.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;
Subject: [PATCH 89/89] sched/headers: Clean up &lt;linux/sched.h&gt;
Message-ID: &lt;20170206133614.GB22208@gmail.com&gt;
References: &lt;1486387772-18837-1-git-send-email-mingo@kernel.org&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;1486387772-18837-1-git-send-email-mingo@kernel.org&gt;
User-Agent: Mutt/1.5.24 (2015-08-30)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Feb. 6, 2017, 1:36 p.m.</div>
<pre class="content">
Now that &lt;linux/sched.h&gt; dependencies have been sorted out,
do various trivial cleanups:

 - remove unnecessary structure predeclarations
 - fix various typos
 - update comments where necessary
 - remove pointless comments
 - use consistent types
 - tabulate consistently
 - use a consistent comment style
 - clean up the header section a bit
 - use a consistent style of a single field per line
 - remove line-breaks where they make the code look worse
 - etc ...

No change in functionality.

Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Mike Galbraith &lt;efault@gmx.de&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org
<span class="signed-off-by">Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
---
 include/linux/sched.h | 1224 +++++++++++++++++++++++++++++++++++++--------------------------------
 1 file changed, 652 insertions(+), 572 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index e59686ce0bfd..92e3321d9269 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -1,38 +1,38 @@</span> <span class="p_context"></span>
 #ifndef _LINUX_SCHED_H
 #define _LINUX_SCHED_H
 
<span class="p_del">-#include &lt;uapi/linux/sched.h&gt;</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Define &#39;struct task_struct&#39; and provide the main scheduler</span>
<span class="p_add">+ * APIs (schedule(), wakeup variants, etc.)</span>
<span class="p_add">+ */</span>
 
<span class="p_del">-#include &lt;linux/sched/prio.h&gt;</span>
<span class="p_del">-#include &lt;linux/nodemask.h&gt;</span>
<span class="p_add">+#include &lt;uapi/linux/sched.h&gt;</span>
 
<span class="p_del">-#include &lt;linux/mutex.h&gt;</span>
<span class="p_del">-#include &lt;linux/plist.h&gt;</span>
<span class="p_del">-#include &lt;linux/mm_types_task.h&gt;</span>
<span class="p_add">+#include &lt;asm/current.h&gt;</span>
 
<span class="p_add">+#include &lt;linux/pid.h&gt;</span>
 #include &lt;linux/sem.h&gt;
 #include &lt;linux/shm.h&gt;
<span class="p_del">-#include &lt;linux/signal_types.h&gt;</span>
<span class="p_del">-#include &lt;linux/pid.h&gt;</span>
<span class="p_add">+#include &lt;linux/kcov.h&gt;</span>
<span class="p_add">+#include &lt;linux/mutex.h&gt;</span>
<span class="p_add">+#include &lt;linux/plist.h&gt;</span>
<span class="p_add">+#include &lt;linux/hrtimer.h&gt;</span>
 #include &lt;linux/seccomp.h&gt;
<span class="p_add">+#include &lt;linux/nodemask.h&gt;</span>
 #include &lt;linux/rcupdate.h&gt;
<span class="p_del">-</span>
 #include &lt;linux/resource.h&gt;
<span class="p_del">-#include &lt;linux/hrtimer.h&gt;</span>
<span class="p_del">-#include &lt;linux/kcov.h&gt;</span>
<span class="p_del">-#include &lt;linux/task_io_accounting.h&gt;</span>
 #include &lt;linux/latencytop.h&gt;
<span class="p_add">+#include &lt;linux/sched/prio.h&gt;</span>
<span class="p_add">+#include &lt;linux/signal_types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm_types_task.h&gt;</span>
<span class="p_add">+#include &lt;linux/task_io_accounting.h&gt;</span>
 
<span class="p_del">-#include &lt;asm/current.h&gt;</span>
<span class="p_del">-</span>
<span class="p_del">-/* task_struct member predeclarations: */</span>
<span class="p_add">+/* task_struct member predeclarations (sorted alphabetically): */</span>
 struct audit_context;
<span class="p_del">-struct autogroup;</span>
 struct backing_dev_info;
 struct bio_list;
 struct blk_plug;
 struct cfs_rq;
<span class="p_del">-struct filename;</span>
 struct fs_struct;
 struct futex_pi_state;
 struct io_context;
<span class="p_chunk">@@ -52,8 +52,6 @@</span> <span class="p_context"> struct sighand_struct;</span>
 struct signal_struct;
 struct task_delay_info;
 struct task_group;
<span class="p_del">-struct task_struct;</span>
<span class="p_del">-struct uts_namespace;</span>
 
 /*
  * Task state bitmask. NOTE! These bits are also
<span class="p_chunk">@@ -65,50 +63,53 @@</span> <span class="p_context"> struct uts_namespace;</span>
  * modifying one set can&#39;t modify the other one by
  * mistake.
  */
<span class="p_del">-#define TASK_RUNNING		0</span>
<span class="p_del">-#define TASK_INTERRUPTIBLE	1</span>
<span class="p_del">-#define TASK_UNINTERRUPTIBLE	2</span>
<span class="p_del">-#define __TASK_STOPPED		4</span>
<span class="p_del">-#define __TASK_TRACED		8</span>
<span class="p_del">-/* in tsk-&gt;exit_state */</span>
<span class="p_del">-#define EXIT_DEAD		16</span>
<span class="p_del">-#define EXIT_ZOMBIE		32</span>
<span class="p_del">-#define EXIT_TRACE		(EXIT_ZOMBIE | EXIT_DEAD)</span>
<span class="p_del">-/* in tsk-&gt;state again */</span>
<span class="p_del">-#define TASK_DEAD		64</span>
<span class="p_del">-#define TASK_WAKEKILL		128</span>
<span class="p_del">-#define TASK_WAKING		256</span>
<span class="p_del">-#define TASK_PARKED		512</span>
<span class="p_del">-#define TASK_NOLOAD		1024</span>
<span class="p_del">-#define TASK_NEW		2048</span>
<span class="p_del">-#define TASK_STATE_MAX		4096</span>
<span class="p_del">-</span>
<span class="p_del">-#define TASK_STATE_TO_CHAR_STR &quot;RSDTtXZxKWPNn&quot;</span>
<span class="p_del">-</span>
<span class="p_del">-/* Convenience macros for the sake of set_current_state */</span>
<span class="p_del">-#define TASK_KILLABLE		(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)</span>
<span class="p_del">-#define TASK_STOPPED		(TASK_WAKEKILL | __TASK_STOPPED)</span>
<span class="p_del">-#define TASK_TRACED		(TASK_WAKEKILL | __TASK_TRACED)</span>
<span class="p_del">-</span>
<span class="p_del">-#define TASK_IDLE		(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)</span>
<span class="p_del">-</span>
<span class="p_del">-/* Convenience macros for the sake of wake_up */</span>
<span class="p_del">-#define TASK_NORMAL		(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)</span>
<span class="p_del">-#define TASK_ALL		(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)</span>
<span class="p_del">-</span>
<span class="p_del">-/* get_task_state() */</span>
<span class="p_del">-#define TASK_REPORT		(TASK_RUNNING | TASK_INTERRUPTIBLE | \</span>
<span class="p_del">-				 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \</span>
<span class="p_del">-				 __TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD)</span>
<span class="p_del">-</span>
<span class="p_del">-#define task_is_traced(task)	((task-&gt;state &amp; __TASK_TRACED) != 0)</span>
<span class="p_del">-#define task_is_stopped(task)	((task-&gt;state &amp; __TASK_STOPPED) != 0)</span>
<span class="p_del">-#define task_is_stopped_or_traced(task)	\</span>
<span class="p_del">-			((task-&gt;state &amp; (__TASK_STOPPED | __TASK_TRACED)) != 0)</span>
<span class="p_del">-#define task_contributes_to_load(task)	\</span>
<span class="p_del">-				((task-&gt;state &amp; TASK_UNINTERRUPTIBLE) != 0 &amp;&amp; \</span>
<span class="p_del">-				 (task-&gt;flags &amp; PF_FROZEN) == 0 &amp;&amp; \</span>
<span class="p_del">-				 (task-&gt;state &amp; TASK_NOLOAD) == 0)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Used in tsk-&gt;state: */</span>
<span class="p_add">+#define TASK_RUNNING			0</span>
<span class="p_add">+#define TASK_INTERRUPTIBLE		1</span>
<span class="p_add">+#define TASK_UNINTERRUPTIBLE		2</span>
<span class="p_add">+#define __TASK_STOPPED			4</span>
<span class="p_add">+#define __TASK_TRACED			8</span>
<span class="p_add">+/* Used in tsk-&gt;exit_state: */</span>
<span class="p_add">+#define EXIT_DEAD			16</span>
<span class="p_add">+#define EXIT_ZOMBIE			32</span>
<span class="p_add">+#define EXIT_TRACE			(EXIT_ZOMBIE | EXIT_DEAD)</span>
<span class="p_add">+/* Used in tsk-&gt;state again: */</span>
<span class="p_add">+#define TASK_DEAD			64</span>
<span class="p_add">+#define TASK_WAKEKILL			128</span>
<span class="p_add">+#define TASK_WAKING			256</span>
<span class="p_add">+#define TASK_PARKED			512</span>
<span class="p_add">+#define TASK_NOLOAD			1024</span>
<span class="p_add">+#define TASK_NEW			2048</span>
<span class="p_add">+#define TASK_STATE_MAX			4096</span>
<span class="p_add">+</span>
<span class="p_add">+#define TASK_STATE_TO_CHAR_STR		&quot;RSDTtXZxKWPNn&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Convenience macros for the sake of set_current_state: */</span>
<span class="p_add">+#define TASK_KILLABLE			(TASK_WAKEKILL | TASK_UNINTERRUPTIBLE)</span>
<span class="p_add">+#define TASK_STOPPED			(TASK_WAKEKILL | __TASK_STOPPED)</span>
<span class="p_add">+#define TASK_TRACED			(TASK_WAKEKILL | __TASK_TRACED)</span>
<span class="p_add">+</span>
<span class="p_add">+#define TASK_IDLE			(TASK_UNINTERRUPTIBLE | TASK_NOLOAD)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Convenience macros for the sake of wake_up(): */</span>
<span class="p_add">+#define TASK_NORMAL			(TASK_INTERRUPTIBLE | TASK_UNINTERRUPTIBLE)</span>
<span class="p_add">+#define TASK_ALL			(TASK_NORMAL | __TASK_STOPPED | __TASK_TRACED)</span>
<span class="p_add">+</span>
<span class="p_add">+/* get_task_state(): */</span>
<span class="p_add">+#define TASK_REPORT			(TASK_RUNNING | TASK_INTERRUPTIBLE | \</span>
<span class="p_add">+					 TASK_UNINTERRUPTIBLE | __TASK_STOPPED | \</span>
<span class="p_add">+					 __TASK_TRACED | EXIT_ZOMBIE | EXIT_DEAD)</span>
<span class="p_add">+</span>
<span class="p_add">+#define task_is_traced(task)		((task-&gt;state &amp; __TASK_TRACED) != 0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define task_is_stopped(task)		((task-&gt;state &amp; __TASK_STOPPED) != 0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define task_is_stopped_or_traced(task)	((task-&gt;state &amp; (__TASK_STOPPED | __TASK_TRACED)) != 0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define task_contributes_to_load(task)	((task-&gt;state &amp; TASK_UNINTERRUPTIBLE) != 0 &amp;&amp; \</span>
<span class="p_add">+					 (task-&gt;flags &amp; PF_FROZEN) == 0 &amp;&amp; \</span>
<span class="p_add">+					 (task-&gt;state &amp; TASK_NOLOAD) == 0)</span>
 
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
 
<span class="p_chunk">@@ -158,26 +159,24 @@</span> <span class="p_context"> struct uts_namespace;</span>
  *
  * Also see the comments of try_to_wake_up().
  */
<span class="p_del">-#define __set_current_state(state_value)		\</span>
<span class="p_del">-	do { current-&gt;state = (state_value); } while (0)</span>
<span class="p_del">-#define set_current_state(state_value)			\</span>
<span class="p_del">-	smp_store_mb(current-&gt;state, (state_value))</span>
<span class="p_del">-</span>
<span class="p_add">+#define __set_current_state(state_value) do { current-&gt;state = (state_value); } while (0)</span>
<span class="p_add">+#define set_current_state(state_value)	 smp_store_mb(current-&gt;state, (state_value))</span>
 #endif
 
<span class="p_del">-/* Task command name length */</span>
<span class="p_del">-#define TASK_COMM_LEN 16</span>
<span class="p_add">+/* Task command name length: */</span>
<span class="p_add">+#define TASK_COMM_LEN			16</span>
 
<span class="p_del">-extern cpumask_var_t cpu_isolated_map;</span>
<span class="p_add">+extern cpumask_var_t			cpu_isolated_map;</span>
 
 extern void scheduler_tick(void);
 
<span class="p_del">-#define	MAX_SCHEDULE_TIMEOUT	LONG_MAX</span>
<span class="p_del">-extern signed long schedule_timeout(signed long timeout);</span>
<span class="p_del">-extern signed long schedule_timeout_interruptible(signed long timeout);</span>
<span class="p_del">-extern signed long schedule_timeout_killable(signed long timeout);</span>
<span class="p_del">-extern signed long schedule_timeout_uninterruptible(signed long timeout);</span>
<span class="p_del">-extern signed long schedule_timeout_idle(signed long timeout);</span>
<span class="p_add">+#define	MAX_SCHEDULE_TIMEOUT		LONG_MAX</span>
<span class="p_add">+</span>
<span class="p_add">+extern long schedule_timeout(long timeout);</span>
<span class="p_add">+extern long schedule_timeout_interruptible(long timeout);</span>
<span class="p_add">+extern long schedule_timeout_killable(long timeout);</span>
<span class="p_add">+extern long schedule_timeout_uninterruptible(long timeout);</span>
<span class="p_add">+extern long schedule_timeout_idle(long timeout);</span>
 asmlinkage void schedule(void);
 extern void schedule_preempt_disabled(void);
 
<span class="p_chunk">@@ -197,9 +196,9 @@</span> <span class="p_context"> extern void io_schedule(void);</span>
  */
 struct prev_cputime {
 #ifndef CONFIG_VIRT_CPU_ACCOUNTING_NATIVE
<span class="p_del">-	u64 utime;</span>
<span class="p_del">-	u64 stime;</span>
<span class="p_del">-	raw_spinlock_t lock;</span>
<span class="p_add">+	u64				utime;</span>
<span class="p_add">+	u64				stime;</span>
<span class="p_add">+	raw_spinlock_t			lock;</span>
 #endif
 };
 
<span class="p_chunk">@@ -214,25 +213,34 @@</span> <span class="p_context"> struct prev_cputime {</span>
  * these counts together and treat all three of them in parallel.
  */
 struct task_cputime {
<span class="p_del">-	u64 utime;</span>
<span class="p_del">-	u64 stime;</span>
<span class="p_del">-	unsigned long long sum_exec_runtime;</span>
<span class="p_add">+	u64				utime;</span>
<span class="p_add">+	u64				stime;</span>
<span class="p_add">+	unsigned long long		sum_exec_runtime;</span>
 };
 
<span class="p_del">-/* Alternate field names when used to cache expirations. */</span>
<span class="p_del">-#define virt_exp	utime</span>
<span class="p_del">-#define prof_exp	stime</span>
<span class="p_del">-#define sched_exp	sum_exec_runtime</span>
<span class="p_add">+/* Alternate field names when used on cache expirations: */</span>
<span class="p_add">+#define virt_exp			utime</span>
<span class="p_add">+#define prof_exp			stime</span>
<span class="p_add">+#define sched_exp			sum_exec_runtime</span>
 
 struct sched_info {
 #ifdef CONFIG_SCHED_INFO
<span class="p_del">-	/* cumulative counters */</span>
<span class="p_del">-	unsigned long pcount;	      /* # of times run on this cpu */</span>
<span class="p_del">-	unsigned long long run_delay; /* time spent waiting on a runqueue */</span>
<span class="p_add">+	/* Cumulative counters: */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* # of times we have run on this CPU: */</span>
<span class="p_add">+	unsigned long			pcount;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Time spent waiting on a runqueue: */</span>
<span class="p_add">+	unsigned long long		run_delay;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Timestamps: */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* When did we last run on a CPU? */</span>
<span class="p_add">+	unsigned long long		last_arrival;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* When were we last queued to run? */</span>
<span class="p_add">+	unsigned long long		last_queued;</span>
 
<span class="p_del">-	/* timestamps */</span>
<span class="p_del">-	unsigned long long last_arrival,/* when we last ran on a cpu */</span>
<span class="p_del">-			   last_queued;	/* when we were last queued to run */</span>
 #endif /* CONFIG_SCHED_INFO */
 };
 
<span class="p_chunk">@@ -243,12 +251,12 @@</span> <span class="p_context"> struct sched_info {</span>
  * We define a basic fixed point arithmetic range, and then formalize
  * all these metrics based on that basic range.
  */
<span class="p_del">-# define SCHED_FIXEDPOINT_SHIFT	10</span>
<span class="p_del">-# define SCHED_FIXEDPOINT_SCALE	(1L &lt;&lt; SCHED_FIXEDPOINT_SHIFT)</span>
<span class="p_add">+# define SCHED_FIXEDPOINT_SHIFT		10</span>
<span class="p_add">+# define SCHED_FIXEDPOINT_SCALE		(1L &lt;&lt; SCHED_FIXEDPOINT_SHIFT)</span>
 
 struct load_weight {
<span class="p_del">-	unsigned long weight;</span>
<span class="p_del">-	u32 inv_weight;</span>
<span class="p_add">+	unsigned long			weight;</span>
<span class="p_add">+	u32				inv_weight;</span>
 };
 
 /*
<span class="p_chunk">@@ -304,69 +312,73 @@</span> <span class="p_context"> struct load_weight {</span>
  * issues.
  */
 struct sched_avg {
<span class="p_del">-	u64 last_update_time, load_sum;</span>
<span class="p_del">-	u32 util_sum, period_contrib;</span>
<span class="p_del">-	unsigned long load_avg, util_avg;</span>
<span class="p_add">+	u64				last_update_time;</span>
<span class="p_add">+	u64				load_sum;</span>
<span class="p_add">+	u32				util_sum;</span>
<span class="p_add">+	u32				period_contrib;</span>
<span class="p_add">+	unsigned long			load_avg;</span>
<span class="p_add">+	unsigned long			util_avg;</span>
 };
 
 struct sched_statistics {
 #ifdef CONFIG_SCHEDSTATS
<span class="p_del">-	u64			wait_start;</span>
<span class="p_del">-	u64			wait_max;</span>
<span class="p_del">-	u64			wait_count;</span>
<span class="p_del">-	u64			wait_sum;</span>
<span class="p_del">-	u64			iowait_count;</span>
<span class="p_del">-	u64			iowait_sum;</span>
<span class="p_del">-</span>
<span class="p_del">-	u64			sleep_start;</span>
<span class="p_del">-	u64			sleep_max;</span>
<span class="p_del">-	s64			sum_sleep_runtime;</span>
<span class="p_del">-</span>
<span class="p_del">-	u64			block_start;</span>
<span class="p_del">-	u64			block_max;</span>
<span class="p_del">-	u64			exec_max;</span>
<span class="p_del">-	u64			slice_max;</span>
<span class="p_del">-</span>
<span class="p_del">-	u64			nr_migrations_cold;</span>
<span class="p_del">-	u64			nr_failed_migrations_affine;</span>
<span class="p_del">-	u64			nr_failed_migrations_running;</span>
<span class="p_del">-	u64			nr_failed_migrations_hot;</span>
<span class="p_del">-	u64			nr_forced_migrations;</span>
<span class="p_del">-</span>
<span class="p_del">-	u64			nr_wakeups;</span>
<span class="p_del">-	u64			nr_wakeups_sync;</span>
<span class="p_del">-	u64			nr_wakeups_migrate;</span>
<span class="p_del">-	u64			nr_wakeups_local;</span>
<span class="p_del">-	u64			nr_wakeups_remote;</span>
<span class="p_del">-	u64			nr_wakeups_affine;</span>
<span class="p_del">-	u64			nr_wakeups_affine_attempts;</span>
<span class="p_del">-	u64			nr_wakeups_passive;</span>
<span class="p_del">-	u64			nr_wakeups_idle;</span>
<span class="p_add">+	u64				wait_start;</span>
<span class="p_add">+	u64				wait_max;</span>
<span class="p_add">+	u64				wait_count;</span>
<span class="p_add">+	u64				wait_sum;</span>
<span class="p_add">+	u64				iowait_count;</span>
<span class="p_add">+	u64				iowait_sum;</span>
<span class="p_add">+</span>
<span class="p_add">+	u64				sleep_start;</span>
<span class="p_add">+	u64				sleep_max;</span>
<span class="p_add">+	s64				sum_sleep_runtime;</span>
<span class="p_add">+</span>
<span class="p_add">+	u64				block_start;</span>
<span class="p_add">+	u64				block_max;</span>
<span class="p_add">+	u64				exec_max;</span>
<span class="p_add">+	u64				slice_max;</span>
<span class="p_add">+</span>
<span class="p_add">+	u64				nr_migrations_cold;</span>
<span class="p_add">+	u64				nr_failed_migrations_affine;</span>
<span class="p_add">+	u64				nr_failed_migrations_running;</span>
<span class="p_add">+	u64				nr_failed_migrations_hot;</span>
<span class="p_add">+	u64				nr_forced_migrations;</span>
<span class="p_add">+</span>
<span class="p_add">+	u64				nr_wakeups;</span>
<span class="p_add">+	u64				nr_wakeups_sync;</span>
<span class="p_add">+	u64				nr_wakeups_migrate;</span>
<span class="p_add">+	u64				nr_wakeups_local;</span>
<span class="p_add">+	u64				nr_wakeups_remote;</span>
<span class="p_add">+	u64				nr_wakeups_affine;</span>
<span class="p_add">+	u64				nr_wakeups_affine_attempts;</span>
<span class="p_add">+	u64				nr_wakeups_passive;</span>
<span class="p_add">+	u64				nr_wakeups_idle;</span>
 #endif
 };
 
 struct sched_entity {
<span class="p_del">-	struct load_weight	load;		/* for load-balancing */</span>
<span class="p_del">-	struct rb_node		run_node;</span>
<span class="p_del">-	struct list_head	group_node;</span>
<span class="p_del">-	unsigned int		on_rq;</span>
<span class="p_add">+	/* For load-balancing: */</span>
<span class="p_add">+	struct load_weight		load;</span>
<span class="p_add">+	struct rb_node			run_node;</span>
<span class="p_add">+	struct list_head		group_node;</span>
<span class="p_add">+	unsigned int			on_rq;</span>
 
<span class="p_del">-	u64			exec_start;</span>
<span class="p_del">-	u64			sum_exec_runtime;</span>
<span class="p_del">-	u64			vruntime;</span>
<span class="p_del">-	u64			prev_sum_exec_runtime;</span>
<span class="p_add">+	u64				exec_start;</span>
<span class="p_add">+	u64				sum_exec_runtime;</span>
<span class="p_add">+	u64				vruntime;</span>
<span class="p_add">+	u64				prev_sum_exec_runtime;</span>
 
<span class="p_del">-	u64			nr_migrations;</span>
<span class="p_add">+	u64				nr_migrations;</span>
 
<span class="p_del">-	struct sched_statistics statistics;</span>
<span class="p_add">+	struct sched_statistics		statistics;</span>
 
 #ifdef CONFIG_FAIR_GROUP_SCHED
<span class="p_del">-	int			depth;</span>
<span class="p_del">-	struct sched_entity	*parent;</span>
<span class="p_add">+	int				depth;</span>
<span class="p_add">+	struct sched_entity		*parent;</span>
 	/* rq on which this entity is (to be) queued: */
<span class="p_del">-	struct cfs_rq		*cfs_rq;</span>
<span class="p_add">+	struct cfs_rq			*cfs_rq;</span>
 	/* rq &quot;owned&quot; by this entity/group: */
<span class="p_del">-	struct cfs_rq		*my_q;</span>
<span class="p_add">+	struct cfs_rq			*my_q;</span>
 #endif
 
 #ifdef CONFIG_SMP
<span class="p_chunk">@@ -376,49 +388,49 @@</span> <span class="p_context"> struct sched_entity {</span>
 	 * Put into separate cache line so it does not
 	 * collide with read-mostly values above.
 	 */
<span class="p_del">-	struct sched_avg	avg ____cacheline_aligned_in_smp;</span>
<span class="p_add">+	struct sched_avg		avg ____cacheline_aligned_in_smp;</span>
 #endif
 };
 
 struct sched_rt_entity {
<span class="p_del">-	struct list_head run_list;</span>
<span class="p_del">-	unsigned long timeout;</span>
<span class="p_del">-	unsigned long watchdog_stamp;</span>
<span class="p_del">-	unsigned int time_slice;</span>
<span class="p_del">-	unsigned short on_rq;</span>
<span class="p_del">-	unsigned short on_list;</span>
<span class="p_del">-</span>
<span class="p_del">-	struct sched_rt_entity *back;</span>
<span class="p_add">+	struct list_head		run_list;</span>
<span class="p_add">+	unsigned long			timeout;</span>
<span class="p_add">+	unsigned long			watchdog_stamp;</span>
<span class="p_add">+	unsigned int			time_slice;</span>
<span class="p_add">+	unsigned short			on_rq;</span>
<span class="p_add">+	unsigned short			on_list;</span>
<span class="p_add">+</span>
<span class="p_add">+	struct sched_rt_entity		*back;</span>
 #ifdef CONFIG_RT_GROUP_SCHED
<span class="p_del">-	struct sched_rt_entity	*parent;</span>
<span class="p_add">+	struct sched_rt_entity		*parent;</span>
 	/* rq on which this entity is (to be) queued: */
<span class="p_del">-	struct rt_rq		*rt_rq;</span>
<span class="p_add">+	struct rt_rq			*rt_rq;</span>
 	/* rq &quot;owned&quot; by this entity/group: */
<span class="p_del">-	struct rt_rq		*my_q;</span>
<span class="p_add">+	struct rt_rq			*my_q;</span>
 #endif
 };
 
 struct sched_dl_entity {
<span class="p_del">-	struct rb_node	rb_node;</span>
<span class="p_add">+	struct rb_node			rb_node;</span>
 
 	/*
 	 * Original scheduling parameters. Copied here from sched_attr
 	 * during sched_setattr(), they will remain the same until
 	 * the next sched_setattr().
 	 */
<span class="p_del">-	u64 dl_runtime;		/* maximum runtime for each instance	*/</span>
<span class="p_del">-	u64 dl_deadline;	/* relative deadline of each instance	*/</span>
<span class="p_del">-	u64 dl_period;		/* separation of two instances (period) */</span>
<span class="p_del">-	u64 dl_bw;		/* dl_runtime / dl_deadline		*/</span>
<span class="p_add">+	u64				dl_runtime;	/* Maximum runtime for each instance	*/</span>
<span class="p_add">+	u64				dl_deadline;	/* Relative deadline of each instance	*/</span>
<span class="p_add">+	u64				dl_period;	/* Separation of two instances (period) */</span>
<span class="p_add">+	u64				dl_bw;		/* dl_runtime / dl_deadline		*/</span>
 
 	/*
 	 * Actual scheduling parameters. Initialized with the values above,
 	 * they are continously updated during task execution. Note that
 	 * the remaining runtime could be &lt; 0 in case we are in overrun.
 	 */
<span class="p_del">-	s64 runtime;		/* remaining runtime for this instance	*/</span>
<span class="p_del">-	u64 deadline;		/* absolute deadline for this instance	*/</span>
<span class="p_del">-	unsigned int flags;	/* specifying the scheduler behaviour	*/</span>
<span class="p_add">+	s64				runtime;	/* Remaining runtime for this instance	*/</span>
<span class="p_add">+	u64				deadline;	/* Absolute deadline for this instance	*/</span>
<span class="p_add">+	unsigned int			flags;		/* Specifying the scheduler behaviour	*/</span>
 
 	/*
 	 * Some bool flags:
<span class="p_chunk">@@ -431,24 +443,28 @@</span> <span class="p_context"> struct sched_dl_entity {</span>
 	 * outside bandwidth enforcement mechanism (but only until we
 	 * exit the critical section);
 	 *
<span class="p_del">-	 * @dl_yielded tells if task gave up the cpu before consuming</span>
<span class="p_add">+	 * @dl_yielded tells if task gave up the CPU before consuming</span>
 	 * all its available runtime during the last job.
 	 */
<span class="p_del">-	int dl_throttled, dl_boosted, dl_yielded;</span>
<span class="p_add">+	int				dl_throttled;</span>
<span class="p_add">+	int				dl_boosted;</span>
<span class="p_add">+	int				dl_yielded;</span>
 
 	/*
 	 * Bandwidth enforcement timer. Each -deadline task has its
 	 * own bandwidth to be enforced, thus we need one timer per task.
 	 */
<span class="p_del">-	struct hrtimer dl_timer;</span>
<span class="p_add">+	struct hrtimer			dl_timer;</span>
 };
 
 union rcu_special {
 	struct {
<span class="p_del">-		u8 blocked;</span>
<span class="p_del">-		u8 need_qs;</span>
<span class="p_del">-		u8 exp_need_qs;</span>
<span class="p_del">-		u8 pad;	/* Otherwise the compiler can store garbage here. */</span>
<span class="p_add">+		u8			blocked;</span>
<span class="p_add">+		u8			need_qs;</span>
<span class="p_add">+		u8			exp_need_qs;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Otherwise the compiler can store garbage here: */</span>
<span class="p_add">+		u8			pad;</span>
 	} b; /* Bits. */
 	u32 s; /* Set of bits. */
 };
<span class="p_chunk">@@ -466,361 +482,417 @@</span> <span class="p_context"> struct task_struct {</span>
 	 * For reasons of header soup (see current_thread_info()), this
 	 * must be the first element of task_struct.
 	 */
<span class="p_del">-	struct thread_info thread_info;</span>
<span class="p_add">+	struct thread_info		thread_info;</span>
 #endif
<span class="p_del">-	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */</span>
<span class="p_del">-	void *stack;</span>
<span class="p_del">-	atomic_t usage;</span>
<span class="p_del">-	unsigned int flags;	/* per process flags, defined below */</span>
<span class="p_del">-	unsigned int ptrace;</span>
<span class="p_add">+	/* -1 unrunnable, 0 runnable, &gt;0 stopped: */</span>
<span class="p_add">+	volatile long			state;</span>
<span class="p_add">+	void				*stack;</span>
<span class="p_add">+	atomic_t			usage;</span>
<span class="p_add">+	/* Per task flags (PF_*), defined further below: */</span>
<span class="p_add">+	unsigned int			flags;</span>
<span class="p_add">+	unsigned int			ptrace;</span>
 
 #ifdef CONFIG_SMP
<span class="p_del">-	struct llist_node wake_entry;</span>
<span class="p_del">-	int on_cpu;</span>
<span class="p_add">+	struct llist_node		wake_entry;</span>
<span class="p_add">+	int				on_cpu;</span>
 #ifdef CONFIG_THREAD_INFO_IN_TASK
<span class="p_del">-	unsigned int cpu;	/* current CPU */</span>
<span class="p_add">+	/* Current CPU: */</span>
<span class="p_add">+	unsigned int			cpu;</span>
 #endif
<span class="p_del">-	unsigned int wakee_flips;</span>
<span class="p_del">-	unsigned long wakee_flip_decay_ts;</span>
<span class="p_del">-	struct task_struct *last_wakee;</span>
<span class="p_add">+	unsigned int			wakee_flips;</span>
<span class="p_add">+	unsigned long			wakee_flip_decay_ts;</span>
<span class="p_add">+	struct task_struct		*last_wakee;</span>
 
<span class="p_del">-	int wake_cpu;</span>
<span class="p_add">+	int				wake_cpu;</span>
 #endif
<span class="p_del">-	int on_rq;</span>
<span class="p_add">+	int				on_rq;</span>
<span class="p_add">+</span>
<span class="p_add">+	int				prio;</span>
<span class="p_add">+	int				static_prio;</span>
<span class="p_add">+	int				normal_prio;</span>
<span class="p_add">+	unsigned int			rt_priority;</span>
 
<span class="p_del">-	int prio, static_prio, normal_prio;</span>
<span class="p_del">-	unsigned int rt_priority;</span>
<span class="p_del">-	const struct sched_class *sched_class;</span>
<span class="p_del">-	struct sched_entity se;</span>
<span class="p_del">-	struct sched_rt_entity rt;</span>
<span class="p_add">+	const struct sched_class	*sched_class;</span>
<span class="p_add">+	struct sched_entity		se;</span>
<span class="p_add">+	struct sched_rt_entity		rt;</span>
 #ifdef CONFIG_CGROUP_SCHED
<span class="p_del">-	struct task_group *sched_task_group;</span>
<span class="p_add">+	struct task_group		*sched_task_group;</span>
 #endif
<span class="p_del">-	struct sched_dl_entity dl;</span>
<span class="p_add">+	struct sched_dl_entity		dl;</span>
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
<span class="p_del">-	/* list of struct preempt_notifier: */</span>
<span class="p_del">-	struct hlist_head preempt_notifiers;</span>
<span class="p_add">+	/* List of struct preempt_notifier: */</span>
<span class="p_add">+	struct hlist_head		preempt_notifiers;</span>
 #endif
 
 #ifdef CONFIG_BLK_DEV_IO_TRACE
<span class="p_del">-	unsigned int btrace_seq;</span>
<span class="p_add">+	unsigned int			btrace_seq;</span>
 #endif
 
<span class="p_del">-	unsigned int policy;</span>
<span class="p_del">-	int nr_cpus_allowed;</span>
<span class="p_del">-	cpumask_t cpus_allowed;</span>
<span class="p_add">+	unsigned int			policy;</span>
<span class="p_add">+	int				nr_cpus_allowed;</span>
<span class="p_add">+	cpumask_t			cpus_allowed;</span>
 
 #ifdef CONFIG_PREEMPT_RCU
<span class="p_del">-	int rcu_read_lock_nesting;</span>
<span class="p_del">-	union rcu_special rcu_read_unlock_special;</span>
<span class="p_del">-	struct list_head rcu_node_entry;</span>
<span class="p_del">-	struct rcu_node *rcu_blocked_node;</span>
<span class="p_add">+	int				rcu_read_lock_nesting;</span>
<span class="p_add">+	union rcu_special		rcu_read_unlock_special;</span>
<span class="p_add">+	struct list_head		rcu_node_entry;</span>
<span class="p_add">+	struct rcu_node			*rcu_blocked_node;</span>
 #endif /* #ifdef CONFIG_PREEMPT_RCU */
<span class="p_add">+</span>
 #ifdef CONFIG_TASKS_RCU
<span class="p_del">-	unsigned long rcu_tasks_nvcsw;</span>
<span class="p_del">-	bool rcu_tasks_holdout;</span>
<span class="p_del">-	struct list_head rcu_tasks_holdout_list;</span>
<span class="p_del">-	int rcu_tasks_idle_cpu;</span>
<span class="p_add">+	unsigned long			rcu_tasks_nvcsw;</span>
<span class="p_add">+	bool				rcu_tasks_holdout;</span>
<span class="p_add">+	struct list_head		rcu_tasks_holdout_list;</span>
<span class="p_add">+	int				rcu_tasks_idle_cpu;</span>
 #endif /* #ifdef CONFIG_TASKS_RCU */
 
<span class="p_del">-	struct sched_info sched_info;</span>
<span class="p_add">+	struct sched_info		sched_info;</span>
 
<span class="p_del">-	struct list_head tasks;</span>
<span class="p_add">+	struct list_head		tasks;</span>
 #ifdef CONFIG_SMP
<span class="p_del">-	struct plist_node pushable_tasks;</span>
<span class="p_del">-	struct rb_node pushable_dl_tasks;</span>
<span class="p_add">+	struct plist_node		pushable_tasks;</span>
<span class="p_add">+	struct rb_node			pushable_dl_tasks;</span>
 #endif
 
<span class="p_del">-	struct mm_struct *mm, *active_mm;</span>
<span class="p_add">+	struct mm_struct		*mm;</span>
<span class="p_add">+	struct mm_struct		*active_mm;</span>
 
 	/* Per-thread vma caching: */
<span class="p_del">-	struct vmacache vmacache;</span>
<span class="p_del">-</span>
<span class="p_del">-#if defined(SPLIT_RSS_COUNTING)</span>
<span class="p_del">-	struct task_rss_stat	rss_stat;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-/* task state */</span>
<span class="p_del">-	int exit_state;</span>
<span class="p_del">-	int exit_code, exit_signal;</span>
<span class="p_del">-	int pdeath_signal;  /*  The signal sent when the parent dies  */</span>
<span class="p_del">-	unsigned long jobctl;	/* JOBCTL_*, siglock protected */</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Used for emulating ABI behavior of previous Linux versions */</span>
<span class="p_del">-	unsigned int personality;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* scheduler bits, serialized by scheduler locks */</span>
<span class="p_del">-	unsigned sched_reset_on_fork:1;</span>
<span class="p_del">-	unsigned sched_contributes_to_load:1;</span>
<span class="p_del">-	unsigned sched_migrated:1;</span>
<span class="p_del">-	unsigned sched_remote_wakeup:1;</span>
<span class="p_del">-	unsigned :0; /* force alignment to the next boundary */</span>
<span class="p_del">-</span>
<span class="p_del">-	/* unserialized, strictly &#39;current&#39; */</span>
<span class="p_del">-	unsigned in_execve:1; /* bit to tell LSMs we&#39;re in execve */</span>
<span class="p_del">-	unsigned in_iowait:1;</span>
<span class="p_del">-#if !defined(TIF_RESTORE_SIGMASK)</span>
<span class="p_del">-	unsigned restore_sigmask:1;</span>
<span class="p_add">+	struct vmacache			vmacache;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef SPLIT_RSS_COUNTING</span>
<span class="p_add">+	struct task_rss_stat		rss_stat;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	int				exit_state;</span>
<span class="p_add">+	int				exit_code;</span>
<span class="p_add">+	int				exit_signal;</span>
<span class="p_add">+	/* The signal sent when the parent dies: */</span>
<span class="p_add">+	int				pdeath_signal;</span>
<span class="p_add">+	/* JOBCTL_*, siglock protected: */</span>
<span class="p_add">+	unsigned long			jobctl;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Used for emulating ABI behavior of previous Linux versions: */</span>
<span class="p_add">+	unsigned int			personality;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Scheduler bits, serialized by scheduler locks: */</span>
<span class="p_add">+	unsigned			sched_reset_on_fork:1;</span>
<span class="p_add">+	unsigned			sched_contributes_to_load:1;</span>
<span class="p_add">+	unsigned			sched_migrated:1;</span>
<span class="p_add">+	unsigned			sched_remote_wakeup:1;</span>
<span class="p_add">+	/* Force alignment to the next boundary: */</span>
<span class="p_add">+	unsigned			:0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unserialized, strictly &#39;current&#39; */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Bit to tell LSMs we&#39;re in execve(): */</span>
<span class="p_add">+	unsigned			in_execve:1;</span>
<span class="p_add">+	unsigned			in_iowait:1;</span>
<span class="p_add">+#ifndef TIF_RESTORE_SIGMASK</span>
<span class="p_add">+	unsigned			restore_sigmask:1;</span>
 #endif
 #ifdef CONFIG_MEMCG
<span class="p_del">-	unsigned memcg_may_oom:1;</span>
<span class="p_add">+	unsigned			memcg_may_oom:1;</span>
 #ifndef CONFIG_SLOB
<span class="p_del">-	unsigned memcg_kmem_skip_account:1;</span>
<span class="p_add">+	unsigned			memcg_kmem_skip_account:1;</span>
 #endif
 #endif
 #ifdef CONFIG_COMPAT_BRK
<span class="p_del">-	unsigned brk_randomized:1;</span>
<span class="p_add">+	unsigned			brk_randomized:1;</span>
 #endif
 
<span class="p_del">-	unsigned long atomic_flags; /* Flags needing atomic access. */</span>
<span class="p_add">+	unsigned long			atomic_flags; /* Flags requiring atomic access. */</span>
 
<span class="p_del">-	struct restart_block restart_block;</span>
<span class="p_add">+	struct restart_block		restart_block;</span>
 
<span class="p_del">-	pid_t pid;</span>
<span class="p_del">-	pid_t tgid;</span>
<span class="p_add">+	pid_t				pid;</span>
<span class="p_add">+	pid_t				tgid;</span>
 
 #ifdef CONFIG_CC_STACKPROTECTOR
<span class="p_del">-	/* Canary value for the -fstack-protector gcc feature */</span>
<span class="p_del">-	unsigned long stack_canary;</span>
<span class="p_add">+	/* Canary value for the -fstack-protector GCC feature: */</span>
<span class="p_add">+	unsigned long			stack_canary;</span>
 #endif
 	/*
<span class="p_del">-	 * pointers to (original) parent process, youngest child, younger sibling,</span>
<span class="p_add">+	 * Pointers to the (original) parent process, youngest child, younger sibling,</span>
 	 * older sibling, respectively.  (p-&gt;father can be replaced with
 	 * p-&gt;real_parent-&gt;pid)
 	 */
<span class="p_del">-	struct task_struct __rcu *real_parent; /* real parent process */</span>
<span class="p_del">-	struct task_struct __rcu *parent; /* recipient of SIGCHLD, wait4() reports */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Real parent process: */</span>
<span class="p_add">+	struct task_struct __rcu	*real_parent;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Recipient of SIGCHLD, wait4() reports: */</span>
<span class="p_add">+	struct task_struct __rcu	*parent;</span>
<span class="p_add">+</span>
 	/*
<span class="p_del">-	 * children/sibling forms the list of my natural children</span>
<span class="p_add">+	 * Children/sibling form the list of natural children:</span>
 	 */
<span class="p_del">-	struct list_head children;	/* list of my children */</span>
<span class="p_del">-	struct list_head sibling;	/* linkage in my parent&#39;s children list */</span>
<span class="p_del">-	struct task_struct *group_leader;	/* threadgroup leader */</span>
<span class="p_add">+	struct list_head		children;</span>
<span class="p_add">+	struct list_head		sibling;</span>
<span class="p_add">+	struct task_struct		*group_leader;</span>
 
 	/*
<span class="p_del">-	 * ptraced is the list of tasks this task is using ptrace on.</span>
<span class="p_add">+	 * &#39;ptraced&#39; is the list of tasks this task is using ptrace() on.</span>
<span class="p_add">+	 *</span>
 	 * This includes both natural children and PTRACE_ATTACH targets.
<span class="p_del">-	 * p-&gt;ptrace_entry is p&#39;s link on the p-&gt;parent-&gt;ptraced list.</span>
<span class="p_add">+	 * &#39;ptrace_entry&#39; is this task&#39;s link on the p-&gt;parent-&gt;ptraced list.</span>
 	 */
<span class="p_del">-	struct list_head ptraced;</span>
<span class="p_del">-	struct list_head ptrace_entry;</span>
<span class="p_add">+	struct list_head		ptraced;</span>
<span class="p_add">+	struct list_head		ptrace_entry;</span>
 
 	/* PID/PID hash table linkage. */
<span class="p_del">-	struct pid_link pids[PIDTYPE_MAX];</span>
<span class="p_del">-	struct list_head thread_group;</span>
<span class="p_del">-	struct list_head thread_node;</span>
<span class="p_add">+	struct pid_link			pids[PIDTYPE_MAX];</span>
<span class="p_add">+	struct list_head		thread_group;</span>
<span class="p_add">+	struct list_head		thread_node;</span>
<span class="p_add">+</span>
<span class="p_add">+	struct completion		*vfork_done;</span>
 
<span class="p_del">-	struct completion *vfork_done;		/* for vfork() */</span>
<span class="p_del">-	int __user *set_child_tid;		/* CLONE_CHILD_SETTID */</span>
<span class="p_del">-	int __user *clear_child_tid;		/* CLONE_CHILD_CLEARTID */</span>
<span class="p_add">+	/* CLONE_CHILD_SETTID: */</span>
<span class="p_add">+	int __user			*set_child_tid;</span>
 
<span class="p_del">-	u64 utime, stime;</span>
<span class="p_add">+	/* CLONE_CHILD_CLEARTID: */</span>
<span class="p_add">+	int __user			*clear_child_tid;</span>
<span class="p_add">+</span>
<span class="p_add">+	u64				utime;</span>
<span class="p_add">+	u64				stime;</span>
 #ifdef CONFIG_ARCH_HAS_SCALED_CPUTIME
<span class="p_del">-	u64 utimescaled, stimescaled;</span>
<span class="p_add">+	u64				utimescaled;</span>
<span class="p_add">+	u64				stimescaled;</span>
 #endif
<span class="p_del">-	u64 gtime;</span>
<span class="p_del">-	struct prev_cputime prev_cputime;</span>
<span class="p_add">+	u64				gtime;</span>
<span class="p_add">+	struct prev_cputime		prev_cputime;</span>
 #ifdef CONFIG_VIRT_CPU_ACCOUNTING_GEN
<span class="p_del">-	seqcount_t vtime_seqcount;</span>
<span class="p_del">-	unsigned long long vtime_snap;</span>
<span class="p_add">+	seqcount_t			vtime_seqcount;</span>
<span class="p_add">+	unsigned long long		vtime_snap;</span>
 	enum {
<span class="p_del">-		/* Task is sleeping or running in a CPU with VTIME inactive */</span>
<span class="p_add">+		/* Task is sleeping or running in a CPU with VTIME inactive: */</span>
 		VTIME_INACTIVE = 0,
<span class="p_del">-		/* Task runs in userspace in a CPU with VTIME active */</span>
<span class="p_add">+		/* Task runs in userspace in a CPU with VTIME active: */</span>
 		VTIME_USER,
<span class="p_del">-		/* Task runs in kernelspace in a CPU with VTIME active */</span>
<span class="p_add">+		/* Task runs in kernelspace in a CPU with VTIME active: */</span>
 		VTIME_SYS,
 	} vtime_snap_whence;
 #endif
 
 #ifdef CONFIG_NO_HZ_FULL
<span class="p_del">-	atomic_t tick_dep_mask;</span>
<span class="p_add">+	atomic_t			tick_dep_mask;</span>
 #endif
<span class="p_del">-	unsigned long nvcsw, nivcsw; /* context switch counts */</span>
<span class="p_del">-	u64 start_time;		/* monotonic time in nsec */</span>
<span class="p_del">-	u64 real_start_time;	/* boot based time in nsec */</span>
<span class="p_del">-/* mm fault and swap info: this can arguably be seen as either mm-specific or thread-specific */</span>
<span class="p_del">-	unsigned long min_flt, maj_flt;</span>
<span class="p_add">+	/* Context switch counts: */</span>
<span class="p_add">+	unsigned long			nvcsw;</span>
<span class="p_add">+	unsigned long			nivcsw;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Monotonic time in nsecs: */</span>
<span class="p_add">+	u64				start_time;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Boot based time in nsecs: */</span>
<span class="p_add">+	u64				real_start_time;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* MM fault and swap info: this can arguably be seen as either mm-specific or thread-specific: */</span>
<span class="p_add">+	unsigned long			min_flt;</span>
<span class="p_add">+	unsigned long			maj_flt;</span>
 
 #ifdef CONFIG_POSIX_TIMERS
<span class="p_del">-	struct task_cputime cputime_expires;</span>
<span class="p_del">-	struct list_head cpu_timers[3];</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/* process credentials */</span>
<span class="p_del">-	const struct cred __rcu *ptracer_cred; /* Tracer&#39;s credentials at attach */</span>
<span class="p_del">-	const struct cred __rcu *real_cred; /* objective and real subjective task</span>
<span class="p_del">-					 * credentials (COW) */</span>
<span class="p_del">-	const struct cred __rcu *cred;	/* effective (overridable) subjective task</span>
<span class="p_del">-					 * credentials (COW) */</span>
<span class="p_del">-	char comm[TASK_COMM_LEN]; /* executable name excluding path</span>
<span class="p_del">-				     - access with [gs]et_task_comm (which lock</span>
<span class="p_del">-				       it with task_lock())</span>
<span class="p_del">-				     - initialized normally by setup_new_exec */</span>
<span class="p_del">-/* file system info */</span>
<span class="p_del">-	struct nameidata *nameidata;</span>
<span class="p_add">+	struct task_cputime		cputime_expires;</span>
<span class="p_add">+	struct list_head		cpu_timers[3];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Process credentials: */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Tracer&#39;s credentials at attach: */</span>
<span class="p_add">+	const struct cred __rcu		*ptracer_cred;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Objective and real subjective task credentials (COW): */</span>
<span class="p_add">+	const struct cred __rcu		*real_cred;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Effective (overridable) subjective task credentials (COW): */</span>
<span class="p_add">+	const struct cred __rcu		*cred;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * executable name, excluding path.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * - normally initialized setup_new_exec()</span>
<span class="p_add">+	 * - access it with [gs]et_task_comm()</span>
<span class="p_add">+	 * - lock it with task_lock()</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	char				comm[TASK_COMM_LEN];</span>
<span class="p_add">+</span>
<span class="p_add">+	struct nameidata		*nameidata;</span>
<span class="p_add">+</span>
 #ifdef CONFIG_SYSVIPC
<span class="p_del">-/* ipc stuff */</span>
<span class="p_del">-	struct sysv_sem sysvsem;</span>
<span class="p_del">-	struct sysv_shm sysvshm;</span>
<span class="p_add">+	struct sysv_sem			sysvsem;</span>
<span class="p_add">+	struct sysv_shm			sysvshm;</span>
 #endif
 #ifdef CONFIG_DETECT_HUNG_TASK
<span class="p_del">-/* hung task detection */</span>
<span class="p_del">-	unsigned long last_switch_count;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-/* filesystem information */</span>
<span class="p_del">-	struct fs_struct *fs;</span>
<span class="p_del">-/* open file information */</span>
<span class="p_del">-	struct files_struct *files;</span>
<span class="p_del">-/* namespaces */</span>
<span class="p_del">-	struct nsproxy *nsproxy;</span>
<span class="p_del">-/* signal handlers */</span>
<span class="p_del">-	struct signal_struct *signal;</span>
<span class="p_del">-	struct sighand_struct *sighand;</span>
<span class="p_del">-</span>
<span class="p_del">-	sigset_t blocked, real_blocked;</span>
<span class="p_del">-	sigset_t saved_sigmask;	/* restored if set_restore_sigmask() was used */</span>
<span class="p_del">-	struct sigpending pending;</span>
<span class="p_del">-</span>
<span class="p_del">-	unsigned long sas_ss_sp;</span>
<span class="p_del">-	size_t sas_ss_size;</span>
<span class="p_del">-	unsigned sas_ss_flags;</span>
<span class="p_del">-</span>
<span class="p_del">-	struct callback_head *task_works;</span>
<span class="p_del">-</span>
<span class="p_del">-	struct audit_context *audit_context;</span>
<span class="p_add">+	unsigned long			last_switch_count;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	/* Filesystem information: */</span>
<span class="p_add">+	struct fs_struct		*fs;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Open file information: */</span>
<span class="p_add">+	struct files_struct		*files;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Namespaces: */</span>
<span class="p_add">+	struct nsproxy			*nsproxy;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Signal handlers: */</span>
<span class="p_add">+	struct signal_struct		*signal;</span>
<span class="p_add">+	struct sighand_struct		*sighand;</span>
<span class="p_add">+	sigset_t			blocked;</span>
<span class="p_add">+	sigset_t			real_blocked;</span>
<span class="p_add">+	/* Restored if set_restore_sigmask() was used: */</span>
<span class="p_add">+	sigset_t			saved_sigmask;</span>
<span class="p_add">+	struct sigpending		pending;</span>
<span class="p_add">+	unsigned long			sas_ss_sp;</span>
<span class="p_add">+	size_t				sas_ss_size;</span>
<span class="p_add">+	unsigned int			sas_ss_flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	struct callback_head		*task_works;</span>
<span class="p_add">+</span>
<span class="p_add">+	struct audit_context		*audit_context;</span>
 #ifdef CONFIG_AUDITSYSCALL
<span class="p_del">-	kuid_t loginuid;</span>
<span class="p_del">-	unsigned int sessionid;</span>
<span class="p_add">+	kuid_t				loginuid;</span>
<span class="p_add">+	unsigned int			sessionid;</span>
 #endif
<span class="p_del">-	struct seccomp seccomp;</span>
<span class="p_add">+	struct seccomp			seccomp;</span>
 
<span class="p_del">-/* Thread group tracking */</span>
<span class="p_del">-   	u32 parent_exec_id;</span>
<span class="p_del">-   	u32 self_exec_id;</span>
<span class="p_del">-/* Protection of (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed,</span>
<span class="p_del">- * mempolicy */</span>
<span class="p_del">-	spinlock_t alloc_lock;</span>
<span class="p_add">+	/* Thread group tracking: */</span>
<span class="p_add">+	u32				parent_exec_id;</span>
<span class="p_add">+	u32				self_exec_id;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Protection against (de-)allocation: mm, files, fs, tty, keyrings, mems_allowed, mempolicy: */</span>
<span class="p_add">+	spinlock_t			alloc_lock;</span>
 
 	/* Protection of the PI data structures: */
<span class="p_del">-	raw_spinlock_t pi_lock;</span>
<span class="p_add">+	raw_spinlock_t			pi_lock;</span>
 
<span class="p_del">-	void *wake_q;</span>
<span class="p_add">+	void				*wake_q;</span>
 
 #ifdef CONFIG_RT_MUTEXES
<span class="p_del">-	/* PI waiters blocked on a rt_mutex held by this task */</span>
<span class="p_del">-	struct rb_root pi_waiters;</span>
<span class="p_del">-	struct rb_node *pi_waiters_leftmost;</span>
<span class="p_del">-	/* Deadlock detection and priority inheritance handling */</span>
<span class="p_del">-	struct rt_mutex_waiter *pi_blocked_on;</span>
<span class="p_add">+	/* PI waiters blocked on a rt_mutex held by this task: */</span>
<span class="p_add">+	struct rb_root			pi_waiters;</span>
<span class="p_add">+	struct rb_node			*pi_waiters_leftmost;</span>
<span class="p_add">+	/* Deadlock detection and priority inheritance handling: */</span>
<span class="p_add">+	struct rt_mutex_waiter		*pi_blocked_on;</span>
 #endif
 
 #ifdef CONFIG_DEBUG_MUTEXES
<span class="p_del">-	/* mutex deadlock detection */</span>
<span class="p_del">-	struct mutex_waiter *blocked_on;</span>
<span class="p_add">+	/* Mutex deadlock detection: */</span>
<span class="p_add">+	struct mutex_waiter		*blocked_on;</span>
 #endif
<span class="p_add">+</span>
 #ifdef CONFIG_TRACE_IRQFLAGS
<span class="p_del">-	unsigned int irq_events;</span>
<span class="p_del">-	unsigned long hardirq_enable_ip;</span>
<span class="p_del">-	unsigned long hardirq_disable_ip;</span>
<span class="p_del">-	unsigned int hardirq_enable_event;</span>
<span class="p_del">-	unsigned int hardirq_disable_event;</span>
<span class="p_del">-	int hardirqs_enabled;</span>
<span class="p_del">-	int hardirq_context;</span>
<span class="p_del">-	unsigned long softirq_disable_ip;</span>
<span class="p_del">-	unsigned long softirq_enable_ip;</span>
<span class="p_del">-	unsigned int softirq_disable_event;</span>
<span class="p_del">-	unsigned int softirq_enable_event;</span>
<span class="p_del">-	int softirqs_enabled;</span>
<span class="p_del">-	int softirq_context;</span>
<span class="p_add">+	unsigned int			irq_events;</span>
<span class="p_add">+	unsigned long			hardirq_enable_ip;</span>
<span class="p_add">+	unsigned long			hardirq_disable_ip;</span>
<span class="p_add">+	unsigned int			hardirq_enable_event;</span>
<span class="p_add">+	unsigned int			hardirq_disable_event;</span>
<span class="p_add">+	int				hardirqs_enabled;</span>
<span class="p_add">+	int				hardirq_context;</span>
<span class="p_add">+	unsigned long			softirq_disable_ip;</span>
<span class="p_add">+	unsigned long			softirq_enable_ip;</span>
<span class="p_add">+	unsigned int			softirq_disable_event;</span>
<span class="p_add">+	unsigned int			softirq_enable_event;</span>
<span class="p_add">+	int				softirqs_enabled;</span>
<span class="p_add">+	int				softirq_context;</span>
 #endif
<span class="p_add">+</span>
 #ifdef CONFIG_LOCKDEP
<span class="p_del">-# define MAX_LOCK_DEPTH 48UL</span>
<span class="p_del">-	u64 curr_chain_key;</span>
<span class="p_del">-	int lockdep_depth;</span>
<span class="p_del">-	unsigned int lockdep_recursion;</span>
<span class="p_del">-	struct held_lock held_locks[MAX_LOCK_DEPTH];</span>
<span class="p_del">-	gfp_t lockdep_reclaim_gfp;</span>
<span class="p_add">+# define MAX_LOCK_DEPTH			48UL</span>
<span class="p_add">+	u64				curr_chain_key;</span>
<span class="p_add">+	int				lockdep_depth;</span>
<span class="p_add">+	unsigned int			lockdep_recursion;</span>
<span class="p_add">+	struct held_lock		held_locks[MAX_LOCK_DEPTH];</span>
<span class="p_add">+	gfp_t				lockdep_reclaim_gfp;</span>
 #endif
<span class="p_add">+</span>
 #ifdef CONFIG_UBSAN
<span class="p_del">-	unsigned int in_ubsan;</span>
<span class="p_add">+	unsigned int			in_ubsan;</span>
 #endif
 
<span class="p_del">-/* journalling filesystem info */</span>
<span class="p_del">-	void *journal_info;</span>
<span class="p_add">+	/* Journalling filesystem info: */</span>
<span class="p_add">+	void				*journal_info;</span>
 
<span class="p_del">-/* stacked block device info */</span>
<span class="p_del">-	struct bio_list *bio_list;</span>
<span class="p_add">+	/* Stacked block device info: */</span>
<span class="p_add">+	struct bio_list			*bio_list;</span>
 
 #ifdef CONFIG_BLOCK
<span class="p_del">-/* stack plugging */</span>
<span class="p_del">-	struct blk_plug *plug;</span>
<span class="p_add">+	/* Stack plugging: */</span>
<span class="p_add">+	struct blk_plug			*plug;</span>
 #endif
 
<span class="p_del">-/* VM state */</span>
<span class="p_del">-	struct reclaim_state *reclaim_state;</span>
<span class="p_add">+	/* VM state: */</span>
<span class="p_add">+	struct reclaim_state		*reclaim_state;</span>
<span class="p_add">+</span>
<span class="p_add">+	struct backing_dev_info		*backing_dev_info;</span>
 
<span class="p_del">-	struct backing_dev_info *backing_dev_info;</span>
<span class="p_add">+	struct io_context		*io_context;</span>
 
<span class="p_del">-	struct io_context *io_context;</span>
<span class="p_add">+	/* Ptrace state: */</span>
<span class="p_add">+	unsigned long			ptrace_message;</span>
<span class="p_add">+	siginfo_t			*last_siginfo;</span>
 
<span class="p_del">-	unsigned long ptrace_message;</span>
<span class="p_del">-	siginfo_t *last_siginfo; /* For ptrace use.  */</span>
<span class="p_del">-	struct task_io_accounting ioac;</span>
<span class="p_del">-#if defined(CONFIG_TASK_XACCT)</span>
<span class="p_del">-	u64 acct_rss_mem1;	/* accumulated rss usage */</span>
<span class="p_del">-	u64 acct_vm_mem1;	/* accumulated virtual memory usage */</span>
<span class="p_del">-	u64 acct_timexpd;	/* stime + utime since last update */</span>
<span class="p_add">+	struct task_io_accounting	ioac;</span>
<span class="p_add">+#ifdef CONFIG_TASK_XACCT</span>
<span class="p_add">+	/* Accumulated RSS usage: */</span>
<span class="p_add">+	u64				acct_rss_mem1;</span>
<span class="p_add">+	/* Accumulated virtual memory usage: */</span>
<span class="p_add">+	u64				acct_vm_mem1;</span>
<span class="p_add">+	/* stime + utime since last update: */</span>
<span class="p_add">+	u64				acct_timexpd;</span>
 #endif
 #ifdef CONFIG_CPUSETS
<span class="p_del">-	nodemask_t mems_allowed;	/* Protected by alloc_lock */</span>
<span class="p_del">-	seqcount_t mems_allowed_seq;	/* Seqence no to catch updates */</span>
<span class="p_del">-	int cpuset_mem_spread_rotor;</span>
<span class="p_del">-	int cpuset_slab_spread_rotor;</span>
<span class="p_add">+	/* Protected by -&gt;alloc_lock: */</span>
<span class="p_add">+	nodemask_t			mems_allowed;</span>
<span class="p_add">+	/* Seqence number to catch updates: */</span>
<span class="p_add">+	seqcount_t			mems_allowed_seq;</span>
<span class="p_add">+	int				cpuset_mem_spread_rotor;</span>
<span class="p_add">+	int				cpuset_slab_spread_rotor;</span>
 #endif
 #ifdef CONFIG_CGROUPS
<span class="p_del">-	/* Control Group info protected by css_set_lock */</span>
<span class="p_del">-	struct css_set __rcu *cgroups;</span>
<span class="p_del">-	/* cg_list protected by css_set_lock and tsk-&gt;alloc_lock */</span>
<span class="p_del">-	struct list_head cg_list;</span>
<span class="p_add">+	/* Control Group info protected by css_set_lock: */</span>
<span class="p_add">+	struct css_set __rcu		*cgroups;</span>
<span class="p_add">+	/* cg_list protected by css_set_lock and tsk-&gt;alloc_lock: */</span>
<span class="p_add">+	struct list_head		cg_list;</span>
 #endif
 #ifdef CONFIG_INTEL_RDT_A
<span class="p_del">-	int closid;</span>
<span class="p_add">+	int				closid;</span>
 #endif
 #ifdef CONFIG_FUTEX
<span class="p_del">-	struct robust_list_head __user *robust_list;</span>
<span class="p_add">+	struct robust_list_head __user	*robust_list;</span>
 #ifdef CONFIG_COMPAT
 	struct compat_robust_list_head __user *compat_robust_list;
 #endif
<span class="p_del">-	struct list_head pi_state_list;</span>
<span class="p_del">-	struct futex_pi_state *pi_state_cache;</span>
<span class="p_add">+	struct list_head		pi_state_list;</span>
<span class="p_add">+	struct futex_pi_state		*pi_state_cache;</span>
 #endif
 #ifdef CONFIG_PERF_EVENTS
<span class="p_del">-	struct perf_event_context *perf_event_ctxp[perf_nr_task_contexts];</span>
<span class="p_del">-	struct mutex perf_event_mutex;</span>
<span class="p_del">-	struct list_head perf_event_list;</span>
<span class="p_add">+	struct perf_event_context	*perf_event_ctxp[perf_nr_task_contexts];</span>
<span class="p_add">+	struct mutex			perf_event_mutex;</span>
<span class="p_add">+	struct list_head		perf_event_list;</span>
 #endif
 #ifdef CONFIG_DEBUG_PREEMPT
<span class="p_del">-	unsigned long preempt_disable_ip;</span>
<span class="p_add">+	unsigned long			preempt_disable_ip;</span>
 #endif
 #ifdef CONFIG_NUMA
<span class="p_del">-	struct mempolicy *mempolicy;	/* Protected by alloc_lock */</span>
<span class="p_del">-	short il_next;</span>
<span class="p_del">-	short pref_node_fork;</span>
<span class="p_add">+	/* Protected by alloc_lock: */</span>
<span class="p_add">+	struct mempolicy		*mempolicy;</span>
<span class="p_add">+	short				il_next;</span>
<span class="p_add">+	short				pref_node_fork;</span>
 #endif
 #ifdef CONFIG_NUMA_BALANCING
<span class="p_del">-	int numa_scan_seq;</span>
<span class="p_del">-	unsigned int numa_scan_period;</span>
<span class="p_del">-	unsigned int numa_scan_period_max;</span>
<span class="p_del">-	int numa_preferred_nid;</span>
<span class="p_del">-	unsigned long numa_migrate_retry;</span>
<span class="p_del">-	u64 node_stamp;			/* migration stamp  */</span>
<span class="p_del">-	u64 last_task_numa_placement;</span>
<span class="p_del">-	u64 last_sum_exec_runtime;</span>
<span class="p_del">-	struct callback_head numa_work;</span>
<span class="p_del">-</span>
<span class="p_del">-	struct list_head numa_entry;</span>
<span class="p_del">-	struct numa_group *numa_group;</span>
<span class="p_add">+	int				numa_scan_seq;</span>
<span class="p_add">+	unsigned int			numa_scan_period;</span>
<span class="p_add">+	unsigned int			numa_scan_period_max;</span>
<span class="p_add">+	int				numa_preferred_nid;</span>
<span class="p_add">+	unsigned long			numa_migrate_retry;</span>
<span class="p_add">+	/* Migration stamp: */</span>
<span class="p_add">+	u64				node_stamp;</span>
<span class="p_add">+	u64				last_task_numa_placement;</span>
<span class="p_add">+	u64				last_sum_exec_runtime;</span>
<span class="p_add">+	struct callback_head		numa_work;</span>
<span class="p_add">+</span>
<span class="p_add">+	struct list_head		numa_entry;</span>
<span class="p_add">+	struct numa_group		*numa_group;</span>
 
 	/*
 	 * numa_faults is an array split into four regions:
<span class="p_chunk">@@ -836,8 +908,8 @@</span> <span class="p_context"> struct task_struct {</span>
 	 * during the current scan window. When the scan completes, the counts
 	 * in faults_memory and faults_cpu decay and these values are copied.
 	 */
<span class="p_del">-	unsigned long *numa_faults;</span>
<span class="p_del">-	unsigned long total_numa_faults;</span>
<span class="p_add">+	unsigned long			*numa_faults;</span>
<span class="p_add">+	unsigned long			total_numa_faults;</span>
 
 	/*
 	 * numa_faults_locality tracks if faults recorded during the last
<span class="p_chunk">@@ -845,117 +917,130 @@</span> <span class="p_context"> struct task_struct {</span>
 	 * period is adapted based on the locality of the faults with different
 	 * weights depending on whether they were shared or private faults
 	 */
<span class="p_del">-	unsigned long numa_faults_locality[3];</span>
<span class="p_add">+	unsigned long			numa_faults_locality[3];</span>
 
<span class="p_del">-	unsigned long numa_pages_migrated;</span>
<span class="p_add">+	unsigned long			numa_pages_migrated;</span>
 #endif /* CONFIG_NUMA_BALANCING */
 
<span class="p_del">-	struct tlbflush_unmap_batch tlb_ubc;</span>
<span class="p_add">+	struct tlbflush_unmap_batch	tlb_ubc;</span>
 
<span class="p_del">-	struct rcu_head rcu;</span>
<span class="p_add">+	struct rcu_head			rcu;</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * cache last used pipe for splice</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	struct pipe_inode_info *splice_pipe;</span>
<span class="p_add">+	/* Cache last used pipe for splice(): */</span>
<span class="p_add">+	struct pipe_inode_info		*splice_pipe;</span>
 
<span class="p_del">-	struct page_frag task_frag;</span>
<span class="p_add">+	struct page_frag		task_frag;</span>
 
<span class="p_del">-	struct task_delay_info *delays;</span>
<span class="p_add">+	struct task_delay_info		*delays;</span>
 
 #ifdef CONFIG_FAULT_INJECTION
<span class="p_del">-	int make_it_fail;</span>
<span class="p_add">+	int				make_it_fail;</span>
 #endif
 	/*
<span class="p_del">-	 * when (nr_dirtied &gt;= nr_dirtied_pause), it&#39;s time to call</span>
<span class="p_del">-	 * balance_dirty_pages() for some dirty throttling pause</span>
<span class="p_add">+	 * When (nr_dirtied &gt;= nr_dirtied_pause), it&#39;s time to call</span>
<span class="p_add">+	 * balance_dirty_pages() for a dirty throttling pause:</span>
 	 */
<span class="p_del">-	int nr_dirtied;</span>
<span class="p_del">-	int nr_dirtied_pause;</span>
<span class="p_del">-	unsigned long dirty_paused_when; /* start of a write-and-pause period */</span>
<span class="p_add">+	int				nr_dirtied;</span>
<span class="p_add">+	int				nr_dirtied_pause;</span>
<span class="p_add">+	/* Start of a write-and-pause period: */</span>
<span class="p_add">+	unsigned long			dirty_paused_when;</span>
 
 #ifdef CONFIG_LATENCYTOP
<span class="p_del">-	int latency_record_count;</span>
<span class="p_del">-	struct latency_record latency_record[LT_SAVECOUNT];</span>
<span class="p_add">+	int				latency_record_count;</span>
<span class="p_add">+	struct latency_record		latency_record[LT_SAVECOUNT];</span>
 #endif
 	/*
<span class="p_del">-	 * time slack values; these are used to round up poll() and</span>
<span class="p_add">+	 * Time slack values; these are used to round up poll() and</span>
 	 * select() etc timeout values. These are in nanoseconds.
 	 */
<span class="p_del">-	u64 timer_slack_ns;</span>
<span class="p_del">-	u64 default_timer_slack_ns;</span>
<span class="p_add">+	u64				timer_slack_ns;</span>
<span class="p_add">+	u64				default_timer_slack_ns;</span>
 
 #ifdef CONFIG_KASAN
<span class="p_del">-	unsigned int kasan_depth;</span>
<span class="p_add">+	unsigned int			kasan_depth;</span>
 #endif
<span class="p_add">+</span>
 #ifdef CONFIG_FUNCTION_GRAPH_TRACER
<span class="p_del">-	/* Index of current stored address in ret_stack */</span>
<span class="p_del">-	int curr_ret_stack;</span>
<span class="p_del">-	/* Stack of return addresses for return function tracing */</span>
<span class="p_del">-	struct ftrace_ret_stack	*ret_stack;</span>
<span class="p_del">-	/* time stamp for last schedule */</span>
<span class="p_del">-	unsigned long long ftrace_timestamp;</span>
<span class="p_add">+	/* Index of current stored address in ret_stack: */</span>
<span class="p_add">+	int				curr_ret_stack;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Stack of return addresses for return function tracing: */</span>
<span class="p_add">+	struct ftrace_ret_stack		*ret_stack;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Timestamp for last schedule: */</span>
<span class="p_add">+	unsigned long long		ftrace_timestamp;</span>
<span class="p_add">+</span>
 	/*
 	 * Number of functions that haven&#39;t been traced
<span class="p_del">-	 * because of depth overrun.</span>
<span class="p_add">+	 * because of depth overrun:</span>
 	 */
<span class="p_del">-	atomic_t trace_overrun;</span>
<span class="p_del">-	/* Pause for the tracing */</span>
<span class="p_del">-	atomic_t tracing_graph_pause;</span>
<span class="p_add">+	atomic_t			trace_overrun;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Pause tracing: */</span>
<span class="p_add">+	atomic_t			tracing_graph_pause;</span>
 #endif
<span class="p_add">+</span>
 #ifdef CONFIG_TRACING
<span class="p_del">-	/* state flags for use by tracers */</span>
<span class="p_del">-	unsigned long trace;</span>
<span class="p_del">-	/* bitmask and counter of trace recursion */</span>
<span class="p_del">-	unsigned long trace_recursion;</span>
<span class="p_add">+	/* State flags for use by tracers: */</span>
<span class="p_add">+	unsigned long			trace;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Bitmask and counter of trace recursion: */</span>
<span class="p_add">+	unsigned long			trace_recursion;</span>
 #endif /* CONFIG_TRACING */
<span class="p_add">+</span>
 #ifdef CONFIG_KCOV
<span class="p_del">-	/* Coverage collection mode enabled for this task (0 if disabled). */</span>
<span class="p_del">-	enum kcov_mode kcov_mode;</span>
<span class="p_del">-	/* Size of the kcov_area. */</span>
<span class="p_del">-	unsigned	kcov_size;</span>
<span class="p_del">-	/* Buffer for coverage collection. */</span>
<span class="p_del">-	void		*kcov_area;</span>
<span class="p_del">-	/* kcov desciptor wired with this task or NULL. */</span>
<span class="p_del">-	struct kcov	*kcov;</span>
<span class="p_add">+	/* Coverage collection mode enabled for this task (0 if disabled): */</span>
<span class="p_add">+	enum kcov_mode			kcov_mode;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Size of the kcov_area: */</span>
<span class="p_add">+	unsigned int			kcov_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Buffer for coverage collection: */</span>
<span class="p_add">+	void				*kcov_area;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* KCOV descriptor wired with this task or NULL: */</span>
<span class="p_add">+	struct kcov			*kcov;</span>
 #endif
<span class="p_add">+</span>
 #ifdef CONFIG_MEMCG
<span class="p_del">-	struct mem_cgroup *memcg_in_oom;</span>
<span class="p_del">-	gfp_t memcg_oom_gfp_mask;</span>
<span class="p_del">-	int memcg_oom_order;</span>
<span class="p_add">+	struct mem_cgroup		*memcg_in_oom;</span>
<span class="p_add">+	gfp_t				memcg_oom_gfp_mask;</span>
<span class="p_add">+	int				memcg_oom_order;</span>
 
<span class="p_del">-	/* number of pages to reclaim on returning to userland */</span>
<span class="p_del">-	unsigned int memcg_nr_pages_over_high;</span>
<span class="p_add">+	/* Number of pages to reclaim on returning to userland: */</span>
<span class="p_add">+	unsigned int			memcg_nr_pages_over_high;</span>
 #endif
<span class="p_add">+</span>
 #ifdef CONFIG_UPROBES
<span class="p_del">-	struct uprobe_task *utask;</span>
<span class="p_add">+	struct uprobe_task		*utask;</span>
 #endif
 #if defined(CONFIG_BCACHE) || defined(CONFIG_BCACHE_MODULE)
<span class="p_del">-	unsigned int	sequential_io;</span>
<span class="p_del">-	unsigned int	sequential_io_avg;</span>
<span class="p_add">+	unsigned int			sequential_io;</span>
<span class="p_add">+	unsigned int			sequential_io_avg;</span>
 #endif
 #ifdef CONFIG_DEBUG_ATOMIC_SLEEP
<span class="p_del">-	unsigned long	task_state_change;</span>
<span class="p_add">+	unsigned long			task_state_change;</span>
 #endif
<span class="p_del">-	int pagefault_disabled;</span>
<span class="p_add">+	int				pagefault_disabled;</span>
 #ifdef CONFIG_MMU
<span class="p_del">-	struct task_struct *oom_reaper_list;</span>
<span class="p_add">+	struct task_struct		*oom_reaper_list;</span>
 #endif
 #ifdef CONFIG_VMAP_STACK
<span class="p_del">-	struct vm_struct *stack_vm_area;</span>
<span class="p_add">+	struct vm_struct		*stack_vm_area;</span>
 #endif
 #ifdef CONFIG_THREAD_INFO_IN_TASK
<span class="p_del">-	/* A live task holds one reference. */</span>
<span class="p_del">-	atomic_t stack_refcount;</span>
<span class="p_add">+	/* A live task holds one reference: */</span>
<span class="p_add">+	atomic_t			stack_refcount;</span>
 #endif
<span class="p_del">-/* CPU-specific state of this task */</span>
<span class="p_del">-	struct thread_struct thread;</span>
<span class="p_del">-/*</span>
<span class="p_del">- * WARNING: on x86, &#39;thread_struct&#39; contains a variable-sized</span>
<span class="p_del">- * structure.  It *MUST* be at the end of &#39;task_struct&#39;.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Do not put anything below here!</span>
<span class="p_del">- */</span>
<span class="p_add">+	/* CPU-specific state of this task: */</span>
<span class="p_add">+	struct thread_struct		thread;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * WARNING: on x86, &#39;thread_struct&#39; contains a variable-sized</span>
<span class="p_add">+	 * structure.  It *MUST* be at the end of &#39;task_struct&#39;.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Do not put anything below here!</span>
<span class="p_add">+	 */</span>
 };
 
 static inline struct pid *task_pid(struct task_struct *task)
<span class="p_chunk">@@ -969,7 +1054,7 @@</span> <span class="p_context"> static inline struct pid *task_tgid(struct task_struct *task)</span>
 }
 
 /*
<span class="p_del">- * Without tasklist or rcu lock it is not safe to dereference</span>
<span class="p_add">+ * Without tasklist or RCU lock it is not safe to dereference</span>
  * the result of task_pgrp/task_session even if task == current,
  * we can race with another thread doing sys_setsid/sys_setpgid.
  */
<span class="p_chunk">@@ -996,16 +1081,14 @@</span> <span class="p_context"> static inline struct pid *task_session(struct task_struct *task)</span>
  *
  * see also pid_nr() etc in include/linux/pid.h
  */
<span class="p_del">-pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type,</span>
<span class="p_del">-			struct pid_namespace *ns);</span>
<span class="p_add">+pid_t __task_pid_nr_ns(struct task_struct *task, enum pid_type type, struct pid_namespace *ns);</span>
 
 static inline pid_t task_pid_nr(struct task_struct *tsk)
 {
 	return tsk-&gt;pid;
 }
 
<span class="p_del">-static inline pid_t task_pid_nr_ns(struct task_struct *tsk,</span>
<span class="p_del">-					struct pid_namespace *ns)</span>
<span class="p_add">+static inline pid_t task_pid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)</span>
 {
 	return __task_pid_nr_ns(tsk, PIDTYPE_PID, ns);
 }
<span class="p_chunk">@@ -1021,15 +1104,28 @@</span> <span class="p_context"> static inline pid_t task_tgid_nr(struct task_struct *tsk)</span>
 	return tsk-&gt;tgid;
 }
 
<span class="p_del">-pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);</span>
<span class="p_add">+extern pid_t task_tgid_nr_ns(struct task_struct *tsk, struct pid_namespace *ns);</span>
 
 static inline pid_t task_tgid_vnr(struct task_struct *tsk)
 {
 	return pid_vnr(task_tgid(tsk));
 }
 
<span class="p_add">+/**</span>
<span class="p_add">+ * pid_alive - check that a task structure is not stale</span>
<span class="p_add">+ * @p: Task structure to be checked.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Test if a process is not yet dead (at most zombie state)</span>
<span class="p_add">+ * If pid_alive fails, then pointers within the task structure</span>
<span class="p_add">+ * can be stale and must not be dereferenced.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Return: 1 if the process is alive. 0 otherwise.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int pid_alive(const struct task_struct *p)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return p-&gt;pids[PIDTYPE_PID].pid != NULL;</span>
<span class="p_add">+}</span>
 
<span class="p_del">-static inline int pid_alive(const struct task_struct *p);</span>
 static inline pid_t task_ppid_nr_ns(const struct task_struct *tsk, struct pid_namespace *ns)
 {
 	pid_t pid = 0;
<span class="p_chunk">@@ -1047,8 +1143,7 @@</span> <span class="p_context"> static inline pid_t task_ppid_nr(const struct task_struct *tsk)</span>
 	return task_ppid_nr_ns(tsk, &amp;init_pid_ns);
 }
 
<span class="p_del">-static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk,</span>
<span class="p_del">-					struct pid_namespace *ns)</span>
<span class="p_add">+static inline pid_t task_pgrp_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)</span>
 {
 	return __task_pid_nr_ns(tsk, PIDTYPE_PGID, ns);
 }
<span class="p_chunk">@@ -1059,8 +1154,7 @@</span> <span class="p_context"> static inline pid_t task_pgrp_vnr(struct task_struct *tsk)</span>
 }
 
 
<span class="p_del">-static inline pid_t task_session_nr_ns(struct task_struct *tsk,</span>
<span class="p_del">-					struct pid_namespace *ns)</span>
<span class="p_add">+static inline pid_t task_session_nr_ns(struct task_struct *tsk, struct pid_namespace *ns)</span>
 {
 	return __task_pid_nr_ns(tsk, PIDTYPE_SID, ns);
 }
<span class="p_chunk">@@ -1070,28 +1164,13 @@</span> <span class="p_context"> static inline pid_t task_session_vnr(struct task_struct *tsk)</span>
 	return __task_pid_nr_ns(tsk, PIDTYPE_SID, NULL);
 }
 
<span class="p_del">-/* obsolete, do not use */</span>
<span class="p_add">+/* Obsolete, do not use: */</span>
 static inline pid_t task_pgrp_nr(struct task_struct *tsk)
 {
 	return task_pgrp_nr_ns(tsk, &amp;init_pid_ns);
 }
 
 /**
<span class="p_del">- * pid_alive - check that a task structure is not stale</span>
<span class="p_del">- * @p: Task structure to be checked.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Test if a process is not yet dead (at most zombie state)</span>
<span class="p_del">- * If pid_alive fails, then pointers within the task structure</span>
<span class="p_del">- * can be stale and must not be dereferenced.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Return: 1 if the process is alive. 0 otherwise.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline int pid_alive(const struct task_struct *p)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return p-&gt;pids[PIDTYPE_PID].pid != NULL;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/**</span>
  * is_global_init - check if a task structure is init. Since init
  * is free to have sub-threads we need to check tgid.
  * @tsk: Task structure to be checked.
<span class="p_chunk">@@ -1110,34 +1189,34 @@</span> <span class="p_context"> extern struct pid *cad_pid;</span>
 /*
  * Per process flags
  */
<span class="p_del">-#define PF_IDLE		0x00000002	/* I am an IDLE thread */</span>
<span class="p_del">-#define PF_EXITING	0x00000004	/* getting shut down */</span>
<span class="p_del">-#define PF_EXITPIDONE	0x00000008	/* pi exit done on shut down */</span>
<span class="p_del">-#define PF_VCPU		0x00000010	/* I&#39;m a virtual CPU */</span>
<span class="p_del">-#define PF_WQ_WORKER	0x00000020	/* I&#39;m a workqueue worker */</span>
<span class="p_del">-#define PF_FORKNOEXEC	0x00000040	/* forked but didn&#39;t exec */</span>
<span class="p_del">-#define PF_MCE_PROCESS  0x00000080      /* process policy on mce errors */</span>
<span class="p_del">-#define PF_SUPERPRIV	0x00000100	/* used super-user privileges */</span>
<span class="p_del">-#define PF_DUMPCORE	0x00000200	/* dumped core */</span>
<span class="p_del">-#define PF_SIGNALED	0x00000400	/* killed by a signal */</span>
<span class="p_del">-#define PF_MEMALLOC	0x00000800	/* Allocating memory */</span>
<span class="p_del">-#define PF_NPROC_EXCEEDED 0x00001000	/* set_user noticed that RLIMIT_NPROC was exceeded */</span>
<span class="p_del">-#define PF_USED_MATH	0x00002000	/* if unset the fpu must be initialized before use */</span>
<span class="p_del">-#define PF_USED_ASYNC	0x00004000	/* used async_schedule*(), used by module init */</span>
<span class="p_del">-#define PF_NOFREEZE	0x00008000	/* this thread should not be frozen */</span>
<span class="p_del">-#define PF_FROZEN	0x00010000	/* frozen for system suspend */</span>
<span class="p_del">-#define PF_FSTRANS	0x00020000	/* inside a filesystem transaction */</span>
<span class="p_del">-#define PF_KSWAPD	0x00040000	/* I am kswapd */</span>
<span class="p_del">-#define PF_MEMALLOC_NOIO 0x00080000	/* Allocating memory without IO involved */</span>
<span class="p_del">-#define PF_LESS_THROTTLE 0x00100000	/* Throttle me less: I clean memory */</span>
<span class="p_del">-#define PF_KTHREAD	0x00200000	/* I am a kernel thread */</span>
<span class="p_del">-#define PF_RANDOMIZE	0x00400000	/* randomize virtual address space */</span>
<span class="p_del">-#define PF_SWAPWRITE	0x00800000	/* Allowed to write to swap */</span>
<span class="p_del">-#define PF_NO_SETAFFINITY 0x04000000	/* Userland is not allowed to meddle with cpus_allowed */</span>
<span class="p_del">-#define PF_MCE_EARLY    0x08000000      /* Early kill for mce process policy */</span>
<span class="p_del">-#define PF_MUTEX_TESTER	0x20000000	/* Thread belongs to the rt mutex tester */</span>
<span class="p_del">-#define PF_FREEZER_SKIP	0x40000000	/* Freezer should not count it as freezable */</span>
<span class="p_del">-#define PF_SUSPEND_TASK 0x80000000      /* this thread called freeze_processes and should not be frozen */</span>
<span class="p_add">+#define PF_IDLE			0x00000002	/* I am an IDLE thread */</span>
<span class="p_add">+#define PF_EXITING		0x00000004	/* Getting shut down */</span>
<span class="p_add">+#define PF_EXITPIDONE		0x00000008	/* PI exit done on shut down */</span>
<span class="p_add">+#define PF_VCPU			0x00000010	/* I&#39;m a virtual CPU */</span>
<span class="p_add">+#define PF_WQ_WORKER		0x00000020	/* I&#39;m a workqueue worker */</span>
<span class="p_add">+#define PF_FORKNOEXEC		0x00000040	/* Forked but didn&#39;t exec */</span>
<span class="p_add">+#define PF_MCE_PROCESS		0x00000080      /* Process policy on mce errors */</span>
<span class="p_add">+#define PF_SUPERPRIV		0x00000100	/* Used super-user privileges */</span>
<span class="p_add">+#define PF_DUMPCORE		0x00000200	/* Dumped core */</span>
<span class="p_add">+#define PF_SIGNALED		0x00000400	/* Killed by a signal */</span>
<span class="p_add">+#define PF_MEMALLOC		0x00000800	/* Allocating memory */</span>
<span class="p_add">+#define PF_NPROC_EXCEEDED	0x00001000	/* set_user() noticed that RLIMIT_NPROC was exceeded */</span>
<span class="p_add">+#define PF_USED_MATH		0x00002000	/* If unset the fpu must be initialized before use */</span>
<span class="p_add">+#define PF_USED_ASYNC		0x00004000	/* Used async_schedule*(), used by module init */</span>
<span class="p_add">+#define PF_NOFREEZE		0x00008000	/* This thread should not be frozen */</span>
<span class="p_add">+#define PF_FROZEN		0x00010000	/* Frozen for system suspend */</span>
<span class="p_add">+#define PF_FSTRANS		0x00020000	/* Inside a filesystem transaction */</span>
<span class="p_add">+#define PF_KSWAPD		0x00040000	/* I am kswapd */</span>
<span class="p_add">+#define PF_MEMALLOC_NOIO	0x00080000	/* Allocating memory without IO involved */</span>
<span class="p_add">+#define PF_LESS_THROTTLE	0x00100000	/* Throttle me less: I clean memory */</span>
<span class="p_add">+#define PF_KTHREAD		0x00200000	/* I am a kernel thread */</span>
<span class="p_add">+#define PF_RANDOMIZE		0x00400000	/* Randomize virtual address space */</span>
<span class="p_add">+#define PF_SWAPWRITE		0x00800000	/* Allowed to write to swap */</span>
<span class="p_add">+#define PF_NO_SETAFFINITY	0x04000000	/* Userland is not allowed to meddle with cpus_allowed */</span>
<span class="p_add">+#define PF_MCE_EARLY		0x08000000      /* Early kill for mce process policy */</span>
<span class="p_add">+#define PF_MUTEX_TESTER		0x20000000	/* Thread belongs to the rt mutex tester */</span>
<span class="p_add">+#define PF_FREEZER_SKIP		0x40000000	/* Freezer should not count it as freezable */</span>
<span class="p_add">+#define PF_SUSPEND_TASK		0x80000000      /* This thread called freeze_processes() and should not be frozen */</span>
 
 /*
  * Only the _current_ task can read/write to tsk-&gt;flags, but other
<span class="p_chunk">@@ -1150,33 +1229,38 @@</span> <span class="p_context"> extern struct pid *cad_pid;</span>
  * child is not running and in turn not changing child-&gt;flags
  * at the same time the parent does it.
  */
<span class="p_del">-#define clear_stopped_child_used_math(child) do { (child)-&gt;flags &amp;= ~PF_USED_MATH; } while (0)</span>
<span class="p_del">-#define set_stopped_child_used_math(child) do { (child)-&gt;flags |= PF_USED_MATH; } while (0)</span>
<span class="p_del">-#define clear_used_math() clear_stopped_child_used_math(current)</span>
<span class="p_del">-#define set_used_math() set_stopped_child_used_math(current)</span>
<span class="p_add">+#define clear_stopped_child_used_math(child)	do { (child)-&gt;flags &amp;= ~PF_USED_MATH; } while (0)</span>
<span class="p_add">+#define set_stopped_child_used_math(child)	do { (child)-&gt;flags |= PF_USED_MATH; } while (0)</span>
<span class="p_add">+#define clear_used_math()			clear_stopped_child_used_math(current)</span>
<span class="p_add">+#define set_used_math()				set_stopped_child_used_math(current)</span>
<span class="p_add">+</span>
 #define conditional_stopped_child_used_math(condition, child) \
 	do { (child)-&gt;flags &amp;= ~PF_USED_MATH, (child)-&gt;flags |= (condition) ? PF_USED_MATH : 0; } while (0)
<span class="p_del">-#define conditional_used_math(condition) \</span>
<span class="p_del">-	conditional_stopped_child_used_math(condition, current)</span>
<span class="p_add">+</span>
<span class="p_add">+#define conditional_used_math(condition)	conditional_stopped_child_used_math(condition, current)</span>
<span class="p_add">+</span>
 #define copy_to_stopped_child_used_math(child) \
 	do { (child)-&gt;flags &amp;= ~PF_USED_MATH, (child)-&gt;flags |= current-&gt;flags &amp; PF_USED_MATH; } while (0)
<span class="p_add">+</span>
 /* NOTE: this will return 0 or PF_USED_MATH, it will never return 1 */
<span class="p_del">-#define tsk_used_math(p) ((p)-&gt;flags &amp; PF_USED_MATH)</span>
<span class="p_del">-#define used_math() tsk_used_math(current)</span>
<span class="p_add">+#define tsk_used_math(p)			((p)-&gt;flags &amp; PF_USED_MATH)</span>
<span class="p_add">+#define used_math()				tsk_used_math(current)</span>
 
 /* Per-process atomic flags. */
<span class="p_del">-#define PFA_NO_NEW_PRIVS 0	/* May not gain new privileges. */</span>
<span class="p_del">-#define PFA_SPREAD_PAGE  1      /* Spread page cache over cpuset */</span>
<span class="p_del">-#define PFA_SPREAD_SLAB  2      /* Spread some slab caches over cpuset */</span>
<span class="p_del">-#define PFA_LMK_WAITING  3      /* Lowmemorykiller is waiting */</span>
<span class="p_add">+#define PFA_NO_NEW_PRIVS		0	/* May not gain new privileges. */</span>
<span class="p_add">+#define PFA_SPREAD_PAGE			1	/* Spread page cache over cpuset */</span>
<span class="p_add">+#define PFA_SPREAD_SLAB			2	/* Spread some slab caches over cpuset */</span>
<span class="p_add">+#define PFA_LMK_WAITING			3	/* Lowmemorykiller is waiting */</span>
 
 
 #define TASK_PFA_TEST(name, func)					\
 	static inline bool task_##func(struct task_struct *p)		\
 	{ return test_bit(PFA_##name, &amp;p-&gt;atomic_flags); }
<span class="p_add">+</span>
 #define TASK_PFA_SET(name, func)					\
 	static inline void task_set_##func(struct task_struct *p)	\
 	{ set_bit(PFA_##name, &amp;p-&gt;atomic_flags); }
<span class="p_add">+</span>
 #define TASK_PFA_CLEAR(name, func)					\
 	static inline void task_clear_##func(struct task_struct *p)	\
 	{ clear_bit(PFA_##name, &amp;p-&gt;atomic_flags); }
<span class="p_chunk">@@ -1195,30 +1279,23 @@</span> <span class="p_context"> TASK_PFA_CLEAR(SPREAD_SLAB, spread_slab)</span>
 TASK_PFA_TEST(LMK_WAITING, lmk_waiting)
 TASK_PFA_SET(LMK_WAITING, lmk_waiting)
 
<span class="p_del">-static inline void tsk_restore_flags(struct task_struct *task,</span>
<span class="p_del">-				unsigned long orig_flags, unsigned long flags)</span>
<span class="p_add">+static inline void</span>
<span class="p_add">+tsk_restore_flags(struct task_struct *task, unsigned long orig_flags, unsigned long flags)</span>
 {
 	task-&gt;flags &amp;= ~flags;
 	task-&gt;flags |= orig_flags &amp; flags;
 }
 
<span class="p_del">-extern int cpuset_cpumask_can_shrink(const struct cpumask *cur,</span>
<span class="p_del">-				     const struct cpumask *trial);</span>
<span class="p_del">-extern int task_can_attach(struct task_struct *p,</span>
<span class="p_del">-			   const struct cpumask *cs_cpus_allowed);</span>
<span class="p_add">+extern int cpuset_cpumask_can_shrink(const struct cpumask *cur, const struct cpumask *trial);</span>
<span class="p_add">+extern int task_can_attach(struct task_struct *p, const struct cpumask *cs_cpus_allowed);</span>
 #ifdef CONFIG_SMP
<span class="p_del">-extern void do_set_cpus_allowed(struct task_struct *p,</span>
<span class="p_del">-			       const struct cpumask *new_mask);</span>
<span class="p_del">-</span>
<span class="p_del">-extern int set_cpus_allowed_ptr(struct task_struct *p,</span>
<span class="p_del">-				const struct cpumask *new_mask);</span>
<span class="p_add">+extern void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask);</span>
<span class="p_add">+extern int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask);</span>
 #else
<span class="p_del">-static inline void do_set_cpus_allowed(struct task_struct *p,</span>
<span class="p_del">-				      const struct cpumask *new_mask)</span>
<span class="p_add">+static inline void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)</span>
 {
 }
<span class="p_del">-static inline int set_cpus_allowed_ptr(struct task_struct *p,</span>
<span class="p_del">-				       const struct cpumask *new_mask)</span>
<span class="p_add">+static inline int set_cpus_allowed_ptr(struct task_struct *p, const struct cpumask *new_mask)</span>
 {
 	if (!cpumask_test_cpu(0, new_mask))
 		return -EINVAL;
<span class="p_chunk">@@ -1233,6 +1310,7 @@</span> <span class="p_context"> static inline int set_cpus_allowed_ptr(struct task_struct *p,</span>
 extern int yield_to(struct task_struct *p, bool preempt);
 extern void set_user_nice(struct task_struct *p, long nice);
 extern int task_prio(const struct task_struct *p);
<span class="p_add">+</span>
 /**
  * task_nice - return the nice value of a given task.
  * @p: the task in question.
<span class="p_chunk">@@ -1243,16 +1321,15 @@</span> <span class="p_context"> static inline int task_nice(const struct task_struct *p)</span>
 {
 	return PRIO_TO_NICE((p)-&gt;static_prio);
 }
<span class="p_add">+</span>
 extern int can_nice(const struct task_struct *p, const int nice);
 extern int task_curr(const struct task_struct *p);
 extern int idle_cpu(int cpu);
<span class="p_del">-extern int sched_setscheduler(struct task_struct *, int,</span>
<span class="p_del">-			      const struct sched_param *);</span>
<span class="p_del">-extern int sched_setscheduler_nocheck(struct task_struct *, int,</span>
<span class="p_del">-				      const struct sched_param *);</span>
<span class="p_del">-extern int sched_setattr(struct task_struct *,</span>
<span class="p_del">-			 const struct sched_attr *);</span>
<span class="p_add">+extern int sched_setscheduler(struct task_struct *, int, const struct sched_param *);</span>
<span class="p_add">+extern int sched_setscheduler_nocheck(struct task_struct *, int, const struct sched_param *);</span>
<span class="p_add">+extern int sched_setattr(struct task_struct *, const struct sched_attr *);</span>
 extern struct task_struct *idle_task(int cpu);
<span class="p_add">+</span>
 /**
  * is_idle_task - is the specified task an idle task?
  * @p: the task in question.
<span class="p_chunk">@@ -1263,6 +1340,7 @@</span> <span class="p_context"> static inline bool is_idle_task(const struct task_struct *p)</span>
 {
 	return !!(p-&gt;flags &amp; PF_IDLE);
 }
<span class="p_add">+</span>
 extern struct task_struct *curr_task(int cpu);
 extern void ia64_set_curr_task(int cpu, struct task_struct *p);
 
<span class="p_chunk">@@ -1296,23 +1374,25 @@</span> <span class="p_context"> static inline struct thread_info *task_thread_info(struct task_struct *task)</span>
  */
 
 extern struct task_struct *find_task_by_vpid(pid_t nr);
<span class="p_del">-extern struct task_struct *find_task_by_pid_ns(pid_t nr,</span>
<span class="p_del">-		struct pid_namespace *ns);</span>
<span class="p_add">+extern struct task_struct *find_task_by_pid_ns(pid_t nr, struct pid_namespace *ns);</span>
 
 extern int wake_up_state(struct task_struct *tsk, unsigned int state);
 extern int wake_up_process(struct task_struct *tsk);
 extern void wake_up_new_task(struct task_struct *tsk);
<span class="p_add">+</span>
 #ifdef CONFIG_SMP
<span class="p_del">- extern void kick_process(struct task_struct *tsk);</span>
<span class="p_add">+extern void kick_process(struct task_struct *tsk);</span>
 #else
<span class="p_del">- static inline void kick_process(struct task_struct *tsk) { }</span>
<span class="p_add">+static inline void kick_process(struct task_struct *tsk) { }</span>
 #endif
 
 extern void __set_task_comm(struct task_struct *tsk, const char *from, bool exec);
<span class="p_add">+</span>
 static inline void set_task_comm(struct task_struct *tsk, const char *from)
 {
 	__set_task_comm(tsk, from, false);
 }
<span class="p_add">+</span>
 extern char *get_task_comm(char *to, struct task_struct *tsk);
 
 #ifdef CONFIG_SMP
<span class="p_chunk">@@ -1320,15 +1400,15 @@</span> <span class="p_context"> void scheduler_ipi(void);</span>
 extern unsigned long wait_task_inactive(struct task_struct *, long match_state);
 #else
 static inline void scheduler_ipi(void) { }
<span class="p_del">-static inline unsigned long wait_task_inactive(struct task_struct *p,</span>
<span class="p_del">-					       long match_state)</span>
<span class="p_add">+static inline unsigned long wait_task_inactive(struct task_struct *p, long match_state)</span>
 {
 	return 1;
 }
 #endif
 
<span class="p_del">-/* set thread flags in other task&#39;s structures</span>
<span class="p_del">- * - see asm/thread_info.h for TIF_xxxx flags available</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Set thread flags in other task&#39;s structures.</span>
<span class="p_add">+ * See asm/thread_info.h for TIF_xxxx flags available:</span>
  */
 static inline void set_tsk_thread_flag(struct task_struct *tsk, int flag)
 {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



