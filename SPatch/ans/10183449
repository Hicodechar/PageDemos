
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,2/2] x86/ibpb: Prevent missed IBPB flush - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,2/2] x86/ibpb: Prevent missed IBPB flush</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=7886">tim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 25, 2018, 12:36 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;332d3ac8a7018b98d8182ec86b5fc41239c667c6.1516840211.git.tim.c.chen@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10183449/mbox/"
   >mbox</a>
|
   <a href="/patch/10183449/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10183449/">/patch/10183449/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	77CE46037F for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 25 Jan 2018 00:57:21 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 67D9E289A7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 25 Jan 2018 00:57:21 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5A2F9289C7; Thu, 25 Jan 2018 00:57:21 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id ECFFC289A7
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 25 Jan 2018 00:57:20 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933055AbeAYA5T (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 24 Jan 2018 19:57:19 -0500
Received: from mga09.intel.com ([134.134.136.24]:55857 &quot;EHLO mga09.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932250AbeAYA5Q (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 24 Jan 2018 19:57:16 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from orsmga003.jf.intel.com ([10.7.209.27])
	by orsmga102.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	24 Jan 2018 16:57:15 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.46,409,1511856000&quot;; d=&quot;scan&#39;208&quot;;a=&quot;22116104&quot;
Received: from skl-02.jf.intel.com ([10.54.74.43])
	by orsmga003.jf.intel.com with ESMTP; 24 Jan 2018 16:57:15 -0800
From: Tim Chen &lt;tim.c.chen@linux.intel.com&gt;
To: linux-kernel@vger.kernel.org
Cc: Tim Chen &lt;tim.c.chen@linux.intel.com&gt;,
	KarimAllah Ahmed &lt;karahmed@amazon.de&gt;, Andi Kleen &lt;ak@linux.intel.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;,
	Arjan van de Ven &lt;arjan@linux.intel.com&gt;,
	Ashok Raj &lt;ashok.raj@intel.com&gt;, Asit Mallick &lt;asit.k.mallick@intel.com&gt;,
	Borislav Petkov &lt;bp@suse.de&gt;, Dan Williams &lt;dan.j.williams@intel.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;, David Woodhouse &lt;dwmw@amazon.co.uk&gt;,
	Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;,
	&quot;H . Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	Janakarajan Natarajan &lt;Janakarajan.Natarajan@amd.com&gt;,
	Joerg Roedel &lt;joro@8bytes.org&gt;, Jun Nakajima &lt;jun.nakajima@intel.com&gt;,
	Laura Abbott &lt;labbott@redhat.com&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Masami Hiramatsu &lt;mhiramat@kernel.org&gt;,
	Paolo Bonzini &lt;pbonzini@redhat.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;, rkrcmar@redhat.com,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Tom Lendacky &lt;thomas.lendacky@amd.com&gt;, x86@kernel.org
Subject: [RFC PATCH 2/2] x86/ibpb: Prevent missed IBPB flush
Date: Wed, 24 Jan 2018 16:36:42 -0800
Message-Id: &lt;332d3ac8a7018b98d8182ec86b5fc41239c667c6.1516840211.git.tim.c.chen@linux.intel.com&gt;
X-Mailer: git-send-email 2.9.4
In-Reply-To: &lt;c5cc429013c2248ddebe0a8480b172ae70b29733.1516840211.git.tim.c.chen@linux.intel.com&gt;
References: &lt;c5cc429013c2248ddebe0a8480b172ae70b29733.1516840211.git.tim.c.chen@linux.intel.com&gt;
In-Reply-To: &lt;c5cc429013c2248ddebe0a8480b172ae70b29733.1516840211.git.tim.c.chen@linux.intel.com&gt;
References: &lt;c5cc429013c2248ddebe0a8480b172ae70b29733.1516840211.git.tim.c.chen@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7886">tim</a> - Jan. 25, 2018, 12:36 a.m.</div>
<pre class="content">
It is possible that the last uesr mm that we recorded for a cpu was
released, and a new mm with identical address was allocated when we
check it again.  We could skip IBPB flush here for the process with
the new mm.

It is a difficult to exploit case as we have to exit() a process on a
cpu, free the mm, and fork() the victim to use the mm pointer on that
cpu. The exploiter needs the old mm to get recycled to the
newly forked process and no other processes run on the target cpu.

Nevertheless, the patch below is one way to close this hole by
adding a ref count to prevent the last user mm from being released.
It does add ref counting overhead, and extra memory cost of keeping an mm
(though not the VMAs and most of page tables) around longer than we will
otherwise need to. Any better solutions are welcomed.

Suggested-by: Dave Hansen &lt;dave.hansen@intel.com&gt;
<span class="signed-off-by">Signed-off-by: Tim Chen &lt;tim.c.chen@linux.intel.com&gt;</span>
---
 arch/x86/mm/tlb.c | 13 ++++++++++++-
 1 file changed, 12 insertions(+), 1 deletion(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=308">David Woodhouse</a> - Jan. 25, 2018, 8:20 a.m.</div>
<pre class="content">
On Wed, 2018-01-24 at 16:36 -0800, Tim Chen wrote:
<span class="quote">&gt; It is possible that the last uesr mm that we recorded for a cpu was</span>
<span class="quote">&gt; released, and a new mm with identical address was allocated when we</span>
<span class="quote">&gt; check it again.Â  We could skip IBPB flush here for the process with</span>
<span class="quote">&gt; the new mm.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is a difficult to exploit case as we have to exit() a process on a</span>
<span class="quote">&gt; cpu, free the mm, and fork() the victim to use the mm pointer on that</span>
<span class="quote">&gt; cpu. The exploiter needs the old mm to get recycled to the</span>
<span class="quote">&gt; newly forked process and no other processes run on the target cpu.</span>

That&#39;s what it takes to have the victim process leak information into
the cache. In order to *harvest* that information, the attacker must
then get run on the same CPU again? And since her first process had to
exits, as described above, she needs a new process for that?

I confess, with all the other wildly theoretical loopholes that exist,
I wasn&#39;t losing much sleep over this one.
<span class="quote">
&gt; Nevertheless, the patch below is one way to close this hole by</span>
<span class="quote">&gt; adding a ref count to prevent the last user mm from being released.</span>
<span class="quote">&gt; It does add ref counting overhead, and extra memory cost of keeping an mm</span>
<span class="quote">&gt; (though not the VMAs and most of page tables) around longer than we will</span>
<span class="quote">&gt; otherwise need to. Any better solutions are welcomed.</span>

This has no upper bound on the amount of time the user mm gets held,
right? If a given CPU remains idle for ever (and what happens if it&#39;s
taken offline?) we&#39;ll never do that mmdrop() ?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7886">tim</a> - Jan. 25, 2018, 4:56 p.m.</div>
<pre class="content">
On 01/25/2018 12:20 AM, David Woodhouse wrote:
<span class="quote">&gt; On Wed, 2018-01-24 at 16:36 -0800, Tim Chen wrote:</span>
<span class="quote">&gt;&gt; It is possible that the last uesr mm that we recorded for a cpu was</span>
<span class="quote">&gt;&gt; released, and a new mm with identical address was allocated when we</span>
<span class="quote">&gt;&gt; check it again.  We could skip IBPB flush here for the process with</span>
<span class="quote">&gt;&gt; the new mm.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It is a difficult to exploit case as we have to exit() a process on a</span>
<span class="quote">&gt;&gt; cpu, free the mm, and fork() the victim to use the mm pointer on that</span>
<span class="quote">&gt;&gt; cpu. The exploiter needs the old mm to get recycled to the</span>
<span class="quote">&gt;&gt; newly forked process and no other processes run on the target cpu.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s what it takes to have the victim process leak information into</span>
<span class="quote">&gt; the cache. In order to *harvest* that information, the attacker must</span>
<span class="quote">&gt; then get run on the same CPU again? And since her first process had to</span>
<span class="quote">&gt; exits, as described above, she needs a new process for that?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I confess, with all the other wildly theoretical loopholes that exist,</span>
<span class="quote">&gt; I wasn&#39;t losing much sleep over this one.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Nevertheless, the patch below is one way to close this hole by</span>
<span class="quote">&gt;&gt; adding a ref count to prevent the last user mm from being released.</span>
<span class="quote">&gt;&gt; It does add ref counting overhead, and extra memory cost of keeping an mm</span>
<span class="quote">&gt;&gt; (though not the VMAs and most of page tables) around longer than we will</span>
<span class="quote">&gt;&gt; otherwise need to. Any better solutions are welcomed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This has no upper bound on the amount of time the user mm gets held,</span>
<span class="quote">&gt; right? If a given CPU remains idle for ever (and what happens if it&#39;s</span>
<span class="quote">&gt; taken offline?) we&#39;ll never do that mmdrop() ?</span>
<span class="quote">&gt; </span>

The downside with this approach is we do hold on to the mm longer
than we needs to.

Yes, the offline path does need to be fixed up.

Tim
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 86ed07f..3bdaa10 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/export.h&gt;
 #include &lt;linux/cpu.h&gt;
 #include &lt;linux/debugfs.h&gt;
<span class="p_add">+#include &lt;linux/sched/mm.h&gt;</span>
 
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/mmu_context.h&gt;
<span class="p_chunk">@@ -291,8 +292,17 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, 0);
 		}
 
<span class="p_del">-		if (next != &amp;init_mm &amp;&amp; next != last)</span>
<span class="p_add">+		if (next != &amp;init_mm &amp;&amp; next != last) {</span>
<span class="p_add">+			if (last != NULL)</span>
<span class="p_add">+				mmdrop(last);</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Keep &#39;next&#39; allocated until we switch to another mm.</span>
<span class="p_add">+			 * This keeps us from missing a flush of the branch predictors</span>
<span class="p_add">+			 * if &#39;next&#39; gets freed and reallocated.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			mmgrab(next);</span>
 			this_cpu_write(cpu_tlbstate.last_usr_mm, next);
<span class="p_add">+		}</span>
 		this_cpu_write(cpu_tlbstate.loaded_mm, next);
 		this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);
 	}
<span class="p_chunk">@@ -370,6 +380,7 @@</span> <span class="p_context"> void initialize_tlbstate_and_flush(void)</span>
 	write_cr3(build_cr3(mm-&gt;pgd, 0));
 
 	/* Reinitialize tlbstate. */
<span class="p_add">+	this_cpu_write(cpu_tlbstate.last_usr_mm, NULL);</span>
 	this_cpu_write(cpu_tlbstate.loaded_mm_asid, 0);
 	this_cpu_write(cpu_tlbstate.next_asid, 1);
 	this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, mm-&gt;context.ctx_id);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



