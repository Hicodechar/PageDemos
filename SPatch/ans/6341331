
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[207/208] x86/fpu: Add FPU performance measurement subsystem - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [207/208] x86/fpu: Add FPU performance measurement subsystem</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 5, 2015, 5:58 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1430848712-28064-47-git-send-email-mingo@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6341331/mbox/"
   >mbox</a>
|
   <a href="/patch/6341331/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6341331/">/patch/6341331/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 426C39F373
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  5 May 2015 18:01:21 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 561C42017E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  5 May 2015 18:01:19 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 51914201CD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  5 May 2015 18:01:17 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S2992440AbbEESBL (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 5 May 2015 14:01:11 -0400
Received: from mail-wg0-f41.google.com ([74.125.82.41]:34629 &quot;EHLO
	mail-wg0-f41.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1755547AbbEESAJ (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 5 May 2015 14:00:09 -0400
Received: by wgso17 with SMTP id o17so192332865wgs.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 05 May 2015 11:00:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=SI5+zPGeVVnSKq1wRL9y23lyNOyxbGds8ZypP/CxxH8=;
	b=CaF63NVPJjePQ28D0LiYSZxkadtnPqVYlFW81K5aRygweboXrx9608djB1H8pjdrSc
	NgQILJ5OQ4X1CwNl2PCSZaw+UM8z/J+2w4avPjsvrm99pJYz4okUHZ3p7yXFExulCin6
	8gzySPMz4lxfHJCvTuCWuBq+ftsTLmrt6cZ5FE1TSTEj6Mlm/RQdeI0NbrL9NCygTZvH
	QvZwsrMZIM/l/O1uBDZfawvYuEyrpUFdvyW7qLlPiBhDg+H8Mhvy54E9IPYXrTN18s8X
	tktnQoqcdQDWQJXq4WL/ME96Jn7W2xXCUU1ely+Qg7ta0xxTG5fYjHu++lPY90RsRo5G
	Jy9Q==
X-Received: by 10.194.97.196 with SMTP id ec4mr15424839wjb.3.1430848808006; 
	Tue, 05 May 2015 11:00:08 -0700 (PDT)
Received: from localhost.localdomain (2E8BA3FA.catv.pool.telekom.hu.
	[46.139.163.250])
	by mx.google.com with ESMTPSA id k9sm32607wia.6.2015.05.05.11.00.06
	(version=TLSv1.2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Tue, 05 May 2015 11:00:07 -0700 (PDT)
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: linux-kernel@vger.kernel.org
Cc: Andy Lutomirski &lt;luto@amacapital.net&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Fenghua Yu &lt;fenghua.yu@intel.com&gt;, &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Oleg Nesterov &lt;oleg@redhat.com&gt;, Thomas Gleixner &lt;tglx@linutronix.de&gt;
Subject: [PATCH 207/208] x86/fpu: Add FPU performance measurement subsystem
Date: Tue,  5 May 2015 19:58:31 +0200
Message-Id: &lt;1430848712-28064-47-git-send-email-mingo@kernel.org&gt;
X-Mailer: git-send-email 2.1.0
In-Reply-To: &lt;1430848712-28064-1-git-send-email-mingo@kernel.org&gt;
References: &lt;1430848712-28064-1-git-send-email-mingo@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.8 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID,T_RP_MATCHES_RCVD,UNPARSEABLE_RELAY
	autolearn=ham version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - May 5, 2015, 5:58 p.m.</div>
<pre class="content">
Add a short FPU performance suite that runs once during bootup.

It can be enabled via CONFIG_X86_DEBUG_FPU_PERFORMANCE=y.

  x86/fpu:##################################################################
  x86/fpu: Running FPU performance measurement suite (cache hot):
  x86/fpu: Cost of: null                                      :   108 cycles
  x86/fpu:########  CPU instructions:           ############################
  x86/fpu: Cost of: NOP                         insn          :     0 cycles
  x86/fpu: Cost of: RDTSC                       insn          :    12 cycles
  x86/fpu: Cost of: RDMSR                       insn          :   100 cycles
  x86/fpu: Cost of: WRMSR                       insn          :   396 cycles
  x86/fpu: Cost of: CLI                         insn  same-IF :     0 cycles
  x86/fpu: Cost of: CLI                         insn  flip-IF :     0 cycles
  x86/fpu: Cost of: STI                         insn  same-IF :     0 cycles
  x86/fpu: Cost of: STI                         insn  flip-IF :     0 cycles
  x86/fpu: Cost of: PUSHF                       insn          :     0 cycles
  x86/fpu: Cost of: POPF                        insn  same-IF :    20 cycles
  x86/fpu: Cost of: POPF                        insn  flip-IF :    28 cycles
  x86/fpu:########  IRQ save/restore APIs:      ############################
  x86/fpu: Cost of: local_irq_save()            fn            :    20 cycles
  x86/fpu: Cost of: local_irq_restore()         fn    same-IF :    24 cycles
  x86/fpu: Cost of: local_irq_restore()         fn    flip-IF :    28 cycles
  x86/fpu: Cost of: irq_save()+restore()        fn    same-IF :    48 cycles
  x86/fpu: Cost of: irq_save()+restore()        fn    flip-IF :    48 cycles
  x86/fpu:########  locking APIs:               ############################
  x86/fpu: Cost of: smp_mb()                    fn            :    40 cycles
  x86/fpu: Cost of: cpu_relax()                 fn            :     8 cycles
  x86/fpu: Cost of: spin_lock()+unlock()        fn            :    64 cycles
  x86/fpu: Cost of: read_lock()+unlock()        fn            :    76 cycles
  x86/fpu: Cost of: write_lock()+unlock()       fn            :    52 cycles
  x86/fpu: Cost of: rcu_read_lock()+unlock()    fn            :    16 cycles
  x86/fpu: Cost of: preempt_disable()+enable()  fn            :    20 cycles
  x86/fpu: Cost of: mutex_lock()+unlock()       fn            :    56 cycles
  x86/fpu:########  MM instructions:            ############################
  x86/fpu: Cost of: __flush_tlb()               fn            :   132 cycles
  x86/fpu: Cost of: __flush_tlb_global()        fn            :   920 cycles
  x86/fpu: Cost of: __flush_tlb_one()           fn            :   288 cycles
  x86/fpu: Cost of: __flush_tlb_range()         fn            :   412 cycles
  x86/fpu:########  FPU instructions:           ############################
  x86/fpu: Cost of: CR0                         read          :     4 cycles
  x86/fpu: Cost of: CR0                         write         :   208 cycles
  x86/fpu: Cost of: CR0::TS                     fault         :  1156 cycles
  x86/fpu: Cost of: FNINIT                      insn          :    76 cycles
  x86/fpu: Cost of: FWAIT                       insn          :     0 cycles
  x86/fpu: Cost of: FSAVE                       insn          :   168 cycles
  x86/fpu: Cost of: FRSTOR                      insn          :   160 cycles
  x86/fpu: Cost of: FXSAVE                      insn          :    84 cycles
  x86/fpu: Cost of: FXRSTOR                     insn          :    44 cycles
  x86/fpu: Cost of: FXRSTOR                     fault         :   688 cycles
  x86/fpu: Cost of: XSAVE                       insn          :   104 cycles
  x86/fpu: Cost of: XRSTOR                      insn          :    80 cycles
  x86/fpu: Cost of: XRSTOR                      fault         :   884 cycles
  x86/fpu:##################################################################

on an AMD system:

  x86/fpu:##################################################################
  x86/fpu: Running FPU performance measurement suite (cache hot):
  x86/fpu: Cost of: null                                      :   144 cycles
  x86/fpu:########  CPU instructions:           ############################
  x86/fpu: Cost of: NOP                         insn          :     4 cycles
  x86/fpu: Cost of: RDTSC                       insn          :    71 cycles
  x86/fpu: Cost of: RDMSR                       insn          :    43 cycles
  x86/fpu: Cost of: WRMSR                       insn          :   148 cycles
  x86/fpu: Cost of: CLI                         insn  same-IF :     8 cycles
  x86/fpu: Cost of: CLI                         insn  flip-IF :     5 cycles
  x86/fpu: Cost of: STI                         insn  same-IF :    28 cycles
  x86/fpu: Cost of: STI                         insn  flip-IF :     0 cycles
  x86/fpu: Cost of: PUSHF                       insn          :    15 cycles
  x86/fpu: Cost of: POPF                        insn  same-IF :     8 cycles
  x86/fpu: Cost of: POPF                        insn  flip-IF :    12 cycles
  x86/fpu:########  IRQ save/restore APIs:      ############################
  x86/fpu: Cost of: local_irq_save()            fn            :     0 cycles
  x86/fpu: Cost of: local_irq_restore()         fn    same-IF :     7 cycles
  x86/fpu: Cost of: local_irq_restore()         fn    flip-IF :    20 cycles
  x86/fpu: Cost of: irq_save()+restore()        fn    same-IF :    20 cycles
  x86/fpu: Cost of: irq_save()+restore()        fn    flip-IF :    20 cycles
  x86/fpu:########  locking APIs:               ############################
  x86/fpu: Cost of: smp_mb()                    fn            :    38 cycles
  x86/fpu: Cost of: cpu_relax()                 fn            :     7 cycles
  x86/fpu: Cost of: spin_lock()+unlock()        fn            :    89 cycles
  x86/fpu: Cost of: read_lock()+unlock()        fn            :    91 cycles
  x86/fpu: Cost of: write_lock()+unlock()       fn            :    85 cycles
  x86/fpu: Cost of: rcu_read_lock()+unlock()    fn            :    30 cycles
  x86/fpu: Cost of: preempt_disable()+enable()  fn            :    38 cycles
  x86/fpu: Cost of: mutex_lock()+unlock()       fn            :    64 cycles
  x86/fpu:########  MM instructions:            ############################
  x86/fpu: Cost of: __flush_tlb()               fn            :   134 cycles
  x86/fpu: Cost of: __flush_tlb_global()        fn            :   547 cycles
  x86/fpu: Cost of: __flush_tlb_one()           fn            :   128 cycles
  x86/fpu: Cost of: __flush_tlb_range()         fn            :   539 cycles
  x86/fpu:########  FPU instructions:           ############################
  x86/fpu: Cost of: CR0                         read          :    16 cycles
  x86/fpu: Cost of: CR0                         write         :    83 cycles
  x86/fpu: Cost of: CR0::TS                     fault         :   691 cycles
  x86/fpu: Cost of: FNINIT                      insn          :   118 cycles
  x86/fpu: Cost of: FWAIT                       insn          :     4 cycles
  x86/fpu: Cost of: FSAVE                       insn          :   156 cycles
  x86/fpu: Cost of: FRSTOR                      insn          :   151 cycles
  x86/fpu: Cost of: FXSAVE                      insn          :    73 cycles
  x86/fpu: Cost of: FXRSTOR                     insn          :    86 cycles
  x86/fpu: Cost of: FXRSTOR                     fault         :   441 cycles
  x86/fpu:##################################################################

Note that there can be some jitter in the results between bootups.
The measurement takes the shortest of all runs, which is relatively
but not completely stable. So for example in the above test,
NOPs obviously don&#39;t take 3 cycles. Results are expected to be
relatively accurate for more complex instructions.

Cc: Andy Lutomirski &lt;luto@amacapital.net&gt;
Cc: Borislav Petkov &lt;bp@alien8.de&gt;
Cc: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;
Cc: Fenghua Yu &lt;fenghua.yu@intel.com&gt;
Cc: H. Peter Anvin &lt;hpa@zytor.com&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Oleg Nesterov &lt;oleg@redhat.com&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
<span class="signed-off-by">Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
---
 arch/x86/Kconfig.debug             |  15 ++
 arch/x86/include/asm/fpu/measure.h |  13 ++
 arch/x86/kernel/cpu/bugs.c         |   2 +
 arch/x86/kernel/cpu/bugs_64.c      |   2 +
 arch/x86/kernel/fpu/Makefile       |   8 +-
 arch/x86/kernel/fpu/measure.c      | 509 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++
 6 files changed, 548 insertions(+), 1 deletion(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - May 5, 2015, 7:15 p.m.</div>
<pre class="content">
On 05/05/2015 10:58 AM, Ingo Molnar wrote:
<span class="quote">&gt;   x86/fpu: Cost of: XSAVE                       insn          :   104 cycles</span>
<span class="quote">&gt;   x86/fpu: Cost of: XRSTOR                      insn          :    80 cycles</span>

Isn&#39;t there going to be pretty huge variability here depending on how
much state you are xsave/xrstor&#39;ing and if the init/modified
optimizations are in play?

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - May 5, 2015, 7:22 p.m.</div>
<pre class="content">
On Tue, May 05, 2015 at 12:15:00PM -0700, Dave Hansen wrote:
<span class="quote">&gt; On 05/05/2015 10:58 AM, Ingo Molnar wrote:</span>
<span class="quote">&gt; &gt;   x86/fpu: Cost of: XSAVE                       insn          :   104 cycles</span>
<span class="quote">&gt; &gt;   x86/fpu: Cost of: XRSTOR                      insn          :    80 cycles</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Isn&#39;t there going to be pretty huge variability here depending on how</span>
<span class="quote">&gt; much state you are xsave/xrstor&#39;ing and if the init/modified</span>
<span class="quote">&gt; optimizations are in play?</span>

If this is a module, one could modprobe/rmmod it multiple times for
an average. But yeah, it would depend in the end on what the system
does/has been doing in the recent past and thus how much state has been
&quot;accumulated&quot;.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - May 6, 2015, 12:52 a.m.</div>
<pre class="content">
On May 5, 2015 11:30 PM, &quot;Ingo Molnar&quot; &lt;mingo@kernel.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; Add a short FPU performance suite that runs once during bootup.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It can be enabled via CONFIG_X86_DEBUG_FPU_PERFORMANCE=y.</span>

Neat!

Can you change &quot;cycles&quot; to &quot;TSC ticks&quot;?  They&#39;re not quite the same thing.

--Andy
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131071">Ingo Molnar</a> - May 6, 2015, 4:11 a.m.</div>
<pre class="content">
* Dave Hansen &lt;dave.hansen@linux.intel.com&gt; wrote:
<span class="quote">
&gt; On 05/05/2015 10:58 AM, Ingo Molnar wrote:</span>
<span class="quote">&gt; &gt;   x86/fpu: Cost of: XSAVE                       insn          :   104 cycles</span>
<span class="quote">&gt; &gt;   x86/fpu: Cost of: XRSTOR                      insn          :    80 cycles</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Isn&#39;t there going to be pretty huge variability here depending on </span>
<span class="quote">&gt; how much state you are xsave/xrstor&#39;ing and if the init/modified </span>
<span class="quote">&gt; optimizations are in play?</span>

Hopefully there&#39;s such variability! :)

I thought to add measurements for that as well:

 - to see the costs of this instruction family when various xstate 
   components are in &#39;init state&#39; or not

 - maybe even measure whether it can optimize based on whether things
   got changed since the last save (which the SDM kind of alludes to 
   but which I doubt the hw does)?

This initial version only measures trivial init state save/restore 
cost.

Thanks,

	Ingo
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131071">Ingo Molnar</a> - May 6, 2015, 4:52 a.m.</div>
<pre class="content">
* Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">
&gt; On May 5, 2015 11:30 PM, &quot;Ingo Molnar&quot; &lt;mingo@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Add a short FPU performance suite that runs once during bootup.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; It can be enabled via CONFIG_X86_DEBUG_FPU_PERFORMANCE=y.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Neat!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can you change &quot;cycles&quot; to &quot;TSC ticks&quot;?  They&#39;re not quite the same thing.</span>

Yeah, with constant TSC we have the magic TSC frequency that is used 
by RDTSC.

I&#39;m torn: &#39;TSC ticks&#39; will mean very little to most people reading 
that output. We could convert it to nsecs with a little bit of 
calibration - but that makes it depend on small differences in CPU 
model frequencies, while the (cached) cycle costs are typically 
constant per microarchitecture.

I suspect we could snatch a performance counter temporarily, to get 
the real cycles count, and maybe even add a uops column. Most of this 
needs to run in kernel space, so it&#39;s not a tooling project.

I also wanted to add cache-cold numbers which are very interesting as 
well, just awfully hard to measure in a stable fashion. For cache-cold 
numbers the natural unit would be memory bus cycles.

Thanks,

	Ingo
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7500">Borislav Petkov</a> - May 6, 2015, 3:53 p.m.</div>
<pre class="content">
On Wed, May 06, 2015 at 06:52:39AM +0200, Ingo Molnar wrote:
<span class="quote">&gt; &gt; Can you change &quot;cycles&quot; to &quot;TSC ticks&quot;?  They&#39;re not quite the same thing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, with constant TSC we have the magic TSC frequency that is used </span>
<span class="quote">&gt; by RDTSC.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m torn: &#39;TSC ticks&#39; will mean very little to most people reading </span>
<span class="quote">&gt; that output. We could convert it to nsecs with a little bit of </span>
<span class="quote">&gt; calibration - but that makes it depend on small differences in CPU </span>
<span class="quote">&gt; model frequencies, while the (cached) cycle costs are typically </span>
<span class="quote">&gt; constant per microarchitecture.</span>

I think the best we should do is convert the TSC ticks to the unboosted
P0 frequency, i.e. if the P0 freq is say, 4GHz, we have 4*10^9 core
cycles per second. And then convert the counted TSC ticks to those
cycles.

For that we would need to measure in the beginning how TSC ticks relate
to P0 cycles and then use that number for conversion...

Hmmm.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - May 7, 2015, 2:52 a.m.</div>
<pre class="content">
On May 6, 2015 10:22 AM, &quot;Ingo Molnar&quot; &lt;mingo2.kernel.org@gmail.com&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; * Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &gt; On May 5, 2015 11:30 PM, &quot;Ingo Molnar&quot; &lt;mingo@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Add a short FPU performance suite that runs once during bootup.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; It can be enabled via CONFIG_X86_DEBUG_FPU_PERFORMANCE=y.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Neat!</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Can you change &quot;cycles&quot; to &quot;TSC ticks&quot;?  They&#39;re not quite the same thing.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yeah, with constant TSC we have the magic TSC frequency that is used</span>
<span class="quote">&gt; by RDTSC.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m torn: &#39;TSC ticks&#39; will mean very little to most people reading</span>
<span class="quote">&gt; that output. We could convert it to nsecs with a little bit of</span>
<span class="quote">&gt; calibration - but that makes it depend on small differences in CPU</span>
<span class="quote">&gt; model frequencies, while the (cached) cycle costs are typically</span>
<span class="quote">&gt; constant per microarchitecture.</span>

Isn&#39;t it dependent on the ratio of max turbo frequency to TSC freq?
Typical non-ultra-mobile systems should be at or near max turbo
frequency during bootup.
<span class="quote">
&gt;</span>
<span class="quote">&gt; I suspect we could snatch a performance counter temporarily, to get</span>
<span class="quote">&gt; the real cycles count, and maybe even add a uops column. Most of this</span>
<span class="quote">&gt; needs to run in kernel space, so it&#39;s not a tooling project.</span>

This will suck under KVM without extra care.  I know, because I&#39;m
working on a similar userspace tool that uses RDPMC.

Another option would be rdmsr(MSR_IA32_APERF), but that isn&#39;t
available under KVM either.
<span class="quote">
&gt;</span>
<span class="quote">&gt; I also wanted to add cache-cold numbers which are very interesting as</span>
<span class="quote">&gt; well, just awfully hard to measure in a stable fashion. For cache-cold</span>
<span class="quote">&gt; numbers the natural unit would be memory bus cycles.</span>

Yeah, maybe it&#39;s worth wiring up perf counters at some point.

--Andy
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig.debug b/arch/x86/Kconfig.debug</span>
<span class="p_header">index 2fd3ebbb4e33..8329635101f8 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig.debug</span>
<span class="p_header">+++ b/arch/x86/Kconfig.debug</span>
<span class="p_chunk">@@ -344,4 +344,19 @@</span> <span class="p_context"> config X86_DEBUG_FPU</span>
 
 	  If unsure, say N.
 
<span class="p_add">+config X86_DEBUG_FPU_PERFORMANCE</span>
<span class="p_add">+	bool &quot;Measure x86 FPU performance&quot;</span>
<span class="p_add">+	depends on DEBUG_KERNEL</span>
<span class="p_add">+	---help---</span>
<span class="p_add">+	  If this option is enabled then the kernel will run a short</span>
<span class="p_add">+	  FPU (Floating Point Unit) benchmarking suite during bootup,</span>
<span class="p_add">+	  to measure the cost of various FPU hardware operations and</span>
<span class="p_add">+	  other kernel APIs.</span>
<span class="p_add">+</span>
<span class="p_add">+	  The results are printed to the kernel log.</span>
<span class="p_add">+</span>
<span class="p_add">+	  This extra benchmarking code will be freed after bootup.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If unsure, say N.</span>
<span class="p_add">+</span>
 endmenu
<span class="p_header">diff --git a/arch/x86/include/asm/fpu/measure.h b/arch/x86/include/asm/fpu/measure.h</span>
new file mode 100644
<span class="p_header">index 000000000000..d003809491c2</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/fpu/measure.h</span>
<span class="p_chunk">@@ -0,0 +1,13 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * x86 FPU performance measurement methods:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef _ASM_X86_FPU_MEASURE_H</span>
<span class="p_add">+#define _ASM_X86_FPU_MEASURE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_DEBUG_FPU_PERFORMANCE</span>
<span class="p_add">+extern void fpu__measure(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void fpu__measure(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_FPU_MEASURE_H */</span>
<span class="p_header">diff --git a/arch/x86/kernel/cpu/bugs.c b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">index bd17db15a2c1..1b947415d903 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/bugs.c</span>
<span class="p_chunk">@@ -13,6 +13,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/processor-flags.h&gt;
 #include &lt;asm/fpu/internal.h&gt;
<span class="p_add">+#include &lt;asm/fpu/measure.h&gt;</span>
 #include &lt;asm/msr.h&gt;
 #include &lt;asm/paravirt.h&gt;
 #include &lt;asm/alternative.h&gt;
<span class="p_chunk">@@ -37,6 +38,7 @@</span> <span class="p_context"> void __init check_bugs(void)</span>
 
 	init_utsname()-&gt;machine[1] =
 		&#39;0&#39; + (boot_cpu_data.x86 &gt; 6 ? 6 : boot_cpu_data.x86);
<span class="p_add">+	fpu__measure();</span>
 	alternative_instructions();
 
 	fpu__init_check_bugs();
<span class="p_header">diff --git a/arch/x86/kernel/cpu/bugs_64.c b/arch/x86/kernel/cpu/bugs_64.c</span>
<span class="p_header">index 04f0fe5af83e..846c24aa14cf 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/bugs_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/bugs_64.c</span>
<span class="p_chunk">@@ -8,6 +8,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/alternative.h&gt;
 #include &lt;asm/bugs.h&gt;
 #include &lt;asm/processor.h&gt;
<span class="p_add">+#include &lt;asm/fpu/measure.h&gt;</span>
 #include &lt;asm/mtrr.h&gt;
 #include &lt;asm/cacheflush.h&gt;
 
<span class="p_chunk">@@ -18,6 +19,7 @@</span> <span class="p_context"> void __init check_bugs(void)</span>
 	printk(KERN_INFO &quot;CPU: &quot;);
 	print_cpu_info(&amp;boot_cpu_data);
 #endif
<span class="p_add">+	fpu__measure();</span>
 	alternative_instructions();
 
 	/*
<span class="p_header">diff --git a/arch/x86/kernel/fpu/Makefile b/arch/x86/kernel/fpu/Makefile</span>
<span class="p_header">index 68279efb811a..e7676c20bdde 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/Makefile</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/Makefile</span>
<span class="p_chunk">@@ -2,4 +2,10 @@</span> <span class="p_context"></span>
 # Build rules for the FPU support code:
 #
 
<span class="p_del">-obj-y				+= init.o bugs.o core.o regset.o signal.o xstate.o</span>
<span class="p_add">+obj-y					+= init.o bugs.o core.o regset.o signal.o xstate.o</span>
<span class="p_add">+</span>
<span class="p_add">+# Make the measured functions as simple as possible:</span>
<span class="p_add">+CFLAGS_measure.o += -fomit-frame-pointer</span>
<span class="p_add">+CFLAGS_REMOVE_measure.o = -pg</span>
<span class="p_add">+</span>
<span class="p_add">+obj-$(CONFIG_X86_DEBUG_FPU_PERFORMANCE) += measure.o</span>
<span class="p_header">diff --git a/arch/x86/kernel/fpu/measure.c b/arch/x86/kernel/fpu/measure.c</span>
new file mode 100644
<span class="p_header">index 000000000000..6232cdf240d8</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/measure.c</span>
<span class="p_chunk">@@ -0,0 +1,509 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * FPU performance measurement routines</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;asm/fpu/internal.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Number of repeated measurements we do. We pick the fastest one:</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int loops = 1000;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Various small functions, whose overhead we measure:</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+typedef void (*bench_fn_t)(void) __aligned(32);</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_empty(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Basic instructions: */</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_nop(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;nop&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_rdtsc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 low, high;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile (&quot;rdtsc&quot;: &quot;=a&quot;(low), &quot;=d&quot;(high));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_rdmsr(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 efer;</span>
<span class="p_add">+</span>
<span class="p_add">+	rdmsrl_safe(MSR_EFER, &amp;efer);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_wrmsr(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 efer;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!rdmsrl_safe(MSR_EFER, &amp;efer))</span>
<span class="p_add">+		wrmsrl_safe(MSR_EFER, efer);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_cli_same(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;cli&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_cli_flip(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;sti&quot;);</span>
<span class="p_add">+	asm volatile (&quot;cli&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_sti_same(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;sti&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_sti_flip(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;cli&quot;);</span>
<span class="p_add">+	asm volatile (&quot;sti&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_pushf(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arch_local_save_flags();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_popf_baseline(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	arch_local_save_flags();</span>
<span class="p_add">+	asm volatile (&quot;cli&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_popf_flip(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags = arch_local_save_flags();</span>
<span class="p_add">+	asm volatile (&quot;cli&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_popf_same(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags = arch_local_save_flags();</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Basic IRQ save/restore APIs: */</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_irq_save_baseline(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_irq_save(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_irq_restore_flip(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_irq_restore_same(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_disable();</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_irq_save_restore_flip(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_irq_save_restore_same(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Basic locking primitives: */</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_smp_mb(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	smp_mb();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_cpu_relax(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	cpu_relax();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static DEFINE_SPINLOCK(test_spinlock);</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_spin_lock_unlock(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spin_lock(&amp;test_spinlock);</span>
<span class="p_add">+	spin_unlock(&amp;test_spinlock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static DEFINE_RWLOCK(test_rwlock);</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_read_lock_unlock(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	read_lock(&amp;test_rwlock);</span>
<span class="p_add">+	read_unlock(&amp;test_rwlock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_write_lock_unlock(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	write_lock(&amp;test_rwlock);</span>
<span class="p_add">+	write_unlock(&amp;test_rwlock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_rcu_read_lock_unlock(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	rcu_read_lock();</span>
<span class="p_add">+	rcu_read_unlock();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_preempt_disable_enable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static DEFINE_MUTEX(test_mutex);</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_mutex_lock_unlock(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;test_mutex);</span>
<span class="p_add">+	mutex_unlock(&amp;test_mutex);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* MM instructions: */</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_flush_tlb(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__flush_tlb();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_flush_tlb_global(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__flush_tlb_global();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static char tlb_flush_target[PAGE_SIZE] __aligned(4096);</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_flush_tlb_one(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = (unsigned long)&amp;tlb_flush_target;</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_flush_target[0]++;</span>
<span class="p_add">+	__flush_tlb_one(addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_flush_tlb_range(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long start = (unsigned long)&amp;tlb_flush_target;</span>
<span class="p_add">+	unsigned long end = start+PAGE_SIZE;</span>
<span class="p_add">+	struct mm_struct *mm_saved;</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_flush_target[0]++;</span>
<span class="p_add">+</span>
<span class="p_add">+	mm_saved = current-&gt;mm;</span>
<span class="p_add">+	current-&gt;mm = current-&gt;active_mm;</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_mm_range(current-&gt;active_mm, start, end, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	current-&gt;mm = mm_saved;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* FPU instructions: */</span>
<span class="p_add">+/* FPU instructions: */</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_read_cr0(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	read_cr0();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_rw_cr0(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	write_cr0(read_cr0());</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_cr0_fault(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct fpu *fpu = &amp;current-&gt;thread.fpu;</span>
<span class="p_add">+	u32 cr0 = read_cr0();</span>
<span class="p_add">+</span>
<span class="p_add">+	write_cr0(cr0 | X86_CR0_TS);</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;fwait&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Zap the FP state we created via the fault: */</span>
<span class="p_add">+	fpu-&gt;fpregs_active = 0;</span>
<span class="p_add">+	fpu-&gt;fpstate_active = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	write_cr0(cr0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_fninit(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;fninit&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_fwait(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile(&quot;fwait&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_fsave(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	static struct fregs_state fstate __aligned(32);</span>
<span class="p_add">+</span>
<span class="p_add">+	copy_fregs_to_user(&amp;fstate);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_frstor(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	static struct fregs_state fstate __aligned(32);</span>
<span class="p_add">+</span>
<span class="p_add">+	copy_fregs_to_user(&amp;fstate);</span>
<span class="p_add">+	copy_user_to_fregs(&amp;fstate);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_fxsave(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct fxregs_state fxstate __aligned(32);</span>
<span class="p_add">+</span>
<span class="p_add">+	copy_fxregs_to_user(&amp;fxstate);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_fxrstor(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	static struct fxregs_state fxstate __aligned(32);</span>
<span class="p_add">+</span>
<span class="p_add">+	copy_fxregs_to_user(&amp;fxstate);</span>
<span class="p_add">+	copy_user_to_fxregs(&amp;fxstate);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Provoke #GP on invalid FXRSTOR:</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void fn_fxrstor_fault(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	static struct fxregs_state fxstate __aligned(32);</span>
<span class="p_add">+	struct fpu *fpu = &amp;current-&gt;thread.fpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	copy_fxregs_to_user(&amp;fxstate);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Set invalid MXCSR value, this will generate a #GP: */</span>
<span class="p_add">+	fxstate.mxcsr = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	copy_user_to_fxregs(&amp;fxstate);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Zap any FP state we created via the fault: */</span>
<span class="p_add">+	fpu-&gt;fpregs_active = 0;</span>
<span class="p_add">+	fpu-&gt;fpstate_active = 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_xsave(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	static struct xregs_state x __aligned(32);</span>
<span class="p_add">+</span>
<span class="p_add">+	copy_xregs_to_kernel_booting(&amp;x);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void fn_xrstor(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	static struct xregs_state x __aligned(32);</span>
<span class="p_add">+</span>
<span class="p_add">+	copy_xregs_to_kernel_booting(&amp;x);</span>
<span class="p_add">+	copy_kernel_to_xregs_booting(&amp;x, -1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Provoke #GP on invalid XRSTOR:</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void fn_xrstor_fault(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	static struct xregs_state x __aligned(32);</span>
<span class="p_add">+</span>
<span class="p_add">+	copy_xregs_to_kernel_booting(&amp;x);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Set invalid MXCSR value, this will generate a #GP: */</span>
<span class="p_add">+	x.i387.mxcsr = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	copy_kernel_to_xregs_booting(&amp;x, -1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static s64</span>
<span class="p_add">+measure(s64 null_overhead, bench_fn_t bench_fn,</span>
<span class="p_add">+	const char *txt_1, const char *txt_2, const char *txt_3)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	u32 cr0_saved;</span>
<span class="p_add">+	int eager_saved;</span>
<span class="p_add">+	u64 t0, t1;</span>
<span class="p_add">+	s64 delta, delta_min;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	delta_min = LONG_MAX;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Disable eagerfpu, so that we can provoke CR0::TS faults: */</span>
<span class="p_add">+	eager_saved = boot_cpu_has(X86_FEATURE_EAGER_FPU);</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_EAGER_FPU);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Save CR0 so that we can freely set it to any value during measurement: */</span>
<span class="p_add">+	cr0_saved = read_cr0();</span>
<span class="p_add">+	/* Clear TS, so that we can measure FPU ops by default: */</span>
<span class="p_add">+	write_cr0(cr0_saved &amp; ~X86_CR0_TS);</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile (&quot;.align 32\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; loops; i++) {</span>
<span class="p_add">+		rdtscll(t0);</span>
<span class="p_add">+		mb();</span>
<span class="p_add">+</span>
<span class="p_add">+		bench_fn();</span>
<span class="p_add">+</span>
<span class="p_add">+		mb();</span>
<span class="p_add">+		rdtscll(t1);</span>
<span class="p_add">+		delta = t1-t0;</span>
<span class="p_add">+		if (delta &lt;= 0)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		delta_min = min(delta_min, delta);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+	write_cr0(cr0_saved);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (eager_saved)</span>
<span class="p_add">+		setup_force_cpu_cap(X86_FEATURE_EAGER_FPU);</span>
<span class="p_add">+</span>
<span class="p_add">+	delta_min = max(0LL, delta_min-null_overhead);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (txt_1) {</span>
<span class="p_add">+		if (!txt_2)</span>
<span class="p_add">+			txt_2 = &quot;&quot;;</span>
<span class="p_add">+		if (!txt_3)</span>
<span class="p_add">+			txt_3 = &quot;&quot;;</span>
<span class="p_add">+		pr_info(&quot;x86/fpu: Cost of: %-27s %-5s %-8s: %5Ld cycles\n&quot;, txt_1, txt_2, txt_3, delta_min);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return delta_min;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Measure all the above primitives:</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init fpu__measure(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	s64 cost;</span>
<span class="p_add">+	s64 rdmsr_cost;</span>
<span class="p_add">+	s64 cli_cost, sti_cost, popf_cost, irq_save_cost;</span>
<span class="p_add">+	s64 cr0_read_cost, cr0_write_cost;</span>
<span class="p_add">+	s64 save_cost;</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;x86/fpu:##################################################################\n&quot;);</span>
<span class="p_add">+	pr_info(&quot;x86/fpu: Running FPU performance measurement suite (cache hot):\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	cost = measure(0, fn_empty, &quot;null&quot;, NULL, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;x86/fpu:########  CPU instructions:           ############################\n&quot;);</span>
<span class="p_add">+	measure(cost, fn_nop, &quot;NOP&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_rdtsc, &quot;RDTSC&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	rdmsr_cost = measure(cost, fn_rdmsr, &quot;RDMSR&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+	measure(cost+rdmsr_cost, fn_wrmsr,&quot;WRMSR&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	cli_cost = measure(cost, fn_cli_same, &quot;CLI&quot;, &quot;insn&quot;, &quot;same-IF&quot;);</span>
<span class="p_add">+	measure(cost+cli_cost, fn_cli_flip, &quot;CLI&quot;, &quot;insn&quot;, &quot;flip-IF&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	sti_cost = measure(cost, fn_sti_same, &quot;STI&quot;, &quot;insn&quot;, &quot;same-IF&quot;);</span>
<span class="p_add">+	measure(cost+sti_cost, fn_sti_flip, &quot;STI&quot;, &quot;insn&quot;, &quot;flip-IF&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	measure(cost, fn_pushf,	&quot;PUSHF&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	popf_cost = measure(cost, fn_popf_baseline, NULL, NULL, NULL);</span>
<span class="p_add">+	measure(cost+popf_cost, fn_popf_same, &quot;POPF&quot;, &quot;insn&quot;, &quot;same-IF&quot;);</span>
<span class="p_add">+	measure(cost+popf_cost, fn_popf_flip, &quot;POPF&quot;, &quot;insn&quot;, &quot;flip-IF&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;x86/fpu:########  IRQ save/restore APIs:      ############################\n&quot;);</span>
<span class="p_add">+	irq_save_cost = measure(cost, fn_irq_save_baseline, NULL, NULL, NULL);</span>
<span class="p_add">+	irq_save_cost += measure(cost+irq_save_cost, fn_irq_save, &quot;local_irq_save()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost+irq_save_cost, fn_irq_restore_same, &quot;local_irq_restore()&quot;, &quot;fn&quot;, &quot;same-IF&quot;);</span>
<span class="p_add">+	measure(cost+irq_save_cost, fn_irq_restore_flip, &quot;local_irq_restore()&quot;, &quot;fn&quot;, &quot;flip-IF&quot;);</span>
<span class="p_add">+	measure(cost+sti_cost, fn_irq_save_restore_same, &quot;irq_save()+restore()&quot;, &quot;fn&quot;, &quot;same-IF&quot;);</span>
<span class="p_add">+	measure(cost+sti_cost, fn_irq_save_restore_flip, &quot;irq_save()+restore()&quot;, &quot;fn&quot;, &quot;flip-IF&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;x86/fpu:########  locking APIs:               ############################\n&quot;);</span>
<span class="p_add">+	measure(cost, fn_smp_mb, &quot;smp_mb()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_cpu_relax, &quot;cpu_relax()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_spin_lock_unlock, &quot;spin_lock()+unlock()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_read_lock_unlock, &quot;read_lock()+unlock()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_write_lock_unlock, &quot;write_lock()+unlock()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_rcu_read_lock_unlock, &quot;rcu_read_lock()+unlock()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_preempt_disable_enable, &quot;preempt_disable()+enable()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost+sti_cost, fn_mutex_lock_unlock, &quot;mutex_lock()+unlock()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;x86/fpu:########  MM instructions:            ############################\n&quot;);</span>
<span class="p_add">+	measure(cost, fn_flush_tlb, &quot;__flush_tlb()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_flush_tlb_global, &quot;__flush_tlb_global()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_flush_tlb_one, &quot;__flush_tlb_one()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_flush_tlb_range, &quot;__flush_tlb_range()&quot;, &quot;fn&quot;, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;x86/fpu:########  FPU instructions:           ############################\n&quot;);</span>
<span class="p_add">+	cr0_read_cost = measure(cost, fn_read_cr0, &quot;CR0&quot;, &quot;read&quot;, NULL);</span>
<span class="p_add">+	cr0_write_cost = measure(cost+cr0_read_cost, fn_rw_cr0,	&quot;CR0&quot;, &quot;write&quot;, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	measure(cost+cr0_read_cost+cr0_write_cost, fn_cr0_fault, &quot;CR0::TS&quot;, &quot;fault&quot;, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	measure(cost, fn_fninit, &quot;FNINIT&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+	measure(cost, fn_fwait,	&quot;FWAIT&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	save_cost = measure(cost, fn_fsave, &quot;FSAVE&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+	measure(cost+save_cost, fn_frstor, &quot;FRSTOR&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpu_has_fxsr) {</span>
<span class="p_add">+		save_cost = measure(cost, fn_fxsave, &quot;FXSAVE&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+		measure(cost+save_cost, fn_fxrstor, &quot;FXRSTOR&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+		measure(cost+save_cost, fn_fxrstor_fault,&quot;FXRSTOR&quot;, &quot;fault&quot;, NULL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (cpu_has_xsaveopt) {</span>
<span class="p_add">+		save_cost = measure(cost, fn_xsave, &quot;XSAVE&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+		measure(cost+save_cost, fn_xrstor, &quot;XRSTOR&quot;, &quot;insn&quot;, NULL);</span>
<span class="p_add">+		measure(cost+save_cost, fn_xrstor_fault, &quot;XRSTOR&quot;, &quot;fault&quot;, NULL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	pr_info(&quot;x86/fpu:##################################################################\n&quot;);</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



