
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,4/4] hugetlb: add support for preferred node to alloc_huge_page_nodemask - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,4/4] hugetlb: add support for preferred node to alloc_huge_page_nodemask</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 12, 2017, 9:06 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170612090656.GD7476@dhcp22.suse.cz&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9780599/mbox/"
   >mbox</a>
|
   <a href="/patch/9780599/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9780599/">/patch/9780599/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	F392260212 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Jun 2017 09:07:09 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E5B7E283BF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Jun 2017 09:07:09 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id DA3922841C; Mon, 12 Jun 2017 09:07:09 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B9E4A283BF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 12 Jun 2017 09:07:08 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752092AbdFLJHB (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 12 Jun 2017 05:07:01 -0400
Received: from mx2.suse.de ([195.135.220.15]:45829 &quot;EHLO mx1.suse.de&quot;
	rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
	id S1751968AbdFLJG7 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 12 Jun 2017 05:06:59 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay1.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx1.suse.de (Postfix) with ESMTP id 0C759AABA;
	Mon, 12 Jun 2017 09:06:58 +0000 (UTC)
Date: Mon, 12 Jun 2017 11:06:56 +0200
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Vlastimil Babka &lt;vbabka@suse.cz&gt;
Cc: linux-mm@kvack.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Xishi Qiu &lt;qiuxishi@huawei.com&gt;, zhong jiang &lt;zhongjiang@huawei.com&gt;,
	Joonsoo Kim &lt;js1304@gmail.com&gt;, LKML &lt;linux-kernel@vger.kernel.org&gt;
Subject: Re: [RFC PATCH 4/4] hugetlb: add support for preferred node to
	alloc_huge_page_nodemask
Message-ID: &lt;20170612090656.GD7476@dhcp22.suse.cz&gt;
References: &lt;20170608074553.22152-1-mhocko@kernel.org&gt;
	&lt;20170608074553.22152-5-mhocko@kernel.org&gt;
	&lt;a41926b2-1e49-d6a6-f92e-5ebf2fa101e3@suse.cz&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;a41926b2-1e49-d6a6-f92e-5ebf2fa101e3@suse.cz&gt;
User-Agent: Mutt/1.5.23 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 12, 2017, 9:06 a.m.</div>
<pre class="content">
On Thu 08-06-17 10:38:06, Vlastimil Babka wrote:
<span class="quote">&gt; On 06/08/2017 09:45 AM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; alloc_huge_page_nodemask tries to allocate from any numa node in the</span>
<span class="quote">&gt; &gt; allowed node mask. This might lead to filling up low NUMA nodes while</span>
<span class="quote">&gt; &gt; others are not used. We can reduce this risk by introducing a concept</span>
<span class="quote">&gt; &gt; of the preferred node similar to what we have in the regular page</span>
<span class="quote">&gt; &gt; allocator. We will start allocating from the preferred nid and then</span>
<span class="quote">&gt; &gt; iterate over all allowed nodes until we try them all. Introduce</span>
<span class="quote">&gt; &gt; for_each_node_mask_preferred helper which does the iteration and reuse</span>
<span class="quote">&gt; &gt; the available preferred node in new_page_nodemask which is currently</span>
<span class="quote">&gt; &gt; the only caller of alloc_huge_page_nodemask.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s better, yeah. I don&#39;t think it would be too hard to use a</span>
<span class="quote">&gt; zonelist though. What do others think?</span>

OK, so I&#39;ve given it a try. This is untested yet but it doesn&#39;t look all
that bad. dequeue_huge_page_node will most proably see some clean up on
top but I&#39;ve kept it for simplicity for now.
---
From 597ab787ac081b57db13ce5576700163d0c1208c Mon Sep 17 00:00:00 2001
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
Date: Wed, 7 Jun 2017 10:31:59 +0200
Subject: [PATCH] hugetlb: add support for preferred node to
 alloc_huge_page_nodemask

alloc_huge_page_nodemask tries to allocate from any numa node in the
allowed node mask. This might lead to filling up low NUMA nodes while
others are not used. We can reduce this risk by introducing a concept
of the preferred node similar to what we have in the regular page
allocator. We will start allocating from the preferred nid and then
iterate over all allowed nodes in the zonelist order until we try them
all.

This is mimicking the page allocator logic except it operates on
per-node mempools. dequeue_huge_page_vma already does this so distill
the zonelist logic into a more generic dequeue_huge_page_nodemask
and use it in alloc_huge_page_nodemask.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 include/linux/hugetlb.h |   3 +-
 include/linux/migrate.h |   2 +-
 mm/hugetlb.c            | 111 +++++++++++++++++++++++++-----------------------
 3 files changed, 60 insertions(+), 56 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 12, 2017, 11:48 a.m.</div>
<pre class="content">
On Mon 12-06-17 11:06:56, Michal Hocko wrote:
[...]
<span class="quote">&gt; @@ -1723,29 +1729,26 @@ struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="quote">&gt; +struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="quote">&gt; +		nodemask_t *nmask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt; -	int node;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {</span>
<span class="quote">&gt; -		for_each_node_mask(node, *nmask) {</span>
<span class="quote">&gt; -			page = dequeue_huge_page_node_exact(h, node);</span>
<span class="quote">&gt; -			if (page)</span>
<span class="quote">&gt; -				break;</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; +		page = dequeue_huge_page_nodemask(h, preferred_nid, nmask);</span>
<span class="quote">&gt; +		if (page)</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +unlock:</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  	if (page)</span>
<span class="quote">&gt;  		return page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* No reservations, try to overcommit */</span>
<span class="quote">&gt; -	for_each_node_mask(node, *nmask) {</span>
<span class="quote">&gt; -		page = __alloc_buddy_huge_page_no_mpol(h, node);</span>
<span class="quote">&gt; -		if (page)</span>
<span class="quote">&gt; -			return page;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +	page = __alloc_buddy_huge_page_no_mpol(h, preferred_nid);</span>
<span class="quote">&gt; +	if (page)</span>
<span class="quote">&gt; +		return page;</span>

I was too quick. The fallback allocation needs some more love. I am
working on this but it quickly gets quite hairy so let&#39;s see whether
this still can converge to something reasonable.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - June 12, 2017, 11:53 a.m.</div>
<pre class="content">
On 06/12/2017 11:06 AM, Michal Hocko wrote:
<span class="quote">&gt; On Thu 08-06-17 10:38:06, Vlastimil Babka wrote:</span>
<span class="quote">&gt;&gt; On 06/08/2017 09:45 AM, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt;&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; alloc_huge_page_nodemask tries to allocate from any numa node in the</span>
<span class="quote">&gt;&gt;&gt; allowed node mask. This might lead to filling up low NUMA nodes while</span>
<span class="quote">&gt;&gt;&gt; others are not used. We can reduce this risk by introducing a concept</span>
<span class="quote">&gt;&gt;&gt; of the preferred node similar to what we have in the regular page</span>
<span class="quote">&gt;&gt;&gt; allocator. We will start allocating from the preferred nid and then</span>
<span class="quote">&gt;&gt;&gt; iterate over all allowed nodes until we try them all. Introduce</span>
<span class="quote">&gt;&gt;&gt; for_each_node_mask_preferred helper which does the iteration and reuse</span>
<span class="quote">&gt;&gt;&gt; the available preferred node in new_page_nodemask which is currently</span>
<span class="quote">&gt;&gt;&gt; the only caller of alloc_huge_page_nodemask.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; That&#39;s better, yeah. I don&#39;t think it would be too hard to use a</span>
<span class="quote">&gt;&gt; zonelist though. What do others think?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK, so I&#39;ve given it a try. This is untested yet but it doesn&#39;t look all</span>
<span class="quote">&gt; that bad. dequeue_huge_page_node will most proably see some clean up on</span>
<span class="quote">&gt; top but I&#39;ve kept it for simplicity for now.</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; From 597ab787ac081b57db13ce5576700163d0c1208c Mon Sep 17 00:00:00 2001</span>
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Date: Wed, 7 Jun 2017 10:31:59 +0200</span>
<span class="quote">&gt; Subject: [PATCH] hugetlb: add support for preferred node to</span>
<span class="quote">&gt;  alloc_huge_page_nodemask</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; alloc_huge_page_nodemask tries to allocate from any numa node in the</span>
<span class="quote">&gt; allowed node mask. This might lead to filling up low NUMA nodes while</span>
<span class="quote">&gt; others are not used. We can reduce this risk by introducing a concept</span>
<span class="quote">&gt; of the preferred node similar to what we have in the regular page</span>
<span class="quote">&gt; allocator. We will start allocating from the preferred nid and then</span>
<span class="quote">&gt; iterate over all allowed nodes in the zonelist order until we try them</span>
<span class="quote">&gt; all.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is mimicking the page allocator logic except it operates on</span>
<span class="quote">&gt; per-node mempools. dequeue_huge_page_vma already does this so distill</span>
<span class="quote">&gt; the zonelist logic into a more generic dequeue_huge_page_nodemask</span>
<span class="quote">&gt; and use it in alloc_huge_page_nodemask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/hugetlb.h |   3 +-</span>
<span class="quote">&gt;  include/linux/migrate.h |   2 +-</span>
<span class="quote">&gt;  mm/hugetlb.c            | 111 +++++++++++++++++++++++++-----------------------</span>
<span class="quote">&gt;  3 files changed, 60 insertions(+), 56 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="quote">&gt; index c469191bb13b..d4c33a8583be 100644</span>
<span class="quote">&gt; --- a/include/linux/hugetlb.h</span>
<span class="quote">&gt; +++ b/include/linux/hugetlb.h</span>
<span class="quote">&gt; @@ -349,7 +349,8 @@ struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  struct page *alloc_huge_page_node(struct hstate *h, int nid);</span>
<span class="quote">&gt;  struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				unsigned long addr, int avoid_reserve);</span>
<span class="quote">&gt; -struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask);</span>
<span class="quote">&gt; +struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="quote">&gt; +				nodemask_t *nmask);</span>
<span class="quote">&gt;  int huge_add_to_page_cache(struct page *page, struct address_space *mapping,</span>
<span class="quote">&gt;  			pgoff_t idx);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="quote">&gt; index f80c9882403a..af3ccf93efaa 100644</span>
<span class="quote">&gt; --- a/include/linux/migrate.h</span>
<span class="quote">&gt; +++ b/include/linux/migrate.h</span>
<span class="quote">&gt; @@ -38,7 +38,7 @@ static inline struct page *new_page_nodemask(struct page *page, int preferred_ni</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (PageHuge(page))</span>
<span class="quote">&gt;  		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),</span>
<span class="quote">&gt; -				nodemask);</span>
<span class="quote">&gt; +				preferred_nid, nodemask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (PageHighMem(page)</span>
<span class="quote">&gt;  	    || (zone_idx(page_zone(page)) == ZONE_MOVABLE))</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 01c11ceb47d6..bbb3a1a46c64 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -897,29 +897,58 @@ static struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt; +/* Movability of hugepages depends on migration support. */</span>
<span class="quote">&gt; +static inline gfp_t htlb_alloc_mask(struct hstate *h)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct page *page;</span>
<span class="quote">&gt; -	int node;</span>
<span class="quote">&gt; +	if (hugepages_treat_as_movable || hugepage_migration_supported(h))</span>
<span class="quote">&gt; +		return GFP_HIGHUSER_MOVABLE;</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		return GFP_HIGHUSER;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (nid != NUMA_NO_NODE)</span>
<span class="quote">&gt; -		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="quote">&gt; +static struct page *dequeue_huge_page_nodemask(struct hstate *h, int nid,</span>
<span class="quote">&gt; +		nodemask_t *nmask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned int cpuset_mems_cookie;</span>
<span class="quote">&gt; +	struct zonelist *zonelist;</span>
<span class="quote">&gt; +	struct page *page = NULL;</span>
<span class="quote">&gt; +	struct zone *zone;</span>
<span class="quote">&gt; +	struct zoneref *z;</span>
<span class="quote">&gt; +	gfp_t gfp_mask;</span>
<span class="quote">&gt; +	int node = -1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	gfp_mask = htlb_alloc_mask(h);</span>
<span class="quote">&gt; +	zonelist = node_zonelist(nid, gfp_mask);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +retry_cpuset:</span>
<span class="quote">&gt; +	cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="quote">&gt; +	for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {</span>
<span class="quote">&gt; +		if (!cpuset_zone_allowed(zone, gfp_mask))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * no need to ask again on the same node. Pool is node rather than</span>
<span class="quote">&gt; +		 * zone aware</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (zone_to_nid(zone) == node)</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		node = zone_to_nid(zone);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	for_each_online_node(node) {</span>
<span class="quote">&gt;  		page = dequeue_huge_page_node_exact(h, node);</span>
<span class="quote">&gt;  		if (page)</span>
<span class="quote">&gt; -			return page;</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +	if (unlikely(!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie)))</span>
<span class="quote">&gt; +		goto retry_cpuset;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	return NULL;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/* Movability of hugepages depends on migration support. */</span>
<span class="quote">&gt; -static inline gfp_t htlb_alloc_mask(struct hstate *h)</span>
<span class="quote">&gt; +static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (hugepages_treat_as_movable || hugepage_migration_supported(h))</span>
<span class="quote">&gt; -		return GFP_HIGHUSER_MOVABLE;</span>
<span class="quote">&gt; -	else</span>
<span class="quote">&gt; -		return GFP_HIGHUSER;</span>
<span class="quote">&gt; +	if (nid != NUMA_NO_NODE)</span>
<span class="quote">&gt; +		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return dequeue_huge_page_nodemask(h, nid, NULL);</span>

This with nid == NUMA_NO_NODE will break at node_zonelist(nid,
gfp_mask); in dequeue_huge_page_nodemask(). I guess just use the local
node as preferred.
<span class="quote">
&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static struct page *dequeue_huge_page_vma(struct hstate *h,</span>
<span class="quote">&gt; @@ -927,15 +956,10 @@ static struct page *dequeue_huge_page_vma(struct hstate *h,</span>
<span class="quote">&gt;  				unsigned long address, int avoid_reserve,</span>
<span class="quote">&gt;  				long chg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct page *page = NULL;</span>
<span class="quote">&gt; +	struct page *page;</span>
<span class="quote">&gt;  	struct mempolicy *mpol;</span>
<span class="quote">&gt;  	nodemask_t *nodemask;</span>
<span class="quote">&gt; -	gfp_t gfp_mask;</span>
<span class="quote">&gt;  	int nid;</span>
<span class="quote">&gt; -	struct zonelist *zonelist;</span>
<span class="quote">&gt; -	struct zone *zone;</span>
<span class="quote">&gt; -	struct zoneref *z;</span>
<span class="quote">&gt; -	unsigned int cpuset_mems_cookie;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * A child process with MAP_PRIVATE mappings created by their parent</span>
<span class="quote">&gt; @@ -950,32 +974,14 @@ static struct page *dequeue_huge_page_vma(struct hstate *h,</span>
<span class="quote">&gt;  	if (avoid_reserve &amp;&amp; h-&gt;free_huge_pages - h-&gt;resv_huge_pages == 0)</span>
<span class="quote">&gt;  		goto err;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -retry_cpuset:</span>
<span class="quote">&gt; -	cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="quote">&gt; -	gfp_mask = htlb_alloc_mask(h);</span>
<span class="quote">&gt; -	nid = huge_node(vma, address, gfp_mask, &amp;mpol, &amp;nodemask);</span>
<span class="quote">&gt; -	zonelist = node_zonelist(nid, gfp_mask);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	for_each_zone_zonelist_nodemask(zone, z, zonelist,</span>
<span class="quote">&gt; -						MAX_NR_ZONES - 1, nodemask) {</span>
<span class="quote">&gt; -		if (cpuset_zone_allowed(zone, gfp_mask)) {</span>
<span class="quote">&gt; -			page = dequeue_huge_page_node(h, zone_to_nid(zone));</span>
<span class="quote">&gt; -			if (page) {</span>
<span class="quote">&gt; -				if (avoid_reserve)</span>
<span class="quote">&gt; -					break;</span>
<span class="quote">&gt; -				if (!vma_has_reserves(vma, chg))</span>
<span class="quote">&gt; -					break;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -				SetPagePrivate(page);</span>
<span class="quote">&gt; -				h-&gt;resv_huge_pages--;</span>
<span class="quote">&gt; -				break;</span>
<span class="quote">&gt; -			}</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; +	nid = huge_node(vma, address, htlb_alloc_mask(h), &amp;mpol, &amp;nodemask);</span>
<span class="quote">&gt; +	page = dequeue_huge_page_nodemask(h, nid, nodemask);</span>
<span class="quote">&gt; +	if (page &amp;&amp; !(avoid_reserve || (!vma_has_reserves(vma, chg)))) {</span>

Ugh that&#39;s hard to parse.
What about: if (page &amp;&amp; !avoid_reserve &amp;&amp; vma_has_reserves(...)) ?
<span class="quote">
&gt; +		SetPagePrivate(page);</span>
<span class="quote">&gt; +		h-&gt;resv_huge_pages--;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mpol_cond_put(mpol);</span>
<span class="quote">&gt; -	if (unlikely(!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie)))</span>
<span class="quote">&gt; -		goto retry_cpuset;</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  err:</span>
<span class="quote">&gt; @@ -1723,29 +1729,26 @@ struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="quote">&gt; +struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="quote">&gt; +		nodemask_t *nmask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt; -	int node;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {</span>
<span class="quote">&gt; -		for_each_node_mask(node, *nmask) {</span>
<span class="quote">&gt; -			page = dequeue_huge_page_node_exact(h, node);</span>
<span class="quote">&gt; -			if (page)</span>
<span class="quote">&gt; -				break;</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; +		page = dequeue_huge_page_nodemask(h, preferred_nid, nmask);</span>
<span class="quote">&gt; +		if (page)</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +unlock:</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  	if (page)</span>
<span class="quote">&gt;  		return page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* No reservations, try to overcommit */</span>
<span class="quote">&gt; -	for_each_node_mask(node, *nmask) {</span>
<span class="quote">&gt; -		page = __alloc_buddy_huge_page_no_mpol(h, node);</span>
<span class="quote">&gt; -		if (page)</span>
<span class="quote">&gt; -			return page;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +	page = __alloc_buddy_huge_page_no_mpol(h, preferred_nid);</span>
<span class="quote">&gt; +	if (page)</span>
<span class="quote">&gt; +		return page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return NULL;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 12, 2017, 12:20 p.m.</div>
<pre class="content">
On Mon 12-06-17 13:53:51, Vlastimil Babka wrote:
<span class="quote">&gt; On 06/12/2017 11:06 AM, Michal Hocko wrote:</span>
[...]
<span class="quote">&gt; &gt; -/* Movability of hugepages depends on migration support. */</span>
<span class="quote">&gt; &gt; -static inline gfp_t htlb_alloc_mask(struct hstate *h)</span>
<span class="quote">&gt; &gt; +static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; -	if (hugepages_treat_as_movable || hugepage_migration_supported(h))</span>
<span class="quote">&gt; &gt; -		return GFP_HIGHUSER_MOVABLE;</span>
<span class="quote">&gt; &gt; -	else</span>
<span class="quote">&gt; &gt; -		return GFP_HIGHUSER;</span>
<span class="quote">&gt; &gt; +	if (nid != NUMA_NO_NODE)</span>
<span class="quote">&gt; &gt; +		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return dequeue_huge_page_nodemask(h, nid, NULL);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This with nid == NUMA_NO_NODE will break at node_zonelist(nid,</span>
<span class="quote">&gt; gfp_mask); in dequeue_huge_page_nodemask(). I guess just use the local</span>
<span class="quote">&gt; node as preferred.</span>

You are right. Anyway I have a patch to remove this helper altogether.
<span class="quote">
&gt; &gt; -retry_cpuset:</span>
<span class="quote">&gt; &gt; -	cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="quote">&gt; &gt; -	gfp_mask = htlb_alloc_mask(h);</span>
<span class="quote">&gt; &gt; -	nid = huge_node(vma, address, gfp_mask, &amp;mpol, &amp;nodemask);</span>
<span class="quote">&gt; &gt; -	zonelist = node_zonelist(nid, gfp_mask);</span>
<span class="quote">&gt; &gt; -</span>
<span class="quote">&gt; &gt; -	for_each_zone_zonelist_nodemask(zone, z, zonelist,</span>
<span class="quote">&gt; &gt; -						MAX_NR_ZONES - 1, nodemask) {</span>
<span class="quote">&gt; &gt; -		if (cpuset_zone_allowed(zone, gfp_mask)) {</span>
<span class="quote">&gt; &gt; -			page = dequeue_huge_page_node(h, zone_to_nid(zone));</span>
<span class="quote">&gt; &gt; -			if (page) {</span>
<span class="quote">&gt; &gt; -				if (avoid_reserve)</span>
<span class="quote">&gt; &gt; -					break;</span>
<span class="quote">&gt; &gt; -				if (!vma_has_reserves(vma, chg))</span>
<span class="quote">&gt; &gt; -					break;</span>
<span class="quote">&gt; &gt; -</span>
<span class="quote">&gt; &gt; -				SetPagePrivate(page);</span>
<span class="quote">&gt; &gt; -				h-&gt;resv_huge_pages--;</span>
<span class="quote">&gt; &gt; -				break;</span>
<span class="quote">&gt; &gt; -			}</span>
<span class="quote">&gt; &gt; -		}</span>
<span class="quote">&gt; &gt; +	nid = huge_node(vma, address, htlb_alloc_mask(h), &amp;mpol, &amp;nodemask);</span>
<span class="quote">&gt; &gt; +	page = dequeue_huge_page_nodemask(h, nid, nodemask);</span>
<span class="quote">&gt; &gt; +	if (page &amp;&amp; !(avoid_reserve || (!vma_has_reserves(vma, chg)))) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ugh that&#39;s hard to parse.</span>
<span class="quote">&gt; What about: if (page &amp;&amp; !avoid_reserve &amp;&amp; vma_has_reserves(...)) ?</span>

Yeah, I have just translated the two breaks into a single condition
without scratching my head to much. If you think that this face of De Morgan
is nicer I can use it.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index c469191bb13b..d4c33a8583be 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -349,7 +349,8 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask);</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="p_add">+				nodemask_t *nmask);</span>
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
<span class="p_header">diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="p_header">index f80c9882403a..af3ccf93efaa 100644</span>
<span class="p_header">--- a/include/linux/migrate.h</span>
<span class="p_header">+++ b/include/linux/migrate.h</span>
<span class="p_chunk">@@ -38,7 +38,7 @@</span> <span class="p_context"> static inline struct page *new_page_nodemask(struct page *page, int preferred_ni</span>
 
 	if (PageHuge(page))
 		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),
<span class="p_del">-				nodemask);</span>
<span class="p_add">+				preferred_nid, nodemask);</span>
 
 	if (PageHighMem(page)
 	    || (zone_idx(page_zone(page)) == ZONE_MOVABLE))
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 01c11ceb47d6..bbb3a1a46c64 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -897,29 +897,58 @@</span> <span class="p_context"> static struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)</span>
 	return page;
 }
 
<span class="p_del">-static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
<span class="p_add">+/* Movability of hugepages depends on migration support. */</span>
<span class="p_add">+static inline gfp_t htlb_alloc_mask(struct hstate *h)</span>
 {
<span class="p_del">-	struct page *page;</span>
<span class="p_del">-	int node;</span>
<span class="p_add">+	if (hugepages_treat_as_movable || hugepage_migration_supported(h))</span>
<span class="p_add">+		return GFP_HIGHUSER_MOVABLE;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return GFP_HIGHUSER;</span>
<span class="p_add">+}</span>
 
<span class="p_del">-	if (nid != NUMA_NO_NODE)</span>
<span class="p_del">-		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="p_add">+static struct page *dequeue_huge_page_nodemask(struct hstate *h, int nid,</span>
<span class="p_add">+		nodemask_t *nmask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int cpuset_mems_cookie;</span>
<span class="p_add">+	struct zonelist *zonelist;</span>
<span class="p_add">+	struct page *page = NULL;</span>
<span class="p_add">+	struct zone *zone;</span>
<span class="p_add">+	struct zoneref *z;</span>
<span class="p_add">+	gfp_t gfp_mask;</span>
<span class="p_add">+	int node = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	gfp_mask = htlb_alloc_mask(h);</span>
<span class="p_add">+	zonelist = node_zonelist(nid, gfp_mask);</span>
<span class="p_add">+</span>
<span class="p_add">+retry_cpuset:</span>
<span class="p_add">+	cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="p_add">+	for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {</span>
<span class="p_add">+		if (!cpuset_zone_allowed(zone, gfp_mask))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * no need to ask again on the same node. Pool is node rather than</span>
<span class="p_add">+		 * zone aware</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (zone_to_nid(zone) == node)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		node = zone_to_nid(zone);</span>
 
<span class="p_del">-	for_each_online_node(node) {</span>
 		page = dequeue_huge_page_node_exact(h, node);
 		if (page)
<span class="p_del">-			return page;</span>
<span class="p_add">+			break;</span>
 	}
<span class="p_add">+	if (unlikely(!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie)))</span>
<span class="p_add">+		goto retry_cpuset;</span>
<span class="p_add">+</span>
 	return NULL;
 }
 
<span class="p_del">-/* Movability of hugepages depends on migration support. */</span>
<span class="p_del">-static inline gfp_t htlb_alloc_mask(struct hstate *h)</span>
<span class="p_add">+static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
 {
<span class="p_del">-	if (hugepages_treat_as_movable || hugepage_migration_supported(h))</span>
<span class="p_del">-		return GFP_HIGHUSER_MOVABLE;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		return GFP_HIGHUSER;</span>
<span class="p_add">+	if (nid != NUMA_NO_NODE)</span>
<span class="p_add">+		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="p_add">+</span>
<span class="p_add">+	return dequeue_huge_page_nodemask(h, nid, NULL);</span>
 }
 
 static struct page *dequeue_huge_page_vma(struct hstate *h,
<span class="p_chunk">@@ -927,15 +956,10 @@</span> <span class="p_context"> static struct page *dequeue_huge_page_vma(struct hstate *h,</span>
 				unsigned long address, int avoid_reserve,
 				long chg)
 {
<span class="p_del">-	struct page *page = NULL;</span>
<span class="p_add">+	struct page *page;</span>
 	struct mempolicy *mpol;
 	nodemask_t *nodemask;
<span class="p_del">-	gfp_t gfp_mask;</span>
 	int nid;
<span class="p_del">-	struct zonelist *zonelist;</span>
<span class="p_del">-	struct zone *zone;</span>
<span class="p_del">-	struct zoneref *z;</span>
<span class="p_del">-	unsigned int cpuset_mems_cookie;</span>
 
 	/*
 	 * A child process with MAP_PRIVATE mappings created by their parent
<span class="p_chunk">@@ -950,32 +974,14 @@</span> <span class="p_context"> static struct page *dequeue_huge_page_vma(struct hstate *h,</span>
 	if (avoid_reserve &amp;&amp; h-&gt;free_huge_pages - h-&gt;resv_huge_pages == 0)
 		goto err;
 
<span class="p_del">-retry_cpuset:</span>
<span class="p_del">-	cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="p_del">-	gfp_mask = htlb_alloc_mask(h);</span>
<span class="p_del">-	nid = huge_node(vma, address, gfp_mask, &amp;mpol, &amp;nodemask);</span>
<span class="p_del">-	zonelist = node_zonelist(nid, gfp_mask);</span>
<span class="p_del">-</span>
<span class="p_del">-	for_each_zone_zonelist_nodemask(zone, z, zonelist,</span>
<span class="p_del">-						MAX_NR_ZONES - 1, nodemask) {</span>
<span class="p_del">-		if (cpuset_zone_allowed(zone, gfp_mask)) {</span>
<span class="p_del">-			page = dequeue_huge_page_node(h, zone_to_nid(zone));</span>
<span class="p_del">-			if (page) {</span>
<span class="p_del">-				if (avoid_reserve)</span>
<span class="p_del">-					break;</span>
<span class="p_del">-				if (!vma_has_reserves(vma, chg))</span>
<span class="p_del">-					break;</span>
<span class="p_del">-</span>
<span class="p_del">-				SetPagePrivate(page);</span>
<span class="p_del">-				h-&gt;resv_huge_pages--;</span>
<span class="p_del">-				break;</span>
<span class="p_del">-			}</span>
<span class="p_del">-		}</span>
<span class="p_add">+	nid = huge_node(vma, address, htlb_alloc_mask(h), &amp;mpol, &amp;nodemask);</span>
<span class="p_add">+	page = dequeue_huge_page_nodemask(h, nid, nodemask);</span>
<span class="p_add">+	if (page &amp;&amp; !(avoid_reserve || (!vma_has_reserves(vma, chg)))) {</span>
<span class="p_add">+		SetPagePrivate(page);</span>
<span class="p_add">+		h-&gt;resv_huge_pages--;</span>
 	}
 
 	mpol_cond_put(mpol);
<span class="p_del">-	if (unlikely(!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie)))</span>
<span class="p_del">-		goto retry_cpuset;</span>
 	return page;
 
 err:
<span class="p_chunk">@@ -1723,29 +1729,26 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
 	return page;
 }
 
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="p_add">+		nodemask_t *nmask)</span>
 {
 	struct page *page = NULL;
<span class="p_del">-	int node;</span>
 
 	spin_lock(&amp;hugetlb_lock);
 	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {
<span class="p_del">-		for_each_node_mask(node, *nmask) {</span>
<span class="p_del">-			page = dequeue_huge_page_node_exact(h, node);</span>
<span class="p_del">-			if (page)</span>
<span class="p_del">-				break;</span>
<span class="p_del">-		}</span>
<span class="p_add">+		page = dequeue_huge_page_nodemask(h, preferred_nid, nmask);</span>
<span class="p_add">+		if (page)</span>
<span class="p_add">+			goto unlock;</span>
 	}
<span class="p_add">+unlock:</span>
 	spin_unlock(&amp;hugetlb_lock);
 	if (page)
 		return page;
 
 	/* No reservations, try to overcommit */
<span class="p_del">-	for_each_node_mask(node, *nmask) {</span>
<span class="p_del">-		page = __alloc_buddy_huge_page_no_mpol(h, node);</span>
<span class="p_del">-		if (page)</span>
<span class="p_del">-			return page;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	page = __alloc_buddy_huge_page_no_mpol(h, preferred_nid);</span>
<span class="p_add">+	if (page)</span>
<span class="p_add">+		return page;</span>
 
 	return NULL;
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



