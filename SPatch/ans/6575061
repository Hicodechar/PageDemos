
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,4/4] pagemap: switch to the new format and do some cleanup - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,4/4] pagemap: switch to the new format and do some cleanup</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=80221">Konstantin Khlebnikov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 9, 2015, 8 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20150609200021.21971.13598.stgit@zurg&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6575061/mbox/"
   >mbox</a>
|
   <a href="/patch/6575061/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6575061/">/patch/6575061/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 968E99F399
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 20:01:51 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 37D0E204A2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 20:01:50 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 6A61C20549
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 20:01:48 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933328AbbFIUBe (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 9 Jun 2015 16:01:34 -0400
Received: from mail-la0-f68.google.com ([209.85.215.68]:33479 &quot;EHLO
	mail-la0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753610AbbFIUA2 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 9 Jun 2015 16:00:28 -0400
Received: by lamq1 with SMTP id q1so3236476lam.0;
	Tue, 09 Jun 2015 13:00:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=subject:from:to:cc:date:message-id:in-reply-to:references
	:user-agent:mime-version:content-type:content-transfer-encoding;
	bh=HXBGB/VWCT50T3JsiHT8WHobt0O+OCfCdgif9X4nuPw=;
	b=HFPOCa9kCkhp7z0L4+Ztd2F4txpGJ1G39MXZ1K1gRw1gHZP2xoWsiWpur4r+CanIUE
	DPjzsxIYZrRNvlMo+jxWInDVmQwfwzn6RAPnCixy6J96PQJuWs2yZLa/IEJXVSKHarij
	Odqpx1HK2bSGkjzRgiqm3QPg8ZYl2criENpQsBltEOUEIVVj3Qy7nlrVtwpx+H9zowVZ
	RTTIDpQ7Mv8/eY8cTRcelS0u101DxnywZ78EfomaUTTFKkJZ3wFJ7hTdSa5WTgM2aQqp
	1KMonUtOTxp/VBEO2u/VkTeJo+tpzA/gBepkyvrh8wNl65qOqDzZ9shWYXJNgMJgFSQr
	jD5A==
X-Received: by 10.112.213.108 with SMTP id
	nr12mr24420516lbc.42.1433880026422; 
	Tue, 09 Jun 2015 13:00:26 -0700 (PDT)
Received: from localhost (37-145-42-244.broadband.corbina.ru.
	[37.145.42.244]) by mx.google.com with ESMTPSA id
	yq5sm1636670lab.32.2015.06.09.13.00.23
	(version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 09 Jun 2015 13:00:25 -0700 (PDT)
Subject: [PATCH v3 4/4] pagemap: switch to the new format and do some cleanup
From: Konstantin Khlebnikov &lt;koct9i@gmail.com&gt;
To: linux-mm@kvack.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;
Cc: linux-api@vger.kernel.org,
	Mark Williamson &lt;mwilliamson@undo-software.com&gt;,
	linux-kernel@vger.kernel.org, &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;
Date: Tue, 09 Jun 2015 23:00:21 +0300
Message-ID: &lt;20150609200021.21971.13598.stgit@zurg&gt;
In-Reply-To: &lt;20150609195333.21971.58194.stgit@zurg&gt;
References: &lt;20150609195333.21971.58194.stgit@zurg&gt;
User-Agent: StGit/0.17.1-dirty
MIME-Version: 1.0
Content-Type: text/plain; charset=&quot;utf-8&quot;
Content-Transfer-Encoding: 7bit
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.8 required=5.0 tests=BAYES_00,
	DKIM_ADSP_CUSTOM_MED, 
	DKIM_SIGNED, FREEMAIL_FROM, RCVD_IN_DNSWL_HI, T_DKIM_INVALID,
	T_RP_MATCHES_RCVD, 
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80221">Konstantin Khlebnikov</a> - June 9, 2015, 8 p.m.</div>
<pre class="content">
<span class="from">From: Konstantin Khlebnikov &lt;khlebnikov@yandex-team.ru&gt;</span>

This patch removes page-shift bits (scheduled to remove since 3.11) and
completes migration to the new bit layout. Also it cleans messy macro.
<span class="signed-off-by">
Signed-off-by: Konstantin Khlebnikov &lt;khlebnikov@yandex-team.ru&gt;</span>
---
 fs/proc/task_mmu.c    |  147 ++++++++++++++++---------------------------------
 tools/vm/page-types.c |   29 +++-------
 2 files changed, 58 insertions(+), 118 deletions(-)


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=131641">Mark Williamson</a> - June 12, 2015, 6:49 p.m.</div>
<pre class="content">
One tiny nitpick / typo, inline below - functionally, this looks good
from our side...
<span class="reviewed-by">
Reviewed-by: mwilliamson@undo-software.com</span>
<span class="tested-by">Tested-by: mwilliamson@undo-software.com</span>

On Tue, Jun 9, 2015 at 9:00 PM, Konstantin Khlebnikov &lt;koct9i@gmail.com&gt; wrote:
<span class="quote">&gt; From: Konstantin Khlebnikov &lt;khlebnikov@yandex-team.ru&gt;</span>

&lt;...snip...&gt;
<span class="quote">
&gt; -#define __PM_SOFT_DIRTY      (1LL)</span>
<span class="quote">&gt; -#define __PM_MMAP_EXCLUSIVE  (2LL)</span>
<span class="quote">&gt; -#define PM_PRESENT          PM_STATUS(4LL)</span>
<span class="quote">&gt; -#define PM_SWAP             PM_STATUS(2LL)</span>
<span class="quote">&gt; -#define PM_FILE             PM_STATUS(1LL)</span>
<span class="quote">&gt; -#define PM_NOT_PRESENT(v2)  PM_STATUS2(v2, 0)</span>
<span class="quote">&gt; +#define PM_ENTRY_BYTES         sizeof(pagemap_entry_t)</span>
<span class="quote">&gt; +#define PM_PFEAME_BITS         54</span>
<span class="quote">&gt; +#define PM_PFRAME_MASK         GENMASK_ULL(PM_PFEAME_BITS - 1, 0)</span>

s/PM_FEAME_BITS/PM_FRAME_BITS/ I presume?
<span class="quote">
&gt; +#define PM_SOFT_DIRTY          BIT_ULL(55)</span>
<span class="quote">&gt; +#define PM_MMAP_EXCLUSIVE      BIT_ULL(56)</span>
<span class="quote">&gt; +#define PM_FILE                        BIT_ULL(61)</span>
<span class="quote">&gt; +#define PM_SWAP                        BIT_ULL(62)</span>
<span class="quote">&gt; +#define PM_PRESENT             BIT_ULL(63)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define PM_END_OF_BUFFER    1</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -static inline pagemap_entry_t make_pme(u64 val)</span>
<span class="quote">&gt; +static inline pagemap_entry_t make_pme(u64 frame, u64 flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       return (pagemap_entry_t) { .pme = val };</span>
<span class="quote">&gt; +       return (pagemap_entry_t) { .pme = (frame &amp; PM_PFRAME_MASK) | flags };</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static int add_to_pagemap(unsigned long addr, pagemap_entry_t *pme,</span>
<span class="quote">&gt; @@ -1013,7 +977,7 @@ static int pagemap_pte_hole(unsigned long start, unsigned long end,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         while (addr &lt; end) {</span>
<span class="quote">&gt;                 struct vm_area_struct *vma = find_vma(walk-&gt;mm, addr);</span>
<span class="quote">&gt; -               pagemap_entry_t pme = make_pme(PM_NOT_PRESENT(pm-&gt;v2));</span>
<span class="quote">&gt; +               pagemap_entry_t pme = make_pme(0, 0);</span>
<span class="quote">&gt;                 /* End of address space hole, which we mark as non-present. */</span>
<span class="quote">&gt;                 unsigned long hole_end;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -1033,7 +997,7 @@ static int pagemap_pte_hole(unsigned long start, unsigned long end,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 /* Addresses in the VMA. */</span>
<span class="quote">&gt;                 if (vma-&gt;vm_flags &amp; VM_SOFTDIRTY)</span>
<span class="quote">&gt; -                       pme.pme |= PM_STATUS2(pm-&gt;v2, __PM_SOFT_DIRTY);</span>
<span class="quote">&gt; +                       pme = make_pme(0, PM_SOFT_DIRTY);</span>
<span class="quote">&gt;                 for (; addr &lt; min(end, vma-&gt;vm_end); addr += PAGE_SIZE) {</span>
<span class="quote">&gt;                         err = add_to_pagemap(addr, &amp;pme, pm);</span>
<span class="quote">&gt;                         if (err)</span>
<span class="quote">&gt; @@ -1044,50 +1008,44 @@ out:</span>
<span class="quote">&gt;         return err;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -static void pte_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,</span>
<span class="quote">&gt; +static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt;                 struct vm_area_struct *vma, unsigned long addr, pte_t pte)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -       u64 frame = 0, flags;</span>
<span class="quote">&gt; +       u64 frame = 0, flags = 0;</span>
<span class="quote">&gt;         struct page *page = NULL;</span>
<span class="quote">&gt; -       int flags2 = 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (pte_present(pte)) {</span>
<span class="quote">&gt;                 if (pm-&gt;show_pfn)</span>
<span class="quote">&gt;                         frame = pte_pfn(pte);</span>
<span class="quote">&gt; -               flags = PM_PRESENT;</span>
<span class="quote">&gt; +               flags |= PM_PRESENT;</span>
<span class="quote">&gt;                 page = vm_normal_page(vma, addr, pte);</span>
<span class="quote">&gt;                 if (pte_soft_dirty(pte))</span>
<span class="quote">&gt; -                       flags2 |= __PM_SOFT_DIRTY;</span>
<span class="quote">&gt; +                       flags |= PM_SOFT_DIRTY;</span>
<span class="quote">&gt;         } else if (is_swap_pte(pte)) {</span>
<span class="quote">&gt;                 swp_entry_t entry;</span>
<span class="quote">&gt;                 if (pte_swp_soft_dirty(pte))</span>
<span class="quote">&gt; -                       flags2 |= __PM_SOFT_DIRTY;</span>
<span class="quote">&gt; +                       flags |= PM_SOFT_DIRTY;</span>
<span class="quote">&gt;                 entry = pte_to_swp_entry(pte);</span>
<span class="quote">&gt;                 frame = swp_type(entry) |</span>
<span class="quote">&gt;                         (swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);</span>
<span class="quote">&gt; -               flags = PM_SWAP;</span>
<span class="quote">&gt; +               flags |= PM_SWAP;</span>
<span class="quote">&gt;                 if (is_migration_entry(entry))</span>
<span class="quote">&gt;                         page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; -       } else {</span>
<span class="quote">&gt; -               if (vma-&gt;vm_flags &amp; VM_SOFTDIRTY)</span>
<span class="quote">&gt; -                       flags2 |= __PM_SOFT_DIRTY;</span>
<span class="quote">&gt; -               *pme = make_pme(PM_NOT_PRESENT(pm-&gt;v2) | PM_STATUS2(pm-&gt;v2, flags2));</span>
<span class="quote">&gt; -               return;</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (page &amp;&amp; !PageAnon(page))</span>
<span class="quote">&gt;                 flags |= PM_FILE;</span>
<span class="quote">&gt;         if (page &amp;&amp; page_mapcount(page) == 1)</span>
<span class="quote">&gt; -               flags2 |= __PM_MMAP_EXCLUSIVE;</span>
<span class="quote">&gt; -       if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY))</span>
<span class="quote">&gt; -               flags2 |= __PM_SOFT_DIRTY;</span>
<span class="quote">&gt; +               flags |= PM_MMAP_EXCLUSIVE;</span>
<span class="quote">&gt; +       if (vma-&gt;vm_flags &amp; VM_SOFTDIRTY)</span>
<span class="quote">&gt; +               flags |= PM_SOFT_DIRTY;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       *pme = make_pme(PM_PFRAME(frame) | PM_STATUS2(pm-&gt;v2, flags2) | flags);</span>
<span class="quote">&gt; +       return make_pme(frame, flags);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="quote">&gt; -static void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,</span>
<span class="quote">&gt; -               pmd_t pmd, int offset, int pmd_flags2)</span>
<span class="quote">&gt; +static pagemap_entry_t thp_pmd_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt; +               pmd_t pmd, int offset, u64 flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         u64 frame = 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -1099,15 +1057,16 @@ static void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *p</span>
<span class="quote">&gt;         if (pmd_present(pmd)) {</span>
<span class="quote">&gt;                 if (pm-&gt;show_pfn)</span>
<span class="quote">&gt;                         frame = pmd_pfn(pmd) + offset;</span>
<span class="quote">&gt; -               *pme = make_pme(PM_PFRAME(frame) | PM_PRESENT |</span>
<span class="quote">&gt; -                               PM_STATUS2(pm-&gt;v2, pmd_flags2));</span>
<span class="quote">&gt; -       } else</span>
<span class="quote">&gt; -               *pme = make_pme(PM_NOT_PRESENT(pm-&gt;v2) | PM_STATUS2(pm-&gt;v2, pmd_flags2));</span>
<span class="quote">&gt; +               flags |= PM_PRESENT;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       return make_pme(frame, flags);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; -static inline void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,</span>
<span class="quote">&gt; -               pmd_t pmd, int offset, int pmd_flags2)</span>
<span class="quote">&gt; +static pagemap_entry_t thp_pmd_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt; +               pmd_t pmd, int offset, u64 flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +       return make_pme(0, 0);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -1121,18 +1080,16 @@ static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;         int err = 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {</span>
<span class="quote">&gt; -               int pmd_flags2;</span>
<span class="quote">&gt; +               u64 flags = 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(*pmd))</span>
<span class="quote">&gt; -                       pmd_flags2 = __PM_SOFT_DIRTY;</span>
<span class="quote">&gt; -               else</span>
<span class="quote">&gt; -                       pmd_flags2 = 0;</span>
<span class="quote">&gt; +                       flags |= PM_SOFT_DIRTY;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 if (pmd_present(*pmd)) {</span>
<span class="quote">&gt;                         struct page *page = pmd_page(*pmd);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                         if (page_mapcount(page) == 1)</span>
<span class="quote">&gt; -                               pmd_flags2 |= __PM_MMAP_EXCLUSIVE;</span>
<span class="quote">&gt; +                               flags |= PM_MMAP_EXCLUSIVE;</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 for (; addr != end; addr += PAGE_SIZE) {</span>
<span class="quote">&gt; @@ -1141,7 +1098,7 @@ static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                         offset = (addr &amp; ~PAGEMAP_WALK_MASK) &gt;&gt;</span>
<span class="quote">&gt;                                         PAGE_SHIFT;</span>
<span class="quote">&gt; -                       thp_pmd_to_pagemap_entry(&amp;pme, pm, *pmd, offset, pmd_flags2);</span>
<span class="quote">&gt; +                       pme = thp_pmd_to_pagemap_entry(pm, *pmd, offset, flags);</span>
<span class="quote">&gt;                         err = add_to_pagemap(addr, &amp;pme, pm);</span>
<span class="quote">&gt;                         if (err)</span>
<span class="quote">&gt;                                 break;</span>
<span class="quote">&gt; @@ -1161,7 +1118,7 @@ static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;         for (; addr &lt; end; pte++, addr += PAGE_SIZE) {</span>
<span class="quote">&gt;                 pagemap_entry_t pme;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -               pte_to_pagemap_entry(&amp;pme, pm, vma, addr, *pte);</span>
<span class="quote">&gt; +               pme = pte_to_pagemap_entry(pm, vma, addr, *pte);</span>
<span class="quote">&gt;                 err = add_to_pagemap(addr, &amp;pme, pm);</span>
<span class="quote">&gt;                 if (err)</span>
<span class="quote">&gt;                         break;</span>
<span class="quote">&gt; @@ -1174,19 +1131,18 @@ static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #ifdef CONFIG_HUGETLB_PAGE</span>
<span class="quote">&gt; -static void huge_pte_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,</span>
<span class="quote">&gt; -                                       pte_t pte, int offset, int flags2)</span>
<span class="quote">&gt; +static pagemap_entry_t huge_pte_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt; +                                       pte_t pte, int offset, u64 flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         u64 frame = 0;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (pte_present(pte)) {</span>
<span class="quote">&gt;                 if (pm-&gt;show_pfn)</span>
<span class="quote">&gt;                         frame = pte_pfn(pte) + offset;</span>
<span class="quote">&gt; -               *pme = make_pme(PM_PFRAME(frame) | PM_PRESENT |</span>
<span class="quote">&gt; -                               PM_STATUS2(pm-&gt;v2, flags2));</span>
<span class="quote">&gt; -       } else</span>
<span class="quote">&gt; -               *pme = make_pme(PM_NOT_PRESENT(pm-&gt;v2)                  |</span>
<span class="quote">&gt; -                               PM_STATUS2(pm-&gt;v2, flags2));</span>
<span class="quote">&gt; +               flags |= PM_PRESENT;</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       return make_pme(frame, flags);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /* This function walks within one hugetlb entry in the single call */</span>
<span class="quote">&gt; @@ -1197,17 +1153,15 @@ static int pagemap_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
<span class="quote">&gt;         struct pagemapread *pm = walk-&gt;private;</span>
<span class="quote">&gt;         struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt;         int err = 0;</span>
<span class="quote">&gt; -       int flags2;</span>
<span class="quote">&gt; +       u64 flags = 0;</span>
<span class="quote">&gt;         pagemap_entry_t pme;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (vma-&gt;vm_flags &amp; VM_SOFTDIRTY)</span>
<span class="quote">&gt; -               flags2 = __PM_SOFT_DIRTY;</span>
<span class="quote">&gt; -       else</span>
<span class="quote">&gt; -               flags2 = 0;</span>
<span class="quote">&gt; +               flags |= PM_SOFT_DIRTY;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         for (; addr != end; addr += PAGE_SIZE) {</span>
<span class="quote">&gt;                 int offset = (addr &amp; ~hmask) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; -               huge_pte_to_pagemap_entry(&amp;pme, pm, *pte, offset, flags2);</span>
<span class="quote">&gt; +               pme = huge_pte_to_pagemap_entry(pm, *pte, offset, flags);</span>
<span class="quote">&gt;                 err = add_to_pagemap(addr, &amp;pme, pm);</span>
<span class="quote">&gt;                 if (err)</span>
<span class="quote">&gt;                         return err;</span>
<span class="quote">&gt; @@ -1228,7 +1182,9 @@ static int pagemap_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
<span class="quote">&gt;   * Bits 0-54  page frame number (PFN) if present</span>
<span class="quote">&gt;   * Bits 0-4   swap type if swapped</span>
<span class="quote">&gt;   * Bits 5-54  swap offset if swapped</span>
<span class="quote">&gt; - * Bits 55-60 page shift (page size = 1&lt;&lt;page shift)</span>
<span class="quote">&gt; + * Bit  55    pte is soft-dirty (see Documentation/vm/soft-dirty.txt)</span>
<span class="quote">&gt; + * Bit  56    page exclusively mapped</span>
<span class="quote">&gt; + * Bits 57-60 zero</span>
<span class="quote">&gt;   * Bit  61    page is file-page or shared-anon</span>
<span class="quote">&gt;   * Bit  62    page swapped</span>
<span class="quote">&gt;   * Bit  63    page present</span>
<span class="quote">&gt; @@ -1269,7 +1225,6 @@ static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         /* do not disclose physical addresses: attack vector */</span>
<span class="quote">&gt;         pm.show_pfn = file_ns_capable(file, &amp;init_user_ns, CAP_SYS_ADMIN);</span>
<span class="quote">&gt; -       pm.v2 = soft_dirty_cleared;</span>
<span class="quote">&gt;         pm.len = (PAGEMAP_WALK_SIZE &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt;         pm.buffer = kmalloc(pm.len * PM_ENTRY_BYTES, GFP_TEMPORARY);</span>
<span class="quote">&gt;         ret = -ENOMEM;</span>
<span class="quote">&gt; @@ -1339,10 +1294,6 @@ static int pagemap_open(struct inode *inode, struct file *file)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;         struct mm_struct *mm;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       pr_warn_once(&quot;Bits 55-60 of /proc/PID/pagemap entries are about &quot;</span>
<span class="quote">&gt; -                       &quot;to stop being page-shift some time soon. See the &quot;</span>
<span class="quote">&gt; -                       &quot;linux/Documentation/vm/pagemap.txt for details.\n&quot;);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;         mm = proc_mem_open(inode, PTRACE_MODE_READ);</span>
<span class="quote">&gt;         if (IS_ERR(mm))</span>
<span class="quote">&gt;                 return PTR_ERR(mm);</span>
<span class="quote">&gt; diff --git a/tools/vm/page-types.c b/tools/vm/page-types.c</span>
<span class="quote">&gt; index 3a9f193..1fa872e 100644</span>
<span class="quote">&gt; --- a/tools/vm/page-types.c</span>
<span class="quote">&gt; +++ b/tools/vm/page-types.c</span>
<span class="quote">&gt; @@ -57,26 +57,15 @@</span>
<span class="quote">&gt;   * pagemap kernel ABI bits</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -#define PM_ENTRY_BYTES      sizeof(uint64_t)</span>
<span class="quote">&gt; -#define PM_STATUS_BITS      3</span>
<span class="quote">&gt; -#define PM_STATUS_OFFSET    (64 - PM_STATUS_BITS)</span>
<span class="quote">&gt; -#define PM_STATUS_MASK      (((1LL &lt;&lt; PM_STATUS_BITS) - 1) &lt;&lt; PM_STATUS_OFFSET)</span>
<span class="quote">&gt; -#define PM_STATUS(nr)       (((nr) &lt;&lt; PM_STATUS_OFFSET) &amp; PM_STATUS_MASK)</span>
<span class="quote">&gt; -#define PM_PSHIFT_BITS      6</span>
<span class="quote">&gt; -#define PM_PSHIFT_OFFSET    (PM_STATUS_OFFSET - PM_PSHIFT_BITS)</span>
<span class="quote">&gt; -#define PM_PSHIFT_MASK      (((1LL &lt;&lt; PM_PSHIFT_BITS) - 1) &lt;&lt; PM_PSHIFT_OFFSET)</span>
<span class="quote">&gt; -#define __PM_PSHIFT(x)      (((uint64_t) (x) &lt;&lt; PM_PSHIFT_OFFSET) &amp; PM_PSHIFT_MASK)</span>
<span class="quote">&gt; -#define PM_PFRAME_MASK      ((1LL &lt;&lt; PM_PSHIFT_OFFSET) - 1)</span>
<span class="quote">&gt; -#define PM_PFRAME(x)        ((x) &amp; PM_PFRAME_MASK)</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#define __PM_SOFT_DIRTY      (1LL)</span>
<span class="quote">&gt; -#define __PM_MMAP_EXCLUSIVE  (2LL)</span>
<span class="quote">&gt; -#define PM_PRESENT          PM_STATUS(4LL)</span>
<span class="quote">&gt; -#define PM_SWAP             PM_STATUS(2LL)</span>
<span class="quote">&gt; -#define PM_FILE             PM_STATUS(1LL)</span>
<span class="quote">&gt; -#define PM_SOFT_DIRTY       __PM_PSHIFT(__PM_SOFT_DIRTY)</span>
<span class="quote">&gt; -#define PM_MMAP_EXCLUSIVE   __PM_PSHIFT(__PM_MMAP_EXCLUSIVE)</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +#define PM_ENTRY_BYTES         8</span>
<span class="quote">&gt; +#define PM_PFEAME_BITS         54</span>
<span class="quote">&gt; +#define PM_PFRAME_MASK         ((1LL &lt;&lt; PM_PFEAME_BITS) - 1)</span>
<span class="quote">&gt; +#define PM_PFRAME(x)           ((x) &amp; PM_PFRAME_MASK)</span>
<span class="quote">&gt; +#define PM_SOFT_DIRTY          (1ULL &lt;&lt; 55)</span>
<span class="quote">&gt; +#define PM_MMAP_EXCLUSIVE      (1ULL &lt;&lt; 56)</span>
<span class="quote">&gt; +#define PM_FILE                        (1ULL &lt;&lt; 61)</span>
<span class="quote">&gt; +#define PM_SWAP                        (1ULL &lt;&lt; 62)</span>
<span class="quote">&gt; +#define PM_PRESENT             (1ULL &lt;&lt; 63)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * kernel page flags</span>
<span class="quote">&gt;</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index f1b9ae8..0e134bf 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -710,23 +710,6 @@</span> <span class="p_context"> const struct file_operations proc_tid_smaps_operations = {</span>
 	.release	= proc_map_release,
 };
 
<span class="p_del">-/*</span>
<span class="p_del">- * We do not want to have constant page-shift bits sitting in</span>
<span class="p_del">- * pagemap entries and are about to reuse them some time soon.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Here&#39;s the &quot;migration strategy&quot;:</span>
<span class="p_del">- * 1. when the system boots these bits remain what they are,</span>
<span class="p_del">- *    but a warning about future change is printed in log;</span>
<span class="p_del">- * 2. once anyone clears soft-dirty bits via clear_refs file,</span>
<span class="p_del">- *    these flag is set to denote, that user is aware of the</span>
<span class="p_del">- *    new API and those page-shift bits change their meaning.</span>
<span class="p_del">- *    The respective warning is printed in dmesg;</span>
<span class="p_del">- * 3. In a couple of releases we will remove all the mentions</span>
<span class="p_del">- *    of page-shift in pagemap entries.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-static bool soft_dirty_cleared __read_mostly;</span>
<span class="p_del">-</span>
 enum clear_refs_types {
 	CLEAR_REFS_ALL = 1,
 	CLEAR_REFS_ANON,
<span class="p_chunk">@@ -887,13 +870,6 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 	if (type &lt; CLEAR_REFS_ALL || type &gt;= CLEAR_REFS_LAST)
 		return -EINVAL;
 
<span class="p_del">-	if (type == CLEAR_REFS_SOFT_DIRTY) {</span>
<span class="p_del">-		soft_dirty_cleared = true;</span>
<span class="p_del">-		pr_warn_once(&quot;The pagemap bits 55-60 has changed their meaning!&quot;</span>
<span class="p_del">-			     &quot; See the linux/Documentation/vm/pagemap.txt for &quot;</span>
<span class="p_del">-			     &quot;details.\n&quot;);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	task = get_proc_task(file_inode(file));
 	if (!task)
 		return -ESRCH;
<span class="p_chunk">@@ -961,38 +937,26 @@</span> <span class="p_context"> typedef struct {</span>
 struct pagemapread {
 	int pos, len;		/* units: PM_ENTRY_BYTES, not bytes */
 	pagemap_entry_t *buffer;
<span class="p_del">-	bool v2;</span>
 	bool show_pfn;
 };
 
 #define PAGEMAP_WALK_SIZE	(PMD_SIZE)
 #define PAGEMAP_WALK_MASK	(PMD_MASK)
 
<span class="p_del">-#define PM_ENTRY_BYTES      sizeof(pagemap_entry_t)</span>
<span class="p_del">-#define PM_STATUS_BITS      3</span>
<span class="p_del">-#define PM_STATUS_OFFSET    (64 - PM_STATUS_BITS)</span>
<span class="p_del">-#define PM_STATUS_MASK      (((1LL &lt;&lt; PM_STATUS_BITS) - 1) &lt;&lt; PM_STATUS_OFFSET)</span>
<span class="p_del">-#define PM_STATUS(nr)       (((nr) &lt;&lt; PM_STATUS_OFFSET) &amp; PM_STATUS_MASK)</span>
<span class="p_del">-#define PM_PSHIFT_BITS      6</span>
<span class="p_del">-#define PM_PSHIFT_OFFSET    (PM_STATUS_OFFSET - PM_PSHIFT_BITS)</span>
<span class="p_del">-#define PM_PSHIFT_MASK      (((1LL &lt;&lt; PM_PSHIFT_BITS) - 1) &lt;&lt; PM_PSHIFT_OFFSET)</span>
<span class="p_del">-#define __PM_PSHIFT(x)      (((u64) (x) &lt;&lt; PM_PSHIFT_OFFSET) &amp; PM_PSHIFT_MASK)</span>
<span class="p_del">-#define PM_PFRAME_MASK      ((1LL &lt;&lt; PM_PSHIFT_OFFSET) - 1)</span>
<span class="p_del">-#define PM_PFRAME(x)        ((x) &amp; PM_PFRAME_MASK)</span>
<span class="p_del">-/* in &quot;new&quot; pagemap pshift bits are occupied with more status bits */</span>
<span class="p_del">-#define PM_STATUS2(v2, x)   (__PM_PSHIFT(v2 ? x : PAGE_SHIFT))</span>
<span class="p_del">-</span>
<span class="p_del">-#define __PM_SOFT_DIRTY      (1LL)</span>
<span class="p_del">-#define __PM_MMAP_EXCLUSIVE  (2LL)</span>
<span class="p_del">-#define PM_PRESENT          PM_STATUS(4LL)</span>
<span class="p_del">-#define PM_SWAP             PM_STATUS(2LL)</span>
<span class="p_del">-#define PM_FILE             PM_STATUS(1LL)</span>
<span class="p_del">-#define PM_NOT_PRESENT(v2)  PM_STATUS2(v2, 0)</span>
<span class="p_add">+#define PM_ENTRY_BYTES		sizeof(pagemap_entry_t)</span>
<span class="p_add">+#define PM_PFEAME_BITS		54</span>
<span class="p_add">+#define PM_PFRAME_MASK		GENMASK_ULL(PM_PFEAME_BITS - 1, 0)</span>
<span class="p_add">+#define PM_SOFT_DIRTY		BIT_ULL(55)</span>
<span class="p_add">+#define PM_MMAP_EXCLUSIVE	BIT_ULL(56)</span>
<span class="p_add">+#define PM_FILE			BIT_ULL(61)</span>
<span class="p_add">+#define PM_SWAP			BIT_ULL(62)</span>
<span class="p_add">+#define PM_PRESENT		BIT_ULL(63)</span>
<span class="p_add">+</span>
 #define PM_END_OF_BUFFER    1
 
<span class="p_del">-static inline pagemap_entry_t make_pme(u64 val)</span>
<span class="p_add">+static inline pagemap_entry_t make_pme(u64 frame, u64 flags)</span>
 {
<span class="p_del">-	return (pagemap_entry_t) { .pme = val };</span>
<span class="p_add">+	return (pagemap_entry_t) { .pme = (frame &amp; PM_PFRAME_MASK) | flags };</span>
 }
 
 static int add_to_pagemap(unsigned long addr, pagemap_entry_t *pme,
<span class="p_chunk">@@ -1013,7 +977,7 @@</span> <span class="p_context"> static int pagemap_pte_hole(unsigned long start, unsigned long end,</span>
 
 	while (addr &lt; end) {
 		struct vm_area_struct *vma = find_vma(walk-&gt;mm, addr);
<span class="p_del">-		pagemap_entry_t pme = make_pme(PM_NOT_PRESENT(pm-&gt;v2));</span>
<span class="p_add">+		pagemap_entry_t pme = make_pme(0, 0);</span>
 		/* End of address space hole, which we mark as non-present. */
 		unsigned long hole_end;
 
<span class="p_chunk">@@ -1033,7 +997,7 @@</span> <span class="p_context"> static int pagemap_pte_hole(unsigned long start, unsigned long end,</span>
 
 		/* Addresses in the VMA. */
 		if (vma-&gt;vm_flags &amp; VM_SOFTDIRTY)
<span class="p_del">-			pme.pme |= PM_STATUS2(pm-&gt;v2, __PM_SOFT_DIRTY);</span>
<span class="p_add">+			pme = make_pme(0, PM_SOFT_DIRTY);</span>
 		for (; addr &lt; min(end, vma-&gt;vm_end); addr += PAGE_SIZE) {
 			err = add_to_pagemap(addr, &amp;pme, pm);
 			if (err)
<span class="p_chunk">@@ -1044,50 +1008,44 @@</span> <span class="p_context"> out:</span>
 	return err;
 }
 
<span class="p_del">-static void pte_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,</span>
<span class="p_add">+static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
 		struct vm_area_struct *vma, unsigned long addr, pte_t pte)
 {
<span class="p_del">-	u64 frame = 0, flags;</span>
<span class="p_add">+	u64 frame = 0, flags = 0;</span>
 	struct page *page = NULL;
<span class="p_del">-	int flags2 = 0;</span>
 
 	if (pte_present(pte)) {
 		if (pm-&gt;show_pfn)
 			frame = pte_pfn(pte);
<span class="p_del">-		flags = PM_PRESENT;</span>
<span class="p_add">+		flags |= PM_PRESENT;</span>
 		page = vm_normal_page(vma, addr, pte);
 		if (pte_soft_dirty(pte))
<span class="p_del">-			flags2 |= __PM_SOFT_DIRTY;</span>
<span class="p_add">+			flags |= PM_SOFT_DIRTY;</span>
 	} else if (is_swap_pte(pte)) {
 		swp_entry_t entry;
 		if (pte_swp_soft_dirty(pte))
<span class="p_del">-			flags2 |= __PM_SOFT_DIRTY;</span>
<span class="p_add">+			flags |= PM_SOFT_DIRTY;</span>
 		entry = pte_to_swp_entry(pte);
 		frame = swp_type(entry) |
 			(swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);
<span class="p_del">-		flags = PM_SWAP;</span>
<span class="p_add">+		flags |= PM_SWAP;</span>
 		if (is_migration_entry(entry))
 			page = migration_entry_to_page(entry);
<span class="p_del">-	} else {</span>
<span class="p_del">-		if (vma-&gt;vm_flags &amp; VM_SOFTDIRTY)</span>
<span class="p_del">-			flags2 |= __PM_SOFT_DIRTY;</span>
<span class="p_del">-		*pme = make_pme(PM_NOT_PRESENT(pm-&gt;v2) | PM_STATUS2(pm-&gt;v2, flags2));</span>
<span class="p_del">-		return;</span>
 	}
 
 	if (page &amp;&amp; !PageAnon(page))
 		flags |= PM_FILE;
 	if (page &amp;&amp; page_mapcount(page) == 1)
<span class="p_del">-		flags2 |= __PM_MMAP_EXCLUSIVE;</span>
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY))</span>
<span class="p_del">-		flags2 |= __PM_SOFT_DIRTY;</span>
<span class="p_add">+		flags |= PM_MMAP_EXCLUSIVE;</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_SOFTDIRTY)</span>
<span class="p_add">+		flags |= PM_SOFT_DIRTY;</span>
 
<span class="p_del">-	*pme = make_pme(PM_PFRAME(frame) | PM_STATUS2(pm-&gt;v2, flags2) | flags);</span>
<span class="p_add">+	return make_pme(frame, flags);</span>
 }
 
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_del">-static void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,</span>
<span class="p_del">-		pmd_t pmd, int offset, int pmd_flags2)</span>
<span class="p_add">+static pagemap_entry_t thp_pmd_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="p_add">+		pmd_t pmd, int offset, u64 flags)</span>
 {
 	u64 frame = 0;
 
<span class="p_chunk">@@ -1099,15 +1057,16 @@</span> <span class="p_context"> static void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *p</span>
 	if (pmd_present(pmd)) {
 		if (pm-&gt;show_pfn)
 			frame = pmd_pfn(pmd) + offset;
<span class="p_del">-		*pme = make_pme(PM_PFRAME(frame) | PM_PRESENT |</span>
<span class="p_del">-				PM_STATUS2(pm-&gt;v2, pmd_flags2));</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		*pme = make_pme(PM_NOT_PRESENT(pm-&gt;v2) | PM_STATUS2(pm-&gt;v2, pmd_flags2));</span>
<span class="p_add">+		flags |= PM_PRESENT;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return make_pme(frame, flags);</span>
 }
 #else
<span class="p_del">-static inline void thp_pmd_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,</span>
<span class="p_del">-		pmd_t pmd, int offset, int pmd_flags2)</span>
<span class="p_add">+static pagemap_entry_t thp_pmd_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="p_add">+		pmd_t pmd, int offset, u64 flags)</span>
 {
<span class="p_add">+	return make_pme(0, 0);</span>
 }
 #endif
 
<span class="p_chunk">@@ -1121,18 +1080,16 @@</span> <span class="p_context"> static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 	int err = 0;
 
 	if (pmd_trans_huge_lock(pmd, vma, &amp;ptl) == 1) {
<span class="p_del">-		int pmd_flags2;</span>
<span class="p_add">+		u64 flags = 0;</span>
 
 		if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(*pmd))
<span class="p_del">-			pmd_flags2 = __PM_SOFT_DIRTY;</span>
<span class="p_del">-		else</span>
<span class="p_del">-			pmd_flags2 = 0;</span>
<span class="p_add">+			flags |= PM_SOFT_DIRTY;</span>
 
 		if (pmd_present(*pmd)) {
 			struct page *page = pmd_page(*pmd);
 
 			if (page_mapcount(page) == 1)
<span class="p_del">-				pmd_flags2 |= __PM_MMAP_EXCLUSIVE;</span>
<span class="p_add">+				flags |= PM_MMAP_EXCLUSIVE;</span>
 		}
 
 		for (; addr != end; addr += PAGE_SIZE) {
<span class="p_chunk">@@ -1141,7 +1098,7 @@</span> <span class="p_context"> static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 
 			offset = (addr &amp; ~PAGEMAP_WALK_MASK) &gt;&gt;
 					PAGE_SHIFT;
<span class="p_del">-			thp_pmd_to_pagemap_entry(&amp;pme, pm, *pmd, offset, pmd_flags2);</span>
<span class="p_add">+			pme = thp_pmd_to_pagemap_entry(pm, *pmd, offset, flags);</span>
 			err = add_to_pagemap(addr, &amp;pme, pm);
 			if (err)
 				break;
<span class="p_chunk">@@ -1161,7 +1118,7 @@</span> <span class="p_context"> static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 	for (; addr &lt; end; pte++, addr += PAGE_SIZE) {
 		pagemap_entry_t pme;
 
<span class="p_del">-		pte_to_pagemap_entry(&amp;pme, pm, vma, addr, *pte);</span>
<span class="p_add">+		pme = pte_to_pagemap_entry(pm, vma, addr, *pte);</span>
 		err = add_to_pagemap(addr, &amp;pme, pm);
 		if (err)
 			break;
<span class="p_chunk">@@ -1174,19 +1131,18 @@</span> <span class="p_context"> static int pagemap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 }
 
 #ifdef CONFIG_HUGETLB_PAGE
<span class="p_del">-static void huge_pte_to_pagemap_entry(pagemap_entry_t *pme, struct pagemapread *pm,</span>
<span class="p_del">-					pte_t pte, int offset, int flags2)</span>
<span class="p_add">+static pagemap_entry_t huge_pte_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="p_add">+					pte_t pte, int offset, u64 flags)</span>
 {
 	u64 frame = 0;
 
 	if (pte_present(pte)) {
 		if (pm-&gt;show_pfn)
 			frame = pte_pfn(pte) + offset;
<span class="p_del">-		*pme = make_pme(PM_PFRAME(frame) | PM_PRESENT |</span>
<span class="p_del">-				PM_STATUS2(pm-&gt;v2, flags2));</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		*pme = make_pme(PM_NOT_PRESENT(pm-&gt;v2)			|</span>
<span class="p_del">-				PM_STATUS2(pm-&gt;v2, flags2));</span>
<span class="p_add">+		flags |= PM_PRESENT;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return make_pme(frame, flags);</span>
 }
 
 /* This function walks within one hugetlb entry in the single call */
<span class="p_chunk">@@ -1197,17 +1153,15 @@</span> <span class="p_context"> static int pagemap_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
 	struct pagemapread *pm = walk-&gt;private;
 	struct vm_area_struct *vma = walk-&gt;vma;
 	int err = 0;
<span class="p_del">-	int flags2;</span>
<span class="p_add">+	u64 flags = 0;</span>
 	pagemap_entry_t pme;
 
 	if (vma-&gt;vm_flags &amp; VM_SOFTDIRTY)
<span class="p_del">-		flags2 = __PM_SOFT_DIRTY;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		flags2 = 0;</span>
<span class="p_add">+		flags |= PM_SOFT_DIRTY;</span>
 
 	for (; addr != end; addr += PAGE_SIZE) {
 		int offset = (addr &amp; ~hmask) &gt;&gt; PAGE_SHIFT;
<span class="p_del">-		huge_pte_to_pagemap_entry(&amp;pme, pm, *pte, offset, flags2);</span>
<span class="p_add">+		pme = huge_pte_to_pagemap_entry(pm, *pte, offset, flags);</span>
 		err = add_to_pagemap(addr, &amp;pme, pm);
 		if (err)
 			return err;
<span class="p_chunk">@@ -1228,7 +1182,9 @@</span> <span class="p_context"> static int pagemap_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
  * Bits 0-54  page frame number (PFN) if present
  * Bits 0-4   swap type if swapped
  * Bits 5-54  swap offset if swapped
<span class="p_del">- * Bits 55-60 page shift (page size = 1&lt;&lt;page shift)</span>
<span class="p_add">+ * Bit  55    pte is soft-dirty (see Documentation/vm/soft-dirty.txt)</span>
<span class="p_add">+ * Bit  56    page exclusively mapped</span>
<span class="p_add">+ * Bits 57-60 zero</span>
  * Bit  61    page is file-page or shared-anon
  * Bit  62    page swapped
  * Bit  63    page present
<span class="p_chunk">@@ -1269,7 +1225,6 @@</span> <span class="p_context"> static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
 
 	/* do not disclose physical addresses: attack vector */
 	pm.show_pfn = file_ns_capable(file, &amp;init_user_ns, CAP_SYS_ADMIN);
<span class="p_del">-	pm.v2 = soft_dirty_cleared;</span>
 	pm.len = (PAGEMAP_WALK_SIZE &gt;&gt; PAGE_SHIFT);
 	pm.buffer = kmalloc(pm.len * PM_ENTRY_BYTES, GFP_TEMPORARY);
 	ret = -ENOMEM;
<span class="p_chunk">@@ -1339,10 +1294,6 @@</span> <span class="p_context"> static int pagemap_open(struct inode *inode, struct file *file)</span>
 {
 	struct mm_struct *mm;
 
<span class="p_del">-	pr_warn_once(&quot;Bits 55-60 of /proc/PID/pagemap entries are about &quot;</span>
<span class="p_del">-			&quot;to stop being page-shift some time soon. See the &quot;</span>
<span class="p_del">-			&quot;linux/Documentation/vm/pagemap.txt for details.\n&quot;);</span>
<span class="p_del">-</span>
 	mm = proc_mem_open(inode, PTRACE_MODE_READ);
 	if (IS_ERR(mm))
 		return PTR_ERR(mm);
<span class="p_header">diff --git a/tools/vm/page-types.c b/tools/vm/page-types.c</span>
<span class="p_header">index 3a9f193..1fa872e 100644</span>
<span class="p_header">--- a/tools/vm/page-types.c</span>
<span class="p_header">+++ b/tools/vm/page-types.c</span>
<span class="p_chunk">@@ -57,26 +57,15 @@</span> <span class="p_context"></span>
  * pagemap kernel ABI bits
  */
 
<span class="p_del">-#define PM_ENTRY_BYTES      sizeof(uint64_t)</span>
<span class="p_del">-#define PM_STATUS_BITS      3</span>
<span class="p_del">-#define PM_STATUS_OFFSET    (64 - PM_STATUS_BITS)</span>
<span class="p_del">-#define PM_STATUS_MASK      (((1LL &lt;&lt; PM_STATUS_BITS) - 1) &lt;&lt; PM_STATUS_OFFSET)</span>
<span class="p_del">-#define PM_STATUS(nr)       (((nr) &lt;&lt; PM_STATUS_OFFSET) &amp; PM_STATUS_MASK)</span>
<span class="p_del">-#define PM_PSHIFT_BITS      6</span>
<span class="p_del">-#define PM_PSHIFT_OFFSET    (PM_STATUS_OFFSET - PM_PSHIFT_BITS)</span>
<span class="p_del">-#define PM_PSHIFT_MASK      (((1LL &lt;&lt; PM_PSHIFT_BITS) - 1) &lt;&lt; PM_PSHIFT_OFFSET)</span>
<span class="p_del">-#define __PM_PSHIFT(x)      (((uint64_t) (x) &lt;&lt; PM_PSHIFT_OFFSET) &amp; PM_PSHIFT_MASK)</span>
<span class="p_del">-#define PM_PFRAME_MASK      ((1LL &lt;&lt; PM_PSHIFT_OFFSET) - 1)</span>
<span class="p_del">-#define PM_PFRAME(x)        ((x) &amp; PM_PFRAME_MASK)</span>
<span class="p_del">-</span>
<span class="p_del">-#define __PM_SOFT_DIRTY      (1LL)</span>
<span class="p_del">-#define __PM_MMAP_EXCLUSIVE  (2LL)</span>
<span class="p_del">-#define PM_PRESENT          PM_STATUS(4LL)</span>
<span class="p_del">-#define PM_SWAP             PM_STATUS(2LL)</span>
<span class="p_del">-#define PM_FILE             PM_STATUS(1LL)</span>
<span class="p_del">-#define PM_SOFT_DIRTY       __PM_PSHIFT(__PM_SOFT_DIRTY)</span>
<span class="p_del">-#define PM_MMAP_EXCLUSIVE   __PM_PSHIFT(__PM_MMAP_EXCLUSIVE)</span>
<span class="p_del">-</span>
<span class="p_add">+#define PM_ENTRY_BYTES		8</span>
<span class="p_add">+#define PM_PFEAME_BITS		54</span>
<span class="p_add">+#define PM_PFRAME_MASK		((1LL &lt;&lt; PM_PFEAME_BITS) - 1)</span>
<span class="p_add">+#define PM_PFRAME(x)		((x) &amp; PM_PFRAME_MASK)</span>
<span class="p_add">+#define PM_SOFT_DIRTY		(1ULL &lt;&lt; 55)</span>
<span class="p_add">+#define PM_MMAP_EXCLUSIVE	(1ULL &lt;&lt; 56)</span>
<span class="p_add">+#define PM_FILE			(1ULL &lt;&lt; 61)</span>
<span class="p_add">+#define PM_SWAP			(1ULL &lt;&lt; 62)</span>
<span class="p_add">+#define PM_PRESENT		(1ULL &lt;&lt; 63)</span>
 
 /*
  * kernel page flags

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



