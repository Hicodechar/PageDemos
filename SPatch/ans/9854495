
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/2] drivers: dma-coherent: Introduce interface for default DMA pool - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/2] drivers: dma-coherent: Introduce interface for default DMA pool</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=109631">Vladimir Murzin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 20, 2017, 10:19 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1500545999-17177-2-git-send-email-vladimir.murzin@arm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9854495/mbox/"
   >mbox</a>
|
   <a href="/patch/9854495/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9854495/">/patch/9854495/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1493C602BA for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Jul 2017 10:21:19 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F2FF8286FC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Jul 2017 10:21:18 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E7D0728784; Thu, 20 Jul 2017 10:21:18 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B9674286FC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Jul 2017 10:21:17 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S936168AbdGTKVO (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 20 Jul 2017 06:21:14 -0400
Received: from usa-sjc-mx-foss1.foss.arm.com ([217.140.101.70]:51114 &quot;EHLO
	foss.arm.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S936124AbdGTKUa (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 20 Jul 2017 06:20:30 -0400
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
	by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id 8B24B13D5;
	Thu, 20 Jul 2017 03:20:30 -0700 (PDT)
Received: from bc-e11-3-13.euhpc.arm.com. (bc-e11-3-13.euhpc.arm.com
	[10.6.16.164])
	by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPA id
	93C1B3F578; Thu, 20 Jul 2017 03:20:27 -0700 (PDT)
From: Vladimir Murzin &lt;vladimir.murzin@arm.com&gt;
To: linux-kernel@vger.kernel.org
Cc: linux-arm-kernel@lists.infradead.org, linux@armlinux.org.uk,
	sza@esh.hu, arnd@arndb.de, gregkh@linuxfoundation.org,
	akpm@linux-foundation.org, alexandre.torgue@st.com,
	robin.murphy@arm.com, kbuild-all@01.org,
	benjamin.gaignard@linaro.org, hch@lst.de, m.szyprowski@samsung.com,
	vitaly_kuzmichev@mentor.com, george_davis@mentor.com,
	Vineet Gupta &lt;vgupta@synopsys.com&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;, Ralf Baechle &lt;ralf@linux-mips.org&gt;
Subject: [PATCH 1/2] drivers: dma-coherent: Introduce interface for default
	DMA pool
Date: Thu, 20 Jul 2017 11:19:58 +0100
Message-Id: &lt;1500545999-17177-2-git-send-email-vladimir.murzin@arm.com&gt;
X-Mailer: git-send-email 2.0.0
In-Reply-To: &lt;1500545999-17177-1-git-send-email-vladimir.murzin@arm.com&gt;
References: &lt;1500545999-17177-1-git-send-email-vladimir.murzin@arm.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=109631">Vladimir Murzin</a> - July 20, 2017, 10:19 a.m.</div>
<pre class="content">
Christoph noticed [1] that default DMA pool in current form overload
the DMA coherent infrastructure. In reply, Robin suggested [2] to
split the per-device vs. global pool interfaces, so allocation/release
from default DMA pool is driven by dma ops implementation.

This patch implements Robin&#39;s idea and provide interface to
allocate/release/mmap the default (aka global) DMA pool.

To make it clear that existing *_from_coherent routines work on
per-device pool rename them to *_from_dev_coherent.

[1] https://lkml.org/lkml/2017/7/7/370
[2] https://lkml.org/lkml/2017/7/7/431

Cc: Vineet Gupta &lt;vgupta@synopsys.com&gt;
Cc: Russell King &lt;linux@armlinux.org.uk&gt;
Cc: Catalin Marinas &lt;catalin.marinas@arm.com&gt;
Cc: Will Deacon &lt;will.deacon@arm.com&gt;
Cc: Ralf Baechle &lt;ralf@linux-mips.org&gt;
Suggested-by: Robin Murphy &lt;robin.murphy@arm.com&gt;
<span class="tested-by">Tested-by: Andras Szemzo &lt;sza@esh.hu&gt;</span>
<span class="signed-off-by">Signed-off-by: Vladimir Murzin &lt;vladimir.murzin@arm.com&gt;</span>
---
 arch/arc/mm/dma.c           |   2 +-
 arch/arm/mm/dma-mapping.c   |   2 +-
 arch/arm64/mm/dma-mapping.c |   4 +-
 arch/mips/mm/dma-default.c  |   2 +-
 drivers/base/dma-coherent.c | 169 +++++++++++++++++++++++++++++---------------
 drivers/base/dma-mapping.c  |   2 +-
 include/linux/dma-mapping.h |  40 ++++++++---
 7 files changed, 149 insertions(+), 72 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77581">Robin Murphy</a> - July 20, 2017, 1:45 p.m.</div>
<pre class="content">
On 20/07/17 11:19, Vladimir Murzin wrote:
<span class="quote">&gt; Christoph noticed [1] that default DMA pool in current form overload</span>
<span class="quote">&gt; the DMA coherent infrastructure. In reply, Robin suggested [2] to</span>
<span class="quote">&gt; split the per-device vs. global pool interfaces, so allocation/release</span>
<span class="quote">&gt; from default DMA pool is driven by dma ops implementation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch implements Robin&#39;s idea and provide interface to</span>
<span class="quote">&gt; allocate/release/mmap the default (aka global) DMA pool.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To make it clear that existing *_from_coherent routines work on</span>
<span class="quote">&gt; per-device pool rename them to *_from_dev_coherent.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [1] https://lkml.org/lkml/2017/7/7/370</span>
<span class="quote">&gt; [2] https://lkml.org/lkml/2017/7/7/431</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Vineet Gupta &lt;vgupta@synopsys.com&gt;</span>
<span class="quote">&gt; Cc: Russell King &lt;linux@armlinux.org.uk&gt;</span>
<span class="quote">&gt; Cc: Catalin Marinas &lt;catalin.marinas@arm.com&gt;</span>
<span class="quote">&gt; Cc: Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="quote">&gt; Cc: Ralf Baechle &lt;ralf@linux-mips.org&gt;</span>
<span class="quote">&gt; Suggested-by: Robin Murphy &lt;robin.murphy@arm.com&gt;</span>
<span class="quote">&gt; Tested-by: Andras Szemzo &lt;sza@esh.hu&gt;</span>
<span class="quote">&gt; Signed-off-by: Vladimir Murzin &lt;vladimir.murzin@arm.com&gt;</span>

I&#39;d have left the factored-out static helpers named __*_from_coherent(),
as the _dev is not strictly true there, but it really doesn&#39;t matter as
far as I&#39;m concerned:
<span class="reviewed-by">
Reviewed-by: Robin Murphy &lt;robin.murphy@arm.com&gt;</span>

Thanks for sorting it out.

Robin.
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  arch/arc/mm/dma.c           |   2 +-</span>
<span class="quote">&gt;  arch/arm/mm/dma-mapping.c   |   2 +-</span>
<span class="quote">&gt;  arch/arm64/mm/dma-mapping.c |   4 +-</span>
<span class="quote">&gt;  arch/mips/mm/dma-default.c  |   2 +-</span>
<span class="quote">&gt;  drivers/base/dma-coherent.c | 169 +++++++++++++++++++++++++++++---------------</span>
<span class="quote">&gt;  drivers/base/dma-mapping.c  |   2 +-</span>
<span class="quote">&gt;  include/linux/dma-mapping.h |  40 ++++++++---</span>
<span class="quote">&gt;  7 files changed, 149 insertions(+), 72 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arc/mm/dma.c b/arch/arc/mm/dma.c</span>
<span class="quote">&gt; index 2a07e6e..71d3eff 100644</span>
<span class="quote">&gt; --- a/arch/arc/mm/dma.c</span>
<span class="quote">&gt; +++ b/arch/arc/mm/dma.c</span>
<span class="quote">&gt; @@ -117,7 +117,7 @@ static int arc_dma_mmap(struct device *dev, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	vma-&gt;vm_page_prot = pgprot_noncached(vma-&gt;vm_page_prot);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt; +	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (off &lt; count &amp;&amp; user_count &lt;= (count - off)) {</span>
<span class="quote">&gt; diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; index e7380ba..fcf1473 100644</span>
<span class="quote">&gt; --- a/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -851,7 +851,7 @@ static int __arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	unsigned long pfn = dma_to_pfn(dev, dma_addr);</span>
<span class="quote">&gt;  	unsigned long off = vma-&gt;vm_pgoff;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt; +	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (off &lt; nr_pages &amp;&amp; nr_vma_pages &lt;= (nr_pages - off)) {</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; index e90cd1d..f27d4dd 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -329,7 +329,7 @@ static int __swiotlb_mmap(struct device *dev,</span>
<span class="quote">&gt;  	vma-&gt;vm_page_prot = __get_dma_pgprot(attrs, vma-&gt;vm_page_prot,</span>
<span class="quote">&gt;  					     is_device_dma_coherent(dev));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt; +	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return __swiotlb_mmap_pfn(vma, pfn, size);</span>
<span class="quote">&gt; @@ -706,7 +706,7 @@ static int __iommu_mmap_attrs(struct device *dev, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	vma-&gt;vm_page_prot = __get_dma_pgprot(attrs, vma-&gt;vm_page_prot,</span>
<span class="quote">&gt;  					     is_device_dma_coherent(dev));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt; +	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (attrs &amp; DMA_ATTR_FORCE_CONTIGUOUS) {</span>
<span class="quote">&gt; diff --git a/arch/mips/mm/dma-default.c b/arch/mips/mm/dma-default.c</span>
<span class="quote">&gt; index e08598c..8e78251 100644</span>
<span class="quote">&gt; --- a/arch/mips/mm/dma-default.c</span>
<span class="quote">&gt; +++ b/arch/mips/mm/dma-default.c</span>
<span class="quote">&gt; @@ -232,7 +232,7 @@ static int mips_dma_mmap(struct device *dev, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  		vma-&gt;vm_page_prot = pgprot_noncached(vma-&gt;vm_page_prot);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt; +	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (off &lt; count &amp;&amp; user_count &lt;= (count - off)) {</span>
<span class="quote">&gt; diff --git a/drivers/base/dma-coherent.c b/drivers/base/dma-coherent.c</span>
<span class="quote">&gt; index 2ae24c2..d6f231c 100644</span>
<span class="quote">&gt; --- a/drivers/base/dma-coherent.c</span>
<span class="quote">&gt; +++ b/drivers/base/dma-coherent.c</span>
<span class="quote">&gt; @@ -25,7 +25,7 @@ static inline struct dma_coherent_mem *dev_get_coherent_memory(struct device *de</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (dev &amp;&amp; dev-&gt;dma_mem)</span>
<span class="quote">&gt;  		return dev-&gt;dma_mem;</span>
<span class="quote">&gt; -	return dma_coherent_default_memory;</span>
<span class="quote">&gt; +	return NULL;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline dma_addr_t dma_get_device_base(struct device *dev,</span>
<span class="quote">&gt; @@ -165,34 +165,15 @@ void *dma_mark_declared_memory_occupied(struct device *dev,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(dma_mark_declared_memory_occupied);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/**</span>
<span class="quote">&gt; - * dma_alloc_from_coherent() - try to allocate memory from the per-device coherent area</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * @dev:	device from which we allocate memory</span>
<span class="quote">&gt; - * @size:	size of requested memory area</span>
<span class="quote">&gt; - * @dma_handle:	This will be filled with the correct dma handle</span>
<span class="quote">&gt; - * @ret:	This pointer will be filled with the virtual address</span>
<span class="quote">&gt; - *		to allocated area.</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * This function should be only called from per-arch dma_alloc_coherent()</span>
<span class="quote">&gt; - * to support allocation from per-device coherent memory pools.</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * Returns 0 if dma_alloc_coherent should continue with allocating from</span>
<span class="quote">&gt; - * generic memory areas, or !0 if dma_alloc_coherent should return @ret.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -int dma_alloc_from_coherent(struct device *dev, ssize_t size,</span>
<span class="quote">&gt; -				       dma_addr_t *dma_handle, void **ret)</span>
<span class="quote">&gt; +static void *__dma_alloc_from_dev_coherent(struct dma_coherent_mem *mem,</span>
<span class="quote">&gt; +				       ssize_t size, dma_addr_t *dma_handle)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);</span>
<span class="quote">&gt;  	int order = get_order(size);</span>
<span class="quote">&gt;  	unsigned long flags;</span>
<span class="quote">&gt;  	int pageno;</span>
<span class="quote">&gt;  	int dma_memory_map;</span>
<span class="quote">&gt; +	void *ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!mem)</span>
<span class="quote">&gt; -		return 0;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	*ret = NULL;</span>
<span class="quote">&gt;  	spin_lock_irqsave(&amp;mem-&gt;spinlock, flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (unlikely(size &gt; (mem-&gt;size &lt;&lt; PAGE_SHIFT)))</span>
<span class="quote">&gt; @@ -203,21 +184,51 @@ int dma_alloc_from_coherent(struct device *dev, ssize_t size,</span>
<span class="quote">&gt;  		goto err;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; -	 * Memory was found in the per-device area.</span>
<span class="quote">&gt; +	 * Memory was found in the coherent area.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	*dma_handle = dma_get_device_base(dev, mem) + (pageno &lt;&lt; PAGE_SHIFT);</span>
<span class="quote">&gt; -	*ret = mem-&gt;virt_base + (pageno &lt;&lt; PAGE_SHIFT);</span>
<span class="quote">&gt; +	*dma_handle = mem-&gt;device_base + (pageno &lt;&lt; PAGE_SHIFT);</span>
<span class="quote">&gt; +	ret = mem-&gt;virt_base + (pageno &lt;&lt; PAGE_SHIFT);</span>
<span class="quote">&gt;  	dma_memory_map = (mem-&gt;flags &amp; DMA_MEMORY_MAP);</span>
<span class="quote">&gt;  	spin_unlock_irqrestore(&amp;mem-&gt;spinlock, flags);</span>
<span class="quote">&gt;  	if (dma_memory_map)</span>
<span class="quote">&gt; -		memset(*ret, 0, size);</span>
<span class="quote">&gt; +		memset(ret, 0, size);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		memset_io(*ret, 0, size);</span>
<span class="quote">&gt; +		memset_io(ret, 0, size);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	return 1;</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  err:</span>
<span class="quote">&gt;  	spin_unlock_irqrestore(&amp;mem-&gt;spinlock, flags);</span>
<span class="quote">&gt; +	return NULL;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * dma_alloc_from_dev_coherent() - try to allocate memory from the per-device coherent area</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @dev:	device from which we allocate memory</span>
<span class="quote">&gt; + * @size:	size of requested memory area</span>
<span class="quote">&gt; + * @dma_handle:	This will be filled with the correct dma handle</span>
<span class="quote">&gt; + * @ret:	This pointer will be filled with the virtual address</span>
<span class="quote">&gt; + *		to allocated area.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This function should be only called from per-arch dma_alloc_coherent()</span>
<span class="quote">&gt; + * to support allocation from per-device coherent memory pools.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Returns 0 if dma_alloc_coherent should continue with allocating from</span>
<span class="quote">&gt; + * generic memory areas, or !0 if dma_alloc_coherent should return @ret.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +int dma_alloc_from_dev_coherent(struct device *dev, ssize_t size,</span>
<span class="quote">&gt; +			    dma_addr_t *dma_handle, void **ret)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!mem)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	*ret = __dma_alloc_from_dev_coherent(mem, size, dma_handle);</span>
<span class="quote">&gt; +	if (*ret)</span>
<span class="quote">&gt; +		return 1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * In the case where the allocation can not be satisfied from the</span>
<span class="quote">&gt;  	 * per-device area, try to fall back to generic memory if the</span>
<span class="quote">&gt; @@ -225,25 +236,20 @@ int dma_alloc_from_coherent(struct device *dev, ssize_t size,</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	return mem-&gt;flags &amp; DMA_MEMORY_EXCLUSIVE;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; -EXPORT_SYMBOL(dma_alloc_from_coherent);</span>
<span class="quote">&gt; +EXPORT_SYMBOL(dma_alloc_from_dev_coherent);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/**</span>
<span class="quote">&gt; - * dma_release_from_coherent() - try to free the memory allocated from per-device coherent memory pool</span>
<span class="quote">&gt; - * @dev:	device from which the memory was allocated</span>
<span class="quote">&gt; - * @order:	the order of pages allocated</span>
<span class="quote">&gt; - * @vaddr:	virtual address of allocated pages</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * This checks whether the memory was allocated from the per-device</span>
<span class="quote">&gt; - * coherent memory pool and if so, releases that memory.</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * Returns 1 if we correctly released the memory, or 0 if</span>
<span class="quote">&gt; - * dma_release_coherent() should proceed with releasing memory from</span>
<span class="quote">&gt; - * generic pools.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -int dma_release_from_coherent(struct device *dev, int order, void *vaddr)</span>
<span class="quote">&gt; +void *dma_alloc_from_global_coherent(ssize_t size, dma_addr_t *dma_handle)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);</span>
<span class="quote">&gt; +	if (!dma_coherent_default_memory)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	return __dma_alloc_from_dev_coherent(dma_coherent_default_memory, size, dma_handle);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int __dma_release_from_dev_coherent(struct dma_coherent_mem *mem,</span>
<span class="quote">&gt; +				       int order, void *vaddr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt;  	if (mem &amp;&amp; vaddr &gt;= mem-&gt;virt_base &amp;&amp; vaddr &lt;</span>
<span class="quote">&gt;  		   (mem-&gt;virt_base + (mem-&gt;size &lt;&lt; PAGE_SHIFT))) {</span>
<span class="quote">&gt;  		int page = (vaddr - mem-&gt;virt_base) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; @@ -256,28 +262,42 @@ int dma_release_from_coherent(struct device *dev, int order, void *vaddr)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; -EXPORT_SYMBOL(dma_release_from_coherent);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt; - * dma_mmap_from_coherent() - try to mmap the memory allocated from</span>
<span class="quote">&gt; - * per-device coherent memory pool to userspace</span>
<span class="quote">&gt; + * dma_release_from_dev_coherent() - try to free the memory allocated from per-device coherent memory pool</span>
<span class="quote">&gt;   * @dev:	device from which the memory was allocated</span>
<span class="quote">&gt; - * @vma:	vm_area for the userspace memory</span>
<span class="quote">&gt; - * @vaddr:	cpu address returned by dma_alloc_from_coherent</span>
<span class="quote">&gt; - * @size:	size of the memory buffer allocated by dma_alloc_from_coherent</span>
<span class="quote">&gt; - * @ret:	result from remap_pfn_range()</span>
<span class="quote">&gt; + * @order:	the order of pages allocated</span>
<span class="quote">&gt; + * @vaddr:	virtual address of allocated pages</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * This checks whether the memory was allocated from the per-device</span>
<span class="quote">&gt; - * coherent memory pool and if so, maps that memory to the provided vma.</span>
<span class="quote">&gt; + * coherent memory pool and if so, releases that memory.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * Returns 1 if we correctly mapped the memory, or 0 if the caller should</span>
<span class="quote">&gt; - * proceed with mapping memory from generic pools.</span>
<span class="quote">&gt; + * Returns 1 if we correctly released the memory, or 0 if</span>
<span class="quote">&gt; + * dma_release_coherent() should proceed with releasing memory from</span>
<span class="quote">&gt; + * generic pools.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,</span>
<span class="quote">&gt; -			   void *vaddr, size_t size, int *ret)</span>
<span class="quote">&gt; +int dma_release_from_dev_coherent(struct device *dev, int order, void *vaddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	return __dma_release_from_dev_coherent(mem, order, vaddr);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(dma_release_from_dev_coherent);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +int dma_release_from_global_coherent(int order, void *vaddr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!dma_coherent_default_memory)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return __dma_release_from_dev_coherent(dma_coherent_default_memory,</span>
<span class="quote">&gt; +					   order, vaddr);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int __dma_mmap_from_dev_coherent(struct dma_coherent_mem *mem,</span>
<span class="quote">&gt; +				    struct vm_area_struct *vma, void *vaddr,</span>
<span class="quote">&gt; +				    size_t size, int *ret)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt;  	if (mem &amp;&amp; vaddr &gt;= mem-&gt;virt_base &amp;&amp; vaddr + size &lt;=</span>
<span class="quote">&gt;  		   (mem-&gt;virt_base + (mem-&gt;size &lt;&lt; PAGE_SHIFT))) {</span>
<span class="quote">&gt;  		unsigned long off = vma-&gt;vm_pgoff;</span>
<span class="quote">&gt; @@ -296,7 +316,40 @@ int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; -EXPORT_SYMBOL(dma_mmap_from_coherent);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * dma_mmap_from_dev_coherent() - try to mmap the memory allocated from</span>
<span class="quote">&gt; + * per-device coherent memory pool to userspace</span>
<span class="quote">&gt; + * @dev:	device from which the memory was allocated</span>
<span class="quote">&gt; + * @vma:	vm_area for the userspace memory</span>
<span class="quote">&gt; + * @vaddr:	cpu address returned by dma_alloc_from_dev_coherent</span>
<span class="quote">&gt; + * @size:	size of the memory buffer allocated by dma_alloc_from_dev_coherent</span>
<span class="quote">&gt; + * @ret:	result from remap_pfn_range()</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This checks whether the memory was allocated from the per-device</span>
<span class="quote">&gt; + * coherent memory pool and if so, maps that memory to the provided vma.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Returns 1 if we correctly mapped the memory, or 0 if the caller should</span>
<span class="quote">&gt; + * proceed with mapping memory from generic pools.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +int dma_mmap_from_dev_coherent(struct device *dev, struct vm_area_struct *vma,</span>
<span class="quote">&gt; +			   void *vaddr, size_t size, int *ret)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return __dma_mmap_from_dev_coherent(mem, vma, vaddr, size, ret);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(dma_mmap_from_dev_coherent);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +int dma_mmap_from_global_coherent(struct vm_area_struct *vma, void *vaddr,</span>
<span class="quote">&gt; +				   size_t size, int *ret)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!dma_coherent_default_memory)</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return __dma_mmap_from_dev_coherent(dma_coherent_default_memory, vma,</span>
<span class="quote">&gt; +					vaddr, size, ret);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Support for reserved memory regions defined in device tree</span>
<span class="quote">&gt; diff --git a/drivers/base/dma-mapping.c b/drivers/base/dma-mapping.c</span>
<span class="quote">&gt; index 5096755..b555ff9 100644</span>
<span class="quote">&gt; --- a/drivers/base/dma-mapping.c</span>
<span class="quote">&gt; +++ b/drivers/base/dma-mapping.c</span>
<span class="quote">&gt; @@ -235,7 +235,7 @@ int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	vma-&gt;vm_page_prot = pgprot_noncached(vma-&gt;vm_page_prot);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt; +	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (off &lt; count &amp;&amp; user_count &lt;= (count - off)) {</span>
<span class="quote">&gt; diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h</span>
<span class="quote">&gt; index 843ab86..03c0196 100644</span>
<span class="quote">&gt; --- a/include/linux/dma-mapping.h</span>
<span class="quote">&gt; +++ b/include/linux/dma-mapping.h</span>
<span class="quote">&gt; @@ -157,16 +157,40 @@ static inline int is_device_dma_capable(struct device *dev)</span>
<span class="quote">&gt;   * These three functions are only for dma allocator.</span>
<span class="quote">&gt;   * Don&#39;t use them in device drivers.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -int dma_alloc_from_coherent(struct device *dev, ssize_t size,</span>
<span class="quote">&gt; +int dma_alloc_from_dev_coherent(struct device *dev, ssize_t size,</span>
<span class="quote">&gt;  				       dma_addr_t *dma_handle, void **ret);</span>
<span class="quote">&gt; -int dma_release_from_coherent(struct device *dev, int order, void *vaddr);</span>
<span class="quote">&gt; +int dma_release_from_dev_coherent(struct device *dev, int order, void *vaddr);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,</span>
<span class="quote">&gt; +int dma_mmap_from_dev_coherent(struct device *dev, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			    void *cpu_addr, size_t size, int *ret);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void *dma_alloc_from_global_coherent(ssize_t size, dma_addr_t *dma_handle);</span>
<span class="quote">&gt; +int dma_release_from_global_coherent(int order, void *vaddr);</span>
<span class="quote">&gt; +int dma_mmap_from_global_coherent(struct vm_area_struct *vma, void *cpu_addr,</span>
<span class="quote">&gt; +				  size_t size, int *ret);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; -#define dma_alloc_from_coherent(dev, size, handle, ret) (0)</span>
<span class="quote">&gt; -#define dma_release_from_coherent(dev, order, vaddr) (0)</span>
<span class="quote">&gt; -#define dma_mmap_from_coherent(dev, vma, vaddr, order, ret) (0)</span>
<span class="quote">&gt; +#define dma_alloc_from_dev_coherent(dev, size, handle, ret) (0)</span>
<span class="quote">&gt; +#define dma_release_from_dev_coherent(dev, order, vaddr) (0)</span>
<span class="quote">&gt; +#define dma_mmap_from_dev_coherent(dev, vma, vaddr, order, ret) (0)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void *dma_alloc_from_global_coherent(ssize_t size,</span>
<span class="quote">&gt; +						   dma_addr_t *dma_handle)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return NULL;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int dma_release_from_global_coherent(int order, void *vaddr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int dma_mmap_from_global_coherent(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +						void *cpu_addr, size_t size,</span>
<span class="quote">&gt; +						int *ret)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_HAS_DMA</span>
<span class="quote">&gt; @@ -481,7 +505,7 @@ static inline void *dma_alloc_attrs(struct device *dev, size_t size,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	BUG_ON(!ops);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (dma_alloc_from_coherent(dev, size, dma_handle, &amp;cpu_addr))</span>
<span class="quote">&gt; +	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &amp;cpu_addr))</span>
<span class="quote">&gt;  		return cpu_addr;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!arch_dma_alloc_attrs(&amp;dev, &amp;flag))</span>
<span class="quote">&gt; @@ -503,7 +527,7 @@ static inline void dma_free_attrs(struct device *dev, size_t size,</span>
<span class="quote">&gt;  	BUG_ON(!ops);</span>
<span class="quote">&gt;  	WARN_ON(irqs_disabled());</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (dma_release_from_coherent(dev, get_order(size), cpu_addr))</span>
<span class="quote">&gt; +	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!ops-&gt;free || !cpu_addr)</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - July 20, 2017, 2:11 p.m.</div>
<pre class="content">
On Thu, Jul 20, 2017 at 02:45:26PM +0100, Robin Murphy wrote:
<span class="quote">&gt; I&#39;d have left the factored-out static helpers named __*_from_coherent(),</span>
<span class="quote">&gt; as the _dev is not strictly true there, but it really doesn&#39;t matter as</span>
<span class="quote">&gt; far as I&#39;m concerned:</span>

I&#39;ve fixed that and a few other minor style bits up, and applied it
to the dma-mapping tree.  If all goes right I&#39;ll send it to Linus
on Saturday.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arc/mm/dma.c b/arch/arc/mm/dma.c</span>
<span class="p_header">index 2a07e6e..71d3eff 100644</span>
<span class="p_header">--- a/arch/arc/mm/dma.c</span>
<span class="p_header">+++ b/arch/arc/mm/dma.c</span>
<span class="p_chunk">@@ -117,7 +117,7 @@</span> <span class="p_context"> static int arc_dma_mmap(struct device *dev, struct vm_area_struct *vma,</span>
 
 	vma-&gt;vm_page_prot = pgprot_noncached(vma-&gt;vm_page_prot);
 
<span class="p_del">-	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="p_add">+	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
 		return ret;
 
 	if (off &lt; count &amp;&amp; user_count &lt;= (count - off)) {
<span class="p_header">diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c</span>
<span class="p_header">index e7380ba..fcf1473 100644</span>
<span class="p_header">--- a/arch/arm/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -851,7 +851,7 @@</span> <span class="p_context"> static int __arm_dma_mmap(struct device *dev, struct vm_area_struct *vma,</span>
 	unsigned long pfn = dma_to_pfn(dev, dma_addr);
 	unsigned long off = vma-&gt;vm_pgoff;
 
<span class="p_del">-	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="p_add">+	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
 		return ret;
 
 	if (off &lt; nr_pages &amp;&amp; nr_vma_pages &lt;= (nr_pages - off)) {
<span class="p_header">diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">index e90cd1d..f27d4dd 100644</span>
<span class="p_header">--- a/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -329,7 +329,7 @@</span> <span class="p_context"> static int __swiotlb_mmap(struct device *dev,</span>
 	vma-&gt;vm_page_prot = __get_dma_pgprot(attrs, vma-&gt;vm_page_prot,
 					     is_device_dma_coherent(dev));
 
<span class="p_del">-	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="p_add">+	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
 		return ret;
 
 	return __swiotlb_mmap_pfn(vma, pfn, size);
<span class="p_chunk">@@ -706,7 +706,7 @@</span> <span class="p_context"> static int __iommu_mmap_attrs(struct device *dev, struct vm_area_struct *vma,</span>
 	vma-&gt;vm_page_prot = __get_dma_pgprot(attrs, vma-&gt;vm_page_prot,
 					     is_device_dma_coherent(dev));
 
<span class="p_del">-	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="p_add">+	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
 		return ret;
 
 	if (attrs &amp; DMA_ATTR_FORCE_CONTIGUOUS) {
<span class="p_header">diff --git a/arch/mips/mm/dma-default.c b/arch/mips/mm/dma-default.c</span>
<span class="p_header">index e08598c..8e78251 100644</span>
<span class="p_header">--- a/arch/mips/mm/dma-default.c</span>
<span class="p_header">+++ b/arch/mips/mm/dma-default.c</span>
<span class="p_chunk">@@ -232,7 +232,7 @@</span> <span class="p_context"> static int mips_dma_mmap(struct device *dev, struct vm_area_struct *vma,</span>
 	else
 		vma-&gt;vm_page_prot = pgprot_noncached(vma-&gt;vm_page_prot);
 
<span class="p_del">-	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="p_add">+	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
 		return ret;
 
 	if (off &lt; count &amp;&amp; user_count &lt;= (count - off)) {
<span class="p_header">diff --git a/drivers/base/dma-coherent.c b/drivers/base/dma-coherent.c</span>
<span class="p_header">index 2ae24c2..d6f231c 100644</span>
<span class="p_header">--- a/drivers/base/dma-coherent.c</span>
<span class="p_header">+++ b/drivers/base/dma-coherent.c</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"> static inline struct dma_coherent_mem *dev_get_coherent_memory(struct device *de</span>
 {
 	if (dev &amp;&amp; dev-&gt;dma_mem)
 		return dev-&gt;dma_mem;
<span class="p_del">-	return dma_coherent_default_memory;</span>
<span class="p_add">+	return NULL;</span>
 }
 
 static inline dma_addr_t dma_get_device_base(struct device *dev,
<span class="p_chunk">@@ -165,34 +165,15 @@</span> <span class="p_context"> void *dma_mark_declared_memory_occupied(struct device *dev,</span>
 }
 EXPORT_SYMBOL(dma_mark_declared_memory_occupied);
 
<span class="p_del">-/**</span>
<span class="p_del">- * dma_alloc_from_coherent() - try to allocate memory from the per-device coherent area</span>
<span class="p_del">- *</span>
<span class="p_del">- * @dev:	device from which we allocate memory</span>
<span class="p_del">- * @size:	size of requested memory area</span>
<span class="p_del">- * @dma_handle:	This will be filled with the correct dma handle</span>
<span class="p_del">- * @ret:	This pointer will be filled with the virtual address</span>
<span class="p_del">- *		to allocated area.</span>
<span class="p_del">- *</span>
<span class="p_del">- * This function should be only called from per-arch dma_alloc_coherent()</span>
<span class="p_del">- * to support allocation from per-device coherent memory pools.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Returns 0 if dma_alloc_coherent should continue with allocating from</span>
<span class="p_del">- * generic memory areas, or !0 if dma_alloc_coherent should return @ret.</span>
<span class="p_del">- */</span>
<span class="p_del">-int dma_alloc_from_coherent(struct device *dev, ssize_t size,</span>
<span class="p_del">-				       dma_addr_t *dma_handle, void **ret)</span>
<span class="p_add">+static void *__dma_alloc_from_dev_coherent(struct dma_coherent_mem *mem,</span>
<span class="p_add">+				       ssize_t size, dma_addr_t *dma_handle)</span>
 {
<span class="p_del">-	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);</span>
 	int order = get_order(size);
 	unsigned long flags;
 	int pageno;
 	int dma_memory_map;
<span class="p_add">+	void *ret;</span>
 
<span class="p_del">-	if (!mem)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	*ret = NULL;</span>
 	spin_lock_irqsave(&amp;mem-&gt;spinlock, flags);
 
 	if (unlikely(size &gt; (mem-&gt;size &lt;&lt; PAGE_SHIFT)))
<span class="p_chunk">@@ -203,21 +184,51 @@</span> <span class="p_context"> int dma_alloc_from_coherent(struct device *dev, ssize_t size,</span>
 		goto err;
 
 	/*
<span class="p_del">-	 * Memory was found in the per-device area.</span>
<span class="p_add">+	 * Memory was found in the coherent area.</span>
 	 */
<span class="p_del">-	*dma_handle = dma_get_device_base(dev, mem) + (pageno &lt;&lt; PAGE_SHIFT);</span>
<span class="p_del">-	*ret = mem-&gt;virt_base + (pageno &lt;&lt; PAGE_SHIFT);</span>
<span class="p_add">+	*dma_handle = mem-&gt;device_base + (pageno &lt;&lt; PAGE_SHIFT);</span>
<span class="p_add">+	ret = mem-&gt;virt_base + (pageno &lt;&lt; PAGE_SHIFT);</span>
 	dma_memory_map = (mem-&gt;flags &amp; DMA_MEMORY_MAP);
 	spin_unlock_irqrestore(&amp;mem-&gt;spinlock, flags);
 	if (dma_memory_map)
<span class="p_del">-		memset(*ret, 0, size);</span>
<span class="p_add">+		memset(ret, 0, size);</span>
 	else
<span class="p_del">-		memset_io(*ret, 0, size);</span>
<span class="p_add">+		memset_io(ret, 0, size);</span>
 
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return ret;</span>
 
 err:
 	spin_unlock_irqrestore(&amp;mem-&gt;spinlock, flags);
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * dma_alloc_from_dev_coherent() - try to allocate memory from the per-device coherent area</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @dev:	device from which we allocate memory</span>
<span class="p_add">+ * @size:	size of requested memory area</span>
<span class="p_add">+ * @dma_handle:	This will be filled with the correct dma handle</span>
<span class="p_add">+ * @ret:	This pointer will be filled with the virtual address</span>
<span class="p_add">+ *		to allocated area.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function should be only called from per-arch dma_alloc_coherent()</span>
<span class="p_add">+ * to support allocation from per-device coherent memory pools.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns 0 if dma_alloc_coherent should continue with allocating from</span>
<span class="p_add">+ * generic memory areas, or !0 if dma_alloc_coherent should return @ret.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int dma_alloc_from_dev_coherent(struct device *dev, ssize_t size,</span>
<span class="p_add">+			    dma_addr_t *dma_handle, void **ret)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mem)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	*ret = __dma_alloc_from_dev_coherent(mem, size, dma_handle);</span>
<span class="p_add">+	if (*ret)</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+</span>
 	/*
 	 * In the case where the allocation can not be satisfied from the
 	 * per-device area, try to fall back to generic memory if the
<span class="p_chunk">@@ -225,25 +236,20 @@</span> <span class="p_context"> int dma_alloc_from_coherent(struct device *dev, ssize_t size,</span>
 	 */
 	return mem-&gt;flags &amp; DMA_MEMORY_EXCLUSIVE;
 }
<span class="p_del">-EXPORT_SYMBOL(dma_alloc_from_coherent);</span>
<span class="p_add">+EXPORT_SYMBOL(dma_alloc_from_dev_coherent);</span>
 
<span class="p_del">-/**</span>
<span class="p_del">- * dma_release_from_coherent() - try to free the memory allocated from per-device coherent memory pool</span>
<span class="p_del">- * @dev:	device from which the memory was allocated</span>
<span class="p_del">- * @order:	the order of pages allocated</span>
<span class="p_del">- * @vaddr:	virtual address of allocated pages</span>
<span class="p_del">- *</span>
<span class="p_del">- * This checks whether the memory was allocated from the per-device</span>
<span class="p_del">- * coherent memory pool and if so, releases that memory.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Returns 1 if we correctly released the memory, or 0 if</span>
<span class="p_del">- * dma_release_coherent() should proceed with releasing memory from</span>
<span class="p_del">- * generic pools.</span>
<span class="p_del">- */</span>
<span class="p_del">-int dma_release_from_coherent(struct device *dev, int order, void *vaddr)</span>
<span class="p_add">+void *dma_alloc_from_global_coherent(ssize_t size, dma_addr_t *dma_handle)</span>
 {
<span class="p_del">-	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);</span>
<span class="p_add">+	if (!dma_coherent_default_memory)</span>
<span class="p_add">+		return NULL;</span>
 
<span class="p_add">+	return __dma_alloc_from_dev_coherent(dma_coherent_default_memory, size, dma_handle);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+static int __dma_release_from_dev_coherent(struct dma_coherent_mem *mem,</span>
<span class="p_add">+				       int order, void *vaddr)</span>
<span class="p_add">+{</span>
 	if (mem &amp;&amp; vaddr &gt;= mem-&gt;virt_base &amp;&amp; vaddr &lt;
 		   (mem-&gt;virt_base + (mem-&gt;size &lt;&lt; PAGE_SHIFT))) {
 		int page = (vaddr - mem-&gt;virt_base) &gt;&gt; PAGE_SHIFT;
<span class="p_chunk">@@ -256,28 +262,42 @@</span> <span class="p_context"> int dma_release_from_coherent(struct device *dev, int order, void *vaddr)</span>
 	}
 	return 0;
 }
<span class="p_del">-EXPORT_SYMBOL(dma_release_from_coherent);</span>
 
 /**
<span class="p_del">- * dma_mmap_from_coherent() - try to mmap the memory allocated from</span>
<span class="p_del">- * per-device coherent memory pool to userspace</span>
<span class="p_add">+ * dma_release_from_dev_coherent() - try to free the memory allocated from per-device coherent memory pool</span>
  * @dev:	device from which the memory was allocated
<span class="p_del">- * @vma:	vm_area for the userspace memory</span>
<span class="p_del">- * @vaddr:	cpu address returned by dma_alloc_from_coherent</span>
<span class="p_del">- * @size:	size of the memory buffer allocated by dma_alloc_from_coherent</span>
<span class="p_del">- * @ret:	result from remap_pfn_range()</span>
<span class="p_add">+ * @order:	the order of pages allocated</span>
<span class="p_add">+ * @vaddr:	virtual address of allocated pages</span>
  *
  * This checks whether the memory was allocated from the per-device
<span class="p_del">- * coherent memory pool and if so, maps that memory to the provided vma.</span>
<span class="p_add">+ * coherent memory pool and if so, releases that memory.</span>
  *
<span class="p_del">- * Returns 1 if we correctly mapped the memory, or 0 if the caller should</span>
<span class="p_del">- * proceed with mapping memory from generic pools.</span>
<span class="p_add">+ * Returns 1 if we correctly released the memory, or 0 if</span>
<span class="p_add">+ * dma_release_coherent() should proceed with releasing memory from</span>
<span class="p_add">+ * generic pools.</span>
  */
<span class="p_del">-int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,</span>
<span class="p_del">-			   void *vaddr, size_t size, int *ret)</span>
<span class="p_add">+int dma_release_from_dev_coherent(struct device *dev, int order, void *vaddr)</span>
 {
 	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);
 
<span class="p_add">+	return __dma_release_from_dev_coherent(mem, order, vaddr);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(dma_release_from_dev_coherent);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+int dma_release_from_global_coherent(int order, void *vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!dma_coherent_default_memory)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return __dma_release_from_dev_coherent(dma_coherent_default_memory,</span>
<span class="p_add">+					   order, vaddr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __dma_mmap_from_dev_coherent(struct dma_coherent_mem *mem,</span>
<span class="p_add">+				    struct vm_area_struct *vma, void *vaddr,</span>
<span class="p_add">+				    size_t size, int *ret)</span>
<span class="p_add">+{</span>
 	if (mem &amp;&amp; vaddr &gt;= mem-&gt;virt_base &amp;&amp; vaddr + size &lt;=
 		   (mem-&gt;virt_base + (mem-&gt;size &lt;&lt; PAGE_SHIFT))) {
 		unsigned long off = vma-&gt;vm_pgoff;
<span class="p_chunk">@@ -296,7 +316,40 @@</span> <span class="p_context"> int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,</span>
 	}
 	return 0;
 }
<span class="p_del">-EXPORT_SYMBOL(dma_mmap_from_coherent);</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * dma_mmap_from_dev_coherent() - try to mmap the memory allocated from</span>
<span class="p_add">+ * per-device coherent memory pool to userspace</span>
<span class="p_add">+ * @dev:	device from which the memory was allocated</span>
<span class="p_add">+ * @vma:	vm_area for the userspace memory</span>
<span class="p_add">+ * @vaddr:	cpu address returned by dma_alloc_from_dev_coherent</span>
<span class="p_add">+ * @size:	size of the memory buffer allocated by dma_alloc_from_dev_coherent</span>
<span class="p_add">+ * @ret:	result from remap_pfn_range()</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This checks whether the memory was allocated from the per-device</span>
<span class="p_add">+ * coherent memory pool and if so, maps that memory to the provided vma.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns 1 if we correctly mapped the memory, or 0 if the caller should</span>
<span class="p_add">+ * proceed with mapping memory from generic pools.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int dma_mmap_from_dev_coherent(struct device *dev, struct vm_area_struct *vma,</span>
<span class="p_add">+			   void *vaddr, size_t size, int *ret)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct dma_coherent_mem *mem = dev_get_coherent_memory(dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	return __dma_mmap_from_dev_coherent(mem, vma, vaddr, size, ret);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(dma_mmap_from_dev_coherent);</span>
<span class="p_add">+</span>
<span class="p_add">+int dma_mmap_from_global_coherent(struct vm_area_struct *vma, void *vaddr,</span>
<span class="p_add">+				   size_t size, int *ret)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!dma_coherent_default_memory)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return __dma_mmap_from_dev_coherent(dma_coherent_default_memory, vma,</span>
<span class="p_add">+					vaddr, size, ret);</span>
<span class="p_add">+}</span>
 
 /*
  * Support for reserved memory regions defined in device tree
<span class="p_header">diff --git a/drivers/base/dma-mapping.c b/drivers/base/dma-mapping.c</span>
<span class="p_header">index 5096755..b555ff9 100644</span>
<span class="p_header">--- a/drivers/base/dma-mapping.c</span>
<span class="p_header">+++ b/drivers/base/dma-mapping.c</span>
<span class="p_chunk">@@ -235,7 +235,7 @@</span> <span class="p_context"> int dma_common_mmap(struct device *dev, struct vm_area_struct *vma,</span>
 
 	vma-&gt;vm_page_prot = pgprot_noncached(vma-&gt;vm_page_prot);
 
<span class="p_del">-	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="p_add">+	if (dma_mmap_from_dev_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
 		return ret;
 
 	if (off &lt; count &amp;&amp; user_count &lt;= (count - off)) {
<span class="p_header">diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h</span>
<span class="p_header">index 843ab86..03c0196 100644</span>
<span class="p_header">--- a/include/linux/dma-mapping.h</span>
<span class="p_header">+++ b/include/linux/dma-mapping.h</span>
<span class="p_chunk">@@ -157,16 +157,40 @@</span> <span class="p_context"> static inline int is_device_dma_capable(struct device *dev)</span>
  * These three functions are only for dma allocator.
  * Don&#39;t use them in device drivers.
  */
<span class="p_del">-int dma_alloc_from_coherent(struct device *dev, ssize_t size,</span>
<span class="p_add">+int dma_alloc_from_dev_coherent(struct device *dev, ssize_t size,</span>
 				       dma_addr_t *dma_handle, void **ret);
<span class="p_del">-int dma_release_from_coherent(struct device *dev, int order, void *vaddr);</span>
<span class="p_add">+int dma_release_from_dev_coherent(struct device *dev, int order, void *vaddr);</span>
 
<span class="p_del">-int dma_mmap_from_coherent(struct device *dev, struct vm_area_struct *vma,</span>
<span class="p_add">+int dma_mmap_from_dev_coherent(struct device *dev, struct vm_area_struct *vma,</span>
 			    void *cpu_addr, size_t size, int *ret);
<span class="p_add">+</span>
<span class="p_add">+void *dma_alloc_from_global_coherent(ssize_t size, dma_addr_t *dma_handle);</span>
<span class="p_add">+int dma_release_from_global_coherent(int order, void *vaddr);</span>
<span class="p_add">+int dma_mmap_from_global_coherent(struct vm_area_struct *vma, void *cpu_addr,</span>
<span class="p_add">+				  size_t size, int *ret);</span>
<span class="p_add">+</span>
 #else
<span class="p_del">-#define dma_alloc_from_coherent(dev, size, handle, ret) (0)</span>
<span class="p_del">-#define dma_release_from_coherent(dev, order, vaddr) (0)</span>
<span class="p_del">-#define dma_mmap_from_coherent(dev, vma, vaddr, order, ret) (0)</span>
<span class="p_add">+#define dma_alloc_from_dev_coherent(dev, size, handle, ret) (0)</span>
<span class="p_add">+#define dma_release_from_dev_coherent(dev, order, vaddr) (0)</span>
<span class="p_add">+#define dma_mmap_from_dev_coherent(dev, vma, vaddr, order, ret) (0)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void *dma_alloc_from_global_coherent(ssize_t size,</span>
<span class="p_add">+						   dma_addr_t *dma_handle)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int dma_release_from_global_coherent(int order, void *vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int dma_mmap_from_global_coherent(struct vm_area_struct *vma,</span>
<span class="p_add">+						void *cpu_addr, size_t size,</span>
<span class="p_add">+						int *ret)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 #endif /* CONFIG_HAVE_GENERIC_DMA_COHERENT */
 
 #ifdef CONFIG_HAS_DMA
<span class="p_chunk">@@ -481,7 +505,7 @@</span> <span class="p_context"> static inline void *dma_alloc_attrs(struct device *dev, size_t size,</span>
 
 	BUG_ON(!ops);
 
<span class="p_del">-	if (dma_alloc_from_coherent(dev, size, dma_handle, &amp;cpu_addr))</span>
<span class="p_add">+	if (dma_alloc_from_dev_coherent(dev, size, dma_handle, &amp;cpu_addr))</span>
 		return cpu_addr;
 
 	if (!arch_dma_alloc_attrs(&amp;dev, &amp;flag))
<span class="p_chunk">@@ -503,7 +527,7 @@</span> <span class="p_context"> static inline void dma_free_attrs(struct device *dev, size_t size,</span>
 	BUG_ON(!ops);
 	WARN_ON(irqs_disabled());
 
<span class="p_del">-	if (dma_release_from_coherent(dev, get_order(size), cpu_addr))</span>
<span class="p_add">+	if (dma_release_from_dev_coherent(dev, get_order(size), cpu_addr))</span>
 		return;
 
 	if (!ops-&gt;free || !cpu_addr)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



