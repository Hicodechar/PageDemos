
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86/mm changes for v4.6 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86/mm changes for v4.6</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 14, 2016, 1:25 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160314132537.GA8641@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8579151/mbox/"
   >mbox</a>
|
   <a href="/patch/8579151/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8579151/">/patch/8579151/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id C8C96C0553
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 14 Mar 2016 13:25:53 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 130E1203DC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 14 Mar 2016 13:25:52 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 1FCE2203E3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 14 Mar 2016 13:25:50 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S934421AbcCNNZo (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 14 Mar 2016 09:25:44 -0400
Received: from mail-wm0-f66.google.com ([74.125.82.66]:35124 &quot;EHLO
	mail-wm0-f66.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S934227AbcCNNZl (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 14 Mar 2016 09:25:41 -0400
Received: by mail-wm0-f66.google.com with SMTP id n205so15056062wmf.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 14 Mar 2016 06:25:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:user-agent;
	bh=YFpl+WXEYjQMUyR3wS70CirQ6q89rq7kedfYonvh8ys=;
	b=r+/xs0qschPWnrhnik5fbHYrTEg7cxDwCviStMBKjOp5t/cL5hd01G3azn8d99IG81
	y3ND4FQCHLbIk0GTKL5LA58XYD7xfVe9EeCJPxcr3tKzlbf5dQqtdrDcWtKJLZ00X6Ls
	eTn9/brNRiHcN7neqP2ZGNZmyWegCVQIyaDXxf//3vjVo5DXxJBx/P7yf0tRgS+tS2SP
	aSK9a2jjvC0TbMUOK1h/WwxhgYa7mdMNsM0BYdfSFmef9DQbUozs+hdKTgd4VV5eWH4d
	fI9Mr6Vi4vtQjY6TTeE9qGPDwFhVyt3WV80JNLMLFlR9CNgOF0FKi9QPyg6mc37N3bzO
	2SiA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:user-agent;
	bh=YFpl+WXEYjQMUyR3wS70CirQ6q89rq7kedfYonvh8ys=;
	b=lDL50w85RdWnJzGP6m9z6K8GaA0C+0PqnCLw32iBAye+dqMLOsHXPUWK2A4kxejiI2
	tOjQ4eF/ljWuhscwEBgtVdxRnQc2n7WQN3wlPDEXAKvEibx8wPg7NXiy4wJN/kZBT0XN
	pSyfZtUatJLvwmD7UTiAnvcJ82sxuIDc+rpJIO+04Fq5rWVH37yGaqvDmIWEet0qMfAB
	65/VKmOt4tv0ICM/AE0QVPF77ubfsPE69gQgVo3cp2QyEBrXS0S6qCD50kj2MBNTwUPy
	1uTsJb8K86HD5bbJxiSE6GKltec5VdwTnJ+xwDYtTbzw4zzl/fdU1kq45it0YMoBivFC
	mVpQ==
X-Gm-Message-State: AD7BkJKlKIicWS2Jo9eaOW2JC+XK7C+xQarPu48KUTiQEDSL6JAGsOv9HZl/lxu+IH4loA==
X-Received: by 10.28.88.15 with SMTP id m15mr17142244wmb.60.1457961940147;
	Mon, 14 Mar 2016 06:25:40 -0700 (PDT)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	n66sm16300338wmg.20.2016.03.14.06.25.38
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 14 Mar 2016 06:25:39 -0700 (PDT)
Date: Mon, 14 Mar 2016 14:25:37 +0100
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] x86/mm changes for v4.6
Message-ID: &lt;20160314132537.GA8641@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.23 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.8 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,UNPARSEABLE_RELAY
	autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - March 14, 2016, 1:25 p.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-mm-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-mm-for-linus

   # HEAD: 8b8addf891de8a00e4d39fc32f93f7c5eb8feceb x86/mm/32: Enable full randomization on i386 and X86_32

The main changes in this cycle were:

 - Enable full ASLR randomization for 32-bit programs (Hector Marco-Gisbert)

 - Add initial minimal INVPCI support, to flush global mappings (Andy Lutomirski)

 - Add KASAN enhancements (Andrey Ryabinin)

 - Fix mmiotrace for huge pages (Karol Herbst)

 - ... misc cleanups and small enhancements.

 Thanks,

	Ingo

------------------&gt;
Andrey Ryabinin (2):
      x86/kasan: Clear kasan_zero_page after TLB flush
      x86/kasan: Write protect kasan zero shadow

Andy Lutomirski (6):
      x86/mm/32: Set NX in __supported_pte_mask before enabling paging
      x86/mm: Make kmap_prot into a #define
      x86/mm: Add INVPCID helpers
      x86/mm: Add a &#39;noinvpcid&#39; boot option to turn off INVPCID
      x86/mm: If INVPCID is available, use it to flush global mappings
      x86/dmi: Switch dmi_remap() from ioremap() [uncached] to ioremap_cache()

Borislav Petkov (2):
      x86/mm: Fix INVPCID asm constraint
      x86/mm/ptdump: Remove paravirt_enabled()

Hector Marco-Gisbert (1):
      x86/mm/32: Enable full randomization on i386 and X86_32

Ingo Molnar (2):
      x86/mm/numa: Clean up numa_clear_kernel_node_hotplug()
      x86/mm/numa: Check for failures in numa_clear_kernel_node_hotplug()

Jan Beulich (1):
      x86/mm: Avoid premature success when changing page attributes

Karol Herbst (1):
      x86/mm/kmmio: Fix mmiotrace for hugepages

Seth Jennings (1):
      x86/mm: Streamline and restore probe_memory_block_size()


 Documentation/kernel-parameters.txt |  2 +
 arch/x86/include/asm/dmi.h          |  2 +-
 arch/x86/include/asm/fixmap.h       |  2 +-
 arch/x86/include/asm/tlbflush.h     | 57 ++++++++++++++++++++++++
 arch/x86/kernel/cpu/common.c        | 16 +++++++
 arch/x86/kernel/head_32.S           |  6 +++
 arch/x86/mm/dump_pagetables.c       | 11 +++--
 arch/x86/mm/init_32.c               |  3 --
 arch/x86/mm/init_64.c               | 24 +++-------
 arch/x86/mm/kasan_init_64.c         | 17 +++++--
 arch/x86/mm/kmmio.c                 | 88 +++++++++++++++++++++++++------------
 arch/x86/mm/mmap.c                  | 14 +-----
 arch/x86/mm/numa.c                  | 67 ++++++++++++++++++----------
 arch/x86/mm/pageattr.c              |  4 +-
 arch/x86/mm/setup_nx.c              |  5 +--
 15 files changed, 217 insertions(+), 101 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/kernel-parameters.txt b/Documentation/kernel-parameters.txt</span>
<span class="p_header">index 551ecf09c8dd..e4c4d2a5a28d 100644</span>
<span class="p_header">--- a/Documentation/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2566,6 +2566,8 @@</span> <span class="p_context"> bytes respectively. Such letter suffixes can also be entirely omitted.</span>
 
 	nointroute	[IA-64]
 
<span class="p_add">+	noinvpcid	[X86] Disable the INVPCID cpu feature.</span>
<span class="p_add">+</span>
 	nojitter	[IA-64] Disables jitter checking for ITC timers.
 
 	no-kvmclock	[X86,KVM] Disable paravirtualized KVM clock driver
<span class="p_header">diff --git a/arch/x86/include/asm/dmi.h b/arch/x86/include/asm/dmi.h</span>
<span class="p_header">index 535192f6bfad..3c69fed215c5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/dmi.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/dmi.h</span>
<span class="p_chunk">@@ -15,7 +15,7 @@</span> <span class="p_context"> static __always_inline __init void *dmi_alloc(unsigned len)</span>
 /* Use early IO mappings for DMI because it&#39;s initialized early */
 #define dmi_early_remap		early_ioremap
 #define dmi_early_unmap		early_iounmap
<span class="p_del">-#define dmi_remap		ioremap</span>
<span class="p_add">+#define dmi_remap		ioremap_cache</span>
 #define dmi_unmap		iounmap
 
 #endif /* _ASM_X86_DMI_H */
<span class="p_header">diff --git a/arch/x86/include/asm/fixmap.h b/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">index 6d7d0e52ed5a..8554f960e21b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fixmap.h</span>
<span class="p_chunk">@@ -138,7 +138,7 @@</span> <span class="p_context"> extern void reserve_top_address(unsigned long reserve);</span>
 extern int fixmaps_set;
 
 extern pte_t *kmap_pte;
<span class="p_del">-extern pgprot_t kmap_prot;</span>
<span class="p_add">+#define kmap_prot PAGE_KERNEL</span>
 extern pte_t *pkmap_page_table;
 
 void __native_set_fixmap(enum fixed_addresses idx, pte_t pte);
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 6df2029405a3..d0cce90b0855 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -7,6 +7,54 @@</span> <span class="p_context"></span>
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/special_insns.h&gt;
 
<span class="p_add">+static inline void __invpcid(unsigned long pcid, unsigned long addr,</span>
<span class="p_add">+			     unsigned long type)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct { u64 d[2]; } desc = { { pcid, addr } };</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The memory clobber is because the whole point is to invalidate</span>
<span class="p_add">+	 * stale TLB entries and, especially if we&#39;re flushing global</span>
<span class="p_add">+	 * mappings, we don&#39;t want the compiler to reorder any subsequent</span>
<span class="p_add">+	 * memory accesses before the TLB flush.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The hex opcode is invpcid (%ecx), %eax in 32-bit mode and</span>
<span class="p_add">+	 * invpcid (%rcx), %rax in long mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	asm volatile (&quot;.byte 0x66, 0x0f, 0x38, 0x82, 0x01&quot;</span>
<span class="p_add">+		      : : &quot;m&quot; (desc), &quot;a&quot; (type), &quot;c&quot; (&amp;desc) : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define INVPCID_TYPE_INDIV_ADDR		0</span>
<span class="p_add">+#define INVPCID_TYPE_SINGLE_CTXT	1</span>
<span class="p_add">+#define INVPCID_TYPE_ALL_INCL_GLOBAL	2</span>
<span class="p_add">+#define INVPCID_TYPE_ALL_NON_GLOBAL	3</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings for a given pcid and addr, not including globals. */</span>
<span class="p_add">+static inline void invpcid_flush_one(unsigned long pcid,</span>
<span class="p_add">+				     unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(pcid, addr, INVPCID_TYPE_INDIV_ADDR);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings for a given PCID, not including globals. */</span>
<span class="p_add">+static inline void invpcid_flush_single_context(unsigned long pcid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(pcid, 0, INVPCID_TYPE_SINGLE_CTXT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings, including globals, for all PCIDs. */</span>
<span class="p_add">+static inline void invpcid_flush_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(0, 0, INVPCID_TYPE_ALL_INCL_GLOBAL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush all mappings for all PCIDs except globals. */</span>
<span class="p_add">+static inline void invpcid_flush_all_nonglobals(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__invpcid(0, 0, INVPCID_TYPE_ALL_NON_GLOBAL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_PARAVIRT
 #include &lt;asm/paravirt.h&gt;
 #else
<span class="p_chunk">@@ -104,6 +152,15 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
 {
 	unsigned long flags;
 
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_INVPCID)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Using INVPCID is considerably faster than a pair of writes</span>
<span class="p_add">+		 * to CR4 sandwiched inside an IRQ flag save/restore.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		invpcid_flush_all();</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * Read-modify-write to CR4 - protect it from preemption and
 	 * from interrupts. (Use the raw variant because this code can
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 37830de8f60a..f4d0aa64d934 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -162,6 +162,22 @@</span> <span class="p_context"> static int __init x86_mpx_setup(char *s)</span>
 }
 __setup(&quot;nompx&quot;, x86_mpx_setup);
 
<span class="p_add">+static int __init x86_noinvpcid_setup(char *s)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* noinvpcid doesn&#39;t accept parameters */</span>
<span class="p_add">+	if (s)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* do not emit a message if the feature is not present */</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_INVPCID))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	setup_clear_cpu_cap(X86_FEATURE_INVPCID);</span>
<span class="p_add">+	pr_info(&quot;noinvpcid: INVPCID feature disabled\n&quot;);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+early_param(&quot;noinvpcid&quot;, x86_noinvpcid_setup);</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_32
 static int cachesize_override = -1;
 static int disable_x86_serial_nr = 1;
<span class="p_header">diff --git a/arch/x86/kernel/head_32.S b/arch/x86/kernel/head_32.S</span>
<span class="p_header">index 6bc9ae24b6d2..57fc3f8c85fd 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_32.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_32.S</span>
<span class="p_chunk">@@ -389,6 +389,12 @@</span> <span class="p_context"> ENTRY(startup_32_smp)</span>
 	/* Make changes effective */
 	wrmsr
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * And make sure that all the mappings we set up have NX set from</span>
<span class="p_add">+	 * the beginning.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	orl $(1 &lt;&lt; (_PAGE_BIT_NX - 32)), pa(__supported_pte_mask + 4)</span>
<span class="p_add">+</span>
 enable_paging:
 
 /*
<span class="p_header">diff --git a/arch/x86/mm/dump_pagetables.c b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">index 4a6f1d9b5106..99bfb192803f 100644</span>
<span class="p_header">--- a/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_chunk">@@ -358,20 +358,19 @@</span> <span class="p_context"> static void walk_pud_level(struct seq_file *m, struct pg_state *st, pgd_t addr,</span>
 #define pgd_none(a)  pud_none(__pud(pgd_val(a)))
 #endif
 
<span class="p_del">-#ifdef CONFIG_X86_64</span>
 static inline bool is_hypervisor_range(int idx)
 {
<span class="p_add">+#ifdef CONFIG_X86_64</span>
 	/*
 	 * ffff800000000000 - ffff87ffffffffff is reserved for
 	 * the hypervisor.
 	 */
<span class="p_del">-	return paravirt_enabled() &amp;&amp;</span>
<span class="p_del">-		(idx &gt;= pgd_index(__PAGE_OFFSET) - 16) &amp;&amp;</span>
<span class="p_del">-		(idx &lt; pgd_index(__PAGE_OFFSET));</span>
<span class="p_del">-}</span>
<span class="p_add">+	return	(idx &gt;= pgd_index(__PAGE_OFFSET) - 16) &amp;&amp;</span>
<span class="p_add">+		(idx &lt;  pgd_index(__PAGE_OFFSET));</span>
 #else
<span class="p_del">-static inline bool is_hypervisor_range(int idx) { return false; }</span>
<span class="p_add">+	return false;</span>
 #endif
<span class="p_add">+}</span>
 
 static void ptdump_walk_pgd_level_core(struct seq_file *m, pgd_t *pgd,
 				       bool checkwx)
<span class="p_header">diff --git a/arch/x86/mm/init_32.c b/arch/x86/mm/init_32.c</span>
<span class="p_header">index cb4ef3de61f9..a4bb1c7ab65e 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_32.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_32.c</span>
<span class="p_chunk">@@ -388,7 +388,6 @@</span> <span class="p_context"> kernel_physical_mapping_init(unsigned long start,</span>
 }
 
 pte_t *kmap_pte;
<span class="p_del">-pgprot_t kmap_prot;</span>
 
 static inline pte_t *kmap_get_fixmap_pte(unsigned long vaddr)
 {
<span class="p_chunk">@@ -405,8 +404,6 @@</span> <span class="p_context"> static void __init kmap_init(void)</span>
 	 */
 	kmap_vstart = __fix_to_virt(FIX_KMAP_BEGIN);
 	kmap_pte = kmap_get_fixmap_pte(kmap_vstart);
<span class="p_del">-</span>
<span class="p_del">-	kmap_prot = PAGE_KERNEL;</span>
 }
 
 #ifdef CONFIG_HIGHMEM
<span class="p_header">diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c</span>
<span class="p_header">index 5488d21123bd..9686535edfb5 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_64.c</span>
<span class="p_chunk">@@ -53,6 +53,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/numa.h&gt;
 #include &lt;asm/cacheflush.h&gt;
 #include &lt;asm/init.h&gt;
<span class="p_add">+#include &lt;asm/uv/uv.h&gt;</span>
 #include &lt;asm/setup.h&gt;
 
 #include &quot;mm_internal.h&quot;
<span class="p_chunk">@@ -1206,26 +1207,13 @@</span> <span class="p_context"> int kern_addr_valid(unsigned long addr)</span>
 
 static unsigned long probe_memory_block_size(void)
 {
<span class="p_del">-	/* start from 2g */</span>
<span class="p_del">-	unsigned long bz = 1UL&lt;&lt;31;</span>
<span class="p_add">+	unsigned long bz = MIN_MEMORY_BLOCK_SIZE;</span>
 
<span class="p_del">-	if (totalram_pages &gt;= (64ULL &lt;&lt; (30 - PAGE_SHIFT))) {</span>
<span class="p_del">-		pr_info(&quot;Using 2GB memory block size for large-memory system\n&quot;);</span>
<span class="p_del">-		return 2UL * 1024 * 1024 * 1024;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* less than 64g installed */</span>
<span class="p_del">-	if ((max_pfn &lt;&lt; PAGE_SHIFT) &lt; (16UL &lt;&lt; 32))</span>
<span class="p_del">-		return MIN_MEMORY_BLOCK_SIZE;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* get the tail size */</span>
<span class="p_del">-	while (bz &gt; MIN_MEMORY_BLOCK_SIZE) {</span>
<span class="p_del">-		if (!((max_pfn &lt;&lt; PAGE_SHIFT) &amp; (bz - 1)))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		bz &gt;&gt;= 1;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	/* if system is UV or has 64GB of RAM or more, use large blocks */</span>
<span class="p_add">+	if (is_uv_system() || ((max_pfn &lt;&lt; PAGE_SHIFT) &gt;= (64UL &lt;&lt; 30)))</span>
<span class="p_add">+		bz = 2UL &lt;&lt; 30; /* 2GB */</span>
 
<span class="p_del">-	printk(KERN_DEBUG &quot;memory block size : %ldMB\n&quot;, bz &gt;&gt; 20);</span>
<span class="p_add">+	pr_info(&quot;x86/mm: Memory block size: %ldMB\n&quot;, bz &gt;&gt; 20);</span>
 
 	return bz;
 }
<span class="p_header">diff --git a/arch/x86/mm/kasan_init_64.c b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">index d470cf219a2d..1b1110fa0057 100644</span>
<span class="p_header">--- a/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_chunk">@@ -120,11 +120,22 @@</span> <span class="p_context"> void __init kasan_init(void)</span>
 	kasan_populate_zero_shadow(kasan_mem_to_shadow((void *)MODULES_END),
 			(void *)KASAN_SHADOW_END);
 
<span class="p_del">-	memset(kasan_zero_page, 0, PAGE_SIZE);</span>
<span class="p_del">-</span>
 	load_cr3(init_level4_pgt);
 	__flush_tlb_all();
<span class="p_del">-	init_task.kasan_depth = 0;</span>
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * kasan_zero_page has been used as early shadow memory, thus it may</span>
<span class="p_add">+	 * contain some garbage. Now we can clear and write protect it, since</span>
<span class="p_add">+	 * after the TLB flush no one should write to it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	memset(kasan_zero_page, 0, PAGE_SIZE);</span>
<span class="p_add">+	for (i = 0; i &lt; PTRS_PER_PTE; i++) {</span>
<span class="p_add">+		pte_t pte = __pte(__pa(kasan_zero_page) | __PAGE_KERNEL_RO);</span>
<span class="p_add">+		set_pte(&amp;kasan_zero_pte[i], pte);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/* Flush TLBs again to be sure that write protection applied. */</span>
<span class="p_add">+	__flush_tlb_all();</span>
<span class="p_add">+</span>
<span class="p_add">+	init_task.kasan_depth = 0;</span>
 	pr_info(&quot;KernelAddressSanitizer initialized\n&quot;);
 }
<span class="p_header">diff --git a/arch/x86/mm/kmmio.c b/arch/x86/mm/kmmio.c</span>
<span class="p_header">index 637ab34ed632..ddb2244b06a1 100644</span>
<span class="p_header">--- a/arch/x86/mm/kmmio.c</span>
<span class="p_header">+++ b/arch/x86/mm/kmmio.c</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"></span>
 struct kmmio_fault_page {
 	struct list_head list;
 	struct kmmio_fault_page *release_next;
<span class="p_del">-	unsigned long page; /* location of the fault page */</span>
<span class="p_add">+	unsigned long addr; /* the requested address */</span>
 	pteval_t old_presence; /* page presence prior to arming */
 	bool armed;
 
<span class="p_chunk">@@ -70,9 +70,16 @@</span> <span class="p_context"> unsigned int kmmio_count;</span>
 static struct list_head kmmio_page_table[KMMIO_PAGE_TABLE_SIZE];
 static LIST_HEAD(kmmio_probes);
 
<span class="p_del">-static struct list_head *kmmio_page_list(unsigned long page)</span>
<span class="p_add">+static struct list_head *kmmio_page_list(unsigned long addr)</span>
 {
<span class="p_del">-	return &amp;kmmio_page_table[hash_long(page, KMMIO_PAGE_HASH_BITS)];</span>
<span class="p_add">+	unsigned int l;</span>
<span class="p_add">+	pte_t *pte = lookup_address(addr, &amp;l);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pte)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	addr &amp;= page_level_mask(l);</span>
<span class="p_add">+</span>
<span class="p_add">+	return &amp;kmmio_page_table[hash_long(addr, KMMIO_PAGE_HASH_BITS)];</span>
 }
 
 /* Accessed per-cpu */
<span class="p_chunk">@@ -98,15 +105,19 @@</span> <span class="p_context"> static struct kmmio_probe *get_kmmio_probe(unsigned long addr)</span>
 }
 
 /* You must be holding RCU read lock. */
<span class="p_del">-static struct kmmio_fault_page *get_kmmio_fault_page(unsigned long page)</span>
<span class="p_add">+static struct kmmio_fault_page *get_kmmio_fault_page(unsigned long addr)</span>
 {
 	struct list_head *head;
 	struct kmmio_fault_page *f;
<span class="p_add">+	unsigned int l;</span>
<span class="p_add">+	pte_t *pte = lookup_address(addr, &amp;l);</span>
 
<span class="p_del">-	page &amp;= PAGE_MASK;</span>
<span class="p_del">-	head = kmmio_page_list(page);</span>
<span class="p_add">+	if (!pte)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	addr &amp;= page_level_mask(l);</span>
<span class="p_add">+	head = kmmio_page_list(addr);</span>
 	list_for_each_entry_rcu(f, head, list) {
<span class="p_del">-		if (f-&gt;page == page)</span>
<span class="p_add">+		if (f-&gt;addr == addr)</span>
 			return f;
 	}
 	return NULL;
<span class="p_chunk">@@ -137,10 +148,10 @@</span> <span class="p_context"> static void clear_pte_presence(pte_t *pte, bool clear, pteval_t *old)</span>
 static int clear_page_presence(struct kmmio_fault_page *f, bool clear)
 {
 	unsigned int level;
<span class="p_del">-	pte_t *pte = lookup_address(f-&gt;page, &amp;level);</span>
<span class="p_add">+	pte_t *pte = lookup_address(f-&gt;addr, &amp;level);</span>
 
 	if (!pte) {
<span class="p_del">-		pr_err(&quot;no pte for page 0x%08lx\n&quot;, f-&gt;page);</span>
<span class="p_add">+		pr_err(&quot;no pte for addr 0x%08lx\n&quot;, f-&gt;addr);</span>
 		return -1;
 	}
 
<span class="p_chunk">@@ -156,7 +167,7 @@</span> <span class="p_context"> static int clear_page_presence(struct kmmio_fault_page *f, bool clear)</span>
 		return -1;
 	}
 
<span class="p_del">-	__flush_tlb_one(f-&gt;page);</span>
<span class="p_add">+	__flush_tlb_one(f-&gt;addr);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -176,12 +187,12 @@</span> <span class="p_context"> static int arm_kmmio_fault_page(struct kmmio_fault_page *f)</span>
 	int ret;
 	WARN_ONCE(f-&gt;armed, KERN_ERR pr_fmt(&quot;kmmio page already armed.\n&quot;));
 	if (f-&gt;armed) {
<span class="p_del">-		pr_warning(&quot;double-arm: page 0x%08lx, ref %d, old %d\n&quot;,</span>
<span class="p_del">-			   f-&gt;page, f-&gt;count, !!f-&gt;old_presence);</span>
<span class="p_add">+		pr_warning(&quot;double-arm: addr 0x%08lx, ref %d, old %d\n&quot;,</span>
<span class="p_add">+			   f-&gt;addr, f-&gt;count, !!f-&gt;old_presence);</span>
 	}
 	ret = clear_page_presence(f, true);
<span class="p_del">-	WARN_ONCE(ret &lt; 0, KERN_ERR pr_fmt(&quot;arming 0x%08lx failed.\n&quot;),</span>
<span class="p_del">-		  f-&gt;page);</span>
<span class="p_add">+	WARN_ONCE(ret &lt; 0, KERN_ERR pr_fmt(&quot;arming at 0x%08lx failed.\n&quot;),</span>
<span class="p_add">+		  f-&gt;addr);</span>
 	f-&gt;armed = true;
 	return ret;
 }
<span class="p_chunk">@@ -191,7 +202,7 @@</span> <span class="p_context"> static void disarm_kmmio_fault_page(struct kmmio_fault_page *f)</span>
 {
 	int ret = clear_page_presence(f, false);
 	WARN_ONCE(ret &lt; 0,
<span class="p_del">-			KERN_ERR &quot;kmmio disarming 0x%08lx failed.\n&quot;, f-&gt;page);</span>
<span class="p_add">+			KERN_ERR &quot;kmmio disarming at 0x%08lx failed.\n&quot;, f-&gt;addr);</span>
 	f-&gt;armed = false;
 }
 
<span class="p_chunk">@@ -215,6 +226,12 @@</span> <span class="p_context"> int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
 	struct kmmio_context *ctx;
 	struct kmmio_fault_page *faultpage;
 	int ret = 0; /* default to fault not handled */
<span class="p_add">+	unsigned long page_base = addr;</span>
<span class="p_add">+	unsigned int l;</span>
<span class="p_add">+	pte_t *pte = lookup_address(addr, &amp;l);</span>
<span class="p_add">+	if (!pte)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	page_base &amp;= page_level_mask(l);</span>
 
 	/*
 	 * Preemption is now disabled to prevent process switch during
<span class="p_chunk">@@ -227,7 +244,7 @@</span> <span class="p_context"> int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
 	preempt_disable();
 	rcu_read_lock();
 
<span class="p_del">-	faultpage = get_kmmio_fault_page(addr);</span>
<span class="p_add">+	faultpage = get_kmmio_fault_page(page_base);</span>
 	if (!faultpage) {
 		/*
 		 * Either this page fault is not caused by kmmio, or
<span class="p_chunk">@@ -239,7 +256,7 @@</span> <span class="p_context"> int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
 
 	ctx = &amp;get_cpu_var(kmmio_ctx);
 	if (ctx-&gt;active) {
<span class="p_del">-		if (addr == ctx-&gt;addr) {</span>
<span class="p_add">+		if (page_base == ctx-&gt;addr) {</span>
 			/*
 			 * A second fault on the same page means some other
 			 * condition needs handling by do_page_fault(), the
<span class="p_chunk">@@ -267,9 +284,9 @@</span> <span class="p_context"> int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
 	ctx-&gt;active++;
 
 	ctx-&gt;fpage = faultpage;
<span class="p_del">-	ctx-&gt;probe = get_kmmio_probe(addr);</span>
<span class="p_add">+	ctx-&gt;probe = get_kmmio_probe(page_base);</span>
 	ctx-&gt;saved_flags = (regs-&gt;flags &amp; (X86_EFLAGS_TF | X86_EFLAGS_IF));
<span class="p_del">-	ctx-&gt;addr = addr;</span>
<span class="p_add">+	ctx-&gt;addr = page_base;</span>
 
 	if (ctx-&gt;probe &amp;&amp; ctx-&gt;probe-&gt;pre_handler)
 		ctx-&gt;probe-&gt;pre_handler(ctx-&gt;probe, regs, addr);
<span class="p_chunk">@@ -354,12 +371,11 @@</span> <span class="p_context"> static int post_kmmio_handler(unsigned long condition, struct pt_regs *regs)</span>
 }
 
 /* You must be holding kmmio_lock. */
<span class="p_del">-static int add_kmmio_fault_page(unsigned long page)</span>
<span class="p_add">+static int add_kmmio_fault_page(unsigned long addr)</span>
 {
 	struct kmmio_fault_page *f;
 
<span class="p_del">-	page &amp;= PAGE_MASK;</span>
<span class="p_del">-	f = get_kmmio_fault_page(page);</span>
<span class="p_add">+	f = get_kmmio_fault_page(addr);</span>
 	if (f) {
 		if (!f-&gt;count)
 			arm_kmmio_fault_page(f);
<span class="p_chunk">@@ -372,26 +388,25 @@</span> <span class="p_context"> static int add_kmmio_fault_page(unsigned long page)</span>
 		return -1;
 
 	f-&gt;count = 1;
<span class="p_del">-	f-&gt;page = page;</span>
<span class="p_add">+	f-&gt;addr = addr;</span>
 
 	if (arm_kmmio_fault_page(f)) {
 		kfree(f);
 		return -1;
 	}
 
<span class="p_del">-	list_add_rcu(&amp;f-&gt;list, kmmio_page_list(f-&gt;page));</span>
<span class="p_add">+	list_add_rcu(&amp;f-&gt;list, kmmio_page_list(f-&gt;addr));</span>
 
 	return 0;
 }
 
 /* You must be holding kmmio_lock. */
<span class="p_del">-static void release_kmmio_fault_page(unsigned long page,</span>
<span class="p_add">+static void release_kmmio_fault_page(unsigned long addr,</span>
 				struct kmmio_fault_page **release_list)
 {
 	struct kmmio_fault_page *f;
 
<span class="p_del">-	page &amp;= PAGE_MASK;</span>
<span class="p_del">-	f = get_kmmio_fault_page(page);</span>
<span class="p_add">+	f = get_kmmio_fault_page(addr);</span>
 	if (!f)
 		return;
 
<span class="p_chunk">@@ -420,18 +435,27 @@</span> <span class="p_context"> int register_kmmio_probe(struct kmmio_probe *p)</span>
 	int ret = 0;
 	unsigned long size = 0;
 	const unsigned long size_lim = p-&gt;len + (p-&gt;addr &amp; ~PAGE_MASK);
<span class="p_add">+	unsigned int l;</span>
<span class="p_add">+	pte_t *pte;</span>
 
 	spin_lock_irqsave(&amp;kmmio_lock, flags);
 	if (get_kmmio_probe(p-&gt;addr)) {
 		ret = -EEXIST;
 		goto out;
 	}
<span class="p_add">+</span>
<span class="p_add">+	pte = lookup_address(p-&gt;addr, &amp;l);</span>
<span class="p_add">+	if (!pte) {</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	kmmio_count++;
 	list_add_rcu(&amp;p-&gt;list, &amp;kmmio_probes);
 	while (size &lt; size_lim) {
 		if (add_kmmio_fault_page(p-&gt;addr + size))
 			pr_err(&quot;Unable to set page fault.\n&quot;);
<span class="p_del">-		size += PAGE_SIZE;</span>
<span class="p_add">+		size += page_level_size(l);</span>
 	}
 out:
 	spin_unlock_irqrestore(&amp;kmmio_lock, flags);
<span class="p_chunk">@@ -506,11 +530,17 @@</span> <span class="p_context"> void unregister_kmmio_probe(struct kmmio_probe *p)</span>
 	const unsigned long size_lim = p-&gt;len + (p-&gt;addr &amp; ~PAGE_MASK);
 	struct kmmio_fault_page *release_list = NULL;
 	struct kmmio_delayed_release *drelease;
<span class="p_add">+	unsigned int l;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = lookup_address(p-&gt;addr, &amp;l);</span>
<span class="p_add">+	if (!pte)</span>
<span class="p_add">+		return;</span>
 
 	spin_lock_irqsave(&amp;kmmio_lock, flags);
 	while (size &lt; size_lim) {
 		release_kmmio_fault_page(p-&gt;addr + size, &amp;release_list);
<span class="p_del">-		size += PAGE_SIZE;</span>
<span class="p_add">+		size += page_level_size(l);</span>
 	}
 	list_del_rcu(&amp;p-&gt;list);
 	kmmio_count--;
<span class="p_header">diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c</span>
<span class="p_header">index 96bd1e2bffaf..389939f74dd5 100644</span>
<span class="p_header">--- a/arch/x86/mm/mmap.c</span>
<span class="p_header">+++ b/arch/x86/mm/mmap.c</span>
<span class="p_chunk">@@ -94,18 +94,6 @@</span> <span class="p_context"> static unsigned long mmap_base(unsigned long rnd)</span>
 }
 
 /*
<span class="p_del">- * Bottom-up (legacy) layout on X86_32 did not support randomization, X86_64</span>
<span class="p_del">- * does, but not when emulating X86_32</span>
<span class="p_del">- */</span>
<span class="p_del">-static unsigned long mmap_legacy_base(unsigned long rnd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (mmap_is_ia32())</span>
<span class="p_del">-		return TASK_UNMAPPED_BASE;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		return TASK_UNMAPPED_BASE + rnd;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * This function, called very early during the creation of a new
  * process VM image, sets up which VM layout function to use:
  */
<span class="p_chunk">@@ -116,7 +104,7 @@</span> <span class="p_context"> void arch_pick_mmap_layout(struct mm_struct *mm)</span>
 	if (current-&gt;flags &amp; PF_RANDOMIZE)
 		random_factor = arch_mmap_rnd();
 
<span class="p_del">-	mm-&gt;mmap_legacy_base = mmap_legacy_base(random_factor);</span>
<span class="p_add">+	mm-&gt;mmap_legacy_base = TASK_UNMAPPED_BASE + random_factor;</span>
 
 	if (mmap_is_legacy()) {
 		mm-&gt;mmap_base = mm-&gt;mmap_legacy_base;
<span class="p_header">diff --git a/arch/x86/mm/numa.c b/arch/x86/mm/numa.c</span>
<span class="p_header">index d04f8094bc23..f70c1ff46125 100644</span>
<span class="p_header">--- a/arch/x86/mm/numa.c</span>
<span class="p_header">+++ b/arch/x86/mm/numa.c</span>
<span class="p_chunk">@@ -465,46 +465,67 @@</span> <span class="p_context"> static bool __init numa_meminfo_cover_memory(const struct numa_meminfo *mi)</span>
 	return true;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Mark all currently memblock-reserved physical memory (which covers the</span>
<span class="p_add">+ * kernel&#39;s own memory ranges) as hot-unswappable.</span>
<span class="p_add">+ */</span>
 static void __init numa_clear_kernel_node_hotplug(void)
 {
<span class="p_del">-	int i, nid;</span>
<span class="p_del">-	nodemask_t numa_kernel_nodes = NODE_MASK_NONE;</span>
<span class="p_del">-	phys_addr_t start, end;</span>
<span class="p_del">-	struct memblock_region *r;</span>
<span class="p_add">+	nodemask_t reserved_nodemask = NODE_MASK_NONE;</span>
<span class="p_add">+	struct memblock_region *mb_region;</span>
<span class="p_add">+	int i;</span>
 
 	/*
<span class="p_add">+	 * We have to do some preprocessing of memblock regions, to</span>
<span class="p_add">+	 * make them suitable for reservation.</span>
<span class="p_add">+	 *</span>
 	 * At this time, all memory regions reserved by memblock are
<span class="p_del">-	 * used by the kernel. Set the nid in memblock.reserved will</span>
<span class="p_del">-	 * mark out all the nodes the kernel resides in.</span>
<span class="p_add">+	 * used by the kernel, but those regions are not split up</span>
<span class="p_add">+	 * along node boundaries yet, and don&#39;t necessarily have their</span>
<span class="p_add">+	 * node ID set yet either.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * So iterate over all memory known to the x86 architecture,</span>
<span class="p_add">+	 * and use those ranges to set the nid in memblock.reserved.</span>
<span class="p_add">+	 * This will split up the memblock regions along node</span>
<span class="p_add">+	 * boundaries and will set the node IDs as well.</span>
 	 */
 	for (i = 0; i &lt; numa_meminfo.nr_blks; i++) {
<span class="p_del">-		struct numa_memblk *mb = &amp;numa_meminfo.blk[i];</span>
<span class="p_add">+		struct numa_memblk *mb = numa_meminfo.blk + i;</span>
<span class="p_add">+		int ret;</span>
 
<span class="p_del">-		memblock_set_node(mb-&gt;start, mb-&gt;end - mb-&gt;start,</span>
<span class="p_del">-				  &amp;memblock.reserved, mb-&gt;nid);</span>
<span class="p_add">+		ret = memblock_set_node(mb-&gt;start, mb-&gt;end - mb-&gt;start, &amp;memblock.reserved, mb-&gt;nid);</span>
<span class="p_add">+		WARN_ON_ONCE(ret);</span>
 	}
 
 	/*
<span class="p_del">-	 * Mark all kernel nodes.</span>
<span class="p_add">+	 * Now go over all reserved memblock regions, to construct a</span>
<span class="p_add">+	 * node mask of all kernel reserved memory areas.</span>
 	 *
<span class="p_del">-	 * When booting with mem=nn[kMG] or in a kdump kernel, numa_meminfo</span>
<span class="p_del">-	 * may not include all the memblock.reserved memory ranges because</span>
<span class="p_del">-	 * trim_snb_memory() reserves specific pages for Sandy Bridge graphics.</span>
<span class="p_add">+	 * [ Note, when booting with mem=nn[kMG] or in a kdump kernel,</span>
<span class="p_add">+	 *   numa_meminfo might not include all memblock.reserved</span>
<span class="p_add">+	 *   memory ranges, because quirks such as trim_snb_memory()</span>
<span class="p_add">+	 *   reserve specific pages for Sandy Bridge graphics. ]</span>
 	 */
<span class="p_del">-	for_each_memblock(reserved, r)</span>
<span class="p_del">-		if (r-&gt;nid != MAX_NUMNODES)</span>
<span class="p_del">-			node_set(r-&gt;nid, numa_kernel_nodes);</span>
<span class="p_add">+	for_each_memblock(reserved, mb_region) {</span>
<span class="p_add">+		if (mb_region-&gt;nid != MAX_NUMNODES)</span>
<span class="p_add">+			node_set(mb_region-&gt;nid, reserved_nodemask);</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	/* Clear MEMBLOCK_HOTPLUG flag for memory in kernel nodes. */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Finally, clear the MEMBLOCK_HOTPLUG flag for all memory</span>
<span class="p_add">+	 * belonging to the reserved node mask.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that this will include memory regions that reside</span>
<span class="p_add">+	 * on nodes that contain kernel memory - entire nodes</span>
<span class="p_add">+	 * become hot-unpluggable:</span>
<span class="p_add">+	 */</span>
 	for (i = 0; i &lt; numa_meminfo.nr_blks; i++) {
<span class="p_del">-		nid = numa_meminfo.blk[i].nid;</span>
<span class="p_del">-		if (!node_isset(nid, numa_kernel_nodes))</span>
<span class="p_del">-			continue;</span>
<span class="p_add">+		struct numa_memblk *mb = numa_meminfo.blk + i;</span>
 
<span class="p_del">-		start = numa_meminfo.blk[i].start;</span>
<span class="p_del">-		end = numa_meminfo.blk[i].end;</span>
<span class="p_add">+		if (!node_isset(mb-&gt;nid, reserved_nodemask))</span>
<span class="p_add">+			continue;</span>
 
<span class="p_del">-		memblock_clear_hotplug(start, end - start);</span>
<span class="p_add">+		memblock_clear_hotplug(mb-&gt;start, mb-&gt;end - mb-&gt;start);</span>
 	}
 }
 
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index 2440814b0069..3dd6afd2c0e5 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -1122,8 +1122,10 @@</span> <span class="p_context"> static int __cpa_process_fault(struct cpa_data *cpa, unsigned long vaddr,</span>
 	/*
 	 * Ignore all non primary paths.
 	 */
<span class="p_del">-	if (!primary)</span>
<span class="p_add">+	if (!primary) {</span>
<span class="p_add">+		cpa-&gt;numpages = 1;</span>
 		return 0;
<span class="p_add">+	}</span>
 
 	/*
 	 * Ignore the NULL PTE for kernel identity mapping, as it is expected
<span class="p_header">diff --git a/arch/x86/mm/setup_nx.c b/arch/x86/mm/setup_nx.c</span>
<span class="p_header">index 92e2eacb3321..78f5d5907f98 100644</span>
<span class="p_header">--- a/arch/x86/mm/setup_nx.c</span>
<span class="p_header">+++ b/arch/x86/mm/setup_nx.c</span>
<span class="p_chunk">@@ -31,9 +31,8 @@</span> <span class="p_context"> early_param(&quot;noexec&quot;, noexec_setup);</span>
 
 void x86_configure_nx(void)
 {
<span class="p_del">-	if (boot_cpu_has(X86_FEATURE_NX) &amp;&amp; !disable_nx)</span>
<span class="p_del">-		__supported_pte_mask |= _PAGE_NX;</span>
<span class="p_del">-	else</span>
<span class="p_add">+	/* If disable_nx is set, clear NX on all new mappings going forward. */</span>
<span class="p_add">+	if (disable_nx)</span>
 		__supported_pte_mask &amp;= ~_PAGE_NX;
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



