
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4.1,42/65] kaiser: enhanced by kernel and user PCIDs - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4.1,42/65] kaiser: enhanced by kernel and user PCIDs</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=173041">Pavel Tatashin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 6, 2018, 12:25 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180306002538.1761-43-pasha.tatashin@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10260379/mbox/"
   >mbox</a>
|
   <a href="/patch/10260379/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10260379/">/patch/10260379/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	8DE3160365 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Mar 2018 00:33:50 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7F49528CB3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Mar 2018 00:33:50 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 72F7528D22; Tue,  6 Mar 2018 00:33:50 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, RCVD_IN_DNSWL_HI,
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 37D4928CB3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Mar 2018 00:33:49 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933760AbeCFAd3 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 5 Mar 2018 19:33:29 -0500
Received: from userp2130.oracle.com ([156.151.31.86]:40852 &quot;EHLO
	userp2130.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S933340AbeCFA0m (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 5 Mar 2018 19:26:42 -0500
Received: from pps.filterd (userp2130.oracle.com [127.0.0.1])
	by userp2130.oracle.com (8.16.0.22/8.16.0.22) with SMTP id
	w260LTpa187154; Tue, 6 Mar 2018 00:26:38 GMT
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=oracle.com;
	h=from : to : subject :
	date : message-id : in-reply-to : references; s=corp-2017-10-26;
	bh=miKudcJsOFUcuBNsFt0BiUZgcrfFIjlJ98w9mhT8j6s=;
	b=Qb3GppGP66LskpxJ7FBbduTqX+q7xkHrcoeMOf51q+JbCyXMkBH0iabcCJlHSTw1x1Zl
	FNAWOGEFoE3/o6f0hM+UmjcsFixW7qAj58QwqOm6eT9tYHbUd6YUwFrdeD7abeCA3lp+
	RMzbFV/GxkWgG/j978YYtE2U2naLJaoAb7pRJB8j8lmP+IIkFGzHXJDXTSo3ERxwtUXz
	WJ0FbL2T98PXPivTnTmKDANEaXWlSrbrjPbai4fz+Cr44AFz1clAAQD+08+UJhGycUMk
	9niKh9LoD1OeZ5ksTz5C7turWOecjjbshpPXAgvqO+0MFE+8N+7PzHVZgjrzrnHspQZ7
	+w== 
Received: from aserv0022.oracle.com (aserv0022.oracle.com [141.146.126.234])
	by userp2130.oracle.com with ESMTP id 2ghdxf8jrm-1
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Tue, 06 Mar 2018 00:26:38 +0000
Received: from userv0122.oracle.com (userv0122.oracle.com [156.151.31.75])
	by aserv0022.oracle.com (8.14.4/8.14.4) with ESMTP id w260Qbwp030919
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=FAIL); Tue, 6 Mar 2018 00:26:37 GMT
Received: from abhmp0008.oracle.com (abhmp0008.oracle.com [141.146.116.14])
	by userv0122.oracle.com (8.14.4/8.14.4) with ESMTP id
	w260Qa6C025222; Tue, 6 Mar 2018 00:26:36 GMT
Received: from localhost.localdomain (/98.216.35.41)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Mon, 05 Mar 2018 16:26:36 -0800
From: Pavel Tatashin &lt;pasha.tatashin@oracle.com&gt;
To: steven.sistare@oracle.com, daniel.m.jordan@oracle.com,
	linux-kernel@vger.kernel.org, Alexander.Levin@microsoft.com,
	dan.j.williams@intel.com, sathyanarayanan.kuppuswamy@intel.com,
	pankaj.laxminarayan.bharadiya@intel.com, akuster@mvista.com,
	cminyard@mvista.com, pasha.tatashin@oracle.com,
	gregkh@linuxfoundation.org, stable@vger.kernel.org
Subject: [PATCH 4.1 42/65] kaiser: enhanced by kernel and user PCIDs
Date: Mon,  5 Mar 2018 19:25:15 -0500
Message-Id: &lt;20180306002538.1761-43-pasha.tatashin@oracle.com&gt;
X-Mailer: git-send-email 2.16.2
In-Reply-To: &lt;20180306002538.1761-1-pasha.tatashin@oracle.com&gt;
References: &lt;20180306002538.1761-1-pasha.tatashin@oracle.com&gt;
X-Proofpoint-Virus-Version: vendor=nai engine=5900 definitions=8823
	signatures=668683
X-Proofpoint-Spam-Details: rule=notspam policy=default score=0
	suspectscore=0 malwarescore=0
	phishscore=0 bulkscore=0 spamscore=0 mlxscore=0 mlxlogscore=999
	adultscore=0 classifier=spam adjust=0 reason=mlx scancount=1
	engine=8.0.1-1711220000 definitions=main-1803060003
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173041">Pavel Tatashin</a> - March 6, 2018, 12:25 a.m.</div>
<pre class="content">
<span class="from">From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>

Merged performance improvements to Kaiser, using distinct kernel
and user Process Context Identifiers to minimize the TLB flushing.
<span class="signed-off-by">
Signed-off-by: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="acked-by">Acked-by: Jiri Kosina &lt;jkosina@suse.cz&gt;</span>
<span class="signed-off-by">Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;</span>
(cherry picked from commit eb82151d0b1df53d1ad8d060ecd554ca12eb552a)
<span class="signed-off-by">Signed-off-by: Pavel Tatashin &lt;pasha.tatashin@oracle.com&gt;</span>

Conflicts:
	arch/x86/entry/entry_64.S (not in this tree)
	arch/x86/kernel/entry_64.S (patched instead of that)
	arch/x86/entry/entry_64_compat.S (not in this tree)
	arch/x86/ia32/ia32entry.S (patched instead of that)
	arch/x86/include/asm/tlbflush.h
---
 arch/x86/ia32/ia32entry.S                   |  1 +
 arch/x86/include/asm/cpufeature.h           |  1 +
 arch/x86/include/asm/kaiser.h               | 15 ++++++++--
 arch/x86/include/asm/pgtable_types.h        | 26 ++++++++++++++++
 arch/x86/include/asm/tlbflush.h             | 37 +++++++++++++++++++++--
 arch/x86/include/uapi/asm/processor-flags.h |  3 +-
 arch/x86/kernel/cpu/common.c                | 34 +++++++++++++++++++++
 arch/x86/kernel/entry_64.S                  | 10 +++++--
 arch/x86/kvm/x86.c                          |  3 +-
 arch/x86/mm/kaiser.c                        |  7 +++++
 arch/x86/mm/tlb.c                           | 46 +++++++++++++++++++++++++++--
 11 files changed, 172 insertions(+), 11 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/ia32/ia32entry.S b/arch/x86/ia32/ia32entry.S</span>
<span class="p_header">index 665e2c7887fe..070470d161f6 100644</span>
<span class="p_header">--- a/arch/x86/ia32/ia32entry.S</span>
<span class="p_header">+++ b/arch/x86/ia32/ia32entry.S</span>
<span class="p_chunk">@@ -15,6 +15,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/irqflags.h&gt;
 #include &lt;asm/asm.h&gt;
 #include &lt;asm/smap.h&gt;
<span class="p_add">+#include &lt;asm/pgtable_types.h&gt;</span>
 #include &lt;asm/kaiser.h&gt;
 #include &lt;linux/linkage.h&gt;
 #include &lt;linux/err.h&gt;
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">index 3d6606fb97d0..80a2b9487b29 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_chunk">@@ -185,6 +185,7 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_ARAT	( 7*32+ 1) /* Always Running APIC Timer */
 #define X86_FEATURE_CPB		( 7*32+ 2) /* AMD Core Performance Boost */
 #define X86_FEATURE_EPB		( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */
<span class="p_add">+#define X86_FEATURE_INVPCID_SINGLE ( 7*32+ 4) /* Effectively INVPCID &amp;&amp; CR4.PCIDE=1 */</span>
 #define X86_FEATURE_PLN		( 7*32+ 5) /* Intel Power Limit Notification */
 #define X86_FEATURE_PTS		( 7*32+ 6) /* Intel Package Thermal Status */
 #define X86_FEATURE_DTHERM	( 7*32+ 7) /* Digital Thermal Sensor */
<span class="p_header">diff --git a/arch/x86/include/asm/kaiser.h b/arch/x86/include/asm/kaiser.h</span>
<span class="p_header">index e0fc45e77aee..360ff3bc44a9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/kaiser.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/kaiser.h</span>
<span class="p_chunk">@@ -1,5 +1,8 @@</span> <span class="p_context"></span>
 #ifndef _ASM_X86_KAISER_H
 #define _ASM_X86_KAISER_H
<span class="p_add">+</span>
<span class="p_add">+#include &lt;uapi/asm/processor-flags.h&gt; /* For PCID constants */</span>
<span class="p_add">+</span>
 /*
  * This file includes the definitions for the KAISER feature.
  * KAISER is a counter measure against x86_64 side channel attacks on
<span class="p_chunk">@@ -21,13 +24,21 @@</span> <span class="p_context"></span>
 
 .macro _SWITCH_TO_KERNEL_CR3 reg
 movq %cr3, \reg
<span class="p_del">-andq $(~KAISER_SHADOW_PGD_OFFSET), \reg</span>
<span class="p_add">+andq $(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), \reg</span>
<span class="p_add">+orq  X86_CR3_PCID_KERN_VAR, \reg</span>
 movq \reg, %cr3
 .endm
 
 .macro _SWITCH_TO_USER_CR3 reg
 movq %cr3, \reg
<span class="p_del">-orq $(KAISER_SHADOW_PGD_OFFSET), \reg</span>
<span class="p_add">+andq $(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), \reg</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This can obviously be one instruction by putting the</span>
<span class="p_add">+ * KAISER_SHADOW_PGD_OFFSET bit in the X86_CR3_PCID_USER_VAR.</span>
<span class="p_add">+ * But, just leave it now for simplicity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+orq  X86_CR3_PCID_USER_VAR, \reg</span>
<span class="p_add">+orq  $(KAISER_SHADOW_PGD_OFFSET), \reg</span>
 movq \reg, %cr3
 .endm
 
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">index 7a5fc4410c00..b3c77e6529c2 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_chunk">@@ -106,6 +106,32 @@</span> <span class="p_context"></span>
 			 _PAGE_SOFT_DIRTY)
 #define _HPAGE_CHG_MASK (_PAGE_CHG_MASK | _PAGE_PSE)
 
<span class="p_add">+/* The ASID is the lower 12 bits of CR3 */</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_MASK  (_AC((1&lt;&lt;12)-1,UL))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Mask for all the PCID-related bits in CR3: */</span>
<span class="p_add">+#define X86_CR3_PCID_MASK       (X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_MASK)</span>
<span class="p_add">+#if defined(CONFIG_KAISER) &amp;&amp; defined(CONFIG_X86_64)</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_KERN  (_AC(0x4,UL))</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_USER  (_AC(0x6,UL))</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_FLUSH		(X86_CR3_PCID_ASID_KERN)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_FLUSH		(X86_CR3_PCID_ASID_USER)</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_NOFLUSH	(X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_KERN)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_NOFLUSH	(X86_CR3_PCID_NOFLUSH | X86_CR3_PCID_ASID_USER)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_KERN  (_AC(0x0,UL))</span>
<span class="p_add">+#define X86_CR3_PCID_ASID_USER  (_AC(0x0,UL))</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PCIDs are unsupported on 32-bit and none of these bits can be</span>
<span class="p_add">+ * set in CR3:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_FLUSH		(0)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_FLUSH		(0)</span>
<span class="p_add">+#define X86_CR3_PCID_KERN_NOFLUSH	(0)</span>
<span class="p_add">+#define X86_CR3_PCID_USER_NOFLUSH	(0)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * The cache modes defined here are used to translate between pure SW usage
  * and the HW defined cache mode bits and/or PAT entries.
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 09a70d8d293e..b558d1996621 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -12,7 +12,6 @@</span> <span class="p_context"> static inline void __invpcid(unsigned long pcid, unsigned long addr,</span>
 			     unsigned long type)
 {
 	struct { u64 d[2]; } desc = { { pcid, addr } };
<span class="p_del">-</span>
 	/*
 	 * The memory clobber is because the whole point is to invalidate
 	 * stale TLB entries and, especially if we&#39;re flushing global
<span class="p_chunk">@@ -134,6 +133,14 @@</span> <span class="p_context"> static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
 static inline void __native_flush_tlb(void)
 {
 	native_write_cr3(native_read_cr3());
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We are no longer using globals with KAISER, so a</span>
<span class="p_add">+	 * &quot;nonglobals&quot; flush would work too. But, this is more</span>
<span class="p_add">+	 * conservative.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note, this works with CR4.PCIDE=0 or 1.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	invpcid_flush_all();</span>
 }
 
 static inline void __native_flush_tlb_global_irq_disabled(void)
<span class="p_chunk">@@ -155,6 +162,8 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
 		/*
 		 * Using INVPCID is considerably faster than a pair of writes
 		 * to CR4 sandwiched inside an IRQ flag save/restore.
<span class="p_add">+		 *</span>
<span class="p_add">+	 	 * Note, this works with CR4.PCIDE=0 or 1.</span>
 		 */
 		invpcid_flush_all();
 		return;
<span class="p_chunk">@@ -174,7 +183,31 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
 
 static inline void __native_flush_tlb_single(unsigned long addr)
 {
<span class="p_del">-	asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * SIMICS #GP&#39;s if you run INVPCID with type 2/3</span>
<span class="p_add">+	 * and X86_CR4_PCIDE clear.  Shame!</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The ASIDs used below are hard-coded.  But, we must not</span>
<span class="p_add">+	 * call invpcid(type=1/2) before CR4.PCIDE=1.  Just call</span>
<span class="p_add">+	 * invpcid in the case we are called early.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!this_cpu_has(X86_FEATURE_INVPCID_SINGLE)) {</span>
<span class="p_add">+		asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/* Flush the address out of both PCIDs. */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * An optimization here might be to determine addresses</span>
<span class="p_add">+	 * that are only kernel-mapped and only flush the kernel</span>
<span class="p_add">+	 * ASID.  But, userspace flushes are probably much more</span>
<span class="p_add">+	 * important performance-wise.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Make sure to do only a single invpcid when KAISER is</span>
<span class="p_add">+	 * disabled and we have only a single ASID.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (X86_CR3_PCID_ASID_KERN != X86_CR3_PCID_ASID_USER)</span>
<span class="p_add">+		invpcid_flush_one(X86_CR3_PCID_ASID_KERN, addr);</span>
<span class="p_add">+	invpcid_flush_one(X86_CR3_PCID_ASID_USER, addr);</span>
 }
 
 static inline void __flush_tlb_all(void)
<span class="p_header">diff --git a/arch/x86/include/uapi/asm/processor-flags.h b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">index 180a0c3c224d..bd4513b7b877 100644</span>
<span class="p_header">--- a/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_chunk">@@ -79,7 +79,8 @@</span> <span class="p_context"></span>
 #define X86_CR3_PWT		_BITUL(X86_CR3_PWT_BIT)
 #define X86_CR3_PCD_BIT		4 /* Page Cache Disable */
 #define X86_CR3_PCD		_BITUL(X86_CR3_PCD_BIT)
<span class="p_del">-#define X86_CR3_PCID_MASK	_AC(0x00000fff,UL) /* PCID Mask */</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH_BIT 63 /* Preserve old PCID */</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH    _BITULL(X86_CR3_PCID_NOFLUSH_BIT)</span>
 
 /*
  * Intel CPU features in CR4
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 47194fac7eb7..6fb0d332f440 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -339,11 +339,45 @@</span> <span class="p_context"> static __always_inline void setup_smap(struct cpuinfo_x86 *c)</span>
 	}
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * These can have bit 63 set, so we can not just use a plain &quot;or&quot;</span>
<span class="p_add">+ * instruction to get their value or&#39;d into CR3.  It would take</span>
<span class="p_add">+ * another register.  So, we use a memory reference to these</span>
<span class="p_add">+ * instead.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is also handy because systems that do not support</span>
<span class="p_add">+ * PCIDs just end up or&#39;ing a 0 into their CR3, which does</span>
<span class="p_add">+ * no harm.</span>
<span class="p_add">+ */</span>
<span class="p_add">+__aligned(PAGE_SIZE) unsigned long X86_CR3_PCID_KERN_VAR = 0;</span>
<span class="p_add">+__aligned(PAGE_SIZE) unsigned long X86_CR3_PCID_USER_VAR = 0;</span>
<span class="p_add">+</span>
 static void setup_pcid(struct cpuinfo_x86 *c)
 {
 	if (cpu_has(c, X86_FEATURE_PCID)) {
 		if (cpu_has(c, X86_FEATURE_PGE)) {
 			cr4_set_bits(X86_CR4_PCIDE);
<span class="p_add">+			/*</span>
<span class="p_add">+			 * These variables are used by the entry/exit</span>
<span class="p_add">+			 * code to change PCIDs.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+#ifdef CONFIG_KAISER</span>
<span class="p_add">+			X86_CR3_PCID_KERN_VAR = X86_CR3_PCID_KERN_NOFLUSH;</span>
<span class="p_add">+			X86_CR3_PCID_USER_VAR = X86_CR3_PCID_USER_NOFLUSH;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * INVPCID has two &quot;groups&quot; of types:</span>
<span class="p_add">+			 * 1/2: Invalidate an individual address</span>
<span class="p_add">+			 * 3/4: Invalidate all contexts</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * 1/2 take a PCID, but 3/4 do not.  So, 3/4</span>
<span class="p_add">+			 * ignore the PCID argument in the descriptor.</span>
<span class="p_add">+			 * But, we have to be careful not to call 1/2</span>
<span class="p_add">+			 * with an actual non-zero PCID in them before</span>
<span class="p_add">+			 * we do the above cr4_set_bits().</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (cpu_has(c, X86_FEATURE_INVPCID))</span>
<span class="p_add">+				set_cpu_cap(c, X86_FEATURE_INVPCID_SINGLE);</span>
 		} else {
 			/*
 			 * flush_tlb_all(), as currently implemented, won&#39;t
<span class="p_header">diff --git a/arch/x86/kernel/entry_64.S b/arch/x86/kernel/entry_64.S</span>
<span class="p_header">index d2b2372e01c6..0f7bba928264 100644</span>
<span class="p_header">--- a/arch/x86/kernel/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/entry_64.S</span>
<span class="p_chunk">@@ -1541,7 +1541,10 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 	/* %rax is saved above, so OK to clobber here */
 	movq	%cr3, %rax
 	pushq	%rax
<span class="p_del">-	andq	$(~KAISER_SHADOW_PGD_OFFSET), %rax</span>
<span class="p_add">+	/* mask off &quot;user&quot; bit of pgd address and 12 PCID bits: */</span>
<span class="p_add">+	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax</span>
<span class="p_add">+	/* Add back kernel PCID and &quot;no flush&quot; bit */</span>
<span class="p_add">+	orq	X86_CR3_PCID_KERN_VAR, %rax</span>
 	movq	%rax, %cr3
 #endif
 	call	do_nmi
<span class="p_chunk">@@ -1777,7 +1780,10 @@</span> <span class="p_context"> end_repeat_nmi:</span>
 	/* %rax is saved above, so OK to clobber here */
 	movq	%cr3, %rax
 	pushq	%rax
<span class="p_del">-	andq	$(~KAISER_SHADOW_PGD_OFFSET), %rax</span>
<span class="p_add">+	/* mask off &quot;user&quot; bit of pgd address and 12 PCID bits: */</span>
<span class="p_add">+	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax</span>
<span class="p_add">+	/* Add back kernel PCID and &quot;no flush&quot; bit */</span>
<span class="p_add">+	orq	X86_CR3_PCID_KERN_VAR, %rax</span>
 	movq	%rax, %cr3
 #endif
 	DEFAULT_FRAME 0				/* XXX: Do we need this? */
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index e4e7d45fd551..153afbb6c080 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -733,7 +733,8 @@</span> <span class="p_context"> int kvm_set_cr4(struct kvm_vcpu *vcpu, unsigned long cr4)</span>
 			return 1;
 
 		/* PCID can not be enabled when cr3[11:0]!=000H or EFER.LMA=0 */
<span class="p_del">-		if ((kvm_read_cr3(vcpu) &amp; X86_CR3_PCID_MASK) || !is_long_mode(vcpu))</span>
<span class="p_add">+		if ((kvm_read_cr3(vcpu) &amp; X86_CR3_PCID_ASID_MASK) ||</span>
<span class="p_add">+		    !is_long_mode(vcpu))</span>
 			return 1;
 	}
 
<span class="p_header">diff --git a/arch/x86/mm/kaiser.c b/arch/x86/mm/kaiser.c</span>
<span class="p_header">index 50d650799f39..91968328ccdf 100644</span>
<span class="p_header">--- a/arch/x86/mm/kaiser.c</span>
<span class="p_header">+++ b/arch/x86/mm/kaiser.c</span>
<span class="p_chunk">@@ -240,6 +240,8 @@</span> <span class="p_context"> static void __init kaiser_init_all_pgds(void)</span>
 } while (0)
 
 extern char __per_cpu_user_mapped_start[], __per_cpu_user_mapped_end[];
<span class="p_add">+extern unsigned long X86_CR3_PCID_KERN_VAR;</span>
<span class="p_add">+extern unsigned long X86_CR3_PCID_USER_VAR;</span>
 /*
  * If anything in here fails, we will likely die on one of the
  * first kernel-&gt;user transitions and init will die.  But, we
<span class="p_chunk">@@ -290,6 +292,11 @@</span> <span class="p_context"> void __init kaiser_init(void)</span>
 	kaiser_add_user_map_early(&amp;debug_idt_table,
 				  sizeof(gate_desc) * NR_VECTORS,
 				  __PAGE_KERNEL);
<span class="p_add">+</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;X86_CR3_PCID_KERN_VAR, PAGE_SIZE,</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
<span class="p_add">+	kaiser_add_user_map_early(&amp;X86_CR3_PCID_USER_VAR, PAGE_SIZE,</span>
<span class="p_add">+				  __PAGE_KERNEL);</span>
 }
 
 /* Add a mapping to the shadow mapping, and synchronize the mappings */
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 95624d903c0e..7176496112ff 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -34,6 +34,46 @@</span> <span class="p_context"> struct flush_tlb_info {</span>
 	unsigned long flush_end;
 };
 
<span class="p_add">+static void load_new_mm_cr3(pgd_t *pgdir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long new_mm_cr3 = __pa(pgdir);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * KAISER, plus PCIDs needs some extra work here.  But,</span>
<span class="p_add">+	 * if either of features is not present, we need no</span>
<span class="p_add">+	 * PCIDs here and just do a normal, full TLB flush with</span>
<span class="p_add">+	 * the write_cr3()</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_KAISER) ||</span>
<span class="p_add">+	    !cpu_feature_enabled(X86_FEATURE_PCID))</span>
<span class="p_add">+		goto out_set_cr3;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We reuse the same PCID for different tasks, so we must</span>
<span class="p_add">+	 * flush all the entires for the PCID out when we change</span>
<span class="p_add">+	 * tasks.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	new_mm_cr3 = X86_CR3_PCID_KERN_FLUSH | __pa(pgdir);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The flush from load_cr3() may leave old TLB entries</span>
<span class="p_add">+	 * for userspace in place.  We must flush that context</span>
<span class="p_add">+	 * separately.  We can theoretically delay doing this</span>
<span class="p_add">+	 * until we actually load up the userspace CR3, but</span>
<span class="p_add">+	 * that&#39;s a bit tricky.  We have to have the &quot;need to</span>
<span class="p_add">+	 * flush userspace PCID&quot; bit per-cpu and check it in the</span>
<span class="p_add">+	 * exit-to-userspace paths.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	invpcid_flush_single_context(X86_CR3_PCID_ASID_USER);</span>
<span class="p_add">+</span>
<span class="p_add">+out_set_cr3:</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Caution: many callers of this function expect</span>
<span class="p_add">+	 * that load_cr3() is serializing and orders TLB</span>
<span class="p_add">+	 * fills with respect to the mm_cpumask writes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	write_cr3(new_mm_cr3);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * We cannot call mmdrop() because we are in interrupt context,
  * instead update mm-&gt;cpu_vm_mask.
<span class="p_chunk">@@ -45,7 +85,7 @@</span> <span class="p_context"> void leave_mm(int cpu)</span>
 		BUG();
 	if (cpumask_test_cpu(cpu, mm_cpumask(active_mm))) {
 		cpumask_clear_cpu(cpu, mm_cpumask(active_mm));
<span class="p_del">-		load_cr3(swapper_pg_dir);</span>
<span class="p_add">+		load_new_mm_cr3(swapper_pg_dir);</span>
 		/*
 		 * This gets called in the idle path where RCU
 		 * functions differently.  Tracing normally
<span class="p_chunk">@@ -105,7 +145,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		 * ordering guarantee we need.
 		 *
 		 */
<span class="p_del">-		load_cr3(next-&gt;pgd);</span>
<span class="p_add">+		load_new_mm_cr3(next-&gt;pgd);</span>
 
 		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 
<span class="p_chunk">@@ -150,7 +190,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 			 * As above, load_cr3() is serializing and orders TLB
 			 * fills with respect to the mm_cpumask write.
 			 */
<span class="p_del">-			load_cr3(next-&gt;pgd);</span>
<span class="p_add">+			load_new_mm_cr3(next-&gt;pgd);</span>
 			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 			load_mm_cr4(next);
 			load_mm_ldt(next);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



