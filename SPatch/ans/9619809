
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[21/26] x86/mm: add support of additional page table level during early boot - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [21/26] x86/mm: add support of additional page table level during early boot</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 13, 2017, 5:50 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170313055020.69655-22-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9619809/mbox/"
   >mbox</a>
|
   <a href="/patch/9619809/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9619809/">/patch/9619809/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7C48E60244 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Mar 2017 05:58:18 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6067C27A98
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Mar 2017 05:58:18 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 54D1428451; Mon, 13 Mar 2017 05:58:18 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E836E27A98
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Mar 2017 05:58:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751422AbdCMF6P (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 13 Mar 2017 01:58:15 -0400
Received: from mga03.intel.com ([134.134.136.65]:47442 &quot;EHLO mga03.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751122AbdCMFvr (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 13 Mar 2017 01:51:47 -0400
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
	by orsmga103.jf.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	12 Mar 2017 22:50:44 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.36,157,1486454400&quot;; d=&quot;scan&#39;208&quot;;a=&quot;1121727306&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by fmsmga001.fm.intel.com with ESMTP; 12 Mar 2017 22:50:40 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 99D2E48B; Mon, 13 Mar 2017 07:50:23 +0200 (EET)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, x86@kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;, linux-arch@vger.kernel.org,
	linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCH 21/26] x86/mm: add support of additional page table level
	during early boot
Date: Mon, 13 Mar 2017 08:50:15 +0300
Message-Id: &lt;20170313055020.69655-22-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170313055020.69655-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20170313055020.69655-1-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - March 13, 2017, 5:50 a.m.</div>
<pre class="content">
This patch adds support for 5-level paging during early boot.
It generalizes boot for 4- and 5-level paging on 64-bit systems with
compile-time switch between them.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
---
 arch/x86/boot/compressed/head_64.S          | 23 +++++++++--
 arch/x86/include/asm/pgtable.h              |  2 +-
 arch/x86/include/asm/pgtable_64.h           |  6 ++-
 arch/x86/include/uapi/asm/processor-flags.h |  2 +
 arch/x86/kernel/espfix_64.c                 |  2 +-
 arch/x86/kernel/head64.c                    | 40 +++++++++++++-----
 arch/x86/kernel/head_64.S                   | 63 +++++++++++++++++++++--------
 arch/x86/kernel/machine_kexec_64.c          |  2 +-
 arch/x86/mm/dump_pagetables.c               |  2 +-
 arch/x86/mm/kasan_init_64.c                 | 12 +++---
 arch/x86/realmode/init.c                    |  2 +-
 arch/x86/xen/mmu.c                          | 38 ++++++++++-------
 arch/x86/xen/xen-pvh.S                      |  2 +-
 13 files changed, 136 insertions(+), 60 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - March 13, 2017, 7:18 a.m.</div>
<pre class="content">
* Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt; wrote:
<span class="quote">
&gt; This patch adds support for 5-level paging during early boot.</span>
<span class="quote">&gt; It generalizes boot for 4- and 5-level paging on 64-bit systems with</span>
<span class="quote">&gt; compile-time switch between them.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/boot/compressed/head_64.S          | 23 +++++++++--</span>
<span class="quote">&gt;  arch/x86/include/asm/pgtable.h              |  2 +-</span>
<span class="quote">&gt;  arch/x86/include/asm/pgtable_64.h           |  6 ++-</span>
<span class="quote">&gt;  arch/x86/include/uapi/asm/processor-flags.h |  2 +</span>
<span class="quote">&gt;  arch/x86/kernel/espfix_64.c                 |  2 +-</span>
<span class="quote">&gt;  arch/x86/kernel/head64.c                    | 40 +++++++++++++-----</span>
<span class="quote">&gt;  arch/x86/kernel/head_64.S                   | 63 +++++++++++++++++++++--------</span>

Ok, here I&#39;d like to have a C version instead of further complicating an already 
complex assembly version...

I.e. the existing setup code should be converted to C in one patch, and then 
another patch should add 5-level paging support to the C code.

See how this was done for the 32-bit setup code already:

  5a7670ee23f2 x86/boot/32: Convert the 32-bit pgtable setup code from assembly to C

Also, please split it up into per boot path and topic, i.e. have a sparate patch 
for the Xen bits, the KASAN bits, the kexec extension, etc.

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - April 5, 2017, 11:36 a.m.</div>
<pre class="content">
On Mon, Mar 13, 2017 at 08:18:10AM +0100, Ingo Molnar wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; * Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; This patch adds support for 5-level paging during early boot.</span>
<span class="quote">&gt; &gt; It generalizes boot for 4- and 5-level paging on 64-bit systems with</span>
<span class="quote">&gt; &gt; compile-time switch between them.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  arch/x86/boot/compressed/head_64.S          | 23 +++++++++--</span>
<span class="quote">&gt; &gt;  arch/x86/include/asm/pgtable.h              |  2 +-</span>
<span class="quote">&gt; &gt;  arch/x86/include/asm/pgtable_64.h           |  6 ++-</span>
<span class="quote">&gt; &gt;  arch/x86/include/uapi/asm/processor-flags.h |  2 +</span>
<span class="quote">&gt; &gt;  arch/x86/kernel/espfix_64.c                 |  2 +-</span>
<span class="quote">&gt; &gt;  arch/x86/kernel/head64.c                    | 40 +++++++++++++-----</span>
<span class="quote">&gt; &gt;  arch/x86/kernel/head_64.S                   | 63 +++++++++++++++++++++--------</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, here I&#39;d like to have a C version instead of further complicating an already </span>
<span class="quote">&gt; complex assembly version...</span>

Just head up: I work on this.

It&#39;s great deal of frustration (I can&#39;t really read assembly), but I&#39;m
slowly moving forward.

Most of logic in startup_64 in arch/x86/kernel/head_64.S is converted
to C. Dealing with secondary_startup_64 now.

Not sure if it&#39;s possible to convert code in
arch/x86/boot/compressed/head_64.S to C.

Assembly code there is in 32-bit mode, but if we move it to C it will
compiled as 64-bit. I&#39;ve tried to put it in separate translation unit and
compile with -m32, but then link phase breaks as object files have
different types.

Any suggestion how I can get out of the situation?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - April 5, 2017, 3:32 p.m.</div>
<pre class="content">
On Wed, Apr 05, 2017 at 02:36:24PM +0300, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Mon, Mar 13, 2017 at 08:18:10AM +0100, Ingo Molnar wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; * Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt; wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This patch adds support for 5-level paging during early boot.</span>
<span class="quote">&gt; &gt; &gt; It generalizes boot for 4- and 5-level paging on 64-bit systems with</span>
<span class="quote">&gt; &gt; &gt; compile-time switch between them.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt; &gt; ---</span>
<span class="quote">&gt; &gt; &gt;  arch/x86/boot/compressed/head_64.S          | 23 +++++++++--</span>
<span class="quote">&gt; &gt; &gt;  arch/x86/include/asm/pgtable.h              |  2 +-</span>
<span class="quote">&gt; &gt; &gt;  arch/x86/include/asm/pgtable_64.h           |  6 ++-</span>
<span class="quote">&gt; &gt; &gt;  arch/x86/include/uapi/asm/processor-flags.h |  2 +</span>
<span class="quote">&gt; &gt; &gt;  arch/x86/kernel/espfix_64.c                 |  2 +-</span>
<span class="quote">&gt; &gt; &gt;  arch/x86/kernel/head64.c                    | 40 +++++++++++++-----</span>
<span class="quote">&gt; &gt; &gt;  arch/x86/kernel/head_64.S                   | 63 +++++++++++++++++++++--------</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Ok, here I&#39;d like to have a C version instead of further complicating an already </span>
<span class="quote">&gt; &gt; complex assembly version...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just head up: I work on this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s great deal of frustration (I can&#39;t really read assembly), but I&#39;m</span>
<span class="quote">&gt; slowly moving forward.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Most of logic in startup_64 in arch/x86/kernel/head_64.S is converted</span>
<span class="quote">&gt; to C. Dealing with secondary_startup_64 now.</span>

I&#39;m stuck with secondary_startup_64 too. Stack breaks as soon as we switch
to new page tables when onlining secondary CPUs. I don&#39;t know how to get
around this.

Would it be sufficient if I only convert startup_64 in
arch/x86/kernel/head_64.S to C?
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/boot/compressed/head_64.S b/arch/x86/boot/compressed/head_64.S</span>
<span class="p_header">index d2ae1f821e0c..3ed26769810b 100644</span>
<span class="p_header">--- a/arch/x86/boot/compressed/head_64.S</span>
<span class="p_header">+++ b/arch/x86/boot/compressed/head_64.S</span>
<span class="p_chunk">@@ -122,9 +122,12 @@</span> <span class="p_context"> ENTRY(startup_32)</span>
 	addl	%ebp, gdt+2(%ebp)
 	lgdt	gdt(%ebp)
 
<span class="p_del">-	/* Enable PAE mode */</span>
<span class="p_add">+	/* Enable PAE and LA57 mode */</span>
 	movl	%cr4, %eax
 	orl	$X86_CR4_PAE, %eax
<span class="p_add">+#ifdef CONFIG_X86_5LEVEL</span>
<span class="p_add">+	orl	$X86_CR4_LA57, %eax</span>
<span class="p_add">+#endif</span>
 	movl	%eax, %cr4
 
  /*
<span class="p_chunk">@@ -136,13 +139,24 @@</span> <span class="p_context"> ENTRY(startup_32)</span>
 	movl	$(BOOT_INIT_PGT_SIZE/4), %ecx
 	rep	stosl
 
<span class="p_add">+	xorl	%edx, %edx</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Build Top Level */</span>
<span class="p_add">+	leal	pgtable(%ebx,%edx,1), %edi</span>
<span class="p_add">+	leal	0x1007 (%edi), %eax</span>
<span class="p_add">+	movl	%eax, 0(%edi)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_5LEVEL</span>
 	/* Build Level 4 */
<span class="p_del">-	leal	pgtable + 0(%ebx), %edi</span>
<span class="p_add">+	addl	$0x1000, %edx</span>
<span class="p_add">+	leal	pgtable(%ebx,%edx), %edi</span>
 	leal	0x1007 (%edi), %eax
 	movl	%eax, 0(%edi)
<span class="p_add">+#endif</span>
 
 	/* Build Level 3 */
<span class="p_del">-	leal	pgtable + 0x1000(%ebx), %edi</span>
<span class="p_add">+	addl	$0x1000, %edx</span>
<span class="p_add">+	leal	pgtable(%ebx,%edx), %edi</span>
 	leal	0x1007(%edi), %eax
 	movl	$4, %ecx
 1:	movl	%eax, 0x00(%edi)
<span class="p_chunk">@@ -152,7 +166,8 @@</span> <span class="p_context"> ENTRY(startup_32)</span>
 	jnz	1b
 
 	/* Build Level 2 */
<span class="p_del">-	leal	pgtable + 0x2000(%ebx), %edi</span>
<span class="p_add">+	addl	$0x1000, %edx</span>
<span class="p_add">+	leal	pgtable(%ebx,%edx), %edi</span>
 	movl	$0x00000183, %eax
 	movl	$2048, %ecx
 1:	movl	%eax, 0(%edi)
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index 90f32116acd8..6cefd861ac65 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -917,7 +917,7 @@</span> <span class="p_context"> extern pgd_t trampoline_pgd_entry;</span>
 static inline void __meminit init_trampoline_default(void)
 {
 	/* Default trampoline pgd value */
<span class="p_del">-	trampoline_pgd_entry = init_level4_pgt[pgd_index(__PAGE_OFFSET)];</span>
<span class="p_add">+	trampoline_pgd_entry = init_top_pgt[pgd_index(__PAGE_OFFSET)];</span>
 }
 # ifdef CONFIG_RANDOMIZE_MEMORY
 void __meminit init_trampoline(void);
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 9991224f6238..c9e41f1599dd 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -14,15 +14,17 @@</span> <span class="p_context"></span>
 #include &lt;linux/bitops.h&gt;
 #include &lt;linux/threads.h&gt;
 
<span class="p_add">+extern p4d_t level4_kernel_pgt[512];</span>
<span class="p_add">+extern p4d_t level4_ident_pgt[512];</span>
 extern pud_t level3_kernel_pgt[512];
 extern pud_t level3_ident_pgt[512];
 extern pmd_t level2_kernel_pgt[512];
 extern pmd_t level2_fixmap_pgt[512];
 extern pmd_t level2_ident_pgt[512];
 extern pte_t level1_fixmap_pgt[512];
<span class="p_del">-extern pgd_t init_level4_pgt[];</span>
<span class="p_add">+extern pgd_t init_top_pgt[];</span>
 
<span class="p_del">-#define swapper_pg_dir init_level4_pgt</span>
<span class="p_add">+#define swapper_pg_dir init_top_pgt</span>
 
 extern void paging_init(void);
 
<span class="p_header">diff --git a/arch/x86/include/uapi/asm/processor-flags.h b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">index 567de50a4c2a..185f3d10c194 100644</span>
<span class="p_header">--- a/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_chunk">@@ -104,6 +104,8 @@</span> <span class="p_context"></span>
 #define X86_CR4_OSFXSR		_BITUL(X86_CR4_OSFXSR_BIT)
 #define X86_CR4_OSXMMEXCPT_BIT	10 /* enable unmasked SSE exceptions */
 #define X86_CR4_OSXMMEXCPT	_BITUL(X86_CR4_OSXMMEXCPT_BIT)
<span class="p_add">+#define X86_CR4_LA57_BIT	12 /* enable 5-level page tables */</span>
<span class="p_add">+#define X86_CR4_LA57		_BITUL(X86_CR4_LA57_BIT)</span>
 #define X86_CR4_VMXE_BIT	13 /* enable VMX virtualization */
 #define X86_CR4_VMXE		_BITUL(X86_CR4_VMXE_BIT)
 #define X86_CR4_SMXE_BIT	14 /* enable safer mode (TXT) */
<span class="p_header">diff --git a/arch/x86/kernel/espfix_64.c b/arch/x86/kernel/espfix_64.c</span>
<span class="p_header">index 8e598a1ad986..6b91e2eb8d3f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/espfix_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/espfix_64.c</span>
<span class="p_chunk">@@ -125,7 +125,7 @@</span> <span class="p_context"> void __init init_espfix_bsp(void)</span>
 	p4d_t *p4d;
 
 	/* Install the espfix pud into the kernel page directory */
<span class="p_del">-	pgd = &amp;init_level4_pgt[pgd_index(ESPFIX_BASE_ADDR)];</span>
<span class="p_add">+	pgd = &amp;init_top_pgt[pgd_index(ESPFIX_BASE_ADDR)];</span>
 	p4d = p4d_alloc(&amp;init_mm, pgd, ESPFIX_BASE_ADDR);
 	p4d_populate(&amp;init_mm, p4d, espfix_pud_page);
 
<span class="p_header">diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c</span>
<span class="p_header">index 54a2372f5dbb..f32d22986f47 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/head64.c</span>
<span class="p_chunk">@@ -32,7 +32,7 @@</span> <span class="p_context"></span>
 /*
  * Manage page tables very early on.
  */
<span class="p_del">-extern pgd_t early_level4_pgt[PTRS_PER_PGD];</span>
<span class="p_add">+extern pgd_t early_top_pgt[PTRS_PER_PGD];</span>
 extern pmd_t early_dynamic_pgts[EARLY_DYNAMIC_PAGE_TABLES][PTRS_PER_PMD];
 static unsigned int __initdata next_early_pgt = 2;
 pmdval_t early_pmd_flags = __PAGE_KERNEL_LARGE &amp; ~(_PAGE_GLOBAL | _PAGE_NX);
<span class="p_chunk">@@ -40,9 +40,9 @@</span> <span class="p_context"> pmdval_t early_pmd_flags = __PAGE_KERNEL_LARGE &amp; ~(_PAGE_GLOBAL | _PAGE_NX);</span>
 /* Wipe all early page tables except for the kernel symbol map */
 static void __init reset_early_page_tables(void)
 {
<span class="p_del">-	memset(early_level4_pgt, 0, sizeof(pgd_t)*(PTRS_PER_PGD-1));</span>
<span class="p_add">+	memset(early_top_pgt, 0, sizeof(pgd_t)*(PTRS_PER_PGD-1));</span>
 	next_early_pgt = 0;
<span class="p_del">-	write_cr3(__pa_nodebug(early_level4_pgt));</span>
<span class="p_add">+	write_cr3(__pa_nodebug(early_top_pgt));</span>
 }
 
 /* Create a new PMD entry */
<span class="p_chunk">@@ -50,15 +50,16 @@</span> <span class="p_context"> int __init early_make_pgtable(unsigned long address)</span>
 {
 	unsigned long physaddr = address - __PAGE_OFFSET;
 	pgdval_t pgd, *pgd_p;
<span class="p_add">+	p4dval_t p4d, *p4d_p;</span>
 	pudval_t pud, *pud_p;
 	pmdval_t pmd, *pmd_p;
 
 	/* Invalid address or early pgt is done ?  */
<span class="p_del">-	if (physaddr &gt;= MAXMEM || read_cr3() != __pa_nodebug(early_level4_pgt))</span>
<span class="p_add">+	if (physaddr &gt;= MAXMEM || read_cr3() != __pa_nodebug(early_top_pgt))</span>
 		return -1;
 
 again:
<span class="p_del">-	pgd_p = &amp;early_level4_pgt[pgd_index(address)].pgd;</span>
<span class="p_add">+	pgd_p = &amp;early_top_pgt[pgd_index(address)].pgd;</span>
 	pgd = *pgd_p;
 
 	/*
<span class="p_chunk">@@ -66,8 +67,25 @@</span> <span class="p_context"> int __init early_make_pgtable(unsigned long address)</span>
 	 * critical -- __PAGE_OFFSET would point us back into the dynamic
 	 * range and we might end up looping forever...
 	 */
<span class="p_del">-	if (pgd)</span>
<span class="p_del">-		pud_p = (pudval_t *)((pgd &amp; PTE_PFN_MASK) + __START_KERNEL_map - phys_base);</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_X86_5LEVEL))</span>
<span class="p_add">+		p4d_p = pgd_p;</span>
<span class="p_add">+	else if (pgd)</span>
<span class="p_add">+		p4d_p = (p4dval_t *)((pgd &amp; PTE_PFN_MASK) + __START_KERNEL_map - phys_base);</span>
<span class="p_add">+	else {</span>
<span class="p_add">+		if (next_early_pgt &gt;= EARLY_DYNAMIC_PAGE_TABLES) {</span>
<span class="p_add">+			reset_early_page_tables();</span>
<span class="p_add">+			goto again;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		p4d_p = (p4dval_t *)early_dynamic_pgts[next_early_pgt++];</span>
<span class="p_add">+		memset(p4d_p, 0, sizeof(*p4d_p) * PTRS_PER_P4D);</span>
<span class="p_add">+		*pgd_p = (pgdval_t)p4d_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	p4d_p += p4d_index(address);</span>
<span class="p_add">+	p4d = *p4d_p;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (p4d)</span>
<span class="p_add">+		pud_p = (pudval_t *)((p4d &amp; PTE_PFN_MASK) + __START_KERNEL_map - phys_base);</span>
 	else {
 		if (next_early_pgt &gt;= EARLY_DYNAMIC_PAGE_TABLES) {
 			reset_early_page_tables();
<span class="p_chunk">@@ -76,7 +94,7 @@</span> <span class="p_context"> int __init early_make_pgtable(unsigned long address)</span>
 
 		pud_p = (pudval_t *)early_dynamic_pgts[next_early_pgt++];
 		memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);
<span class="p_del">-		*pgd_p = (pgdval_t)pud_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;</span>
<span class="p_add">+		*p4d_p = (p4dval_t)pud_p - __START_KERNEL_map + phys_base + _KERNPG_TABLE;</span>
 	}
 	pud_p += pud_index(address);
 	pud = *pud_p;
<span class="p_chunk">@@ -155,7 +173,7 @@</span> <span class="p_context"> asmlinkage __visible void __init x86_64_start_kernel(char * real_mode_data)</span>
 
 	clear_bss();
 
<span class="p_del">-	clear_page(init_level4_pgt);</span>
<span class="p_add">+	clear_page(init_top_pgt);</span>
 
 	kasan_early_init();
 
<span class="p_chunk">@@ -170,8 +188,8 @@</span> <span class="p_context"> asmlinkage __visible void __init x86_64_start_kernel(char * real_mode_data)</span>
 	 */
 	load_ucode_bsp();
 
<span class="p_del">-	/* set init_level4_pgt kernel high mapping*/</span>
<span class="p_del">-	init_level4_pgt[511] = early_level4_pgt[511];</span>
<span class="p_add">+	/* set init_top_pgt kernel high mapping*/</span>
<span class="p_add">+	init_top_pgt[511] = early_top_pgt[511];</span>
 
 	x86_64_start_reservations(real_mode_data);
 }
<span class="p_header">diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S</span>
<span class="p_header">index b467b14b03eb..fd1f88d94d6b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S</span>
<span class="p_chunk">@@ -37,10 +37,14 @@</span> <span class="p_context"></span>
  *
  */
 
<span class="p_add">+#define p4d_index(x)	(((x) &gt;&gt; P4D_SHIFT) &amp; (PTRS_PER_P4D-1))</span>
 #define pud_index(x)	(((x) &gt;&gt; PUD_SHIFT) &amp; (PTRS_PER_PUD-1))
 
<span class="p_del">-L4_PAGE_OFFSET = pgd_index(__PAGE_OFFSET_BASE)</span>
<span class="p_del">-L4_START_KERNEL = pgd_index(__START_KERNEL_map)</span>
<span class="p_add">+PGD_PAGE_OFFSET = pgd_index(__PAGE_OFFSET_BASE)</span>
<span class="p_add">+PGD_START_KERNEL = pgd_index(__START_KERNEL_map)</span>
<span class="p_add">+#ifdef CONFIG_X86_5LEVEL</span>
<span class="p_add">+L4_START_KERNEL = p4d_index(__START_KERNEL_map)</span>
<span class="p_add">+#endif</span>
 L3_START_KERNEL = pud_index(__START_KERNEL_map)
 
 	.text
<span class="p_chunk">@@ -93,7 +97,11 @@</span> <span class="p_context"> startup_64:</span>
 	/*
 	 * Fixup the physical addresses in the page table
 	 */
<span class="p_del">-	addq	%rbp, early_level4_pgt + (L4_START_KERNEL*8)(%rip)</span>
<span class="p_add">+	addq	%rbp, early_top_pgt + (PGD_START_KERNEL*8)(%rip)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_5LEVEL</span>
<span class="p_add">+	addq	%rbp, level4_kernel_pgt + (511*8)(%rip)</span>
<span class="p_add">+#endif</span>
 
 	addq	%rbp, level3_kernel_pgt + (510*8)(%rip)
 	addq	%rbp, level3_kernel_pgt + (511*8)(%rip)
<span class="p_chunk">@@ -107,7 +115,7 @@</span> <span class="p_context"> startup_64:</span>
 	 * it avoids problems around wraparound.
 	 */
 	leaq	_text(%rip), %rdi
<span class="p_del">-	leaq	early_level4_pgt(%rip), %rbx</span>
<span class="p_add">+	leaq	early_top_pgt(%rip), %rbx</span>
 
 	movq	%rdi, %rax
 	shrq	$PGDIR_SHIFT, %rax
<span class="p_chunk">@@ -116,16 +124,26 @@</span> <span class="p_context"> startup_64:</span>
 	movq	%rdx, 0(%rbx,%rax,8)
 	movq	%rdx, 8(%rbx,%rax,8)
 
<span class="p_add">+#ifdef CONFIG_X86_5LEVEL</span>
<span class="p_add">+	addq	$PAGE_SIZE, %rbx</span>
<span class="p_add">+	addq	$PAGE_SIZE, %rdx</span>
<span class="p_add">+	movq	%rdi, %rax</span>
<span class="p_add">+	shrq	$P4D_SHIFT, %rax</span>
<span class="p_add">+	andl	$(PTRS_PER_P4D-1), %eax</span>
<span class="p_add">+	movq	%rdx, 0(%rbx,%rax,8)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	addq	$PAGE_SIZE, %rbx</span>
 	addq	$PAGE_SIZE, %rdx
 	movq	%rdi, %rax
 	shrq	$PUD_SHIFT, %rax
 	andl	$(PTRS_PER_PUD-1), %eax
<span class="p_del">-	movq	%rdx, PAGE_SIZE(%rbx,%rax,8)</span>
<span class="p_add">+	movq	%rdx, 0(%rbx,%rax,8)</span>
 	incl	%eax
 	andl	$(PTRS_PER_PUD-1), %eax
<span class="p_del">-	movq	%rdx, PAGE_SIZE(%rbx,%rax,8)</span>
<span class="p_add">+	movq	%rdx, 0(%rbx,%rax,8)</span>
 
<span class="p_del">-	addq	$PAGE_SIZE * 2, %rbx</span>
<span class="p_add">+	addq	$PAGE_SIZE, %rbx</span>
 	movq	%rdi, %rax
 	shrq	$PMD_SHIFT, %rdi
 	addq	$(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL), %rax
<span class="p_chunk">@@ -166,7 +184,7 @@</span> <span class="p_context"> startup_64:</span>
 	addq	%rbp, phys_base(%rip)
 
 .Lskip_fixup:
<span class="p_del">-	movq	$(early_level4_pgt - __START_KERNEL_map), %rax</span>
<span class="p_add">+	movq	$(early_top_pgt - __START_KERNEL_map), %rax</span>
 	jmp 1f
 ENTRY(secondary_startup_64)
 	/*
<span class="p_chunk">@@ -186,14 +204,17 @@</span> <span class="p_context"> ENTRY(secondary_startup_64)</span>
 	/* Sanitize CPU configuration */
 	call verify_cpu
 
<span class="p_del">-	movq	$(init_level4_pgt - __START_KERNEL_map), %rax</span>
<span class="p_add">+	movq	$(init_top_pgt - __START_KERNEL_map), %rax</span>
 1:
 
<span class="p_del">-	/* Enable PAE mode and PGE */</span>
<span class="p_add">+	/* Enable PAE mode, PGE and LA57 */</span>
 	movl	$(X86_CR4_PAE | X86_CR4_PGE), %ecx
<span class="p_add">+#ifdef CONFIG_X86_5LEVEL</span>
<span class="p_add">+	orl	$X86_CR4_LA57, %ecx</span>
<span class="p_add">+#endif</span>
 	movq	%rcx, %cr4
 
<span class="p_del">-	/* Setup early boot stage 4 level pagetables. */</span>
<span class="p_add">+	/* Setup early boot stage 4-/5-level pagetables. */</span>
 	addq	phys_base(%rip), %rax
 	movq	%rax, %cr3
 
<span class="p_chunk">@@ -419,9 +440,13 @@</span> <span class="p_context"> GLOBAL(name)</span>
 	.endr
 
 	__INITDATA
<span class="p_del">-NEXT_PAGE(early_level4_pgt)</span>
<span class="p_add">+NEXT_PAGE(early_top_pgt)</span>
 	.fill	511,8,0
<span class="p_add">+#ifdef CONFIG_X86_5LEVEL</span>
<span class="p_add">+	.quad	level4_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE</span>
<span class="p_add">+#else</span>
 	.quad	level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE
<span class="p_add">+#endif</span>
 
 NEXT_PAGE(early_dynamic_pgts)
 	.fill	512*EARLY_DYNAMIC_PAGE_TABLES,8,0
<span class="p_chunk">@@ -429,14 +454,14 @@</span> <span class="p_context"> NEXT_PAGE(early_dynamic_pgts)</span>
 	.data
 
 #ifndef CONFIG_XEN
<span class="p_del">-NEXT_PAGE(init_level4_pgt)</span>
<span class="p_add">+NEXT_PAGE(init_top_pgt)</span>
 	.fill	512,8,0
 #else
<span class="p_del">-NEXT_PAGE(init_level4_pgt)</span>
<span class="p_add">+NEXT_PAGE(init_top_pgt)</span>
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
<span class="p_del">-	.org    init_level4_pgt + L4_PAGE_OFFSET*8, 0</span>
<span class="p_add">+	.org    init_top_pgt + PGD_PAGE_OFFSET*8, 0</span>
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE
<span class="p_del">-	.org    init_level4_pgt + L4_START_KERNEL*8, 0</span>
<span class="p_add">+	.org    init_top_pgt + PGD_START_KERNEL*8, 0</span>
 	/* (2^48-(2*1024*1024*1024))/(2^39) = 511 */
 	.quad   level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE
 
<span class="p_chunk">@@ -450,6 +475,12 @@</span> <span class="p_context"> NEXT_PAGE(level2_ident_pgt)</span>
 	PMDS(0, __PAGE_KERNEL_IDENT_LARGE_EXEC, PTRS_PER_PMD)
 #endif
 
<span class="p_add">+#ifdef CONFIG_X86_5LEVEL</span>
<span class="p_add">+NEXT_PAGE(level4_kernel_pgt)</span>
<span class="p_add">+	.fill	511,8,0</span>
<span class="p_add">+	.quad	level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 NEXT_PAGE(level3_kernel_pgt)
 	.fill	L3_START_KERNEL,8,0
 	/* (2^48-(2*1024*1024*1024)-((2^39)*511))/(2^30) = 510 */
<span class="p_header">diff --git a/arch/x86/kernel/machine_kexec_64.c b/arch/x86/kernel/machine_kexec_64.c</span>
<span class="p_header">index 085c3b300d32..42f502b45e62 100644</span>
<span class="p_header">--- a/arch/x86/kernel/machine_kexec_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/machine_kexec_64.c</span>
<span class="p_chunk">@@ -342,7 +342,7 @@</span> <span class="p_context"> void machine_kexec(struct kimage *image)</span>
 void arch_crash_save_vmcoreinfo(void)
 {
 	VMCOREINFO_NUMBER(phys_base);
<span class="p_del">-	VMCOREINFO_SYMBOL(init_level4_pgt);</span>
<span class="p_add">+	VMCOREINFO_SYMBOL(init_top_pgt);</span>
 
 #ifdef CONFIG_NUMA
 	VMCOREINFO_SYMBOL(node_data);
<span class="p_header">diff --git a/arch/x86/mm/dump_pagetables.c b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">index 0effac6989cd..0431bfd5e09f 100644</span>
<span class="p_header">--- a/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_chunk">@@ -435,7 +435,7 @@</span> <span class="p_context"> static void ptdump_walk_pgd_level_core(struct seq_file *m, pgd_t *pgd,</span>
 				       bool checkwx)
 {
 #ifdef CONFIG_X86_64
<span class="p_del">-	pgd_t *start = (pgd_t *) &amp;init_level4_pgt;</span>
<span class="p_add">+	pgd_t *start = (pgd_t *) &amp;init_top_pgt;</span>
 #else
 	pgd_t *start = swapper_pg_dir;
 #endif
<span class="p_header">diff --git a/arch/x86/mm/kasan_init_64.c b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">index bcabc56e0dc4..a25dd40a0683 100644</span>
<span class="p_header">--- a/arch/x86/mm/kasan_init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/kasan_init_64.c</span>
<span class="p_chunk">@@ -10,7 +10,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/sections.h&gt;
 
<span class="p_del">-extern pgd_t early_level4_pgt[PTRS_PER_PGD];</span>
<span class="p_add">+extern pgd_t early_top_pgt[PTRS_PER_PGD];</span>
 extern struct range pfn_mapped[E820_X_MAX];
 
 static int __init map_range(struct range *range)
<span class="p_chunk">@@ -103,8 +103,8 @@</span> <span class="p_context"> void __init kasan_early_init(void)</span>
 	for (i = 0; CONFIG_PGTABLE_LEVELS &gt;= 5 &amp;&amp; i &lt; PTRS_PER_P4D; i++)
 		kasan_zero_p4d[i] = __p4d(p4d_val);
 
<span class="p_del">-	kasan_map_early_shadow(early_level4_pgt);</span>
<span class="p_del">-	kasan_map_early_shadow(init_level4_pgt);</span>
<span class="p_add">+	kasan_map_early_shadow(early_top_pgt);</span>
<span class="p_add">+	kasan_map_early_shadow(init_top_pgt);</span>
 }
 
 void __init kasan_init(void)
<span class="p_chunk">@@ -115,8 +115,8 @@</span> <span class="p_context"> void __init kasan_init(void)</span>
 	register_die_notifier(&amp;kasan_die_notifier);
 #endif
 
<span class="p_del">-	memcpy(early_level4_pgt, init_level4_pgt, sizeof(early_level4_pgt));</span>
<span class="p_del">-	load_cr3(early_level4_pgt);</span>
<span class="p_add">+	memcpy(early_top_pgt, init_top_pgt, sizeof(early_top_pgt));</span>
<span class="p_add">+	load_cr3(early_top_pgt);</span>
 	__flush_tlb_all();
 
 	clear_pgds(KASAN_SHADOW_START, KASAN_SHADOW_END);
<span class="p_chunk">@@ -142,7 +142,7 @@</span> <span class="p_context"> void __init kasan_init(void)</span>
 	kasan_populate_zero_shadow(kasan_mem_to_shadow((void *)MODULES_END),
 			(void *)KASAN_SHADOW_END);
 
<span class="p_del">-	load_cr3(init_level4_pgt);</span>
<span class="p_add">+	load_cr3(init_top_pgt);</span>
 	__flush_tlb_all();
 
 	/*
<span class="p_header">diff --git a/arch/x86/realmode/init.c b/arch/x86/realmode/init.c</span>
<span class="p_header">index 5db706f14111..dc0836d5c5eb 100644</span>
<span class="p_header">--- a/arch/x86/realmode/init.c</span>
<span class="p_header">+++ b/arch/x86/realmode/init.c</span>
<span class="p_chunk">@@ -102,7 +102,7 @@</span> <span class="p_context"> static void __init setup_real_mode(void)</span>
 
 	trampoline_pgd = (u64 *) __va(real_mode_header-&gt;trampoline_pgd);
 	trampoline_pgd[0] = trampoline_pgd_entry.pgd;
<span class="p_del">-	trampoline_pgd[511] = init_level4_pgt[511].pgd;</span>
<span class="p_add">+	trampoline_pgd[511] = init_top_pgt[511].pgd;</span>
 #endif
 }
 
<span class="p_header">diff --git a/arch/x86/xen/mmu.c b/arch/x86/xen/mmu.c</span>
<span class="p_header">index abb3a7701bc7..d66b7e79781a 100644</span>
<span class="p_header">--- a/arch/x86/xen/mmu.c</span>
<span class="p_header">+++ b/arch/x86/xen/mmu.c</span>
<span class="p_chunk">@@ -97,7 +97,11 @@</span> <span class="p_context"> static RESERVE_BRK_ARRAY(pte_t, level1_ident_pgt, LEVEL1_IDENT_ENTRIES);</span>
 #endif
 #ifdef CONFIG_X86_64
 /* l3 pud for userspace vsyscall mapping */
<span class="p_del">-static pud_t level3_user_vsyscall[PTRS_PER_PUD] __page_aligned_bss;</span>
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS == 5</span>
<span class="p_add">+static p4d_t user_vsyscall[PTRS_PER_P4D] __page_aligned_bss;</span>
<span class="p_add">+#else</span>
<span class="p_add">+static pud_t user_vsyscall[PTRS_PER_PUD] __page_aligned_bss;</span>
<span class="p_add">+#endif</span>
 #endif /* CONFIG_X86_64 */
 
 /*
<span class="p_chunk">@@ -504,7 +508,7 @@</span> <span class="p_context"> __visible pmd_t xen_make_pmd(pmdval_t pmd)</span>
 }
 PV_CALLEE_SAVE_REGS_THUNK(xen_make_pmd);
 
<span class="p_del">-#if CONFIG_PGTABLE_LEVELS == 4</span>
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS &gt;= 4</span>
 __visible pudval_t xen_pud_val(pud_t pud)
 {
 	return pte_mfn_to_pfn(pud.pud);
<span class="p_chunk">@@ -1531,8 +1535,8 @@</span> <span class="p_context"> static void xen_write_cr3(unsigned long cr3)</span>
  * At the start of the day - when Xen launches a guest, it has already
  * built pagetables for the guest. We diligently look over them
  * in xen_setup_kernel_pagetable and graft as appropriate them in the
<span class="p_del">- * init_level4_pgt and its friends. Then when we are happy we load</span>
<span class="p_del">- * the new init_level4_pgt - and continue on.</span>
<span class="p_add">+ * init_top_pgt and its friends. Then when we are happy we load</span>
<span class="p_add">+ * the new init_top_pgt - and continue on.</span>
  *
  * The generic code starts (start_kernel) and &#39;init_mem_mapping&#39; sets
  * up the rest of the pagetables. When it has completed it loads the cr3.
<span class="p_chunk">@@ -1585,7 +1589,7 @@</span> <span class="p_context"> static int xen_pgd_alloc(struct mm_struct *mm)</span>
 		if (user_pgd != NULL) {
 #ifdef CONFIG_X86_VSYSCALL_EMULATION
 			user_pgd[pgd_index(VSYSCALL_ADDR)] =
<span class="p_del">-				__pgd(__pa(level3_user_vsyscall) | _PAGE_TABLE);</span>
<span class="p_add">+				__pgd(__pa(user_vsyscall) | _PAGE_TABLE);</span>
 #endif
 			ret = 0;
 		}
<span class="p_chunk">@@ -1975,13 +1979,13 @@</span> <span class="p_context"> void __init xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pfn)</span>
 	pt_end = pt_base + xen_start_info-&gt;nr_pt_frames;
 
 	/* Zap identity mapping */
<span class="p_del">-	init_level4_pgt[0] = __pgd(0);</span>
<span class="p_add">+	init_top_pgt[0] = __pgd(0);</span>
 
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
 		/* Pre-constructed entries are in pfn, so convert to mfn */
 		/* L4[272] -&gt; level3_ident_pgt
 		 * L4[511] -&gt; level3_kernel_pgt */
<span class="p_del">-		convert_pfn_mfn(init_level4_pgt);</span>
<span class="p_add">+		convert_pfn_mfn(init_top_pgt);</span>
 
 		/* L3_i[0] -&gt; level2_ident_pgt */
 		convert_pfn_mfn(level3_ident_pgt);
<span class="p_chunk">@@ -2012,14 +2016,14 @@</span> <span class="p_context"> void __init xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pfn)</span>
 	/* Copy the initial P-&gt;M table mappings if necessary. */
 	i = pgd_index(xen_start_info-&gt;mfn_list);
 	if (i &amp;&amp; i &lt; pgd_index(__START_KERNEL_map))
<span class="p_del">-		init_level4_pgt[i] = ((pgd_t *)xen_start_info-&gt;pt_base)[i];</span>
<span class="p_add">+		init_top_pgt[i] = ((pgd_t *)xen_start_info-&gt;pt_base)[i];</span>
 
 	if (!xen_feature(XENFEAT_auto_translated_physmap)) {
 		/* Make pagetable pieces RO */
<span class="p_del">-		set_page_prot(init_level4_pgt, PAGE_KERNEL_RO);</span>
<span class="p_add">+		set_page_prot(init_top_pgt, PAGE_KERNEL_RO);</span>
 		set_page_prot(level3_ident_pgt, PAGE_KERNEL_RO);
 		set_page_prot(level3_kernel_pgt, PAGE_KERNEL_RO);
<span class="p_del">-		set_page_prot(level3_user_vsyscall, PAGE_KERNEL_RO);</span>
<span class="p_add">+		set_page_prot(user_vsyscall, PAGE_KERNEL_RO);</span>
 		set_page_prot(level2_ident_pgt, PAGE_KERNEL_RO);
 		set_page_prot(level2_kernel_pgt, PAGE_KERNEL_RO);
 		set_page_prot(level2_fixmap_pgt, PAGE_KERNEL_RO);
<span class="p_chunk">@@ -2027,7 +2031,7 @@</span> <span class="p_context"> void __init xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pfn)</span>
 
 		/* Pin down new L4 */
 		pin_pagetable_pfn(MMUEXT_PIN_L4_TABLE,
<span class="p_del">-				  PFN_DOWN(__pa_symbol(init_level4_pgt)));</span>
<span class="p_add">+				  PFN_DOWN(__pa_symbol(init_top_pgt)));</span>
 
 		/* Unpin Xen-provided one */
 		pin_pagetable_pfn(MMUEXT_UNPIN_TABLE, PFN_DOWN(__pa(pgd)));
<span class="p_chunk">@@ -2038,10 +2042,10 @@</span> <span class="p_context"> void __init xen_setup_kernel_pagetable(pgd_t *pgd, unsigned long max_pfn)</span>
 		 * pgd.
 		 */
 		xen_mc_batch();
<span class="p_del">-		__xen_write_cr3(true, __pa(init_level4_pgt));</span>
<span class="p_add">+		__xen_write_cr3(true, __pa(init_top_pgt));</span>
 		xen_mc_issue(PARAVIRT_LAZY_CPU);
 	} else
<span class="p_del">-		native_write_cr3(__pa(init_level4_pgt));</span>
<span class="p_add">+		native_write_cr3(__pa(init_top_pgt));</span>
 
 	/* We can&#39;t that easily rip out L3 and L2, as the Xen pagetables are
 	 * set out this way: [L4], [L1], [L2], [L3], [L1], [L1] ...  for
<span class="p_chunk">@@ -2446,7 +2450,11 @@</span> <span class="p_context"> static void xen_set_fixmap(unsigned idx, phys_addr_t phys, pgprot_t prot)</span>
 	   pagetable vsyscall mapping. */
 	if (idx == VSYSCALL_PAGE) {
 		unsigned long vaddr = __fix_to_virt(idx);
<span class="p_del">-		set_pte_vaddr_pud(level3_user_vsyscall, vaddr, pte);</span>
<span class="p_add">+#if CONFIG_PGTABLE_LEVELS == 5</span>
<span class="p_add">+		set_pte_vaddr_p4d(user_vsyscall, vaddr, pte);</span>
<span class="p_add">+#else</span>
<span class="p_add">+		set_pte_vaddr_pud(user_vsyscall, vaddr, pte);</span>
<span class="p_add">+#endif</span>
 	}
 #endif
 }
<span class="p_chunk">@@ -2477,7 +2485,7 @@</span> <span class="p_context"> static void __init xen_post_allocator_init(void)</span>
 
 #ifdef CONFIG_X86_64
 	pv_mmu_ops.write_cr3 = &amp;xen_write_cr3;
<span class="p_del">-	SetPagePinned(virt_to_page(level3_user_vsyscall));</span>
<span class="p_add">+	SetPagePinned(virt_to_page(user_vsyscall));</span>
 #endif
 	xen_mark_init_mm_pinned();
 }
<span class="p_header">diff --git a/arch/x86/xen/xen-pvh.S b/arch/x86/xen/xen-pvh.S</span>
<span class="p_header">index 5e246716d58f..e1a5fbeae08d 100644</span>
<span class="p_header">--- a/arch/x86/xen/xen-pvh.S</span>
<span class="p_header">+++ b/arch/x86/xen/xen-pvh.S</span>
<span class="p_chunk">@@ -87,7 +87,7 @@</span> <span class="p_context"> ENTRY(pvh_start_xen)</span>
 	wrmsr
 
 	/* Enable pre-constructed page tables. */
<span class="p_del">-	mov $_pa(init_level4_pgt), %eax</span>
<span class="p_add">+	mov $_pa(init_top_pgt), %eax</span>
 	mov %eax, %cr3
 	mov $(X86_CR0_PG | X86_CR0_PE), %eax
 	mov %eax, %cr0

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



