
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4/4,RFC!] mm: &#39;struct mm_struct&#39; reference counting debugging - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4/4,RFC!] mm: &#39;struct mm_struct&#39; reference counting debugging</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=83701">vegard.nossum@oracle.com</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 16, 2016, 8:22 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20161216082202.21044-4-vegard.nossum@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9477675/mbox/"
   >mbox</a>
|
   <a href="/patch/9477675/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9477675/">/patch/9477675/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	267E86047D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Dec 2016 10:00:01 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 25DC328793
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Dec 2016 10:00:01 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 19D67287B0; Fri, 16 Dec 2016 10:00:01 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D853A28793
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 16 Dec 2016 09:59:56 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933162AbcLPJ7t (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 16 Dec 2016 04:59:49 -0500
Received: from aserp1050.oracle.com ([141.146.126.70]:32666 &quot;EHLO
	aserp1050.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1760314AbcLPJ71 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 16 Dec 2016 04:59:27 -0500
Received: from aserp1040.oracle.com (aserp1040.oracle.com [141.146.126.69])
	by aserp1050.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id uBG8PFFX019816
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK)
	for &lt;linux-kernel@vger.kernel.org&gt;; Fri, 16 Dec 2016 08:25:15 GMT
Received: from aserv0021.oracle.com (aserv0021.oracle.com [141.146.126.233])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id uBG8MYMe014551
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Fri, 16 Dec 2016 08:22:34 GMT
Received: from lenuta.oracle.com
	(dhcp-ukc1-twvpn-3-vpnpool-10-175-224-163.vpn.oracle.com
	[10.175.224.163])
	by aserv0021.oracle.com (8.13.8/8.14.4) with ESMTP id uBG8MMGY005136; 
	Fri, 16 Dec 2016 08:22:31 GMT
From: Vegard Nossum &lt;vegard.nossum@oracle.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc: Rik van Riel &lt;riel@redhat.com&gt;, Matthew Wilcox &lt;mawilcox@microsoft.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;, Al Viro &lt;viro@zeniv.linux.org.uk&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Vegard Nossum &lt;vegard.nossum@oracle.com&gt;
Subject: [PATCH 4/4] [RFC!] mm: &#39;struct mm_struct&#39; reference counting
	debugging
Date: Fri, 16 Dec 2016 09:22:02 +0100
Message-Id: &lt;20161216082202.21044-4-vegard.nossum@oracle.com&gt;
X-Mailer: git-send-email 2.11.0.1.gaa10c3f
In-Reply-To: &lt;20161216082202.21044-1-vegard.nossum@oracle.com&gt;
References: &lt;20161216082202.21044-1-vegard.nossum@oracle.com&gt;
X-Source-IP: aserp1040.oracle.com [141.146.126.69]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=83701">vegard.nossum@oracle.com</a> - Dec. 16, 2016, 8:22 a.m.</div>
<pre class="content">
Reference counting bugs are hard to debug by their nature since the actual
manifestation of one can occur very far from where the error is introduced
(e.g. a missing get() only manifest as a use-after-free when the reference
count prematurely drops to 0, which could be arbitrarily long after where
the get() should have happened if there are other users). I wrote this patch
to try to track down a suspected &#39;mm_struct&#39; reference counting bug.

The basic idea is to keep track of all references, not just with a reference
counter, but with an actual reference _list_. Whenever you get() or put() a
reference, you also add or remove yourself, respectively, from the reference
list. This really helps debugging because (for example) you always put a
specific reference, meaning that if that reference was not yours to put, you
will notice it immediately (rather than when the reference counter goes to 0
and you still have an active reference).

The main interface is in &lt;linux/mm_ref_types.h&gt; and &lt;linux/mm_ref.h&gt;, while
the implementation lives in mm/mm_ref.c. Since &#39;struct mm_struct&#39; has both
-&gt;mm_users and -&gt;mm_count, we introduce helpers for both of them, but use
the same data structure for each (struct mm_ref). The low-level rules (i.e.
the ones we have to follow, but which nobody else should really have to
care about since they use the higher-level interface) are:

 - after incrementing -&gt;mm_count you also have to call get_mm_ref()

 - before decrementing -&gt;mm_count you also have to call put_mm_ref()

 - after incrementing -&gt;mm_users you also have to call get_mm_users_ref()

 - before decrementing -&gt;mm_users you also have to call put_mm_users_ref()

The rules that most of the rest of the kernel will care about are:

 - functions that acquire and return a mm_struct should take a
   &#39;struct mm_ref *&#39; which it can pass on to mmget()/mmgrab()/etc.

 - functions that release an mm_struct passed as a parameter should also
   take a &#39;struct mm_ref *&#39; which it can pass on to mmput()/mmdrop()/etc.

 - any function that temporarily acquires a mm_struct reference should
   use MM_REF() to define an on-stack reference and pass it on to
   mmget()/mmput()/mmgrab()/mmdrop()/etc.

 - any structure that holds an mm_struct pointer must also include a
   &#39;struct mm_ref&#39; member; when the mm_struct pointer is modified you
   would typically also call mmget()/mmgrab()/mmput()/mmdrop() and they
   should be called with this mm_ref

 - you can convert (for example) an on-stack reference to an in-struct
   reference using move_mm_ref(). This is semantically equivalent to
   (atomically) taking the new reference and dropping the old one, but
   doesn&#39;t actually need to modify the reference count

I don&#39;t really have any delusions about getting this into mainline
(_especially_ not without a CONFIG_MM_REF toggle and zero impact in the =n
case), but I&#39;m posting it in case somebody would find it useful and maybe
to start a discussion about whether this is something that can be usefully
generalized to other core data structures with complicated
reference/ownership models.

The patch really does make it very explicit who holds every reference
taken and where references are implicitly transferred, for example in
finish_task_switch() where the ownership of the reference to &#39;oldmm&#39; is
implicitly transferred from &#39;prev-&gt;mm&#39; to &#39;rq-&gt;prev_mm&#39;, or in
flush_old_exec() where the ownership of the &#39;bprm-&gt;mm&#39; is implicitly
transferred from &#39;bprm&#39; to &#39;current-&gt;mm&#39;. These ones are a bit subtle
because there is no explicit get()/put() in the code.

There are some users which haven&#39;t been converted by this patch (and
many more that haven&#39;t been tested) -- x86-64 defconfig should work out of
the box, though. The conversion for the rest of the kernel should be
mostly straightforward (the main challenge was fork/exec).

Thanks-to: Rik van Riel &lt;riel@redhat.com&gt;
Thanks-to: Matthew Wilcox &lt;mawilcox@microsoft.com&gt;
Thanks-to: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Michal Hocko &lt;mhocko@suse.com&gt;
Cc: Al Viro &lt;viro@zeniv.linux.org.uk&gt;
Cc: Ingo Molnar &lt;mingo@kernel.org&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
<span class="signed-off-by">Signed-off-by: Vegard Nossum &lt;vegard.nossum@oracle.com&gt;</span>
---
 arch/x86/kernel/cpu/common.c            |   2 +-
 drivers/gpu/drm/i915/i915_gem_userptr.c |  25 +++--
 drivers/vhost/vhost.c                   |   7 +-
 drivers/vhost/vhost.h                   |   1 +
 fs/exec.c                               |  20 +++-
 fs/proc/array.c                         |  15 +--
 fs/proc/base.c                          |  97 ++++++++++++-------
 fs/proc/internal.h                      |   4 +-
 fs/proc/task_mmu.c                      |  50 +++++++---
 include/linux/binfmts.h                 |   1 +
 include/linux/init_task.h               |   1 +
 include/linux/kvm_host.h                |   3 +
 include/linux/mm_ref.h                  |  48 ++++++++++
 include/linux/mm_ref_types.h            |  41 ++++++++
 include/linux/mm_types.h                |   8 ++
 include/linux/mmu_notifier.h            |   8 +-
 include/linux/sched.h                   |  42 +++++---
 kernel/cpuset.c                         |  22 +++--
 kernel/events/core.c                    |   5 +-
 kernel/exit.c                           |   5 +-
 kernel/fork.c                           |  51 ++++++----
 kernel/futex.c                          | 124 +++++++++++++-----------
 kernel/sched/core.c                     |  14 ++-
 kernel/sched/sched.h                    |   1 +
 kernel/sys.c                            |   5 +-
 kernel/trace/trace_output.c             |   5 +-
 kernel/tsacct.c                         |   5 +-
 mm/Makefile                             |   2 +-
 mm/init-mm.c                            |   6 ++
 mm/memory.c                             |   5 +-
 mm/mempolicy.c                          |   5 +-
 mm/migrate.c                            |   5 +-
 mm/mm_ref.c                             | 163 ++++++++++++++++++++++++++++++++
 mm/mmu_context.c                        |   9 +-
 mm/mmu_notifier.c                       |  20 ++--
 mm/oom_kill.c                           |  12 ++-
 mm/process_vm_access.c                  |   5 +-
 mm/swapfile.c                           |  29 +++---
 mm/util.c                               |   5 +-
 virt/kvm/async_pf.c                     |   9 +-
 virt/kvm/kvm_main.c                     |  16 +++-
 41 files changed, 668 insertions(+), 233 deletions(-)
 create mode 100644 include/linux/mm_ref.h
 create mode 100644 include/linux/mm_ref_types.h
 create mode 100644 mm/mm_ref.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index b580da4582e1..edf16f695130 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -1555,7 +1555,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	for (i = 0; i &lt;= IO_BITMAP_LONGS; i++)
 		t-&gt;io_bitmap[i] = ~0UL;
 
<span class="p_del">-	mmgrab(&amp;init_mm);</span>
<span class="p_add">+	mmgrab(&amp;init_mm, &amp;me-&gt;mm_ref);</span>
 	me-&gt;active_mm = &amp;init_mm;
 	BUG_ON(me-&gt;mm);
 	enter_lazy_tlb(&amp;init_mm, me);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">index e97f9ade99fc..498d311e1a80 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_chunk">@@ -34,8 +34,10 @@</span> <span class="p_context"></span>
 
 struct i915_mm_struct {
 	struct mm_struct *mm;
<span class="p_add">+	struct mm_ref mm_ref;</span>
 	struct drm_i915_private *i915;
 	struct i915_mmu_notifier *mn;
<span class="p_add">+	struct mm_ref mn_ref;</span>
 	struct hlist_node node;
 	struct kref kref;
 	struct work_struct work;
<span class="p_chunk">@@ -159,7 +161,7 @@</span> <span class="p_context"> static const struct mmu_notifier_ops i915_gem_userptr_notifier = {</span>
 };
 
 static struct i915_mmu_notifier *
<span class="p_del">-i915_mmu_notifier_create(struct mm_struct *mm)</span>
<span class="p_add">+i915_mmu_notifier_create(struct mm_struct *mm, struct mm_ref *mm_ref)</span>
 {
 	struct i915_mmu_notifier *mn;
 	int ret;
<span class="p_chunk">@@ -178,7 +180,7 @@</span> <span class="p_context"> i915_mmu_notifier_create(struct mm_struct *mm)</span>
 	}
 
 	 /* Protected by mmap_sem (write-lock) */
<span class="p_del">-	ret = __mmu_notifier_register(&amp;mn-&gt;mn, mm);</span>
<span class="p_add">+	ret = __mmu_notifier_register(&amp;mn-&gt;mn, mm, mm_ref);</span>
 	if (ret) {
 		destroy_workqueue(mn-&gt;wq);
 		kfree(mn);
<span class="p_chunk">@@ -217,7 +219,7 @@</span> <span class="p_context"> i915_mmu_notifier_find(struct i915_mm_struct *mm)</span>
 	down_write(&amp;mm-&gt;mm-&gt;mmap_sem);
 	mutex_lock(&amp;mm-&gt;i915-&gt;mm_lock);
 	if ((mn = mm-&gt;mn) == NULL) {
<span class="p_del">-		mn = i915_mmu_notifier_create(mm-&gt;mm);</span>
<span class="p_add">+		mn = i915_mmu_notifier_create(mm-&gt;mm, &amp;mm-&gt;mn_ref);</span>
 		if (!IS_ERR(mn))
 			mm-&gt;mn = mn;
 	}
<span class="p_chunk">@@ -260,12 +262,12 @@</span> <span class="p_context"> i915_gem_userptr_init__mmu_notifier(struct drm_i915_gem_object *obj,</span>
 
 static void
 i915_mmu_notifier_free(struct i915_mmu_notifier *mn,
<span class="p_del">-		       struct mm_struct *mm)</span>
<span class="p_add">+		       struct mm_struct *mm, struct mm_ref *mm_ref)</span>
 {
 	if (mn == NULL)
 		return;
 
<span class="p_del">-	mmu_notifier_unregister(&amp;mn-&gt;mn, mm);</span>
<span class="p_add">+	mmu_notifier_unregister(&amp;mn-&gt;mn, mm, mm_ref);</span>
 	destroy_workqueue(mn-&gt;wq);
 	kfree(mn);
 }
<span class="p_chunk">@@ -341,9 +343,11 @@</span> <span class="p_context"> i915_gem_userptr_init__mm_struct(struct drm_i915_gem_object *obj)</span>
 		mm-&gt;i915 = to_i915(obj-&gt;base.dev);
 
 		mm-&gt;mm = current-&gt;mm;
<span class="p_del">-		mmgrab(current-&gt;mm);</span>
<span class="p_add">+		INIT_MM_REF(&amp;mm-&gt;mm_ref);</span>
<span class="p_add">+		mmgrab(current-&gt;mm, &amp;mm-&gt;mm_ref);</span>
 
 		mm-&gt;mn = NULL;
<span class="p_add">+		INIT_MM_REF(&amp;mm-&gt;mn_ref);</span>
 
 		/* Protected by dev_priv-&gt;mm_lock */
 		hash_add(dev_priv-&gt;mm_structs,
<span class="p_chunk">@@ -361,8 +365,8 @@</span> <span class="p_context"> static void</span>
 __i915_mm_struct_free__worker(struct work_struct *work)
 {
 	struct i915_mm_struct *mm = container_of(work, typeof(*mm), work);
<span class="p_del">-	i915_mmu_notifier_free(mm-&gt;mn, mm-&gt;mm);</span>
<span class="p_del">-	mmdrop(mm-&gt;mm);</span>
<span class="p_add">+	i915_mmu_notifier_free(mm-&gt;mn, mm-&gt;mm, &amp;mm-&gt;mn_ref);</span>
<span class="p_add">+	mmdrop(mm-&gt;mm, &amp;mm-&gt;mm_ref);</span>
 	kfree(mm);
 }
 
<span class="p_chunk">@@ -508,13 +512,14 @@</span> <span class="p_context"> __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
 	pvec = drm_malloc_gfp(npages, sizeof(struct page *), GFP_TEMPORARY);
 	if (pvec != NULL) {
 		struct mm_struct *mm = obj-&gt;userptr.mm-&gt;mm;
<span class="p_add">+		MM_REF(mm_ref);</span>
 		unsigned int flags = 0;
 
 		if (!obj-&gt;userptr.read_only)
 			flags |= FOLL_WRITE;
 
 		ret = -EFAULT;
<span class="p_del">-		if (mmget_not_zero(mm)) {</span>
<span class="p_add">+		if (mmget_not_zero(mm, &amp;mm_ref)) {</span>
 			down_read(&amp;mm-&gt;mmap_sem);
 			while (pinned &lt; npages) {
 				ret = get_user_pages_remote
<span class="p_chunk">@@ -529,7 +534,7 @@</span> <span class="p_context"> __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
 				pinned += ret;
 			}
 			up_read(&amp;mm-&gt;mmap_sem);
<span class="p_del">-			mmput(mm);</span>
<span class="p_add">+			mmput(mm, &amp;mm_ref);</span>
 		}
 	}
 
<span class="p_header">diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="p_header">index c6f2d89c0e97..4470abf94fe8 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.c</span>
<span class="p_header">+++ b/drivers/vhost/vhost.c</span>
<span class="p_chunk">@@ -407,6 +407,7 @@</span> <span class="p_context"> void vhost_dev_init(struct vhost_dev *dev,</span>
 	dev-&gt;umem = NULL;
 	dev-&gt;iotlb = NULL;
 	dev-&gt;mm = NULL;
<span class="p_add">+	INIT_MM_REF(&amp;dev-&gt;mm_ref);</span>
 	dev-&gt;worker = NULL;
 	init_llist_head(&amp;dev-&gt;work_list);
 	init_waitqueue_head(&amp;dev-&gt;wait);
<span class="p_chunk">@@ -483,7 +484,7 @@</span> <span class="p_context"> long vhost_dev_set_owner(struct vhost_dev *dev)</span>
 	}
 
 	/* No owner, become one */
<span class="p_del">-	dev-&gt;mm = get_task_mm(current);</span>
<span class="p_add">+	dev-&gt;mm = get_task_mm(current, &amp;dev-&gt;mm_ref);</span>
 	worker = kthread_create(vhost_worker, dev, &quot;vhost-%d&quot;, current-&gt;pid);
 	if (IS_ERR(worker)) {
 		err = PTR_ERR(worker);
<span class="p_chunk">@@ -507,7 +508,7 @@</span> <span class="p_context"> long vhost_dev_set_owner(struct vhost_dev *dev)</span>
 	dev-&gt;worker = NULL;
 err_worker:
 	if (dev-&gt;mm)
<span class="p_del">-		mmput(dev-&gt;mm);</span>
<span class="p_add">+		mmput(dev-&gt;mm, &amp;dev-&gt;mm_ref);</span>
 	dev-&gt;mm = NULL;
 err_mm:
 	return err;
<span class="p_chunk">@@ -639,7 +640,7 @@</span> <span class="p_context"> void vhost_dev_cleanup(struct vhost_dev *dev, bool locked)</span>
 		dev-&gt;worker = NULL;
 	}
 	if (dev-&gt;mm)
<span class="p_del">-		mmput(dev-&gt;mm);</span>
<span class="p_add">+		mmput(dev-&gt;mm, &amp;dev-&gt;mm_ref);</span>
 	dev-&gt;mm = NULL;
 }
 EXPORT_SYMBOL_GPL(vhost_dev_cleanup);
<span class="p_header">diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h</span>
<span class="p_header">index 78f3c5fc02e4..64fdcfa9cf67 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.h</span>
<span class="p_header">+++ b/drivers/vhost/vhost.h</span>
<span class="p_chunk">@@ -151,6 +151,7 @@</span> <span class="p_context"> struct vhost_msg_node {</span>
 
 struct vhost_dev {
 	struct mm_struct *mm;
<span class="p_add">+	struct mm_ref mm_ref;</span>
 	struct mutex mutex;
 	struct vhost_virtqueue **vqs;
 	int nvqs;
<span class="p_header">diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="p_header">index 4e497b9ee71e..13afedb2821d 100644</span>
<span class="p_header">--- a/fs/exec.c</span>
<span class="p_header">+++ b/fs/exec.c</span>
<span class="p_chunk">@@ -380,7 +380,7 @@</span> <span class="p_context"> static int bprm_mm_init(struct linux_binprm *bprm)</span>
 	int err;
 	struct mm_struct *mm = NULL;
 
<span class="p_del">-	bprm-&gt;mm = mm = mm_alloc();</span>
<span class="p_add">+	bprm-&gt;mm = mm = mm_alloc(&amp;bprm-&gt;mm_ref);</span>
 	err = -ENOMEM;
 	if (!mm)
 		goto err;
<span class="p_chunk">@@ -394,7 +394,7 @@</span> <span class="p_context"> static int bprm_mm_init(struct linux_binprm *bprm)</span>
 err:
 	if (mm) {
 		bprm-&gt;mm = NULL;
<span class="p_del">-		mmdrop(mm);</span>
<span class="p_add">+		mmdrop(mm, &amp;bprm-&gt;mm_ref);</span>
 	}
 
 	return err;
<span class="p_chunk">@@ -996,6 +996,8 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 {
 	struct task_struct *tsk;
 	struct mm_struct *old_mm, *active_mm;
<span class="p_add">+	MM_REF(old_mm_ref);</span>
<span class="p_add">+	MM_REF(active_mm_ref);</span>
 
 	/* Notify parent that we&#39;re no longer interested in the old VM */
 	tsk = current;
<span class="p_chunk">@@ -1015,9 +1017,14 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 			up_read(&amp;old_mm-&gt;mmap_sem);
 			return -EINTR;
 		}
<span class="p_add">+</span>
<span class="p_add">+		move_mm_users_ref(old_mm, &amp;current-&gt;mm_ref, &amp;old_mm_ref);</span>
 	}
 	task_lock(tsk);
<span class="p_add">+</span>
 	active_mm = tsk-&gt;active_mm;
<span class="p_add">+	if (!old_mm)</span>
<span class="p_add">+		move_mm_ref(active_mm, &amp;tsk-&gt;mm_ref, &amp;active_mm_ref);</span>
 	tsk-&gt;mm = mm;
 	tsk-&gt;active_mm = mm;
 	activate_mm(active_mm, mm);
<span class="p_chunk">@@ -1029,10 +1036,10 @@</span> <span class="p_context"> static int exec_mmap(struct mm_struct *mm)</span>
 		BUG_ON(active_mm != old_mm);
 		setmax_mm_hiwater_rss(&amp;tsk-&gt;signal-&gt;maxrss, old_mm);
 		mm_update_next_owner(old_mm);
<span class="p_del">-		mmput(old_mm);</span>
<span class="p_add">+		mmput(old_mm, &amp;old_mm_ref);</span>
 		return 0;
 	}
<span class="p_del">-	mmdrop(active_mm);</span>
<span class="p_add">+	mmdrop(active_mm, &amp;active_mm_ref);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -1258,6 +1265,7 @@</span> <span class="p_context"> int flush_old_exec(struct linux_binprm * bprm)</span>
 	if (retval)
 		goto out;
 
<span class="p_add">+	move_mm_ref(bprm-&gt;mm, &amp;bprm-&gt;mm_ref, &amp;current-&gt;mm_ref);</span>
 	bprm-&gt;mm = NULL;		/* We&#39;re using it now */
 
 	set_fs(USER_DS);
<span class="p_chunk">@@ -1674,6 +1682,8 @@</span> <span class="p_context"> static int do_execveat_common(int fd, struct filename *filename,</span>
 	if (!bprm)
 		goto out_files;
 
<span class="p_add">+	INIT_MM_REF(&amp;bprm-&gt;mm_ref);</span>
<span class="p_add">+</span>
 	retval = prepare_bprm_creds(bprm);
 	if (retval)
 		goto out_free;
<span class="p_chunk">@@ -1760,7 +1770,7 @@</span> <span class="p_context"> static int do_execveat_common(int fd, struct filename *filename,</span>
 out:
 	if (bprm-&gt;mm) {
 		acct_arg_size(bprm, 0);
<span class="p_del">-		mmput(bprm-&gt;mm);</span>
<span class="p_add">+		mmput(bprm-&gt;mm, &amp;bprm-&gt;mm_ref);</span>
 	}
 
 out_unmark:
<span class="p_header">diff --git a/fs/proc/array.c b/fs/proc/array.c</span>
<span class="p_header">index 81818adb8e9e..3e02be82c2f4 100644</span>
<span class="p_header">--- a/fs/proc/array.c</span>
<span class="p_header">+++ b/fs/proc/array.c</span>
<span class="p_chunk">@@ -367,14 +367,15 @@</span> <span class="p_context"> static void task_cpus_allowed(struct seq_file *m, struct task_struct *task)</span>
 int proc_pid_status(struct seq_file *m, struct pid_namespace *ns,
 			struct pid *pid, struct task_struct *task)
 {
<span class="p_del">-	struct mm_struct *mm = get_task_mm(task);</span>
<span class="p_add">+	MM_REF(mm_ref);</span>
<span class="p_add">+	struct mm_struct *mm = get_task_mm(task, &amp;mm_ref);</span>
 
 	task_name(m, task);
 	task_state(m, ns, pid, task);
 
 	if (mm) {
 		task_mem(m, mm);
<span class="p_del">-		mmput(mm);</span>
<span class="p_add">+		mmput(mm, &amp;mm_ref);</span>
 	}
 	task_sig(m, task);
 	task_cap(m, task);
<span class="p_chunk">@@ -397,6 +398,7 @@</span> <span class="p_context"> static int do_task_stat(struct seq_file *m, struct pid_namespace *ns,</span>
 	int num_threads = 0;
 	int permitted;
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	unsigned long long start_time;
 	unsigned long cmin_flt = 0, cmaj_flt = 0;
 	unsigned long  min_flt = 0,  maj_flt = 0;
<span class="p_chunk">@@ -409,7 +411,7 @@</span> <span class="p_context"> static int do_task_stat(struct seq_file *m, struct pid_namespace *ns,</span>
 	state = *get_task_state(task);
 	vsize = eip = esp = 0;
 	permitted = ptrace_may_access(task, PTRACE_MODE_READ_FSCREDS | PTRACE_MODE_NOAUDIT);
<span class="p_del">-	mm = get_task_mm(task);</span>
<span class="p_add">+	mm = get_task_mm(task, &amp;mm_ref);</span>
 	if (mm) {
 		vsize = task_vsize(mm);
 		/*
<span class="p_chunk">@@ -562,7 +564,7 @@</span> <span class="p_context"> static int do_task_stat(struct seq_file *m, struct pid_namespace *ns,</span>
 
 	seq_putc(m, &#39;\n&#39;);
 	if (mm)
<span class="p_del">-		mmput(mm);</span>
<span class="p_add">+		mmput(mm, &amp;mm_ref);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -582,11 +584,12 @@</span> <span class="p_context"> int proc_pid_statm(struct seq_file *m, struct pid_namespace *ns,</span>
 			struct pid *pid, struct task_struct *task)
 {
 	unsigned long size = 0, resident = 0, shared = 0, text = 0, data = 0;
<span class="p_del">-	struct mm_struct *mm = get_task_mm(task);</span>
<span class="p_add">+	MM_REF(mm_ref);</span>
<span class="p_add">+	struct mm_struct *mm = get_task_mm(task, &amp;mm_ref);</span>
 
 	if (mm) {
 		size = task_statm(mm, &amp;shared, &amp;text, &amp;data, &amp;resident);
<span class="p_del">-		mmput(mm);</span>
<span class="p_add">+		mmput(mm, &amp;mm_ref);</span>
 	}
 	/*
 	 * For quick read, open code by putting numbers directly
<span class="p_header">diff --git a/fs/proc/base.c b/fs/proc/base.c</span>
<span class="p_header">index 87fd5bf07578..9c8bbfc0ab45 100644</span>
<span class="p_header">--- a/fs/proc/base.c</span>
<span class="p_header">+++ b/fs/proc/base.c</span>
<span class="p_chunk">@@ -201,6 +201,7 @@</span> <span class="p_context"> static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,</span>
 {
 	struct task_struct *tsk;
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	char *page;
 	unsigned long count = _count;
 	unsigned long arg_start, arg_end, env_start, env_end;
<span class="p_chunk">@@ -214,7 +215,7 @@</span> <span class="p_context"> static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,</span>
 	tsk = get_proc_task(file_inode(file));
 	if (!tsk)
 		return -ESRCH;
<span class="p_del">-	mm = get_task_mm(tsk);</span>
<span class="p_add">+	mm = get_task_mm(tsk, &amp;mm_ref);</span>
 	put_task_struct(tsk);
 	if (!mm)
 		return 0;
<span class="p_chunk">@@ -389,7 +390,7 @@</span> <span class="p_context"> static ssize_t proc_pid_cmdline_read(struct file *file, char __user *buf,</span>
 out_free_page:
 	free_page((unsigned long)page);
 out_mmput:
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 	if (rv &gt; 0)
 		*pos += rv;
 	return rv;
<span class="p_chunk">@@ -784,34 +785,50 @@</span> <span class="p_context"> static const struct file_operations proc_single_file_operations = {</span>
 };
 
 
<span class="p_del">-struct mm_struct *proc_mem_open(struct inode *inode, unsigned int mode)</span>
<span class="p_add">+struct mm_struct *proc_mem_open(struct inode *inode, unsigned int mode, struct mm_ref *mm_ref)</span>
 {
 	struct task_struct *task = get_proc_task(inode);
 	struct mm_struct *mm = ERR_PTR(-ESRCH);
<span class="p_add">+	MM_REF(tmp_ref);</span>
 
 	if (task) {
<span class="p_del">-		mm = mm_access(task, mode | PTRACE_MODE_FSCREDS);</span>
<span class="p_add">+		mm = mm_access(task, mode | PTRACE_MODE_FSCREDS, &amp;tmp_ref);</span>
 		put_task_struct(task);
 
 		if (!IS_ERR_OR_NULL(mm)) {
 			/* ensure this mm_struct can&#39;t be freed */
<span class="p_del">-			mmgrab(mm);</span>
<span class="p_add">+			mmgrab(mm, mm_ref);</span>
 			/* but do not pin its memory */
<span class="p_del">-			mmput(mm);</span>
<span class="p_add">+			mmput(mm, &amp;tmp_ref);</span>
 		}
 	}
 
 	return mm;
 }
 
<span class="p_add">+struct mem_private {</span>
<span class="p_add">+	struct mm_struct *mm;</span>
<span class="p_add">+	struct mm_ref mm_ref;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 static int __mem_open(struct inode *inode, struct file *file, unsigned int mode)
 {
<span class="p_del">-	struct mm_struct *mm = proc_mem_open(inode, mode);</span>
<span class="p_add">+	struct mem_private *priv;</span>
<span class="p_add">+	struct mm_struct *mm;</span>
 
<span class="p_del">-	if (IS_ERR(mm))</span>
<span class="p_add">+	priv = kmalloc(sizeof(struct mem_private), GFP_KERNEL);</span>
<span class="p_add">+	if (!priv)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	INIT_MM_REF(&amp;priv-&gt;mm_ref);</span>
<span class="p_add">+	mm = proc_mem_open(inode, mode, &amp;priv-&gt;mm_ref);</span>
<span class="p_add">+	if (IS_ERR(mm)) {</span>
<span class="p_add">+		kfree(priv);</span>
 		return PTR_ERR(mm);
<span class="p_add">+	}</span>
 
<span class="p_del">-	file-&gt;private_data = mm;</span>
<span class="p_add">+	priv-&gt;mm = mm;</span>
<span class="p_add">+	file-&gt;private_data = priv;</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -828,7 +845,9 @@</span> <span class="p_context"> static int mem_open(struct inode *inode, struct file *file)</span>
 static ssize_t mem_rw(struct file *file, char __user *buf,
 			size_t count, loff_t *ppos, int write)
 {
<span class="p_del">-	struct mm_struct *mm = file-&gt;private_data;</span>
<span class="p_add">+	struct mem_private *priv = file-&gt;private_data;</span>
<span class="p_add">+	struct mm_struct *mm = priv-&gt;mm;</span>
<span class="p_add">+	MM_REF(mm_ref);</span>
 	unsigned long addr = *ppos;
 	ssize_t copied;
 	char *page;
<span class="p_chunk">@@ -842,7 +861,7 @@</span> <span class="p_context"> static ssize_t mem_rw(struct file *file, char __user *buf,</span>
 		return -ENOMEM;
 
 	copied = 0;
<span class="p_del">-	if (!mmget_not_zero(mm))</span>
<span class="p_add">+	if (!mmget_not_zero(mm, &amp;mm_ref))</span>
 		goto free;
 
 	/* Maybe we should limit FOLL_FORCE to actual ptrace users? */
<span class="p_chunk">@@ -877,7 +896,7 @@</span> <span class="p_context"> static ssize_t mem_rw(struct file *file, char __user *buf,</span>
 	}
 	*ppos = addr;
 
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 free:
 	free_page((unsigned long) page);
 	return copied;
<span class="p_chunk">@@ -913,9 +932,11 @@</span> <span class="p_context"> loff_t mem_lseek(struct file *file, loff_t offset, int orig)</span>
 
 static int mem_release(struct inode *inode, struct file *file)
 {
<span class="p_del">-	struct mm_struct *mm = file-&gt;private_data;</span>
<span class="p_add">+	struct mem_private *priv = file-&gt;private_data;</span>
<span class="p_add">+	struct mm_struct *mm = priv-&gt;mm;</span>
 	if (mm)
<span class="p_del">-		mmdrop(mm);</span>
<span class="p_add">+		mmdrop(mm, &amp;priv-&gt;mm_ref);</span>
<span class="p_add">+	kfree(priv);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -935,10 +956,12 @@</span> <span class="p_context"> static int environ_open(struct inode *inode, struct file *file)</span>
 static ssize_t environ_read(struct file *file, char __user *buf,
 			size_t count, loff_t *ppos)
 {
<span class="p_add">+	struct mem_private *priv = file-&gt;private_data;</span>
 	char *page;
 	unsigned long src = *ppos;
 	int ret = 0;
<span class="p_del">-	struct mm_struct *mm = file-&gt;private_data;</span>
<span class="p_add">+	struct mm_struct *mm = priv-&gt;mm;</span>
<span class="p_add">+	MM_REF(mm_ref);</span>
 	unsigned long env_start, env_end;
 
 	/* Ensure the process spawned far enough to have an environment. */
<span class="p_chunk">@@ -950,7 +973,7 @@</span> <span class="p_context"> static ssize_t environ_read(struct file *file, char __user *buf,</span>
 		return -ENOMEM;
 
 	ret = 0;
<span class="p_del">-	if (!mmget_not_zero(mm))</span>
<span class="p_add">+	if (!mmget_not_zero(mm, &amp;mm_ref))</span>
 		goto free;
 
 	down_read(&amp;mm-&gt;mmap_sem);
<span class="p_chunk">@@ -988,7 +1011,7 @@</span> <span class="p_context"> static ssize_t environ_read(struct file *file, char __user *buf,</span>
 		count -= retval;
 	}
 	*ppos = src;
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 
 free:
 	free_page((unsigned long) page);
<span class="p_chunk">@@ -1010,7 +1033,8 @@</span> <span class="p_context"> static int auxv_open(struct inode *inode, struct file *file)</span>
 static ssize_t auxv_read(struct file *file, char __user *buf,
 			size_t count, loff_t *ppos)
 {
<span class="p_del">-	struct mm_struct *mm = file-&gt;private_data;</span>
<span class="p_add">+	struct mem_private *priv = file-&gt;private_data;</span>
<span class="p_add">+	struct mm_struct *mm = priv-&gt;mm;</span>
 	unsigned int nwords = 0;
 
 	if (!mm)
<span class="p_chunk">@@ -1053,6 +1077,7 @@</span> <span class="p_context"> static int __set_oom_adj(struct file *file, int oom_adj, bool legacy)</span>
 {
 	static DEFINE_MUTEX(oom_adj_mutex);
 	struct mm_struct *mm = NULL;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	struct task_struct *task;
 	int err = 0;
 
<span class="p_chunk">@@ -1093,7 +1118,7 @@</span> <span class="p_context"> static int __set_oom_adj(struct file *file, int oom_adj, bool legacy)</span>
 		if (p) {
 			if (atomic_read(&amp;p-&gt;mm-&gt;mm_users) &gt; 1) {
 				mm = p-&gt;mm;
<span class="p_del">-				mmgrab(mm);</span>
<span class="p_add">+				mmgrab(mm, &amp;mm_ref);</span>
 			}
 			task_unlock(p);
 		}
<span class="p_chunk">@@ -1129,7 +1154,7 @@</span> <span class="p_context"> static int __set_oom_adj(struct file *file, int oom_adj, bool legacy)</span>
 			task_unlock(p);
 		}
 		rcu_read_unlock();
<span class="p_del">-		mmdrop(mm);</span>
<span class="p_add">+		mmdrop(mm, &amp;mm_ref);</span>
 	}
 err_unlock:
 	mutex_unlock(&amp;oom_adj_mutex);
<span class="p_chunk">@@ -1875,6 +1900,7 @@</span> <span class="p_context"> static int map_files_d_revalidate(struct dentry *dentry, unsigned int flags)</span>
 	unsigned long vm_start, vm_end;
 	bool exact_vma_exists = false;
 	struct mm_struct *mm = NULL;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	struct task_struct *task;
 	const struct cred *cred;
 	struct inode *inode;
<span class="p_chunk">@@ -1888,7 +1914,7 @@</span> <span class="p_context"> static int map_files_d_revalidate(struct dentry *dentry, unsigned int flags)</span>
 	if (!task)
 		goto out_notask;
 
<span class="p_del">-	mm = mm_access(task, PTRACE_MODE_READ_FSCREDS);</span>
<span class="p_add">+	mm = mm_access(task, PTRACE_MODE_READ_FSCREDS, &amp;mm_ref);</span>
 	if (IS_ERR_OR_NULL(mm))
 		goto out;
 
<span class="p_chunk">@@ -1898,7 +1924,7 @@</span> <span class="p_context"> static int map_files_d_revalidate(struct dentry *dentry, unsigned int flags)</span>
 		up_read(&amp;mm-&gt;mmap_sem);
 	}
 
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 
 	if (exact_vma_exists) {
 		if (task_dumpable(task)) {
<span class="p_chunk">@@ -1933,6 +1959,7 @@</span> <span class="p_context"> static int map_files_get_link(struct dentry *dentry, struct path *path)</span>
 	struct vm_area_struct *vma;
 	struct task_struct *task;
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	int rc;
 
 	rc = -ENOENT;
<span class="p_chunk">@@ -1940,7 +1967,7 @@</span> <span class="p_context"> static int map_files_get_link(struct dentry *dentry, struct path *path)</span>
 	if (!task)
 		goto out;
 
<span class="p_del">-	mm = get_task_mm(task);</span>
<span class="p_add">+	mm = get_task_mm(task, &amp;mm_ref);</span>
 	put_task_struct(task);
 	if (!mm)
 		goto out;
<span class="p_chunk">@@ -1960,7 +1987,7 @@</span> <span class="p_context"> static int map_files_get_link(struct dentry *dentry, struct path *path)</span>
 	up_read(&amp;mm-&gt;mmap_sem);
 
 out_mmput:
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 out:
 	return rc;
 }
<span class="p_chunk">@@ -2034,6 +2061,7 @@</span> <span class="p_context"> static struct dentry *proc_map_files_lookup(struct inode *dir,</span>
 	struct task_struct *task;
 	int result;
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 
 	result = -ENOENT;
 	task = get_proc_task(dir);
<span class="p_chunk">@@ -2048,7 +2076,7 @@</span> <span class="p_context"> static struct dentry *proc_map_files_lookup(struct inode *dir,</span>
 	if (dname_to_vma_addr(dentry, &amp;vm_start, &amp;vm_end))
 		goto out_put_task;
 
<span class="p_del">-	mm = get_task_mm(task);</span>
<span class="p_add">+	mm = get_task_mm(task, &amp;mm_ref);</span>
 	if (!mm)
 		goto out_put_task;
 
<span class="p_chunk">@@ -2063,7 +2091,7 @@</span> <span class="p_context"> static struct dentry *proc_map_files_lookup(struct inode *dir,</span>
 
 out_no_vma:
 	up_read(&amp;mm-&gt;mmap_sem);
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 out_put_task:
 	put_task_struct(task);
 out:
<span class="p_chunk">@@ -2082,6 +2110,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 	struct vm_area_struct *vma;
 	struct task_struct *task;
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	unsigned long nr_files, pos, i;
 	struct flex_array *fa = NULL;
 	struct map_files_info info;
<span class="p_chunk">@@ -2101,7 +2130,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 	if (!dir_emit_dots(file, ctx))
 		goto out_put_task;
 
<span class="p_del">-	mm = get_task_mm(task);</span>
<span class="p_add">+	mm = get_task_mm(task, &amp;mm_ref);</span>
 	if (!mm)
 		goto out_put_task;
 	down_read(&amp;mm-&gt;mmap_sem);
<span class="p_chunk">@@ -2132,7 +2161,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 			if (fa)
 				flex_array_free(fa);
 			up_read(&amp;mm-&gt;mmap_sem);
<span class="p_del">-			mmput(mm);</span>
<span class="p_add">+			mmput(mm, &amp;mm_ref);</span>
 			goto out_put_task;
 		}
 		for (i = 0, vma = mm-&gt;mmap, pos = 2; vma;
<span class="p_chunk">@@ -2164,7 +2193,7 @@</span> <span class="p_context"> proc_map_files_readdir(struct file *file, struct dir_context *ctx)</span>
 	}
 	if (fa)
 		flex_array_free(fa);
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 
 out_put_task:
 	put_task_struct(task);
<span class="p_chunk">@@ -2567,6 +2596,7 @@</span> <span class="p_context"> static ssize_t proc_coredump_filter_read(struct file *file, char __user *buf,</span>
 {
 	struct task_struct *task = get_proc_task(file_inode(file));
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	char buffer[PROC_NUMBUF];
 	size_t len;
 	int ret;
<span class="p_chunk">@@ -2575,12 +2605,12 @@</span> <span class="p_context"> static ssize_t proc_coredump_filter_read(struct file *file, char __user *buf,</span>
 		return -ESRCH;
 
 	ret = 0;
<span class="p_del">-	mm = get_task_mm(task);</span>
<span class="p_add">+	mm = get_task_mm(task, &amp;mm_ref);</span>
 	if (mm) {
 		len = snprintf(buffer, sizeof(buffer), &quot;%08lx\n&quot;,
 			       ((mm-&gt;flags &amp; MMF_DUMP_FILTER_MASK) &gt;&gt;
 				MMF_DUMP_FILTER_SHIFT));
<span class="p_del">-		mmput(mm);</span>
<span class="p_add">+		mmput(mm, &amp;mm_ref);</span>
 		ret = simple_read_from_buffer(buf, count, ppos, buffer, len);
 	}
 
<span class="p_chunk">@@ -2596,6 +2626,7 @@</span> <span class="p_context"> static ssize_t proc_coredump_filter_write(struct file *file,</span>
 {
 	struct task_struct *task;
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	unsigned int val;
 	int ret;
 	int i;
<span class="p_chunk">@@ -2610,7 +2641,7 @@</span> <span class="p_context"> static ssize_t proc_coredump_filter_write(struct file *file,</span>
 	if (!task)
 		goto out_no_task;
 
<span class="p_del">-	mm = get_task_mm(task);</span>
<span class="p_add">+	mm = get_task_mm(task, &amp;mm_ref);</span>
 	if (!mm)
 		goto out_no_mm;
 	ret = 0;
<span class="p_chunk">@@ -2622,7 +2653,7 @@</span> <span class="p_context"> static ssize_t proc_coredump_filter_write(struct file *file,</span>
 			clear_bit(i + MMF_DUMP_FILTER_SHIFT, &amp;mm-&gt;flags);
 	}
 
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
  out_no_mm:
 	put_task_struct(task);
  out_no_task:
<span class="p_header">diff --git a/fs/proc/internal.h b/fs/proc/internal.h</span>
<span class="p_header">index 5378441ec1b7..9aed2e391b15 100644</span>
<span class="p_header">--- a/fs/proc/internal.h</span>
<span class="p_header">+++ b/fs/proc/internal.h</span>
<span class="p_chunk">@@ -280,6 +280,8 @@</span> <span class="p_context"> struct proc_maps_private {</span>
 	struct inode *inode;
 	struct task_struct *task;
 	struct mm_struct *mm;
<span class="p_add">+	struct mm_ref mm_open_ref;</span>
<span class="p_add">+	struct mm_ref mm_start_ref;</span>
 #ifdef CONFIG_MMU
 	struct vm_area_struct *tail_vma;
 #endif
<span class="p_chunk">@@ -288,7 +290,7 @@</span> <span class="p_context"> struct proc_maps_private {</span>
 #endif
 };
 
<span class="p_del">-struct mm_struct *proc_mem_open(struct inode *inode, unsigned int mode);</span>
<span class="p_add">+struct mm_struct *proc_mem_open(struct inode *inode, unsigned int mode, struct mm_ref *mm_ref);</span>
 
 extern const struct file_operations proc_pid_maps_operations;
 extern const struct file_operations proc_tid_maps_operations;
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index c71975293dc8..06ed5d67dd84 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -133,7 +133,7 @@</span> <span class="p_context"> static void vma_stop(struct proc_maps_private *priv)</span>
 
 	release_task_mempolicy(priv);
 	up_read(&amp;mm-&gt;mmap_sem);
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;priv-&gt;mm_start_ref);</span>
 }
 
 static struct vm_area_struct *
<span class="p_chunk">@@ -167,7 +167,7 @@</span> <span class="p_context"> static void *m_start(struct seq_file *m, loff_t *ppos)</span>
 		return ERR_PTR(-ESRCH);
 
 	mm = priv-&gt;mm;
<span class="p_del">-	if (!mm || !mmget_not_zero(mm))</span>
<span class="p_add">+	if (!mm || !mmget_not_zero(mm, &amp;priv-&gt;mm_start_ref))</span>
 		return NULL;
 
 	down_read(&amp;mm-&gt;mmap_sem);
<span class="p_chunk">@@ -232,7 +232,9 @@</span> <span class="p_context"> static int proc_maps_open(struct inode *inode, struct file *file,</span>
 		return -ENOMEM;
 
 	priv-&gt;inode = inode;
<span class="p_del">-	priv-&gt;mm = proc_mem_open(inode, PTRACE_MODE_READ);</span>
<span class="p_add">+	INIT_MM_REF(&amp;priv-&gt;mm_open_ref);</span>
<span class="p_add">+	INIT_MM_REF(&amp;priv-&gt;mm_start_ref);</span>
<span class="p_add">+	priv-&gt;mm = proc_mem_open(inode, PTRACE_MODE_READ, &amp;priv-&gt;mm_open_ref);</span>
 	if (IS_ERR(priv-&gt;mm)) {
 		int err = PTR_ERR(priv-&gt;mm);
 
<span class="p_chunk">@@ -249,7 +251,7 @@</span> <span class="p_context"> static int proc_map_release(struct inode *inode, struct file *file)</span>
 	struct proc_maps_private *priv = seq-&gt;private;
 
 	if (priv-&gt;mm)
<span class="p_del">-		mmdrop(priv-&gt;mm);</span>
<span class="p_add">+		mmdrop(priv-&gt;mm, &amp;priv-&gt;mm_open_ref);</span>
 
 	return seq_release_private(inode, file);
 }
<span class="p_chunk">@@ -997,6 +999,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 	struct task_struct *task;
 	char buffer[PROC_NUMBUF];
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	struct vm_area_struct *vma;
 	enum clear_refs_types type;
 	int itype;
<span class="p_chunk">@@ -1017,7 +1020,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 	task = get_proc_task(file_inode(file));
 	if (!task)
 		return -ESRCH;
<span class="p_del">-	mm = get_task_mm(task);</span>
<span class="p_add">+	mm = get_task_mm(task, &amp;mm_ref);</span>
 	if (mm) {
 		struct clear_refs_private cp = {
 			.type = type,
<span class="p_chunk">@@ -1069,7 +1072,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 		flush_tlb_mm(mm);
 		up_read(&amp;mm-&gt;mmap_sem);
 out_mm:
<span class="p_del">-		mmput(mm);</span>
<span class="p_add">+		mmput(mm, &amp;mm_ref);</span>
 	}
 	put_task_struct(task);
 
<span class="p_chunk">@@ -1340,10 +1343,17 @@</span> <span class="p_context"> static int pagemap_hugetlb_range(pte_t *ptep, unsigned long hmask,</span>
  * determine which areas of memory are actually mapped and llseek to
  * skip over unmapped regions.
  */
<span class="p_add">+struct pagemap_private {</span>
<span class="p_add">+	struct mm_struct *mm;</span>
<span class="p_add">+	struct mm_ref mm_ref;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 static ssize_t pagemap_read(struct file *file, char __user *buf,
 			    size_t count, loff_t *ppos)
 {
<span class="p_del">-	struct mm_struct *mm = file-&gt;private_data;</span>
<span class="p_add">+	struct pagemap_private *priv = file-&gt;private_data;</span>
<span class="p_add">+	struct mm_struct *mm = priv-&gt;mm;</span>
<span class="p_add">+	MM_REF(mm_ref);</span>
 	struct pagemapread pm;
 	struct mm_walk pagemap_walk = {};
 	unsigned long src;
<span class="p_chunk">@@ -1352,7 +1362,7 @@</span> <span class="p_context"> static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
 	unsigned long end_vaddr;
 	int ret = 0, copied = 0;
 
<span class="p_del">-	if (!mm || !mmget_not_zero(mm))</span>
<span class="p_add">+	if (!mm || !mmget_not_zero(mm, &amp;mm_ref))</span>
 		goto out;
 
 	ret = -EINVAL;
<span class="p_chunk">@@ -1427,28 +1437,40 @@</span> <span class="p_context"> static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
 out_free:
 	kfree(pm.buffer);
 out_mm:
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 out:
 	return ret;
 }
 
 static int pagemap_open(struct inode *inode, struct file *file)
 {
<span class="p_add">+	struct pagemap_private *priv;</span>
 	struct mm_struct *mm;
 
<span class="p_del">-	mm = proc_mem_open(inode, PTRACE_MODE_READ);</span>
<span class="p_del">-	if (IS_ERR(mm))</span>
<span class="p_add">+	priv = kmalloc(sizeof(struct pagemap_private), GFP_KERNEL);</span>
<span class="p_add">+	if (!priv)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	mm = proc_mem_open(inode, PTRACE_MODE_READ, &amp;priv-&gt;mm_ref);</span>
<span class="p_add">+	if (IS_ERR(mm)) {</span>
<span class="p_add">+		kfree(priv);</span>
 		return PTR_ERR(mm);
<span class="p_del">-	file-&gt;private_data = mm;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	priv-&gt;mm = mm;</span>
<span class="p_add">+	file-&gt;private_data = priv;</span>
 	return 0;
 }
 
 static int pagemap_release(struct inode *inode, struct file *file)
 {
<span class="p_del">-	struct mm_struct *mm = file-&gt;private_data;</span>
<span class="p_add">+	struct pagemap_private *priv = file-&gt;private_data;</span>
<span class="p_add">+	struct mm_struct *mm = priv-&gt;mm;</span>
 
 	if (mm)
<span class="p_del">-		mmdrop(mm);</span>
<span class="p_add">+		mmdrop(mm, &amp;priv-&gt;mm_ref);</span>
<span class="p_add">+</span>
<span class="p_add">+	kfree(priv);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/include/linux/binfmts.h b/include/linux/binfmts.h</span>
<span class="p_header">index 1303b570b18c..8bee41838bd5 100644</span>
<span class="p_header">--- a/include/linux/binfmts.h</span>
<span class="p_header">+++ b/include/linux/binfmts.h</span>
<span class="p_chunk">@@ -21,6 +21,7 @@</span> <span class="p_context"> struct linux_binprm {</span>
 	struct page *page[MAX_ARG_PAGES];
 #endif
 	struct mm_struct *mm;
<span class="p_add">+	struct mm_ref mm_ref;</span>
 	unsigned long p; /* current top of mem */
 	unsigned int
 		cred_prepared:1,/* true if creds already prepared (multiple
<span class="p_header">diff --git a/include/linux/init_task.h b/include/linux/init_task.h</span>
<span class="p_header">index 325f649d77ff..02c9ecf243d1 100644</span>
<span class="p_header">--- a/include/linux/init_task.h</span>
<span class="p_header">+++ b/include/linux/init_task.h</span>
<span class="p_chunk">@@ -211,6 +211,7 @@</span> <span class="p_context"> extern struct task_group root_task_group;</span>
 	.cpus_allowed	= CPU_MASK_ALL,					\
 	.nr_cpus_allowed= NR_CPUS,					\
 	.mm		= NULL,						\
<span class="p_add">+	.mm_ref		= MM_REF_INIT(tsk.mm_ref),			\</span>
 	.active_mm	= &amp;init_mm,					\
 	.restart_block = {						\
 		.fn = do_no_restart_syscall,				\
<span class="p_header">diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h</span>
<span class="p_header">index 01c0b9cc3915..635d4a84f03b 100644</span>
<span class="p_header">--- a/include/linux/kvm_host.h</span>
<span class="p_header">+++ b/include/linux/kvm_host.h</span>
<span class="p_chunk">@@ -174,6 +174,7 @@</span> <span class="p_context"> struct kvm_async_pf {</span>
 	struct list_head queue;
 	struct kvm_vcpu *vcpu;
 	struct mm_struct *mm;
<span class="p_add">+	struct mm_ref mm_ref;</span>
 	gva_t gva;
 	unsigned long addr;
 	struct kvm_arch_async_pf arch;
<span class="p_chunk">@@ -376,6 +377,7 @@</span> <span class="p_context"> struct kvm {</span>
 	spinlock_t mmu_lock;
 	struct mutex slots_lock;
 	struct mm_struct *mm; /* userspace tied to this vm */
<span class="p_add">+	struct mm_ref mm_ref;</span>
 	struct kvm_memslots *memslots[KVM_ADDRESS_SPACE_NUM];
 	struct srcu_struct srcu;
 	struct srcu_struct irq_srcu;
<span class="p_chunk">@@ -424,6 +426,7 @@</span> <span class="p_context"> struct kvm {</span>
 
 #if defined(CONFIG_MMU_NOTIFIER) &amp;&amp; defined(KVM_ARCH_WANT_MMU_NOTIFIER)
 	struct mmu_notifier mmu_notifier;
<span class="p_add">+	struct mm_ref mmu_notifier_ref;</span>
 	unsigned long mmu_notifier_seq;
 	long mmu_notifier_count;
 #endif
<span class="p_header">diff --git a/include/linux/mm_ref.h b/include/linux/mm_ref.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0de29bd64542</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/mm_ref.h</span>
<span class="p_chunk">@@ -0,0 +1,48 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef LINUX_MM_REF_H</span>
<span class="p_add">+#define LINUX_MM_REF_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm_types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm_ref_types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct mm_struct;</span>
<span class="p_add">+</span>
<span class="p_add">+extern void INIT_MM_REF(struct mm_ref *ref);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void _get_mm_ref(struct mm_struct *mm, struct list_head *list,</span>
<span class="p_add">+	struct mm_ref *ref);</span>
<span class="p_add">+extern void _put_mm_ref(struct mm_struct *mm, struct list_head *list,</span>
<span class="p_add">+	struct mm_ref *ref);</span>
<span class="p_add">+extern void _move_mm_ref(struct mm_struct *mm, struct list_head *list,</span>
<span class="p_add">+	struct mm_ref *old_ref, struct mm_ref *new_ref);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void get_mm_ref(struct mm_struct *mm, struct mm_ref *ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	_get_mm_ref(mm, &amp;mm-&gt;mm_count_list, ref);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void put_mm_ref(struct mm_struct *mm, struct mm_ref *ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	_put_mm_ref(mm, &amp;mm-&gt;mm_count_list, ref);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void move_mm_ref(struct mm_struct *mm, struct mm_ref *old_ref, struct mm_ref *new_ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	_move_mm_ref(mm, &amp;mm-&gt;mm_count_list, old_ref, new_ref);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void get_mm_users_ref(struct mm_struct *mm, struct mm_ref *ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	_get_mm_ref(mm, &amp;mm-&gt;mm_users_list, ref);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void put_mm_users_ref(struct mm_struct *mm, struct mm_ref *ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	_put_mm_ref(mm, &amp;mm-&gt;mm_users_list, ref);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void move_mm_users_ref(struct mm_struct *mm, struct mm_ref *old_ref, struct mm_ref *new_ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	_move_mm_ref(mm, &amp;mm-&gt;mm_users_list, old_ref, new_ref);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/include/linux/mm_ref_types.h b/include/linux/mm_ref_types.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5c45995688bd</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/mm_ref_types.h</span>
<span class="p_chunk">@@ -0,0 +1,41 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef LINUX_MM_REF_TYPES_H</span>
<span class="p_add">+#define LINUX_MM_REF_TYPES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/list.h&gt;</span>
<span class="p_add">+#include &lt;linux/stacktrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define NR_MM_REF_STACK_ENTRIES 10</span>
<span class="p_add">+</span>
<span class="p_add">+enum mm_ref_state {</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Pick 0 as uninitialized so we have a chance at catching</span>
<span class="p_add">+	 * uninitialized references by noticing that they are zero.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The rest are random 32-bit integers.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	MM_REF_UNINITIALIZED	= 0,</span>
<span class="p_add">+	MM_REF_INITIALIZED	= 0x28076894UL,</span>
<span class="p_add">+	MM_REF_ACTIVE		= 0xdaf46189UL,</span>
<span class="p_add">+	MM_REF_INACTIVE		= 0xf5358bafUL,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct mm_ref {</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * See -&gt;mm_users_list/-&gt;mm_count_list in struct mm_struct.</span>
<span class="p_add">+	 * Access is protected by -&gt;mm_refs_lock.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct list_head list_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	enum mm_ref_state state;</span>
<span class="p_add">+	int pid;</span>
<span class="p_add">+	struct stack_trace trace;</span>
<span class="p_add">+	unsigned long trace_entries[NR_MM_REF_STACK_ENTRIES];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define MM_REF_INIT(name) \</span>
<span class="p_add">+	{ LIST_HEAD_INIT(name.list_entry), MM_REF_INITIALIZED, }</span>
<span class="p_add">+</span>
<span class="p_add">+#define MM_REF(name) \</span>
<span class="p_add">+	struct mm_ref name = MM_REF_INIT(name)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index 4a8acedf4b7d..520cde63305d 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -13,6 +13,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/uprobes.h&gt;
 #include &lt;linux/page-flags-layout.h&gt;
 #include &lt;linux/workqueue.h&gt;
<span class="p_add">+#include &lt;linux/mm_ref_types.h&gt;</span>
 #include &lt;asm/page.h&gt;
 #include &lt;asm/mmu.h&gt;
 
<span class="p_chunk">@@ -407,8 +408,14 @@</span> <span class="p_context"> struct mm_struct {</span>
 	unsigned long task_size;		/* size of task vm space */
 	unsigned long highest_vm_end;		/* highest vma end address */
 	pgd_t * pgd;
<span class="p_add">+</span>
<span class="p_add">+	spinlock_t mm_refs_lock;		/* Protects mm_users_list and mm_count_list */</span>
 	atomic_t mm_users;			/* How many users with user space? */
<span class="p_add">+	struct list_head mm_users_list;</span>
<span class="p_add">+	struct mm_ref mm_users_ref;</span>
 	atomic_t mm_count;			/* How many references to &quot;struct mm_struct&quot; (users count as 1) */
<span class="p_add">+	struct list_head mm_count_list;</span>
<span class="p_add">+</span>
 	atomic_long_t nr_ptes;			/* PTE page table pages */
 #if CONFIG_PGTABLE_LEVELS &gt; 2
 	atomic_long_t nr_pmds;			/* PMD page table pages */
<span class="p_chunk">@@ -516,6 +523,7 @@</span> <span class="p_context"> struct mm_struct {</span>
 	atomic_long_t hugetlb_usage;
 #endif
 	struct work_struct async_put_work;
<span class="p_add">+	struct mm_ref async_put_ref;</span>
 };
 
 static inline void mm_init_cpumask(struct mm_struct *mm)
<span class="p_header">diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="p_header">index a1a210d59961..e67867bec2d1 100644</span>
<span class="p_header">--- a/include/linux/mmu_notifier.h</span>
<span class="p_header">+++ b/include/linux/mmu_notifier.h</span>
<span class="p_chunk">@@ -201,13 +201,13 @@</span> <span class="p_context"> static inline int mm_has_notifiers(struct mm_struct *mm)</span>
 }
 
 extern int mmu_notifier_register(struct mmu_notifier *mn,
<span class="p_del">-				 struct mm_struct *mm);</span>
<span class="p_add">+				 struct mm_struct *mm, struct mm_ref *mm_ref);</span>
 extern int __mmu_notifier_register(struct mmu_notifier *mn,
<span class="p_del">-				   struct mm_struct *mm);</span>
<span class="p_add">+				   struct mm_struct *mm, struct mm_ref *mm_ref);</span>
 extern void mmu_notifier_unregister(struct mmu_notifier *mn,
<span class="p_del">-				    struct mm_struct *mm);</span>
<span class="p_add">+				    struct mm_struct *mm, struct mm_ref *mm_ref);</span>
 extern void mmu_notifier_unregister_no_release(struct mmu_notifier *mn,
<span class="p_del">-					       struct mm_struct *mm);</span>
<span class="p_add">+					       struct mm_struct *mm, struct mm_ref *ref);</span>
 extern void __mmu_notifier_mm_destroy(struct mm_struct *mm);
 extern void __mmu_notifier_release(struct mm_struct *mm);
 extern int __mmu_notifier_clear_flush_young(struct mm_struct *mm,
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 2ca3e15dad3b..293c64a15dfa 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -25,6 +25,7 @@</span> <span class="p_context"> struct sched_param {</span>
 #include &lt;linux/errno.h&gt;
 #include &lt;linux/nodemask.h&gt;
 #include &lt;linux/mm_types.h&gt;
<span class="p_add">+#include &lt;linux/mm_ref.h&gt;</span>
 #include &lt;linux/preempt.h&gt;
 
 #include &lt;asm/page.h&gt;
<span class="p_chunk">@@ -808,6 +809,7 @@</span> <span class="p_context"> struct signal_struct {</span>
 					 * Only settable by CAP_SYS_RESOURCE. */
 	struct mm_struct *oom_mm;	/* recorded mm when the thread group got
 					 * killed by the oom killer */
<span class="p_add">+	struct mm_ref oom_mm_ref;</span>
 
 	struct mutex cred_guard_mutex;	/* guard against foreign influences on
 					 * credential calculations
<span class="p_chunk">@@ -1546,7 +1548,16 @@</span> <span class="p_context"> struct task_struct {</span>
 	struct rb_node pushable_dl_tasks;
 #endif
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * -&gt;mm and -&gt;active_mm share the mm_ref. Not ideal IMHO, but that&#39;s</span>
<span class="p_add">+	 * how it&#39;s done. For kernel threads, -&gt;mm == NULL, and for user</span>
<span class="p_add">+	 * threads, -&gt;mm == -&gt;active_mm, so we only need one reference.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * See &lt;Documentation/vm/active_mm.txt&gt; for more information.</span>
<span class="p_add">+	 */</span>
 	struct mm_struct *mm, *active_mm;
<span class="p_add">+	struct mm_ref mm_ref;</span>
<span class="p_add">+</span>
 	/* per-thread vma caching */
 	u32 vmacache_seqnum;
 	struct vm_area_struct *vmacache[VMACACHE_SIZE];
<span class="p_chunk">@@ -2639,6 +2650,7 @@</span> <span class="p_context"> extern union thread_union init_thread_union;</span>
 extern struct task_struct init_task;
 
 extern struct   mm_struct init_mm;
<span class="p_add">+extern struct mm_ref init_mm_ref;</span>
 
 extern struct pid_namespace init_pid_ns;
 
<span class="p_chunk">@@ -2870,17 +2882,19 @@</span> <span class="p_context"> static inline unsigned long sigsp(unsigned long sp, struct ksignal *ksig)</span>
 /*
  * Routines for handling mm_structs
  */
<span class="p_del">-extern struct mm_struct * mm_alloc(void);</span>
<span class="p_add">+extern struct mm_struct * mm_alloc(struct mm_ref *ref);</span>
 
<span class="p_del">-static inline void mmgrab(struct mm_struct *mm)</span>
<span class="p_add">+static inline void mmgrab(struct mm_struct *mm, struct mm_ref *ref)</span>
 {
 	atomic_inc(&amp;mm-&gt;mm_count);
<span class="p_add">+	get_mm_ref(mm, ref);</span>
 }
 
 /* mmdrop drops the mm and the page tables */
 extern void __mmdrop(struct mm_struct *);
<span class="p_del">-static inline void mmdrop(struct mm_struct *mm)</span>
<span class="p_add">+static inline void mmdrop(struct mm_struct *mm, struct mm_ref *ref)</span>
 {
<span class="p_add">+	put_mm_ref(mm, ref);</span>
 	if (unlikely(atomic_dec_and_test(&amp;mm-&gt;mm_count)))
 		__mmdrop(mm);
 }
<span class="p_chunk">@@ -2891,41 +2905,47 @@</span> <span class="p_context"> static inline void mmdrop_async_fn(struct work_struct *work)</span>
 	__mmdrop(mm);
 }
 
<span class="p_del">-static inline void mmdrop_async(struct mm_struct *mm)</span>
<span class="p_add">+static inline void mmdrop_async(struct mm_struct *mm, struct mm_ref *ref)</span>
 {
<span class="p_add">+	put_mm_ref(mm, ref);</span>
 	if (unlikely(atomic_dec_and_test(&amp;mm-&gt;mm_count))) {
 		INIT_WORK(&amp;mm-&gt;async_put_work, mmdrop_async_fn);
 		schedule_work(&amp;mm-&gt;async_put_work);
 	}
 }
 
<span class="p_del">-static inline void mmget(struct mm_struct *mm)</span>
<span class="p_add">+static inline void mmget(struct mm_struct *mm, struct mm_ref *ref)</span>
 {
 	atomic_inc(&amp;mm-&gt;mm_users);
<span class="p_add">+	get_mm_users_ref(mm, ref);</span>
 }
 
<span class="p_del">-static inline bool mmget_not_zero(struct mm_struct *mm)</span>
<span class="p_add">+static inline bool mmget_not_zero(struct mm_struct *mm, struct mm_ref *ref)</span>
 {
<span class="p_del">-	return atomic_inc_not_zero(&amp;mm-&gt;mm_users);</span>
<span class="p_add">+	bool not_zero = atomic_inc_not_zero(&amp;mm-&gt;mm_users);</span>
<span class="p_add">+	if (not_zero)</span>
<span class="p_add">+		get_mm_users_ref(mm, ref);</span>
<span class="p_add">+</span>
<span class="p_add">+	return not_zero;</span>
 }
 
 /* mmput gets rid of the mappings and all user-space */
<span class="p_del">-extern void mmput(struct mm_struct *);</span>
<span class="p_add">+extern void mmput(struct mm_struct *, struct mm_ref *);</span>
 #ifdef CONFIG_MMU
 /* same as above but performs the slow path from the async context. Can
  * be called from the atomic context as well
  */
<span class="p_del">-extern void mmput_async(struct mm_struct *);</span>
<span class="p_add">+extern void mmput_async(struct mm_struct *, struct mm_ref *ref);</span>
 #endif
 
 /* Grab a reference to a task&#39;s mm, if it is not already going away */
<span class="p_del">-extern struct mm_struct *get_task_mm(struct task_struct *task);</span>
<span class="p_add">+extern struct mm_struct *get_task_mm(struct task_struct *task, struct mm_ref *mm_ref);</span>
 /*
  * Grab a reference to a task&#39;s mm, if it is not already going away
  * and ptrace_may_access with the mode parameter passed to it
  * succeeds.
  */
<span class="p_del">-extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode);</span>
<span class="p_add">+extern struct mm_struct *mm_access(struct task_struct *task, unsigned int mode, struct mm_ref *mm_ref);</span>
 /* Remove the current tasks stale references to the old mm_struct */
 extern void mm_release(struct task_struct *, struct mm_struct *);
 
<span class="p_header">diff --git a/kernel/cpuset.c b/kernel/cpuset.c</span>
<span class="p_header">index 29f815d2ef7e..66c5778f4052 100644</span>
<span class="p_header">--- a/kernel/cpuset.c</span>
<span class="p_header">+++ b/kernel/cpuset.c</span>
<span class="p_chunk">@@ -994,6 +994,7 @@</span> <span class="p_context"> static int update_cpumask(struct cpuset *cs, struct cpuset *trialcs,</span>
 struct cpuset_migrate_mm_work {
 	struct work_struct	work;
 	struct mm_struct	*mm;
<span class="p_add">+	struct mm_ref		mm_ref;</span>
 	nodemask_t		from;
 	nodemask_t		to;
 };
<span class="p_chunk">@@ -1005,24 +1006,25 @@</span> <span class="p_context"> static void cpuset_migrate_mm_workfn(struct work_struct *work)</span>
 
 	/* on a wq worker, no need to worry about %current&#39;s mems_allowed */
 	do_migrate_pages(mwork-&gt;mm, &amp;mwork-&gt;from, &amp;mwork-&gt;to, MPOL_MF_MOVE_ALL);
<span class="p_del">-	mmput(mwork-&gt;mm);</span>
<span class="p_add">+	mmput(mwork-&gt;mm, &amp;mwork-&gt;mm_ref);</span>
 	kfree(mwork);
 }
 
 static void cpuset_migrate_mm(struct mm_struct *mm, const nodemask_t *from,
<span class="p_del">-							const nodemask_t *to)</span>
<span class="p_add">+							const nodemask_t *to, struct mm_ref *mm_ref)</span>
 {
 	struct cpuset_migrate_mm_work *mwork;
 
 	mwork = kzalloc(sizeof(*mwork), GFP_KERNEL);
 	if (mwork) {
 		mwork-&gt;mm = mm;
<span class="p_add">+		move_mm_users_ref(mm, mm_ref, &amp;mwork-&gt;mm_ref);</span>
 		mwork-&gt;from = *from;
 		mwork-&gt;to = *to;
 		INIT_WORK(&amp;mwork-&gt;work, cpuset_migrate_mm_workfn);
 		queue_work(cpuset_migrate_mm_wq, &amp;mwork-&gt;work);
 	} else {
<span class="p_del">-		mmput(mm);</span>
<span class="p_add">+		mmput(mm, mm_ref);</span>
 	}
 }
 
<span class="p_chunk">@@ -1107,11 +1109,12 @@</span> <span class="p_context"> static void update_tasks_nodemask(struct cpuset *cs)</span>
 	css_task_iter_start(&amp;cs-&gt;css, &amp;it);
 	while ((task = css_task_iter_next(&amp;it))) {
 		struct mm_struct *mm;
<span class="p_add">+		MM_REF(mm_ref);</span>
 		bool migrate;
 
 		cpuset_change_task_nodemask(task, &amp;newmems);
 
<span class="p_del">-		mm = get_task_mm(task);</span>
<span class="p_add">+		mm = get_task_mm(task, &amp;mm_ref);</span>
 		if (!mm)
 			continue;
 
<span class="p_chunk">@@ -1119,9 +1122,9 @@</span> <span class="p_context"> static void update_tasks_nodemask(struct cpuset *cs)</span>
 
 		mpol_rebind_mm(mm, &amp;cs-&gt;mems_allowed);
 		if (migrate)
<span class="p_del">-			cpuset_migrate_mm(mm, &amp;cs-&gt;old_mems_allowed, &amp;newmems);</span>
<span class="p_add">+			cpuset_migrate_mm(mm, &amp;cs-&gt;old_mems_allowed, &amp;newmems, &amp;mm_ref);</span>
 		else
<span class="p_del">-			mmput(mm);</span>
<span class="p_add">+			mmput(mm, &amp;mm_ref);</span>
 	}
 	css_task_iter_end(&amp;it);
 
<span class="p_chunk">@@ -1556,7 +1559,8 @@</span> <span class="p_context"> static void cpuset_attach(struct cgroup_taskset *tset)</span>
 	 */
 	cpuset_attach_nodemask_to = cs-&gt;effective_mems;
 	cgroup_taskset_for_each_leader(leader, css, tset) {
<span class="p_del">-		struct mm_struct *mm = get_task_mm(leader);</span>
<span class="p_add">+		MM_REF(mm_ref);</span>
<span class="p_add">+		struct mm_struct *mm = get_task_mm(leader, &amp;mm_ref);</span>
 
 		if (mm) {
 			mpol_rebind_mm(mm, &amp;cpuset_attach_nodemask_to);
<span class="p_chunk">@@ -1571,9 +1575,9 @@</span> <span class="p_context"> static void cpuset_attach(struct cgroup_taskset *tset)</span>
 			 */
 			if (is_memory_migrate(cs))
 				cpuset_migrate_mm(mm, &amp;oldcs-&gt;old_mems_allowed,
<span class="p_del">-						  &amp;cpuset_attach_nodemask_to);</span>
<span class="p_add">+						  &amp;cpuset_attach_nodemask_to, &amp;mm_ref);</span>
 			else
<span class="p_del">-				mmput(mm);</span>
<span class="p_add">+				mmput(mm, &amp;mm_ref);</span>
 		}
 	}
 
<span class="p_header">diff --git a/kernel/events/core.c b/kernel/events/core.c</span>
<span class="p_header">index 02c8421f8c01..2909d6db3b7a 100644</span>
<span class="p_header">--- a/kernel/events/core.c</span>
<span class="p_header">+++ b/kernel/events/core.c</span>
<span class="p_chunk">@@ -7965,6 +7965,7 @@</span> <span class="p_context"> static void perf_event_addr_filters_apply(struct perf_event *event)</span>
 	struct task_struct *task = READ_ONCE(event-&gt;ctx-&gt;task);
 	struct perf_addr_filter *filter;
 	struct mm_struct *mm = NULL;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	unsigned int count = 0;
 	unsigned long flags;
 
<span class="p_chunk">@@ -7975,7 +7976,7 @@</span> <span class="p_context"> static void perf_event_addr_filters_apply(struct perf_event *event)</span>
 	if (task == TASK_TOMBSTONE)
 		return;
 
<span class="p_del">-	mm = get_task_mm(event-&gt;ctx-&gt;task);</span>
<span class="p_add">+	mm = get_task_mm(event-&gt;ctx-&gt;task, &amp;mm_ref);</span>
 	if (!mm)
 		goto restart;
 
<span class="p_chunk">@@ -8001,7 +8002,7 @@</span> <span class="p_context"> static void perf_event_addr_filters_apply(struct perf_event *event)</span>
 
 	up_read(&amp;mm-&gt;mmap_sem);
 
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 
 restart:
 	perf_event_stop(event, 1);
<span class="p_header">diff --git a/kernel/exit.c b/kernel/exit.c</span>
<span class="p_header">index b12753840050..d367ef9bcfe6 100644</span>
<span class="p_header">--- a/kernel/exit.c</span>
<span class="p_header">+++ b/kernel/exit.c</span>
<span class="p_chunk">@@ -462,6 +462,7 @@</span> <span class="p_context"> void mm_update_next_owner(struct mm_struct *mm)</span>
 static void exit_mm(struct task_struct *tsk)
 {
 	struct mm_struct *mm = tsk-&gt;mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	struct core_state *core_state;
 
 	mm_release(tsk, mm);
<span class="p_chunk">@@ -500,7 +501,7 @@</span> <span class="p_context"> static void exit_mm(struct task_struct *tsk)</span>
 		__set_task_state(tsk, TASK_RUNNING);
 		down_read(&amp;mm-&gt;mmap_sem);
 	}
<span class="p_del">-	mmgrab(mm);</span>
<span class="p_add">+	mmgrab(mm, &amp;mm_ref);</span>
 	BUG_ON(mm != tsk-&gt;active_mm);
 	/* more a memory barrier than a real lock */
 	task_lock(tsk);
<span class="p_chunk">@@ -509,7 +510,7 @@</span> <span class="p_context"> static void exit_mm(struct task_struct *tsk)</span>
 	enter_lazy_tlb(mm, current);
 	task_unlock(tsk);
 	mm_update_next_owner(mm);
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 	if (test_thread_flag(TIF_MEMDIE))
 		exit_oom_victim();
 }
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index f9c32dc6ccbc..a431a52375d7 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -367,7 +367,7 @@</span> <span class="p_context"> static inline void free_signal_struct(struct signal_struct *sig)</span>
 	 * pgd_dtor so postpone it to the async context
 	 */
 	if (sig-&gt;oom_mm)
<span class="p_del">-		mmdrop_async(sig-&gt;oom_mm);</span>
<span class="p_add">+		mmdrop_async(sig-&gt;oom_mm, &amp;sig-&gt;oom_mm_ref);</span>
 	kmem_cache_free(signal_cachep, sig);
 }
 
<span class="p_chunk">@@ -745,13 +745,22 @@</span> <span class="p_context"> static void mm_init_owner(struct mm_struct *mm, struct task_struct *p)</span>
 #endif
 }
 
<span class="p_del">-static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)</span>
<span class="p_add">+static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p, struct mm_ref *mm_ref)</span>
 {
 	mm-&gt;mmap = NULL;
 	mm-&gt;mm_rb = RB_ROOT;
 	mm-&gt;vmacache_seqnum = 0;
<span class="p_add">+</span>
 	atomic_set(&amp;mm-&gt;mm_users, 1);
<span class="p_add">+	INIT_LIST_HEAD(&amp;mm-&gt;mm_users_list);</span>
<span class="p_add">+	INIT_MM_REF(&amp;mm-&gt;mm_users_ref);</span>
<span class="p_add">+</span>
 	atomic_set(&amp;mm-&gt;mm_count, 1);
<span class="p_add">+	INIT_LIST_HEAD(&amp;mm-&gt;mm_count_list);</span>
<span class="p_add">+</span>
<span class="p_add">+	get_mm_ref(mm, mm_ref);</span>
<span class="p_add">+	get_mm_users_ref(mm, &amp;mm-&gt;mm_users_ref);</span>
<span class="p_add">+</span>
 	init_rwsem(&amp;mm-&gt;mmap_sem);
 	INIT_LIST_HEAD(&amp;mm-&gt;mmlist);
 	mm-&gt;core_state = NULL;
<span class="p_chunk">@@ -821,7 +830,7 @@</span> <span class="p_context"> static void check_mm(struct mm_struct *mm)</span>
 /*
  * Allocate and initialize an mm_struct.
  */
<span class="p_del">-struct mm_struct *mm_alloc(void)</span>
<span class="p_add">+struct mm_struct *mm_alloc(struct mm_ref *ref)</span>
 {
 	struct mm_struct *mm;
 
<span class="p_chunk">@@ -830,7 +839,7 @@</span> <span class="p_context"> struct mm_struct *mm_alloc(void)</span>
 		return NULL;
 
 	memset(mm, 0, sizeof(*mm));
<span class="p_del">-	return mm_init(mm, current);</span>
<span class="p_add">+	return mm_init(mm, current, ref);</span>
 }
 
 /*
<span class="p_chunk">@@ -868,16 +877,17 @@</span> <span class="p_context"> static inline void __mmput(struct mm_struct *mm)</span>
 	if (mm-&gt;binfmt)
 		module_put(mm-&gt;binfmt-&gt;module);
 	set_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags);
<span class="p_del">-	mmdrop(mm);</span>
<span class="p_add">+	mmdrop(mm, &amp;mm-&gt;mm_users_ref);</span>
 }
 
 /*
  * Decrement the use count and release all resources for an mm.
  */
<span class="p_del">-void mmput(struct mm_struct *mm)</span>
<span class="p_add">+void mmput(struct mm_struct *mm, struct mm_ref *ref)</span>
 {
 	might_sleep();
 
<span class="p_add">+	put_mm_users_ref(mm, ref);</span>
 	if (atomic_dec_and_test(&amp;mm-&gt;mm_users))
 		__mmput(mm);
 }
<span class="p_chunk">@@ -890,8 +900,9 @@</span> <span class="p_context"> static void mmput_async_fn(struct work_struct *work)</span>
 	__mmput(mm);
 }
 
<span class="p_del">-void mmput_async(struct mm_struct *mm)</span>
<span class="p_add">+void mmput_async(struct mm_struct *mm, struct mm_ref *ref)</span>
 {
<span class="p_add">+	put_mm_users_ref(mm, ref);</span>
 	if (atomic_dec_and_test(&amp;mm-&gt;mm_users)) {
 		INIT_WORK(&amp;mm-&gt;async_put_work, mmput_async_fn);
 		schedule_work(&amp;mm-&gt;async_put_work);
<span class="p_chunk">@@ -979,7 +990,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(get_task_exe_file);</span>
  * bumping up the use count.  User must release the mm via mmput()
  * after use.  Typically used by /proc and ptrace.
  */
<span class="p_del">-struct mm_struct *get_task_mm(struct task_struct *task)</span>
<span class="p_add">+struct mm_struct *get_task_mm(struct task_struct *task, struct mm_ref *mm_ref)</span>
 {
 	struct mm_struct *mm;
 
<span class="p_chunk">@@ -989,14 +1000,14 @@</span> <span class="p_context"> struct mm_struct *get_task_mm(struct task_struct *task)</span>
 		if (task-&gt;flags &amp; PF_KTHREAD)
 			mm = NULL;
 		else
<span class="p_del">-			mmget(mm);</span>
<span class="p_add">+			mmget(mm, mm_ref);</span>
 	}
 	task_unlock(task);
 	return mm;
 }
 EXPORT_SYMBOL_GPL(get_task_mm);
 
<span class="p_del">-struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)</span>
<span class="p_add">+struct mm_struct *mm_access(struct task_struct *task, unsigned int mode, struct mm_ref *mm_ref)</span>
 {
 	struct mm_struct *mm;
 	int err;
<span class="p_chunk">@@ -1005,10 +1016,10 @@</span> <span class="p_context"> struct mm_struct *mm_access(struct task_struct *task, unsigned int mode)</span>
 	if (err)
 		return ERR_PTR(err);
 
<span class="p_del">-	mm = get_task_mm(task);</span>
<span class="p_add">+	mm = get_task_mm(task, mm_ref);</span>
 	if (mm &amp;&amp; mm != current-&gt;mm &amp;&amp;
 			!ptrace_may_access(task, mode)) {
<span class="p_del">-		mmput(mm);</span>
<span class="p_add">+		mmput(mm, mm_ref);</span>
 		mm = ERR_PTR(-EACCES);
 	}
 	mutex_unlock(&amp;task-&gt;signal-&gt;cred_guard_mutex);
<span class="p_chunk">@@ -1115,7 +1126,7 @@</span> <span class="p_context"> void mm_release(struct task_struct *tsk, struct mm_struct *mm)</span>
  * Allocate a new mm structure and copy contents from the
  * mm structure of the passed in task structure.
  */
<span class="p_del">-static struct mm_struct *dup_mm(struct task_struct *tsk)</span>
<span class="p_add">+static struct mm_struct *dup_mm(struct task_struct *tsk, struct mm_ref *ref)</span>
 {
 	struct mm_struct *mm, *oldmm = current-&gt;mm;
 	int err;
<span class="p_chunk">@@ -1126,7 +1137,7 @@</span> <span class="p_context"> static struct mm_struct *dup_mm(struct task_struct *tsk)</span>
 
 	memcpy(mm, oldmm, sizeof(*mm));
 
<span class="p_del">-	if (!mm_init(mm, tsk))</span>
<span class="p_add">+	if (!mm_init(mm, tsk, ref))</span>
 		goto fail_nomem;
 
 	err = dup_mmap(mm, oldmm);
<span class="p_chunk">@@ -1144,7 +1155,7 @@</span> <span class="p_context"> static struct mm_struct *dup_mm(struct task_struct *tsk)</span>
 free_pt:
 	/* don&#39;t put binfmt in mmput, we haven&#39;t got module yet */
 	mm-&gt;binfmt = NULL;
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, ref);</span>
 
 fail_nomem:
 	return NULL;
<span class="p_chunk">@@ -1163,6 +1174,7 @@</span> <span class="p_context"> static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)</span>
 
 	tsk-&gt;mm = NULL;
 	tsk-&gt;active_mm = NULL;
<span class="p_add">+	INIT_MM_REF(&amp;tsk-&gt;mm_ref);</span>
 
 	/*
 	 * Are we cloning a kernel thread?
<span class="p_chunk">@@ -1177,13 +1189,13 @@</span> <span class="p_context"> static int copy_mm(unsigned long clone_flags, struct task_struct *tsk)</span>
 	vmacache_flush(tsk);
 
 	if (clone_flags &amp; CLONE_VM) {
<span class="p_del">-		mmget(oldmm);</span>
<span class="p_add">+		mmget(oldmm, &amp;tsk-&gt;mm_ref);</span>
 		mm = oldmm;
 		goto good_mm;
 	}
 
 	retval = -ENOMEM;
<span class="p_del">-	mm = dup_mm(tsk);</span>
<span class="p_add">+	mm = dup_mm(tsk, &amp;tsk-&gt;mm_ref);</span>
 	if (!mm)
 		goto fail_nomem;
 
<span class="p_chunk">@@ -1360,6 +1372,9 @@</span> <span class="p_context"> static int copy_signal(unsigned long clone_flags, struct task_struct *tsk)</span>
 	sig-&gt;oom_score_adj = current-&gt;signal-&gt;oom_score_adj;
 	sig-&gt;oom_score_adj_min = current-&gt;signal-&gt;oom_score_adj_min;
 
<span class="p_add">+	sig-&gt;oom_mm = NULL;</span>
<span class="p_add">+	INIT_MM_REF(&amp;sig-&gt;oom_mm_ref);</span>
<span class="p_add">+</span>
 	sig-&gt;has_child_subreaper = current-&gt;signal-&gt;has_child_subreaper ||
 				   current-&gt;signal-&gt;is_child_subreaper;
 
<span class="p_chunk">@@ -1839,7 +1854,7 @@</span> <span class="p_context"> static __latent_entropy struct task_struct *copy_process(</span>
 	exit_task_namespaces(p);
 bad_fork_cleanup_mm:
 	if (p-&gt;mm)
<span class="p_del">-		mmput(p-&gt;mm);</span>
<span class="p_add">+		mmput(p-&gt;mm, &amp;p-&gt;mm_ref);</span>
 bad_fork_cleanup_signal:
 	if (!(clone_flags &amp; CLONE_THREAD))
 		free_signal_struct(p-&gt;signal);
<span class="p_header">diff --git a/kernel/futex.c b/kernel/futex.c</span>
<span class="p_header">index cbe6056c17c1..3a279ee2166b 100644</span>
<span class="p_header">--- a/kernel/futex.c</span>
<span class="p_header">+++ b/kernel/futex.c</span>
<span class="p_chunk">@@ -240,6 +240,7 @@</span> <span class="p_context"> struct futex_q {</span>
 	struct task_struct *task;
 	spinlock_t *lock_ptr;
 	union futex_key key;
<span class="p_add">+	struct mm_ref mm_ref;</span>
 	struct futex_pi_state *pi_state;
 	struct rt_mutex_waiter *rt_waiter;
 	union futex_key *requeue_pi_key;
<span class="p_chunk">@@ -249,6 +250,7 @@</span> <span class="p_context"> struct futex_q {</span>
 static const struct futex_q futex_q_init = {
 	/* list gets initialized in queue_me()*/
 	.key = FUTEX_KEY_INIT,
<span class="p_add">+	/* .mm_ref must be initialized for each futex_q */</span>
 	.bitset = FUTEX_BITSET_MATCH_ANY
 };
 
<span class="p_chunk">@@ -336,9 +338,9 @@</span> <span class="p_context"> static inline bool should_fail_futex(bool fshared)</span>
 }
 #endif /* CONFIG_FAIL_FUTEX */
 
<span class="p_del">-static inline void futex_get_mm(union futex_key *key)</span>
<span class="p_add">+static inline void futex_get_mm(union futex_key *key, struct mm_ref *ref)</span>
 {
<span class="p_del">-	mmgrab(key-&gt;private.mm);</span>
<span class="p_add">+	mmgrab(key-&gt;private.mm, ref);</span>
 	/*
 	 * Ensure futex_get_mm() implies a full barrier such that
 	 * get_futex_key() implies a full barrier. This is relied upon
<span class="p_chunk">@@ -417,7 +419,7 @@</span> <span class="p_context"> static inline int match_futex(union futex_key *key1, union futex_key *key2)</span>
  * Can be called while holding spinlocks.
  *
  */
<span class="p_del">-static void get_futex_key_refs(union futex_key *key)</span>
<span class="p_add">+static void get_futex_key_refs(union futex_key *key, struct mm_ref *ref)</span>
 {
 	if (!key-&gt;both.ptr)
 		return;
<span class="p_chunk">@@ -437,7 +439,7 @@</span> <span class="p_context"> static void get_futex_key_refs(union futex_key *key)</span>
 		ihold(key-&gt;shared.inode); /* implies smp_mb(); (B) */
 		break;
 	case FUT_OFF_MMSHARED:
<span class="p_del">-		futex_get_mm(key); /* implies smp_mb(); (B) */</span>
<span class="p_add">+		futex_get_mm(key, ref); /* implies smp_mb(); (B) */</span>
 		break;
 	default:
 		/*
<span class="p_chunk">@@ -455,7 +457,7 @@</span> <span class="p_context"> static void get_futex_key_refs(union futex_key *key)</span>
  * a no-op for private futexes, see comment in the get
  * counterpart.
  */
<span class="p_del">-static void drop_futex_key_refs(union futex_key *key)</span>
<span class="p_add">+static void drop_futex_key_refs(union futex_key *key, struct mm_ref *ref)</span>
 {
 	if (!key-&gt;both.ptr) {
 		/* If we&#39;re here then we tried to put a key we failed to get */
<span class="p_chunk">@@ -471,7 +473,7 @@</span> <span class="p_context"> static void drop_futex_key_refs(union futex_key *key)</span>
 		iput(key-&gt;shared.inode);
 		break;
 	case FUT_OFF_MMSHARED:
<span class="p_del">-		mmdrop(key-&gt;private.mm);</span>
<span class="p_add">+		mmdrop(key-&gt;private.mm, ref);</span>
 		break;
 	}
 }
<span class="p_chunk">@@ -495,7 +497,7 @@</span> <span class="p_context"> static void drop_futex_key_refs(union futex_key *key)</span>
  * lock_page() might sleep, the caller should not hold a spinlock.
  */
 static int
<span class="p_del">-get_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)</span>
<span class="p_add">+get_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw, struct mm_ref *mm_ref)</span>
 {
 	unsigned long address = (unsigned long)uaddr;
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_chunk">@@ -527,7 +529,7 @@</span> <span class="p_context"> get_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)</span>
 	if (!fshared) {
 		key-&gt;private.mm = mm;
 		key-&gt;private.address = address;
<span class="p_del">-		get_futex_key_refs(key);  /* implies smp_mb(); (B) */</span>
<span class="p_add">+		get_futex_key_refs(key, mm_ref);  /* implies smp_mb(); (B) */</span>
 		return 0;
 	}
 
<span class="p_chunk">@@ -630,7 +632,7 @@</span> <span class="p_context"> get_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)</span>
 		key-&gt;private.mm = mm;
 		key-&gt;private.address = address;
 
<span class="p_del">-		get_futex_key_refs(key); /* implies smp_mb(); (B) */</span>
<span class="p_add">+		get_futex_key_refs(key, mm_ref); /* implies smp_mb(); (B) */</span>
 
 	} else {
 		struct inode *inode;
<span class="p_chunk">@@ -701,9 +703,9 @@</span> <span class="p_context"> get_futex_key(u32 __user *uaddr, int fshared, union futex_key *key, int rw)</span>
 	return err;
 }
 
<span class="p_del">-static inline void put_futex_key(union futex_key *key)</span>
<span class="p_add">+static inline void put_futex_key(union futex_key *key, struct mm_ref *mm_ref)</span>
 {
<span class="p_del">-	drop_futex_key_refs(key);</span>
<span class="p_add">+	drop_futex_key_refs(key, mm_ref);</span>
 }
 
 /**
<span class="p_chunk">@@ -1414,13 +1416,14 @@</span> <span class="p_context"> futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)</span>
 	struct futex_hash_bucket *hb;
 	struct futex_q *this, *next;
 	union futex_key key = FUTEX_KEY_INIT;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	int ret;
 	WAKE_Q(wake_q);
 
 	if (!bitset)
 		return -EINVAL;
 
<span class="p_del">-	ret = get_futex_key(uaddr, flags &amp; FLAGS_SHARED, &amp;key, VERIFY_READ);</span>
<span class="p_add">+	ret = get_futex_key(uaddr, flags &amp; FLAGS_SHARED, &amp;key, VERIFY_READ, &amp;mm_ref);</span>
 	if (unlikely(ret != 0))
 		goto out;
 
<span class="p_chunk">@@ -1452,7 +1455,7 @@</span> <span class="p_context"> futex_wake(u32 __user *uaddr, unsigned int flags, int nr_wake, u32 bitset)</span>
 	spin_unlock(&amp;hb-&gt;lock);
 	wake_up_q(&amp;wake_q);
 out_put_key:
<span class="p_del">-	put_futex_key(&amp;key);</span>
<span class="p_add">+	put_futex_key(&amp;key, &amp;mm_ref);</span>
 out:
 	return ret;
 }
<span class="p_chunk">@@ -1466,16 +1469,18 @@</span> <span class="p_context"> futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,</span>
 	      int nr_wake, int nr_wake2, int op)
 {
 	union futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;
<span class="p_add">+	MM_REF(mm_ref1);</span>
<span class="p_add">+	MM_REF(mm_ref2);</span>
 	struct futex_hash_bucket *hb1, *hb2;
 	struct futex_q *this, *next;
 	int ret, op_ret;
 	WAKE_Q(wake_q);
 
 retry:
<span class="p_del">-	ret = get_futex_key(uaddr1, flags &amp; FLAGS_SHARED, &amp;key1, VERIFY_READ);</span>
<span class="p_add">+	ret = get_futex_key(uaddr1, flags &amp; FLAGS_SHARED, &amp;key1, VERIFY_READ, &amp;mm_ref1);</span>
 	if (unlikely(ret != 0))
 		goto out;
<span class="p_del">-	ret = get_futex_key(uaddr2, flags &amp; FLAGS_SHARED, &amp;key2, VERIFY_WRITE);</span>
<span class="p_add">+	ret = get_futex_key(uaddr2, flags &amp; FLAGS_SHARED, &amp;key2, VERIFY_WRITE, &amp;mm_ref2);</span>
 	if (unlikely(ret != 0))
 		goto out_put_key1;
 
<span class="p_chunk">@@ -1510,8 +1515,8 @@</span> <span class="p_context"> futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,</span>
 		if (!(flags &amp; FLAGS_SHARED))
 			goto retry_private;
 
<span class="p_del">-		put_futex_key(&amp;key2);</span>
<span class="p_del">-		put_futex_key(&amp;key1);</span>
<span class="p_add">+		put_futex_key(&amp;key2, &amp;mm_ref2);</span>
<span class="p_add">+		put_futex_key(&amp;key1, &amp;mm_ref1);</span>
 		goto retry;
 	}
 
<span class="p_chunk">@@ -1547,9 +1552,9 @@</span> <span class="p_context"> futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,</span>
 	double_unlock_hb(hb1, hb2);
 	wake_up_q(&amp;wake_q);
 out_put_keys:
<span class="p_del">-	put_futex_key(&amp;key2);</span>
<span class="p_add">+	put_futex_key(&amp;key2, &amp;mm_ref2);</span>
 out_put_key1:
<span class="p_del">-	put_futex_key(&amp;key1);</span>
<span class="p_add">+	put_futex_key(&amp;key1, &amp;mm_ref1);</span>
 out:
 	return ret;
 }
<span class="p_chunk">@@ -1563,7 +1568,7 @@</span> <span class="p_context"> futex_wake_op(u32 __user *uaddr1, unsigned int flags, u32 __user *uaddr2,</span>
  */
 static inline
 void requeue_futex(struct futex_q *q, struct futex_hash_bucket *hb1,
<span class="p_del">-		   struct futex_hash_bucket *hb2, union futex_key *key2)</span>
<span class="p_add">+		   struct futex_hash_bucket *hb2, union futex_key *key2, struct mm_ref *mm_ref2)</span>
 {
 
 	/*
<span class="p_chunk">@@ -1577,7 +1582,7 @@</span> <span class="p_context"> void requeue_futex(struct futex_q *q, struct futex_hash_bucket *hb1,</span>
 		plist_add(&amp;q-&gt;list, &amp;hb2-&gt;chain);
 		q-&gt;lock_ptr = &amp;hb2-&gt;lock;
 	}
<span class="p_del">-	get_futex_key_refs(key2);</span>
<span class="p_add">+	get_futex_key_refs(key2, mm_ref2);</span>
 	q-&gt;key = *key2;
 }
 
<span class="p_chunk">@@ -1597,9 +1602,9 @@</span> <span class="p_context"> void requeue_futex(struct futex_q *q, struct futex_hash_bucket *hb1,</span>
  */
 static inline
 void requeue_pi_wake_futex(struct futex_q *q, union futex_key *key,
<span class="p_del">-			   struct futex_hash_bucket *hb)</span>
<span class="p_add">+			   struct futex_hash_bucket *hb, struct mm_ref *ref)</span>
 {
<span class="p_del">-	get_futex_key_refs(key);</span>
<span class="p_add">+	get_futex_key_refs(key, ref);</span>
 	q-&gt;key = *key;
 
 	__unqueue_futex(q);
<span class="p_chunk">@@ -1636,7 +1641,8 @@</span> <span class="p_context"> static int futex_proxy_trylock_atomic(u32 __user *pifutex,</span>
 				 struct futex_hash_bucket *hb1,
 				 struct futex_hash_bucket *hb2,
 				 union futex_key *key1, union futex_key *key2,
<span class="p_del">-				 struct futex_pi_state **ps, int set_waiters)</span>
<span class="p_add">+				 struct futex_pi_state **ps, int set_waiters,</span>
<span class="p_add">+				 struct mm_ref *mm_ref2)</span>
 {
 	struct futex_q *top_waiter = NULL;
 	u32 curval;
<span class="p_chunk">@@ -1675,7 +1681,7 @@</span> <span class="p_context"> static int futex_proxy_trylock_atomic(u32 __user *pifutex,</span>
 	ret = futex_lock_pi_atomic(pifutex, hb2, key2, ps, top_waiter-&gt;task,
 				   set_waiters);
 	if (ret == 1) {
<span class="p_del">-		requeue_pi_wake_futex(top_waiter, key2, hb2);</span>
<span class="p_add">+		requeue_pi_wake_futex(top_waiter, key2, hb2, mm_ref2);</span>
 		return vpid;
 	}
 	return ret;
<span class="p_chunk">@@ -1704,6 +1710,8 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 			 u32 *cmpval, int requeue_pi)
 {
 	union futex_key key1 = FUTEX_KEY_INIT, key2 = FUTEX_KEY_INIT;
<span class="p_add">+	MM_REF(mm_ref1);</span>
<span class="p_add">+	MM_REF(mm_ref2);</span>
 	int drop_count = 0, task_count = 0, ret;
 	struct futex_pi_state *pi_state = NULL;
 	struct futex_hash_bucket *hb1, *hb2;
<span class="p_chunk">@@ -1739,11 +1747,11 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 	}
 
 retry:
<span class="p_del">-	ret = get_futex_key(uaddr1, flags &amp; FLAGS_SHARED, &amp;key1, VERIFY_READ);</span>
<span class="p_add">+	ret = get_futex_key(uaddr1, flags &amp; FLAGS_SHARED, &amp;key1, VERIFY_READ, &amp;mm_ref1);</span>
 	if (unlikely(ret != 0))
 		goto out;
 	ret = get_futex_key(uaddr2, flags &amp; FLAGS_SHARED, &amp;key2,
<span class="p_del">-			    requeue_pi ? VERIFY_WRITE : VERIFY_READ);</span>
<span class="p_add">+			    requeue_pi ? VERIFY_WRITE : VERIFY_READ, &amp;mm_ref2);</span>
 	if (unlikely(ret != 0))
 		goto out_put_key1;
 
<span class="p_chunk">@@ -1779,8 +1787,8 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 			if (!(flags &amp; FLAGS_SHARED))
 				goto retry_private;
 
<span class="p_del">-			put_futex_key(&amp;key2);</span>
<span class="p_del">-			put_futex_key(&amp;key1);</span>
<span class="p_add">+			put_futex_key(&amp;key2, &amp;mm_ref2);</span>
<span class="p_add">+			put_futex_key(&amp;key1, &amp;mm_ref1);</span>
 			goto retry;
 		}
 		if (curval != *cmpval) {
<span class="p_chunk">@@ -1797,7 +1805,7 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 		 * faults rather in the requeue loop below.
 		 */
 		ret = futex_proxy_trylock_atomic(uaddr2, hb1, hb2, &amp;key1,
<span class="p_del">-						 &amp;key2, &amp;pi_state, nr_requeue);</span>
<span class="p_add">+						 &amp;key2, &amp;pi_state, nr_requeue, &amp;mm_ref2);</span>
 
 		/*
 		 * At this point the top_waiter has either taken uaddr2 or is
<span class="p_chunk">@@ -1836,8 +1844,8 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 		case -EFAULT:
 			double_unlock_hb(hb1, hb2);
 			hb_waiters_dec(hb2);
<span class="p_del">-			put_futex_key(&amp;key2);</span>
<span class="p_del">-			put_futex_key(&amp;key1);</span>
<span class="p_add">+			put_futex_key(&amp;key2, &amp;mm_ref2);</span>
<span class="p_add">+			put_futex_key(&amp;key1, &amp;mm_ref1);</span>
 			ret = fault_in_user_writeable(uaddr2);
 			if (!ret)
 				goto retry;
<span class="p_chunk">@@ -1851,8 +1859,8 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 			 */
 			double_unlock_hb(hb1, hb2);
 			hb_waiters_dec(hb2);
<span class="p_del">-			put_futex_key(&amp;key2);</span>
<span class="p_del">-			put_futex_key(&amp;key1);</span>
<span class="p_add">+			put_futex_key(&amp;key2, &amp;mm_ref2);</span>
<span class="p_add">+			put_futex_key(&amp;key1, &amp;mm_ref1);</span>
 			cond_resched();
 			goto retry;
 		default:
<span class="p_chunk">@@ -1921,7 +1929,7 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 				 * value. It will drop the refcount after
 				 * doing so.
 				 */
<span class="p_del">-				requeue_pi_wake_futex(this, &amp;key2, hb2);</span>
<span class="p_add">+				requeue_pi_wake_futex(this, &amp;key2, hb2, &amp;mm_ref2);</span>
 				drop_count++;
 				continue;
 			} else if (ret) {
<span class="p_chunk">@@ -1942,7 +1950,7 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 				break;
 			}
 		}
<span class="p_del">-		requeue_futex(this, hb1, hb2, &amp;key2);</span>
<span class="p_add">+		requeue_futex(this, hb1, hb2, &amp;key2, &amp;mm_ref2);</span>
 		drop_count++;
 	}
 
<span class="p_chunk">@@ -1965,12 +1973,12 @@</span> <span class="p_context"> static int futex_requeue(u32 __user *uaddr1, unsigned int flags,</span>
 	 * hold the references to key1.
 	 */
 	while (--drop_count &gt;= 0)
<span class="p_del">-		drop_futex_key_refs(&amp;key1);</span>
<span class="p_add">+		drop_futex_key_refs(&amp;key1, &amp;mm_ref1);</span>
 
 out_put_keys:
<span class="p_del">-	put_futex_key(&amp;key2);</span>
<span class="p_add">+	put_futex_key(&amp;key2, &amp;mm_ref2);</span>
 out_put_key1:
<span class="p_del">-	put_futex_key(&amp;key1);</span>
<span class="p_add">+	put_futex_key(&amp;key1, &amp;mm_ref1);</span>
 out:
 	return ret ? ret : task_count;
 }
<span class="p_chunk">@@ -2091,7 +2099,7 @@</span> <span class="p_context"> static int unqueue_me(struct futex_q *q)</span>
 		ret = 1;
 	}
 
<span class="p_del">-	drop_futex_key_refs(&amp;q-&gt;key);</span>
<span class="p_add">+	drop_futex_key_refs(&amp;q-&gt;key, &amp;q-&gt;mm_ref);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -2365,7 +2373,7 @@</span> <span class="p_context"> static int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,</span>
 	 * while the syscall executes.
 	 */
 retry:
<span class="p_del">-	ret = get_futex_key(uaddr, flags &amp; FLAGS_SHARED, &amp;q-&gt;key, VERIFY_READ);</span>
<span class="p_add">+	ret = get_futex_key(uaddr, flags &amp; FLAGS_SHARED, &amp;q-&gt;key, VERIFY_READ, &amp;q-&gt;mm_ref);</span>
 	if (unlikely(ret != 0))
 		return ret;
 
<span class="p_chunk">@@ -2384,7 +2392,7 @@</span> <span class="p_context"> static int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,</span>
 		if (!(flags &amp; FLAGS_SHARED))
 			goto retry_private;
 
<span class="p_del">-		put_futex_key(&amp;q-&gt;key);</span>
<span class="p_add">+		put_futex_key(&amp;q-&gt;key, &amp;q-&gt;mm_ref);</span>
 		goto retry;
 	}
 
<span class="p_chunk">@@ -2395,7 +2403,7 @@</span> <span class="p_context"> static int futex_wait_setup(u32 __user *uaddr, u32 val, unsigned int flags,</span>
 
 out:
 	if (ret)
<span class="p_del">-		put_futex_key(&amp;q-&gt;key);</span>
<span class="p_add">+		put_futex_key(&amp;q-&gt;key, &amp;q-&gt;mm_ref);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -2408,6 +2416,8 @@</span> <span class="p_context"> static int futex_wait(u32 __user *uaddr, unsigned int flags, u32 val,</span>
 	struct futex_q q = futex_q_init;
 	int ret;
 
<span class="p_add">+	INIT_MM_REF(&amp;q.mm_ref);</span>
<span class="p_add">+</span>
 	if (!bitset)
 		return -EINVAL;
 	q.bitset = bitset;
<span class="p_chunk">@@ -2507,6 +2517,8 @@</span> <span class="p_context"> static int futex_lock_pi(u32 __user *uaddr, unsigned int flags,</span>
 	struct futex_q q = futex_q_init;
 	int res, ret;
 
<span class="p_add">+	INIT_MM_REF(&amp;q.mm_ref);</span>
<span class="p_add">+</span>
 	if (refill_pi_state_cache())
 		return -ENOMEM;
 
<span class="p_chunk">@@ -2519,7 +2531,7 @@</span> <span class="p_context"> static int futex_lock_pi(u32 __user *uaddr, unsigned int flags,</span>
 	}
 
 retry:
<span class="p_del">-	ret = get_futex_key(uaddr, flags &amp; FLAGS_SHARED, &amp;q.key, VERIFY_WRITE);</span>
<span class="p_add">+	ret = get_futex_key(uaddr, flags &amp; FLAGS_SHARED, &amp;q.key, VERIFY_WRITE, &amp;q.mm_ref);</span>
 	if (unlikely(ret != 0))
 		goto out;
 
<span class="p_chunk">@@ -2547,7 +2559,7 @@</span> <span class="p_context"> static int futex_lock_pi(u32 __user *uaddr, unsigned int flags,</span>
 			 * - The user space value changed.
 			 */
 			queue_unlock(hb);
<span class="p_del">-			put_futex_key(&amp;q.key);</span>
<span class="p_add">+			put_futex_key(&amp;q.key, &amp;q.mm_ref);</span>
 			cond_resched();
 			goto retry;
 		default:
<span class="p_chunk">@@ -2601,7 +2613,7 @@</span> <span class="p_context"> static int futex_lock_pi(u32 __user *uaddr, unsigned int flags,</span>
 	queue_unlock(hb);
 
 out_put_key:
<span class="p_del">-	put_futex_key(&amp;q.key);</span>
<span class="p_add">+	put_futex_key(&amp;q.key, &amp;q.mm_ref);</span>
 out:
 	if (to)
 		destroy_hrtimer_on_stack(&amp;to-&gt;timer);
<span class="p_chunk">@@ -2617,7 +2629,7 @@</span> <span class="p_context"> static int futex_lock_pi(u32 __user *uaddr, unsigned int flags,</span>
 	if (!(flags &amp; FLAGS_SHARED))
 		goto retry_private;
 
<span class="p_del">-	put_futex_key(&amp;q.key);</span>
<span class="p_add">+	put_futex_key(&amp;q.key, &amp;q.mm_ref);</span>
 	goto retry;
 }
 
<span class="p_chunk">@@ -2630,6 +2642,7 @@</span> <span class="p_context"> static int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)</span>
 {
 	u32 uninitialized_var(curval), uval, vpid = task_pid_vnr(current);
 	union futex_key key = FUTEX_KEY_INIT;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	struct futex_hash_bucket *hb;
 	struct futex_q *match;
 	int ret;
<span class="p_chunk">@@ -2643,7 +2656,7 @@</span> <span class="p_context"> static int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)</span>
 	if ((uval &amp; FUTEX_TID_MASK) != vpid)
 		return -EPERM;
 
<span class="p_del">-	ret = get_futex_key(uaddr, flags &amp; FLAGS_SHARED, &amp;key, VERIFY_WRITE);</span>
<span class="p_add">+	ret = get_futex_key(uaddr, flags &amp; FLAGS_SHARED, &amp;key, VERIFY_WRITE, &amp;mm_ref);</span>
 	if (ret)
 		return ret;
 
<span class="p_chunk">@@ -2676,7 +2689,7 @@</span> <span class="p_context"> static int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)</span>
 		 */
 		if (ret == -EAGAIN) {
 			spin_unlock(&amp;hb-&gt;lock);
<span class="p_del">-			put_futex_key(&amp;key);</span>
<span class="p_add">+			put_futex_key(&amp;key, &amp;mm_ref);</span>
 			goto retry;
 		}
 		/*
<span class="p_chunk">@@ -2704,12 +2717,12 @@</span> <span class="p_context"> static int futex_unlock_pi(u32 __user *uaddr, unsigned int flags)</span>
 out_unlock:
 	spin_unlock(&amp;hb-&gt;lock);
 out_putkey:
<span class="p_del">-	put_futex_key(&amp;key);</span>
<span class="p_add">+	put_futex_key(&amp;key, &amp;mm_ref);</span>
 	return ret;
 
 pi_faulted:
 	spin_unlock(&amp;hb-&gt;lock);
<span class="p_del">-	put_futex_key(&amp;key);</span>
<span class="p_add">+	put_futex_key(&amp;key, &amp;mm_ref);</span>
 
 	ret = fault_in_user_writeable(uaddr);
 	if (!ret)
<span class="p_chunk">@@ -2816,9 +2829,12 @@</span> <span class="p_context"> static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,</span>
 	struct rt_mutex *pi_mutex = NULL;
 	struct futex_hash_bucket *hb;
 	union futex_key key2 = FUTEX_KEY_INIT;
<span class="p_add">+	MM_REF(mm_ref2);</span>
 	struct futex_q q = futex_q_init;
 	int res, ret;
 
<span class="p_add">+	INIT_MM_REF(&amp;q.mm_ref);</span>
<span class="p_add">+</span>
 	if (uaddr == uaddr2)
 		return -EINVAL;
 
<span class="p_chunk">@@ -2844,7 +2860,7 @@</span> <span class="p_context"> static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,</span>
 	RB_CLEAR_NODE(&amp;rt_waiter.tree_entry);
 	rt_waiter.task = NULL;
 
<span class="p_del">-	ret = get_futex_key(uaddr2, flags &amp; FLAGS_SHARED, &amp;key2, VERIFY_WRITE);</span>
<span class="p_add">+	ret = get_futex_key(uaddr2, flags &amp; FLAGS_SHARED, &amp;key2, VERIFY_WRITE, &amp;mm_ref2);</span>
 	if (unlikely(ret != 0))
 		goto out;
 
<span class="p_chunk">@@ -2951,9 +2967,9 @@</span> <span class="p_context"> static int futex_wait_requeue_pi(u32 __user *uaddr, unsigned int flags,</span>
 	}
 
 out_put_keys:
<span class="p_del">-	put_futex_key(&amp;q.key);</span>
<span class="p_add">+	put_futex_key(&amp;q.key, &amp;q.mm_ref);</span>
 out_key2:
<span class="p_del">-	put_futex_key(&amp;key2);</span>
<span class="p_add">+	put_futex_key(&amp;key2, &amp;mm_ref2);</span>
 
 out:
 	if (to) {
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index ee1fb0070544..460c57d0d9af 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -2771,7 +2771,7 @@</span> <span class="p_context"> static struct rq *finish_task_switch(struct task_struct *prev)</span>
 
 	fire_sched_in_preempt_notifiers(current);
 	if (mm)
<span class="p_del">-		mmdrop(mm);</span>
<span class="p_add">+		mmdrop(mm, &amp;rq-&gt;prev_mm_ref);</span>
 	if (unlikely(prev_state == TASK_DEAD)) {
 		if (prev-&gt;sched_class-&gt;task_dead)
 			prev-&gt;sched_class-&gt;task_dead(prev);
<span class="p_chunk">@@ -2877,12 +2877,14 @@</span> <span class="p_context"> context_switch(struct rq *rq, struct task_struct *prev,</span>
 
 	if (!mm) {
 		next-&gt;active_mm = oldmm;
<span class="p_del">-		mmgrab(oldmm);</span>
<span class="p_add">+		mmgrab(oldmm, &amp;next-&gt;mm_ref);</span>
 		enter_lazy_tlb(oldmm, next);
 	} else
 		switch_mm_irqs_off(oldmm, mm, next);
 
 	if (!prev-&gt;mm) {
<span class="p_add">+		if (oldmm)</span>
<span class="p_add">+			move_mm_ref(oldmm, &amp;prev-&gt;mm_ref, &amp;rq-&gt;prev_mm_ref);</span>
 		prev-&gt;active_mm = NULL;
 		rq-&gt;prev_mm = oldmm;
 	}
<span class="p_chunk">@@ -5472,7 +5474,7 @@</span> <span class="p_context"> void idle_task_exit(void)</span>
 		switch_mm_irqs_off(mm, &amp;init_mm, current);
 		finish_arch_post_lock_switch();
 	}
<span class="p_del">-	mmdrop(mm);</span>
<span class="p_add">+	mmdrop(mm, &amp;current-&gt;mm_ref);</span>
 }
 
 /*
<span class="p_chunk">@@ -7640,6 +7642,10 @@</span> <span class="p_context"> void __init sched_init(void)</span>
 		rq-&gt;balance_callback = NULL;
 		rq-&gt;active_balance = 0;
 		rq-&gt;next_balance = jiffies;
<span class="p_add">+</span>
<span class="p_add">+		BUG_ON(rq-&gt;prev_mm != NULL);</span>
<span class="p_add">+		INIT_MM_REF(&amp;rq-&gt;prev_mm_ref);</span>
<span class="p_add">+</span>
 		rq-&gt;push_cpu = 0;
 		rq-&gt;cpu = i;
 		rq-&gt;online = 0;
<span class="p_chunk">@@ -7667,7 +7673,7 @@</span> <span class="p_context"> void __init sched_init(void)</span>
 	/*
 	 * The boot idle thread does lazy MMU switching as well:
 	 */
<span class="p_del">-	mmgrab(&amp;init_mm);</span>
<span class="p_add">+	mmgrab(&amp;init_mm, &amp;init_mm_ref);</span>
 	enter_lazy_tlb(&amp;init_mm, current);
 
 	/*
<span class="p_header">diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="p_header">index 055f935d4421..98680abb882a 100644</span>
<span class="p_header">--- a/kernel/sched/sched.h</span>
<span class="p_header">+++ b/kernel/sched/sched.h</span>
<span class="p_chunk">@@ -636,6 +636,7 @@</span> <span class="p_context"> struct rq {</span>
 	struct task_struct *curr, *idle, *stop;
 	unsigned long next_balance;
 	struct mm_struct *prev_mm;
<span class="p_add">+	struct mm_ref prev_mm_ref;</span>
 
 	unsigned int clock_skip_update;
 	u64 clock;
<span class="p_header">diff --git a/kernel/sys.c b/kernel/sys.c</span>
<span class="p_header">index 89d5be418157..01a5bd227a53 100644</span>
<span class="p_header">--- a/kernel/sys.c</span>
<span class="p_header">+++ b/kernel/sys.c</span>
<span class="p_chunk">@@ -1603,11 +1603,12 @@</span> <span class="p_context"> static void k_getrusage(struct task_struct *p, int who, struct rusage *r)</span>
 	cputime_to_timeval(stime, &amp;r-&gt;ru_stime);
 
 	if (who != RUSAGE_CHILDREN) {
<span class="p_del">-		struct mm_struct *mm = get_task_mm(p);</span>
<span class="p_add">+		MM_REF(mm_ref);</span>
<span class="p_add">+		struct mm_struct *mm = get_task_mm(p, &amp;mm_ref);</span>
 
 		if (mm) {
 			setmax_mm_hiwater_rss(&amp;maxrss, mm);
<span class="p_del">-			mmput(mm);</span>
<span class="p_add">+			mmput(mm, &amp;mm_ref);</span>
 		}
 	}
 	r-&gt;ru_maxrss = maxrss * (PAGE_SIZE / 1024); /* convert pages to KBs */
<span class="p_header">diff --git a/kernel/trace/trace_output.c b/kernel/trace/trace_output.c</span>
<span class="p_header">index 3fc20422c166..decd72ec58e1 100644</span>
<span class="p_header">--- a/kernel/trace/trace_output.c</span>
<span class="p_header">+++ b/kernel/trace/trace_output.c</span>
<span class="p_chunk">@@ -1046,6 +1046,7 @@</span> <span class="p_context"> static enum print_line_t trace_user_stack_print(struct trace_iterator *iter,</span>
 	struct userstack_entry *field;
 	struct trace_seq *s = &amp;iter-&gt;seq;
 	struct mm_struct *mm = NULL;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	unsigned int i;
 
 	trace_assign_type(field, iter-&gt;ent);
<span class="p_chunk">@@ -1061,7 +1062,7 @@</span> <span class="p_context"> static enum print_line_t trace_user_stack_print(struct trace_iterator *iter,</span>
 		rcu_read_lock();
 		task = find_task_by_vpid(field-&gt;tgid);
 		if (task)
<span class="p_del">-			mm = get_task_mm(task);</span>
<span class="p_add">+			mm = get_task_mm(task, &amp;mm_ref);</span>
 		rcu_read_unlock();
 	}
 
<span class="p_chunk">@@ -1084,7 +1085,7 @@</span> <span class="p_context"> static enum print_line_t trace_user_stack_print(struct trace_iterator *iter,</span>
 	}
 
 	if (mm)
<span class="p_del">-		mmput(mm);</span>
<span class="p_add">+		mmput(mm, &amp;mm_ref);</span>
 
 	return trace_handle_return(s);
 }
<span class="p_header">diff --git a/kernel/tsacct.c b/kernel/tsacct.c</span>
<span class="p_header">index f8e26ab963ed..58595a3dca3f 100644</span>
<span class="p_header">--- a/kernel/tsacct.c</span>
<span class="p_header">+++ b/kernel/tsacct.c</span>
<span class="p_chunk">@@ -92,18 +92,19 @@</span> <span class="p_context"> void bacct_add_tsk(struct user_namespace *user_ns,</span>
 void xacct_add_tsk(struct taskstats *stats, struct task_struct *p)
 {
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 
 	/* convert pages-nsec/1024 to Mbyte-usec, see __acct_update_integrals */
 	stats-&gt;coremem = p-&gt;acct_rss_mem1 * PAGE_SIZE;
 	do_div(stats-&gt;coremem, 1000 * KB);
 	stats-&gt;virtmem = p-&gt;acct_vm_mem1 * PAGE_SIZE;
 	do_div(stats-&gt;virtmem, 1000 * KB);
<span class="p_del">-	mm = get_task_mm(p);</span>
<span class="p_add">+	mm = get_task_mm(p, &amp;mm_ref);</span>
 	if (mm) {
 		/* adjust to KB unit */
 		stats-&gt;hiwater_rss   = get_mm_hiwater_rss(mm) * PAGE_SIZE / KB;
 		stats-&gt;hiwater_vm    = get_mm_hiwater_vm(mm)  * PAGE_SIZE / KB;
<span class="p_del">-		mmput(mm);</span>
<span class="p_add">+		mmput(mm, &amp;mm_ref);</span>
 	}
 	stats-&gt;read_char	= p-&gt;ioac.rchar &amp; KB_MASK;
 	stats-&gt;write_char	= p-&gt;ioac.wchar &amp; KB_MASK;
<span class="p_header">diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="p_header">index 295bd7a9f76b..1d6acdf0a4a7 100644</span>
<span class="p_header">--- a/mm/Makefile</span>
<span class="p_header">+++ b/mm/Makefile</span>
<span class="p_chunk">@@ -37,7 +37,7 @@</span> <span class="p_context"> obj-y			:= filemap.o mempool.o oom_kill.o \</span>
 			   mm_init.o mmu_context.o percpu.o slab_common.o \
 			   compaction.o vmacache.o \
 			   interval_tree.o list_lru.o workingset.o \
<span class="p_del">-			   debug.o $(mmu-y)</span>
<span class="p_add">+			   debug.o mm_ref.o $(mmu-y)</span>
 
 obj-y += init-mm.o
 
<span class="p_header">diff --git a/mm/init-mm.c b/mm/init-mm.c</span>
<span class="p_header">index a56a851908d2..deb315a4c240 100644</span>
<span class="p_header">--- a/mm/init-mm.c</span>
<span class="p_header">+++ b/mm/init-mm.c</span>
<span class="p_chunk">@@ -16,10 +16,16 @@</span> <span class="p_context"></span>
 struct mm_struct init_mm = {
 	.mm_rb		= RB_ROOT,
 	.pgd		= swapper_pg_dir,
<span class="p_add">+	.mm_refs_lock	= __SPIN_LOCK_UNLOCKED(init_mm.mm_refs_lock),</span>
 	.mm_users	= ATOMIC_INIT(2),
<span class="p_add">+	.mm_users_list	= LIST_HEAD_INIT(init_mm.mm_users_list),</span>
<span class="p_add">+	.mm_users_ref	= MM_REF_INIT(init_mm.mm_users_ref),</span>
 	.mm_count	= ATOMIC_INIT(1),
<span class="p_add">+	.mm_count_list	= LIST_HEAD_INIT(init_mm.mm_count_list),</span>
 	.mmap_sem	= __RWSEM_INITIALIZER(init_mm.mmap_sem),
 	.page_table_lock =  __SPIN_LOCK_UNLOCKED(init_mm.page_table_lock),
 	.mmlist		= LIST_HEAD_INIT(init_mm.mmlist),
 	INIT_MM_CONTEXT(init_mm)
 };
<span class="p_add">+</span>
<span class="p_add">+MM_REF(init_mm_ref);</span>
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index e18c57bdc75c..3be253b54c04 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -3954,15 +3954,16 @@</span> <span class="p_context"> int access_process_vm(struct task_struct *tsk, unsigned long addr,</span>
 		void *buf, int len, unsigned int gup_flags)
 {
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	int ret;
 
<span class="p_del">-	mm = get_task_mm(tsk);</span>
<span class="p_add">+	mm = get_task_mm(tsk, &amp;mm_ref);</span>
 	if (!mm)
 		return 0;
 
 	ret = __access_remote_vm(tsk, mm, addr, buf, len, gup_flags);
 
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 
 	return ret;
 }
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index 0b859af06b87..4790274af596 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -1374,6 +1374,7 @@</span> <span class="p_context"> SYSCALL_DEFINE4(migrate_pages, pid_t, pid, unsigned long, maxnode,</span>
 {
 	const struct cred *cred = current_cred(), *tcred;
 	struct mm_struct *mm = NULL;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	struct task_struct *task;
 	nodemask_t task_nodes;
 	int err;
<span class="p_chunk">@@ -1439,7 +1440,7 @@</span> <span class="p_context"> SYSCALL_DEFINE4(migrate_pages, pid_t, pid, unsigned long, maxnode,</span>
 	if (err)
 		goto out_put;
 
<span class="p_del">-	mm = get_task_mm(task);</span>
<span class="p_add">+	mm = get_task_mm(task, &amp;mm_ref);</span>
 	put_task_struct(task);
 
 	if (!mm) {
<span class="p_chunk">@@ -1450,7 +1451,7 @@</span> <span class="p_context"> SYSCALL_DEFINE4(migrate_pages, pid_t, pid, unsigned long, maxnode,</span>
 	err = do_migrate_pages(mm, old, new,
 		capable(CAP_SYS_NICE) ? MPOL_MF_MOVE_ALL : MPOL_MF_MOVE);
 
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 out:
 	NODEMASK_SCRATCH_FREE(scratch);
 
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 99250aee1ac1..593942dabbc1 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1659,6 +1659,7 @@</span> <span class="p_context"> SYSCALL_DEFINE6(move_pages, pid_t, pid, unsigned long, nr_pages,</span>
 	const struct cred *cred = current_cred(), *tcred;
 	struct task_struct *task;
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	int err;
 	nodemask_t task_nodes;
 
<span class="p_chunk">@@ -1699,7 +1700,7 @@</span> <span class="p_context"> SYSCALL_DEFINE6(move_pages, pid_t, pid, unsigned long, nr_pages,</span>
 		goto out;
 
 	task_nodes = cpuset_mems_allowed(task);
<span class="p_del">-	mm = get_task_mm(task);</span>
<span class="p_add">+	mm = get_task_mm(task, &amp;mm_ref);</span>
 	put_task_struct(task);
 
 	if (!mm)
<span class="p_chunk">@@ -1711,7 +1712,7 @@</span> <span class="p_context"> SYSCALL_DEFINE6(move_pages, pid_t, pid, unsigned long, nr_pages,</span>
 	else
 		err = do_pages_stat(mm, nr_pages, pages, status);
 
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 	return err;
 
 out:
<span class="p_header">diff --git a/mm/mm_ref.c b/mm/mm_ref.c</span>
new file mode 100644
<span class="p_header">index 000000000000..cf14334aec58</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/mm_ref.c</span>
<span class="p_chunk">@@ -0,0 +1,163 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;linux/list.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm_ref.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm_types.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/stacktrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static void _mm_ref_save_trace(struct mm_ref *ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	ref-&gt;pid = current-&gt;pid;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Save stack trace */</span>
<span class="p_add">+	ref-&gt;trace.nr_entries = 0;</span>
<span class="p_add">+	ref-&gt;trace.entries = ref-&gt;trace_entries;</span>
<span class="p_add">+	ref-&gt;trace.max_entries = NR_MM_REF_STACK_ENTRIES;</span>
<span class="p_add">+	ref-&gt;trace.skip = 1;</span>
<span class="p_add">+	save_stack_trace(&amp;ref-&gt;trace);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void INIT_MM_REF(struct mm_ref *ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	_mm_ref_save_trace(ref);</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;ref-&gt;list_entry);</span>
<span class="p_add">+	ref-&gt;state = MM_REF_INITIALIZED;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dump_refs_list(const char *label, struct list_head *list)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_ref *ref;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (list_empty(list)) {</span>
<span class="p_add">+		printk(KERN_ERR &quot;%s: no refs\n&quot;, label);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	printk(KERN_ERR &quot;%s:\n&quot;, label);</span>
<span class="p_add">+	list_for_each_entry(ref, list, list_entry) {</span>
<span class="p_add">+		printk(KERN_ERR &quot; - %p %x acquired by %d at:%s\n&quot;,</span>
<span class="p_add">+			ref, ref-&gt;state,</span>
<span class="p_add">+			ref-&gt;pid,</span>
<span class="p_add">+			ref-&gt;state == MM_REF_ACTIVE ? &quot;&quot; : &quot; (bogus)&quot;);</span>
<span class="p_add">+		if (ref-&gt;state == MM_REF_ACTIVE)</span>
<span class="p_add">+			print_stack_trace(&amp;ref-&gt;trace, 2);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dump_refs(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;mm-&gt;mm_refs_lock, flags);</span>
<span class="p_add">+	printk(KERN_ERR &quot;mm_users = %u\n&quot;, atomic_read(&amp;mm-&gt;mm_users));</span>
<span class="p_add">+	dump_refs_list(&quot;mm_users_list&quot;, &amp;mm-&gt;mm_users_list);</span>
<span class="p_add">+	printk(KERN_ERR &quot;mm_count = %u\n&quot;, atomic_read(&amp;mm-&gt;mm_count));</span>
<span class="p_add">+	dump_refs_list(&quot;mm_count_list&quot;, &amp;mm-&gt;mm_count_list);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;mm-&gt;mm_refs_lock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static bool _mm_ref_expect_inactive(struct mm_struct *mm, struct mm_ref *ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (ref-&gt;state == MM_REF_INITIALIZED || ref-&gt;state == MM_REF_INACTIVE)</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ref-&gt;state == MM_REF_ACTIVE) {</span>
<span class="p_add">+		printk(KERN_ERR &quot;trying to overwrite active ref %p to mm %p\n&quot;, ref, mm);</span>
<span class="p_add">+		printk(KERN_ERR &quot;previous ref taken by %d at:\n&quot;, ref-&gt;pid);</span>
<span class="p_add">+		print_stack_trace(&amp;ref-&gt;trace, 0);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printk(KERN_ERR &quot;trying to overwrite ref %p in unknown state %x to mm %p\n&quot;,</span>
<span class="p_add">+			ref, ref-&gt;state, mm);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static bool _mm_ref_expect_active(struct mm_struct *mm, struct mm_ref *ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (ref-&gt;state == MM_REF_ACTIVE)</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ref-&gt;state == MM_REF_INITIALIZED || ref-&gt;state == MM_REF_INACTIVE) {</span>
<span class="p_add">+		printk(KERN_ERR &quot;trying to put inactive ref %p to mm %p\n&quot;, ref, mm);</span>
<span class="p_add">+		if (ref-&gt;state == MM_REF_INITIALIZED)</span>
<span class="p_add">+			printk(KERN_ERR &quot;ref initialized by %d at:\n&quot;, ref-&gt;pid);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			printk(KERN_ERR &quot;previous ref dropped by %d at:\n&quot;, ref-&gt;pid);</span>
<span class="p_add">+		print_stack_trace(&amp;ref-&gt;trace, 0);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		printk(KERN_ERR &quot;trying to put ref %p in unknown state %x to mm %p\n&quot;,</span>
<span class="p_add">+			ref, ref-&gt;state, mm);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void _get_mm_ref(struct mm_struct *mm, struct list_head *list, struct mm_ref *ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!_mm_ref_expect_inactive(mm, ref)) {</span>
<span class="p_add">+		dump_refs(mm);</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	_mm_ref_save_trace(ref);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;mm-&gt;mm_refs_lock, flags);</span>
<span class="p_add">+	list_add_tail(&amp;ref-&gt;list_entry, list);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;mm-&gt;mm_refs_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	ref-&gt;state = MM_REF_ACTIVE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void _put_mm_ref(struct mm_struct *mm, struct list_head *list, struct mm_ref *ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!_mm_ref_expect_active(mm, ref)) {</span>
<span class="p_add">+		dump_refs(mm);</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	_mm_ref_save_trace(ref);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;mm-&gt;mm_refs_lock, flags);</span>
<span class="p_add">+	BUG_ON(list_empty(&amp;ref-&gt;list_entry));</span>
<span class="p_add">+	list_del_init(&amp;ref-&gt;list_entry);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;mm-&gt;mm_refs_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	ref-&gt;state = MM_REF_INACTIVE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * TODO: we have a choice here whether to ignore mm == NULL or</span>
<span class="p_add">+ * treat it as an error.</span>
<span class="p_add">+ * TODO: there&#39;s also a question about whether old_ref == new_ref</span>
<span class="p_add">+ * is an error or not.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void _move_mm_ref(struct mm_struct *mm, struct list_head *list,</span>
<span class="p_add">+	struct mm_ref *old_ref, struct mm_ref *new_ref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!_mm_ref_expect_active(mm, old_ref)) {</span>
<span class="p_add">+		dump_refs(mm);</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (!_mm_ref_expect_inactive(mm, new_ref)) {</span>
<span class="p_add">+		dump_refs(mm);</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	_mm_ref_save_trace(old_ref);</span>
<span class="p_add">+	_mm_ref_save_trace(new_ref);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;mm-&gt;mm_refs_lock, flags);</span>
<span class="p_add">+	BUG_ON(list_empty(&amp;old_ref-&gt;list_entry));</span>
<span class="p_add">+	list_del_init(&amp;old_ref-&gt;list_entry);</span>
<span class="p_add">+	list_add_tail(&amp;new_ref-&gt;list_entry, list);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;mm-&gt;mm_refs_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	old_ref-&gt;state = MM_REF_INACTIVE;</span>
<span class="p_add">+	new_ref-&gt;state = MM_REF_ACTIVE;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/mm/mmu_context.c b/mm/mmu_context.c</span>
<span class="p_header">index daf67bb02b4a..3e28db145982 100644</span>
<span class="p_header">--- a/mm/mmu_context.c</span>
<span class="p_header">+++ b/mm/mmu_context.c</span>
<span class="p_chunk">@@ -20,12 +20,14 @@</span> <span class="p_context"></span>
 void use_mm(struct mm_struct *mm)
 {
 	struct mm_struct *active_mm;
<span class="p_add">+	struct mm_ref active_mm_ref;</span>
 	struct task_struct *tsk = current;
 
 	task_lock(tsk);
 	active_mm = tsk-&gt;active_mm;
 	if (active_mm != mm) {
<span class="p_del">-		mmgrab(mm);</span>
<span class="p_add">+		move_mm_ref(mm, &amp;tsk-&gt;mm_ref, &amp;active_mm_ref);</span>
<span class="p_add">+		mmgrab(mm, &amp;tsk-&gt;mm_ref);</span>
 		tsk-&gt;active_mm = mm;
 	}
 	tsk-&gt;mm = mm;
<span class="p_chunk">@@ -35,8 +37,9 @@</span> <span class="p_context"> void use_mm(struct mm_struct *mm)</span>
 	finish_arch_post_lock_switch();
 #endif
 
<span class="p_del">-	if (active_mm != mm)</span>
<span class="p_del">-		mmdrop(active_mm);</span>
<span class="p_add">+	if (active_mm != mm) {</span>
<span class="p_add">+		mmdrop(active_mm, &amp;active_mm_ref);</span>
<span class="p_add">+	}</span>
 }
 EXPORT_SYMBOL_GPL(use_mm);
 
<span class="p_header">diff --git a/mm/mmu_notifier.c b/mm/mmu_notifier.c</span>
<span class="p_header">index 32bc9f2ff7eb..8187d46c8d05 100644</span>
<span class="p_header">--- a/mm/mmu_notifier.c</span>
<span class="p_header">+++ b/mm/mmu_notifier.c</span>
<span class="p_chunk">@@ -244,7 +244,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(__mmu_notifier_invalidate_range);</span>
 
 static int do_mmu_notifier_register(struct mmu_notifier *mn,
 				    struct mm_struct *mm,
<span class="p_del">-				    int take_mmap_sem)</span>
<span class="p_add">+				    int take_mmap_sem, struct mm_ref *mm_ref)</span>
 {
 	struct mmu_notifier_mm *mmu_notifier_mm;
 	int ret;
<span class="p_chunk">@@ -275,7 +275,7 @@</span> <span class="p_context"> static int do_mmu_notifier_register(struct mmu_notifier *mn,</span>
 		mm-&gt;mmu_notifier_mm = mmu_notifier_mm;
 		mmu_notifier_mm = NULL;
 	}
<span class="p_del">-	mmgrab(mm);</span>
<span class="p_add">+	mmgrab(mm, mm_ref);</span>
 
 	/*
 	 * Serialize the update against mmu_notifier_unregister. A
<span class="p_chunk">@@ -312,9 +312,9 @@</span> <span class="p_context"> static int do_mmu_notifier_register(struct mmu_notifier *mn,</span>
  * after exit_mmap. -&gt;release will always be called before exit_mmap
  * frees the pages.
  */
<span class="p_del">-int mmu_notifier_register(struct mmu_notifier *mn, struct mm_struct *mm)</span>
<span class="p_add">+int mmu_notifier_register(struct mmu_notifier *mn, struct mm_struct *mm, struct mm_ref *mm_ref)</span>
 {
<span class="p_del">-	return do_mmu_notifier_register(mn, mm, 1);</span>
<span class="p_add">+	return do_mmu_notifier_register(mn, mm, 1, mm_ref);</span>
 }
 EXPORT_SYMBOL_GPL(mmu_notifier_register);
 
<span class="p_chunk">@@ -322,9 +322,9 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(mmu_notifier_register);</span>
  * Same as mmu_notifier_register but here the caller must hold the
  * mmap_sem in write mode.
  */
<span class="p_del">-int __mmu_notifier_register(struct mmu_notifier *mn, struct mm_struct *mm)</span>
<span class="p_add">+int __mmu_notifier_register(struct mmu_notifier *mn, struct mm_struct *mm, struct mm_ref *mm_ref)</span>
 {
<span class="p_del">-	return do_mmu_notifier_register(mn, mm, 0);</span>
<span class="p_add">+	return do_mmu_notifier_register(mn, mm, 0, mm_ref);</span>
 }
 EXPORT_SYMBOL_GPL(__mmu_notifier_register);
 
<span class="p_chunk">@@ -346,7 +346,7 @@</span> <span class="p_context"> void __mmu_notifier_mm_destroy(struct mm_struct *mm)</span>
  * and only after mmu_notifier_unregister returned we&#39;re guaranteed
  * that -&gt;release or any other method can&#39;t run anymore.
  */
<span class="p_del">-void mmu_notifier_unregister(struct mmu_notifier *mn, struct mm_struct *mm)</span>
<span class="p_add">+void mmu_notifier_unregister(struct mmu_notifier *mn, struct mm_struct *mm, struct mm_ref *mm_ref)</span>
 {
 	BUG_ON(atomic_read(&amp;mm-&gt;mm_count) &lt;= 0);
 
<span class="p_chunk">@@ -383,7 +383,7 @@</span> <span class="p_context"> void mmu_notifier_unregister(struct mmu_notifier *mn, struct mm_struct *mm)</span>
 
 	BUG_ON(atomic_read(&amp;mm-&gt;mm_count) &lt;= 0);
 
<span class="p_del">-	mmdrop(mm);</span>
<span class="p_add">+	mmdrop(mm, mm_ref);</span>
 }
 EXPORT_SYMBOL_GPL(mmu_notifier_unregister);
 
<span class="p_chunk">@@ -391,7 +391,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(mmu_notifier_unregister);</span>
  * Same as mmu_notifier_unregister but no callback and no srcu synchronization.
  */
 void mmu_notifier_unregister_no_release(struct mmu_notifier *mn,
<span class="p_del">-					struct mm_struct *mm)</span>
<span class="p_add">+					struct mm_struct *mm, struct mm_ref *mm_ref)</span>
 {
 	spin_lock(&amp;mm-&gt;mmu_notifier_mm-&gt;lock);
 	/*
<span class="p_chunk">@@ -402,7 +402,7 @@</span> <span class="p_context"> void mmu_notifier_unregister_no_release(struct mmu_notifier *mn,</span>
 	spin_unlock(&amp;mm-&gt;mmu_notifier_mm-&gt;lock);
 
 	BUG_ON(atomic_read(&amp;mm-&gt;mm_count) &lt;= 0);
<span class="p_del">-	mmdrop(mm);</span>
<span class="p_add">+	mmdrop(mm, mm_ref);</span>
 }
 EXPORT_SYMBOL_GPL(mmu_notifier_unregister_no_release);
 
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index ead093c6f2a6..0aa0b364ec0e 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -463,6 +463,7 @@</span> <span class="p_context"> static DEFINE_SPINLOCK(oom_reaper_lock);</span>
 
 static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)
 {
<span class="p_add">+	MM_REF(mm_ref);</span>
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
 	struct zap_details details = {.check_swap_entries = true,
<span class="p_chunk">@@ -495,7 +496,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 * that the mmput_async is called only when we have reaped something
 	 * and delayed __mmput doesn&#39;t matter that much
 	 */
<span class="p_del">-	if (!mmget_not_zero(mm)) {</span>
<span class="p_add">+	if (!mmget_not_zero(mm, &amp;mm_ref)) {</span>
 		up_read(&amp;mm-&gt;mmap_sem);
 		goto unlock_oom;
 	}
<span class="p_chunk">@@ -547,7 +548,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 * different context because we shouldn&#39;t risk we get stuck there and
 	 * put the oom_reaper out of the way.
 	 */
<span class="p_del">-	mmput_async(mm);</span>
<span class="p_add">+	mmput_async(mm, &amp;mm_ref);</span>
 unlock_oom:
 	mutex_unlock(&amp;oom_lock);
 	return ret;
<span class="p_chunk">@@ -660,7 +661,7 @@</span> <span class="p_context"> static void mark_oom_victim(struct task_struct *tsk)</span>
 
 	/* oom_mm is bound to the signal struct life time. */
 	if (!cmpxchg(&amp;tsk-&gt;signal-&gt;oom_mm, NULL, mm))
<span class="p_del">-		mmgrab(tsk-&gt;signal-&gt;oom_mm);</span>
<span class="p_add">+		mmgrab(tsk-&gt;signal-&gt;oom_mm, &amp;tsk-&gt;signal-&gt;oom_mm_ref);</span>
 
 	/*
 	 * Make sure that the task is woken up from uninterruptible sleep
<span class="p_chunk">@@ -812,6 +813,7 @@</span> <span class="p_context"> static void oom_kill_process(struct oom_control *oc, const char *message)</span>
 	struct task_struct *child;
 	struct task_struct *t;
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	unsigned int victim_points = 0;
 	static DEFINE_RATELIMIT_STATE(oom_rs, DEFAULT_RATELIMIT_INTERVAL,
 					      DEFAULT_RATELIMIT_BURST);
<span class="p_chunk">@@ -877,7 +879,7 @@</span> <span class="p_context"> static void oom_kill_process(struct oom_control *oc, const char *message)</span>
 
 	/* Get a reference to safely compare mm after task_unlock(victim) */
 	mm = victim-&gt;mm;
<span class="p_del">-	mmgrab(mm);</span>
<span class="p_add">+	mmgrab(mm, &amp;mm_ref);</span>
 	/*
 	 * We should send SIGKILL before setting TIF_MEMDIE in order to prevent
 	 * the OOM victim from depleting the memory reserves from the user
<span class="p_chunk">@@ -928,7 +930,7 @@</span> <span class="p_context"> static void oom_kill_process(struct oom_control *oc, const char *message)</span>
 	if (can_oom_reap)
 		wake_oom_reaper(victim);
 
<span class="p_del">-	mmdrop(mm);</span>
<span class="p_add">+	mmdrop(mm, &amp;mm_ref);</span>
 	put_task_struct(victim);
 }
 #undef K
<span class="p_header">diff --git a/mm/process_vm_access.c b/mm/process_vm_access.c</span>
<span class="p_header">index be8dc8d1edb9..8eef73c5ed81 100644</span>
<span class="p_header">--- a/mm/process_vm_access.c</span>
<span class="p_header">+++ b/mm/process_vm_access.c</span>
<span class="p_chunk">@@ -155,6 +155,7 @@</span> <span class="p_context"> static ssize_t process_vm_rw_core(pid_t pid, struct iov_iter *iter,</span>
 	struct page *pp_stack[PVM_MAX_PP_ARRAY_COUNT];
 	struct page **process_pages = pp_stack;
 	struct mm_struct *mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 	unsigned long i;
 	ssize_t rc = 0;
 	unsigned long nr_pages = 0;
<span class="p_chunk">@@ -202,7 +203,7 @@</span> <span class="p_context"> static ssize_t process_vm_rw_core(pid_t pid, struct iov_iter *iter,</span>
 		goto free_proc_pages;
 	}
 
<span class="p_del">-	mm = mm_access(task, PTRACE_MODE_ATTACH_REALCREDS);</span>
<span class="p_add">+	mm = mm_access(task, PTRACE_MODE_ATTACH_REALCREDS, &amp;mm_ref);</span>
 	if (!mm || IS_ERR(mm)) {
 		rc = IS_ERR(mm) ? PTR_ERR(mm) : -ESRCH;
 		/*
<span class="p_chunk">@@ -228,7 +229,7 @@</span> <span class="p_context"> static ssize_t process_vm_rw_core(pid_t pid, struct iov_iter *iter,</span>
 	if (total_len)
 		rc = total_len;
 
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 
 put_task_struct:
 	put_task_struct(task);
<span class="p_header">diff --git a/mm/swapfile.c b/mm/swapfile.c</span>
<span class="p_header">index 8c92829326cb..781122d8be77 100644</span>
<span class="p_header">--- a/mm/swapfile.c</span>
<span class="p_header">+++ b/mm/swapfile.c</span>
<span class="p_chunk">@@ -1376,6 +1376,7 @@</span> <span class="p_context"> int try_to_unuse(unsigned int type, bool frontswap,</span>
 {
 	struct swap_info_struct *si = swap_info[type];
 	struct mm_struct *start_mm;
<span class="p_add">+	MM_REF(start_mm_ref);</span>
 	volatile unsigned char *swap_map; /* swap_map is accessed without
 					   * locking. Mark it as volatile
 					   * to prevent compiler doing
<span class="p_chunk">@@ -1402,7 +1403,7 @@</span> <span class="p_context"> int try_to_unuse(unsigned int type, bool frontswap,</span>
 	 * that.
 	 */
 	start_mm = &amp;init_mm;
<span class="p_del">-	mmget(&amp;init_mm);</span>
<span class="p_add">+	mmget(&amp;init_mm, &amp;start_mm_ref);</span>
 
 	/*
 	 * Keep on scanning until all entries have gone.  Usually,
<span class="p_chunk">@@ -1449,9 +1450,9 @@</span> <span class="p_context"> int try_to_unuse(unsigned int type, bool frontswap,</span>
 		 * Don&#39;t hold on to start_mm if it looks like exiting.
 		 */
 		if (atomic_read(&amp;start_mm-&gt;mm_users) == 1) {
<span class="p_del">-			mmput(start_mm);</span>
<span class="p_add">+			mmput(start_mm, &amp;start_mm_ref);</span>
 			start_mm = &amp;init_mm;
<span class="p_del">-			mmget(&amp;init_mm);</span>
<span class="p_add">+			mmget(&amp;init_mm, &amp;start_mm_ref);</span>
 		}
 
 		/*
<span class="p_chunk">@@ -1485,19 +1486,22 @@</span> <span class="p_context"> int try_to_unuse(unsigned int type, bool frontswap,</span>
 			int set_start_mm = (*swap_map &gt;= swcount);
 			struct list_head *p = &amp;start_mm-&gt;mmlist;
 			struct mm_struct *new_start_mm = start_mm;
<span class="p_add">+			MM_REF(new_start_mm_ref);</span>
 			struct mm_struct *prev_mm = start_mm;
<span class="p_add">+			MM_REF(prev_mm_ref);</span>
 			struct mm_struct *mm;
<span class="p_add">+			MM_REF(mm_ref);</span>
 
<span class="p_del">-			mmget(new_start_mm);</span>
<span class="p_del">-			mmget(prev_mm);</span>
<span class="p_add">+			mmget(new_start_mm, &amp;new_start_mm_ref);</span>
<span class="p_add">+			mmget(prev_mm, &amp;prev_mm_ref);</span>
 			spin_lock(&amp;mmlist_lock);
 			while (swap_count(*swap_map) &amp;&amp; !retval &amp;&amp;
 					(p = p-&gt;next) != &amp;start_mm-&gt;mmlist) {
 				mm = list_entry(p, struct mm_struct, mmlist);
<span class="p_del">-				if (!mmget_not_zero(mm))</span>
<span class="p_add">+				if (!mmget_not_zero(mm, &amp;mm_ref))</span>
 					continue;
 				spin_unlock(&amp;mmlist_lock);
<span class="p_del">-				mmput(prev_mm);</span>
<span class="p_add">+				mmput(prev_mm, &amp;prev_mm_ref);</span>
 				prev_mm = mm;
 
 				cond_resched();
<span class="p_chunk">@@ -1511,17 +1515,18 @@</span> <span class="p_context"> int try_to_unuse(unsigned int type, bool frontswap,</span>
 					retval = unuse_mm(mm, entry, page);
 
 				if (set_start_mm &amp;&amp; *swap_map &lt; swcount) {
<span class="p_del">-					mmput(new_start_mm);</span>
<span class="p_del">-					mmget(mm);</span>
<span class="p_add">+					mmput(new_start_mm, &amp;new_start_mm_ref);</span>
<span class="p_add">+					mmget(mm, &amp;mm_ref);</span>
 					new_start_mm = mm;
 					set_start_mm = 0;
 				}
 				spin_lock(&amp;mmlist_lock);
 			}
 			spin_unlock(&amp;mmlist_lock);
<span class="p_del">-			mmput(prev_mm);</span>
<span class="p_del">-			mmput(start_mm);</span>
<span class="p_add">+			mmput(prev_mm, &amp;prev_mm_ref);</span>
<span class="p_add">+			mmput(start_mm, &amp;start_mm_ref);</span>
 			start_mm = new_start_mm;
<span class="p_add">+			move_mm_users_ref(start_mm, &amp;new_start_mm_ref, &amp;start_mm_ref);</span>
 		}
 		if (retval) {
 			unlock_page(page);
<span class="p_chunk">@@ -1590,7 +1595,7 @@</span> <span class="p_context"> int try_to_unuse(unsigned int type, bool frontswap,</span>
 		}
 	}
 
<span class="p_del">-	mmput(start_mm);</span>
<span class="p_add">+	mmput(start_mm, &amp;start_mm_ref);</span>
 	return retval;
 }
 
<span class="p_header">diff --git a/mm/util.c b/mm/util.c</span>
<span class="p_header">index 1a41553db866..9bace6820707 100644</span>
<span class="p_header">--- a/mm/util.c</span>
<span class="p_header">+++ b/mm/util.c</span>
<span class="p_chunk">@@ -607,7 +607,8 @@</span> <span class="p_context"> int get_cmdline(struct task_struct *task, char *buffer, int buflen)</span>
 {
 	int res = 0;
 	unsigned int len;
<span class="p_del">-	struct mm_struct *mm = get_task_mm(task);</span>
<span class="p_add">+	MM_REF(mm_ref);</span>
<span class="p_add">+	struct mm_struct *mm = get_task_mm(task, &amp;mm_ref);</span>
 	unsigned long arg_start, arg_end, env_start, env_end;
 	if (!mm)
 		goto out;
<span class="p_chunk">@@ -647,7 +648,7 @@</span> <span class="p_context"> int get_cmdline(struct task_struct *task, char *buffer, int buflen)</span>
 		}
 	}
 out_mm:
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;mm_ref);</span>
 out:
 	return res;
 }
<span class="p_header">diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c</span>
<span class="p_header">index 9ec9cef2b207..972084e84bd6 100644</span>
<span class="p_header">--- a/virt/kvm/async_pf.c</span>
<span class="p_header">+++ b/virt/kvm/async_pf.c</span>
<span class="p_chunk">@@ -108,7 +108,7 @@</span> <span class="p_context"> static void async_pf_execute(struct work_struct *work)</span>
 	if (swait_active(&amp;vcpu-&gt;wq))
 		swake_up(&amp;vcpu-&gt;wq);
 
<span class="p_del">-	mmput(mm);</span>
<span class="p_add">+	mmput(mm, &amp;apf-&gt;mm_ref);</span>
 	kvm_put_kvm(vcpu-&gt;kvm);
 }
 
<span class="p_chunk">@@ -135,7 +135,7 @@</span> <span class="p_context"> void kvm_clear_async_pf_completion_queue(struct kvm_vcpu *vcpu)</span>
 		flush_work(&amp;work-&gt;work);
 #else
 		if (cancel_work_sync(&amp;work-&gt;work)) {
<span class="p_del">-			mmput(work-&gt;mm);</span>
<span class="p_add">+			mmput(work-&gt;mm, &amp;work-&gt;mm_ref);</span>
 			kvm_put_kvm(vcpu-&gt;kvm); /* == work-&gt;vcpu-&gt;kvm */
 			kmem_cache_free(async_pf_cache, work);
 		}
<span class="p_chunk">@@ -200,7 +200,8 @@</span> <span class="p_context"> int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, unsigned long hva,</span>
 	work-&gt;addr = hva;
 	work-&gt;arch = *arch;
 	work-&gt;mm = current-&gt;mm;
<span class="p_del">-	mmget(work-&gt;mm);</span>
<span class="p_add">+	INIT_MM_REF(&amp;work-&gt;mm_ref);</span>
<span class="p_add">+	mmget(work-&gt;mm, &amp;work-&gt;mm_ref);</span>
 	kvm_get_kvm(work-&gt;vcpu-&gt;kvm);
 
 	/* this can&#39;t really happen otherwise gfn_to_pfn_async
<span class="p_chunk">@@ -218,7 +219,7 @@</span> <span class="p_context"> int kvm_setup_async_pf(struct kvm_vcpu *vcpu, gva_t gva, unsigned long hva,</span>
 	return 1;
 retry_sync:
 	kvm_put_kvm(work-&gt;vcpu-&gt;kvm);
<span class="p_del">-	mmput(work-&gt;mm);</span>
<span class="p_add">+	mmput(work-&gt;mm, &amp;work-&gt;mm_ref);</span>
 	kmem_cache_free(async_pf_cache, work);
 	return 0;
 }
<span class="p_header">diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="p_header">index 43914b981691..d608457033d5 100644</span>
<span class="p_header">--- a/virt/kvm/kvm_main.c</span>
<span class="p_header">+++ b/virt/kvm/kvm_main.c</span>
<span class="p_chunk">@@ -482,7 +482,8 @@</span> <span class="p_context"> static const struct mmu_notifier_ops kvm_mmu_notifier_ops = {</span>
 static int kvm_init_mmu_notifier(struct kvm *kvm)
 {
 	kvm-&gt;mmu_notifier.ops = &amp;kvm_mmu_notifier_ops;
<span class="p_del">-	return mmu_notifier_register(&amp;kvm-&gt;mmu_notifier, current-&gt;mm);</span>
<span class="p_add">+	return mmu_notifier_register(&amp;kvm-&gt;mmu_notifier, current-&gt;mm,</span>
<span class="p_add">+		&amp;kvm-&gt;mmu_notifier_ref);</span>
 }
 
 #else  /* !(CONFIG_MMU_NOTIFIER &amp;&amp; KVM_ARCH_WANT_MMU_NOTIFIER) */
<span class="p_chunk">@@ -608,12 +609,13 @@</span> <span class="p_context"> static struct kvm *kvm_create_vm(unsigned long type)</span>
 {
 	int r, i;
 	struct kvm *kvm = kvm_arch_alloc_vm();
<span class="p_add">+	MM_REF(mm_ref);</span>
 
 	if (!kvm)
 		return ERR_PTR(-ENOMEM);
 
 	spin_lock_init(&amp;kvm-&gt;mmu_lock);
<span class="p_del">-	mmgrab(current-&gt;mm);</span>
<span class="p_add">+	mmgrab(current-&gt;mm, &amp;kvm-&gt;mm_ref);</span>
 	kvm-&gt;mm = current-&gt;mm;
 	kvm_eventfd_init(kvm);
 	mutex_init(&amp;kvm-&gt;lock);
<span class="p_chunk">@@ -654,6 +656,7 @@</span> <span class="p_context"> static struct kvm *kvm_create_vm(unsigned long type)</span>
 			goto out_err;
 	}
 
<span class="p_add">+	INIT_MM_REF(&amp;kvm-&gt;mmu_notifier_ref);</span>
 	r = kvm_init_mmu_notifier(kvm);
 	if (r)
 		goto out_err;
<span class="p_chunk">@@ -677,8 +680,9 @@</span> <span class="p_context"> static struct kvm *kvm_create_vm(unsigned long type)</span>
 		kfree(kvm-&gt;buses[i]);
 	for (i = 0; i &lt; KVM_ADDRESS_SPACE_NUM; i++)
 		kvm_free_memslots(kvm, kvm-&gt;memslots[i]);
<span class="p_add">+	move_mm_ref(kvm-&gt;mm, &amp;kvm-&gt;mm_ref, &amp;mm_ref);</span>
 	kvm_arch_free_vm(kvm);
<span class="p_del">-	mmdrop(current-&gt;mm);</span>
<span class="p_add">+	mmdrop(current-&gt;mm, &amp;mm_ref);</span>
 	return ERR_PTR(r);
 }
 
<span class="p_chunk">@@ -713,6 +717,7 @@</span> <span class="p_context"> static void kvm_destroy_vm(struct kvm *kvm)</span>
 {
 	int i;
 	struct mm_struct *mm = kvm-&gt;mm;
<span class="p_add">+	MM_REF(mm_ref);</span>
 
 	kvm_destroy_vm_debugfs(kvm);
 	kvm_arch_sync_events(kvm);
<span class="p_chunk">@@ -724,7 +729,7 @@</span> <span class="p_context"> static void kvm_destroy_vm(struct kvm *kvm)</span>
 		kvm_io_bus_destroy(kvm-&gt;buses[i]);
 	kvm_coalesced_mmio_free(kvm);
 #if defined(CONFIG_MMU_NOTIFIER) &amp;&amp; defined(KVM_ARCH_WANT_MMU_NOTIFIER)
<span class="p_del">-	mmu_notifier_unregister(&amp;kvm-&gt;mmu_notifier, kvm-&gt;mm);</span>
<span class="p_add">+	mmu_notifier_unregister(&amp;kvm-&gt;mmu_notifier, kvm-&gt;mm, &amp;kvm-&gt;mmu_notifier_ref);</span>
 #else
 	kvm_arch_flush_shadow_all(kvm);
 #endif
<span class="p_chunk">@@ -734,10 +739,11 @@</span> <span class="p_context"> static void kvm_destroy_vm(struct kvm *kvm)</span>
 		kvm_free_memslots(kvm, kvm-&gt;memslots[i]);
 	cleanup_srcu_struct(&amp;kvm-&gt;irq_srcu);
 	cleanup_srcu_struct(&amp;kvm-&gt;srcu);
<span class="p_add">+	move_mm_ref(mm, &amp;kvm-&gt;mm_ref, &amp;mm_ref);</span>
 	kvm_arch_free_vm(kvm);
 	preempt_notifier_dec();
 	hardware_disable_all();
<span class="p_del">-	mmdrop(mm);</span>
<span class="p_add">+	mmdrop(mm, &amp;mm_ref);</span>
 }
 
 void kvm_get_kvm(struct kvm *kvm)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



