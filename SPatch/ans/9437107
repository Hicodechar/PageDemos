
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,v13,08/18] mm/hmm: heterogeneous memory management (HMM for short) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,v13,08/18] mm/hmm: heterogeneous memory management (HMM for short)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 18, 2016, 6:18 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1479493107-982-9-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9437107/mbox/"
   >mbox</a>
|
   <a href="/patch/9437107/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9437107/">/patch/9437107/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7DDCD60469 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 18 Nov 2016 17:18:13 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6B31A299BE
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 18 Nov 2016 17:18:13 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 601CF299C2; Fri, 18 Nov 2016 17:18:13 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9D42B299C0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 18 Nov 2016 17:18:11 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754361AbcKRRR4 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 18 Nov 2016 12:17:56 -0500
Received: from mx1.redhat.com ([209.132.183.28]:45272 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932280AbcKRRRu (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 18 Nov 2016 12:17:50 -0500
Received: from int-mx09.intmail.prod.int.phx2.redhat.com
	(int-mx09.intmail.prod.int.phx2.redhat.com [10.5.11.22])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 26E03C05B1CB;
	Fri, 18 Nov 2016 17:17:50 +0000 (UTC)
Received: from xgl-cortex.ml2.eng.bos.redhat.com
	(xgl-cortex.ml2.eng.bos.redhat.com [10.19.160.80])
	by int-mx09.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id uAIHHd94016767; Fri, 18 Nov 2016 12:17:49 -0500
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Jatin Kumar &lt;jakumar@nvidia.com&gt;, Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;
Subject: [HMM v13 08/18] mm/hmm: heterogeneous memory management (HMM for
	short)
Date: Fri, 18 Nov 2016 13:18:17 -0500
Message-Id: &lt;1479493107-982-9-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1479493107-982-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1479493107-982-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.22
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.32]);
	Fri, 18 Nov 2016 17:17:50 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Nov. 18, 2016, 6:18 p.m.</div>
<pre class="content">
HMM provides 3 separate functionality :
    - Mirroring: synchronize CPU page table and device page table
    - Device memory: allocating struct page for device memory
    - Migration: migrating regular memory to device memory

This patch introduces some common helpers and definitions to all of
those 3 functionality.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Jatin Kumar &lt;jakumar@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
---
 MAINTAINERS              |   7 +++
 include/linux/hmm.h      | 139 +++++++++++++++++++++++++++++++++++++++++++++++
 include/linux/mm_types.h |   5 ++
 kernel/fork.c            |   2 +
 mm/Kconfig               |  11 ++++
 mm/Makefile              |   1 +
 mm/hmm.c                 |  86 +++++++++++++++++++++++++++++
 7 files changed, 251 insertions(+)
 create mode 100644 include/linux/hmm.h
 create mode 100644 mm/hmm.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - Nov. 21, 2016, 2:29 a.m.</div>
<pre class="content">
On 19/11/16 05:18, Jérôme Glisse wrote:
<span class="quote">&gt; HMM provides 3 separate functionality :</span>
<span class="quote">&gt;     - Mirroring: synchronize CPU page table and device page table</span>
<span class="quote">&gt;     - Device memory: allocating struct page for device memory</span>
<span class="quote">&gt;     - Migration: migrating regular memory to device memory</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch introduces some common helpers and definitions to all of</span>
<span class="quote">&gt; those 3 functionality.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Jatin Kumar &lt;jakumar@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  MAINTAINERS              |   7 +++</span>
<span class="quote">&gt;  include/linux/hmm.h      | 139 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  include/linux/mm_types.h |   5 ++</span>
<span class="quote">&gt;  kernel/fork.c            |   2 +</span>
<span class="quote">&gt;  mm/Kconfig               |  11 ++++</span>
<span class="quote">&gt;  mm/Makefile              |   1 +</span>
<span class="quote">&gt;  mm/hmm.c                 |  86 +++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  7 files changed, 251 insertions(+)</span>
<span class="quote">&gt;  create mode 100644 include/linux/hmm.h</span>
<span class="quote">&gt;  create mode 100644 mm/hmm.c</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="quote">&gt; index f593300..41cd63d 100644</span>
<span class="quote">&gt; --- a/MAINTAINERS</span>
<span class="quote">&gt; +++ b/MAINTAINERS</span>
<span class="quote">&gt; @@ -5582,6 +5582,13 @@ S:	Supported</span>
<span class="quote">&gt;  F:	drivers/scsi/hisi_sas/</span>
<span class="quote">&gt;  F:	Documentation/devicetree/bindings/scsi/hisilicon-sas.txt</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +HMM - Heterogeneous Memory Management</span>
<span class="quote">&gt; +M:	Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; +L:	linux-mm@kvack.org</span>
<span class="quote">&gt; +S:	Maintained</span>
<span class="quote">&gt; +F:	mm/hmm*</span>
<span class="quote">&gt; +F:	include/linux/hmm*</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  HOST AP DRIVER</span>
<span class="quote">&gt;  M:	Jouni Malinen &lt;j@w1.fi&gt;</span>
<span class="quote">&gt;  L:	hostap@shmoo.com (subscribers-only)</span>
<span class="quote">&gt; diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..54dd529</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/include/linux/hmm.h</span>
<span class="quote">&gt; @@ -0,0 +1,139 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright 2013 Red Hat Inc.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify</span>
<span class="quote">&gt; + * it under the terms of the GNU General Public License as published by</span>
<span class="quote">&gt; + * the Free Software Foundation; either version 2 of the License, or</span>
<span class="quote">&gt; + * (at your option) any later version.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; + * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; + * GNU General Public License for more details.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * HMM provides 3 separate functionality :</span>
<span class="quote">&gt; + *   - Mirroring: synchronize CPU page table and device page table</span>
<span class="quote">&gt; + *   - Device memory: allocating struct page for device memory</span>
<span class="quote">&gt; + *   - Migration: migrating regular memory to device memory</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Each can be use independently from the others.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Mirroring:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * HMM provide helpers to mirror process address space on a device. For this it</span>
<span class="quote">&gt; + * provides several helpers to order device page table update in respect to CPU</span>
<span class="quote">&gt; + * page table update. Requirement is that for any given virtual address the CPU</span>
<span class="quote">&gt; + * and device page table can not point to different physical page. It uses the</span>
<span class="quote">&gt; + * mmu_notifier API and introduce virtual address range lock which block CPU</span>
<span class="quote">&gt; + * page table update for a range while the device page table is being updated.</span>
<span class="quote">&gt; + * Usage pattern is:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *      hmm_vma_range_lock(vma, start, end);</span>
<span class="quote">&gt; + *      // snap shot CPU page table</span>
<span class="quote">&gt; + *      // update device page table from snapshot</span>
<span class="quote">&gt; + *      hmm_vma_range_unlock(vma, start, end);</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Any CPU page table update that conflict with a range lock will wait until</span>
<span class="quote">&gt; + * range is unlock. This garanty proper serialization of CPU and device page</span>
<span class="quote">&gt; + * table update.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Device memory:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * HMM provides helpers to help leverage device memory either addressable like</span>
<span class="quote">&gt; + * regular memory by the CPU or un-addressable at all. In both case the device</span>
<span class="quote">&gt; + * memory is associated to dedicated structs page (which are allocated like for</span>
<span class="quote">&gt; + * hotplug memory). Device memory management is under the responsability of the</span>
<span class="quote">&gt; + * device driver. HMM only allocate and initialize the struct pages associated</span>
<span class="quote">&gt; + * with the device memory.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Allocating struct page for device memory allow to use device memory allmost</span>
<span class="quote">&gt; + * like any regular memory. Unlike regular memory it can not be added to the</span>
<span class="quote">&gt; + * lru, nor can any memory allocation can use device memory directly. Device</span>
<span class="quote">&gt; + * memory will only end up to be use in a process if device driver migrate some</span>
				   in use 
<span class="quote">&gt; + * of the process memory from regular memory to device memory.</span>
<span class="quote">&gt; + *</span>

A process can never directly allocate device memory?
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + * Migration:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Existing memory migration mechanism (mm/migrate.c) does not allow to use</span>
<span class="quote">&gt; + * something else than the CPU to copy from source to destination memory. More</span>
<span class="quote">&gt; + * over existing code is not tailor to drive migration from process virtual</span>
				tailored
<span class="quote">&gt; + * address rather than from list of pages. Finaly the migration flow does not</span>
					      Finally 
<span class="quote">&gt; + * allow for graceful failure at different step of the migration process.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * HMM solves all of the above though simple API :</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *      hmm_vma_migrate(vma, start, end, ops);</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * With ops struct providing 2 callback alloc_and_copy() which allocated the</span>
<span class="quote">&gt; + * destination memory and initialize it using source memory. Migration can fail</span>
<span class="quote">&gt; + * after this step and thus last callback finalize_and_map() allow the device</span>
<span class="quote">&gt; + * driver to know which page were successfully migrated and which were not.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This can easily be use outside of HMM intended use case.</span>
<span class="quote">&gt; + *</span>

I think it is a good API to have
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + * This header file contain all the API related to this 3 functionality and</span>
<span class="quote">&gt; + * each functions and struct are more thouroughly documented in below comments.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#ifndef LINUX_HMM_H</span>
<span class="quote">&gt; +#define LINUX_HMM_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/kconfig.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_HMM)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * hmm_pfn_t - HMM use its own pfn type to keep several flags per page</span>
		      uses
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Flags:</span>
<span class="quote">&gt; + * HMM_PFN_VALID: pfn is valid</span>
<span class="quote">&gt; + * HMM_PFN_WRITE: CPU page table have the write permission set</span>
				    has
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define HMM_PFN_VALID (1 &lt;&lt; 0)</span>
<span class="quote">&gt; +#define HMM_PFN_WRITE (1 &lt;&lt; 1)</span>
<span class="quote">&gt; +#define HMM_PFN_SHIFT 2</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +	return pfn_to_page(pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline unsigned long hmm_pfn_to_pfn(hmm_pfn_t pfn)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="quote">&gt; +		return -1UL;</span>
<span class="quote">&gt; +	return (pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>

What is pfn_to_pfn? I presume it means CPU PFN to device PFN
or is it the reverse? Please add some comments
<span class="quote">
&gt; +static inline hmm_pfn_t hmm_pfn_from_page(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (page_to_pfn(page) &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline hmm_pfn_t hmm_pfn_from_pfn(unsigned long pfn)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (pfn &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>

Same as above
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +/* Below are for HMM internal use only ! Not to be use by device driver ! */</span>
<span class="quote">&gt; +void hmm_mm_destroy(struct mm_struct *mm);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#else /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Below are for HMM internal use only ! Not to be use by device driver ! */</span>
<span class="quote">&gt; +static inline void hmm_mm_destroy(struct mm_struct *mm) {}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="quote">&gt; +#endif /* LINUX_HMM_H */</span>
<span class="quote">&gt; diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="quote">&gt; index 4a8aced..4effdbf 100644</span>
<span class="quote">&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; @@ -23,6 +23,7 @@</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct address_space;</span>
<span class="quote">&gt;  struct mem_cgroup;</span>
<span class="quote">&gt; +struct hmm;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define USE_SPLIT_PTE_PTLOCKS	(NR_CPUS &gt;= CONFIG_SPLIT_PTLOCK_CPUS)</span>
<span class="quote">&gt;  #define USE_SPLIT_PMD_PTLOCKS	(USE_SPLIT_PTE_PTLOCKS &amp;&amp; \</span>
<span class="quote">&gt; @@ -516,6 +517,10 @@ struct mm_struct {</span>
<span class="quote">&gt;  	atomic_long_t hugetlb_usage;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	struct work_struct async_put_work;</span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_HMM)</span>
<span class="quote">&gt; +	/* HMM need to track few things per mm */</span>
<span class="quote">&gt; +	struct hmm *hmm;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void mm_init_cpumask(struct mm_struct *mm)</span>
<span class="quote">&gt; diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="quote">&gt; index 690a1aad..af0eec8 100644</span>
<span class="quote">&gt; --- a/kernel/fork.c</span>
<span class="quote">&gt; +++ b/kernel/fork.c</span>
<span class="quote">&gt; @@ -27,6 +27,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/binfmts.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mman.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mmu_notifier.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/hmm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/fs.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/vmacache.h&gt;</span>
<span class="quote">&gt; @@ -702,6 +703,7 @@ void __mmdrop(struct mm_struct *mm)</span>
<span class="quote">&gt;  	BUG_ON(mm == &amp;init_mm);</span>
<span class="quote">&gt;  	mm_free_pgd(mm);</span>
<span class="quote">&gt;  	destroy_context(mm);</span>
<span class="quote">&gt; +	hmm_mm_destroy(mm);</span>
<span class="quote">&gt;  	mmu_notifier_mm_destroy(mm);</span>
<span class="quote">&gt;  	check_mm(mm);</span>
<span class="quote">&gt;  	free_mm(mm);</span>
<span class="quote">&gt; diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="quote">&gt; index 0a21411..be18cc2 100644</span>
<span class="quote">&gt; --- a/mm/Kconfig</span>
<span class="quote">&gt; +++ b/mm/Kconfig</span>
<span class="quote">&gt; @@ -289,6 +289,17 @@ config MIGRATION</span>
<span class="quote">&gt;  config ARCH_ENABLE_HUGEPAGE_MIGRATION</span>
<span class="quote">&gt;  	bool</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config HMM</span>
<span class="quote">&gt; +	bool &quot;Heterogeneous memory management (HMM)&quot;</span>
<span class="quote">&gt; +	depends on MMU</span>
<span class="quote">&gt; +	default n</span>
<span class="quote">&gt; +	help</span>
<span class="quote">&gt; +	  Heterogeneous memory management, set of helpers for:</span>
<span class="quote">&gt; +	    - mirroring of process address space on a device</span>
<span class="quote">&gt; +	    - using device memory transparently inside a process</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  If unsure, say N to disable HMM.</span>
<span class="quote">&gt; +</span>

It would be nice to split this into HMM, HMM_MIGRATE and HMM_MIRROR
<span class="quote">
&gt;  config PHYS_ADDR_T_64BIT</span>
<span class="quote">&gt;  	def_bool 64BIT || ARCH_PHYS_ADDR_T_64BIT</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="quote">&gt; index 2ca1faf..6ac1284 100644</span>
<span class="quote">&gt; --- a/mm/Makefile</span>
<span class="quote">&gt; +++ b/mm/Makefile</span>
<span class="quote">&gt; @@ -76,6 +76,7 @@ obj-$(CONFIG_FAILSLAB) += failslab.o</span>
<span class="quote">&gt;  obj-$(CONFIG_MEMORY_HOTPLUG) += memory_hotplug.o</span>
<span class="quote">&gt;  obj-$(CONFIG_MEMTEST)		+= memtest.o</span>
<span class="quote">&gt;  obj-$(CONFIG_MIGRATION) += migrate.o</span>
<span class="quote">&gt; +obj-$(CONFIG_HMM) += hmm.o</span>
<span class="quote">&gt;  obj-$(CONFIG_QUICKLIST) += quicklist.o</span>
<span class="quote">&gt;  obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o</span>
<span class="quote">&gt;  obj-$(CONFIG_PAGE_COUNTER) += page_counter.o</span>
<span class="quote">&gt; diff --git a/mm/hmm.c b/mm/hmm.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..342b596</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/mm/hmm.c</span>
<span class="quote">&gt; @@ -0,0 +1,86 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright 2013 Red Hat Inc.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify</span>
<span class="quote">&gt; + * it under the terms of the GNU General Public License as published by</span>
<span class="quote">&gt; + * the Free Software Foundation; either version 2 of the License, or</span>
<span class="quote">&gt; + * (at your option) any later version.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; + * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; + * GNU General Public License for more details.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Refer to include/linux/hmm.h for informations about heterogeneous memory</span>
<span class="quote">&gt; + * management or HMM for short.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/hmm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * struct hmm - HMM per mm struct</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @mm: mm struct this HMM struct is bound to</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct hmm {</span>
<span class="quote">&gt; +	struct mm_struct	*mm;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * hmm_register - register HMM against an mm (HMM internal)</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @mm: mm struct to attach to</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This is not intended to be use directly by device driver but by other HMM</span>
<span class="quote">&gt; + * component. It allocates an HMM struct if mm does not have one and initialize</span>
<span class="quote">&gt; + * it.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static struct hmm *hmm_register(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hmm *hmm = NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!mm-&gt;hmm) {</span>
<span class="quote">&gt; +		hmm = kmalloc(sizeof(*hmm), GFP_KERNEL);</span>
<span class="quote">&gt; +		if (!hmm)</span>
<span class="quote">&gt; +			return NULL;</span>
<span class="quote">&gt; +		hmm-&gt;mm = mm;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt; +	if (!mm-&gt;hmm)</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * The hmm struct can only be free once mm_struct goes away</span>
<span class="quote">&gt; +		 * hence we should always have pre-allocated an new hmm struct</span>
<span class="quote">&gt; +		 * above.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		mm-&gt;hmm = hmm;</span>
<span class="quote">&gt; +	else if (hmm)</span>
<span class="quote">&gt; +		kfree(hmm);</span>
<span class="quote">&gt; +	hmm = mm-&gt;hmm;</span>
<span class="quote">&gt; +	spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return hmm;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void hmm_mm_destroy(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hmm *hmm;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We should not need to lock here as no one should be able to register</span>
<span class="quote">&gt; +	 * a new HMM while an mm is being destroy. But just to be safe ...</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt; +	hmm = mm-&gt;hmm;</span>
<span class="quote">&gt; +	mm-&gt;hmm = NULL;</span>
<span class="quote">&gt; +	spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt; +	if (!hmm)</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>

kfree can deal with NULL pointers, you can remove the if check
<span class="quote">
&gt; +	kfree(hmm);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Nov. 21, 2016, 5:14 a.m.</div>
<pre class="content">
On Mon, Nov 21, 2016 at 01:29:23PM +1100, Balbir Singh wrote:
<span class="quote">&gt; On 19/11/16 05:18, Jérôme Glisse wrote:</span>
<span class="quote">&gt; &gt; HMM provides 3 separate functionality :</span>
<span class="quote">&gt; &gt;     - Mirroring: synchronize CPU page table and device page table</span>
<span class="quote">&gt; &gt;     - Device memory: allocating struct page for device memory</span>
<span class="quote">&gt; &gt;     - Migration: migrating regular memory to device memory</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This patch introduces some common helpers and definitions to all of</span>
<span class="quote">&gt; &gt; those 3 functionality.</span>
<span class="quote">&gt; &gt; </span>

[...]
<span class="quote">
&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * HMM provides 3 separate functionality :</span>
<span class="quote">&gt; &gt; + *   - Mirroring: synchronize CPU page table and device page table</span>
<span class="quote">&gt; &gt; + *   - Device memory: allocating struct page for device memory</span>
<span class="quote">&gt; &gt; + *   - Migration: migrating regular memory to device memory</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Each can be use independently from the others.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Mirroring:</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * HMM provide helpers to mirror process address space on a device. For this it</span>
<span class="quote">&gt; &gt; + * provides several helpers to order device page table update in respect to CPU</span>
<span class="quote">&gt; &gt; + * page table update. Requirement is that for any given virtual address the CPU</span>
<span class="quote">&gt; &gt; + * and device page table can not point to different physical page. It uses the</span>
<span class="quote">&gt; &gt; + * mmu_notifier API and introduce virtual address range lock which block CPU</span>
<span class="quote">&gt; &gt; + * page table update for a range while the device page table is being updated.</span>
<span class="quote">&gt; &gt; + * Usage pattern is:</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + *      hmm_vma_range_lock(vma, start, end);</span>
<span class="quote">&gt; &gt; + *      // snap shot CPU page table</span>
<span class="quote">&gt; &gt; + *      // update device page table from snapshot</span>
<span class="quote">&gt; &gt; + *      hmm_vma_range_unlock(vma, start, end);</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Any CPU page table update that conflict with a range lock will wait until</span>
<span class="quote">&gt; &gt; + * range is unlock. This garanty proper serialization of CPU and device page</span>
<span class="quote">&gt; &gt; + * table update.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Device memory:</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * HMM provides helpers to help leverage device memory either addressable like</span>
<span class="quote">&gt; &gt; + * regular memory by the CPU or un-addressable at all. In both case the device</span>
<span class="quote">&gt; &gt; + * memory is associated to dedicated structs page (which are allocated like for</span>
<span class="quote">&gt; &gt; + * hotplug memory). Device memory management is under the responsability of the</span>
<span class="quote">&gt; &gt; + * device driver. HMM only allocate and initialize the struct pages associated</span>
<span class="quote">&gt; &gt; + * with the device memory.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Allocating struct page for device memory allow to use device memory allmost</span>
<span class="quote">&gt; &gt; + * like any regular memory. Unlike regular memory it can not be added to the</span>
<span class="quote">&gt; &gt; + * lru, nor can any memory allocation can use device memory directly. Device</span>
<span class="quote">&gt; &gt; + * memory will only end up to be use in a process if device driver migrate some</span>
<span class="quote">&gt; 				   in use </span>
<span class="quote">&gt; &gt; + * of the process memory from regular memory to device memory.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A process can never directly allocate device memory?</span>

Well yes and no, if the device driver is first to trigger a page fault on some
memory then it can decide to directly allocate device memory. But usual CPU page
fault would not trigger allocation of device memory. A new mechanism can be added
to achieve that if that make sense but for my main target (x86/pcie) it does not.
<span class="quote">
&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Migration:</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Existing memory migration mechanism (mm/migrate.c) does not allow to use</span>
<span class="quote">&gt; &gt; + * something else than the CPU to copy from source to destination memory. More</span>
<span class="quote">&gt; &gt; + * over existing code is not tailor to drive migration from process virtual</span>
<span class="quote">&gt; 				tailored</span>
<span class="quote">&gt; &gt; + * address rather than from list of pages. Finaly the migration flow does not</span>
<span class="quote">&gt; 					      Finally </span>
<span class="quote">&gt; &gt; + * allow for graceful failure at different step of the migration process.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * HMM solves all of the above though simple API :</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + *      hmm_vma_migrate(vma, start, end, ops);</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * With ops struct providing 2 callback alloc_and_copy() which allocated the</span>
<span class="quote">&gt; &gt; + * destination memory and initialize it using source memory. Migration can fail</span>
<span class="quote">&gt; &gt; + * after this step and thus last callback finalize_and_map() allow the device</span>
<span class="quote">&gt; &gt; + * driver to know which page were successfully migrated and which were not.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This can easily be use outside of HMM intended use case.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think it is a good API to have</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This header file contain all the API related to this 3 functionality and</span>
<span class="quote">&gt; &gt; + * each functions and struct are more thouroughly documented in below comments.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +#ifndef LINUX_HMM_H</span>
<span class="quote">&gt; &gt; +#define LINUX_HMM_H</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#include &lt;linux/kconfig.h&gt;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#if IS_ENABLED(CONFIG_HMM)</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * hmm_pfn_t - HMM use its own pfn type to keep several flags per page</span>
<span class="quote">&gt; 		      uses</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Flags:</span>
<span class="quote">&gt; &gt; + * HMM_PFN_VALID: pfn is valid</span>
<span class="quote">&gt; &gt; + * HMM_PFN_WRITE: CPU page table have the write permission set</span>
<span class="quote">&gt; 				    has</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_VALID (1 &lt;&lt; 0)</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_WRITE (1 &lt;&lt; 1)</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_SHIFT 2</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="quote">&gt; &gt; +		return NULL;</span>
<span class="quote">&gt; &gt; +	return pfn_to_page(pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline unsigned long hmm_pfn_to_pfn(hmm_pfn_t pfn)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="quote">&gt; &gt; +		return -1UL;</span>
<span class="quote">&gt; &gt; +	return (pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What is pfn_to_pfn? I presume it means CPU PFN to device PFN</span>
<span class="quote">&gt; or is it the reverse? Please add some comments</span>

It is hmm_pfn_t to pfn value as an unsigned long. The memory the pfn
points to can be anything (regular system memory, device memory, ...).

hmm_pfn_t is just a pfn with a set of flags.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +static inline hmm_pfn_t hmm_pfn_from_page(struct page *page)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return (page_to_pfn(page) &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline hmm_pfn_t hmm_pfn_from_pfn(unsigned long pfn)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return (pfn &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Same as above</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/* Below are for HMM internal use only ! Not to be use by device driver ! */</span>
<span class="quote">&gt; &gt; +void hmm_mm_destroy(struct mm_struct *mm);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#else /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/* Below are for HMM internal use only ! Not to be use by device driver ! */</span>
<span class="quote">&gt; &gt; +static inline void hmm_mm_destroy(struct mm_struct *mm) {}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#endif /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="quote">&gt; &gt; +#endif /* LINUX_HMM_H */</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="quote">&gt; &gt; index 4a8aced..4effdbf 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; &gt; @@ -23,6 +23,7 @@</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  struct address_space;</span>
<span class="quote">&gt; &gt;  struct mem_cgroup;</span>
<span class="quote">&gt; &gt; +struct hmm;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  #define USE_SPLIT_PTE_PTLOCKS	(NR_CPUS &gt;= CONFIG_SPLIT_PTLOCK_CPUS)</span>
<span class="quote">&gt; &gt;  #define USE_SPLIT_PMD_PTLOCKS	(USE_SPLIT_PTE_PTLOCKS &amp;&amp; \</span>
<span class="quote">&gt; &gt; @@ -516,6 +517,10 @@ struct mm_struct {</span>
<span class="quote">&gt; &gt;  	atomic_long_t hugetlb_usage;</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt;  	struct work_struct async_put_work;</span>
<span class="quote">&gt; &gt; +#if IS_ENABLED(CONFIG_HMM)</span>
<span class="quote">&gt; &gt; +	/* HMM need to track few things per mm */</span>
<span class="quote">&gt; &gt; +	struct hmm *hmm;</span>
<span class="quote">&gt; &gt; +#endif</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static inline void mm_init_cpumask(struct mm_struct *mm)</span>
<span class="quote">&gt; &gt; diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="quote">&gt; &gt; index 690a1aad..af0eec8 100644</span>
<span class="quote">&gt; &gt; --- a/kernel/fork.c</span>
<span class="quote">&gt; &gt; +++ b/kernel/fork.c</span>
<span class="quote">&gt; &gt; @@ -27,6 +27,7 @@</span>
<span class="quote">&gt; &gt;  #include &lt;linux/binfmts.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/mman.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/mmu_notifier.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/hmm.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/fs.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/vmacache.h&gt;</span>
<span class="quote">&gt; &gt; @@ -702,6 +703,7 @@ void __mmdrop(struct mm_struct *mm)</span>
<span class="quote">&gt; &gt;  	BUG_ON(mm == &amp;init_mm);</span>
<span class="quote">&gt; &gt;  	mm_free_pgd(mm);</span>
<span class="quote">&gt; &gt;  	destroy_context(mm);</span>
<span class="quote">&gt; &gt; +	hmm_mm_destroy(mm);</span>
<span class="quote">&gt; &gt;  	mmu_notifier_mm_destroy(mm);</span>
<span class="quote">&gt; &gt;  	check_mm(mm);</span>
<span class="quote">&gt; &gt;  	free_mm(mm);</span>
<span class="quote">&gt; &gt; diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="quote">&gt; &gt; index 0a21411..be18cc2 100644</span>
<span class="quote">&gt; &gt; --- a/mm/Kconfig</span>
<span class="quote">&gt; &gt; +++ b/mm/Kconfig</span>
<span class="quote">&gt; &gt; @@ -289,6 +289,17 @@ config MIGRATION</span>
<span class="quote">&gt; &gt;  config ARCH_ENABLE_HUGEPAGE_MIGRATION</span>
<span class="quote">&gt; &gt;  	bool</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +config HMM</span>
<span class="quote">&gt; &gt; +	bool &quot;Heterogeneous memory management (HMM)&quot;</span>
<span class="quote">&gt; &gt; +	depends on MMU</span>
<span class="quote">&gt; &gt; +	default n</span>
<span class="quote">&gt; &gt; +	help</span>
<span class="quote">&gt; &gt; +	  Heterogeneous memory management, set of helpers for:</span>
<span class="quote">&gt; &gt; +	    - mirroring of process address space on a device</span>
<span class="quote">&gt; &gt; +	    - using device memory transparently inside a process</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	  If unsure, say N to disable HMM.</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It would be nice to split this into HMM, HMM_MIGRATE and HMM_MIRROR</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;  config PHYS_ADDR_T_64BIT</span>
<span class="quote">&gt; &gt;  	def_bool 64BIT || ARCH_PHYS_ADDR_T_64BIT</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="quote">&gt; &gt; index 2ca1faf..6ac1284 100644</span>
<span class="quote">&gt; &gt; --- a/mm/Makefile</span>
<span class="quote">&gt; &gt; +++ b/mm/Makefile</span>
<span class="quote">&gt; &gt; @@ -76,6 +76,7 @@ obj-$(CONFIG_FAILSLAB) += failslab.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_MEMORY_HOTPLUG) += memory_hotplug.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_MEMTEST)		+= memtest.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_MIGRATION) += migrate.o</span>
<span class="quote">&gt; &gt; +obj-$(CONFIG_HMM) += hmm.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_QUICKLIST) += quicklist.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o</span>
<span class="quote">&gt; &gt;  obj-$(CONFIG_PAGE_COUNTER) += page_counter.o</span>
<span class="quote">&gt; &gt; diff --git a/mm/hmm.c b/mm/hmm.c</span>
<span class="quote">&gt; &gt; new file mode 100644</span>
<span class="quote">&gt; &gt; index 0000000..342b596</span>
<span class="quote">&gt; &gt; --- /dev/null</span>
<span class="quote">&gt; &gt; +++ b/mm/hmm.c</span>
<span class="quote">&gt; &gt; @@ -0,0 +1,86 @@</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Copyright 2013 Red Hat Inc.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This program is free software; you can redistribute it and/or modify</span>
<span class="quote">&gt; &gt; + * it under the terms of the GNU General Public License as published by</span>
<span class="quote">&gt; &gt; + * the Free Software Foundation; either version 2 of the License, or</span>
<span class="quote">&gt; &gt; + * (at your option) any later version.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; &gt; + * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; &gt; + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; &gt; + * GNU General Public License for more details.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Refer to include/linux/hmm.h for informations about heterogeneous memory</span>
<span class="quote">&gt; &gt; + * management or HMM for short.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/hmm.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * struct hmm - HMM per mm struct</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * @mm: mm struct this HMM struct is bound to</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +struct hmm {</span>
<span class="quote">&gt; &gt; +	struct mm_struct	*mm;</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * hmm_register - register HMM against an mm (HMM internal)</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * @mm: mm struct to attach to</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This is not intended to be use directly by device driver but by other HMM</span>
<span class="quote">&gt; &gt; + * component. It allocates an HMM struct if mm does not have one and initialize</span>
<span class="quote">&gt; &gt; + * it.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +static struct hmm *hmm_register(struct mm_struct *mm)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hmm *hmm = NULL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!mm-&gt;hmm) {</span>
<span class="quote">&gt; &gt; +		hmm = kmalloc(sizeof(*hmm), GFP_KERNEL);</span>
<span class="quote">&gt; &gt; +		if (!hmm)</span>
<span class="quote">&gt; &gt; +			return NULL;</span>
<span class="quote">&gt; &gt; +		hmm-&gt;mm = mm;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt; &gt; +	if (!mm-&gt;hmm)</span>
<span class="quote">&gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; +		 * The hmm struct can only be free once mm_struct goes away</span>
<span class="quote">&gt; &gt; +		 * hence we should always have pre-allocated an new hmm struct</span>
<span class="quote">&gt; &gt; +		 * above.</span>
<span class="quote">&gt; &gt; +		 */</span>
<span class="quote">&gt; &gt; +		mm-&gt;hmm = hmm;</span>
<span class="quote">&gt; &gt; +	else if (hmm)</span>
<span class="quote">&gt; &gt; +		kfree(hmm);</span>
<span class="quote">&gt; &gt; +	hmm = mm-&gt;hmm;</span>
<span class="quote">&gt; &gt; +	spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return hmm;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +void hmm_mm_destroy(struct mm_struct *mm)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct hmm *hmm;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * We should not need to lock here as no one should be able to register</span>
<span class="quote">&gt; &gt; +	 * a new HMM while an mm is being destroy. But just to be safe ...</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt; &gt; +	hmm = mm-&gt;hmm;</span>
<span class="quote">&gt; &gt; +	mm-&gt;hmm = NULL;</span>
<span class="quote">&gt; &gt; +	spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt; &gt; +	if (!hmm)</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; kfree can deal with NULL pointers, you can remove the if check</span>

Yeah.

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - Nov. 23, 2016, 4:03 a.m.</div>
<pre class="content">
On 11/18/2016 11:48 PM, Jérôme Glisse wrote:
<span class="quote">&gt; HMM provides 3 separate functionality :</span>
<span class="quote">&gt;     - Mirroring: synchronize CPU page table and device page table</span>
<span class="quote">&gt;     - Device memory: allocating struct page for device memory</span>
<span class="quote">&gt;     - Migration: migrating regular memory to device memory</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch introduces some common helpers and definitions to all of</span>
<span class="quote">&gt; those 3 functionality.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Jatin Kumar &lt;jakumar@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  MAINTAINERS              |   7 +++</span>
<span class="quote">&gt;  include/linux/hmm.h      | 139 +++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  include/linux/mm_types.h |   5 ++</span>
<span class="quote">&gt;  kernel/fork.c            |   2 +</span>
<span class="quote">&gt;  mm/Kconfig               |  11 ++++</span>
<span class="quote">&gt;  mm/Makefile              |   1 +</span>
<span class="quote">&gt;  mm/hmm.c                 |  86 +++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  7 files changed, 251 insertions(+)</span>
<span class="quote">&gt;  create mode 100644 include/linux/hmm.h</span>
<span class="quote">&gt;  create mode 100644 mm/hmm.c</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="quote">&gt; index f593300..41cd63d 100644</span>
<span class="quote">&gt; --- a/MAINTAINERS</span>
<span class="quote">&gt; +++ b/MAINTAINERS</span>
<span class="quote">&gt; @@ -5582,6 +5582,13 @@ S:	Supported</span>
<span class="quote">&gt;  F:	drivers/scsi/hisi_sas/</span>
<span class="quote">&gt;  F:	Documentation/devicetree/bindings/scsi/hisilicon-sas.txt</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +HMM - Heterogeneous Memory Management</span>
<span class="quote">&gt; +M:	Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; +L:	linux-mm@kvack.org</span>
<span class="quote">&gt; +S:	Maintained</span>
<span class="quote">&gt; +F:	mm/hmm*</span>
<span class="quote">&gt; +F:	include/linux/hmm*</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  HOST AP DRIVER</span>
<span class="quote">&gt;  M:	Jouni Malinen &lt;j@w1.fi&gt;</span>
<span class="quote">&gt;  L:	hostap@shmoo.com (subscribers-only)</span>
<span class="quote">&gt; diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..54dd529</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/include/linux/hmm.h</span>
<span class="quote">&gt; @@ -0,0 +1,139 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright 2013 Red Hat Inc.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify</span>
<span class="quote">&gt; + * it under the terms of the GNU General Public License as published by</span>
<span class="quote">&gt; + * the Free Software Foundation; either version 2 of the License, or</span>
<span class="quote">&gt; + * (at your option) any later version.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; + * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; + * GNU General Public License for more details.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * HMM provides 3 separate functionality :</span>
<span class="quote">&gt; + *   - Mirroring: synchronize CPU page table and device page table</span>
<span class="quote">&gt; + *   - Device memory: allocating struct page for device memory</span>
<span class="quote">&gt; + *   - Migration: migrating regular memory to device memory</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Each can be use independently from the others.</span>

Small nit s/use/used/
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Mirroring:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * HMM provide helpers to mirror process address space on a device. For this it</span>
<span class="quote">&gt; + * provides several helpers to order device page table update in respect to CPU</span>
<span class="quote">&gt; + * page table update. Requirement is that for any given virtual address the CPU</span>
<span class="quote">&gt; + * and device page table can not point to different physical page. It uses the</span>
<span class="quote">&gt; + * mmu_notifier API and introduce virtual address range lock which block CPU</span>
<span class="quote">&gt; + * page table update for a range while the device page table is being updated.</span>
<span class="quote">&gt; + * Usage pattern is:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *      hmm_vma_range_lock(vma, start, end);</span>
<span class="quote">&gt; + *      // snap shot CPU page table</span>
<span class="quote">&gt; + *      // update device page table from snapshot</span>
<span class="quote">&gt; + *      hmm_vma_range_unlock(vma, start, end);</span>

This code block can be explained better in more detail.
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + * Any CPU page table update that conflict with a range lock will wait until</span>
<span class="quote">&gt; + * range is unlock. This garanty proper serialization of CPU and device page</span>
<span class="quote">&gt; + * table update.</span>
<span class="quote">&gt; + *</span>

Small typo in here      ^^^^^^^^^^^^
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + * Device memory:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * HMM provides helpers to help leverage device memory either addressable like</span>
<span class="quote">&gt; + * regular memory by the CPU or un-addressable at all. In both case the device</span>
<span class="quote">&gt; + * memory is associated to dedicated structs page (which are allocated like for</span>
<span class="quote">&gt; + * hotplug memory). Device memory management is under the responsability of the</span>

Typo in here                                               ^^^^^^^^^^^^^^^^
<span class="quote">
&gt; + * device driver. HMM only allocate and initialize the struct pages associated</span>
<span class="quote">&gt; + * with the device memory.</span>

We should also mention that its hot plugged into the kernel as a ZONE_DEVICE
based memory.
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + * Allocating struct page for device memory allow to use device memory allmost</span>
<span class="quote">&gt; + * like any regular memory. Unlike regular memory it can not be added to the</span>
<span class="quote">&gt; + * lru, nor can any memory allocation can use device memory directly. Device</span>
<span class="quote">&gt; + * memory will only end up to be use in a process if device driver migrate some</span>
<span class="quote">&gt; + * of the process memory from regular memory to device memory.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Migration:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Existing memory migration mechanism (mm/migrate.c) does not allow to use</span>
<span class="quote">&gt; + * something else than the CPU to copy from source to destination memory. More</span>
<span class="quote">&gt; + * over existing code is not tailor to drive migration from process virtual</span>
<span class="quote">&gt; + * address rather than from list of pages. Finaly the migration flow does not</span>
<span class="quote">&gt; + * allow for graceful failure at different step of the migration process.</span>

The primary reason being the fact that migrate_pages() interface handles system
memory LRU pages and at this point it cannot handle these new ZONE_DEVICE based
pages whether they are addressable or not. IIUC HMM migration API layer intends
to handle both LRU system ram pages and non LRU ZONE_DEVICE struct pages and
achieve the migration both ways. The API should also include a struct page list
based migration (like migrate_pages()) along with the proposed virtual range
based migration. So the driver can choose either approach. Going forward this
API layer should also include migration interface for the addressable ZONE_DEVICE
pages like persistent memory.
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + * HMM solves all of the above though simple API :</span>

I guess you meant &quot;through&quot; instead of &quot;though&quot;.
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + *      hmm_vma_migrate(vma, start, end, ops);</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * With ops struct providing 2 callback alloc_and_copy() which allocated the</span>
<span class="quote">&gt; + * destination memory and initialize it using source memory. Migration can fail</span>
<span class="quote">&gt; + * after this step and thus last callback finalize_and_map() allow the device</span>
<span class="quote">&gt; + * driver to know which page were successfully migrated and which were not.</span>

So we have page-&gt;pgmap-&gt;free_devpage() to release the individual page back
into the device driver management during migration and also we have this ops
based finalize_and_mmap() to check on the failed instances inside a single
migration context which can contain set of pages at a time.
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + * This can easily be use outside of HMM intended use case.</span>

Where you think this can be used outside of HMM ?
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This header file contain all the API related to this 3 functionality and</span>
<span class="quote">&gt; + * each functions and struct are more thouroughly documented in below comments.</span>

Typo s/thouroughly/thoroughly/
<span class="quote">
&gt; + */</span>
<span class="quote">&gt; +#ifndef LINUX_HMM_H</span>
<span class="quote">&gt; +#define LINUX_HMM_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/kconfig.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_HMM)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * hmm_pfn_t - HMM use its own pfn type to keep several flags per page</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Flags:</span>
<span class="quote">&gt; + * HMM_PFN_VALID: pfn is valid</span>
<span class="quote">&gt; + * HMM_PFN_WRITE: CPU page table have the write permission set</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define HMM_PFN_VALID (1 &lt;&lt; 0)</span>
<span class="quote">&gt; +#define HMM_PFN_WRITE (1 &lt;&lt; 1)</span>
<span class="quote">&gt; +#define HMM_PFN_SHIFT 2</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +	return pfn_to_page(pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline unsigned long hmm_pfn_to_pfn(hmm_pfn_t pfn)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="quote">&gt; +		return -1UL;</span>
<span class="quote">&gt; +	return (pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline hmm_pfn_t hmm_pfn_from_page(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (page_to_pfn(page) &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline hmm_pfn_t hmm_pfn_from_pfn(unsigned long pfn)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (pfn &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="quote">&gt; +}</span>

Hmm, so if we use last two bits on PFN as flags, it does reduce the number of
bits available for the actual PFN range. But given that we support maximum of
64TB on POWER (not sure about X86) we can live with this two bits going away
from the unsigned long. But what is the purpose of tracking validity and write
flag inside the PFN ?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Below are for HMM internal use only ! Not to be use by device driver ! */</span>

s/use/used/
<span class="quote">
&gt; +void hmm_mm_destroy(struct mm_struct *mm);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#else /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Below are for HMM internal use only ! Not to be use by device driver ! */</span>


ditto
<span class="quote">
&gt; +static inline void hmm_mm_destroy(struct mm_struct *mm) {}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="quote">&gt; +#endif /* LINUX_HMM_H */</span>
<span class="quote">&gt; diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="quote">&gt; index 4a8aced..4effdbf 100644</span>
<span class="quote">&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; @@ -23,6 +23,7 @@</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct address_space;</span>
<span class="quote">&gt;  struct mem_cgroup;</span>
<span class="quote">&gt; +struct hmm;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define USE_SPLIT_PTE_PTLOCKS	(NR_CPUS &gt;= CONFIG_SPLIT_PTLOCK_CPUS)</span>
<span class="quote">&gt;  #define USE_SPLIT_PMD_PTLOCKS	(USE_SPLIT_PTE_PTLOCKS &amp;&amp; \</span>
<span class="quote">&gt; @@ -516,6 +517,10 @@ struct mm_struct {</span>
<span class="quote">&gt;  	atomic_long_t hugetlb_usage;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	struct work_struct async_put_work;</span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_HMM)</span>
<span class="quote">&gt; +	/* HMM need to track few things per mm */</span>
<span class="quote">&gt; +	struct hmm *hmm;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  };</span>

hmm, so the HMM structure is one for each mm context.
<span class="quote">
&gt;  </span>
<span class="quote">&gt;  static inline void mm_init_cpumask(struct mm_struct *mm)</span>
<span class="quote">&gt; diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="quote">&gt; index 690a1aad..af0eec8 100644</span>
<span class="quote">&gt; --- a/kernel/fork.c</span>
<span class="quote">&gt; +++ b/kernel/fork.c</span>
<span class="quote">&gt; @@ -27,6 +27,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/binfmts.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mman.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mmu_notifier.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/hmm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/fs.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/vmacache.h&gt;</span>
<span class="quote">&gt; @@ -702,6 +703,7 @@ void __mmdrop(struct mm_struct *mm)</span>
<span class="quote">&gt;  	BUG_ON(mm == &amp;init_mm);</span>
<span class="quote">&gt;  	mm_free_pgd(mm);</span>
<span class="quote">&gt;  	destroy_context(mm);</span>
<span class="quote">&gt; +	hmm_mm_destroy(mm);</span>
<span class="quote">&gt;  	mmu_notifier_mm_destroy(mm);</span>
<span class="quote">&gt;  	check_mm(mm);</span>
<span class="quote">&gt;  	free_mm(mm);</span>
<span class="quote">&gt; diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="quote">&gt; index 0a21411..be18cc2 100644</span>
<span class="quote">&gt; --- a/mm/Kconfig</span>
<span class="quote">&gt; +++ b/mm/Kconfig</span>
<span class="quote">&gt; @@ -289,6 +289,17 @@ config MIGRATION</span>
<span class="quote">&gt;  config ARCH_ENABLE_HUGEPAGE_MIGRATION</span>
<span class="quote">&gt;  	bool</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config HMM</span>
<span class="quote">&gt; +	bool &quot;Heterogeneous memory management (HMM)&quot;</span>
<span class="quote">&gt; +	depends on MMU</span>
<span class="quote">&gt; +	default n</span>
<span class="quote">&gt; +	help</span>
<span class="quote">&gt; +	  Heterogeneous memory management, set of helpers for:</span>
<span class="quote">&gt; +	    - mirroring of process address space on a device</span>
<span class="quote">&gt; +	    - using device memory transparently inside a process</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  If unsure, say N to disable HMM.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config PHYS_ADDR_T_64BIT</span>
<span class="quote">&gt;  	def_bool 64BIT || ARCH_PHYS_ADDR_T_64BIT</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="quote">&gt; index 2ca1faf..6ac1284 100644</span>
<span class="quote">&gt; --- a/mm/Makefile</span>
<span class="quote">&gt; +++ b/mm/Makefile</span>
<span class="quote">&gt; @@ -76,6 +76,7 @@ obj-$(CONFIG_FAILSLAB) += failslab.o</span>
<span class="quote">&gt;  obj-$(CONFIG_MEMORY_HOTPLUG) += memory_hotplug.o</span>
<span class="quote">&gt;  obj-$(CONFIG_MEMTEST)		+= memtest.o</span>
<span class="quote">&gt;  obj-$(CONFIG_MIGRATION) += migrate.o</span>
<span class="quote">&gt; +obj-$(CONFIG_HMM) += hmm.o</span>
<span class="quote">&gt;  obj-$(CONFIG_QUICKLIST) += quicklist.o</span>
<span class="quote">&gt;  obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o</span>
<span class="quote">&gt;  obj-$(CONFIG_PAGE_COUNTER) += page_counter.o</span>
<span class="quote">&gt; diff --git a/mm/hmm.c b/mm/hmm.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 0000000..342b596</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/mm/hmm.c</span>
<span class="quote">&gt; @@ -0,0 +1,86 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright 2013 Red Hat Inc.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify</span>
<span class="quote">&gt; + * it under the terms of the GNU General Public License as published by</span>
<span class="quote">&gt; + * the Free Software Foundation; either version 2 of the License, or</span>
<span class="quote">&gt; + * (at your option) any later version.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is distributed in the hope that it will be useful,</span>
<span class="quote">&gt; + * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="quote">&gt; + * GNU General Public License for more details.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Refer to include/linux/hmm.h for informations about heterogeneous memory</span>

s/informations/information/
<span class="quote">
&gt; + * management or HMM for short.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/hmm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * struct hmm - HMM per mm struct</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * @mm: mm struct this HMM struct is bound to</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct hmm {</span>
<span class="quote">&gt; +	struct mm_struct	*mm;</span>
<span class="quote">&gt; +};</span>

So right now its empty other than this link back to the struct mm.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Nov. 27, 2016, 1:10 p.m.</div>
<pre class="content">
On Wed, Nov 23, 2016 at 09:33:35AM +0530, Anshuman Khandual wrote:
<span class="quote">&gt; On 11/18/2016 11:48 PM, Jérôme Glisse wrote:</span>

[...]
<span class="quote">
&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + *      hmm_vma_migrate(vma, start, end, ops);</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * With ops struct providing 2 callback alloc_and_copy() which allocated the</span>
<span class="quote">&gt; &gt; + * destination memory and initialize it using source memory. Migration can fail</span>
<span class="quote">&gt; &gt; + * after this step and thus last callback finalize_and_map() allow the device</span>
<span class="quote">&gt; &gt; + * driver to know which page were successfully migrated and which were not.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So we have page-&gt;pgmap-&gt;free_devpage() to release the individual page back</span>
<span class="quote">&gt; into the device driver management during migration and also we have this ops</span>
<span class="quote">&gt; based finalize_and_mmap() to check on the failed instances inside a single</span>
<span class="quote">&gt; migration context which can contain set of pages at a time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This can easily be use outside of HMM intended use case.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Where you think this can be used outside of HMM ?</span>

Well on the radar is new memory hierarchy that seems to be on every CPU designer
roadmap. Where you have a fast small HBM like memory package with the CPU and then
you have the regular memory.

In the embedded world they want to migrate active process to fast CPU memory and
shutdown the regular memory to save power.

In the HPC world they want to migrate hot data of hot process to this fast memory.

In both case we are talking about process base memory migration and in case of
embedded they also have DMA engine they can use to offload the copy operation
itself.

This are the useful case i have in mind but other people might see that code and
realise they could also use it for their own specific corner case.

[...]
<span class="quote">
&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * hmm_pfn_t - HMM use its own pfn type to keep several flags per page</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * Flags:</span>
<span class="quote">&gt; &gt; + * HMM_PFN_VALID: pfn is valid</span>
<span class="quote">&gt; &gt; + * HMM_PFN_WRITE: CPU page table have the write permission set</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_VALID (1 &lt;&lt; 0)</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_WRITE (1 &lt;&lt; 1)</span>
<span class="quote">&gt; &gt; +#define HMM_PFN_SHIFT 2</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="quote">&gt; &gt; +		return NULL;</span>
<span class="quote">&gt; &gt; +	return pfn_to_page(pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline unsigned long hmm_pfn_to_pfn(hmm_pfn_t pfn)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="quote">&gt; &gt; +		return -1UL;</span>
<span class="quote">&gt; &gt; +	return (pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline hmm_pfn_t hmm_pfn_from_page(struct page *page)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return (page_to_pfn(page) &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline hmm_pfn_t hmm_pfn_from_pfn(unsigned long pfn)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return (pfn &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm, so if we use last two bits on PFN as flags, it does reduce the number of</span>
<span class="quote">&gt; bits available for the actual PFN range. But given that we support maximum of</span>
<span class="quote">&gt; 64TB on POWER (not sure about X86) we can live with this two bits going away</span>
<span class="quote">&gt; from the unsigned long. But what is the purpose of tracking validity and write</span>
<span class="quote">&gt; flag inside the PFN ?</span>

So 2^46 so with 12bits PAGE_SHIFT we only need 34 bits for pfns value hence i
should have enough place for my flag or is unsigned long not 64bits on powerpc ?

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - Nov. 28, 2016, 2:58 a.m.</div>
<pre class="content">
On 11/27/2016 06:40 PM, Jerome Glisse wrote:
<span class="quote">&gt; On Wed, Nov 23, 2016 at 09:33:35AM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt; On 11/18/2016 11:48 PM, Jérôme Glisse wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; + *</span>
<span class="quote">&gt;&gt;&gt; + *      hmm_vma_migrate(vma, start, end, ops);</span>
<span class="quote">&gt;&gt;&gt; + *</span>
<span class="quote">&gt;&gt;&gt; + * With ops struct providing 2 callback alloc_and_copy() which allocated the</span>
<span class="quote">&gt;&gt;&gt; + * destination memory and initialize it using source memory. Migration can fail</span>
<span class="quote">&gt;&gt;&gt; + * after this step and thus last callback finalize_and_map() allow the device</span>
<span class="quote">&gt;&gt;&gt; + * driver to know which page were successfully migrated and which were not.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So we have page-&gt;pgmap-&gt;free_devpage() to release the individual page back</span>
<span class="quote">&gt;&gt; into the device driver management during migration and also we have this ops</span>
<span class="quote">&gt;&gt; based finalize_and_mmap() to check on the failed instances inside a single</span>
<span class="quote">&gt;&gt; migration context which can contain set of pages at a time.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; + *</span>
<span class="quote">&gt;&gt;&gt; + * This can easily be use outside of HMM intended use case.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Where you think this can be used outside of HMM ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well on the radar is new memory hierarchy that seems to be on every CPU designer</span>
<span class="quote">&gt; roadmap. Where you have a fast small HBM like memory package with the CPU and then</span>
<span class="quote">&gt; you have the regular memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In the embedded world they want to migrate active process to fast CPU memory and</span>
<span class="quote">&gt; shutdown the regular memory to save power.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In the HPC world they want to migrate hot data of hot process to this fast memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In both case we are talking about process base memory migration and in case of</span>
<span class="quote">&gt; embedded they also have DMA engine they can use to offload the copy operation</span>
<span class="quote">&gt; itself.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This are the useful case i have in mind but other people might see that code and</span>
<span class="quote">&gt; realise they could also use it for their own specific corner case.</span>

If there are plans for HBM or specialized type of memory which will be
packaged inside the CPU (without any other device accessing it like in
the case of GPU or Network Card), then I think in that case using HMM
is not ideal. CPU will be the only thing accessing this memory and
there is never going to be any other device or context which can access
this outside of CPU. Hence role of a device driver is redundant, it
should be initialized and used as a basic platform component.

In that case what we need is a core VM managed memory with certain kind
of restrictions around the allocation and a way of explicit allocation
into it if required. Representing these memory like a cpu less restrictive
coherent device memory node is a better solution IMHO. These RFCs what I
have posted regarding CDM representation are efforts in this direction.

[RFC Specialized Zonelists]    https://lkml.org/lkml/2016/10/24/19
[RFC Restrictive mems_allowed] https://lkml.org/lkml/2016/11/22/339

I believe both HMM and CDM have their own use cases and will complement
each other.
<span class="quote">
&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; +/*</span>
<span class="quote">&gt;&gt;&gt; + * hmm_pfn_t - HMM use its own pfn type to keep several flags per page</span>
<span class="quote">&gt;&gt;&gt; + *</span>
<span class="quote">&gt;&gt;&gt; + * Flags:</span>
<span class="quote">&gt;&gt;&gt; + * HMM_PFN_VALID: pfn is valid</span>
<span class="quote">&gt;&gt;&gt; + * HMM_PFN_WRITE: CPU page table have the write permission set</span>
<span class="quote">&gt;&gt;&gt; + */</span>
<span class="quote">&gt;&gt;&gt; +typedef unsigned long hmm_pfn_t;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#define HMM_PFN_VALID (1 &lt;&lt; 0)</span>
<span class="quote">&gt;&gt;&gt; +#define HMM_PFN_WRITE (1 &lt;&lt; 1)</span>
<span class="quote">&gt;&gt;&gt; +#define HMM_PFN_SHIFT 2</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="quote">&gt;&gt;&gt; +		return NULL;</span>
<span class="quote">&gt;&gt;&gt; +	return pfn_to_page(pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +static inline unsigned long hmm_pfn_to_pfn(hmm_pfn_t pfn)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="quote">&gt;&gt;&gt; +		return -1UL;</span>
<span class="quote">&gt;&gt;&gt; +	return (pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +static inline hmm_pfn_t hmm_pfn_from_page(struct page *page)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	return (page_to_pfn(page) &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +static inline hmm_pfn_t hmm_pfn_from_pfn(unsigned long pfn)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	return (pfn &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Hmm, so if we use last two bits on PFN as flags, it does reduce the number of</span>
<span class="quote">&gt;&gt; bits available for the actual PFN range. But given that we support maximum of</span>
<span class="quote">&gt;&gt; 64TB on POWER (not sure about X86) we can live with this two bits going away</span>
<span class="quote">&gt;&gt; from the unsigned long. But what is the purpose of tracking validity and write</span>
<span class="quote">&gt;&gt; flag inside the PFN ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So 2^46 so with 12bits PAGE_SHIFT we only need 34 bits for pfns value hence i</span>
<span class="quote">&gt; should have enough place for my flag or is unsigned long not 64bits on powerpc ?</span>

Yeah it is 64 bits on POWER, we use 12 bits of PAGE_SHIFT for 4K
pages and 16 bits of PAGE_SHIFT for 64K pages.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Nov. 28, 2016, 9:41 a.m.</div>
<pre class="content">
<span class="quote">&gt; On 11/27/2016 06:40 PM, Jerome Glisse wrote:</span>
<span class="quote">&gt; &gt; On Wed, Nov 23, 2016 at 09:33:35AM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt; &gt;&gt; On 11/18/2016 11:48 PM, Jérôme Glisse wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [...]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt;&gt; + *</span>
<span class="quote">&gt; &gt;&gt;&gt; + *      hmm_vma_migrate(vma, start, end, ops);</span>
<span class="quote">&gt; &gt;&gt;&gt; + *</span>
<span class="quote">&gt; &gt;&gt;&gt; + * With ops struct providing 2 callback alloc_and_copy() which allocated</span>
<span class="quote">&gt; &gt;&gt;&gt; the</span>
<span class="quote">&gt; &gt;&gt;&gt; + * destination memory and initialize it using source memory. Migration</span>
<span class="quote">&gt; &gt;&gt;&gt; can fail</span>
<span class="quote">&gt; &gt;&gt;&gt; + * after this step and thus last callback finalize_and_map() allow the</span>
<span class="quote">&gt; &gt;&gt;&gt; device</span>
<span class="quote">&gt; &gt;&gt;&gt; + * driver to know which page were successfully migrated and which were</span>
<span class="quote">&gt; &gt;&gt;&gt; not.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; So we have page-&gt;pgmap-&gt;free_devpage() to release the individual page back</span>
<span class="quote">&gt; &gt;&gt; into the device driver management during migration and also we have this</span>
<span class="quote">&gt; &gt;&gt; ops</span>
<span class="quote">&gt; &gt;&gt; based finalize_and_mmap() to check on the failed instances inside a single</span>
<span class="quote">&gt; &gt;&gt; migration context which can contain set of pages at a time.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; + *</span>
<span class="quote">&gt; &gt;&gt;&gt; + * This can easily be use outside of HMM intended use case.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Where you think this can be used outside of HMM ?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Well on the radar is new memory hierarchy that seems to be on every CPU</span>
<span class="quote">&gt; &gt; designer</span>
<span class="quote">&gt; &gt; roadmap. Where you have a fast small HBM like memory package with the CPU</span>
<span class="quote">&gt; &gt; and then</span>
<span class="quote">&gt; &gt; you have the regular memory.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In the embedded world they want to migrate active process to fast CPU</span>
<span class="quote">&gt; &gt; memory and</span>
<span class="quote">&gt; &gt; shutdown the regular memory to save power.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In the HPC world they want to migrate hot data of hot process to this fast</span>
<span class="quote">&gt; &gt; memory.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; In both case we are talking about process base memory migration and in case</span>
<span class="quote">&gt; &gt; of</span>
<span class="quote">&gt; &gt; embedded they also have DMA engine they can use to offload the copy</span>
<span class="quote">&gt; &gt; operation</span>
<span class="quote">&gt; &gt; itself.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This are the useful case i have in mind but other people might see that</span>
<span class="quote">&gt; &gt; code and</span>
<span class="quote">&gt; &gt; realise they could also use it for their own specific corner case.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If there are plans for HBM or specialized type of memory which will be</span>
<span class="quote">&gt; packaged inside the CPU (without any other device accessing it like in</span>
<span class="quote">&gt; the case of GPU or Network Card), then I think in that case using HMM</span>
<span class="quote">&gt; is not ideal. CPU will be the only thing accessing this memory and</span>
<span class="quote">&gt; there is never going to be any other device or context which can access</span>
<span class="quote">&gt; this outside of CPU. Hence role of a device driver is redundant, it</span>
<span class="quote">&gt; should be initialized and used as a basic platform component.</span>

AFAIK no CPU can saturate the bandwidth of this memory and thus they only
make sense when there is something like a GPU on die. So in my mind this
kind of memory is always use preferably by a GPU but could still be use by
CPU. In that context you also always have a DMA engine to offload memory
from CPU. I was more selling the HMM migration code in that context :)
<span class="quote">
 
&gt; In that case what we need is a core VM managed memory with certain kind</span>
<span class="quote">&gt; of restrictions around the allocation and a way of explicit allocation</span>
<span class="quote">&gt; into it if required. Representing these memory like a cpu less restrictive</span>
<span class="quote">&gt; coherent device memory node is a better solution IMHO. These RFCs what I</span>
<span class="quote">&gt; have posted regarding CDM representation are efforts in this direction.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [RFC Specialized Zonelists]    https://lkml.org/lkml/2016/10/24/19</span>
<span class="quote">&gt; [RFC Restrictive mems_allowed] https://lkml.org/lkml/2016/11/22/339</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I believe both HMM and CDM have their own use cases and will complement</span>
<span class="quote">&gt; each other.</span>

Yes how this memory is represented probably better be represented by something
like CDM. 

Cheers,
Jérôme
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="p_header">index f593300..41cd63d 100644</span>
<span class="p_header">--- a/MAINTAINERS</span>
<span class="p_header">+++ b/MAINTAINERS</span>
<span class="p_chunk">@@ -5582,6 +5582,13 @@</span> <span class="p_context"> S:	Supported</span>
 F:	drivers/scsi/hisi_sas/
 F:	Documentation/devicetree/bindings/scsi/hisilicon-sas.txt
 
<span class="p_add">+HMM - Heterogeneous Memory Management</span>
<span class="p_add">+M:	Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+L:	linux-mm@kvack.org</span>
<span class="p_add">+S:	Maintained</span>
<span class="p_add">+F:	mm/hmm*</span>
<span class="p_add">+F:	include/linux/hmm*</span>
<span class="p_add">+</span>
 HOST AP DRIVER
 M:	Jouni Malinen &lt;j@w1.fi&gt;
 L:	hostap@shmoo.com (subscribers-only)
<span class="p_header">diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
new file mode 100644
<span class="p_header">index 0000000..54dd529</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/hmm.h</span>
<span class="p_chunk">@@ -0,0 +1,139 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright 2013 Red Hat Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * HMM provides 3 separate functionality :</span>
<span class="p_add">+ *   - Mirroring: synchronize CPU page table and device page table</span>
<span class="p_add">+ *   - Device memory: allocating struct page for device memory</span>
<span class="p_add">+ *   - Migration: migrating regular memory to device memory</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each can be use independently from the others.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Mirroring:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM provide helpers to mirror process address space on a device. For this it</span>
<span class="p_add">+ * provides several helpers to order device page table update in respect to CPU</span>
<span class="p_add">+ * page table update. Requirement is that for any given virtual address the CPU</span>
<span class="p_add">+ * and device page table can not point to different physical page. It uses the</span>
<span class="p_add">+ * mmu_notifier API and introduce virtual address range lock which block CPU</span>
<span class="p_add">+ * page table update for a range while the device page table is being updated.</span>
<span class="p_add">+ * Usage pattern is:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *      hmm_vma_range_lock(vma, start, end);</span>
<span class="p_add">+ *      // snap shot CPU page table</span>
<span class="p_add">+ *      // update device page table from snapshot</span>
<span class="p_add">+ *      hmm_vma_range_unlock(vma, start, end);</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Any CPU page table update that conflict with a range lock will wait until</span>
<span class="p_add">+ * range is unlock. This garanty proper serialization of CPU and device page</span>
<span class="p_add">+ * table update.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Device memory:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM provides helpers to help leverage device memory either addressable like</span>
<span class="p_add">+ * regular memory by the CPU or un-addressable at all. In both case the device</span>
<span class="p_add">+ * memory is associated to dedicated structs page (which are allocated like for</span>
<span class="p_add">+ * hotplug memory). Device memory management is under the responsability of the</span>
<span class="p_add">+ * device driver. HMM only allocate and initialize the struct pages associated</span>
<span class="p_add">+ * with the device memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Allocating struct page for device memory allow to use device memory allmost</span>
<span class="p_add">+ * like any regular memory. Unlike regular memory it can not be added to the</span>
<span class="p_add">+ * lru, nor can any memory allocation can use device memory directly. Device</span>
<span class="p_add">+ * memory will only end up to be use in a process if device driver migrate some</span>
<span class="p_add">+ * of the process memory from regular memory to device memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Migration:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Existing memory migration mechanism (mm/migrate.c) does not allow to use</span>
<span class="p_add">+ * something else than the CPU to copy from source to destination memory. More</span>
<span class="p_add">+ * over existing code is not tailor to drive migration from process virtual</span>
<span class="p_add">+ * address rather than from list of pages. Finaly the migration flow does not</span>
<span class="p_add">+ * allow for graceful failure at different step of the migration process.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM solves all of the above though simple API :</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *      hmm_vma_migrate(vma, start, end, ops);</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With ops struct providing 2 callback alloc_and_copy() which allocated the</span>
<span class="p_add">+ * destination memory and initialize it using source memory. Migration can fail</span>
<span class="p_add">+ * after this step and thus last callback finalize_and_map() allow the device</span>
<span class="p_add">+ * driver to know which page were successfully migrated and which were not.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This can easily be use outside of HMM intended use case.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This header file contain all the API related to this 3 functionality and</span>
<span class="p_add">+ * each functions and struct are more thouroughly documented in below comments.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef LINUX_HMM_H</span>
<span class="p_add">+#define LINUX_HMM_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kconfig.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#if IS_ENABLED(CONFIG_HMM)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_t - HMM use its own pfn type to keep several flags per page</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Flags:</span>
<span class="p_add">+ * HMM_PFN_VALID: pfn is valid</span>
<span class="p_add">+ * HMM_PFN_WRITE: CPU page table have the write permission set</span>
<span class="p_add">+ */</span>
<span class="p_add">+typedef unsigned long hmm_pfn_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define HMM_PFN_VALID (1 &lt;&lt; 0)</span>
<span class="p_add">+#define HMM_PFN_WRITE (1 &lt;&lt; 1)</span>
<span class="p_add">+#define HMM_PFN_SHIFT 2</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	return pfn_to_page(pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long hmm_pfn_to_pfn(hmm_pfn_t pfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="p_add">+		return -1UL;</span>
<span class="p_add">+	return (pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline hmm_pfn_t hmm_pfn_from_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (page_to_pfn(page) &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline hmm_pfn_t hmm_pfn_from_pfn(unsigned long pfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pfn &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Below are for HMM internal use only ! Not to be use by device driver ! */</span>
<span class="p_add">+void hmm_mm_destroy(struct mm_struct *mm);</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Below are for HMM internal use only ! Not to be use by device driver ! */</span>
<span class="p_add">+static inline void hmm_mm_destroy(struct mm_struct *mm) {}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="p_add">+#endif /* LINUX_HMM_H */</span>
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index 4a8aced..4effdbf 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -23,6 +23,7 @@</span> <span class="p_context"></span>
 
 struct address_space;
 struct mem_cgroup;
<span class="p_add">+struct hmm;</span>
 
 #define USE_SPLIT_PTE_PTLOCKS	(NR_CPUS &gt;= CONFIG_SPLIT_PTLOCK_CPUS)
 #define USE_SPLIT_PMD_PTLOCKS	(USE_SPLIT_PTE_PTLOCKS &amp;&amp; \
<span class="p_chunk">@@ -516,6 +517,10 @@</span> <span class="p_context"> struct mm_struct {</span>
 	atomic_long_t hugetlb_usage;
 #endif
 	struct work_struct async_put_work;
<span class="p_add">+#if IS_ENABLED(CONFIG_HMM)</span>
<span class="p_add">+	/* HMM need to track few things per mm */</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+#endif</span>
 };
 
 static inline void mm_init_cpumask(struct mm_struct *mm)
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 690a1aad..af0eec8 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -27,6 +27,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/binfmts.h&gt;
 #include &lt;linux/mman.h&gt;
 #include &lt;linux/mmu_notifier.h&gt;
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
 #include &lt;linux/fs.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/vmacache.h&gt;
<span class="p_chunk">@@ -702,6 +703,7 @@</span> <span class="p_context"> void __mmdrop(struct mm_struct *mm)</span>
 	BUG_ON(mm == &amp;init_mm);
 	mm_free_pgd(mm);
 	destroy_context(mm);
<span class="p_add">+	hmm_mm_destroy(mm);</span>
 	mmu_notifier_mm_destroy(mm);
 	check_mm(mm);
 	free_mm(mm);
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index 0a21411..be18cc2 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -289,6 +289,17 @@</span> <span class="p_context"> config MIGRATION</span>
 config ARCH_ENABLE_HUGEPAGE_MIGRATION
 	bool
 
<span class="p_add">+config HMM</span>
<span class="p_add">+	bool &quot;Heterogeneous memory management (HMM)&quot;</span>
<span class="p_add">+	depends on MMU</span>
<span class="p_add">+	default n</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Heterogeneous memory management, set of helpers for:</span>
<span class="p_add">+	    - mirroring of process address space on a device</span>
<span class="p_add">+	    - using device memory transparently inside a process</span>
<span class="p_add">+</span>
<span class="p_add">+	  If unsure, say N to disable HMM.</span>
<span class="p_add">+</span>
 config PHYS_ADDR_T_64BIT
 	def_bool 64BIT || ARCH_PHYS_ADDR_T_64BIT
 
<span class="p_header">diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="p_header">index 2ca1faf..6ac1284 100644</span>
<span class="p_header">--- a/mm/Makefile</span>
<span class="p_header">+++ b/mm/Makefile</span>
<span class="p_chunk">@@ -76,6 +76,7 @@</span> <span class="p_context"> obj-$(CONFIG_FAILSLAB) += failslab.o</span>
 obj-$(CONFIG_MEMORY_HOTPLUG) += memory_hotplug.o
 obj-$(CONFIG_MEMTEST)		+= memtest.o
 obj-$(CONFIG_MIGRATION) += migrate.o
<span class="p_add">+obj-$(CONFIG_HMM) += hmm.o</span>
 obj-$(CONFIG_QUICKLIST) += quicklist.o
 obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o
 obj-$(CONFIG_PAGE_COUNTER) += page_counter.o
<span class="p_header">diff --git a/mm/hmm.c b/mm/hmm.c</span>
new file mode 100644
<span class="p_header">index 0000000..342b596</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/hmm.c</span>
<span class="p_chunk">@@ -0,0 +1,86 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright 2013 Red Hat Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Refer to include/linux/hmm.h for informations about heterogeneous memory</span>
<span class="p_add">+ * management or HMM for short.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * struct hmm - HMM per mm struct</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mm: mm struct this HMM struct is bound to</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm {</span>
<span class="p_add">+	struct mm_struct	*mm;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_register - register HMM against an mm (HMM internal)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mm: mm struct to attach to</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is not intended to be use directly by device driver but by other HMM</span>
<span class="p_add">+ * component. It allocates an HMM struct if mm does not have one and initialize</span>
<span class="p_add">+ * it.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct hmm *hmm_register(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm *hmm = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mm-&gt;hmm) {</span>
<span class="p_add">+		hmm = kmalloc(sizeof(*hmm), GFP_KERNEL);</span>
<span class="p_add">+		if (!hmm)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+		hmm-&gt;mm = mm;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+	if (!mm-&gt;hmm)</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * The hmm struct can only be free once mm_struct goes away</span>
<span class="p_add">+		 * hence we should always have pre-allocated an new hmm struct</span>
<span class="p_add">+		 * above.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		mm-&gt;hmm = hmm;</span>
<span class="p_add">+	else if (hmm)</span>
<span class="p_add">+		kfree(hmm);</span>
<span class="p_add">+	hmm = mm-&gt;hmm;</span>
<span class="p_add">+	spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	return hmm;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void hmm_mm_destroy(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We should not need to lock here as no one should be able to register</span>
<span class="p_add">+	 * a new HMM while an mm is being destroy. But just to be safe ...</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+	hmm = mm-&gt;hmm;</span>
<span class="p_add">+	mm-&gt;hmm = NULL;</span>
<span class="p_add">+	spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+	if (!hmm)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	kfree(hmm);</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



