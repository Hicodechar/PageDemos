
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,2/3] mm: memfd: split out memfd for use by multiple filesystems - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,2/3] mm: memfd: split out memfd for use by multiple filesystems</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 9, 2017, 1:41 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171109014109.21077-3-mike.kravetz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10049959/mbox/"
   >mbox</a>
|
   <a href="/patch/10049959/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10049959/">/patch/10049959/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	8FC116032D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Nov 2017 01:42:06 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8135A29817
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Nov 2017 01:42:06 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7658229A59; Thu,  9 Nov 2017 01:42:06 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	UNPARSEABLE_RELAY autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7D2F829A6D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Nov 2017 01:42:05 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753276AbdKIBmC (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 8 Nov 2017 20:42:02 -0500
Received: from userp1040.oracle.com ([156.151.31.81]:26311 &quot;EHLO
	userp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753200AbdKIBlh (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 8 Nov 2017 20:41:37 -0500
Received: from aserv0022.oracle.com (aserv0022.oracle.com [141.146.126.234])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id vA91fUgX026688
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Thu, 9 Nov 2017 01:41:30 GMT
Received: from userv0121.oracle.com (userv0121.oracle.com [156.151.31.72])
	by aserv0022.oracle.com (8.14.4/8.14.4) with ESMTP id vA91fTSg021371
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=OK); Thu, 9 Nov 2017 01:41:30 GMT
Received: from abhmp0016.oracle.com (abhmp0016.oracle.com [141.146.116.22])
	by userv0121.oracle.com (8.14.4/8.13.8) with ESMTP id
	vA91fToe008654; Thu, 9 Nov 2017 01:41:29 GMT
Received: from monkey.oracle.com (/98.246.252.205)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Wed, 08 Nov 2017 17:41:28 -0800
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc: Hugh Dickins &lt;hughd@google.com&gt;, Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;,
	=?UTF-8?q?Marc-Andr=C3=A9=20Lureau?= &lt;marcandre.lureau@redhat.com&gt;,
	David Herrmann &lt;dh.herrmann@gmail.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Subject: [RFC PATCH 2/3] mm: memfd: split out memfd for use by multiple
	filesystems
Date: Wed,  8 Nov 2017 17:41:08 -0800
Message-Id: &lt;20171109014109.21077-3-mike.kravetz@oracle.com&gt;
X-Mailer: git-send-email 2.13.6
In-Reply-To: &lt;20171109014109.21077-1-mike.kravetz@oracle.com&gt;
References: &lt;20171109014109.21077-1-mike.kravetz@oracle.com&gt;
X-Source-IP: aserv0022.oracle.com [141.146.126.234]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Nov. 9, 2017, 1:41 a.m.</div>
<pre class="content">
When memfd_create support was originally written, it only provided
suport for tmpfs.  Support has recently been added for hugetlbfs.
memfd originally depended on tmpfs.  In an effort to make it depend
on tmpfs -or- hugetlbfs, split out the required code to separate
files.
<span class="signed-off-by">
Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
---
 include/linux/memfd.h |  16 +++
 mm/memfd.c            | 341 ++++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 357 insertions(+)
 create mode 100644 include/linux/memfd.h
 create mode 100644 mm/memfd.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/memfd.h b/include/linux/memfd.h</span>
new file mode 100644
<span class="p_header">index 000000000000..54999bc10e18</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/memfd.h</span>
<span class="p_chunk">@@ -0,0 +1,16 @@</span> <span class="p_context"></span>
<span class="p_add">+/* SPDX-License-Identifier: GPL-2.0 */</span>
<span class="p_add">+#ifndef __LINUX_MEMFD_H</span>
<span class="p_add">+#define __LINUX_MEMFD_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/file.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MEMFD_CREATE</span>
<span class="p_add">+extern long memfd_fcntl(struct file *file, unsigned int cmd, unsigned long arg);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline long memfd_fcntl(struct file *f, unsigned int c, unsigned long a)</span>
<span class="p_add">+{</span>
<span class="p_add">+       return -EINVAL;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __LINUX_MEMFD_H */</span>
<span class="p_header">diff --git a/mm/memfd.c b/mm/memfd.c</span>
new file mode 100644
<span class="p_header">index 000000000000..8edabe9c2699</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/memfd.c</span>
<span class="p_chunk">@@ -0,0 +1,341 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * memfd_create system call and file sealing support</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Code was originally included in shmem.c, and broken out to facilitate</span>
<span class="p_add">+ * use by hugetlbfs as well as tmpfs.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This file is released under the GPL.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/fs.h&gt;</span>
<span class="p_add">+#include &lt;linux/vfs.h&gt;</span>
<span class="p_add">+#include &lt;linux/pagemap.h&gt;</span>
<span class="p_add">+#include &lt;linux/file.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched/signal.h&gt;</span>
<span class="p_add">+#include &lt;linux/khugepaged.h&gt;</span>
<span class="p_add">+#include &lt;linux/syscalls.h&gt;</span>
<span class="p_add">+#include &lt;linux/hugetlb.h&gt;</span>
<span class="p_add">+#include &lt;linux/shmem_fs.h&gt;</span>
<span class="p_add">+#include &lt;uapi/linux/memfd.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * We need a tag: a new tag would expand every radix_tree_node by 8 bytes,</span>
<span class="p_add">+ * so reuse a tag which we firmly believe is never set or cleared on shmem.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define SHMEM_TAG_PINNED        PAGECACHE_TAG_TOWRITE</span>
<span class="p_add">+#define LAST_SCAN               4       /* about 150ms max */</span>
<span class="p_add">+</span>
<span class="p_add">+static void shmem_tag_pins(struct address_space *mapping)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct radix_tree_iter iter;</span>
<span class="p_add">+	void **slot;</span>
<span class="p_add">+	pgoff_t start;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	lru_add_drain();</span>
<span class="p_add">+	start = 0;</span>
<span class="p_add">+	rcu_read_lock();</span>
<span class="p_add">+</span>
<span class="p_add">+	radix_tree_for_each_slot(slot, &amp;mapping-&gt;page_tree, &amp;iter, start) {</span>
<span class="p_add">+		page = radix_tree_deref_slot(slot);</span>
<span class="p_add">+		if (!page || radix_tree_exception(page)) {</span>
<span class="p_add">+			if (radix_tree_deref_retry(page)) {</span>
<span class="p_add">+				slot = radix_tree_iter_retry(&amp;iter);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		} else if (page_count(page) - page_mapcount(page) &gt; 1) {</span>
<span class="p_add">+			spin_lock_irq(&amp;mapping-&gt;tree_lock);</span>
<span class="p_add">+			radix_tree_tag_set(&amp;mapping-&gt;page_tree, iter.index,</span>
<span class="p_add">+					   SHMEM_TAG_PINNED);</span>
<span class="p_add">+			spin_unlock_irq(&amp;mapping-&gt;tree_lock);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (need_resched()) {</span>
<span class="p_add">+			slot = radix_tree_iter_resume(slot, &amp;iter);</span>
<span class="p_add">+			cond_resched_rcu();</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	rcu_read_unlock();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Setting SEAL_WRITE requires us to verify there&#39;s no pending writer. However,</span>
<span class="p_add">+ * via get_user_pages(), drivers might have some pending I/O without any active</span>
<span class="p_add">+ * user-space mappings (eg., direct-IO, AIO). Therefore, we look at all pages</span>
<span class="p_add">+ * and see whether it has an elevated ref-count. If so, we tag them and wait for</span>
<span class="p_add">+ * them to be dropped.</span>
<span class="p_add">+ * The caller must guarantee that no new user will acquire writable references</span>
<span class="p_add">+ * to those pages to avoid races.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int shmem_wait_for_pins(struct address_space *mapping)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct radix_tree_iter iter;</span>
<span class="p_add">+	void **slot;</span>
<span class="p_add">+	pgoff_t start;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	int error, scan;</span>
<span class="p_add">+</span>
<span class="p_add">+	shmem_tag_pins(mapping);</span>
<span class="p_add">+</span>
<span class="p_add">+	error = 0;</span>
<span class="p_add">+	for (scan = 0; scan &lt;= LAST_SCAN; scan++) {</span>
<span class="p_add">+		if (!radix_tree_tagged(&amp;mapping-&gt;page_tree, SHMEM_TAG_PINNED))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!scan)</span>
<span class="p_add">+			lru_add_drain_all();</span>
<span class="p_add">+		else if (schedule_timeout_killable((HZ &lt;&lt; scan) / 200))</span>
<span class="p_add">+			scan = LAST_SCAN;</span>
<span class="p_add">+</span>
<span class="p_add">+		start = 0;</span>
<span class="p_add">+		rcu_read_lock();</span>
<span class="p_add">+		radix_tree_for_each_tagged(slot, &amp;mapping-&gt;page_tree, &amp;iter,</span>
<span class="p_add">+					   start, SHMEM_TAG_PINNED) {</span>
<span class="p_add">+</span>
<span class="p_add">+			page = radix_tree_deref_slot(slot);</span>
<span class="p_add">+			if (radix_tree_exception(page)) {</span>
<span class="p_add">+				if (radix_tree_deref_retry(page)) {</span>
<span class="p_add">+					slot = radix_tree_iter_retry(&amp;iter);</span>
<span class="p_add">+					continue;</span>
<span class="p_add">+				}</span>
<span class="p_add">+</span>
<span class="p_add">+				page = NULL;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (page &amp;&amp;</span>
<span class="p_add">+			    page_count(page) - page_mapcount(page) != 1) {</span>
<span class="p_add">+				if (scan &lt; LAST_SCAN)</span>
<span class="p_add">+					goto continue_resched;</span>
<span class="p_add">+</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * On the last scan, we clean up all those tags</span>
<span class="p_add">+				 * we inserted; but make a note that we still</span>
<span class="p_add">+				 * found pages pinned.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				error = -EBUSY;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			spin_lock_irq(&amp;mapping-&gt;tree_lock);</span>
<span class="p_add">+			radix_tree_tag_clear(&amp;mapping-&gt;page_tree,</span>
<span class="p_add">+					     iter.index, SHMEM_TAG_PINNED);</span>
<span class="p_add">+			spin_unlock_irq(&amp;mapping-&gt;tree_lock);</span>
<span class="p_add">+continue_resched:</span>
<span class="p_add">+			if (need_resched()) {</span>
<span class="p_add">+				slot = radix_tree_iter_resume(slot, &amp;iter);</span>
<span class="p_add">+				cond_resched_rcu();</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+		rcu_read_unlock();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return error;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned int *memfd_file_seals_ptr(struct file *file)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (shmem_file(file))</span>
<span class="p_add">+		return &amp;SHMEM_I(file_inode(file))-&gt;seals;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (is_file_hugepages(file))</span>
<span class="p_add">+		return &amp;HUGETLBFS_I(file_inode(file))-&gt;seals;</span>
<span class="p_add">+</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define F_ALL_SEALS (F_SEAL_SEAL | \</span>
<span class="p_add">+		     F_SEAL_SHRINK | \</span>
<span class="p_add">+		     F_SEAL_GROW | \</span>
<span class="p_add">+		     F_SEAL_WRITE)</span>
<span class="p_add">+</span>
<span class="p_add">+static int memfd_add_seals(struct file *file, unsigned int seals)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct inode *inode = file_inode(file);</span>
<span class="p_add">+	unsigned int *file_seals;</span>
<span class="p_add">+	int error;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * SEALING</span>
<span class="p_add">+	 * Sealing allows multiple parties to share a tmpfs or hugetlbfs file</span>
<span class="p_add">+         * but restrict access to a specific subset of file operations. Seals</span>
<span class="p_add">+	 * can only be added, but never removed. This way, mutually untrusted</span>
<span class="p_add">+	 * parties can share common memory regions with a well-defined policy.</span>
<span class="p_add">+	 * A malicious peer can thus never perform unwanted operations on a</span>
<span class="p_add">+	 * shared object.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Seals are only supported on special tmpfs or hugetlbfs files and</span>
<span class="p_add">+	 * always affect the whole underlying inode. Once a seal is set, it</span>
<span class="p_add">+	 * may prevent some kinds of access to the file. Currently, the</span>
<span class="p_add">+	 * following seals are defined:</span>
<span class="p_add">+	 *   SEAL_SEAL: Prevent further seals from being set on this file</span>
<span class="p_add">+	 *   SEAL_SHRINK: Prevent the file from shrinking</span>
<span class="p_add">+	 *   SEAL_GROW: Prevent the file from growing</span>
<span class="p_add">+	 *   SEAL_WRITE: Prevent write access to the file</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * As we don&#39;t require any trust relationship between two parties, we</span>
<span class="p_add">+	 * must prevent seals from being removed. Therefore, sealing a file</span>
<span class="p_add">+	 * only adds a given set of seals to the file, it never touches</span>
<span class="p_add">+	 * existing seals. Furthermore, the &quot;setting seals&quot;-operation can be</span>
<span class="p_add">+	 * sealed itself, which basically prevents any further seal from being</span>
<span class="p_add">+	 * added.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Semantics of sealing are only defined on volatile files. Only</span>
<span class="p_add">+	 * anonymous tmpfs and hugetlbfs files support sealing. More</span>
<span class="p_add">+	 * importantly, seals are never written to disk. Therefore, there&#39;s</span>
<span class="p_add">+	 * no plan to support it on other file types.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(file-&gt;f_mode &amp; FMODE_WRITE))</span>
<span class="p_add">+		return -EPERM;</span>
<span class="p_add">+	if (seals &amp; ~(unsigned int)F_ALL_SEALS)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	inode_lock(inode);</span>
<span class="p_add">+</span>
<span class="p_add">+	file_seals = memfd_file_seals_ptr(file);</span>
<span class="p_add">+	if (!file_seals) {</span>
<span class="p_add">+		error = -EINVAL;</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (*file_seals &amp; F_SEAL_SEAL) {</span>
<span class="p_add">+		error = -EPERM;</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((seals &amp; F_SEAL_WRITE) &amp;&amp; !(*file_seals &amp; F_SEAL_WRITE)) {</span>
<span class="p_add">+		error = mapping_deny_writable(file-&gt;f_mapping);</span>
<span class="p_add">+		if (error)</span>
<span class="p_add">+			goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+		error = shmem_wait_for_pins(file-&gt;f_mapping);</span>
<span class="p_add">+		if (error) {</span>
<span class="p_add">+			mapping_allow_writable(file-&gt;f_mapping);</span>
<span class="p_add">+			goto unlock;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	*file_seals |= seals;</span>
<span class="p_add">+	error = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+unlock:</span>
<span class="p_add">+	inode_unlock(inode);</span>
<span class="p_add">+	return error;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int memfd_get_seals(struct file *file)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int *seals = memfd_file_seals_ptr(file);</span>
<span class="p_add">+</span>
<span class="p_add">+	return seals ? *seals : -EINVAL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+long memfd_fcntl(struct file *file, unsigned int cmd, unsigned long arg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long error;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (cmd) {</span>
<span class="p_add">+	case F_ADD_SEALS:</span>
<span class="p_add">+		/* disallow upper 32bit */</span>
<span class="p_add">+		if (arg &gt; UINT_MAX)</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+		error = memfd_add_seals(file, arg);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case F_GET_SEALS:</span>
<span class="p_add">+		error = memfd_get_seals(file);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		error = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return error;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define MFD_NAME_PREFIX &quot;memfd:&quot;</span>
<span class="p_add">+#define MFD_NAME_PREFIX_LEN (sizeof(MFD_NAME_PREFIX) - 1)</span>
<span class="p_add">+#define MFD_NAME_MAX_LEN (NAME_MAX - MFD_NAME_PREFIX_LEN)</span>
<span class="p_add">+</span>
<span class="p_add">+#define MFD_ALL_FLAGS (MFD_CLOEXEC | MFD_ALLOW_SEALING | MFD_HUGETLB)</span>
<span class="p_add">+</span>
<span class="p_add">+SYSCALL_DEFINE2(memfd_create,</span>
<span class="p_add">+		const char __user *, uname,</span>
<span class="p_add">+		unsigned int, flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int *file_seals;</span>
<span class="p_add">+	struct file *file;</span>
<span class="p_add">+	int fd, error;</span>
<span class="p_add">+	char *name;</span>
<span class="p_add">+	long len;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(flags &amp; MFD_HUGETLB)) {</span>
<span class="p_add">+		if (flags &amp; ~(unsigned int)MFD_ALL_FLAGS)</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* Allow huge page size encoding in flags. */</span>
<span class="p_add">+		if (flags &amp; ~(unsigned int)(MFD_ALL_FLAGS |</span>
<span class="p_add">+				(MFD_HUGE_MASK &lt;&lt; MFD_HUGE_SHIFT)))</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* length includes terminating zero */</span>
<span class="p_add">+	len = strnlen_user(uname, MFD_NAME_MAX_LEN + 1);</span>
<span class="p_add">+	if (len &lt;= 0)</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	if (len &gt; MFD_NAME_MAX_LEN + 1)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	name = kmalloc(len + MFD_NAME_PREFIX_LEN, GFP_KERNEL);</span>
<span class="p_add">+	if (!name)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	strcpy(name, MFD_NAME_PREFIX);</span>
<span class="p_add">+	if (copy_from_user(&amp;name[MFD_NAME_PREFIX_LEN], uname, len)) {</span>
<span class="p_add">+		error = -EFAULT;</span>
<span class="p_add">+		goto err_name;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* terminating-zero may have changed after strnlen_user() returned */</span>
<span class="p_add">+	if (name[len + MFD_NAME_PREFIX_LEN - 1]) {</span>
<span class="p_add">+		error = -EFAULT;</span>
<span class="p_add">+		goto err_name;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	fd = get_unused_fd_flags((flags &amp; MFD_CLOEXEC) ? O_CLOEXEC : 0);</span>
<span class="p_add">+	if (fd &lt; 0) {</span>
<span class="p_add">+		error = fd;</span>
<span class="p_add">+		goto err_name;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flags &amp; MFD_HUGETLB) {</span>
<span class="p_add">+		struct user_struct *user = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		file = hugetlb_file_setup(name, 0, VM_NORESERVE, &amp;user,</span>
<span class="p_add">+					HUGETLB_ANONHUGE_INODE,</span>
<span class="p_add">+					(flags &gt;&gt; MFD_HUGE_SHIFT) &amp;</span>
<span class="p_add">+					MFD_HUGE_MASK);</span>
<span class="p_add">+	} else</span>
<span class="p_add">+		file = shmem_file_setup(name, 0, VM_NORESERVE);</span>
<span class="p_add">+	if (IS_ERR(file)) {</span>
<span class="p_add">+		error = PTR_ERR(file);</span>
<span class="p_add">+		goto err_fd;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	file-&gt;f_mode |= FMODE_LSEEK | FMODE_PREAD | FMODE_PWRITE;</span>
<span class="p_add">+	file-&gt;f_flags |= O_RDWR | O_LARGEFILE;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flags &amp; MFD_ALLOW_SEALING) {</span>
<span class="p_add">+		file_seals = memfd_file_seals_ptr(file);</span>
<span class="p_add">+		*file_seals &amp;= ~F_SEAL_SEAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	fd_install(fd, file);</span>
<span class="p_add">+	kfree(name);</span>
<span class="p_add">+	return fd;</span>
<span class="p_add">+</span>
<span class="p_add">+err_fd:</span>
<span class="p_add">+	put_unused_fd(fd);</span>
<span class="p_add">+err_name:</span>
<span class="p_add">+	kfree(name);</span>
<span class="p_add">+	return error;</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



