
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,2/3] powerpc: get hugetlbpage handling more generic - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,2/3] powerpc: get hugetlbpage handling more generic</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=11492">LEROY Christophe</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 21, 2016, 8:11 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;69b1226ce134761fd7ab81a498bdb85cd737280f.1474441302.git.christophe.leroy@c-s.fr&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9343007/mbox/"
   >mbox</a>
|
   <a href="/patch/9343007/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9343007/">/patch/9343007/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C03B2601C2 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Sep 2016 08:13:47 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id ABB442A2AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Sep 2016 08:13:47 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A07A32A2B8; Wed, 21 Sep 2016 08:13:47 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BCC6D2A2AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Sep 2016 08:13:46 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755961AbcIUINn (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 21 Sep 2016 04:13:43 -0400
Received: from pegase1.c-s.fr ([93.17.236.30]:8882 &quot;EHLO pegase1.c-s.fr&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754783AbcIUINR (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 21 Sep 2016 04:13:17 -0400
Received: from localhost (unknown [192.168.12.234])
	by localhost (Postfix) with ESMTP id 3sfC6q4Jnxz9ttDr;
	Wed, 21 Sep 2016 10:13:07 +0200 (CEST)
X-Virus-Scanned: Debian amavisd-new at c-s.fr
Received: from pegase1.c-s.fr ([192.168.12.234])
	by localhost (pegase1.c-s.fr [192.168.12.234]) (amavisd-new,
	port 10024)
	with ESMTP id 5BgclVMMjHY9; Wed, 21 Sep 2016 10:13:07 +0200 (CEST)
Received: from messagerie.si.c-s.fr (messagerie.si.c-s.fr [192.168.25.192])
	by pegase1.c-s.fr (Postfix) with ESMTP id 3sfC6q2GDzz9ttFB;
	Wed, 21 Sep 2016 10:13:07 +0200 (CEST)
Received: from localhost (localhost [127.0.0.1])
	by messagerie.si.c-s.fr (Postfix) with ESMTP id 758DD8B781;
	Wed, 21 Sep 2016 10:13:07 +0200 (CEST)
X-Virus-Scanned: amavisd-new at c-s.fr
Received: from messagerie.si.c-s.fr ([127.0.0.1])
	by localhost (messagerie.si.c-s.fr [127.0.0.1]) (amavisd-new,
	port 10023)
	with ESMTP id d3D5feLNEaF7; Wed, 21 Sep 2016 10:13:07 +0200 (CEST)
Received: from PO10863.localdomain (po10863.idsi0.si.c-s.fr [172.25.231.27])
	by messagerie.si.c-s.fr (Postfix) with ESMTP id 36CB58B78B;
	Wed, 21 Sep 2016 10:13:07 +0200 (CEST)
Received: by localhost.localdomain (Postfix, from userid 0)
	id 3EA101A2452; Wed, 21 Sep 2016 10:11:54 +0200 (CEST)
Message-Id: &lt;69b1226ce134761fd7ab81a498bdb85cd737280f.1474441302.git.christophe.leroy@c-s.fr&gt;
In-Reply-To: &lt;cover.1474441301.git.christophe.leroy@c-s.fr&gt;
References: &lt;cover.1474441301.git.christophe.leroy@c-s.fr&gt;
From: Christophe Leroy &lt;christophe.leroy@c-s.fr&gt;
Subject: [PATCH v3 2/3] powerpc: get hugetlbpage handling more generic
To: Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;,
	Paul Mackerras &lt;paulus@samba.org&gt;, Michael Ellerman &lt;mpe@ellerman.id.au&gt;,
	Scott Wood &lt;oss@buserror.net&gt;
Cc: linux-kernel@vger.kernel.org, linuxppc-dev@lists.ozlabs.org
Date: Wed, 21 Sep 2016 10:11:54 +0200 (CEST)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11492">LEROY Christophe</a> - Sept. 21, 2016, 8:11 a.m.</div>
<pre class="content">
Today there are two implementations of hugetlbpages which are managed
by exclusive #ifdefs:
* FSL_BOOKE: several directory entries points to the same single hugepage
* BOOK3S: one upper level directory entry points to a table of hugepages

In preparation of implementation of hugepage support on the 8xx, we
need a mix of the two above solutions, because the 8xx needs both cases
depending on the size of pages:
* In 4k page size mode, each PGD entry covers a 4M bytes area. It means
that 2 PGD entries will be necessary to cover an 8M hugepage while a
single PGD entry will cover 8x 512k hugepages.
* In 16 page size mode, each PGD entry covers a 64M bytes area. It means
that 8x 8M hugepages will be covered by one PGD entry and 64x 512k
hugepages will be covers by one PGD entry.

This patch:
* removes #ifdefs in favor of if/else based on the range sizes
* merges the two huge_pte_alloc() functions as they are pretty similar
* merges the two hugetlbpage_init() functions as they are pretty similar
<span class="signed-off-by">
Signed-off-by: Christophe Leroy &lt;christophe.leroy@c-s.fr&gt;</span>
---
v2: This part is new and results from a split of last patch of v1 serie in
two parts

v3:
- Only allocate hugepte_cache on FSL_BOOKE. Not needed on BOOK3S_64
- Removed the BUG in the unused hugepd_free(), made it
static inline {} instead.

 arch/powerpc/mm/hugetlbpage.c | 188 +++++++++++++++++-------------------------
 1 file changed, 76 insertions(+), 112 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - Sept. 21, 2016, 4:58 p.m.</div>
<pre class="content">
Christophe Leroy &lt;christophe.leroy@c-s.fr&gt; writes:
<span class="quote">
&gt; Today there are two implementations of hugetlbpages which are managed</span>
<span class="quote">&gt; by exclusive #ifdefs:</span>
<span class="quote">&gt; * FSL_BOOKE: several directory entries points to the same single hugepage</span>
<span class="quote">&gt; * BOOK3S: one upper level directory entry points to a table of hugepages</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In preparation of implementation of hugepage support on the 8xx, we</span>
<span class="quote">&gt; need a mix of the two above solutions, because the 8xx needs both cases</span>
<span class="quote">&gt; depending on the size of pages:</span>
<span class="quote">&gt; * In 4k page size mode, each PGD entry covers a 4M bytes area. It means</span>
<span class="quote">&gt; that 2 PGD entries will be necessary to cover an 8M hugepage while a</span>
<span class="quote">&gt; single PGD entry will cover 8x 512k hugepages.</span>
<span class="quote">&gt; * In 16 page size mode, each PGD entry covers a 64M bytes area. It means</span>
<span class="quote">&gt; that 8x 8M hugepages will be covered by one PGD entry and 64x 512k</span>
<span class="quote">&gt; hugepages will be covers by one PGD entry.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch:</span>
<span class="quote">&gt; * removes #ifdefs in favor of if/else based on the range sizes</span>
<span class="quote">&gt; * merges the two huge_pte_alloc() functions as they are pretty similar</span>
<span class="quote">&gt; * merges the two hugetlbpage_init() functions as they are pretty similar</span>
<span class="reviewed-by">
Reviewed-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">

&gt;</span>
<span class="quote">&gt; Signed-off-by: Christophe Leroy &lt;christophe.leroy@c-s.fr&gt;</span>
<span class="quote">&gt; ---</span>

-aneesh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=151011">Scott Wood</a> - Nov. 24, 2016, 5:23 a.m.</div>
<pre class="content">
On Wed, Sep 21, 2016 at 10:11:54AM +0200, Christophe Leroy wrote:
<span class="quote">&gt; Today there are two implementations of hugetlbpages which are managed</span>
<span class="quote">&gt; by exclusive #ifdefs:</span>
<span class="quote">&gt; * FSL_BOOKE: several directory entries points to the same single hugepage</span>
<span class="quote">&gt; * BOOK3S: one upper level directory entry points to a table of hugepages</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In preparation of implementation of hugepage support on the 8xx, we</span>
<span class="quote">&gt; need a mix of the two above solutions, because the 8xx needs both cases</span>
<span class="quote">&gt; depending on the size of pages:</span>
<span class="quote">&gt; * In 4k page size mode, each PGD entry covers a 4M bytes area. It means</span>
<span class="quote">&gt; that 2 PGD entries will be necessary to cover an 8M hugepage while a</span>
<span class="quote">&gt; single PGD entry will cover 8x 512k hugepages.</span>
<span class="quote">&gt; * In 16 page size mode, each PGD entry covers a 64M bytes area. It means</span>
<span class="quote">&gt; that 8x 8M hugepages will be covered by one PGD entry and 64x 512k</span>
<span class="quote">&gt; hugepages will be covers by one PGD entry.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch:</span>
<span class="quote">&gt; * removes #ifdefs in favor of if/else based on the range sizes</span>
<span class="quote">&gt; * merges the two huge_pte_alloc() functions as they are pretty similar</span>
<span class="quote">&gt; * merges the two hugetlbpage_init() functions as they are pretty similar</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Christophe Leroy &lt;christophe.leroy@c-s.fr&gt;</span>
<span class="quote">&gt; Reviewed-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>

With this patch on e6500, running the hugetlb testsuite results in the
system hanging in a storm of OOM killer invocations (I&#39;ll try to debug
more deeply later).  This patch also changes the default hugepage size on
FSL book3e from 4M to 16M.

-Scott
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11492">LEROY Christophe</a> - Nov. 25, 2016, 8:14 a.m.</div>
<pre class="content">
Le 24/11/2016 à 06:23, Scott Wood a écrit :
<span class="quote">&gt; On Wed, Sep 21, 2016 at 10:11:54AM +0200, Christophe Leroy wrote:</span>
<span class="quote">&gt;&gt; Today there are two implementations of hugetlbpages which are managed</span>
<span class="quote">&gt;&gt; by exclusive #ifdefs:</span>
<span class="quote">&gt;&gt; * FSL_BOOKE: several directory entries points to the same single hugepage</span>
<span class="quote">&gt;&gt; * BOOK3S: one upper level directory entry points to a table of hugepages</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; In preparation of implementation of hugepage support on the 8xx, we</span>
<span class="quote">&gt;&gt; need a mix of the two above solutions, because the 8xx needs both cases</span>
<span class="quote">&gt;&gt; depending on the size of pages:</span>
<span class="quote">&gt;&gt; * In 4k page size mode, each PGD entry covers a 4M bytes area. It means</span>
<span class="quote">&gt;&gt; that 2 PGD entries will be necessary to cover an 8M hugepage while a</span>
<span class="quote">&gt;&gt; single PGD entry will cover 8x 512k hugepages.</span>
<span class="quote">&gt;&gt; * In 16 page size mode, each PGD entry covers a 64M bytes area. It means</span>
<span class="quote">&gt;&gt; that 8x 8M hugepages will be covered by one PGD entry and 64x 512k</span>
<span class="quote">&gt;&gt; hugepages will be covers by one PGD entry.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch:</span>
<span class="quote">&gt;&gt; * removes #ifdefs in favor of if/else based on the range sizes</span>
<span class="quote">&gt;&gt; * merges the two huge_pte_alloc() functions as they are pretty similar</span>
<span class="quote">&gt;&gt; * merges the two hugetlbpage_init() functions as they are pretty similar</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Christophe Leroy &lt;christophe.leroy@c-s.fr&gt;</span>
<span class="quote">&gt;&gt; Reviewed-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; With this patch on e6500, running the hugetlb testsuite results in the</span>
<span class="quote">&gt; system hanging in a storm of OOM killer invocations (I&#39;ll try to debug</span>
<span class="quote">&gt; more deeply later).  This patch also changes the default hugepage size on</span>
<span class="quote">&gt; FSL book3e from 4M to 16M.</span>
<span class="quote">&gt;</span>

Regarding the default hugepage size, it is a result of the merge of the 
two hugetlbpage_init().
Should I add an ifdef to get 4M on FSL book3e by default ?
What&#39;s the reason for selecting different hugepage sizes depending on 
the CPU ? I thought default size was selected based on what was existing.

What testsuite do you run exactly ?

Christophe
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=151011">Scott Wood</a> - Dec. 5, 2016, 2:58 a.m.</div>
<pre class="content">
On Fri, 2016-11-25 at 09:14 +0100, Christophe LEROY wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; Le 24/11/2016 à 06:23, Scott Wood a écrit :</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On Wed, Sep 21, 2016 at 10:11:54AM +0200, Christophe Leroy wrote:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Today there are two implementations of hugetlbpages which are managed</span>
<span class="quote">&gt; &gt; &gt; by exclusive #ifdefs:</span>
<span class="quote">&gt; &gt; &gt; * FSL_BOOKE: several directory entries points to the same single</span>
<span class="quote">&gt; &gt; &gt; hugepage</span>
<span class="quote">&gt; &gt; &gt; * BOOK3S: one upper level directory entry points to a table of hugepages</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; In preparation of implementation of hugepage support on the 8xx, we</span>
<span class="quote">&gt; &gt; &gt; need a mix of the two above solutions, because the 8xx needs both cases</span>
<span class="quote">&gt; &gt; &gt; depending on the size of pages:</span>
<span class="quote">&gt; &gt; &gt; * In 4k page size mode, each PGD entry covers a 4M bytes area. It means</span>
<span class="quote">&gt; &gt; &gt; that 2 PGD entries will be necessary to cover an 8M hugepage while a</span>
<span class="quote">&gt; &gt; &gt; single PGD entry will cover 8x 512k hugepages.</span>
<span class="quote">&gt; &gt; &gt; * In 16 page size mode, each PGD entry covers a 64M bytes area. It means</span>
<span class="quote">&gt; &gt; &gt; that 8x 8M hugepages will be covered by one PGD entry and 64x 512k</span>
<span class="quote">&gt; &gt; &gt; hugepages will be covers by one PGD entry.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This patch:</span>
<span class="quote">&gt; &gt; &gt; * removes #ifdefs in favor of if/else based on the range sizes</span>
<span class="quote">&gt; &gt; &gt; * merges the two huge_pte_alloc() functions as they are pretty similar</span>
<span class="quote">&gt; &gt; &gt; * merges the two hugetlbpage_init() functions as they are pretty similar</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Signed-off-by: Christophe Leroy &lt;christophe.leroy@c-s.fr&gt;</span>
<span class="quote">&gt; &gt; &gt; Reviewed-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="quote">&gt; &gt; With this patch on e6500, running the hugetlb testsuite results in the</span>
<span class="quote">&gt; &gt; system hanging in a storm of OOM killer invocations (I&#39;ll try to debug</span>
<span class="quote">&gt; &gt; more deeply later).  This patch also changes the default hugepage size on</span>
<span class="quote">&gt; &gt; FSL book3e from 4M to 16M.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; Regarding the default hugepage size, it is a result of the merge of the </span>
<span class="quote">&gt; two hugetlbpage_init().</span>
<span class="quote">&gt; Should I add an ifdef to get 4M on FSL book3e by default ?</span>
<span class="quote">&gt; What&#39;s the reason for selecting different hugepage sizes depending on </span>
<span class="quote">&gt; the CPU ? </span>

I&#39;m not sure what the original reason was, but a change in defaults could
disturb existing users.
<span class="quote">
&gt; I thought default size was selected based on what was existing.</span>

...by code that doesn&#39;t expect 4M to ever exist.  Both 4M and 16M (and a bunch
of other sizes) are available on FSL book3e.

What is the reason for preferring 16M over 1M, but preferring 1M over 2M?
 Seems arbitrary.

If we want to preserve the exsiting behavior without an ifdef, we could put a
check for 4M before the other sizes, with a comment explaining why we&#39;re
making the selection look even more arbitrary.  Or we could try to figure out
what size actually makes a better default.
<span class="quote">
&gt; What testsuite do you run exactly ?</span>

The one that comes with libhugetlbfs (not a particularly recent version, but
not sure exactly how old).

-Scott
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=151011">Scott Wood</a> - Dec. 6, 2016, 1:18 a.m.</div>
<pre class="content">
On Wed, 2016-09-21 at 10:11 +0200, Christophe Leroy wrote:
<span class="quote">&gt; Today there are two implementations of hugetlbpages which are managed</span>
<span class="quote">&gt; by exclusive #ifdefs:</span>
<span class="quote">&gt; * FSL_BOOKE: several directory entries points to the same single hugepage</span>
<span class="quote">&gt; * BOOK3S: one upper level directory entry points to a table of hugepages</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In preparation of implementation of hugepage support on the 8xx, we</span>
<span class="quote">&gt; need a mix of the two above solutions, because the 8xx needs both cases</span>
<span class="quote">&gt; depending on the size of pages:</span>
<span class="quote">&gt; * In 4k page size mode, each PGD entry covers a 4M bytes area. It means</span>
<span class="quote">&gt; that 2 PGD entries will be necessary to cover an 8M hugepage while a</span>
<span class="quote">&gt; single PGD entry will cover 8x 512k hugepages.</span>
<span class="quote">&gt; * In 16 page size mode, each PGD entry covers a 64M bytes area. It means</span>
<span class="quote">&gt; that 8x 8M hugepages will be covered by one PGD entry and 64x 512k</span>
<span class="quote">&gt; hugepages will be covers by one PGD entry.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch:</span>
<span class="quote">&gt; * removes #ifdefs in favor of if/else based on the range sizes</span>
<span class="quote">&gt; * merges the two huge_pte_alloc() functions as they are pretty similar</span>
<span class="quote">&gt; * merges the two hugetlbpage_init() functions as they are pretty similar</span>
[snip]
<span class="quote">&gt; @@ -860,16 +803,34 @@ static int __init hugetlbpage_init(void)</span>
<span class="quote">&gt;  		 * if we have pdshift and shift value same, we don&#39;t</span>
<span class="quote">&gt;  		 * use pgt cache for hugepd.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (pdshift != shift) {</span>
<span class="quote">&gt; +		if (pdshift &gt; shift) {</span>
<span class="quote">&gt;  			pgtable_cache_add(pdshift - shift, NULL);</span>
<span class="quote">&gt;  			if (!PGT_CACHE(pdshift - shift))</span>
<span class="quote">&gt;  				panic(&quot;hugetlbpage_init(): could not create</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt;  				      &quot;pgtable cache for %d bit</span>
<span class="quote">&gt; pagesize\n&quot;, shift);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +#ifdef CONFIG_PPC_FSL_BOOK3E</span>
<span class="quote">&gt; +		else if (!hugepte_cache) {</span>

This else never triggers on book3e, because the way this function calculates
pdshift is wrong for book3e (it uses PyD_SHIFT instead of HUGEPD_PxD_SHIFT).
 We later get OOMs because huge_pte_alloc() calculates pdshift correctly,
tries to use hugepte_cache, and fails.

If the point of this patch is to remove the compile-time decision on whether
to do things the book3e way, why are there still ifdefs such as the ones
controlling the definition of HUGEPD_PxD_SHIFT?  How does what you&#39;re doing on
8xx (for certain page sizes) differ from book3e?

-Scott
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11492">LEROY Christophe</a> - Dec. 6, 2016, 6:34 a.m.</div>
<pre class="content">
Le 06/12/2016 à 02:18, Scott Wood a écrit :
<span class="quote">&gt; On Wed, 2016-09-21 at 10:11 +0200, Christophe Leroy wrote:</span>
<span class="quote">&gt;&gt; Today there are two implementations of hugetlbpages which are managed</span>
<span class="quote">&gt;&gt; by exclusive #ifdefs:</span>
<span class="quote">&gt;&gt; * FSL_BOOKE: several directory entries points to the same single hugepage</span>
<span class="quote">&gt;&gt; * BOOK3S: one upper level directory entry points to a table of hugepages</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; In preparation of implementation of hugepage support on the 8xx, we</span>
<span class="quote">&gt;&gt; need a mix of the two above solutions, because the 8xx needs both cases</span>
<span class="quote">&gt;&gt; depending on the size of pages:</span>
<span class="quote">&gt;&gt; * In 4k page size mode, each PGD entry covers a 4M bytes area. It means</span>
<span class="quote">&gt;&gt; that 2 PGD entries will be necessary to cover an 8M hugepage while a</span>
<span class="quote">&gt;&gt; single PGD entry will cover 8x 512k hugepages.</span>
<span class="quote">&gt;&gt; * In 16 page size mode, each PGD entry covers a 64M bytes area. It means</span>
<span class="quote">&gt;&gt; that 8x 8M hugepages will be covered by one PGD entry and 64x 512k</span>
<span class="quote">&gt;&gt; hugepages will be covers by one PGD entry.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch:</span>
<span class="quote">&gt;&gt; * removes #ifdefs in favor of if/else based on the range sizes</span>
<span class="quote">&gt;&gt; * merges the two huge_pte_alloc() functions as they are pretty similar</span>
<span class="quote">&gt;&gt; * merges the two hugetlbpage_init() functions as they are pretty similar</span>
<span class="quote">&gt; [snip]</span>
<span class="quote">&gt;&gt; @@ -860,16 +803,34 @@ static int __init hugetlbpage_init(void)</span>
<span class="quote">&gt;&gt;  		 * if we have pdshift and shift value same, we don&#39;t</span>
<span class="quote">&gt;&gt;  		 * use pgt cache for hugepd.</span>
<span class="quote">&gt;&gt;  		 */</span>
<span class="quote">&gt;&gt; -		if (pdshift != shift) {</span>
<span class="quote">&gt;&gt; +		if (pdshift &gt; shift) {</span>
<span class="quote">&gt;&gt;  			pgtable_cache_add(pdshift - shift, NULL);</span>
<span class="quote">&gt;&gt;  			if (!PGT_CACHE(pdshift - shift))</span>
<span class="quote">&gt;&gt;  				panic(&quot;hugetlbpage_init(): could not create</span>
<span class="quote">&gt;&gt; &quot;</span>
<span class="quote">&gt;&gt;  				      &quot;pgtable cache for %d bit</span>
<span class="quote">&gt;&gt; pagesize\n&quot;, shift);</span>
<span class="quote">&gt;&gt;  		}</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_PPC_FSL_BOOK3E</span>
<span class="quote">&gt;&gt; +		else if (!hugepte_cache) {</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This else never triggers on book3e, because the way this function calculates</span>
<span class="quote">&gt; pdshift is wrong for book3e (it uses PyD_SHIFT instead of HUGEPD_PxD_SHIFT).</span>
<span class="quote">&gt;  We later get OOMs because huge_pte_alloc() calculates pdshift correctly,</span>
<span class="quote">&gt; tries to use hugepte_cache, and fails.</span>

Ok, I&#39;ll check it again, I was expecting it to still work properly on 
book3e, because after applying patch 3 it works properly on the 8xx.
<span class="quote">
&gt;</span>
<span class="quote">&gt; If the point of this patch is to remove the compile-time decision on whether</span>
<span class="quote">&gt; to do things the book3e way, why are there still ifdefs such as the ones</span>
<span class="quote">&gt; controlling the definition of HUGEPD_PxD_SHIFT?  How does what you&#39;re doing on</span>
<span class="quote">&gt; 8xx (for certain page sizes) differ from book3e?</span>

Some of the things done for book3e are common to 8xx, but differ from 
book3s. For that reason, in the following patch (3/3), there is in 
several places:
-#ifdef CONFIG_PPC_FSL_BOOK3E
+#if defined(CONFIG_PPC_FSL_BOOK3E) || defined(CONFIG_PPC_8xx)

Christophe
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=151011">Scott Wood</a> - Dec. 7, 2016, 1:06 a.m.</div>
<pre class="content">
On Tue, 2016-12-06 at 07:34 +0100, Christophe LEROY wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; Le 06/12/2016 à 02:18, Scott Wood a écrit :</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On Wed, 2016-09-21 at 10:11 +0200, Christophe Leroy wrote:</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Today there are two implementations of hugetlbpages which are managed</span>
<span class="quote">&gt; &gt; &gt; by exclusive #ifdefs:</span>
<span class="quote">&gt; &gt; &gt; * FSL_BOOKE: several directory entries points to the same single</span>
<span class="quote">&gt; &gt; &gt; hugepage</span>
<span class="quote">&gt; &gt; &gt; * BOOK3S: one upper level directory entry points to a table of hugepages</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; In preparation of implementation of hugepage support on the 8xx, we</span>
<span class="quote">&gt; &gt; &gt; need a mix of the two above solutions, because the 8xx needs both cases</span>
<span class="quote">&gt; &gt; &gt; depending on the size of pages:</span>
<span class="quote">&gt; &gt; &gt; * In 4k page size mode, each PGD entry covers a 4M bytes area. It means</span>
<span class="quote">&gt; &gt; &gt; that 2 PGD entries will be necessary to cover an 8M hugepage while a</span>
<span class="quote">&gt; &gt; &gt; single PGD entry will cover 8x 512k hugepages.</span>
<span class="quote">&gt; &gt; &gt; * In 16 page size mode, each PGD entry covers a 64M bytes area. It means</span>
<span class="quote">&gt; &gt; &gt; that 8x 8M hugepages will be covered by one PGD entry and 64x 512k</span>
<span class="quote">&gt; &gt; &gt; hugepages will be covers by one PGD entry.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This patch:</span>
<span class="quote">&gt; &gt; &gt; * removes #ifdefs in favor of if/else based on the range sizes</span>
<span class="quote">&gt; &gt; &gt; * merges the two huge_pte_alloc() functions as they are pretty similar</span>
<span class="quote">&gt; &gt; &gt; * merges the two hugetlbpage_init() functions as they are pretty similar</span>
<span class="quote">&gt; &gt; [snip]</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; @@ -860,16 +803,34 @@ static int __init hugetlbpage_init(void)</span>
<span class="quote">&gt; &gt; &gt;  		 * if we have pdshift and shift value same, we don&#39;t</span>
<span class="quote">&gt; &gt; &gt;  		 * use pgt cache for hugepd.</span>
<span class="quote">&gt; &gt; &gt;  		 */</span>
<span class="quote">&gt; &gt; &gt; -		if (pdshift != shift) {</span>
<span class="quote">&gt; &gt; &gt; +		if (pdshift &gt; shift) {</span>
<span class="quote">&gt; &gt; &gt;  			pgtable_cache_add(pdshift - shift, NULL);</span>
<span class="quote">&gt; &gt; &gt;  			if (!PGT_CACHE(pdshift - shift))</span>
<span class="quote">&gt; &gt; &gt;  				panic(&quot;hugetlbpage_init(): could not</span>
<span class="quote">&gt; &gt; &gt; create</span>
<span class="quote">&gt; &gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; &gt;  				      &quot;pgtable cache for %d bit</span>
<span class="quote">&gt; &gt; &gt; pagesize\n&quot;, shift);</span>
<span class="quote">&gt; &gt; &gt;  		}</span>
<span class="quote">&gt; &gt; &gt; +#ifdef CONFIG_PPC_FSL_BOOK3E</span>
<span class="quote">&gt; &gt; &gt; +		else if (!hugepte_cache) {</span>
<span class="quote">&gt; &gt; This else never triggers on book3e, because the way this function</span>
<span class="quote">&gt; &gt; calculates</span>
<span class="quote">&gt; &gt; pdshift is wrong for book3e (it uses PyD_SHIFT instead of</span>
<span class="quote">&gt; &gt; HUGEPD_PxD_SHIFT).</span>
<span class="quote">&gt; &gt;  We later get OOMs because huge_pte_alloc() calculates pdshift correctly,</span>
<span class="quote">&gt; &gt; tries to use hugepte_cache, and fails.</span>
<span class="quote">&gt; Ok, I&#39;ll check it again, I was expecting it to still work properly on </span>
<span class="quote">&gt; book3e, because after applying patch 3 it works properly on the 8xx.</span>

On 8xx you probably happen to have a page size that yields &quot;pdshift &lt;= shift&quot;
even with the incorrect pdshift calculation, causing hugepte_cache to be
allocated.  The smallest hugepage size on 8xx is 512k compared to 4M on fsl-
book3e.

-Scott
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11492">LEROY Christophe</a> - Dec. 7, 2016, 6:59 a.m.</div>
<pre class="content">
Le 07/12/2016 à 02:06, Scott Wood a écrit :
<span class="quote">&gt; On Tue, 2016-12-06 at 07:34 +0100, Christophe LEROY wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Le 06/12/2016 à 02:18, Scott Wood a écrit :</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On Wed, 2016-09-21 at 10:11 +0200, Christophe Leroy wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Today there are two implementations of hugetlbpages which are managed</span>
<span class="quote">&gt;&gt;&gt;&gt; by exclusive #ifdefs:</span>
<span class="quote">&gt;&gt;&gt;&gt; * FSL_BOOKE: several directory entries points to the same single</span>
<span class="quote">&gt;&gt;&gt;&gt; hugepage</span>
<span class="quote">&gt;&gt;&gt;&gt; * BOOK3S: one upper level directory entry points to a table of hugepages</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; In preparation of implementation of hugepage support on the 8xx, we</span>
<span class="quote">&gt;&gt;&gt;&gt; need a mix of the two above solutions, because the 8xx needs both cases</span>
<span class="quote">&gt;&gt;&gt;&gt; depending on the size of pages:</span>
<span class="quote">&gt;&gt;&gt;&gt; * In 4k page size mode, each PGD entry covers a 4M bytes area. It means</span>
<span class="quote">&gt;&gt;&gt;&gt; that 2 PGD entries will be necessary to cover an 8M hugepage while a</span>
<span class="quote">&gt;&gt;&gt;&gt; single PGD entry will cover 8x 512k hugepages.</span>
<span class="quote">&gt;&gt;&gt;&gt; * In 16 page size mode, each PGD entry covers a 64M bytes area. It means</span>
<span class="quote">&gt;&gt;&gt;&gt; that 8x 8M hugepages will be covered by one PGD entry and 64x 512k</span>
<span class="quote">&gt;&gt;&gt;&gt; hugepages will be covers by one PGD entry.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This patch:</span>
<span class="quote">&gt;&gt;&gt;&gt; * removes #ifdefs in favor of if/else based on the range sizes</span>
<span class="quote">&gt;&gt;&gt;&gt; * merges the two huge_pte_alloc() functions as they are pretty similar</span>
<span class="quote">&gt;&gt;&gt;&gt; * merges the two hugetlbpage_init() functions as they are pretty similar</span>
<span class="quote">&gt;&gt;&gt; [snip]</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -860,16 +803,34 @@ static int __init hugetlbpage_init(void)</span>
<span class="quote">&gt;&gt;&gt;&gt;  		 * if we have pdshift and shift value same, we don&#39;t</span>
<span class="quote">&gt;&gt;&gt;&gt;  		 * use pgt cache for hugepd.</span>
<span class="quote">&gt;&gt;&gt;&gt;  		 */</span>
<span class="quote">&gt;&gt;&gt;&gt; -		if (pdshift != shift) {</span>
<span class="quote">&gt;&gt;&gt;&gt; +		if (pdshift &gt; shift) {</span>
<span class="quote">&gt;&gt;&gt;&gt;  			pgtable_cache_add(pdshift - shift, NULL);</span>
<span class="quote">&gt;&gt;&gt;&gt;  			if (!PGT_CACHE(pdshift - shift))</span>
<span class="quote">&gt;&gt;&gt;&gt;  				panic(&quot;hugetlbpage_init(): could not</span>
<span class="quote">&gt;&gt;&gt;&gt; create</span>
<span class="quote">&gt;&gt;&gt;&gt; &quot;</span>
<span class="quote">&gt;&gt;&gt;&gt;  				      &quot;pgtable cache for %d bit</span>
<span class="quote">&gt;&gt;&gt;&gt; pagesize\n&quot;, shift);</span>
<span class="quote">&gt;&gt;&gt;&gt;  		}</span>
<span class="quote">&gt;&gt;&gt;&gt; +#ifdef CONFIG_PPC_FSL_BOOK3E</span>
<span class="quote">&gt;&gt;&gt;&gt; +		else if (!hugepte_cache) {</span>
<span class="quote">&gt;&gt;&gt; This else never triggers on book3e, because the way this function</span>
<span class="quote">&gt;&gt;&gt; calculates</span>
<span class="quote">&gt;&gt;&gt; pdshift is wrong for book3e (it uses PyD_SHIFT instead of</span>
<span class="quote">&gt;&gt;&gt; HUGEPD_PxD_SHIFT).</span>
<span class="quote">&gt;&gt;&gt;  We later get OOMs because huge_pte_alloc() calculates pdshift correctly,</span>
<span class="quote">&gt;&gt;&gt; tries to use hugepte_cache, and fails.</span>
<span class="quote">&gt;&gt; Ok, I&#39;ll check it again, I was expecting it to still work properly on</span>
<span class="quote">&gt;&gt; book3e, because after applying patch 3 it works properly on the 8xx.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On 8xx you probably happen to have a page size that yields &quot;pdshift &lt;= shift&quot;</span>
<span class="quote">&gt; even with the incorrect pdshift calculation, causing hugepte_cache to be</span>
<span class="quote">&gt; allocated.  The smallest hugepage size on 8xx is 512k compared to 4M on fsl-</span>
<span class="quote">&gt; book3e.</span>
<span class="quote">&gt;</span>

Indeed it works because on 8xx, PUD_SHIFT == PMD_SHIFT == PGDIR_SHIFT

Christophe
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/mm/hugetlbpage.c b/arch/powerpc/mm/hugetlbpage.c</span>
<span class="p_header">index a5d3ecd..a0d049a 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -64,14 +64,16 @@</span> <span class="p_context"> static int __hugepte_alloc(struct mm_struct *mm, hugepd_t *hpdp,</span>
 {
 	struct kmem_cache *cachep;
 	pte_t *new;
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_PPC_FSL_BOOK3E</span>
 	int i;
<span class="p_del">-	int num_hugepd = 1 &lt;&lt; (pshift - pdshift);</span>
<span class="p_del">-	cachep = hugepte_cache;</span>
<span class="p_del">-#else</span>
<span class="p_del">-	cachep = PGT_CACHE(pdshift - pshift);</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	int num_hugepd;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pshift &gt;= pdshift) {</span>
<span class="p_add">+		cachep = hugepte_cache;</span>
<span class="p_add">+		num_hugepd = 1 &lt;&lt; (pshift - pdshift);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		cachep = PGT_CACHE(pdshift - pshift);</span>
<span class="p_add">+		num_hugepd = 1;</span>
<span class="p_add">+	}</span>
 
 	new = kmem_cache_zalloc(cachep, GFP_KERNEL);
 
<span class="p_chunk">@@ -89,7 +91,7 @@</span> <span class="p_context"> static int __hugepte_alloc(struct mm_struct *mm, hugepd_t *hpdp,</span>
 	smp_wmb();
 
 	spin_lock(&amp;mm-&gt;page_table_lock);
<span class="p_del">-#ifdef CONFIG_PPC_FSL_BOOK3E</span>
<span class="p_add">+</span>
 	/*
 	 * We have multiple higher-level entries that point to the same
 	 * actual pte location.  Fill in each as we go and backtrack on error.
<span class="p_chunk">@@ -100,8 +102,13 @@</span> <span class="p_context"> static int __hugepte_alloc(struct mm_struct *mm, hugepd_t *hpdp,</span>
 		if (unlikely(!hugepd_none(*hpdp)))
 			break;
 		else
<span class="p_add">+#ifdef CONFIG_PPC_BOOK3S_64</span>
<span class="p_add">+			hpdp-&gt;pd = __pa(new) |</span>
<span class="p_add">+				   (shift_to_mmu_psize(pshift) &lt;&lt; 2);</span>
<span class="p_add">+#else</span>
 			/* We use the old format for PPC_FSL_BOOK3E */
 			hpdp-&gt;pd = ((unsigned long)new &amp; ~PD_HUGE) | pshift;
<span class="p_add">+#endif</span>
 	}
 	/* If we bailed from the for loop early, an error occurred, clean up */
 	if (i &lt; num_hugepd) {
<span class="p_chunk">@@ -109,17 +116,6 @@</span> <span class="p_context"> static int __hugepte_alloc(struct mm_struct *mm, hugepd_t *hpdp,</span>
 			hpdp-&gt;pd = 0;
 		kmem_cache_free(cachep, new);
 	}
<span class="p_del">-#else</span>
<span class="p_del">-	if (!hugepd_none(*hpdp))</span>
<span class="p_del">-		kmem_cache_free(cachep, new);</span>
<span class="p_del">-	else {</span>
<span class="p_del">-#ifdef CONFIG_PPC_BOOK3S_64</span>
<span class="p_del">-		hpdp-&gt;pd = __pa(new) | (shift_to_mmu_psize(pshift) &lt;&lt; 2);</span>
<span class="p_del">-#else</span>
<span class="p_del">-		hpdp-&gt;pd = ((unsigned long)new &amp; ~PD_HUGE) | pshift;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	}</span>
<span class="p_del">-#endif</span>
 	spin_unlock(&amp;mm-&gt;page_table_lock);
 	return 0;
 }
<span class="p_chunk">@@ -136,7 +132,6 @@</span> <span class="p_context"> static int __hugepte_alloc(struct mm_struct *mm, hugepd_t *hpdp,</span>
 #define HUGEPD_PUD_SHIFT PMD_SHIFT
 #endif
 
<span class="p_del">-#ifdef CONFIG_PPC_BOOK3S_64</span>
 /*
  * At this point we do the placement change only for BOOK3S 64. This would
  * possibly work on other subarchs.
<span class="p_chunk">@@ -153,6 +148,7 @@</span> <span class="p_context"> pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr, unsigned long sz</span>
 	addr &amp;= ~(sz-1);
 	pg = pgd_offset(mm, addr);
 
<span class="p_add">+#ifdef CONFIG_PPC_BOOK3S_64</span>
 	if (pshift == PGDIR_SHIFT)
 		/* 16GB huge page */
 		return (pte_t *) pg;
<span class="p_chunk">@@ -178,32 +174,7 @@</span> <span class="p_context"> pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr, unsigned long sz</span>
 				hpdp = (hugepd_t *)pm;
 		}
 	}
<span class="p_del">-	if (!hpdp)</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	BUG_ON(!hugepd_none(*hpdp) &amp;&amp; !hugepd_ok(*hpdp));</span>
<span class="p_del">-</span>
<span class="p_del">-	if (hugepd_none(*hpdp) &amp;&amp; __hugepte_alloc(mm, hpdp, addr, pdshift, pshift))</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	return hugepte_offset(*hpdp, addr, pdshift);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #else
<span class="p_del">-</span>
<span class="p_del">-pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr, unsigned long sz)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pgd_t *pg;</span>
<span class="p_del">-	pud_t *pu;</span>
<span class="p_del">-	pmd_t *pm;</span>
<span class="p_del">-	hugepd_t *hpdp = NULL;</span>
<span class="p_del">-	unsigned pshift = __ffs(sz);</span>
<span class="p_del">-	unsigned pdshift = PGDIR_SHIFT;</span>
<span class="p_del">-</span>
<span class="p_del">-	addr &amp;= ~(sz-1);</span>
<span class="p_del">-</span>
<span class="p_del">-	pg = pgd_offset(mm, addr);</span>
<span class="p_del">-</span>
 	if (pshift &gt;= HUGEPD_PGD_SHIFT) {
 		hpdp = (hugepd_t *)pg;
 	} else {
<span class="p_chunk">@@ -217,7 +188,7 @@</span> <span class="p_context"> pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr, unsigned long sz</span>
 			hpdp = (hugepd_t *)pm;
 		}
 	}
<span class="p_del">-</span>
<span class="p_add">+#endif</span>
 	if (!hpdp)
 		return NULL;
 
<span class="p_chunk">@@ -228,7 +199,6 @@</span> <span class="p_context"> pte_t *huge_pte_alloc(struct mm_struct *mm, unsigned long addr, unsigned long sz</span>
 
 	return hugepte_offset(*hpdp, addr, pdshift);
 }
<span class="p_del">-#endif</span>
 
 #ifdef CONFIG_PPC_FSL_BOOK3E
 /* Build list of addresses of gigantic pages.  This function is used in early
<span class="p_chunk">@@ -310,7 +280,11 @@</span> <span class="p_context"> static int __init do_gpage_early_setup(char *param, char *val,</span>
 				npages = 0;
 			if (npages &gt; MAX_NUMBER_GPAGES) {
 				pr_warn(&quot;MMU: %lu pages requested for page &quot;
<span class="p_add">+#ifdef CONFIG_PHYS_ADDR_T_64BIT</span>
 					&quot;size %llu KB, limiting to &quot;
<span class="p_add">+#else</span>
<span class="p_add">+					&quot;size %u KB, limiting to &quot;</span>
<span class="p_add">+#endif</span>
 					__stringify(MAX_NUMBER_GPAGES) &quot;\n&quot;,
 					npages, size / 1024);
 				npages = MAX_NUMBER_GPAGES;
<span class="p_chunk">@@ -442,6 +416,8 @@</span> <span class="p_context"> static void hugepd_free(struct mmu_gather *tlb, void *hugepte)</span>
 	}
 	put_cpu_var(hugepd_freelist_cur);
 }
<span class="p_add">+#else</span>
<span class="p_add">+static inline void hugepd_free(struct mmu_gather *tlb, void *hugepte) {}</span>
 #endif
 
 static void free_hugepd_range(struct mmu_gather *tlb, hugepd_t *hpdp, int pdshift,
<span class="p_chunk">@@ -453,13 +429,11 @@</span> <span class="p_context"> static void free_hugepd_range(struct mmu_gather *tlb, hugepd_t *hpdp, int pdshif</span>
 
 	unsigned long pdmask = ~((1UL &lt;&lt; pdshift) - 1);
 	unsigned int num_hugepd = 1;
<span class="p_add">+	unsigned int shift = hugepd_shift(*hpdp);</span>
 
<span class="p_del">-#ifdef CONFIG_PPC_FSL_BOOK3E</span>
 	/* Note: On fsl the hpdp may be the first of several */
<span class="p_del">-	num_hugepd = (1 &lt;&lt; (hugepd_shift(*hpdp) - pdshift));</span>
<span class="p_del">-#else</span>
<span class="p_del">-	unsigned int shift = hugepd_shift(*hpdp);</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	if (shift &gt; pdshift)</span>
<span class="p_add">+		num_hugepd = 1 &lt;&lt; (shift - pdshift);</span>
 
 	start &amp;= pdmask;
 	if (start &lt; floor)
<span class="p_chunk">@@ -475,11 +449,10 @@</span> <span class="p_context"> static void free_hugepd_range(struct mmu_gather *tlb, hugepd_t *hpdp, int pdshif</span>
 	for (i = 0; i &lt; num_hugepd; i++, hpdp++)
 		hpdp-&gt;pd = 0;
 
<span class="p_del">-#ifdef CONFIG_PPC_FSL_BOOK3E</span>
<span class="p_del">-	hugepd_free(tlb, hugepte);</span>
<span class="p_del">-#else</span>
<span class="p_del">-	pgtable_free_tlb(tlb, hugepte, pdshift - shift);</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	if (shift &gt;= pdshift)</span>
<span class="p_add">+		hugepd_free(tlb, hugepte);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		pgtable_free_tlb(tlb, hugepte, pdshift - shift);</span>
 }
 
 static void hugetlb_free_pmd_range(struct mmu_gather *tlb, pud_t *pud,
<span class="p_chunk">@@ -492,6 +465,8 @@</span> <span class="p_context"> static void hugetlb_free_pmd_range(struct mmu_gather *tlb, pud_t *pud,</span>
 
 	start = addr;
 	do {
<span class="p_add">+		unsigned long more;</span>
<span class="p_add">+</span>
 		pmd = pmd_offset(pud, addr);
 		next = pmd_addr_end(addr, end);
 		if (!is_hugepd(__hugepd(pmd_val(*pmd)))) {
<span class="p_chunk">@@ -502,15 +477,16 @@</span> <span class="p_context"> static void hugetlb_free_pmd_range(struct mmu_gather *tlb, pud_t *pud,</span>
 			WARN_ON(!pmd_none_or_clear_bad(pmd));
 			continue;
 		}
<span class="p_del">-#ifdef CONFIG_PPC_FSL_BOOK3E</span>
 		/*
 		 * Increment next by the size of the huge mapping since
 		 * there may be more than one entry at this level for a
 		 * single hugepage, but all of them point to
 		 * the same kmem cache that holds the hugepte.
 		 */
<span class="p_del">-		next = addr + (1 &lt;&lt; hugepd_shift(*(hugepd_t *)pmd));</span>
<span class="p_del">-#endif</span>
<span class="p_add">+		more = addr + (1 &lt;&lt; hugepd_shift(*(hugepd_t *)pmd));</span>
<span class="p_add">+		if (more &gt; next)</span>
<span class="p_add">+			next = more;</span>
<span class="p_add">+</span>
 		free_hugepd_range(tlb, (hugepd_t *)pmd, PMD_SHIFT,
 				  addr, next, floor, ceiling);
 	} while (addr = next, addr != end);
<span class="p_chunk">@@ -550,15 +526,17 @@</span> <span class="p_context"> static void hugetlb_free_pud_range(struct mmu_gather *tlb, pgd_t *pgd,</span>
 			hugetlb_free_pmd_range(tlb, pud, addr, next, floor,
 					       ceiling);
 		} else {
<span class="p_del">-#ifdef CONFIG_PPC_FSL_BOOK3E</span>
<span class="p_add">+			unsigned long more;</span>
 			/*
 			 * Increment next by the size of the huge mapping since
 			 * there may be more than one entry at this level for a
 			 * single hugepage, but all of them point to
 			 * the same kmem cache that holds the hugepte.
 			 */
<span class="p_del">-			next = addr + (1 &lt;&lt; hugepd_shift(*(hugepd_t *)pud));</span>
<span class="p_del">-#endif</span>
<span class="p_add">+			more = addr + (1 &lt;&lt; hugepd_shift(*(hugepd_t *)pud));</span>
<span class="p_add">+			if (more &gt; next)</span>
<span class="p_add">+				next = more;</span>
<span class="p_add">+</span>
 			free_hugepd_range(tlb, (hugepd_t *)pud, PUD_SHIFT,
 					  addr, next, floor, ceiling);
 		}
<span class="p_chunk">@@ -615,15 +593,17 @@</span> <span class="p_context"> void hugetlb_free_pgd_range(struct mmu_gather *tlb,</span>
 				continue;
 			hugetlb_free_pud_range(tlb, pgd, addr, next, floor, ceiling);
 		} else {
<span class="p_del">-#ifdef CONFIG_PPC_FSL_BOOK3E</span>
<span class="p_add">+			unsigned long more;</span>
 			/*
 			 * Increment next by the size of the huge mapping since
 			 * there may be more than one entry at the pgd level
 			 * for a single hugepage, but all of them point to the
 			 * same kmem cache that holds the hugepte.
 			 */
<span class="p_del">-			next = addr + (1 &lt;&lt; hugepd_shift(*(hugepd_t *)pgd));</span>
<span class="p_del">-#endif</span>
<span class="p_add">+			more = addr + (1 &lt;&lt; hugepd_shift(*(hugepd_t *)pgd));</span>
<span class="p_add">+			if (more &gt; next)</span>
<span class="p_add">+				next = more;</span>
<span class="p_add">+</span>
 			free_hugepd_range(tlb, (hugepd_t *)pgd, PGDIR_SHIFT,
 					  addr, next, floor, ceiling);
 		}
<span class="p_chunk">@@ -753,12 +733,13 @@</span> <span class="p_context"> static int __init add_huge_page_size(unsigned long long size)</span>
 
 	/* Check that it is a page size supported by the hardware and
 	 * that it fits within pagetable and slice limits. */
<span class="p_add">+	if (size &lt;= PAGE_SIZE)</span>
<span class="p_add">+		return -EINVAL;</span>
 #ifdef CONFIG_PPC_FSL_BOOK3E
<span class="p_del">-	if ((size &lt; PAGE_SIZE) || !is_power_of_4(size))</span>
<span class="p_add">+	if (!is_power_of_4(size))</span>
 		return -EINVAL;
 #else
<span class="p_del">-	if (!is_power_of_2(size)</span>
<span class="p_del">-	    || (shift &gt; SLICE_HIGH_SHIFT) || (shift &lt;= PAGE_SHIFT))</span>
<span class="p_add">+	if (!is_power_of_2(size) || (shift &gt; SLICE_HIGH_SHIFT))</span>
 		return -EINVAL;
 #endif
 
<span class="p_chunk">@@ -791,53 +772,15 @@</span> <span class="p_context"> static int __init hugepage_setup_sz(char *str)</span>
 }
 __setup(&quot;hugepagesz=&quot;, hugepage_setup_sz);
 
<span class="p_del">-#ifdef CONFIG_PPC_FSL_BOOK3E</span>
 struct kmem_cache *hugepte_cache;
 static int __init hugetlbpage_init(void)
 {
 	int psize;
 
<span class="p_del">-	for (psize = 0; psize &lt; MMU_PAGE_COUNT; ++psize) {</span>
<span class="p_del">-		unsigned shift;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (!mmu_psize_defs[psize].shift)</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-</span>
<span class="p_del">-		shift = mmu_psize_to_shift(psize);</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Don&#39;t treat normal page sizes as huge... */</span>
<span class="p_del">-		if (shift != PAGE_SHIFT)</span>
<span class="p_del">-			if (add_huge_page_size(1ULL &lt;&lt; shift) &lt; 0)</span>
<span class="p_del">-				continue;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Create a kmem cache for hugeptes.  The bottom bits in the pte have</span>
<span class="p_del">-	 * size information encoded in them, so align them to allow this</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	hugepte_cache =  kmem_cache_create(&quot;hugepte-cache&quot;, sizeof(pte_t),</span>
<span class="p_del">-					   HUGEPD_SHIFT_MASK + 1, 0, NULL);</span>
<span class="p_del">-	if (hugepte_cache == NULL)</span>
<span class="p_del">-		panic(&quot;%s: Unable to create kmem cache for hugeptes\n&quot;,</span>
<span class="p_del">-		      __func__);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Default hpage size = 4M */</span>
<span class="p_del">-	if (mmu_psize_defs[MMU_PAGE_4M].shift)</span>
<span class="p_del">-		HPAGE_SHIFT = mmu_psize_defs[MMU_PAGE_4M].shift;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		panic(&quot;%s: Unable to set default huge page size\n&quot;, __func__);</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-#else</span>
<span class="p_del">-static int __init hugetlbpage_init(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int psize;</span>
<span class="p_del">-</span>
<span class="p_add">+#if !defined(CONFIG_PPC_FSL_BOOK3E)</span>
 	if (!radix_enabled() &amp;&amp; !mmu_has_feature(MMU_FTR_16M_PAGE))
 		return -ENODEV;
<span class="p_del">-</span>
<span class="p_add">+#endif</span>
 	for (psize = 0; psize &lt; MMU_PAGE_COUNT; ++psize) {
 		unsigned shift;
 		unsigned pdshift;
<span class="p_chunk">@@ -860,16 +803,34 @@</span> <span class="p_context"> static int __init hugetlbpage_init(void)</span>
 		 * if we have pdshift and shift value same, we don&#39;t
 		 * use pgt cache for hugepd.
 		 */
<span class="p_del">-		if (pdshift != shift) {</span>
<span class="p_add">+		if (pdshift &gt; shift) {</span>
 			pgtable_cache_add(pdshift - shift, NULL);
 			if (!PGT_CACHE(pdshift - shift))
 				panic(&quot;hugetlbpage_init(): could not create &quot;
 				      &quot;pgtable cache for %d bit pagesize\n&quot;, shift);
 		}
<span class="p_add">+#ifdef CONFIG_PPC_FSL_BOOK3E</span>
<span class="p_add">+		else if (!hugepte_cache) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Create a kmem cache for hugeptes.  The bottom bits in</span>
<span class="p_add">+			 * the pte have size information encoded in them, so</span>
<span class="p_add">+			 * align them to allow this</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			hugepte_cache = kmem_cache_create(&quot;hugepte-cache&quot;,</span>
<span class="p_add">+							  sizeof(pte_t),</span>
<span class="p_add">+							  HUGEPD_SHIFT_MASK + 1,</span>
<span class="p_add">+							  0, NULL);</span>
<span class="p_add">+			if (hugepte_cache == NULL)</span>
<span class="p_add">+				panic(&quot;%s: Unable to create kmem cache &quot;</span>
<span class="p_add">+				      &quot;for hugeptes\n&quot;, __func__);</span>
<span class="p_add">+</span>
<span class="p_add">+		}</span>
<span class="p_add">+#endif</span>
 	}
 
 	/* Set default large page size. Currently, we pick 16M or 1M
 	 * depending on what is available
<span class="p_add">+	 * We select 4M on other ones.</span>
 	 */
 	if (mmu_psize_defs[MMU_PAGE_16M].shift)
 		HPAGE_SHIFT = mmu_psize_defs[MMU_PAGE_16M].shift;
<span class="p_chunk">@@ -877,11 +838,14 @@</span> <span class="p_context"> static int __init hugetlbpage_init(void)</span>
 		HPAGE_SHIFT = mmu_psize_defs[MMU_PAGE_1M].shift;
 	else if (mmu_psize_defs[MMU_PAGE_2M].shift)
 		HPAGE_SHIFT = mmu_psize_defs[MMU_PAGE_2M].shift;
<span class="p_del">-</span>
<span class="p_add">+	else if (mmu_psize_defs[MMU_PAGE_4M].shift)</span>
<span class="p_add">+		HPAGE_SHIFT = mmu_psize_defs[MMU_PAGE_4M].shift;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		panic(&quot;%s: Unable to set default huge page size\n&quot;, __func__);</span>
 
 	return 0;
 }
<span class="p_del">-#endif</span>
<span class="p_add">+</span>
 arch_initcall(hugetlbpage_init);
 
 void flush_dcache_icache_hugepage(struct page *page)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



