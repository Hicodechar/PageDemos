
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,1/3] x86/ldt: Make modify_ldt synchronous - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,1/3] x86/ldt: Make modify_ldt synchronous</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 21, 2015, 7:59 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;a000512a2baa2c4753971a7550747ca7cb5c62fa.1437508486.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6837951/mbox/"
   >mbox</a>
|
   <a href="/patch/6837951/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6837951/">/patch/6837951/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 77FF2C05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jul 2015 20:00:14 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 96B54206F6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jul 2015 20:00:12 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id C5348206F4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jul 2015 20:00:10 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755030AbbGUT7o (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 21 Jul 2015 15:59:44 -0400
Received: from mail.kernel.org ([198.145.29.136]:45909 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754777AbbGUT7j (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 21 Jul 2015 15:59:39 -0400
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 4E2B4206E9;
	Tue, 21 Jul 2015 19:59:37 +0000 (UTC)
Received: from localhost (199-83-221-254.PUBLIC.monkeybrains.net
	[199.83.221.254])
	(using TLSv1.2 with cipher AES128-GCM-SHA256 (128/128 bits))
	(No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id BA9B2206E1;
	Tue, 21 Jul 2015 19:59:35 +0000 (UTC)
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Steven Rostedt &lt;rostedt@goodmis.org&gt;
Cc: &quot;security@kernel.org&quot; &lt;security@kernel.org&gt;,
	X86 ML &lt;x86@kernel.org&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Sasha Levin &lt;sasha.levin@oracle.com&gt;, linux-kernel@vger.kernel.org,
	Konrad Rzeszutek Wilk &lt;konrad.wilk@oracle.com&gt;,
	Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;, stable@vger.kernel.org
Subject: [PATCH v2 1/3] x86/ldt: Make modify_ldt synchronous
Date: Tue, 21 Jul 2015 12:59:29 -0700
Message-Id: &lt;a000512a2baa2c4753971a7550747ca7cb5c62fa.1437508486.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.4.3
In-Reply-To: &lt;cover.1437508485.git.luto@kernel.org&gt;
References: &lt;cover.1437508485.git.luto@kernel.org&gt;
In-Reply-To: &lt;cover.1437508485.git.luto@kernel.org&gt;
References: &lt;cover.1437508485.git.luto@kernel.org&gt;
X-Spam-Status: No, score=-8.1 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - July 21, 2015, 7:59 p.m.</div>
<pre class="content">
modify_ldt has questionable locking and does not synchronize
threads.  Improve it: redesign the locking and synchronize all
threads&#39; LDTs using an IPI on all modifications.

This will dramatically slow down modify_ldt in multithreaded
programs, but there shouldn&#39;t be any multithreaded programs that
care about modify_ldt&#39;s performance in the first place.

Cc: stable@vger.kernel.org
<span class="signed-off-by">Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---
 arch/x86/include/asm/desc.h        |  15 ---
 arch/x86/include/asm/mmu.h         |   3 +-
 arch/x86/include/asm/mmu_context.h |  48 ++++++-
 arch/x86/kernel/cpu/common.c       |   4 +-
 arch/x86/kernel/cpu/perf_event.c   |  12 +-
 arch/x86/kernel/ldt.c              | 247 +++++++++++++++++++------------------
 arch/x86/kernel/process_64.c       |   4 +-
 arch/x86/kernel/step.c             |   6 +-
 arch/x86/power/cpu.c               |   3 +-
 9 files changed, 192 insertions(+), 150 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60131">Boris Ostrovsky</a> - July 21, 2015, 9:53 p.m.</div>
<pre class="content">
On 07/21/2015 03:59 PM, Andy Lutomirski wrote:
<span class="quote">&gt; modify_ldt has questionable locking and does not synchronize</span>
<span class="quote">&gt; threads.  Improve it: redesign the locking and synchronize all</span>
<span class="quote">&gt; threads&#39; LDTs using an IPI on all modifications.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This will dramatically slow down modify_ldt in multithreaded</span>
<span class="quote">&gt; programs, but there shouldn&#39;t be any multithreaded programs that</span>
<span class="quote">&gt; care about modify_ldt&#39;s performance in the first place.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Cc: stable@vger.kernel.org</span>
<span class="quote">&gt; Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;   arch/x86/include/asm/desc.h        |  15 ---</span>
<span class="quote">&gt;   arch/x86/include/asm/mmu.h         |   3 +-</span>
<span class="quote">&gt;   arch/x86/include/asm/mmu_context.h |  48 ++++++-</span>
<span class="quote">&gt;   arch/x86/kernel/cpu/common.c       |   4 +-</span>
<span class="quote">&gt;   arch/x86/kernel/cpu/perf_event.c   |  12 +-</span>
<span class="quote">&gt;   arch/x86/kernel/ldt.c              | 247 +++++++++++++++++++------------------</span>
<span class="quote">&gt;   arch/x86/kernel/process_64.c       |   4 +-</span>
<span class="quote">&gt;   arch/x86/kernel/step.c             |   6 +-</span>
<span class="quote">&gt;   arch/x86/power/cpu.c               |   3 +-</span>
<span class="quote">&gt;   9 files changed, 192 insertions(+), 150 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="quote">&gt; index a0bf89fd2647..4e10d73cf018 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/desc.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/desc.h</span>
<span class="quote">&gt; @@ -280,21 +280,6 @@ static inline void clear_LDT(void)</span>
<span class="quote">&gt;   	set_ldt(NULL, 0);</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * load one particular LDT into the current CPU</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -static inline void load_LDT_nolock(mm_context_t *pc)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	set_ldt(pc-&gt;ldt, pc-&gt;size);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void load_LDT(mm_context_t *pc)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	preempt_disable();</span>
<span class="quote">&gt; -	load_LDT_nolock(pc);</span>
<span class="quote">&gt; -	preempt_enable();</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;   static inline unsigned long get_desc_base(const struct desc_struct *desc)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt;   	return (unsigned)(desc-&gt;base0 | ((desc-&gt;base1) &lt;&lt; 16) | ((desc-&gt;base2) &lt;&lt; 24));</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/mmu.h b/arch/x86/include/asm/mmu.h</span>
<span class="quote">&gt; index 09b9620a73b4..364d27481a52 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/mmu.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/mmu.h</span>
<span class="quote">&gt; @@ -9,8 +9,7 @@</span>
<span class="quote">&gt;    * we put the segment information here.</span>
<span class="quote">&gt;    */</span>
<span class="quote">&gt;   typedef struct {</span>
<span class="quote">&gt; -	void *ldt;</span>
<span class="quote">&gt; -	int size;</span>
<span class="quote">&gt; +	struct ldt_struct *ldt;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   #ifdef CONFIG_X86_64</span>
<span class="quote">&gt;   	/* True if mm supports a task running in 32 bit compatibility mode. */</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; index 5e8daee7c5c9..1ff121fbf366 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; @@ -34,6 +34,44 @@ static inline void load_mm_cr4(struct mm_struct *mm) {}</span>
<span class="quote">&gt;   #endif</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   /*</span>
<span class="quote">&gt; + * ldt_structs can be allocated, used, and freed, but they are never</span>
<span class="quote">&gt; + * modified while live.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct ldt_struct {</span>
<span class="quote">&gt; +	int size;</span>
<span class="quote">&gt; +	int __pad;	/* keep the descriptors naturally aligned. */</span>
<span class="quote">&gt; +	struct desc_struct entries[];</span>
<span class="quote">&gt; +};</span>



This breaks Xen which expects LDT to be page-aligned. Not sure why.

Jan, Andrew?

-boris
<span class="quote">

&gt; +</span>
<span class="quote">&gt; +static inline void load_mm_ldt(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct ldt_struct *ldt;</span>
<span class="quote">&gt; +	DEBUG_LOCKS_WARN_ON(!irqs_disabled());</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* lockless_dereference synchronizes with smp_store_release */</span>
<span class="quote">&gt; +	ldt = lockless_dereference(mm-&gt;context.ldt);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Any change to mm-&gt;context.ldt is followed by an IPI to all</span>
<span class="quote">&gt; +	 * CPUs with the mm active.  The LDT will not be freed until</span>
<span class="quote">&gt; +	 * after the IPI is handled by all such CPUs.  This means that,</span>
<span class="quote">&gt; +	 * if the ldt_struct changes before we return, the values we see</span>
<span class="quote">&gt; +	 * will be safe, and the new values will be loaded before we run</span>
<span class="quote">&gt; +	 * any user code.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * NB: don&#39;t try to convert this to use RCU without extreme care.</span>
<span class="quote">&gt; +	 * We would still need IRQs off, because we don&#39;t want to change</span>
<span class="quote">&gt; +	 * the local LDT after an IPI loaded a newer value than the one</span>
<span class="quote">&gt; +	 * that we can see.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(ldt))</span>
<span class="quote">&gt; +		set_ldt(ldt-&gt;entries, ldt-&gt;size);</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		clear_LDT();</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;    * Used for LDT copy/destruction.</span>
<span class="quote">&gt;    */</span>
<span class="quote">&gt;   int init_new_context(struct task_struct *tsk, struct mm_struct *mm);</span>
<span class="quote">&gt; @@ -78,12 +116,12 @@ static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="quote">&gt;   		 * was called and then modify_ldt changed</span>
<span class="quote">&gt;   		 * prev-&gt;context.ldt but suppressed an IPI to this CPU.</span>
<span class="quote">&gt;   		 * In this case, prev-&gt;context.ldt != NULL, because we</span>
<span class="quote">&gt; -		 * never free an LDT while the mm still exists.  That</span>
<span class="quote">&gt; -		 * means that next-&gt;context.ldt != prev-&gt;context.ldt,</span>
<span class="quote">&gt; -		 * because mms never share an LDT.</span>
<span class="quote">&gt; +		 * never set context.ldt to NULL while the mm still</span>
<span class="quote">&gt; +		 * exists.  That means that next-&gt;context.ldt !=</span>
<span class="quote">&gt; +		 * prev-&gt;context.ldt, because mms never share an LDT.</span>
<span class="quote">&gt;   		 */</span>
<span class="quote">&gt;   		if (unlikely(prev-&gt;context.ldt != next-&gt;context.ldt))</span>
<span class="quote">&gt; -			load_LDT_nolock(&amp;next-&gt;context);</span>
<span class="quote">&gt; +			load_mm_ldt(next);</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt;   #ifdef CONFIG_SMP</span>
<span class="quote">&gt;   	  else {</span>
<span class="quote">&gt; @@ -106,7 +144,7 @@ static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="quote">&gt;   			load_cr3(next-&gt;pgd);</span>
<span class="quote">&gt;   			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="quote">&gt;   			load_mm_cr4(next);</span>
<span class="quote">&gt; -			load_LDT_nolock(&amp;next-&gt;context);</span>
<span class="quote">&gt; +			load_mm_ldt(next);</span>
<span class="quote">&gt;   		}</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt;   #endif</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="quote">&gt; index 922c5e0cea4c..cb9e5df42dd2 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/cpu/common.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/cpu/common.c</span>
<span class="quote">&gt; @@ -1410,7 +1410,7 @@ void cpu_init(void)</span>
<span class="quote">&gt;   	load_sp0(t, &amp;current-&gt;thread);</span>
<span class="quote">&gt;   	set_tss_desc(cpu, t);</span>
<span class="quote">&gt;   	load_TR_desc();</span>
<span class="quote">&gt; -	load_LDT(&amp;init_mm.context);</span>
<span class="quote">&gt; +	load_mm_ldt(&amp;init_mm);</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   	clear_all_debug_regs();</span>
<span class="quote">&gt;   	dbg_restore_debug_regs();</span>
<span class="quote">&gt; @@ -1459,7 +1459,7 @@ void cpu_init(void)</span>
<span class="quote">&gt;   	load_sp0(t, thread);</span>
<span class="quote">&gt;   	set_tss_desc(cpu, t);</span>
<span class="quote">&gt;   	load_TR_desc();</span>
<span class="quote">&gt; -	load_LDT(&amp;init_mm.context);</span>
<span class="quote">&gt; +	load_mm_ldt(&amp;init_mm);</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   	t-&gt;x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/cpu/perf_event.c b/arch/x86/kernel/cpu/perf_event.c</span>
<span class="quote">&gt; index 3658de47900f..9469dfa55607 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/cpu/perf_event.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/cpu/perf_event.c</span>
<span class="quote">&gt; @@ -2179,21 +2179,25 @@ static unsigned long get_segment_base(unsigned int segment)</span>
<span class="quote">&gt;   	int idx = segment &gt;&gt; 3;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   	if ((segment &amp; SEGMENT_TI_MASK) == SEGMENT_LDT) {</span>
<span class="quote">&gt; +		struct ldt_struct *ldt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   		if (idx &gt; LDT_ENTRIES)</span>
<span class="quote">&gt;   			return 0;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -		if (idx &gt; current-&gt;active_mm-&gt;context.size)</span>
<span class="quote">&gt; +		/* IRQs are off, so this synchronizes with smp_store_release */</span>
<span class="quote">&gt; +		ldt = lockless_dereference(current-&gt;active_mm-&gt;context.ldt);</span>
<span class="quote">&gt; +		if (!ldt || idx &gt; ldt-&gt;size)</span>
<span class="quote">&gt;   			return 0;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -		desc = current-&gt;active_mm-&gt;context.ldt;</span>
<span class="quote">&gt; +		desc = &amp;ldt-&gt;entries[idx];</span>
<span class="quote">&gt;   	} else {</span>
<span class="quote">&gt;   		if (idx &gt; GDT_ENTRIES)</span>
<span class="quote">&gt;   			return 0;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -		desc = raw_cpu_ptr(gdt_page.gdt);</span>
<span class="quote">&gt; +		desc = raw_cpu_ptr(gdt_page.gdt) + idx;</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	return get_desc_base(desc + idx);</span>
<span class="quote">&gt; +	return get_desc_base(desc);</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   #ifdef CONFIG_COMPAT</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="quote">&gt; index c37886d759cc..d65e6ec90338 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/ldt.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/ldt.c</span>
<span class="quote">&gt; @@ -20,82 +20,70 @@</span>
<span class="quote">&gt;   #include &lt;asm/mmu_context.h&gt;</span>
<span class="quote">&gt;   #include &lt;asm/syscalls.h&gt;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -#ifdef CONFIG_SMP</span>
<span class="quote">&gt;   static void flush_ldt(void *current_mm)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; -	if (current-&gt;active_mm == current_mm)</span>
<span class="quote">&gt; -		load_LDT(&amp;current-&gt;active_mm-&gt;context);</span>
<span class="quote">&gt; +	if (current-&gt;active_mm == current_mm) {</span>
<span class="quote">&gt; +		/* context.lock is held for us, so we don&#39;t need any locking. */</span>
<span class="quote">&gt; +		mm_context_t *pc = &amp;current-&gt;active_mm-&gt;context;</span>
<span class="quote">&gt; +		set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;size);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -static int alloc_ldt(mm_context_t *pc, int mincount, int reload)</span>
<span class="quote">&gt; +static struct ldt_struct *alloc_ldt_struct(int size)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; -	void *oldldt, *newldt;</span>
<span class="quote">&gt; -	int oldsize;</span>
<span class="quote">&gt; +	struct ldt_struct *new_ldt;</span>
<span class="quote">&gt; +	int alloc_size;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	if (mincount &lt;= pc-&gt;size)</span>
<span class="quote">&gt; -		return 0;</span>
<span class="quote">&gt; -	oldsize = pc-&gt;size;</span>
<span class="quote">&gt; -	mincount = (mincount + (PAGE_SIZE / LDT_ENTRY_SIZE - 1)) &amp;</span>
<span class="quote">&gt; -			(~(PAGE_SIZE / LDT_ENTRY_SIZE - 1));</span>
<span class="quote">&gt; -	if (mincount * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="quote">&gt; -		newldt = vmalloc(mincount * LDT_ENTRY_SIZE);</span>
<span class="quote">&gt; +	if (size &gt; LDT_ENTRIES)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	BUILD_BUG_ON(LDT_ENTRY_SIZE != sizeof(struct desc_struct));</span>
<span class="quote">&gt; +	alloc_size = sizeof(struct ldt_struct) + size * LDT_ENTRY_SIZE;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (alloc_size &gt; PAGE_SIZE)</span>
<span class="quote">&gt; +		new_ldt = vmalloc(alloc_size);</span>
<span class="quote">&gt;   	else</span>
<span class="quote">&gt; -		newldt = (void *)__get_free_page(GFP_KERNEL);</span>
<span class="quote">&gt; +		new_ldt = (void *)__get_free_page(GFP_KERNEL);</span>
<span class="quote">&gt; +	if (!new_ldt)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	if (!newldt)</span>
<span class="quote">&gt; -		return -ENOMEM;</span>
<span class="quote">&gt; +	new_ldt-&gt;size = size;</span>
<span class="quote">&gt; +	paravirt_alloc_ldt(new_ldt-&gt;entries, size);</span>
<span class="quote">&gt; +	return new_ldt;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	if (oldsize)</span>
<span class="quote">&gt; -		memcpy(newldt, pc-&gt;ldt, oldsize * LDT_ENTRY_SIZE);</span>
<span class="quote">&gt; -	oldldt = pc-&gt;ldt;</span>
<span class="quote">&gt; -	memset(newldt + oldsize * LDT_ENTRY_SIZE, 0,</span>
<span class="quote">&gt; -	       (mincount - oldsize) * LDT_ENTRY_SIZE);</span>
<span class="quote">&gt; +static void install_ldt(struct mm_struct *current_mm,</span>
<span class="quote">&gt; +			struct ldt_struct *ldt)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/* context.lock is held */</span>
<span class="quote">&gt; +	preempt_disable();</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	paravirt_alloc_ldt(newldt, mincount);</span>
<span class="quote">&gt; +	/* Synchronizes with lockless_dereference in load_mm_ldt. */</span>
<span class="quote">&gt; +	smp_store_release(&amp;current_mm-&gt;context.ldt, ldt);</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -#ifdef CONFIG_X86_64</span>
<span class="quote">&gt; -	/* CHECKME: Do we really need this ? */</span>
<span class="quote">&gt; -	wmb();</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -	pc-&gt;ldt = newldt;</span>
<span class="quote">&gt; -	wmb();</span>
<span class="quote">&gt; -	pc-&gt;size = mincount;</span>
<span class="quote">&gt; -	wmb();</span>
<span class="quote">&gt; +	/* Activate for this CPU. */</span>
<span class="quote">&gt; +	flush_ldt(current-&gt;mm);</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	if (reload) {</span>
<span class="quote">&gt;   #ifdef CONFIG_SMP</span>
<span class="quote">&gt; -		preempt_disable();</span>
<span class="quote">&gt; -		load_LDT(pc);</span>
<span class="quote">&gt; -		if (!cpumask_equal(mm_cpumask(current-&gt;mm),</span>
<span class="quote">&gt; -				   cpumask_of(smp_processor_id())))</span>
<span class="quote">&gt; -			smp_call_function(flush_ldt, current-&gt;mm, 1);</span>
<span class="quote">&gt; -		preempt_enable();</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -		load_LDT(pc);</span>
<span class="quote">&gt; +	/* Synchronize with other CPUs. */</span>
<span class="quote">&gt; +	if (!cpumask_equal(mm_cpumask(current_mm),</span>
<span class="quote">&gt; +			   cpumask_of(smp_processor_id())))</span>
<span class="quote">&gt; +		smp_call_function(flush_ldt, current_mm, 1);</span>
<span class="quote">&gt;   #endif</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -	if (oldsize) {</span>
<span class="quote">&gt; -		paravirt_free_ldt(oldldt, oldsize);</span>
<span class="quote">&gt; -		if (oldsize * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="quote">&gt; -			vfree(oldldt);</span>
<span class="quote">&gt; -		else</span>
<span class="quote">&gt; -			put_page(virt_to_page(oldldt));</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -	return 0;</span>
<span class="quote">&gt; +	preempt_enable();</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -static inline int copy_ldt(mm_context_t *new, mm_context_t *old)</span>
<span class="quote">&gt; +static void free_ldt_struct(struct ldt_struct *ldt)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; -	int err = alloc_ldt(new, old-&gt;size, 0);</span>
<span class="quote">&gt; -	int i;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (err &lt; 0)</span>
<span class="quote">&gt; -		return err;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	for (i = 0; i &lt; old-&gt;size; i++)</span>
<span class="quote">&gt; -		write_ldt_entry(new-&gt;ldt, i, old-&gt;ldt + i * LDT_ENTRY_SIZE);</span>
<span class="quote">&gt; -	return 0;</span>
<span class="quote">&gt; +	if (unlikely(ldt)) {</span>
<span class="quote">&gt; +		int alloc_size = sizeof(struct ldt_struct) +</span>
<span class="quote">&gt; +			ldt-&gt;size * LDT_ENTRY_SIZE;</span>
<span class="quote">&gt; +		paravirt_free_ldt(ldt-&gt;entries, ldt-&gt;size);</span>
<span class="quote">&gt; +		if (alloc_size &gt; PAGE_SIZE)</span>
<span class="quote">&gt; +			vfree(ldt);</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +			put_page(virt_to_page(ldt));</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   /*</span>
<span class="quote">&gt; @@ -106,15 +94,35 @@ int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt;   	struct mm_struct *old_mm;</span>
<span class="quote">&gt;   	int retval = 0;</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   	mutex_init(&amp;mm-&gt;context.lock);</span>
<span class="quote">&gt; -	mm-&gt;context.size = 0;</span>
<span class="quote">&gt;   	old_mm = current-&gt;mm;</span>
<span class="quote">&gt; -	if (old_mm &amp;&amp; old_mm-&gt;context.size &gt; 0) {</span>
<span class="quote">&gt; -		mutex_lock(&amp;old_mm-&gt;context.lock);</span>
<span class="quote">&gt; -		retval = copy_ldt(&amp;mm-&gt;context, &amp;old_mm-&gt;context);</span>
<span class="quote">&gt; -		mutex_unlock(&amp;old_mm-&gt;context.lock);</span>
<span class="quote">&gt; +	if (!old_mm) {</span>
<span class="quote">&gt; +		mm-&gt;context.ldt = NULL;</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mutex_lock(&amp;old_mm-&gt;context.lock);</span>
<span class="quote">&gt; +	if (old_mm-&gt;context.ldt) {</span>
<span class="quote">&gt; +		struct ldt_struct *new_ldt =</span>
<span class="quote">&gt; +			alloc_ldt_struct(old_mm-&gt;context.ldt-&gt;size);</span>
<span class="quote">&gt; +		if (!new_ldt) {</span>
<span class="quote">&gt; +			retval = -ENOMEM;</span>
<span class="quote">&gt; +			goto out_unlock;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		for (i = 0; i &lt; old_mm-&gt;context.ldt-&gt;size; i++)</span>
<span class="quote">&gt; +			write_ldt_entry(new_ldt-&gt;entries, i,</span>
<span class="quote">&gt; +					&amp;old_mm-&gt;context.ldt-&gt;entries[i]);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		mm-&gt;context.ldt = new_ldt;</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		mm-&gt;context.ldt = NULL;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +out_unlock:</span>
<span class="quote">&gt; +	mutex_unlock(&amp;old_mm-&gt;context.lock);</span>
<span class="quote">&gt;   	return retval;</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; @@ -125,53 +133,47 @@ int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;    */</span>
<span class="quote">&gt;   void destroy_context(struct mm_struct *mm)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; -	if (mm-&gt;context.size) {</span>
<span class="quote">&gt; -#ifdef CONFIG_X86_32</span>
<span class="quote">&gt; -		/* CHECKME: Can this ever happen ? */</span>
<span class="quote">&gt; -		if (mm == current-&gt;active_mm)</span>
<span class="quote">&gt; -			clear_LDT();</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -		paravirt_free_ldt(mm-&gt;context.ldt, mm-&gt;context.size);</span>
<span class="quote">&gt; -		if (mm-&gt;context.size * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="quote">&gt; -			vfree(mm-&gt;context.ldt);</span>
<span class="quote">&gt; -		else</span>
<span class="quote">&gt; -			put_page(virt_to_page(mm-&gt;context.ldt));</span>
<span class="quote">&gt; -		mm-&gt;context.size = 0;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +	free_ldt_struct(mm-&gt;context.ldt);</span>
<span class="quote">&gt; +	mm-&gt;context.ldt = NULL;</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   static int read_ldt(void __user *ptr, unsigned long bytecount)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; -	int err;</span>
<span class="quote">&gt; +	int retval;</span>
<span class="quote">&gt;   	unsigned long size;</span>
<span class="quote">&gt;   	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	if (!mm-&gt;context.size)</span>
<span class="quote">&gt; -		return 0;</span>
<span class="quote">&gt; +	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!mm-&gt;context.ldt) {</span>
<span class="quote">&gt; +		retval = 0;</span>
<span class="quote">&gt; +		goto out_unlock;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   	if (bytecount &gt; LDT_ENTRY_SIZE * LDT_ENTRIES)</span>
<span class="quote">&gt;   		bytecount = LDT_ENTRY_SIZE * LDT_ENTRIES;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="quote">&gt; -	size = mm-&gt;context.size * LDT_ENTRY_SIZE;</span>
<span class="quote">&gt; +	size = mm-&gt;context.ldt-&gt;size * LDT_ENTRY_SIZE;</span>
<span class="quote">&gt;   	if (size &gt; bytecount)</span>
<span class="quote">&gt;   		size = bytecount;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	err = 0;</span>
<span class="quote">&gt; -	if (copy_to_user(ptr, mm-&gt;context.ldt, size))</span>
<span class="quote">&gt; -		err = -EFAULT;</span>
<span class="quote">&gt; -	mutex_unlock(&amp;mm-&gt;context.lock);</span>
<span class="quote">&gt; -	if (err &lt; 0)</span>
<span class="quote">&gt; -		goto error_return;</span>
<span class="quote">&gt; +	if (copy_to_user(ptr, mm-&gt;context.ldt-&gt;entries, size)) {</span>
<span class="quote">&gt; +		retval = -EFAULT;</span>
<span class="quote">&gt; +		goto out_unlock;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   	if (size != bytecount) {</span>
<span class="quote">&gt; -		/* zero-fill the rest */</span>
<span class="quote">&gt; +		/* Zero-fill the rest and pretend we read bytecount bytes. */</span>
<span class="quote">&gt;   		if (clear_user(ptr + size, bytecount - size) != 0) {</span>
<span class="quote">&gt; -			err = -EFAULT;</span>
<span class="quote">&gt; -			goto error_return;</span>
<span class="quote">&gt; +			retval = -EFAULT;</span>
<span class="quote">&gt; +			goto out_unlock;</span>
<span class="quote">&gt;   		}</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt; -	return bytecount;</span>
<span class="quote">&gt; -error_return:</span>
<span class="quote">&gt; -	return err;</span>
<span class="quote">&gt; +	retval = bytecount;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +out_unlock:</span>
<span class="quote">&gt; +	mutex_unlock(&amp;mm-&gt;context.lock);</span>
<span class="quote">&gt; +	return retval;</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   static int read_default_ldt(void __user *ptr, unsigned long bytecount)</span>
<span class="quote">&gt; @@ -189,12 +191,16 @@ static int read_default_ldt(void __user *ptr, unsigned long bytecount)</span>
<span class="quote">&gt;   	return bytecount;</span>
<span class="quote">&gt;   }</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; +static struct desc_struct blank_desc;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;   static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt;   	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;   	struct desc_struct ldt;</span>
<span class="quote">&gt; -	int error;</span>
<span class="quote">&gt; +	int error, i;</span>
<span class="quote">&gt;   	struct user_desc ldt_info;</span>
<span class="quote">&gt; +	int oldsize, newsize;</span>
<span class="quote">&gt; +	struct ldt_struct *new_ldt, *old_ldt;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   	error = -EINVAL;</span>
<span class="quote">&gt;   	if (bytecount != sizeof(ldt_info))</span>
<span class="quote">&gt; @@ -213,34 +219,41 @@ static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
<span class="quote">&gt;   			goto out;</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="quote">&gt; -	if (ldt_info.entry_number &gt;= mm-&gt;context.size) {</span>
<span class="quote">&gt; -		error = alloc_ldt(&amp;current-&gt;mm-&gt;context,</span>
<span class="quote">&gt; -				  ldt_info.entry_number + 1, 1);</span>
<span class="quote">&gt; -		if (error &lt; 0)</span>
<span class="quote">&gt; -			goto out_unlock;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* Allow LDTs to be cleared by the user. */</span>
<span class="quote">&gt; -	if (ldt_info.base_addr == 0 &amp;&amp; ldt_info.limit == 0) {</span>
<span class="quote">&gt; -		if (oldmode || LDT_empty(&amp;ldt_info)) {</span>
<span class="quote">&gt; -			memset(&amp;ldt, 0, sizeof(ldt));</span>
<span class="quote">&gt; -			goto install;</span>
<span class="quote">&gt; +	if ((oldmode &amp;&amp; ldt_info.base_addr == 0 &amp;&amp; ldt_info.limit == 0) ||</span>
<span class="quote">&gt; +	    LDT_empty(&amp;ldt_info)) {</span>
<span class="quote">&gt; +		/* The user wants to clear the entry. */</span>
<span class="quote">&gt; +		memset(&amp;ldt, 0, sizeof(ldt));</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		if (!IS_ENABLED(CONFIG_X86_16BIT) &amp;&amp; !ldt_info.seg_32bit) {</span>
<span class="quote">&gt; +			error = -EINVAL;</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt;   		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		fill_ldt(&amp;ldt, &amp;ldt_info);</span>
<span class="quote">&gt; +		if (oldmode)</span>
<span class="quote">&gt; +			ldt.avl = 0;</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	if (!IS_ENABLED(CONFIG_X86_16BIT) &amp;&amp; !ldt_info.seg_32bit) {</span>
<span class="quote">&gt; -		error = -EINVAL;</span>
<span class="quote">&gt; +	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	old_ldt = mm-&gt;context.ldt;</span>
<span class="quote">&gt; +	oldsize = old_ldt ? old_ldt-&gt;size : 0;</span>
<span class="quote">&gt; +	newsize = max((int)(ldt_info.entry_number + 1), oldsize);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	error = -ENOMEM;</span>
<span class="quote">&gt; +	new_ldt = alloc_ldt_struct(newsize);</span>
<span class="quote">&gt; +	if (!new_ldt)</span>
<span class="quote">&gt;   		goto out_unlock;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	fill_ldt(&amp;ldt, &amp;ldt_info);</span>
<span class="quote">&gt; -	if (oldmode)</span>
<span class="quote">&gt; -		ldt.avl = 0;</span>
<span class="quote">&gt; +	for (i = 0; i &lt; oldsize; i++)</span>
<span class="quote">&gt; +		write_ldt_entry(new_ldt-&gt;entries, i,</span>
<span class="quote">&gt; +				&amp;old_ldt-&gt;entries[i]);</span>
<span class="quote">&gt; +	for (i = oldsize; i &lt; newsize; i++)</span>
<span class="quote">&gt; +		write_ldt_entry(new_ldt-&gt;entries, i, &amp;blank_desc);</span>
<span class="quote">&gt; +	write_ldt_entry(new_ldt-&gt;entries, ldt_info.entry_number, &amp;ldt);</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt; -	/* Install the new entry ...  */</span>
<span class="quote">&gt; -install:</span>
<span class="quote">&gt; -	write_ldt_entry(mm-&gt;context.ldt, ldt_info.entry_number, &amp;ldt);</span>
<span class="quote">&gt; +	install_ldt(mm, new_ldt);</span>
<span class="quote">&gt; +	free_ldt_struct(old_ldt);</span>
<span class="quote">&gt;   	error = 0;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   out_unlock:</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="quote">&gt; index 71d7849a07f7..f6b916387590 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/process_64.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/process_64.c</span>
<span class="quote">&gt; @@ -121,11 +121,11 @@ void __show_regs(struct pt_regs *regs, int all)</span>
<span class="quote">&gt;   void release_thread(struct task_struct *dead_task)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt;   	if (dead_task-&gt;mm) {</span>
<span class="quote">&gt; -		if (dead_task-&gt;mm-&gt;context.size) {</span>
<span class="quote">&gt; +		if (dead_task-&gt;mm-&gt;context.ldt) {</span>
<span class="quote">&gt;   			pr_warn(&quot;WARNING: dead process %s still has LDT? &lt;%p/%d&gt;\n&quot;,</span>
<span class="quote">&gt;   				dead_task-&gt;comm,</span>
<span class="quote">&gt;   				dead_task-&gt;mm-&gt;context.ldt,</span>
<span class="quote">&gt; -				dead_task-&gt;mm-&gt;context.size);</span>
<span class="quote">&gt; +				dead_task-&gt;mm-&gt;context.ldt-&gt;size);</span>
<span class="quote">&gt;   			BUG();</span>
<span class="quote">&gt;   		}</span>
<span class="quote">&gt;   	}</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/step.c b/arch/x86/kernel/step.c</span>
<span class="quote">&gt; index 9b4d51d0c0d0..6273324186ac 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/step.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/step.c</span>
<span class="quote">&gt; @@ -5,6 +5,7 @@</span>
<span class="quote">&gt;   #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;   #include &lt;linux/ptrace.h&gt;</span>
<span class="quote">&gt;   #include &lt;asm/desc.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mmu_context.h&gt;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   unsigned long convert_ip_to_linear(struct task_struct *child, struct pt_regs *regs)</span>
<span class="quote">&gt;   {</span>
<span class="quote">&gt; @@ -30,10 +31,11 @@ unsigned long convert_ip_to_linear(struct task_struct *child, struct pt_regs *re</span>
<span class="quote">&gt;   		seg &amp;= ~7UL;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   		mutex_lock(&amp;child-&gt;mm-&gt;context.lock);</span>
<span class="quote">&gt; -		if (unlikely((seg &gt;&gt; 3) &gt;= child-&gt;mm-&gt;context.size))</span>
<span class="quote">&gt; +		if (unlikely(!child-&gt;mm-&gt;context.ldt ||</span>
<span class="quote">&gt; +			     (seg &gt;&gt; 3) &gt;= child-&gt;mm-&gt;context.ldt-&gt;size))</span>
<span class="quote">&gt;   			addr = -1L; /* bogus selector, access would fault */</span>
<span class="quote">&gt;   		else {</span>
<span class="quote">&gt; -			desc = child-&gt;mm-&gt;context.ldt + seg;</span>
<span class="quote">&gt; +			desc = &amp;child-&gt;mm-&gt;context.ldt-&gt;entries[seg];</span>
<span class="quote">&gt;   			base = get_desc_base(desc);</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   			/* 16-bit code segment? */</span>
<span class="quote">&gt; diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c</span>
<span class="quote">&gt; index 0d7dd1f5ac36..9ab52791fed5 100644</span>
<span class="quote">&gt; --- a/arch/x86/power/cpu.c</span>
<span class="quote">&gt; +++ b/arch/x86/power/cpu.c</span>
<span class="quote">&gt; @@ -22,6 +22,7 @@</span>
<span class="quote">&gt;   #include &lt;asm/fpu/internal.h&gt;</span>
<span class="quote">&gt;   #include &lt;asm/debugreg.h&gt;</span>
<span class="quote">&gt;   #include &lt;asm/cpu.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mmu_context.h&gt;</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   #ifdef CONFIG_X86_32</span>
<span class="quote">&gt;   __visible unsigned long saved_context_ebx;</span>
<span class="quote">&gt; @@ -153,7 +154,7 @@ static void fix_processor_context(void)</span>
<span class="quote">&gt;   	syscall_init();				/* This sets MSR_*STAR and related */</span>
<span class="quote">&gt;   #endif</span>
<span class="quote">&gt;   	load_TR_desc();				/* This does ltr */</span>
<span class="quote">&gt; -	load_LDT(&amp;current-&gt;active_mm-&gt;context);	/* This does lldt */</span>
<span class="quote">&gt; +	load_mm_ldt(current-&gt;active_mm);	/* This does lldt */</span>
<span class="quote">&gt;   </span>
<span class="quote">&gt;   	fpu__resume_cpu();</span>
<span class="quote">&gt;   }</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=53881">Andrew Cooper</a> - July 21, 2015, 11:38 p.m.</div>
<pre class="content">
On 21/07/2015 22:53, Boris Ostrovsky wrote:
<span class="quote">&gt; On 07/21/2015 03:59 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt; @@ -34,6 +34,44 @@ static inline void load_mm_cr4(struct mm_struct</span>
<span class="quote">&gt;&gt; *mm) {}</span>
<span class="quote">&gt;&gt;   #endif</span>
<span class="quote">&gt;&gt;     /*</span>
<span class="quote">&gt;&gt; + * ldt_structs can be allocated, used, and freed, but they are never</span>
<span class="quote">&gt;&gt; + * modified while live.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +struct ldt_struct {</span>
<span class="quote">&gt;&gt; +    int size;</span>
<span class="quote">&gt;&gt; +    int __pad;    /* keep the descriptors naturally aligned. */</span>
<span class="quote">&gt;&gt; +    struct desc_struct entries[];</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This breaks Xen which expects LDT to be page-aligned. Not sure why.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Jan, Andrew?</span>

PV guests are not permitted to have writeable mappings to the frames
making up the GDT and LDT, so it cannot make unaudited changes to
loadable descriptors.  In particular, for a 32bit PV guest, it is only
the segment limit which protects Xen from the ring1 guest kernel.

A lot of this code hasn&#39;t been touched in years, and it certainly
predates me.  The alignment requirement appears to come from the virtual
region Xen uses to map the guests GDT and LDT.  Strict alignment is
required for the GDT so Xen&#39;s descriptors starting at 0xe0xx are
correct, but the LDT alignment seems to be a side effect of similar
codepaths.

For an LDT smaller than 8192 entries, I can&#39;t see any specific reason
for enforcing alignment, other than &quot;that&#39;s the way it has always been&quot;.

However, the guest would still have to relinquish write access to all
frames which make up the LDT, which looks to be a bit of an issue given
the snippet above.

I think I have a solution, but I doubt it is going to be very popular.

* Make a new paravirt hook for allocation of ldt_struct, so the paravirt
backend can choose an alignment if needed
* Make absolutely certain that __pad has the value 0 (so size and __pad
combined don&#39;t look like a present descriptor)
* Never hand selector 0x0008 to unsuspecting users.

This will allow ldt_struct itself to be page aligned, and for the size
field to sit across the base/limit field of what would logically be
selector 0x0008  There would be some issues accessing size.  To load
frames as an LDT, a guest must drop all refs to the page so that its
type may be changed from writeable to segdesc.  After that, an
update_descriptor hypercall can be used to change size, and I believe
the guest may subsequently recreate read-only mappings to the frames in
question (although frankly it is getting late so you will want to double
check all of this).

Anyhow, this looks like an issue which should be fixed up with slightly
more PVOps, rather than enforcing a Xen view of the world on native Linux.

~Andrew
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - July 22, 2015, 12:07 a.m.</div>
<pre class="content">
On Tue, Jul 21, 2015 at 4:38 PM, Andrew Cooper
&lt;andrew.cooper3@citrix.com&gt; wrote:
<span class="quote">&gt; On 21/07/2015 22:53, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt; On 07/21/2015 03:59 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt;&gt; @@ -34,6 +34,44 @@ static inline void load_mm_cr4(struct mm_struct</span>
<span class="quote">&gt;&gt;&gt; *mm) {}</span>
<span class="quote">&gt;&gt;&gt;   #endif</span>
<span class="quote">&gt;&gt;&gt;     /*</span>
<span class="quote">&gt;&gt;&gt; + * ldt_structs can be allocated, used, and freed, but they are never</span>
<span class="quote">&gt;&gt;&gt; + * modified while live.</span>
<span class="quote">&gt;&gt;&gt; + */</span>
<span class="quote">&gt;&gt;&gt; +struct ldt_struct {</span>
<span class="quote">&gt;&gt;&gt; +    int size;</span>
<span class="quote">&gt;&gt;&gt; +    int __pad;    /* keep the descriptors naturally aligned. */</span>
<span class="quote">&gt;&gt;&gt; +    struct desc_struct entries[];</span>
<span class="quote">&gt;&gt;&gt; +};</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This breaks Xen which expects LDT to be page-aligned. Not sure why.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Jan, Andrew?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; PV guests are not permitted to have writeable mappings to the frames</span>
<span class="quote">&gt; making up the GDT and LDT, so it cannot make unaudited changes to</span>
<span class="quote">&gt; loadable descriptors.  In particular, for a 32bit PV guest, it is only</span>
<span class="quote">&gt; the segment limit which protects Xen from the ring1 guest kernel.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; A lot of this code hasn&#39;t been touched in years, and it certainly</span>
<span class="quote">&gt; predates me.  The alignment requirement appears to come from the virtual</span>
<span class="quote">&gt; region Xen uses to map the guests GDT and LDT.  Strict alignment is</span>
<span class="quote">&gt; required for the GDT so Xen&#39;s descriptors starting at 0xe0xx are</span>
<span class="quote">&gt; correct, but the LDT alignment seems to be a side effect of similar</span>
<span class="quote">&gt; codepaths.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; For an LDT smaller than 8192 entries, I can&#39;t see any specific reason</span>
<span class="quote">&gt; for enforcing alignment, other than &quot;that&#39;s the way it has always been&quot;.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; However, the guest would still have to relinquish write access to all</span>
<span class="quote">&gt; frames which make up the LDT, which looks to be a bit of an issue given</span>
<span class="quote">&gt; the snippet above.</span>

Does the LDT itself need to be aligned or just the address passed to
paravirt_alloc_ldt?
<span class="quote">
&gt;</span>
<span class="quote">&gt; I think I have a solution, but I doubt it is going to be very popular.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; * Make a new paravirt hook for allocation of ldt_struct, so the paravirt</span>
<span class="quote">&gt; backend can choose an alignment if needed</span>
<span class="quote">&gt; * Make absolutely certain that __pad has the value 0 (so size and __pad</span>
<span class="quote">&gt; combined don&#39;t look like a present descriptor)</span>
<span class="quote">&gt; * Never hand selector 0x0008 to unsuspecting users.</span>

Yuck.
<span class="quote">
&gt;</span>
<span class="quote">&gt; This will allow ldt_struct itself to be page aligned, and for the size</span>
<span class="quote">&gt; field to sit across the base/limit field of what would logically be</span>
<span class="quote">&gt; selector 0x0008  There would be some issues accessing size.  To load</span>
<span class="quote">&gt; frames as an LDT, a guest must drop all refs to the page so that its</span>
<span class="quote">&gt; type may be changed from writeable to segdesc.  After that, an</span>
<span class="quote">&gt; update_descriptor hypercall can be used to change size, and I believe</span>
<span class="quote">&gt; the guest may subsequently recreate read-only mappings to the frames in</span>
<span class="quote">&gt; question (although frankly it is getting late so you will want to double</span>
<span class="quote">&gt; check all of this).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Anyhow, this looks like an issue which should be fixed up with slightly</span>
<span class="quote">&gt; more PVOps, rather than enforcing a Xen view of the world on native Linux.</span>
<span class="quote">&gt;</span>

I could presumably make the allocation the other way around so the
size is at the end.  I could even use two separate allocations if
needed.

--Andy
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=53881">Andrew Cooper</a> - July 22, 2015, 12:21 a.m.</div>
<pre class="content">
On 22/07/2015 01:07, Andy Lutomirski wrote:
<span class="quote">&gt; On Tue, Jul 21, 2015 at 4:38 PM, Andrew Cooper</span>
<span class="quote">&gt; &lt;andrew.cooper3@citrix.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On 21/07/2015 22:53, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt;&gt; On 07/21/2015 03:59 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt;&gt;&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -34,6 +34,44 @@ static inline void load_mm_cr4(struct mm_struct</span>
<span class="quote">&gt;&gt;&gt;&gt; *mm) {}</span>
<span class="quote">&gt;&gt;&gt;&gt;   #endif</span>
<span class="quote">&gt;&gt;&gt;&gt;     /*</span>
<span class="quote">&gt;&gt;&gt;&gt; + * ldt_structs can be allocated, used, and freed, but they are never</span>
<span class="quote">&gt;&gt;&gt;&gt; + * modified while live.</span>
<span class="quote">&gt;&gt;&gt;&gt; + */</span>
<span class="quote">&gt;&gt;&gt;&gt; +struct ldt_struct {</span>
<span class="quote">&gt;&gt;&gt;&gt; +    int size;</span>
<span class="quote">&gt;&gt;&gt;&gt; +    int __pad;    /* keep the descriptors naturally aligned. */</span>
<span class="quote">&gt;&gt;&gt;&gt; +    struct desc_struct entries[];</span>
<span class="quote">&gt;&gt;&gt;&gt; +};</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This breaks Xen which expects LDT to be page-aligned. Not sure why.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Jan, Andrew?</span>
<span class="quote">&gt;&gt; PV guests are not permitted to have writeable mappings to the frames</span>
<span class="quote">&gt;&gt; making up the GDT and LDT, so it cannot make unaudited changes to</span>
<span class="quote">&gt;&gt; loadable descriptors.  In particular, for a 32bit PV guest, it is only</span>
<span class="quote">&gt;&gt; the segment limit which protects Xen from the ring1 guest kernel.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; A lot of this code hasn&#39;t been touched in years, and it certainly</span>
<span class="quote">&gt;&gt; predates me.  The alignment requirement appears to come from the virtual</span>
<span class="quote">&gt;&gt; region Xen uses to map the guests GDT and LDT.  Strict alignment is</span>
<span class="quote">&gt;&gt; required for the GDT so Xen&#39;s descriptors starting at 0xe0xx are</span>
<span class="quote">&gt;&gt; correct, but the LDT alignment seems to be a side effect of similar</span>
<span class="quote">&gt;&gt; codepaths.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; For an LDT smaller than 8192 entries, I can&#39;t see any specific reason</span>
<span class="quote">&gt;&gt; for enforcing alignment, other than &quot;that&#39;s the way it has always been&quot;.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; However, the guest would still have to relinquish write access to all</span>
<span class="quote">&gt;&gt; frames which make up the LDT, which looks to be a bit of an issue given</span>
<span class="quote">&gt;&gt; the snippet above.</span>
<span class="quote">&gt; Does the LDT itself need to be aligned or just the address passed to</span>
<span class="quote">&gt; paravirt_alloc_ldt?</span>

The address which Xen receives needs to be aligned.

It looks like xen_alloc_ldt() blindly assumes that the desc_struct *ldt
it is passed is page aligned, and passes it straight through.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; I think I have a solution, but I doubt it is going to be very popular.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; * Make a new paravirt hook for allocation of ldt_struct, so the paravirt</span>
<span class="quote">&gt;&gt; backend can choose an alignment if needed</span>
<span class="quote">&gt;&gt; * Make absolutely certain that __pad has the value 0 (so size and __pad</span>
<span class="quote">&gt;&gt; combined don&#39;t look like a present descriptor)</span>
<span class="quote">&gt;&gt; * Never hand selector 0x0008 to unsuspecting users.</span>
<span class="quote">&gt; Yuck.</span>

I actually meant 0x0004, but yes.  Very much yuck.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; This will allow ldt_struct itself to be page aligned, and for the size</span>
<span class="quote">&gt;&gt; field to sit across the base/limit field of what would logically be</span>
<span class="quote">&gt;&gt; selector 0x0008  There would be some issues accessing size.  To load</span>
<span class="quote">&gt;&gt; frames as an LDT, a guest must drop all refs to the page so that its</span>
<span class="quote">&gt;&gt; type may be changed from writeable to segdesc.  After that, an</span>
<span class="quote">&gt;&gt; update_descriptor hypercall can be used to change size, and I believe</span>
<span class="quote">&gt;&gt; the guest may subsequently recreate read-only mappings to the frames in</span>
<span class="quote">&gt;&gt; question (although frankly it is getting late so you will want to double</span>
<span class="quote">&gt;&gt; check all of this).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Anyhow, this looks like an issue which should be fixed up with slightly</span>
<span class="quote">&gt;&gt; more PVOps, rather than enforcing a Xen view of the world on native Linux.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; I could presumably make the allocation the other way around so the</span>
<span class="quote">&gt; size is at the end.  I could even use two separate allocations if</span>
<span class="quote">&gt; needed.</span>

I suspect two separate allocations would be the better solution, as it
means that the size field doesn&#39;t need to be subject to funny page
permissions.

~Andrew
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - July 22, 2015, 12:28 a.m.</div>
<pre class="content">
On Tue, Jul 21, 2015 at 5:21 PM, Andrew Cooper
&lt;andrew.cooper3@citrix.com&gt; wrote:
<span class="quote">&gt; On 22/07/2015 01:07, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; On Tue, Jul 21, 2015 at 4:38 PM, Andrew Cooper</span>
<span class="quote">&gt;&gt; &lt;andrew.cooper3@citrix.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; On 21/07/2015 22:53, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On 07/21/2015 03:59 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; @@ -34,6 +34,44 @@ static inline void load_mm_cr4(struct mm_struct</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; *mm) {}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;   #endif</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;     /*</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; + * ldt_structs can be allocated, used, and freed, but they are never</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; + * modified while live.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; + */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +struct ldt_struct {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +    int size;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +    int __pad;    /* keep the descriptors naturally aligned. */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +    struct desc_struct entries[];</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +};</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This breaks Xen which expects LDT to be page-aligned. Not sure why.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Jan, Andrew?</span>
<span class="quote">&gt;&gt;&gt; PV guests are not permitted to have writeable mappings to the frames</span>
<span class="quote">&gt;&gt;&gt; making up the GDT and LDT, so it cannot make unaudited changes to</span>
<span class="quote">&gt;&gt;&gt; loadable descriptors.  In particular, for a 32bit PV guest, it is only</span>
<span class="quote">&gt;&gt;&gt; the segment limit which protects Xen from the ring1 guest kernel.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; A lot of this code hasn&#39;t been touched in years, and it certainly</span>
<span class="quote">&gt;&gt;&gt; predates me.  The alignment requirement appears to come from the virtual</span>
<span class="quote">&gt;&gt;&gt; region Xen uses to map the guests GDT and LDT.  Strict alignment is</span>
<span class="quote">&gt;&gt;&gt; required for the GDT so Xen&#39;s descriptors starting at 0xe0xx are</span>
<span class="quote">&gt;&gt;&gt; correct, but the LDT alignment seems to be a side effect of similar</span>
<span class="quote">&gt;&gt;&gt; codepaths.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; For an LDT smaller than 8192 entries, I can&#39;t see any specific reason</span>
<span class="quote">&gt;&gt;&gt; for enforcing alignment, other than &quot;that&#39;s the way it has always been&quot;.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; However, the guest would still have to relinquish write access to all</span>
<span class="quote">&gt;&gt;&gt; frames which make up the LDT, which looks to be a bit of an issue given</span>
<span class="quote">&gt;&gt;&gt; the snippet above.</span>
<span class="quote">&gt;&gt; Does the LDT itself need to be aligned or just the address passed to</span>
<span class="quote">&gt;&gt; paravirt_alloc_ldt?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The address which Xen receives needs to be aligned.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It looks like xen_alloc_ldt() blindly assumes that the desc_struct *ldt</span>
<span class="quote">&gt; it is passed is page aligned, and passes it straight through.</span>

xen_alloc_ldt is just fiddling with protection though, I think.  Isn&#39;t
it xen_set_ldt that&#39;s the meat?  We could easily pass xen_alloc_ldt a
pointer to the ldt_struct.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I think I have a solution, but I doubt it is going to be very popular.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; * Make a new paravirt hook for allocation of ldt_struct, so the paravirt</span>
<span class="quote">&gt;&gt;&gt; backend can choose an alignment if needed</span>
<span class="quote">&gt;&gt;&gt; * Make absolutely certain that __pad has the value 0 (so size and __pad</span>
<span class="quote">&gt;&gt;&gt; combined don&#39;t look like a present descriptor)</span>
<span class="quote">&gt;&gt;&gt; * Never hand selector 0x0008 to unsuspecting users.</span>
<span class="quote">&gt;&gt; Yuck.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I actually meant 0x0004, but yes.  Very much yuck.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This will allow ldt_struct itself to be page aligned, and for the size</span>
<span class="quote">&gt;&gt;&gt; field to sit across the base/limit field of what would logically be</span>
<span class="quote">&gt;&gt;&gt; selector 0x0008  There would be some issues accessing size.  To load</span>
<span class="quote">&gt;&gt;&gt; frames as an LDT, a guest must drop all refs to the page so that its</span>
<span class="quote">&gt;&gt;&gt; type may be changed from writeable to segdesc.  After that, an</span>
<span class="quote">&gt;&gt;&gt; update_descriptor hypercall can be used to change size, and I believe</span>
<span class="quote">&gt;&gt;&gt; the guest may subsequently recreate read-only mappings to the frames in</span>
<span class="quote">&gt;&gt;&gt; question (although frankly it is getting late so you will want to double</span>
<span class="quote">&gt;&gt;&gt; check all of this).</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Anyhow, this looks like an issue which should be fixed up with slightly</span>
<span class="quote">&gt;&gt;&gt; more PVOps, rather than enforcing a Xen view of the world on native Linux.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt; I could presumably make the allocation the other way around so the</span>
<span class="quote">&gt;&gt; size is at the end.  I could even use two separate allocations if</span>
<span class="quote">&gt;&gt; needed.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I suspect two separate allocations would be the better solution, as it</span>
<span class="quote">&gt; means that the size field doesn&#39;t need to be subject to funny page</span>
<span class="quote">&gt; permissions.</span>

True.  OTOH we never write to the size field after allocating the thing.

--Andy
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=53881">Andrew Cooper</a> - July 22, 2015, 12:49 a.m.</div>
<pre class="content">
On 22/07/2015 01:28, Andy Lutomirski wrote:
<span class="quote">&gt; On Tue, Jul 21, 2015 at 5:21 PM, Andrew Cooper</span>
<span class="quote">&gt; &lt;andrew.cooper3@citrix.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On 22/07/2015 01:07, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt; On Tue, Jul 21, 2015 at 4:38 PM, Andrew Cooper</span>
<span class="quote">&gt;&gt;&gt; &lt;andrew.cooper3@citrix.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On 21/07/2015 22:53, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On 07/21/2015 03:59 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; @@ -34,6 +34,44 @@ static inline void load_mm_cr4(struct mm_struct</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; *mm) {}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;   #endif</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;     /*</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; + * ldt_structs can be allocated, used, and freed, but they are never</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; + * modified while live.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; + */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +struct ldt_struct {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +    int size;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +    int __pad;    /* keep the descriptors naturally aligned. */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +    struct desc_struct entries[];</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +};</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; This breaks Xen which expects LDT to be page-aligned. Not sure why.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Jan, Andrew?</span>
<span class="quote">&gt;&gt;&gt;&gt; PV guests are not permitted to have writeable mappings to the frames</span>
<span class="quote">&gt;&gt;&gt;&gt; making up the GDT and LDT, so it cannot make unaudited changes to</span>
<span class="quote">&gt;&gt;&gt;&gt; loadable descriptors.  In particular, for a 32bit PV guest, it is only</span>
<span class="quote">&gt;&gt;&gt;&gt; the segment limit which protects Xen from the ring1 guest kernel.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; A lot of this code hasn&#39;t been touched in years, and it certainly</span>
<span class="quote">&gt;&gt;&gt;&gt; predates me.  The alignment requirement appears to come from the virtual</span>
<span class="quote">&gt;&gt;&gt;&gt; region Xen uses to map the guests GDT and LDT.  Strict alignment is</span>
<span class="quote">&gt;&gt;&gt;&gt; required for the GDT so Xen&#39;s descriptors starting at 0xe0xx are</span>
<span class="quote">&gt;&gt;&gt;&gt; correct, but the LDT alignment seems to be a side effect of similar</span>
<span class="quote">&gt;&gt;&gt;&gt; codepaths.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; For an LDT smaller than 8192 entries, I can&#39;t see any specific reason</span>
<span class="quote">&gt;&gt;&gt;&gt; for enforcing alignment, other than &quot;that&#39;s the way it has always been&quot;.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; However, the guest would still have to relinquish write access to all</span>
<span class="quote">&gt;&gt;&gt;&gt; frames which make up the LDT, which looks to be a bit of an issue given</span>
<span class="quote">&gt;&gt;&gt;&gt; the snippet above.</span>
<span class="quote">&gt;&gt;&gt; Does the LDT itself need to be aligned or just the address passed to</span>
<span class="quote">&gt;&gt;&gt; paravirt_alloc_ldt?</span>
<span class="quote">&gt;&gt; The address which Xen receives needs to be aligned.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It looks like xen_alloc_ldt() blindly assumes that the desc_struct *ldt</span>
<span class="quote">&gt;&gt; it is passed is page aligned, and passes it straight through.</span>
<span class="quote">&gt; xen_alloc_ldt is just fiddling with protection though, I think.  Isn&#39;t</span>
<span class="quote">&gt; it xen_set_ldt that&#39;s the meat?  We could easily pass xen_alloc_ldt a</span>
<span class="quote">&gt; pointer to the ldt_struct.</span>

So it is.  It is the linear_addr in xen_set_ldt() which Xen currently
audits to be page aligned.
<span class="quote">
&gt;&gt;&gt;&gt; This will allow ldt_struct itself to be page aligned, and for the size</span>
<span class="quote">&gt;&gt;&gt;&gt; field to sit across the base/limit field of what would logically be</span>
<span class="quote">&gt;&gt;&gt;&gt; selector 0x0008  There would be some issues accessing size.  To load</span>
<span class="quote">&gt;&gt;&gt;&gt; frames as an LDT, a guest must drop all refs to the page so that its</span>
<span class="quote">&gt;&gt;&gt;&gt; type may be changed from writeable to segdesc.  After that, an</span>
<span class="quote">&gt;&gt;&gt;&gt; update_descriptor hypercall can be used to change size, and I believe</span>
<span class="quote">&gt;&gt;&gt;&gt; the guest may subsequently recreate read-only mappings to the frames in</span>
<span class="quote">&gt;&gt;&gt;&gt; question (although frankly it is getting late so you will want to double</span>
<span class="quote">&gt;&gt;&gt;&gt; check all of this).</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Anyhow, this looks like an issue which should be fixed up with slightly</span>
<span class="quote">&gt;&gt;&gt;&gt; more PVOps, rather than enforcing a Xen view of the world on native Linux.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I could presumably make the allocation the other way around so the</span>
<span class="quote">&gt;&gt;&gt; size is at the end.  I could even use two separate allocations if</span>
<span class="quote">&gt;&gt;&gt; needed.</span>
<span class="quote">&gt;&gt; I suspect two separate allocations would be the better solution, as it</span>
<span class="quote">&gt;&gt; means that the size field doesn&#39;t need to be subject to funny page</span>
<span class="quote">&gt;&gt; permissions.</span>
<span class="quote">&gt; True.  OTOH we never write to the size field after allocating the thing.</span>

Right, but even reading it is going to cause problems if one of the
paravirt ops can&#39;t re-establish ro mappings.

~Andrew
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - July 22, 2015, 1:06 a.m.</div>
<pre class="content">
On Tue, Jul 21, 2015 at 5:49 PM, Andrew Cooper
&lt;andrew.cooper3@citrix.com&gt; wrote:
<span class="quote">&gt; On 22/07/2015 01:28, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; On Tue, Jul 21, 2015 at 5:21 PM, Andrew Cooper</span>
<span class="quote">&gt;&gt; &lt;andrew.cooper3@citrix.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; On 22/07/2015 01:07, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Tue, Jul 21, 2015 at 4:38 PM, Andrew Cooper</span>
<span class="quote">&gt;&gt;&gt;&gt; &lt;andrew.cooper3@citrix.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On 21/07/2015 22:53, Boris Ostrovsky wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; On 07/21/2015 03:59 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; @@ -34,6 +34,44 @@ static inline void load_mm_cr4(struct mm_struct</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; *mm) {}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;   #endif</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt;     /*</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; + * ldt_structs can be allocated, used, and freed, but they are never</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; + * modified while live.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; + */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +struct ldt_struct {</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +    int size;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +    int __pad;    /* keep the descriptors naturally aligned. */</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +    struct desc_struct entries[];</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;&gt; +};</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; This breaks Xen which expects LDT to be page-aligned. Not sure why.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; Jan, Andrew?</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; PV guests are not permitted to have writeable mappings to the frames</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; making up the GDT and LDT, so it cannot make unaudited changes to</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; loadable descriptors.  In particular, for a 32bit PV guest, it is only</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; the segment limit which protects Xen from the ring1 guest kernel.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; A lot of this code hasn&#39;t been touched in years, and it certainly</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; predates me.  The alignment requirement appears to come from the virtual</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; region Xen uses to map the guests GDT and LDT.  Strict alignment is</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; required for the GDT so Xen&#39;s descriptors starting at 0xe0xx are</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; correct, but the LDT alignment seems to be a side effect of similar</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; codepaths.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; For an LDT smaller than 8192 entries, I can&#39;t see any specific reason</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; for enforcing alignment, other than &quot;that&#39;s the way it has always been&quot;.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; However, the guest would still have to relinquish write access to all</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; frames which make up the LDT, which looks to be a bit of an issue given</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; the snippet above.</span>
<span class="quote">&gt;&gt;&gt;&gt; Does the LDT itself need to be aligned or just the address passed to</span>
<span class="quote">&gt;&gt;&gt;&gt; paravirt_alloc_ldt?</span>
<span class="quote">&gt;&gt;&gt; The address which Xen receives needs to be aligned.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; It looks like xen_alloc_ldt() blindly assumes that the desc_struct *ldt</span>
<span class="quote">&gt;&gt;&gt; it is passed is page aligned, and passes it straight through.</span>
<span class="quote">&gt;&gt; xen_alloc_ldt is just fiddling with protection though, I think.  Isn&#39;t</span>
<span class="quote">&gt;&gt; it xen_set_ldt that&#39;s the meat?  We could easily pass xen_alloc_ldt a</span>
<span class="quote">&gt;&gt; pointer to the ldt_struct.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So it is.  It is the linear_addr in xen_set_ldt() which Xen currently</span>
<span class="quote">&gt; audits to be page aligned.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; This will allow ldt_struct itself to be page aligned, and for the size</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; field to sit across the base/limit field of what would logically be</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; selector 0x0008  There would be some issues accessing size.  To load</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; frames as an LDT, a guest must drop all refs to the page so that its</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; type may be changed from writeable to segdesc.  After that, an</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; update_descriptor hypercall can be used to change size, and I believe</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; the guest may subsequently recreate read-only mappings to the frames in</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; question (although frankly it is getting late so you will want to double</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; check all of this).</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Anyhow, this looks like an issue which should be fixed up with slightly</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; more PVOps, rather than enforcing a Xen view of the world on native Linux.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; I could presumably make the allocation the other way around so the</span>
<span class="quote">&gt;&gt;&gt;&gt; size is at the end.  I could even use two separate allocations if</span>
<span class="quote">&gt;&gt;&gt;&gt; needed.</span>
<span class="quote">&gt;&gt;&gt; I suspect two separate allocations would be the better solution, as it</span>
<span class="quote">&gt;&gt;&gt; means that the size field doesn&#39;t need to be subject to funny page</span>
<span class="quote">&gt;&gt;&gt; permissions.</span>
<span class="quote">&gt;&gt; True.  OTOH we never write to the size field after allocating the thing.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Right, but even reading it is going to cause problems if one of the</span>
<span class="quote">&gt; paravirt ops can&#39;t re-establish ro mappings.</span>

Does paravirt_alloc_ldt completely deny access or does it just set it RO?

--Andy
<span class="quote">
&gt;</span>
<span class="quote">&gt; ~Andrew</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=188">Brian Gerst</a> - July 22, 2015, 2:01 a.m.</div>
<pre class="content">
On Tue, Jul 21, 2015 at 3:59 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:
<span class="quote">&gt; modify_ldt has questionable locking and does not synchronize</span>
<span class="quote">&gt; threads.  Improve it: redesign the locking and synchronize all</span>
<span class="quote">&gt; threads&#39; LDTs using an IPI on all modifications.</span>

What does this fix?  I can see sending an IPI if the LDT is
reallocated, but on every update seems unnecessary.

--
Brian Gerst
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - July 22, 2015, 2:12 a.m.</div>
<pre class="content">
On Tue, Jul 21, 2015 at 7:01 PM, Brian Gerst &lt;brgerst@gmail.com&gt; wrote:
<span class="quote">&gt; On Tue, Jul 21, 2015 at 3:59 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:</span>
<span class="quote">&gt;&gt; modify_ldt has questionable locking and does not synchronize</span>
<span class="quote">&gt;&gt; threads.  Improve it: redesign the locking and synchronize all</span>
<span class="quote">&gt;&gt; threads&#39; LDTs using an IPI on all modifications.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What does this fix?  I can see sending an IPI if the LDT is</span>
<span class="quote">&gt; reallocated, but on every update seems unnecessary.</span>
<span class="quote">&gt;</span>

It prevents nastiness in which you&#39;re in user mode with an impossible
CS or SS, resulting in potentially interesting artifacts in
interrupts, NMIs, etc.
<span class="quote">
&gt; --</span>
<span class="quote">&gt; Brian Gerst</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=188">Brian Gerst</a> - July 22, 2015, 2:53 a.m.</div>
<pre class="content">
On Tue, Jul 21, 2015 at 10:12 PM, Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:
<span class="quote">&gt; On Tue, Jul 21, 2015 at 7:01 PM, Brian Gerst &lt;brgerst@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On Tue, Jul 21, 2015 at 3:59 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; modify_ldt has questionable locking and does not synchronize</span>
<span class="quote">&gt;&gt;&gt; threads.  Improve it: redesign the locking and synchronize all</span>
<span class="quote">&gt;&gt;&gt; threads&#39; LDTs using an IPI on all modifications.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; What does this fix?  I can see sending an IPI if the LDT is</span>
<span class="quote">&gt;&gt; reallocated, but on every update seems unnecessary.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It prevents nastiness in which you&#39;re in user mode with an impossible</span>
<span class="quote">&gt; CS or SS, resulting in potentially interesting artifacts in</span>
<span class="quote">&gt; interrupts, NMIs, etc.</span>

By impossible, do you mean a partially updated descriptor when the
interrupt occurs?  Would making sure that the descriptor is atomically
updated (using set_64bit()) fix that?

--
Brian Gerst
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - July 22, 2015, 4:22 a.m.</div>
<pre class="content">
On Tue, Jul 21, 2015 at 7:53 PM, Brian Gerst &lt;brgerst@gmail.com&gt; wrote:
<span class="quote">&gt; On Tue, Jul 21, 2015 at 10:12 PM, Andy Lutomirski &lt;luto@amacapital.net&gt; wrote:</span>
<span class="quote">&gt;&gt; On Tue, Jul 21, 2015 at 7:01 PM, Brian Gerst &lt;brgerst@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; On Tue, Jul 21, 2015 at 3:59 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; modify_ldt has questionable locking and does not synchronize</span>
<span class="quote">&gt;&gt;&gt;&gt; threads.  Improve it: redesign the locking and synchronize all</span>
<span class="quote">&gt;&gt;&gt;&gt; threads&#39; LDTs using an IPI on all modifications.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; What does this fix?  I can see sending an IPI if the LDT is</span>
<span class="quote">&gt;&gt;&gt; reallocated, but on every update seems unnecessary.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It prevents nastiness in which you&#39;re in user mode with an impossible</span>
<span class="quote">&gt;&gt; CS or SS, resulting in potentially interesting artifacts in</span>
<span class="quote">&gt;&gt; interrupts, NMIs, etc.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; By impossible, do you mean a partially updated descriptor when the</span>
<span class="quote">&gt; interrupt occurs?  Would making sure that the descriptor is atomically</span>
<span class="quote">&gt; updated (using set_64bit()) fix that?</span>
<span class="quote">&gt;</span>

I tried to exploit that once and didn&#39;t get very far.  If I could
cause the LDT to be populated one bit at a time, I think I could
materialize a call gate out of thin air.  The docs are unclear on
what, if anything, the memory ordering rules are.

I&#39;m more concerned about the case where a segment register caches a
value that is no longer in the LDT.  If it&#39;s DS, ES, FS, or GS, it
results in nondeterministic behavior but is otherwise not a big deal.
If it&#39;s CS or SS, then an interrupt or exception will write a stack
frame with the selectors but IRET can fault.

If the interrupt is an NMI and certain other conditions are met and
your kernel is older than 4.2-rc3, then you should read the
CVE-2015-5157 advisory I&#39;ll send tomorrow :)  Even on 4.2-rc3, the NMI
code still struggles a bit if this happens.

With this patch, you can never be in user mode with CS or SS selectors
that point at the LDT but don&#39;t match the LDT.  That makes me a lot
more comfortable with modify_ldt.

--Andy
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="p_header">index a0bf89fd2647..4e10d73cf018 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -280,21 +280,6 @@</span> <span class="p_context"> static inline void clear_LDT(void)</span>
 	set_ldt(NULL, 0);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * load one particular LDT into the current CPU</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline void load_LDT_nolock(mm_context_t *pc)</span>
<span class="p_del">-{</span>
<span class="p_del">-	set_ldt(pc-&gt;ldt, pc-&gt;size);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void load_LDT(mm_context_t *pc)</span>
<span class="p_del">-{</span>
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-	load_LDT_nolock(pc);</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static inline unsigned long get_desc_base(const struct desc_struct *desc)
 {
 	return (unsigned)(desc-&gt;base0 | ((desc-&gt;base1) &lt;&lt; 16) | ((desc-&gt;base2) &lt;&lt; 24));
<span class="p_header">diff --git a/arch/x86/include/asm/mmu.h b/arch/x86/include/asm/mmu.h</span>
<span class="p_header">index 09b9620a73b4..364d27481a52 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu.h</span>
<span class="p_chunk">@@ -9,8 +9,7 @@</span> <span class="p_context"></span>
  * we put the segment information here.
  */
 typedef struct {
<span class="p_del">-	void *ldt;</span>
<span class="p_del">-	int size;</span>
<span class="p_add">+	struct ldt_struct *ldt;</span>
 
 #ifdef CONFIG_X86_64
 	/* True if mm supports a task running in 32 bit compatibility mode. */
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 5e8daee7c5c9..1ff121fbf366 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -34,6 +34,44 @@</span> <span class="p_context"> static inline void load_mm_cr4(struct mm_struct *mm) {}</span>
 #endif
 
 /*
<span class="p_add">+ * ldt_structs can be allocated, used, and freed, but they are never</span>
<span class="p_add">+ * modified while live.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct ldt_struct {</span>
<span class="p_add">+	int size;</span>
<span class="p_add">+	int __pad;	/* keep the descriptors naturally aligned. */</span>
<span class="p_add">+	struct desc_struct entries[];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void load_mm_ldt(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct ldt_struct *ldt;</span>
<span class="p_add">+	DEBUG_LOCKS_WARN_ON(!irqs_disabled());</span>
<span class="p_add">+</span>
<span class="p_add">+	/* lockless_dereference synchronizes with smp_store_release */</span>
<span class="p_add">+	ldt = lockless_dereference(mm-&gt;context.ldt);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Any change to mm-&gt;context.ldt is followed by an IPI to all</span>
<span class="p_add">+	 * CPUs with the mm active.  The LDT will not be freed until</span>
<span class="p_add">+	 * after the IPI is handled by all such CPUs.  This means that,</span>
<span class="p_add">+	 * if the ldt_struct changes before we return, the values we see</span>
<span class="p_add">+	 * will be safe, and the new values will be loaded before we run</span>
<span class="p_add">+	 * any user code.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * NB: don&#39;t try to convert this to use RCU without extreme care.</span>
<span class="p_add">+	 * We would still need IRQs off, because we don&#39;t want to change</span>
<span class="p_add">+	 * the local LDT after an IPI loaded a newer value than the one</span>
<span class="p_add">+	 * that we can see.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(ldt))</span>
<span class="p_add">+		set_ldt(ldt-&gt;entries, ldt-&gt;size);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		clear_LDT();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Used for LDT copy/destruction.
  */
 int init_new_context(struct task_struct *tsk, struct mm_struct *mm);
<span class="p_chunk">@@ -78,12 +116,12 @@</span> <span class="p_context"> static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
 		 * was called and then modify_ldt changed
 		 * prev-&gt;context.ldt but suppressed an IPI to this CPU.
 		 * In this case, prev-&gt;context.ldt != NULL, because we
<span class="p_del">-		 * never free an LDT while the mm still exists.  That</span>
<span class="p_del">-		 * means that next-&gt;context.ldt != prev-&gt;context.ldt,</span>
<span class="p_del">-		 * because mms never share an LDT.</span>
<span class="p_add">+		 * never set context.ldt to NULL while the mm still</span>
<span class="p_add">+		 * exists.  That means that next-&gt;context.ldt !=</span>
<span class="p_add">+		 * prev-&gt;context.ldt, because mms never share an LDT.</span>
 		 */
 		if (unlikely(prev-&gt;context.ldt != next-&gt;context.ldt))
<span class="p_del">-			load_LDT_nolock(&amp;next-&gt;context);</span>
<span class="p_add">+			load_mm_ldt(next);</span>
 	}
 #ifdef CONFIG_SMP
 	  else {
<span class="p_chunk">@@ -106,7 +144,7 @@</span> <span class="p_context"> static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
 			load_cr3(next-&gt;pgd);
 			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 			load_mm_cr4(next);
<span class="p_del">-			load_LDT_nolock(&amp;next-&gt;context);</span>
<span class="p_add">+			load_mm_ldt(next);</span>
 		}
 	}
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 922c5e0cea4c..cb9e5df42dd2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -1410,7 +1410,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	load_sp0(t, &amp;current-&gt;thread);
 	set_tss_desc(cpu, t);
 	load_TR_desc();
<span class="p_del">-	load_LDT(&amp;init_mm.context);</span>
<span class="p_add">+	load_mm_ldt(&amp;init_mm);</span>
 
 	clear_all_debug_regs();
 	dbg_restore_debug_regs();
<span class="p_chunk">@@ -1459,7 +1459,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	load_sp0(t, thread);
 	set_tss_desc(cpu, t);
 	load_TR_desc();
<span class="p_del">-	load_LDT(&amp;init_mm.context);</span>
<span class="p_add">+	load_mm_ldt(&amp;init_mm);</span>
 
 	t-&gt;x86_tss.io_bitmap_base = offsetof(struct tss_struct, io_bitmap);
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/perf_event.c b/arch/x86/kernel/cpu/perf_event.c</span>
<span class="p_header">index 3658de47900f..9469dfa55607 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/perf_event.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/perf_event.c</span>
<span class="p_chunk">@@ -2179,21 +2179,25 @@</span> <span class="p_context"> static unsigned long get_segment_base(unsigned int segment)</span>
 	int idx = segment &gt;&gt; 3;
 
 	if ((segment &amp; SEGMENT_TI_MASK) == SEGMENT_LDT) {
<span class="p_add">+		struct ldt_struct *ldt;</span>
<span class="p_add">+</span>
 		if (idx &gt; LDT_ENTRIES)
 			return 0;
 
<span class="p_del">-		if (idx &gt; current-&gt;active_mm-&gt;context.size)</span>
<span class="p_add">+		/* IRQs are off, so this synchronizes with smp_store_release */</span>
<span class="p_add">+		ldt = lockless_dereference(current-&gt;active_mm-&gt;context.ldt);</span>
<span class="p_add">+		if (!ldt || idx &gt; ldt-&gt;size)</span>
 			return 0;
 
<span class="p_del">-		desc = current-&gt;active_mm-&gt;context.ldt;</span>
<span class="p_add">+		desc = &amp;ldt-&gt;entries[idx];</span>
 	} else {
 		if (idx &gt; GDT_ENTRIES)
 			return 0;
 
<span class="p_del">-		desc = raw_cpu_ptr(gdt_page.gdt);</span>
<span class="p_add">+		desc = raw_cpu_ptr(gdt_page.gdt) + idx;</span>
 	}
 
<span class="p_del">-	return get_desc_base(desc + idx);</span>
<span class="p_add">+	return get_desc_base(desc);</span>
 }
 
 #ifdef CONFIG_COMPAT
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index c37886d759cc..d65e6ec90338 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -20,82 +20,70 @@</span> <span class="p_context"></span>
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/syscalls.h&gt;
 
<span class="p_del">-#ifdef CONFIG_SMP</span>
 static void flush_ldt(void *current_mm)
 {
<span class="p_del">-	if (current-&gt;active_mm == current_mm)</span>
<span class="p_del">-		load_LDT(&amp;current-&gt;active_mm-&gt;context);</span>
<span class="p_add">+	if (current-&gt;active_mm == current_mm) {</span>
<span class="p_add">+		/* context.lock is held for us, so we don&#39;t need any locking. */</span>
<span class="p_add">+		mm_context_t *pc = &amp;current-&gt;active_mm-&gt;context;</span>
<span class="p_add">+		set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;size);</span>
<span class="p_add">+	}</span>
 }
<span class="p_del">-#endif</span>
 
<span class="p_del">-static int alloc_ldt(mm_context_t *pc, int mincount, int reload)</span>
<span class="p_add">+static struct ldt_struct *alloc_ldt_struct(int size)</span>
 {
<span class="p_del">-	void *oldldt, *newldt;</span>
<span class="p_del">-	int oldsize;</span>
<span class="p_add">+	struct ldt_struct *new_ldt;</span>
<span class="p_add">+	int alloc_size;</span>
 
<span class="p_del">-	if (mincount &lt;= pc-&gt;size)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-	oldsize = pc-&gt;size;</span>
<span class="p_del">-	mincount = (mincount + (PAGE_SIZE / LDT_ENTRY_SIZE - 1)) &amp;</span>
<span class="p_del">-			(~(PAGE_SIZE / LDT_ENTRY_SIZE - 1));</span>
<span class="p_del">-	if (mincount * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_del">-		newldt = vmalloc(mincount * LDT_ENTRY_SIZE);</span>
<span class="p_add">+	if (size &gt; LDT_ENTRIES)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(LDT_ENTRY_SIZE != sizeof(struct desc_struct));</span>
<span class="p_add">+	alloc_size = sizeof(struct ldt_struct) + size * LDT_ENTRY_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (alloc_size &gt; PAGE_SIZE)</span>
<span class="p_add">+		new_ldt = vmalloc(alloc_size);</span>
 	else
<span class="p_del">-		newldt = (void *)__get_free_page(GFP_KERNEL);</span>
<span class="p_add">+		new_ldt = (void *)__get_free_page(GFP_KERNEL);</span>
<span class="p_add">+	if (!new_ldt)</span>
<span class="p_add">+		return NULL;</span>
 
<span class="p_del">-	if (!newldt)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_add">+	new_ldt-&gt;size = size;</span>
<span class="p_add">+	paravirt_alloc_ldt(new_ldt-&gt;entries, size);</span>
<span class="p_add">+	return new_ldt;</span>
<span class="p_add">+}</span>
 
<span class="p_del">-	if (oldsize)</span>
<span class="p_del">-		memcpy(newldt, pc-&gt;ldt, oldsize * LDT_ENTRY_SIZE);</span>
<span class="p_del">-	oldldt = pc-&gt;ldt;</span>
<span class="p_del">-	memset(newldt + oldsize * LDT_ENTRY_SIZE, 0,</span>
<span class="p_del">-	       (mincount - oldsize) * LDT_ENTRY_SIZE);</span>
<span class="p_add">+static void install_ldt(struct mm_struct *current_mm,</span>
<span class="p_add">+			struct ldt_struct *ldt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* context.lock is held */</span>
<span class="p_add">+	preempt_disable();</span>
 
<span class="p_del">-	paravirt_alloc_ldt(newldt, mincount);</span>
<span class="p_add">+	/* Synchronizes with lockless_dereference in load_mm_ldt. */</span>
<span class="p_add">+	smp_store_release(&amp;current_mm-&gt;context.ldt, ldt);</span>
 
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	/* CHECKME: Do we really need this ? */</span>
<span class="p_del">-	wmb();</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	pc-&gt;ldt = newldt;</span>
<span class="p_del">-	wmb();</span>
<span class="p_del">-	pc-&gt;size = mincount;</span>
<span class="p_del">-	wmb();</span>
<span class="p_add">+	/* Activate for this CPU. */</span>
<span class="p_add">+	flush_ldt(current-&gt;mm);</span>
 
<span class="p_del">-	if (reload) {</span>
 #ifdef CONFIG_SMP
<span class="p_del">-		preempt_disable();</span>
<span class="p_del">-		load_LDT(pc);</span>
<span class="p_del">-		if (!cpumask_equal(mm_cpumask(current-&gt;mm),</span>
<span class="p_del">-				   cpumask_of(smp_processor_id())))</span>
<span class="p_del">-			smp_call_function(flush_ldt, current-&gt;mm, 1);</span>
<span class="p_del">-		preempt_enable();</span>
<span class="p_del">-#else</span>
<span class="p_del">-		load_LDT(pc);</span>
<span class="p_add">+	/* Synchronize with other CPUs. */</span>
<span class="p_add">+	if (!cpumask_equal(mm_cpumask(current_mm),</span>
<span class="p_add">+			   cpumask_of(smp_processor_id())))</span>
<span class="p_add">+		smp_call_function(flush_ldt, current_mm, 1);</span>
 #endif
<span class="p_del">-	}</span>
<span class="p_del">-	if (oldsize) {</span>
<span class="p_del">-		paravirt_free_ldt(oldldt, oldsize);</span>
<span class="p_del">-		if (oldsize * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_del">-			vfree(oldldt);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			put_page(virt_to_page(oldldt));</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_add">+	preempt_enable();</span>
 }
 
<span class="p_del">-static inline int copy_ldt(mm_context_t *new, mm_context_t *old)</span>
<span class="p_add">+static void free_ldt_struct(struct ldt_struct *ldt)</span>
 {
<span class="p_del">-	int err = alloc_ldt(new, old-&gt;size, 0);</span>
<span class="p_del">-	int i;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (err &lt; 0)</span>
<span class="p_del">-		return err;</span>
<span class="p_del">-</span>
<span class="p_del">-	for (i = 0; i &lt; old-&gt;size; i++)</span>
<span class="p_del">-		write_ldt_entry(new-&gt;ldt, i, old-&gt;ldt + i * LDT_ENTRY_SIZE);</span>
<span class="p_del">-	return 0;</span>
<span class="p_add">+	if (unlikely(ldt)) {</span>
<span class="p_add">+		int alloc_size = sizeof(struct ldt_struct) +</span>
<span class="p_add">+			ldt-&gt;size * LDT_ENTRY_SIZE;</span>
<span class="p_add">+		paravirt_free_ldt(ldt-&gt;entries, ldt-&gt;size);</span>
<span class="p_add">+		if (alloc_size &gt; PAGE_SIZE)</span>
<span class="p_add">+			vfree(ldt);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			put_page(virt_to_page(ldt));</span>
<span class="p_add">+	}</span>
 }
 
 /*
<span class="p_chunk">@@ -106,15 +94,35 @@</span> <span class="p_context"> int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
 {
 	struct mm_struct *old_mm;
 	int retval = 0;
<span class="p_add">+	int i;</span>
 
 	mutex_init(&amp;mm-&gt;context.lock);
<span class="p_del">-	mm-&gt;context.size = 0;</span>
 	old_mm = current-&gt;mm;
<span class="p_del">-	if (old_mm &amp;&amp; old_mm-&gt;context.size &gt; 0) {</span>
<span class="p_del">-		mutex_lock(&amp;old_mm-&gt;context.lock);</span>
<span class="p_del">-		retval = copy_ldt(&amp;mm-&gt;context, &amp;old_mm-&gt;context);</span>
<span class="p_del">-		mutex_unlock(&amp;old_mm-&gt;context.lock);</span>
<span class="p_add">+	if (!old_mm) {</span>
<span class="p_add">+		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+		return 0;</span>
 	}
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;old_mm-&gt;context.lock);</span>
<span class="p_add">+	if (old_mm-&gt;context.ldt) {</span>
<span class="p_add">+		struct ldt_struct *new_ldt =</span>
<span class="p_add">+			alloc_ldt_struct(old_mm-&gt;context.ldt-&gt;size);</span>
<span class="p_add">+		if (!new_ldt) {</span>
<span class="p_add">+			retval = -ENOMEM;</span>
<span class="p_add">+			goto out_unlock;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		for (i = 0; i &lt; old_mm-&gt;context.ldt-&gt;size; i++)</span>
<span class="p_add">+			write_ldt_entry(new_ldt-&gt;entries, i,</span>
<span class="p_add">+					&amp;old_mm-&gt;context.ldt-&gt;entries[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		mm-&gt;context.ldt = new_ldt;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+out_unlock:</span>
<span class="p_add">+	mutex_unlock(&amp;old_mm-&gt;context.lock);</span>
 	return retval;
 }
 
<span class="p_chunk">@@ -125,53 +133,47 @@</span> <span class="p_context"> int init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
  */
 void destroy_context(struct mm_struct *mm)
 {
<span class="p_del">-	if (mm-&gt;context.size) {</span>
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-		/* CHECKME: Can this ever happen ? */</span>
<span class="p_del">-		if (mm == current-&gt;active_mm)</span>
<span class="p_del">-			clear_LDT();</span>
<span class="p_del">-#endif</span>
<span class="p_del">-		paravirt_free_ldt(mm-&gt;context.ldt, mm-&gt;context.size);</span>
<span class="p_del">-		if (mm-&gt;context.size * LDT_ENTRY_SIZE &gt; PAGE_SIZE)</span>
<span class="p_del">-			vfree(mm-&gt;context.ldt);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			put_page(virt_to_page(mm-&gt;context.ldt));</span>
<span class="p_del">-		mm-&gt;context.size = 0;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	free_ldt_struct(mm-&gt;context.ldt);</span>
<span class="p_add">+	mm-&gt;context.ldt = NULL;</span>
 }
 
 static int read_ldt(void __user *ptr, unsigned long bytecount)
 {
<span class="p_del">-	int err;</span>
<span class="p_add">+	int retval;</span>
 	unsigned long size;
 	struct mm_struct *mm = current-&gt;mm;
 
<span class="p_del">-	if (!mm-&gt;context.size)</span>
<span class="p_del">-		return 0;</span>
<span class="p_add">+	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mm-&gt;context.ldt) {</span>
<span class="p_add">+		retval = 0;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (bytecount &gt; LDT_ENTRY_SIZE * LDT_ENTRIES)
 		bytecount = LDT_ENTRY_SIZE * LDT_ENTRIES;
 
<span class="p_del">-	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_del">-	size = mm-&gt;context.size * LDT_ENTRY_SIZE;</span>
<span class="p_add">+	size = mm-&gt;context.ldt-&gt;size * LDT_ENTRY_SIZE;</span>
 	if (size &gt; bytecount)
 		size = bytecount;
 
<span class="p_del">-	err = 0;</span>
<span class="p_del">-	if (copy_to_user(ptr, mm-&gt;context.ldt, size))</span>
<span class="p_del">-		err = -EFAULT;</span>
<span class="p_del">-	mutex_unlock(&amp;mm-&gt;context.lock);</span>
<span class="p_del">-	if (err &lt; 0)</span>
<span class="p_del">-		goto error_return;</span>
<span class="p_add">+	if (copy_to_user(ptr, mm-&gt;context.ldt-&gt;entries, size)) {</span>
<span class="p_add">+		retval = -EFAULT;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (size != bytecount) {
<span class="p_del">-		/* zero-fill the rest */</span>
<span class="p_add">+		/* Zero-fill the rest and pretend we read bytecount bytes. */</span>
 		if (clear_user(ptr + size, bytecount - size) != 0) {
<span class="p_del">-			err = -EFAULT;</span>
<span class="p_del">-			goto error_return;</span>
<span class="p_add">+			retval = -EFAULT;</span>
<span class="p_add">+			goto out_unlock;</span>
 		}
 	}
<span class="p_del">-	return bytecount;</span>
<span class="p_del">-error_return:</span>
<span class="p_del">-	return err;</span>
<span class="p_add">+	retval = bytecount;</span>
<span class="p_add">+</span>
<span class="p_add">+out_unlock:</span>
<span class="p_add">+	mutex_unlock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+	return retval;</span>
 }
 
 static int read_default_ldt(void __user *ptr, unsigned long bytecount)
<span class="p_chunk">@@ -189,12 +191,16 @@</span> <span class="p_context"> static int read_default_ldt(void __user *ptr, unsigned long bytecount)</span>
 	return bytecount;
 }
 
<span class="p_add">+static struct desc_struct blank_desc;</span>
<span class="p_add">+</span>
 static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct desc_struct ldt;
<span class="p_del">-	int error;</span>
<span class="p_add">+	int error, i;</span>
 	struct user_desc ldt_info;
<span class="p_add">+	int oldsize, newsize;</span>
<span class="p_add">+	struct ldt_struct *new_ldt, *old_ldt;</span>
 
 	error = -EINVAL;
 	if (bytecount != sizeof(ldt_info))
<span class="p_chunk">@@ -213,34 +219,41 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 			goto out;
 	}
 
<span class="p_del">-	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_del">-	if (ldt_info.entry_number &gt;= mm-&gt;context.size) {</span>
<span class="p_del">-		error = alloc_ldt(&amp;current-&gt;mm-&gt;context,</span>
<span class="p_del">-				  ldt_info.entry_number + 1, 1);</span>
<span class="p_del">-		if (error &lt; 0)</span>
<span class="p_del">-			goto out_unlock;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Allow LDTs to be cleared by the user. */</span>
<span class="p_del">-	if (ldt_info.base_addr == 0 &amp;&amp; ldt_info.limit == 0) {</span>
<span class="p_del">-		if (oldmode || LDT_empty(&amp;ldt_info)) {</span>
<span class="p_del">-			memset(&amp;ldt, 0, sizeof(ldt));</span>
<span class="p_del">-			goto install;</span>
<span class="p_add">+	if ((oldmode &amp;&amp; ldt_info.base_addr == 0 &amp;&amp; ldt_info.limit == 0) ||</span>
<span class="p_add">+	    LDT_empty(&amp;ldt_info)) {</span>
<span class="p_add">+		/* The user wants to clear the entry. */</span>
<span class="p_add">+		memset(&amp;ldt, 0, sizeof(ldt));</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (!IS_ENABLED(CONFIG_X86_16BIT) &amp;&amp; !ldt_info.seg_32bit) {</span>
<span class="p_add">+			error = -EINVAL;</span>
<span class="p_add">+			goto out;</span>
 		}
<span class="p_add">+</span>
<span class="p_add">+		fill_ldt(&amp;ldt, &amp;ldt_info);</span>
<span class="p_add">+		if (oldmode)</span>
<span class="p_add">+			ldt.avl = 0;</span>
 	}
 
<span class="p_del">-	if (!IS_ENABLED(CONFIG_X86_16BIT) &amp;&amp; !ldt_info.seg_32bit) {</span>
<span class="p_del">-		error = -EINVAL;</span>
<span class="p_add">+	mutex_lock(&amp;mm-&gt;context.lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	old_ldt = mm-&gt;context.ldt;</span>
<span class="p_add">+	oldsize = old_ldt ? old_ldt-&gt;size : 0;</span>
<span class="p_add">+	newsize = max((int)(ldt_info.entry_number + 1), oldsize);</span>
<span class="p_add">+</span>
<span class="p_add">+	error = -ENOMEM;</span>
<span class="p_add">+	new_ldt = alloc_ldt_struct(newsize);</span>
<span class="p_add">+	if (!new_ldt)</span>
 		goto out_unlock;
<span class="p_del">-	}</span>
 
<span class="p_del">-	fill_ldt(&amp;ldt, &amp;ldt_info);</span>
<span class="p_del">-	if (oldmode)</span>
<span class="p_del">-		ldt.avl = 0;</span>
<span class="p_add">+	for (i = 0; i &lt; oldsize; i++)</span>
<span class="p_add">+		write_ldt_entry(new_ldt-&gt;entries, i,</span>
<span class="p_add">+				&amp;old_ldt-&gt;entries[i]);</span>
<span class="p_add">+	for (i = oldsize; i &lt; newsize; i++)</span>
<span class="p_add">+		write_ldt_entry(new_ldt-&gt;entries, i, &amp;blank_desc);</span>
<span class="p_add">+	write_ldt_entry(new_ldt-&gt;entries, ldt_info.entry_number, &amp;ldt);</span>
 
<span class="p_del">-	/* Install the new entry ...  */</span>
<span class="p_del">-install:</span>
<span class="p_del">-	write_ldt_entry(mm-&gt;context.ldt, ldt_info.entry_number, &amp;ldt);</span>
<span class="p_add">+	install_ldt(mm, new_ldt);</span>
<span class="p_add">+	free_ldt_struct(old_ldt);</span>
 	error = 0;
 
 out_unlock:
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index 71d7849a07f7..f6b916387590 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -121,11 +121,11 @@</span> <span class="p_context"> void __show_regs(struct pt_regs *regs, int all)</span>
 void release_thread(struct task_struct *dead_task)
 {
 	if (dead_task-&gt;mm) {
<span class="p_del">-		if (dead_task-&gt;mm-&gt;context.size) {</span>
<span class="p_add">+		if (dead_task-&gt;mm-&gt;context.ldt) {</span>
 			pr_warn(&quot;WARNING: dead process %s still has LDT? &lt;%p/%d&gt;\n&quot;,
 				dead_task-&gt;comm,
 				dead_task-&gt;mm-&gt;context.ldt,
<span class="p_del">-				dead_task-&gt;mm-&gt;context.size);</span>
<span class="p_add">+				dead_task-&gt;mm-&gt;context.ldt-&gt;size);</span>
 			BUG();
 		}
 	}
<span class="p_header">diff --git a/arch/x86/kernel/step.c b/arch/x86/kernel/step.c</span>
<span class="p_header">index 9b4d51d0c0d0..6273324186ac 100644</span>
<span class="p_header">--- a/arch/x86/kernel/step.c</span>
<span class="p_header">+++ b/arch/x86/kernel/step.c</span>
<span class="p_chunk">@@ -5,6 +5,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/ptrace.h&gt;
 #include &lt;asm/desc.h&gt;
<span class="p_add">+#include &lt;asm/mmu_context.h&gt;</span>
 
 unsigned long convert_ip_to_linear(struct task_struct *child, struct pt_regs *regs)
 {
<span class="p_chunk">@@ -30,10 +31,11 @@</span> <span class="p_context"> unsigned long convert_ip_to_linear(struct task_struct *child, struct pt_regs *re</span>
 		seg &amp;= ~7UL;
 
 		mutex_lock(&amp;child-&gt;mm-&gt;context.lock);
<span class="p_del">-		if (unlikely((seg &gt;&gt; 3) &gt;= child-&gt;mm-&gt;context.size))</span>
<span class="p_add">+		if (unlikely(!child-&gt;mm-&gt;context.ldt ||</span>
<span class="p_add">+			     (seg &gt;&gt; 3) &gt;= child-&gt;mm-&gt;context.ldt-&gt;size))</span>
 			addr = -1L; /* bogus selector, access would fault */
 		else {
<span class="p_del">-			desc = child-&gt;mm-&gt;context.ldt + seg;</span>
<span class="p_add">+			desc = &amp;child-&gt;mm-&gt;context.ldt-&gt;entries[seg];</span>
 			base = get_desc_base(desc);
 
 			/* 16-bit code segment? */
<span class="p_header">diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c</span>
<span class="p_header">index 0d7dd1f5ac36..9ab52791fed5 100644</span>
<span class="p_header">--- a/arch/x86/power/cpu.c</span>
<span class="p_header">+++ b/arch/x86/power/cpu.c</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/fpu/internal.h&gt;
 #include &lt;asm/debugreg.h&gt;
 #include &lt;asm/cpu.h&gt;
<span class="p_add">+#include &lt;asm/mmu_context.h&gt;</span>
 
 #ifdef CONFIG_X86_32
 __visible unsigned long saved_context_ebx;
<span class="p_chunk">@@ -153,7 +154,7 @@</span> <span class="p_context"> static void fix_processor_context(void)</span>
 	syscall_init();				/* This sets MSR_*STAR and related */
 #endif
 	load_TR_desc();				/* This does ltr */
<span class="p_del">-	load_LDT(&amp;current-&gt;active_mm-&gt;context);	/* This does lldt */</span>
<span class="p_add">+	load_mm_ldt(current-&gt;active_mm);	/* This does lldt */</span>
 
 	fpu__resume_cpu();
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



