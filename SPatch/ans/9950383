
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv3,11/11] mm: Use updated pmdp_invalidate() interface to track dirty/accessed bits - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv3,11/11] mm: Use updated pmdp_invalidate() interface to track dirty/accessed bits</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 13, 2017, 2:08 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;87tw07uz7p.fsf@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9950383/mbox/"
   >mbox</a>
|
   <a href="/patch/9950383/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9950383/">/patch/9950383/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	9D0FB603F3 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 13 Sep 2017 02:09:26 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 911E2290E9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 13 Sep 2017 02:09:26 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 85C18290F4; Wed, 13 Sep 2017 02:09:26 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 94212290E9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 13 Sep 2017 02:09:24 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751719AbdIMCJT (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 12 Sep 2017 22:09:19 -0400
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:43499 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1751031AbdIMCJR (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 12 Sep 2017 22:09:17 -0400
Received: from pps.filterd (m0098393.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.21/8.16.0.21) with SMTP id
	v8D28cnQ058435
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 12 Sep 2017 22:09:17 -0400
Received: from e23smtp05.au.ibm.com (e23smtp05.au.ibm.com [202.81.31.147])
	by mx0a-001b2d01.pphosted.com with ESMTP id 2cxt3d387u-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 12 Sep 2017 22:09:16 -0400
Received: from localhost
	by e23smtp05.au.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from
	&lt;aneesh.kumar@linux.vnet.ibm.com&gt;; Wed, 13 Sep 2017 12:09:14 +1000
Received: from d23relay06.au.ibm.com (202.81.31.225)
	by e23smtp05.au.ibm.com (202.81.31.211) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Wed, 13 Sep 2017 12:09:12 +1000
Received: from d23av02.au.ibm.com (d23av02.au.ibm.com [9.190.235.138])
	by d23relay06.au.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
	v8D29CqX35651778; Wed, 13 Sep 2017 12:09:12 +1000
Received: from d23av02.au.ibm.com (localhost [127.0.0.1])
	by d23av02.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id
	v8D2915Y008657; Wed, 13 Sep 2017 12:09:03 +1000
Received: from skywalker ([9.85.130.115])
	by d23av02.au.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with SMTP id
	v8D28odD008419; Wed, 13 Sep 2017 12:08:52 +1000
Received: (nullmailer pid 20857 invoked by uid 1000);
	Wed, 13 Sep 2017 02:08:58 -0000
From: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
To: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;, Vineet Gupta &lt;vgupta@synopsys.com&gt;,
	Russell King &lt;linux@armlinux.org.uk&gt;, Will Deacon &lt;will.deacon@arm.com&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Ralf Baechle &lt;ralf@linux-mips.org&gt;,
	&quot;David S. Miller&quot; &lt;davem@davemloft.net&gt;,
	Martin Schwidefsky &lt;schwidefsky@de.ibm.com&gt;,
	Heiko Carstens &lt;heiko.carstens@de.ibm.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Cc: linux-arch@vger.kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: Re: [PATCHv3 11/11] mm: Use updated pmdp_invalidate() interface to
	track dirty/accessed bits
In-Reply-To: &lt;20170912153941.47012-12-kirill.shutemov@linux.intel.com&gt;
References: &lt;20170912153941.47012-1-kirill.shutemov@linux.intel.com&gt;
	&lt;20170912153941.47012-12-kirill.shutemov@linux.intel.com&gt;
Date: Wed, 13 Sep 2017 07:38:58 +0530
MIME-Version: 1.0
Content-Type: text/plain
X-TM-AS-MML: disable
x-cbid: 17091302-0016-0000-0000-000002645ADA
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17091302-0017-0000-0000-000006E89A1E
Message-Id: &lt;87tw07uz7p.fsf@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-09-13_01:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=7
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1707230000
	definitions=main-1709130031
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - Sept. 13, 2017, 2:08 a.m.</div>
<pre class="content">
How about this additional patch ?. This results in code reduction.

From fed62d0541ae78206a1a25caeb46a3ffa7ade9c8 Mon Sep 17 00:00:00 2001
<span class="from">From: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
Date: Thu, 27 Jul 2017 12:21:33 +0530
Subject: [PATCH] mm/thp: Remove pmd_huge_split_prepare

Instead of marking the pmd ready for split, invalidate the pmd. This should
take care of powerpc requirement. Only side effect is that we mark the pmd
invalid early. This can result in us blocking access to the page a bit longer
if we race against a thp split.
<span class="signed-off-by">
Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
---
 arch/powerpc/include/asm/book3s/64/hash-4k.h  |  2 -
 arch/powerpc/include/asm/book3s/64/hash-64k.h |  2 -
 arch/powerpc/include/asm/book3s/64/pgtable.h  |  9 ----
 arch/powerpc/include/asm/book3s/64/radix.h    |  6 ---
 arch/powerpc/mm/pgtable-hash64.c              | 22 --------
 include/asm-generic/pgtable.h                 |  8 ---
 mm/huge_memory.c                              | 73 +++++++++++++--------------
 7 files changed, 35 insertions(+), 87 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - Dec. 13, 2017, 10:13 a.m.</div>
<pre class="content">
On Wed, Sep 13, 2017 at 02:08:58AM +0000, Aneesh Kumar K.V wrote:
<span class="quote">&gt; @@ -2011,6 +2036,8 @@ static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  			if (soft_dirty)</span>
<span class="quote">&gt;  				entry = pte_mksoft_dirty(entry);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +		if (dirty)</span>
<span class="quote">&gt; +			SetPageDirty(page + i);</span>
<span class="quote">&gt;  		pte = pte_offset_map(&amp;_pmd, addr);</span>
<span class="quote">&gt;  		BUG_ON(!pte_none(*pte));</span>
<span class="quote">&gt;  		set_pte_at(mm, addr, pte, entry);</span>

The patch is fine. But we don&#39;t need to set every 4k dirty. We have single
dirty bit for whole THP. I&#39;ll change this part and sent the patch as part
of the series.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/hash-4k.h b/arch/powerpc/include/asm/book3s/64/hash-4k.h</span>
<span class="p_header">index d65dcb5826ff..2416edb74d28 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/hash-4k.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/hash-4k.h</span>
<span class="p_chunk">@@ -112,8 +112,6 @@</span> <span class="p_context"> extern pmd_t hash__pmdp_collapse_flush(struct vm_area_struct *vma,</span>
 extern void hash__pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 					 pgtable_t pgtable);
 extern pgtable_t hash__pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp);
<span class="p_del">-extern void hash__pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-				      unsigned long address, pmd_t *pmdp);</span>
 extern pmd_t hash__pmdp_huge_get_and_clear(struct mm_struct *mm,
 				       unsigned long addr, pmd_t *pmdp);
 extern int hash__has_transparent_hugepage(void);
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/hash-64k.h b/arch/powerpc/include/asm/book3s/64/hash-64k.h</span>
<span class="p_header">index ab36323b8a3e..001202cabedf 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/hash-64k.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/hash-64k.h</span>
<span class="p_chunk">@@ -162,8 +162,6 @@</span> <span class="p_context"> extern pmd_t hash__pmdp_collapse_flush(struct vm_area_struct *vma,</span>
 extern void hash__pgtable_trans_huge_deposit(struct mm_struct *mm, pmd_t *pmdp,
 					 pgtable_t pgtable);
 extern pgtable_t hash__pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp);
<span class="p_del">-extern void hash__pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-				      unsigned long address, pmd_t *pmdp);</span>
 extern pmd_t hash__pmdp_huge_get_and_clear(struct mm_struct *mm,
 				       unsigned long addr, pmd_t *pmdp);
 extern int hash__has_transparent_hugepage(void);
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/pgtable.h b/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="p_header">index 6cf53dc70efc..fee01ffe3b60 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/pgtable.h</span>
<span class="p_chunk">@@ -1114,15 +1114,6 @@</span> <span class="p_context"> static inline pgtable_t pgtable_trans_huge_withdraw(struct mm_struct *mm,</span>
 extern pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,
 			     pmd_t *pmdp);
 
<span class="p_del">-#define __HAVE_ARCH_PMDP_HUGE_SPLIT_PREPARE</span>
<span class="p_del">-static inline void pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long address, pmd_t *pmdp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (radix_enabled())</span>
<span class="p_del">-		return radix__pmdp_huge_split_prepare(vma, address, pmdp);</span>
<span class="p_del">-	return hash__pmdp_huge_split_prepare(vma, address, pmdp);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #define pmd_move_must_withdraw pmd_move_must_withdraw
 struct spinlock;
 static inline int pmd_move_must_withdraw(struct spinlock *new_pmd_ptl,
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/radix.h b/arch/powerpc/include/asm/book3s/64/radix.h</span>
<span class="p_header">index f5ece365d929..389be8b6c9f7 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/radix.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/radix.h</span>
<span class="p_chunk">@@ -272,12 +272,6 @@</span> <span class="p_context"> static inline pmd_t radix__pmd_mkhuge(pmd_t pmd)</span>
 		return __pmd(pmd_val(pmd) | _PAGE_PTE | R_PAGE_LARGE);
 	return __pmd(pmd_val(pmd) | _PAGE_PTE);
 }
<span class="p_del">-static inline void radix__pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-					    unsigned long address, pmd_t *pmdp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/* Nothing to do for radix. */</span>
<span class="p_del">-	return;</span>
<span class="p_del">-}</span>
 
 extern unsigned long radix__pmd_hugepage_update(struct mm_struct *mm, unsigned long addr,
 					  pmd_t *pmdp, unsigned long clr,
<span class="p_header">diff --git a/arch/powerpc/mm/pgtable-hash64.c b/arch/powerpc/mm/pgtable-hash64.c</span>
<span class="p_header">index ec277913e01b..469808e77e58 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/pgtable-hash64.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/pgtable-hash64.c</span>
<span class="p_chunk">@@ -296,28 +296,6 @@</span> <span class="p_context"> pgtable_t hash__pgtable_trans_huge_withdraw(struct mm_struct *mm, pmd_t *pmdp)</span>
 	return pgtable;
 }
 
<span class="p_del">-void hash__pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-			       unsigned long address, pmd_t *pmdp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="p_del">-	VM_BUG_ON(REGION_ID(address) != USER_REGION_ID);</span>
<span class="p_del">-	VM_BUG_ON(pmd_devmap(*pmdp));</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We can&#39;t mark the pmd none here, because that will cause a race</span>
<span class="p_del">-	 * against exit_mmap. We need to continue mark pmd TRANS HUGE, while</span>
<span class="p_del">-	 * we spilt, but at the same time we wan&#39;t rest of the ppc64 code</span>
<span class="p_del">-	 * not to insert hash pte on this, because we will be modifying</span>
<span class="p_del">-	 * the deposited pgtable in the caller of this function. Hence</span>
<span class="p_del">-	 * clear the _PAGE_USER so that we move the fault handling to</span>
<span class="p_del">-	 * higher level function and that will serialize against ptl.</span>
<span class="p_del">-	 * We need to flush existing hash pte entries here even though,</span>
<span class="p_del">-	 * the translation is still valid, because we will withdraw</span>
<span class="p_del">-	 * pgtable_t after this.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	pmd_hugepage_update(vma-&gt;vm_mm, address, pmdp, 0, _PAGE_PRIVILEGED);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /*
  * A linux hugepage PMD was changed and the corresponding hash table entries
  * neesd to be flushed.
<span class="p_header">diff --git a/include/asm-generic/pgtable.h b/include/asm-generic/pgtable.h</span>
<span class="p_header">index ece5e399567a..b934e41277ac 100644</span>
<span class="p_header">--- a/include/asm-generic/pgtable.h</span>
<span class="p_header">+++ b/include/asm-generic/pgtable.h</span>
<span class="p_chunk">@@ -313,14 +313,6 @@</span> <span class="p_context"> extern pmd_t pmdp_invalidate(struct vm_area_struct *vma, unsigned long address,</span>
 			    pmd_t *pmdp);
 #endif
 
<span class="p_del">-#ifndef __HAVE_ARCH_PMDP_HUGE_SPLIT_PREPARE</span>
<span class="p_del">-static inline void pmdp_huge_split_prepare(struct vm_area_struct *vma,</span>
<span class="p_del">-					   unsigned long address, pmd_t *pmdp)</span>
<span class="p_del">-{</span>
<span class="p_del">-</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 #ifndef __HAVE_ARCH_PTE_SAME
 static inline int pte_same(pte_t pte_a, pte_t pte_b)
 {
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index d72c2d20e9c6..59ec8c916368 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1944,8 +1944,8 @@</span> <span class="p_context"> static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct page *page;
 	pgtable_t pgtable;
<span class="p_del">-	pmd_t old, _pmd;</span>
<span class="p_del">-	bool young, write, soft_dirty;</span>
<span class="p_add">+	pmd_t old_pmd, _pmd;</span>
<span class="p_add">+	bool young, write, dirty, soft_dirty;</span>
 	unsigned long addr;
 	int i;
 
<span class="p_chunk">@@ -1977,14 +1977,39 @@</span> <span class="p_context"> static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 		return __split_huge_zero_page_pmd(vma, haddr, pmd);
 	}
 
<span class="p_del">-	page = pmd_page(*pmd);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Up to this point the pmd is present and huge and userland has the</span>
<span class="p_add">+	 * whole access to the hugepage during the split (which happens in</span>
<span class="p_add">+	 * place). If we overwrite the pmd with the not-huge version pointing</span>
<span class="p_add">+	 * to the pte here (which of course we could if all CPUs were bug</span>
<span class="p_add">+	 * free), userland could trigger a small page size TLB miss on the</span>
<span class="p_add">+	 * small sized TLB while the hugepage TLB entry is still established in</span>
<span class="p_add">+	 * the huge TLB. Some CPU doesn&#39;t like that.</span>
<span class="p_add">+	 * See http://support.amd.com/us/Processor_TechDocs/41322.pdf, Erratum</span>
<span class="p_add">+	 * 383 on page 93. Intel should be safe but is also warns that it&#39;s</span>
<span class="p_add">+	 * only safe if the permission and cache attributes of the two entries</span>
<span class="p_add">+	 * loaded in the two TLB is identical (which should be the case here).</span>
<span class="p_add">+	 * But it is generally safer to never allow small and huge TLB entries</span>
<span class="p_add">+	 * for the same virtual address to be loaded simultaneously. So instead</span>
<span class="p_add">+	 * of doing &quot;pmd_populate(); flush_pmd_tlb_range();&quot; we first mark the</span>
<span class="p_add">+	 * current pmd notpresent (atomically because here the pmd_trans_huge</span>
<span class="p_add">+	 * and pmd_trans_splitting must remain set at all times on the pmd</span>
<span class="p_add">+	 * until the split is complete for this pmd), then we flush the SMP TLB</span>
<span class="p_add">+	 * and finally we write the non-huge version of the pmd entry with</span>
<span class="p_add">+	 * pmd_populate.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	old_pmd = pmdp_invalidate(vma, haddr, pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+	page = pmd_page(old_pmd);</span>
 	VM_BUG_ON_PAGE(!page_count(page), page);
 	page_ref_add(page, HPAGE_PMD_NR - 1);
<span class="p_del">-	write = pmd_write(*pmd);</span>
<span class="p_del">-	young = pmd_young(*pmd);</span>
<span class="p_del">-	soft_dirty = pmd_soft_dirty(*pmd);</span>
<span class="p_del">-</span>
<span class="p_del">-	pmdp_huge_split_prepare(vma, haddr, pmd);</span>
<span class="p_add">+	write = pmd_write(old_pmd);</span>
<span class="p_add">+	young = pmd_young(old_pmd);</span>
<span class="p_add">+	dirty = pmd_dirty(old_pmd);</span>
<span class="p_add">+	soft_dirty = pmd_soft_dirty(old_pmd);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * withdraw the table only after we mark the pmd entry invalid</span>
<span class="p_add">+	 */</span>
 	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
 	pmd_populate(mm, &amp;_pmd, pgtable);
 
<span class="p_chunk">@@ -2011,6 +2036,8 @@</span> <span class="p_context"> static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 			if (soft_dirty)
 				entry = pte_mksoft_dirty(entry);
 		}
<span class="p_add">+		if (dirty)</span>
<span class="p_add">+			SetPageDirty(page + i);</span>
 		pte = pte_offset_map(&amp;_pmd, addr);
 		BUG_ON(!pte_none(*pte));
 		set_pte_at(mm, addr, pte, entry);
<span class="p_chunk">@@ -2038,36 +2065,6 @@</span> <span class="p_context"> static void __split_huge_pmd_locked(struct vm_area_struct *vma, pmd_t *pmd,</span>
 	}
 
 	smp_wmb(); /* make pte visible before pmd */
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Up to this point the pmd is present and huge and userland has the</span>
<span class="p_del">-	 * whole access to the hugepage during the split (which happens in</span>
<span class="p_del">-	 * place). If we overwrite the pmd with the not-huge version pointing</span>
<span class="p_del">-	 * to the pte here (which of course we could if all CPUs were bug</span>
<span class="p_del">-	 * free), userland could trigger a small page size TLB miss on the</span>
<span class="p_del">-	 * small sized TLB while the hugepage TLB entry is still established in</span>
<span class="p_del">-	 * the huge TLB. Some CPU doesn&#39;t like that.</span>
<span class="p_del">-	 * See http://support.amd.com/us/Processor_TechDocs/41322.pdf, Erratum</span>
<span class="p_del">-	 * 383 on page 93. Intel should be safe but is also warns that it&#39;s</span>
<span class="p_del">-	 * only safe if the permission and cache attributes of the two entries</span>
<span class="p_del">-	 * loaded in the two TLB is identical (which should be the case here).</span>
<span class="p_del">-	 * But it is generally safer to never allow small and huge TLB entries</span>
<span class="p_del">-	 * for the same virtual address to be loaded simultaneously. So instead</span>
<span class="p_del">-	 * of doing &quot;pmd_populate(); flush_pmd_tlb_range();&quot; we first mark the</span>
<span class="p_del">-	 * current pmd notpresent (atomically because here the pmd_trans_huge</span>
<span class="p_del">-	 * and pmd_trans_splitting must remain set at all times on the pmd</span>
<span class="p_del">-	 * until the split is complete for this pmd), then we flush the SMP TLB</span>
<span class="p_del">-	 * and finally we write the non-huge version of the pmd entry with</span>
<span class="p_del">-	 * pmd_populate.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	old = pmdp_invalidate(vma, haddr, pmd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Transfer dirty bit using value returned by pmd_invalidate() to be</span>
<span class="p_del">-	 * sure we don&#39;t race with CPU that can set the bit under us.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (pmd_dirty(old))</span>
<span class="p_del">-		SetPageDirty(page);</span>
<span class="p_del">-</span>
 	pmd_populate(mm, pmd, pgtable);
 
 	if (freeze) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



