
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm/hugetlb: Unmap pages if page fault raced with hole punch - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm/hugetlb: Unmap pages if page fault raced with hole punch</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 29, 2015, 10:33 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1446158038-25815-1-git-send-email-mike.kravetz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7521681/mbox/"
   >mbox</a>
|
   <a href="/patch/7521681/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7521681/">/patch/7521681/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 60FBC9F36A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 29 Oct 2015 22:35:08 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 4056B20789
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 29 Oct 2015 22:35:07 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 18140207BF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 29 Oct 2015 22:35:06 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757467AbbJ2We1 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 29 Oct 2015 18:34:27 -0400
Received: from userp1040.oracle.com ([156.151.31.81]:35870 &quot;EHLO
	userp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751917AbbJ2WeZ (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 29 Oct 2015 18:34:25 -0400
Received: from aserv0021.oracle.com (aserv0021.oracle.com [141.146.126.233])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2)
	with ESMTP id t9TMYGvd012365
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Thu, 29 Oct 2015 22:34:16 GMT
Received: from userv0122.oracle.com (userv0122.oracle.com [156.151.31.75])
	by aserv0021.oracle.com (8.13.8/8.13.8) with ESMTP id t9TMYFZg009996
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL); 
	Thu, 29 Oct 2015 22:34:15 GMT
Received: from abhmp0007.oracle.com (abhmp0007.oracle.com [141.146.116.13])
	by userv0122.oracle.com (8.13.8/8.13.8) with ESMTP id
	t9TMYE0h014332; Thu, 29 Oct 2015 22:34:15 GMT
Received: from monkey.oracle.com (/50.53.81.168)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Thu, 29 Oct 2015 15:34:14 -0700
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Hugh Dickins &lt;hughd@google.com&gt;, Davidlohr Bueso &lt;dave@stgolabs.net&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Subject: [PATCH] mm/hugetlb: Unmap pages if page fault raced with hole punch
Date: Thu, 29 Oct 2015 15:33:58 -0700
Message-Id: &lt;1446158038-25815-1-git-send-email-mike.kravetz@oracle.com&gt;
X-Mailer: git-send-email 2.4.3
X-Source-IP: aserv0021.oracle.com [141.146.126.233]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Oct. 29, 2015, 10:33 p.m.</div>
<pre class="content">
This patch is a combination of:
[PATCH v2 4/4] mm/hugetlb: Unmap pages to remove if page fault raced
	with hole punch  and,
[PATCH] mm/hugetlb: i_mmap_lock_write before unmapping in
	remove_inode_hugepages
This patch can replace the entire series:
[PATCH v2 0/4] hugetlbfs fallocate hole punch race with page faults
	and
[PATCH] mm/hugetlb: i_mmap_lock_write before unmapping in
	remove_inode_hugepages
It is being provided in an effort to possibly make tree management easier.

Page faults can race with fallocate hole punch.  If a page fault happens
between the unmap and remove operations, the page is not removed and
remains within the hole.  This is not the desired behavior.

If this race is detected and a page is mapped, the remove operation
(remove_inode_hugepages) will unmap the page before removing.  The unmap
within remove_inode_hugepages occurs with the hugetlb_fault_mutex held
so that no other faults can occur until the page is removed.

The (unmodified) routine hugetlb_vmdelete_list was moved ahead of
remove_inode_hugepages to satisfy the new reference.
<span class="signed-off-by">
Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
---
 fs/hugetlbfs/inode.c | 125 ++++++++++++++++++++++++++-------------------------
 1 file changed, 65 insertions(+), 60 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7851">Hugh Dickins</a> - Oct. 30, 2015, 3:32 a.m.</div>
<pre class="content">
On Thu, 29 Oct 2015, Mike Kravetz wrote:
<span class="quote">
&gt; This patch is a combination of:</span>
<span class="quote">&gt; [PATCH v2 4/4] mm/hugetlb: Unmap pages to remove if page fault raced</span>
<span class="quote">&gt; 	with hole punch  and,</span>
<span class="quote">&gt; [PATCH] mm/hugetlb: i_mmap_lock_write before unmapping in</span>
<span class="quote">&gt; 	remove_inode_hugepages</span>
<span class="quote">&gt; This patch can replace the entire series:</span>
<span class="quote">&gt; [PATCH v2 0/4] hugetlbfs fallocate hole punch race with page faults</span>
<span class="quote">&gt; 	and</span>
<span class="quote">&gt; [PATCH] mm/hugetlb: i_mmap_lock_write before unmapping in</span>
<span class="quote">&gt; 	remove_inode_hugepages</span>
<span class="quote">&gt; It is being provided in an effort to possibly make tree management easier.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Page faults can race with fallocate hole punch.  If a page fault happens</span>
<span class="quote">&gt; between the unmap and remove operations, the page is not removed and</span>
<span class="quote">&gt; remains within the hole.  This is not the desired behavior.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If this race is detected and a page is mapped, the remove operation</span>
<span class="quote">&gt; (remove_inode_hugepages) will unmap the page before removing.  The unmap</span>
<span class="quote">&gt; within remove_inode_hugepages occurs with the hugetlb_fault_mutex held</span>
<span class="quote">&gt; so that no other faults can occur until the page is removed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The (unmodified) routine hugetlb_vmdelete_list was moved ahead of</span>
<span class="quote">&gt; remove_inode_hugepages to satisfy the new reference.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>

Sorry, I came here to give this a quick Ack, but find I cannot:
you&#39;re adding to the remove_inode_hugepages() loop (heading towards
4.3 final), but its use of &quot;next&quot; looks wrong to me already.

Doesn&#39;t &quot;next&quot; need to be assigned from page-&gt;index much earlier?
If there&#39;s a hole in the file (which there very well might be, since
you&#39;ve just implemented holepunch!), doesn&#39;t it do the wrong thing?

And the loop itself is a bit weird, though that probably doesn&#39;t
matter very much: I said before, seeing the &quot;while (next &lt; end)&quot;,
that it&#39;s a straightforward scan from start to end, and sometimes
it would work that way; but buried inside is &quot;next = start; continue;&quot;
from a contrasting &quot;pincer&quot; loop (which goes back to squeeze every
page out of the range, lest faults raced truncation or holepunch).
I know the originals in truncate.c or shmem.c are quite tricky,
but this being different again would take time to validate.

No cond_resched() either.

Hugh
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  fs/hugetlbfs/inode.c | 125 ++++++++++++++++++++++++++-------------------------</span>
<span class="quote">&gt;  1 file changed, 65 insertions(+), 60 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; index 316adb9..8b8e5e8 100644</span>
<span class="quote">&gt; --- a/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; +++ b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; @@ -324,11 +324,44 @@ static void remove_huge_page(struct page *page)</span>
<span class="quote">&gt;  	delete_from_page_cache(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline void</span>
<span class="quote">&gt; +hugetlb_vmdelete_list(struct rb_root *root, pgoff_t start, pgoff_t end)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_area_struct *vma;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * end == 0 indicates that the entire range after</span>
<span class="quote">&gt; +	 * start should be unmapped.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	vma_interval_tree_foreach(vma, root, start, end ? end : ULONG_MAX) {</span>
<span class="quote">&gt; +		unsigned long v_offset;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Can the expression below overflow on 32-bit arches?</span>
<span class="quote">&gt; +		 * No, because the interval tree returns us only those vmas</span>
<span class="quote">&gt; +		 * which overlap the truncated area starting at pgoff,</span>
<span class="quote">&gt; +		 * and no vma on a 32-bit arch can span beyond the 4GB.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (vma-&gt;vm_pgoff &lt; start)</span>
<span class="quote">&gt; +			v_offset = (start - vma-&gt;vm_pgoff) &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +			v_offset = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (end) {</span>
<span class="quote">&gt; +			end = ((end - start) &lt;&lt; PAGE_SHIFT) +</span>
<span class="quote">&gt; +			       vma-&gt;vm_start + v_offset;</span>
<span class="quote">&gt; +			if (end &gt; vma-&gt;vm_end)</span>
<span class="quote">&gt; +				end = vma-&gt;vm_end;</span>
<span class="quote">&gt; +		} else</span>
<span class="quote">&gt; +			end = vma-&gt;vm_end;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		unmap_hugepage_range(vma, vma-&gt;vm_start + v_offset, end, NULL);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * remove_inode_hugepages handles two distinct cases: truncation and hole</span>
<span class="quote">&gt;   * punch.  There are subtle differences in operation for each case.</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;   * truncation is indicated by end of range being LLONG_MAX</span>
<span class="quote">&gt;   *	In this case, we first scan the range and release found pages.</span>
<span class="quote">&gt;   *	After releasing pages, hugetlb_unreserve_pages cleans up region/reserv</span>
<span class="quote">&gt; @@ -381,12 +414,27 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
<span class="quote">&gt;  		for (i = 0; i &lt; pagevec_count(&amp;pvec); ++i) {</span>
<span class="quote">&gt;  			struct page *page = pvec.pages[i];</span>
<span class="quote">&gt;  			u32 hash;</span>
<span class="quote">&gt; +			bool rsv_on_error;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			hash = hugetlb_fault_mutex_hash(h, current-&gt;mm,</span>
<span class="quote">&gt;  							&amp;pseudo_vma,</span>
<span class="quote">&gt;  							mapping, next, 0);</span>
<span class="quote">&gt;  			mutex_lock(&amp;hugetlb_fault_mutex_table[hash]);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * If page is mapped, it was faulted in after being</span>
<span class="quote">&gt; +			 * unmapped in caller.  Unmap (again) now after taking</span>
<span class="quote">&gt; +			 * the fault mutex.  The mutex will prevent faults</span>
<span class="quote">&gt; +			 * until we finish removing the page.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (page_mapped(page)) {</span>
<span class="quote">&gt; +				i_mmap_lock_write(mapping);</span>
<span class="quote">&gt; +				hugetlb_vmdelete_list(&amp;mapping-&gt;i_mmap,</span>
<span class="quote">&gt; +					next * pages_per_huge_page(h),</span>
<span class="quote">&gt; +					(next + 1) * pages_per_huge_page(h));</span>
<span class="quote">&gt; +				i_mmap_unlock_write(mapping);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  			lock_page(page);</span>
<span class="quote">&gt;  			if (page-&gt;index &gt;= end) {</span>
<span class="quote">&gt;  				unlock_page(page);</span>
<span class="quote">&gt; @@ -396,31 +444,23 @@ static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt; -			 * If page is mapped, it was faulted in after being</span>
<span class="quote">&gt; -			 * unmapped.  Do nothing in this race case.  In the</span>
<span class="quote">&gt; -			 * normal case page is not mapped.</span>
<span class="quote">&gt; +			 * We must free the huge page and remove from page</span>
<span class="quote">&gt; +			 * cache (remove_huge_page) BEFORE removing the</span>
<span class="quote">&gt; +			 * region/reserve map (hugetlb_unreserve_pages).</span>
<span class="quote">&gt; +			 * In rare out of memory conditions, removal of the</span>
<span class="quote">&gt; +			 * region/reserve map could fail.  Before free&#39;ing</span>
<span class="quote">&gt; +			 * the page, note PagePrivate which is used in case</span>
<span class="quote">&gt; +			 * of error.</span>
<span class="quote">&gt;  			 */</span>
<span class="quote">&gt; -			if (!page_mapped(page)) {</span>
<span class="quote">&gt; -				bool rsv_on_error = !PagePrivate(page);</span>
<span class="quote">&gt; -				/*</span>
<span class="quote">&gt; -				 * We must free the huge page and remove</span>
<span class="quote">&gt; -				 * from page cache (remove_huge_page) BEFORE</span>
<span class="quote">&gt; -				 * removing the region/reserve map</span>
<span class="quote">&gt; -				 * (hugetlb_unreserve_pages).  In rare out</span>
<span class="quote">&gt; -				 * of memory conditions, removal of the</span>
<span class="quote">&gt; -				 * region/reserve map could fail.  Before</span>
<span class="quote">&gt; -				 * free&#39;ing the page, note PagePrivate which</span>
<span class="quote">&gt; -				 * is used in case of error.</span>
<span class="quote">&gt; -				 */</span>
<span class="quote">&gt; -				remove_huge_page(page);</span>
<span class="quote">&gt; -				freed++;</span>
<span class="quote">&gt; -				if (!truncate_op) {</span>
<span class="quote">&gt; -					if (unlikely(hugetlb_unreserve_pages(</span>
<span class="quote">&gt; -							inode, next,</span>
<span class="quote">&gt; -							next + 1, 1)))</span>
<span class="quote">&gt; -						hugetlb_fix_reserve_counts(</span>
<span class="quote">&gt; -							inode, rsv_on_error);</span>
<span class="quote">&gt; -				}</span>
<span class="quote">&gt; +			rsv_on_error = !PagePrivate(page);</span>
<span class="quote">&gt; +			remove_huge_page(page);</span>
<span class="quote">&gt; +			freed++;</span>
<span class="quote">&gt; +			if (!truncate_op) {</span>
<span class="quote">&gt; +				if (unlikely(hugetlb_unreserve_pages(inode,</span>
<span class="quote">&gt; +								next, next + 1,</span>
<span class="quote">&gt; +								1)))</span>
<span class="quote">&gt; +					hugetlb_fix_reserve_counts(inode,</span>
<span class="quote">&gt; +								rsv_on_error);</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			if (page-&gt;index &gt; next)</span>
<span class="quote">&gt; @@ -450,41 +490,6 @@ static void hugetlbfs_evict_inode(struct inode *inode)</span>
<span class="quote">&gt;  	clear_inode(inode);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static inline void</span>
<span class="quote">&gt; -hugetlb_vmdelete_list(struct rb_root *root, pgoff_t start, pgoff_t end)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	struct vm_area_struct *vma;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * end == 0 indicates that the entire range after</span>
<span class="quote">&gt; -	 * start should be unmapped.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	vma_interval_tree_foreach(vma, root, start, end ? end : ULONG_MAX) {</span>
<span class="quote">&gt; -		unsigned long v_offset;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * Can the expression below overflow on 32-bit arches?</span>
<span class="quote">&gt; -		 * No, because the interval tree returns us only those vmas</span>
<span class="quote">&gt; -		 * which overlap the truncated area starting at pgoff,</span>
<span class="quote">&gt; -		 * and no vma on a 32-bit arch can span beyond the 4GB.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		if (vma-&gt;vm_pgoff &lt; start)</span>
<span class="quote">&gt; -			v_offset = (start - vma-&gt;vm_pgoff) &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; -		else</span>
<span class="quote">&gt; -			v_offset = 0;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		if (end) {</span>
<span class="quote">&gt; -			end = ((end - start) &lt;&lt; PAGE_SHIFT) +</span>
<span class="quote">&gt; -			       vma-&gt;vm_start + v_offset;</span>
<span class="quote">&gt; -			if (end &gt; vma-&gt;vm_end)</span>
<span class="quote">&gt; -				end = vma-&gt;vm_end;</span>
<span class="quote">&gt; -		} else</span>
<span class="quote">&gt; -			end = vma-&gt;vm_end;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		unmap_hugepage_range(vma, vma-&gt;vm_start + v_offset, end, NULL);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pgoff_t pgoff;</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.4.3</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Oct. 30, 2015, 4:45 p.m.</div>
<pre class="content">
On 10/29/2015 08:32 PM, Hugh Dickins wrote:
<span class="quote">&gt; On Thu, 29 Oct 2015, Mike Kravetz wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; This patch is a combination of:</span>
<span class="quote">&gt;&gt; [PATCH v2 4/4] mm/hugetlb: Unmap pages to remove if page fault raced</span>
<span class="quote">&gt;&gt; 	with hole punch  and,</span>
<span class="quote">&gt;&gt; [PATCH] mm/hugetlb: i_mmap_lock_write before unmapping in</span>
<span class="quote">&gt;&gt; 	remove_inode_hugepages</span>
<span class="quote">&gt;&gt; This patch can replace the entire series:</span>
<span class="quote">&gt;&gt; [PATCH v2 0/4] hugetlbfs fallocate hole punch race with page faults</span>
<span class="quote">&gt;&gt; 	and</span>
<span class="quote">&gt;&gt; [PATCH] mm/hugetlb: i_mmap_lock_write before unmapping in</span>
<span class="quote">&gt;&gt; 	remove_inode_hugepages</span>
<span class="quote">&gt;&gt; It is being provided in an effort to possibly make tree management easier.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Page faults can race with fallocate hole punch.  If a page fault happens</span>
<span class="quote">&gt;&gt; between the unmap and remove operations, the page is not removed and</span>
<span class="quote">&gt;&gt; remains within the hole.  This is not the desired behavior.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; If this race is detected and a page is mapped, the remove operation</span>
<span class="quote">&gt;&gt; (remove_inode_hugepages) will unmap the page before removing.  The unmap</span>
<span class="quote">&gt;&gt; within remove_inode_hugepages occurs with the hugetlb_fault_mutex held</span>
<span class="quote">&gt;&gt; so that no other faults can occur until the page is removed.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The (unmodified) routine hugetlb_vmdelete_list was moved ahead of</span>
<span class="quote">&gt;&gt; remove_inode_hugepages to satisfy the new reference.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sorry, I came here to give this a quick Ack, but find I cannot:</span>
<span class="quote">&gt; you&#39;re adding to the remove_inode_hugepages() loop (heading towards</span>
<span class="quote">&gt; 4.3 final), but its use of &quot;next&quot; looks wrong to me already.</span>

You are correct, the (current) code is wrong.

The hugetlbfs fallocate code started with shmem as an example.  Some
of the complexities of that code are not needed in hugetlbfs.  However,
some remnants were left.

I&#39;ll create a patch to fix the existing code, then when that is acceptable
refactor this patch.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Doesn&#39;t &quot;next&quot; need to be assigned from page-&gt;index much earlier?</span>
<span class="quote">&gt; If there&#39;s a hole in the file (which there very well might be, since</span>
<span class="quote">&gt; you&#39;ve just implemented holepunch!), doesn&#39;t it do the wrong thing?</span>

Yes, I think it will.
<span class="quote">
&gt; </span>
<span class="quote">&gt; And the loop itself is a bit weird, though that probably doesn&#39;t</span>
<span class="quote">&gt; matter very much: I said before, seeing the &quot;while (next &lt; end)&quot;,</span>
<span class="quote">&gt; that it&#39;s a straightforward scan from start to end, and sometimes</span>
<span class="quote">&gt; it would work that way; but buried inside is &quot;next = start; continue;&quot;</span>

Correct, that next = start should not be there.

Thanks
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Oct. 30, 2015, 8:56 p.m.</div>
<pre class="content">
On 10/30/2015 09:45 AM, Mike Kravetz wrote:
<span class="quote">&gt; On 10/29/2015 08:32 PM, Hugh Dickins wrote:</span>
<span class="quote">&gt;&gt; On Thu, 29 Oct 2015, Mike Kravetz wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This patch is a combination of:</span>
<span class="quote">&gt;&gt;&gt; [PATCH v2 4/4] mm/hugetlb: Unmap pages to remove if page fault raced</span>
<span class="quote">&gt;&gt;&gt; 	with hole punch  and,</span>
<span class="quote">&gt;&gt;&gt; [PATCH] mm/hugetlb: i_mmap_lock_write before unmapping in</span>
<span class="quote">&gt;&gt;&gt; 	remove_inode_hugepages</span>
<span class="quote">&gt;&gt;&gt; This patch can replace the entire series:</span>
<span class="quote">&gt;&gt;&gt; [PATCH v2 0/4] hugetlbfs fallocate hole punch race with page faults</span>
<span class="quote">&gt;&gt;&gt; 	and</span>
<span class="quote">&gt;&gt;&gt; [PATCH] mm/hugetlb: i_mmap_lock_write before unmapping in</span>
<span class="quote">&gt;&gt;&gt; 	remove_inode_hugepages</span>
<span class="quote">&gt;&gt;&gt; It is being provided in an effort to possibly make tree management easier.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Page faults can race with fallocate hole punch.  If a page fault happens</span>
<span class="quote">&gt;&gt;&gt; between the unmap and remove operations, the page is not removed and</span>
<span class="quote">&gt;&gt;&gt; remains within the hole.  This is not the desired behavior.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; If this race is detected and a page is mapped, the remove operation</span>
<span class="quote">&gt;&gt;&gt; (remove_inode_hugepages) will unmap the page before removing.  The unmap</span>
<span class="quote">&gt;&gt;&gt; within remove_inode_hugepages occurs with the hugetlb_fault_mutex held</span>
<span class="quote">&gt;&gt;&gt; so that no other faults can occur until the page is removed.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The (unmodified) routine hugetlb_vmdelete_list was moved ahead of</span>
<span class="quote">&gt;&gt;&gt; remove_inode_hugepages to satisfy the new reference.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Sorry, I came here to give this a quick Ack, but find I cannot:</span>
<span class="quote">&gt;&gt; you&#39;re adding to the remove_inode_hugepages() loop (heading towards</span>
<span class="quote">&gt;&gt; 4.3 final), but its use of &quot;next&quot; looks wrong to me already.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You are correct, the (current) code is wrong.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The hugetlbfs fallocate code started with shmem as an example.  Some</span>
<span class="quote">&gt; of the complexities of that code are not needed in hugetlbfs.  However,</span>
<span class="quote">&gt; some remnants were left.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ll create a patch to fix the existing code, then when that is acceptable</span>
<span class="quote">&gt; refactor this patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Doesn&#39;t &quot;next&quot; need to be assigned from page-&gt;index much earlier?</span>
<span class="quote">&gt;&gt; If there&#39;s a hole in the file (which there very well might be, since</span>
<span class="quote">&gt;&gt; you&#39;ve just implemented holepunch!), doesn&#39;t it do the wrong thing?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, I think it will.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; And the loop itself is a bit weird, though that probably doesn&#39;t</span>
<span class="quote">&gt;&gt; matter very much: I said before, seeing the &quot;while (next &lt; end)&quot;,</span>
<span class="quote">&gt;&gt; that it&#39;s a straightforward scan from start to end, and sometimes</span>
<span class="quote">&gt;&gt; it would work that way; but buried inside is &quot;next = start; continue;&quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Correct, that next = start should not be there.</span>

The &#39;next = start&#39; code is actually from the original truncate_hugepages
routine.  This functionality was combined with that needed for hole punch
to create remove_inode_hugepages().

The following code was in truncate_hugepages:

	next = start;
	while (1) {
		if (!pagevec_lookup(&amp;pvec, mapping, next, PAGEVEC_SIZE)) {
			if (next == start)
				break;
			next = start;
			continue;
		}


So, in the truncate case pages starting at &#39;start&#39; are deleted until
pagevec_lookup fails.  Then, we call pagevec_lookup() again.  If no
pages are found we are done.  Else, we repeat the whole process.

Does anyone recall the reason for going back and looking for pages at
index&#39;es already deleted?  Git doesn&#39;t help as that was part of initial
commit.  My thought is that truncate can race with page faults.  The
truncate code sets inode offset before unmapping and deleting pages.
So, faults after the new offset is set should fail.  But, I suppose a
fault could race with setting offset and deleting of pages.  Does this
sound right?  Or, is there some other reason I am missing?

I would like to continue having remove_inode_hugepages handle both the
truncate and hole punch case.  So, what to make sure the code correctly
handles both cases.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7851">Hugh Dickins</a> - Nov. 9, 2015, 7:42 a.m.</div>
<pre class="content">
On Fri, 30 Oct 2015, Mike Kravetz wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; The &#39;next = start&#39; code is actually from the original truncate_hugepages</span>
<span class="quote">&gt; routine.  This functionality was combined with that needed for hole punch</span>
<span class="quote">&gt; to create remove_inode_hugepages().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The following code was in truncate_hugepages:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	next = start;</span>
<span class="quote">&gt; 	while (1) {</span>
<span class="quote">&gt; 		if (!pagevec_lookup(&amp;pvec, mapping, next, PAGEVEC_SIZE)) {</span>
<span class="quote">&gt; 			if (next == start)</span>
<span class="quote">&gt; 				break;</span>
<span class="quote">&gt; 			next = start;</span>
<span class="quote">&gt; 			continue;</span>
<span class="quote">&gt; 		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So, in the truncate case pages starting at &#39;start&#39; are deleted until</span>
<span class="quote">&gt; pagevec_lookup fails.  Then, we call pagevec_lookup() again.  If no</span>
<span class="quote">&gt; pages are found we are done.  Else, we repeat the whole process.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Does anyone recall the reason for going back and looking for pages at</span>
<span class="quote">&gt; index&#39;es already deleted?  Git doesn&#39;t help as that was part of initial</span>
<span class="quote">&gt; commit.  My thought is that truncate can race with page faults.  The</span>
<span class="quote">&gt; truncate code sets inode offset before unmapping and deleting pages.</span>
<span class="quote">&gt; So, faults after the new offset is set should fail.  But, I suppose a</span>
<span class="quote">&gt; fault could race with setting offset and deleting of pages.  Does this</span>
<span class="quote">&gt; sound right?  Or, is there some other reason I am missing?</span>

I believe your thinking is correct.  But remember that
truncate_inode_pages_range() is shared by almost all filesystems,
and different filesystems have different internal locking conventions,
and different propensities to such a race: it&#39;s trying to cover for
all of them.

Typically, writing is well serialized (by i_mutex) against truncation,
but faulting (like reading) sails through without enough of a lock.
We resort to i_size checks to avoid the worst of it, but there&#39;s often
a corner or two in which those checks are not quite good enough -
it&#39;s easy to check i_size at the beginning, but it needs to be checked
again at the end too, and what&#39;s been done undone - can be awkward.

I hope that in the case of hugetlbfs, since you already have the
additional fault_mutex to handle races between faults and punching,
it should be possible to get away without that &quot;pincer&quot; restarting.

Hugh
<span class="quote">
&gt; </span>
<span class="quote">&gt; I would like to continue having remove_inode_hugepages handle both the</span>
<span class="quote">&gt; truncate and hole punch case.  So, what to make sure the code correctly</span>
<span class="quote">&gt; handles both cases.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; Mike Kravetz</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Nov. 9, 2015, 10:55 p.m.</div>
<pre class="content">
On 11/08/2015 11:42 PM, Hugh Dickins wrote:
<span class="quote">&gt; On Fri, 30 Oct 2015, Mike Kravetz wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The &#39;next = start&#39; code is actually from the original truncate_hugepages</span>
<span class="quote">&gt;&gt; routine.  This functionality was combined with that needed for hole punch</span>
<span class="quote">&gt;&gt; to create remove_inode_hugepages().</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The following code was in truncate_hugepages:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; 	next = start;</span>
<span class="quote">&gt;&gt; 	while (1) {</span>
<span class="quote">&gt;&gt; 		if (!pagevec_lookup(&amp;pvec, mapping, next, PAGEVEC_SIZE)) {</span>
<span class="quote">&gt;&gt; 			if (next == start)</span>
<span class="quote">&gt;&gt; 				break;</span>
<span class="quote">&gt;&gt; 			next = start;</span>
<span class="quote">&gt;&gt; 			continue;</span>
<span class="quote">&gt;&gt; 		}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So, in the truncate case pages starting at &#39;start&#39; are deleted until</span>
<span class="quote">&gt;&gt; pagevec_lookup fails.  Then, we call pagevec_lookup() again.  If no</span>
<span class="quote">&gt;&gt; pages are found we are done.  Else, we repeat the whole process.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Does anyone recall the reason for going back and looking for pages at</span>
<span class="quote">&gt;&gt; index&#39;es already deleted?  Git doesn&#39;t help as that was part of initial</span>
<span class="quote">&gt;&gt; commit.  My thought is that truncate can race with page faults.  The</span>
<span class="quote">&gt;&gt; truncate code sets inode offset before unmapping and deleting pages.</span>
<span class="quote">&gt;&gt; So, faults after the new offset is set should fail.  But, I suppose a</span>
<span class="quote">&gt;&gt; fault could race with setting offset and deleting of pages.  Does this</span>
<span class="quote">&gt;&gt; sound right?  Or, is there some other reason I am missing?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I believe your thinking is correct.  But remember that</span>
<span class="quote">&gt; truncate_inode_pages_range() is shared by almost all filesystems,</span>
<span class="quote">&gt; and different filesystems have different internal locking conventions,</span>
<span class="quote">&gt; and different propensities to such a race: it&#39;s trying to cover for</span>
<span class="quote">&gt; all of them.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Typically, writing is well serialized (by i_mutex) against truncation,</span>
<span class="quote">&gt; but faulting (like reading) sails through without enough of a lock.</span>
<span class="quote">&gt; We resort to i_size checks to avoid the worst of it, but there&#39;s often</span>
<span class="quote">&gt; a corner or two in which those checks are not quite good enough -</span>
<span class="quote">&gt; it&#39;s easy to check i_size at the beginning, but it needs to be checked</span>
<span class="quote">&gt; again at the end too, and what&#39;s been done undone - can be awkward.</span>

Well, it looks like the hugetlb_no_page() routine is checking i_size both
before and after.  It appears to be doing the right thing to handle the
race, but I need to stare at the code some more to make sure.

Because of the way the truncate code went back and did an extra lookup
when done with the range, I assumed it was covering some race.  However,
that may not be the case.
<span class="quote">
&gt; </span>
<span class="quote">&gt; I hope that in the case of hugetlbfs, since you already have the</span>
<span class="quote">&gt; additional fault_mutex to handle races between faults and punching,</span>
<span class="quote">&gt; it should be possible to get away without that &quot;pincer&quot; restarting.</span>

Yes, it looks like this may work as a straight loop over the range of
pages.  I just need to study the code some more to make sure I am not
missing something.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Nov. 10, 2015, 10:41 p.m.</div>
<pre class="content">
On 11/09/2015 02:55 PM, Mike Kravetz wrote:
<span class="quote">&gt; On 11/08/2015 11:42 PM, Hugh Dickins wrote:</span>
<span class="quote">&gt;&gt; On Fri, 30 Oct 2015, Mike Kravetz wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The &#39;next = start&#39; code is actually from the original truncate_hugepages</span>
<span class="quote">&gt;&gt;&gt; routine.  This functionality was combined with that needed for hole punch</span>
<span class="quote">&gt;&gt;&gt; to create remove_inode_hugepages().</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The following code was in truncate_hugepages:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; 	next = start;</span>
<span class="quote">&gt;&gt;&gt; 	while (1) {</span>
<span class="quote">&gt;&gt;&gt; 		if (!pagevec_lookup(&amp;pvec, mapping, next, PAGEVEC_SIZE)) {</span>
<span class="quote">&gt;&gt;&gt; 			if (next == start)</span>
<span class="quote">&gt;&gt;&gt; 				break;</span>
<span class="quote">&gt;&gt;&gt; 			next = start;</span>
<span class="quote">&gt;&gt;&gt; 			continue;</span>
<span class="quote">&gt;&gt;&gt; 		}</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; So, in the truncate case pages starting at &#39;start&#39; are deleted until</span>
<span class="quote">&gt;&gt;&gt; pagevec_lookup fails.  Then, we call pagevec_lookup() again.  If no</span>
<span class="quote">&gt;&gt;&gt; pages are found we are done.  Else, we repeat the whole process.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Does anyone recall the reason for going back and looking for pages at</span>
<span class="quote">&gt;&gt;&gt; index&#39;es already deleted?  Git doesn&#39;t help as that was part of initial</span>
<span class="quote">&gt;&gt;&gt; commit.  My thought is that truncate can race with page faults.  The</span>
<span class="quote">&gt;&gt;&gt; truncate code sets inode offset before unmapping and deleting pages.</span>
<span class="quote">&gt;&gt;&gt; So, faults after the new offset is set should fail.  But, I suppose a</span>
<span class="quote">&gt;&gt;&gt; fault could race with setting offset and deleting of pages.  Does this</span>
<span class="quote">&gt;&gt;&gt; sound right?  Or, is there some other reason I am missing?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I believe your thinking is correct.  But remember that</span>
<span class="quote">&gt;&gt; truncate_inode_pages_range() is shared by almost all filesystems,</span>
<span class="quote">&gt;&gt; and different filesystems have different internal locking conventions,</span>
<span class="quote">&gt;&gt; and different propensities to such a race: it&#39;s trying to cover for</span>
<span class="quote">&gt;&gt; all of them.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Typically, writing is well serialized (by i_mutex) against truncation,</span>
<span class="quote">&gt;&gt; but faulting (like reading) sails through without enough of a lock.</span>
<span class="quote">&gt;&gt; We resort to i_size checks to avoid the worst of it, but there&#39;s often</span>
<span class="quote">&gt;&gt; a corner or two in which those checks are not quite good enough -</span>
<span class="quote">&gt;&gt; it&#39;s easy to check i_size at the beginning, but it needs to be checked</span>
<span class="quote">&gt;&gt; again at the end too, and what&#39;s been done undone - can be awkward.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well, it looks like the hugetlb_no_page() routine is checking i_size both</span>
<span class="quote">&gt; before and after.  It appears to be doing the right thing to handle the</span>
<span class="quote">&gt; race, but I need to stare at the code some more to make sure.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Because of the way the truncate code went back and did an extra lookup</span>
<span class="quote">&gt; when done with the range, I assumed it was covering some race.  However,</span>
<span class="quote">&gt; that may not be the case.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I hope that in the case of hugetlbfs, since you already have the</span>
<span class="quote">&gt;&gt; additional fault_mutex to handle races between faults and punching,</span>
<span class="quote">&gt;&gt; it should be possible to get away without that &quot;pincer&quot; restarting.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, it looks like this may work as a straight loop over the range of</span>
<span class="quote">&gt; pages.  I just need to study the code some more to make sure I am not</span>
<span class="quote">&gt; missing something.</span>

I have convinced myself that hugetlb_no_page is coded such that page
faults can not race with truncate.  hugetlb_no_page handles the case
where there is no PTE for a faulted in address.  The general flow in
hugetlb_no_page for the no page found case is:
- check index against i_size, end if beyond
- allocate huge page
- take page table lock for huge page
- check index against i_size again,  if beyond free page and return
- add huge page to page table
- unlock page table lock for huge page

The flow for the truncate operation in hugetlb_vmtruncate is:
- set i_size
- take inode/mapping write lock
- hugetlb_vmdelete_list() which removes page table entries.  The page
  table lock will be taken for each huge page in the range
- release inode/mapping write lock
- remove_inode_hugepages() to actually remove pages

The truncate/page fault race we are concerned with is if a page is faulted
in after hugetlb_vmtruncate sets i_size and unmaps the page, but before
actually removing the page.  Obviously, any entry into hugetlb_no_page
after i_size is set will check the value and not allow the fault.  In
addition, if the value of i_size is set before the second check in
hugetlb_no_page, it will do the right thing.  Therefore, the only place to
race is after the second i_size check in hugetlb_no_page.

Note that the second check for i_size is with the page table lock for
the huge page held.  It is not possible for hugetlb_vmtruncate to unmap
the huge page before the page fault completes, as it must acquire the page
table lock.  This is the same as a fault happening before the truncate
operation starts and is handled correctly by hugetlb_vmtruncate.

Another way to look at this is by asking the question, Is it possible to
fault on a page in the truncate range after it is unmapped by
hugetlb_vmtruncate/hugetlb_vmdelete_list?  To unmap a page,
hugetlb_vmtruncate will:
- set i_size
- take page table lock for huge page
- unmap page
- release page table lock for page

In order to fault in the page, it must take the same page table lock and
check i_size.  I do not know of any way for the faulting code to get an
old value for i_size.

Please let me know if my reasoning is incorrect.  I will code up a new
(simpler) version of remove_inode_hugepages with the assumption that
truncate can not race with page faults.

Also, I wrote a fairly simple test to have truncate race with page faults.
It was quite easy to hit the second check in hugetlb_no_page where it
notices index is beyond i_size and backs out of the fault.  Even after
adding delays in strategic locations of the fault and truncate code, I
could not cause a race as observed by remove_inode_hugepages.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7851">Hugh Dickins</a> - Nov. 14, 2015, 12:36 a.m.</div>
<pre class="content">
On Tue, 10 Nov 2015, Mike Kravetz wrote:
<span class="quote">&gt; On 11/09/2015 02:55 PM, Mike Kravetz wrote:</span>
<span class="quote">&gt; &gt; On 11/08/2015 11:42 PM, Hugh Dickins wrote:</span>
<span class="quote">&gt; &gt;&gt; On Fri, 30 Oct 2015, Mike Kravetz wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; The &#39;next = start&#39; code is actually from the original truncate_hugepages</span>
<span class="quote">&gt; &gt;&gt;&gt; routine.  This functionality was combined with that needed for hole punch</span>
<span class="quote">&gt; &gt;&gt;&gt; to create remove_inode_hugepages().</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; The following code was in truncate_hugepages:</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; 	next = start;</span>
<span class="quote">&gt; &gt;&gt;&gt; 	while (1) {</span>
<span class="quote">&gt; &gt;&gt;&gt; 		if (!pagevec_lookup(&amp;pvec, mapping, next, PAGEVEC_SIZE)) {</span>
<span class="quote">&gt; &gt;&gt;&gt; 			if (next == start)</span>
<span class="quote">&gt; &gt;&gt;&gt; 				break;</span>
<span class="quote">&gt; &gt;&gt;&gt; 			next = start;</span>
<span class="quote">&gt; &gt;&gt;&gt; 			continue;</span>
<span class="quote">&gt; &gt;&gt;&gt; 		}</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; So, in the truncate case pages starting at &#39;start&#39; are deleted until</span>
<span class="quote">&gt; &gt;&gt;&gt; pagevec_lookup fails.  Then, we call pagevec_lookup() again.  If no</span>
<span class="quote">&gt; &gt;&gt;&gt; pages are found we are done.  Else, we repeat the whole process.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Does anyone recall the reason for going back and looking for pages at</span>
<span class="quote">&gt; &gt;&gt;&gt; index&#39;es already deleted?  Git doesn&#39;t help as that was part of initial</span>
<span class="quote">&gt; &gt;&gt;&gt; commit.  My thought is that truncate can race with page faults.  The</span>
<span class="quote">&gt; &gt;&gt;&gt; truncate code sets inode offset before unmapping and deleting pages.</span>
<span class="quote">&gt; &gt;&gt;&gt; So, faults after the new offset is set should fail.  But, I suppose a</span>
<span class="quote">&gt; &gt;&gt;&gt; fault could race with setting offset and deleting of pages.  Does this</span>
<span class="quote">&gt; &gt;&gt;&gt; sound right?  Or, is there some other reason I am missing?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I believe your thinking is correct.  But remember that</span>
<span class="quote">&gt; &gt;&gt; truncate_inode_pages_range() is shared by almost all filesystems,</span>
<span class="quote">&gt; &gt;&gt; and different filesystems have different internal locking conventions,</span>
<span class="quote">&gt; &gt;&gt; and different propensities to such a race: it&#39;s trying to cover for</span>
<span class="quote">&gt; &gt;&gt; all of them.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Typically, writing is well serialized (by i_mutex) against truncation,</span>
<span class="quote">&gt; &gt;&gt; but faulting (like reading) sails through without enough of a lock.</span>
<span class="quote">&gt; &gt;&gt; We resort to i_size checks to avoid the worst of it, but there&#39;s often</span>
<span class="quote">&gt; &gt;&gt; a corner or two in which those checks are not quite good enough -</span>
<span class="quote">&gt; &gt;&gt; it&#39;s easy to check i_size at the beginning, but it needs to be checked</span>
<span class="quote">&gt; &gt;&gt; again at the end too, and what&#39;s been done undone - can be awkward.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Well, it looks like the hugetlb_no_page() routine is checking i_size both</span>
<span class="quote">&gt; &gt; before and after.  It appears to be doing the right thing to handle the</span>
<span class="quote">&gt; &gt; race, but I need to stare at the code some more to make sure.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Because of the way the truncate code went back and did an extra lookup</span>
<span class="quote">&gt; &gt; when done with the range, I assumed it was covering some race.  However,</span>
<span class="quote">&gt; &gt; that may not be the case.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I hope that in the case of hugetlbfs, since you already have the</span>
<span class="quote">&gt; &gt;&gt; additional fault_mutex to handle races between faults and punching,</span>
<span class="quote">&gt; &gt;&gt; it should be possible to get away without that &quot;pincer&quot; restarting.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yes, it looks like this may work as a straight loop over the range of</span>
<span class="quote">&gt; &gt; pages.  I just need to study the code some more to make sure I am not</span>
<span class="quote">&gt; &gt; missing something.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have convinced myself that hugetlb_no_page is coded such that page</span>
<span class="quote">&gt; faults can not race with truncate.  hugetlb_no_page handles the case</span>
<span class="quote">&gt; where there is no PTE for a faulted in address.  The general flow in</span>
<span class="quote">&gt; hugetlb_no_page for the no page found case is:</span>
<span class="quote">&gt; - check index against i_size, end if beyond</span>
<span class="quote">&gt; - allocate huge page</span>
<span class="quote">&gt; - take page table lock for huge page</span>
<span class="quote">&gt; - check index against i_size again,  if beyond free page and return</span>
<span class="quote">&gt; - add huge page to page table</span>
<span class="quote">&gt; - unlock page table lock for huge page</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The flow for the truncate operation in hugetlb_vmtruncate is:</span>
<span class="quote">&gt; - set i_size</span>
<span class="quote">&gt; - take inode/mapping write lock</span>
<span class="quote">&gt; - hugetlb_vmdelete_list() which removes page table entries.  The page</span>
<span class="quote">&gt;   table lock will be taken for each huge page in the range</span>
<span class="quote">&gt; - release inode/mapping write lock</span>
<span class="quote">&gt; - remove_inode_hugepages() to actually remove pages</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The truncate/page fault race we are concerned with is if a page is faulted</span>
<span class="quote">&gt; in after hugetlb_vmtruncate sets i_size and unmaps the page, but before</span>
<span class="quote">&gt; actually removing the page.  Obviously, any entry into hugetlb_no_page</span>
<span class="quote">&gt; after i_size is set will check the value and not allow the fault.  In</span>
<span class="quote">&gt; addition, if the value of i_size is set before the second check in</span>
<span class="quote">&gt; hugetlb_no_page, it will do the right thing.  Therefore, the only place to</span>
<span class="quote">&gt; race is after the second i_size check in hugetlb_no_page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note that the second check for i_size is with the page table lock for</span>
<span class="quote">&gt; the huge page held.  It is not possible for hugetlb_vmtruncate to unmap</span>
<span class="quote">&gt; the huge page before the page fault completes, as it must acquire the page</span>
<span class="quote">&gt; table lock.  This is the same as a fault happening before the truncate</span>
<span class="quote">&gt; operation starts and is handled correctly by hugetlb_vmtruncate.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Another way to look at this is by asking the question, Is it possible to</span>
<span class="quote">&gt; fault on a page in the truncate range after it is unmapped by</span>
<span class="quote">&gt; hugetlb_vmtruncate/hugetlb_vmdelete_list?  To unmap a page,</span>
<span class="quote">&gt; hugetlb_vmtruncate will:</span>
<span class="quote">&gt; - set i_size</span>
<span class="quote">&gt; - take page table lock for huge page</span>
<span class="quote">&gt; - unmap page</span>
<span class="quote">&gt; - release page table lock for page</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In order to fault in the page, it must take the same page table lock and</span>
<span class="quote">&gt; check i_size.  I do not know of any way for the faulting code to get an</span>
<span class="quote">&gt; old value for i_size.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please let me know if my reasoning is incorrect.  I will code up a new</span>
<span class="quote">&gt; (simpler) version of remove_inode_hugepages with the assumption that</span>
<span class="quote">&gt; truncate can not race with page faults.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, I wrote a fairly simple test to have truncate race with page faults.</span>
<span class="quote">&gt; It was quite easy to hit the second check in hugetlb_no_page where it</span>
<span class="quote">&gt; notices index is beyond i_size and backs out of the fault.  Even after</span>
<span class="quote">&gt; adding delays in strategic locations of the fault and truncate code, I</span>
<span class="quote">&gt; could not cause a race as observed by remove_inode_hugepages.</span>

Thank you for working it out and writing it down, Mike: I agree with you.

Easy for someone like me to come along and &quot;optimize&quot; something
(&quot;ooh, looks like no pte there so let&#39;s not take the page table lock&quot;),
but in fact (perhaps) break it.  But that&#39;s a criticism of me, not the
code: we couldn&#39;t write anything if that were an argument against it!

Ack to your v3 patch coming up now.

Hugh
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index 316adb9..8b8e5e8 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -324,11 +324,44 @@</span> <span class="p_context"> static void remove_huge_page(struct page *page)</span>
 	delete_from_page_cache(page);
 }
 
<span class="p_add">+static inline void</span>
<span class="p_add">+hugetlb_vmdelete_list(struct rb_root *root, pgoff_t start, pgoff_t end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_area_struct *vma;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * end == 0 indicates that the entire range after</span>
<span class="p_add">+	 * start should be unmapped.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	vma_interval_tree_foreach(vma, root, start, end ? end : ULONG_MAX) {</span>
<span class="p_add">+		unsigned long v_offset;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Can the expression below overflow on 32-bit arches?</span>
<span class="p_add">+		 * No, because the interval tree returns us only those vmas</span>
<span class="p_add">+		 * which overlap the truncated area starting at pgoff,</span>
<span class="p_add">+		 * and no vma on a 32-bit arch can span beyond the 4GB.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (vma-&gt;vm_pgoff &lt; start)</span>
<span class="p_add">+			v_offset = (start - vma-&gt;vm_pgoff) &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			v_offset = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (end) {</span>
<span class="p_add">+			end = ((end - start) &lt;&lt; PAGE_SHIFT) +</span>
<span class="p_add">+			       vma-&gt;vm_start + v_offset;</span>
<span class="p_add">+			if (end &gt; vma-&gt;vm_end)</span>
<span class="p_add">+				end = vma-&gt;vm_end;</span>
<span class="p_add">+		} else</span>
<span class="p_add">+			end = vma-&gt;vm_end;</span>
<span class="p_add">+</span>
<span class="p_add">+		unmap_hugepage_range(vma, vma-&gt;vm_start + v_offset, end, NULL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
 
 /*
  * remove_inode_hugepages handles two distinct cases: truncation and hole
  * punch.  There are subtle differences in operation for each case.
<span class="p_del">-</span>
  * truncation is indicated by end of range being LLONG_MAX
  *	In this case, we first scan the range and release found pages.
  *	After releasing pages, hugetlb_unreserve_pages cleans up region/reserv
<span class="p_chunk">@@ -381,12 +414,27 @@</span> <span class="p_context"> static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
 		for (i = 0; i &lt; pagevec_count(&amp;pvec); ++i) {
 			struct page *page = pvec.pages[i];
 			u32 hash;
<span class="p_add">+			bool rsv_on_error;</span>
 
 			hash = hugetlb_fault_mutex_hash(h, current-&gt;mm,
 							&amp;pseudo_vma,
 							mapping, next, 0);
 			mutex_lock(&amp;hugetlb_fault_mutex_table[hash]);
 
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If page is mapped, it was faulted in after being</span>
<span class="p_add">+			 * unmapped in caller.  Unmap (again) now after taking</span>
<span class="p_add">+			 * the fault mutex.  The mutex will prevent faults</span>
<span class="p_add">+			 * until we finish removing the page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (page_mapped(page)) {</span>
<span class="p_add">+				i_mmap_lock_write(mapping);</span>
<span class="p_add">+				hugetlb_vmdelete_list(&amp;mapping-&gt;i_mmap,</span>
<span class="p_add">+					next * pages_per_huge_page(h),</span>
<span class="p_add">+					(next + 1) * pages_per_huge_page(h));</span>
<span class="p_add">+				i_mmap_unlock_write(mapping);</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
 			lock_page(page);
 			if (page-&gt;index &gt;= end) {
 				unlock_page(page);
<span class="p_chunk">@@ -396,31 +444,23 @@</span> <span class="p_context"> static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
 			}
 
 			/*
<span class="p_del">-			 * If page is mapped, it was faulted in after being</span>
<span class="p_del">-			 * unmapped.  Do nothing in this race case.  In the</span>
<span class="p_del">-			 * normal case page is not mapped.</span>
<span class="p_add">+			 * We must free the huge page and remove from page</span>
<span class="p_add">+			 * cache (remove_huge_page) BEFORE removing the</span>
<span class="p_add">+			 * region/reserve map (hugetlb_unreserve_pages).</span>
<span class="p_add">+			 * In rare out of memory conditions, removal of the</span>
<span class="p_add">+			 * region/reserve map could fail.  Before free&#39;ing</span>
<span class="p_add">+			 * the page, note PagePrivate which is used in case</span>
<span class="p_add">+			 * of error.</span>
 			 */
<span class="p_del">-			if (!page_mapped(page)) {</span>
<span class="p_del">-				bool rsv_on_error = !PagePrivate(page);</span>
<span class="p_del">-				/*</span>
<span class="p_del">-				 * We must free the huge page and remove</span>
<span class="p_del">-				 * from page cache (remove_huge_page) BEFORE</span>
<span class="p_del">-				 * removing the region/reserve map</span>
<span class="p_del">-				 * (hugetlb_unreserve_pages).  In rare out</span>
<span class="p_del">-				 * of memory conditions, removal of the</span>
<span class="p_del">-				 * region/reserve map could fail.  Before</span>
<span class="p_del">-				 * free&#39;ing the page, note PagePrivate which</span>
<span class="p_del">-				 * is used in case of error.</span>
<span class="p_del">-				 */</span>
<span class="p_del">-				remove_huge_page(page);</span>
<span class="p_del">-				freed++;</span>
<span class="p_del">-				if (!truncate_op) {</span>
<span class="p_del">-					if (unlikely(hugetlb_unreserve_pages(</span>
<span class="p_del">-							inode, next,</span>
<span class="p_del">-							next + 1, 1)))</span>
<span class="p_del">-						hugetlb_fix_reserve_counts(</span>
<span class="p_del">-							inode, rsv_on_error);</span>
<span class="p_del">-				}</span>
<span class="p_add">+			rsv_on_error = !PagePrivate(page);</span>
<span class="p_add">+			remove_huge_page(page);</span>
<span class="p_add">+			freed++;</span>
<span class="p_add">+			if (!truncate_op) {</span>
<span class="p_add">+				if (unlikely(hugetlb_unreserve_pages(inode,</span>
<span class="p_add">+								next, next + 1,</span>
<span class="p_add">+								1)))</span>
<span class="p_add">+					hugetlb_fix_reserve_counts(inode,</span>
<span class="p_add">+								rsv_on_error);</span>
 			}
 
 			if (page-&gt;index &gt; next)
<span class="p_chunk">@@ -450,41 +490,6 @@</span> <span class="p_context"> static void hugetlbfs_evict_inode(struct inode *inode)</span>
 	clear_inode(inode);
 }
 
<span class="p_del">-static inline void</span>
<span class="p_del">-hugetlb_vmdelete_list(struct rb_root *root, pgoff_t start, pgoff_t end)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct vm_area_struct *vma;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * end == 0 indicates that the entire range after</span>
<span class="p_del">-	 * start should be unmapped.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	vma_interval_tree_foreach(vma, root, start, end ? end : ULONG_MAX) {</span>
<span class="p_del">-		unsigned long v_offset;</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Can the expression below overflow on 32-bit arches?</span>
<span class="p_del">-		 * No, because the interval tree returns us only those vmas</span>
<span class="p_del">-		 * which overlap the truncated area starting at pgoff,</span>
<span class="p_del">-		 * and no vma on a 32-bit arch can span beyond the 4GB.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (vma-&gt;vm_pgoff &lt; start)</span>
<span class="p_del">-			v_offset = (start - vma-&gt;vm_pgoff) &lt;&lt; PAGE_SHIFT;</span>
<span class="p_del">-		else</span>
<span class="p_del">-			v_offset = 0;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (end) {</span>
<span class="p_del">-			end = ((end - start) &lt;&lt; PAGE_SHIFT) +</span>
<span class="p_del">-			       vma-&gt;vm_start + v_offset;</span>
<span class="p_del">-			if (end &gt; vma-&gt;vm_end)</span>
<span class="p_del">-				end = vma-&gt;vm_end;</span>
<span class="p_del">-		} else</span>
<span class="p_del">-			end = vma-&gt;vm_end;</span>
<span class="p_del">-</span>
<span class="p_del">-		unmap_hugepage_range(vma, vma-&gt;vm_start + v_offset, end, NULL);</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)
 {
 	pgoff_t pgoff;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



