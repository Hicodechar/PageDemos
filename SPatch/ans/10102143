
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,09/33] nds32: Cache and TLB routines - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,09/33] nds32: Cache and TLB routines</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 8, 2017, 9:11 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;9d0e976c3e60bf6834dffb6ef798ad2a03e1c9ac.1512723245.git.green.hu@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10102143/mbox/"
   >mbox</a>
|
   <a href="/patch/10102143/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10102143/">/patch/10102143/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	AA02C605BA for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  8 Dec 2017 10:01:28 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 81963286C2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  8 Dec 2017 10:01:28 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7623128AE9; Fri,  8 Dec 2017 10:01:28 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3A61328942
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  8 Dec 2017 10:01:26 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753471AbdLHKBO (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 8 Dec 2017 05:01:14 -0500
Received: from mail-pf0-f196.google.com ([209.85.192.196]:44849 &quot;EHLO
	mail-pf0-f196.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753550AbdLHJi0 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 8 Dec 2017 04:38:26 -0500
Received: by mail-pf0-f196.google.com with SMTP id m26so6897905pfj.11;
	Fri, 08 Dec 2017 01:38:26 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=from:to:cc:subject:date:message-id:in-reply-to:references
	:in-reply-to:references;
	bh=WfTeom1wn2sr+kjm2AWb3r0OrFbbEjHVaoHZYDEy2WE=;
	b=DQooOKkVzqLSxrCL2JC68gV7Lvl671Gbrucgq08wLzNJTOGLFtiwrQonWq8rp0F/vJ
	iUDFzVxT0+WmvhxWzVL0cEVbN+iVSRYS2hEkCJMo3nFLhTI/z5Y7NWhKezi0QUmYqlwP
	8irHZIMpWXTQG/qOtF1ZyowbgUOyjuxLOu7qWJpyIzawOGtHCGuMlXmtI0LsTt6eglCL
	6h7ebZ0FY93zPLRR7FR8CMySKD5qoSk9QpwOODXHizNnsjkYYdH0J/aIZZeSN16lRE9u
	mXc9v0+D3fJmq7XUwLNELBOo5RF3KlqQ2i5kMvtym6/r63uGTR/4UcaIRJ9nm/aRu4mu
	iAIg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references:in-reply-to:references;
	bh=WfTeom1wn2sr+kjm2AWb3r0OrFbbEjHVaoHZYDEy2WE=;
	b=abFvZPmmgFLhD7bzuB5MKmDm6N+YVdtZfrgCIsmGyydH686QVPAynHWSOdo79kn/3p
	49YOpJGUKXn0OLgdQQ/p+SCOIZ0Rkk6JhilSpmpCs6k4qZGEoDoxxMPm6OaETB2F9SUh
	rg7AB6r1HAXu9+VzxrQZosdLxjrG3zDWtJHF4Gs9fqlo8ByKvIuZL+P8PRXIEGTZ6SQS
	3XipxQQqCDeTbxbqFprpxSmhjEgkQCEO90B9aIP9vUidRk4l0xK46E+YSDwG/D3iX79Z
	Qrs8WsnudBIbhZfx8pc+4+FPp9xqo/T3cK8pBXHbmc9IEQjhSfuMAUJMv6Gq2naJ6skZ
	xXCw==
X-Gm-Message-State: AJaThX5s/WQeW3Dm9B0oPRz31dWJNB8gY8maKGbf7CdvPzll3AktCzjj
	HCxivYF/4YxtTOtA4qd705Q=
X-Google-Smtp-Source: AGs4zMamlR6lZnlL82ADVisNOwyvgHy+wQ7swYfN+BIQOA362gnMrxzpJvOqrfG5GkwhS97szxep+A==
X-Received: by 10.99.125.16 with SMTP id y16mr29529685pgc.417.1512725905071; 
	Fri, 08 Dec 2017 01:38:25 -0800 (PST)
Received: from app09.andestech.com ([118.163.51.199])
	by smtp.gmail.com with ESMTPSA id
	g10sm14827405pfe.77.2017.12.08.01.38.21
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Fri, 08 Dec 2017 01:38:24 -0800 (PST)
From: Greentime Hu &lt;green.hu@gmail.com&gt;
To: greentime@andestech.com, linux-kernel@vger.kernel.org,
	arnd@arndb.de, linux-arch@vger.kernel.org, tglx@linutronix.de,
	jason@lakedaemon.net, marc.zyngier@arm.com, robh+dt@kernel.org,
	netdev@vger.kernel.org, deanbo422@gmail.com,
	devicetree@vger.kernel.org, viro@zeniv.linux.org.uk,
	dhowells@redhat.com, will.deacon@arm.com,
	daniel.lezcano@linaro.org, linux-serial@vger.kernel.org,
	geert.uytterhoeven@gmail.com, linus.walleij@linaro.org,
	mark.rutland@arm.com, greg@kroah.com
Cc: green.hu@gmail.com, Vincent Chen &lt;vincentc@andestech.com&gt;
Subject: [PATCH v3 09/33] nds32: Cache and TLB routines
Date: Fri,  8 Dec 2017 17:11:52 +0800
Message-Id: &lt;9d0e976c3e60bf6834dffb6ef798ad2a03e1c9ac.1512723245.git.green.hu@gmail.com&gt;
X-Mailer: git-send-email 1.7.9.5
In-Reply-To: &lt;cover.1512723245.git.green.hu@gmail.com&gt;
References: &lt;cover.1512723245.git.green.hu@gmail.com&gt;
In-Reply-To: &lt;cover.1512723245.git.green.hu@gmail.com&gt;
References: &lt;cover.1512723245.git.green.hu@gmail.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Dec. 8, 2017, 9:11 a.m.</div>
<pre class="content">
<span class="from">From: Greentime Hu &lt;greentime@andestech.com&gt;</span>

This patch contains cache and TLB maintenance functions.
<span class="signed-off-by">
Signed-off-by: Vincent Chen &lt;vincentc@andestech.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Greentime Hu &lt;greentime@andestech.com&gt;</span>
---
 arch/nds32/include/asm/cache.h         |   25 ++
 arch/nds32/include/asm/cache_info.h    |   26 ++
 arch/nds32/include/asm/cacheflush.h    |   57 ++++
 arch/nds32/include/asm/mmu_context.h   |   81 +++++
 arch/nds32/include/asm/proc-fns.h      |   57 ++++
 arch/nds32/include/asm/tlb.h           |   41 +++
 arch/nds32/include/asm/tlbflush.h      |   60 ++++
 arch/nds32/include/uapi/asm/cachectl.h |   19 ++
 arch/nds32/kernel/cacheinfo.c          |   62 ++++
 arch/nds32/mm/cacheflush.c             |  331 ++++++++++++++++++++
 arch/nds32/mm/proc.c                   |  527 ++++++++++++++++++++++++++++++++
 arch/nds32/mm/tlb.c                    |   63 ++++
 12 files changed, 1349 insertions(+)
 create mode 100644 arch/nds32/include/asm/cache.h
 create mode 100644 arch/nds32/include/asm/cache_info.h
 create mode 100644 arch/nds32/include/asm/cacheflush.h
 create mode 100644 arch/nds32/include/asm/mmu_context.h
 create mode 100644 arch/nds32/include/asm/proc-fns.h
 create mode 100644 arch/nds32/include/asm/tlb.h
 create mode 100644 arch/nds32/include/asm/tlbflush.h
 create mode 100644 arch/nds32/include/uapi/asm/cachectl.h
 create mode 100644 arch/nds32/kernel/cacheinfo.c
 create mode 100644 arch/nds32/mm/cacheflush.c
 create mode 100644 arch/nds32/mm/proc.c
 create mode 100644 arch/nds32/mm/tlb.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=177875">Guo Ren</a> - Dec. 13, 2017, 2:16 a.m.</div>
<pre class="content">
On Fri, Dec 08, 2017 at 05:11:52PM +0800, Greentime Hu wrote:
<span class="quote">&gt; From: Greentime Hu &lt;greentime@andestech.com&gt;</span>
 [...]
<span class="quote">&gt; diff --git a/arch/nds32/mm/cacheflush.c b/arch/nds32/mm/cacheflush.c</span>
 [...]
<span class="quote">&gt; +#ifndef CONFIG_CPU_CACHE_ALIASING</span>
<span class="quote">&gt; +void update_mmu_cache(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="quote">&gt; +		      pte_t * pte)</span>
 [...]
<span class="quote">&gt; +	if (vma-&gt;vm_mm == current-&gt;active_mm) {</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		__nds32__mtsr_dsb(addr, NDS32_SR_TLB_VPN);</span>
<span class="quote">&gt; +		__nds32__tlbop_rwr(*pte);</span>
<span class="quote">&gt; +		__nds32__isb();</span>
If there is an interruption between &quot;mtsr_dsb&quot; and &quot;tlbop_rwr&quot; and a
update_mmu_cache() is invoked again, then an error page mapping is
set up in your tlb-buffer when tlbop_rwr is excuted from interrupt.
Because it&#39;s another addr in NDS32_SR_TLB_VPN.

It seems that tlb-hardrefill can help build tlb-buffer mapping, why you
update it in this software way?

 Guo Ren
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Dec. 13, 2017, 5:45 a.m.</div>
<pre class="content">
2017-12-13 10:16 GMT+08:00 Guo Ren &lt;ren_guo@c-sky.com&gt;:
<span class="quote">&gt; On Fri, Dec 08, 2017 at 05:11:52PM +0800, Greentime Hu wrote:</span>
<span class="quote">&gt;&gt; From: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="quote">&gt;  [...]</span>
<span class="quote">&gt;&gt; diff --git a/arch/nds32/mm/cacheflush.c b/arch/nds32/mm/cacheflush.c</span>
<span class="quote">&gt;  [...]</span>
<span class="quote">&gt;&gt; +#ifndef CONFIG_CPU_CACHE_ALIASING</span>
<span class="quote">&gt;&gt; +void update_mmu_cache(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="quote">&gt;&gt; +                   pte_t * pte)</span>
<span class="quote">&gt;  [...]</span>
<span class="quote">&gt;&gt; +     if (vma-&gt;vm_mm == current-&gt;active_mm) {</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +             __nds32__mtsr_dsb(addr, NDS32_SR_TLB_VPN);</span>
<span class="quote">&gt;&gt; +             __nds32__tlbop_rwr(*pte);</span>
<span class="quote">&gt;&gt; +             __nds32__isb();</span>
<span class="quote">&gt; If there is an interruption between &quot;mtsr_dsb&quot; and &quot;tlbop_rwr&quot; and a</span>
<span class="quote">&gt; update_mmu_cache() is invoked again, then an error page mapping is</span>
<span class="quote">&gt; set up in your tlb-buffer when tlbop_rwr is excuted from interrupt.</span>
<span class="quote">&gt; Because it&#39;s another addr in NDS32_SR_TLB_VPN.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It seems that tlb-hardrefill can help build tlb-buffer mapping, why you</span>
<span class="quote">&gt; update it in this software way?</span>
<span class="quote">&gt;</span>

Hi, Guo Ren:

I think it should be fine if an interruption between mtsr_dsb and
tlbop_rwr because this is a optimization by sw.
The page mapping has been created and it is just not in TLB yet.
What we did is to insert this mapping to TLB by SW thus it can prevent
TLB miss one time.
It will be fine even if TLB miss. HW will walk through page table and
insert this mapping to TLB anyway.

Based on Documentation/cachetlb.txt
5) ``void update_mmu_cache(struct vm_area_struct *vma,
   unsigned long address, pte_t *ptep)``

        At the end of every page fault, this routine is invoked to
        tell the architecture specific code that a translation
        now exists at virtual address &quot;address&quot; for address space
        &quot;vma-&gt;vm_mm&quot;, in the software page tables.

        A port may use this information in any way it so chooses.
        For example, it could use this event to pre-load TLB
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        translations for software managed TLB configurations.
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=177875">Guo Ren</a> - Dec. 13, 2017, 8:19 a.m.</div>
<pre class="content">
On Wed, Dec 13, 2017 at 01:45:02PM +0800, Greentime Hu wrote:
<span class="quote"> 
&gt; I think it should be fine if an interruption between mtsr_dsb and</span>
<span class="quote">&gt; tlbop_rwr because this is a optimization by sw.</span>

Fine? When there is an unexpected vaddr in SR_TLB_VPN, tlbop_rwr(*pte) will
break that vaddr&#39;s pfn in the CPU tlb-buffer entry. When linux access the 
vaddr, it will get wrong data unless the entry has been replaced out.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Dec. 13, 2017, 8:30 a.m.</div>
<pre class="content">
2017-12-13 16:19 GMT+08:00 Guo Ren &lt;ren_guo@c-sky.com&gt;:
<span class="quote">&gt; On Wed, Dec 13, 2017 at 01:45:02PM +0800, Greentime Hu wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; I think it should be fine if an interruption between mtsr_dsb and</span>
<span class="quote">&gt;&gt; tlbop_rwr because this is a optimization by sw.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Fine? When there is an unexpected vaddr in SR_TLB_VPN, tlbop_rwr(*pte) will</span>
<span class="quote">&gt; break that vaddr&#39;s pfn in the CPU tlb-buffer entry. When linux access the</span>
<span class="quote">&gt; vaddr, it will get wrong data unless the entry has been replaced out.</span>

Hi, Guo Ren:

Thanks. I get your point.
It is needed to be protected.
I will fix it in the next version patch.

if (vma-&gt;vm_mm == current-&gt;active_mm) {
        local_irq_save(flags);
        __nds32__mtsr_dsb(addr, NDS32_SR_TLB_VPN);
        __nds32__tlbop_rwr(*pte);
        __nds32__isb();
        local_irq_restore(flags);
}
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=177875">Guo Ren</a> - Dec. 13, 2017, 8:53 a.m.</div>
<pre class="content">
On Wed, Dec 13, 2017 at 04:30:41PM +0800, Greentime Hu wrote:
<span class="quote">&gt; 2017-12-13 16:19 GMT+08:00 Guo Ren &lt;ren_guo@c-sky.com&gt;:</span>
<span class="quote">&gt; &gt; On Wed, Dec 13, 2017 at 01:45:02PM +0800, Greentime Hu wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; I think it should be fine if an interruption between mtsr_dsb and</span>
<span class="quote">&gt; &gt;&gt; tlbop_rwr because this is a optimization by sw.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Fine? When there is an unexpected vaddr in SR_TLB_VPN, tlbop_rwr(*pte) will</span>
<span class="quote">&gt; &gt; break that vaddr&#39;s pfn in the CPU tlb-buffer entry. When linux access the</span>
<span class="quote">&gt; &gt; vaddr, it will get wrong data unless the entry has been replaced out.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi, Guo Ren:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks. I get your point.</span>
<span class="quote">&gt; It is needed to be protected.</span>
<span class="quote">&gt; I will fix it in the next version patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; if (vma-&gt;vm_mm == current-&gt;active_mm) {</span>
<span class="quote">&gt;         local_irq_save(flags);</span>
<span class="quote">&gt;         __nds32__mtsr_dsb(addr, NDS32_SR_TLB_VPN);</span>
<span class="quote">&gt;         __nds32__tlbop_rwr(*pte);</span>
<span class="quote">&gt;         __nds32__isb();</span>
<span class="quote">&gt;         local_irq_restore(flags);</span>
<span class="quote">&gt; }</span>

If hardware tlbop_rwr could invalid NDS32_SR_TLB_VPN, then you needn&#39;t
protect.
I mean:
mtsr addr1 NDS32_SR_TLB_VPN
mtsr addr2 NDS32_SR_TLB_VPN
tlbop_rwr(*pte) // OK, and it will hit a hardware invalid bit internal.
tlbop_rwr(*pte) // SR_TLB_VPN invalided, then it will not cause problem.

:) How my idea?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Dec. 13, 2017, 9:03 a.m.</div>
<pre class="content">
2017-12-13 16:53 GMT+08:00 Guo Ren &lt;ren_guo@c-sky.com&gt;:
<span class="quote">&gt; On Wed, Dec 13, 2017 at 04:30:41PM +0800, Greentime Hu wrote:</span>
<span class="quote">&gt;&gt; 2017-12-13 16:19 GMT+08:00 Guo Ren &lt;ren_guo@c-sky.com&gt;:</span>
<span class="quote">&gt;&gt; &gt; On Wed, Dec 13, 2017 at 01:45:02PM +0800, Greentime Hu wrote:</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; I think it should be fine if an interruption between mtsr_dsb and</span>
<span class="quote">&gt;&gt; &gt;&gt; tlbop_rwr because this is a optimization by sw.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Fine? When there is an unexpected vaddr in SR_TLB_VPN, tlbop_rwr(*pte) will</span>
<span class="quote">&gt;&gt; &gt; break that vaddr&#39;s pfn in the CPU tlb-buffer entry. When linux access the</span>
<span class="quote">&gt;&gt; &gt; vaddr, it will get wrong data unless the entry has been replaced out.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Hi, Guo Ren:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Thanks. I get your point.</span>
<span class="quote">&gt;&gt; It is needed to be protected.</span>
<span class="quote">&gt;&gt; I will fix it in the next version patch.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; if (vma-&gt;vm_mm == current-&gt;active_mm) {</span>
<span class="quote">&gt;&gt;         local_irq_save(flags);</span>
<span class="quote">&gt;&gt;         __nds32__mtsr_dsb(addr, NDS32_SR_TLB_VPN);</span>
<span class="quote">&gt;&gt;         __nds32__tlbop_rwr(*pte);</span>
<span class="quote">&gt;&gt;         __nds32__isb();</span>
<span class="quote">&gt;&gt;         local_irq_restore(flags);</span>
<span class="quote">&gt;&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If hardware tlbop_rwr could invalid NDS32_SR_TLB_VPN, then you needn&#39;t</span>
<span class="quote">&gt; protect.</span>
<span class="quote">&gt; I mean:</span>
<span class="quote">&gt; mtsr addr1 NDS32_SR_TLB_VPN</span>
<span class="quote">&gt; mtsr addr2 NDS32_SR_TLB_VPN</span>
<span class="quote">&gt; tlbop_rwr(*pte) // OK, and it will hit a hardware invalid bit internal.</span>
<span class="quote">&gt; tlbop_rwr(*pte) // SR_TLB_VPN invalided, then it will not cause problem.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; :) How my idea?</span>

Hi, Guo Ren:

The reason I think it might have problem is that

mtsr addr1 NDS32_SR_TLB_VPN
    interrupt coming
        mtsr addr2 NDS32_SR_TLB_VPN &lt;- TLB_VPN has been set to addr2
        tlbop_rwr(*pte);
    interrupt finish
tlbop_rwr(*pte); &lt;- it will use the wrong TLB_VPN

I think all these TLB operations should be protected because tlbop
will operate with TLB_VPN.
Thanks :)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=177875">Guo Ren</a> - Dec. 13, 2017, 9:45 a.m.</div>
<pre class="content">
Hello,

CPU team could improve the tlbop_*. Eg: Design a hardware
internal flag bit for SR_TLB_VPN, tlbop_* will invalid it and mtsr
SR_TLB_VPN will valid it.

So:
On Wed, Dec 13, 2017 at 05:03:33PM +0800, Greentime Hu wrote:
<span class="quote">&gt; mtsr addr1 NDS32_SR_TLB_VPN</span>
<span class="quote">&gt;     interrupt coming</span>
<span class="quote">&gt;         mtsr addr2 NDS32_SR_TLB_VPN &lt;- TLB_VPN has been set to addr2 </span>
	mtsr SR_TLB_VPN will valid the flag bit
<span class="quote">&gt;         tlbop_rwr(*pte);</span>
	tlbop_rwr will invalid SR_TLB_VPN flag bit
<span class="quote">&gt;     interrupt finish</span>
<span class="quote">&gt; tlbop_rwr(*pte); &lt;- it will use the wrong TLB_VPN</span>
Because SR_TLB_VPN is in a invalid state, no operation happen on
tlbop_rwr.

Then they are atomic safe ,no spin_lock_irq need.
:)

 Guo Ren
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Dec. 13, 2017, 10:04 a.m.</div>
<pre class="content">
2017-12-13 17:45 GMT+08:00 Guo Ren &lt;ren_guo@c-sky.com&gt;:
<span class="quote">&gt; Hello,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; CPU team could improve the tlbop_*. Eg: Design a hardware</span>
<span class="quote">&gt; internal flag bit for SR_TLB_VPN, tlbop_* will invalid it and mtsr</span>
<span class="quote">&gt; SR_TLB_VPN will valid it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So:</span>
<span class="quote">&gt; On Wed, Dec 13, 2017 at 05:03:33PM +0800, Greentime Hu wrote:</span>
<span class="quote">&gt;&gt; mtsr addr1 NDS32_SR_TLB_VPN</span>
<span class="quote">&gt;&gt;     interrupt coming</span>
<span class="quote">&gt;&gt;         mtsr addr2 NDS32_SR_TLB_VPN &lt;- TLB_VPN has been set to addr2</span>
<span class="quote">&gt;         mtsr SR_TLB_VPN will valid the flag bit</span>
<span class="quote">&gt;&gt;         tlbop_rwr(*pte);</span>
<span class="quote">&gt;         tlbop_rwr will invalid SR_TLB_VPN flag bit</span>
<span class="quote">&gt;&gt;     interrupt finish</span>
<span class="quote">&gt;&gt; tlbop_rwr(*pte); &lt;- it will use the wrong TLB_VPN</span>
<span class="quote">&gt; Because SR_TLB_VPN is in a invalid state, no operation happen on</span>
<span class="quote">&gt; tlbop_rwr.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Then they are atomic safe ,no spin_lock_irq need.</span>
<span class="quote">&gt; :)</span>
<span class="quote">&gt;</span>

Oh, I see. I may propose this idea to our ARCH colleagues for the next
version design.
Many thanks.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/nds32/include/asm/cache.h b/arch/nds32/include/asm/cache.h</span>
new file mode 100644
<span class="p_header">index 0000000..36ec549</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/asm/cache.h</span>
<span class="p_chunk">@@ -0,0 +1,25 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __NDS32_CACHE_H__</span>
<span class="p_add">+#define __NDS32_CACHE_H__</span>
<span class="p_add">+</span>
<span class="p_add">+#define L1_CACHE_BYTES	32</span>
<span class="p_add">+#define L1_CACHE_SHIFT	5</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_DMA_MINALIGN   L1_CACHE_BYTES</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __NDS32_CACHE_H__ */</span>
<span class="p_header">diff --git a/arch/nds32/include/asm/cache_info.h b/arch/nds32/include/asm/cache_info.h</span>
new file mode 100644
<span class="p_header">index 0000000..2b17b7d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/asm/cache_info.h</span>
<span class="p_chunk">@@ -0,0 +1,26 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+struct cache_info {</span>
<span class="p_add">+	unsigned char ways;</span>
<span class="p_add">+	unsigned char line_size;</span>
<span class="p_add">+	unsigned short sets;</span>
<span class="p_add">+	unsigned short size;</span>
<span class="p_add">+#if defined(CONFIG_CPU_CACHE_ALIASING)</span>
<span class="p_add">+	unsigned short aliasing_num;</span>
<span class="p_add">+	unsigned int aliasing_mask;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+};</span>
<span class="p_header">diff --git a/arch/nds32/include/asm/cacheflush.h b/arch/nds32/include/asm/cacheflush.h</span>
new file mode 100644
<span class="p_header">index 0000000..5b95a6c</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/asm/cacheflush.h</span>
<span class="p_chunk">@@ -0,0 +1,57 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __NDS32_CACHEFLUSH_H__</span>
<span class="p_add">+#define __NDS32_CACHEFLUSH_H__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define PG_dcache_dirty PG_arch_1</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_CPU_CACHE_ALIASING</span>
<span class="p_add">+void flush_cache_mm(struct mm_struct *mm);</span>
<span class="p_add">+void flush_cache_dup_mm(struct mm_struct *mm);</span>
<span class="p_add">+void flush_cache_range(struct vm_area_struct *vma,</span>
<span class="p_add">+		       unsigned long start, unsigned long end);</span>
<span class="p_add">+void flush_cache_page(struct vm_area_struct *vma,</span>
<span class="p_add">+		      unsigned long addr, unsigned long pfn);</span>
<span class="p_add">+void flush_cache_kmaps(void);</span>
<span class="p_add">+void flush_cache_vmap(unsigned long start, unsigned long end);</span>
<span class="p_add">+void flush_cache_vunmap(unsigned long start, unsigned long end);</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_IMPLEMENTS_FLUSH_DCACHE_PAGE 1</span>
<span class="p_add">+void flush_dcache_page(struct page *page);</span>
<span class="p_add">+void copy_to_user_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="p_add">+		       unsigned long vaddr, void *dst, void *src, int len);</span>
<span class="p_add">+void copy_from_user_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="p_add">+			 unsigned long vaddr, void *dst, void *src, int len);</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_HAS_FLUSH_ANON_PAGE</span>
<span class="p_add">+void flush_anon_page(struct vm_area_struct *vma,</span>
<span class="p_add">+		     struct page *page, unsigned long vaddr);</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_HAS_FLUSH_KERNEL_DCACHE_PAGE</span>
<span class="p_add">+void flush_kernel_dcache_page(struct page *page);</span>
<span class="p_add">+void flush_icache_range(unsigned long start, unsigned long end);</span>
<span class="p_add">+void flush_icache_page(struct vm_area_struct *vma, struct page *page);</span>
<span class="p_add">+#define flush_dcache_mmap_lock(mapping)   spin_lock_irq(&amp;(mapping)-&gt;tree_lock)</span>
<span class="p_add">+#define flush_dcache_mmap_unlock(mapping) spin_unlock_irq(&amp;(mapping)-&gt;tree_lock)</span>
<span class="p_add">+</span>
<span class="p_add">+#else</span>
<span class="p_add">+#include &lt;asm-generic/cacheflush.h&gt;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __NDS32_CACHEFLUSH_H__ */</span>
<span class="p_header">diff --git a/arch/nds32/include/asm/mmu_context.h b/arch/nds32/include/asm/mmu_context.h</span>
new file mode 100644
<span class="p_header">index 0000000..4a59b5d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -0,0 +1,81 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASM_NDS32_MMU_CONTEXT_H</span>
<span class="p_add">+#define __ASM_NDS32_MMU_CONTEXT_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/proc-fns.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/mm_hooks.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int</span>
<span class="p_add">+init_new_context(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mm-&gt;context.id = 0;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define destroy_context(mm)	do { } while(0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CID_BITS	9</span>
<span class="p_add">+extern spinlock_t cid_lock;</span>
<span class="p_add">+extern unsigned int cpu_last_cid;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __new_context(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int cid;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;cid_lock, flags);</span>
<span class="p_add">+	cid = cpu_last_cid;</span>
<span class="p_add">+	cpu_last_cid += 1 &lt;&lt; TLB_MISC_offCID;</span>
<span class="p_add">+	if (cpu_last_cid == 0)</span>
<span class="p_add">+		cpu_last_cid = 1 &lt;&lt; TLB_MISC_offCID &lt;&lt; CID_BITS;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((cid &amp; TLB_MISC_mskCID) == 0)</span>
<span class="p_add">+		flush_tlb_all();</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;cid_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	mm-&gt;context.id = cid;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void check_context(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (unlikely</span>
<span class="p_add">+	    ((mm-&gt;context.id ^ cpu_last_cid) &gt;&gt; TLB_MISC_offCID &gt;&gt; CID_BITS))</span>
<span class="p_add">+		__new_context(mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void enter_lazy_tlb(struct mm_struct *mm, struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_add">+			     struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int cpu = smp_processor_id();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpumask_test_and_set_cpu(cpu, mm_cpumask(next)) || prev != next) {</span>
<span class="p_add">+		check_context(next);</span>
<span class="p_add">+		cpu_switch_mm(next);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define deactivate_mm(tsk,mm)	do { } while (0)</span>
<span class="p_add">+#define activate_mm(prev,next)	switch_mm(prev, next, NULL)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/nds32/include/asm/proc-fns.h b/arch/nds32/include/asm/proc-fns.h</span>
new file mode 100644
<span class="p_header">index 0000000..b1a56d2</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/asm/proc-fns.h</span>
<span class="p_chunk">@@ -0,0 +1,57 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __NDS32_PROCFNS_H__</span>
<span class="p_add">+#define __NDS32_PROCFNS_H__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+#include &lt;asm/page.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct mm_struct;</span>
<span class="p_add">+struct vm_area_struct;</span>
<span class="p_add">+extern void cpu_proc_init(void);</span>
<span class="p_add">+extern void cpu_proc_fin(void);</span>
<span class="p_add">+extern void cpu_do_idle(void);</span>
<span class="p_add">+extern void cpu_reset(unsigned long reset);</span>
<span class="p_add">+extern void cpu_switch_mm(struct mm_struct *mm);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void cpu_dcache_inval_all(void);</span>
<span class="p_add">+extern void cpu_dcache_wbinval_all(void);</span>
<span class="p_add">+extern void cpu_dcache_inval_page(unsigned long page);</span>
<span class="p_add">+extern void cpu_dcache_wb_page(unsigned long page);</span>
<span class="p_add">+extern void cpu_dcache_wbinval_page(unsigned long page);</span>
<span class="p_add">+extern void cpu_dcache_inval_range(unsigned long start, unsigned long end);</span>
<span class="p_add">+extern void cpu_dcache_wb_range(unsigned long start, unsigned long end);</span>
<span class="p_add">+extern void cpu_dcache_wbinval_range(unsigned long start, unsigned long end);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void cpu_icache_inval_all(void);</span>
<span class="p_add">+extern void cpu_icache_inval_page(unsigned long page);</span>
<span class="p_add">+extern void cpu_icache_inval_range(unsigned long start, unsigned long end);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void cpu_cache_wbinval_page(unsigned long page, int flushi);</span>
<span class="p_add">+extern void cpu_cache_wbinval_range(unsigned long start,</span>
<span class="p_add">+				    unsigned long end, int flushi);</span>
<span class="p_add">+extern void cpu_cache_wbinval_range_check(struct vm_area_struct *vma,</span>
<span class="p_add">+					  unsigned long start,</span>
<span class="p_add">+					  unsigned long end, bool flushi,</span>
<span class="p_add">+					  bool wbd);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void cpu_dma_wb_range(unsigned long start, unsigned long end);</span>
<span class="p_add">+extern void cpu_dma_inval_range(unsigned long start, unsigned long end);</span>
<span class="p_add">+extern void cpu_dma_wbinval_range(unsigned long start, unsigned long end);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+#endif /* __NDS32_PROCFNS_H__ */</span>
<span class="p_header">diff --git a/arch/nds32/include/asm/tlb.h b/arch/nds32/include/asm/tlb.h</span>
new file mode 100644
<span class="p_header">index 0000000..c599827</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/asm/tlb.h</span>
<span class="p_chunk">@@ -0,0 +1,41 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASMNDS32_TLB_H</span>
<span class="p_add">+#define __ASMNDS32_TLB_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define tlb_start_vma(tlb,vma)						\</span>
<span class="p_add">+	do {								\</span>
<span class="p_add">+		if (!tlb-&gt;fullmm)					\</span>
<span class="p_add">+			flush_cache_range(vma, vma-&gt;vm_start, vma-&gt;vm_end); \</span>
<span class="p_add">+	} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define tlb_end_vma(tlb,vma)				\</span>
<span class="p_add">+	do { 						\</span>
<span class="p_add">+		if(!tlb-&gt;fullmm)			\</span>
<span class="p_add">+			flush_tlb_range(vma, vma-&gt;vm_start, vma-&gt;vm_end); \</span>
<span class="p_add">+	} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __tlb_remove_tlb_entry(tlb, pte, addr) do { } while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define tlb_flush(tlb)	flush_tlb_mm((tlb)-&gt;mm)</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/tlb.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pte_free_tlb(tlb, pte, addr)	pte_free((tlb)-&gt;mm, pte)</span>
<span class="p_add">+#define __pmd_free_tlb(tlb, pmd, addr)	pmd_free((tln)-&gt;mm, pmd)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/nds32/include/asm/tlbflush.h b/arch/nds32/include/asm/tlbflush.h</span>
new file mode 100644
<span class="p_header">index 0000000..680aeb3</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -0,0 +1,60 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASMNDS32_TLBFLUSH_H</span>
<span class="p_add">+#define _ASMNDS32_TLBFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;nds32_intrinsic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void local_flush_tlb_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__nds32__tlbop_flua();</span>
<span class="p_add">+	__nds32__isb();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void local_flush_tlb_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__nds32__tlbop_flua();</span>
<span class="p_add">+	__nds32__isb();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void local_flush_tlb_kernel_range(unsigned long start,</span>
<span class="p_add">+						unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (start &lt; end) {</span>
<span class="p_add">+		__nds32__tlbop_inv(start);</span>
<span class="p_add">+		__nds32__isb();</span>
<span class="p_add">+		start += PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void local_flush_tlb_range(struct vm_area_struct *vma,</span>
<span class="p_add">+			   unsigned long start, unsigned long end);</span>
<span class="p_add">+void local_flush_tlb_page(struct vm_area_struct *vma, unsigned long addr);</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all		local_flush_tlb_all</span>
<span class="p_add">+#define flush_tlb_mm		local_flush_tlb_mm</span>
<span class="p_add">+#define flush_tlb_range		local_flush_tlb_range</span>
<span class="p_add">+#define flush_tlb_page		local_flush_tlb_page</span>
<span class="p_add">+#define flush_tlb_kernel_range	local_flush_tlb_kernel_range</span>
<span class="p_add">+</span>
<span class="p_add">+void update_mmu_cache(struct vm_area_struct *vma,</span>
<span class="p_add">+		      unsigned long address, pte_t * pte);</span>
<span class="p_add">+void tlb_migrate_finish(struct mm_struct *mm);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/nds32/include/uapi/asm/cachectl.h b/arch/nds32/include/uapi/asm/cachectl.h</span>
new file mode 100644
<span class="p_header">index 0000000..a56a37d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/uapi/asm/cachectl.h</span>
<span class="p_chunk">@@ -0,0 +1,19 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This file is subject to the terms and conditions of the GNU General Public</span>
<span class="p_add">+ * License.  See the file &quot;COPYING&quot; in the main directory of this archive</span>
<span class="p_add">+ * for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 1994, 1995, 1996 by Ralf Baechle</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef	_ASM_CACHECTL</span>
<span class="p_add">+#define	_ASM_CACHECTL</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Options for cacheflush system call</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define	ICACHE	0		/* flush instruction cache        */</span>
<span class="p_add">+#define	DCACHE	1		/* writeback and flush data cache */</span>
<span class="p_add">+#define	BCACHE	2		/* flush instruction cache + writeback and flush data cache */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_CACHECTL */</span>
<span class="p_header">diff --git a/arch/nds32/kernel/cacheinfo.c b/arch/nds32/kernel/cacheinfo.c</span>
new file mode 100644
<span class="p_header">index 0000000..edc0fda</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/kernel/cacheinfo.c</span>
<span class="p_chunk">@@ -0,0 +1,62 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/bitops.h&gt;</span>
<span class="p_add">+#include &lt;linux/cacheinfo.h&gt;</span>
<span class="p_add">+#include &lt;linux/cpu.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static void ci_leaf_init(struct cacheinfo *this_leaf,</span>
<span class="p_add">+			 enum cache_type type, unsigned int level)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char cache_type = (type &amp; CACHE_TYPE_INST ? ICACHE : DCACHE);</span>
<span class="p_add">+</span>
<span class="p_add">+	this_leaf-&gt;level = level;</span>
<span class="p_add">+	this_leaf-&gt;type = type;</span>
<span class="p_add">+	this_leaf-&gt;coherency_line_size = CACHE_LINE_SIZE(cache_type);</span>
<span class="p_add">+	this_leaf-&gt;number_of_sets = CACHE_SET(cache_type);;</span>
<span class="p_add">+	this_leaf-&gt;ways_of_associativity = CACHE_WAY(cache_type);</span>
<span class="p_add">+	this_leaf-&gt;size = this_leaf-&gt;number_of_sets *</span>
<span class="p_add">+	    this_leaf-&gt;coherency_line_size * this_leaf-&gt;ways_of_associativity;</span>
<span class="p_add">+#if defined(CONFIG_CPU_DCACHE_WRITETHROUGH)</span>
<span class="p_add">+	this_leaf-&gt;attributes = CACHE_WRITE_THROUGH;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	this_leaf-&gt;attributes = CACHE_WRITE_BACK;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int init_cache_level(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Only 1 level and I/D cache seperate. */</span>
<span class="p_add">+	this_cpu_ci-&gt;num_levels = 1;</span>
<span class="p_add">+	this_cpu_ci-&gt;num_leaves = 2;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int populate_cache_leaves(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int level, idx;</span>
<span class="p_add">+	struct cpu_cacheinfo *this_cpu_ci = get_cpu_cacheinfo(cpu);</span>
<span class="p_add">+	struct cacheinfo *this_leaf = this_cpu_ci-&gt;info_list;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (idx = 0, level = 1; level &lt;= this_cpu_ci-&gt;num_levels &amp;&amp;</span>
<span class="p_add">+	     idx &lt; this_cpu_ci-&gt;num_leaves; idx++, level++) {</span>
<span class="p_add">+		ci_leaf_init(this_leaf++, CACHE_TYPE_DATA, level);</span>
<span class="p_add">+		ci_leaf_init(this_leaf++, CACHE_TYPE_INST, level);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/nds32/mm/cacheflush.c b/arch/nds32/mm/cacheflush.c</span>
new file mode 100644
<span class="p_header">index 0000000..69a85f7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/mm/cacheflush.c</span>
<span class="p_chunk">@@ -0,0 +1,331 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/fs.h&gt;</span>
<span class="p_add">+#include &lt;linux/pagemap.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+#include &lt;asm/cacheflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/proc-fns.h&gt;</span>
<span class="p_add">+#include &lt;asm/shmparam.h&gt;</span>
<span class="p_add">+#include &lt;asm/cache_info.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct cache_info L1_cache_info[2];</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_CPU_CACHE_ALIASING</span>
<span class="p_add">+void update_mmu_cache(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="p_add">+		      pte_t * pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	unsigned long pfn = pte_pfn(*pte);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pfn_valid(pfn))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_mm == current-&gt;active_mm) {</span>
<span class="p_add">+</span>
<span class="p_add">+		__nds32__mtsr_dsb(addr, NDS32_SR_TLB_VPN);</span>
<span class="p_add">+		__nds32__tlbop_rwr(*pte);</span>
<span class="p_add">+		__nds32__isb();</span>
<span class="p_add">+	}</span>
<span class="p_add">+	page = pfn_to_page(pfn);</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((test_and_clear_bit(PG_dcache_dirty, &amp;page-&gt;flags)) ||</span>
<span class="p_add">+	    (vma-&gt;vm_flags &amp; VM_EXEC)) {</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!PageHighMem(page)) {</span>
<span class="p_add">+			cpu_cache_wbinval_page((unsigned long)</span>
<span class="p_add">+					       page_address(page),</span>
<span class="p_add">+					       vma-&gt;vm_flags &amp; VM_EXEC);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			unsigned long kaddr = (unsigned long)kmap_atomic(page);</span>
<span class="p_add">+			cpu_cache_wbinval_page(kaddr, vma-&gt;vm_flags &amp; VM_EXEC);</span>
<span class="p_add">+			kunmap_atomic((void *)kaddr);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+extern pte_t va_present(struct mm_struct *mm, unsigned long addr);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long aliasing(unsigned long addr, unsigned long page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ((addr &amp; PAGE_MASK) ^ page) &amp; (SHMLBA - 1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long kremap0(unsigned long uaddr, unsigned long pa)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long kaddr, pte;</span>
<span class="p_add">+</span>
<span class="p_add">+#define BASE_ADDR0 0xffffc000</span>
<span class="p_add">+	kaddr = BASE_ADDR0 | (uaddr &amp; L1_cache_info[DCACHE].aliasing_mask);</span>
<span class="p_add">+	pte = (pa | PAGE_KERNEL);</span>
<span class="p_add">+	__nds32__mtsr_dsb(kaddr, NDS32_SR_TLB_VPN);</span>
<span class="p_add">+	__nds32__tlbop_rwlk(pte);</span>
<span class="p_add">+	__nds32__isb();</span>
<span class="p_add">+	return kaddr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void kunmap01(unsigned long kaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__nds32__tlbop_unlk(kaddr);</span>
<span class="p_add">+	__nds32__tlbop_inv(kaddr);</span>
<span class="p_add">+	__nds32__isb();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long kremap1(unsigned long uaddr, unsigned long pa)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long kaddr, pte;</span>
<span class="p_add">+</span>
<span class="p_add">+#define BASE_ADDR1 0xffff8000</span>
<span class="p_add">+	kaddr = BASE_ADDR1 | (uaddr &amp; L1_cache_info[DCACHE].aliasing_mask);</span>
<span class="p_add">+	pte = (pa | PAGE_KERNEL);</span>
<span class="p_add">+	__nds32__mtsr_dsb(kaddr, NDS32_SR_TLB_VPN);</span>
<span class="p_add">+	__nds32__tlbop_rwlk(pte);</span>
<span class="p_add">+	__nds32__isb();</span>
<span class="p_add">+	return kaddr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_cache_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cpu_dcache_wbinval_all();</span>
<span class="p_add">+	cpu_icache_inval_all();</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_cache_dup_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_cache_range(struct vm_area_struct *vma,</span>
<span class="p_add">+		       unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((end - start) &gt; 8 * PAGE_SIZE) {</span>
<span class="p_add">+		cpu_dcache_wbinval_all();</span>
<span class="p_add">+		if (vma-&gt;vm_flags &amp; VM_EXEC)</span>
<span class="p_add">+			cpu_icache_inval_all();</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	while (start &lt; end) {</span>
<span class="p_add">+		if (va_present(vma-&gt;vm_mm, start))</span>
<span class="p_add">+			cpu_cache_wbinval_page(start, vma-&gt;vm_flags &amp; VM_EXEC);</span>
<span class="p_add">+		start += PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_cache_page(struct vm_area_struct *vma,</span>
<span class="p_add">+		      unsigned long addr, unsigned long pfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vto, flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	vto = kremap0(addr, pfn &lt;&lt; PAGE_SHIFT);</span>
<span class="p_add">+	cpu_cache_wbinval_page(vto, vma-&gt;vm_flags &amp; VM_EXEC);</span>
<span class="p_add">+	kunmap01(vto);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_cache_vmap(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	cpu_dcache_wbinval_all();</span>
<span class="p_add">+	cpu_icache_inval_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_cache_vunmap(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	cpu_dcache_wbinval_all();</span>
<span class="p_add">+	cpu_icache_inval_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void copy_user_highpage(struct page *to, struct page *from,</span>
<span class="p_add">+			unsigned long vaddr, struct vm_area_struct *vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vto, vfrom, flags, kto, kfrom, pfrom, pto;</span>
<span class="p_add">+	kto = ((unsigned long)page_address(to) &amp; PAGE_MASK);</span>
<span class="p_add">+	kfrom = ((unsigned long)page_address(from) &amp; PAGE_MASK);</span>
<span class="p_add">+	pto = page_to_phys(to);</span>
<span class="p_add">+	pfrom = page_to_phys(from);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (aliasing(vaddr, (unsigned long)kfrom))</span>
<span class="p_add">+		cpu_dcache_wb_page((unsigned long)kfrom);</span>
<span class="p_add">+	if (aliasing(vaddr, (unsigned long)kto))</span>
<span class="p_add">+		cpu_dcache_inval_page((unsigned long)kto);</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	vto = kremap0(vaddr, pto);</span>
<span class="p_add">+	vfrom = kremap1(vaddr, pfrom);</span>
<span class="p_add">+	copy_page((void *)vto, (void *)vfrom);</span>
<span class="p_add">+	kunmap01(vfrom);</span>
<span class="p_add">+	kunmap01(vto);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+EXPORT_SYMBOL(copy_user_highpage);</span>
<span class="p_add">+</span>
<span class="p_add">+void clear_user_highpage(struct page *page, unsigned long vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vto, flags, kto;</span>
<span class="p_add">+</span>
<span class="p_add">+	kto = ((unsigned long)page_address(page) &amp; PAGE_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	if (aliasing(kto, vaddr) &amp;&amp; kto != 0) {</span>
<span class="p_add">+		cpu_dcache_inval_page(kto);</span>
<span class="p_add">+		cpu_icache_inval_page(kto);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	vto = kremap0(vaddr, page_to_phys(page));</span>
<span class="p_add">+	clear_page((void *)vto);</span>
<span class="p_add">+	kunmap01(vto);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+EXPORT_SYMBOL(clear_user_highpage);</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_dcache_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct address_space *mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+	mapping = page_mapping(page);</span>
<span class="p_add">+	if (mapping &amp;&amp; !mapping_mapped(mapping))</span>
<span class="p_add">+		set_bit(PG_dcache_dirty, &amp;page-&gt;flags);</span>
<span class="p_add">+	else {</span>
<span class="p_add">+		int i, pc;</span>
<span class="p_add">+		unsigned long vto, kaddr, flags;</span>
<span class="p_add">+		kaddr = (unsigned long)page_address(page);</span>
<span class="p_add">+		cpu_dcache_wbinval_page(kaddr);</span>
<span class="p_add">+		pc = CACHE_SET(DCACHE) * CACHE_LINE_SIZE(DCACHE) / PAGE_SIZE;</span>
<span class="p_add">+		local_irq_save(flags);</span>
<span class="p_add">+		for (i = 0; i &lt; pc; i++) {</span>
<span class="p_add">+			vto =</span>
<span class="p_add">+			    kremap0(kaddr + i * PAGE_SIZE, page_to_phys(page));</span>
<span class="p_add">+			cpu_dcache_wbinval_page(vto);</span>
<span class="p_add">+			kunmap01(vto);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		local_irq_restore(flags);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void copy_to_user_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="p_add">+		       unsigned long vaddr, void *dst, void *src, int len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size, start, end, vto, flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	vto = kremap0(vaddr, page_to_phys(page));</span>
<span class="p_add">+	dst = (void *)(vto | (vaddr &amp; (PAGE_SIZE - 1)));</span>
<span class="p_add">+	memcpy(dst, src, len);</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_EXEC) {</span>
<span class="p_add">+		line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+		start = (unsigned long)dst &amp; ~(line_size - 1);</span>
<span class="p_add">+		end =</span>
<span class="p_add">+		    ((unsigned long)dst + len + line_size - 1) &amp; ~(line_size -</span>
<span class="p_add">+								   1);</span>
<span class="p_add">+		cpu_cache_wbinval_range(start, end, 1);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	kunmap01(vto);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void copy_from_user_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="p_add">+			 unsigned long vaddr, void *dst, void *src, int len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vto, flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	vto = kremap0(vaddr, page_to_phys(page));</span>
<span class="p_add">+	src = (void *)(vto | (vaddr &amp; (PAGE_SIZE - 1)));</span>
<span class="p_add">+	memcpy(dst, src, len);</span>
<span class="p_add">+	kunmap01(vto);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_anon_page(struct vm_area_struct *vma,</span>
<span class="p_add">+		     struct page *page, unsigned long vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	if (!PageAnon(page))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_mm != current-&gt;active_mm)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; VM_EXEC)</span>
<span class="p_add">+		cpu_icache_inval_page(vaddr &amp; PAGE_MASK);</span>
<span class="p_add">+	cpu_dcache_wbinval_page((unsigned long)page_address(page));</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_kernel_dcache_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cpu_dcache_wbinval_page((unsigned long)page_address(page));</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_icache_range(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size, flags;</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+	start = start &amp; ~(line_size - 1);</span>
<span class="p_add">+	end = (end + line_size - 1) &amp; ~(line_size - 1);</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cpu_cache_wbinval_range(start, end, 1);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void flush_icache_page(struct vm_area_struct *vma, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cpu_cache_wbinval_page((unsigned long)page_address(page),</span>
<span class="p_add">+			       vma-&gt;vm_flags &amp; VM_EXEC);</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void update_mmu_cache(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="p_add">+		      pte_t * pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	unsigned long pfn = pte_pfn(*pte);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pfn_valid(pfn))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vma-&gt;vm_mm == current-&gt;active_mm) {</span>
<span class="p_add">+		__nds32__mtsr_dsb(addr, NDS32_SR_TLB_VPN);</span>
<span class="p_add">+		__nds32__tlbop_rwr(*pte);</span>
<span class="p_add">+		__nds32__isb();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	page = pfn_to_page(pfn);</span>
<span class="p_add">+	if (test_and_clear_bit(PG_dcache_dirty, &amp;page-&gt;flags) ||</span>
<span class="p_add">+	    (vma-&gt;vm_flags &amp; VM_EXEC)) {</span>
<span class="p_add">+		local_irq_save(flags);</span>
<span class="p_add">+		cpu_dcache_wbinval_page((unsigned long)page_address(page));</span>
<span class="p_add">+		local_irq_restore(flags);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/nds32/mm/proc.c b/arch/nds32/mm/proc.c</span>
new file mode 100644
<span class="p_header">index 0000000..b78c25a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/mm/proc.c</span>
<span class="p_chunk">@@ -0,0 +1,527 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;asm/nds32.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/cacheflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/l2_cache.h&gt;</span>
<span class="p_add">+#include &lt;nds32_intrinsic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cache_info.h&gt;</span>
<span class="p_add">+extern struct cache_info L1_cache_info[2];</span>
<span class="p_add">+</span>
<span class="p_add">+int va_kernel_present(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *ptep, pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pgd_offset_k(addr), addr);</span>
<span class="p_add">+	if (!pmd_none(*pmd)) {</span>
<span class="p_add">+		ptep = pte_offset_map(pmd, addr);</span>
<span class="p_add">+		pte = *ptep;</span>
<span class="p_add">+		if (pte_present(pte))</span>
<span class="p_add">+			return pte;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+pte_t va_present(struct mm_struct * mm, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *ptep, pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = pgd_offset(mm, addr);</span>
<span class="p_add">+	if (!pgd_none(*pgd)) {</span>
<span class="p_add">+		pud = pud_offset(pgd, addr);</span>
<span class="p_add">+		if (!pud_none(*pud)) {</span>
<span class="p_add">+			pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+			if (!pmd_none(*pmd)) {</span>
<span class="p_add">+				ptep = pte_offset_map(pmd, addr);</span>
<span class="p_add">+				pte = *ptep;</span>
<span class="p_add">+				if (pte_present(pte))</span>
<span class="p_add">+					return pte;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int va_readable(struct pt_regs *regs, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	pte_t pte;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (user_mode(regs)) {</span>
<span class="p_add">+		/* user mode */</span>
<span class="p_add">+		pte = va_present(mm, addr);</span>
<span class="p_add">+		if (!pte &amp;&amp; pte_read(pte))</span>
<span class="p_add">+			ret = 1;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* superuser mode is always readable, so we can only</span>
<span class="p_add">+		 * check it is present or not*/</span>
<span class="p_add">+		return (! !va_kernel_present(addr));</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int va_writable(struct pt_regs *regs, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	pte_t pte;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (user_mode(regs)) {</span>
<span class="p_add">+		/* user mode */</span>
<span class="p_add">+		pte = va_present(mm, addr);</span>
<span class="p_add">+		if (!pte &amp;&amp; pte_write(pte))</span>
<span class="p_add">+			ret = 1;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* superuser mode */</span>
<span class="p_add">+		pte = va_kernel_present(addr);</span>
<span class="p_add">+		if (!pte &amp;&amp; pte_kernel_write(pte))</span>
<span class="p_add">+			ret = 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * All</span>
<span class="p_add">+ */</span>
<span class="p_add">+void cpu_icache_inval_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long end, line_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	line_size = L1_cache_info[ICACHE].line_size;</span>
<span class="p_add">+	end =</span>
<span class="p_add">+	    line_size * L1_cache_info[ICACHE].ways * L1_cache_info[ICACHE].sets;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1I_IX_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1I_IX_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1I_IX_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1I_IX_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+	} while (end &gt; 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dcache_inval_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__nds32__cctl_l1d_invalall();</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dcache_wb_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_CACHE_L2</span>
<span class="p_add">+	if (atl2c_base)</span>
<span class="p_add">+		__nds32__cctl_l1d_wball_alvl();</span>
<span class="p_add">+	else</span>
<span class="p_add">+		__nds32__cctl_l1d_wball_one_lvl();</span>
<span class="p_add">+#else</span>
<span class="p_add">+	__nds32__cctl_l1d_wball_one_lvl();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dcache_wbinval_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifndef CONFIG_CPU_DCACHE_WRITETHROUGH</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	cpu_dcache_wb_all();</span>
<span class="p_add">+	cpu_dcache_inval_all();</span>
<span class="p_add">+#ifndef CONFIG_CPU_DCACHE_WRITETHROUGH</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Page</span>
<span class="p_add">+ */</span>
<span class="p_add">+void cpu_icache_inval_page(unsigned long start)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size, end;</span>
<span class="p_add">+</span>
<span class="p_add">+	line_size = L1_cache_info[ICACHE].line_size;</span>
<span class="p_add">+	end = start + PAGE_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1I_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1I_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1I_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1I_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+	} while (end != start);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dcache_inval_page(unsigned long start)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size, end;</span>
<span class="p_add">+</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+	end = start + PAGE_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+	} while (end != start);</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dcache_wb_page(unsigned long start)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifndef CONFIG_CPU_DCACHE_WRITETHROUGH</span>
<span class="p_add">+	unsigned long line_size, end;</span>
<span class="p_add">+</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+	end = start + PAGE_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_WB&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_WB&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_WB&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_WB&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+	} while (end != start);</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dcache_wbinval_page(unsigned long start)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size, end;</span>
<span class="p_add">+</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+	end = start + PAGE_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+#ifndef CONFIG_CPU_DCACHE_WRITETHROUGH</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_WB&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+#ifndef CONFIG_CPU_DCACHE_WRITETHROUGH</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_WB&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+#ifndef CONFIG_CPU_DCACHE_WRITETHROUGH</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_WB&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+		end -= line_size;</span>
<span class="p_add">+#ifndef CONFIG_CPU_DCACHE_WRITETHROUGH</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_WB&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_INVAL&quot;::&quot;r&quot; (end));</span>
<span class="p_add">+	} while (end != start);</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_cache_wbinval_page(unsigned long page, int flushi)</span>
<span class="p_add">+{</span>
<span class="p_add">+	cpu_dcache_wbinval_page(page);</span>
<span class="p_add">+	if (flushi)</span>
<span class="p_add">+		cpu_icache_inval_page(page);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Range</span>
<span class="p_add">+ */</span>
<span class="p_add">+void cpu_icache_inval_range(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	line_size = L1_cache_info[ICACHE].line_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (end &gt; start) {</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1I_VA_INVAL&quot;::&quot;r&quot; (start));</span>
<span class="p_add">+		start += line_size;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dcache_inval_range(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (end &gt; start) {</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_INVAL&quot;::&quot;r&quot; (start));</span>
<span class="p_add">+		start += line_size;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dcache_wb_range(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifndef CONFIG_CPU_DCACHE_WRITETHROUGH</span>
<span class="p_add">+	unsigned long line_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (end &gt; start) {</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_WB&quot;::&quot;r&quot; (start));</span>
<span class="p_add">+		start += line_size;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dcache_wbinval_range(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (end &gt; start) {</span>
<span class="p_add">+#ifndef CONFIG_CPU_DCACHE_WRITETHROUGH</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_WB&quot;::&quot;r&quot; (start));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		__asm__ volatile (&quot;\n\tcctl %0, L1D_VA_INVAL&quot;::&quot;r&quot; (start));</span>
<span class="p_add">+		start += line_size;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_cache_wbinval_range(unsigned long start, unsigned long end, int flushi)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size, align_start, align_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+	align_start = start &amp; ~(line_size - 1);</span>
<span class="p_add">+	align_end = (end + line_size - 1) &amp; ~(line_size - 1);</span>
<span class="p_add">+	cpu_dcache_wbinval_range(align_start, align_end);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flushi) {</span>
<span class="p_add">+		line_size = L1_cache_info[ICACHE].line_size;</span>
<span class="p_add">+		align_start = start &amp; ~(line_size - 1);</span>
<span class="p_add">+		align_end = (end + line_size - 1) &amp; ~(line_size - 1);</span>
<span class="p_add">+		cpu_icache_inval_range(align_start, align_end);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_cache_wbinval_range_check(struct vm_area_struct *vma,</span>
<span class="p_add">+				   unsigned long start, unsigned long end,</span>
<span class="p_add">+				   bool flushi, bool wbd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size, t_start, t_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!flushi &amp;&amp; !wbd)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+	start = start &amp; ~(line_size - 1);</span>
<span class="p_add">+	end = (end + line_size - 1) &amp; ~(line_size - 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((end - start) &gt; (8 * PAGE_SIZE)) {</span>
<span class="p_add">+		if (wbd)</span>
<span class="p_add">+			cpu_dcache_wbinval_all();</span>
<span class="p_add">+		if (flushi)</span>
<span class="p_add">+			cpu_icache_inval_all();</span>
<span class="p_add">+		__nds32__msync_all();</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	t_start = (start + PAGE_SIZE) &amp; PAGE_MASK;</span>
<span class="p_add">+	t_end = ((end - 1) &amp; PAGE_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((start &amp; PAGE_MASK) == t_end) {</span>
<span class="p_add">+		if (va_present(vma-&gt;vm_mm, start)) {</span>
<span class="p_add">+			if (wbd)</span>
<span class="p_add">+				cpu_dcache_wbinval_range(start, end);</span>
<span class="p_add">+			if (flushi)</span>
<span class="p_add">+				cpu_icache_inval_range(start, end);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		__nds32__msync_all();</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (va_present(vma-&gt;vm_mm, start)) {</span>
<span class="p_add">+		if (wbd)</span>
<span class="p_add">+			cpu_dcache_wbinval_range(start, t_start);</span>
<span class="p_add">+		if (flushi)</span>
<span class="p_add">+			cpu_icache_inval_range(start, t_start);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (va_present(vma-&gt;vm_mm, end - 1)) {</span>
<span class="p_add">+		if (wbd)</span>
<span class="p_add">+			cpu_dcache_wbinval_range(t_end, end);</span>
<span class="p_add">+		if (flushi)</span>
<span class="p_add">+			cpu_icache_inval_range(t_end, end);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	while (t_start &lt; t_end) {</span>
<span class="p_add">+		if (va_present(vma-&gt;vm_mm, t_start)) {</span>
<span class="p_add">+			if (wbd)</span>
<span class="p_add">+				cpu_dcache_wbinval_page(t_start);</span>
<span class="p_add">+			if (flushi)</span>
<span class="p_add">+				cpu_icache_inval_page(t_start);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		t_start += PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void cpu_l2cache_op(unsigned long start, unsigned long end, unsigned long op)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_CACHE_L2</span>
<span class="p_add">+	if (atl2c_base) {</span>
<span class="p_add">+		unsigned long p_start = __pa(start);</span>
<span class="p_add">+		unsigned long p_end = __pa(end);</span>
<span class="p_add">+		unsigned long cmd;</span>
<span class="p_add">+		unsigned long line_size;</span>
<span class="p_add">+		/* TODO Can Use PAGE Mode to optimize if range large than PAGE_SIZE */</span>
<span class="p_add">+		line_size = L2_CACHE_LINE_SIZE();</span>
<span class="p_add">+		p_start = p_start &amp; (~(line_size - 1));</span>
<span class="p_add">+		p_end = (p_end + line_size - 1) &amp; (~(line_size - 1));</span>
<span class="p_add">+		cmd =</span>
<span class="p_add">+		    (p_start &amp; ~(line_size - 1)) | op |</span>
<span class="p_add">+		    CCTL_SINGLE_CMD;</span>
<span class="p_add">+		do {</span>
<span class="p_add">+			L2_CMD_RDY();</span>
<span class="p_add">+			L2C_W_REG(L2_CCTL_CMD_OFF, cmd);</span>
<span class="p_add">+			cmd += line_size;</span>
<span class="p_add">+			p_start += line_size;</span>
<span class="p_add">+		} while (p_end &gt; p_start);</span>
<span class="p_add">+		cmd = CCTL_CMD_L2_SYNC;</span>
<span class="p_add">+		L2_CMD_RDY();</span>
<span class="p_add">+		L2C_W_REG(L2_CCTL_CMD_OFF, cmd);</span>
<span class="p_add">+		L2_CMD_RDY();</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * DMA</span>
<span class="p_add">+ */</span>
<span class="p_add">+void cpu_dma_wb_range(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+	start = start &amp; (~(line_size - 1));</span>
<span class="p_add">+	end = (end + line_size - 1) &amp; (~(line_size - 1));</span>
<span class="p_add">+	if (unlikely(start == end))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cpu_dcache_wb_range(start, end);</span>
<span class="p_add">+	cpu_l2cache_op(start, end, CCTL_CMD_L2_PA_WB);</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dma_inval_range(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size;</span>
<span class="p_add">+	unsigned long old_start = start;</span>
<span class="p_add">+	unsigned long old_end = end;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+	start = start &amp; (~(line_size - 1));</span>
<span class="p_add">+	end = (end + line_size - 1) &amp; (~(line_size - 1));</span>
<span class="p_add">+	if (unlikely(start == end))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	if (start != old_start) {</span>
<span class="p_add">+		cpu_dcache_wbinval_range(start, start + line_size);</span>
<span class="p_add">+		cpu_l2cache_op(start, start + line_size, CCTL_CMD_L2_PA_WBINVAL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (end != old_end) {</span>
<span class="p_add">+		cpu_dcache_wbinval_range(end - line_size, end);</span>
<span class="p_add">+		cpu_l2cache_op(end - line_size, end, CCTL_CMD_L2_PA_WBINVAL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	cpu_dcache_inval_range(start, end);</span>
<span class="p_add">+	cpu_l2cache_op(start, end, CCTL_CMD_L2_PA_INVAL);</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_dma_wbinval_range(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long line_size;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	line_size = L1_cache_info[DCACHE].line_size;</span>
<span class="p_add">+	start = start &amp; (~(line_size - 1));</span>
<span class="p_add">+	end = (end + line_size - 1) &amp; (~(line_size - 1));</span>
<span class="p_add">+	if (unlikely(start == end))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+	cpu_dcache_wbinval_range(start, end);</span>
<span class="p_add">+	cpu_l2cache_op(start, end, CCTL_CMD_L2_PA_WBINVAL);</span>
<span class="p_add">+	__nds32__msync_all();</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_proc_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_proc_fin(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_do_idle(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__nds32__standby_no_wake_grant();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_reset(unsigned long reset)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 tmp;</span>
<span class="p_add">+	GIE_DISABLE();</span>
<span class="p_add">+	tmp = __nds32__mfsr(NDS32_SR_CACHE_CTL);</span>
<span class="p_add">+	tmp &amp;= ~(CACHE_CTL_mskIC_EN | CACHE_CTL_mskDC_EN);</span>
<span class="p_add">+	__nds32__mtsr_isb(tmp, NDS32_SR_CACHE_CTL);</span>
<span class="p_add">+	cpu_dcache_wbinval_all();</span>
<span class="p_add">+	cpu_icache_inval_all();</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(&quot;jr.toff %0\n\t&quot;::&quot;r&quot;(reset));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void cpu_switch_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long cid;</span>
<span class="p_add">+	cid = __nds32__mfsr(NDS32_SR_TLB_MISC);</span>
<span class="p_add">+	cid = (cid &amp; ~TLB_MISC_mskCID) | mm-&gt;context.id;</span>
<span class="p_add">+	__nds32__mtsr_dsb(cid, NDS32_SR_TLB_MISC);</span>
<span class="p_add">+	__nds32__mtsr_isb(__pa(mm-&gt;pgd), NDS32_SR_L1_PPTB);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/nds32/mm/tlb.c b/arch/nds32/mm/tlb.c</span>
new file mode 100644
<span class="p_header">index 0000000..b397b9f</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/mm/tlb.c</span>
<span class="p_chunk">@@ -0,0 +1,63 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/spinlock_types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;asm/nds32.h&gt;</span>
<span class="p_add">+#include &lt;nds32_intrinsic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned int cpu_last_cid = { TLB_MISC_mskCID + (2 &lt;&lt; TLB_MISC_offCID) };</span>
<span class="p_add">+</span>
<span class="p_add">+DEFINE_SPINLOCK(cid_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+void local_flush_tlb_range(struct vm_area_struct *vma,</span>
<span class="p_add">+			   unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags, ocid, ncid;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((end - start) &gt; 0x400000) {</span>
<span class="p_add">+		__nds32__tlbop_flua();</span>
<span class="p_add">+		__nds32__isb();</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;cid_lock, flags);</span>
<span class="p_add">+	ocid = __nds32__mfsr(NDS32_SR_TLB_MISC);</span>
<span class="p_add">+	ncid = (ocid &amp; ~TLB_MISC_mskCID) | vma-&gt;vm_mm-&gt;context.id;</span>
<span class="p_add">+	__nds32__mtsr_dsb(ncid, NDS32_SR_TLB_MISC);</span>
<span class="p_add">+	while (start &lt; end) {</span>
<span class="p_add">+		__nds32__tlbop_inv(start);</span>
<span class="p_add">+		__nds32__isb();</span>
<span class="p_add">+		start += PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	__nds32__mtsr_dsb(ocid, NDS32_SR_TLB_MISC);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;cid_lock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void local_flush_tlb_page(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags, ocid, ncid;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;cid_lock, flags);</span>
<span class="p_add">+	ocid = __nds32__mfsr(NDS32_SR_TLB_MISC);</span>
<span class="p_add">+	ncid = (ocid &amp; ~TLB_MISC_mskCID) | vma-&gt;vm_mm-&gt;context.id;</span>
<span class="p_add">+	__nds32__mtsr_dsb(ncid, NDS32_SR_TLB_MISC);</span>
<span class="p_add">+	__nds32__tlbop_inv(addr);</span>
<span class="p_add">+	__nds32__isb();</span>
<span class="p_add">+	__nds32__mtsr_dsb(ocid, NDS32_SR_TLB_MISC);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;cid_lock, flags);</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



