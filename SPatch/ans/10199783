
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[06/64] mm: teach pagefault paths about range locking - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [06/64] mm: teach pagefault paths about range locking</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=109101">Davidlohr Bueso</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 5, 2018, 1:26 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180205012754.23615-7-dbueso@wotan.suse.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10199783/mbox/"
   >mbox</a>
|
   <a href="/patch/10199783/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10199783/">/patch/10199783/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	EADDA60247 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  5 Feb 2018 01:42:19 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C1E8D283ED
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  5 Feb 2018 01:42:19 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B4BB5285CB; Mon,  5 Feb 2018 01:42:19 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B6B03285D5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  5 Feb 2018 01:42:09 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752760AbeBEBmC (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 4 Feb 2018 20:42:02 -0500
Received: from mx2.suse.de ([195.135.220.15]:44098 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752294AbeBEB2U (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 4 Feb 2018 20:28:20 -0500
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay1.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id C392BADDB;
	Mon,  5 Feb 2018 01:27:59 +0000 (UTC)
From: Davidlohr Bueso &lt;dbueso@suse.de&gt;
To: akpm@linux-foundation.org, mingo@kernel.org
Cc: peterz@infradead.org, ldufour@linux.vnet.ibm.com, jack@suse.cz,
	mhocko@kernel.org, kirill.shutemov@linux.intel.com,
	mawilcox@microsoft.com, mgorman@techsingularity.net,
	dave@stgolabs.net, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org, Davidlohr Bueso &lt;dbueso@suse.de&gt;
Subject: [PATCH 06/64] mm: teach pagefault paths about range locking
Date: Mon,  5 Feb 2018 02:26:56 +0100
Message-Id: &lt;20180205012754.23615-7-dbueso@wotan.suse.de&gt;
X-Mailer: git-send-email 2.12.3
In-Reply-To: &lt;20180205012754.23615-1-dbueso@wotan.suse.de&gt;
References: &lt;20180205012754.23615-1-dbueso@wotan.suse.de&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=109101">Davidlohr Bueso</a> - Feb. 5, 2018, 1:26 a.m.</div>
<pre class="content">
<span class="from">From: Davidlohr Bueso &lt;dave@stgolabs.net&gt;</span>

In handle_mm_fault() we need to remember the range lock specified
when the mmap_sem was first taken as pf paths can drop the lock.
Although this patch may seem far too big at first, it is so due to
bisectability, and later conversion patches become quite easy to
follow. Furthermore, most of what this patch does is pass a pointer
to an &#39;mmrange&#39; stack allocated parameter that is later used by the
vm_fault structure. The new interfaces are pretty much all in the
following areas:

- vma handling (vma_merge(), vma_adjust(), split_vma(), copy_vma())
- gup family (all except get_user_pages_unlocked(), which internally
  passes the mmrange).
- mm walking (walk_page_vma())
- mmap/unmap (do_mmap(), do_munmap())
- handle_mm_fault(), fixup_user_fault()

Most of the pain of the patch is updating all callers in the kernel
for this. While tedious, it is not that hard to review, I hope.
The idea is to use a local variable (no concurrency) whenever the
mmap_sem is taken and we end up in pf paths that end up retaking
the lock. Ie:

  DEFINE_RANGE_LOCK_FULL(mmrange);

  down_write(&amp;mm-&gt;mmap_sem);
  some_fn(a, b, c, &amp;mmrange);
  ....
   ....
    ...
     handle_mm_fault(vma, addr, flags, mmrange);
    ...
  up_write(&amp;mm-&gt;mmap_sem);

Semantically nothing changes at all, and the &#39;mmrange&#39; ends up
being unused for now. Later patches will use the variable when
the mmap_sem wrappers replace straightforward down/up.

Compile tested defconfigs on various non-x86 archs without breaking.
<span class="signed-off-by">
Signed-off-by: Davidlohr Bueso &lt;dbueso@suse.de&gt;</span>
---
 arch/alpha/mm/fault.c                      |  3 +-
 arch/arc/mm/fault.c                        |  3 +-
 arch/arm/mm/fault.c                        |  8 ++-
 arch/arm/probes/uprobes/core.c             |  5 +-
 arch/arm64/mm/fault.c                      |  7 ++-
 arch/cris/mm/fault.c                       |  3 +-
 arch/frv/mm/fault.c                        |  3 +-
 arch/hexagon/mm/vm_fault.c                 |  3 +-
 arch/ia64/mm/fault.c                       |  3 +-
 arch/m32r/mm/fault.c                       |  3 +-
 arch/m68k/mm/fault.c                       |  3 +-
 arch/metag/mm/fault.c                      |  3 +-
 arch/microblaze/mm/fault.c                 |  3 +-
 arch/mips/kernel/vdso.c                    |  3 +-
 arch/mips/mm/fault.c                       |  3 +-
 arch/mn10300/mm/fault.c                    |  3 +-
 arch/nios2/mm/fault.c                      |  3 +-
 arch/openrisc/mm/fault.c                   |  3 +-
 arch/parisc/mm/fault.c                     |  3 +-
 arch/powerpc/include/asm/mmu_context.h     |  3 +-
 arch/powerpc/include/asm/powernv.h         |  5 +-
 arch/powerpc/mm/copro_fault.c              |  4 +-
 arch/powerpc/mm/fault.c                    |  3 +-
 arch/powerpc/platforms/powernv/npu-dma.c   |  5 +-
 arch/riscv/mm/fault.c                      |  3 +-
 arch/s390/include/asm/gmap.h               | 14 +++--
 arch/s390/kvm/gaccess.c                    | 31 ++++++----
 arch/s390/mm/fault.c                       |  3 +-
 arch/s390/mm/gmap.c                        | 80 +++++++++++++++---------
 arch/score/mm/fault.c                      |  3 +-
 arch/sh/mm/fault.c                         |  3 +-
 arch/sparc/mm/fault_32.c                   |  6 +-
 arch/sparc/mm/fault_64.c                   |  3 +-
 arch/tile/mm/fault.c                       |  3 +-
 arch/um/include/asm/mmu_context.h          |  3 +-
 arch/um/kernel/trap.c                      |  3 +-
 arch/unicore32/mm/fault.c                  |  8 ++-
 arch/x86/entry/vdso/vma.c                  |  3 +-
 arch/x86/include/asm/mmu_context.h         |  5 +-
 arch/x86/include/asm/mpx.h                 |  6 +-
 arch/x86/mm/fault.c                        |  3 +-
 arch/x86/mm/mpx.c                          | 41 ++++++++-----
 arch/xtensa/mm/fault.c                     |  3 +-
 drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c    |  3 +-
 drivers/gpu/drm/i915/i915_gem_userptr.c    |  4 +-
 drivers/gpu/drm/radeon/radeon_ttm.c        |  4 +-
 drivers/infiniband/core/umem.c             |  3 +-
 drivers/infiniband/core/umem_odp.c         |  3 +-
 drivers/infiniband/hw/qib/qib_user_pages.c |  7 ++-
 drivers/infiniband/hw/usnic/usnic_uiom.c   |  3 +-
 drivers/iommu/amd_iommu_v2.c               |  5 +-
 drivers/iommu/intel-svm.c                  |  5 +-
 drivers/media/v4l2-core/videobuf-dma-sg.c  | 18 ++++--
 drivers/misc/mic/scif/scif_rma.c           |  3 +-
 drivers/misc/sgi-gru/grufault.c            | 43 ++++++++-----
 drivers/vfio/vfio_iommu_type1.c            |  3 +-
 fs/aio.c                                   |  3 +-
 fs/binfmt_elf.c                            |  3 +-
 fs/exec.c                                  | 20 ++++--
 fs/proc/internal.h                         |  3 +
 fs/proc/task_mmu.c                         | 29 ++++++---
 fs/proc/vmcore.c                           | 14 ++++-
 fs/userfaultfd.c                           | 18 +++---
 include/asm-generic/mm_hooks.h             |  3 +-
 include/linux/hmm.h                        |  4 +-
 include/linux/ksm.h                        |  6 +-
 include/linux/migrate.h                    |  4 +-
 include/linux/mm.h                         | 73 +++++++++++++---------
 include/linux/uprobes.h                    | 15 +++--
 ipc/shm.c                                  | 14 +++--
 kernel/events/uprobes.c                    | 49 +++++++++------
 kernel/futex.c                             |  3 +-
 mm/frame_vector.c                          |  4 +-
 mm/gup.c                                   | 60 ++++++++++--------
 mm/hmm.c                                   | 37 ++++++-----
 mm/internal.h                              |  3 +-
 mm/ksm.c                                   | 24 +++++---
 mm/madvise.c                               | 58 ++++++++++-------
 mm/memcontrol.c                            | 13 ++--
 mm/memory.c                                | 10 +--
 mm/mempolicy.c                             | 35 ++++++-----
 mm/migrate.c                               | 20 +++---
 mm/mincore.c                               | 24 +++++---
 mm/mlock.c                                 | 33 ++++++----
 mm/mmap.c                                  | 99 +++++++++++++++++-------------
 mm/mprotect.c                              | 14 +++--
 mm/mremap.c                                | 30 +++++----
 mm/nommu.c                                 | 32 ++++++----
 mm/pagewalk.c                              | 56 +++++++++--------
 mm/process_vm_access.c                     |  4 +-
 mm/util.c                                  |  3 +-
 security/tomoyo/domain.c                   |  3 +-
 virt/kvm/async_pf.c                        |  3 +-
 virt/kvm/kvm_main.c                        | 16 +++--
 94 files changed, 784 insertions(+), 474 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Feb. 5, 2018, 4:09 p.m.</div>
<pre class="content">
On 05/02/2018 02:26, Davidlohr Bueso wrote:
<span class="quote">&gt; From: Davidlohr Bueso &lt;dave@stgolabs.net&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In handle_mm_fault() we need to remember the range lock specified</span>
<span class="quote">&gt; when the mmap_sem was first taken as pf paths can drop the lock.</span>
<span class="quote">&gt; Although this patch may seem far too big at first, it is so due to</span>
<span class="quote">&gt; bisectability, and later conversion patches become quite easy to</span>
<span class="quote">&gt; follow. Furthermore, most of what this patch does is pass a pointer</span>
<span class="quote">&gt; to an &#39;mmrange&#39; stack allocated parameter that is later used by the</span>
<span class="quote">&gt; vm_fault structure. The new interfaces are pretty much all in the</span>
<span class="quote">&gt; following areas:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - vma handling (vma_merge(), vma_adjust(), split_vma(), copy_vma())</span>
<span class="quote">&gt; - gup family (all except get_user_pages_unlocked(), which internally</span>
<span class="quote">&gt;   passes the mmrange).</span>
<span class="quote">&gt; - mm walking (walk_page_vma())</span>
<span class="quote">&gt; - mmap/unmap (do_mmap(), do_munmap())</span>
<span class="quote">&gt; - handle_mm_fault(), fixup_user_fault()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Most of the pain of the patch is updating all callers in the kernel</span>
<span class="quote">&gt; for this. While tedious, it is not that hard to review, I hope.</span>
<span class="quote">&gt; The idea is to use a local variable (no concurrency) whenever the</span>
<span class="quote">&gt; mmap_sem is taken and we end up in pf paths that end up retaking</span>
<span class="quote">&gt; the lock. Ie:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;   some_fn(a, b, c, &amp;mmrange);</span>
<span class="quote">&gt;   ....</span>
<span class="quote">&gt;    ....</span>
<span class="quote">&gt;     ...</span>
<span class="quote">&gt;      handle_mm_fault(vma, addr, flags, mmrange);</span>
<span class="quote">&gt;     ...</span>
<span class="quote">&gt;   up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Semantically nothing changes at all, and the &#39;mmrange&#39; ends up</span>
<span class="quote">&gt; being unused for now. Later patches will use the variable when</span>
<span class="quote">&gt; the mmap_sem wrappers replace straightforward down/up.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Compile tested defconfigs on various non-x86 archs without breaking.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Davidlohr Bueso &lt;dbueso@suse.de&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/alpha/mm/fault.c                      |  3 +-</span>
<span class="quote">&gt;  arch/arc/mm/fault.c                        |  3 +-</span>
<span class="quote">&gt;  arch/arm/mm/fault.c                        |  8 ++-</span>
<span class="quote">&gt;  arch/arm/probes/uprobes/core.c             |  5 +-</span>
<span class="quote">&gt;  arch/arm64/mm/fault.c                      |  7 ++-</span>
<span class="quote">&gt;  arch/cris/mm/fault.c                       |  3 +-</span>
<span class="quote">&gt;  arch/frv/mm/fault.c                        |  3 +-</span>
<span class="quote">&gt;  arch/hexagon/mm/vm_fault.c                 |  3 +-</span>
<span class="quote">&gt;  arch/ia64/mm/fault.c                       |  3 +-</span>
<span class="quote">&gt;  arch/m32r/mm/fault.c                       |  3 +-</span>
<span class="quote">&gt;  arch/m68k/mm/fault.c                       |  3 +-</span>
<span class="quote">&gt;  arch/metag/mm/fault.c                      |  3 +-</span>
<span class="quote">&gt;  arch/microblaze/mm/fault.c                 |  3 +-</span>
<span class="quote">&gt;  arch/mips/kernel/vdso.c                    |  3 +-</span>
<span class="quote">&gt;  arch/mips/mm/fault.c                       |  3 +-</span>
<span class="quote">&gt;  arch/mn10300/mm/fault.c                    |  3 +-</span>
<span class="quote">&gt;  arch/nios2/mm/fault.c                      |  3 +-</span>
<span class="quote">&gt;  arch/openrisc/mm/fault.c                   |  3 +-</span>
<span class="quote">&gt;  arch/parisc/mm/fault.c                     |  3 +-</span>
<span class="quote">&gt;  arch/powerpc/include/asm/mmu_context.h     |  3 +-</span>
<span class="quote">&gt;  arch/powerpc/include/asm/powernv.h         |  5 +-</span>
<span class="quote">&gt;  arch/powerpc/mm/copro_fault.c              |  4 +-</span>
<span class="quote">&gt;  arch/powerpc/mm/fault.c                    |  3 +-</span>
<span class="quote">&gt;  arch/powerpc/platforms/powernv/npu-dma.c   |  5 +-</span>
<span class="quote">&gt;  arch/riscv/mm/fault.c                      |  3 +-</span>
<span class="quote">&gt;  arch/s390/include/asm/gmap.h               | 14 +++--</span>
<span class="quote">&gt;  arch/s390/kvm/gaccess.c                    | 31 ++++++----</span>
<span class="quote">&gt;  arch/s390/mm/fault.c                       |  3 +-</span>
<span class="quote">&gt;  arch/s390/mm/gmap.c                        | 80 +++++++++++++++---------</span>
<span class="quote">&gt;  arch/score/mm/fault.c                      |  3 +-</span>
<span class="quote">&gt;  arch/sh/mm/fault.c                         |  3 +-</span>
<span class="quote">&gt;  arch/sparc/mm/fault_32.c                   |  6 +-</span>
<span class="quote">&gt;  arch/sparc/mm/fault_64.c                   |  3 +-</span>
<span class="quote">&gt;  arch/tile/mm/fault.c                       |  3 +-</span>
<span class="quote">&gt;  arch/um/include/asm/mmu_context.h          |  3 +-</span>
<span class="quote">&gt;  arch/um/kernel/trap.c                      |  3 +-</span>
<span class="quote">&gt;  arch/unicore32/mm/fault.c                  |  8 ++-</span>
<span class="quote">&gt;  arch/x86/entry/vdso/vma.c                  |  3 +-</span>
<span class="quote">&gt;  arch/x86/include/asm/mmu_context.h         |  5 +-</span>
<span class="quote">&gt;  arch/x86/include/asm/mpx.h                 |  6 +-</span>
<span class="quote">&gt;  arch/x86/mm/fault.c                        |  3 +-</span>
<span class="quote">&gt;  arch/x86/mm/mpx.c                          | 41 ++++++++-----</span>
<span class="quote">&gt;  arch/xtensa/mm/fault.c                     |  3 +-</span>
<span class="quote">&gt;  drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c    |  3 +-</span>
<span class="quote">&gt;  drivers/gpu/drm/i915/i915_gem_userptr.c    |  4 +-</span>
<span class="quote">&gt;  drivers/gpu/drm/radeon/radeon_ttm.c        |  4 +-</span>
<span class="quote">&gt;  drivers/infiniband/core/umem.c             |  3 +-</span>
<span class="quote">&gt;  drivers/infiniband/core/umem_odp.c         |  3 +-</span>
<span class="quote">&gt;  drivers/infiniband/hw/qib/qib_user_pages.c |  7 ++-</span>
<span class="quote">&gt;  drivers/infiniband/hw/usnic/usnic_uiom.c   |  3 +-</span>
<span class="quote">&gt;  drivers/iommu/amd_iommu_v2.c               |  5 +-</span>
<span class="quote">&gt;  drivers/iommu/intel-svm.c                  |  5 +-</span>
<span class="quote">&gt;  drivers/media/v4l2-core/videobuf-dma-sg.c  | 18 ++++--</span>
<span class="quote">&gt;  drivers/misc/mic/scif/scif_rma.c           |  3 +-</span>
<span class="quote">&gt;  drivers/misc/sgi-gru/grufault.c            | 43 ++++++++-----</span>
<span class="quote">&gt;  drivers/vfio/vfio_iommu_type1.c            |  3 +-</span>
<span class="quote">&gt;  fs/aio.c                                   |  3 +-</span>
<span class="quote">&gt;  fs/binfmt_elf.c                            |  3 +-</span>
<span class="quote">&gt;  fs/exec.c                                  | 20 ++++--</span>
<span class="quote">&gt;  fs/proc/internal.h                         |  3 +</span>
<span class="quote">&gt;  fs/proc/task_mmu.c                         | 29 ++++++---</span>
<span class="quote">&gt;  fs/proc/vmcore.c                           | 14 ++++-</span>
<span class="quote">&gt;  fs/userfaultfd.c                           | 18 +++---</span>
<span class="quote">&gt;  include/asm-generic/mm_hooks.h             |  3 +-</span>
<span class="quote">&gt;  include/linux/hmm.h                        |  4 +-</span>
<span class="quote">&gt;  include/linux/ksm.h                        |  6 +-</span>
<span class="quote">&gt;  include/linux/migrate.h                    |  4 +-</span>
<span class="quote">&gt;  include/linux/mm.h                         | 73 +++++++++++++---------</span>
<span class="quote">&gt;  include/linux/uprobes.h                    | 15 +++--</span>
<span class="quote">&gt;  ipc/shm.c                                  | 14 +++--</span>
<span class="quote">&gt;  kernel/events/uprobes.c                    | 49 +++++++++------</span>
<span class="quote">&gt;  kernel/futex.c                             |  3 +-</span>
<span class="quote">&gt;  mm/frame_vector.c                          |  4 +-</span>
<span class="quote">&gt;  mm/gup.c                                   | 60 ++++++++++--------</span>
<span class="quote">&gt;  mm/hmm.c                                   | 37 ++++++-----</span>
<span class="quote">&gt;  mm/internal.h                              |  3 +-</span>
<span class="quote">&gt;  mm/ksm.c                                   | 24 +++++---</span>
<span class="quote">&gt;  mm/madvise.c                               | 58 ++++++++++-------</span>
<span class="quote">&gt;  mm/memcontrol.c                            | 13 ++--</span>
<span class="quote">&gt;  mm/memory.c                                | 10 +--</span>
<span class="quote">&gt;  mm/mempolicy.c                             | 35 ++++++-----</span>
<span class="quote">&gt;  mm/migrate.c                               | 20 +++---</span>
<span class="quote">&gt;  mm/mincore.c                               | 24 +++++---</span>
<span class="quote">&gt;  mm/mlock.c                                 | 33 ++++++----</span>
<span class="quote">&gt;  mm/mmap.c                                  | 99 +++++++++++++++++-------------</span>
<span class="quote">&gt;  mm/mprotect.c                              | 14 +++--</span>
<span class="quote">&gt;  mm/mremap.c                                | 30 +++++----</span>
<span class="quote">&gt;  mm/nommu.c                                 | 32 ++++++----</span>
<span class="quote">&gt;  mm/pagewalk.c                              | 56 +++++++++--------</span>
<span class="quote">&gt;  mm/process_vm_access.c                     |  4 +-</span>
<span class="quote">&gt;  mm/util.c                                  |  3 +-</span>
<span class="quote">&gt;  security/tomoyo/domain.c                   |  3 +-</span>
<span class="quote">&gt;  virt/kvm/async_pf.c                        |  3 +-</span>
<span class="quote">&gt;  virt/kvm/kvm_main.c                        | 16 +++--</span>
<span class="quote">&gt;  94 files changed, 784 insertions(+), 474 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/alpha/mm/fault.c b/arch/alpha/mm/fault.c</span>
<span class="quote">&gt; index cd3c572ee912..690d86a00a20 100644</span>
<span class="quote">&gt; --- a/arch/alpha/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/alpha/mm/fault.c</span>
<span class="quote">&gt; @@ -90,6 +90,7 @@ do_page_fault(unsigned long address, unsigned long mmcsr,</span>
<span class="quote">&gt;  	int fault, si_code = SEGV_MAPERR;</span>
<span class="quote">&gt;  	siginfo_t info;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* As of EV6, a load into $31/$f31 is a prefetch, and never faults</span>
<span class="quote">&gt;  	   (or is suppressed by the PALcode).  Support that for older CPUs</span>
<span class="quote">&gt; @@ -148,7 +149,7 @@ do_page_fault(unsigned long address, unsigned long mmcsr,</span>
<span class="quote">&gt;  	/* If for any reason at all we couldn&#39;t handle the fault,</span>
<span class="quote">&gt;  	   make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	   the fault.  */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/arch/arc/mm/fault.c b/arch/arc/mm/fault.c</span>
<span class="quote">&gt; index a0b7bd6d030d..e423f764f159 100644</span>
<span class="quote">&gt; --- a/arch/arc/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/arc/mm/fault.c</span>
<span class="quote">&gt; @@ -69,6 +69,7 @@ void do_page_fault(unsigned long address, struct pt_regs *regs)</span>
<span class="quote">&gt;  	int fault, ret;</span>
<span class="quote">&gt;  	int write = regs-&gt;ecr_cause &amp; ECR_C_PROTV_STORE;  /* ST/EX */</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * We fault-in kernel-space virtual memory on-demand. The</span>
<span class="quote">&gt; @@ -137,7 +138,7 @@ void do_page_fault(unsigned long address, struct pt_regs *regs)</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* If Pagefault was interrupted by SIGKILL, exit page fault &quot;early&quot; */</span>
<span class="quote">&gt;  	if (unlikely(fatal_signal_pending(current))) {</span>
<span class="quote">&gt; diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c</span>
<span class="quote">&gt; index b75eada23d0a..99ae40b5851a 100644</span>
<span class="quote">&gt; --- a/arch/arm/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/arm/mm/fault.c</span>
<span class="quote">&gt; @@ -221,7 +221,8 @@ static inline bool access_error(unsigned int fsr, struct vm_area_struct *vma)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int __kprobes</span>
<span class="quote">&gt;  __do_page_fault(struct mm_struct *mm, unsigned long addr, unsigned int fsr,</span>
<span class="quote">&gt; -		unsigned int flags, struct task_struct *tsk)</span>
<span class="quote">&gt; +		unsigned int flags, struct task_struct *tsk,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt; @@ -243,7 +244,7 @@ __do_page_fault(struct mm_struct *mm, unsigned long addr, unsigned int fsr,</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	return handle_mm_fault(vma, addr &amp; PAGE_MASK, flags);</span>
<span class="quote">&gt; +	return handle_mm_fault(vma, addr &amp; PAGE_MASK, flags, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  check_stack:</span>
<span class="quote">&gt;  	/* Don&#39;t allow expansion below FIRST_USER_ADDRESS */</span>
<span class="quote">&gt; @@ -261,6 +262,7 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)</span>
<span class="quote">&gt;  	struct mm_struct *mm;</span>
<span class="quote">&gt;  	int fault, sig, code;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (notify_page_fault(regs, fsr))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; @@ -308,7 +310,7 @@ do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	fault = __do_page_fault(mm, addr, fsr, flags, tsk);</span>
<span class="quote">&gt; +	fault = __do_page_fault(mm, addr, fsr, flags, tsk, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* If we need to retry but a fatal signal is pending, handle the</span>
<span class="quote">&gt;  	 * signal first. We do not need to release the mmap_sem because</span>
<span class="quote">&gt; diff --git a/arch/arm/probes/uprobes/core.c b/arch/arm/probes/uprobes/core.c</span>
<span class="quote">&gt; index d1329f1ba4e4..e8b893eaebcf 100644</span>
<span class="quote">&gt; --- a/arch/arm/probes/uprobes/core.c</span>
<span class="quote">&gt; +++ b/arch/arm/probes/uprobes/core.c</span>
<span class="quote">&gt; @@ -30,10 +30,11 @@ bool is_swbp_insn(uprobe_opcode_t *insn)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  int set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm,</span>
<span class="quote">&gt; -	     unsigned long vaddr)</span>
<span class="quote">&gt; +	     unsigned long vaddr, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return uprobe_write_opcode(mm, vaddr,</span>
<span class="quote">&gt; -		   __opcode_to_mem_arm(auprobe-&gt;bpinsn));</span>
<span class="quote">&gt; +				   __opcode_to_mem_arm(auprobe-&gt;bpinsn),</span>
<span class="quote">&gt; +				   mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  bool arch_uprobe_ignore(struct arch_uprobe *auprobe, struct pt_regs *regs)</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c</span>
<span class="quote">&gt; index ce441d29e7f6..1f3ad9e4f214 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/fault.c</span>
<span class="quote">&gt; @@ -342,7 +342,7 @@ static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int __do_page_fault(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;  			   unsigned int mm_flags, unsigned long vm_flags,</span>
<span class="quote">&gt; -			   struct task_struct *tsk)</span>
<span class="quote">&gt; +			   struct task_struct *tsk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt; @@ -368,7 +368,7 @@ static int __do_page_fault(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	return handle_mm_fault(vma, addr &amp; PAGE_MASK, mm_flags);</span>
<span class="quote">&gt; +	return handle_mm_fault(vma, addr &amp; PAGE_MASK, mm_flags, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  check_stack:</span>
<span class="quote">&gt;  	if (vma-&gt;vm_flags &amp; VM_GROWSDOWN &amp;&amp; !expand_stack(vma, addr))</span>
<span class="quote">&gt; @@ -390,6 +390,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,</span>
<span class="quote">&gt;  	int fault, sig, code, major = 0;</span>
<span class="quote">&gt;  	unsigned long vm_flags = VM_READ | VM_WRITE;</span>
<span class="quote">&gt;  	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (notify_page_fault(regs, esr))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; @@ -450,7 +451,7 @@ static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	fault = __do_page_fault(mm, addr, mm_flags, vm_flags, tsk);</span>
<span class="quote">&gt; +	fault = __do_page_fault(mm, addr, mm_flags, vm_flags, tsk, &amp;mmrange);</span>
<span class="quote">&gt;  	major |= fault &amp; VM_FAULT_MAJOR;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (fault &amp; VM_FAULT_RETRY) {</span>
<span class="quote">&gt; diff --git a/arch/cris/mm/fault.c b/arch/cris/mm/fault.c</span>
<span class="quote">&gt; index 29cc58038b98..16af16d77269 100644</span>
<span class="quote">&gt; --- a/arch/cris/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/cris/mm/fault.c</span>
<span class="quote">&gt; @@ -61,6 +61,7 @@ do_page_fault(unsigned long address, struct pt_regs *regs,</span>
<span class="quote">&gt;  	siginfo_t info;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	D(printk(KERN_DEBUG</span>
<span class="quote">&gt;  		 &quot;Page fault for %lX on %X at %lX, prot %d write %d\n&quot;,</span>
<span class="quote">&gt; @@ -170,7 +171,7 @@ do_page_fault(unsigned long address, struct pt_regs *regs,</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/arch/frv/mm/fault.c b/arch/frv/mm/fault.c</span>
<span class="quote">&gt; index cbe7aec863e3..494d33b628fc 100644</span>
<span class="quote">&gt; --- a/arch/frv/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/frv/mm/fault.c</span>
<span class="quote">&gt; @@ -41,6 +41,7 @@ asmlinkage void do_page_fault(int datammu, unsigned long esr0, unsigned long ear</span>
<span class="quote">&gt;  	pud_t *pue;</span>
<span class="quote">&gt;  	pte_t *pte;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #if 0</span>
<span class="quote">&gt;  	const char *atxc[16] = {</span>
<span class="quote">&gt; @@ -165,7 +166,7 @@ asmlinkage void do_page_fault(int datammu, unsigned long esr0, unsigned long ear</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, ear0, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, ear0, flags, &amp;mmrange);</span>
<span class="quote">&gt;  	if (unlikely(fault &amp; VM_FAULT_ERROR)) {</span>
<span class="quote">&gt;  		if (fault &amp; VM_FAULT_OOM)</span>
<span class="quote">&gt;  			goto out_of_memory;</span>
<span class="quote">&gt; diff --git a/arch/hexagon/mm/vm_fault.c b/arch/hexagon/mm/vm_fault.c</span>
<span class="quote">&gt; index 3eec33c5cfd7..7d6ada2c2230 100644</span>
<span class="quote">&gt; --- a/arch/hexagon/mm/vm_fault.c</span>
<span class="quote">&gt; +++ b/arch/hexagon/mm/vm_fault.c</span>
<span class="quote">&gt; @@ -55,6 +55,7 @@ void do_page_fault(unsigned long address, long cause, struct pt_regs *regs)</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	const struct exception_table_entry *fixup;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * If we&#39;re in an interrupt or have no user context,</span>
<span class="quote">&gt; @@ -102,7 +103,7 @@ void do_page_fault(unsigned long address, long cause, struct pt_regs *regs)</span>
<span class="quote">&gt;  		break;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/arch/ia64/mm/fault.c b/arch/ia64/mm/fault.c</span>
<span class="quote">&gt; index dfdc152d6737..44f0ec5f77c2 100644</span>
<span class="quote">&gt; --- a/arch/ia64/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/ia64/mm/fault.c</span>
<span class="quote">&gt; @@ -89,6 +89,7 @@ ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *re</span>
<span class="quote">&gt;  	unsigned long mask;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	mask = ((((isr &gt;&gt; IA64_ISR_X_BIT) &amp; 1UL) &lt;&lt; VM_EXEC_BIT)</span>
<span class="quote">&gt;  		| (((isr &gt;&gt; IA64_ISR_W_BIT) &amp; 1UL) &lt;&lt; VM_WRITE_BIT));</span>
<span class="quote">&gt; @@ -162,7 +163,7 @@ ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *re</span>
<span class="quote">&gt;  	 * sure we exit gracefully rather than endlessly redo the</span>
<span class="quote">&gt;  	 * fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/arch/m32r/mm/fault.c b/arch/m32r/mm/fault.c</span>
<span class="quote">&gt; index 46d9a5ca0e3a..0129aea46729 100644</span>
<span class="quote">&gt; --- a/arch/m32r/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/m32r/mm/fault.c</span>
<span class="quote">&gt; @@ -82,6 +82,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
<span class="quote">&gt;  	unsigned long flags = 0;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	siginfo_t info;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * If BPSW IE bit enable --&gt; set PSW IE bit</span>
<span class="quote">&gt; @@ -197,7 +198,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	addr = (address &amp; PAGE_MASK);</span>
<span class="quote">&gt;  	set_thread_fault_code(error_code);</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, addr, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, addr, flags, &amp;mmrange);</span>
<span class="quote">&gt;  	if (unlikely(fault &amp; VM_FAULT_ERROR)) {</span>
<span class="quote">&gt;  		if (fault &amp; VM_FAULT_OOM)</span>
<span class="quote">&gt;  			goto out_of_memory;</span>
<span class="quote">&gt; diff --git a/arch/m68k/mm/fault.c b/arch/m68k/mm/fault.c</span>
<span class="quote">&gt; index 03253c4f8e6a..ec32a193726f 100644</span>
<span class="quote">&gt; --- a/arch/m68k/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/m68k/mm/fault.c</span>
<span class="quote">&gt; @@ -75,6 +75,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
<span class="quote">&gt;  	struct vm_area_struct * vma;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	pr_debug(&quot;do page fault:\nregs-&gt;sr=%#x, regs-&gt;pc=%#lx, address=%#lx, %ld, %p\n&quot;,</span>
<span class="quote">&gt;  		regs-&gt;sr, regs-&gt;pc, address, error_code, mm ? mm-&gt;pgd : NULL);</span>
<span class="quote">&gt; @@ -138,7 +139,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt;  	pr_debug(&quot;handle_mm_fault returns %d\n&quot;, fault);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt; diff --git a/arch/metag/mm/fault.c b/arch/metag/mm/fault.c</span>
<span class="quote">&gt; index de54fe686080..e16ba0ea7ea1 100644</span>
<span class="quote">&gt; --- a/arch/metag/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/metag/mm/fault.c</span>
<span class="quote">&gt; @@ -56,6 +56,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
<span class="quote">&gt;  	siginfo_t info;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	tsk = current;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -135,7 +136,7 @@ int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; diff --git a/arch/microblaze/mm/fault.c b/arch/microblaze/mm/fault.c</span>
<span class="quote">&gt; index f91b30f8aaa8..fd49efbdfbf4 100644</span>
<span class="quote">&gt; --- a/arch/microblaze/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/microblaze/mm/fault.c</span>
<span class="quote">&gt; @@ -93,6 +93,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long address,</span>
<span class="quote">&gt;  	int is_write = error_code &amp; ESR_S;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	regs-&gt;ear = address;</span>
<span class="quote">&gt;  	regs-&gt;esr = error_code;</span>
<span class="quote">&gt; @@ -216,7 +217,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long address,</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c</span>
<span class="quote">&gt; index 019035d7225c..56b7c29991db 100644</span>
<span class="quote">&gt; --- a/arch/mips/kernel/vdso.c</span>
<span class="quote">&gt; +++ b/arch/mips/kernel/vdso.c</span>
<span class="quote">&gt; @@ -102,6 +102,7 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)</span>
<span class="quote">&gt;  	unsigned long gic_size, vvar_size, size, base, data_addr, vdso_addr, gic_pfn;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="quote">&gt;  		return -EINTR;</span>
<span class="quote">&gt; @@ -110,7 +111,7 @@ int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)</span>
<span class="quote">&gt;  	base = mmap_region(NULL, STACK_TOP, PAGE_SIZE,</span>
<span class="quote">&gt;  			   VM_READ|VM_WRITE|VM_EXEC|</span>
<span class="quote">&gt;  			   VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,</span>
<span class="quote">&gt; -			   0, NULL);</span>
<span class="quote">&gt; +			   0, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  	if (IS_ERR_VALUE(base)) {</span>
<span class="quote">&gt;  		ret = base;</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c</span>
<span class="quote">&gt; index 4f8f5bf46977..1433edd01d09 100644</span>
<span class="quote">&gt; --- a/arch/mips/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/mips/mm/fault.c</span>
<span class="quote">&gt; @@ -47,6 +47,7 @@ static void __kprobes __do_page_fault(struct pt_regs *regs, unsigned long write,</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	static DEFINE_RATELIMIT_STATE(ratelimit_state, 5 * HZ, 10);</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #if 0</span>
<span class="quote">&gt;  	printk(&quot;Cpu%d[%s:%d:%0*lx:%ld:%0*lx]\n&quot;, raw_smp_processor_id(),</span>
<span class="quote">&gt; @@ -152,7 +153,7 @@ static void __kprobes __do_page_fault(struct pt_regs *regs, unsigned long write,</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/arch/mn10300/mm/fault.c b/arch/mn10300/mm/fault.c</span>
<span class="quote">&gt; index f0bfa1448744..71c38f0c8702 100644</span>
<span class="quote">&gt; --- a/arch/mn10300/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/mn10300/mm/fault.c</span>
<span class="quote">&gt; @@ -125,6 +125,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long fault_code,</span>
<span class="quote">&gt;  	siginfo_t info;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_GDBSTUB</span>
<span class="quote">&gt;  	/* handle GDB stub causing a fault */</span>
<span class="quote">&gt; @@ -254,7 +255,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long fault_code,</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/arch/nios2/mm/fault.c b/arch/nios2/mm/fault.c</span>
<span class="quote">&gt; index b804dd06ea1c..768678b685af 100644</span>
<span class="quote">&gt; --- a/arch/nios2/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/nios2/mm/fault.c</span>
<span class="quote">&gt; @@ -49,6 +49,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long cause,</span>
<span class="quote">&gt;  	int code = SEGV_MAPERR;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	cause &gt;&gt;= 2;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -132,7 +133,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long cause,</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/arch/openrisc/mm/fault.c b/arch/openrisc/mm/fault.c</span>
<span class="quote">&gt; index d0021dfae20a..75ddb1e8e7e7 100644</span>
<span class="quote">&gt; --- a/arch/openrisc/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/openrisc/mm/fault.c</span>
<span class="quote">&gt; @@ -55,6 +55,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long address,</span>
<span class="quote">&gt;  	siginfo_t info;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	tsk = current;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -163,7 +164,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long address,</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/arch/parisc/mm/fault.c b/arch/parisc/mm/fault.c</span>
<span class="quote">&gt; index e247edbca68e..79db33a0cb0c 100644</span>
<span class="quote">&gt; --- a/arch/parisc/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/parisc/mm/fault.c</span>
<span class="quote">&gt; @@ -264,6 +264,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long code,</span>
<span class="quote">&gt;  	unsigned long acc_type;</span>
<span class="quote">&gt;  	int fault = 0;</span>
<span class="quote">&gt;  	unsigned int flags;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (faulthandler_disabled())</span>
<span class="quote">&gt;  		goto no_context;</span>
<span class="quote">&gt; @@ -301,7 +302,7 @@ void do_page_fault(struct pt_regs *regs, unsigned long code,</span>
<span class="quote">&gt;  	 * fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/mmu_context.h b/arch/powerpc/include/asm/mmu_context.h</span>
<span class="quote">&gt; index 051b3d63afe3..089b3cf948eb 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/mmu_context.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/mmu_context.h</span>
<span class="quote">&gt; @@ -176,7 +176,8 @@ extern void arch_exit_mmap(struct mm_struct *mm);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static inline void arch_unmap(struct mm_struct *mm,</span>
<span class="quote">&gt;  			      struct vm_area_struct *vma,</span>
<span class="quote">&gt; -			      unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +			      unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +			      struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (start &lt;= mm-&gt;context.vdso_base &amp;&amp; mm-&gt;context.vdso_base &lt; end)</span>
<span class="quote">&gt;  		mm-&gt;context.vdso_base = 0;</span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/powernv.h b/arch/powerpc/include/asm/powernv.h</span>
<span class="quote">&gt; index dc5f6a5d4575..805ff3ba94e1 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/powernv.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/powernv.h</span>
<span class="quote">&gt; @@ -21,7 +21,7 @@ extern void pnv_npu2_destroy_context(struct npu_context *context,</span>
<span class="quote">&gt;  				struct pci_dev *gpdev);</span>
<span class="quote">&gt;  extern int pnv_npu2_handle_fault(struct npu_context *context, uintptr_t *ea,</span>
<span class="quote">&gt;  				unsigned long *flags, unsigned long *status,</span>
<span class="quote">&gt; -				int count);</span>
<span class="quote">&gt; +				int count, struct range_lock *mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  void pnv_tm_init(void);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; @@ -35,7 +35,8 @@ static inline void pnv_npu2_destroy_context(struct npu_context *context,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static inline int pnv_npu2_handle_fault(struct npu_context *context,</span>
<span class="quote">&gt;  					uintptr_t *ea, unsigned long *flags,</span>
<span class="quote">&gt; -					unsigned long *status, int count) {</span>
<span class="quote">&gt; +					unsigned long *status, int count,</span>
<span class="quote">&gt; +					struct range_lock *mmrange) {</span>
<span class="quote">&gt;  	return -ENODEV;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/powerpc/mm/copro_fault.c b/arch/powerpc/mm/copro_fault.c</span>
<span class="quote">&gt; index 697b70ad1195..8f5e604828a1 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/mm/copro_fault.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/mm/copro_fault.c</span>
<span class="quote">&gt; @@ -39,6 +39,7 @@ int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	unsigned long is_write;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (mm == NULL)</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt; @@ -77,7 +78,8 @@ int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	ret = 0;</span>
<span class="quote">&gt; -	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0);</span>
<span class="quote">&gt; +	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0,</span>
<span class="quote">&gt; +			       &amp;mmrange);</span>
<span class="quote">&gt;  	if (unlikely(*flt &amp; VM_FAULT_ERROR)) {</span>
<span class="quote">&gt;  		if (*flt &amp; VM_FAULT_OOM) {</span>
<span class="quote">&gt;  			ret = -ENOMEM;</span>
<span class="quote">&gt; diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c</span>
<span class="quote">&gt; index 866446cf2d9a..d562dc88687d 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/mm/fault.c</span>
<span class="quote">&gt; @@ -399,6 +399,7 @@ static int __do_page_fault(struct pt_regs *regs, unsigned long address,</span>
<span class="quote">&gt;  	int is_write = page_fault_is_write(error_code);</span>
<span class="quote">&gt;  	int fault, major = 0;</span>
<span class="quote">&gt;  	bool store_update_sp = false;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (notify_page_fault(regs))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; @@ -514,7 +515,7 @@ static int __do_page_fault(struct pt_regs *regs, unsigned long address,</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_PPC_MEM_KEYS</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; diff --git a/arch/powerpc/platforms/powernv/npu-dma.c b/arch/powerpc/platforms/powernv/npu-dma.c</span>
<span class="quote">&gt; index 0a253b64ac5f..759e9a4c7479 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/platforms/powernv/npu-dma.c</span>
<span class="quote">&gt; +++ b/arch/powerpc/platforms/powernv/npu-dma.c</span>
<span class="quote">&gt; @@ -789,7 +789,8 @@ EXPORT_SYMBOL(pnv_npu2_destroy_context);</span>
<span class="quote">&gt;   * Assumes mmap_sem is held for the contexts associated mm.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int pnv_npu2_handle_fault(struct npu_context *context, uintptr_t *ea,</span>
<span class="quote">&gt; -			unsigned long *flags, unsigned long *status, int count)</span>
<span class="quote">&gt; +			  unsigned long *flags, unsigned long *status,</span>
<span class="quote">&gt; +			  int count, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	u64 rc = 0, result = 0;</span>
<span class="quote">&gt;  	int i, is_write;</span>
<span class="quote">&gt; @@ -807,7 +808,7 @@ int pnv_npu2_handle_fault(struct npu_context *context, uintptr_t *ea,</span>
<span class="quote">&gt;  		is_write = flags[i] &amp; NPU2_WRITE;</span>
<span class="quote">&gt;  		rc = get_user_pages_remote(NULL, mm, ea[i], 1,</span>
<span class="quote">&gt;  					is_write ? FOLL_WRITE : 0,</span>
<span class="quote">&gt; -					page, NULL, NULL);</span>
<span class="quote">&gt; +					page, NULL, NULL, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * To support virtualised environments we will have to do an</span>
<span class="quote">&gt; diff --git a/arch/riscv/mm/fault.c b/arch/riscv/mm/fault.c</span>
<span class="quote">&gt; index 148c98ca9b45..75d15e73ba39 100644</span>
<span class="quote">&gt; --- a/arch/riscv/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/riscv/mm/fault.c</span>
<span class="quote">&gt; @@ -42,6 +42,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs)</span>
<span class="quote">&gt;  	unsigned long addr, cause;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt;  	int fault, code = SEGV_MAPERR;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	cause = regs-&gt;scause;</span>
<span class="quote">&gt;  	addr = regs-&gt;sbadaddr;</span>
<span class="quote">&gt; @@ -119,7 +120,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs)</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, addr, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, addr, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * If we need to retry but a fatal signal is pending, handle the</span>
<span class="quote">&gt; diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h</span>
<span class="quote">&gt; index e07cce88dfb0..117c19a947c9 100644</span>
<span class="quote">&gt; --- a/arch/s390/include/asm/gmap.h</span>
<span class="quote">&gt; +++ b/arch/s390/include/asm/gmap.h</span>
<span class="quote">&gt; @@ -107,22 +107,24 @@ void gmap_discard(struct gmap *, unsigned long from, unsigned long to);</span>
<span class="quote">&gt;  void __gmap_zap(struct gmap *, unsigned long gaddr);</span>
<span class="quote">&gt;  void gmap_unlink(struct mm_struct *, unsigned long *table, unsigned long vmaddr);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val);</span>
<span class="quote">&gt; +int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val,</span>
<span class="quote">&gt; +		    struct range_lock *mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,</span>
<span class="quote">&gt;  			 int edat_level);</span>
<span class="quote">&gt;  int gmap_shadow_valid(struct gmap *sg, unsigned long asce, int edat_level);</span>
<span class="quote">&gt;  int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t,</span>
<span class="quote">&gt; -		    int fake);</span>
<span class="quote">&gt; +		    int fake, struct range_lock *mmrange);</span>
<span class="quote">&gt;  int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t,</span>
<span class="quote">&gt; -		    int fake);</span>
<span class="quote">&gt; +		    int fake, struct range_lock *mmrange);</span>
<span class="quote">&gt;  int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt,</span>
<span class="quote">&gt; -		    int fake);</span>
<span class="quote">&gt; +		    int fake, struct range_lock *mmrange);</span>
<span class="quote">&gt;  int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt,</span>
<span class="quote">&gt; -		    int fake);</span>
<span class="quote">&gt; +		    int fake, struct range_lock *mmrange);</span>
<span class="quote">&gt;  int gmap_shadow_pgt_lookup(struct gmap *sg, unsigned long saddr,</span>
<span class="quote">&gt;  			   unsigned long *pgt, int *dat_protection, int *fake);</span>
<span class="quote">&gt; -int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte);</span>
<span class="quote">&gt; +int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte,</span>
<span class="quote">&gt; +		     struct range_lock *mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  void gmap_register_pte_notifier(struct gmap_notifier *);</span>
<span class="quote">&gt;  void gmap_unregister_pte_notifier(struct gmap_notifier *);</span>
<span class="quote">&gt; diff --git a/arch/s390/kvm/gaccess.c b/arch/s390/kvm/gaccess.c</span>
<span class="quote">&gt; index c24bfa72baf7..ff739b86df36 100644</span>
<span class="quote">&gt; --- a/arch/s390/kvm/gaccess.c</span>
<span class="quote">&gt; +++ b/arch/s390/kvm/gaccess.c</span>
<span class="quote">&gt; @@ -978,10 +978,11 @@ int kvm_s390_check_low_addr_prot_real(struct kvm_vcpu *vcpu, unsigned long gra)</span>
<span class="quote">&gt;   * @saddr: faulting address in the shadow gmap</span>
<span class="quote">&gt;   * @pgt: pointer to the page table address result</span>
<span class="quote">&gt;   * @fake: pgt references contiguous guest memory block, not a pgtable</span>
<span class="quote">&gt; + * @mmrange: address space range locking</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
<span class="quote">&gt;  				  unsigned long *pgt, int *dat_protection,</span>
<span class="quote">&gt; -				  int *fake)</span>
<span class="quote">&gt; +				  int *fake, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct gmap *parent;</span>
<span class="quote">&gt;  	union asce asce;</span>
<span class="quote">&gt; @@ -1034,7 +1035,8 @@ static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
<span class="quote">&gt;  			rfte.val = ptr;</span>
<span class="quote">&gt;  			goto shadow_r2t;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		rc = gmap_read_table(parent, ptr + vaddr.rfx * 8, &amp;rfte.val);</span>
<span class="quote">&gt; +		rc = gmap_read_table(parent, ptr + vaddr.rfx * 8, &amp;rfte.val,</span>
<span class="quote">&gt; +				     mmrange);</span>
<span class="quote">&gt;  		if (rc)</span>
<span class="quote">&gt;  			return rc;</span>
<span class="quote">&gt;  		if (rfte.i)</span>
<span class="quote">&gt; @@ -1047,7 +1049,7 @@ static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
<span class="quote">&gt;  			*dat_protection |= rfte.p;</span>
<span class="quote">&gt;  		ptr = rfte.rto * PAGE_SIZE;</span>
<span class="quote">&gt;  shadow_r2t:</span>
<span class="quote">&gt; -		rc = gmap_shadow_r2t(sg, saddr, rfte.val, *fake);</span>
<span class="quote">&gt; +		rc = gmap_shadow_r2t(sg, saddr, rfte.val, *fake, mmrange);</span>
<span class="quote">&gt;  		if (rc)</span>
<span class="quote">&gt;  			return rc;</span>
<span class="quote">&gt;  		/* fallthrough */</span>
<span class="quote">&gt; @@ -1060,7 +1062,8 @@ static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
<span class="quote">&gt;  			rste.val = ptr;</span>
<span class="quote">&gt;  			goto shadow_r3t;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		rc = gmap_read_table(parent, ptr + vaddr.rsx * 8, &amp;rste.val);</span>
<span class="quote">&gt; +		rc = gmap_read_table(parent, ptr + vaddr.rsx * 8, &amp;rste.val,</span>
<span class="quote">&gt; +				     mmrange);</span>
<span class="quote">&gt;  		if (rc)</span>
<span class="quote">&gt;  			return rc;</span>
<span class="quote">&gt;  		if (rste.i)</span>
<span class="quote">&gt; @@ -1074,7 +1077,7 @@ static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
<span class="quote">&gt;  		ptr = rste.rto * PAGE_SIZE;</span>
<span class="quote">&gt;  shadow_r3t:</span>
<span class="quote">&gt;  		rste.p |= *dat_protection;</span>
<span class="quote">&gt; -		rc = gmap_shadow_r3t(sg, saddr, rste.val, *fake);</span>
<span class="quote">&gt; +		rc = gmap_shadow_r3t(sg, saddr, rste.val, *fake, mmrange);</span>
<span class="quote">&gt;  		if (rc)</span>
<span class="quote">&gt;  			return rc;</span>
<span class="quote">&gt;  		/* fallthrough */</span>
<span class="quote">&gt; @@ -1087,7 +1090,8 @@ static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
<span class="quote">&gt;  			rtte.val = ptr;</span>
<span class="quote">&gt;  			goto shadow_sgt;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		rc = gmap_read_table(parent, ptr + vaddr.rtx * 8, &amp;rtte.val);</span>
<span class="quote">&gt; +		rc = gmap_read_table(parent, ptr + vaddr.rtx * 8, &amp;rtte.val,</span>
<span class="quote">&gt; +				     mmrange);</span>
<span class="quote">&gt;  		if (rc)</span>
<span class="quote">&gt;  			return rc;</span>
<span class="quote">&gt;  		if (rtte.i)</span>
<span class="quote">&gt; @@ -1110,7 +1114,7 @@ static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
<span class="quote">&gt;  		ptr = rtte.fc0.sto * PAGE_SIZE;</span>
<span class="quote">&gt;  shadow_sgt:</span>
<span class="quote">&gt;  		rtte.fc0.p |= *dat_protection;</span>
<span class="quote">&gt; -		rc = gmap_shadow_sgt(sg, saddr, rtte.val, *fake);</span>
<span class="quote">&gt; +		rc = gmap_shadow_sgt(sg, saddr, rtte.val, *fake, mmrange);</span>
<span class="quote">&gt;  		if (rc)</span>
<span class="quote">&gt;  			return rc;</span>
<span class="quote">&gt;  		/* fallthrough */</span>
<span class="quote">&gt; @@ -1123,7 +1127,8 @@ static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
<span class="quote">&gt;  			ste.val = ptr;</span>
<span class="quote">&gt;  			goto shadow_pgt;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		rc = gmap_read_table(parent, ptr + vaddr.sx * 8, &amp;ste.val);</span>
<span class="quote">&gt; +		rc = gmap_read_table(parent, ptr + vaddr.sx * 8, &amp;ste.val,</span>
<span class="quote">&gt; +				     mmrange);</span>
<span class="quote">&gt;  		if (rc)</span>
<span class="quote">&gt;  			return rc;</span>
<span class="quote">&gt;  		if (ste.i)</span>
<span class="quote">&gt; @@ -1142,7 +1147,7 @@ static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
<span class="quote">&gt;  		ptr = ste.fc0.pto * (PAGE_SIZE / 2);</span>
<span class="quote">&gt;  shadow_pgt:</span>
<span class="quote">&gt;  		ste.fc0.p |= *dat_protection;</span>
<span class="quote">&gt; -		rc = gmap_shadow_pgt(sg, saddr, ste.val, *fake);</span>
<span class="quote">&gt; +		rc = gmap_shadow_pgt(sg, saddr, ste.val, *fake, mmrange);</span>
<span class="quote">&gt;  		if (rc)</span>
<span class="quote">&gt;  			return rc;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1172,6 +1177,7 @@ int kvm_s390_shadow_fault(struct kvm_vcpu *vcpu, struct gmap *sg,</span>
<span class="quote">&gt;  	unsigned long pgt;</span>
<span class="quote">&gt;  	int dat_protection, fake;</span>
<span class="quote">&gt;  	int rc;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_read(&amp;sg-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -1184,7 +1190,7 @@ int kvm_s390_shadow_fault(struct kvm_vcpu *vcpu, struct gmap *sg,</span>
<span class="quote">&gt;  	rc = gmap_shadow_pgt_lookup(sg, saddr, &amp;pgt, &amp;dat_protection, &amp;fake);</span>
<span class="quote">&gt;  	if (rc)</span>
<span class="quote">&gt;  		rc = kvm_s390_shadow_tables(sg, saddr, &amp;pgt, &amp;dat_protection,</span>
<span class="quote">&gt; -					    &amp;fake);</span>
<span class="quote">&gt; +					    &amp;fake, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	vaddr.addr = saddr;</span>
<span class="quote">&gt;  	if (fake) {</span>
<span class="quote">&gt; @@ -1192,7 +1198,8 @@ int kvm_s390_shadow_fault(struct kvm_vcpu *vcpu, struct gmap *sg,</span>
<span class="quote">&gt;  		goto shadow_page;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if (!rc)</span>
<span class="quote">&gt; -		rc = gmap_read_table(sg-&gt;parent, pgt + vaddr.px * 8, &amp;pte.val);</span>
<span class="quote">&gt; +		rc = gmap_read_table(sg-&gt;parent, pgt + vaddr.px * 8,</span>
<span class="quote">&gt; +				     &amp;pte.val, &amp;mmrange);</span>
<span class="quote">&gt;  	if (!rc &amp;&amp; pte.i)</span>
<span class="quote">&gt;  		rc = PGM_PAGE_TRANSLATION;</span>
<span class="quote">&gt;  	if (!rc &amp;&amp; pte.z)</span>
<span class="quote">&gt; @@ -1200,7 +1207,7 @@ int kvm_s390_shadow_fault(struct kvm_vcpu *vcpu, struct gmap *sg,</span>
<span class="quote">&gt;  shadow_page:</span>
<span class="quote">&gt;  	pte.p |= dat_protection;</span>
<span class="quote">&gt;  	if (!rc)</span>
<span class="quote">&gt; -		rc = gmap_shadow_page(sg, saddr, __pte(pte.val));</span>
<span class="quote">&gt; +		rc = gmap_shadow_page(sg, saddr, __pte(pte.val), &amp;mmrange);</span>
<span class="quote">&gt;  	ipte_unlock(vcpu);</span>
<span class="quote">&gt;  	up_read(&amp;sg-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	return rc;</span>
<span class="quote">&gt; diff --git a/arch/s390/mm/fault.c b/arch/s390/mm/fault.c</span>
<span class="quote">&gt; index 93faeca52284..17ba3c402f9d 100644</span>
<span class="quote">&gt; --- a/arch/s390/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/s390/mm/fault.c</span>
<span class="quote">&gt; @@ -421,6 +421,7 @@ static inline int do_exception(struct pt_regs *regs, int access)</span>
<span class="quote">&gt;  	unsigned long address;</span>
<span class="quote">&gt;  	unsigned int flags;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	tsk = current;</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -507,7 +508,7 @@ static inline int do_exception(struct pt_regs *regs, int access)</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt;  	/* No reason to continue if interrupted by SIGKILL. */</span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current)) {</span>
<span class="quote">&gt;  		fault = VM_FAULT_SIGNAL;</span>
<span class="quote">&gt; diff --git a/arch/s390/mm/gmap.c b/arch/s390/mm/gmap.c</span>
<span class="quote">&gt; index 2c55a2b9d6c6..b12a44813022 100644</span>
<span class="quote">&gt; --- a/arch/s390/mm/gmap.c</span>
<span class="quote">&gt; +++ b/arch/s390/mm/gmap.c</span>
<span class="quote">&gt; @@ -621,6 +621,7 @@ int gmap_fault(struct gmap *gmap, unsigned long gaddr,</span>
<span class="quote">&gt;  	unsigned long vmaddr;</span>
<span class="quote">&gt;  	int rc;</span>
<span class="quote">&gt;  	bool unlocked;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_read(&amp;gmap-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -632,7 +633,7 @@ int gmap_fault(struct gmap *gmap, unsigned long gaddr,</span>
<span class="quote">&gt;  		goto out_up;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if (fixup_user_fault(current, gmap-&gt;mm, vmaddr, fault_flags,</span>
<span class="quote">&gt; -			     &amp;unlocked)) {</span>
<span class="quote">&gt; +			     &amp;unlocked, &amp;mmrange)) {</span>
<span class="quote">&gt;  		rc = -EFAULT;</span>
<span class="quote">&gt;  		goto out_up;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -835,13 +836,15 @@ static pte_t *gmap_pte_op_walk(struct gmap *gmap, unsigned long gaddr,</span>
<span class="quote">&gt;   * @gaddr: virtual address in the guest address space</span>
<span class="quote">&gt;   * @vmaddr: address in the host process address space</span>
<span class="quote">&gt;   * @prot: indicates access rights: PROT_NONE, PROT_READ or PROT_WRITE</span>
<span class="quote">&gt; + * @mmrange: address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Returns 0 if the caller can retry __gmap_translate (might fail again),</span>
<span class="quote">&gt;   * -ENOMEM if out of memory and -EFAULT if anything goes wrong while fixing</span>
<span class="quote">&gt;   * up or connecting the gmap page table.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int gmap_pte_op_fixup(struct gmap *gmap, unsigned long gaddr,</span>
<span class="quote">&gt; -			     unsigned long vmaddr, int prot)</span>
<span class="quote">&gt; +			     unsigned long vmaddr, int prot,</span>
<span class="quote">&gt; +			     struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = gmap-&gt;mm;</span>
<span class="quote">&gt;  	unsigned int fault_flags;</span>
<span class="quote">&gt; @@ -849,7 +852,8 @@ static int gmap_pte_op_fixup(struct gmap *gmap, unsigned long gaddr,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	BUG_ON(gmap_is_shadow(gmap));</span>
<span class="quote">&gt;  	fault_flags = (prot == PROT_WRITE) ? FAULT_FLAG_WRITE : 0;</span>
<span class="quote">&gt; -	if (fixup_user_fault(current, mm, vmaddr, fault_flags, &amp;unlocked))</span>
<span class="quote">&gt; +	if (fixup_user_fault(current, mm, vmaddr, fault_flags, &amp;unlocked,</span>
<span class="quote">&gt; +			     mmrange))</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt;  	if (unlocked)</span>
<span class="quote">&gt;  		/* lost mmap_sem, caller has to retry __gmap_translate */</span>
<span class="quote">&gt; @@ -874,6 +878,7 @@ static void gmap_pte_op_end(spinlock_t *ptl)</span>
<span class="quote">&gt;   * @len: size of area</span>
<span class="quote">&gt;   * @prot: indicates access rights: PROT_NONE, PROT_READ or PROT_WRITE</span>
<span class="quote">&gt;   * @bits: pgste notification bits to set</span>
<span class="quote">&gt; + * @mmrange: address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Returns 0 if successfully protected, -ENOMEM if out of memory and</span>
<span class="quote">&gt;   * -EFAULT if gaddr is invalid (or mapping for shadows is missing).</span>
<span class="quote">&gt; @@ -881,7 +886,8 @@ static void gmap_pte_op_end(spinlock_t *ptl)</span>
<span class="quote">&gt;   * Called with sg-&gt;mm-&gt;mmap_sem in read.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int gmap_protect_range(struct gmap *gmap, unsigned long gaddr,</span>
<span class="quote">&gt; -			      unsigned long len, int prot, unsigned long bits)</span>
<span class="quote">&gt; +			      unsigned long len, int prot, unsigned long bits,</span>
<span class="quote">&gt; +			      struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long vmaddr;</span>
<span class="quote">&gt;  	spinlock_t *ptl;</span>
<span class="quote">&gt; @@ -900,7 +906,8 @@ static int gmap_protect_range(struct gmap *gmap, unsigned long gaddr,</span>
<span class="quote">&gt;  			vmaddr = __gmap_translate(gmap, gaddr);</span>
<span class="quote">&gt;  			if (IS_ERR_VALUE(vmaddr))</span>
<span class="quote">&gt;  				return vmaddr;</span>
<span class="quote">&gt; -			rc = gmap_pte_op_fixup(gmap, gaddr, vmaddr, prot);</span>
<span class="quote">&gt; +			rc = gmap_pte_op_fixup(gmap, gaddr, vmaddr, prot,</span>
<span class="quote">&gt; +					       mmrange);</span>
<span class="quote">&gt;  			if (rc)</span>
<span class="quote">&gt;  				return rc;</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; @@ -929,13 +936,14 @@ int gmap_mprotect_notify(struct gmap *gmap, unsigned long gaddr,</span>
<span class="quote">&gt;  			 unsigned long len, int prot)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int rc;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((gaddr &amp; ~PAGE_MASK) || (len &amp; ~PAGE_MASK) || gmap_is_shadow(gmap))</span>
<span class="quote">&gt;  		return -EINVAL;</span>
<span class="quote">&gt;  	if (!MACHINE_HAS_ESOP &amp;&amp; prot == PROT_READ)</span>
<span class="quote">&gt;  		return -EINVAL;</span>
<span class="quote">&gt;  	down_read(&amp;gmap-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; -	rc = gmap_protect_range(gmap, gaddr, len, prot, PGSTE_IN_BIT);</span>
<span class="quote">&gt; +	rc = gmap_protect_range(gmap, gaddr, len, prot, PGSTE_IN_BIT, &amp;mmrange);</span>
<span class="quote">&gt;  	up_read(&amp;gmap-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	return rc;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -947,6 +955,7 @@ EXPORT_SYMBOL_GPL(gmap_mprotect_notify);</span>
<span class="quote">&gt;   * @gmap: pointer to guest mapping meta data structure</span>
<span class="quote">&gt;   * @gaddr: virtual address in the guest address space</span>
<span class="quote">&gt;   * @val: pointer to the unsigned long value to return</span>
<span class="quote">&gt; + * @mmrange: address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Returns 0 if the value was read, -ENOMEM if out of memory and -EFAULT</span>
<span class="quote">&gt;   * if reading using the virtual address failed. -EINVAL if called on a gmap</span>
<span class="quote">&gt; @@ -954,7 +963,8 @@ EXPORT_SYMBOL_GPL(gmap_mprotect_notify);</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Called with gmap-&gt;mm-&gt;mmap_sem in read.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val)</span>
<span class="quote">&gt; +int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val,</span>
<span class="quote">&gt; +		    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long address, vmaddr;</span>
<span class="quote">&gt;  	spinlock_t *ptl;</span>
<span class="quote">&gt; @@ -986,7 +996,7 @@ int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val)</span>
<span class="quote">&gt;  			rc = vmaddr;</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		rc = gmap_pte_op_fixup(gmap, gaddr, vmaddr, PROT_READ);</span>
<span class="quote">&gt; +		rc = gmap_pte_op_fixup(gmap, gaddr, vmaddr, PROT_READ, mmrange);</span>
<span class="quote">&gt;  		if (rc)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1026,12 +1036,14 @@ static inline void gmap_insert_rmap(struct gmap *sg, unsigned long vmaddr,</span>
<span class="quote">&gt;   * @raddr: rmap address in the shadow gmap</span>
<span class="quote">&gt;   * @paddr: address in the parent guest address space</span>
<span class="quote">&gt;   * @len: length of the memory area to protect</span>
<span class="quote">&gt; + * @mmrange: address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Returns 0 if successfully protected and the rmap was created, -ENOMEM</span>
<span class="quote">&gt;   * if out of memory and -EFAULT if paddr is invalid.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int gmap_protect_rmap(struct gmap *sg, unsigned long raddr,</span>
<span class="quote">&gt; -			     unsigned long paddr, unsigned long len)</span>
<span class="quote">&gt; +			     unsigned long paddr, unsigned long len,</span>
<span class="quote">&gt; +			     struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct gmap *parent;</span>
<span class="quote">&gt;  	struct gmap_rmap *rmap;</span>
<span class="quote">&gt; @@ -1069,7 +1081,7 @@ static int gmap_protect_rmap(struct gmap *sg, unsigned long raddr,</span>
<span class="quote">&gt;  		radix_tree_preload_end();</span>
<span class="quote">&gt;  		if (rc) {</span>
<span class="quote">&gt;  			kfree(rmap);</span>
<span class="quote">&gt; -			rc = gmap_pte_op_fixup(parent, paddr, vmaddr, PROT_READ);</span>
<span class="quote">&gt; +			rc = gmap_pte_op_fixup(parent, paddr, vmaddr, PROT_READ, mmrange);</span>
<span class="quote">&gt;  			if (rc)</span>
<span class="quote">&gt;  				return rc;</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; @@ -1473,6 +1485,7 @@ struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,</span>
<span class="quote">&gt;  	struct gmap *sg, *new;</span>
<span class="quote">&gt;  	unsigned long limit;</span>
<span class="quote">&gt;  	int rc;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	BUG_ON(gmap_is_shadow(parent));</span>
<span class="quote">&gt;  	spin_lock(&amp;parent-&gt;shadow_lock);</span>
<span class="quote">&gt; @@ -1526,7 +1539,7 @@ struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,</span>
<span class="quote">&gt;  	down_read(&amp;parent-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	rc = gmap_protect_range(parent, asce &amp; _ASCE_ORIGIN,</span>
<span class="quote">&gt;  				((asce &amp; _ASCE_TABLE_LENGTH) + 1) * PAGE_SIZE,</span>
<span class="quote">&gt; -				PROT_READ, PGSTE_VSIE_BIT);</span>
<span class="quote">&gt; +				PROT_READ, PGSTE_VSIE_BIT, &amp;mmrange);</span>
<span class="quote">&gt;  	up_read(&amp;parent-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	spin_lock(&amp;parent-&gt;shadow_lock);</span>
<span class="quote">&gt;  	new-&gt;initialized = true;</span>
<span class="quote">&gt; @@ -1546,6 +1559,7 @@ EXPORT_SYMBOL_GPL(gmap_shadow);</span>
<span class="quote">&gt;   * @saddr: faulting address in the shadow gmap</span>
<span class="quote">&gt;   * @r2t: parent gmap address of the region 2 table to get shadowed</span>
<span class="quote">&gt;   * @fake: r2t references contiguous guest memory block, not a r2t</span>
<span class="quote">&gt; + * @mmrange: address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * The r2t parameter specifies the address of the source table. The</span>
<span class="quote">&gt;   * four pages of the source table are made read-only in the parent gmap</span>
<span class="quote">&gt; @@ -1559,7 +1573,7 @@ EXPORT_SYMBOL_GPL(gmap_shadow);</span>
<span class="quote">&gt;   * Called with sg-&gt;mm-&gt;mmap_sem in read.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t,</span>
<span class="quote">&gt; -		    int fake)</span>
<span class="quote">&gt; +		    int fake, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long raddr, origin, offset, len;</span>
<span class="quote">&gt;  	unsigned long *s_r2t, *table;</span>
<span class="quote">&gt; @@ -1608,7 +1622,7 @@ int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t,</span>
<span class="quote">&gt;  	origin = r2t &amp; _REGION_ENTRY_ORIGIN;</span>
<span class="quote">&gt;  	offset = ((r2t &amp; _REGION_ENTRY_OFFSET) &gt;&gt; 6) * PAGE_SIZE;</span>
<span class="quote">&gt;  	len = ((r2t &amp; _REGION_ENTRY_LENGTH) + 1) * PAGE_SIZE - offset;</span>
<span class="quote">&gt; -	rc = gmap_protect_rmap(sg, raddr, origin + offset, len);</span>
<span class="quote">&gt; +	rc = gmap_protect_rmap(sg, raddr, origin + offset, len, mmrange);</span>
<span class="quote">&gt;  	spin_lock(&amp;sg-&gt;guest_table_lock);</span>
<span class="quote">&gt;  	if (!rc) {</span>
<span class="quote">&gt;  		table = gmap_table_walk(sg, saddr, 4);</span>
<span class="quote">&gt; @@ -1635,6 +1649,7 @@ EXPORT_SYMBOL_GPL(gmap_shadow_r2t);</span>
<span class="quote">&gt;   * @saddr: faulting address in the shadow gmap</span>
<span class="quote">&gt;   * @r3t: parent gmap address of the region 3 table to get shadowed</span>
<span class="quote">&gt;   * @fake: r3t references contiguous guest memory block, not a r3t</span>
<span class="quote">&gt; + * @mmrange: address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Returns 0 if successfully shadowed or already shadowed, -EAGAIN if the</span>
<span class="quote">&gt;   * shadow table structure is incomplete, -ENOMEM if out of memory and</span>
<span class="quote">&gt; @@ -1643,7 +1658,7 @@ EXPORT_SYMBOL_GPL(gmap_shadow_r2t);</span>
<span class="quote">&gt;   * Called with sg-&gt;mm-&gt;mmap_sem in read.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t,</span>
<span class="quote">&gt; -		    int fake)</span>
<span class="quote">&gt; +		    int fake, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long raddr, origin, offset, len;</span>
<span class="quote">&gt;  	unsigned long *s_r3t, *table;</span>
<span class="quote">&gt; @@ -1691,7 +1706,7 @@ int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t,</span>
<span class="quote">&gt;  	origin = r3t &amp; _REGION_ENTRY_ORIGIN;</span>
<span class="quote">&gt;  	offset = ((r3t &amp; _REGION_ENTRY_OFFSET) &gt;&gt; 6) * PAGE_SIZE;</span>
<span class="quote">&gt;  	len = ((r3t &amp; _REGION_ENTRY_LENGTH) + 1) * PAGE_SIZE - offset;</span>
<span class="quote">&gt; -	rc = gmap_protect_rmap(sg, raddr, origin + offset, len);</span>
<span class="quote">&gt; +	rc = gmap_protect_rmap(sg, raddr, origin + offset, len, mmrange);</span>
<span class="quote">&gt;  	spin_lock(&amp;sg-&gt;guest_table_lock);</span>
<span class="quote">&gt;  	if (!rc) {</span>
<span class="quote">&gt;  		table = gmap_table_walk(sg, saddr, 3);</span>
<span class="quote">&gt; @@ -1718,6 +1733,7 @@ EXPORT_SYMBOL_GPL(gmap_shadow_r3t);</span>
<span class="quote">&gt;   * @saddr: faulting address in the shadow gmap</span>
<span class="quote">&gt;   * @sgt: parent gmap address of the segment table to get shadowed</span>
<span class="quote">&gt;   * @fake: sgt references contiguous guest memory block, not a sgt</span>
<span class="quote">&gt; + * @mmrange: address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Returns: 0 if successfully shadowed or already shadowed, -EAGAIN if the</span>
<span class="quote">&gt;   * shadow table structure is incomplete, -ENOMEM if out of memory and</span>
<span class="quote">&gt; @@ -1726,7 +1742,7 @@ EXPORT_SYMBOL_GPL(gmap_shadow_r3t);</span>
<span class="quote">&gt;   * Called with sg-&gt;mm-&gt;mmap_sem in read.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt,</span>
<span class="quote">&gt; -		    int fake)</span>
<span class="quote">&gt; +		    int fake, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long raddr, origin, offset, len;</span>
<span class="quote">&gt;  	unsigned long *s_sgt, *table;</span>
<span class="quote">&gt; @@ -1775,7 +1791,7 @@ int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt,</span>
<span class="quote">&gt;  	origin = sgt &amp; _REGION_ENTRY_ORIGIN;</span>
<span class="quote">&gt;  	offset = ((sgt &amp; _REGION_ENTRY_OFFSET) &gt;&gt; 6) * PAGE_SIZE;</span>
<span class="quote">&gt;  	len = ((sgt &amp; _REGION_ENTRY_LENGTH) + 1) * PAGE_SIZE - offset;</span>
<span class="quote">&gt; -	rc = gmap_protect_rmap(sg, raddr, origin + offset, len);</span>
<span class="quote">&gt; +	rc = gmap_protect_rmap(sg, raddr, origin + offset, len, mmrange);</span>
<span class="quote">&gt;  	spin_lock(&amp;sg-&gt;guest_table_lock);</span>
<span class="quote">&gt;  	if (!rc) {</span>
<span class="quote">&gt;  		table = gmap_table_walk(sg, saddr, 2);</span>
<span class="quote">&gt; @@ -1842,6 +1858,7 @@ EXPORT_SYMBOL_GPL(gmap_shadow_pgt_lookup);</span>
<span class="quote">&gt;   * @saddr: faulting address in the shadow gmap</span>
<span class="quote">&gt;   * @pgt: parent gmap address of the page table to get shadowed</span>
<span class="quote">&gt;   * @fake: pgt references contiguous guest memory block, not a pgtable</span>
<span class="quote">&gt; + * @mmrange: address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Returns 0 if successfully shadowed or already shadowed, -EAGAIN if the</span>
<span class="quote">&gt;   * shadow table structure is incomplete, -ENOMEM if out of memory,</span>
<span class="quote">&gt; @@ -1850,7 +1867,7 @@ EXPORT_SYMBOL_GPL(gmap_shadow_pgt_lookup);</span>
<span class="quote">&gt;   * Called with gmap-&gt;mm-&gt;mmap_sem in read</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt,</span>
<span class="quote">&gt; -		    int fake)</span>
<span class="quote">&gt; +		    int fake, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long raddr, origin;</span>
<span class="quote">&gt;  	unsigned long *s_pgt, *table;</span>
<span class="quote">&gt; @@ -1894,7 +1911,7 @@ int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt,</span>
<span class="quote">&gt;  	/* Make pgt read-only in parent gmap page table (not the pgste) */</span>
<span class="quote">&gt;  	raddr = (saddr &amp; _SEGMENT_MASK) | _SHADOW_RMAP_SEGMENT;</span>
<span class="quote">&gt;  	origin = pgt &amp; _SEGMENT_ENTRY_ORIGIN &amp; PAGE_MASK;</span>
<span class="quote">&gt; -	rc = gmap_protect_rmap(sg, raddr, origin, PAGE_SIZE);</span>
<span class="quote">&gt; +	rc = gmap_protect_rmap(sg, raddr, origin, PAGE_SIZE, mmrange);</span>
<span class="quote">&gt;  	spin_lock(&amp;sg-&gt;guest_table_lock);</span>
<span class="quote">&gt;  	if (!rc) {</span>
<span class="quote">&gt;  		table = gmap_table_walk(sg, saddr, 1);</span>
<span class="quote">&gt; @@ -1921,6 +1938,7 @@ EXPORT_SYMBOL_GPL(gmap_shadow_pgt);</span>
<span class="quote">&gt;   * @sg: pointer to the shadow guest address space structure</span>
<span class="quote">&gt;   * @saddr: faulting address in the shadow gmap</span>
<span class="quote">&gt;   * @pte: pte in parent gmap address space to get shadowed</span>
<span class="quote">&gt; + * @mmrange: address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Returns 0 if successfully shadowed or already shadowed, -EAGAIN if the</span>
<span class="quote">&gt;   * shadow table structure is incomplete, -ENOMEM if out of memory and</span>
<span class="quote">&gt; @@ -1928,7 +1946,8 @@ EXPORT_SYMBOL_GPL(gmap_shadow_pgt);</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Called with sg-&gt;mm-&gt;mmap_sem in read.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte)</span>
<span class="quote">&gt; +int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte,</span>
<span class="quote">&gt; +		     struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct gmap *parent;</span>
<span class="quote">&gt;  	struct gmap_rmap *rmap;</span>
<span class="quote">&gt; @@ -1982,7 +2001,7 @@ int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte)</span>
<span class="quote">&gt;  		radix_tree_preload_end();</span>
<span class="quote">&gt;  		if (!rc)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; -		rc = gmap_pte_op_fixup(parent, paddr, vmaddr, prot);</span>
<span class="quote">&gt; +		rc = gmap_pte_op_fixup(parent, paddr, vmaddr, prot, mmrange);</span>
<span class="quote">&gt;  		if (rc)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -2117,7 +2136,8 @@ static inline void thp_split_mm(struct mm_struct *mm)</span>
<span class="quote">&gt;   * - This must be called after THP was enabled</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int __zap_zero_pages(pmd_t *pmd, unsigned long start,</span>
<span class="quote">&gt; -			   unsigned long end, struct mm_walk *walk)</span>
<span class="quote">&gt; +			    unsigned long end, struct mm_walk *walk,</span>
<span class="quote">&gt; +			    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long addr;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -2133,12 +2153,13 @@ static int __zap_zero_pages(pmd_t *pmd, unsigned long start,</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static inline void zap_zero_pages(struct mm_struct *mm)</span>
<span class="quote">&gt; +static inline void zap_zero_pages(struct mm_struct *mm,</span>
<span class="quote">&gt; +				  struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_walk walk = { .pmd_entry = __zap_zero_pages };</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	walk.mm = mm;</span>
<span class="quote">&gt; -	walk_page_range(0, TASK_SIZE, &amp;walk);</span>
<span class="quote">&gt; +	walk_page_range(0, TASK_SIZE, &amp;walk, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -2147,6 +2168,7 @@ static inline void zap_zero_pages(struct mm_struct *mm)</span>
<span class="quote">&gt;  int s390_enable_sie(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Do we have pgstes? if yes, we are done */</span>
<span class="quote">&gt;  	if (mm_has_pgste(mm))</span>
<span class="quote">&gt; @@ -2158,7 +2180,7 @@ int s390_enable_sie(void)</span>
<span class="quote">&gt;  	mm-&gt;context.has_pgste = 1;</span>
<span class="quote">&gt;  	/* split thp mappings and disable thp for future mappings */</span>
<span class="quote">&gt;  	thp_split_mm(mm);</span>
<span class="quote">&gt; -	zap_zero_pages(mm);</span>
<span class="quote">&gt; +	zap_zero_pages(mm, &amp;mmrange);</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2182,6 +2204,7 @@ int s390_enable_skey(void)</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	int rc = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	if (mm_use_skey(mm))</span>
<span class="quote">&gt; @@ -2190,7 +2213,7 @@ int s390_enable_skey(void)</span>
<span class="quote">&gt;  	mm-&gt;context.use_skey = 1;</span>
<span class="quote">&gt;  	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {</span>
<span class="quote">&gt;  		if (ksm_madvise(vma, vma-&gt;vm_start, vma-&gt;vm_end,</span>
<span class="quote">&gt; -				MADV_UNMERGEABLE, &amp;vma-&gt;vm_flags)) {</span>
<span class="quote">&gt; +				MADV_UNMERGEABLE, &amp;vma-&gt;vm_flags, &amp;mmrange)) {</span>
<span class="quote">&gt;  			mm-&gt;context.use_skey = 0;</span>
<span class="quote">&gt;  			rc = -ENOMEM;</span>
<span class="quote">&gt;  			goto out_up;</span>
<span class="quote">&gt; @@ -2199,7 +2222,7 @@ int s390_enable_skey(void)</span>
<span class="quote">&gt;  	mm-&gt;def_flags &amp;= ~VM_MERGEABLE;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	walk.mm = mm;</span>
<span class="quote">&gt; -	walk_page_range(0, TASK_SIZE, &amp;walk);</span>
<span class="quote">&gt; +	walk_page_range(0, TASK_SIZE, &amp;walk, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  out_up:</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; @@ -2220,10 +2243,11 @@ static int __s390_reset_cmma(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt;  void s390_reset_cmma(struct mm_struct *mm)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_walk walk = { .pte_entry = __s390_reset_cmma };</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	walk.mm = mm;</span>
<span class="quote">&gt; -	walk_page_range(0, TASK_SIZE, &amp;walk);</span>
<span class="quote">&gt; +	walk_page_range(0, TASK_SIZE, &amp;walk, &amp;mmrange);</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(s390_reset_cmma);</span>
<span class="quote">&gt; diff --git a/arch/score/mm/fault.c b/arch/score/mm/fault.c</span>
<span class="quote">&gt; index b85fad4f0874..07a8637ad142 100644</span>
<span class="quote">&gt; --- a/arch/score/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/score/mm/fault.c</span>
<span class="quote">&gt; @@ -51,6 +51,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,</span>
<span class="quote">&gt;  	unsigned long flags = 0;</span>
<span class="quote">&gt;  	siginfo_t info;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	info.si_code = SEGV_MAPERR;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -111,7 +112,7 @@ asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,</span>
<span class="quote">&gt;  	* make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	* the fault.</span>
<span class="quote">&gt;  	*/</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, mmrange);</span>
<span class="quote">&gt;  	if (unlikely(fault &amp; VM_FAULT_ERROR)) {</span>
<span class="quote">&gt;  		if (fault &amp; VM_FAULT_OOM)</span>
<span class="quote">&gt;  			goto out_of_memory;</span>
<span class="quote">&gt; diff --git a/arch/sh/mm/fault.c b/arch/sh/mm/fault.c</span>
<span class="quote">&gt; index 6fd1bf7481c7..d36106564728 100644</span>
<span class="quote">&gt; --- a/arch/sh/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/sh/mm/fault.c</span>
<span class="quote">&gt; @@ -405,6 +405,7 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,</span>
<span class="quote">&gt;  	struct vm_area_struct * vma;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	tsk = current;</span>
<span class="quote">&gt;  	mm = tsk-&gt;mm;</span>
<span class="quote">&gt; @@ -488,7 +489,7 @@ asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (unlikely(fault &amp; (VM_FAULT_RETRY | VM_FAULT_ERROR)))</span>
<span class="quote">&gt;  		if (mm_fault_error(regs, error_code, address, fault))</span>
<span class="quote">&gt; diff --git a/arch/sparc/mm/fault_32.c b/arch/sparc/mm/fault_32.c</span>
<span class="quote">&gt; index a8103a84b4ac..ebb2406dbe7c 100644</span>
<span class="quote">&gt; --- a/arch/sparc/mm/fault_32.c</span>
<span class="quote">&gt; +++ b/arch/sparc/mm/fault_32.c</span>
<span class="quote">&gt; @@ -176,6 +176,7 @@ asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,</span>
<span class="quote">&gt;  	int from_user = !(regs-&gt;psr &amp; PSR_PS);</span>
<span class="quote">&gt;  	int fault, code;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (text_fault)</span>
<span class="quote">&gt;  		address = regs-&gt;pc;</span>
<span class="quote">&gt; @@ -242,7 +243,7 @@ asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; @@ -389,6 +390,7 @@ static void force_user_fault(unsigned long address, int write)</span>
<span class="quote">&gt;  	struct mm_struct *mm = tsk-&gt;mm;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_USER;</span>
<span class="quote">&gt;  	int code;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	code = SEGV_MAPERR;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -412,7 +414,7 @@ static void force_user_fault(unsigned long address, int write)</span>
<span class="quote">&gt;  		if (!(vma-&gt;vm_flags &amp; (VM_READ | VM_EXEC)))</span>
<span class="quote">&gt;  			goto bad_area;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	switch (handle_mm_fault(vma, address, flags)) {</span>
<span class="quote">&gt; +	switch (handle_mm_fault(vma, address, flags, &amp;mmrange)) {</span>
<span class="quote">&gt;  	case VM_FAULT_SIGBUS:</span>
<span class="quote">&gt;  	case VM_FAULT_OOM:</span>
<span class="quote">&gt;  		goto do_sigbus;</span>
<span class="quote">&gt; diff --git a/arch/sparc/mm/fault_64.c b/arch/sparc/mm/fault_64.c</span>
<span class="quote">&gt; index 41363f46797b..e0a3c36b0fa1 100644</span>
<span class="quote">&gt; --- a/arch/sparc/mm/fault_64.c</span>
<span class="quote">&gt; +++ b/arch/sparc/mm/fault_64.c</span>
<span class="quote">&gt; @@ -287,6 +287,7 @@ asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)</span>
<span class="quote">&gt;  	int si_code, fault_code, fault;</span>
<span class="quote">&gt;  	unsigned long address, mm_rss;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	fault_code = get_thread_fault_code();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -438,7 +439,7 @@ asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)</span>
<span class="quote">&gt;  			goto bad_area;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		goto exit_exception;</span>
<span class="quote">&gt; diff --git a/arch/tile/mm/fault.c b/arch/tile/mm/fault.c</span>
<span class="quote">&gt; index f58fa06a2214..09f053eb146f 100644</span>
<span class="quote">&gt; --- a/arch/tile/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/tile/mm/fault.c</span>
<span class="quote">&gt; @@ -275,6 +275,7 @@ static int handle_page_fault(struct pt_regs *regs,</span>
<span class="quote">&gt;  	int is_kernel_mode;</span>
<span class="quote">&gt;  	pgd_t *pgd;</span>
<span class="quote">&gt;  	unsigned int flags;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* on TILE, protection faults are always writes */</span>
<span class="quote">&gt;  	if (!is_page_fault)</span>
<span class="quote">&gt; @@ -437,7 +438,7 @@ static int handle_page_fault(struct pt_regs *regs,</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h</span>
<span class="quote">&gt; index fca34b2177e2..98cc3e36385a 100644</span>
<span class="quote">&gt; --- a/arch/um/include/asm/mmu_context.h</span>
<span class="quote">&gt; +++ b/arch/um/include/asm/mmu_context.h</span>
<span class="quote">&gt; @@ -23,7 +23,8 @@ static inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)</span>
<span class="quote">&gt;  extern void arch_exit_mmap(struct mm_struct *mm);</span>
<span class="quote">&gt;  static inline void arch_unmap(struct mm_struct *mm,</span>
<span class="quote">&gt;  			struct vm_area_struct *vma,</span>
<span class="quote">&gt; -			unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +			unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +			struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  static inline void arch_bprm_mm_init(struct mm_struct *mm,</span>
<span class="quote">&gt; diff --git a/arch/um/kernel/trap.c b/arch/um/kernel/trap.c</span>
<span class="quote">&gt; index b2b02df9896e..e632a14e896e 100644</span>
<span class="quote">&gt; --- a/arch/um/kernel/trap.c</span>
<span class="quote">&gt; +++ b/arch/um/kernel/trap.c</span>
<span class="quote">&gt; @@ -33,6 +33,7 @@ int handle_page_fault(unsigned long address, unsigned long ip,</span>
<span class="quote">&gt;  	pte_t *pte;</span>
<span class="quote">&gt;  	int err = -EFAULT;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	*code_out = SEGV_MAPERR;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -74,7 +75,7 @@ int handle_page_fault(unsigned long address, unsigned long ip,</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt;  		int fault;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -		fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +		fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  			goto out_nosemaphore;</span>
<span class="quote">&gt; diff --git a/arch/unicore32/mm/fault.c b/arch/unicore32/mm/fault.c</span>
<span class="quote">&gt; index bbefcc46a45e..dd35b6191798 100644</span>
<span class="quote">&gt; --- a/arch/unicore32/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/unicore32/mm/fault.c</span>
<span class="quote">&gt; @@ -168,7 +168,8 @@ static inline bool access_error(unsigned int fsr, struct vm_area_struct *vma)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int __do_pf(struct mm_struct *mm, unsigned long addr, unsigned int fsr,</span>
<span class="quote">&gt; -		unsigned int flags, struct task_struct *tsk)</span>
<span class="quote">&gt; +		   unsigned int flags, struct task_struct *tsk,</span>
<span class="quote">&gt; +		   struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt; @@ -194,7 +195,7 @@ static int __do_pf(struct mm_struct *mm, unsigned long addr, unsigned int fsr,</span>
<span class="quote">&gt;  	 * If for any reason at all we couldn&#39;t handle the fault, make</span>
<span class="quote">&gt;  	 * sure we exit gracefully rather than endlessly redo the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, addr &amp; PAGE_MASK, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, addr &amp; PAGE_MASK, flags, mmrange);</span>
<span class="quote">&gt;  	return fault;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  check_stack:</span>
<span class="quote">&gt; @@ -210,6 +211,7 @@ static int do_pf(unsigned long addr, unsigned int fsr, struct pt_regs *regs)</span>
<span class="quote">&gt;  	struct mm_struct *mm;</span>
<span class="quote">&gt;  	int fault, sig, code;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	tsk = current;</span>
<span class="quote">&gt;  	mm = tsk-&gt;mm;</span>
<span class="quote">&gt; @@ -251,7 +253,7 @@ static int do_pf(unsigned long addr, unsigned int fsr, struct pt_regs *regs)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	fault = __do_pf(mm, addr, fsr, flags, tsk);</span>
<span class="quote">&gt; +	fault = __do_pf(mm, addr, fsr, flags, tsk, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* If we need to retry but a fatal signal is pending, handle the</span>
<span class="quote">&gt;  	 * signal first. We do not need to release the mmap_sem because</span>
<span class="quote">&gt; diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c</span>
<span class="quote">&gt; index 5b8b556dbb12..2e0bdf6a3aaf 100644</span>
<span class="quote">&gt; --- a/arch/x86/entry/vdso/vma.c</span>
<span class="quote">&gt; +++ b/arch/x86/entry/vdso/vma.c</span>
<span class="quote">&gt; @@ -155,6 +155,7 @@ static int map_vdso(const struct vdso_image *image, unsigned long addr)</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	unsigned long text_start;</span>
<span class="quote">&gt;  	int ret = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="quote">&gt;  		return -EINTR;</span>
<span class="quote">&gt; @@ -192,7 +193,7 @@ static int map_vdso(const struct vdso_image *image, unsigned long addr)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (IS_ERR(vma)) {</span>
<span class="quote">&gt;  		ret = PTR_ERR(vma);</span>
<span class="quote">&gt; -		do_munmap(mm, text_start, image-&gt;size, NULL);</span>
<span class="quote">&gt; +		do_munmap(mm, text_start, image-&gt;size, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt;  		current-&gt;mm-&gt;context.vdso = (void __user *)text_start;</span>
<span class="quote">&gt;  		current-&gt;mm-&gt;context.vdso_image = image;</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; index c931b88982a0..31fb02ed4770 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="quote">&gt; @@ -263,7 +263,8 @@ static inline void arch_bprm_mm_init(struct mm_struct *mm,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; -			      unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +			      unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +			      struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * mpx_notify_unmap() goes and reads a rarely-hot</span>
<span class="quote">&gt; @@ -283,7 +284,7 @@ static inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	 * consistently wrong.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (unlikely(cpu_feature_enabled(X86_FEATURE_MPX)))</span>
<span class="quote">&gt; -		mpx_notify_unmap(mm, vma, start, end);</span>
<span class="quote">&gt; +		mpx_notify_unmap(mm, vma, start, end, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/mpx.h b/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt; index 61eb4b63c5ec..c26099224a17 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt; @@ -73,7 +73,8 @@ static inline void mpx_mm_init(struct mm_struct *mm)</span>
<span class="quote">&gt;  	mm-&gt;context.bd_addr = MPX_INVALID_BOUNDS_DIR;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; -		      unsigned long start, unsigned long end);</span>
<span class="quote">&gt; +		      unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +		      struct range_lock *mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="quote">&gt;  		unsigned long flags);</span>
<span class="quote">&gt; @@ -95,7 +96,8 @@ static inline void mpx_mm_init(struct mm_struct *mm)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  static inline void mpx_notify_unmap(struct mm_struct *mm,</span>
<span class="quote">&gt;  				    struct vm_area_struct *vma,</span>
<span class="quote">&gt; -				    unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +				    unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +				    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="quote">&gt; index 800de815519c..93f1b8d4c88e 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/fault.c</span>
<span class="quote">&gt; @@ -1244,6 +1244,7 @@ __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
<span class="quote">&gt;  	int fault, major = 0;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt;  	u32 pkey;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	tsk = current;</span>
<span class="quote">&gt;  	mm = tsk-&gt;mm;</span>
<span class="quote">&gt; @@ -1423,7 +1424,7 @@ __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
<span class="quote">&gt;  	 * fault, so we read the pkey beforehand.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	pkey = vma_pkey(vma);</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt;  	major |= fault &amp; VM_FAULT_MAJOR;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="quote">&gt; index e500949bae24..51c3e1f7e6be 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/mpx.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/mpx.c</span>
<span class="quote">&gt; @@ -47,6 +47,7 @@ static unsigned long mpx_mmap(unsigned long len)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	unsigned long addr, populate;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Only bounds table can be allocated here */</span>
<span class="quote">&gt;  	if (len != mpx_bt_size_bytes(mm))</span>
<span class="quote">&gt; @@ -54,7 +55,8 @@ static unsigned long mpx_mmap(unsigned long len)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	addr = do_mmap(NULL, 0, len, PROT_READ | PROT_WRITE,</span>
<span class="quote">&gt; -		       MAP_ANONYMOUS | MAP_PRIVATE, VM_MPX, 0, &amp;populate, NULL);</span>
<span class="quote">&gt; +		       MAP_ANONYMOUS | MAP_PRIVATE, VM_MPX, 0, &amp;populate, NULL,</span>
<span class="quote">&gt; +		       &amp;mmrange);</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	if (populate)</span>
<span class="quote">&gt;  		mm_populate(addr, populate);</span>
<span class="quote">&gt; @@ -427,13 +429,15 @@ int mpx_handle_bd_fault(void)</span>
<span class="quote">&gt;   * A thin wrapper around get_user_pages().  Returns 0 if the</span>
<span class="quote">&gt;   * fault was resolved or -errno if not.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static int mpx_resolve_fault(long __user *addr, int write)</span>
<span class="quote">&gt; +static int mpx_resolve_fault(long __user *addr, int write,</span>
<span class="quote">&gt; +			     struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	long gup_ret;</span>
<span class="quote">&gt;  	int nr_pages = 1;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	gup_ret = get_user_pages((unsigned long)addr, nr_pages,</span>
<span class="quote">&gt; -			write ? FOLL_WRITE : 0,	NULL, NULL);</span>
<span class="quote">&gt; +		       write ? FOLL_WRITE : 0,	NULL, NULL,</span>
<span class="quote">&gt; +		       mmrange);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * get_user_pages() returns number of pages gotten.</span>
<span class="quote">&gt;  	 * 0 means we failed to fault in and get anything,</span>
<span class="quote">&gt; @@ -500,7 +504,8 @@ static int get_user_bd_entry(struct mm_struct *mm, unsigned long *bd_entry_ret,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int get_bt_addr(struct mm_struct *mm,</span>
<span class="quote">&gt;  			long __user *bd_entry_ptr,</span>
<span class="quote">&gt; -			unsigned long *bt_addr_result)</span>
<span class="quote">&gt; +		        unsigned long *bt_addr_result,</span>
<span class="quote">&gt; +		        struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  	int valid_bit;</span>
<span class="quote">&gt; @@ -519,7 +524,8 @@ static int get_bt_addr(struct mm_struct *mm,</span>
<span class="quote">&gt;  		if (!ret)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  		if (ret == -EFAULT)</span>
<span class="quote">&gt; -			ret = mpx_resolve_fault(bd_entry_ptr, need_write);</span>
<span class="quote">&gt; +			ret = mpx_resolve_fault(bd_entry_ptr,</span>
<span class="quote">&gt; +						need_write, mmrange);</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * If we could not resolve the fault, consider it</span>
<span class="quote">&gt;  		 * userspace&#39;s fault and error out.</span>
<span class="quote">&gt; @@ -730,7 +736,8 @@ static unsigned long mpx_get_bd_entry_offset(struct mm_struct *mm,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int unmap_entire_bt(struct mm_struct *mm,</span>
<span class="quote">&gt; -		long __user *bd_entry, unsigned long bt_addr)</span>
<span class="quote">&gt; +		long __user *bd_entry, unsigned long bt_addr,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long expected_old_val = bt_addr | MPX_BD_ENTRY_VALID_FLAG;</span>
<span class="quote">&gt;  	unsigned long uninitialized_var(actual_old_val);</span>
<span class="quote">&gt; @@ -747,7 +754,7 @@ static int unmap_entire_bt(struct mm_struct *mm,</span>
<span class="quote">&gt;  		if (!ret)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  		if (ret == -EFAULT)</span>
<span class="quote">&gt; -			ret = mpx_resolve_fault(bd_entry, need_write);</span>
<span class="quote">&gt; +			ret = mpx_resolve_fault(bd_entry, need_write, mmrange);</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * If we could not resolve the fault, consider it</span>
<span class="quote">&gt;  		 * userspace&#39;s fault and error out.</span>
<span class="quote">&gt; @@ -780,11 +787,12 @@ static int unmap_entire_bt(struct mm_struct *mm,</span>
<span class="quote">&gt;  	 * avoid recursion, do_munmap() will check whether it comes</span>
<span class="quote">&gt;  	 * from one bounds table through VM_MPX flag.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	return do_munmap(mm, bt_addr, mpx_bt_size_bytes(mm), NULL);</span>
<span class="quote">&gt; +	return do_munmap(mm, bt_addr, mpx_bt_size_bytes(mm), NULL, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int try_unmap_single_bt(struct mm_struct *mm,</span>
<span class="quote">&gt; -	       unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +	       unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +	       struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *next;</span>
<span class="quote">&gt;  	struct vm_area_struct *prev;</span>
<span class="quote">&gt; @@ -835,7 +843,7 @@ static int try_unmap_single_bt(struct mm_struct *mm,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	bde_vaddr = mm-&gt;context.bd_addr + mpx_get_bd_entry_offset(mm, start);</span>
<span class="quote">&gt; -	ret = get_bt_addr(mm, bde_vaddr, &amp;bt_addr);</span>
<span class="quote">&gt; +	ret = get_bt_addr(mm, bde_vaddr, &amp;bt_addr, mmrange);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * No bounds table there, so nothing to unmap.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; @@ -853,12 +861,13 @@ static int try_unmap_single_bt(struct mm_struct *mm,</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if ((start == bta_start_vaddr) &amp;&amp;</span>
<span class="quote">&gt;  	    (end == bta_end_vaddr))</span>
<span class="quote">&gt; -		return unmap_entire_bt(mm, bde_vaddr, bt_addr);</span>
<span class="quote">&gt; +		return unmap_entire_bt(mm, bde_vaddr, bt_addr, mmrange);</span>
<span class="quote">&gt;  	return zap_bt_entries_mapping(mm, bt_addr, start, end);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int mpx_unmap_tables(struct mm_struct *mm,</span>
<span class="quote">&gt; -		unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +			    unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +			    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long one_unmap_start;</span>
<span class="quote">&gt;  	trace_mpx_unmap_search(start, end);</span>
<span class="quote">&gt; @@ -876,7 +885,8 @@ static int mpx_unmap_tables(struct mm_struct *mm,</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		if (one_unmap_end &gt; next_unmap_start)</span>
<span class="quote">&gt;  			one_unmap_end = next_unmap_start;</span>
<span class="quote">&gt; -		ret = try_unmap_single_bt(mm, one_unmap_start, one_unmap_end);</span>
<span class="quote">&gt; +		ret = try_unmap_single_bt(mm, one_unmap_start, one_unmap_end,</span>
<span class="quote">&gt; +					  mmrange);</span>
<span class="quote">&gt;  		if (ret)</span>
<span class="quote">&gt;  			return ret;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -894,7 +904,8 @@ static int mpx_unmap_tables(struct mm_struct *mm,</span>
<span class="quote">&gt;   * necessary, and the &#39;vma&#39; is the first vma in this range (start -&gt; end).</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; -		unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +		unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -920,7 +931,7 @@ void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		vma = vma-&gt;vm_next;</span>
<span class="quote">&gt;  	} while (vma &amp;&amp; vma-&gt;vm_start &lt; end);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	ret = mpx_unmap_tables(mm, start, end);</span>
<span class="quote">&gt; +	ret = mpx_unmap_tables(mm, start, end, mmrange);</span>
<span class="quote">&gt;  	if (ret)</span>
<span class="quote">&gt;  		force_sig(SIGSEGV, current);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/arch/xtensa/mm/fault.c b/arch/xtensa/mm/fault.c</span>
<span class="quote">&gt; index 8b9b6f44bb06..6f8e3e7cccb5 100644</span>
<span class="quote">&gt; --- a/arch/xtensa/mm/fault.c</span>
<span class="quote">&gt; +++ b/arch/xtensa/mm/fault.c</span>
<span class="quote">&gt; @@ -44,6 +44,7 @@ void do_page_fault(struct pt_regs *regs)</span>
<span class="quote">&gt;  	int is_write, is_exec;</span>
<span class="quote">&gt;  	int fault;</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	info.si_code = SEGV_MAPERR;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -108,7 +109,7 @@ void do_page_fault(struct pt_regs *regs)</span>
<span class="quote">&gt;  	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="quote">&gt;  	 * the fault.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	fault = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c</span>
<span class="quote">&gt; index e4bb435e614b..bd464a599341 100644</span>
<span class="quote">&gt; --- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c</span>
<span class="quote">&gt; +++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c</span>
<span class="quote">&gt; @@ -691,6 +691,7 @@ int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)</span>
<span class="quote">&gt;  	unsigned int flags = 0;</span>
<span class="quote">&gt;  	unsigned pinned = 0;</span>
<span class="quote">&gt;  	int r;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (!(gtt-&gt;userflags &amp; AMDGPU_GEM_USERPTR_READONLY))</span>
<span class="quote">&gt;  		flags |= FOLL_WRITE;</span>
<span class="quote">&gt; @@ -721,7 +722,7 @@ int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)</span>
<span class="quote">&gt;  		list_add(&amp;guptask.list, &amp;gtt-&gt;guptasks);</span>
<span class="quote">&gt;  		spin_unlock(&amp;gtt-&gt;guptasklock);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -		r = get_user_pages(userptr, num_pages, flags, p, NULL);</span>
<span class="quote">&gt; +		r = get_user_pages(userptr, num_pages, flags, p, NULL, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		spin_lock(&amp;gtt-&gt;guptasklock);</span>
<span class="quote">&gt;  		list_del(&amp;guptask.list);</span>
<span class="quote">&gt; diff --git a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="quote">&gt; index 382a77a1097e..881bcc7d663a 100644</span>
<span class="quote">&gt; --- a/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="quote">&gt; +++ b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="quote">&gt; @@ -512,6 +512,8 @@ __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		ret = -EFAULT;</span>
<span class="quote">&gt;  		if (mmget_not_zero(mm)) {</span>
<span class="quote">&gt; +			DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  			down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  			while (pinned &lt; npages) {</span>
<span class="quote">&gt;  				ret = get_user_pages_remote</span>
<span class="quote">&gt; @@ -519,7 +521,7 @@ __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
<span class="quote">&gt;  					 obj-&gt;userptr.ptr + pinned * PAGE_SIZE,</span>
<span class="quote">&gt;  					 npages - pinned,</span>
<span class="quote">&gt;  					 flags,</span>
<span class="quote">&gt; -					 pvec + pinned, NULL, NULL);</span>
<span class="quote">&gt; +					 pvec + pinned, NULL, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  				if (ret &lt; 0)</span>
<span class="quote">&gt;  					break;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/gpu/drm/radeon/radeon_ttm.c b/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="quote">&gt; index a0a839bc39bf..9fc3a4f86945 100644</span>
<span class="quote">&gt; --- a/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="quote">&gt; +++ b/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="quote">&gt; @@ -545,6 +545,8 @@ static int radeon_ttm_tt_pin_userptr(struct ttm_tt *ttm)</span>
<span class="quote">&gt;  	struct radeon_ttm_tt *gtt = (void *)ttm;</span>
<span class="quote">&gt;  	unsigned pinned = 0, nents;</span>
<span class="quote">&gt;  	int r;</span>
<span class="quote">&gt; +	// XXX: this is wrong!!</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	int write = !(gtt-&gt;userflags &amp; RADEON_GEM_USERPTR_READONLY);</span>
<span class="quote">&gt;  	enum dma_data_direction direction = write ?</span>
<span class="quote">&gt; @@ -569,7 +571,7 @@ static int radeon_ttm_tt_pin_userptr(struct ttm_tt *ttm)</span>
<span class="quote">&gt;  		struct page **pages = ttm-&gt;pages + pinned;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		r = get_user_pages(userptr, num_pages, write ? FOLL_WRITE : 0,</span>
<span class="quote">&gt; -				   pages, NULL);</span>
<span class="quote">&gt; +				   pages, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  		if (r &lt; 0)</span>
<span class="quote">&gt;  			goto release_pages;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/infiniband/core/umem.c b/drivers/infiniband/core/umem.c</span>
<span class="quote">&gt; index 9a4e899d94b3..fd9601ed5b84 100644</span>
<span class="quote">&gt; --- a/drivers/infiniband/core/umem.c</span>
<span class="quote">&gt; +++ b/drivers/infiniband/core/umem.c</span>
<span class="quote">&gt; @@ -96,6 +96,7 @@ struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
<span class="quote">&gt;  	struct scatterlist *sg, *sg_list_start;</span>
<span class="quote">&gt;  	int need_release = 0;</span>
<span class="quote">&gt;  	unsigned int gup_flags = FOLL_WRITE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (dmasync)</span>
<span class="quote">&gt;  		dma_attrs |= DMA_ATTR_WRITE_BARRIER;</span>
<span class="quote">&gt; @@ -194,7 +195,7 @@ struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
<span class="quote">&gt;  		ret = get_user_pages_longterm(cur_base,</span>
<span class="quote">&gt;  				     min_t(unsigned long, npages,</span>
<span class="quote">&gt;  					   PAGE_SIZE / sizeof (struct page *)),</span>
<span class="quote">&gt; -				     gup_flags, page_list, vma_list);</span>
<span class="quote">&gt; +				      gup_flags, page_list, vma_list, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (ret &lt; 0)</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt; diff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c</span>
<span class="quote">&gt; index 2aadf5813a40..0572953260e8 100644</span>
<span class="quote">&gt; --- a/drivers/infiniband/core/umem_odp.c</span>
<span class="quote">&gt; +++ b/drivers/infiniband/core/umem_odp.c</span>
<span class="quote">&gt; @@ -632,6 +632,7 @@ int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,</span>
<span class="quote">&gt;  	int j, k, ret = 0, start_idx, npages = 0, page_shift;</span>
<span class="quote">&gt;  	unsigned int flags = 0;</span>
<span class="quote">&gt;  	phys_addr_t p = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (access_mask == 0)</span>
<span class="quote">&gt;  		return -EINVAL;</span>
<span class="quote">&gt; @@ -683,7 +684,7 @@ int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		npages = get_user_pages_remote(owning_process, owning_mm,</span>
<span class="quote">&gt;  				user_virt, gup_num_pages,</span>
<span class="quote">&gt; -				flags, local_page_list, NULL, NULL);</span>
<span class="quote">&gt; +				flags, local_page_list, NULL, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  		up_read(&amp;owning_mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (npages &lt; 0)</span>
<span class="quote">&gt; diff --git a/drivers/infiniband/hw/qib/qib_user_pages.c b/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="quote">&gt; index ce83ba9a12ef..6bcb4f9f9b30 100644</span>
<span class="quote">&gt; --- a/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="quote">&gt; +++ b/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="quote">&gt; @@ -53,7 +53,7 @@ static void __qib_release_user_pages(struct page **p, size_t num_pages,</span>
<span class="quote">&gt;   * Call with current-&gt;mm-&gt;mmap_sem held.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int __qib_get_user_pages(unsigned long start_page, size_t num_pages,</span>
<span class="quote">&gt; -				struct page **p)</span>
<span class="quote">&gt; +				struct page **p, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long lock_limit;</span>
<span class="quote">&gt;  	size_t got;</span>
<span class="quote">&gt; @@ -70,7 +70,7 @@ static int __qib_get_user_pages(unsigned long start_page, size_t num_pages,</span>
<span class="quote">&gt;  		ret = get_user_pages(start_page + got * PAGE_SIZE,</span>
<span class="quote">&gt;  				     num_pages - got,</span>
<span class="quote">&gt;  				     FOLL_WRITE | FOLL_FORCE,</span>
<span class="quote">&gt; -				     p + got, NULL);</span>
<span class="quote">&gt; +				     p + got, NULL, mmrange);</span>
<span class="quote">&gt;  		if (ret &lt; 0)</span>
<span class="quote">&gt;  			goto bail_release;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -134,10 +134,11 @@ int qib_get_user_pages(unsigned long start_page, size_t num_pages,</span>
<span class="quote">&gt;  		       struct page **p)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	ret = __qib_get_user_pages(start_page, num_pages, p);</span>
<span class="quote">&gt; +	ret = __qib_get_user_pages(start_page, num_pages, p, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/infiniband/hw/usnic/usnic_uiom.c b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="quote">&gt; index 4381c0a9a873..5f36c6d2e21b 100644</span>
<span class="quote">&gt; --- a/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="quote">&gt; +++ b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="quote">&gt; @@ -113,6 +113,7 @@ static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
<span class="quote">&gt;  	int flags;</span>
<span class="quote">&gt;  	dma_addr_t pa;</span>
<span class="quote">&gt;  	unsigned int gup_flags;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (!can_do_mlock())</span>
<span class="quote">&gt;  		return -EPERM;</span>
<span class="quote">&gt; @@ -146,7 +147,7 @@ static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
<span class="quote">&gt;  		ret = get_user_pages(cur_base,</span>
<span class="quote">&gt;  					min_t(unsigned long, npages,</span>
<span class="quote">&gt;  					PAGE_SIZE / sizeof(struct page *)),</span>
<span class="quote">&gt; -					gup_flags, page_list, NULL);</span>
<span class="quote">&gt; +					gup_flags, page_list, NULL, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (ret &lt; 0)</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt; diff --git a/drivers/iommu/amd_iommu_v2.c b/drivers/iommu/amd_iommu_v2.c</span>
<span class="quote">&gt; index 1d0b53a04a08..15a7103fd84c 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/amd_iommu_v2.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/amd_iommu_v2.c</span>
<span class="quote">&gt; @@ -512,6 +512,7 @@ static void do_fault(struct work_struct *work)</span>
<span class="quote">&gt;  	unsigned int flags = 0;</span>
<span class="quote">&gt;  	struct mm_struct *mm;</span>
<span class="quote">&gt;  	u64 address;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	mm = fault-&gt;state-&gt;mm;</span>
<span class="quote">&gt;  	address = fault-&gt;address;</span>
<span class="quote">&gt; @@ -523,7 +524,7 @@ static void do_fault(struct work_struct *work)</span>
<span class="quote">&gt;  	flags |= FAULT_FLAG_REMOTE;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; -	vma = find_extend_vma(mm, address);</span>
<span class="quote">&gt; +	vma = find_extend_vma(mm, address, &amp;mmrange);</span>
<span class="quote">&gt;  	if (!vma || address &lt; vma-&gt;vm_start)</span>
<span class="quote">&gt;  		/* failed to get a vma in the right range */</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; @@ -532,7 +533,7 @@ static void do_fault(struct work_struct *work)</span>
<span class="quote">&gt;  	if (access_error(vma, fault))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	ret = handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +	ret = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/iommu/intel-svm.c b/drivers/iommu/intel-svm.c</span>
<span class="quote">&gt; index 35a408d0ae4f..6a74386ee83f 100644</span>
<span class="quote">&gt; --- a/drivers/iommu/intel-svm.c</span>
<span class="quote">&gt; +++ b/drivers/iommu/intel-svm.c</span>
<span class="quote">&gt; @@ -585,6 +585,7 @@ static irqreturn_t prq_event_thread(int irq, void *d)</span>
<span class="quote">&gt;  	struct intel_iommu *iommu = d;</span>
<span class="quote">&gt;  	struct intel_svm *svm = NULL;</span>
<span class="quote">&gt;  	int head, tail, handled = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Clear PPR bit before reading head/tail registers, to</span>
<span class="quote">&gt;  	 * ensure that we get a new interrupt if needed. */</span>
<span class="quote">&gt; @@ -643,7 +644,7 @@ static irqreturn_t prq_event_thread(int irq, void *d)</span>
<span class="quote">&gt;  			goto bad_req;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		down_read(&amp;svm-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; -		vma = find_extend_vma(svm-&gt;mm, address);</span>
<span class="quote">&gt; +		vma = find_extend_vma(svm-&gt;mm, address, &amp;mmrange);</span>
<span class="quote">&gt;  		if (!vma || address &lt; vma-&gt;vm_start)</span>
<span class="quote">&gt;  			goto invalid;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -651,7 +652,7 @@ static irqreturn_t prq_event_thread(int irq, void *d)</span>
<span class="quote">&gt;  			goto invalid;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		ret = handle_mm_fault(vma, address,</span>
<span class="quote">&gt; -				      req-&gt;wr_req ? FAULT_FLAG_WRITE : 0);</span>
<span class="quote">&gt; +				      req-&gt;wr_req ? FAULT_FLAG_WRITE : 0, &amp;mmrange);</span>
<span class="quote">&gt;  		if (ret &amp; VM_FAULT_ERROR)</span>
<span class="quote">&gt;  			goto invalid;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/media/v4l2-core/videobuf-dma-sg.c b/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="quote">&gt; index f412429cf5ba..64a4cd62eeb3 100644</span>
<span class="quote">&gt; --- a/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="quote">&gt; +++ b/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="quote">&gt; @@ -152,7 +152,8 @@ static void videobuf_dma_init(struct videobuf_dmabuf *dma)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int videobuf_dma_init_user_locked(struct videobuf_dmabuf *dma,</span>
<span class="quote">&gt; -			int direction, unsigned long data, unsigned long size)</span>
<span class="quote">&gt; +			int direction, unsigned long data, unsigned long size,</span>
<span class="quote">&gt; +			struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long first, last;</span>
<span class="quote">&gt;  	int err, rw = 0;</span>
<span class="quote">&gt; @@ -186,7 +187,7 @@ static int videobuf_dma_init_user_locked(struct videobuf_dmabuf *dma,</span>
<span class="quote">&gt;  		data, size, dma-&gt;nr_pages);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	err = get_user_pages_longterm(data &amp; PAGE_MASK, dma-&gt;nr_pages,</span>
<span class="quote">&gt; -			     flags, dma-&gt;pages, NULL);</span>
<span class="quote">&gt; +				      flags, dma-&gt;pages, NULL, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (err != dma-&gt;nr_pages) {</span>
<span class="quote">&gt;  		dma-&gt;nr_pages = (err &gt;= 0) ? err : 0;</span>
<span class="quote">&gt; @@ -201,9 +202,10 @@ static int videobuf_dma_init_user(struct videobuf_dmabuf *dma, int direction,</span>
<span class="quote">&gt;  			   unsigned long data, unsigned long size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; -	ret = videobuf_dma_init_user_locked(dma, direction, data, size);</span>
<span class="quote">&gt; +	ret = videobuf_dma_init_user_locked(dma, direction, data, size, &amp;mmrange);</span>
<span class="quote">&gt;  	up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt; @@ -539,9 +541,14 @@ static int __videobuf_iolock(struct videobuf_queue *q,</span>
<span class="quote">&gt;  			we take current-&gt;mm-&gt;mmap_sem there, to prevent</span>
<span class="quote">&gt;  			locking inversion, so don&#39;t take it here */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +			/* XXX: can we use a local mmrange here? */</span>
<span class="quote">&gt; +			DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  			err = videobuf_dma_init_user_locked(&amp;mem-&gt;dma,</span>
<span class="quote">&gt; -						      DMA_FROM_DEVICE,</span>
<span class="quote">&gt; -						      vb-&gt;baddr, vb-&gt;bsize);</span>
<span class="quote">&gt; +							    DMA_FROM_DEVICE,</span>
<span class="quote">&gt; +							    vb-&gt;baddr,</span>
<span class="quote">&gt; +							    vb-&gt;bsize,</span>
<span class="quote">&gt; +							    &amp;mmrange);</span>
<span class="quote">&gt;  			if (0 != err)</span>
<span class="quote">&gt;  				return err;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -555,6 +562,7 @@ static int __videobuf_iolock(struct videobuf_queue *q,</span>
<span class="quote">&gt;  		 * building for PAE. Compiler doesn&#39;t like direct casting</span>
<span class="quote">&gt;  		 * of a 32 bit ptr to 64 bit integer.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		bus   = (dma_addr_t)(unsigned long)fbuf-&gt;base + vb-&gt;boff;</span>
<span class="quote">&gt;  		pages = PAGE_ALIGN(vb-&gt;size) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  		err = videobuf_dma_init_overlay(&amp;mem-&gt;dma, DMA_FROM_DEVICE,</span>
<span class="quote">&gt; diff --git a/drivers/misc/mic/scif/scif_rma.c b/drivers/misc/mic/scif/scif_rma.c</span>
<span class="quote">&gt; index c824329f7012..6ecac843e5f3 100644</span>
<span class="quote">&gt; --- a/drivers/misc/mic/scif/scif_rma.c</span>
<span class="quote">&gt; +++ b/drivers/misc/mic/scif/scif_rma.c</span>
<span class="quote">&gt; @@ -1332,6 +1332,7 @@ int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
<span class="quote">&gt;  	int prot = *out_prot;</span>
<span class="quote">&gt;  	int ulimit = 0;</span>
<span class="quote">&gt;  	struct mm_struct *mm = NULL;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Unsupported flags */</span>
<span class="quote">&gt;  	if (map_flags &amp; ~(SCIF_MAP_KERNEL | SCIF_MAP_ULIMIT))</span>
<span class="quote">&gt; @@ -1400,7 +1401,7 @@ int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
<span class="quote">&gt;  				nr_pages,</span>
<span class="quote">&gt;  				(prot &amp; SCIF_PROT_WRITE) ? FOLL_WRITE : 0,</span>
<span class="quote">&gt;  				pinned_pages-&gt;pages,</span>
<span class="quote">&gt; -				NULL);</span>
<span class="quote">&gt; +				NULL, &amp;mmrange);</span>
<span class="quote">&gt;  		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  		if (nr_pages != pinned_pages-&gt;nr_pages) {</span>
<span class="quote">&gt;  			if (try_upgrade) {</span>
<span class="quote">&gt; diff --git a/drivers/misc/sgi-gru/grufault.c b/drivers/misc/sgi-gru/grufault.c</span>
<span class="quote">&gt; index 93be82fc338a..b35d60bb2197 100644</span>
<span class="quote">&gt; --- a/drivers/misc/sgi-gru/grufault.c</span>
<span class="quote">&gt; +++ b/drivers/misc/sgi-gru/grufault.c</span>
<span class="quote">&gt; @@ -189,7 +189,8 @@ static void get_clear_fault_map(struct gru_state *gru,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int non_atomic_pte_lookup(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				 unsigned long vaddr, int write,</span>
<span class="quote">&gt; -				 unsigned long *paddr, int *pageshift)</span>
<span class="quote">&gt; +				 unsigned long *paddr, int *pageshift,</span>
<span class="quote">&gt; +				 struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -198,7 +199,8 @@ static int non_atomic_pte_lookup(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  	*pageshift = PAGE_SHIFT;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; -	if (get_user_pages(vaddr, 1, write ? FOLL_WRITE : 0, &amp;page, NULL) &lt;= 0)</span>
<span class="quote">&gt; +	if (get_user_pages(vaddr, 1, write ? FOLL_WRITE : 0,</span>
<span class="quote">&gt; +			   &amp;page, NULL, mmrange) &lt;= 0)</span>

There is no need to pass down the range here since underlying called
__get_user_pages_locked() is told to not unlock the mmap_sem.
In general get_user_pages() doesn&#39;t need a range parameter.
<span class="quote">
&gt;  		return -EFAULT;</span>
<span class="quote">&gt;  	*paddr = page_to_phys(page);</span>
<span class="quote">&gt;  	put_page(page);</span>
<span class="quote">&gt; @@ -263,7 +265,8 @@ static int atomic_pte_lookup(struct vm_area_struct *vma, unsigned long vaddr,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int gru_vtop(struct gru_thread_state *gts, unsigned long vaddr,</span>
<span class="quote">&gt; -		    int write, int atomic, unsigned long *gpa, int *pageshift)</span>
<span class="quote">&gt; +		    int write, int atomic, unsigned long *gpa, int *pageshift,</span>
<span class="quote">&gt; +		    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = gts-&gt;ts_mm;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt; @@ -283,7 +286,8 @@ static int gru_vtop(struct gru_thread_state *gts, unsigned long vaddr,</span>
<span class="quote">&gt;  	if (ret) {</span>
<span class="quote">&gt;  		if (atomic)</span>
<span class="quote">&gt;  			goto upm;</span>
<span class="quote">&gt; -		if (non_atomic_pte_lookup(vma, vaddr, write, &amp;paddr, &amp;ps))</span>
<span class="quote">&gt; +		if (non_atomic_pte_lookup(vma, vaddr, write, &amp;paddr,</span>
<span class="quote">&gt; +					  &amp;ps, mmrange))</span>
<span class="quote">&gt;  			goto inval;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if (is_gru_paddr(paddr))</span>
<span class="quote">&gt; @@ -324,7 +328,8 @@ static void gru_preload_tlb(struct gru_state *gru,</span>
<span class="quote">&gt;  			unsigned long fault_vaddr, int asid, int write,</span>
<span class="quote">&gt;  			unsigned char tlb_preload_count,</span>
<span class="quote">&gt;  			struct gru_tlb_fault_handle *tfh,</span>
<span class="quote">&gt; -			struct gru_control_block_extended *cbe)</span>
<span class="quote">&gt; +			struct gru_control_block_extended *cbe,</span>
<span class="quote">&gt; +			struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long vaddr = 0, gpa;</span>
<span class="quote">&gt;  	int ret, pageshift;</span>
<span class="quote">&gt; @@ -342,7 +347,7 @@ static void gru_preload_tlb(struct gru_state *gru,</span>
<span class="quote">&gt;  	vaddr = min(vaddr, fault_vaddr + tlb_preload_count * PAGE_SIZE);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	while (vaddr &gt; fault_vaddr) {</span>
<span class="quote">&gt; -		ret = gru_vtop(gts, vaddr, write, atomic, &amp;gpa, &amp;pageshift);</span>
<span class="quote">&gt; +		ret = gru_vtop(gts, vaddr, write, atomic, &amp;gpa, &amp;pageshift, mmrange);</span>
<span class="quote">&gt;  		if (ret || tfh_write_only(tfh, gpa, GAA_RAM, vaddr, asid, write,</span>
<span class="quote">&gt;  					  GRU_PAGESIZE(pageshift)))</span>
<span class="quote">&gt;  			return;</span>
<span class="quote">&gt; @@ -368,7 +373,8 @@ static void gru_preload_tlb(struct gru_state *gru,</span>
<span class="quote">&gt;  static int gru_try_dropin(struct gru_state *gru,</span>
<span class="quote">&gt;  			  struct gru_thread_state *gts,</span>
<span class="quote">&gt;  			  struct gru_tlb_fault_handle *tfh,</span>
<span class="quote">&gt; -			  struct gru_instruction_bits *cbk)</span>
<span class="quote">&gt; +			  struct gru_instruction_bits *cbk,</span>
<span class="quote">&gt; +			  struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct gru_control_block_extended *cbe = NULL;</span>
<span class="quote">&gt;  	unsigned char tlb_preload_count = gts-&gt;ts_tlb_preload_count;</span>
<span class="quote">&gt; @@ -423,7 +429,7 @@ static int gru_try_dropin(struct gru_state *gru,</span>
<span class="quote">&gt;  	if (atomic_read(&amp;gts-&gt;ts_gms-&gt;ms_range_active))</span>
<span class="quote">&gt;  		goto failactive;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	ret = gru_vtop(gts, vaddr, write, atomic, &amp;gpa, &amp;pageshift);</span>
<span class="quote">&gt; +	ret = gru_vtop(gts, vaddr, write, atomic, &amp;gpa, &amp;pageshift, mmrange);</span>
<span class="quote">&gt;  	if (ret == VTOP_INVALID)</span>
<span class="quote">&gt;  		goto failinval;</span>
<span class="quote">&gt;  	if (ret == VTOP_RETRY)</span>
<span class="quote">&gt; @@ -438,7 +444,8 @@ static int gru_try_dropin(struct gru_state *gru,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (unlikely(cbe) &amp;&amp; pageshift == PAGE_SHIFT) {</span>
<span class="quote">&gt; -		gru_preload_tlb(gru, gts, atomic, vaddr, asid, write, tlb_preload_count, tfh, cbe);</span>
<span class="quote">&gt; +		gru_preload_tlb(gru, gts, atomic, vaddr, asid, write,</span>
<span class="quote">&gt; +				tlb_preload_count, tfh, cbe, mmrange);</span>
<span class="quote">&gt;  		gru_flush_cache_cbe(cbe);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -587,10 +594,13 @@ static irqreturn_t gru_intr(int chiplet, int blade)</span>
<span class="quote">&gt;  		 * If it fails, retry the fault in user context.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		gts-&gt;ustats.fmm_tlbmiss++;</span>
<span class="quote">&gt; -		if (!gts-&gt;ts_force_cch_reload &amp;&amp;</span>
<span class="quote">&gt; -					down_read_trylock(&amp;gts-&gt;ts_mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt; -			gru_try_dropin(gru, gts, tfh, NULL);</span>
<span class="quote">&gt; -			up_read(&amp;gts-&gt;ts_mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +		if (!gts-&gt;ts_force_cch_reload) {</span>
<span class="quote">&gt; +			DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (down_read_trylock(&amp;gts-&gt;ts_mm-&gt;mmap_sem)) {</span>
<span class="quote">&gt; +				gru_try_dropin(gru, gts, tfh, NULL, &amp;mmrange);</span>
<span class="quote">&gt; +				up_read(&amp;gts-&gt;ts_mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt;  			tfh_user_polling_mode(tfh);</span>
<span class="quote">&gt;  			STAT(intr_mm_lock_failed);</span>
<span class="quote">&gt; @@ -625,7 +635,7 @@ irqreturn_t gru_intr_mblade(int irq, void *dev_id)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int gru_user_dropin(struct gru_thread_state *gts,</span>
<span class="quote">&gt;  			   struct gru_tlb_fault_handle *tfh,</span>
<span class="quote">&gt; -			   void *cb)</span>
<span class="quote">&gt; +			   void *cb, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct gru_mm_struct *gms = gts-&gt;ts_gms;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; @@ -635,7 +645,7 @@ static int gru_user_dropin(struct gru_thread_state *gts,</span>
<span class="quote">&gt;  		wait_event(gms-&gt;ms_wait_queue,</span>
<span class="quote">&gt;  			   atomic_read(&amp;gms-&gt;ms_range_active) == 0);</span>
<span class="quote">&gt;  		prefetchw(tfh);	/* Helps on hdw, required for emulator */</span>
<span class="quote">&gt; -		ret = gru_try_dropin(gts-&gt;ts_gru, gts, tfh, cb);</span>
<span class="quote">&gt; +		ret = gru_try_dropin(gts-&gt;ts_gru, gts, tfh, cb, mmrange);</span>
<span class="quote">&gt;  		if (ret &lt;= 0)</span>
<span class="quote">&gt;  			return ret;</span>
<span class="quote">&gt;  		STAT(call_os_wait_queue);</span>
<span class="quote">&gt; @@ -653,6 +663,7 @@ int gru_handle_user_call_os(unsigned long cb)</span>
<span class="quote">&gt;  	struct gru_thread_state *gts;</span>
<span class="quote">&gt;  	void *cbk;</span>
<span class="quote">&gt;  	int ucbnum, cbrnum, ret = -EINVAL;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	STAT(call_os);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -685,7 +696,7 @@ int gru_handle_user_call_os(unsigned long cb)</span>
<span class="quote">&gt;  		tfh = get_tfh_by_index(gts-&gt;ts_gru, cbrnum);</span>
<span class="quote">&gt;  		cbk = get_gseg_base_address_cb(gts-&gt;ts_gru-&gt;gs_gru_base_vaddr,</span>
<span class="quote">&gt;  				gts-&gt;ts_ctxnum, ucbnum);</span>
<span class="quote">&gt; -		ret = gru_user_dropin(gts, tfh, cbk);</span>
<span class="quote">&gt; +		ret = gru_user_dropin(gts, tfh, cbk, &amp;mmrange);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  exit:</span>
<span class="quote">&gt;  	gru_unlock_gts(gts);</span>
<span class="quote">&gt; diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; index e30e29ae4819..1b3b103da637 100644</span>
<span class="quote">&gt; --- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; +++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; @@ -345,13 +345,14 @@ static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,</span>
<span class="quote">&gt;  					  page);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt;  		unsigned int flags = 0;</span>
<span class="quote">&gt; +		DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (prot &amp; IOMMU_WRITE)</span>
<span class="quote">&gt;  			flags |= FOLL_WRITE;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  		ret = get_user_pages_remote(NULL, mm, vaddr, 1, flags, page,</span>
<span class="quote">&gt; -					    NULL, NULL);</span>
<span class="quote">&gt; +					    NULL, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/fs/aio.c b/fs/aio.c</span>
<span class="quote">&gt; index a062d75109cb..31774b75c372 100644</span>
<span class="quote">&gt; --- a/fs/aio.c</span>
<span class="quote">&gt; +++ b/fs/aio.c</span>
<span class="quote">&gt; @@ -457,6 +457,7 @@ static int aio_setup_ring(struct kioctx *ctx, unsigned int nr_events)</span>
<span class="quote">&gt;  	int nr_pages;</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt;  	struct file *file;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Compensate for the ring buffer&#39;s head/tail overlap entry */</span>
<span class="quote">&gt;  	nr_events += 2;	/* 1 is required, 2 for good luck */</span>
<span class="quote">&gt; @@ -519,7 +520,7 @@ static int aio_setup_ring(struct kioctx *ctx, unsigned int nr_events)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	ctx-&gt;mmap_base = do_mmap_pgoff(ctx-&gt;aio_ring_file, 0, ctx-&gt;mmap_size,</span>
<span class="quote">&gt;  				       PROT_READ | PROT_WRITE,</span>
<span class="quote">&gt; -				       MAP_SHARED, 0, &amp;unused, NULL);</span>
<span class="quote">&gt; +				       MAP_SHARED, 0, &amp;unused, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	if (IS_ERR((void *)ctx-&gt;mmap_base)) {</span>
<span class="quote">&gt;  		ctx-&gt;mmap_size = 0;</span>
<span class="quote">&gt; diff --git a/fs/binfmt_elf.c b/fs/binfmt_elf.c</span>
<span class="quote">&gt; index 2f492dfcabde..9aea808d55d7 100644</span>
<span class="quote">&gt; --- a/fs/binfmt_elf.c</span>
<span class="quote">&gt; +++ b/fs/binfmt_elf.c</span>
<span class="quote">&gt; @@ -180,6 +180,7 @@ create_elf_tables(struct linux_binprm *bprm, struct elfhdr *exec,</span>
<span class="quote">&gt;  	int ei_index = 0;</span>
<span class="quote">&gt;  	const struct cred *cred = current_cred();</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * In some cases (e.g. Hyper-Threading), we want to avoid L1</span>
<span class="quote">&gt; @@ -300,7 +301,7 @@ create_elf_tables(struct linux_binprm *bprm, struct elfhdr *exec,</span>
<span class="quote">&gt;  	 * Grow the stack manually; some architectures have a limit on how</span>
<span class="quote">&gt;  	 * far ahead a user-space access may be in order to grow the stack.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	vma = find_extend_vma(current-&gt;mm, bprm-&gt;p);</span>
<span class="quote">&gt; +	vma = find_extend_vma(current-&gt;mm, bprm-&gt;p, &amp;mmrange);</span>
<span class="quote">&gt;  	if (!vma)</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="quote">&gt; index e7b69e14649f..e46752874b47 100644</span>
<span class="quote">&gt; --- a/fs/exec.c</span>
<span class="quote">&gt; +++ b/fs/exec.c</span>
<span class="quote">&gt; @@ -197,6 +197,11 @@ static struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  	unsigned int gup_flags = FOLL_FORCE;</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * No concurrency for the bprm-&gt;mm yet -- this is exec path;</span>
<span class="quote">&gt; +	 * but gup needs an mmrange.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_STACK_GROWSUP</span>
<span class="quote">&gt;  	if (write) {</span>
<span class="quote">&gt; @@ -214,7 +219,7 @@ static struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,</span>
<span class="quote">&gt;  	 * doing the exec and bprm-&gt;mm is the new process&#39;s mm.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	ret = get_user_pages_remote(current, bprm-&gt;mm, pos, 1, gup_flags,</span>
<span class="quote">&gt; -			&amp;page, NULL, NULL);</span>
<span class="quote">&gt; +				    &amp;page, NULL, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  	if (ret &lt;= 0)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -615,7 +620,8 @@ EXPORT_SYMBOL(copy_strings_kernel);</span>
<span class="quote">&gt;   * 4) Free up any cleared pgd range.</span>
<span class="quote">&gt;   * 5) Shrink the vma to cover only the new range.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)</span>
<span class="quote">&gt; +static int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift,</span>
<span class="quote">&gt; +			   struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	unsigned long old_start = vma-&gt;vm_start;</span>
<span class="quote">&gt; @@ -637,7 +643,8 @@ static int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * cover the whole range: [new_start, old_end)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (vma_adjust(vma, new_start, old_end, vma-&gt;vm_pgoff, NULL))</span>
<span class="quote">&gt; +	if (vma_adjust(vma, new_start, old_end, vma-&gt;vm_pgoff, NULL,</span>
<span class="quote">&gt; +		    mmrange))</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -671,7 +678,7 @@ static int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Shrink the vma to just the new range.  Always succeeds.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	vma_adjust(vma, new_start, new_end, vma-&gt;vm_pgoff, NULL);</span>
<span class="quote">&gt; +	vma_adjust(vma, new_start, new_end, vma-&gt;vm_pgoff, NULL, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -694,6 +701,7 @@ int setup_arg_pages(struct linux_binprm *bprm,</span>
<span class="quote">&gt;  	unsigned long stack_size;</span>
<span class="quote">&gt;  	unsigned long stack_expand;</span>
<span class="quote">&gt;  	unsigned long rlim_stack;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_STACK_GROWSUP</span>
<span class="quote">&gt;  	/* Limit stack size */</span>
<span class="quote">&gt; @@ -749,14 +757,14 @@ int setup_arg_pages(struct linux_binprm *bprm,</span>
<span class="quote">&gt;  	vm_flags |= VM_STACK_INCOMPLETE_SETUP;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	ret = mprotect_fixup(vma, &amp;prev, vma-&gt;vm_start, vma-&gt;vm_end,</span>
<span class="quote">&gt; -			vm_flags);</span>
<span class="quote">&gt; +			     vm_flags, &amp;mmrange);</span>
<span class="quote">&gt;  	if (ret)</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt;  	BUG_ON(prev != vma);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Move stack pages down in memory. */</span>
<span class="quote">&gt;  	if (stack_shift) {</span>
<span class="quote">&gt; -		ret = shift_arg_pages(vma, stack_shift);</span>
<span class="quote">&gt; +		ret = shift_arg_pages(vma, stack_shift, &amp;mmrange);</span>
<span class="quote">&gt;  		if (ret)</span>
<span class="quote">&gt;  			goto out_unlock;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/fs/proc/internal.h b/fs/proc/internal.h</span>
<span class="quote">&gt; index d697c8ab0a14..791f9f93643c 100644</span>
<span class="quote">&gt; --- a/fs/proc/internal.h</span>
<span class="quote">&gt; +++ b/fs/proc/internal.h</span>
<span class="quote">&gt; @@ -16,6 +16,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/binfmts.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/sched/coredump.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/sched/task.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/range_lock.h&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  struct ctl_table_header;</span>
<span class="quote">&gt;  struct mempolicy;</span>
<span class="quote">&gt; @@ -263,6 +264,8 @@ struct proc_maps_private {</span>
<span class="quote">&gt;  #ifdef CONFIG_NUMA</span>
<span class="quote">&gt;  	struct mempolicy *task_mempolicy;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +	/* mmap_sem is held across all stages of seqfile */</span>
<span class="quote">&gt; +	struct range_lock mmrange;</span>
<span class="quote">&gt;  } __randomize_layout;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  struct mm_struct *proc_mem_open(struct inode *inode, unsigned int mode);</span>
<span class="quote">&gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; index b66fc8de7d34..7c0a79a937b5 100644</span>
<span class="quote">&gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; @@ -174,6 +174,7 @@ static void *m_start(struct seq_file *m, loff_t *ppos)</span>
<span class="quote">&gt;  	if (!mm || !mmget_not_zero(mm))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +	range_lock_init_full(&amp;priv-&gt;mmrange);</span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	hold_task_mempolicy(priv);</span>
<span class="quote">&gt;  	priv-&gt;tail_vma = get_gate_vma(mm);</span>
<span class="quote">&gt; @@ -514,7 +515,7 @@ static void smaps_account(struct mem_size_stats *mss, struct page *page,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_SHMEM</span>
<span class="quote">&gt;  static int smaps_pte_hole(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -		struct mm_walk *walk)</span>
<span class="quote">&gt; +			  struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mem_size_stats *mss = walk-&gt;private;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -605,7 +606,7 @@ static void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -			   struct mm_walk *walk)</span>
<span class="quote">&gt; +			   struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt;  	pte_t *pte;</span>
<span class="quote">&gt; @@ -797,7 +798,7 @@ static int show_smap(struct seq_file *m, void *v, int is_pid)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* mmap_sem is held in m_start */</span>
<span class="quote">&gt; -	walk_page_vma(vma, &amp;smaps_walk);</span>
<span class="quote">&gt; +	walk_page_vma(vma, &amp;smaps_walk, &amp;priv-&gt;mmrange);</span>
<span class="quote">&gt;  	if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="quote">&gt;  		mss-&gt;pss_locked += mss-&gt;pss;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1012,7 +1013,8 @@ static inline void clear_soft_dirty_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; -				unsigned long end, struct mm_walk *walk)</span>
<span class="quote">&gt; +				unsigned long end, struct mm_walk *walk,</span>
<span class="quote">&gt; +				struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct clear_refs_private *cp = walk-&gt;private;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt; @@ -1103,6 +1105,7 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
<span class="quote">&gt;  	struct mmu_gather tlb;</span>
<span class="quote">&gt;  	int itype;</span>
<span class="quote">&gt;  	int rv;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	memset(buffer, 0, sizeof(buffer));</span>
<span class="quote">&gt;  	if (count &gt; sizeof(buffer) - 1)</span>
<span class="quote">&gt; @@ -1166,7 +1169,8 @@ static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  			mmu_notifier_invalidate_range_start(mm, 0, -1);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		walk_page_range(0, mm-&gt;highest_vm_end, &amp;clear_refs_walk);</span>
<span class="quote">&gt; +		walk_page_range(0, mm-&gt;highest_vm_end, &amp;clear_refs_walk,</span>
<span class="quote">&gt; +				&amp;mmrange);</span>
<span class="quote">&gt;  		if (type == CLEAR_REFS_SOFT_DIRTY)</span>
<span class="quote">&gt;  			mmu_notifier_invalidate_range_end(mm, 0, -1);</span>
<span class="quote">&gt;  		tlb_finish_mmu(&amp;tlb, 0, -1);</span>
<span class="quote">&gt; @@ -1223,7 +1227,7 @@ static int add_to_pagemap(unsigned long addr, pagemap_entry_t *pme,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int pagemap_pte_hole(unsigned long start, unsigned long end,</span>
<span class="quote">&gt; -				struct mm_walk *walk)</span>
<span class="quote">&gt; +			    struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct pagemapread *pm = walk-&gt;private;</span>
<span class="quote">&gt;  	unsigned long addr = start;</span>
<span class="quote">&gt; @@ -1301,7 +1305,7 @@ static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -			     struct mm_walk *walk)</span>
<span class="quote">&gt; +			     struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt;  	struct pagemapread *pm = walk-&gt;private;</span>
<span class="quote">&gt; @@ -1467,6 +1471,8 @@ static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
<span class="quote">&gt;  	unsigned long start_vaddr;</span>
<span class="quote">&gt;  	unsigned long end_vaddr;</span>
<span class="quote">&gt;  	int ret = 0, copied = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(tmprange);</span>
<span class="quote">&gt; +	struct range_lock *mmrange = &amp;tmprange;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (!mm || !mmget_not_zero(mm))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; @@ -1523,7 +1529,8 @@ static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
<span class="quote">&gt;  		if (end &lt; start_vaddr || end &gt; end_vaddr)</span>
<span class="quote">&gt;  			end = end_vaddr;</span>
<span class="quote">&gt;  		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; -		ret = walk_page_range(start_vaddr, end, &amp;pagemap_walk);</span>
<span class="quote">&gt; +		ret = walk_page_range(start_vaddr, end, &amp;pagemap_walk,</span>
<span class="quote">&gt; +				      mmrange);</span>
<span class="quote">&gt;  		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  		start_vaddr = end;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1671,7 +1678,8 @@ static struct page *can_gather_numa_stats_pmd(pmd_t pmd,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int gather_pte_stats(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; -		unsigned long end, struct mm_walk *walk)</span>
<span class="quote">&gt; +			    unsigned long end, struct mm_walk *walk,</span>
<span class="quote">&gt; +			    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct numa_maps *md = walk-&gt;private;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt; @@ -1740,6 +1748,7 @@ static int gather_hugetlb_stats(pte_t *pte, unsigned long hmask,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int show_numa_map(struct seq_file *m, void *v, int is_pid)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	struct proc_maps_private *priv = m-&gt;private;</span>
<span class="quote">&gt;  	struct numa_maps_private *numa_priv = m-&gt;private;</span>
<span class="quote">&gt;  	struct proc_maps_private *proc_priv = &amp;numa_priv-&gt;proc_maps;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = v;</span>
<span class="quote">&gt; @@ -1785,7 +1794,7 @@ static int show_numa_map(struct seq_file *m, void *v, int is_pid)</span>
<span class="quote">&gt;  		seq_puts(m, &quot; huge&quot;);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* mmap_sem is held by m_start */</span>
<span class="quote">&gt; -	walk_page_vma(vma, &amp;walk);</span>
<span class="quote">&gt; +	walk_page_vma(vma, &amp;walk, &amp;priv-&gt;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (!md-&gt;pages)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; diff --git a/fs/proc/vmcore.c b/fs/proc/vmcore.c</span>
<span class="quote">&gt; index a45f0af22a60..3768955c10bc 100644</span>
<span class="quote">&gt; --- a/fs/proc/vmcore.c</span>
<span class="quote">&gt; +++ b/fs/proc/vmcore.c</span>
<span class="quote">&gt; @@ -350,6 +350,11 @@ static int remap_oldmem_pfn_checked(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	unsigned long pos_start, pos_end, pos;</span>
<span class="quote">&gt;  	unsigned long zeropage_pfn = my_zero_pfn(0);</span>
<span class="quote">&gt;  	size_t len = 0;</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * No concurrency for the bprm-&gt;mm yet -- this is a vmcore path,</span>
<span class="quote">&gt; +	 * but do_munmap() needs an mmrange.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	pos_start = pfn;</span>
<span class="quote">&gt;  	pos_end = pfn + (size &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt; @@ -388,7 +393,7 @@ static int remap_oldmem_pfn_checked(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  fail:</span>
<span class="quote">&gt; -	do_munmap(vma-&gt;vm_mm, from, len, NULL);</span>
<span class="quote">&gt; +	do_munmap(vma-&gt;vm_mm, from, len, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  	return -EAGAIN;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -411,6 +416,11 @@ static int mmap_vmcore(struct file *file, struct vm_area_struct *vma)</span>
<span class="quote">&gt;  	size_t size = vma-&gt;vm_end - vma-&gt;vm_start;</span>
<span class="quote">&gt;  	u64 start, end, len, tsz;</span>
<span class="quote">&gt;  	struct vmcore *m;</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * No concurrency for the bprm-&gt;mm yet -- this is a vmcore path,</span>
<span class="quote">&gt; +	 * but do_munmap() needs an mmrange.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	start = (u64)vma-&gt;vm_pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	end = start + size;</span>
<span class="quote">&gt; @@ -481,7 +491,7 @@ static int mmap_vmcore(struct file *file, struct vm_area_struct *vma)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  fail:</span>
<span class="quote">&gt; -	do_munmap(vma-&gt;vm_mm, vma-&gt;vm_start, len, NULL);</span>
<span class="quote">&gt; +	do_munmap(vma-&gt;vm_mm, vma-&gt;vm_start, len, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  	return -EAGAIN;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c</span>
<span class="quote">&gt; index 87a13a7c8270..e3089865fd52 100644</span>
<span class="quote">&gt; --- a/fs/userfaultfd.c</span>
<span class="quote">&gt; +++ b/fs/userfaultfd.c</span>
<span class="quote">&gt; @@ -851,6 +851,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)</span>
<span class="quote">&gt;  	/* len == 0 means wake all */</span>
<span class="quote">&gt;  	struct userfaultfd_wake_range range = { .len = 0, };</span>
<span class="quote">&gt;  	unsigned long new_flags;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	WRITE_ONCE(ctx-&gt;released, true);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -880,7 +881,7 @@ static int userfaultfd_release(struct inode *inode, struct file *file)</span>
<span class="quote">&gt;  				 new_flags, vma-&gt;anon_vma,</span>
<span class="quote">&gt;  				 vma-&gt;vm_file, vma-&gt;vm_pgoff,</span>
<span class="quote">&gt;  				 vma_policy(vma),</span>
<span class="quote">&gt; -				 NULL_VM_UFFD_CTX);</span>
<span class="quote">&gt; +				 NULL_VM_UFFD_CTX, &amp;mmrange);</span>
<span class="quote">&gt;  		if (prev)</span>
<span class="quote">&gt;  			vma = prev;</span>
<span class="quote">&gt;  		else</span>
<span class="quote">&gt; @@ -1276,6 +1277,7 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
<span class="quote">&gt;  	bool found;</span>
<span class="quote">&gt;  	bool basic_ioctls;</span>
<span class="quote">&gt;  	unsigned long start, end, vma_end;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	user_uffdio_register = (struct uffdio_register __user *) arg;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1413,18 +1415,19 @@ static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
<span class="quote">&gt;  		prev = vma_merge(mm, prev, start, vma_end, new_flags,</span>
<span class="quote">&gt;  				 vma-&gt;anon_vma, vma-&gt;vm_file, vma-&gt;vm_pgoff,</span>
<span class="quote">&gt;  				 vma_policy(vma),</span>
<span class="quote">&gt; -				 ((struct vm_userfaultfd_ctx){ ctx }));</span>
<span class="quote">&gt; +				 ((struct vm_userfaultfd_ctx){ ctx }),</span>
<span class="quote">&gt; +				 &amp;mmrange);</span>
<span class="quote">&gt;  		if (prev) {</span>
<span class="quote">&gt;  			vma = prev;</span>
<span class="quote">&gt;  			goto next;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (vma-&gt;vm_start &lt; start) {</span>
<span class="quote">&gt; -			ret = split_vma(mm, vma, start, 1);</span>
<span class="quote">&gt; +			ret = split_vma(mm, vma, start, 1, &amp;mmrange);</span>
<span class="quote">&gt;  			if (ret)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (vma-&gt;vm_end &gt; end) {</span>
<span class="quote">&gt; -			ret = split_vma(mm, vma, end, 0);</span>
<span class="quote">&gt; +			ret = split_vma(mm, vma, end, 0, &amp;mmrange);</span>
<span class="quote">&gt;  			if (ret)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -1471,6 +1474,7 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
<span class="quote">&gt;  	bool found;</span>
<span class="quote">&gt;  	unsigned long start, end, vma_end;</span>
<span class="quote">&gt;  	const void __user *buf = (void __user *)arg;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	ret = -EFAULT;</span>
<span class="quote">&gt;  	if (copy_from_user(&amp;uffdio_unregister, buf, sizeof(uffdio_unregister)))</span>
<span class="quote">&gt; @@ -1571,18 +1575,18 @@ static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
<span class="quote">&gt;  		prev = vma_merge(mm, prev, start, vma_end, new_flags,</span>
<span class="quote">&gt;  				 vma-&gt;anon_vma, vma-&gt;vm_file, vma-&gt;vm_pgoff,</span>
<span class="quote">&gt;  				 vma_policy(vma),</span>
<span class="quote">&gt; -				 NULL_VM_UFFD_CTX);</span>
<span class="quote">&gt; +				 NULL_VM_UFFD_CTX, &amp;mmrange);</span>
<span class="quote">&gt;  		if (prev) {</span>
<span class="quote">&gt;  			vma = prev;</span>
<span class="quote">&gt;  			goto next;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (vma-&gt;vm_start &lt; start) {</span>
<span class="quote">&gt; -			ret = split_vma(mm, vma, start, 1);</span>
<span class="quote">&gt; +			ret = split_vma(mm, vma, start, 1, &amp;mmrange);</span>
<span class="quote">&gt;  			if (ret)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (vma-&gt;vm_end &gt; end) {</span>
<span class="quote">&gt; -			ret = split_vma(mm, vma, end, 0);</span>
<span class="quote">&gt; +			ret = split_vma(mm, vma, end, 0, &amp;mmrange);</span>
<span class="quote">&gt;  			if (ret)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; diff --git a/include/asm-generic/mm_hooks.h b/include/asm-generic/mm_hooks.h</span>
<span class="quote">&gt; index 8ac4e68a12f0..2115deceded1 100644</span>
<span class="quote">&gt; --- a/include/asm-generic/mm_hooks.h</span>
<span class="quote">&gt; +++ b/include/asm-generic/mm_hooks.h</span>
<span class="quote">&gt; @@ -19,7 +19,8 @@ static inline void arch_exit_mmap(struct mm_struct *mm)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static inline void arch_unmap(struct mm_struct *mm,</span>
<span class="quote">&gt;  			struct vm_area_struct *vma,</span>
<span class="quote">&gt; -			unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +			unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +			struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
<span class="quote">&gt; index 325017ad9311..da004594d831 100644</span>
<span class="quote">&gt; --- a/include/linux/hmm.h</span>
<span class="quote">&gt; +++ b/include/linux/hmm.h</span>
<span class="quote">&gt; @@ -295,7 +295,7 @@ int hmm_vma_get_pfns(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		     struct hmm_range *range,</span>
<span class="quote">&gt;  		     unsigned long start,</span>
<span class="quote">&gt;  		     unsigned long end,</span>
<span class="quote">&gt; -		     hmm_pfn_t *pfns);</span>
<span class="quote">&gt; +		     hmm_pfn_t *pfns, struct range_lock *mmrange);</span>
<span class="quote">&gt;  bool hmm_vma_range_done(struct vm_area_struct *vma, struct hmm_range *range);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -323,7 +323,7 @@ int hmm_vma_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		  unsigned long end,</span>
<span class="quote">&gt;  		  hmm_pfn_t *pfns,</span>
<span class="quote">&gt;  		  bool write,</span>
<span class="quote">&gt; -		  bool block);</span>
<span class="quote">&gt; +		  bool block, struct range_lock *mmrange);</span>
<span class="quote">&gt;  #endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/ksm.h b/include/linux/ksm.h</span>
<span class="quote">&gt; index 44368b19b27e..19667b75f73c 100644</span>
<span class="quote">&gt; --- a/include/linux/ksm.h</span>
<span class="quote">&gt; +++ b/include/linux/ksm.h</span>
<span class="quote">&gt; @@ -20,7 +20,8 @@ struct mem_cgroup;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_KSM</span>
<span class="quote">&gt;  int ksm_madvise(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt; -		unsigned long end, int advice, unsigned long *vm_flags);</span>
<span class="quote">&gt; +		unsigned long end, int advice, unsigned long *vm_flags,</span>
<span class="quote">&gt; +		struct range_lock *mmrange);</span>
<span class="quote">&gt;  int __ksm_enter(struct mm_struct *mm);</span>
<span class="quote">&gt;  void __ksm_exit(struct mm_struct *mm);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -78,7 +79,8 @@ static inline void ksm_exit(struct mm_struct *mm)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_MMU</span>
<span class="quote">&gt;  static inline int ksm_madvise(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt; -		unsigned long end, int advice, unsigned long *vm_flags)</span>
<span class="quote">&gt; +		      unsigned long end, int advice, unsigned long *vm_flags,</span>
<span class="quote">&gt; +		      struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="quote">&gt; index 0c6fe904bc97..fa08e348a295 100644</span>
<span class="quote">&gt; --- a/include/linux/migrate.h</span>
<span class="quote">&gt; +++ b/include/linux/migrate.h</span>
<span class="quote">&gt; @@ -272,7 +272,7 @@ int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="quote">&gt;  		unsigned long end,</span>
<span class="quote">&gt;  		unsigned long *src,</span>
<span class="quote">&gt;  		unsigned long *dst,</span>
<span class="quote">&gt; -		void *private);</span>
<span class="quote">&gt; +		void *private, struct range_lock *mmrange);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="quote">&gt;  			      struct vm_area_struct *vma,</span>
<span class="quote">&gt; @@ -280,7 +280,7 @@ static inline int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="quote">&gt;  			      unsigned long end,</span>
<span class="quote">&gt;  			      unsigned long *src,</span>
<span class="quote">&gt;  			      unsigned long *dst,</span>
<span class="quote">&gt; -			      void *private)</span>
<span class="quote">&gt; +			      void *private, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return -EINVAL;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="quote">&gt; index bcf2509d448d..fc4e7fdc3e76 100644</span>
<span class="quote">&gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt; @@ -1295,11 +1295,12 @@ struct mm_walk {</span>
<span class="quote">&gt;  	int (*pud_entry)(pud_t *pud, unsigned long addr,</span>
<span class="quote">&gt;  			 unsigned long next, struct mm_walk *walk);</span>
<span class="quote">&gt;  	int (*pmd_entry)(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; -			 unsigned long next, struct mm_walk *walk);</span>
<span class="quote">&gt; +			 unsigned long next, struct mm_walk *walk,</span>
<span class="quote">&gt; +			 struct range_lock *mmrange);</span>
<span class="quote">&gt;  	int (*pte_entry)(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt;  			 unsigned long next, struct mm_walk *walk);</span>
<span class="quote">&gt;  	int (*pte_hole)(unsigned long addr, unsigned long next,</span>
<span class="quote">&gt; -			struct mm_walk *walk);</span>
<span class="quote">&gt; +			struct mm_walk *walk, struct range_lock *mmrange);</span>
<span class="quote">&gt;  	int (*hugetlb_entry)(pte_t *pte, unsigned long hmask,</span>
<span class="quote">&gt;  			     unsigned long addr, unsigned long next,</span>
<span class="quote">&gt;  			     struct mm_walk *walk);</span>
<span class="quote">&gt; @@ -1311,8 +1312,9 @@ struct mm_walk {</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  int walk_page_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -		struct mm_walk *walk);</span>
<span class="quote">&gt; -int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk);</span>
<span class="quote">&gt; +		    struct mm_walk *walk, struct range_lock *mmrange);</span>
<span class="quote">&gt; +int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk,</span>
<span class="quote">&gt; +		  struct range_lock *mmrange);</span>
<span class="quote">&gt;  void free_pgd_range(struct mmu_gather *tlb, unsigned long addr,</span>
<span class="quote">&gt;  		unsigned long end, unsigned long floor, unsigned long ceiling);</span>
<span class="quote">&gt;  int copy_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
<span class="quote">&gt; @@ -1337,17 +1339,18 @@ int invalidate_inode_page(struct page *page);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_MMU</span>
<span class="quote">&gt;  extern int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; -		unsigned int flags);</span>
<span class="quote">&gt; +			   unsigned int flags, struct range_lock *mmrange);</span>
<span class="quote">&gt;  extern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  			    unsigned long address, unsigned int fault_flags,</span>
<span class="quote">&gt; -			    bool *unlocked);</span>
<span class="quote">&gt; +			    bool *unlocked, struct range_lock *mmrange);</span>
<span class="quote">&gt;  void unmap_mapping_pages(struct address_space *mapping,</span>
<span class="quote">&gt;  		pgoff_t start, pgoff_t nr, bool even_cows);</span>
<span class="quote">&gt;  void unmap_mapping_range(struct address_space *mapping,</span>
<span class="quote">&gt;  		loff_t const holebegin, loff_t const holelen, int even_cows);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline int handle_mm_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt; -		unsigned long address, unsigned int flags)</span>
<span class="quote">&gt; +		unsigned long address, unsigned int flags,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	/* should never happen if there&#39;s no MMU */</span>
<span class="quote">&gt;  	BUG();</span>
<span class="quote">&gt; @@ -1355,7 +1358,8 @@ static inline int handle_mm_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  static inline int fixup_user_fault(struct task_struct *tsk,</span>
<span class="quote">&gt;  		struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt; -		unsigned int fault_flags, bool *unlocked)</span>
<span class="quote">&gt; +		unsigned int fault_flags, bool *unlocked,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	/* should never happen if there&#39;s no MMU */</span>
<span class="quote">&gt;  	BUG();</span>
<span class="quote">&gt; @@ -1383,24 +1387,28 @@ extern int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  long get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  			    unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  			    unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; -			    struct vm_area_struct **vmas, int *locked);</span>
<span class="quote">&gt; +			    struct vm_area_struct **vmas, int *locked,</span>
<span class="quote">&gt; +			    struct range_lock *mmrange);</span>
<span class="quote">&gt;  long get_user_pages(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt; -			    unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; -			    struct vm_area_struct **vmas);</span>
<span class="quote">&gt; +		    unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; +		    struct vm_area_struct **vmas, struct range_lock *mmrange);</span>
<span class="quote">&gt;  long get_user_pages_locked(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt; -		    unsigned int gup_flags, struct page **pages, int *locked);</span>
<span class="quote">&gt; +			   unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; +			   int *locked, struct range_lock *mmrange);</span>
<span class="quote">&gt;  long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  		    struct page **pages, unsigned int gup_flags);</span>
<span class="quote">&gt;  #ifdef CONFIG_FS_DAX</span>
<span class="quote">&gt;  long get_user_pages_longterm(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt; -			    unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; -			    struct vm_area_struct **vmas);</span>
<span class="quote">&gt; +			     unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; +			     struct vm_area_struct **vmas,</span>
<span class="quote">&gt; +			     struct range_lock *mmrange);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline long get_user_pages_longterm(unsigned long start,</span>
<span class="quote">&gt;  		unsigned long nr_pages, unsigned int gup_flags,</span>
<span class="quote">&gt; -		struct page **pages, struct vm_area_struct **vmas)</span>
<span class="quote">&gt; +		struct page **pages, struct vm_area_struct **vmas,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return get_user_pages(start, nr_pages, gup_flags, pages, vmas);</span>
<span class="quote">&gt; +	return get_user_pages(start, nr_pages, gup_flags, pages, vmas, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif /* CONFIG_FS_DAX */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1505,7 +1513,8 @@ extern unsigned long change_protection(struct vm_area_struct *vma, unsigned long</span>
<span class="quote">&gt;  			      int dirty_accountable, int prot_numa);</span>
<span class="quote">&gt;  extern int mprotect_fixup(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			  struct vm_area_struct **pprev, unsigned long start,</span>
<span class="quote">&gt; -			  unsigned long end, unsigned long newflags);</span>
<span class="quote">&gt; +			  unsigned long end, unsigned long newflags,</span>
<span class="quote">&gt; +			  struct range_lock *mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * doesn&#39;t attempt to fault and will return short.</span>
<span class="quote">&gt; @@ -2149,28 +2158,30 @@ void anon_vma_interval_tree_verify(struct anon_vma_chain *node);</span>
<span class="quote">&gt;  extern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);</span>
<span class="quote">&gt;  extern int __vma_adjust(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt;  	unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,</span>
<span class="quote">&gt; -	struct vm_area_struct *expand);</span>
<span class="quote">&gt; +	struct vm_area_struct *expand, struct range_lock *mmrange);</span>
<span class="quote">&gt;  static inline int vma_adjust(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt; -	unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert)</span>
<span class="quote">&gt; +	unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,</span>
<span class="quote">&gt; +	struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return __vma_adjust(vma, start, end, pgoff, insert, NULL);</span>
<span class="quote">&gt; +	return __vma_adjust(vma, start, end, pgoff, insert, NULL, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  extern struct vm_area_struct *vma_merge(struct mm_struct *,</span>
<span class="quote">&gt;  	struct vm_area_struct *prev, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  	unsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,</span>
<span class="quote">&gt; -	struct mempolicy *, struct vm_userfaultfd_ctx);</span>
<span class="quote">&gt; +	struct mempolicy *, struct vm_userfaultfd_ctx,</span>
<span class="quote">&gt; +	struct range_lock *mmrange);</span>
<span class="quote">&gt;  extern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);</span>
<span class="quote">&gt;  extern int __split_vma(struct mm_struct *, struct vm_area_struct *,</span>
<span class="quote">&gt; -	unsigned long addr, int new_below);</span>
<span class="quote">&gt; +	unsigned long addr, int new_below, struct range_lock *mmrange);</span>
<span class="quote">&gt;  extern int split_vma(struct mm_struct *, struct vm_area_struct *,</span>
<span class="quote">&gt; -	unsigned long addr, int new_below);</span>
<span class="quote">&gt; +	unsigned long addr, int new_below, struct range_lock *mmrange);</span>
<span class="quote">&gt;  extern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);</span>
<span class="quote">&gt;  extern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,</span>
<span class="quote">&gt;  	struct rb_node **, struct rb_node *);</span>
<span class="quote">&gt;  extern void unlink_file_vma(struct vm_area_struct *);</span>
<span class="quote">&gt;  extern struct vm_area_struct *copy_vma(struct vm_area_struct **,</span>
<span class="quote">&gt;  	unsigned long addr, unsigned long len, pgoff_t pgoff,</span>
<span class="quote">&gt; -	bool *need_rmap_locks);</span>
<span class="quote">&gt; +	bool *need_rmap_locks, struct range_lock *mmrange);</span>
<span class="quote">&gt;  extern void exit_mmap(struct mm_struct *);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static inline int check_data_rlimit(unsigned long rlim,</span>
<span class="quote">&gt; @@ -2212,21 +2223,22 @@ extern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned lo</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  extern unsigned long mmap_region(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;  	unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,</span>
<span class="quote">&gt; -	struct list_head *uf);</span>
<span class="quote">&gt; +	struct list_head *uf, struct range_lock *mmrange);</span>
<span class="quote">&gt;  extern unsigned long do_mmap(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;  	unsigned long len, unsigned long prot, unsigned long flags,</span>
<span class="quote">&gt;  	vm_flags_t vm_flags, unsigned long pgoff, unsigned long *populate,</span>
<span class="quote">&gt; -	struct list_head *uf);</span>
<span class="quote">&gt; +	struct list_head *uf, struct range_lock *mmrange);</span>
<span class="quote">&gt;  extern int do_munmap(struct mm_struct *, unsigned long, size_t,</span>
<span class="quote">&gt; -		     struct list_head *uf);</span>
<span class="quote">&gt; +		     struct list_head *uf, struct range_lock *mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static inline unsigned long</span>
<span class="quote">&gt;  do_mmap_pgoff(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;  	unsigned long len, unsigned long prot, unsigned long flags,</span>
<span class="quote">&gt;  	unsigned long pgoff, unsigned long *populate,</span>
<span class="quote">&gt; -	struct list_head *uf)</span>
<span class="quote">&gt; +	struct list_head *uf, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return do_mmap(file, addr, len, prot, flags, 0, pgoff, populate, uf);</span>
<span class="quote">&gt; +	return do_mmap(file, addr, len, prot, flags, 0, pgoff, populate,</span>
<span class="quote">&gt; +		       uf, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_MMU</span>
<span class="quote">&gt; @@ -2405,7 +2417,8 @@ unsigned long change_prot_numa(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			unsigned long start, unsigned long end);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -struct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);</span>
<span class="quote">&gt; +struct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr,</span>
<span class="quote">&gt; +				       struct range_lock *);</span>
<span class="quote">&gt;  int remap_pfn_range(struct vm_area_struct *, unsigned long addr,</span>
<span class="quote">&gt;  			unsigned long pfn, unsigned long size, pgprot_t);</span>
<span class="quote">&gt;  int vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);</span>
<span class="quote">&gt; diff --git a/include/linux/uprobes.h b/include/linux/uprobes.h</span>
<span class="quote">&gt; index 0a294e950df8..79eb735e7c95 100644</span>
<span class="quote">&gt; --- a/include/linux/uprobes.h</span>
<span class="quote">&gt; +++ b/include/linux/uprobes.h</span>
<span class="quote">&gt; @@ -34,6 +34,7 @@ struct mm_struct;</span>
<span class="quote">&gt;  struct inode;</span>
<span class="quote">&gt;  struct notifier_block;</span>
<span class="quote">&gt;  struct page;</span>
<span class="quote">&gt; +struct range_lock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #define UPROBE_HANDLER_REMOVE		1</span>
<span class="quote">&gt;  #define UPROBE_HANDLER_MASK		1</span>
<span class="quote">&gt; @@ -115,17 +116,20 @@ struct uprobes_state {</span>
<span class="quote">&gt;  	struct xol_area		*xol_area;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -extern int set_swbp(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);</span>
<span class="quote">&gt; -extern int set_orig_insn(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);</span>
<span class="quote">&gt; +extern int set_swbp(struct arch_uprobe *aup, struct mm_struct *mm,</span>
<span class="quote">&gt; +		    unsigned long vaddr, struct range_lock *mmrange);</span>
<span class="quote">&gt; +extern int set_orig_insn(struct arch_uprobe *aup, struct mm_struct *mm,</span>
<span class="quote">&gt; +			 unsigned long vaddr, struct range_lock *mmrange);</span>
<span class="quote">&gt;  extern bool is_swbp_insn(uprobe_opcode_t *insn);</span>
<span class="quote">&gt;  extern bool is_trap_insn(uprobe_opcode_t *insn);</span>
<span class="quote">&gt;  extern unsigned long uprobe_get_swbp_addr(struct pt_regs *regs);</span>
<span class="quote">&gt;  extern unsigned long uprobe_get_trap_addr(struct pt_regs *regs);</span>
<span class="quote">&gt; -extern int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr, uprobe_opcode_t);</span>
<span class="quote">&gt; +extern int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,</span>
<span class="quote">&gt; +			       uprobe_opcode_t, struct range_lock *mmrange);</span>
<span class="quote">&gt;  extern int uprobe_register(struct inode *inode, loff_t offset, struct uprobe_consumer *uc);</span>
<span class="quote">&gt;  extern int uprobe_apply(struct inode *inode, loff_t offset, struct uprobe_consumer *uc, bool);</span>
<span class="quote">&gt;  extern void uprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc);</span>
<span class="quote">&gt; -extern int uprobe_mmap(struct vm_area_struct *vma);</span>
<span class="quote">&gt; +extern int uprobe_mmap(struct vm_area_struct *vma, struct range_lock *mmrange);;</span>
<span class="quote">&gt;  extern void uprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned long end);</span>
<span class="quote">&gt;  extern void uprobe_start_dup_mmap(void);</span>
<span class="quote">&gt;  extern void uprobe_end_dup_mmap(void);</span>
<span class="quote">&gt; @@ -169,7 +173,8 @@ static inline void</span>
<span class="quote">&gt;  uprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; -static inline int uprobe_mmap(struct vm_area_struct *vma)</span>
<span class="quote">&gt; +static inline int uprobe_mmap(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +			      struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/ipc/shm.c b/ipc/shm.c</span>
<span class="quote">&gt; index 4643865e9171..6c29c791c7f2 100644</span>
<span class="quote">&gt; --- a/ipc/shm.c</span>
<span class="quote">&gt; +++ b/ipc/shm.c</span>
<span class="quote">&gt; @@ -1293,6 +1293,7 @@ long do_shmat(int shmid, char __user *shmaddr, int shmflg,</span>
<span class="quote">&gt;  	struct path path;</span>
<span class="quote">&gt;  	fmode_t f_mode;</span>
<span class="quote">&gt;  	unsigned long populate = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	err = -EINVAL;</span>
<span class="quote">&gt;  	if (shmid &lt; 0)</span>
<span class="quote">&gt; @@ -1411,7 +1412,8 @@ long do_shmat(int shmid, char __user *shmaddr, int shmflg,</span>
<span class="quote">&gt;  			goto invalid;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	addr = do_mmap_pgoff(file, addr, size, prot, flags, 0, &amp;populate, NULL);</span>
<span class="quote">&gt; +	addr = do_mmap_pgoff(file, addr, size, prot, flags, 0, &amp;populate, NULL,</span>
<span class="quote">&gt; +			     &amp;mmrange);</span>
<span class="quote">&gt;  	*raddr = addr;</span>
<span class="quote">&gt;  	err = 0;</span>
<span class="quote">&gt;  	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt; @@ -1487,6 +1489,7 @@ SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
<span class="quote">&gt;  	struct file *file;</span>
<span class="quote">&gt;  	struct vm_area_struct *next;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (addr &amp; ~PAGE_MASK)</span>
<span class="quote">&gt;  		return retval;</span>
<span class="quote">&gt; @@ -1537,7 +1540,8 @@ SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
<span class="quote">&gt;  			 */</span>
<span class="quote">&gt;  			file = vma-&gt;vm_file;</span>
<span class="quote">&gt;  			size = i_size_read(file_inode(vma-&gt;vm_file));</span>
<span class="quote">&gt; -			do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start, NULL);</span>
<span class="quote">&gt; +			do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start,</span>
<span class="quote">&gt; +				  NULL, &amp;mmrange);</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt;  			 * We discovered the size of the shm segment, so</span>
<span class="quote">&gt;  			 * break out of here and fall through to the next</span>
<span class="quote">&gt; @@ -1564,7 +1568,8 @@ SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
<span class="quote">&gt;  		if ((vma-&gt;vm_ops == &amp;shm_vm_ops) &amp;&amp;</span>
<span class="quote">&gt;  		    ((vma-&gt;vm_start - addr)/PAGE_SIZE == vma-&gt;vm_pgoff) &amp;&amp;</span>
<span class="quote">&gt;  		    (vma-&gt;vm_file == file))</span>
<span class="quote">&gt; -			do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start, NULL);</span>
<span class="quote">&gt; +			do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start,</span>
<span class="quote">&gt; +				  NULL, &amp;mmrange);</span>
<span class="quote">&gt;  		vma = next;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1573,7 +1578,8 @@ SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
<span class="quote">&gt;  	 * given</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (vma &amp;&amp; vma-&gt;vm_start == addr &amp;&amp; vma-&gt;vm_ops == &amp;shm_vm_ops) {</span>
<span class="quote">&gt; -		do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start, NULL);</span>
<span class="quote">&gt; +		do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start,</span>
<span class="quote">&gt; +			  NULL, &amp;mmrange);</span>
<span class="quote">&gt;  		retval = 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="quote">&gt; index ce6848e46e94..60e12b39182c 100644</span>
<span class="quote">&gt; --- a/kernel/events/uprobes.c</span>
<span class="quote">&gt; +++ b/kernel/events/uprobes.c</span>
<span class="quote">&gt; @@ -300,7 +300,7 @@ static int verify_opcode(struct page *page, unsigned long vaddr, uprobe_opcode_t</span>
<span class="quote">&gt;   * Return 0 (success) or a negative errno.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,</span>
<span class="quote">&gt; -			uprobe_opcode_t opcode)</span>
<span class="quote">&gt; +			uprobe_opcode_t opcode, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *old_page, *new_page;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt; @@ -309,7 +309,8 @@ int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,</span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt;  	/* Read the page with vaddr into memory */</span>
<span class="quote">&gt;  	ret = get_user_pages_remote(NULL, mm, vaddr, 1,</span>
<span class="quote">&gt; -			FOLL_FORCE | FOLL_SPLIT, &amp;old_page, &amp;vma, NULL);</span>
<span class="quote">&gt; +			FOLL_FORCE | FOLL_SPLIT, &amp;old_page, &amp;vma, NULL,</span>
<span class="quote">&gt; +			mmrange);</span>

There is no need to pass down the range here as get_user_pages_remote() is
told to not unlock the mmap_sem.
There are other places where passing range parameter down is not necessary
and is making this series bigger than needed, adding extra parameter to a
lot of functions which doesn&#39;t need it.

Laurent.
<span class="quote">
&gt;  	if (ret &lt;= 0)</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -349,9 +350,10 @@ int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,</span>
<span class="quote">&gt;   * For mm @mm, store the breakpoint instruction at @vaddr.</span>
<span class="quote">&gt;   * Return 0 (success) or a negative errno.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -int __weak set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr)</span>
<span class="quote">&gt; +int __weak set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm,</span>
<span class="quote">&gt; +		    unsigned long vaddr, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return uprobe_write_opcode(mm, vaddr, UPROBE_SWBP_INSN);</span>
<span class="quote">&gt; +	return uprobe_write_opcode(mm, vaddr, UPROBE_SWBP_INSN, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt; @@ -364,9 +366,12 @@ int __weak set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned</span>
<span class="quote">&gt;   * Return 0 (success) or a negative errno.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int __weak</span>
<span class="quote">&gt; -set_orig_insn(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr)</span>
<span class="quote">&gt; +set_orig_insn(struct arch_uprobe *auprobe, struct mm_struct *mm,</span>
<span class="quote">&gt; +	      unsigned long vaddr, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return uprobe_write_opcode(mm, vaddr, *(uprobe_opcode_t *)&amp;auprobe-&gt;insn);</span>
<span class="quote">&gt; +	return uprobe_write_opcode(mm, vaddr,</span>
<span class="quote">&gt; +				   *(uprobe_opcode_t *)&amp;auprobe-&gt;insn,</span>
<span class="quote">&gt; +				   mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static struct uprobe *get_uprobe(struct uprobe *uprobe)</span>
<span class="quote">&gt; @@ -650,7 +655,8 @@ static bool filter_chain(struct uprobe *uprobe,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int</span>
<span class="quote">&gt;  install_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,</span>
<span class="quote">&gt; -			struct vm_area_struct *vma, unsigned long vaddr)</span>
<span class="quote">&gt; +		   struct vm_area_struct *vma, unsigned long vaddr,</span>
<span class="quote">&gt; +		   struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	bool first_uprobe;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; @@ -667,7 +673,7 @@ install_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,</span>
<span class="quote">&gt;  	if (first_uprobe)</span>
<span class="quote">&gt;  		set_bit(MMF_HAS_UPROBES, &amp;mm-&gt;flags);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	ret = set_swbp(&amp;uprobe-&gt;arch, mm, vaddr);</span>
<span class="quote">&gt; +	ret = set_swbp(&amp;uprobe-&gt;arch, mm, vaddr, mmrange);</span>
<span class="quote">&gt;  	if (!ret)</span>
<span class="quote">&gt;  		clear_bit(MMF_RECALC_UPROBES, &amp;mm-&gt;flags);</span>
<span class="quote">&gt;  	else if (first_uprobe)</span>
<span class="quote">&gt; @@ -677,10 +683,11 @@ install_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int</span>
<span class="quote">&gt; -remove_breakpoint(struct uprobe *uprobe, struct mm_struct *mm, unsigned long vaddr)</span>
<span class="quote">&gt; +remove_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,</span>
<span class="quote">&gt; +		  unsigned long vaddr, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	set_bit(MMF_RECALC_UPROBES, &amp;mm-&gt;flags);</span>
<span class="quote">&gt; -	return set_orig_insn(&amp;uprobe-&gt;arch, mm, vaddr);</span>
<span class="quote">&gt; +	return set_orig_insn(&amp;uprobe-&gt;arch, mm, vaddr, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static inline bool uprobe_is_active(struct uprobe *uprobe)</span>
<span class="quote">&gt; @@ -794,6 +801,7 @@ register_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)</span>
<span class="quote">&gt;  	bool is_register = !!new;</span>
<span class="quote">&gt;  	struct map_info *info;</span>
<span class="quote">&gt;  	int err = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	percpu_down_write(&amp;dup_mmap_sem);</span>
<span class="quote">&gt;  	info = build_map_info(uprobe-&gt;inode-&gt;i_mapping,</span>
<span class="quote">&gt; @@ -824,11 +832,13 @@ register_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)</span>
<span class="quote">&gt;  			/* consult only the &quot;caller&quot;, new consumer. */</span>
<span class="quote">&gt;  			if (consumer_filter(new,</span>
<span class="quote">&gt;  					UPROBE_FILTER_REGISTER, mm))</span>
<span class="quote">&gt; -				err = install_breakpoint(uprobe, mm, vma, info-&gt;vaddr);</span>
<span class="quote">&gt; +				err = install_breakpoint(uprobe, mm, vma,</span>
<span class="quote">&gt; +							 info-&gt;vaddr, &amp;mmrange);</span>
<span class="quote">&gt;  		} else if (test_bit(MMF_HAS_UPROBES, &amp;mm-&gt;flags)) {</span>
<span class="quote">&gt;  			if (!filter_chain(uprobe,</span>
<span class="quote">&gt;  					UPROBE_FILTER_UNREGISTER, mm))</span>
<span class="quote">&gt; -				err |= remove_breakpoint(uprobe, mm, info-&gt;vaddr);</span>
<span class="quote">&gt; +				err |= remove_breakpoint(uprobe, mm,</span>
<span class="quote">&gt; +							 info-&gt;vaddr, &amp;mmrange);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   unlock:</span>
<span class="quote">&gt; @@ -972,6 +982,7 @@ static int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	int err = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {</span>
<span class="quote">&gt; @@ -988,7 +999,7 @@ static int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		vaddr = offset_to_vaddr(vma, uprobe-&gt;offset);</span>
<span class="quote">&gt; -		err |= remove_breakpoint(uprobe, mm, vaddr);</span>
<span class="quote">&gt; +		err |= remove_breakpoint(uprobe, mm, vaddr, &amp;mmrange);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1063,7 +1074,7 @@ static void build_probe_list(struct inode *inode,</span>
<span class="quote">&gt;   * Currently we ignore all errors and always return 0, the callers</span>
<span class="quote">&gt;   * can&#39;t handle the failure anyway.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -int uprobe_mmap(struct vm_area_struct *vma)</span>
<span class="quote">&gt; +int uprobe_mmap(struct vm_area_struct *vma, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct list_head tmp_list;</span>
<span class="quote">&gt;  	struct uprobe *uprobe, *u;</span>
<span class="quote">&gt; @@ -1087,7 +1098,7 @@ int uprobe_mmap(struct vm_area_struct *vma)</span>
<span class="quote">&gt;  		if (!fatal_signal_pending(current) &amp;&amp;</span>
<span class="quote">&gt;  		    filter_chain(uprobe, UPROBE_FILTER_MMAP, vma-&gt;vm_mm)) {</span>
<span class="quote">&gt;  			unsigned long vaddr = offset_to_vaddr(vma, uprobe-&gt;offset);</span>
<span class="quote">&gt; -			install_breakpoint(uprobe, vma-&gt;vm_mm, vma, vaddr);</span>
<span class="quote">&gt; +			install_breakpoint(uprobe, vma-&gt;vm_mm, vma, vaddr, mmrange);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		put_uprobe(uprobe);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1698,7 +1709,8 @@ static void mmf_recalc_uprobes(struct mm_struct *mm)</span>
<span class="quote">&gt;  	clear_bit(MMF_HAS_UPROBES, &amp;mm-&gt;flags);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static int is_trap_at_addr(struct mm_struct *mm, unsigned long vaddr)</span>
<span class="quote">&gt; +static int is_trap_at_addr(struct mm_struct *mm, unsigned long vaddr,</span>
<span class="quote">&gt; +			   struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt;  	uprobe_opcode_t opcode;</span>
<span class="quote">&gt; @@ -1718,7 +1730,7 @@ static int is_trap_at_addr(struct mm_struct *mm, unsigned long vaddr)</span>
<span class="quote">&gt;  	 * essentially a kernel access to the memory.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	result = get_user_pages_remote(NULL, mm, vaddr, 1, FOLL_FORCE, &amp;page,</span>
<span class="quote">&gt; -			NULL, NULL);</span>
<span class="quote">&gt; +				       NULL, NULL, mmrange);</span>
<span class="quote">&gt;  	if (result &lt; 0)</span>
<span class="quote">&gt;  		return result;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1734,6 +1746,7 @@ static struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	struct uprobe *uprobe = NULL;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	vma = find_vma(mm, bp_vaddr);</span>
<span class="quote">&gt; @@ -1746,7 +1759,7 @@ static struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (!uprobe)</span>
<span class="quote">&gt; -			*is_swbp = is_trap_at_addr(mm, bp_vaddr);</span>
<span class="quote">&gt; +			*is_swbp = is_trap_at_addr(mm, bp_vaddr, &amp;mmrange);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt;  		*is_swbp = -EFAULT;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; diff --git a/kernel/futex.c b/kernel/futex.c</span>
<span class="quote">&gt; index 1f450e092c74..09a0d86f80a0 100644</span>
<span class="quote">&gt; --- a/kernel/futex.c</span>
<span class="quote">&gt; +++ b/kernel/futex.c</span>
<span class="quote">&gt; @@ -725,10 +725,11 @@ static int fault_in_user_writeable(u32 __user *uaddr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	ret = fixup_user_fault(current, mm, (unsigned long)uaddr,</span>
<span class="quote">&gt; -			       FAULT_FLAG_WRITE, NULL);</span>
<span class="quote">&gt; +			       FAULT_FLAG_WRITE, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	return ret &lt; 0 ? ret : 0;</span>
<span class="quote">&gt; diff --git a/mm/frame_vector.c b/mm/frame_vector.c</span>
<span class="quote">&gt; index c64dca6e27c2..d3dccd80c6ee 100644</span>
<span class="quote">&gt; --- a/mm/frame_vector.c</span>
<span class="quote">&gt; +++ b/mm/frame_vector.c</span>
<span class="quote">&gt; @@ -39,6 +39,7 @@ int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
<span class="quote">&gt;  	int ret = 0;</span>
<span class="quote">&gt;  	int err;</span>
<span class="quote">&gt;  	int locked;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (nr_frames == 0)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; @@ -71,7 +72,8 @@ int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
<span class="quote">&gt;  		vec-&gt;got_ref = true;</span>
<span class="quote">&gt;  		vec-&gt;is_pfns = false;</span>
<span class="quote">&gt;  		ret = get_user_pages_locked(start, nr_frames,</span>
<span class="quote">&gt; -			gup_flags, (struct page **)(vec-&gt;ptrs), &amp;locked);</span>
<span class="quote">&gt; +			gup_flags, (struct page **)(vec-&gt;ptrs), &amp;locked,</span>
<span class="quote">&gt; +			&amp;mmrange);</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="quote">&gt; index 1b46e6e74881..01983a7b3750 100644</span>
<span class="quote">&gt; --- a/mm/gup.c</span>
<span class="quote">&gt; +++ b/mm/gup.c</span>
<span class="quote">&gt; @@ -478,7 +478,8 @@ static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt;   * If it is, *@nonblocking will be set to 0 and -EBUSY returned.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,</span>
<span class="quote">&gt; -		unsigned long address, unsigned int *flags, int *nonblocking)</span>
<span class="quote">&gt; +		unsigned long address, unsigned int *flags, int *nonblocking,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned int fault_flags = 0;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; @@ -499,7 +500,7 @@ static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		fault_flags |= FAULT_FLAG_TRIED;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	ret = handle_mm_fault(vma, address, fault_flags);</span>
<span class="quote">&gt; +	ret = handle_mm_fault(vma, address, fault_flags, mmrange);</span>
<span class="quote">&gt;  	if (ret &amp; VM_FAULT_ERROR) {</span>
<span class="quote">&gt;  		int err = vm_fault_to_errno(ret, *flags);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -592,6 +593,7 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)</span>
<span class="quote">&gt;   * @vmas:	array of pointers to vmas corresponding to each page.</span>
<span class="quote">&gt;   *		Or NULL if the caller does not require them.</span>
<span class="quote">&gt;   * @nonblocking: whether waiting for disk IO or mmap_sem contention</span>
<span class="quote">&gt; + * @mmrange:	mm address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Returns number of pages pinned. This may be fewer than the number</span>
<span class="quote">&gt;   * requested. If nr_pages is 0 or negative, returns 0. If no pages</span>
<span class="quote">&gt; @@ -638,7 +640,8 @@ static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)</span>
<span class="quote">&gt;  static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  		unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  		unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; -		struct vm_area_struct **vmas, int *nonblocking)</span>
<span class="quote">&gt; +		struct vm_area_struct **vmas, int *nonblocking,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	long i = 0;</span>
<span class="quote">&gt;  	unsigned int page_mask;</span>
<span class="quote">&gt; @@ -664,7 +667,7 @@ static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		/* first iteration or cross vma bound */</span>
<span class="quote">&gt;  		if (!vma || start &gt;= vma-&gt;vm_end) {</span>
<span class="quote">&gt; -			vma = find_extend_vma(mm, start);</span>
<span class="quote">&gt; +			vma = find_extend_vma(mm, start, mmrange);</span>
<span class="quote">&gt;  			if (!vma &amp;&amp; in_gate_area(mm, start)) {</span>
<span class="quote">&gt;  				int ret;</span>
<span class="quote">&gt;  				ret = get_gate_page(mm, start &amp; PAGE_MASK,</span>
<span class="quote">&gt; @@ -697,7 +700,7 @@ static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  		if (!page) {</span>
<span class="quote">&gt;  			int ret;</span>
<span class="quote">&gt;  			ret = faultin_page(tsk, vma, start, &amp;foll_flags,</span>
<span class="quote">&gt; -					nonblocking);</span>
<span class="quote">&gt; +					   nonblocking, mmrange);</span>
<span class="quote">&gt;  			switch (ret) {</span>
<span class="quote">&gt;  			case 0:</span>
<span class="quote">&gt;  				goto retry;</span>
<span class="quote">&gt; @@ -796,7 +799,7 @@ static bool vma_permits_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  		     unsigned long address, unsigned int fault_flags,</span>
<span class="quote">&gt; -		     bool *unlocked)</span>
<span class="quote">&gt; +		     bool *unlocked, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	int ret, major = 0;</span>
<span class="quote">&gt; @@ -805,14 +808,14 @@ int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  		fault_flags |= FAULT_FLAG_ALLOW_RETRY;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt; -	vma = find_extend_vma(mm, address);</span>
<span class="quote">&gt; +	vma = find_extend_vma(mm, address, mmrange);</span>
<span class="quote">&gt;  	if (!vma || address &lt; vma-&gt;vm_start)</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (!vma_permits_fault(vma, fault_flags))</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	ret = handle_mm_fault(vma, address, fault_flags);</span>
<span class="quote">&gt; +	ret = handle_mm_fault(vma, address, fault_flags, mmrange);</span>
<span class="quote">&gt;  	major |= ret &amp; VM_FAULT_MAJOR;</span>
<span class="quote">&gt;  	if (ret &amp; VM_FAULT_ERROR) {</span>
<span class="quote">&gt;  		int err = vm_fault_to_errno(ret, 0);</span>
<span class="quote">&gt; @@ -849,7 +852,8 @@ static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
<span class="quote">&gt;  						struct page **pages,</span>
<span class="quote">&gt;  						struct vm_area_struct **vmas,</span>
<span class="quote">&gt;  						int *locked,</span>
<span class="quote">&gt; -						unsigned int flags)</span>
<span class="quote">&gt; +						unsigned int flags,</span>
<span class="quote">&gt; +						struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	long ret, pages_done;</span>
<span class="quote">&gt;  	bool lock_dropped;</span>
<span class="quote">&gt; @@ -868,7 +872,7 @@ static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
<span class="quote">&gt;  	lock_dropped = false;</span>
<span class="quote">&gt;  	for (;;) {</span>
<span class="quote">&gt;  		ret = __get_user_pages(tsk, mm, start, nr_pages, flags, pages,</span>
<span class="quote">&gt; -				       vmas, locked);</span>
<span class="quote">&gt; +				       vmas, locked, mmrange);</span>
<span class="quote">&gt;  		if (!locked)</span>
<span class="quote">&gt;  			/* VM_FAULT_RETRY couldn&#39;t trigger, bypass */</span>
<span class="quote">&gt;  			return ret;</span>
<span class="quote">&gt; @@ -908,7 +912,7 @@ static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
<span class="quote">&gt;  		lock_dropped = true;</span>
<span class="quote">&gt;  		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  		ret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,</span>
<span class="quote">&gt; -				       pages, NULL, NULL);</span>
<span class="quote">&gt; +				       pages, NULL, NULL, mmrange);</span>
<span class="quote">&gt;  		if (ret != 1) {</span>
<span class="quote">&gt;  			BUG_ON(ret &gt; 1);</span>
<span class="quote">&gt;  			if (!pages_done)</span>
<span class="quote">&gt; @@ -956,11 +960,11 @@ static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  long get_user_pages_locked(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  			   unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; -			   int *locked)</span>
<span class="quote">&gt; +			   int *locked, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return __get_user_pages_locked(current, current-&gt;mm, start, nr_pages,</span>
<span class="quote">&gt;  				       pages, NULL, locked,</span>
<span class="quote">&gt; -				       gup_flags | FOLL_TOUCH);</span>
<span class="quote">&gt; +				       gup_flags | FOLL_TOUCH, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(get_user_pages_locked);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -985,10 +989,11 @@ long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	int locked = 1;</span>
<span class="quote">&gt;  	long ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	ret = __get_user_pages_locked(current, mm, start, nr_pages, pages, NULL,</span>
<span class="quote">&gt; -				      &amp;locked, gup_flags | FOLL_TOUCH);</span>
<span class="quote">&gt; +				      &amp;locked, gup_flags | FOLL_TOUCH, &amp;mmrange);</span>
<span class="quote">&gt;  	if (locked)</span>
<span class="quote">&gt;  		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt; @@ -1054,11 +1059,13 @@ EXPORT_SYMBOL(get_user_pages_unlocked);</span>
<span class="quote">&gt;  long get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  		unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  		unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; -		struct vm_area_struct **vmas, int *locked)</span>
<span class="quote">&gt; +	        struct vm_area_struct **vmas, int *locked,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,</span>
<span class="quote">&gt;  				       locked,</span>
<span class="quote">&gt; -				       gup_flags | FOLL_TOUCH | FOLL_REMOTE);</span>
<span class="quote">&gt; +				       gup_flags | FOLL_TOUCH | FOLL_REMOTE,</span>
<span class="quote">&gt; +				       mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(get_user_pages_remote);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1071,11 +1078,11 @@ EXPORT_SYMBOL(get_user_pages_remote);</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  long get_user_pages(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  		unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; -		struct vm_area_struct **vmas)</span>
<span class="quote">&gt; +		struct vm_area_struct **vmas, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return __get_user_pages_locked(current, current-&gt;mm, start, nr_pages,</span>
<span class="quote">&gt;  				       pages, vmas, NULL,</span>
<span class="quote">&gt; -				       gup_flags | FOLL_TOUCH);</span>
<span class="quote">&gt; +				       gup_flags | FOLL_TOUCH, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(get_user_pages);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1094,7 +1101,8 @@ EXPORT_SYMBOL(get_user_pages);</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  long get_user_pages_longterm(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  		unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; -		struct vm_area_struct **vmas_arg)</span>
<span class="quote">&gt; +	        struct vm_area_struct **vmas_arg,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct **vmas = vmas_arg;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma_prev = NULL;</span>
<span class="quote">&gt; @@ -1110,7 +1118,7 @@ long get_user_pages_longterm(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  			return -ENOMEM;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	rc = get_user_pages(start, nr_pages, gup_flags, pages, vmas);</span>
<span class="quote">&gt; +	rc = get_user_pages(start, nr_pages, gup_flags, pages, vmas, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	for (i = 0; i &lt; rc; i++) {</span>
<span class="quote">&gt;  		struct vm_area_struct *vma = vmas[i];</span>
<span class="quote">&gt; @@ -1149,6 +1157,7 @@ EXPORT_SYMBOL(get_user_pages_longterm);</span>
<span class="quote">&gt;   * @start: start address</span>
<span class="quote">&gt;   * @end:   end address</span>
<span class="quote">&gt;   * @nonblocking:</span>
<span class="quote">&gt; + * @mmrange: mm address space range locking</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * This takes care of mlocking the pages too if VM_LOCKED is set.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; @@ -1163,7 +1172,8 @@ EXPORT_SYMBOL(get_user_pages_longterm);</span>
<span class="quote">&gt;   * released.  If it&#39;s released, *@nonblocking will be set to 0.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  long populate_vma_page_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt; -		unsigned long start, unsigned long end, int *nonblocking)</span>
<span class="quote">&gt; +		unsigned long start, unsigned long end, int *nonblocking,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	unsigned long nr_pages = (end - start) / PAGE_SIZE;</span>
<span class="quote">&gt; @@ -1198,7 +1208,7 @@ long populate_vma_page_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	 * not result in a stack expansion that recurses back here.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	return __get_user_pages(current, mm, start, nr_pages, gup_flags,</span>
<span class="quote">&gt; -				NULL, NULL, nonblocking);</span>
<span class="quote">&gt; +				NULL, NULL, nonblocking, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -1215,6 +1225,7 @@ int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = NULL;</span>
<span class="quote">&gt;  	int locked = 0;</span>
<span class="quote">&gt;  	long ret = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	VM_BUG_ON(start &amp; ~PAGE_MASK);</span>
<span class="quote">&gt;  	VM_BUG_ON(len != PAGE_ALIGN(len));</span>
<span class="quote">&gt; @@ -1247,7 +1258,7 @@ int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
<span class="quote">&gt;  		 * double checks the vma flags, so that it won&#39;t mlock pages</span>
<span class="quote">&gt;  		 * if the vma was already munlocked.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		ret = populate_vma_page_range(vma, nstart, nend, &amp;locked);</span>
<span class="quote">&gt; +		ret = populate_vma_page_range(vma, nstart, nend, &amp;locked, &amp;mmrange);</span>
<span class="quote">&gt;  		if (ret &lt; 0) {</span>
<span class="quote">&gt;  			if (ignore_errors) {</span>
<span class="quote">&gt;  				ret = 0;</span>
<span class="quote">&gt; @@ -1282,10 +1293,11 @@ struct page *get_dump_page(unsigned long addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (__get_user_pages(current, current-&gt;mm, addr, 1,</span>
<span class="quote">&gt;  			     FOLL_FORCE | FOLL_DUMP | FOLL_GET, &amp;page, &amp;vma,</span>
<span class="quote">&gt; -			     NULL) &lt; 1)</span>
<span class="quote">&gt; +			     NULL, &amp;mmrange) &lt; 1)</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  	flush_cache_page(vma, addr, page_to_pfn(page));</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt; diff --git a/mm/hmm.c b/mm/hmm.c</span>
<span class="quote">&gt; index 320545b98ff5..b14e6869689e 100644</span>
<span class="quote">&gt; --- a/mm/hmm.c</span>
<span class="quote">&gt; +++ b/mm/hmm.c</span>
<span class="quote">&gt; @@ -245,7 +245,8 @@ struct hmm_vma_walk {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int hmm_vma_do_fault(struct mm_walk *walk,</span>
<span class="quote">&gt;  			    unsigned long addr,</span>
<span class="quote">&gt; -			    hmm_pfn_t *pfn)</span>
<span class="quote">&gt; +			    hmm_pfn_t *pfn,</span>
<span class="quote">&gt; +			    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_REMOTE;</span>
<span class="quote">&gt;  	struct hmm_vma_walk *hmm_vma_walk = walk-&gt;private;</span>
<span class="quote">&gt; @@ -254,7 +255,7 @@ static int hmm_vma_do_fault(struct mm_walk *walk,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	flags |= hmm_vma_walk-&gt;block ? 0 : FAULT_FLAG_ALLOW_RETRY;</span>
<span class="quote">&gt;  	flags |= hmm_vma_walk-&gt;write ? FAULT_FLAG_WRITE : 0;</span>
<span class="quote">&gt; -	r = handle_mm_fault(vma, addr, flags);</span>
<span class="quote">&gt; +	r = handle_mm_fault(vma, addr, flags, mmrange);</span>
<span class="quote">&gt;  	if (r &amp; VM_FAULT_RETRY)</span>
<span class="quote">&gt;  		return -EBUSY;</span>
<span class="quote">&gt;  	if (r &amp; VM_FAULT_ERROR) {</span>
<span class="quote">&gt; @@ -298,7 +299,9 @@ static void hmm_pfns_clear(hmm_pfn_t *pfns,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int hmm_vma_walk_hole(unsigned long addr,</span>
<span class="quote">&gt;  			     unsigned long end,</span>
<span class="quote">&gt; -			     struct mm_walk *walk)</span>
<span class="quote">&gt; +			     struct mm_walk *walk,</span>
<span class="quote">&gt; +			     struct range_lock *mmrange)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct hmm_vma_walk *hmm_vma_walk = walk-&gt;private;</span>
<span class="quote">&gt;  	struct hmm_range *range = hmm_vma_walk-&gt;range;</span>
<span class="quote">&gt; @@ -312,7 +315,7 @@ static int hmm_vma_walk_hole(unsigned long addr,</span>
<span class="quote">&gt;  		if (hmm_vma_walk-&gt;fault) {</span>
<span class="quote">&gt;  			int ret;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -			ret = hmm_vma_do_fault(walk, addr, &amp;pfns[i]);</span>
<span class="quote">&gt; +			ret = hmm_vma_do_fault(walk, addr, &amp;pfns[i], mmrange);</span>
<span class="quote">&gt;  			if (ret != -EAGAIN)</span>
<span class="quote">&gt;  				return ret;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -323,7 +326,8 @@ static int hmm_vma_walk_hole(unsigned long addr,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int hmm_vma_walk_clear(unsigned long addr,</span>
<span class="quote">&gt;  			      unsigned long end,</span>
<span class="quote">&gt; -			      struct mm_walk *walk)</span>
<span class="quote">&gt; +			      struct mm_walk *walk,</span>
<span class="quote">&gt; +			      struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct hmm_vma_walk *hmm_vma_walk = walk-&gt;private;</span>
<span class="quote">&gt;  	struct hmm_range *range = hmm_vma_walk-&gt;range;</span>
<span class="quote">&gt; @@ -337,7 +341,7 @@ static int hmm_vma_walk_clear(unsigned long addr,</span>
<span class="quote">&gt;  		if (hmm_vma_walk-&gt;fault) {</span>
<span class="quote">&gt;  			int ret;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -			ret = hmm_vma_do_fault(walk, addr, &amp;pfns[i]);</span>
<span class="quote">&gt; +			ret = hmm_vma_do_fault(walk, addr, &amp;pfns[i], mmrange);</span>
<span class="quote">&gt;  			if (ret != -EAGAIN)</span>
<span class="quote">&gt;  				return ret;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -349,7 +353,8 @@ static int hmm_vma_walk_clear(unsigned long addr,</span>
<span class="quote">&gt;  static int hmm_vma_walk_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt;  			    unsigned long start,</span>
<span class="quote">&gt;  			    unsigned long end,</span>
<span class="quote">&gt; -			    struct mm_walk *walk)</span>
<span class="quote">&gt; +			    struct mm_walk *walk,</span>
<span class="quote">&gt; +			    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct hmm_vma_walk *hmm_vma_walk = walk-&gt;private;</span>
<span class="quote">&gt;  	struct hmm_range *range = hmm_vma_walk-&gt;range;</span>
<span class="quote">&gt; @@ -366,7 +371,7 @@ static int hmm_vma_walk_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt;  	if (pmd_none(*pmdp))</span>
<span class="quote">&gt; -		return hmm_vma_walk_hole(start, end, walk);</span>
<span class="quote">&gt; +		return hmm_vma_walk_hole(start, end, walk, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (pmd_huge(*pmdp) &amp;&amp; vma-&gt;vm_flags &amp; VM_HUGETLB)</span>
<span class="quote">&gt;  		return hmm_pfns_bad(start, end, walk);</span>
<span class="quote">&gt; @@ -389,10 +394,10 @@ static int hmm_vma_walk_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt;  		if (!pmd_devmap(pmd) &amp;&amp; !pmd_trans_huge(pmd))</span>
<span class="quote">&gt;  			goto again;</span>
<span class="quote">&gt;  		if (pmd_protnone(pmd))</span>
<span class="quote">&gt; -			return hmm_vma_walk_clear(start, end, walk);</span>
<span class="quote">&gt; +			return hmm_vma_walk_clear(start, end, walk, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (write_fault &amp;&amp; !pmd_write(pmd))</span>
<span class="quote">&gt; -			return hmm_vma_walk_clear(start, end, walk);</span>
<span class="quote">&gt; +			return hmm_vma_walk_clear(start, end, walk, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		pfn = pmd_pfn(pmd) + pte_index(addr);</span>
<span class="quote">&gt;  		flag |= pmd_write(pmd) ? HMM_PFN_WRITE : 0;</span>
<span class="quote">&gt; @@ -464,7 +469,7 @@ static int hmm_vma_walk_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt;  fault:</span>
<span class="quote">&gt;  		pte_unmap(ptep);</span>
<span class="quote">&gt;  		/* Fault all pages in range */</span>
<span class="quote">&gt; -		return hmm_vma_walk_clear(start, end, walk);</span>
<span class="quote">&gt; +		return hmm_vma_walk_clear(start, end, walk, mmrange);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	pte_unmap(ptep - 1);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -495,7 +500,8 @@ int hmm_vma_get_pfns(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		     struct hmm_range *range,</span>
<span class="quote">&gt;  		     unsigned long start,</span>
<span class="quote">&gt;  		     unsigned long end,</span>
<span class="quote">&gt; -		     hmm_pfn_t *pfns)</span>
<span class="quote">&gt; +		     hmm_pfn_t *pfns,</span>
<span class="quote">&gt; +		     struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct hmm_vma_walk hmm_vma_walk;</span>
<span class="quote">&gt;  	struct mm_walk mm_walk;</span>
<span class="quote">&gt; @@ -541,7 +547,7 @@ int hmm_vma_get_pfns(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	mm_walk.pmd_entry = hmm_vma_walk_pmd;</span>
<span class="quote">&gt;  	mm_walk.pte_hole = hmm_vma_walk_hole;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	walk_page_range(start, end, &amp;mm_walk);</span>
<span class="quote">&gt; +	walk_page_range(start, end, &amp;mm_walk, mmrange);</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(hmm_vma_get_pfns);</span>
<span class="quote">&gt; @@ -664,7 +670,8 @@ int hmm_vma_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		  unsigned long end,</span>
<span class="quote">&gt;  		  hmm_pfn_t *pfns,</span>
<span class="quote">&gt;  		  bool write,</span>
<span class="quote">&gt; -		  bool block)</span>
<span class="quote">&gt; +		  bool block,</span>
<span class="quote">&gt; +		  struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct hmm_vma_walk hmm_vma_walk;</span>
<span class="quote">&gt;  	struct mm_walk mm_walk;</span>
<span class="quote">&gt; @@ -717,7 +724,7 @@ int hmm_vma_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	mm_walk.pte_hole = hmm_vma_walk_hole;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt; -		ret = walk_page_range(start, end, &amp;mm_walk);</span>
<span class="quote">&gt; +		ret = walk_page_range(start, end, &amp;mm_walk, mmrange);</span>
<span class="quote">&gt;  		start = hmm_vma_walk.last;</span>
<span class="quote">&gt;  	} while (ret == -EAGAIN);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="quote">&gt; index 62d8c34e63d5..abf1de31e524 100644</span>
<span class="quote">&gt; --- a/mm/internal.h</span>
<span class="quote">&gt; +++ b/mm/internal.h</span>
<span class="quote">&gt; @@ -289,7 +289,8 @@ void __vma_link_list(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_MMU</span>
<span class="quote">&gt;  extern long populate_vma_page_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt; -		unsigned long start, unsigned long end, int *nonblocking);</span>
<span class="quote">&gt; +			unsigned long start, unsigned long end, int *nonblocking,</span>
<span class="quote">&gt; +			struct range_lock *mmrange);</span>
<span class="quote">&gt;  extern void munlock_vma_pages_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			unsigned long start, unsigned long end);</span>
<span class="quote">&gt;  static inline void munlock_vma_pages_all(struct vm_area_struct *vma)</span>
<span class="quote">&gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt; index 293721f5da70..66c350cd9799 100644</span>
<span class="quote">&gt; --- a/mm/ksm.c</span>
<span class="quote">&gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt; @@ -448,7 +448,8 @@ static inline bool ksm_test_exit(struct mm_struct *mm)</span>
<span class="quote">&gt;   * of the process that owns &#39;vma&#39;.  We also do not want to enforce</span>
<span class="quote">&gt;   * protection keys here anyway.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static int break_ksm(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="quote">&gt; +static int break_ksm(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="quote">&gt; +		     struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt;  	int ret = 0;</span>
<span class="quote">&gt; @@ -461,7 +462,8 @@ static int break_ksm(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  		if (PageKsm(page))</span>
<span class="quote">&gt;  			ret = handle_mm_fault(vma, addr,</span>
<span class="quote">&gt; -					FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE);</span>
<span class="quote">&gt; +					FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE,</span>
<span class="quote">&gt; +					mmrange);</span>
<span class="quote">&gt;  		else</span>
<span class="quote">&gt;  			ret = VM_FAULT_WRITE;</span>
<span class="quote">&gt;  		put_page(page);</span>
<span class="quote">&gt; @@ -516,6 +518,7 @@ static void break_cow(struct rmap_item *rmap_item)</span>
<span class="quote">&gt;  	struct mm_struct *mm = rmap_item-&gt;mm;</span>
<span class="quote">&gt;  	unsigned long addr = rmap_item-&gt;address;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * It is not an accident that whenever we want to break COW</span>
<span class="quote">&gt; @@ -526,7 +529,7 @@ static void break_cow(struct rmap_item *rmap_item)</span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	vma = find_mergeable_vma(mm, addr);</span>
<span class="quote">&gt;  	if (vma)</span>
<span class="quote">&gt; -		break_ksm(vma, addr);</span>
<span class="quote">&gt; +		break_ksm(vma, addr, &amp;mmrange);</span>
<span class="quote">&gt;  	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -807,7 +810,8 @@ static void remove_trailing_rmap_items(struct mm_slot *mm_slot,</span>
<span class="quote">&gt;   * in cmp_and_merge_page on one of the rmap_items we would be removing.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int unmerge_ksm_pages(struct vm_area_struct *vma,</span>
<span class="quote">&gt; -			     unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +			     unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +			     struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long addr;</span>
<span class="quote">&gt;  	int err = 0;</span>
<span class="quote">&gt; @@ -818,7 +822,7 @@ static int unmerge_ksm_pages(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		if (signal_pending(current))</span>
<span class="quote">&gt;  			err = -ERESTARTSYS;</span>
<span class="quote">&gt;  		else</span>
<span class="quote">&gt; -			err = break_ksm(vma, addr);</span>
<span class="quote">&gt; +			err = break_ksm(vma, addr, mmrange);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return err;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -922,6 +926,7 @@ static int unmerge_and_remove_all_rmap_items(void)</span>
<span class="quote">&gt;  	struct mm_struct *mm;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	int err = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	spin_lock(&amp;ksm_mmlist_lock);</span>
<span class="quote">&gt;  	ksm_scan.mm_slot = list_entry(ksm_mm_head.mm_list.next,</span>
<span class="quote">&gt; @@ -937,8 +942,8 @@ static int unmerge_and_remove_all_rmap_items(void)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  			if (!(vma-&gt;vm_flags &amp; VM_MERGEABLE) || !vma-&gt;anon_vma)</span>
<span class="quote">&gt;  				continue;</span>
<span class="quote">&gt; -			err = unmerge_ksm_pages(vma,</span>
<span class="quote">&gt; -						vma-&gt;vm_start, vma-&gt;vm_end);</span>
<span class="quote">&gt; +			err = unmerge_ksm_pages(vma, vma-&gt;vm_start,</span>
<span class="quote">&gt; +						vma-&gt;vm_end, &amp;mmrange);</span>
<span class="quote">&gt;  			if (err)</span>
<span class="quote">&gt;  				goto error;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -2350,7 +2355,8 @@ static int ksm_scan_thread(void *nothing)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  int ksm_madvise(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt; -		unsigned long end, int advice, unsigned long *vm_flags)</span>
<span class="quote">&gt; +		unsigned long end, int advice, unsigned long *vm_flags,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	int err;</span>
<span class="quote">&gt; @@ -2384,7 +2390,7 @@ int ksm_madvise(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt;  			return 0;		/* just ignore the advice */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (vma-&gt;anon_vma) {</span>
<span class="quote">&gt; -			err = unmerge_ksm_pages(vma, start, end);</span>
<span class="quote">&gt; +			err = unmerge_ksm_pages(vma, start, end, mmrange);</span>
<span class="quote">&gt;  			if (err)</span>
<span class="quote">&gt;  				return err;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; index 4d3c922ea1a1..eaec6bfc2b08 100644</span>
<span class="quote">&gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; @@ -54,7 +54,8 @@ static int madvise_need_mmap_write(int behavior)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static long madvise_behavior(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		     struct vm_area_struct **prev,</span>
<span class="quote">&gt; -		     unsigned long start, unsigned long end, int behavior)</span>
<span class="quote">&gt; +		     unsigned long start, unsigned long end, int behavior,</span>
<span class="quote">&gt; +		     struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	int error = 0;</span>
<span class="quote">&gt; @@ -104,7 +105,8 @@ static long madvise_behavior(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		break;</span>
<span class="quote">&gt;  	case MADV_MERGEABLE:</span>
<span class="quote">&gt;  	case MADV_UNMERGEABLE:</span>
<span class="quote">&gt; -		error = ksm_madvise(vma, start, end, behavior, &amp;new_flags);</span>
<span class="quote">&gt; +		error = ksm_madvise(vma, start, end, behavior,</span>
<span class="quote">&gt; +				    &amp;new_flags, mmrange);</span>
<span class="quote">&gt;  		if (error) {</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt;  			 * madvise() returns EAGAIN if kernel resources, such as</span>
<span class="quote">&gt; @@ -138,7 +140,7 @@ static long madvise_behavior(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	pgoff = vma-&gt;vm_pgoff + ((start - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt;  	*prev = vma_merge(mm, *prev, start, end, new_flags, vma-&gt;anon_vma,</span>
<span class="quote">&gt;  			  vma-&gt;vm_file, pgoff, vma_policy(vma),</span>
<span class="quote">&gt; -			  vma-&gt;vm_userfaultfd_ctx);</span>
<span class="quote">&gt; +			  vma-&gt;vm_userfaultfd_ctx, mmrange);</span>
<span class="quote">&gt;  	if (*prev) {</span>
<span class="quote">&gt;  		vma = *prev;</span>
<span class="quote">&gt;  		goto success;</span>
<span class="quote">&gt; @@ -151,7 +153,7 @@ static long madvise_behavior(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			error = -ENOMEM;</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		error = __split_vma(mm, vma, start, 1);</span>
<span class="quote">&gt; +		error = __split_vma(mm, vma, start, 1, mmrange);</span>
<span class="quote">&gt;  		if (error) {</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt;  			 * madvise() returns EAGAIN if kernel resources, such as</span>
<span class="quote">&gt; @@ -168,7 +170,7 @@ static long madvise_behavior(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			error = -ENOMEM;</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		error = __split_vma(mm, vma, end, 0);</span>
<span class="quote">&gt; +		error = __split_vma(mm, vma, end, 0, mmrange);</span>
<span class="quote">&gt;  		if (error) {</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt;  			 * madvise() returns EAGAIN if kernel resources, such as</span>
<span class="quote">&gt; @@ -191,7 +193,8 @@ static long madvise_behavior(struct vm_area_struct *vma,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifdef CONFIG_SWAP</span>
<span class="quote">&gt;  static int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,</span>
<span class="quote">&gt; -	unsigned long end, struct mm_walk *walk)</span>
<span class="quote">&gt; +				 unsigned long end, struct mm_walk *walk,</span>
<span class="quote">&gt; +				 struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pte_t *orig_pte;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;private;</span>
<span class="quote">&gt; @@ -226,7 +229,8 @@ static int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static void force_swapin_readahead(struct vm_area_struct *vma,</span>
<span class="quote">&gt; -		unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +				   unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +				   struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_walk walk = {</span>
<span class="quote">&gt;  		.mm = vma-&gt;vm_mm,</span>
<span class="quote">&gt; @@ -234,7 +238,7 @@ static void force_swapin_readahead(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		.private = vma,</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	walk_page_range(start, end, &amp;walk);</span>
<span class="quote">&gt; +	walk_page_range(start, end, &amp;walk, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	lru_add_drain();	/* Push any new pages onto the LRU now */</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -272,14 +276,15 @@ static void force_shm_swapin_readahead(struct vm_area_struct *vma,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static long madvise_willneed(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			     struct vm_area_struct **prev,</span>
<span class="quote">&gt; -			     unsigned long start, unsigned long end)</span>
<span class="quote">&gt; +			     unsigned long start, unsigned long end,</span>
<span class="quote">&gt; +			     struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct file *file = vma-&gt;vm_file;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	*prev = vma;</span>
<span class="quote">&gt;  #ifdef CONFIG_SWAP</span>
<span class="quote">&gt;  	if (!file) {</span>
<span class="quote">&gt; -		force_swapin_readahead(vma, start, end);</span>
<span class="quote">&gt; +		force_swapin_readahead(vma, start, end, mmrange);</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -308,7 +313,8 @@ static long madvise_willneed(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; -				unsigned long end, struct mm_walk *walk)</span>
<span class="quote">&gt; +				  unsigned long end, struct mm_walk *walk,</span>
<span class="quote">&gt; +				  struct range_lock *mmrange)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mmu_gather *tlb = walk-&gt;private;</span>
<span class="quote">&gt; @@ -442,7 +448,8 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static void madvise_free_page_range(struct mmu_gather *tlb,</span>
<span class="quote">&gt;  			     struct vm_area_struct *vma,</span>
<span class="quote">&gt; -			     unsigned long addr, unsigned long end)</span>
<span class="quote">&gt; +			     unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; +			     struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_walk free_walk = {</span>
<span class="quote">&gt;  		.pmd_entry = madvise_free_pte_range,</span>
<span class="quote">&gt; @@ -451,12 +458,14 @@ static void madvise_free_page_range(struct mmu_gather *tlb,</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	tlb_start_vma(tlb, vma);</span>
<span class="quote">&gt; -	walk_page_range(addr, end, &amp;free_walk);</span>
<span class="quote">&gt; +	walk_page_range(addr, end, &amp;free_walk, mmrange);</span>
<span class="quote">&gt;  	tlb_end_vma(tlb, vma);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt; -			unsigned long start_addr, unsigned long end_addr)</span>
<span class="quote">&gt; +				   unsigned long start_addr,</span>
<span class="quote">&gt; +				   unsigned long end_addr,</span>
<span class="quote">&gt; +				   struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long start, end;</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; @@ -478,7 +487,7 @@ static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	update_hiwater_rss(mm);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_start(mm, start, end);</span>
<span class="quote">&gt; -	madvise_free_page_range(&amp;tlb, vma, start, end);</span>
<span class="quote">&gt; +	madvise_free_page_range(&amp;tlb, vma, start, end, mmrange);</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt;  	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -514,7 +523,7 @@ static long madvise_dontneed_single_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  static long madvise_dontneed_free(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				  struct vm_area_struct **prev,</span>
<span class="quote">&gt;  				  unsigned long start, unsigned long end,</span>
<span class="quote">&gt; -				  int behavior)</span>
<span class="quote">&gt; +				  int behavior, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	*prev = vma;</span>
<span class="quote">&gt;  	if (!can_madv_dontneed_vma(vma))</span>
<span class="quote">&gt; @@ -562,7 +571,7 @@ static long madvise_dontneed_free(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if (behavior == MADV_DONTNEED)</span>
<span class="quote">&gt;  		return madvise_dontneed_single_vma(vma, start, end);</span>
<span class="quote">&gt;  	else if (behavior == MADV_FREE)</span>
<span class="quote">&gt; -		return madvise_free_single_vma(vma, start, end);</span>
<span class="quote">&gt; +		return madvise_free_single_vma(vma, start, end, mmrange);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  		return -EINVAL;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -676,18 +685,21 @@ static int madvise_inject_error(int behavior,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static long</span>
<span class="quote">&gt;  madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
<span class="quote">&gt; -		unsigned long start, unsigned long end, int behavior)</span>
<span class="quote">&gt; +	    unsigned long start, unsigned long end, int behavior,</span>
<span class="quote">&gt; +	    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	switch (behavior) {</span>
<span class="quote">&gt;  	case MADV_REMOVE:</span>
<span class="quote">&gt;  		return madvise_remove(vma, prev, start, end);</span>
<span class="quote">&gt;  	case MADV_WILLNEED:</span>
<span class="quote">&gt; -		return madvise_willneed(vma, prev, start, end);</span>
<span class="quote">&gt; +		return madvise_willneed(vma, prev, start, end, mmrange);</span>
<span class="quote">&gt;  	case MADV_FREE:</span>
<span class="quote">&gt;  	case MADV_DONTNEED:</span>
<span class="quote">&gt; -		return madvise_dontneed_free(vma, prev, start, end, behavior);</span>
<span class="quote">&gt; +		return madvise_dontneed_free(vma, prev, start, end, behavior,</span>
<span class="quote">&gt; +					     mmrange);</span>
<span class="quote">&gt;  	default:</span>
<span class="quote">&gt; -		return madvise_behavior(vma, prev, start, end, behavior);</span>
<span class="quote">&gt; +		return madvise_behavior(vma, prev, start, end, behavior,</span>
<span class="quote">&gt; +					mmrange);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -797,7 +809,7 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
<span class="quote">&gt;  	int write;</span>
<span class="quote">&gt;  	size_t len;</span>
<span class="quote">&gt;  	struct blk_plug plug;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt;  	if (!madvise_behavior_valid(behavior))</span>
<span class="quote">&gt;  		return error;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -860,7 +872,7 @@ SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
<span class="quote">&gt;  			tmp = end;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		/* Here vma-&gt;vm_start &lt;= start &lt; tmp &lt;= (end|vma-&gt;vm_end). */</span>
<span class="quote">&gt; -		error = madvise_vma(vma, &amp;prev, start, tmp, behavior);</span>
<span class="quote">&gt; +		error = madvise_vma(vma, &amp;prev, start, tmp, behavior, &amp;mmrange);</span>
<span class="quote">&gt;  		if (error)</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		start = tmp;</span>
<span class="quote">&gt; diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="quote">&gt; index 88c1af32fd67..a7ac5a14b22e 100644</span>
<span class="quote">&gt; --- a/mm/memcontrol.c</span>
<span class="quote">&gt; +++ b/mm/memcontrol.c</span>
<span class="quote">&gt; @@ -4881,7 +4881,8 @@ static inline enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,</span>
<span class="quote">&gt;  					unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -					struct mm_walk *walk)</span>
<span class="quote">&gt; +					struct mm_walk *walk,</span>
<span class="quote">&gt; +					struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt;  	pte_t *pte;</span>
<span class="quote">&gt; @@ -4915,6 +4916,7 @@ static int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,</span>
<span class="quote">&gt;  static unsigned long mem_cgroup_count_precharge(struct mm_struct *mm)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long precharge;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	struct mm_walk mem_cgroup_count_precharge_walk = {</span>
<span class="quote">&gt;  		.pmd_entry = mem_cgroup_count_precharge_pte_range,</span>
<span class="quote">&gt; @@ -4922,7 +4924,7 @@ static unsigned long mem_cgroup_count_precharge(struct mm_struct *mm)</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	walk_page_range(0, mm-&gt;highest_vm_end,</span>
<span class="quote">&gt; -			&amp;mem_cgroup_count_precharge_walk);</span>
<span class="quote">&gt; +			&amp;mem_cgroup_count_precharge_walk, &amp;mmrange);</span>
<span class="quote">&gt;  	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	precharge = mc.precharge;</span>
<span class="quote">&gt; @@ -5081,7 +5083,8 @@ static void mem_cgroup_cancel_attach(struct cgroup_taskset *tset)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int mem_cgroup_move_charge_pte_range(pmd_t *pmd,</span>
<span class="quote">&gt;  				unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -				struct mm_walk *walk)</span>
<span class="quote">&gt; +				struct mm_walk *walk,</span>
<span class="quote">&gt; +				struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret = 0;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt; @@ -5197,6 +5200,7 @@ static void mem_cgroup_move_charge(void)</span>
<span class="quote">&gt;  		.pmd_entry = mem_cgroup_move_charge_pte_range,</span>
<span class="quote">&gt;  		.mm = mc.mm,</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	lru_add_drain_all();</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -5223,7 +5227,8 @@ static void mem_cgroup_move_charge(void)</span>
<span class="quote">&gt;  	 * When we have consumed all precharges and failed in doing</span>
<span class="quote">&gt;  	 * additional charge, the page walk just aborts.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	walk_page_range(0, mc.mm-&gt;highest_vm_end, &amp;mem_cgroup_move_charge_walk);</span>
<span class="quote">&gt; +	walk_page_range(0, mc.mm-&gt;highest_vm_end, &amp;mem_cgroup_move_charge_walk,</span>
<span class="quote">&gt; +			&amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	up_read(&amp;mc.mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	atomic_dec(&amp;mc.from-&gt;moving_account);</span>
<span class="quote">&gt; diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="quote">&gt; index 5ec6433d6a5c..b3561a052939 100644</span>
<span class="quote">&gt; --- a/mm/memory.c</span>
<span class="quote">&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt; @@ -4021,7 +4021,7 @@ static int handle_pte_fault(struct vm_fault *vmf)</span>
<span class="quote">&gt;   * return value.  See filemap_fault() and __lock_page_or_retry().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; -		unsigned int flags)</span>
<span class="quote">&gt; +			     unsigned int flags, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_fault vmf = {</span>
<span class="quote">&gt;  		.vma = vma,</span>
<span class="quote">&gt; @@ -4029,6 +4029,7 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  		.flags = flags,</span>
<span class="quote">&gt;  		.pgoff = linear_page_index(vma, address),</span>
<span class="quote">&gt;  		.gfp_mask = __get_fault_gfp_mask(vma),</span>
<span class="quote">&gt; +		.lockrange = mmrange,</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt;  	unsigned int dirty = flags &amp; FAULT_FLAG_WRITE;</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; @@ -4110,7 +4111,7 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;   * return value.  See filemap_fault() and __lock_page_or_retry().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; -		unsigned int flags)</span>
<span class="quote">&gt; +		    unsigned int flags, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -4137,7 +4138,7 @@ int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  	if (unlikely(is_vm_hugetlb_page(vma)))</span>
<span class="quote">&gt;  		ret = hugetlb_fault(vma-&gt;vm_mm, vma, address, flags);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		ret = __handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +		ret = __handle_mm_fault(vma, address, flags, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (flags &amp; FAULT_FLAG_USER) {</span>
<span class="quote">&gt;  		mem_cgroup_oom_disable();</span>
<span class="quote">&gt; @@ -4425,6 +4426,7 @@ int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	void *old_buf = buf;</span>
<span class="quote">&gt;  	int write = gup_flags &amp; FOLL_WRITE;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	/* ignore errors, just check how much was successfully transferred */</span>
<span class="quote">&gt; @@ -4434,7 +4436,7 @@ int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  		struct page *page = NULL;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		ret = get_user_pages_remote(tsk, mm, addr, 1,</span>
<span class="quote">&gt; -				gup_flags, &amp;page, &amp;vma, NULL);</span>
<span class="quote">&gt; +					    gup_flags, &amp;page, &amp;vma, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  		if (ret &lt;= 0) {</span>
<span class="quote">&gt;  #ifndef CONFIG_HAVE_IOREMAP_PROT</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="quote">&gt; index a8b7d59002e8..001dc176abc1 100644</span>
<span class="quote">&gt; --- a/mm/mempolicy.c</span>
<span class="quote">&gt; +++ b/mm/mempolicy.c</span>
<span class="quote">&gt; @@ -467,7 +467,8 @@ static int queue_pages_pmd(pmd_t *pmd, spinlock_t *ptl, unsigned long addr,</span>
<span class="quote">&gt;   * and move them to the pagelist if they do.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int queue_pages_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; -			unsigned long end, struct mm_walk *walk)</span>
<span class="quote">&gt; +				 unsigned long end, struct mm_walk *walk,</span>
<span class="quote">&gt; +				 struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt; @@ -618,7 +619,7 @@ static int queue_pages_test_walk(unsigned long start, unsigned long end,</span>
<span class="quote">&gt;  static int</span>
<span class="quote">&gt;  queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,</span>
<span class="quote">&gt;  		nodemask_t *nodes, unsigned long flags,</span>
<span class="quote">&gt; -		struct list_head *pagelist)</span>
<span class="quote">&gt; +		struct list_head *pagelist, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct queue_pages qp = {</span>
<span class="quote">&gt;  		.pagelist = pagelist,</span>
<span class="quote">&gt; @@ -634,7 +635,7 @@ queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,</span>
<span class="quote">&gt;  		.private = &amp;qp,</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	return walk_page_range(start, end, &amp;queue_pages_walk);</span>
<span class="quote">&gt; +	return walk_page_range(start, end, &amp;queue_pages_walk, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -675,7 +676,8 @@ static int vma_replace_policy(struct vm_area_struct *vma,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /* Step 2: apply policy to a range and do splits. */</span>
<span class="quote">&gt;  static int mbind_range(struct mm_struct *mm, unsigned long start,</span>
<span class="quote">&gt; -		       unsigned long end, struct mempolicy *new_pol)</span>
<span class="quote">&gt; +		       unsigned long end, struct mempolicy *new_pol,</span>
<span class="quote">&gt; +		       struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *next;</span>
<span class="quote">&gt;  	struct vm_area_struct *prev;</span>
<span class="quote">&gt; @@ -705,7 +707,7 @@ static int mbind_range(struct mm_struct *mm, unsigned long start,</span>
<span class="quote">&gt;  			((vmstart - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt;  		prev = vma_merge(mm, prev, vmstart, vmend, vma-&gt;vm_flags,</span>
<span class="quote">&gt;  				 vma-&gt;anon_vma, vma-&gt;vm_file, pgoff,</span>
<span class="quote">&gt; -				 new_pol, vma-&gt;vm_userfaultfd_ctx);</span>
<span class="quote">&gt; +				 new_pol, vma-&gt;vm_userfaultfd_ctx, mmrange);</span>
<span class="quote">&gt;  		if (prev) {</span>
<span class="quote">&gt;  			vma = prev;</span>
<span class="quote">&gt;  			next = vma-&gt;vm_next;</span>
<span class="quote">&gt; @@ -715,12 +717,12 @@ static int mbind_range(struct mm_struct *mm, unsigned long start,</span>
<span class="quote">&gt;  			goto replace;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (vma-&gt;vm_start != vmstart) {</span>
<span class="quote">&gt; -			err = split_vma(vma-&gt;vm_mm, vma, vmstart, 1);</span>
<span class="quote">&gt; +			err = split_vma(vma-&gt;vm_mm, vma, vmstart, 1, mmrange);</span>
<span class="quote">&gt;  			if (err)</span>
<span class="quote">&gt;  				goto out;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (vma-&gt;vm_end != vmend) {</span>
<span class="quote">&gt; -			err = split_vma(vma-&gt;vm_mm, vma, vmend, 0);</span>
<span class="quote">&gt; +			err = split_vma(vma-&gt;vm_mm, vma, vmend, 0, mmrange);</span>
<span class="quote">&gt;  			if (err)</span>
<span class="quote">&gt;  				goto out;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -797,12 +799,12 @@ static void get_policy_nodemask(struct mempolicy *p, nodemask_t *nodes)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static int lookup_node(unsigned long addr)</span>
<span class="quote">&gt; +static int lookup_node(unsigned long addr, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *p;</span>
<span class="quote">&gt;  	int err;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	err = get_user_pages(addr &amp; PAGE_MASK, 1, 0, &amp;p, NULL);</span>
<span class="quote">&gt; +	err = get_user_pages(addr &amp; PAGE_MASK, 1, 0, &amp;p, NULL, mmrange);</span>
<span class="quote">&gt;  	if (err &gt;= 0) {</span>
<span class="quote">&gt;  		err = page_to_nid(p);</span>
<span class="quote">&gt;  		put_page(p);</span>
<span class="quote">&gt; @@ -818,6 +820,7 @@ static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = NULL;</span>
<span class="quote">&gt;  	struct mempolicy *pol = current-&gt;mempolicy;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (flags &amp;</span>
<span class="quote">&gt;  		~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))</span>
<span class="quote">&gt; @@ -857,7 +860,7 @@ static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (flags &amp; MPOL_F_NODE) {</span>
<span class="quote">&gt;  		if (flags &amp; MPOL_F_ADDR) {</span>
<span class="quote">&gt; -			err = lookup_node(addr);</span>
<span class="quote">&gt; +			err = lookup_node(addr, &amp;mmrange);</span>
<span class="quote">&gt;  			if (err &lt; 0)</span>
<span class="quote">&gt;  				goto out;</span>
<span class="quote">&gt;  			*policy = err;</span>
<span class="quote">&gt; @@ -943,7 +946,7 @@ struct page *alloc_new_node_page(struct page *page, unsigned long node)</span>
<span class="quote">&gt;   * Returns error or the number of pages not migrated.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int migrate_to_node(struct mm_struct *mm, int source, int dest,</span>
<span class="quote">&gt; -			   int flags)</span>
<span class="quote">&gt; +			   int flags, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	nodemask_t nmask;</span>
<span class="quote">&gt;  	LIST_HEAD(pagelist);</span>
<span class="quote">&gt; @@ -959,7 +962,7 @@ static int migrate_to_node(struct mm_struct *mm, int source, int dest,</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	VM_BUG_ON(!(flags &amp; (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)));</span>
<span class="quote">&gt;  	queue_pages_range(mm, mm-&gt;mmap-&gt;vm_start, mm-&gt;task_size, &amp;nmask,</span>
<span class="quote">&gt; -			flags | MPOL_MF_DISCONTIG_OK, &amp;pagelist);</span>
<span class="quote">&gt; +			  flags | MPOL_MF_DISCONTIG_OK, &amp;pagelist, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (!list_empty(&amp;pagelist)) {</span>
<span class="quote">&gt;  		err = migrate_pages(&amp;pagelist, alloc_new_node_page, NULL, dest,</span>
<span class="quote">&gt; @@ -983,6 +986,7 @@ int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,</span>
<span class="quote">&gt;  	int busy = 0;</span>
<span class="quote">&gt;  	int err;</span>
<span class="quote">&gt;  	nodemask_t tmp;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	err = migrate_prep();</span>
<span class="quote">&gt;  	if (err)</span>
<span class="quote">&gt; @@ -1063,7 +1067,7 @@ int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		node_clear(source, tmp);</span>
<span class="quote">&gt; -		err = migrate_to_node(mm, source, dest, flags);</span>
<span class="quote">&gt; +		err = migrate_to_node(mm, source, dest, flags, &amp;mmrange);</span>
<span class="quote">&gt;  		if (err &gt; 0)</span>
<span class="quote">&gt;  			busy += err;</span>
<span class="quote">&gt;  		if (err &lt; 0)</span>
<span class="quote">&gt; @@ -1143,6 +1147,7 @@ static long do_mbind(unsigned long start, unsigned long len,</span>
<span class="quote">&gt;  	unsigned long end;</span>
<span class="quote">&gt;  	int err;</span>
<span class="quote">&gt;  	LIST_HEAD(pagelist);</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (flags &amp; ~(unsigned long)MPOL_MF_VALID)</span>
<span class="quote">&gt;  		return -EINVAL;</span>
<span class="quote">&gt; @@ -1204,9 +1209,9 @@ static long do_mbind(unsigned long start, unsigned long len,</span>
<span class="quote">&gt;  		goto mpol_out;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	err = queue_pages_range(mm, start, end, nmask,</span>
<span class="quote">&gt; -			  flags | MPOL_MF_INVERT, &amp;pagelist);</span>
<span class="quote">&gt; +				flags | MPOL_MF_INVERT, &amp;pagelist, &amp;mmrange);</span>
<span class="quote">&gt;  	if (!err)</span>
<span class="quote">&gt; -		err = mbind_range(mm, start, end, new);</span>
<span class="quote">&gt; +		err = mbind_range(mm, start, end, new, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (!err) {</span>
<span class="quote">&gt;  		int nr_failed = 0;</span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index 5d0dc7b85f90..7a6afc34dd54 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -2105,7 +2105,8 @@ struct migrate_vma {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int migrate_vma_collect_hole(unsigned long start,</span>
<span class="quote">&gt;  				    unsigned long end,</span>
<span class="quote">&gt; -				    struct mm_walk *walk)</span>
<span class="quote">&gt; +				    struct mm_walk *walk,</span>
<span class="quote">&gt; +				    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct migrate_vma *migrate = walk-&gt;private;</span>
<span class="quote">&gt;  	unsigned long addr;</span>
<span class="quote">&gt; @@ -2138,7 +2139,8 @@ static int migrate_vma_collect_skip(unsigned long start,</span>
<span class="quote">&gt;  static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt;  				   unsigned long start,</span>
<span class="quote">&gt;  				   unsigned long end,</span>
<span class="quote">&gt; -				   struct mm_walk *walk)</span>
<span class="quote">&gt; +				   struct mm_walk *walk,</span>
<span class="quote">&gt; +				   struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct migrate_vma *migrate = walk-&gt;private;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt; @@ -2149,7 +2151,7 @@ static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  again:</span>
<span class="quote">&gt;  	if (pmd_none(*pmdp))</span>
<span class="quote">&gt; -		return migrate_vma_collect_hole(start, end, walk);</span>
<span class="quote">&gt; +		return migrate_vma_collect_hole(start, end, walk, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (pmd_trans_huge(*pmdp)) {</span>
<span class="quote">&gt;  		struct page *page;</span>
<span class="quote">&gt; @@ -2183,7 +2185,7 @@ static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt;  								walk);</span>
<span class="quote">&gt;  			if (pmd_none(*pmdp))</span>
<span class="quote">&gt;  				return migrate_vma_collect_hole(start, end,</span>
<span class="quote">&gt; -								walk);</span>
<span class="quote">&gt; +								walk, mmrange);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -2309,7 +2311,8 @@ static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
<span class="quote">&gt;   * valid page, it updates the src array and takes a reference on the page, in</span>
<span class="quote">&gt;   * order to pin the page until we lock it and unmap it.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static void migrate_vma_collect(struct migrate_vma *migrate)</span>
<span class="quote">&gt; +static void migrate_vma_collect(struct migrate_vma *migrate,</span>
<span class="quote">&gt; +				struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_walk mm_walk;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -2325,7 +2328,7 @@ static void migrate_vma_collect(struct migrate_vma *migrate)</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_start(mm_walk.mm,</span>
<span class="quote">&gt;  					    migrate-&gt;start,</span>
<span class="quote">&gt;  					    migrate-&gt;end);</span>
<span class="quote">&gt; -	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="quote">&gt; +	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk, mmrange);</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(mm_walk.mm,</span>
<span class="quote">&gt;  					  migrate-&gt;start,</span>
<span class="quote">&gt;  					  migrate-&gt;end);</span>
<span class="quote">&gt; @@ -2891,7 +2894,8 @@ int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="quote">&gt;  		unsigned long end,</span>
<span class="quote">&gt;  		unsigned long *src,</span>
<span class="quote">&gt;  		unsigned long *dst,</span>
<span class="quote">&gt; -		void *private)</span>
<span class="quote">&gt; +		void *private,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct migrate_vma migrate;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -2917,7 +2921,7 @@ int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="quote">&gt;  	migrate.vma = vma;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Collect, and try to unmap source pages */</span>
<span class="quote">&gt; -	migrate_vma_collect(&amp;migrate);</span>
<span class="quote">&gt; +	migrate_vma_collect(&amp;migrate, mmrange);</span>
<span class="quote">&gt;  	if (!migrate.cpages)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/mincore.c b/mm/mincore.c</span>
<span class="quote">&gt; index fc37afe226e6..a6875a34aac0 100644</span>
<span class="quote">&gt; --- a/mm/mincore.c</span>
<span class="quote">&gt; +++ b/mm/mincore.c</span>
<span class="quote">&gt; @@ -85,7 +85,9 @@ static unsigned char mincore_page(struct address_space *mapping, pgoff_t pgoff)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int __mincore_unmapped_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -				struct vm_area_struct *vma, unsigned char *vec)</span>
<span class="quote">&gt; +				    struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				    unsigned char *vec,</span>
<span class="quote">&gt; +				    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long nr = (end - addr) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt; @@ -104,15 +106,17 @@ static int __mincore_unmapped_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int mincore_unmapped_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -				   struct mm_walk *walk)</span>
<span class="quote">&gt; +				  struct mm_walk *walk,</span>
<span class="quote">&gt; +				  struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	walk-&gt;private += __mincore_unmapped_range(addr, end,</span>
<span class="quote">&gt; -						  walk-&gt;vma, walk-&gt;private);</span>
<span class="quote">&gt; +						  walk-&gt;vma,</span>
<span class="quote">&gt; +						  walk-&gt;private, mmrange);</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -			struct mm_walk *walk)</span>
<span class="quote">&gt; +			     struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	spinlock_t *ptl;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt; @@ -128,7 +132,7 @@ static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (pmd_trans_unstable(pmd)) {</span>
<span class="quote">&gt; -		__mincore_unmapped_range(addr, end, vma, vec);</span>
<span class="quote">&gt; +		__mincore_unmapped_range(addr, end, vma, vec, mmrange);</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -138,7 +142,7 @@ static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (pte_none(pte))</span>
<span class="quote">&gt;  			__mincore_unmapped_range(addr, addr + PAGE_SIZE,</span>
<span class="quote">&gt; -						 vma, vec);</span>
<span class="quote">&gt; +						 vma, vec, mmrange);</span>
<span class="quote">&gt;  		else if (pte_present(pte))</span>
<span class="quote">&gt;  			*vec = 1;</span>
<span class="quote">&gt;  		else { /* pte is a swap entry */</span>
<span class="quote">&gt; @@ -174,7 +178,8 @@ static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;   * all the arguments, we hold the mmap semaphore: we should</span>
<span class="quote">&gt;   * just return the amount of info we&#39;re asked for.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static long do_mincore(unsigned long addr, unsigned long pages, unsigned char *vec)</span>
<span class="quote">&gt; +static long do_mincore(unsigned long addr, unsigned long pages,</span>
<span class="quote">&gt; +		       unsigned char *vec, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	unsigned long end;</span>
<span class="quote">&gt; @@ -191,7 +196,7 @@ static long do_mincore(unsigned long addr, unsigned long pages, unsigned char *v</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt;  	mincore_walk.mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	end = min(vma-&gt;vm_end, addr + (pages &lt;&lt; PAGE_SHIFT));</span>
<span class="quote">&gt; -	err = walk_page_range(addr, end, &amp;mincore_walk);</span>
<span class="quote">&gt; +	err = walk_page_range(addr, end, &amp;mincore_walk, mmrange);</span>
<span class="quote">&gt;  	if (err &lt; 0)</span>
<span class="quote">&gt;  		return err;</span>
<span class="quote">&gt;  	return (end - addr) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; @@ -227,6 +232,7 @@ SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,</span>
<span class="quote">&gt;  	long retval;</span>
<span class="quote">&gt;  	unsigned long pages;</span>
<span class="quote">&gt;  	unsigned char *tmp;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Check the start address: needs to be page-aligned.. */</span>
<span class="quote">&gt;  	if (start &amp; ~PAGE_MASK)</span>
<span class="quote">&gt; @@ -254,7 +260,7 @@ SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,</span>
<span class="quote">&gt;  		 * the temporary buffer size.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; -		retval = do_mincore(start, min(pages, PAGE_SIZE), tmp);</span>
<span class="quote">&gt; +		retval = do_mincore(start, min(pages, PAGE_SIZE), tmp, &amp;mmrange);</span>
<span class="quote">&gt;  		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (retval &lt;= 0)</span>
<span class="quote">&gt; diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="quote">&gt; index 74e5a6547c3d..3f6bd953e8b0 100644</span>
<span class="quote">&gt; --- a/mm/mlock.c</span>
<span class="quote">&gt; +++ b/mm/mlock.c</span>
<span class="quote">&gt; @@ -517,7 +517,8 @@ void munlock_vma_pages_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt;   * For vmas that pass the filters, merge/split as appropriate.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
<span class="quote">&gt; -	unsigned long start, unsigned long end, vm_flags_t newflags)</span>
<span class="quote">&gt; +	unsigned long start, unsigned long end, vm_flags_t newflags,</span>
<span class="quote">&gt; +	struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	pgoff_t pgoff;</span>
<span class="quote">&gt; @@ -534,20 +535,20 @@ static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
<span class="quote">&gt;  	pgoff = vma-&gt;vm_pgoff + ((start - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt;  	*prev = vma_merge(mm, *prev, start, end, newflags, vma-&gt;anon_vma,</span>
<span class="quote">&gt;  			  vma-&gt;vm_file, pgoff, vma_policy(vma),</span>
<span class="quote">&gt; -			  vma-&gt;vm_userfaultfd_ctx);</span>
<span class="quote">&gt; +			  vma-&gt;vm_userfaultfd_ctx, mmrange);</span>
<span class="quote">&gt;  	if (*prev) {</span>
<span class="quote">&gt;  		vma = *prev;</span>
<span class="quote">&gt;  		goto success;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (start != vma-&gt;vm_start) {</span>
<span class="quote">&gt; -		ret = split_vma(mm, vma, start, 1);</span>
<span class="quote">&gt; +		ret = split_vma(mm, vma, start, 1, mmrange);</span>
<span class="quote">&gt;  		if (ret)</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (end != vma-&gt;vm_end) {</span>
<span class="quote">&gt; -		ret = split_vma(mm, vma, end, 0);</span>
<span class="quote">&gt; +		ret = split_vma(mm, vma, end, 0, mmrange);</span>
<span class="quote">&gt;  		if (ret)</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -580,7 +581,7 @@ static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int apply_vma_lock_flags(unsigned long start, size_t len,</span>
<span class="quote">&gt; -				vm_flags_t flags)</span>
<span class="quote">&gt; +				vm_flags_t flags, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long nstart, end, tmp;</span>
<span class="quote">&gt;  	struct vm_area_struct * vma, * prev;</span>
<span class="quote">&gt; @@ -610,7 +611,7 @@ static int apply_vma_lock_flags(unsigned long start, size_t len,</span>
<span class="quote">&gt;  		tmp = vma-&gt;vm_end;</span>
<span class="quote">&gt;  		if (tmp &gt; end)</span>
<span class="quote">&gt;  			tmp = end;</span>
<span class="quote">&gt; -		error = mlock_fixup(vma, &amp;prev, nstart, tmp, newflags);</span>
<span class="quote">&gt; +		error = mlock_fixup(vma, &amp;prev, nstart, tmp, newflags, mmrange);</span>
<span class="quote">&gt;  		if (error)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  		nstart = tmp;</span>
<span class="quote">&gt; @@ -667,11 +668,13 @@ static int count_mm_mlocked_page_nr(struct mm_struct *mm,</span>
<span class="quote">&gt;  	return count &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t flags)</span>
<span class="quote">&gt; +static __must_check int do_mlock(unsigned long start, size_t len,</span>
<span class="quote">&gt; +				 vm_flags_t flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long locked;</span>
<span class="quote">&gt;  	unsigned long lock_limit;</span>
<span class="quote">&gt;  	int error = -ENOMEM;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (!can_do_mlock())</span>
<span class="quote">&gt;  		return -EPERM;</span>
<span class="quote">&gt; @@ -700,7 +703,7 @@ static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* check against resource limits */</span>
<span class="quote">&gt;  	if ((locked &lt;= lock_limit) || capable(CAP_IPC_LOCK))</span>
<span class="quote">&gt; -		error = apply_vma_lock_flags(start, len, flags);</span>
<span class="quote">&gt; +		error = apply_vma_lock_flags(start, len, flags, &amp;mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	if (error)</span>
<span class="quote">&gt; @@ -733,13 +736,14 @@ SYSCALL_DEFINE3(mlock2, unsigned long, start, size_t, len, int, flags)</span>
<span class="quote">&gt;  SYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	len = PAGE_ALIGN(len + (offset_in_page(start)));</span>
<span class="quote">&gt;  	start &amp;= PAGE_MASK;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="quote">&gt;  		return -EINTR;</span>
<span class="quote">&gt; -	ret = apply_vma_lock_flags(start, len, 0);</span>
<span class="quote">&gt; +	ret = apply_vma_lock_flags(start, len, 0, &amp;mmrange);</span>
<span class="quote">&gt;  	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt; @@ -755,7 +759,7 @@ SYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)</span>
<span class="quote">&gt;   * is called once including the MCL_FUTURE flag and then a second time without</span>
<span class="quote">&gt;   * it, VM_LOCKED and VM_LOCKONFAULT will be cleared from mm-&gt;def_flags.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static int apply_mlockall_flags(int flags)</span>
<span class="quote">&gt; +static int apply_mlockall_flags(int flags, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct * vma, * prev = NULL;</span>
<span class="quote">&gt;  	vm_flags_t to_add = 0;</span>
<span class="quote">&gt; @@ -784,7 +788,8 @@ static int apply_mlockall_flags(int flags)</span>
<span class="quote">&gt;  		newflags |= to_add;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		/* Ignore errors */</span>
<span class="quote">&gt; -		mlock_fixup(vma, &amp;prev, vma-&gt;vm_start, vma-&gt;vm_end, newflags);</span>
<span class="quote">&gt; +		mlock_fixup(vma, &amp;prev, vma-&gt;vm_start, vma-&gt;vm_end, newflags,</span>
<span class="quote">&gt; +			mmrange);</span>
<span class="quote">&gt;  		cond_resched();</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt; @@ -795,6 +800,7 @@ SYSCALL_DEFINE1(mlockall, int, flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long lock_limit;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (!flags || (flags &amp; ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT)))</span>
<span class="quote">&gt;  		return -EINVAL;</span>
<span class="quote">&gt; @@ -811,7 +817,7 @@ SYSCALL_DEFINE1(mlockall, int, flags)</span>
<span class="quote">&gt;  	ret = -ENOMEM;</span>
<span class="quote">&gt;  	if (!(flags &amp; MCL_CURRENT) || (current-&gt;mm-&gt;total_vm &lt;= lock_limit) ||</span>
<span class="quote">&gt;  	    capable(CAP_IPC_LOCK))</span>
<span class="quote">&gt; -		ret = apply_mlockall_flags(flags);</span>
<span class="quote">&gt; +		ret = apply_mlockall_flags(flags, &amp;mmrange);</span>
<span class="quote">&gt;  	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	if (!ret &amp;&amp; (flags &amp; MCL_CURRENT))</span>
<span class="quote">&gt;  		mm_populate(0, TASK_SIZE);</span>
<span class="quote">&gt; @@ -822,10 +828,11 @@ SYSCALL_DEFINE1(mlockall, int, flags)</span>
<span class="quote">&gt;  SYSCALL_DEFINE0(munlockall)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))</span>
<span class="quote">&gt;  		return -EINTR;</span>
<span class="quote">&gt; -	ret = apply_mlockall_flags(0);</span>
<span class="quote">&gt; +	ret = apply_mlockall_flags(0, &amp;mmrange);</span>
<span class="quote">&gt;  	up_write(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="quote">&gt; index 4bb038e7984b..f61d49cb791e 100644</span>
<span class="quote">&gt; --- a/mm/mmap.c</span>
<span class="quote">&gt; +++ b/mm/mmap.c</span>
<span class="quote">&gt; @@ -177,7 +177,8 @@ static struct vm_area_struct *remove_vma(struct vm_area_struct *vma)</span>
<span class="quote">&gt;  	return next;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static int do_brk(unsigned long addr, unsigned long len, struct list_head *uf);</span>
<span class="quote">&gt; +static int do_brk(unsigned long addr, unsigned long len, struct list_head *uf,</span>
<span class="quote">&gt; +		  struct range_lock *mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -188,6 +189,7 @@ SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
<span class="quote">&gt;  	unsigned long min_brk;</span>
<span class="quote">&gt;  	bool populate;</span>
<span class="quote">&gt;  	LIST_HEAD(uf);</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="quote">&gt;  		return -EINTR;</span>
<span class="quote">&gt; @@ -225,7 +227,7 @@ SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Always allow shrinking brk. */</span>
<span class="quote">&gt;  	if (brk &lt;= mm-&gt;brk) {</span>
<span class="quote">&gt; -		if (!do_munmap(mm, newbrk, oldbrk-newbrk, &amp;uf))</span>
<span class="quote">&gt; +		if (!do_munmap(mm, newbrk, oldbrk-newbrk, &amp;uf, &amp;mmrange))</span>
<span class="quote">&gt;  			goto set_brk;</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -236,7 +238,7 @@ SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Ok, looks good - let it rip. */</span>
<span class="quote">&gt; -	if (do_brk(oldbrk, newbrk-oldbrk, &amp;uf) &lt; 0)</span>
<span class="quote">&gt; +	if (do_brk(oldbrk, newbrk-oldbrk, &amp;uf, &amp;mmrange) &lt; 0)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  set_brk:</span>
<span class="quote">&gt; @@ -680,7 +682,7 @@ static inline void __vma_unlink_prev(struct mm_struct *mm,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int __vma_adjust(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt;  	unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,</span>
<span class="quote">&gt; -	struct vm_area_struct *expand)</span>
<span class="quote">&gt; +	struct vm_area_struct *expand, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	struct vm_area_struct *next = vma-&gt;vm_next, *orig_vma = vma;</span>
<span class="quote">&gt; @@ -887,10 +889,10 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt;  		i_mmap_unlock_write(mapping);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (root) {</span>
<span class="quote">&gt; -		uprobe_mmap(vma);</span>
<span class="quote">&gt; +		uprobe_mmap(vma, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (adjust_next)</span>
<span class="quote">&gt; -			uprobe_mmap(next);</span>
<span class="quote">&gt; +			uprobe_mmap(next, mmrange);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (remove_next) {</span>
<span class="quote">&gt; @@ -960,7 +962,7 @@ int __vma_adjust(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if (insert &amp;&amp; file)</span>
<span class="quote">&gt; -		uprobe_mmap(insert);</span>
<span class="quote">&gt; +		uprobe_mmap(insert, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	validate_mm(mm);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1101,7 +1103,8 @@ struct vm_area_struct *vma_merge(struct mm_struct *mm,</span>
<span class="quote">&gt;  			unsigned long end, unsigned long vm_flags,</span>
<span class="quote">&gt;  			struct anon_vma *anon_vma, struct file *file,</span>
<span class="quote">&gt;  			pgoff_t pgoff, struct mempolicy *policy,</span>
<span class="quote">&gt; -			struct vm_userfaultfd_ctx vm_userfaultfd_ctx)</span>
<span class="quote">&gt; +			struct vm_userfaultfd_ctx vm_userfaultfd_ctx,</span>
<span class="quote">&gt; +			struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pgoff_t pglen = (end - addr) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	struct vm_area_struct *area, *next;</span>
<span class="quote">&gt; @@ -1149,10 +1152,11 @@ struct vm_area_struct *vma_merge(struct mm_struct *mm,</span>
<span class="quote">&gt;  							/* cases 1, 6 */</span>
<span class="quote">&gt;  			err = __vma_adjust(prev, prev-&gt;vm_start,</span>
<span class="quote">&gt;  					 next-&gt;vm_end, prev-&gt;vm_pgoff, NULL,</span>
<span class="quote">&gt; -					 prev);</span>
<span class="quote">&gt; +					 prev, mmrange);</span>
<span class="quote">&gt;  		} else					/* cases 2, 5, 7 */</span>
<span class="quote">&gt;  			err = __vma_adjust(prev, prev-&gt;vm_start,</span>
<span class="quote">&gt; -					 end, prev-&gt;vm_pgoff, NULL, prev);</span>
<span class="quote">&gt; +					   end, prev-&gt;vm_pgoff, NULL,</span>
<span class="quote">&gt; +					   prev, mmrange);</span>
<span class="quote">&gt;  		if (err)</span>
<span class="quote">&gt;  			return NULL;</span>
<span class="quote">&gt;  		khugepaged_enter_vma_merge(prev, vm_flags);</span>
<span class="quote">&gt; @@ -1169,10 +1173,12 @@ struct vm_area_struct *vma_merge(struct mm_struct *mm,</span>
<span class="quote">&gt;  					     vm_userfaultfd_ctx)) {</span>
<span class="quote">&gt;  		if (prev &amp;&amp; addr &lt; prev-&gt;vm_end)	/* case 4 */</span>
<span class="quote">&gt;  			err = __vma_adjust(prev, prev-&gt;vm_start,</span>
<span class="quote">&gt; -					 addr, prev-&gt;vm_pgoff, NULL, next);</span>
<span class="quote">&gt; +					   addr, prev-&gt;vm_pgoff, NULL,</span>
<span class="quote">&gt; +					   next, mmrange);</span>
<span class="quote">&gt;  		else {					/* cases 3, 8 */</span>
<span class="quote">&gt;  			err = __vma_adjust(area, addr, next-&gt;vm_end,</span>
<span class="quote">&gt; -					 next-&gt;vm_pgoff - pglen, NULL, next);</span>
<span class="quote">&gt; +					   next-&gt;vm_pgoff - pglen, NULL,</span>
<span class="quote">&gt; +					   next, mmrange);</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt;  			 * In case 3 area is already equal to next and</span>
<span class="quote">&gt;  			 * this is a noop, but in case 8 &quot;area&quot; has</span>
<span class="quote">&gt; @@ -1322,7 +1328,7 @@ unsigned long do_mmap(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;  			unsigned long len, unsigned long prot,</span>
<span class="quote">&gt;  			unsigned long flags, vm_flags_t vm_flags,</span>
<span class="quote">&gt;  			unsigned long pgoff, unsigned long *populate,</span>
<span class="quote">&gt; -			struct list_head *uf)</span>
<span class="quote">&gt; +		        struct list_head *uf, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	int pkey = 0;</span>
<span class="quote">&gt; @@ -1491,7 +1497,7 @@ unsigned long do_mmap(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;  			vm_flags |= VM_NORESERVE;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	addr = mmap_region(file, addr, len, vm_flags, pgoff, uf);</span>
<span class="quote">&gt; +	addr = mmap_region(file, addr, len, vm_flags, pgoff, uf, mmrange);</span>
<span class="quote">&gt;  	if (!IS_ERR_VALUE(addr) &amp;&amp;</span>
<span class="quote">&gt;  	    ((vm_flags &amp; VM_LOCKED) ||</span>
<span class="quote">&gt;  	     (flags &amp; (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))</span>
<span class="quote">&gt; @@ -1628,7 +1634,7 @@ static inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  unsigned long mmap_region(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;  		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,</span>
<span class="quote">&gt; -		struct list_head *uf)</span>
<span class="quote">&gt; +		struct list_head *uf, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma, *prev;</span>
<span class="quote">&gt; @@ -1654,7 +1660,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;  	/* Clear old maps */</span>
<span class="quote">&gt;  	while (find_vma_links(mm, addr, addr + len, &amp;prev, &amp;rb_link,</span>
<span class="quote">&gt;  			      &amp;rb_parent)) {</span>
<span class="quote">&gt; -		if (do_munmap(mm, addr, len, uf))</span>
<span class="quote">&gt; +		if (do_munmap(mm, addr, len, uf, mmrange))</span>
<span class="quote">&gt;  			return -ENOMEM;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1672,7 +1678,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;  	 * Can we just expand an old mapping?</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	vma = vma_merge(mm, prev, addr, addr + len, vm_flags,</span>
<span class="quote">&gt; -			NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX);</span>
<span class="quote">&gt; +			NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX, mmrange);</span>
<span class="quote">&gt;  	if (vma)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1756,7 +1762,7 @@ unsigned long mmap_region(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (file)</span>
<span class="quote">&gt; -		uprobe_mmap(vma);</span>
<span class="quote">&gt; +		uprobe_mmap(vma, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * New (or expanded) vma always get soft dirty status.</span>
<span class="quote">&gt; @@ -2435,7 +2441,8 @@ int expand_stack(struct vm_area_struct *vma, unsigned long address)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  struct vm_area_struct *</span>
<span class="quote">&gt; -find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
<span class="quote">&gt; +find_extend_vma(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma, *prev;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -2446,7 +2453,8 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
<span class="quote">&gt;  	if (!prev || expand_stack(prev, addr))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  	if (prev-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="quote">&gt; -		populate_vma_page_range(prev, addr, prev-&gt;vm_end, NULL);</span>
<span class="quote">&gt; +		populate_vma_page_range(prev, addr, prev-&gt;vm_end,</span>
<span class="quote">&gt; +					NULL, mmrange);</span>
<span class="quote">&gt;  	return prev;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt; @@ -2456,7 +2464,8 @@ int expand_stack(struct vm_area_struct *vma, unsigned long address)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  struct vm_area_struct *</span>
<span class="quote">&gt; -find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
<span class="quote">&gt; +find_extend_vma(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	unsigned long start;</span>
<span class="quote">&gt; @@ -2473,7 +2482,7 @@ find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
<span class="quote">&gt;  	if (expand_stack(vma, addr))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  	if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="quote">&gt; -		populate_vma_page_range(vma, addr, start, NULL);</span>
<span class="quote">&gt; +		populate_vma_page_range(vma, addr, start, NULL, mmrange);</span>
<span class="quote">&gt;  	return vma;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -2561,7 +2570,7 @@ detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;   * has already been checked or doesn&#39;t make sense to fail.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; -		unsigned long addr, int new_below)</span>
<span class="quote">&gt; +		unsigned long addr, int new_below, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *new;</span>
<span class="quote">&gt;  	int err;</span>
<span class="quote">&gt; @@ -2604,9 +2613,11 @@ int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (new_below)</span>
<span class="quote">&gt;  		err = vma_adjust(vma, addr, vma-&gt;vm_end, vma-&gt;vm_pgoff +</span>
<span class="quote">&gt; -			((addr - new-&gt;vm_start) &gt;&gt; PAGE_SHIFT), new);</span>
<span class="quote">&gt; +			  ((addr - new-&gt;vm_start) &gt;&gt; PAGE_SHIFT), new,</span>
<span class="quote">&gt; +			   mmrange);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		err = vma_adjust(vma, vma-&gt;vm_start, addr, vma-&gt;vm_pgoff, new);</span>
<span class="quote">&gt; +		err = vma_adjust(vma, vma-&gt;vm_start, addr, vma-&gt;vm_pgoff, new,</span>
<span class="quote">&gt; +				 mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Success. */</span>
<span class="quote">&gt;  	if (!err)</span>
<span class="quote">&gt; @@ -2630,12 +2641,12 @@ int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;   * either for the first part or the tail.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; -	      unsigned long addr, int new_below)</span>
<span class="quote">&gt; +	      unsigned long addr, int new_below, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (mm-&gt;map_count &gt;= sysctl_max_map_count)</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	return __split_vma(mm, vma, addr, new_below);</span>
<span class="quote">&gt; +	return __split_vma(mm, vma, addr, new_below, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /* Munmap is split into 2 main parts -- this part which finds</span>
<span class="quote">&gt; @@ -2644,7 +2655,7 @@ int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;   * Jeremy Fitzhardinge &lt;jeremy@goop.org&gt;</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,</span>
<span class="quote">&gt; -	      struct list_head *uf)</span>
<span class="quote">&gt; +	      struct list_head *uf, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long end;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma, *prev, *last;</span>
<span class="quote">&gt; @@ -2686,7 +2697,7 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,</span>
<span class="quote">&gt;  		if (end &lt; vma-&gt;vm_end &amp;&amp; mm-&gt;map_count &gt;= sysctl_max_map_count)</span>
<span class="quote">&gt;  			return -ENOMEM;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -		error = __split_vma(mm, vma, start, 0);</span>
<span class="quote">&gt; +		error = __split_vma(mm, vma, start, 0, mmrange);</span>
<span class="quote">&gt;  		if (error)</span>
<span class="quote">&gt;  			return error;</span>
<span class="quote">&gt;  		prev = vma;</span>
<span class="quote">&gt; @@ -2695,7 +2706,7 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,</span>
<span class="quote">&gt;  	/* Does it split the last one? */</span>
<span class="quote">&gt;  	last = find_vma(mm, end);</span>
<span class="quote">&gt;  	if (last &amp;&amp; end &gt; last-&gt;vm_start) {</span>
<span class="quote">&gt; -		int error = __split_vma(mm, last, end, 1);</span>
<span class="quote">&gt; +		int error = __split_vma(mm, last, end, 1, mmrange);</span>
<span class="quote">&gt;  		if (error)</span>
<span class="quote">&gt;  			return error;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -2736,7 +2747,7 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,</span>
<span class="quote">&gt;  	detach_vmas_to_be_unmapped(mm, vma, prev, end);</span>
<span class="quote">&gt;  	unmap_region(mm, vma, prev, start, end);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	arch_unmap(mm, vma, start, end);</span>
<span class="quote">&gt; +	arch_unmap(mm, vma, start, end, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Fix up all other VM information */</span>
<span class="quote">&gt;  	remove_vma_list(mm, vma);</span>
<span class="quote">&gt; @@ -2749,11 +2760,12 @@ int vm_munmap(unsigned long start, size_t len)</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	LIST_HEAD(uf);</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="quote">&gt;  		return -EINTR;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	ret = do_munmap(mm, start, len, &amp;uf);</span>
<span class="quote">&gt; +	ret = do_munmap(mm, start, len, &amp;uf, &amp;mmrange);</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	userfaultfd_unmap_complete(mm, &amp;uf);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt; @@ -2779,6 +2791,7 @@ SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
<span class="quote">&gt;  	unsigned long populate = 0;</span>
<span class="quote">&gt;  	unsigned long ret = -EINVAL;</span>
<span class="quote">&gt;  	struct file *file;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	pr_warn_once(&quot;%s (%d) uses deprecated remap_file_pages() syscall. See Documentation/vm/remap_file_pages.txt.\n&quot;,</span>
<span class="quote">&gt;  		     current-&gt;comm, current-&gt;pid);</span>
<span class="quote">&gt; @@ -2855,7 +2868,7 @@ SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	file = get_file(vma-&gt;vm_file);</span>
<span class="quote">&gt;  	ret = do_mmap_pgoff(vma-&gt;vm_file, start, size,</span>
<span class="quote">&gt; -			prot, flags, pgoff, &amp;populate, NULL);</span>
<span class="quote">&gt; +			    prot, flags, pgoff, &amp;populate, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  	fput(file);</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; @@ -2881,7 +2894,9 @@ static inline void verify_mm_writelocked(struct mm_struct *mm)</span>
<span class="quote">&gt;   *  anonymous maps.  eventually we may be able to do some</span>
<span class="quote">&gt;   *  brk-specific accounting here.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long flags, struct list_head *uf)</span>
<span class="quote">&gt; +static int do_brk_flags(unsigned long addr, unsigned long request,</span>
<span class="quote">&gt; +			unsigned long flags, struct list_head *uf,</span>
<span class="quote">&gt; +			struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma, *prev;</span>
<span class="quote">&gt; @@ -2920,7 +2935,7 @@ static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	while (find_vma_links(mm, addr, addr + len, &amp;prev, &amp;rb_link,</span>
<span class="quote">&gt;  			      &amp;rb_parent)) {</span>
<span class="quote">&gt; -		if (do_munmap(mm, addr, len, uf))</span>
<span class="quote">&gt; +		if (do_munmap(mm, addr, len, uf, mmrange))</span>
<span class="quote">&gt;  			return -ENOMEM;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -2936,7 +2951,7 @@ static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Can we just expand an old private anonymous mapping? */</span>
<span class="quote">&gt;  	vma = vma_merge(mm, prev, addr, addr + len, flags,</span>
<span class="quote">&gt; -			NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX);</span>
<span class="quote">&gt; +			NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX, mmrange);</span>
<span class="quote">&gt;  	if (vma)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -2967,9 +2982,10 @@ static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static int do_brk(unsigned long addr, unsigned long len, struct list_head *uf)</span>
<span class="quote">&gt; +static int do_brk(unsigned long addr, unsigned long len, struct list_head *uf,</span>
<span class="quote">&gt; +		  struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return do_brk_flags(addr, len, 0, uf);</span>
<span class="quote">&gt; +	return do_brk_flags(addr, len, 0, uf, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  int vm_brk_flags(unsigned long addr, unsigned long len, unsigned long flags)</span>
<span class="quote">&gt; @@ -2978,11 +2994,12 @@ int vm_brk_flags(unsigned long addr, unsigned long len, unsigned long flags)</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  	bool populate;</span>
<span class="quote">&gt;  	LIST_HEAD(uf);</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="quote">&gt;  		return -EINTR;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	ret = do_brk_flags(addr, len, flags, &amp;uf);</span>
<span class="quote">&gt; +	ret = do_brk_flags(addr, len, flags, &amp;uf, &amp;mmrange);</span>
<span class="quote">&gt;  	populate = ((mm-&gt;def_flags &amp; VM_LOCKED) != 0);</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	userfaultfd_unmap_complete(mm, &amp;uf);</span>
<span class="quote">&gt; @@ -3105,7 +3122,7 @@ int insert_vm_struct(struct mm_struct *mm, struct vm_area_struct *vma)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,</span>
<span class="quote">&gt;  	unsigned long addr, unsigned long len, pgoff_t pgoff,</span>
<span class="quote">&gt; -	bool *need_rmap_locks)</span>
<span class="quote">&gt; +	bool *need_rmap_locks, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = *vmap;</span>
<span class="quote">&gt;  	unsigned long vma_start = vma-&gt;vm_start;</span>
<span class="quote">&gt; @@ -3127,7 +3144,7 @@ struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,</span>
<span class="quote">&gt;  		return NULL;	/* should never get here */</span>
<span class="quote">&gt;  	new_vma = vma_merge(mm, prev, addr, addr + len, vma-&gt;vm_flags,</span>
<span class="quote">&gt;  			    vma-&gt;anon_vma, vma-&gt;vm_file, pgoff, vma_policy(vma),</span>
<span class="quote">&gt; -			    vma-&gt;vm_userfaultfd_ctx);</span>
<span class="quote">&gt; +			    vma-&gt;vm_userfaultfd_ctx, mmrange);</span>
<span class="quote">&gt;  	if (new_vma) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Source vma may have been merged into new_vma</span>
<span class="quote">&gt; diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="quote">&gt; index e3309fcf586b..b84a70720319 100644</span>
<span class="quote">&gt; --- a/mm/mprotect.c</span>
<span class="quote">&gt; +++ b/mm/mprotect.c</span>
<span class="quote">&gt; @@ -299,7 +299,8 @@ unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  int</span>
<span class="quote">&gt;  mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
<span class="quote">&gt; -	unsigned long start, unsigned long end, unsigned long newflags)</span>
<span class="quote">&gt; +	       unsigned long start, unsigned long end, unsigned long newflags,</span>
<span class="quote">&gt; +	       struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	unsigned long oldflags = vma-&gt;vm_flags;</span>
<span class="quote">&gt; @@ -340,7 +341,7 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
<span class="quote">&gt;  	pgoff = vma-&gt;vm_pgoff + ((start - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt;  	*pprev = vma_merge(mm, *pprev, start, end, newflags,</span>
<span class="quote">&gt;  			   vma-&gt;anon_vma, vma-&gt;vm_file, pgoff, vma_policy(vma),</span>
<span class="quote">&gt; -			   vma-&gt;vm_userfaultfd_ctx);</span>
<span class="quote">&gt; +			   vma-&gt;vm_userfaultfd_ctx, mmrange);</span>
<span class="quote">&gt;  	if (*pprev) {</span>
<span class="quote">&gt;  		vma = *pprev;</span>
<span class="quote">&gt;  		VM_WARN_ON((vma-&gt;vm_flags ^ newflags) &amp; ~VM_SOFTDIRTY);</span>
<span class="quote">&gt; @@ -350,13 +351,13 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
<span class="quote">&gt;  	*pprev = vma;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (start != vma-&gt;vm_start) {</span>
<span class="quote">&gt; -		error = split_vma(mm, vma, start, 1);</span>
<span class="quote">&gt; +		error = split_vma(mm, vma, start, 1, mmrange);</span>
<span class="quote">&gt;  		if (error)</span>
<span class="quote">&gt;  			goto fail;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (end != vma-&gt;vm_end) {</span>
<span class="quote">&gt; -		error = split_vma(mm, vma, end, 0);</span>
<span class="quote">&gt; +		error = split_vma(mm, vma, end, 0, mmrange);</span>
<span class="quote">&gt;  		if (error)</span>
<span class="quote">&gt;  			goto fail;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -379,7 +380,7 @@ mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if ((oldflags &amp; (VM_WRITE | VM_SHARED | VM_LOCKED)) == VM_LOCKED &amp;&amp;</span>
<span class="quote">&gt;  			(newflags &amp; VM_WRITE)) {</span>
<span class="quote">&gt; -		populate_vma_page_range(vma, start, end, NULL);</span>
<span class="quote">&gt; +		populate_vma_page_range(vma, start, end, NULL, mmrange);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	vm_stat_account(mm, oldflags, -nrpages);</span>
<span class="quote">&gt; @@ -404,6 +405,7 @@ static int do_mprotect_pkey(unsigned long start, size_t len,</span>
<span class="quote">&gt;  	const int grows = prot &amp; (PROT_GROWSDOWN|PROT_GROWSUP);</span>
<span class="quote">&gt;  	const bool rier = (current-&gt;personality &amp; READ_IMPLIES_EXEC) &amp;&amp;</span>
<span class="quote">&gt;  				(prot &amp; PROT_READ);</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	prot &amp;= ~(PROT_GROWSDOWN|PROT_GROWSUP);</span>
<span class="quote">&gt;  	if (grows == (PROT_GROWSDOWN|PROT_GROWSUP)) /* can&#39;t be both */</span>
<span class="quote">&gt; @@ -494,7 +496,7 @@ static int do_mprotect_pkey(unsigned long start, size_t len,</span>
<span class="quote">&gt;  		tmp = vma-&gt;vm_end;</span>
<span class="quote">&gt;  		if (tmp &gt; end)</span>
<span class="quote">&gt;  			tmp = end;</span>
<span class="quote">&gt; -		error = mprotect_fixup(vma, &amp;prev, nstart, tmp, newflags);</span>
<span class="quote">&gt; +		error = mprotect_fixup(vma, &amp;prev, nstart, tmp, newflags, &amp;mmrange);</span>
<span class="quote">&gt;  		if (error)</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		nstart = tmp;</span>
<span class="quote">&gt; diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="quote">&gt; index 049470aa1e3e..21a9e2a2baa2 100644</span>
<span class="quote">&gt; --- a/mm/mremap.c</span>
<span class="quote">&gt; +++ b/mm/mremap.c</span>
<span class="quote">&gt; @@ -264,7 +264,8 @@ static unsigned long move_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		unsigned long old_addr, unsigned long old_len,</span>
<span class="quote">&gt;  		unsigned long new_len, unsigned long new_addr,</span>
<span class="quote">&gt;  		bool *locked, struct vm_userfaultfd_ctx *uf,</span>
<span class="quote">&gt; -		struct list_head *uf_unmap)</span>
<span class="quote">&gt; +		struct list_head *uf_unmap,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	struct vm_area_struct *new_vma;</span>
<span class="quote">&gt; @@ -292,13 +293,13 @@ static unsigned long move_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	 * so KSM can come around to merge on vma and new_vma afterwards.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	err = ksm_madvise(vma, old_addr, old_addr + old_len,</span>
<span class="quote">&gt; -						MADV_UNMERGEABLE, &amp;vm_flags);</span>
<span class="quote">&gt; +			  MADV_UNMERGEABLE, &amp;vm_flags, mmrange);</span>
<span class="quote">&gt;  	if (err)</span>
<span class="quote">&gt;  		return err;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	new_pgoff = vma-&gt;vm_pgoff + ((old_addr - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt;  	new_vma = copy_vma(&amp;vma, new_addr, new_len, new_pgoff,</span>
<span class="quote">&gt; -			   &amp;need_rmap_locks);</span>
<span class="quote">&gt; +			   &amp;need_rmap_locks, mmrange);</span>
<span class="quote">&gt;  	if (!new_vma)</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -353,7 +354,7 @@ static unsigned long move_vma(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if (unlikely(vma-&gt;vm_flags &amp; VM_PFNMAP))</span>
<span class="quote">&gt;  		untrack_pfn_moved(vma);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	if (do_munmap(mm, old_addr, old_len, uf_unmap) &lt; 0) {</span>
<span class="quote">&gt; +	if (do_munmap(mm, old_addr, old_len, uf_unmap, mmrange) &lt; 0) {</span>
<span class="quote">&gt;  		/* OOM: unable to split vma, just get accounts right */</span>
<span class="quote">&gt;  		vm_unacct_memory(excess &gt;&gt; PAGE_SHIFT);</span>
<span class="quote">&gt;  		excess = 0;</span>
<span class="quote">&gt; @@ -444,7 +445,8 @@ static unsigned long mremap_to(unsigned long addr, unsigned long old_len,</span>
<span class="quote">&gt;  		unsigned long new_addr, unsigned long new_len, bool *locked,</span>
<span class="quote">&gt;  		struct vm_userfaultfd_ctx *uf,</span>
<span class="quote">&gt;  		struct list_head *uf_unmap_early,</span>
<span class="quote">&gt; -		struct list_head *uf_unmap)</span>
<span class="quote">&gt; +		struct list_head *uf_unmap,</span>
<span class="quote">&gt; +		struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt; @@ -462,12 +464,13 @@ static unsigned long mremap_to(unsigned long addr, unsigned long old_len,</span>
<span class="quote">&gt;  	if (addr + old_len &gt; new_addr &amp;&amp; new_addr + new_len &gt; addr)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	ret = do_munmap(mm, new_addr, new_len, uf_unmap_early);</span>
<span class="quote">&gt; +	ret = do_munmap(mm, new_addr, new_len, uf_unmap_early, mmrange);</span>
<span class="quote">&gt;  	if (ret)</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (old_len &gt;= new_len) {</span>
<span class="quote">&gt; -		ret = do_munmap(mm, addr+new_len, old_len - new_len, uf_unmap);</span>
<span class="quote">&gt; +		ret = do_munmap(mm, addr+new_len, old_len - new_len,</span>
<span class="quote">&gt; +				uf_unmap, mmrange);</span>
<span class="quote">&gt;  		if (ret &amp;&amp; old_len != new_len)</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		old_len = new_len;</span>
<span class="quote">&gt; @@ -490,7 +493,7 @@ static unsigned long mremap_to(unsigned long addr, unsigned long old_len,</span>
<span class="quote">&gt;  		goto out1;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	ret = move_vma(vma, addr, old_len, new_len, new_addr, locked, uf,</span>
<span class="quote">&gt; -		       uf_unmap);</span>
<span class="quote">&gt; +		       uf_unmap, mmrange);</span>
<span class="quote">&gt;  	if (!(offset_in_page(ret)))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  out1:</span>
<span class="quote">&gt; @@ -532,6 +535,7 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
<span class="quote">&gt;  	struct vm_userfaultfd_ctx uf = NULL_VM_UFFD_CTX;</span>
<span class="quote">&gt;  	LIST_HEAD(uf_unmap_early);</span>
<span class="quote">&gt;  	LIST_HEAD(uf_unmap);</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (flags &amp; ~(MREMAP_FIXED | MREMAP_MAYMOVE))</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt; @@ -558,7 +562,8 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (flags &amp; MREMAP_FIXED) {</span>
<span class="quote">&gt;  		ret = mremap_to(addr, old_len, new_addr, new_len,</span>
<span class="quote">&gt; -				&amp;locked, &amp;uf, &amp;uf_unmap_early, &amp;uf_unmap);</span>
<span class="quote">&gt; +				&amp;locked, &amp;uf, &amp;uf_unmap_early,</span>
<span class="quote">&gt; +				&amp;uf_unmap, &amp;mmrange);</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -568,7 +573,8 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
<span class="quote">&gt;  	 * do_munmap does all the needed commit accounting</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (old_len &gt;= new_len) {</span>
<span class="quote">&gt; -		ret = do_munmap(mm, addr+new_len, old_len - new_len, &amp;uf_unmap);</span>
<span class="quote">&gt; +		ret = do_munmap(mm, addr+new_len, old_len - new_len,</span>
<span class="quote">&gt; +				&amp;uf_unmap, &amp;mmrange);</span>
<span class="quote">&gt;  		if (ret &amp;&amp; old_len != new_len)</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		ret = addr;</span>
<span class="quote">&gt; @@ -592,7 +598,7 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
<span class="quote">&gt;  			int pages = (new_len - old_len) &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  			if (vma_adjust(vma, vma-&gt;vm_start, addr + new_len,</span>
<span class="quote">&gt; -				       vma-&gt;vm_pgoff, NULL)) {</span>
<span class="quote">&gt; +				       vma-&gt;vm_pgoff, NULL, &amp;mmrange)) {</span>
<span class="quote">&gt;  				ret = -ENOMEM;</span>
<span class="quote">&gt;  				goto out;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; @@ -628,7 +634,7 @@ SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		ret = move_vma(vma, addr, old_len, new_len, new_addr,</span>
<span class="quote">&gt; -			       &amp;locked, &amp;uf, &amp;uf_unmap);</span>
<span class="quote">&gt; +			       &amp;locked, &amp;uf, &amp;uf_unmap, &amp;mmrange);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	if (offset_in_page(ret)) {</span>
<span class="quote">&gt; diff --git a/mm/nommu.c b/mm/nommu.c</span>
<span class="quote">&gt; index ebb6e618dade..1805f0a788b3 100644</span>
<span class="quote">&gt; --- a/mm/nommu.c</span>
<span class="quote">&gt; +++ b/mm/nommu.c</span>
<span class="quote">&gt; @@ -113,7 +113,8 @@ unsigned int kobjsize(const void *objp)</span>
<span class="quote">&gt;  static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;  		      unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  		      unsigned int foll_flags, struct page **pages,</span>
<span class="quote">&gt; -		      struct vm_area_struct **vmas, int *nonblocking)</span>
<span class="quote">&gt; +		      struct vm_area_struct **vmas, int *nonblocking,</span>
<span class="quote">&gt; +		      struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	unsigned long vm_flags;</span>
<span class="quote">&gt; @@ -162,18 +163,19 @@ static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  long get_user_pages(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  		    unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; -		    struct vm_area_struct **vmas)</span>
<span class="quote">&gt; +		    struct vm_area_struct **vmas,</span>
<span class="quote">&gt; +		    struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return __get_user_pages(current, current-&gt;mm, start, nr_pages,</span>
<span class="quote">&gt; -				gup_flags, pages, vmas, NULL);</span>
<span class="quote">&gt; +				gup_flags, pages, vmas, NULL, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(get_user_pages);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  long get_user_pages_locked(unsigned long start, unsigned long nr_pages,</span>
<span class="quote">&gt;  			    unsigned int gup_flags, struct page **pages,</span>
<span class="quote">&gt; -			    int *locked)</span>
<span class="quote">&gt; +			    int *locked, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return get_user_pages(start, nr_pages, gup_flags, pages, NULL);</span>
<span class="quote">&gt; +	return get_user_pages(start, nr_pages, gup_flags, pages, NULL, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(get_user_pages_locked);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -183,9 +185,11 @@ static long __get_user_pages_unlocked(struct task_struct *tsk,</span>
<span class="quote">&gt;  			unsigned int gup_flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	long ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	ret = __get_user_pages(tsk, mm, start, nr_pages, gup_flags, pages,</span>
<span class="quote">&gt; -				NULL, NULL);</span>
<span class="quote">&gt; +			       NULL, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -836,7 +840,8 @@ EXPORT_SYMBOL(find_vma);</span>
<span class="quote">&gt;   * find a VMA</span>
<span class="quote">&gt;   * - we don&#39;t extend stack VMAs under NOMMU conditions</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -struct vm_area_struct *find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
<span class="quote">&gt; +struct vm_area_struct *find_extend_vma(struct mm_struct *mm, unsigned long addr,</span>
<span class="quote">&gt; +				       struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return find_vma(mm, addr);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1206,7 +1211,8 @@ unsigned long do_mmap(struct file *file,</span>
<span class="quote">&gt;  			vm_flags_t vm_flags,</span>
<span class="quote">&gt;  			unsigned long pgoff,</span>
<span class="quote">&gt;  			unsigned long *populate,</span>
<span class="quote">&gt; -			struct list_head *uf)</span>
<span class="quote">&gt; +			struct list_head *uf,</span>
<span class="quote">&gt; +			struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	struct vm_region *region;</span>
<span class="quote">&gt; @@ -1476,7 +1482,7 @@ SYSCALL_DEFINE1(old_mmap, struct mmap_arg_struct __user *, arg)</span>
<span class="quote">&gt;   * for the first part or the tail.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt; -	      unsigned long addr, int new_below)</span>
<span class="quote">&gt; +	      unsigned long addr, int new_below, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *new;</span>
<span class="quote">&gt;  	struct vm_region *region;</span>
<span class="quote">&gt; @@ -1578,7 +1584,8 @@ static int shrink_vma(struct mm_struct *mm,</span>
<span class="quote">&gt;   * - under NOMMU conditions the chunk to be unmapped must be backed by a single</span>
<span class="quote">&gt;   *   VMA, though it need not cover the whole VMA</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -int do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list_head *uf)</span>
<span class="quote">&gt; +int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,</span>
<span class="quote">&gt; +	      struct list_head *uf, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	unsigned long end;</span>
<span class="quote">&gt; @@ -1624,7 +1631,7 @@ int do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list</span>
<span class="quote">&gt;  		if (end != vma-&gt;vm_end &amp;&amp; offset_in_page(end))</span>
<span class="quote">&gt;  			return -EINVAL;</span>
<span class="quote">&gt;  		if (start != vma-&gt;vm_start &amp;&amp; end != vma-&gt;vm_end) {</span>
<span class="quote">&gt; -			ret = split_vma(mm, vma, start, 1);</span>
<span class="quote">&gt; +			ret = split_vma(mm, vma, start, 1, mmrange);</span>
<span class="quote">&gt;  			if (ret &lt; 0)</span>
<span class="quote">&gt;  				return ret;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; @@ -1642,9 +1649,10 @@ int vm_munmap(unsigned long addr, size_t len)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; -	ret = do_munmap(mm, addr, len, NULL);</span>
<span class="quote">&gt; +	ret = do_munmap(mm, addr, len, NULL, &amp;mmrange);</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/pagewalk.c b/mm/pagewalk.c</span>
<span class="quote">&gt; index 8d2da5dec1e0..44a2507c94fd 100644</span>
<span class="quote">&gt; --- a/mm/pagewalk.c</span>
<span class="quote">&gt; +++ b/mm/pagewalk.c</span>
<span class="quote">&gt; @@ -26,7 +26,7 @@ static int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -			  struct mm_walk *walk)</span>
<span class="quote">&gt; +			  struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pmd_t *pmd;</span>
<span class="quote">&gt;  	unsigned long next;</span>
<span class="quote">&gt; @@ -38,7 +38,7 @@ static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt;  		if (pmd_none(*pmd) || !walk-&gt;vma) {</span>
<span class="quote">&gt;  			if (walk-&gt;pte_hole)</span>
<span class="quote">&gt; -				err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="quote">&gt; +				err = walk-&gt;pte_hole(addr, next, walk, mmrange);</span>
<span class="quote">&gt;  			if (err)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; @@ -48,7 +48,7 @@ static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		 * needs to know about pmd_trans_huge() pmds</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		if (walk-&gt;pmd_entry)</span>
<span class="quote">&gt; -			err = walk-&gt;pmd_entry(pmd, addr, next, walk);</span>
<span class="quote">&gt; +			err = walk-&gt;pmd_entry(pmd, addr, next, walk, mmrange);</span>
<span class="quote">&gt;  		if (err)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -71,7 +71,7 @@ static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -			  struct mm_walk *walk)</span>
<span class="quote">&gt; +			  struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pud_t *pud;</span>
<span class="quote">&gt;  	unsigned long next;</span>
<span class="quote">&gt; @@ -83,7 +83,7 @@ static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		next = pud_addr_end(addr, end);</span>
<span class="quote">&gt;  		if (pud_none(*pud) || !walk-&gt;vma) {</span>
<span class="quote">&gt;  			if (walk-&gt;pte_hole)</span>
<span class="quote">&gt; -				err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="quote">&gt; +				err = walk-&gt;pte_hole(addr, next, walk, mmrange);</span>
<span class="quote">&gt;  			if (err)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; @@ -106,7 +106,7 @@ static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  			goto again;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)</span>
<span class="quote">&gt; -			err = walk_pmd_range(pud, addr, next, walk);</span>
<span class="quote">&gt; +			err = walk_pmd_range(pud, addr, next, walk, mmrange);</span>
<span class="quote">&gt;  		if (err)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  	} while (pud++, addr = next, addr != end);</span>
<span class="quote">&gt; @@ -115,7 +115,7 @@ static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -			  struct mm_walk *walk)</span>
<span class="quote">&gt; +			  struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	p4d_t *p4d;</span>
<span class="quote">&gt;  	unsigned long next;</span>
<span class="quote">&gt; @@ -126,13 +126,13 @@ static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		next = p4d_addr_end(addr, end);</span>
<span class="quote">&gt;  		if (p4d_none_or_clear_bad(p4d)) {</span>
<span class="quote">&gt;  			if (walk-&gt;pte_hole)</span>
<span class="quote">&gt; -				err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="quote">&gt; +				err = walk-&gt;pte_hole(addr, next, walk, mmrange);</span>
<span class="quote">&gt;  			if (err)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)</span>
<span class="quote">&gt; -			err = walk_pud_range(p4d, addr, next, walk);</span>
<span class="quote">&gt; +			err = walk_pud_range(p4d, addr, next, walk, mmrange);</span>
<span class="quote">&gt;  		if (err)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  	} while (p4d++, addr = next, addr != end);</span>
<span class="quote">&gt; @@ -141,7 +141,7 @@ static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int walk_pgd_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -			  struct mm_walk *walk)</span>
<span class="quote">&gt; +			  struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pgd_t *pgd;</span>
<span class="quote">&gt;  	unsigned long next;</span>
<span class="quote">&gt; @@ -152,13 +152,13 @@ static int walk_pgd_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		next = pgd_addr_end(addr, end);</span>
<span class="quote">&gt;  		if (pgd_none_or_clear_bad(pgd)) {</span>
<span class="quote">&gt;  			if (walk-&gt;pte_hole)</span>
<span class="quote">&gt; -				err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="quote">&gt; +				err = walk-&gt;pte_hole(addr, next, walk, mmrange);</span>
<span class="quote">&gt;  			if (err)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)</span>
<span class="quote">&gt; -			err = walk_p4d_range(pgd, addr, next, walk);</span>
<span class="quote">&gt; +			err = walk_p4d_range(pgd, addr, next, walk, mmrange);</span>
<span class="quote">&gt;  		if (err)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  	} while (pgd++, addr = next, addr != end);</span>
<span class="quote">&gt; @@ -175,7 +175,7 @@ static unsigned long hugetlb_entry_end(struct hstate *h, unsigned long addr,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int walk_hugetlb_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -			      struct mm_walk *walk)</span>
<span class="quote">&gt; +			      struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt;  	struct hstate *h = hstate_vma(vma);</span>
<span class="quote">&gt; @@ -192,7 +192,7 @@ static int walk_hugetlb_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		if (pte)</span>
<span class="quote">&gt;  			err = walk-&gt;hugetlb_entry(pte, hmask, addr, next, walk);</span>
<span class="quote">&gt;  		else if (walk-&gt;pte_hole)</span>
<span class="quote">&gt; -			err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="quote">&gt; +			err = walk-&gt;pte_hole(addr, next, walk, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		if (err)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; @@ -203,7 +203,7 @@ static int walk_hugetlb_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #else /* CONFIG_HUGETLB_PAGE */</span>
<span class="quote">&gt;  static int walk_hugetlb_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; -			      struct mm_walk *walk)</span>
<span class="quote">&gt; +			      struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -217,7 +217,7 @@ static int walk_hugetlb_range(unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;   * error, where we abort the current walk.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static int walk_page_test(unsigned long start, unsigned long end,</span>
<span class="quote">&gt; -			struct mm_walk *walk)</span>
<span class="quote">&gt; +			  struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -235,23 +235,23 @@ static int walk_page_test(unsigned long start, unsigned long end,</span>
<span class="quote">&gt;  	if (vma-&gt;vm_flags &amp; VM_PFNMAP) {</span>
<span class="quote">&gt;  		int err = 1;</span>
<span class="quote">&gt;  		if (walk-&gt;pte_hole)</span>
<span class="quote">&gt; -			err = walk-&gt;pte_hole(start, end, walk);</span>
<span class="quote">&gt; +			err = walk-&gt;pte_hole(start, end, walk, mmrange);</span>
<span class="quote">&gt;  		return err ? err : 1;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  static int __walk_page_range(unsigned long start, unsigned long end,</span>
<span class="quote">&gt; -			struct mm_walk *walk)</span>
<span class="quote">&gt; +			     struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int err = 0;</span>
<span class="quote">&gt;  	struct vm_area_struct *vma = walk-&gt;vma;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (vma &amp;&amp; is_vm_hugetlb_page(vma)) {</span>
<span class="quote">&gt;  		if (walk-&gt;hugetlb_entry)</span>
<span class="quote">&gt; -			err = walk_hugetlb_range(start, end, walk);</span>
<span class="quote">&gt; +			err = walk_hugetlb_range(start, end, walk, mmrange);</span>
<span class="quote">&gt;  	} else</span>
<span class="quote">&gt; -		err = walk_pgd_range(start, end, walk);</span>
<span class="quote">&gt; +		err = walk_pgd_range(start, end, walk, mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	return err;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -285,10 +285,11 @@ static int __walk_page_range(unsigned long start, unsigned long end,</span>
<span class="quote">&gt;   * Locking:</span>
<span class="quote">&gt;   *   Callers of walk_page_range() and walk_page_vma() should hold</span>
<span class="quote">&gt;   *   @walk-&gt;mm-&gt;mmap_sem, because these function traverse vma list and/or</span>
<span class="quote">&gt; - *   access to vma&#39;s data.</span>
<span class="quote">&gt; + *   access to vma&#39;s data. As such, the @mmrange will represent the</span>
<span class="quote">&gt; + *   address space range.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int walk_page_range(unsigned long start, unsigned long end,</span>
<span class="quote">&gt; -		    struct mm_walk *walk)</span>
<span class="quote">&gt; +		    struct mm_walk *walk, struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int err = 0;</span>
<span class="quote">&gt;  	unsigned long next;</span>
<span class="quote">&gt; @@ -315,7 +316,7 @@ int walk_page_range(unsigned long start, unsigned long end,</span>
<span class="quote">&gt;  			next = min(end, vma-&gt;vm_end);</span>
<span class="quote">&gt;  			vma = vma-&gt;vm_next;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -			err = walk_page_test(start, next, walk);</span>
<span class="quote">&gt; +			err = walk_page_test(start, next, walk, mmrange);</span>
<span class="quote">&gt;  			if (err &gt; 0) {</span>
<span class="quote">&gt;  				/*</span>
<span class="quote">&gt;  				 * positive return values are purely for</span>
<span class="quote">&gt; @@ -329,14 +330,15 @@ int walk_page_range(unsigned long start, unsigned long end,</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (walk-&gt;vma || walk-&gt;pte_hole)</span>
<span class="quote">&gt; -			err = __walk_page_range(start, next, walk);</span>
<span class="quote">&gt; +			err = __walk_page_range(start, next, walk, mmrange);</span>
<span class="quote">&gt;  		if (err)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  	} while (start = next, start &lt; end);</span>
<span class="quote">&gt;  	return err;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk)</span>
<span class="quote">&gt; +int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk,</span>
<span class="quote">&gt; +		  struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int err;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -346,10 +348,10 @@ int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk)</span>
<span class="quote">&gt;  	VM_BUG_ON(!rwsem_is_locked(&amp;walk-&gt;mm-&gt;mmap_sem));</span>
<span class="quote">&gt;  	VM_BUG_ON(!vma);</span>
<span class="quote">&gt;  	walk-&gt;vma = vma;</span>
<span class="quote">&gt; -	err = walk_page_test(vma-&gt;vm_start, vma-&gt;vm_end, walk);</span>
<span class="quote">&gt; +	err = walk_page_test(vma-&gt;vm_start, vma-&gt;vm_end, walk, mmrange);</span>
<span class="quote">&gt;  	if (err &gt; 0)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	if (err &lt; 0)</span>
<span class="quote">&gt;  		return err;</span>
<span class="quote">&gt; -	return __walk_page_range(vma-&gt;vm_start, vma-&gt;vm_end, walk);</span>
<span class="quote">&gt; +	return __walk_page_range(vma-&gt;vm_start, vma-&gt;vm_end, walk, mmrange);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/process_vm_access.c b/mm/process_vm_access.c</span>
<span class="quote">&gt; index a447092d4635..ff6772b86195 100644</span>
<span class="quote">&gt; --- a/mm/process_vm_access.c</span>
<span class="quote">&gt; +++ b/mm/process_vm_access.c</span>
<span class="quote">&gt; @@ -90,6 +90,7 @@ static int process_vm_rw_single_vec(unsigned long addr,</span>
<span class="quote">&gt;  	unsigned long max_pages_per_loop = PVM_MAX_KMALLOC_PAGES</span>
<span class="quote">&gt;  		/ sizeof(struct pages *);</span>
<span class="quote">&gt;  	unsigned int flags = 0;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* Work out address and page range required */</span>
<span class="quote">&gt;  	if (len == 0)</span>
<span class="quote">&gt; @@ -111,7 +112,8 @@ static int process_vm_rw_single_vec(unsigned long addr,</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  		pages = get_user_pages_remote(task, mm, pa, pages, flags,</span>
<span class="quote">&gt; -					      process_pages, NULL, &amp;locked);</span>
<span class="quote">&gt; +					      process_pages, NULL, &amp;locked,</span>
<span class="quote">&gt; +					      &amp;mmrange);</span>
<span class="quote">&gt;  		if (locked)</span>
<span class="quote">&gt;  			up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  		if (pages &lt;= 0)</span>
<span class="quote">&gt; diff --git a/mm/util.c b/mm/util.c</span>
<span class="quote">&gt; index c1250501364f..b0ec1d88bb71 100644</span>
<span class="quote">&gt; --- a/mm/util.c</span>
<span class="quote">&gt; +++ b/mm/util.c</span>
<span class="quote">&gt; @@ -347,13 +347,14 @@ unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;  	unsigned long populate;</span>
<span class="quote">&gt;  	LIST_HEAD(uf);</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	ret = security_mmap_file(file, prot, flag);</span>
<span class="quote">&gt;  	if (!ret) {</span>
<span class="quote">&gt;  		if (down_write_killable(&amp;mm-&gt;mmap_sem))</span>
<span class="quote">&gt;  			return -EINTR;</span>
<span class="quote">&gt;  		ret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,</span>
<span class="quote">&gt; -				    &amp;populate, &amp;uf);</span>
<span class="quote">&gt; +				    &amp;populate, &amp;uf, &amp;mmrange);</span>
<span class="quote">&gt;  		up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  		userfaultfd_unmap_complete(mm, &amp;uf);</span>
<span class="quote">&gt;  		if (populate)</span>
<span class="quote">&gt; diff --git a/security/tomoyo/domain.c b/security/tomoyo/domain.c</span>
<span class="quote">&gt; index f6758dad981f..c1e36ea2c6fc 100644</span>
<span class="quote">&gt; --- a/security/tomoyo/domain.c</span>
<span class="quote">&gt; +++ b/security/tomoyo/domain.c</span>
<span class="quote">&gt; @@ -868,6 +868,7 @@ bool tomoyo_dump_page(struct linux_binprm *bprm, unsigned long pos,</span>
<span class="quote">&gt;  		      struct tomoyo_page_dump *dump)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange); /* see get_page_arg() in fs/exec.c */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* dump-&gt;data is released by tomoyo_find_next_domain(). */</span>
<span class="quote">&gt;  	if (!dump-&gt;data) {</span>
<span class="quote">&gt; @@ -884,7 +885,7 @@ bool tomoyo_dump_page(struct linux_binprm *bprm, unsigned long pos,</span>
<span class="quote">&gt;  	 * the execve().</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (get_user_pages_remote(current, bprm-&gt;mm, pos, 1,</span>
<span class="quote">&gt; -				FOLL_FORCE, &amp;page, NULL, NULL) &lt;= 0)</span>
<span class="quote">&gt; +				  FOLL_FORCE, &amp;page, NULL, NULL, &amp;mmrange) &lt;= 0)</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  	page = bprm-&gt;page[pos / PAGE_SIZE];</span>
<span class="quote">&gt; diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c</span>
<span class="quote">&gt; index 57bcb27dcf30..4cd2b93bb20c 100644</span>
<span class="quote">&gt; --- a/virt/kvm/async_pf.c</span>
<span class="quote">&gt; +++ b/virt/kvm/async_pf.c</span>
<span class="quote">&gt; @@ -78,6 +78,7 @@ static void async_pf_execute(struct work_struct *work)</span>
<span class="quote">&gt;  	unsigned long addr = apf-&gt;addr;</span>
<span class="quote">&gt;  	gva_t gva = apf-&gt;gva;</span>
<span class="quote">&gt;  	int locked = 1;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	might_sleep();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -88,7 +89,7 @@ static void async_pf_execute(struct work_struct *work)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	get_user_pages_remote(NULL, mm, addr, 1, FOLL_WRITE, NULL, NULL,</span>
<span class="quote">&gt; -			&amp;locked);</span>
<span class="quote">&gt; +			      &amp;locked, &amp;mmrange);</span>
<span class="quote">&gt;  	if (locked)</span>
<span class="quote">&gt;  		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="quote">&gt; index 4501e658e8d6..86ec078f4c3b 100644</span>
<span class="quote">&gt; --- a/virt/kvm/kvm_main.c</span>
<span class="quote">&gt; +++ b/virt/kvm/kvm_main.c</span>
<span class="quote">&gt; @@ -1317,11 +1317,12 @@ unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *w</span>
<span class="quote">&gt;  	return gfn_to_hva_memslot_prot(slot, gfn, writable);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -static inline int check_user_page_hwpoison(unsigned long addr)</span>
<span class="quote">&gt; +static inline int check_user_page_hwpoison(unsigned long addr,</span>
<span class="quote">&gt; +					   struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int rc, flags = FOLL_HWPOISON | FOLL_WRITE;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	rc = get_user_pages(addr, 1, flags, NULL, NULL);</span>
<span class="quote">&gt; +	rc = get_user_pages(addr, 1, flags, NULL, NULL, mmrange);</span>
<span class="quote">&gt;  	return rc == -EHWPOISON;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1411,7 +1412,8 @@ static bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)</span>
<span class="quote">&gt;  static int hva_to_pfn_remapped(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			       unsigned long addr, bool *async,</span>
<span class="quote">&gt;  			       bool write_fault, bool *writable,</span>
<span class="quote">&gt; -			       kvm_pfn_t *p_pfn)</span>
<span class="quote">&gt; +			       kvm_pfn_t *p_pfn,</span>
<span class="quote">&gt; +			       struct range_lock *mmrange)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long pfn;</span>
<span class="quote">&gt;  	int r;</span>
<span class="quote">&gt; @@ -1425,7 +1427,7 @@ static int hva_to_pfn_remapped(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		bool unlocked = false;</span>
<span class="quote">&gt;  		r = fixup_user_fault(current, current-&gt;mm, addr,</span>
<span class="quote">&gt;  				     (write_fault ? FAULT_FLAG_WRITE : 0),</span>
<span class="quote">&gt; -				     &amp;unlocked);</span>
<span class="quote">&gt; +				     &amp;unlocked, mmrange);</span>
<span class="quote">&gt;  		if (unlocked)</span>
<span class="quote">&gt;  			return -EAGAIN;</span>
<span class="quote">&gt;  		if (r)</span>
<span class="quote">&gt; @@ -1477,6 +1479,7 @@ static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
<span class="quote">&gt;  	struct vm_area_struct *vma;</span>
<span class="quote">&gt;  	kvm_pfn_t pfn = 0;</span>
<span class="quote">&gt;  	int npages, r;</span>
<span class="quote">&gt; +	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	/* we can do it either atomically or asynchronously, not both */</span>
<span class="quote">&gt;  	BUG_ON(atomic &amp;&amp; async);</span>
<span class="quote">&gt; @@ -1493,7 +1496,7 @@ static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	if (npages == -EHWPOISON ||</span>
<span class="quote">&gt; -	      (!async &amp;&amp; check_user_page_hwpoison(addr))) {</span>
<span class="quote">&gt; +	    (!async &amp;&amp; check_user_page_hwpoison(addr, &amp;mmrange))) {</span>
<span class="quote">&gt;  		pfn = KVM_PFN_ERR_HWPOISON;</span>
<span class="quote">&gt;  		goto exit;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1504,7 +1507,8 @@ static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
<span class="quote">&gt;  	if (vma == NULL)</span>
<span class="quote">&gt;  		pfn = KVM_PFN_ERR_FAULT;</span>
<span class="quote">&gt;  	else if (vma-&gt;vm_flags &amp; (VM_IO | VM_PFNMAP)) {</span>
<span class="quote">&gt; -		r = hva_to_pfn_remapped(vma, addr, async, write_fault, writable, &amp;pfn);</span>
<span class="quote">&gt; +		r = hva_to_pfn_remapped(vma, addr, async, write_fault, writable,</span>
<span class="quote">&gt; +					&amp;pfn, &amp;mmrange);</span>
<span class="quote">&gt;  		if (r == -EAGAIN)</span>
<span class="quote">&gt;  			goto retry;</span>
<span class="quote">&gt;  		if (r &lt; 0)</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=106071">Davidlohr Bueso</a> - Feb. 6, 2018, 6:32 p.m.</div>
<pre class="content">
On Mon, 05 Feb 2018, Laurent Dufour wrote:
<span class="quote">
&gt;&gt; --- a/drivers/misc/sgi-gru/grufault.c</span>
<span class="quote">&gt;&gt; +++ b/drivers/misc/sgi-gru/grufault.c</span>
<span class="quote">&gt;&gt; @@ -189,7 +189,8 @@ static void get_clear_fault_map(struct gru_state *gru,</span>
<span class="quote">&gt;&gt;   */</span>
<span class="quote">&gt;&gt;  static int non_atomic_pte_lookup(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  				 unsigned long vaddr, int write,</span>
<span class="quote">&gt;&gt; -				 unsigned long *paddr, int *pageshift)</span>
<span class="quote">&gt;&gt; +				 unsigned long *paddr, int *pageshift,</span>
<span class="quote">&gt;&gt; +				 struct range_lock *mmrange)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	struct page *page;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -198,7 +199,8 @@ static int non_atomic_pte_lookup(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  #else</span>
<span class="quote">&gt;&gt;  	*pageshift = PAGE_SHIFT;</span>
<span class="quote">&gt;&gt;  #endif</span>
<span class="quote">&gt;&gt; -	if (get_user_pages(vaddr, 1, write ? FOLL_WRITE : 0, &amp;page, NULL) &lt;= 0)</span>
<span class="quote">&gt;&gt; +	if (get_user_pages(vaddr, 1, write ? FOLL_WRITE : 0,</span>
<span class="quote">&gt;&gt; +			   &amp;page, NULL, mmrange) &lt;= 0)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;There is no need to pass down the range here since underlying called</span>
<span class="quote">&gt;__get_user_pages_locked() is told to not unlock the mmap_sem.</span>
<span class="quote">&gt;In general get_user_pages() doesn&#39;t need a range parameter.</span>

Yeah, you&#39;re right. At least it was a productive exercise for auditing.

Thanks,
Davidlohr
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/alpha/mm/fault.c b/arch/alpha/mm/fault.c</span>
<span class="p_header">index cd3c572ee912..690d86a00a20 100644</span>
<span class="p_header">--- a/arch/alpha/mm/fault.c</span>
<span class="p_header">+++ b/arch/alpha/mm/fault.c</span>
<span class="p_chunk">@@ -90,6 +90,7 @@</span> <span class="p_context"> do_page_fault(unsigned long address, unsigned long mmcsr,</span>
 	int fault, si_code = SEGV_MAPERR;
 	siginfo_t info;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* As of EV6, a load into $31/$f31 is a prefetch, and never faults
 	   (or is suppressed by the PALcode).  Support that for older CPUs
<span class="p_chunk">@@ -148,7 +149,7 @@</span> <span class="p_context"> do_page_fault(unsigned long address, unsigned long mmcsr,</span>
 	/* If for any reason at all we couldn&#39;t handle the fault,
 	   make sure we exit gracefully rather than endlessly redo
 	   the fault.  */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/arch/arc/mm/fault.c b/arch/arc/mm/fault.c</span>
<span class="p_header">index a0b7bd6d030d..e423f764f159 100644</span>
<span class="p_header">--- a/arch/arc/mm/fault.c</span>
<span class="p_header">+++ b/arch/arc/mm/fault.c</span>
<span class="p_chunk">@@ -69,6 +69,7 @@</span> <span class="p_context"> void do_page_fault(unsigned long address, struct pt_regs *regs)</span>
 	int fault, ret;
 	int write = regs-&gt;ecr_cause &amp; ECR_C_PROTV_STORE;  /* ST/EX */
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/*
 	 * We fault-in kernel-space virtual memory on-demand. The
<span class="p_chunk">@@ -137,7 +138,7 @@</span> <span class="p_context"> void do_page_fault(unsigned long address, struct pt_regs *regs)</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	/* If Pagefault was interrupted by SIGKILL, exit page fault &quot;early&quot; */
 	if (unlikely(fatal_signal_pending(current))) {
<span class="p_header">diff --git a/arch/arm/mm/fault.c b/arch/arm/mm/fault.c</span>
<span class="p_header">index b75eada23d0a..99ae40b5851a 100644</span>
<span class="p_header">--- a/arch/arm/mm/fault.c</span>
<span class="p_header">+++ b/arch/arm/mm/fault.c</span>
<span class="p_chunk">@@ -221,7 +221,8 @@</span> <span class="p_context"> static inline bool access_error(unsigned int fsr, struct vm_area_struct *vma)</span>
 
 static int __kprobes
 __do_page_fault(struct mm_struct *mm, unsigned long addr, unsigned int fsr,
<span class="p_del">-		unsigned int flags, struct task_struct *tsk)</span>
<span class="p_add">+		unsigned int flags, struct task_struct *tsk,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma;
 	int fault;
<span class="p_chunk">@@ -243,7 +244,7 @@</span> <span class="p_context"> __do_page_fault(struct mm_struct *mm, unsigned long addr, unsigned int fsr,</span>
 		goto out;
 	}
 
<span class="p_del">-	return handle_mm_fault(vma, addr &amp; PAGE_MASK, flags);</span>
<span class="p_add">+	return handle_mm_fault(vma, addr &amp; PAGE_MASK, flags, mmrange);</span>
 
 check_stack:
 	/* Don&#39;t allow expansion below FIRST_USER_ADDRESS */
<span class="p_chunk">@@ -261,6 +262,7 @@</span> <span class="p_context"> do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)</span>
 	struct mm_struct *mm;
 	int fault, sig, code;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (notify_page_fault(regs, fsr))
 		return 0;
<span class="p_chunk">@@ -308,7 +310,7 @@</span> <span class="p_context"> do_page_fault(unsigned long addr, unsigned int fsr, struct pt_regs *regs)</span>
 #endif
 	}
 
<span class="p_del">-	fault = __do_page_fault(mm, addr, fsr, flags, tsk);</span>
<span class="p_add">+	fault = __do_page_fault(mm, addr, fsr, flags, tsk, &amp;mmrange);</span>
 
 	/* If we need to retry but a fatal signal is pending, handle the
 	 * signal first. We do not need to release the mmap_sem because
<span class="p_header">diff --git a/arch/arm/probes/uprobes/core.c b/arch/arm/probes/uprobes/core.c</span>
<span class="p_header">index d1329f1ba4e4..e8b893eaebcf 100644</span>
<span class="p_header">--- a/arch/arm/probes/uprobes/core.c</span>
<span class="p_header">+++ b/arch/arm/probes/uprobes/core.c</span>
<span class="p_chunk">@@ -30,10 +30,11 @@</span> <span class="p_context"> bool is_swbp_insn(uprobe_opcode_t *insn)</span>
 }
 
 int set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm,
<span class="p_del">-	     unsigned long vaddr)</span>
<span class="p_add">+	     unsigned long vaddr, struct range_lock *mmrange)</span>
 {
 	return uprobe_write_opcode(mm, vaddr,
<span class="p_del">-		   __opcode_to_mem_arm(auprobe-&gt;bpinsn));</span>
<span class="p_add">+				   __opcode_to_mem_arm(auprobe-&gt;bpinsn),</span>
<span class="p_add">+				   mmrange);</span>
 }
 
 bool arch_uprobe_ignore(struct arch_uprobe *auprobe, struct pt_regs *regs)
<span class="p_header">diff --git a/arch/arm64/mm/fault.c b/arch/arm64/mm/fault.c</span>
<span class="p_header">index ce441d29e7f6..1f3ad9e4f214 100644</span>
<span class="p_header">--- a/arch/arm64/mm/fault.c</span>
<span class="p_header">+++ b/arch/arm64/mm/fault.c</span>
<span class="p_chunk">@@ -342,7 +342,7 @@</span> <span class="p_context"> static void do_bad_area(unsigned long addr, unsigned int esr, struct pt_regs *re</span>
 
 static int __do_page_fault(struct mm_struct *mm, unsigned long addr,
 			   unsigned int mm_flags, unsigned long vm_flags,
<span class="p_del">-			   struct task_struct *tsk)</span>
<span class="p_add">+			   struct task_struct *tsk, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma;
 	int fault;
<span class="p_chunk">@@ -368,7 +368,7 @@</span> <span class="p_context"> static int __do_page_fault(struct mm_struct *mm, unsigned long addr,</span>
 		goto out;
 	}
 
<span class="p_del">-	return handle_mm_fault(vma, addr &amp; PAGE_MASK, mm_flags);</span>
<span class="p_add">+	return handle_mm_fault(vma, addr &amp; PAGE_MASK, mm_flags, mmrange);</span>
 
 check_stack:
 	if (vma-&gt;vm_flags &amp; VM_GROWSDOWN &amp;&amp; !expand_stack(vma, addr))
<span class="p_chunk">@@ -390,6 +390,7 @@</span> <span class="p_context"> static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,</span>
 	int fault, sig, code, major = 0;
 	unsigned long vm_flags = VM_READ | VM_WRITE;
 	unsigned int mm_flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (notify_page_fault(regs, esr))
 		return 0;
<span class="p_chunk">@@ -450,7 +451,7 @@</span> <span class="p_context"> static int __kprobes do_page_fault(unsigned long addr, unsigned int esr,</span>
 #endif
 	}
 
<span class="p_del">-	fault = __do_page_fault(mm, addr, mm_flags, vm_flags, tsk);</span>
<span class="p_add">+	fault = __do_page_fault(mm, addr, mm_flags, vm_flags, tsk, &amp;mmrange);</span>
 	major |= fault &amp; VM_FAULT_MAJOR;
 
 	if (fault &amp; VM_FAULT_RETRY) {
<span class="p_header">diff --git a/arch/cris/mm/fault.c b/arch/cris/mm/fault.c</span>
<span class="p_header">index 29cc58038b98..16af16d77269 100644</span>
<span class="p_header">--- a/arch/cris/mm/fault.c</span>
<span class="p_header">+++ b/arch/cris/mm/fault.c</span>
<span class="p_chunk">@@ -61,6 +61,7 @@</span> <span class="p_context"> do_page_fault(unsigned long address, struct pt_regs *regs,</span>
 	siginfo_t info;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	D(printk(KERN_DEBUG
 		 &quot;Page fault for %lX on %X at %lX, prot %d write %d\n&quot;,
<span class="p_chunk">@@ -170,7 +171,7 @@</span> <span class="p_context"> do_page_fault(unsigned long address, struct pt_regs *regs,</span>
 	 * the fault.
 	 */
 
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/arch/frv/mm/fault.c b/arch/frv/mm/fault.c</span>
<span class="p_header">index cbe7aec863e3..494d33b628fc 100644</span>
<span class="p_header">--- a/arch/frv/mm/fault.c</span>
<span class="p_header">+++ b/arch/frv/mm/fault.c</span>
<span class="p_chunk">@@ -41,6 +41,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(int datammu, unsigned long esr0, unsigned long ear</span>
 	pud_t *pue;
 	pte_t *pte;
 	int fault;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 #if 0
 	const char *atxc[16] = {
<span class="p_chunk">@@ -165,7 +166,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(int datammu, unsigned long esr0, unsigned long ear</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, ear0, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, ear0, flags, &amp;mmrange);</span>
 	if (unlikely(fault &amp; VM_FAULT_ERROR)) {
 		if (fault &amp; VM_FAULT_OOM)
 			goto out_of_memory;
<span class="p_header">diff --git a/arch/hexagon/mm/vm_fault.c b/arch/hexagon/mm/vm_fault.c</span>
<span class="p_header">index 3eec33c5cfd7..7d6ada2c2230 100644</span>
<span class="p_header">--- a/arch/hexagon/mm/vm_fault.c</span>
<span class="p_header">+++ b/arch/hexagon/mm/vm_fault.c</span>
<span class="p_chunk">@@ -55,6 +55,7 @@</span> <span class="p_context"> void do_page_fault(unsigned long address, long cause, struct pt_regs *regs)</span>
 	int fault;
 	const struct exception_table_entry *fixup;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/*
 	 * If we&#39;re in an interrupt or have no user context,
<span class="p_chunk">@@ -102,7 +103,7 @@</span> <span class="p_context"> void do_page_fault(unsigned long address, long cause, struct pt_regs *regs)</span>
 		break;
 	}
 
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/arch/ia64/mm/fault.c b/arch/ia64/mm/fault.c</span>
<span class="p_header">index dfdc152d6737..44f0ec5f77c2 100644</span>
<span class="p_header">--- a/arch/ia64/mm/fault.c</span>
<span class="p_header">+++ b/arch/ia64/mm/fault.c</span>
<span class="p_chunk">@@ -89,6 +89,7 @@</span> <span class="p_context"> ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *re</span>
 	unsigned long mask;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	mask = ((((isr &gt;&gt; IA64_ISR_X_BIT) &amp; 1UL) &lt;&lt; VM_EXEC_BIT)
 		| (((isr &gt;&gt; IA64_ISR_W_BIT) &amp; 1UL) &lt;&lt; VM_WRITE_BIT));
<span class="p_chunk">@@ -162,7 +163,7 @@</span> <span class="p_context"> ia64_do_page_fault (unsigned long address, unsigned long isr, struct pt_regs *re</span>
 	 * sure we exit gracefully rather than endlessly redo the
 	 * fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/arch/m32r/mm/fault.c b/arch/m32r/mm/fault.c</span>
<span class="p_header">index 46d9a5ca0e3a..0129aea46729 100644</span>
<span class="p_header">--- a/arch/m32r/mm/fault.c</span>
<span class="p_header">+++ b/arch/m32r/mm/fault.c</span>
<span class="p_chunk">@@ -82,6 +82,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	unsigned long flags = 0;
 	int fault;
 	siginfo_t info;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/*
 	 * If BPSW IE bit enable --&gt; set PSW IE bit
<span class="p_chunk">@@ -197,7 +198,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 */
 	addr = (address &amp; PAGE_MASK);
 	set_thread_fault_code(error_code);
<span class="p_del">-	fault = handle_mm_fault(vma, addr, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, addr, flags, &amp;mmrange);</span>
 	if (unlikely(fault &amp; VM_FAULT_ERROR)) {
 		if (fault &amp; VM_FAULT_OOM)
 			goto out_of_memory;
<span class="p_header">diff --git a/arch/m68k/mm/fault.c b/arch/m68k/mm/fault.c</span>
<span class="p_header">index 03253c4f8e6a..ec32a193726f 100644</span>
<span class="p_header">--- a/arch/m68k/mm/fault.c</span>
<span class="p_header">+++ b/arch/m68k/mm/fault.c</span>
<span class="p_chunk">@@ -75,6 +75,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	struct vm_area_struct * vma;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	pr_debug(&quot;do page fault:\nregs-&gt;sr=%#x, regs-&gt;pc=%#lx, address=%#lx, %ld, %p\n&quot;,
 		regs-&gt;sr, regs-&gt;pc, address, error_code, mm ? mm-&gt;pgd : NULL);
<span class="p_chunk">@@ -138,7 +139,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * the fault.
 	 */
 
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 	pr_debug(&quot;handle_mm_fault returns %d\n&quot;, fault);
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
<span class="p_header">diff --git a/arch/metag/mm/fault.c b/arch/metag/mm/fault.c</span>
<span class="p_header">index de54fe686080..e16ba0ea7ea1 100644</span>
<span class="p_header">--- a/arch/metag/mm/fault.c</span>
<span class="p_header">+++ b/arch/metag/mm/fault.c</span>
<span class="p_chunk">@@ -56,6 +56,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	siginfo_t info;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	tsk = current;
 
<span class="p_chunk">@@ -135,7 +136,7 @@</span> <span class="p_context"> int do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return 0;
<span class="p_header">diff --git a/arch/microblaze/mm/fault.c b/arch/microblaze/mm/fault.c</span>
<span class="p_header">index f91b30f8aaa8..fd49efbdfbf4 100644</span>
<span class="p_header">--- a/arch/microblaze/mm/fault.c</span>
<span class="p_header">+++ b/arch/microblaze/mm/fault.c</span>
<span class="p_chunk">@@ -93,6 +93,7 @@</span> <span class="p_context"> void do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	int is_write = error_code &amp; ESR_S;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	regs-&gt;ear = address;
 	regs-&gt;esr = error_code;
<span class="p_chunk">@@ -216,7 +217,7 @@</span> <span class="p_context"> void do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/arch/mips/kernel/vdso.c b/arch/mips/kernel/vdso.c</span>
<span class="p_header">index 019035d7225c..56b7c29991db 100644</span>
<span class="p_header">--- a/arch/mips/kernel/vdso.c</span>
<span class="p_header">+++ b/arch/mips/kernel/vdso.c</span>
<span class="p_chunk">@@ -102,6 +102,7 @@</span> <span class="p_context"> int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)</span>
 	unsigned long gic_size, vvar_size, size, base, data_addr, vdso_addr, gic_pfn;
 	struct vm_area_struct *vma;
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (down_write_killable(&amp;mm-&gt;mmap_sem))
 		return -EINTR;
<span class="p_chunk">@@ -110,7 +111,7 @@</span> <span class="p_context"> int arch_setup_additional_pages(struct linux_binprm *bprm, int uses_interp)</span>
 	base = mmap_region(NULL, STACK_TOP, PAGE_SIZE,
 			   VM_READ|VM_WRITE|VM_EXEC|
 			   VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,
<span class="p_del">-			   0, NULL);</span>
<span class="p_add">+			   0, NULL, &amp;mmrange);</span>
 	if (IS_ERR_VALUE(base)) {
 		ret = base;
 		goto out;
<span class="p_header">diff --git a/arch/mips/mm/fault.c b/arch/mips/mm/fault.c</span>
<span class="p_header">index 4f8f5bf46977..1433edd01d09 100644</span>
<span class="p_header">--- a/arch/mips/mm/fault.c</span>
<span class="p_header">+++ b/arch/mips/mm/fault.c</span>
<span class="p_chunk">@@ -47,6 +47,7 @@</span> <span class="p_context"> static void __kprobes __do_page_fault(struct pt_regs *regs, unsigned long write,</span>
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 
 	static DEFINE_RATELIMIT_STATE(ratelimit_state, 5 * HZ, 10);
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 #if 0
 	printk(&quot;Cpu%d[%s:%d:%0*lx:%ld:%0*lx]\n&quot;, raw_smp_processor_id(),
<span class="p_chunk">@@ -152,7 +153,7 @@</span> <span class="p_context"> static void __kprobes __do_page_fault(struct pt_regs *regs, unsigned long write,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/arch/mn10300/mm/fault.c b/arch/mn10300/mm/fault.c</span>
<span class="p_header">index f0bfa1448744..71c38f0c8702 100644</span>
<span class="p_header">--- a/arch/mn10300/mm/fault.c</span>
<span class="p_header">+++ b/arch/mn10300/mm/fault.c</span>
<span class="p_chunk">@@ -125,6 +125,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long fault_code,</span>
 	siginfo_t info;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 #ifdef CONFIG_GDBSTUB
 	/* handle GDB stub causing a fault */
<span class="p_chunk">@@ -254,7 +255,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long fault_code,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/arch/nios2/mm/fault.c b/arch/nios2/mm/fault.c</span>
<span class="p_header">index b804dd06ea1c..768678b685af 100644</span>
<span class="p_header">--- a/arch/nios2/mm/fault.c</span>
<span class="p_header">+++ b/arch/nios2/mm/fault.c</span>
<span class="p_chunk">@@ -49,6 +49,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long cause,</span>
 	int code = SEGV_MAPERR;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	cause &gt;&gt;= 2;
 
<span class="p_chunk">@@ -132,7 +133,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long cause,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/arch/openrisc/mm/fault.c b/arch/openrisc/mm/fault.c</span>
<span class="p_header">index d0021dfae20a..75ddb1e8e7e7 100644</span>
<span class="p_header">--- a/arch/openrisc/mm/fault.c</span>
<span class="p_header">+++ b/arch/openrisc/mm/fault.c</span>
<span class="p_chunk">@@ -55,6 +55,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	siginfo_t info;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	tsk = current;
 
<span class="p_chunk">@@ -163,7 +164,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * the fault.
 	 */
 
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/arch/parisc/mm/fault.c b/arch/parisc/mm/fault.c</span>
<span class="p_header">index e247edbca68e..79db33a0cb0c 100644</span>
<span class="p_header">--- a/arch/parisc/mm/fault.c</span>
<span class="p_header">+++ b/arch/parisc/mm/fault.c</span>
<span class="p_chunk">@@ -264,6 +264,7 @@</span> <span class="p_context"> void do_page_fault(struct pt_regs *regs, unsigned long code,</span>
 	unsigned long acc_type;
 	int fault = 0;
 	unsigned int flags;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (faulthandler_disabled())
 		goto no_context;
<span class="p_chunk">@@ -301,7 +302,7 @@</span> <span class="p_context"> void do_page_fault(struct pt_regs *regs, unsigned long code,</span>
 	 * fault.
 	 */
 
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/arch/powerpc/include/asm/mmu_context.h b/arch/powerpc/include/asm/mmu_context.h</span>
<span class="p_header">index 051b3d63afe3..089b3cf948eb 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -176,7 +176,8 @@</span> <span class="p_context"> extern void arch_exit_mmap(struct mm_struct *mm);</span>
 
 static inline void arch_unmap(struct mm_struct *mm,
 			      struct vm_area_struct *vma,
<span class="p_del">-			      unsigned long start, unsigned long end)</span>
<span class="p_add">+			      unsigned long start, unsigned long end,</span>
<span class="p_add">+			      struct range_lock *mmrange)</span>
 {
 	if (start &lt;= mm-&gt;context.vdso_base &amp;&amp; mm-&gt;context.vdso_base &lt; end)
 		mm-&gt;context.vdso_base = 0;
<span class="p_header">diff --git a/arch/powerpc/include/asm/powernv.h b/arch/powerpc/include/asm/powernv.h</span>
<span class="p_header">index dc5f6a5d4575..805ff3ba94e1 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/powernv.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/powernv.h</span>
<span class="p_chunk">@@ -21,7 +21,7 @@</span> <span class="p_context"> extern void pnv_npu2_destroy_context(struct npu_context *context,</span>
 				struct pci_dev *gpdev);
 extern int pnv_npu2_handle_fault(struct npu_context *context, uintptr_t *ea,
 				unsigned long *flags, unsigned long *status,
<span class="p_del">-				int count);</span>
<span class="p_add">+				int count, struct range_lock *mmrange);</span>
 
 void pnv_tm_init(void);
 #else
<span class="p_chunk">@@ -35,7 +35,8 @@</span> <span class="p_context"> static inline void pnv_npu2_destroy_context(struct npu_context *context,</span>
 
 static inline int pnv_npu2_handle_fault(struct npu_context *context,
 					uintptr_t *ea, unsigned long *flags,
<span class="p_del">-					unsigned long *status, int count) {</span>
<span class="p_add">+					unsigned long *status, int count,</span>
<span class="p_add">+					struct range_lock *mmrange) {</span>
 	return -ENODEV;
 }
 
<span class="p_header">diff --git a/arch/powerpc/mm/copro_fault.c b/arch/powerpc/mm/copro_fault.c</span>
<span class="p_header">index 697b70ad1195..8f5e604828a1 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/copro_fault.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/copro_fault.c</span>
<span class="p_chunk">@@ -39,6 +39,7 @@</span> <span class="p_context"> int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
 	struct vm_area_struct *vma;
 	unsigned long is_write;
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (mm == NULL)
 		return -EFAULT;
<span class="p_chunk">@@ -77,7 +78,8 @@</span> <span class="p_context"> int copro_handle_mm_fault(struct mm_struct *mm, unsigned long ea,</span>
 	}
 
 	ret = 0;
<span class="p_del">-	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0);</span>
<span class="p_add">+	*flt = handle_mm_fault(vma, ea, is_write ? FAULT_FLAG_WRITE : 0,</span>
<span class="p_add">+			       &amp;mmrange);</span>
 	if (unlikely(*flt &amp; VM_FAULT_ERROR)) {
 		if (*flt &amp; VM_FAULT_OOM) {
 			ret = -ENOMEM;
<span class="p_header">diff --git a/arch/powerpc/mm/fault.c b/arch/powerpc/mm/fault.c</span>
<span class="p_header">index 866446cf2d9a..d562dc88687d 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/fault.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/fault.c</span>
<span class="p_chunk">@@ -399,6 +399,7 @@</span> <span class="p_context"> static int __do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	int is_write = page_fault_is_write(error_code);
 	int fault, major = 0;
 	bool store_update_sp = false;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (notify_page_fault(regs))
 		return 0;
<span class="p_chunk">@@ -514,7 +515,7 @@</span> <span class="p_context"> static int __do_page_fault(struct pt_regs *regs, unsigned long address,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 #ifdef CONFIG_PPC_MEM_KEYS
 	/*
<span class="p_header">diff --git a/arch/powerpc/platforms/powernv/npu-dma.c b/arch/powerpc/platforms/powernv/npu-dma.c</span>
<span class="p_header">index 0a253b64ac5f..759e9a4c7479 100644</span>
<span class="p_header">--- a/arch/powerpc/platforms/powernv/npu-dma.c</span>
<span class="p_header">+++ b/arch/powerpc/platforms/powernv/npu-dma.c</span>
<span class="p_chunk">@@ -789,7 +789,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(pnv_npu2_destroy_context);</span>
  * Assumes mmap_sem is held for the contexts associated mm.
  */
 int pnv_npu2_handle_fault(struct npu_context *context, uintptr_t *ea,
<span class="p_del">-			unsigned long *flags, unsigned long *status, int count)</span>
<span class="p_add">+			  unsigned long *flags, unsigned long *status,</span>
<span class="p_add">+			  int count, struct range_lock *mmrange)</span>
 {
 	u64 rc = 0, result = 0;
 	int i, is_write;
<span class="p_chunk">@@ -807,7 +808,7 @@</span> <span class="p_context"> int pnv_npu2_handle_fault(struct npu_context *context, uintptr_t *ea,</span>
 		is_write = flags[i] &amp; NPU2_WRITE;
 		rc = get_user_pages_remote(NULL, mm, ea[i], 1,
 					is_write ? FOLL_WRITE : 0,
<span class="p_del">-					page, NULL, NULL);</span>
<span class="p_add">+					page, NULL, NULL, mmrange);</span>
 
 		/*
 		 * To support virtualised environments we will have to do an
<span class="p_header">diff --git a/arch/riscv/mm/fault.c b/arch/riscv/mm/fault.c</span>
<span class="p_header">index 148c98ca9b45..75d15e73ba39 100644</span>
<span class="p_header">--- a/arch/riscv/mm/fault.c</span>
<span class="p_header">+++ b/arch/riscv/mm/fault.c</span>
<span class="p_chunk">@@ -42,6 +42,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs)</span>
 	unsigned long addr, cause;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 	int fault, code = SEGV_MAPERR;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	cause = regs-&gt;scause;
 	addr = regs-&gt;sbadaddr;
<span class="p_chunk">@@ -119,7 +120,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs)</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, addr, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, addr, flags, &amp;mmrange);</span>
 
 	/*
 	 * If we need to retry but a fatal signal is pending, handle the
<span class="p_header">diff --git a/arch/s390/include/asm/gmap.h b/arch/s390/include/asm/gmap.h</span>
<span class="p_header">index e07cce88dfb0..117c19a947c9 100644</span>
<span class="p_header">--- a/arch/s390/include/asm/gmap.h</span>
<span class="p_header">+++ b/arch/s390/include/asm/gmap.h</span>
<span class="p_chunk">@@ -107,22 +107,24 @@</span> <span class="p_context"> void gmap_discard(struct gmap *, unsigned long from, unsigned long to);</span>
 void __gmap_zap(struct gmap *, unsigned long gaddr);
 void gmap_unlink(struct mm_struct *, unsigned long *table, unsigned long vmaddr);
 
<span class="p_del">-int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val);</span>
<span class="p_add">+int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val,</span>
<span class="p_add">+		    struct range_lock *mmrange);</span>
 
 struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,
 			 int edat_level);
 int gmap_shadow_valid(struct gmap *sg, unsigned long asce, int edat_level);
 int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t,
<span class="p_del">-		    int fake);</span>
<span class="p_add">+		    int fake, struct range_lock *mmrange);</span>
 int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t,
<span class="p_del">-		    int fake);</span>
<span class="p_add">+		    int fake, struct range_lock *mmrange);</span>
 int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt,
<span class="p_del">-		    int fake);</span>
<span class="p_add">+		    int fake, struct range_lock *mmrange);</span>
 int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt,
<span class="p_del">-		    int fake);</span>
<span class="p_add">+		    int fake, struct range_lock *mmrange);</span>
 int gmap_shadow_pgt_lookup(struct gmap *sg, unsigned long saddr,
 			   unsigned long *pgt, int *dat_protection, int *fake);
<span class="p_del">-int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte);</span>
<span class="p_add">+int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte,</span>
<span class="p_add">+		     struct range_lock *mmrange);</span>
 
 void gmap_register_pte_notifier(struct gmap_notifier *);
 void gmap_unregister_pte_notifier(struct gmap_notifier *);
<span class="p_header">diff --git a/arch/s390/kvm/gaccess.c b/arch/s390/kvm/gaccess.c</span>
<span class="p_header">index c24bfa72baf7..ff739b86df36 100644</span>
<span class="p_header">--- a/arch/s390/kvm/gaccess.c</span>
<span class="p_header">+++ b/arch/s390/kvm/gaccess.c</span>
<span class="p_chunk">@@ -978,10 +978,11 @@</span> <span class="p_context"> int kvm_s390_check_low_addr_prot_real(struct kvm_vcpu *vcpu, unsigned long gra)</span>
  * @saddr: faulting address in the shadow gmap
  * @pgt: pointer to the page table address result
  * @fake: pgt references contiguous guest memory block, not a pgtable
<span class="p_add">+ * @mmrange: address space range locking</span>
  */
 static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,
 				  unsigned long *pgt, int *dat_protection,
<span class="p_del">-				  int *fake)</span>
<span class="p_add">+				  int *fake, struct range_lock *mmrange)</span>
 {
 	struct gmap *parent;
 	union asce asce;
<span class="p_chunk">@@ -1034,7 +1035,8 @@</span> <span class="p_context"> static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
 			rfte.val = ptr;
 			goto shadow_r2t;
 		}
<span class="p_del">-		rc = gmap_read_table(parent, ptr + vaddr.rfx * 8, &amp;rfte.val);</span>
<span class="p_add">+		rc = gmap_read_table(parent, ptr + vaddr.rfx * 8, &amp;rfte.val,</span>
<span class="p_add">+				     mmrange);</span>
 		if (rc)
 			return rc;
 		if (rfte.i)
<span class="p_chunk">@@ -1047,7 +1049,7 @@</span> <span class="p_context"> static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
 			*dat_protection |= rfte.p;
 		ptr = rfte.rto * PAGE_SIZE;
 shadow_r2t:
<span class="p_del">-		rc = gmap_shadow_r2t(sg, saddr, rfte.val, *fake);</span>
<span class="p_add">+		rc = gmap_shadow_r2t(sg, saddr, rfte.val, *fake, mmrange);</span>
 		if (rc)
 			return rc;
 		/* fallthrough */
<span class="p_chunk">@@ -1060,7 +1062,8 @@</span> <span class="p_context"> static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
 			rste.val = ptr;
 			goto shadow_r3t;
 		}
<span class="p_del">-		rc = gmap_read_table(parent, ptr + vaddr.rsx * 8, &amp;rste.val);</span>
<span class="p_add">+		rc = gmap_read_table(parent, ptr + vaddr.rsx * 8, &amp;rste.val,</span>
<span class="p_add">+				     mmrange);</span>
 		if (rc)
 			return rc;
 		if (rste.i)
<span class="p_chunk">@@ -1074,7 +1077,7 @@</span> <span class="p_context"> static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
 		ptr = rste.rto * PAGE_SIZE;
 shadow_r3t:
 		rste.p |= *dat_protection;
<span class="p_del">-		rc = gmap_shadow_r3t(sg, saddr, rste.val, *fake);</span>
<span class="p_add">+		rc = gmap_shadow_r3t(sg, saddr, rste.val, *fake, mmrange);</span>
 		if (rc)
 			return rc;
 		/* fallthrough */
<span class="p_chunk">@@ -1087,7 +1090,8 @@</span> <span class="p_context"> static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
 			rtte.val = ptr;
 			goto shadow_sgt;
 		}
<span class="p_del">-		rc = gmap_read_table(parent, ptr + vaddr.rtx * 8, &amp;rtte.val);</span>
<span class="p_add">+		rc = gmap_read_table(parent, ptr + vaddr.rtx * 8, &amp;rtte.val,</span>
<span class="p_add">+				     mmrange);</span>
 		if (rc)
 			return rc;
 		if (rtte.i)
<span class="p_chunk">@@ -1110,7 +1114,7 @@</span> <span class="p_context"> static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
 		ptr = rtte.fc0.sto * PAGE_SIZE;
 shadow_sgt:
 		rtte.fc0.p |= *dat_protection;
<span class="p_del">-		rc = gmap_shadow_sgt(sg, saddr, rtte.val, *fake);</span>
<span class="p_add">+		rc = gmap_shadow_sgt(sg, saddr, rtte.val, *fake, mmrange);</span>
 		if (rc)
 			return rc;
 		/* fallthrough */
<span class="p_chunk">@@ -1123,7 +1127,8 @@</span> <span class="p_context"> static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
 			ste.val = ptr;
 			goto shadow_pgt;
 		}
<span class="p_del">-		rc = gmap_read_table(parent, ptr + vaddr.sx * 8, &amp;ste.val);</span>
<span class="p_add">+		rc = gmap_read_table(parent, ptr + vaddr.sx * 8, &amp;ste.val,</span>
<span class="p_add">+				     mmrange);</span>
 		if (rc)
 			return rc;
 		if (ste.i)
<span class="p_chunk">@@ -1142,7 +1147,7 @@</span> <span class="p_context"> static int kvm_s390_shadow_tables(struct gmap *sg, unsigned long saddr,</span>
 		ptr = ste.fc0.pto * (PAGE_SIZE / 2);
 shadow_pgt:
 		ste.fc0.p |= *dat_protection;
<span class="p_del">-		rc = gmap_shadow_pgt(sg, saddr, ste.val, *fake);</span>
<span class="p_add">+		rc = gmap_shadow_pgt(sg, saddr, ste.val, *fake, mmrange);</span>
 		if (rc)
 			return rc;
 	}
<span class="p_chunk">@@ -1172,6 +1177,7 @@</span> <span class="p_context"> int kvm_s390_shadow_fault(struct kvm_vcpu *vcpu, struct gmap *sg,</span>
 	unsigned long pgt;
 	int dat_protection, fake;
 	int rc;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_read(&amp;sg-&gt;mm-&gt;mmap_sem);
 	/*
<span class="p_chunk">@@ -1184,7 +1190,7 @@</span> <span class="p_context"> int kvm_s390_shadow_fault(struct kvm_vcpu *vcpu, struct gmap *sg,</span>
 	rc = gmap_shadow_pgt_lookup(sg, saddr, &amp;pgt, &amp;dat_protection, &amp;fake);
 	if (rc)
 		rc = kvm_s390_shadow_tables(sg, saddr, &amp;pgt, &amp;dat_protection,
<span class="p_del">-					    &amp;fake);</span>
<span class="p_add">+					    &amp;fake, &amp;mmrange);</span>
 
 	vaddr.addr = saddr;
 	if (fake) {
<span class="p_chunk">@@ -1192,7 +1198,8 @@</span> <span class="p_context"> int kvm_s390_shadow_fault(struct kvm_vcpu *vcpu, struct gmap *sg,</span>
 		goto shadow_page;
 	}
 	if (!rc)
<span class="p_del">-		rc = gmap_read_table(sg-&gt;parent, pgt + vaddr.px * 8, &amp;pte.val);</span>
<span class="p_add">+		rc = gmap_read_table(sg-&gt;parent, pgt + vaddr.px * 8,</span>
<span class="p_add">+				     &amp;pte.val, &amp;mmrange);</span>
 	if (!rc &amp;&amp; pte.i)
 		rc = PGM_PAGE_TRANSLATION;
 	if (!rc &amp;&amp; pte.z)
<span class="p_chunk">@@ -1200,7 +1207,7 @@</span> <span class="p_context"> int kvm_s390_shadow_fault(struct kvm_vcpu *vcpu, struct gmap *sg,</span>
 shadow_page:
 	pte.p |= dat_protection;
 	if (!rc)
<span class="p_del">-		rc = gmap_shadow_page(sg, saddr, __pte(pte.val));</span>
<span class="p_add">+		rc = gmap_shadow_page(sg, saddr, __pte(pte.val), &amp;mmrange);</span>
 	ipte_unlock(vcpu);
 	up_read(&amp;sg-&gt;mm-&gt;mmap_sem);
 	return rc;
<span class="p_header">diff --git a/arch/s390/mm/fault.c b/arch/s390/mm/fault.c</span>
<span class="p_header">index 93faeca52284..17ba3c402f9d 100644</span>
<span class="p_header">--- a/arch/s390/mm/fault.c</span>
<span class="p_header">+++ b/arch/s390/mm/fault.c</span>
<span class="p_chunk">@@ -421,6 +421,7 @@</span> <span class="p_context"> static inline int do_exception(struct pt_regs *regs, int access)</span>
 	unsigned long address;
 	unsigned int flags;
 	int fault;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	tsk = current;
 	/*
<span class="p_chunk">@@ -507,7 +508,7 @@</span> <span class="p_context"> static inline int do_exception(struct pt_regs *regs, int access)</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 	/* No reason to continue if interrupted by SIGKILL. */
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current)) {
 		fault = VM_FAULT_SIGNAL;
<span class="p_header">diff --git a/arch/s390/mm/gmap.c b/arch/s390/mm/gmap.c</span>
<span class="p_header">index 2c55a2b9d6c6..b12a44813022 100644</span>
<span class="p_header">--- a/arch/s390/mm/gmap.c</span>
<span class="p_header">+++ b/arch/s390/mm/gmap.c</span>
<span class="p_chunk">@@ -621,6 +621,7 @@</span> <span class="p_context"> int gmap_fault(struct gmap *gmap, unsigned long gaddr,</span>
 	unsigned long vmaddr;
 	int rc;
 	bool unlocked;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_read(&amp;gmap-&gt;mm-&gt;mmap_sem);
 
<span class="p_chunk">@@ -632,7 +633,7 @@</span> <span class="p_context"> int gmap_fault(struct gmap *gmap, unsigned long gaddr,</span>
 		goto out_up;
 	}
 	if (fixup_user_fault(current, gmap-&gt;mm, vmaddr, fault_flags,
<span class="p_del">-			     &amp;unlocked)) {</span>
<span class="p_add">+			     &amp;unlocked, &amp;mmrange)) {</span>
 		rc = -EFAULT;
 		goto out_up;
 	}
<span class="p_chunk">@@ -835,13 +836,15 @@</span> <span class="p_context"> static pte_t *gmap_pte_op_walk(struct gmap *gmap, unsigned long gaddr,</span>
  * @gaddr: virtual address in the guest address space
  * @vmaddr: address in the host process address space
  * @prot: indicates access rights: PROT_NONE, PROT_READ or PROT_WRITE
<span class="p_add">+ * @mmrange: address space range locking</span>
  *
  * Returns 0 if the caller can retry __gmap_translate (might fail again),
  * -ENOMEM if out of memory and -EFAULT if anything goes wrong while fixing
  * up or connecting the gmap page table.
  */
 static int gmap_pte_op_fixup(struct gmap *gmap, unsigned long gaddr,
<span class="p_del">-			     unsigned long vmaddr, int prot)</span>
<span class="p_add">+			     unsigned long vmaddr, int prot,</span>
<span class="p_add">+			     struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = gmap-&gt;mm;
 	unsigned int fault_flags;
<span class="p_chunk">@@ -849,7 +852,8 @@</span> <span class="p_context"> static int gmap_pte_op_fixup(struct gmap *gmap, unsigned long gaddr,</span>
 
 	BUG_ON(gmap_is_shadow(gmap));
 	fault_flags = (prot == PROT_WRITE) ? FAULT_FLAG_WRITE : 0;
<span class="p_del">-	if (fixup_user_fault(current, mm, vmaddr, fault_flags, &amp;unlocked))</span>
<span class="p_add">+	if (fixup_user_fault(current, mm, vmaddr, fault_flags, &amp;unlocked,</span>
<span class="p_add">+			     mmrange))</span>
 		return -EFAULT;
 	if (unlocked)
 		/* lost mmap_sem, caller has to retry __gmap_translate */
<span class="p_chunk">@@ -874,6 +878,7 @@</span> <span class="p_context"> static void gmap_pte_op_end(spinlock_t *ptl)</span>
  * @len: size of area
  * @prot: indicates access rights: PROT_NONE, PROT_READ or PROT_WRITE
  * @bits: pgste notification bits to set
<span class="p_add">+ * @mmrange: address space range locking</span>
  *
  * Returns 0 if successfully protected, -ENOMEM if out of memory and
  * -EFAULT if gaddr is invalid (or mapping for shadows is missing).
<span class="p_chunk">@@ -881,7 +886,8 @@</span> <span class="p_context"> static void gmap_pte_op_end(spinlock_t *ptl)</span>
  * Called with sg-&gt;mm-&gt;mmap_sem in read.
  */
 static int gmap_protect_range(struct gmap *gmap, unsigned long gaddr,
<span class="p_del">-			      unsigned long len, int prot, unsigned long bits)</span>
<span class="p_add">+			      unsigned long len, int prot, unsigned long bits,</span>
<span class="p_add">+			      struct range_lock *mmrange)</span>
 {
 	unsigned long vmaddr;
 	spinlock_t *ptl;
<span class="p_chunk">@@ -900,7 +906,8 @@</span> <span class="p_context"> static int gmap_protect_range(struct gmap *gmap, unsigned long gaddr,</span>
 			vmaddr = __gmap_translate(gmap, gaddr);
 			if (IS_ERR_VALUE(vmaddr))
 				return vmaddr;
<span class="p_del">-			rc = gmap_pte_op_fixup(gmap, gaddr, vmaddr, prot);</span>
<span class="p_add">+			rc = gmap_pte_op_fixup(gmap, gaddr, vmaddr, prot,</span>
<span class="p_add">+					       mmrange);</span>
 			if (rc)
 				return rc;
 			continue;
<span class="p_chunk">@@ -929,13 +936,14 @@</span> <span class="p_context"> int gmap_mprotect_notify(struct gmap *gmap, unsigned long gaddr,</span>
 			 unsigned long len, int prot)
 {
 	int rc;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if ((gaddr &amp; ~PAGE_MASK) || (len &amp; ~PAGE_MASK) || gmap_is_shadow(gmap))
 		return -EINVAL;
 	if (!MACHINE_HAS_ESOP &amp;&amp; prot == PROT_READ)
 		return -EINVAL;
 	down_read(&amp;gmap-&gt;mm-&gt;mmap_sem);
<span class="p_del">-	rc = gmap_protect_range(gmap, gaddr, len, prot, PGSTE_IN_BIT);</span>
<span class="p_add">+	rc = gmap_protect_range(gmap, gaddr, len, prot, PGSTE_IN_BIT, &amp;mmrange);</span>
 	up_read(&amp;gmap-&gt;mm-&gt;mmap_sem);
 	return rc;
 }
<span class="p_chunk">@@ -947,6 +955,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_mprotect_notify);</span>
  * @gmap: pointer to guest mapping meta data structure
  * @gaddr: virtual address in the guest address space
  * @val: pointer to the unsigned long value to return
<span class="p_add">+ * @mmrange: address space range locking</span>
  *
  * Returns 0 if the value was read, -ENOMEM if out of memory and -EFAULT
  * if reading using the virtual address failed. -EINVAL if called on a gmap
<span class="p_chunk">@@ -954,7 +963,8 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_mprotect_notify);</span>
  *
  * Called with gmap-&gt;mm-&gt;mmap_sem in read.
  */
<span class="p_del">-int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val)</span>
<span class="p_add">+int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val,</span>
<span class="p_add">+		    struct range_lock *mmrange)</span>
 {
 	unsigned long address, vmaddr;
 	spinlock_t *ptl;
<span class="p_chunk">@@ -986,7 +996,7 @@</span> <span class="p_context"> int gmap_read_table(struct gmap *gmap, unsigned long gaddr, unsigned long *val)</span>
 			rc = vmaddr;
 			break;
 		}
<span class="p_del">-		rc = gmap_pte_op_fixup(gmap, gaddr, vmaddr, PROT_READ);</span>
<span class="p_add">+		rc = gmap_pte_op_fixup(gmap, gaddr, vmaddr, PROT_READ, mmrange);</span>
 		if (rc)
 			break;
 	}
<span class="p_chunk">@@ -1026,12 +1036,14 @@</span> <span class="p_context"> static inline void gmap_insert_rmap(struct gmap *sg, unsigned long vmaddr,</span>
  * @raddr: rmap address in the shadow gmap
  * @paddr: address in the parent guest address space
  * @len: length of the memory area to protect
<span class="p_add">+ * @mmrange: address space range locking</span>
  *
  * Returns 0 if successfully protected and the rmap was created, -ENOMEM
  * if out of memory and -EFAULT if paddr is invalid.
  */
 static int gmap_protect_rmap(struct gmap *sg, unsigned long raddr,
<span class="p_del">-			     unsigned long paddr, unsigned long len)</span>
<span class="p_add">+			     unsigned long paddr, unsigned long len,</span>
<span class="p_add">+			     struct range_lock *mmrange)</span>
 {
 	struct gmap *parent;
 	struct gmap_rmap *rmap;
<span class="p_chunk">@@ -1069,7 +1081,7 @@</span> <span class="p_context"> static int gmap_protect_rmap(struct gmap *sg, unsigned long raddr,</span>
 		radix_tree_preload_end();
 		if (rc) {
 			kfree(rmap);
<span class="p_del">-			rc = gmap_pte_op_fixup(parent, paddr, vmaddr, PROT_READ);</span>
<span class="p_add">+			rc = gmap_pte_op_fixup(parent, paddr, vmaddr, PROT_READ, mmrange);</span>
 			if (rc)
 				return rc;
 			continue;
<span class="p_chunk">@@ -1473,6 +1485,7 @@</span> <span class="p_context"> struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,</span>
 	struct gmap *sg, *new;
 	unsigned long limit;
 	int rc;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	BUG_ON(gmap_is_shadow(parent));
 	spin_lock(&amp;parent-&gt;shadow_lock);
<span class="p_chunk">@@ -1526,7 +1539,7 @@</span> <span class="p_context"> struct gmap *gmap_shadow(struct gmap *parent, unsigned long asce,</span>
 	down_read(&amp;parent-&gt;mm-&gt;mmap_sem);
 	rc = gmap_protect_range(parent, asce &amp; _ASCE_ORIGIN,
 				((asce &amp; _ASCE_TABLE_LENGTH) + 1) * PAGE_SIZE,
<span class="p_del">-				PROT_READ, PGSTE_VSIE_BIT);</span>
<span class="p_add">+				PROT_READ, PGSTE_VSIE_BIT, &amp;mmrange);</span>
 	up_read(&amp;parent-&gt;mm-&gt;mmap_sem);
 	spin_lock(&amp;parent-&gt;shadow_lock);
 	new-&gt;initialized = true;
<span class="p_chunk">@@ -1546,6 +1559,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_shadow);</span>
  * @saddr: faulting address in the shadow gmap
  * @r2t: parent gmap address of the region 2 table to get shadowed
  * @fake: r2t references contiguous guest memory block, not a r2t
<span class="p_add">+ * @mmrange: address space range locking</span>
  *
  * The r2t parameter specifies the address of the source table. The
  * four pages of the source table are made read-only in the parent gmap
<span class="p_chunk">@@ -1559,7 +1573,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_shadow);</span>
  * Called with sg-&gt;mm-&gt;mmap_sem in read.
  */
 int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t,
<span class="p_del">-		    int fake)</span>
<span class="p_add">+		    int fake, struct range_lock *mmrange)</span>
 {
 	unsigned long raddr, origin, offset, len;
 	unsigned long *s_r2t, *table;
<span class="p_chunk">@@ -1608,7 +1622,7 @@</span> <span class="p_context"> int gmap_shadow_r2t(struct gmap *sg, unsigned long saddr, unsigned long r2t,</span>
 	origin = r2t &amp; _REGION_ENTRY_ORIGIN;
 	offset = ((r2t &amp; _REGION_ENTRY_OFFSET) &gt;&gt; 6) * PAGE_SIZE;
 	len = ((r2t &amp; _REGION_ENTRY_LENGTH) + 1) * PAGE_SIZE - offset;
<span class="p_del">-	rc = gmap_protect_rmap(sg, raddr, origin + offset, len);</span>
<span class="p_add">+	rc = gmap_protect_rmap(sg, raddr, origin + offset, len, mmrange);</span>
 	spin_lock(&amp;sg-&gt;guest_table_lock);
 	if (!rc) {
 		table = gmap_table_walk(sg, saddr, 4);
<span class="p_chunk">@@ -1635,6 +1649,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_shadow_r2t);</span>
  * @saddr: faulting address in the shadow gmap
  * @r3t: parent gmap address of the region 3 table to get shadowed
  * @fake: r3t references contiguous guest memory block, not a r3t
<span class="p_add">+ * @mmrange: address space range locking</span>
  *
  * Returns 0 if successfully shadowed or already shadowed, -EAGAIN if the
  * shadow table structure is incomplete, -ENOMEM if out of memory and
<span class="p_chunk">@@ -1643,7 +1658,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_shadow_r2t);</span>
  * Called with sg-&gt;mm-&gt;mmap_sem in read.
  */
 int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t,
<span class="p_del">-		    int fake)</span>
<span class="p_add">+		    int fake, struct range_lock *mmrange)</span>
 {
 	unsigned long raddr, origin, offset, len;
 	unsigned long *s_r3t, *table;
<span class="p_chunk">@@ -1691,7 +1706,7 @@</span> <span class="p_context"> int gmap_shadow_r3t(struct gmap *sg, unsigned long saddr, unsigned long r3t,</span>
 	origin = r3t &amp; _REGION_ENTRY_ORIGIN;
 	offset = ((r3t &amp; _REGION_ENTRY_OFFSET) &gt;&gt; 6) * PAGE_SIZE;
 	len = ((r3t &amp; _REGION_ENTRY_LENGTH) + 1) * PAGE_SIZE - offset;
<span class="p_del">-	rc = gmap_protect_rmap(sg, raddr, origin + offset, len);</span>
<span class="p_add">+	rc = gmap_protect_rmap(sg, raddr, origin + offset, len, mmrange);</span>
 	spin_lock(&amp;sg-&gt;guest_table_lock);
 	if (!rc) {
 		table = gmap_table_walk(sg, saddr, 3);
<span class="p_chunk">@@ -1718,6 +1733,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_shadow_r3t);</span>
  * @saddr: faulting address in the shadow gmap
  * @sgt: parent gmap address of the segment table to get shadowed
  * @fake: sgt references contiguous guest memory block, not a sgt
<span class="p_add">+ * @mmrange: address space range locking</span>
  *
  * Returns: 0 if successfully shadowed or already shadowed, -EAGAIN if the
  * shadow table structure is incomplete, -ENOMEM if out of memory and
<span class="p_chunk">@@ -1726,7 +1742,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_shadow_r3t);</span>
  * Called with sg-&gt;mm-&gt;mmap_sem in read.
  */
 int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt,
<span class="p_del">-		    int fake)</span>
<span class="p_add">+		    int fake, struct range_lock *mmrange)</span>
 {
 	unsigned long raddr, origin, offset, len;
 	unsigned long *s_sgt, *table;
<span class="p_chunk">@@ -1775,7 +1791,7 @@</span> <span class="p_context"> int gmap_shadow_sgt(struct gmap *sg, unsigned long saddr, unsigned long sgt,</span>
 	origin = sgt &amp; _REGION_ENTRY_ORIGIN;
 	offset = ((sgt &amp; _REGION_ENTRY_OFFSET) &gt;&gt; 6) * PAGE_SIZE;
 	len = ((sgt &amp; _REGION_ENTRY_LENGTH) + 1) * PAGE_SIZE - offset;
<span class="p_del">-	rc = gmap_protect_rmap(sg, raddr, origin + offset, len);</span>
<span class="p_add">+	rc = gmap_protect_rmap(sg, raddr, origin + offset, len, mmrange);</span>
 	spin_lock(&amp;sg-&gt;guest_table_lock);
 	if (!rc) {
 		table = gmap_table_walk(sg, saddr, 2);
<span class="p_chunk">@@ -1842,6 +1858,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_shadow_pgt_lookup);</span>
  * @saddr: faulting address in the shadow gmap
  * @pgt: parent gmap address of the page table to get shadowed
  * @fake: pgt references contiguous guest memory block, not a pgtable
<span class="p_add">+ * @mmrange: address space range locking</span>
  *
  * Returns 0 if successfully shadowed or already shadowed, -EAGAIN if the
  * shadow table structure is incomplete, -ENOMEM if out of memory,
<span class="p_chunk">@@ -1850,7 +1867,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_shadow_pgt_lookup);</span>
  * Called with gmap-&gt;mm-&gt;mmap_sem in read
  */
 int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt,
<span class="p_del">-		    int fake)</span>
<span class="p_add">+		    int fake, struct range_lock *mmrange)</span>
 {
 	unsigned long raddr, origin;
 	unsigned long *s_pgt, *table;
<span class="p_chunk">@@ -1894,7 +1911,7 @@</span> <span class="p_context"> int gmap_shadow_pgt(struct gmap *sg, unsigned long saddr, unsigned long pgt,</span>
 	/* Make pgt read-only in parent gmap page table (not the pgste) */
 	raddr = (saddr &amp; _SEGMENT_MASK) | _SHADOW_RMAP_SEGMENT;
 	origin = pgt &amp; _SEGMENT_ENTRY_ORIGIN &amp; PAGE_MASK;
<span class="p_del">-	rc = gmap_protect_rmap(sg, raddr, origin, PAGE_SIZE);</span>
<span class="p_add">+	rc = gmap_protect_rmap(sg, raddr, origin, PAGE_SIZE, mmrange);</span>
 	spin_lock(&amp;sg-&gt;guest_table_lock);
 	if (!rc) {
 		table = gmap_table_walk(sg, saddr, 1);
<span class="p_chunk">@@ -1921,6 +1938,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_shadow_pgt);</span>
  * @sg: pointer to the shadow guest address space structure
  * @saddr: faulting address in the shadow gmap
  * @pte: pte in parent gmap address space to get shadowed
<span class="p_add">+ * @mmrange: address space range locking</span>
  *
  * Returns 0 if successfully shadowed or already shadowed, -EAGAIN if the
  * shadow table structure is incomplete, -ENOMEM if out of memory and
<span class="p_chunk">@@ -1928,7 +1946,8 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(gmap_shadow_pgt);</span>
  *
  * Called with sg-&gt;mm-&gt;mmap_sem in read.
  */
<span class="p_del">-int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte)</span>
<span class="p_add">+int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte,</span>
<span class="p_add">+		     struct range_lock *mmrange)</span>
 {
 	struct gmap *parent;
 	struct gmap_rmap *rmap;
<span class="p_chunk">@@ -1982,7 +2001,7 @@</span> <span class="p_context"> int gmap_shadow_page(struct gmap *sg, unsigned long saddr, pte_t pte)</span>
 		radix_tree_preload_end();
 		if (!rc)
 			break;
<span class="p_del">-		rc = gmap_pte_op_fixup(parent, paddr, vmaddr, prot);</span>
<span class="p_add">+		rc = gmap_pte_op_fixup(parent, paddr, vmaddr, prot, mmrange);</span>
 		if (rc)
 			break;
 	}
<span class="p_chunk">@@ -2117,7 +2136,8 @@</span> <span class="p_context"> static inline void thp_split_mm(struct mm_struct *mm)</span>
  * - This must be called after THP was enabled
  */
 static int __zap_zero_pages(pmd_t *pmd, unsigned long start,
<span class="p_del">-			   unsigned long end, struct mm_walk *walk)</span>
<span class="p_add">+			    unsigned long end, struct mm_walk *walk,</span>
<span class="p_add">+			    struct range_lock *mmrange)</span>
 {
 	unsigned long addr;
 
<span class="p_chunk">@@ -2133,12 +2153,13 @@</span> <span class="p_context"> static int __zap_zero_pages(pmd_t *pmd, unsigned long start,</span>
 	return 0;
 }
 
<span class="p_del">-static inline void zap_zero_pages(struct mm_struct *mm)</span>
<span class="p_add">+static inline void zap_zero_pages(struct mm_struct *mm,</span>
<span class="p_add">+				  struct range_lock *mmrange)</span>
 {
 	struct mm_walk walk = { .pmd_entry = __zap_zero_pages };
 
 	walk.mm = mm;
<span class="p_del">-	walk_page_range(0, TASK_SIZE, &amp;walk);</span>
<span class="p_add">+	walk_page_range(0, TASK_SIZE, &amp;walk, mmrange);</span>
 }
 
 /*
<span class="p_chunk">@@ -2147,6 +2168,7 @@</span> <span class="p_context"> static inline void zap_zero_pages(struct mm_struct *mm)</span>
 int s390_enable_sie(void)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* Do we have pgstes? if yes, we are done */
 	if (mm_has_pgste(mm))
<span class="p_chunk">@@ -2158,7 +2180,7 @@</span> <span class="p_context"> int s390_enable_sie(void)</span>
 	mm-&gt;context.has_pgste = 1;
 	/* split thp mappings and disable thp for future mappings */
 	thp_split_mm(mm);
<span class="p_del">-	zap_zero_pages(mm);</span>
<span class="p_add">+	zap_zero_pages(mm, &amp;mmrange);</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 	return 0;
 }
<span class="p_chunk">@@ -2182,6 +2204,7 @@</span> <span class="p_context"> int s390_enable_skey(void)</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
 	int rc = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_write(&amp;mm-&gt;mmap_sem);
 	if (mm_use_skey(mm))
<span class="p_chunk">@@ -2190,7 +2213,7 @@</span> <span class="p_context"> int s390_enable_skey(void)</span>
 	mm-&gt;context.use_skey = 1;
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
 		if (ksm_madvise(vma, vma-&gt;vm_start, vma-&gt;vm_end,
<span class="p_del">-				MADV_UNMERGEABLE, &amp;vma-&gt;vm_flags)) {</span>
<span class="p_add">+				MADV_UNMERGEABLE, &amp;vma-&gt;vm_flags, &amp;mmrange)) {</span>
 			mm-&gt;context.use_skey = 0;
 			rc = -ENOMEM;
 			goto out_up;
<span class="p_chunk">@@ -2199,7 +2222,7 @@</span> <span class="p_context"> int s390_enable_skey(void)</span>
 	mm-&gt;def_flags &amp;= ~VM_MERGEABLE;
 
 	walk.mm = mm;
<span class="p_del">-	walk_page_range(0, TASK_SIZE, &amp;walk);</span>
<span class="p_add">+	walk_page_range(0, TASK_SIZE, &amp;walk, &amp;mmrange);</span>
 
 out_up:
 	up_write(&amp;mm-&gt;mmap_sem);
<span class="p_chunk">@@ -2220,10 +2243,11 @@</span> <span class="p_context"> static int __s390_reset_cmma(pte_t *pte, unsigned long addr,</span>
 void s390_reset_cmma(struct mm_struct *mm)
 {
 	struct mm_walk walk = { .pte_entry = __s390_reset_cmma };
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_write(&amp;mm-&gt;mmap_sem);
 	walk.mm = mm;
<span class="p_del">-	walk_page_range(0, TASK_SIZE, &amp;walk);</span>
<span class="p_add">+	walk_page_range(0, TASK_SIZE, &amp;walk, &amp;mmrange);</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 }
 EXPORT_SYMBOL_GPL(s390_reset_cmma);
<span class="p_header">diff --git a/arch/score/mm/fault.c b/arch/score/mm/fault.c</span>
<span class="p_header">index b85fad4f0874..07a8637ad142 100644</span>
<span class="p_header">--- a/arch/score/mm/fault.c</span>
<span class="p_header">+++ b/arch/score/mm/fault.c</span>
<span class="p_chunk">@@ -51,6 +51,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,</span>
 	unsigned long flags = 0;
 	siginfo_t info;
 	int fault;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	info.si_code = SEGV_MAPERR;
 
<span class="p_chunk">@@ -111,7 +112,7 @@</span> <span class="p_context"> asmlinkage void do_page_fault(struct pt_regs *regs, unsigned long write,</span>
 	* make sure we exit gracefully rather than endlessly redo
 	* the fault.
 	*/
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, mmrange);</span>
 	if (unlikely(fault &amp; VM_FAULT_ERROR)) {
 		if (fault &amp; VM_FAULT_OOM)
 			goto out_of_memory;
<span class="p_header">diff --git a/arch/sh/mm/fault.c b/arch/sh/mm/fault.c</span>
<span class="p_header">index 6fd1bf7481c7..d36106564728 100644</span>
<span class="p_header">--- a/arch/sh/mm/fault.c</span>
<span class="p_header">+++ b/arch/sh/mm/fault.c</span>
<span class="p_chunk">@@ -405,6 +405,7 @@</span> <span class="p_context"> asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,</span>
 	struct vm_area_struct * vma;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	tsk = current;
 	mm = tsk-&gt;mm;
<span class="p_chunk">@@ -488,7 +489,7 @@</span> <span class="p_context"> asmlinkage void __kprobes do_page_fault(struct pt_regs *regs,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if (unlikely(fault &amp; (VM_FAULT_RETRY | VM_FAULT_ERROR)))
 		if (mm_fault_error(regs, error_code, address, fault))
<span class="p_header">diff --git a/arch/sparc/mm/fault_32.c b/arch/sparc/mm/fault_32.c</span>
<span class="p_header">index a8103a84b4ac..ebb2406dbe7c 100644</span>
<span class="p_header">--- a/arch/sparc/mm/fault_32.c</span>
<span class="p_header">+++ b/arch/sparc/mm/fault_32.c</span>
<span class="p_chunk">@@ -176,6 +176,7 @@</span> <span class="p_context"> asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,</span>
 	int from_user = !(regs-&gt;psr &amp; PSR_PS);
 	int fault, code;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (text_fault)
 		address = regs-&gt;pc;
<span class="p_chunk">@@ -242,7 +243,7 @@</span> <span class="p_context"> asmlinkage void do_sparc_fault(struct pt_regs *regs, int text_fault, int write,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_chunk">@@ -389,6 +390,7 @@</span> <span class="p_context"> static void force_user_fault(unsigned long address, int write)</span>
 	struct mm_struct *mm = tsk-&gt;mm;
 	unsigned int flags = FAULT_FLAG_USER;
 	int code;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	code = SEGV_MAPERR;
 
<span class="p_chunk">@@ -412,7 +414,7 @@</span> <span class="p_context"> static void force_user_fault(unsigned long address, int write)</span>
 		if (!(vma-&gt;vm_flags &amp; (VM_READ | VM_EXEC)))
 			goto bad_area;
 	}
<span class="p_del">-	switch (handle_mm_fault(vma, address, flags)) {</span>
<span class="p_add">+	switch (handle_mm_fault(vma, address, flags, &amp;mmrange)) {</span>
 	case VM_FAULT_SIGBUS:
 	case VM_FAULT_OOM:
 		goto do_sigbus;
<span class="p_header">diff --git a/arch/sparc/mm/fault_64.c b/arch/sparc/mm/fault_64.c</span>
<span class="p_header">index 41363f46797b..e0a3c36b0fa1 100644</span>
<span class="p_header">--- a/arch/sparc/mm/fault_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/fault_64.c</span>
<span class="p_chunk">@@ -287,6 +287,7 @@</span> <span class="p_context"> asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)</span>
 	int si_code, fault_code, fault;
 	unsigned long address, mm_rss;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	fault_code = get_thread_fault_code();
 
<span class="p_chunk">@@ -438,7 +439,7 @@</span> <span class="p_context"> asmlinkage void __kprobes do_sparc64_fault(struct pt_regs *regs)</span>
 			goto bad_area;
 	}
 
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		goto exit_exception;
<span class="p_header">diff --git a/arch/tile/mm/fault.c b/arch/tile/mm/fault.c</span>
<span class="p_header">index f58fa06a2214..09f053eb146f 100644</span>
<span class="p_header">--- a/arch/tile/mm/fault.c</span>
<span class="p_header">+++ b/arch/tile/mm/fault.c</span>
<span class="p_chunk">@@ -275,6 +275,7 @@</span> <span class="p_context"> static int handle_page_fault(struct pt_regs *regs,</span>
 	int is_kernel_mode;
 	pgd_t *pgd;
 	unsigned int flags;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* on TILE, protection faults are always writes */
 	if (!is_page_fault)
<span class="p_chunk">@@ -437,7 +438,7 @@</span> <span class="p_context"> static int handle_page_fault(struct pt_regs *regs,</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return 0;
<span class="p_header">diff --git a/arch/um/include/asm/mmu_context.h b/arch/um/include/asm/mmu_context.h</span>
<span class="p_header">index fca34b2177e2..98cc3e36385a 100644</span>
<span class="p_header">--- a/arch/um/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/um/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -23,7 +23,8 @@</span> <span class="p_context"> static inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)</span>
 extern void arch_exit_mmap(struct mm_struct *mm);
 static inline void arch_unmap(struct mm_struct *mm,
 			struct vm_area_struct *vma,
<span class="p_del">-			unsigned long start, unsigned long end)</span>
<span class="p_add">+			unsigned long start, unsigned long end,</span>
<span class="p_add">+			struct range_lock *mmrange)</span>
 {
 }
 static inline void arch_bprm_mm_init(struct mm_struct *mm,
<span class="p_header">diff --git a/arch/um/kernel/trap.c b/arch/um/kernel/trap.c</span>
<span class="p_header">index b2b02df9896e..e632a14e896e 100644</span>
<span class="p_header">--- a/arch/um/kernel/trap.c</span>
<span class="p_header">+++ b/arch/um/kernel/trap.c</span>
<span class="p_chunk">@@ -33,6 +33,7 @@</span> <span class="p_context"> int handle_page_fault(unsigned long address, unsigned long ip,</span>
 	pte_t *pte;
 	int err = -EFAULT;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	*code_out = SEGV_MAPERR;
 
<span class="p_chunk">@@ -74,7 +75,7 @@</span> <span class="p_context"> int handle_page_fault(unsigned long address, unsigned long ip,</span>
 	do {
 		int fault;
 
<span class="p_del">-		fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+		fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 		if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 			goto out_nosemaphore;
<span class="p_header">diff --git a/arch/unicore32/mm/fault.c b/arch/unicore32/mm/fault.c</span>
<span class="p_header">index bbefcc46a45e..dd35b6191798 100644</span>
<span class="p_header">--- a/arch/unicore32/mm/fault.c</span>
<span class="p_header">+++ b/arch/unicore32/mm/fault.c</span>
<span class="p_chunk">@@ -168,7 +168,8 @@</span> <span class="p_context"> static inline bool access_error(unsigned int fsr, struct vm_area_struct *vma)</span>
 }
 
 static int __do_pf(struct mm_struct *mm, unsigned long addr, unsigned int fsr,
<span class="p_del">-		unsigned int flags, struct task_struct *tsk)</span>
<span class="p_add">+		   unsigned int flags, struct task_struct *tsk,</span>
<span class="p_add">+		   struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma;
 	int fault;
<span class="p_chunk">@@ -194,7 +195,7 @@</span> <span class="p_context"> static int __do_pf(struct mm_struct *mm, unsigned long addr, unsigned int fsr,</span>
 	 * If for any reason at all we couldn&#39;t handle the fault, make
 	 * sure we exit gracefully rather than endlessly redo the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, addr &amp; PAGE_MASK, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, addr &amp; PAGE_MASK, flags, mmrange);</span>
 	return fault;
 
 check_stack:
<span class="p_chunk">@@ -210,6 +211,7 @@</span> <span class="p_context"> static int do_pf(unsigned long addr, unsigned int fsr, struct pt_regs *regs)</span>
 	struct mm_struct *mm;
 	int fault, sig, code;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	tsk = current;
 	mm = tsk-&gt;mm;
<span class="p_chunk">@@ -251,7 +253,7 @@</span> <span class="p_context"> static int do_pf(unsigned long addr, unsigned int fsr, struct pt_regs *regs)</span>
 #endif
 	}
 
<span class="p_del">-	fault = __do_pf(mm, addr, fsr, flags, tsk);</span>
<span class="p_add">+	fault = __do_pf(mm, addr, fsr, flags, tsk, &amp;mmrange);</span>
 
 	/* If we need to retry but a fatal signal is pending, handle the
 	 * signal first. We do not need to release the mmap_sem because
<span class="p_header">diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">index 5b8b556dbb12..2e0bdf6a3aaf 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vma.c</span>
<span class="p_chunk">@@ -155,6 +155,7 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, unsigned long addr)</span>
 	struct vm_area_struct *vma;
 	unsigned long text_start;
 	int ret = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (down_write_killable(&amp;mm-&gt;mmap_sem))
 		return -EINTR;
<span class="p_chunk">@@ -192,7 +193,7 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, unsigned long addr)</span>
 
 	if (IS_ERR(vma)) {
 		ret = PTR_ERR(vma);
<span class="p_del">-		do_munmap(mm, text_start, image-&gt;size, NULL);</span>
<span class="p_add">+		do_munmap(mm, text_start, image-&gt;size, NULL, &amp;mmrange);</span>
 	} else {
 		current-&gt;mm-&gt;context.vdso = (void __user *)text_start;
 		current-&gt;mm-&gt;context.vdso_image = image;
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index c931b88982a0..31fb02ed4770 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -263,7 +263,8 @@</span> <span class="p_context"> static inline void arch_bprm_mm_init(struct mm_struct *mm,</span>
 }
 
 static inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
<span class="p_del">-			      unsigned long start, unsigned long end)</span>
<span class="p_add">+			      unsigned long start, unsigned long end,</span>
<span class="p_add">+			      struct range_lock *mmrange)</span>
 {
 	/*
 	 * mpx_notify_unmap() goes and reads a rarely-hot
<span class="p_chunk">@@ -283,7 +284,7 @@</span> <span class="p_context"> static inline void arch_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	 * consistently wrong.
 	 */
 	if (unlikely(cpu_feature_enabled(X86_FEATURE_MPX)))
<span class="p_del">-		mpx_notify_unmap(mm, vma, start, end);</span>
<span class="p_add">+		mpx_notify_unmap(mm, vma, start, end, mmrange);</span>
 }
 
 #ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS
<span class="p_header">diff --git a/arch/x86/include/asm/mpx.h b/arch/x86/include/asm/mpx.h</span>
<span class="p_header">index 61eb4b63c5ec..c26099224a17 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mpx.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mpx.h</span>
<span class="p_chunk">@@ -73,7 +73,8 @@</span> <span class="p_context"> static inline void mpx_mm_init(struct mm_struct *mm)</span>
 	mm-&gt;context.bd_addr = MPX_INVALID_BOUNDS_DIR;
 }
 void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
<span class="p_del">-		      unsigned long start, unsigned long end);</span>
<span class="p_add">+		      unsigned long start, unsigned long end,</span>
<span class="p_add">+		      struct range_lock *mmrange);</span>
 
 unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,
 		unsigned long flags);
<span class="p_chunk">@@ -95,7 +96,8 @@</span> <span class="p_context"> static inline void mpx_mm_init(struct mm_struct *mm)</span>
 }
 static inline void mpx_notify_unmap(struct mm_struct *mm,
 				    struct vm_area_struct *vma,
<span class="p_del">-				    unsigned long start, unsigned long end)</span>
<span class="p_add">+				    unsigned long start, unsigned long end,</span>
<span class="p_add">+				    struct range_lock *mmrange)</span>
 {
 }
 
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index 800de815519c..93f1b8d4c88e 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -1244,6 +1244,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	int fault, major = 0;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
 	u32 pkey;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	tsk = current;
 	mm = tsk-&gt;mm;
<span class="p_chunk">@@ -1423,7 +1424,7 @@</span> <span class="p_context"> __do_page_fault(struct pt_regs *regs, unsigned long error_code,</span>
 	 * fault, so we read the pkey beforehand.
 	 */
 	pkey = vma_pkey(vma);
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 	major |= fault &amp; VM_FAULT_MAJOR;
 
 	/*
<span class="p_header">diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="p_header">index e500949bae24..51c3e1f7e6be 100644</span>
<span class="p_header">--- a/arch/x86/mm/mpx.c</span>
<span class="p_header">+++ b/arch/x86/mm/mpx.c</span>
<span class="p_chunk">@@ -47,6 +47,7 @@</span> <span class="p_context"> static unsigned long mpx_mmap(unsigned long len)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long addr, populate;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* Only bounds table can be allocated here */
 	if (len != mpx_bt_size_bytes(mm))
<span class="p_chunk">@@ -54,7 +55,8 @@</span> <span class="p_context"> static unsigned long mpx_mmap(unsigned long len)</span>
 
 	down_write(&amp;mm-&gt;mmap_sem);
 	addr = do_mmap(NULL, 0, len, PROT_READ | PROT_WRITE,
<span class="p_del">-		       MAP_ANONYMOUS | MAP_PRIVATE, VM_MPX, 0, &amp;populate, NULL);</span>
<span class="p_add">+		       MAP_ANONYMOUS | MAP_PRIVATE, VM_MPX, 0, &amp;populate, NULL,</span>
<span class="p_add">+		       &amp;mmrange);</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 	if (populate)
 		mm_populate(addr, populate);
<span class="p_chunk">@@ -427,13 +429,15 @@</span> <span class="p_context"> int mpx_handle_bd_fault(void)</span>
  * A thin wrapper around get_user_pages().  Returns 0 if the
  * fault was resolved or -errno if not.
  */
<span class="p_del">-static int mpx_resolve_fault(long __user *addr, int write)</span>
<span class="p_add">+static int mpx_resolve_fault(long __user *addr, int write,</span>
<span class="p_add">+			     struct range_lock *mmrange)</span>
 {
 	long gup_ret;
 	int nr_pages = 1;
 
 	gup_ret = get_user_pages((unsigned long)addr, nr_pages,
<span class="p_del">-			write ? FOLL_WRITE : 0,	NULL, NULL);</span>
<span class="p_add">+		       write ? FOLL_WRITE : 0,	NULL, NULL,</span>
<span class="p_add">+		       mmrange);</span>
 	/*
 	 * get_user_pages() returns number of pages gotten.
 	 * 0 means we failed to fault in and get anything,
<span class="p_chunk">@@ -500,7 +504,8 @@</span> <span class="p_context"> static int get_user_bd_entry(struct mm_struct *mm, unsigned long *bd_entry_ret,</span>
  */
 static int get_bt_addr(struct mm_struct *mm,
 			long __user *bd_entry_ptr,
<span class="p_del">-			unsigned long *bt_addr_result)</span>
<span class="p_add">+		        unsigned long *bt_addr_result,</span>
<span class="p_add">+		        struct range_lock *mmrange)</span>
 {
 	int ret;
 	int valid_bit;
<span class="p_chunk">@@ -519,7 +524,8 @@</span> <span class="p_context"> static int get_bt_addr(struct mm_struct *mm,</span>
 		if (!ret)
 			break;
 		if (ret == -EFAULT)
<span class="p_del">-			ret = mpx_resolve_fault(bd_entry_ptr, need_write);</span>
<span class="p_add">+			ret = mpx_resolve_fault(bd_entry_ptr,</span>
<span class="p_add">+						need_write, mmrange);</span>
 		/*
 		 * If we could not resolve the fault, consider it
 		 * userspace&#39;s fault and error out.
<span class="p_chunk">@@ -730,7 +736,8 @@</span> <span class="p_context"> static unsigned long mpx_get_bd_entry_offset(struct mm_struct *mm,</span>
 }
 
 static int unmap_entire_bt(struct mm_struct *mm,
<span class="p_del">-		long __user *bd_entry, unsigned long bt_addr)</span>
<span class="p_add">+		long __user *bd_entry, unsigned long bt_addr,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	unsigned long expected_old_val = bt_addr | MPX_BD_ENTRY_VALID_FLAG;
 	unsigned long uninitialized_var(actual_old_val);
<span class="p_chunk">@@ -747,7 +754,7 @@</span> <span class="p_context"> static int unmap_entire_bt(struct mm_struct *mm,</span>
 		if (!ret)
 			break;
 		if (ret == -EFAULT)
<span class="p_del">-			ret = mpx_resolve_fault(bd_entry, need_write);</span>
<span class="p_add">+			ret = mpx_resolve_fault(bd_entry, need_write, mmrange);</span>
 		/*
 		 * If we could not resolve the fault, consider it
 		 * userspace&#39;s fault and error out.
<span class="p_chunk">@@ -780,11 +787,12 @@</span> <span class="p_context"> static int unmap_entire_bt(struct mm_struct *mm,</span>
 	 * avoid recursion, do_munmap() will check whether it comes
 	 * from one bounds table through VM_MPX flag.
 	 */
<span class="p_del">-	return do_munmap(mm, bt_addr, mpx_bt_size_bytes(mm), NULL);</span>
<span class="p_add">+	return do_munmap(mm, bt_addr, mpx_bt_size_bytes(mm), NULL, mmrange);</span>
 }
 
 static int try_unmap_single_bt(struct mm_struct *mm,
<span class="p_del">-	       unsigned long start, unsigned long end)</span>
<span class="p_add">+	       unsigned long start, unsigned long end,</span>
<span class="p_add">+	       struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *next;
 	struct vm_area_struct *prev;
<span class="p_chunk">@@ -835,7 +843,7 @@</span> <span class="p_context"> static int try_unmap_single_bt(struct mm_struct *mm,</span>
 	}
 
 	bde_vaddr = mm-&gt;context.bd_addr + mpx_get_bd_entry_offset(mm, start);
<span class="p_del">-	ret = get_bt_addr(mm, bde_vaddr, &amp;bt_addr);</span>
<span class="p_add">+	ret = get_bt_addr(mm, bde_vaddr, &amp;bt_addr, mmrange);</span>
 	/*
 	 * No bounds table there, so nothing to unmap.
 	 */
<span class="p_chunk">@@ -853,12 +861,13 @@</span> <span class="p_context"> static int try_unmap_single_bt(struct mm_struct *mm,</span>
 	 */
 	if ((start == bta_start_vaddr) &amp;&amp;
 	    (end == bta_end_vaddr))
<span class="p_del">-		return unmap_entire_bt(mm, bde_vaddr, bt_addr);</span>
<span class="p_add">+		return unmap_entire_bt(mm, bde_vaddr, bt_addr, mmrange);</span>
 	return zap_bt_entries_mapping(mm, bt_addr, start, end);
 }
 
 static int mpx_unmap_tables(struct mm_struct *mm,
<span class="p_del">-		unsigned long start, unsigned long end)</span>
<span class="p_add">+			    unsigned long start, unsigned long end,</span>
<span class="p_add">+			    struct range_lock *mmrange)</span>
 {
 	unsigned long one_unmap_start;
 	trace_mpx_unmap_search(start, end);
<span class="p_chunk">@@ -876,7 +885,8 @@</span> <span class="p_context"> static int mpx_unmap_tables(struct mm_struct *mm,</span>
 		 */
 		if (one_unmap_end &gt; next_unmap_start)
 			one_unmap_end = next_unmap_start;
<span class="p_del">-		ret = try_unmap_single_bt(mm, one_unmap_start, one_unmap_end);</span>
<span class="p_add">+		ret = try_unmap_single_bt(mm, one_unmap_start, one_unmap_end,</span>
<span class="p_add">+					  mmrange);</span>
 		if (ret)
 			return ret;
 
<span class="p_chunk">@@ -894,7 +904,8 @@</span> <span class="p_context"> static int mpx_unmap_tables(struct mm_struct *mm,</span>
  * necessary, and the &#39;vma&#39; is the first vma in this range (start -&gt; end).
  */
 void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
<span class="p_del">-		unsigned long start, unsigned long end)</span>
<span class="p_add">+		unsigned long start, unsigned long end,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	int ret;
 
<span class="p_chunk">@@ -920,7 +931,7 @@</span> <span class="p_context"> void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		vma = vma-&gt;vm_next;
 	} while (vma &amp;&amp; vma-&gt;vm_start &lt; end);
 
<span class="p_del">-	ret = mpx_unmap_tables(mm, start, end);</span>
<span class="p_add">+	ret = mpx_unmap_tables(mm, start, end, mmrange);</span>
 	if (ret)
 		force_sig(SIGSEGV, current);
 }
<span class="p_header">diff --git a/arch/xtensa/mm/fault.c b/arch/xtensa/mm/fault.c</span>
<span class="p_header">index 8b9b6f44bb06..6f8e3e7cccb5 100644</span>
<span class="p_header">--- a/arch/xtensa/mm/fault.c</span>
<span class="p_header">+++ b/arch/xtensa/mm/fault.c</span>
<span class="p_chunk">@@ -44,6 +44,7 @@</span> <span class="p_context"> void do_page_fault(struct pt_regs *regs)</span>
 	int is_write, is_exec;
 	int fault;
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	info.si_code = SEGV_MAPERR;
 
<span class="p_chunk">@@ -108,7 +109,7 @@</span> <span class="p_context"> void do_page_fault(struct pt_regs *regs)</span>
 	 * make sure we exit gracefully rather than endlessly redo
 	 * the fault.
 	 */
<span class="p_del">-	fault = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	fault = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 
 	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(current))
 		return;
<span class="p_header">diff --git a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c</span>
<span class="p_header">index e4bb435e614b..bd464a599341 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/amd/amdgpu/amdgpu_ttm.c</span>
<span class="p_chunk">@@ -691,6 +691,7 @@</span> <span class="p_context"> int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)</span>
 	unsigned int flags = 0;
 	unsigned pinned = 0;
 	int r;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (!(gtt-&gt;userflags &amp; AMDGPU_GEM_USERPTR_READONLY))
 		flags |= FOLL_WRITE;
<span class="p_chunk">@@ -721,7 +722,7 @@</span> <span class="p_context"> int amdgpu_ttm_tt_get_user_pages(struct ttm_tt *ttm, struct page **pages)</span>
 		list_add(&amp;guptask.list, &amp;gtt-&gt;guptasks);
 		spin_unlock(&amp;gtt-&gt;guptasklock);
 
<span class="p_del">-		r = get_user_pages(userptr, num_pages, flags, p, NULL);</span>
<span class="p_add">+		r = get_user_pages(userptr, num_pages, flags, p, NULL, &amp;mmrange);</span>
 
 		spin_lock(&amp;gtt-&gt;guptasklock);
 		list_del(&amp;guptask.list);
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem_userptr.c b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">index 382a77a1097e..881bcc7d663a 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem_userptr.c</span>
<span class="p_chunk">@@ -512,6 +512,8 @@</span> <span class="p_context"> __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
 
 		ret = -EFAULT;
 		if (mmget_not_zero(mm)) {
<span class="p_add">+			DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="p_add">+</span>
 			down_read(&amp;mm-&gt;mmap_sem);
 			while (pinned &lt; npages) {
 				ret = get_user_pages_remote
<span class="p_chunk">@@ -519,7 +521,7 @@</span> <span class="p_context"> __i915_gem_userptr_get_pages_worker(struct work_struct *_work)</span>
 					 obj-&gt;userptr.ptr + pinned * PAGE_SIZE,
 					 npages - pinned,
 					 flags,
<span class="p_del">-					 pvec + pinned, NULL, NULL);</span>
<span class="p_add">+					 pvec + pinned, NULL, NULL, &amp;mmrange);</span>
 				if (ret &lt; 0)
 					break;
 
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_ttm.c b/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="p_header">index a0a839bc39bf..9fc3a4f86945 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="p_chunk">@@ -545,6 +545,8 @@</span> <span class="p_context"> static int radeon_ttm_tt_pin_userptr(struct ttm_tt *ttm)</span>
 	struct radeon_ttm_tt *gtt = (void *)ttm;
 	unsigned pinned = 0, nents;
 	int r;
<span class="p_add">+	// XXX: this is wrong!!</span>
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	int write = !(gtt-&gt;userflags &amp; RADEON_GEM_USERPTR_READONLY);
 	enum dma_data_direction direction = write ?
<span class="p_chunk">@@ -569,7 +571,7 @@</span> <span class="p_context"> static int radeon_ttm_tt_pin_userptr(struct ttm_tt *ttm)</span>
 		struct page **pages = ttm-&gt;pages + pinned;
 
 		r = get_user_pages(userptr, num_pages, write ? FOLL_WRITE : 0,
<span class="p_del">-				   pages, NULL);</span>
<span class="p_add">+				   pages, NULL, &amp;mmrange);</span>
 		if (r &lt; 0)
 			goto release_pages;
 
<span class="p_header">diff --git a/drivers/infiniband/core/umem.c b/drivers/infiniband/core/umem.c</span>
<span class="p_header">index 9a4e899d94b3..fd9601ed5b84 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem.c</span>
<span class="p_chunk">@@ -96,6 +96,7 @@</span> <span class="p_context"> struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
 	struct scatterlist *sg, *sg_list_start;
 	int need_release = 0;
 	unsigned int gup_flags = FOLL_WRITE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (dmasync)
 		dma_attrs |= DMA_ATTR_WRITE_BARRIER;
<span class="p_chunk">@@ -194,7 +195,7 @@</span> <span class="p_context"> struct ib_umem *ib_umem_get(struct ib_ucontext *context, unsigned long addr,</span>
 		ret = get_user_pages_longterm(cur_base,
 				     min_t(unsigned long, npages,
 					   PAGE_SIZE / sizeof (struct page *)),
<span class="p_del">-				     gup_flags, page_list, vma_list);</span>
<span class="p_add">+				      gup_flags, page_list, vma_list, &amp;mmrange);</span>
 
 		if (ret &lt; 0)
 			goto out;
<span class="p_header">diff --git a/drivers/infiniband/core/umem_odp.c b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">index 2aadf5813a40..0572953260e8 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/umem_odp.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/umem_odp.c</span>
<span class="p_chunk">@@ -632,6 +632,7 @@</span> <span class="p_context"> int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,</span>
 	int j, k, ret = 0, start_idx, npages = 0, page_shift;
 	unsigned int flags = 0;
 	phys_addr_t p = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (access_mask == 0)
 		return -EINVAL;
<span class="p_chunk">@@ -683,7 +684,7 @@</span> <span class="p_context"> int ib_umem_odp_map_dma_pages(struct ib_umem *umem, u64 user_virt, u64 bcnt,</span>
 		 */
 		npages = get_user_pages_remote(owning_process, owning_mm,
 				user_virt, gup_num_pages,
<span class="p_del">-				flags, local_page_list, NULL, NULL);</span>
<span class="p_add">+				flags, local_page_list, NULL, NULL, &amp;mmrange);</span>
 		up_read(&amp;owning_mm-&gt;mmap_sem);
 
 		if (npages &lt; 0)
<span class="p_header">diff --git a/drivers/infiniband/hw/qib/qib_user_pages.c b/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_header">index ce83ba9a12ef..6bcb4f9f9b30 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/qib/qib_user_pages.c</span>
<span class="p_chunk">@@ -53,7 +53,7 @@</span> <span class="p_context"> static void __qib_release_user_pages(struct page **p, size_t num_pages,</span>
  * Call with current-&gt;mm-&gt;mmap_sem held.
  */
 static int __qib_get_user_pages(unsigned long start_page, size_t num_pages,
<span class="p_del">-				struct page **p)</span>
<span class="p_add">+				struct page **p, struct range_lock *mmrange)</span>
 {
 	unsigned long lock_limit;
 	size_t got;
<span class="p_chunk">@@ -70,7 +70,7 @@</span> <span class="p_context"> static int __qib_get_user_pages(unsigned long start_page, size_t num_pages,</span>
 		ret = get_user_pages(start_page + got * PAGE_SIZE,
 				     num_pages - got,
 				     FOLL_WRITE | FOLL_FORCE,
<span class="p_del">-				     p + got, NULL);</span>
<span class="p_add">+				     p + got, NULL, mmrange);</span>
 		if (ret &lt; 0)
 			goto bail_release;
 	}
<span class="p_chunk">@@ -134,10 +134,11 @@</span> <span class="p_context"> int qib_get_user_pages(unsigned long start_page, size_t num_pages,</span>
 		       struct page **p)
 {
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_write(&amp;current-&gt;mm-&gt;mmap_sem);
 
<span class="p_del">-	ret = __qib_get_user_pages(start_page, num_pages, p);</span>
<span class="p_add">+	ret = __qib_get_user_pages(start_page, num_pages, p, &amp;mmrange);</span>
 
 	up_write(&amp;current-&gt;mm-&gt;mmap_sem);
 
<span class="p_header">diff --git a/drivers/infiniband/hw/usnic/usnic_uiom.c b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_header">index 4381c0a9a873..5f36c6d2e21b 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/usnic/usnic_uiom.c</span>
<span class="p_chunk">@@ -113,6 +113,7 @@</span> <span class="p_context"> static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
 	int flags;
 	dma_addr_t pa;
 	unsigned int gup_flags;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (!can_do_mlock())
 		return -EPERM;
<span class="p_chunk">@@ -146,7 +147,7 @@</span> <span class="p_context"> static int usnic_uiom_get_pages(unsigned long addr, size_t size, int writable,</span>
 		ret = get_user_pages(cur_base,
 					min_t(unsigned long, npages,
 					PAGE_SIZE / sizeof(struct page *)),
<span class="p_del">-					gup_flags, page_list, NULL);</span>
<span class="p_add">+					gup_flags, page_list, NULL, &amp;mmrange);</span>
 
 		if (ret &lt; 0)
 			goto out;
<span class="p_header">diff --git a/drivers/iommu/amd_iommu_v2.c b/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_header">index 1d0b53a04a08..15a7103fd84c 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu_v2.c</span>
<span class="p_chunk">@@ -512,6 +512,7 @@</span> <span class="p_context"> static void do_fault(struct work_struct *work)</span>
 	unsigned int flags = 0;
 	struct mm_struct *mm;
 	u64 address;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	mm = fault-&gt;state-&gt;mm;
 	address = fault-&gt;address;
<span class="p_chunk">@@ -523,7 +524,7 @@</span> <span class="p_context"> static void do_fault(struct work_struct *work)</span>
 	flags |= FAULT_FLAG_REMOTE;
 
 	down_read(&amp;mm-&gt;mmap_sem);
<span class="p_del">-	vma = find_extend_vma(mm, address);</span>
<span class="p_add">+	vma = find_extend_vma(mm, address, &amp;mmrange);</span>
 	if (!vma || address &lt; vma-&gt;vm_start)
 		/* failed to get a vma in the right range */
 		goto out;
<span class="p_chunk">@@ -532,7 +533,7 @@</span> <span class="p_context"> static void do_fault(struct work_struct *work)</span>
 	if (access_error(vma, fault))
 		goto out;
 
<span class="p_del">-	ret = handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+	ret = handle_mm_fault(vma, address, flags, &amp;mmrange);</span>
 out:
 	up_read(&amp;mm-&gt;mmap_sem);
 
<span class="p_header">diff --git a/drivers/iommu/intel-svm.c b/drivers/iommu/intel-svm.c</span>
<span class="p_header">index 35a408d0ae4f..6a74386ee83f 100644</span>
<span class="p_header">--- a/drivers/iommu/intel-svm.c</span>
<span class="p_header">+++ b/drivers/iommu/intel-svm.c</span>
<span class="p_chunk">@@ -585,6 +585,7 @@</span> <span class="p_context"> static irqreturn_t prq_event_thread(int irq, void *d)</span>
 	struct intel_iommu *iommu = d;
 	struct intel_svm *svm = NULL;
 	int head, tail, handled = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* Clear PPR bit before reading head/tail registers, to
 	 * ensure that we get a new interrupt if needed. */
<span class="p_chunk">@@ -643,7 +644,7 @@</span> <span class="p_context"> static irqreturn_t prq_event_thread(int irq, void *d)</span>
 			goto bad_req;
 
 		down_read(&amp;svm-&gt;mm-&gt;mmap_sem);
<span class="p_del">-		vma = find_extend_vma(svm-&gt;mm, address);</span>
<span class="p_add">+		vma = find_extend_vma(svm-&gt;mm, address, &amp;mmrange);</span>
 		if (!vma || address &lt; vma-&gt;vm_start)
 			goto invalid;
 
<span class="p_chunk">@@ -651,7 +652,7 @@</span> <span class="p_context"> static irqreturn_t prq_event_thread(int irq, void *d)</span>
 			goto invalid;
 
 		ret = handle_mm_fault(vma, address,
<span class="p_del">-				      req-&gt;wr_req ? FAULT_FLAG_WRITE : 0);</span>
<span class="p_add">+				      req-&gt;wr_req ? FAULT_FLAG_WRITE : 0, &amp;mmrange);</span>
 		if (ret &amp; VM_FAULT_ERROR)
 			goto invalid;
 
<span class="p_header">diff --git a/drivers/media/v4l2-core/videobuf-dma-sg.c b/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_header">index f412429cf5ba..64a4cd62eeb3 100644</span>
<span class="p_header">--- a/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_header">+++ b/drivers/media/v4l2-core/videobuf-dma-sg.c</span>
<span class="p_chunk">@@ -152,7 +152,8 @@</span> <span class="p_context"> static void videobuf_dma_init(struct videobuf_dmabuf *dma)</span>
 }
 
 static int videobuf_dma_init_user_locked(struct videobuf_dmabuf *dma,
<span class="p_del">-			int direction, unsigned long data, unsigned long size)</span>
<span class="p_add">+			int direction, unsigned long data, unsigned long size,</span>
<span class="p_add">+			struct range_lock *mmrange)</span>
 {
 	unsigned long first, last;
 	int err, rw = 0;
<span class="p_chunk">@@ -186,7 +187,7 @@</span> <span class="p_context"> static int videobuf_dma_init_user_locked(struct videobuf_dmabuf *dma,</span>
 		data, size, dma-&gt;nr_pages);
 
 	err = get_user_pages_longterm(data &amp; PAGE_MASK, dma-&gt;nr_pages,
<span class="p_del">-			     flags, dma-&gt;pages, NULL);</span>
<span class="p_add">+				      flags, dma-&gt;pages, NULL, mmrange);</span>
 
 	if (err != dma-&gt;nr_pages) {
 		dma-&gt;nr_pages = (err &gt;= 0) ? err : 0;
<span class="p_chunk">@@ -201,9 +202,10 @@</span> <span class="p_context"> static int videobuf_dma_init_user(struct videobuf_dmabuf *dma, int direction,</span>
 			   unsigned long data, unsigned long size)
 {
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_read(&amp;current-&gt;mm-&gt;mmap_sem);
<span class="p_del">-	ret = videobuf_dma_init_user_locked(dma, direction, data, size);</span>
<span class="p_add">+	ret = videobuf_dma_init_user_locked(dma, direction, data, size, &amp;mmrange);</span>
 	up_read(&amp;current-&gt;mm-&gt;mmap_sem);
 
 	return ret;
<span class="p_chunk">@@ -539,9 +541,14 @@</span> <span class="p_context"> static int __videobuf_iolock(struct videobuf_queue *q,</span>
 			we take current-&gt;mm-&gt;mmap_sem there, to prevent
 			locking inversion, so don&#39;t take it here */
 
<span class="p_add">+			/* XXX: can we use a local mmrange here? */</span>
<span class="p_add">+			DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="p_add">+</span>
 			err = videobuf_dma_init_user_locked(&amp;mem-&gt;dma,
<span class="p_del">-						      DMA_FROM_DEVICE,</span>
<span class="p_del">-						      vb-&gt;baddr, vb-&gt;bsize);</span>
<span class="p_add">+							    DMA_FROM_DEVICE,</span>
<span class="p_add">+							    vb-&gt;baddr,</span>
<span class="p_add">+							    vb-&gt;bsize,</span>
<span class="p_add">+							    &amp;mmrange);</span>
 			if (0 != err)
 				return err;
 		}
<span class="p_chunk">@@ -555,6 +562,7 @@</span> <span class="p_context"> static int __videobuf_iolock(struct videobuf_queue *q,</span>
 		 * building for PAE. Compiler doesn&#39;t like direct casting
 		 * of a 32 bit ptr to 64 bit integer.
 		 */
<span class="p_add">+</span>
 		bus   = (dma_addr_t)(unsigned long)fbuf-&gt;base + vb-&gt;boff;
 		pages = PAGE_ALIGN(vb-&gt;size) &gt;&gt; PAGE_SHIFT;
 		err = videobuf_dma_init_overlay(&amp;mem-&gt;dma, DMA_FROM_DEVICE,
<span class="p_header">diff --git a/drivers/misc/mic/scif/scif_rma.c b/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_header">index c824329f7012..6ecac843e5f3 100644</span>
<span class="p_header">--- a/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_header">+++ b/drivers/misc/mic/scif/scif_rma.c</span>
<span class="p_chunk">@@ -1332,6 +1332,7 @@</span> <span class="p_context"> int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
 	int prot = *out_prot;
 	int ulimit = 0;
 	struct mm_struct *mm = NULL;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* Unsupported flags */
 	if (map_flags &amp; ~(SCIF_MAP_KERNEL | SCIF_MAP_ULIMIT))
<span class="p_chunk">@@ -1400,7 +1401,7 @@</span> <span class="p_context"> int __scif_pin_pages(void *addr, size_t len, int *out_prot,</span>
 				nr_pages,
 				(prot &amp; SCIF_PROT_WRITE) ? FOLL_WRITE : 0,
 				pinned_pages-&gt;pages,
<span class="p_del">-				NULL);</span>
<span class="p_add">+				NULL, &amp;mmrange);</span>
 		up_write(&amp;mm-&gt;mmap_sem);
 		if (nr_pages != pinned_pages-&gt;nr_pages) {
 			if (try_upgrade) {
<span class="p_header">diff --git a/drivers/misc/sgi-gru/grufault.c b/drivers/misc/sgi-gru/grufault.c</span>
<span class="p_header">index 93be82fc338a..b35d60bb2197 100644</span>
<span class="p_header">--- a/drivers/misc/sgi-gru/grufault.c</span>
<span class="p_header">+++ b/drivers/misc/sgi-gru/grufault.c</span>
<span class="p_chunk">@@ -189,7 +189,8 @@</span> <span class="p_context"> static void get_clear_fault_map(struct gru_state *gru,</span>
  */
 static int non_atomic_pte_lookup(struct vm_area_struct *vma,
 				 unsigned long vaddr, int write,
<span class="p_del">-				 unsigned long *paddr, int *pageshift)</span>
<span class="p_add">+				 unsigned long *paddr, int *pageshift,</span>
<span class="p_add">+				 struct range_lock *mmrange)</span>
 {
 	struct page *page;
 
<span class="p_chunk">@@ -198,7 +199,8 @@</span> <span class="p_context"> static int non_atomic_pte_lookup(struct vm_area_struct *vma,</span>
 #else
 	*pageshift = PAGE_SHIFT;
 #endif
<span class="p_del">-	if (get_user_pages(vaddr, 1, write ? FOLL_WRITE : 0, &amp;page, NULL) &lt;= 0)</span>
<span class="p_add">+	if (get_user_pages(vaddr, 1, write ? FOLL_WRITE : 0,</span>
<span class="p_add">+			   &amp;page, NULL, mmrange) &lt;= 0)</span>
 		return -EFAULT;
 	*paddr = page_to_phys(page);
 	put_page(page);
<span class="p_chunk">@@ -263,7 +265,8 @@</span> <span class="p_context"> static int atomic_pte_lookup(struct vm_area_struct *vma, unsigned long vaddr,</span>
 }
 
 static int gru_vtop(struct gru_thread_state *gts, unsigned long vaddr,
<span class="p_del">-		    int write, int atomic, unsigned long *gpa, int *pageshift)</span>
<span class="p_add">+		    int write, int atomic, unsigned long *gpa, int *pageshift,</span>
<span class="p_add">+		    struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = gts-&gt;ts_mm;
 	struct vm_area_struct *vma;
<span class="p_chunk">@@ -283,7 +286,8 @@</span> <span class="p_context"> static int gru_vtop(struct gru_thread_state *gts, unsigned long vaddr,</span>
 	if (ret) {
 		if (atomic)
 			goto upm;
<span class="p_del">-		if (non_atomic_pte_lookup(vma, vaddr, write, &amp;paddr, &amp;ps))</span>
<span class="p_add">+		if (non_atomic_pte_lookup(vma, vaddr, write, &amp;paddr,</span>
<span class="p_add">+					  &amp;ps, mmrange))</span>
 			goto inval;
 	}
 	if (is_gru_paddr(paddr))
<span class="p_chunk">@@ -324,7 +328,8 @@</span> <span class="p_context"> static void gru_preload_tlb(struct gru_state *gru,</span>
 			unsigned long fault_vaddr, int asid, int write,
 			unsigned char tlb_preload_count,
 			struct gru_tlb_fault_handle *tfh,
<span class="p_del">-			struct gru_control_block_extended *cbe)</span>
<span class="p_add">+			struct gru_control_block_extended *cbe,</span>
<span class="p_add">+			struct range_lock *mmrange)</span>
 {
 	unsigned long vaddr = 0, gpa;
 	int ret, pageshift;
<span class="p_chunk">@@ -342,7 +347,7 @@</span> <span class="p_context"> static void gru_preload_tlb(struct gru_state *gru,</span>
 	vaddr = min(vaddr, fault_vaddr + tlb_preload_count * PAGE_SIZE);
 
 	while (vaddr &gt; fault_vaddr) {
<span class="p_del">-		ret = gru_vtop(gts, vaddr, write, atomic, &amp;gpa, &amp;pageshift);</span>
<span class="p_add">+		ret = gru_vtop(gts, vaddr, write, atomic, &amp;gpa, &amp;pageshift, mmrange);</span>
 		if (ret || tfh_write_only(tfh, gpa, GAA_RAM, vaddr, asid, write,
 					  GRU_PAGESIZE(pageshift)))
 			return;
<span class="p_chunk">@@ -368,7 +373,8 @@</span> <span class="p_context"> static void gru_preload_tlb(struct gru_state *gru,</span>
 static int gru_try_dropin(struct gru_state *gru,
 			  struct gru_thread_state *gts,
 			  struct gru_tlb_fault_handle *tfh,
<span class="p_del">-			  struct gru_instruction_bits *cbk)</span>
<span class="p_add">+			  struct gru_instruction_bits *cbk,</span>
<span class="p_add">+			  struct range_lock *mmrange)</span>
 {
 	struct gru_control_block_extended *cbe = NULL;
 	unsigned char tlb_preload_count = gts-&gt;ts_tlb_preload_count;
<span class="p_chunk">@@ -423,7 +429,7 @@</span> <span class="p_context"> static int gru_try_dropin(struct gru_state *gru,</span>
 	if (atomic_read(&amp;gts-&gt;ts_gms-&gt;ms_range_active))
 		goto failactive;
 
<span class="p_del">-	ret = gru_vtop(gts, vaddr, write, atomic, &amp;gpa, &amp;pageshift);</span>
<span class="p_add">+	ret = gru_vtop(gts, vaddr, write, atomic, &amp;gpa, &amp;pageshift, mmrange);</span>
 	if (ret == VTOP_INVALID)
 		goto failinval;
 	if (ret == VTOP_RETRY)
<span class="p_chunk">@@ -438,7 +444,8 @@</span> <span class="p_context"> static int gru_try_dropin(struct gru_state *gru,</span>
 	}
 
 	if (unlikely(cbe) &amp;&amp; pageshift == PAGE_SHIFT) {
<span class="p_del">-		gru_preload_tlb(gru, gts, atomic, vaddr, asid, write, tlb_preload_count, tfh, cbe);</span>
<span class="p_add">+		gru_preload_tlb(gru, gts, atomic, vaddr, asid, write,</span>
<span class="p_add">+				tlb_preload_count, tfh, cbe, mmrange);</span>
 		gru_flush_cache_cbe(cbe);
 	}
 
<span class="p_chunk">@@ -587,10 +594,13 @@</span> <span class="p_context"> static irqreturn_t gru_intr(int chiplet, int blade)</span>
 		 * If it fails, retry the fault in user context.
 		 */
 		gts-&gt;ustats.fmm_tlbmiss++;
<span class="p_del">-		if (!gts-&gt;ts_force_cch_reload &amp;&amp;</span>
<span class="p_del">-					down_read_trylock(&amp;gts-&gt;ts_mm-&gt;mmap_sem)) {</span>
<span class="p_del">-			gru_try_dropin(gru, gts, tfh, NULL);</span>
<span class="p_del">-			up_read(&amp;gts-&gt;ts_mm-&gt;mmap_sem);</span>
<span class="p_add">+		if (!gts-&gt;ts_force_cch_reload) {</span>
<span class="p_add">+			DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (down_read_trylock(&amp;gts-&gt;ts_mm-&gt;mmap_sem)) {</span>
<span class="p_add">+				gru_try_dropin(gru, gts, tfh, NULL, &amp;mmrange);</span>
<span class="p_add">+				up_read(&amp;gts-&gt;ts_mm-&gt;mmap_sem);</span>
<span class="p_add">+			}</span>
 		} else {
 			tfh_user_polling_mode(tfh);
 			STAT(intr_mm_lock_failed);
<span class="p_chunk">@@ -625,7 +635,7 @@</span> <span class="p_context"> irqreturn_t gru_intr_mblade(int irq, void *dev_id)</span>
 
 static int gru_user_dropin(struct gru_thread_state *gts,
 			   struct gru_tlb_fault_handle *tfh,
<span class="p_del">-			   void *cb)</span>
<span class="p_add">+			   void *cb, struct range_lock *mmrange)</span>
 {
 	struct gru_mm_struct *gms = gts-&gt;ts_gms;
 	int ret;
<span class="p_chunk">@@ -635,7 +645,7 @@</span> <span class="p_context"> static int gru_user_dropin(struct gru_thread_state *gts,</span>
 		wait_event(gms-&gt;ms_wait_queue,
 			   atomic_read(&amp;gms-&gt;ms_range_active) == 0);
 		prefetchw(tfh);	/* Helps on hdw, required for emulator */
<span class="p_del">-		ret = gru_try_dropin(gts-&gt;ts_gru, gts, tfh, cb);</span>
<span class="p_add">+		ret = gru_try_dropin(gts-&gt;ts_gru, gts, tfh, cb, mmrange);</span>
 		if (ret &lt;= 0)
 			return ret;
 		STAT(call_os_wait_queue);
<span class="p_chunk">@@ -653,6 +663,7 @@</span> <span class="p_context"> int gru_handle_user_call_os(unsigned long cb)</span>
 	struct gru_thread_state *gts;
 	void *cbk;
 	int ucbnum, cbrnum, ret = -EINVAL;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	STAT(call_os);
 
<span class="p_chunk">@@ -685,7 +696,7 @@</span> <span class="p_context"> int gru_handle_user_call_os(unsigned long cb)</span>
 		tfh = get_tfh_by_index(gts-&gt;ts_gru, cbrnum);
 		cbk = get_gseg_base_address_cb(gts-&gt;ts_gru-&gt;gs_gru_base_vaddr,
 				gts-&gt;ts_ctxnum, ucbnum);
<span class="p_del">-		ret = gru_user_dropin(gts, tfh, cbk);</span>
<span class="p_add">+		ret = gru_user_dropin(gts, tfh, cbk, &amp;mmrange);</span>
 	}
 exit:
 	gru_unlock_gts(gts);
<span class="p_header">diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">index e30e29ae4819..1b3b103da637 100644</span>
<span class="p_header">--- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">+++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_chunk">@@ -345,13 +345,14 @@</span> <span class="p_context"> static int vaddr_get_pfn(struct mm_struct *mm, unsigned long vaddr,</span>
 					  page);
 	} else {
 		unsigned int flags = 0;
<span class="p_add">+		DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 		if (prot &amp; IOMMU_WRITE)
 			flags |= FOLL_WRITE;
 
 		down_read(&amp;mm-&gt;mmap_sem);
 		ret = get_user_pages_remote(NULL, mm, vaddr, 1, flags, page,
<span class="p_del">-					    NULL, NULL);</span>
<span class="p_add">+					    NULL, NULL, &amp;mmrange);</span>
 		up_read(&amp;mm-&gt;mmap_sem);
 	}
 
<span class="p_header">diff --git a/fs/aio.c b/fs/aio.c</span>
<span class="p_header">index a062d75109cb..31774b75c372 100644</span>
<span class="p_header">--- a/fs/aio.c</span>
<span class="p_header">+++ b/fs/aio.c</span>
<span class="p_chunk">@@ -457,6 +457,7 @@</span> <span class="p_context"> static int aio_setup_ring(struct kioctx *ctx, unsigned int nr_events)</span>
 	int nr_pages;
 	int i;
 	struct file *file;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* Compensate for the ring buffer&#39;s head/tail overlap entry */
 	nr_events += 2;	/* 1 is required, 2 for good luck */
<span class="p_chunk">@@ -519,7 +520,7 @@</span> <span class="p_context"> static int aio_setup_ring(struct kioctx *ctx, unsigned int nr_events)</span>
 
 	ctx-&gt;mmap_base = do_mmap_pgoff(ctx-&gt;aio_ring_file, 0, ctx-&gt;mmap_size,
 				       PROT_READ | PROT_WRITE,
<span class="p_del">-				       MAP_SHARED, 0, &amp;unused, NULL);</span>
<span class="p_add">+				       MAP_SHARED, 0, &amp;unused, NULL, &amp;mmrange);</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 	if (IS_ERR((void *)ctx-&gt;mmap_base)) {
 		ctx-&gt;mmap_size = 0;
<span class="p_header">diff --git a/fs/binfmt_elf.c b/fs/binfmt_elf.c</span>
<span class="p_header">index 2f492dfcabde..9aea808d55d7 100644</span>
<span class="p_header">--- a/fs/binfmt_elf.c</span>
<span class="p_header">+++ b/fs/binfmt_elf.c</span>
<span class="p_chunk">@@ -180,6 +180,7 @@</span> <span class="p_context"> create_elf_tables(struct linux_binprm *bprm, struct elfhdr *exec,</span>
 	int ei_index = 0;
 	const struct cred *cred = current_cred();
 	struct vm_area_struct *vma;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/*
 	 * In some cases (e.g. Hyper-Threading), we want to avoid L1
<span class="p_chunk">@@ -300,7 +301,7 @@</span> <span class="p_context"> create_elf_tables(struct linux_binprm *bprm, struct elfhdr *exec,</span>
 	 * Grow the stack manually; some architectures have a limit on how
 	 * far ahead a user-space access may be in order to grow the stack.
 	 */
<span class="p_del">-	vma = find_extend_vma(current-&gt;mm, bprm-&gt;p);</span>
<span class="p_add">+	vma = find_extend_vma(current-&gt;mm, bprm-&gt;p, &amp;mmrange);</span>
 	if (!vma)
 		return -EFAULT;
 
<span class="p_header">diff --git a/fs/exec.c b/fs/exec.c</span>
<span class="p_header">index e7b69e14649f..e46752874b47 100644</span>
<span class="p_header">--- a/fs/exec.c</span>
<span class="p_header">+++ b/fs/exec.c</span>
<span class="p_chunk">@@ -197,6 +197,11 @@</span> <span class="p_context"> static struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,</span>
 	struct page *page;
 	int ret;
 	unsigned int gup_flags = FOLL_FORCE;
<span class="p_add">+	/*</span>
<span class="p_add">+	 * No concurrency for the bprm-&gt;mm yet -- this is exec path;</span>
<span class="p_add">+	 * but gup needs an mmrange.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 #ifdef CONFIG_STACK_GROWSUP
 	if (write) {
<span class="p_chunk">@@ -214,7 +219,7 @@</span> <span class="p_context"> static struct page *get_arg_page(struct linux_binprm *bprm, unsigned long pos,</span>
 	 * doing the exec and bprm-&gt;mm is the new process&#39;s mm.
 	 */
 	ret = get_user_pages_remote(current, bprm-&gt;mm, pos, 1, gup_flags,
<span class="p_del">-			&amp;page, NULL, NULL);</span>
<span class="p_add">+				    &amp;page, NULL, NULL, &amp;mmrange);</span>
 	if (ret &lt;= 0)
 		return NULL;
 
<span class="p_chunk">@@ -615,7 +620,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(copy_strings_kernel);</span>
  * 4) Free up any cleared pgd range.
  * 5) Shrink the vma to cover only the new range.
  */
<span class="p_del">-static int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)</span>
<span class="p_add">+static int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift,</span>
<span class="p_add">+			   struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	unsigned long old_start = vma-&gt;vm_start;
<span class="p_chunk">@@ -637,7 +643,8 @@</span> <span class="p_context"> static int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)</span>
 	/*
 	 * cover the whole range: [new_start, old_end)
 	 */
<span class="p_del">-	if (vma_adjust(vma, new_start, old_end, vma-&gt;vm_pgoff, NULL))</span>
<span class="p_add">+	if (vma_adjust(vma, new_start, old_end, vma-&gt;vm_pgoff, NULL,</span>
<span class="p_add">+		    mmrange))</span>
 		return -ENOMEM;
 
 	/*
<span class="p_chunk">@@ -671,7 +678,7 @@</span> <span class="p_context"> static int shift_arg_pages(struct vm_area_struct *vma, unsigned long shift)</span>
 	/*
 	 * Shrink the vma to just the new range.  Always succeeds.
 	 */
<span class="p_del">-	vma_adjust(vma, new_start, new_end, vma-&gt;vm_pgoff, NULL);</span>
<span class="p_add">+	vma_adjust(vma, new_start, new_end, vma-&gt;vm_pgoff, NULL, mmrange);</span>
 
 	return 0;
 }
<span class="p_chunk">@@ -694,6 +701,7 @@</span> <span class="p_context"> int setup_arg_pages(struct linux_binprm *bprm,</span>
 	unsigned long stack_size;
 	unsigned long stack_expand;
 	unsigned long rlim_stack;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 #ifdef CONFIG_STACK_GROWSUP
 	/* Limit stack size */
<span class="p_chunk">@@ -749,14 +757,14 @@</span> <span class="p_context"> int setup_arg_pages(struct linux_binprm *bprm,</span>
 	vm_flags |= VM_STACK_INCOMPLETE_SETUP;
 
 	ret = mprotect_fixup(vma, &amp;prev, vma-&gt;vm_start, vma-&gt;vm_end,
<span class="p_del">-			vm_flags);</span>
<span class="p_add">+			     vm_flags, &amp;mmrange);</span>
 	if (ret)
 		goto out_unlock;
 	BUG_ON(prev != vma);
 
 	/* Move stack pages down in memory. */
 	if (stack_shift) {
<span class="p_del">-		ret = shift_arg_pages(vma, stack_shift);</span>
<span class="p_add">+		ret = shift_arg_pages(vma, stack_shift, &amp;mmrange);</span>
 		if (ret)
 			goto out_unlock;
 	}
<span class="p_header">diff --git a/fs/proc/internal.h b/fs/proc/internal.h</span>
<span class="p_header">index d697c8ab0a14..791f9f93643c 100644</span>
<span class="p_header">--- a/fs/proc/internal.h</span>
<span class="p_header">+++ b/fs/proc/internal.h</span>
<span class="p_chunk">@@ -16,6 +16,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/binfmts.h&gt;
 #include &lt;linux/sched/coredump.h&gt;
 #include &lt;linux/sched/task.h&gt;
<span class="p_add">+#include &lt;linux/range_lock.h&gt;</span>
 
 struct ctl_table_header;
 struct mempolicy;
<span class="p_chunk">@@ -263,6 +264,8 @@</span> <span class="p_context"> struct proc_maps_private {</span>
 #ifdef CONFIG_NUMA
 	struct mempolicy *task_mempolicy;
 #endif
<span class="p_add">+	/* mmap_sem is held across all stages of seqfile */</span>
<span class="p_add">+	struct range_lock mmrange;</span>
 } __randomize_layout;
 
 struct mm_struct *proc_mem_open(struct inode *inode, unsigned int mode);
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index b66fc8de7d34..7c0a79a937b5 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -174,6 +174,7 @@</span> <span class="p_context"> static void *m_start(struct seq_file *m, loff_t *ppos)</span>
 	if (!mm || !mmget_not_zero(mm))
 		return NULL;
 
<span class="p_add">+	range_lock_init_full(&amp;priv-&gt;mmrange);</span>
 	down_read(&amp;mm-&gt;mmap_sem);
 	hold_task_mempolicy(priv);
 	priv-&gt;tail_vma = get_gate_vma(mm);
<span class="p_chunk">@@ -514,7 +515,7 @@</span> <span class="p_context"> static void smaps_account(struct mem_size_stats *mss, struct page *page,</span>
 
 #ifdef CONFIG_SHMEM
 static int smaps_pte_hole(unsigned long addr, unsigned long end,
<span class="p_del">-		struct mm_walk *walk)</span>
<span class="p_add">+			  struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	struct mem_size_stats *mss = walk-&gt;private;
 
<span class="p_chunk">@@ -605,7 +606,7 @@</span> <span class="p_context"> static void smaps_pmd_entry(pmd_t *pmd, unsigned long addr,</span>
 #endif
 
 static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
<span class="p_del">-			   struct mm_walk *walk)</span>
<span class="p_add">+			   struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma = walk-&gt;vma;
 	pte_t *pte;
<span class="p_chunk">@@ -797,7 +798,7 @@</span> <span class="p_context"> static int show_smap(struct seq_file *m, void *v, int is_pid)</span>
 #endif
 
 	/* mmap_sem is held in m_start */
<span class="p_del">-	walk_page_vma(vma, &amp;smaps_walk);</span>
<span class="p_add">+	walk_page_vma(vma, &amp;smaps_walk, &amp;priv-&gt;mmrange);</span>
 	if (vma-&gt;vm_flags &amp; VM_LOCKED)
 		mss-&gt;pss_locked += mss-&gt;pss;
 
<span class="p_chunk">@@ -1012,7 +1013,8 @@</span> <span class="p_context"> static inline void clear_soft_dirty_pmd(struct vm_area_struct *vma,</span>
 #endif
 
 static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,
<span class="p_del">-				unsigned long end, struct mm_walk *walk)</span>
<span class="p_add">+				unsigned long end, struct mm_walk *walk,</span>
<span class="p_add">+				struct range_lock *mmrange)</span>
 {
 	struct clear_refs_private *cp = walk-&gt;private;
 	struct vm_area_struct *vma = walk-&gt;vma;
<span class="p_chunk">@@ -1103,6 +1105,7 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 	struct mmu_gather tlb;
 	int itype;
 	int rv;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	memset(buffer, 0, sizeof(buffer));
 	if (count &gt; sizeof(buffer) - 1)
<span class="p_chunk">@@ -1166,7 +1169,8 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 			}
 			mmu_notifier_invalidate_range_start(mm, 0, -1);
 		}
<span class="p_del">-		walk_page_range(0, mm-&gt;highest_vm_end, &amp;clear_refs_walk);</span>
<span class="p_add">+		walk_page_range(0, mm-&gt;highest_vm_end, &amp;clear_refs_walk,</span>
<span class="p_add">+				&amp;mmrange);</span>
 		if (type == CLEAR_REFS_SOFT_DIRTY)
 			mmu_notifier_invalidate_range_end(mm, 0, -1);
 		tlb_finish_mmu(&amp;tlb, 0, -1);
<span class="p_chunk">@@ -1223,7 +1227,7 @@</span> <span class="p_context"> static int add_to_pagemap(unsigned long addr, pagemap_entry_t *pme,</span>
 }
 
 static int pagemap_pte_hole(unsigned long start, unsigned long end,
<span class="p_del">-				struct mm_walk *walk)</span>
<span class="p_add">+			    struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	struct pagemapread *pm = walk-&gt;private;
 	unsigned long addr = start;
<span class="p_chunk">@@ -1301,7 +1305,7 @@</span> <span class="p_context"> static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
 }
 
 static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,
<span class="p_del">-			     struct mm_walk *walk)</span>
<span class="p_add">+			     struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma = walk-&gt;vma;
 	struct pagemapread *pm = walk-&gt;private;
<span class="p_chunk">@@ -1467,6 +1471,8 @@</span> <span class="p_context"> static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
 	unsigned long start_vaddr;
 	unsigned long end_vaddr;
 	int ret = 0, copied = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(tmprange);</span>
<span class="p_add">+	struct range_lock *mmrange = &amp;tmprange;</span>
 
 	if (!mm || !mmget_not_zero(mm))
 		goto out;
<span class="p_chunk">@@ -1523,7 +1529,8 @@</span> <span class="p_context"> static ssize_t pagemap_read(struct file *file, char __user *buf,</span>
 		if (end &lt; start_vaddr || end &gt; end_vaddr)
 			end = end_vaddr;
 		down_read(&amp;mm-&gt;mmap_sem);
<span class="p_del">-		ret = walk_page_range(start_vaddr, end, &amp;pagemap_walk);</span>
<span class="p_add">+		ret = walk_page_range(start_vaddr, end, &amp;pagemap_walk,</span>
<span class="p_add">+				      mmrange);</span>
 		up_read(&amp;mm-&gt;mmap_sem);
 		start_vaddr = end;
 
<span class="p_chunk">@@ -1671,7 +1678,8 @@</span> <span class="p_context"> static struct page *can_gather_numa_stats_pmd(pmd_t pmd,</span>
 #endif
 
 static int gather_pte_stats(pmd_t *pmd, unsigned long addr,
<span class="p_del">-		unsigned long end, struct mm_walk *walk)</span>
<span class="p_add">+			    unsigned long end, struct mm_walk *walk,</span>
<span class="p_add">+			    struct range_lock *mmrange)</span>
 {
 	struct numa_maps *md = walk-&gt;private;
 	struct vm_area_struct *vma = walk-&gt;vma;
<span class="p_chunk">@@ -1740,6 +1748,7 @@</span> <span class="p_context"> static int gather_hugetlb_stats(pte_t *pte, unsigned long hmask,</span>
  */
 static int show_numa_map(struct seq_file *m, void *v, int is_pid)
 {
<span class="p_add">+	struct proc_maps_private *priv = m-&gt;private;</span>
 	struct numa_maps_private *numa_priv = m-&gt;private;
 	struct proc_maps_private *proc_priv = &amp;numa_priv-&gt;proc_maps;
 	struct vm_area_struct *vma = v;
<span class="p_chunk">@@ -1785,7 +1794,7 @@</span> <span class="p_context"> static int show_numa_map(struct seq_file *m, void *v, int is_pid)</span>
 		seq_puts(m, &quot; huge&quot;);
 
 	/* mmap_sem is held by m_start */
<span class="p_del">-	walk_page_vma(vma, &amp;walk);</span>
<span class="p_add">+	walk_page_vma(vma, &amp;walk, &amp;priv-&gt;mmrange);</span>
 
 	if (!md-&gt;pages)
 		goto out;
<span class="p_header">diff --git a/fs/proc/vmcore.c b/fs/proc/vmcore.c</span>
<span class="p_header">index a45f0af22a60..3768955c10bc 100644</span>
<span class="p_header">--- a/fs/proc/vmcore.c</span>
<span class="p_header">+++ b/fs/proc/vmcore.c</span>
<span class="p_chunk">@@ -350,6 +350,11 @@</span> <span class="p_context"> static int remap_oldmem_pfn_checked(struct vm_area_struct *vma,</span>
 	unsigned long pos_start, pos_end, pos;
 	unsigned long zeropage_pfn = my_zero_pfn(0);
 	size_t len = 0;
<span class="p_add">+	/*</span>
<span class="p_add">+	 * No concurrency for the bprm-&gt;mm yet -- this is a vmcore path,</span>
<span class="p_add">+	 * but do_munmap() needs an mmrange.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	pos_start = pfn;
 	pos_end = pfn + (size &gt;&gt; PAGE_SHIFT);
<span class="p_chunk">@@ -388,7 +393,7 @@</span> <span class="p_context"> static int remap_oldmem_pfn_checked(struct vm_area_struct *vma,</span>
 	}
 	return 0;
 fail:
<span class="p_del">-	do_munmap(vma-&gt;vm_mm, from, len, NULL);</span>
<span class="p_add">+	do_munmap(vma-&gt;vm_mm, from, len, NULL, &amp;mmrange);</span>
 	return -EAGAIN;
 }
 
<span class="p_chunk">@@ -411,6 +416,11 @@</span> <span class="p_context"> static int mmap_vmcore(struct file *file, struct vm_area_struct *vma)</span>
 	size_t size = vma-&gt;vm_end - vma-&gt;vm_start;
 	u64 start, end, len, tsz;
 	struct vmcore *m;
<span class="p_add">+	/*</span>
<span class="p_add">+	 * No concurrency for the bprm-&gt;mm yet -- this is a vmcore path,</span>
<span class="p_add">+	 * but do_munmap() needs an mmrange.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	start = (u64)vma-&gt;vm_pgoff &lt;&lt; PAGE_SHIFT;
 	end = start + size;
<span class="p_chunk">@@ -481,7 +491,7 @@</span> <span class="p_context"> static int mmap_vmcore(struct file *file, struct vm_area_struct *vma)</span>
 
 	return 0;
 fail:
<span class="p_del">-	do_munmap(vma-&gt;vm_mm, vma-&gt;vm_start, len, NULL);</span>
<span class="p_add">+	do_munmap(vma-&gt;vm_mm, vma-&gt;vm_start, len, NULL, &amp;mmrange);</span>
 	return -EAGAIN;
 }
 #else
<span class="p_header">diff --git a/fs/userfaultfd.c b/fs/userfaultfd.c</span>
<span class="p_header">index 87a13a7c8270..e3089865fd52 100644</span>
<span class="p_header">--- a/fs/userfaultfd.c</span>
<span class="p_header">+++ b/fs/userfaultfd.c</span>
<span class="p_chunk">@@ -851,6 +851,7 @@</span> <span class="p_context"> static int userfaultfd_release(struct inode *inode, struct file *file)</span>
 	/* len == 0 means wake all */
 	struct userfaultfd_wake_range range = { .len = 0, };
 	unsigned long new_flags;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	WRITE_ONCE(ctx-&gt;released, true);
 
<span class="p_chunk">@@ -880,7 +881,7 @@</span> <span class="p_context"> static int userfaultfd_release(struct inode *inode, struct file *file)</span>
 				 new_flags, vma-&gt;anon_vma,
 				 vma-&gt;vm_file, vma-&gt;vm_pgoff,
 				 vma_policy(vma),
<span class="p_del">-				 NULL_VM_UFFD_CTX);</span>
<span class="p_add">+				 NULL_VM_UFFD_CTX, &amp;mmrange);</span>
 		if (prev)
 			vma = prev;
 		else
<span class="p_chunk">@@ -1276,6 +1277,7 @@</span> <span class="p_context"> static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
 	bool found;
 	bool basic_ioctls;
 	unsigned long start, end, vma_end;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	user_uffdio_register = (struct uffdio_register __user *) arg;
 
<span class="p_chunk">@@ -1413,18 +1415,19 @@</span> <span class="p_context"> static int userfaultfd_register(struct userfaultfd_ctx *ctx,</span>
 		prev = vma_merge(mm, prev, start, vma_end, new_flags,
 				 vma-&gt;anon_vma, vma-&gt;vm_file, vma-&gt;vm_pgoff,
 				 vma_policy(vma),
<span class="p_del">-				 ((struct vm_userfaultfd_ctx){ ctx }));</span>
<span class="p_add">+				 ((struct vm_userfaultfd_ctx){ ctx }),</span>
<span class="p_add">+				 &amp;mmrange);</span>
 		if (prev) {
 			vma = prev;
 			goto next;
 		}
 		if (vma-&gt;vm_start &lt; start) {
<span class="p_del">-			ret = split_vma(mm, vma, start, 1);</span>
<span class="p_add">+			ret = split_vma(mm, vma, start, 1, &amp;mmrange);</span>
 			if (ret)
 				break;
 		}
 		if (vma-&gt;vm_end &gt; end) {
<span class="p_del">-			ret = split_vma(mm, vma, end, 0);</span>
<span class="p_add">+			ret = split_vma(mm, vma, end, 0, &amp;mmrange);</span>
 			if (ret)
 				break;
 		}
<span class="p_chunk">@@ -1471,6 +1474,7 @@</span> <span class="p_context"> static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
 	bool found;
 	unsigned long start, end, vma_end;
 	const void __user *buf = (void __user *)arg;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	ret = -EFAULT;
 	if (copy_from_user(&amp;uffdio_unregister, buf, sizeof(uffdio_unregister)))
<span class="p_chunk">@@ -1571,18 +1575,18 @@</span> <span class="p_context"> static int userfaultfd_unregister(struct userfaultfd_ctx *ctx,</span>
 		prev = vma_merge(mm, prev, start, vma_end, new_flags,
 				 vma-&gt;anon_vma, vma-&gt;vm_file, vma-&gt;vm_pgoff,
 				 vma_policy(vma),
<span class="p_del">-				 NULL_VM_UFFD_CTX);</span>
<span class="p_add">+				 NULL_VM_UFFD_CTX, &amp;mmrange);</span>
 		if (prev) {
 			vma = prev;
 			goto next;
 		}
 		if (vma-&gt;vm_start &lt; start) {
<span class="p_del">-			ret = split_vma(mm, vma, start, 1);</span>
<span class="p_add">+			ret = split_vma(mm, vma, start, 1, &amp;mmrange);</span>
 			if (ret)
 				break;
 		}
 		if (vma-&gt;vm_end &gt; end) {
<span class="p_del">-			ret = split_vma(mm, vma, end, 0);</span>
<span class="p_add">+			ret = split_vma(mm, vma, end, 0, &amp;mmrange);</span>
 			if (ret)
 				break;
 		}
<span class="p_header">diff --git a/include/asm-generic/mm_hooks.h b/include/asm-generic/mm_hooks.h</span>
<span class="p_header">index 8ac4e68a12f0..2115deceded1 100644</span>
<span class="p_header">--- a/include/asm-generic/mm_hooks.h</span>
<span class="p_header">+++ b/include/asm-generic/mm_hooks.h</span>
<span class="p_chunk">@@ -19,7 +19,8 @@</span> <span class="p_context"> static inline void arch_exit_mmap(struct mm_struct *mm)</span>
 
 static inline void arch_unmap(struct mm_struct *mm,
 			struct vm_area_struct *vma,
<span class="p_del">-			unsigned long start, unsigned long end)</span>
<span class="p_add">+			unsigned long start, unsigned long end,</span>
<span class="p_add">+			struct range_lock *mmrange)</span>
 {
 }
 
<span class="p_header">diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
<span class="p_header">index 325017ad9311..da004594d831 100644</span>
<span class="p_header">--- a/include/linux/hmm.h</span>
<span class="p_header">+++ b/include/linux/hmm.h</span>
<span class="p_chunk">@@ -295,7 +295,7 @@</span> <span class="p_context"> int hmm_vma_get_pfns(struct vm_area_struct *vma,</span>
 		     struct hmm_range *range,
 		     unsigned long start,
 		     unsigned long end,
<span class="p_del">-		     hmm_pfn_t *pfns);</span>
<span class="p_add">+		     hmm_pfn_t *pfns, struct range_lock *mmrange);</span>
 bool hmm_vma_range_done(struct vm_area_struct *vma, struct hmm_range *range);
 
 
<span class="p_chunk">@@ -323,7 +323,7 @@</span> <span class="p_context"> int hmm_vma_fault(struct vm_area_struct *vma,</span>
 		  unsigned long end,
 		  hmm_pfn_t *pfns,
 		  bool write,
<span class="p_del">-		  bool block);</span>
<span class="p_add">+		  bool block, struct range_lock *mmrange);</span>
 #endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
 
 
<span class="p_header">diff --git a/include/linux/ksm.h b/include/linux/ksm.h</span>
<span class="p_header">index 44368b19b27e..19667b75f73c 100644</span>
<span class="p_header">--- a/include/linux/ksm.h</span>
<span class="p_header">+++ b/include/linux/ksm.h</span>
<span class="p_chunk">@@ -20,7 +20,8 @@</span> <span class="p_context"> struct mem_cgroup;</span>
 
 #ifdef CONFIG_KSM
 int ksm_madvise(struct vm_area_struct *vma, unsigned long start,
<span class="p_del">-		unsigned long end, int advice, unsigned long *vm_flags);</span>
<span class="p_add">+		unsigned long end, int advice, unsigned long *vm_flags,</span>
<span class="p_add">+		struct range_lock *mmrange);</span>
 int __ksm_enter(struct mm_struct *mm);
 void __ksm_exit(struct mm_struct *mm);
 
<span class="p_chunk">@@ -78,7 +79,8 @@</span> <span class="p_context"> static inline void ksm_exit(struct mm_struct *mm)</span>
 
 #ifdef CONFIG_MMU
 static inline int ksm_madvise(struct vm_area_struct *vma, unsigned long start,
<span class="p_del">-		unsigned long end, int advice, unsigned long *vm_flags)</span>
<span class="p_add">+		      unsigned long end, int advice, unsigned long *vm_flags,</span>
<span class="p_add">+		      struct range_lock *mmrange)</span>
 {
 	return 0;
 }
<span class="p_header">diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="p_header">index 0c6fe904bc97..fa08e348a295 100644</span>
<span class="p_header">--- a/include/linux/migrate.h</span>
<span class="p_header">+++ b/include/linux/migrate.h</span>
<span class="p_chunk">@@ -272,7 +272,7 @@</span> <span class="p_context"> int migrate_vma(const struct migrate_vma_ops *ops,</span>
 		unsigned long end,
 		unsigned long *src,
 		unsigned long *dst,
<span class="p_del">-		void *private);</span>
<span class="p_add">+		void *private, struct range_lock *mmrange);</span>
 #else
 static inline int migrate_vma(const struct migrate_vma_ops *ops,
 			      struct vm_area_struct *vma,
<span class="p_chunk">@@ -280,7 +280,7 @@</span> <span class="p_context"> static inline int migrate_vma(const struct migrate_vma_ops *ops,</span>
 			      unsigned long end,
 			      unsigned long *src,
 			      unsigned long *dst,
<span class="p_del">-			      void *private)</span>
<span class="p_add">+			      void *private, struct range_lock *mmrange)</span>
 {
 	return -EINVAL;
 }
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index bcf2509d448d..fc4e7fdc3e76 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1295,11 +1295,12 @@</span> <span class="p_context"> struct mm_walk {</span>
 	int (*pud_entry)(pud_t *pud, unsigned long addr,
 			 unsigned long next, struct mm_walk *walk);
 	int (*pmd_entry)(pmd_t *pmd, unsigned long addr,
<span class="p_del">-			 unsigned long next, struct mm_walk *walk);</span>
<span class="p_add">+			 unsigned long next, struct mm_walk *walk,</span>
<span class="p_add">+			 struct range_lock *mmrange);</span>
 	int (*pte_entry)(pte_t *pte, unsigned long addr,
 			 unsigned long next, struct mm_walk *walk);
 	int (*pte_hole)(unsigned long addr, unsigned long next,
<span class="p_del">-			struct mm_walk *walk);</span>
<span class="p_add">+			struct mm_walk *walk, struct range_lock *mmrange);</span>
 	int (*hugetlb_entry)(pte_t *pte, unsigned long hmask,
 			     unsigned long addr, unsigned long next,
 			     struct mm_walk *walk);
<span class="p_chunk">@@ -1311,8 +1312,9 @@</span> <span class="p_context"> struct mm_walk {</span>
 };
 
 int walk_page_range(unsigned long addr, unsigned long end,
<span class="p_del">-		struct mm_walk *walk);</span>
<span class="p_del">-int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk);</span>
<span class="p_add">+		    struct mm_walk *walk, struct range_lock *mmrange);</span>
<span class="p_add">+int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk,</span>
<span class="p_add">+		  struct range_lock *mmrange);</span>
 void free_pgd_range(struct mmu_gather *tlb, unsigned long addr,
 		unsigned long end, unsigned long floor, unsigned long ceiling);
 int copy_page_range(struct mm_struct *dst, struct mm_struct *src,
<span class="p_chunk">@@ -1337,17 +1339,18 @@</span> <span class="p_context"> int invalidate_inode_page(struct page *page);</span>
 
 #ifdef CONFIG_MMU
 extern int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
<span class="p_del">-		unsigned int flags);</span>
<span class="p_add">+			   unsigned int flags, struct range_lock *mmrange);</span>
 extern int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 			    unsigned long address, unsigned int fault_flags,
<span class="p_del">-			    bool *unlocked);</span>
<span class="p_add">+			    bool *unlocked, struct range_lock *mmrange);</span>
 void unmap_mapping_pages(struct address_space *mapping,
 		pgoff_t start, pgoff_t nr, bool even_cows);
 void unmap_mapping_range(struct address_space *mapping,
 		loff_t const holebegin, loff_t const holelen, int even_cows);
 #else
 static inline int handle_mm_fault(struct vm_area_struct *vma,
<span class="p_del">-		unsigned long address, unsigned int flags)</span>
<span class="p_add">+		unsigned long address, unsigned int flags,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	/* should never happen if there&#39;s no MMU */
 	BUG();
<span class="p_chunk">@@ -1355,7 +1358,8 @@</span> <span class="p_context"> static inline int handle_mm_fault(struct vm_area_struct *vma,</span>
 }
 static inline int fixup_user_fault(struct task_struct *tsk,
 		struct mm_struct *mm, unsigned long address,
<span class="p_del">-		unsigned int fault_flags, bool *unlocked)</span>
<span class="p_add">+		unsigned int fault_flags, bool *unlocked,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	/* should never happen if there&#39;s no MMU */
 	BUG();
<span class="p_chunk">@@ -1383,24 +1387,28 @@</span> <span class="p_context"> extern int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 long get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,
 			    unsigned long start, unsigned long nr_pages,
 			    unsigned int gup_flags, struct page **pages,
<span class="p_del">-			    struct vm_area_struct **vmas, int *locked);</span>
<span class="p_add">+			    struct vm_area_struct **vmas, int *locked,</span>
<span class="p_add">+			    struct range_lock *mmrange);</span>
 long get_user_pages(unsigned long start, unsigned long nr_pages,
<span class="p_del">-			    unsigned int gup_flags, struct page **pages,</span>
<span class="p_del">-			    struct vm_area_struct **vmas);</span>
<span class="p_add">+		    unsigned int gup_flags, struct page **pages,</span>
<span class="p_add">+		    struct vm_area_struct **vmas, struct range_lock *mmrange);</span>
 long get_user_pages_locked(unsigned long start, unsigned long nr_pages,
<span class="p_del">-		    unsigned int gup_flags, struct page **pages, int *locked);</span>
<span class="p_add">+			   unsigned int gup_flags, struct page **pages,</span>
<span class="p_add">+			   int *locked, struct range_lock *mmrange);</span>
 long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,
 		    struct page **pages, unsigned int gup_flags);
 #ifdef CONFIG_FS_DAX
 long get_user_pages_longterm(unsigned long start, unsigned long nr_pages,
<span class="p_del">-			    unsigned int gup_flags, struct page **pages,</span>
<span class="p_del">-			    struct vm_area_struct **vmas);</span>
<span class="p_add">+			     unsigned int gup_flags, struct page **pages,</span>
<span class="p_add">+			     struct vm_area_struct **vmas,</span>
<span class="p_add">+			     struct range_lock *mmrange);</span>
 #else
 static inline long get_user_pages_longterm(unsigned long start,
 		unsigned long nr_pages, unsigned int gup_flags,
<span class="p_del">-		struct page **pages, struct vm_area_struct **vmas)</span>
<span class="p_add">+		struct page **pages, struct vm_area_struct **vmas,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
<span class="p_del">-	return get_user_pages(start, nr_pages, gup_flags, pages, vmas);</span>
<span class="p_add">+	return get_user_pages(start, nr_pages, gup_flags, pages, vmas, mmrange);</span>
 }
 #endif /* CONFIG_FS_DAX */
 
<span class="p_chunk">@@ -1505,7 +1513,8 @@</span> <span class="p_context"> extern unsigned long change_protection(struct vm_area_struct *vma, unsigned long</span>
 			      int dirty_accountable, int prot_numa);
 extern int mprotect_fixup(struct vm_area_struct *vma,
 			  struct vm_area_struct **pprev, unsigned long start,
<span class="p_del">-			  unsigned long end, unsigned long newflags);</span>
<span class="p_add">+			  unsigned long end, unsigned long newflags,</span>
<span class="p_add">+			  struct range_lock *mmrange);</span>
 
 /*
  * doesn&#39;t attempt to fault and will return short.
<span class="p_chunk">@@ -2149,28 +2158,30 @@</span> <span class="p_context"> void anon_vma_interval_tree_verify(struct anon_vma_chain *node);</span>
 extern int __vm_enough_memory(struct mm_struct *mm, long pages, int cap_sys_admin);
 extern int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 	unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,
<span class="p_del">-	struct vm_area_struct *expand);</span>
<span class="p_add">+	struct vm_area_struct *expand, struct range_lock *mmrange);</span>
 static inline int vma_adjust(struct vm_area_struct *vma, unsigned long start,
<span class="p_del">-	unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert)</span>
<span class="p_add">+	unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,</span>
<span class="p_add">+	struct range_lock *mmrange)</span>
 {
<span class="p_del">-	return __vma_adjust(vma, start, end, pgoff, insert, NULL);</span>
<span class="p_add">+	return __vma_adjust(vma, start, end, pgoff, insert, NULL, mmrange);</span>
 }
 extern struct vm_area_struct *vma_merge(struct mm_struct *,
 	struct vm_area_struct *prev, unsigned long addr, unsigned long end,
 	unsigned long vm_flags, struct anon_vma *, struct file *, pgoff_t,
<span class="p_del">-	struct mempolicy *, struct vm_userfaultfd_ctx);</span>
<span class="p_add">+	struct mempolicy *, struct vm_userfaultfd_ctx,</span>
<span class="p_add">+	struct range_lock *mmrange);</span>
 extern struct anon_vma *find_mergeable_anon_vma(struct vm_area_struct *);
 extern int __split_vma(struct mm_struct *, struct vm_area_struct *,
<span class="p_del">-	unsigned long addr, int new_below);</span>
<span class="p_add">+	unsigned long addr, int new_below, struct range_lock *mmrange);</span>
 extern int split_vma(struct mm_struct *, struct vm_area_struct *,
<span class="p_del">-	unsigned long addr, int new_below);</span>
<span class="p_add">+	unsigned long addr, int new_below, struct range_lock *mmrange);</span>
 extern int insert_vm_struct(struct mm_struct *, struct vm_area_struct *);
 extern void __vma_link_rb(struct mm_struct *, struct vm_area_struct *,
 	struct rb_node **, struct rb_node *);
 extern void unlink_file_vma(struct vm_area_struct *);
 extern struct vm_area_struct *copy_vma(struct vm_area_struct **,
 	unsigned long addr, unsigned long len, pgoff_t pgoff,
<span class="p_del">-	bool *need_rmap_locks);</span>
<span class="p_add">+	bool *need_rmap_locks, struct range_lock *mmrange);</span>
 extern void exit_mmap(struct mm_struct *);
 
 static inline int check_data_rlimit(unsigned long rlim,
<span class="p_chunk">@@ -2212,21 +2223,22 @@</span> <span class="p_context"> extern unsigned long get_unmapped_area(struct file *, unsigned long, unsigned lo</span>
 
 extern unsigned long mmap_region(struct file *file, unsigned long addr,
 	unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
<span class="p_del">-	struct list_head *uf);</span>
<span class="p_add">+	struct list_head *uf, struct range_lock *mmrange);</span>
 extern unsigned long do_mmap(struct file *file, unsigned long addr,
 	unsigned long len, unsigned long prot, unsigned long flags,
 	vm_flags_t vm_flags, unsigned long pgoff, unsigned long *populate,
<span class="p_del">-	struct list_head *uf);</span>
<span class="p_add">+	struct list_head *uf, struct range_lock *mmrange);</span>
 extern int do_munmap(struct mm_struct *, unsigned long, size_t,
<span class="p_del">-		     struct list_head *uf);</span>
<span class="p_add">+		     struct list_head *uf, struct range_lock *mmrange);</span>
 
 static inline unsigned long
 do_mmap_pgoff(struct file *file, unsigned long addr,
 	unsigned long len, unsigned long prot, unsigned long flags,
 	unsigned long pgoff, unsigned long *populate,
<span class="p_del">-	struct list_head *uf)</span>
<span class="p_add">+	struct list_head *uf, struct range_lock *mmrange)</span>
 {
<span class="p_del">-	return do_mmap(file, addr, len, prot, flags, 0, pgoff, populate, uf);</span>
<span class="p_add">+	return do_mmap(file, addr, len, prot, flags, 0, pgoff, populate,</span>
<span class="p_add">+		       uf, mmrange);</span>
 }
 
 #ifdef CONFIG_MMU
<span class="p_chunk">@@ -2405,7 +2417,8 @@</span> <span class="p_context"> unsigned long change_prot_numa(struct vm_area_struct *vma,</span>
 			unsigned long start, unsigned long end);
 #endif
 
<span class="p_del">-struct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr);</span>
<span class="p_add">+struct vm_area_struct *find_extend_vma(struct mm_struct *, unsigned long addr,</span>
<span class="p_add">+				       struct range_lock *);</span>
 int remap_pfn_range(struct vm_area_struct *, unsigned long addr,
 			unsigned long pfn, unsigned long size, pgprot_t);
 int vm_insert_page(struct vm_area_struct *, unsigned long addr, struct page *);
<span class="p_header">diff --git a/include/linux/uprobes.h b/include/linux/uprobes.h</span>
<span class="p_header">index 0a294e950df8..79eb735e7c95 100644</span>
<span class="p_header">--- a/include/linux/uprobes.h</span>
<span class="p_header">+++ b/include/linux/uprobes.h</span>
<span class="p_chunk">@@ -34,6 +34,7 @@</span> <span class="p_context"> struct mm_struct;</span>
 struct inode;
 struct notifier_block;
 struct page;
<span class="p_add">+struct range_lock;</span>
 
 #define UPROBE_HANDLER_REMOVE		1
 #define UPROBE_HANDLER_MASK		1
<span class="p_chunk">@@ -115,17 +116,20 @@</span> <span class="p_context"> struct uprobes_state {</span>
 	struct xol_area		*xol_area;
 };
 
<span class="p_del">-extern int set_swbp(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);</span>
<span class="p_del">-extern int set_orig_insn(struct arch_uprobe *aup, struct mm_struct *mm, unsigned long vaddr);</span>
<span class="p_add">+extern int set_swbp(struct arch_uprobe *aup, struct mm_struct *mm,</span>
<span class="p_add">+		    unsigned long vaddr, struct range_lock *mmrange);</span>
<span class="p_add">+extern int set_orig_insn(struct arch_uprobe *aup, struct mm_struct *mm,</span>
<span class="p_add">+			 unsigned long vaddr, struct range_lock *mmrange);</span>
 extern bool is_swbp_insn(uprobe_opcode_t *insn);
 extern bool is_trap_insn(uprobe_opcode_t *insn);
 extern unsigned long uprobe_get_swbp_addr(struct pt_regs *regs);
 extern unsigned long uprobe_get_trap_addr(struct pt_regs *regs);
<span class="p_del">-extern int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr, uprobe_opcode_t);</span>
<span class="p_add">+extern int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,</span>
<span class="p_add">+			       uprobe_opcode_t, struct range_lock *mmrange);</span>
 extern int uprobe_register(struct inode *inode, loff_t offset, struct uprobe_consumer *uc);
 extern int uprobe_apply(struct inode *inode, loff_t offset, struct uprobe_consumer *uc, bool);
 extern void uprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc);
<span class="p_del">-extern int uprobe_mmap(struct vm_area_struct *vma);</span>
<span class="p_add">+extern int uprobe_mmap(struct vm_area_struct *vma, struct range_lock *mmrange);;</span>
 extern void uprobe_munmap(struct vm_area_struct *vma, unsigned long start, unsigned long end);
 extern void uprobe_start_dup_mmap(void);
 extern void uprobe_end_dup_mmap(void);
<span class="p_chunk">@@ -169,7 +173,8 @@</span> <span class="p_context"> static inline void</span>
 uprobe_unregister(struct inode *inode, loff_t offset, struct uprobe_consumer *uc)
 {
 }
<span class="p_del">-static inline int uprobe_mmap(struct vm_area_struct *vma)</span>
<span class="p_add">+static inline int uprobe_mmap(struct vm_area_struct *vma,</span>
<span class="p_add">+			      struct range_lock *mmrange)</span>
 {
 	return 0;
 }
<span class="p_header">diff --git a/ipc/shm.c b/ipc/shm.c</span>
<span class="p_header">index 4643865e9171..6c29c791c7f2 100644</span>
<span class="p_header">--- a/ipc/shm.c</span>
<span class="p_header">+++ b/ipc/shm.c</span>
<span class="p_chunk">@@ -1293,6 +1293,7 @@</span> <span class="p_context"> long do_shmat(int shmid, char __user *shmaddr, int shmflg,</span>
 	struct path path;
 	fmode_t f_mode;
 	unsigned long populate = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	err = -EINVAL;
 	if (shmid &lt; 0)
<span class="p_chunk">@@ -1411,7 +1412,8 @@</span> <span class="p_context"> long do_shmat(int shmid, char __user *shmaddr, int shmflg,</span>
 			goto invalid;
 	}
 
<span class="p_del">-	addr = do_mmap_pgoff(file, addr, size, prot, flags, 0, &amp;populate, NULL);</span>
<span class="p_add">+	addr = do_mmap_pgoff(file, addr, size, prot, flags, 0, &amp;populate, NULL,</span>
<span class="p_add">+			     &amp;mmrange);</span>
 	*raddr = addr;
 	err = 0;
 	if (IS_ERR_VALUE(addr))
<span class="p_chunk">@@ -1487,6 +1489,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
 	struct file *file;
 	struct vm_area_struct *next;
 #endif
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (addr &amp; ~PAGE_MASK)
 		return retval;
<span class="p_chunk">@@ -1537,7 +1540,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
 			 */
 			file = vma-&gt;vm_file;
 			size = i_size_read(file_inode(vma-&gt;vm_file));
<span class="p_del">-			do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start, NULL);</span>
<span class="p_add">+			do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start,</span>
<span class="p_add">+				  NULL, &amp;mmrange);</span>
 			/*
 			 * We discovered the size of the shm segment, so
 			 * break out of here and fall through to the next
<span class="p_chunk">@@ -1564,7 +1568,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
 		if ((vma-&gt;vm_ops == &amp;shm_vm_ops) &amp;&amp;
 		    ((vma-&gt;vm_start - addr)/PAGE_SIZE == vma-&gt;vm_pgoff) &amp;&amp;
 		    (vma-&gt;vm_file == file))
<span class="p_del">-			do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start, NULL);</span>
<span class="p_add">+			do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start,</span>
<span class="p_add">+				  NULL, &amp;mmrange);</span>
 		vma = next;
 	}
 
<span class="p_chunk">@@ -1573,7 +1578,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(shmdt, char __user *, shmaddr)</span>
 	 * given
 	 */
 	if (vma &amp;&amp; vma-&gt;vm_start == addr &amp;&amp; vma-&gt;vm_ops == &amp;shm_vm_ops) {
<span class="p_del">-		do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start, NULL);</span>
<span class="p_add">+		do_munmap(mm, vma-&gt;vm_start, vma-&gt;vm_end - vma-&gt;vm_start,</span>
<span class="p_add">+			  NULL, &amp;mmrange);</span>
 		retval = 0;
 	}
 
<span class="p_header">diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="p_header">index ce6848e46e94..60e12b39182c 100644</span>
<span class="p_header">--- a/kernel/events/uprobes.c</span>
<span class="p_header">+++ b/kernel/events/uprobes.c</span>
<span class="p_chunk">@@ -300,7 +300,7 @@</span> <span class="p_context"> static int verify_opcode(struct page *page, unsigned long vaddr, uprobe_opcode_t</span>
  * Return 0 (success) or a negative errno.
  */
 int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,
<span class="p_del">-			uprobe_opcode_t opcode)</span>
<span class="p_add">+			uprobe_opcode_t opcode, struct range_lock *mmrange)</span>
 {
 	struct page *old_page, *new_page;
 	struct vm_area_struct *vma;
<span class="p_chunk">@@ -309,7 +309,8 @@</span> <span class="p_context"> int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,</span>
 retry:
 	/* Read the page with vaddr into memory */
 	ret = get_user_pages_remote(NULL, mm, vaddr, 1,
<span class="p_del">-			FOLL_FORCE | FOLL_SPLIT, &amp;old_page, &amp;vma, NULL);</span>
<span class="p_add">+			FOLL_FORCE | FOLL_SPLIT, &amp;old_page, &amp;vma, NULL,</span>
<span class="p_add">+			mmrange);</span>
 	if (ret &lt;= 0)
 		return ret;
 
<span class="p_chunk">@@ -349,9 +350,10 @@</span> <span class="p_context"> int uprobe_write_opcode(struct mm_struct *mm, unsigned long vaddr,</span>
  * For mm @mm, store the breakpoint instruction at @vaddr.
  * Return 0 (success) or a negative errno.
  */
<span class="p_del">-int __weak set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr)</span>
<span class="p_add">+int __weak set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm,</span>
<span class="p_add">+		    unsigned long vaddr, struct range_lock *mmrange)</span>
 {
<span class="p_del">-	return uprobe_write_opcode(mm, vaddr, UPROBE_SWBP_INSN);</span>
<span class="p_add">+	return uprobe_write_opcode(mm, vaddr, UPROBE_SWBP_INSN, mmrange);</span>
 }
 
 /**
<span class="p_chunk">@@ -364,9 +366,12 @@</span> <span class="p_context"> int __weak set_swbp(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned</span>
  * Return 0 (success) or a negative errno.
  */
 int __weak
<span class="p_del">-set_orig_insn(struct arch_uprobe *auprobe, struct mm_struct *mm, unsigned long vaddr)</span>
<span class="p_add">+set_orig_insn(struct arch_uprobe *auprobe, struct mm_struct *mm,</span>
<span class="p_add">+	      unsigned long vaddr, struct range_lock *mmrange)</span>
 {
<span class="p_del">-	return uprobe_write_opcode(mm, vaddr, *(uprobe_opcode_t *)&amp;auprobe-&gt;insn);</span>
<span class="p_add">+	return uprobe_write_opcode(mm, vaddr,</span>
<span class="p_add">+				   *(uprobe_opcode_t *)&amp;auprobe-&gt;insn,</span>
<span class="p_add">+				   mmrange);</span>
 }
 
 static struct uprobe *get_uprobe(struct uprobe *uprobe)
<span class="p_chunk">@@ -650,7 +655,8 @@</span> <span class="p_context"> static bool filter_chain(struct uprobe *uprobe,</span>
 
 static int
 install_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,
<span class="p_del">-			struct vm_area_struct *vma, unsigned long vaddr)</span>
<span class="p_add">+		   struct vm_area_struct *vma, unsigned long vaddr,</span>
<span class="p_add">+		   struct range_lock *mmrange)</span>
 {
 	bool first_uprobe;
 	int ret;
<span class="p_chunk">@@ -667,7 +673,7 @@</span> <span class="p_context"> install_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,</span>
 	if (first_uprobe)
 		set_bit(MMF_HAS_UPROBES, &amp;mm-&gt;flags);
 
<span class="p_del">-	ret = set_swbp(&amp;uprobe-&gt;arch, mm, vaddr);</span>
<span class="p_add">+	ret = set_swbp(&amp;uprobe-&gt;arch, mm, vaddr, mmrange);</span>
 	if (!ret)
 		clear_bit(MMF_RECALC_UPROBES, &amp;mm-&gt;flags);
 	else if (first_uprobe)
<span class="p_chunk">@@ -677,10 +683,11 @@</span> <span class="p_context"> install_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,</span>
 }
 
 static int
<span class="p_del">-remove_breakpoint(struct uprobe *uprobe, struct mm_struct *mm, unsigned long vaddr)</span>
<span class="p_add">+remove_breakpoint(struct uprobe *uprobe, struct mm_struct *mm,</span>
<span class="p_add">+		  unsigned long vaddr, struct range_lock *mmrange)</span>
 {
 	set_bit(MMF_RECALC_UPROBES, &amp;mm-&gt;flags);
<span class="p_del">-	return set_orig_insn(&amp;uprobe-&gt;arch, mm, vaddr);</span>
<span class="p_add">+	return set_orig_insn(&amp;uprobe-&gt;arch, mm, vaddr, mmrange);</span>
 }
 
 static inline bool uprobe_is_active(struct uprobe *uprobe)
<span class="p_chunk">@@ -794,6 +801,7 @@</span> <span class="p_context"> register_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)</span>
 	bool is_register = !!new;
 	struct map_info *info;
 	int err = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	percpu_down_write(&amp;dup_mmap_sem);
 	info = build_map_info(uprobe-&gt;inode-&gt;i_mapping,
<span class="p_chunk">@@ -824,11 +832,13 @@</span> <span class="p_context"> register_for_each_vma(struct uprobe *uprobe, struct uprobe_consumer *new)</span>
 			/* consult only the &quot;caller&quot;, new consumer. */
 			if (consumer_filter(new,
 					UPROBE_FILTER_REGISTER, mm))
<span class="p_del">-				err = install_breakpoint(uprobe, mm, vma, info-&gt;vaddr);</span>
<span class="p_add">+				err = install_breakpoint(uprobe, mm, vma,</span>
<span class="p_add">+							 info-&gt;vaddr, &amp;mmrange);</span>
 		} else if (test_bit(MMF_HAS_UPROBES, &amp;mm-&gt;flags)) {
 			if (!filter_chain(uprobe,
 					UPROBE_FILTER_UNREGISTER, mm))
<span class="p_del">-				err |= remove_breakpoint(uprobe, mm, info-&gt;vaddr);</span>
<span class="p_add">+				err |= remove_breakpoint(uprobe, mm,</span>
<span class="p_add">+							 info-&gt;vaddr, &amp;mmrange);</span>
 		}
 
  unlock:
<span class="p_chunk">@@ -972,6 +982,7 @@</span> <span class="p_context"> static int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)</span>
 {
 	struct vm_area_struct *vma;
 	int err = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_read(&amp;mm-&gt;mmap_sem);
 	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
<span class="p_chunk">@@ -988,7 +999,7 @@</span> <span class="p_context"> static int unapply_uprobe(struct uprobe *uprobe, struct mm_struct *mm)</span>
 			continue;
 
 		vaddr = offset_to_vaddr(vma, uprobe-&gt;offset);
<span class="p_del">-		err |= remove_breakpoint(uprobe, mm, vaddr);</span>
<span class="p_add">+		err |= remove_breakpoint(uprobe, mm, vaddr, &amp;mmrange);</span>
 	}
 	up_read(&amp;mm-&gt;mmap_sem);
 
<span class="p_chunk">@@ -1063,7 +1074,7 @@</span> <span class="p_context"> static void build_probe_list(struct inode *inode,</span>
  * Currently we ignore all errors and always return 0, the callers
  * can&#39;t handle the failure anyway.
  */
<span class="p_del">-int uprobe_mmap(struct vm_area_struct *vma)</span>
<span class="p_add">+int uprobe_mmap(struct vm_area_struct *vma, struct range_lock *mmrange)</span>
 {
 	struct list_head tmp_list;
 	struct uprobe *uprobe, *u;
<span class="p_chunk">@@ -1087,7 +1098,7 @@</span> <span class="p_context"> int uprobe_mmap(struct vm_area_struct *vma)</span>
 		if (!fatal_signal_pending(current) &amp;&amp;
 		    filter_chain(uprobe, UPROBE_FILTER_MMAP, vma-&gt;vm_mm)) {
 			unsigned long vaddr = offset_to_vaddr(vma, uprobe-&gt;offset);
<span class="p_del">-			install_breakpoint(uprobe, vma-&gt;vm_mm, vma, vaddr);</span>
<span class="p_add">+			install_breakpoint(uprobe, vma-&gt;vm_mm, vma, vaddr, mmrange);</span>
 		}
 		put_uprobe(uprobe);
 	}
<span class="p_chunk">@@ -1698,7 +1709,8 @@</span> <span class="p_context"> static void mmf_recalc_uprobes(struct mm_struct *mm)</span>
 	clear_bit(MMF_HAS_UPROBES, &amp;mm-&gt;flags);
 }
 
<span class="p_del">-static int is_trap_at_addr(struct mm_struct *mm, unsigned long vaddr)</span>
<span class="p_add">+static int is_trap_at_addr(struct mm_struct *mm, unsigned long vaddr,</span>
<span class="p_add">+			   struct range_lock *mmrange)</span>
 {
 	struct page *page;
 	uprobe_opcode_t opcode;
<span class="p_chunk">@@ -1718,7 +1730,7 @@</span> <span class="p_context"> static int is_trap_at_addr(struct mm_struct *mm, unsigned long vaddr)</span>
 	 * essentially a kernel access to the memory.
 	 */
 	result = get_user_pages_remote(NULL, mm, vaddr, 1, FOLL_FORCE, &amp;page,
<span class="p_del">-			NULL, NULL);</span>
<span class="p_add">+				       NULL, NULL, mmrange);</span>
 	if (result &lt; 0)
 		return result;
 
<span class="p_chunk">@@ -1734,6 +1746,7 @@</span> <span class="p_context"> static struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct uprobe *uprobe = NULL;
 	struct vm_area_struct *vma;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_read(&amp;mm-&gt;mmap_sem);
 	vma = find_vma(mm, bp_vaddr);
<span class="p_chunk">@@ -1746,7 +1759,7 @@</span> <span class="p_context"> static struct uprobe *find_active_uprobe(unsigned long bp_vaddr, int *is_swbp)</span>
 		}
 
 		if (!uprobe)
<span class="p_del">-			*is_swbp = is_trap_at_addr(mm, bp_vaddr);</span>
<span class="p_add">+			*is_swbp = is_trap_at_addr(mm, bp_vaddr, &amp;mmrange);</span>
 	} else {
 		*is_swbp = -EFAULT;
 	}
<span class="p_header">diff --git a/kernel/futex.c b/kernel/futex.c</span>
<span class="p_header">index 1f450e092c74..09a0d86f80a0 100644</span>
<span class="p_header">--- a/kernel/futex.c</span>
<span class="p_header">+++ b/kernel/futex.c</span>
<span class="p_chunk">@@ -725,10 +725,11 @@</span> <span class="p_context"> static int fault_in_user_writeable(u32 __user *uaddr)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_read(&amp;mm-&gt;mmap_sem);
 	ret = fixup_user_fault(current, mm, (unsigned long)uaddr,
<span class="p_del">-			       FAULT_FLAG_WRITE, NULL);</span>
<span class="p_add">+			       FAULT_FLAG_WRITE, NULL, &amp;mmrange);</span>
 	up_read(&amp;mm-&gt;mmap_sem);
 
 	return ret &lt; 0 ? ret : 0;
<span class="p_header">diff --git a/mm/frame_vector.c b/mm/frame_vector.c</span>
<span class="p_header">index c64dca6e27c2..d3dccd80c6ee 100644</span>
<span class="p_header">--- a/mm/frame_vector.c</span>
<span class="p_header">+++ b/mm/frame_vector.c</span>
<span class="p_chunk">@@ -39,6 +39,7 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 	int ret = 0;
 	int err;
 	int locked;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (nr_frames == 0)
 		return 0;
<span class="p_chunk">@@ -71,7 +72,8 @@</span> <span class="p_context"> int get_vaddr_frames(unsigned long start, unsigned int nr_frames,</span>
 		vec-&gt;got_ref = true;
 		vec-&gt;is_pfns = false;
 		ret = get_user_pages_locked(start, nr_frames,
<span class="p_del">-			gup_flags, (struct page **)(vec-&gt;ptrs), &amp;locked);</span>
<span class="p_add">+			gup_flags, (struct page **)(vec-&gt;ptrs), &amp;locked,</span>
<span class="p_add">+			&amp;mmrange);</span>
 		goto out;
 	}
 
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 1b46e6e74881..01983a7b3750 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -478,7 +478,8 @@</span> <span class="p_context"> static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
  * If it is, *@nonblocking will be set to 0 and -EBUSY returned.
  */
 static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,
<span class="p_del">-		unsigned long address, unsigned int *flags, int *nonblocking)</span>
<span class="p_add">+		unsigned long address, unsigned int *flags, int *nonblocking,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	unsigned int fault_flags = 0;
 	int ret;
<span class="p_chunk">@@ -499,7 +500,7 @@</span> <span class="p_context"> static int faultin_page(struct task_struct *tsk, struct vm_area_struct *vma,</span>
 		fault_flags |= FAULT_FLAG_TRIED;
 	}
 
<span class="p_del">-	ret = handle_mm_fault(vma, address, fault_flags);</span>
<span class="p_add">+	ret = handle_mm_fault(vma, address, fault_flags, mmrange);</span>
 	if (ret &amp; VM_FAULT_ERROR) {
 		int err = vm_fault_to_errno(ret, *flags);
 
<span class="p_chunk">@@ -592,6 +593,7 @@</span> <span class="p_context"> static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)</span>
  * @vmas:	array of pointers to vmas corresponding to each page.
  *		Or NULL if the caller does not require them.
  * @nonblocking: whether waiting for disk IO or mmap_sem contention
<span class="p_add">+ * @mmrange:	mm address space range locking</span>
  *
  * Returns number of pages pinned. This may be fewer than the number
  * requested. If nr_pages is 0 or negative, returns 0. If no pages
<span class="p_chunk">@@ -638,7 +640,8 @@</span> <span class="p_context"> static int check_vma_flags(struct vm_area_struct *vma, unsigned long gup_flags)</span>
 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 		unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
<span class="p_del">-		struct vm_area_struct **vmas, int *nonblocking)</span>
<span class="p_add">+		struct vm_area_struct **vmas, int *nonblocking,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	long i = 0;
 	unsigned int page_mask;
<span class="p_chunk">@@ -664,7 +667,7 @@</span> <span class="p_context"> static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
 
 		/* first iteration or cross vma bound */
 		if (!vma || start &gt;= vma-&gt;vm_end) {
<span class="p_del">-			vma = find_extend_vma(mm, start);</span>
<span class="p_add">+			vma = find_extend_vma(mm, start, mmrange);</span>
 			if (!vma &amp;&amp; in_gate_area(mm, start)) {
 				int ret;
 				ret = get_gate_page(mm, start &amp; PAGE_MASK,
<span class="p_chunk">@@ -697,7 +700,7 @@</span> <span class="p_context"> static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
 		if (!page) {
 			int ret;
 			ret = faultin_page(tsk, vma, start, &amp;foll_flags,
<span class="p_del">-					nonblocking);</span>
<span class="p_add">+					   nonblocking, mmrange);</span>
 			switch (ret) {
 			case 0:
 				goto retry;
<span class="p_chunk">@@ -796,7 +799,7 @@</span> <span class="p_context"> static bool vma_permits_fault(struct vm_area_struct *vma,</span>
  */
 int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,
 		     unsigned long address, unsigned int fault_flags,
<span class="p_del">-		     bool *unlocked)</span>
<span class="p_add">+		     bool *unlocked, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma;
 	int ret, major = 0;
<span class="p_chunk">@@ -805,14 +808,14 @@</span> <span class="p_context"> int fixup_user_fault(struct task_struct *tsk, struct mm_struct *mm,</span>
 		fault_flags |= FAULT_FLAG_ALLOW_RETRY;
 
 retry:
<span class="p_del">-	vma = find_extend_vma(mm, address);</span>
<span class="p_add">+	vma = find_extend_vma(mm, address, mmrange);</span>
 	if (!vma || address &lt; vma-&gt;vm_start)
 		return -EFAULT;
 
 	if (!vma_permits_fault(vma, fault_flags))
 		return -EFAULT;
 
<span class="p_del">-	ret = handle_mm_fault(vma, address, fault_flags);</span>
<span class="p_add">+	ret = handle_mm_fault(vma, address, fault_flags, mmrange);</span>
 	major |= ret &amp; VM_FAULT_MAJOR;
 	if (ret &amp; VM_FAULT_ERROR) {
 		int err = vm_fault_to_errno(ret, 0);
<span class="p_chunk">@@ -849,7 +852,8 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 						struct page **pages,
 						struct vm_area_struct **vmas,
 						int *locked,
<span class="p_del">-						unsigned int flags)</span>
<span class="p_add">+						unsigned int flags,</span>
<span class="p_add">+						struct range_lock *mmrange)</span>
 {
 	long ret, pages_done;
 	bool lock_dropped;
<span class="p_chunk">@@ -868,7 +872,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 	lock_dropped = false;
 	for (;;) {
 		ret = __get_user_pages(tsk, mm, start, nr_pages, flags, pages,
<span class="p_del">-				       vmas, locked);</span>
<span class="p_add">+				       vmas, locked, mmrange);</span>
 		if (!locked)
 			/* VM_FAULT_RETRY couldn&#39;t trigger, bypass */
 			return ret;
<span class="p_chunk">@@ -908,7 +912,7 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
 		lock_dropped = true;
 		down_read(&amp;mm-&gt;mmap_sem);
 		ret = __get_user_pages(tsk, mm, start, 1, flags | FOLL_TRIED,
<span class="p_del">-				       pages, NULL, NULL);</span>
<span class="p_add">+				       pages, NULL, NULL, mmrange);</span>
 		if (ret != 1) {
 			BUG_ON(ret &gt; 1);
 			if (!pages_done)
<span class="p_chunk">@@ -956,11 +960,11 @@</span> <span class="p_context"> static __always_inline long __get_user_pages_locked(struct task_struct *tsk,</span>
  */
 long get_user_pages_locked(unsigned long start, unsigned long nr_pages,
 			   unsigned int gup_flags, struct page **pages,
<span class="p_del">-			   int *locked)</span>
<span class="p_add">+			   int *locked, struct range_lock *mmrange)</span>
 {
 	return __get_user_pages_locked(current, current-&gt;mm, start, nr_pages,
 				       pages, NULL, locked,
<span class="p_del">-				       gup_flags | FOLL_TOUCH);</span>
<span class="p_add">+				       gup_flags | FOLL_TOUCH, mmrange);</span>
 }
 EXPORT_SYMBOL(get_user_pages_locked);
 
<span class="p_chunk">@@ -985,10 +989,11 @@</span> <span class="p_context"> long get_user_pages_unlocked(unsigned long start, unsigned long nr_pages,</span>
 	struct mm_struct *mm = current-&gt;mm;
 	int locked = 1;
 	long ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_read(&amp;mm-&gt;mmap_sem);
 	ret = __get_user_pages_locked(current, mm, start, nr_pages, pages, NULL,
<span class="p_del">-				      &amp;locked, gup_flags | FOLL_TOUCH);</span>
<span class="p_add">+				      &amp;locked, gup_flags | FOLL_TOUCH, &amp;mmrange);</span>
 	if (locked)
 		up_read(&amp;mm-&gt;mmap_sem);
 	return ret;
<span class="p_chunk">@@ -1054,11 +1059,13 @@</span> <span class="p_context"> EXPORT_SYMBOL(get_user_pages_unlocked);</span>
 long get_user_pages_remote(struct task_struct *tsk, struct mm_struct *mm,
 		unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
<span class="p_del">-		struct vm_area_struct **vmas, int *locked)</span>
<span class="p_add">+	        struct vm_area_struct **vmas, int *locked,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	return __get_user_pages_locked(tsk, mm, start, nr_pages, pages, vmas,
 				       locked,
<span class="p_del">-				       gup_flags | FOLL_TOUCH | FOLL_REMOTE);</span>
<span class="p_add">+				       gup_flags | FOLL_TOUCH | FOLL_REMOTE,</span>
<span class="p_add">+				       mmrange);</span>
 }
 EXPORT_SYMBOL(get_user_pages_remote);
 
<span class="p_chunk">@@ -1071,11 +1078,11 @@</span> <span class="p_context"> EXPORT_SYMBOL(get_user_pages_remote);</span>
  */
 long get_user_pages(unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
<span class="p_del">-		struct vm_area_struct **vmas)</span>
<span class="p_add">+		struct vm_area_struct **vmas, struct range_lock *mmrange)</span>
 {
 	return __get_user_pages_locked(current, current-&gt;mm, start, nr_pages,
 				       pages, vmas, NULL,
<span class="p_del">-				       gup_flags | FOLL_TOUCH);</span>
<span class="p_add">+				       gup_flags | FOLL_TOUCH, mmrange);</span>
 }
 EXPORT_SYMBOL(get_user_pages);
 
<span class="p_chunk">@@ -1094,7 +1101,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(get_user_pages);</span>
  */
 long get_user_pages_longterm(unsigned long start, unsigned long nr_pages,
 		unsigned int gup_flags, struct page **pages,
<span class="p_del">-		struct vm_area_struct **vmas_arg)</span>
<span class="p_add">+	        struct vm_area_struct **vmas_arg,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct **vmas = vmas_arg;
 	struct vm_area_struct *vma_prev = NULL;
<span class="p_chunk">@@ -1110,7 +1118,7 @@</span> <span class="p_context"> long get_user_pages_longterm(unsigned long start, unsigned long nr_pages,</span>
 			return -ENOMEM;
 	}
 
<span class="p_del">-	rc = get_user_pages(start, nr_pages, gup_flags, pages, vmas);</span>
<span class="p_add">+	rc = get_user_pages(start, nr_pages, gup_flags, pages, vmas, mmrange);</span>
 
 	for (i = 0; i &lt; rc; i++) {
 		struct vm_area_struct *vma = vmas[i];
<span class="p_chunk">@@ -1149,6 +1157,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(get_user_pages_longterm);</span>
  * @start: start address
  * @end:   end address
  * @nonblocking:
<span class="p_add">+ * @mmrange: mm address space range locking</span>
  *
  * This takes care of mlocking the pages too if VM_LOCKED is set.
  *
<span class="p_chunk">@@ -1163,7 +1172,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(get_user_pages_longterm);</span>
  * released.  If it&#39;s released, *@nonblocking will be set to 0.
  */
 long populate_vma_page_range(struct vm_area_struct *vma,
<span class="p_del">-		unsigned long start, unsigned long end, int *nonblocking)</span>
<span class="p_add">+		unsigned long start, unsigned long end, int *nonblocking,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	unsigned long nr_pages = (end - start) / PAGE_SIZE;
<span class="p_chunk">@@ -1198,7 +1208,7 @@</span> <span class="p_context"> long populate_vma_page_range(struct vm_area_struct *vma,</span>
 	 * not result in a stack expansion that recurses back here.
 	 */
 	return __get_user_pages(current, mm, start, nr_pages, gup_flags,
<span class="p_del">-				NULL, NULL, nonblocking);</span>
<span class="p_add">+				NULL, NULL, nonblocking, mmrange);</span>
 }
 
 /*
<span class="p_chunk">@@ -1215,6 +1225,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 	struct vm_area_struct *vma = NULL;
 	int locked = 0;
 	long ret = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	VM_BUG_ON(start &amp; ~PAGE_MASK);
 	VM_BUG_ON(len != PAGE_ALIGN(len));
<span class="p_chunk">@@ -1247,7 +1258,7 @@</span> <span class="p_context"> int __mm_populate(unsigned long start, unsigned long len, int ignore_errors)</span>
 		 * double checks the vma flags, so that it won&#39;t mlock pages
 		 * if the vma was already munlocked.
 		 */
<span class="p_del">-		ret = populate_vma_page_range(vma, nstart, nend, &amp;locked);</span>
<span class="p_add">+		ret = populate_vma_page_range(vma, nstart, nend, &amp;locked, &amp;mmrange);</span>
 		if (ret &lt; 0) {
 			if (ignore_errors) {
 				ret = 0;
<span class="p_chunk">@@ -1282,10 +1293,11 @@</span> <span class="p_context"> struct page *get_dump_page(unsigned long addr)</span>
 {
 	struct vm_area_struct *vma;
 	struct page *page;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (__get_user_pages(current, current-&gt;mm, addr, 1,
 			     FOLL_FORCE | FOLL_DUMP | FOLL_GET, &amp;page, &amp;vma,
<span class="p_del">-			     NULL) &lt; 1)</span>
<span class="p_add">+			     NULL, &amp;mmrange) &lt; 1)</span>
 		return NULL;
 	flush_cache_page(vma, addr, page_to_pfn(page));
 	return page;
<span class="p_header">diff --git a/mm/hmm.c b/mm/hmm.c</span>
<span class="p_header">index 320545b98ff5..b14e6869689e 100644</span>
<span class="p_header">--- a/mm/hmm.c</span>
<span class="p_header">+++ b/mm/hmm.c</span>
<span class="p_chunk">@@ -245,7 +245,8 @@</span> <span class="p_context"> struct hmm_vma_walk {</span>
 
 static int hmm_vma_do_fault(struct mm_walk *walk,
 			    unsigned long addr,
<span class="p_del">-			    hmm_pfn_t *pfn)</span>
<span class="p_add">+			    hmm_pfn_t *pfn,</span>
<span class="p_add">+			    struct range_lock *mmrange)</span>
 {
 	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_REMOTE;
 	struct hmm_vma_walk *hmm_vma_walk = walk-&gt;private;
<span class="p_chunk">@@ -254,7 +255,7 @@</span> <span class="p_context"> static int hmm_vma_do_fault(struct mm_walk *walk,</span>
 
 	flags |= hmm_vma_walk-&gt;block ? 0 : FAULT_FLAG_ALLOW_RETRY;
 	flags |= hmm_vma_walk-&gt;write ? FAULT_FLAG_WRITE : 0;
<span class="p_del">-	r = handle_mm_fault(vma, addr, flags);</span>
<span class="p_add">+	r = handle_mm_fault(vma, addr, flags, mmrange);</span>
 	if (r &amp; VM_FAULT_RETRY)
 		return -EBUSY;
 	if (r &amp; VM_FAULT_ERROR) {
<span class="p_chunk">@@ -298,7 +299,9 @@</span> <span class="p_context"> static void hmm_pfns_clear(hmm_pfn_t *pfns,</span>
 
 static int hmm_vma_walk_hole(unsigned long addr,
 			     unsigned long end,
<span class="p_del">-			     struct mm_walk *walk)</span>
<span class="p_add">+			     struct mm_walk *walk,</span>
<span class="p_add">+			     struct range_lock *mmrange)</span>
<span class="p_add">+</span>
 {
 	struct hmm_vma_walk *hmm_vma_walk = walk-&gt;private;
 	struct hmm_range *range = hmm_vma_walk-&gt;range;
<span class="p_chunk">@@ -312,7 +315,7 @@</span> <span class="p_context"> static int hmm_vma_walk_hole(unsigned long addr,</span>
 		if (hmm_vma_walk-&gt;fault) {
 			int ret;
 
<span class="p_del">-			ret = hmm_vma_do_fault(walk, addr, &amp;pfns[i]);</span>
<span class="p_add">+			ret = hmm_vma_do_fault(walk, addr, &amp;pfns[i], mmrange);</span>
 			if (ret != -EAGAIN)
 				return ret;
 		}
<span class="p_chunk">@@ -323,7 +326,8 @@</span> <span class="p_context"> static int hmm_vma_walk_hole(unsigned long addr,</span>
 
 static int hmm_vma_walk_clear(unsigned long addr,
 			      unsigned long end,
<span class="p_del">-			      struct mm_walk *walk)</span>
<span class="p_add">+			      struct mm_walk *walk,</span>
<span class="p_add">+			      struct range_lock *mmrange)</span>
 {
 	struct hmm_vma_walk *hmm_vma_walk = walk-&gt;private;
 	struct hmm_range *range = hmm_vma_walk-&gt;range;
<span class="p_chunk">@@ -337,7 +341,7 @@</span> <span class="p_context"> static int hmm_vma_walk_clear(unsigned long addr,</span>
 		if (hmm_vma_walk-&gt;fault) {
 			int ret;
 
<span class="p_del">-			ret = hmm_vma_do_fault(walk, addr, &amp;pfns[i]);</span>
<span class="p_add">+			ret = hmm_vma_do_fault(walk, addr, &amp;pfns[i], mmrange);</span>
 			if (ret != -EAGAIN)
 				return ret;
 		}
<span class="p_chunk">@@ -349,7 +353,8 @@</span> <span class="p_context"> static int hmm_vma_walk_clear(unsigned long addr,</span>
 static int hmm_vma_walk_pmd(pmd_t *pmdp,
 			    unsigned long start,
 			    unsigned long end,
<span class="p_del">-			    struct mm_walk *walk)</span>
<span class="p_add">+			    struct mm_walk *walk,</span>
<span class="p_add">+			    struct range_lock *mmrange)</span>
 {
 	struct hmm_vma_walk *hmm_vma_walk = walk-&gt;private;
 	struct hmm_range *range = hmm_vma_walk-&gt;range;
<span class="p_chunk">@@ -366,7 +371,7 @@</span> <span class="p_context"> static int hmm_vma_walk_pmd(pmd_t *pmdp,</span>
 
 again:
 	if (pmd_none(*pmdp))
<span class="p_del">-		return hmm_vma_walk_hole(start, end, walk);</span>
<span class="p_add">+		return hmm_vma_walk_hole(start, end, walk, mmrange);</span>
 
 	if (pmd_huge(*pmdp) &amp;&amp; vma-&gt;vm_flags &amp; VM_HUGETLB)
 		return hmm_pfns_bad(start, end, walk);
<span class="p_chunk">@@ -389,10 +394,10 @@</span> <span class="p_context"> static int hmm_vma_walk_pmd(pmd_t *pmdp,</span>
 		if (!pmd_devmap(pmd) &amp;&amp; !pmd_trans_huge(pmd))
 			goto again;
 		if (pmd_protnone(pmd))
<span class="p_del">-			return hmm_vma_walk_clear(start, end, walk);</span>
<span class="p_add">+			return hmm_vma_walk_clear(start, end, walk, mmrange);</span>
 
 		if (write_fault &amp;&amp; !pmd_write(pmd))
<span class="p_del">-			return hmm_vma_walk_clear(start, end, walk);</span>
<span class="p_add">+			return hmm_vma_walk_clear(start, end, walk, mmrange);</span>
 
 		pfn = pmd_pfn(pmd) + pte_index(addr);
 		flag |= pmd_write(pmd) ? HMM_PFN_WRITE : 0;
<span class="p_chunk">@@ -464,7 +469,7 @@</span> <span class="p_context"> static int hmm_vma_walk_pmd(pmd_t *pmdp,</span>
 fault:
 		pte_unmap(ptep);
 		/* Fault all pages in range */
<span class="p_del">-		return hmm_vma_walk_clear(start, end, walk);</span>
<span class="p_add">+		return hmm_vma_walk_clear(start, end, walk, mmrange);</span>
 	}
 	pte_unmap(ptep - 1);
 
<span class="p_chunk">@@ -495,7 +500,8 @@</span> <span class="p_context"> int hmm_vma_get_pfns(struct vm_area_struct *vma,</span>
 		     struct hmm_range *range,
 		     unsigned long start,
 		     unsigned long end,
<span class="p_del">-		     hmm_pfn_t *pfns)</span>
<span class="p_add">+		     hmm_pfn_t *pfns,</span>
<span class="p_add">+		     struct range_lock *mmrange)</span>
 {
 	struct hmm_vma_walk hmm_vma_walk;
 	struct mm_walk mm_walk;
<span class="p_chunk">@@ -541,7 +547,7 @@</span> <span class="p_context"> int hmm_vma_get_pfns(struct vm_area_struct *vma,</span>
 	mm_walk.pmd_entry = hmm_vma_walk_pmd;
 	mm_walk.pte_hole = hmm_vma_walk_hole;
 
<span class="p_del">-	walk_page_range(start, end, &amp;mm_walk);</span>
<span class="p_add">+	walk_page_range(start, end, &amp;mm_walk, mmrange);</span>
 	return 0;
 }
 EXPORT_SYMBOL(hmm_vma_get_pfns);
<span class="p_chunk">@@ -664,7 +670,8 @@</span> <span class="p_context"> int hmm_vma_fault(struct vm_area_struct *vma,</span>
 		  unsigned long end,
 		  hmm_pfn_t *pfns,
 		  bool write,
<span class="p_del">-		  bool block)</span>
<span class="p_add">+		  bool block,</span>
<span class="p_add">+		  struct range_lock *mmrange)</span>
 {
 	struct hmm_vma_walk hmm_vma_walk;
 	struct mm_walk mm_walk;
<span class="p_chunk">@@ -717,7 +724,7 @@</span> <span class="p_context"> int hmm_vma_fault(struct vm_area_struct *vma,</span>
 	mm_walk.pte_hole = hmm_vma_walk_hole;
 
 	do {
<span class="p_del">-		ret = walk_page_range(start, end, &amp;mm_walk);</span>
<span class="p_add">+		ret = walk_page_range(start, end, &amp;mm_walk, mmrange);</span>
 		start = hmm_vma_walk.last;
 	} while (ret == -EAGAIN);
 
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 62d8c34e63d5..abf1de31e524 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -289,7 +289,8 @@</span> <span class="p_context"> void __vma_link_list(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 
 #ifdef CONFIG_MMU
 extern long populate_vma_page_range(struct vm_area_struct *vma,
<span class="p_del">-		unsigned long start, unsigned long end, int *nonblocking);</span>
<span class="p_add">+			unsigned long start, unsigned long end, int *nonblocking,</span>
<span class="p_add">+			struct range_lock *mmrange);</span>
 extern void munlock_vma_pages_range(struct vm_area_struct *vma,
 			unsigned long start, unsigned long end);
 static inline void munlock_vma_pages_all(struct vm_area_struct *vma)
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index 293721f5da70..66c350cd9799 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -448,7 +448,8 @@</span> <span class="p_context"> static inline bool ksm_test_exit(struct mm_struct *mm)</span>
  * of the process that owns &#39;vma&#39;.  We also do not want to enforce
  * protection keys here anyway.
  */
<span class="p_del">-static int break_ksm(struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_add">+static int break_ksm(struct vm_area_struct *vma, unsigned long addr,</span>
<span class="p_add">+		     struct range_lock *mmrange)</span>
 {
 	struct page *page;
 	int ret = 0;
<span class="p_chunk">@@ -461,7 +462,8 @@</span> <span class="p_context"> static int break_ksm(struct vm_area_struct *vma, unsigned long addr)</span>
 			break;
 		if (PageKsm(page))
 			ret = handle_mm_fault(vma, addr,
<span class="p_del">-					FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE);</span>
<span class="p_add">+					FAULT_FLAG_WRITE | FAULT_FLAG_REMOTE,</span>
<span class="p_add">+					mmrange);</span>
 		else
 			ret = VM_FAULT_WRITE;
 		put_page(page);
<span class="p_chunk">@@ -516,6 +518,7 @@</span> <span class="p_context"> static void break_cow(struct rmap_item *rmap_item)</span>
 	struct mm_struct *mm = rmap_item-&gt;mm;
 	unsigned long addr = rmap_item-&gt;address;
 	struct vm_area_struct *vma;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/*
 	 * It is not an accident that whenever we want to break COW
<span class="p_chunk">@@ -526,7 +529,7 @@</span> <span class="p_context"> static void break_cow(struct rmap_item *rmap_item)</span>
 	down_read(&amp;mm-&gt;mmap_sem);
 	vma = find_mergeable_vma(mm, addr);
 	if (vma)
<span class="p_del">-		break_ksm(vma, addr);</span>
<span class="p_add">+		break_ksm(vma, addr, &amp;mmrange);</span>
 	up_read(&amp;mm-&gt;mmap_sem);
 }
 
<span class="p_chunk">@@ -807,7 +810,8 @@</span> <span class="p_context"> static void remove_trailing_rmap_items(struct mm_slot *mm_slot,</span>
  * in cmp_and_merge_page on one of the rmap_items we would be removing.
  */
 static int unmerge_ksm_pages(struct vm_area_struct *vma,
<span class="p_del">-			     unsigned long start, unsigned long end)</span>
<span class="p_add">+			     unsigned long start, unsigned long end,</span>
<span class="p_add">+			     struct range_lock *mmrange)</span>
 {
 	unsigned long addr;
 	int err = 0;
<span class="p_chunk">@@ -818,7 +822,7 @@</span> <span class="p_context"> static int unmerge_ksm_pages(struct vm_area_struct *vma,</span>
 		if (signal_pending(current))
 			err = -ERESTARTSYS;
 		else
<span class="p_del">-			err = break_ksm(vma, addr);</span>
<span class="p_add">+			err = break_ksm(vma, addr, mmrange);</span>
 	}
 	return err;
 }
<span class="p_chunk">@@ -922,6 +926,7 @@</span> <span class="p_context"> static int unmerge_and_remove_all_rmap_items(void)</span>
 	struct mm_struct *mm;
 	struct vm_area_struct *vma;
 	int err = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	spin_lock(&amp;ksm_mmlist_lock);
 	ksm_scan.mm_slot = list_entry(ksm_mm_head.mm_list.next,
<span class="p_chunk">@@ -937,8 +942,8 @@</span> <span class="p_context"> static int unmerge_and_remove_all_rmap_items(void)</span>
 				break;
 			if (!(vma-&gt;vm_flags &amp; VM_MERGEABLE) || !vma-&gt;anon_vma)
 				continue;
<span class="p_del">-			err = unmerge_ksm_pages(vma,</span>
<span class="p_del">-						vma-&gt;vm_start, vma-&gt;vm_end);</span>
<span class="p_add">+			err = unmerge_ksm_pages(vma, vma-&gt;vm_start,</span>
<span class="p_add">+						vma-&gt;vm_end, &amp;mmrange);</span>
 			if (err)
 				goto error;
 		}
<span class="p_chunk">@@ -2350,7 +2355,8 @@</span> <span class="p_context"> static int ksm_scan_thread(void *nothing)</span>
 }
 
 int ksm_madvise(struct vm_area_struct *vma, unsigned long start,
<span class="p_del">-		unsigned long end, int advice, unsigned long *vm_flags)</span>
<span class="p_add">+		unsigned long end, int advice, unsigned long *vm_flags,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	int err;
<span class="p_chunk">@@ -2384,7 +2390,7 @@</span> <span class="p_context"> int ksm_madvise(struct vm_area_struct *vma, unsigned long start,</span>
 			return 0;		/* just ignore the advice */
 
 		if (vma-&gt;anon_vma) {
<span class="p_del">-			err = unmerge_ksm_pages(vma, start, end);</span>
<span class="p_add">+			err = unmerge_ksm_pages(vma, start, end, mmrange);</span>
 			if (err)
 				return err;
 		}
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 4d3c922ea1a1..eaec6bfc2b08 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -54,7 +54,8 @@</span> <span class="p_context"> static int madvise_need_mmap_write(int behavior)</span>
  */
 static long madvise_behavior(struct vm_area_struct *vma,
 		     struct vm_area_struct **prev,
<span class="p_del">-		     unsigned long start, unsigned long end, int behavior)</span>
<span class="p_add">+		     unsigned long start, unsigned long end, int behavior,</span>
<span class="p_add">+		     struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	int error = 0;
<span class="p_chunk">@@ -104,7 +105,8 @@</span> <span class="p_context"> static long madvise_behavior(struct vm_area_struct *vma,</span>
 		break;
 	case MADV_MERGEABLE:
 	case MADV_UNMERGEABLE:
<span class="p_del">-		error = ksm_madvise(vma, start, end, behavior, &amp;new_flags);</span>
<span class="p_add">+		error = ksm_madvise(vma, start, end, behavior,</span>
<span class="p_add">+				    &amp;new_flags, mmrange);</span>
 		if (error) {
 			/*
 			 * madvise() returns EAGAIN if kernel resources, such as
<span class="p_chunk">@@ -138,7 +140,7 @@</span> <span class="p_context"> static long madvise_behavior(struct vm_area_struct *vma,</span>
 	pgoff = vma-&gt;vm_pgoff + ((start - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);
 	*prev = vma_merge(mm, *prev, start, end, new_flags, vma-&gt;anon_vma,
 			  vma-&gt;vm_file, pgoff, vma_policy(vma),
<span class="p_del">-			  vma-&gt;vm_userfaultfd_ctx);</span>
<span class="p_add">+			  vma-&gt;vm_userfaultfd_ctx, mmrange);</span>
 	if (*prev) {
 		vma = *prev;
 		goto success;
<span class="p_chunk">@@ -151,7 +153,7 @@</span> <span class="p_context"> static long madvise_behavior(struct vm_area_struct *vma,</span>
 			error = -ENOMEM;
 			goto out;
 		}
<span class="p_del">-		error = __split_vma(mm, vma, start, 1);</span>
<span class="p_add">+		error = __split_vma(mm, vma, start, 1, mmrange);</span>
 		if (error) {
 			/*
 			 * madvise() returns EAGAIN if kernel resources, such as
<span class="p_chunk">@@ -168,7 +170,7 @@</span> <span class="p_context"> static long madvise_behavior(struct vm_area_struct *vma,</span>
 			error = -ENOMEM;
 			goto out;
 		}
<span class="p_del">-		error = __split_vma(mm, vma, end, 0);</span>
<span class="p_add">+		error = __split_vma(mm, vma, end, 0, mmrange);</span>
 		if (error) {
 			/*
 			 * madvise() returns EAGAIN if kernel resources, such as
<span class="p_chunk">@@ -191,7 +193,8 @@</span> <span class="p_context"> static long madvise_behavior(struct vm_area_struct *vma,</span>
 
 #ifdef CONFIG_SWAP
 static int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,
<span class="p_del">-	unsigned long end, struct mm_walk *walk)</span>
<span class="p_add">+				 unsigned long end, struct mm_walk *walk,</span>
<span class="p_add">+				 struct range_lock *mmrange)</span>
 {
 	pte_t *orig_pte;
 	struct vm_area_struct *vma = walk-&gt;private;
<span class="p_chunk">@@ -226,7 +229,8 @@</span> <span class="p_context"> static int swapin_walk_pmd_entry(pmd_t *pmd, unsigned long start,</span>
 }
 
 static void force_swapin_readahead(struct vm_area_struct *vma,
<span class="p_del">-		unsigned long start, unsigned long end)</span>
<span class="p_add">+				   unsigned long start, unsigned long end,</span>
<span class="p_add">+				   struct range_lock *mmrange)</span>
 {
 	struct mm_walk walk = {
 		.mm = vma-&gt;vm_mm,
<span class="p_chunk">@@ -234,7 +238,7 @@</span> <span class="p_context"> static void force_swapin_readahead(struct vm_area_struct *vma,</span>
 		.private = vma,
 	};
 
<span class="p_del">-	walk_page_range(start, end, &amp;walk);</span>
<span class="p_add">+	walk_page_range(start, end, &amp;walk, mmrange);</span>
 
 	lru_add_drain();	/* Push any new pages onto the LRU now */
 }
<span class="p_chunk">@@ -272,14 +276,15 @@</span> <span class="p_context"> static void force_shm_swapin_readahead(struct vm_area_struct *vma,</span>
  */
 static long madvise_willneed(struct vm_area_struct *vma,
 			     struct vm_area_struct **prev,
<span class="p_del">-			     unsigned long start, unsigned long end)</span>
<span class="p_add">+			     unsigned long start, unsigned long end,</span>
<span class="p_add">+			     struct range_lock *mmrange)</span>
 {
 	struct file *file = vma-&gt;vm_file;
 
 	*prev = vma;
 #ifdef CONFIG_SWAP
 	if (!file) {
<span class="p_del">-		force_swapin_readahead(vma, start, end);</span>
<span class="p_add">+		force_swapin_readahead(vma, start, end, mmrange);</span>
 		return 0;
 	}
 
<span class="p_chunk">@@ -308,7 +313,8 @@</span> <span class="p_context"> static long madvise_willneed(struct vm_area_struct *vma,</span>
 }
 
 static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,
<span class="p_del">-				unsigned long end, struct mm_walk *walk)</span>
<span class="p_add">+				  unsigned long end, struct mm_walk *walk,</span>
<span class="p_add">+				  struct range_lock *mmrange)</span>
 
 {
 	struct mmu_gather *tlb = walk-&gt;private;
<span class="p_chunk">@@ -442,7 +448,8 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 
 static void madvise_free_page_range(struct mmu_gather *tlb,
 			     struct vm_area_struct *vma,
<span class="p_del">-			     unsigned long addr, unsigned long end)</span>
<span class="p_add">+			     unsigned long addr, unsigned long end,</span>
<span class="p_add">+			     struct range_lock *mmrange)</span>
 {
 	struct mm_walk free_walk = {
 		.pmd_entry = madvise_free_pte_range,
<span class="p_chunk">@@ -451,12 +458,14 @@</span> <span class="p_context"> static void madvise_free_page_range(struct mmu_gather *tlb,</span>
 	};
 
 	tlb_start_vma(tlb, vma);
<span class="p_del">-	walk_page_range(addr, end, &amp;free_walk);</span>
<span class="p_add">+	walk_page_range(addr, end, &amp;free_walk, mmrange);</span>
 	tlb_end_vma(tlb, vma);
 }
 
 static int madvise_free_single_vma(struct vm_area_struct *vma,
<span class="p_del">-			unsigned long start_addr, unsigned long end_addr)</span>
<span class="p_add">+				   unsigned long start_addr,</span>
<span class="p_add">+				   unsigned long end_addr,</span>
<span class="p_add">+				   struct range_lock *mmrange)</span>
 {
 	unsigned long start, end;
 	struct mm_struct *mm = vma-&gt;vm_mm;
<span class="p_chunk">@@ -478,7 +487,7 @@</span> <span class="p_context"> static int madvise_free_single_vma(struct vm_area_struct *vma,</span>
 	update_hiwater_rss(mm);
 
 	mmu_notifier_invalidate_range_start(mm, start, end);
<span class="p_del">-	madvise_free_page_range(&amp;tlb, vma, start, end);</span>
<span class="p_add">+	madvise_free_page_range(&amp;tlb, vma, start, end, mmrange);</span>
 	mmu_notifier_invalidate_range_end(mm, start, end);
 	tlb_finish_mmu(&amp;tlb, start, end);
 
<span class="p_chunk">@@ -514,7 +523,7 @@</span> <span class="p_context"> static long madvise_dontneed_single_vma(struct vm_area_struct *vma,</span>
 static long madvise_dontneed_free(struct vm_area_struct *vma,
 				  struct vm_area_struct **prev,
 				  unsigned long start, unsigned long end,
<span class="p_del">-				  int behavior)</span>
<span class="p_add">+				  int behavior, struct range_lock *mmrange)</span>
 {
 	*prev = vma;
 	if (!can_madv_dontneed_vma(vma))
<span class="p_chunk">@@ -562,7 +571,7 @@</span> <span class="p_context"> static long madvise_dontneed_free(struct vm_area_struct *vma,</span>
 	if (behavior == MADV_DONTNEED)
 		return madvise_dontneed_single_vma(vma, start, end);
 	else if (behavior == MADV_FREE)
<span class="p_del">-		return madvise_free_single_vma(vma, start, end);</span>
<span class="p_add">+		return madvise_free_single_vma(vma, start, end, mmrange);</span>
 	else
 		return -EINVAL;
 }
<span class="p_chunk">@@ -676,18 +685,21 @@</span> <span class="p_context"> static int madvise_inject_error(int behavior,</span>
 
 static long
 madvise_vma(struct vm_area_struct *vma, struct vm_area_struct **prev,
<span class="p_del">-		unsigned long start, unsigned long end, int behavior)</span>
<span class="p_add">+	    unsigned long start, unsigned long end, int behavior,</span>
<span class="p_add">+	    struct range_lock *mmrange)</span>
 {
 	switch (behavior) {
 	case MADV_REMOVE:
 		return madvise_remove(vma, prev, start, end);
 	case MADV_WILLNEED:
<span class="p_del">-		return madvise_willneed(vma, prev, start, end);</span>
<span class="p_add">+		return madvise_willneed(vma, prev, start, end, mmrange);</span>
 	case MADV_FREE:
 	case MADV_DONTNEED:
<span class="p_del">-		return madvise_dontneed_free(vma, prev, start, end, behavior);</span>
<span class="p_add">+		return madvise_dontneed_free(vma, prev, start, end, behavior,</span>
<span class="p_add">+					     mmrange);</span>
 	default:
<span class="p_del">-		return madvise_behavior(vma, prev, start, end, behavior);</span>
<span class="p_add">+		return madvise_behavior(vma, prev, start, end, behavior,</span>
<span class="p_add">+					mmrange);</span>
 	}
 }
 
<span class="p_chunk">@@ -797,7 +809,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
 	int write;
 	size_t len;
 	struct blk_plug plug;
<span class="p_del">-</span>
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 	if (!madvise_behavior_valid(behavior))
 		return error;
 
<span class="p_chunk">@@ -860,7 +872,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(madvise, unsigned long, start, size_t, len_in, int, behavior)</span>
 			tmp = end;
 
 		/* Here vma-&gt;vm_start &lt;= start &lt; tmp &lt;= (end|vma-&gt;vm_end). */
<span class="p_del">-		error = madvise_vma(vma, &amp;prev, start, tmp, behavior);</span>
<span class="p_add">+		error = madvise_vma(vma, &amp;prev, start, tmp, behavior, &amp;mmrange);</span>
 		if (error)
 			goto out;
 		start = tmp;
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 88c1af32fd67..a7ac5a14b22e 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -4881,7 +4881,8 @@</span> <span class="p_context"> static inline enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
 
 static int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,
 					unsigned long addr, unsigned long end,
<span class="p_del">-					struct mm_walk *walk)</span>
<span class="p_add">+					struct mm_walk *walk,</span>
<span class="p_add">+					struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma = walk-&gt;vma;
 	pte_t *pte;
<span class="p_chunk">@@ -4915,6 +4916,7 @@</span> <span class="p_context"> static int mem_cgroup_count_precharge_pte_range(pmd_t *pmd,</span>
 static unsigned long mem_cgroup_count_precharge(struct mm_struct *mm)
 {
 	unsigned long precharge;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	struct mm_walk mem_cgroup_count_precharge_walk = {
 		.pmd_entry = mem_cgroup_count_precharge_pte_range,
<span class="p_chunk">@@ -4922,7 +4924,7 @@</span> <span class="p_context"> static unsigned long mem_cgroup_count_precharge(struct mm_struct *mm)</span>
 	};
 	down_read(&amp;mm-&gt;mmap_sem);
 	walk_page_range(0, mm-&gt;highest_vm_end,
<span class="p_del">-			&amp;mem_cgroup_count_precharge_walk);</span>
<span class="p_add">+			&amp;mem_cgroup_count_precharge_walk, &amp;mmrange);</span>
 	up_read(&amp;mm-&gt;mmap_sem);
 
 	precharge = mc.precharge;
<span class="p_chunk">@@ -5081,7 +5083,8 @@</span> <span class="p_context"> static void mem_cgroup_cancel_attach(struct cgroup_taskset *tset)</span>
 
 static int mem_cgroup_move_charge_pte_range(pmd_t *pmd,
 				unsigned long addr, unsigned long end,
<span class="p_del">-				struct mm_walk *walk)</span>
<span class="p_add">+				struct mm_walk *walk,</span>
<span class="p_add">+				struct range_lock *mmrange)</span>
 {
 	int ret = 0;
 	struct vm_area_struct *vma = walk-&gt;vma;
<span class="p_chunk">@@ -5197,6 +5200,7 @@</span> <span class="p_context"> static void mem_cgroup_move_charge(void)</span>
 		.pmd_entry = mem_cgroup_move_charge_pte_range,
 		.mm = mc.mm,
 	};
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	lru_add_drain_all();
 	/*
<span class="p_chunk">@@ -5223,7 +5227,8 @@</span> <span class="p_context"> static void mem_cgroup_move_charge(void)</span>
 	 * When we have consumed all precharges and failed in doing
 	 * additional charge, the page walk just aborts.
 	 */
<span class="p_del">-	walk_page_range(0, mc.mm-&gt;highest_vm_end, &amp;mem_cgroup_move_charge_walk);</span>
<span class="p_add">+	walk_page_range(0, mc.mm-&gt;highest_vm_end, &amp;mem_cgroup_move_charge_walk,</span>
<span class="p_add">+			&amp;mmrange);</span>
 
 	up_read(&amp;mc.mm-&gt;mmap_sem);
 	atomic_dec(&amp;mc.from-&gt;moving_account);
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 5ec6433d6a5c..b3561a052939 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -4021,7 +4021,7 @@</span> <span class="p_context"> static int handle_pte_fault(struct vm_fault *vmf)</span>
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
 static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
<span class="p_del">-		unsigned int flags)</span>
<span class="p_add">+			     unsigned int flags, struct range_lock *mmrange)</span>
 {
 	struct vm_fault vmf = {
 		.vma = vma,
<span class="p_chunk">@@ -4029,6 +4029,7 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 		.flags = flags,
 		.pgoff = linear_page_index(vma, address),
 		.gfp_mask = __get_fault_gfp_mask(vma),
<span class="p_add">+		.lockrange = mmrange,</span>
 	};
 	unsigned int dirty = flags &amp; FAULT_FLAG_WRITE;
 	struct mm_struct *mm = vma-&gt;vm_mm;
<span class="p_chunk">@@ -4110,7 +4111,7 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
 int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,
<span class="p_del">-		unsigned int flags)</span>
<span class="p_add">+		    unsigned int flags, struct range_lock *mmrange)</span>
 {
 	int ret;
 
<span class="p_chunk">@@ -4137,7 +4138,7 @@</span> <span class="p_context"> int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	if (unlikely(is_vm_hugetlb_page(vma)))
 		ret = hugetlb_fault(vma-&gt;vm_mm, vma, address, flags);
 	else
<span class="p_del">-		ret = __handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+		ret = __handle_mm_fault(vma, address, flags, mmrange);</span>
 
 	if (flags &amp; FAULT_FLAG_USER) {
 		mem_cgroup_oom_disable();
<span class="p_chunk">@@ -4425,6 +4426,7 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 	struct vm_area_struct *vma;
 	void *old_buf = buf;
 	int write = gup_flags &amp; FOLL_WRITE;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_read(&amp;mm-&gt;mmap_sem);
 	/* ignore errors, just check how much was successfully transferred */
<span class="p_chunk">@@ -4434,7 +4436,7 @@</span> <span class="p_context"> int __access_remote_vm(struct task_struct *tsk, struct mm_struct *mm,</span>
 		struct page *page = NULL;
 
 		ret = get_user_pages_remote(tsk, mm, addr, 1,
<span class="p_del">-				gup_flags, &amp;page, &amp;vma, NULL);</span>
<span class="p_add">+					    gup_flags, &amp;page, &amp;vma, NULL, &amp;mmrange);</span>
 		if (ret &lt;= 0) {
 #ifndef CONFIG_HAVE_IOREMAP_PROT
 			break;
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index a8b7d59002e8..001dc176abc1 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -467,7 +467,8 @@</span> <span class="p_context"> static int queue_pages_pmd(pmd_t *pmd, spinlock_t *ptl, unsigned long addr,</span>
  * and move them to the pagelist if they do.
  */
 static int queue_pages_pte_range(pmd_t *pmd, unsigned long addr,
<span class="p_del">-			unsigned long end, struct mm_walk *walk)</span>
<span class="p_add">+				 unsigned long end, struct mm_walk *walk,</span>
<span class="p_add">+				 struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma = walk-&gt;vma;
 	struct page *page;
<span class="p_chunk">@@ -618,7 +619,7 @@</span> <span class="p_context"> static int queue_pages_test_walk(unsigned long start, unsigned long end,</span>
 static int
 queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,
 		nodemask_t *nodes, unsigned long flags,
<span class="p_del">-		struct list_head *pagelist)</span>
<span class="p_add">+		struct list_head *pagelist, struct range_lock *mmrange)</span>
 {
 	struct queue_pages qp = {
 		.pagelist = pagelist,
<span class="p_chunk">@@ -634,7 +635,7 @@</span> <span class="p_context"> queue_pages_range(struct mm_struct *mm, unsigned long start, unsigned long end,</span>
 		.private = &amp;qp,
 	};
 
<span class="p_del">-	return walk_page_range(start, end, &amp;queue_pages_walk);</span>
<span class="p_add">+	return walk_page_range(start, end, &amp;queue_pages_walk, mmrange);</span>
 }
 
 /*
<span class="p_chunk">@@ -675,7 +676,8 @@</span> <span class="p_context"> static int vma_replace_policy(struct vm_area_struct *vma,</span>
 
 /* Step 2: apply policy to a range and do splits. */
 static int mbind_range(struct mm_struct *mm, unsigned long start,
<span class="p_del">-		       unsigned long end, struct mempolicy *new_pol)</span>
<span class="p_add">+		       unsigned long end, struct mempolicy *new_pol,</span>
<span class="p_add">+		       struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *next;
 	struct vm_area_struct *prev;
<span class="p_chunk">@@ -705,7 +707,7 @@</span> <span class="p_context"> static int mbind_range(struct mm_struct *mm, unsigned long start,</span>
 			((vmstart - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);
 		prev = vma_merge(mm, prev, vmstart, vmend, vma-&gt;vm_flags,
 				 vma-&gt;anon_vma, vma-&gt;vm_file, pgoff,
<span class="p_del">-				 new_pol, vma-&gt;vm_userfaultfd_ctx);</span>
<span class="p_add">+				 new_pol, vma-&gt;vm_userfaultfd_ctx, mmrange);</span>
 		if (prev) {
 			vma = prev;
 			next = vma-&gt;vm_next;
<span class="p_chunk">@@ -715,12 +717,12 @@</span> <span class="p_context"> static int mbind_range(struct mm_struct *mm, unsigned long start,</span>
 			goto replace;
 		}
 		if (vma-&gt;vm_start != vmstart) {
<span class="p_del">-			err = split_vma(vma-&gt;vm_mm, vma, vmstart, 1);</span>
<span class="p_add">+			err = split_vma(vma-&gt;vm_mm, vma, vmstart, 1, mmrange);</span>
 			if (err)
 				goto out;
 		}
 		if (vma-&gt;vm_end != vmend) {
<span class="p_del">-			err = split_vma(vma-&gt;vm_mm, vma, vmend, 0);</span>
<span class="p_add">+			err = split_vma(vma-&gt;vm_mm, vma, vmend, 0, mmrange);</span>
 			if (err)
 				goto out;
 		}
<span class="p_chunk">@@ -797,12 +799,12 @@</span> <span class="p_context"> static void get_policy_nodemask(struct mempolicy *p, nodemask_t *nodes)</span>
 	}
 }
 
<span class="p_del">-static int lookup_node(unsigned long addr)</span>
<span class="p_add">+static int lookup_node(unsigned long addr, struct range_lock *mmrange)</span>
 {
 	struct page *p;
 	int err;
 
<span class="p_del">-	err = get_user_pages(addr &amp; PAGE_MASK, 1, 0, &amp;p, NULL);</span>
<span class="p_add">+	err = get_user_pages(addr &amp; PAGE_MASK, 1, 0, &amp;p, NULL, mmrange);</span>
 	if (err &gt;= 0) {
 		err = page_to_nid(p);
 		put_page(p);
<span class="p_chunk">@@ -818,6 +820,7 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma = NULL;
 	struct mempolicy *pol = current-&gt;mempolicy;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (flags &amp;
 		~(unsigned long)(MPOL_F_NODE|MPOL_F_ADDR|MPOL_F_MEMS_ALLOWED))
<span class="p_chunk">@@ -857,7 +860,7 @@</span> <span class="p_context"> static long do_get_mempolicy(int *policy, nodemask_t *nmask,</span>
 
 	if (flags &amp; MPOL_F_NODE) {
 		if (flags &amp; MPOL_F_ADDR) {
<span class="p_del">-			err = lookup_node(addr);</span>
<span class="p_add">+			err = lookup_node(addr, &amp;mmrange);</span>
 			if (err &lt; 0)
 				goto out;
 			*policy = err;
<span class="p_chunk">@@ -943,7 +946,7 @@</span> <span class="p_context"> struct page *alloc_new_node_page(struct page *page, unsigned long node)</span>
  * Returns error or the number of pages not migrated.
  */
 static int migrate_to_node(struct mm_struct *mm, int source, int dest,
<span class="p_del">-			   int flags)</span>
<span class="p_add">+			   int flags, struct range_lock *mmrange)</span>
 {
 	nodemask_t nmask;
 	LIST_HEAD(pagelist);
<span class="p_chunk">@@ -959,7 +962,7 @@</span> <span class="p_context"> static int migrate_to_node(struct mm_struct *mm, int source, int dest,</span>
 	 */
 	VM_BUG_ON(!(flags &amp; (MPOL_MF_MOVE | MPOL_MF_MOVE_ALL)));
 	queue_pages_range(mm, mm-&gt;mmap-&gt;vm_start, mm-&gt;task_size, &amp;nmask,
<span class="p_del">-			flags | MPOL_MF_DISCONTIG_OK, &amp;pagelist);</span>
<span class="p_add">+			  flags | MPOL_MF_DISCONTIG_OK, &amp;pagelist, mmrange);</span>
 
 	if (!list_empty(&amp;pagelist)) {
 		err = migrate_pages(&amp;pagelist, alloc_new_node_page, NULL, dest,
<span class="p_chunk">@@ -983,6 +986,7 @@</span> <span class="p_context"> int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,</span>
 	int busy = 0;
 	int err;
 	nodemask_t tmp;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	err = migrate_prep();
 	if (err)
<span class="p_chunk">@@ -1063,7 +1067,7 @@</span> <span class="p_context"> int do_migrate_pages(struct mm_struct *mm, const nodemask_t *from,</span>
 			break;
 
 		node_clear(source, tmp);
<span class="p_del">-		err = migrate_to_node(mm, source, dest, flags);</span>
<span class="p_add">+		err = migrate_to_node(mm, source, dest, flags, &amp;mmrange);</span>
 		if (err &gt; 0)
 			busy += err;
 		if (err &lt; 0)
<span class="p_chunk">@@ -1143,6 +1147,7 @@</span> <span class="p_context"> static long do_mbind(unsigned long start, unsigned long len,</span>
 	unsigned long end;
 	int err;
 	LIST_HEAD(pagelist);
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (flags &amp; ~(unsigned long)MPOL_MF_VALID)
 		return -EINVAL;
<span class="p_chunk">@@ -1204,9 +1209,9 @@</span> <span class="p_context"> static long do_mbind(unsigned long start, unsigned long len,</span>
 		goto mpol_out;
 
 	err = queue_pages_range(mm, start, end, nmask,
<span class="p_del">-			  flags | MPOL_MF_INVERT, &amp;pagelist);</span>
<span class="p_add">+				flags | MPOL_MF_INVERT, &amp;pagelist, &amp;mmrange);</span>
 	if (!err)
<span class="p_del">-		err = mbind_range(mm, start, end, new);</span>
<span class="p_add">+		err = mbind_range(mm, start, end, new, &amp;mmrange);</span>
 
 	if (!err) {
 		int nr_failed = 0;
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 5d0dc7b85f90..7a6afc34dd54 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -2105,7 +2105,8 @@</span> <span class="p_context"> struct migrate_vma {</span>
 
 static int migrate_vma_collect_hole(unsigned long start,
 				    unsigned long end,
<span class="p_del">-				    struct mm_walk *walk)</span>
<span class="p_add">+				    struct mm_walk *walk,</span>
<span class="p_add">+				    struct range_lock *mmrange)</span>
 {
 	struct migrate_vma *migrate = walk-&gt;private;
 	unsigned long addr;
<span class="p_chunk">@@ -2138,7 +2139,8 @@</span> <span class="p_context"> static int migrate_vma_collect_skip(unsigned long start,</span>
 static int migrate_vma_collect_pmd(pmd_t *pmdp,
 				   unsigned long start,
 				   unsigned long end,
<span class="p_del">-				   struct mm_walk *walk)</span>
<span class="p_add">+				   struct mm_walk *walk,</span>
<span class="p_add">+				   struct range_lock *mmrange)</span>
 {
 	struct migrate_vma *migrate = walk-&gt;private;
 	struct vm_area_struct *vma = walk-&gt;vma;
<span class="p_chunk">@@ -2149,7 +2151,7 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 
 again:
 	if (pmd_none(*pmdp))
<span class="p_del">-		return migrate_vma_collect_hole(start, end, walk);</span>
<span class="p_add">+		return migrate_vma_collect_hole(start, end, walk, mmrange);</span>
 
 	if (pmd_trans_huge(*pmdp)) {
 		struct page *page;
<span class="p_chunk">@@ -2183,7 +2185,7 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
 								walk);
 			if (pmd_none(*pmdp))
 				return migrate_vma_collect_hole(start, end,
<span class="p_del">-								walk);</span>
<span class="p_add">+								walk, mmrange);</span>
 		}
 	}
 
<span class="p_chunk">@@ -2309,7 +2311,8 @@</span> <span class="p_context"> static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
  * valid page, it updates the src array and takes a reference on the page, in
  * order to pin the page until we lock it and unmap it.
  */
<span class="p_del">-static void migrate_vma_collect(struct migrate_vma *migrate)</span>
<span class="p_add">+static void migrate_vma_collect(struct migrate_vma *migrate,</span>
<span class="p_add">+				struct range_lock *mmrange)</span>
 {
 	struct mm_walk mm_walk;
 
<span class="p_chunk">@@ -2325,7 +2328,7 @@</span> <span class="p_context"> static void migrate_vma_collect(struct migrate_vma *migrate)</span>
 	mmu_notifier_invalidate_range_start(mm_walk.mm,
 					    migrate-&gt;start,
 					    migrate-&gt;end);
<span class="p_del">-	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="p_add">+	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk, mmrange);</span>
 	mmu_notifier_invalidate_range_end(mm_walk.mm,
 					  migrate-&gt;start,
 					  migrate-&gt;end);
<span class="p_chunk">@@ -2891,7 +2894,8 @@</span> <span class="p_context"> int migrate_vma(const struct migrate_vma_ops *ops,</span>
 		unsigned long end,
 		unsigned long *src,
 		unsigned long *dst,
<span class="p_del">-		void *private)</span>
<span class="p_add">+		void *private,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	struct migrate_vma migrate;
 
<span class="p_chunk">@@ -2917,7 +2921,7 @@</span> <span class="p_context"> int migrate_vma(const struct migrate_vma_ops *ops,</span>
 	migrate.vma = vma;
 
 	/* Collect, and try to unmap source pages */
<span class="p_del">-	migrate_vma_collect(&amp;migrate);</span>
<span class="p_add">+	migrate_vma_collect(&amp;migrate, mmrange);</span>
 	if (!migrate.cpages)
 		return 0;
 
<span class="p_header">diff --git a/mm/mincore.c b/mm/mincore.c</span>
<span class="p_header">index fc37afe226e6..a6875a34aac0 100644</span>
<span class="p_header">--- a/mm/mincore.c</span>
<span class="p_header">+++ b/mm/mincore.c</span>
<span class="p_chunk">@@ -85,7 +85,9 @@</span> <span class="p_context"> static unsigned char mincore_page(struct address_space *mapping, pgoff_t pgoff)</span>
 }
 
 static int __mincore_unmapped_range(unsigned long addr, unsigned long end,
<span class="p_del">-				struct vm_area_struct *vma, unsigned char *vec)</span>
<span class="p_add">+				    struct vm_area_struct *vma,</span>
<span class="p_add">+				    unsigned char *vec,</span>
<span class="p_add">+				    struct range_lock *mmrange)</span>
 {
 	unsigned long nr = (end - addr) &gt;&gt; PAGE_SHIFT;
 	int i;
<span class="p_chunk">@@ -104,15 +106,17 @@</span> <span class="p_context"> static int __mincore_unmapped_range(unsigned long addr, unsigned long end,</span>
 }
 
 static int mincore_unmapped_range(unsigned long addr, unsigned long end,
<span class="p_del">-				   struct mm_walk *walk)</span>
<span class="p_add">+				  struct mm_walk *walk,</span>
<span class="p_add">+				  struct range_lock *mmrange)</span>
 {
 	walk-&gt;private += __mincore_unmapped_range(addr, end,
<span class="p_del">-						  walk-&gt;vma, walk-&gt;private);</span>
<span class="p_add">+						  walk-&gt;vma,</span>
<span class="p_add">+						  walk-&gt;private, mmrange);</span>
 	return 0;
 }
 
 static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,
<span class="p_del">-			struct mm_walk *walk)</span>
<span class="p_add">+			     struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	spinlock_t *ptl;
 	struct vm_area_struct *vma = walk-&gt;vma;
<span class="p_chunk">@@ -128,7 +132,7 @@</span> <span class="p_context"> static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 	}
 
 	if (pmd_trans_unstable(pmd)) {
<span class="p_del">-		__mincore_unmapped_range(addr, end, vma, vec);</span>
<span class="p_add">+		__mincore_unmapped_range(addr, end, vma, vec, mmrange);</span>
 		goto out;
 	}
 
<span class="p_chunk">@@ -138,7 +142,7 @@</span> <span class="p_context"> static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 
 		if (pte_none(pte))
 			__mincore_unmapped_range(addr, addr + PAGE_SIZE,
<span class="p_del">-						 vma, vec);</span>
<span class="p_add">+						 vma, vec, mmrange);</span>
 		else if (pte_present(pte))
 			*vec = 1;
 		else { /* pte is a swap entry */
<span class="p_chunk">@@ -174,7 +178,8 @@</span> <span class="p_context"> static int mincore_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
  * all the arguments, we hold the mmap semaphore: we should
  * just return the amount of info we&#39;re asked for.
  */
<span class="p_del">-static long do_mincore(unsigned long addr, unsigned long pages, unsigned char *vec)</span>
<span class="p_add">+static long do_mincore(unsigned long addr, unsigned long pages,</span>
<span class="p_add">+		       unsigned char *vec, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma;
 	unsigned long end;
<span class="p_chunk">@@ -191,7 +196,7 @@</span> <span class="p_context"> static long do_mincore(unsigned long addr, unsigned long pages, unsigned char *v</span>
 		return -ENOMEM;
 	mincore_walk.mm = vma-&gt;vm_mm;
 	end = min(vma-&gt;vm_end, addr + (pages &lt;&lt; PAGE_SHIFT));
<span class="p_del">-	err = walk_page_range(addr, end, &amp;mincore_walk);</span>
<span class="p_add">+	err = walk_page_range(addr, end, &amp;mincore_walk, mmrange);</span>
 	if (err &lt; 0)
 		return err;
 	return (end - addr) &gt;&gt; PAGE_SHIFT;
<span class="p_chunk">@@ -227,6 +232,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,</span>
 	long retval;
 	unsigned long pages;
 	unsigned char *tmp;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* Check the start address: needs to be page-aligned.. */
 	if (start &amp; ~PAGE_MASK)
<span class="p_chunk">@@ -254,7 +260,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(mincore, unsigned long, start, size_t, len,</span>
 		 * the temporary buffer size.
 		 */
 		down_read(&amp;current-&gt;mm-&gt;mmap_sem);
<span class="p_del">-		retval = do_mincore(start, min(pages, PAGE_SIZE), tmp);</span>
<span class="p_add">+		retval = do_mincore(start, min(pages, PAGE_SIZE), tmp, &amp;mmrange);</span>
 		up_read(&amp;current-&gt;mm-&gt;mmap_sem);
 
 		if (retval &lt;= 0)
<span class="p_header">diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="p_header">index 74e5a6547c3d..3f6bd953e8b0 100644</span>
<span class="p_header">--- a/mm/mlock.c</span>
<span class="p_header">+++ b/mm/mlock.c</span>
<span class="p_chunk">@@ -517,7 +517,8 @@</span> <span class="p_context"> void munlock_vma_pages_range(struct vm_area_struct *vma,</span>
  * For vmas that pass the filters, merge/split as appropriate.
  */
 static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,
<span class="p_del">-	unsigned long start, unsigned long end, vm_flags_t newflags)</span>
<span class="p_add">+	unsigned long start, unsigned long end, vm_flags_t newflags,</span>
<span class="p_add">+	struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	pgoff_t pgoff;
<span class="p_chunk">@@ -534,20 +535,20 @@</span> <span class="p_context"> static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
 	pgoff = vma-&gt;vm_pgoff + ((start - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);
 	*prev = vma_merge(mm, *prev, start, end, newflags, vma-&gt;anon_vma,
 			  vma-&gt;vm_file, pgoff, vma_policy(vma),
<span class="p_del">-			  vma-&gt;vm_userfaultfd_ctx);</span>
<span class="p_add">+			  vma-&gt;vm_userfaultfd_ctx, mmrange);</span>
 	if (*prev) {
 		vma = *prev;
 		goto success;
 	}
 
 	if (start != vma-&gt;vm_start) {
<span class="p_del">-		ret = split_vma(mm, vma, start, 1);</span>
<span class="p_add">+		ret = split_vma(mm, vma, start, 1, mmrange);</span>
 		if (ret)
 			goto out;
 	}
 
 	if (end != vma-&gt;vm_end) {
<span class="p_del">-		ret = split_vma(mm, vma, end, 0);</span>
<span class="p_add">+		ret = split_vma(mm, vma, end, 0, mmrange);</span>
 		if (ret)
 			goto out;
 	}
<span class="p_chunk">@@ -580,7 +581,7 @@</span> <span class="p_context"> static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
 }
 
 static int apply_vma_lock_flags(unsigned long start, size_t len,
<span class="p_del">-				vm_flags_t flags)</span>
<span class="p_add">+				vm_flags_t flags, struct range_lock *mmrange)</span>
 {
 	unsigned long nstart, end, tmp;
 	struct vm_area_struct * vma, * prev;
<span class="p_chunk">@@ -610,7 +611,7 @@</span> <span class="p_context"> static int apply_vma_lock_flags(unsigned long start, size_t len,</span>
 		tmp = vma-&gt;vm_end;
 		if (tmp &gt; end)
 			tmp = end;
<span class="p_del">-		error = mlock_fixup(vma, &amp;prev, nstart, tmp, newflags);</span>
<span class="p_add">+		error = mlock_fixup(vma, &amp;prev, nstart, tmp, newflags, mmrange);</span>
 		if (error)
 			break;
 		nstart = tmp;
<span class="p_chunk">@@ -667,11 +668,13 @@</span> <span class="p_context"> static int count_mm_mlocked_page_nr(struct mm_struct *mm,</span>
 	return count &gt;&gt; PAGE_SHIFT;
 }
 
<span class="p_del">-static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t flags)</span>
<span class="p_add">+static __must_check int do_mlock(unsigned long start, size_t len,</span>
<span class="p_add">+				 vm_flags_t flags)</span>
 {
 	unsigned long locked;
 	unsigned long lock_limit;
 	int error = -ENOMEM;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (!can_do_mlock())
 		return -EPERM;
<span class="p_chunk">@@ -700,7 +703,7 @@</span> <span class="p_context"> static __must_check int do_mlock(unsigned long start, size_t len, vm_flags_t fla</span>
 
 	/* check against resource limits */
 	if ((locked &lt;= lock_limit) || capable(CAP_IPC_LOCK))
<span class="p_del">-		error = apply_vma_lock_flags(start, len, flags);</span>
<span class="p_add">+		error = apply_vma_lock_flags(start, len, flags, &amp;mmrange);</span>
 
 	up_write(&amp;current-&gt;mm-&gt;mmap_sem);
 	if (error)
<span class="p_chunk">@@ -733,13 +736,14 @@</span> <span class="p_context"> SYSCALL_DEFINE3(mlock2, unsigned long, start, size_t, len, int, flags)</span>
 SYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)
 {
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	len = PAGE_ALIGN(len + (offset_in_page(start)));
 	start &amp;= PAGE_MASK;
 
 	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))
 		return -EINTR;
<span class="p_del">-	ret = apply_vma_lock_flags(start, len, 0);</span>
<span class="p_add">+	ret = apply_vma_lock_flags(start, len, 0, &amp;mmrange);</span>
 	up_write(&amp;current-&gt;mm-&gt;mmap_sem);
 
 	return ret;
<span class="p_chunk">@@ -755,7 +759,7 @@</span> <span class="p_context"> SYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)</span>
  * is called once including the MCL_FUTURE flag and then a second time without
  * it, VM_LOCKED and VM_LOCKONFAULT will be cleared from mm-&gt;def_flags.
  */
<span class="p_del">-static int apply_mlockall_flags(int flags)</span>
<span class="p_add">+static int apply_mlockall_flags(int flags, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct * vma, * prev = NULL;
 	vm_flags_t to_add = 0;
<span class="p_chunk">@@ -784,7 +788,8 @@</span> <span class="p_context"> static int apply_mlockall_flags(int flags)</span>
 		newflags |= to_add;
 
 		/* Ignore errors */
<span class="p_del">-		mlock_fixup(vma, &amp;prev, vma-&gt;vm_start, vma-&gt;vm_end, newflags);</span>
<span class="p_add">+		mlock_fixup(vma, &amp;prev, vma-&gt;vm_start, vma-&gt;vm_end, newflags,</span>
<span class="p_add">+			mmrange);</span>
 		cond_resched();
 	}
 out:
<span class="p_chunk">@@ -795,6 +800,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 {
 	unsigned long lock_limit;
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (!flags || (flags &amp; ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT)))
 		return -EINVAL;
<span class="p_chunk">@@ -811,7 +817,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 	ret = -ENOMEM;
 	if (!(flags &amp; MCL_CURRENT) || (current-&gt;mm-&gt;total_vm &lt;= lock_limit) ||
 	    capable(CAP_IPC_LOCK))
<span class="p_del">-		ret = apply_mlockall_flags(flags);</span>
<span class="p_add">+		ret = apply_mlockall_flags(flags, &amp;mmrange);</span>
 	up_write(&amp;current-&gt;mm-&gt;mmap_sem);
 	if (!ret &amp;&amp; (flags &amp; MCL_CURRENT))
 		mm_populate(0, TASK_SIZE);
<span class="p_chunk">@@ -822,10 +828,11 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 SYSCALL_DEFINE0(munlockall)
 {
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (down_write_killable(&amp;current-&gt;mm-&gt;mmap_sem))
 		return -EINTR;
<span class="p_del">-	ret = apply_mlockall_flags(0);</span>
<span class="p_add">+	ret = apply_mlockall_flags(0, &amp;mmrange);</span>
 	up_write(&amp;current-&gt;mm-&gt;mmap_sem);
 	return ret;
 }
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 4bb038e7984b..f61d49cb791e 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -177,7 +177,8 @@</span> <span class="p_context"> static struct vm_area_struct *remove_vma(struct vm_area_struct *vma)</span>
 	return next;
 }
 
<span class="p_del">-static int do_brk(unsigned long addr, unsigned long len, struct list_head *uf);</span>
<span class="p_add">+static int do_brk(unsigned long addr, unsigned long len, struct list_head *uf,</span>
<span class="p_add">+		  struct range_lock *mmrange);</span>
 
 SYSCALL_DEFINE1(brk, unsigned long, brk)
 {
<span class="p_chunk">@@ -188,6 +189,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 	unsigned long min_brk;
 	bool populate;
 	LIST_HEAD(uf);
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (down_write_killable(&amp;mm-&gt;mmap_sem))
 		return -EINTR;
<span class="p_chunk">@@ -225,7 +227,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 
 	/* Always allow shrinking brk. */
 	if (brk &lt;= mm-&gt;brk) {
<span class="p_del">-		if (!do_munmap(mm, newbrk, oldbrk-newbrk, &amp;uf))</span>
<span class="p_add">+		if (!do_munmap(mm, newbrk, oldbrk-newbrk, &amp;uf, &amp;mmrange))</span>
 			goto set_brk;
 		goto out;
 	}
<span class="p_chunk">@@ -236,7 +238,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(brk, unsigned long, brk)</span>
 		goto out;
 
 	/* Ok, looks good - let it rip. */
<span class="p_del">-	if (do_brk(oldbrk, newbrk-oldbrk, &amp;uf) &lt; 0)</span>
<span class="p_add">+	if (do_brk(oldbrk, newbrk-oldbrk, &amp;uf, &amp;mmrange) &lt; 0)</span>
 		goto out;
 
 set_brk:
<span class="p_chunk">@@ -680,7 +682,7 @@</span> <span class="p_context"> static inline void __vma_unlink_prev(struct mm_struct *mm,</span>
  */
 int __vma_adjust(struct vm_area_struct *vma, unsigned long start,
 	unsigned long end, pgoff_t pgoff, struct vm_area_struct *insert,
<span class="p_del">-	struct vm_area_struct *expand)</span>
<span class="p_add">+	struct vm_area_struct *expand, struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct vm_area_struct *next = vma-&gt;vm_next, *orig_vma = vma;
<span class="p_chunk">@@ -887,10 +889,10 @@</span> <span class="p_context"> int __vma_adjust(struct vm_area_struct *vma, unsigned long start,</span>
 		i_mmap_unlock_write(mapping);
 
 	if (root) {
<span class="p_del">-		uprobe_mmap(vma);</span>
<span class="p_add">+		uprobe_mmap(vma, mmrange);</span>
 
 		if (adjust_next)
<span class="p_del">-			uprobe_mmap(next);</span>
<span class="p_add">+			uprobe_mmap(next, mmrange);</span>
 	}
 
 	if (remove_next) {
<span class="p_chunk">@@ -960,7 +962,7 @@</span> <span class="p_context"> int __vma_adjust(struct vm_area_struct *vma, unsigned long start,</span>
 		}
 	}
 	if (insert &amp;&amp; file)
<span class="p_del">-		uprobe_mmap(insert);</span>
<span class="p_add">+		uprobe_mmap(insert, mmrange);</span>
 
 	validate_mm(mm);
 
<span class="p_chunk">@@ -1101,7 +1103,8 @@</span> <span class="p_context"> struct vm_area_struct *vma_merge(struct mm_struct *mm,</span>
 			unsigned long end, unsigned long vm_flags,
 			struct anon_vma *anon_vma, struct file *file,
 			pgoff_t pgoff, struct mempolicy *policy,
<span class="p_del">-			struct vm_userfaultfd_ctx vm_userfaultfd_ctx)</span>
<span class="p_add">+			struct vm_userfaultfd_ctx vm_userfaultfd_ctx,</span>
<span class="p_add">+			struct range_lock *mmrange)</span>
 {
 	pgoff_t pglen = (end - addr) &gt;&gt; PAGE_SHIFT;
 	struct vm_area_struct *area, *next;
<span class="p_chunk">@@ -1149,10 +1152,11 @@</span> <span class="p_context"> struct vm_area_struct *vma_merge(struct mm_struct *mm,</span>
 							/* cases 1, 6 */
 			err = __vma_adjust(prev, prev-&gt;vm_start,
 					 next-&gt;vm_end, prev-&gt;vm_pgoff, NULL,
<span class="p_del">-					 prev);</span>
<span class="p_add">+					 prev, mmrange);</span>
 		} else					/* cases 2, 5, 7 */
 			err = __vma_adjust(prev, prev-&gt;vm_start,
<span class="p_del">-					 end, prev-&gt;vm_pgoff, NULL, prev);</span>
<span class="p_add">+					   end, prev-&gt;vm_pgoff, NULL,</span>
<span class="p_add">+					   prev, mmrange);</span>
 		if (err)
 			return NULL;
 		khugepaged_enter_vma_merge(prev, vm_flags);
<span class="p_chunk">@@ -1169,10 +1173,12 @@</span> <span class="p_context"> struct vm_area_struct *vma_merge(struct mm_struct *mm,</span>
 					     vm_userfaultfd_ctx)) {
 		if (prev &amp;&amp; addr &lt; prev-&gt;vm_end)	/* case 4 */
 			err = __vma_adjust(prev, prev-&gt;vm_start,
<span class="p_del">-					 addr, prev-&gt;vm_pgoff, NULL, next);</span>
<span class="p_add">+					   addr, prev-&gt;vm_pgoff, NULL,</span>
<span class="p_add">+					   next, mmrange);</span>
 		else {					/* cases 3, 8 */
 			err = __vma_adjust(area, addr, next-&gt;vm_end,
<span class="p_del">-					 next-&gt;vm_pgoff - pglen, NULL, next);</span>
<span class="p_add">+					   next-&gt;vm_pgoff - pglen, NULL,</span>
<span class="p_add">+					   next, mmrange);</span>
 			/*
 			 * In case 3 area is already equal to next and
 			 * this is a noop, but in case 8 &quot;area&quot; has
<span class="p_chunk">@@ -1322,7 +1328,7 @@</span> <span class="p_context"> unsigned long do_mmap(struct file *file, unsigned long addr,</span>
 			unsigned long len, unsigned long prot,
 			unsigned long flags, vm_flags_t vm_flags,
 			unsigned long pgoff, unsigned long *populate,
<span class="p_del">-			struct list_head *uf)</span>
<span class="p_add">+		        struct list_head *uf, struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	int pkey = 0;
<span class="p_chunk">@@ -1491,7 +1497,7 @@</span> <span class="p_context"> unsigned long do_mmap(struct file *file, unsigned long addr,</span>
 			vm_flags |= VM_NORESERVE;
 	}
 
<span class="p_del">-	addr = mmap_region(file, addr, len, vm_flags, pgoff, uf);</span>
<span class="p_add">+	addr = mmap_region(file, addr, len, vm_flags, pgoff, uf, mmrange);</span>
 	if (!IS_ERR_VALUE(addr) &amp;&amp;
 	    ((vm_flags &amp; VM_LOCKED) ||
 	     (flags &amp; (MAP_POPULATE | MAP_NONBLOCK)) == MAP_POPULATE))
<span class="p_chunk">@@ -1628,7 +1634,7 @@</span> <span class="p_context"> static inline int accountable_mapping(struct file *file, vm_flags_t vm_flags)</span>
 
 unsigned long mmap_region(struct file *file, unsigned long addr,
 		unsigned long len, vm_flags_t vm_flags, unsigned long pgoff,
<span class="p_del">-		struct list_head *uf)</span>
<span class="p_add">+		struct list_head *uf, struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma, *prev;
<span class="p_chunk">@@ -1654,7 +1660,7 @@</span> <span class="p_context"> unsigned long mmap_region(struct file *file, unsigned long addr,</span>
 	/* Clear old maps */
 	while (find_vma_links(mm, addr, addr + len, &amp;prev, &amp;rb_link,
 			      &amp;rb_parent)) {
<span class="p_del">-		if (do_munmap(mm, addr, len, uf))</span>
<span class="p_add">+		if (do_munmap(mm, addr, len, uf, mmrange))</span>
 			return -ENOMEM;
 	}
 
<span class="p_chunk">@@ -1672,7 +1678,7 @@</span> <span class="p_context"> unsigned long mmap_region(struct file *file, unsigned long addr,</span>
 	 * Can we just expand an old mapping?
 	 */
 	vma = vma_merge(mm, prev, addr, addr + len, vm_flags,
<span class="p_del">-			NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX);</span>
<span class="p_add">+			NULL, file, pgoff, NULL, NULL_VM_UFFD_CTX, mmrange);</span>
 	if (vma)
 		goto out;
 
<span class="p_chunk">@@ -1756,7 +1762,7 @@</span> <span class="p_context"> unsigned long mmap_region(struct file *file, unsigned long addr,</span>
 	}
 
 	if (file)
<span class="p_del">-		uprobe_mmap(vma);</span>
<span class="p_add">+		uprobe_mmap(vma, mmrange);</span>
 
 	/*
 	 * New (or expanded) vma always get soft dirty status.
<span class="p_chunk">@@ -2435,7 +2441,8 @@</span> <span class="p_context"> int expand_stack(struct vm_area_struct *vma, unsigned long address)</span>
 }
 
 struct vm_area_struct *
<span class="p_del">-find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
<span class="p_add">+find_extend_vma(struct mm_struct *mm, unsigned long addr,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma, *prev;
 
<span class="p_chunk">@@ -2446,7 +2453,8 @@</span> <span class="p_context"> find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
 	if (!prev || expand_stack(prev, addr))
 		return NULL;
 	if (prev-&gt;vm_flags &amp; VM_LOCKED)
<span class="p_del">-		populate_vma_page_range(prev, addr, prev-&gt;vm_end, NULL);</span>
<span class="p_add">+		populate_vma_page_range(prev, addr, prev-&gt;vm_end,</span>
<span class="p_add">+					NULL, mmrange);</span>
 	return prev;
 }
 #else
<span class="p_chunk">@@ -2456,7 +2464,8 @@</span> <span class="p_context"> int expand_stack(struct vm_area_struct *vma, unsigned long address)</span>
 }
 
 struct vm_area_struct *
<span class="p_del">-find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
<span class="p_add">+find_extend_vma(struct mm_struct *mm, unsigned long addr,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma;
 	unsigned long start;
<span class="p_chunk">@@ -2473,7 +2482,7 @@</span> <span class="p_context"> find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
 	if (expand_stack(vma, addr))
 		return NULL;
 	if (vma-&gt;vm_flags &amp; VM_LOCKED)
<span class="p_del">-		populate_vma_page_range(vma, addr, start, NULL);</span>
<span class="p_add">+		populate_vma_page_range(vma, addr, start, NULL, mmrange);</span>
 	return vma;
 }
 #endif
<span class="p_chunk">@@ -2561,7 +2570,7 @@</span> <span class="p_context"> detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma,</span>
  * has already been checked or doesn&#39;t make sense to fail.
  */
 int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
<span class="p_del">-		unsigned long addr, int new_below)</span>
<span class="p_add">+		unsigned long addr, int new_below, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *new;
 	int err;
<span class="p_chunk">@@ -2604,9 +2613,11 @@</span> <span class="p_context"> int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 
 	if (new_below)
 		err = vma_adjust(vma, addr, vma-&gt;vm_end, vma-&gt;vm_pgoff +
<span class="p_del">-			((addr - new-&gt;vm_start) &gt;&gt; PAGE_SHIFT), new);</span>
<span class="p_add">+			  ((addr - new-&gt;vm_start) &gt;&gt; PAGE_SHIFT), new,</span>
<span class="p_add">+			   mmrange);</span>
 	else
<span class="p_del">-		err = vma_adjust(vma, vma-&gt;vm_start, addr, vma-&gt;vm_pgoff, new);</span>
<span class="p_add">+		err = vma_adjust(vma, vma-&gt;vm_start, addr, vma-&gt;vm_pgoff, new,</span>
<span class="p_add">+				 mmrange);</span>
 
 	/* Success. */
 	if (!err)
<span class="p_chunk">@@ -2630,12 +2641,12 @@</span> <span class="p_context"> int __split_vma(struct mm_struct *mm, struct vm_area_struct *vma,</span>
  * either for the first part or the tail.
  */
 int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
<span class="p_del">-	      unsigned long addr, int new_below)</span>
<span class="p_add">+	      unsigned long addr, int new_below, struct range_lock *mmrange)</span>
 {
 	if (mm-&gt;map_count &gt;= sysctl_max_map_count)
 		return -ENOMEM;
 
<span class="p_del">-	return __split_vma(mm, vma, addr, new_below);</span>
<span class="p_add">+	return __split_vma(mm, vma, addr, new_below, mmrange);</span>
 }
 
 /* Munmap is split into 2 main parts -- this part which finds
<span class="p_chunk">@@ -2644,7 +2655,7 @@</span> <span class="p_context"> int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,</span>
  * Jeremy Fitzhardinge &lt;jeremy@goop.org&gt;
  */
 int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,
<span class="p_del">-	      struct list_head *uf)</span>
<span class="p_add">+	      struct list_head *uf, struct range_lock *mmrange)</span>
 {
 	unsigned long end;
 	struct vm_area_struct *vma, *prev, *last;
<span class="p_chunk">@@ -2686,7 +2697,7 @@</span> <span class="p_context"> int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,</span>
 		if (end &lt; vma-&gt;vm_end &amp;&amp; mm-&gt;map_count &gt;= sysctl_max_map_count)
 			return -ENOMEM;
 
<span class="p_del">-		error = __split_vma(mm, vma, start, 0);</span>
<span class="p_add">+		error = __split_vma(mm, vma, start, 0, mmrange);</span>
 		if (error)
 			return error;
 		prev = vma;
<span class="p_chunk">@@ -2695,7 +2706,7 @@</span> <span class="p_context"> int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,</span>
 	/* Does it split the last one? */
 	last = find_vma(mm, end);
 	if (last &amp;&amp; end &gt; last-&gt;vm_start) {
<span class="p_del">-		int error = __split_vma(mm, last, end, 1);</span>
<span class="p_add">+		int error = __split_vma(mm, last, end, 1, mmrange);</span>
 		if (error)
 			return error;
 	}
<span class="p_chunk">@@ -2736,7 +2747,7 @@</span> <span class="p_context"> int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,</span>
 	detach_vmas_to_be_unmapped(mm, vma, prev, end);
 	unmap_region(mm, vma, prev, start, end);
 
<span class="p_del">-	arch_unmap(mm, vma, start, end);</span>
<span class="p_add">+	arch_unmap(mm, vma, start, end, mmrange);</span>
 
 	/* Fix up all other VM information */
 	remove_vma_list(mm, vma);
<span class="p_chunk">@@ -2749,11 +2760,12 @@</span> <span class="p_context"> int vm_munmap(unsigned long start, size_t len)</span>
 	int ret;
 	struct mm_struct *mm = current-&gt;mm;
 	LIST_HEAD(uf);
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (down_write_killable(&amp;mm-&gt;mmap_sem))
 		return -EINTR;
 
<span class="p_del">-	ret = do_munmap(mm, start, len, &amp;uf);</span>
<span class="p_add">+	ret = do_munmap(mm, start, len, &amp;uf, &amp;mmrange);</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 	userfaultfd_unmap_complete(mm, &amp;uf);
 	return ret;
<span class="p_chunk">@@ -2779,6 +2791,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 	unsigned long populate = 0;
 	unsigned long ret = -EINVAL;
 	struct file *file;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	pr_warn_once(&quot;%s (%d) uses deprecated remap_file_pages() syscall. See Documentation/vm/remap_file_pages.txt.\n&quot;,
 		     current-&gt;comm, current-&gt;pid);
<span class="p_chunk">@@ -2855,7 +2868,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 
 	file = get_file(vma-&gt;vm_file);
 	ret = do_mmap_pgoff(vma-&gt;vm_file, start, size,
<span class="p_del">-			prot, flags, pgoff, &amp;populate, NULL);</span>
<span class="p_add">+			    prot, flags, pgoff, &amp;populate, NULL, &amp;mmrange);</span>
 	fput(file);
 out:
 	up_write(&amp;mm-&gt;mmap_sem);
<span class="p_chunk">@@ -2881,7 +2894,9 @@</span> <span class="p_context"> static inline void verify_mm_writelocked(struct mm_struct *mm)</span>
  *  anonymous maps.  eventually we may be able to do some
  *  brk-specific accounting here.
  */
<span class="p_del">-static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long flags, struct list_head *uf)</span>
<span class="p_add">+static int do_brk_flags(unsigned long addr, unsigned long request,</span>
<span class="p_add">+			unsigned long flags, struct list_head *uf,</span>
<span class="p_add">+			struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma, *prev;
<span class="p_chunk">@@ -2920,7 +2935,7 @@</span> <span class="p_context"> static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long</span>
 	 */
 	while (find_vma_links(mm, addr, addr + len, &amp;prev, &amp;rb_link,
 			      &amp;rb_parent)) {
<span class="p_del">-		if (do_munmap(mm, addr, len, uf))</span>
<span class="p_add">+		if (do_munmap(mm, addr, len, uf, mmrange))</span>
 			return -ENOMEM;
 	}
 
<span class="p_chunk">@@ -2936,7 +2951,7 @@</span> <span class="p_context"> static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long</span>
 
 	/* Can we just expand an old private anonymous mapping? */
 	vma = vma_merge(mm, prev, addr, addr + len, flags,
<span class="p_del">-			NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX);</span>
<span class="p_add">+			NULL, NULL, pgoff, NULL, NULL_VM_UFFD_CTX, mmrange);</span>
 	if (vma)
 		goto out;
 
<span class="p_chunk">@@ -2967,9 +2982,10 @@</span> <span class="p_context"> static int do_brk_flags(unsigned long addr, unsigned long request, unsigned long</span>
 	return 0;
 }
 
<span class="p_del">-static int do_brk(unsigned long addr, unsigned long len, struct list_head *uf)</span>
<span class="p_add">+static int do_brk(unsigned long addr, unsigned long len, struct list_head *uf,</span>
<span class="p_add">+		  struct range_lock *mmrange)</span>
 {
<span class="p_del">-	return do_brk_flags(addr, len, 0, uf);</span>
<span class="p_add">+	return do_brk_flags(addr, len, 0, uf, mmrange);</span>
 }
 
 int vm_brk_flags(unsigned long addr, unsigned long len, unsigned long flags)
<span class="p_chunk">@@ -2978,11 +2994,12 @@</span> <span class="p_context"> int vm_brk_flags(unsigned long addr, unsigned long len, unsigned long flags)</span>
 	int ret;
 	bool populate;
 	LIST_HEAD(uf);
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (down_write_killable(&amp;mm-&gt;mmap_sem))
 		return -EINTR;
 
<span class="p_del">-	ret = do_brk_flags(addr, len, flags, &amp;uf);</span>
<span class="p_add">+	ret = do_brk_flags(addr, len, flags, &amp;uf, &amp;mmrange);</span>
 	populate = ((mm-&gt;def_flags &amp; VM_LOCKED) != 0);
 	up_write(&amp;mm-&gt;mmap_sem);
 	userfaultfd_unmap_complete(mm, &amp;uf);
<span class="p_chunk">@@ -3105,7 +3122,7 @@</span> <span class="p_context"> int insert_vm_struct(struct mm_struct *mm, struct vm_area_struct *vma)</span>
  */
 struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,
 	unsigned long addr, unsigned long len, pgoff_t pgoff,
<span class="p_del">-	bool *need_rmap_locks)</span>
<span class="p_add">+	bool *need_rmap_locks, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma = *vmap;
 	unsigned long vma_start = vma-&gt;vm_start;
<span class="p_chunk">@@ -3127,7 +3144,7 @@</span> <span class="p_context"> struct vm_area_struct *copy_vma(struct vm_area_struct **vmap,</span>
 		return NULL;	/* should never get here */
 	new_vma = vma_merge(mm, prev, addr, addr + len, vma-&gt;vm_flags,
 			    vma-&gt;anon_vma, vma-&gt;vm_file, pgoff, vma_policy(vma),
<span class="p_del">-			    vma-&gt;vm_userfaultfd_ctx);</span>
<span class="p_add">+			    vma-&gt;vm_userfaultfd_ctx, mmrange);</span>
 	if (new_vma) {
 		/*
 		 * Source vma may have been merged into new_vma
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index e3309fcf586b..b84a70720319 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -299,7 +299,8 @@</span> <span class="p_context"> unsigned long change_protection(struct vm_area_struct *vma, unsigned long start,</span>
 
 int
 mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,
<span class="p_del">-	unsigned long start, unsigned long end, unsigned long newflags)</span>
<span class="p_add">+	       unsigned long start, unsigned long end, unsigned long newflags,</span>
<span class="p_add">+	       struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	unsigned long oldflags = vma-&gt;vm_flags;
<span class="p_chunk">@@ -340,7 +341,7 @@</span> <span class="p_context"> mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
 	pgoff = vma-&gt;vm_pgoff + ((start - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);
 	*pprev = vma_merge(mm, *pprev, start, end, newflags,
 			   vma-&gt;anon_vma, vma-&gt;vm_file, pgoff, vma_policy(vma),
<span class="p_del">-			   vma-&gt;vm_userfaultfd_ctx);</span>
<span class="p_add">+			   vma-&gt;vm_userfaultfd_ctx, mmrange);</span>
 	if (*pprev) {
 		vma = *pprev;
 		VM_WARN_ON((vma-&gt;vm_flags ^ newflags) &amp; ~VM_SOFTDIRTY);
<span class="p_chunk">@@ -350,13 +351,13 @@</span> <span class="p_context"> mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
 	*pprev = vma;
 
 	if (start != vma-&gt;vm_start) {
<span class="p_del">-		error = split_vma(mm, vma, start, 1);</span>
<span class="p_add">+		error = split_vma(mm, vma, start, 1, mmrange);</span>
 		if (error)
 			goto fail;
 	}
 
 	if (end != vma-&gt;vm_end) {
<span class="p_del">-		error = split_vma(mm, vma, end, 0);</span>
<span class="p_add">+		error = split_vma(mm, vma, end, 0, mmrange);</span>
 		if (error)
 			goto fail;
 	}
<span class="p_chunk">@@ -379,7 +380,7 @@</span> <span class="p_context"> mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
 	 */
 	if ((oldflags &amp; (VM_WRITE | VM_SHARED | VM_LOCKED)) == VM_LOCKED &amp;&amp;
 			(newflags &amp; VM_WRITE)) {
<span class="p_del">-		populate_vma_page_range(vma, start, end, NULL);</span>
<span class="p_add">+		populate_vma_page_range(vma, start, end, NULL, mmrange);</span>
 	}
 
 	vm_stat_account(mm, oldflags, -nrpages);
<span class="p_chunk">@@ -404,6 +405,7 @@</span> <span class="p_context"> static int do_mprotect_pkey(unsigned long start, size_t len,</span>
 	const int grows = prot &amp; (PROT_GROWSDOWN|PROT_GROWSUP);
 	const bool rier = (current-&gt;personality &amp; READ_IMPLIES_EXEC) &amp;&amp;
 				(prot &amp; PROT_READ);
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	prot &amp;= ~(PROT_GROWSDOWN|PROT_GROWSUP);
 	if (grows == (PROT_GROWSDOWN|PROT_GROWSUP)) /* can&#39;t be both */
<span class="p_chunk">@@ -494,7 +496,7 @@</span> <span class="p_context"> static int do_mprotect_pkey(unsigned long start, size_t len,</span>
 		tmp = vma-&gt;vm_end;
 		if (tmp &gt; end)
 			tmp = end;
<span class="p_del">-		error = mprotect_fixup(vma, &amp;prev, nstart, tmp, newflags);</span>
<span class="p_add">+		error = mprotect_fixup(vma, &amp;prev, nstart, tmp, newflags, &amp;mmrange);</span>
 		if (error)
 			goto out;
 		nstart = tmp;
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index 049470aa1e3e..21a9e2a2baa2 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -264,7 +264,8 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 		unsigned long old_addr, unsigned long old_len,
 		unsigned long new_len, unsigned long new_addr,
 		bool *locked, struct vm_userfaultfd_ctx *uf,
<span class="p_del">-		struct list_head *uf_unmap)</span>
<span class="p_add">+		struct list_head *uf_unmap,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = vma-&gt;vm_mm;
 	struct vm_area_struct *new_vma;
<span class="p_chunk">@@ -292,13 +293,13 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 	 * so KSM can come around to merge on vma and new_vma afterwards.
 	 */
 	err = ksm_madvise(vma, old_addr, old_addr + old_len,
<span class="p_del">-						MADV_UNMERGEABLE, &amp;vm_flags);</span>
<span class="p_add">+			  MADV_UNMERGEABLE, &amp;vm_flags, mmrange);</span>
 	if (err)
 		return err;
 
 	new_pgoff = vma-&gt;vm_pgoff + ((old_addr - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);
 	new_vma = copy_vma(&amp;vma, new_addr, new_len, new_pgoff,
<span class="p_del">-			   &amp;need_rmap_locks);</span>
<span class="p_add">+			   &amp;need_rmap_locks, mmrange);</span>
 	if (!new_vma)
 		return -ENOMEM;
 
<span class="p_chunk">@@ -353,7 +354,7 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 	if (unlikely(vma-&gt;vm_flags &amp; VM_PFNMAP))
 		untrack_pfn_moved(vma);
 
<span class="p_del">-	if (do_munmap(mm, old_addr, old_len, uf_unmap) &lt; 0) {</span>
<span class="p_add">+	if (do_munmap(mm, old_addr, old_len, uf_unmap, mmrange) &lt; 0) {</span>
 		/* OOM: unable to split vma, just get accounts right */
 		vm_unacct_memory(excess &gt;&gt; PAGE_SHIFT);
 		excess = 0;
<span class="p_chunk">@@ -444,7 +445,8 @@</span> <span class="p_context"> static unsigned long mremap_to(unsigned long addr, unsigned long old_len,</span>
 		unsigned long new_addr, unsigned long new_len, bool *locked,
 		struct vm_userfaultfd_ctx *uf,
 		struct list_head *uf_unmap_early,
<span class="p_del">-		struct list_head *uf_unmap)</span>
<span class="p_add">+		struct list_head *uf_unmap,</span>
<span class="p_add">+		struct range_lock *mmrange)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	struct vm_area_struct *vma;
<span class="p_chunk">@@ -462,12 +464,13 @@</span> <span class="p_context"> static unsigned long mremap_to(unsigned long addr, unsigned long old_len,</span>
 	if (addr + old_len &gt; new_addr &amp;&amp; new_addr + new_len &gt; addr)
 		goto out;
 
<span class="p_del">-	ret = do_munmap(mm, new_addr, new_len, uf_unmap_early);</span>
<span class="p_add">+	ret = do_munmap(mm, new_addr, new_len, uf_unmap_early, mmrange);</span>
 	if (ret)
 		goto out;
 
 	if (old_len &gt;= new_len) {
<span class="p_del">-		ret = do_munmap(mm, addr+new_len, old_len - new_len, uf_unmap);</span>
<span class="p_add">+		ret = do_munmap(mm, addr+new_len, old_len - new_len,</span>
<span class="p_add">+				uf_unmap, mmrange);</span>
 		if (ret &amp;&amp; old_len != new_len)
 			goto out;
 		old_len = new_len;
<span class="p_chunk">@@ -490,7 +493,7 @@</span> <span class="p_context"> static unsigned long mremap_to(unsigned long addr, unsigned long old_len,</span>
 		goto out1;
 
 	ret = move_vma(vma, addr, old_len, new_len, new_addr, locked, uf,
<span class="p_del">-		       uf_unmap);</span>
<span class="p_add">+		       uf_unmap, mmrange);</span>
 	if (!(offset_in_page(ret)))
 		goto out;
 out1:
<span class="p_chunk">@@ -532,6 +535,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 	struct vm_userfaultfd_ctx uf = NULL_VM_UFFD_CTX;
 	LIST_HEAD(uf_unmap_early);
 	LIST_HEAD(uf_unmap);
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	if (flags &amp; ~(MREMAP_FIXED | MREMAP_MAYMOVE))
 		return ret;
<span class="p_chunk">@@ -558,7 +562,8 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 
 	if (flags &amp; MREMAP_FIXED) {
 		ret = mremap_to(addr, old_len, new_addr, new_len,
<span class="p_del">-				&amp;locked, &amp;uf, &amp;uf_unmap_early, &amp;uf_unmap);</span>
<span class="p_add">+				&amp;locked, &amp;uf, &amp;uf_unmap_early,</span>
<span class="p_add">+				&amp;uf_unmap, &amp;mmrange);</span>
 		goto out;
 	}
 
<span class="p_chunk">@@ -568,7 +573,8 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 	 * do_munmap does all the needed commit accounting
 	 */
 	if (old_len &gt;= new_len) {
<span class="p_del">-		ret = do_munmap(mm, addr+new_len, old_len - new_len, &amp;uf_unmap);</span>
<span class="p_add">+		ret = do_munmap(mm, addr+new_len, old_len - new_len,</span>
<span class="p_add">+				&amp;uf_unmap, &amp;mmrange);</span>
 		if (ret &amp;&amp; old_len != new_len)
 			goto out;
 		ret = addr;
<span class="p_chunk">@@ -592,7 +598,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 			int pages = (new_len - old_len) &gt;&gt; PAGE_SHIFT;
 
 			if (vma_adjust(vma, vma-&gt;vm_start, addr + new_len,
<span class="p_del">-				       vma-&gt;vm_pgoff, NULL)) {</span>
<span class="p_add">+				       vma-&gt;vm_pgoff, NULL, &amp;mmrange)) {</span>
 				ret = -ENOMEM;
 				goto out;
 			}
<span class="p_chunk">@@ -628,7 +634,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 		}
 
 		ret = move_vma(vma, addr, old_len, new_len, new_addr,
<span class="p_del">-			       &amp;locked, &amp;uf, &amp;uf_unmap);</span>
<span class="p_add">+			       &amp;locked, &amp;uf, &amp;uf_unmap, &amp;mmrange);</span>
 	}
 out:
 	if (offset_in_page(ret)) {
<span class="p_header">diff --git a/mm/nommu.c b/mm/nommu.c</span>
<span class="p_header">index ebb6e618dade..1805f0a788b3 100644</span>
<span class="p_header">--- a/mm/nommu.c</span>
<span class="p_header">+++ b/mm/nommu.c</span>
<span class="p_chunk">@@ -113,7 +113,8 @@</span> <span class="p_context"> unsigned int kobjsize(const void *objp)</span>
 static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,
 		      unsigned long start, unsigned long nr_pages,
 		      unsigned int foll_flags, struct page **pages,
<span class="p_del">-		      struct vm_area_struct **vmas, int *nonblocking)</span>
<span class="p_add">+		      struct vm_area_struct **vmas, int *nonblocking,</span>
<span class="p_add">+		      struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma;
 	unsigned long vm_flags;
<span class="p_chunk">@@ -162,18 +163,19 @@</span> <span class="p_context"> static long __get_user_pages(struct task_struct *tsk, struct mm_struct *mm,</span>
  */
 long get_user_pages(unsigned long start, unsigned long nr_pages,
 		    unsigned int gup_flags, struct page **pages,
<span class="p_del">-		    struct vm_area_struct **vmas)</span>
<span class="p_add">+		    struct vm_area_struct **vmas,</span>
<span class="p_add">+		    struct range_lock *mmrange)</span>
 {
 	return __get_user_pages(current, current-&gt;mm, start, nr_pages,
<span class="p_del">-				gup_flags, pages, vmas, NULL);</span>
<span class="p_add">+				gup_flags, pages, vmas, NULL, mmrange);</span>
 }
 EXPORT_SYMBOL(get_user_pages);
 
 long get_user_pages_locked(unsigned long start, unsigned long nr_pages,
 			    unsigned int gup_flags, struct page **pages,
<span class="p_del">-			    int *locked)</span>
<span class="p_add">+			    int *locked, struct range_lock *mmrange)</span>
 {
<span class="p_del">-	return get_user_pages(start, nr_pages, gup_flags, pages, NULL);</span>
<span class="p_add">+	return get_user_pages(start, nr_pages, gup_flags, pages, NULL, mmrange);</span>
 }
 EXPORT_SYMBOL(get_user_pages_locked);
 
<span class="p_chunk">@@ -183,9 +185,11 @@</span> <span class="p_context"> static long __get_user_pages_unlocked(struct task_struct *tsk,</span>
 			unsigned int gup_flags)
 {
 	long ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
<span class="p_add">+</span>
 	down_read(&amp;mm-&gt;mmap_sem);
 	ret = __get_user_pages(tsk, mm, start, nr_pages, gup_flags, pages,
<span class="p_del">-				NULL, NULL);</span>
<span class="p_add">+			       NULL, NULL, &amp;mmrange);</span>
 	up_read(&amp;mm-&gt;mmap_sem);
 	return ret;
 }
<span class="p_chunk">@@ -836,7 +840,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(find_vma);</span>
  * find a VMA
  * - we don&#39;t extend stack VMAs under NOMMU conditions
  */
<span class="p_del">-struct vm_area_struct *find_extend_vma(struct mm_struct *mm, unsigned long addr)</span>
<span class="p_add">+struct vm_area_struct *find_extend_vma(struct mm_struct *mm, unsigned long addr,</span>
<span class="p_add">+				       struct range_lock *mmrange)</span>
 {
 	return find_vma(mm, addr);
 }
<span class="p_chunk">@@ -1206,7 +1211,8 @@</span> <span class="p_context"> unsigned long do_mmap(struct file *file,</span>
 			vm_flags_t vm_flags,
 			unsigned long pgoff,
 			unsigned long *populate,
<span class="p_del">-			struct list_head *uf)</span>
<span class="p_add">+			struct list_head *uf,</span>
<span class="p_add">+			struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma;
 	struct vm_region *region;
<span class="p_chunk">@@ -1476,7 +1482,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(old_mmap, struct mmap_arg_struct __user *, arg)</span>
  * for the first part or the tail.
  */
 int split_vma(struct mm_struct *mm, struct vm_area_struct *vma,
<span class="p_del">-	      unsigned long addr, int new_below)</span>
<span class="p_add">+	      unsigned long addr, int new_below, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *new;
 	struct vm_region *region;
<span class="p_chunk">@@ -1578,7 +1584,8 @@</span> <span class="p_context"> static int shrink_vma(struct mm_struct *mm,</span>
  * - under NOMMU conditions the chunk to be unmapped must be backed by a single
  *   VMA, though it need not cover the whole VMA
  */
<span class="p_del">-int do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list_head *uf)</span>
<span class="p_add">+int do_munmap(struct mm_struct *mm, unsigned long start, size_t len,</span>
<span class="p_add">+	      struct list_head *uf, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma;
 	unsigned long end;
<span class="p_chunk">@@ -1624,7 +1631,7 @@</span> <span class="p_context"> int do_munmap(struct mm_struct *mm, unsigned long start, size_t len, struct list</span>
 		if (end != vma-&gt;vm_end &amp;&amp; offset_in_page(end))
 			return -EINVAL;
 		if (start != vma-&gt;vm_start &amp;&amp; end != vma-&gt;vm_end) {
<span class="p_del">-			ret = split_vma(mm, vma, start, 1);</span>
<span class="p_add">+			ret = split_vma(mm, vma, start, 1, mmrange);</span>
 			if (ret &lt; 0)
 				return ret;
 		}
<span class="p_chunk">@@ -1642,9 +1649,10 @@</span> <span class="p_context"> int vm_munmap(unsigned long addr, size_t len)</span>
 {
 	struct mm_struct *mm = current-&gt;mm;
 	int ret;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_del">-	ret = do_munmap(mm, addr, len, NULL);</span>
<span class="p_add">+	ret = do_munmap(mm, addr, len, NULL, &amp;mmrange);</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 	return ret;
 }
<span class="p_header">diff --git a/mm/pagewalk.c b/mm/pagewalk.c</span>
<span class="p_header">index 8d2da5dec1e0..44a2507c94fd 100644</span>
<span class="p_header">--- a/mm/pagewalk.c</span>
<span class="p_header">+++ b/mm/pagewalk.c</span>
<span class="p_chunk">@@ -26,7 +26,7 @@</span> <span class="p_context"> static int walk_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 }
 
 static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,
<span class="p_del">-			  struct mm_walk *walk)</span>
<span class="p_add">+			  struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	pmd_t *pmd;
 	unsigned long next;
<span class="p_chunk">@@ -38,7 +38,7 @@</span> <span class="p_context"> static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,</span>
 		next = pmd_addr_end(addr, end);
 		if (pmd_none(*pmd) || !walk-&gt;vma) {
 			if (walk-&gt;pte_hole)
<span class="p_del">-				err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="p_add">+				err = walk-&gt;pte_hole(addr, next, walk, mmrange);</span>
 			if (err)
 				break;
 			continue;
<span class="p_chunk">@@ -48,7 +48,7 @@</span> <span class="p_context"> static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,</span>
 		 * needs to know about pmd_trans_huge() pmds
 		 */
 		if (walk-&gt;pmd_entry)
<span class="p_del">-			err = walk-&gt;pmd_entry(pmd, addr, next, walk);</span>
<span class="p_add">+			err = walk-&gt;pmd_entry(pmd, addr, next, walk, mmrange);</span>
 		if (err)
 			break;
 
<span class="p_chunk">@@ -71,7 +71,7 @@</span> <span class="p_context"> static int walk_pmd_range(pud_t *pud, unsigned long addr, unsigned long end,</span>
 }
 
 static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,
<span class="p_del">-			  struct mm_walk *walk)</span>
<span class="p_add">+			  struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	pud_t *pud;
 	unsigned long next;
<span class="p_chunk">@@ -83,7 +83,7 @@</span> <span class="p_context"> static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,</span>
 		next = pud_addr_end(addr, end);
 		if (pud_none(*pud) || !walk-&gt;vma) {
 			if (walk-&gt;pte_hole)
<span class="p_del">-				err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="p_add">+				err = walk-&gt;pte_hole(addr, next, walk, mmrange);</span>
 			if (err)
 				break;
 			continue;
<span class="p_chunk">@@ -106,7 +106,7 @@</span> <span class="p_context"> static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,</span>
 			goto again;
 
 		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)
<span class="p_del">-			err = walk_pmd_range(pud, addr, next, walk);</span>
<span class="p_add">+			err = walk_pmd_range(pud, addr, next, walk, mmrange);</span>
 		if (err)
 			break;
 	} while (pud++, addr = next, addr != end);
<span class="p_chunk">@@ -115,7 +115,7 @@</span> <span class="p_context"> static int walk_pud_range(p4d_t *p4d, unsigned long addr, unsigned long end,</span>
 }
 
 static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,
<span class="p_del">-			  struct mm_walk *walk)</span>
<span class="p_add">+			  struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	p4d_t *p4d;
 	unsigned long next;
<span class="p_chunk">@@ -126,13 +126,13 @@</span> <span class="p_context"> static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
 		next = p4d_addr_end(addr, end);
 		if (p4d_none_or_clear_bad(p4d)) {
 			if (walk-&gt;pte_hole)
<span class="p_del">-				err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="p_add">+				err = walk-&gt;pte_hole(addr, next, walk, mmrange);</span>
 			if (err)
 				break;
 			continue;
 		}
 		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)
<span class="p_del">-			err = walk_pud_range(p4d, addr, next, walk);</span>
<span class="p_add">+			err = walk_pud_range(p4d, addr, next, walk, mmrange);</span>
 		if (err)
 			break;
 	} while (p4d++, addr = next, addr != end);
<span class="p_chunk">@@ -141,7 +141,7 @@</span> <span class="p_context"> static int walk_p4d_range(pgd_t *pgd, unsigned long addr, unsigned long end,</span>
 }
 
 static int walk_pgd_range(unsigned long addr, unsigned long end,
<span class="p_del">-			  struct mm_walk *walk)</span>
<span class="p_add">+			  struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	pgd_t *pgd;
 	unsigned long next;
<span class="p_chunk">@@ -152,13 +152,13 @@</span> <span class="p_context"> static int walk_pgd_range(unsigned long addr, unsigned long end,</span>
 		next = pgd_addr_end(addr, end);
 		if (pgd_none_or_clear_bad(pgd)) {
 			if (walk-&gt;pte_hole)
<span class="p_del">-				err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="p_add">+				err = walk-&gt;pte_hole(addr, next, walk, mmrange);</span>
 			if (err)
 				break;
 			continue;
 		}
 		if (walk-&gt;pmd_entry || walk-&gt;pte_entry)
<span class="p_del">-			err = walk_p4d_range(pgd, addr, next, walk);</span>
<span class="p_add">+			err = walk_p4d_range(pgd, addr, next, walk, mmrange);</span>
 		if (err)
 			break;
 	} while (pgd++, addr = next, addr != end);
<span class="p_chunk">@@ -175,7 +175,7 @@</span> <span class="p_context"> static unsigned long hugetlb_entry_end(struct hstate *h, unsigned long addr,</span>
 }
 
 static int walk_hugetlb_range(unsigned long addr, unsigned long end,
<span class="p_del">-			      struct mm_walk *walk)</span>
<span class="p_add">+			      struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma = walk-&gt;vma;
 	struct hstate *h = hstate_vma(vma);
<span class="p_chunk">@@ -192,7 +192,7 @@</span> <span class="p_context"> static int walk_hugetlb_range(unsigned long addr, unsigned long end,</span>
 		if (pte)
 			err = walk-&gt;hugetlb_entry(pte, hmask, addr, next, walk);
 		else if (walk-&gt;pte_hole)
<span class="p_del">-			err = walk-&gt;pte_hole(addr, next, walk);</span>
<span class="p_add">+			err = walk-&gt;pte_hole(addr, next, walk, mmrange);</span>
 
 		if (err)
 			break;
<span class="p_chunk">@@ -203,7 +203,7 @@</span> <span class="p_context"> static int walk_hugetlb_range(unsigned long addr, unsigned long end,</span>
 
 #else /* CONFIG_HUGETLB_PAGE */
 static int walk_hugetlb_range(unsigned long addr, unsigned long end,
<span class="p_del">-			      struct mm_walk *walk)</span>
<span class="p_add">+			      struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	return 0;
 }
<span class="p_chunk">@@ -217,7 +217,7 @@</span> <span class="p_context"> static int walk_hugetlb_range(unsigned long addr, unsigned long end,</span>
  * error, where we abort the current walk.
  */
 static int walk_page_test(unsigned long start, unsigned long end,
<span class="p_del">-			struct mm_walk *walk)</span>
<span class="p_add">+			  struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	struct vm_area_struct *vma = walk-&gt;vma;
 
<span class="p_chunk">@@ -235,23 +235,23 @@</span> <span class="p_context"> static int walk_page_test(unsigned long start, unsigned long end,</span>
 	if (vma-&gt;vm_flags &amp; VM_PFNMAP) {
 		int err = 1;
 		if (walk-&gt;pte_hole)
<span class="p_del">-			err = walk-&gt;pte_hole(start, end, walk);</span>
<span class="p_add">+			err = walk-&gt;pte_hole(start, end, walk, mmrange);</span>
 		return err ? err : 1;
 	}
 	return 0;
 }
 
 static int __walk_page_range(unsigned long start, unsigned long end,
<span class="p_del">-			struct mm_walk *walk)</span>
<span class="p_add">+			     struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	int err = 0;
 	struct vm_area_struct *vma = walk-&gt;vma;
 
 	if (vma &amp;&amp; is_vm_hugetlb_page(vma)) {
 		if (walk-&gt;hugetlb_entry)
<span class="p_del">-			err = walk_hugetlb_range(start, end, walk);</span>
<span class="p_add">+			err = walk_hugetlb_range(start, end, walk, mmrange);</span>
 	} else
<span class="p_del">-		err = walk_pgd_range(start, end, walk);</span>
<span class="p_add">+		err = walk_pgd_range(start, end, walk, mmrange);</span>
 
 	return err;
 }
<span class="p_chunk">@@ -285,10 +285,11 @@</span> <span class="p_context"> static int __walk_page_range(unsigned long start, unsigned long end,</span>
  * Locking:
  *   Callers of walk_page_range() and walk_page_vma() should hold
  *   @walk-&gt;mm-&gt;mmap_sem, because these function traverse vma list and/or
<span class="p_del">- *   access to vma&#39;s data.</span>
<span class="p_add">+ *   access to vma&#39;s data. As such, the @mmrange will represent the</span>
<span class="p_add">+ *   address space range.</span>
  */
 int walk_page_range(unsigned long start, unsigned long end,
<span class="p_del">-		    struct mm_walk *walk)</span>
<span class="p_add">+		    struct mm_walk *walk, struct range_lock *mmrange)</span>
 {
 	int err = 0;
 	unsigned long next;
<span class="p_chunk">@@ -315,7 +316,7 @@</span> <span class="p_context"> int walk_page_range(unsigned long start, unsigned long end,</span>
 			next = min(end, vma-&gt;vm_end);
 			vma = vma-&gt;vm_next;
 
<span class="p_del">-			err = walk_page_test(start, next, walk);</span>
<span class="p_add">+			err = walk_page_test(start, next, walk, mmrange);</span>
 			if (err &gt; 0) {
 				/*
 				 * positive return values are purely for
<span class="p_chunk">@@ -329,14 +330,15 @@</span> <span class="p_context"> int walk_page_range(unsigned long start, unsigned long end,</span>
 				break;
 		}
 		if (walk-&gt;vma || walk-&gt;pte_hole)
<span class="p_del">-			err = __walk_page_range(start, next, walk);</span>
<span class="p_add">+			err = __walk_page_range(start, next, walk, mmrange);</span>
 		if (err)
 			break;
 	} while (start = next, start &lt; end);
 	return err;
 }
 
<span class="p_del">-int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk)</span>
<span class="p_add">+int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk,</span>
<span class="p_add">+		  struct range_lock *mmrange)</span>
 {
 	int err;
 
<span class="p_chunk">@@ -346,10 +348,10 @@</span> <span class="p_context"> int walk_page_vma(struct vm_area_struct *vma, struct mm_walk *walk)</span>
 	VM_BUG_ON(!rwsem_is_locked(&amp;walk-&gt;mm-&gt;mmap_sem));
 	VM_BUG_ON(!vma);
 	walk-&gt;vma = vma;
<span class="p_del">-	err = walk_page_test(vma-&gt;vm_start, vma-&gt;vm_end, walk);</span>
<span class="p_add">+	err = walk_page_test(vma-&gt;vm_start, vma-&gt;vm_end, walk, mmrange);</span>
 	if (err &gt; 0)
 		return 0;
 	if (err &lt; 0)
 		return err;
<span class="p_del">-	return __walk_page_range(vma-&gt;vm_start, vma-&gt;vm_end, walk);</span>
<span class="p_add">+	return __walk_page_range(vma-&gt;vm_start, vma-&gt;vm_end, walk, mmrange);</span>
 }
<span class="p_header">diff --git a/mm/process_vm_access.c b/mm/process_vm_access.c</span>
<span class="p_header">index a447092d4635..ff6772b86195 100644</span>
<span class="p_header">--- a/mm/process_vm_access.c</span>
<span class="p_header">+++ b/mm/process_vm_access.c</span>
<span class="p_chunk">@@ -90,6 +90,7 @@</span> <span class="p_context"> static int process_vm_rw_single_vec(unsigned long addr,</span>
 	unsigned long max_pages_per_loop = PVM_MAX_KMALLOC_PAGES
 		/ sizeof(struct pages *);
 	unsigned int flags = 0;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* Work out address and page range required */
 	if (len == 0)
<span class="p_chunk">@@ -111,7 +112,8 @@</span> <span class="p_context"> static int process_vm_rw_single_vec(unsigned long addr,</span>
 		 */
 		down_read(&amp;mm-&gt;mmap_sem);
 		pages = get_user_pages_remote(task, mm, pa, pages, flags,
<span class="p_del">-					      process_pages, NULL, &amp;locked);</span>
<span class="p_add">+					      process_pages, NULL, &amp;locked,</span>
<span class="p_add">+					      &amp;mmrange);</span>
 		if (locked)
 			up_read(&amp;mm-&gt;mmap_sem);
 		if (pages &lt;= 0)
<span class="p_header">diff --git a/mm/util.c b/mm/util.c</span>
<span class="p_header">index c1250501364f..b0ec1d88bb71 100644</span>
<span class="p_header">--- a/mm/util.c</span>
<span class="p_header">+++ b/mm/util.c</span>
<span class="p_chunk">@@ -347,13 +347,14 @@</span> <span class="p_context"> unsigned long vm_mmap_pgoff(struct file *file, unsigned long addr,</span>
 	struct mm_struct *mm = current-&gt;mm;
 	unsigned long populate;
 	LIST_HEAD(uf);
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	ret = security_mmap_file(file, prot, flag);
 	if (!ret) {
 		if (down_write_killable(&amp;mm-&gt;mmap_sem))
 			return -EINTR;
 		ret = do_mmap_pgoff(file, addr, len, prot, flag, pgoff,
<span class="p_del">-				    &amp;populate, &amp;uf);</span>
<span class="p_add">+				    &amp;populate, &amp;uf, &amp;mmrange);</span>
 		up_write(&amp;mm-&gt;mmap_sem);
 		userfaultfd_unmap_complete(mm, &amp;uf);
 		if (populate)
<span class="p_header">diff --git a/security/tomoyo/domain.c b/security/tomoyo/domain.c</span>
<span class="p_header">index f6758dad981f..c1e36ea2c6fc 100644</span>
<span class="p_header">--- a/security/tomoyo/domain.c</span>
<span class="p_header">+++ b/security/tomoyo/domain.c</span>
<span class="p_chunk">@@ -868,6 +868,7 @@</span> <span class="p_context"> bool tomoyo_dump_page(struct linux_binprm *bprm, unsigned long pos,</span>
 		      struct tomoyo_page_dump *dump)
 {
 	struct page *page;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange); /* see get_page_arg() in fs/exec.c */</span>
 
 	/* dump-&gt;data is released by tomoyo_find_next_domain(). */
 	if (!dump-&gt;data) {
<span class="p_chunk">@@ -884,7 +885,7 @@</span> <span class="p_context"> bool tomoyo_dump_page(struct linux_binprm *bprm, unsigned long pos,</span>
 	 * the execve().
 	 */
 	if (get_user_pages_remote(current, bprm-&gt;mm, pos, 1,
<span class="p_del">-				FOLL_FORCE, &amp;page, NULL, NULL) &lt;= 0)</span>
<span class="p_add">+				  FOLL_FORCE, &amp;page, NULL, NULL, &amp;mmrange) &lt;= 0)</span>
 		return false;
 #else
 	page = bprm-&gt;page[pos / PAGE_SIZE];
<span class="p_header">diff --git a/virt/kvm/async_pf.c b/virt/kvm/async_pf.c</span>
<span class="p_header">index 57bcb27dcf30..4cd2b93bb20c 100644</span>
<span class="p_header">--- a/virt/kvm/async_pf.c</span>
<span class="p_header">+++ b/virt/kvm/async_pf.c</span>
<span class="p_chunk">@@ -78,6 +78,7 @@</span> <span class="p_context"> static void async_pf_execute(struct work_struct *work)</span>
 	unsigned long addr = apf-&gt;addr;
 	gva_t gva = apf-&gt;gva;
 	int locked = 1;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	might_sleep();
 
<span class="p_chunk">@@ -88,7 +89,7 @@</span> <span class="p_context"> static void async_pf_execute(struct work_struct *work)</span>
 	 */
 	down_read(&amp;mm-&gt;mmap_sem);
 	get_user_pages_remote(NULL, mm, addr, 1, FOLL_WRITE, NULL, NULL,
<span class="p_del">-			&amp;locked);</span>
<span class="p_add">+			      &amp;locked, &amp;mmrange);</span>
 	if (locked)
 		up_read(&amp;mm-&gt;mmap_sem);
 
<span class="p_header">diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="p_header">index 4501e658e8d6..86ec078f4c3b 100644</span>
<span class="p_header">--- a/virt/kvm/kvm_main.c</span>
<span class="p_header">+++ b/virt/kvm/kvm_main.c</span>
<span class="p_chunk">@@ -1317,11 +1317,12 @@</span> <span class="p_context"> unsigned long kvm_vcpu_gfn_to_hva_prot(struct kvm_vcpu *vcpu, gfn_t gfn, bool *w</span>
 	return gfn_to_hva_memslot_prot(slot, gfn, writable);
 }
 
<span class="p_del">-static inline int check_user_page_hwpoison(unsigned long addr)</span>
<span class="p_add">+static inline int check_user_page_hwpoison(unsigned long addr,</span>
<span class="p_add">+					   struct range_lock *mmrange)</span>
 {
 	int rc, flags = FOLL_HWPOISON | FOLL_WRITE;
 
<span class="p_del">-	rc = get_user_pages(addr, 1, flags, NULL, NULL);</span>
<span class="p_add">+	rc = get_user_pages(addr, 1, flags, NULL, NULL, mmrange);</span>
 	return rc == -EHWPOISON;
 }
 
<span class="p_chunk">@@ -1411,7 +1412,8 @@</span> <span class="p_context"> static bool vma_is_valid(struct vm_area_struct *vma, bool write_fault)</span>
 static int hva_to_pfn_remapped(struct vm_area_struct *vma,
 			       unsigned long addr, bool *async,
 			       bool write_fault, bool *writable,
<span class="p_del">-			       kvm_pfn_t *p_pfn)</span>
<span class="p_add">+			       kvm_pfn_t *p_pfn,</span>
<span class="p_add">+			       struct range_lock *mmrange)</span>
 {
 	unsigned long pfn;
 	int r;
<span class="p_chunk">@@ -1425,7 +1427,7 @@</span> <span class="p_context"> static int hva_to_pfn_remapped(struct vm_area_struct *vma,</span>
 		bool unlocked = false;
 		r = fixup_user_fault(current, current-&gt;mm, addr,
 				     (write_fault ? FAULT_FLAG_WRITE : 0),
<span class="p_del">-				     &amp;unlocked);</span>
<span class="p_add">+				     &amp;unlocked, mmrange);</span>
 		if (unlocked)
 			return -EAGAIN;
 		if (r)
<span class="p_chunk">@@ -1477,6 +1479,7 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 	struct vm_area_struct *vma;
 	kvm_pfn_t pfn = 0;
 	int npages, r;
<span class="p_add">+	DEFINE_RANGE_LOCK_FULL(mmrange);</span>
 
 	/* we can do it either atomically or asynchronously, not both */
 	BUG_ON(atomic &amp;&amp; async);
<span class="p_chunk">@@ -1493,7 +1496,7 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 
 	down_read(&amp;current-&gt;mm-&gt;mmap_sem);
 	if (npages == -EHWPOISON ||
<span class="p_del">-	      (!async &amp;&amp; check_user_page_hwpoison(addr))) {</span>
<span class="p_add">+	    (!async &amp;&amp; check_user_page_hwpoison(addr, &amp;mmrange))) {</span>
 		pfn = KVM_PFN_ERR_HWPOISON;
 		goto exit;
 	}
<span class="p_chunk">@@ -1504,7 +1507,8 @@</span> <span class="p_context"> static kvm_pfn_t hva_to_pfn(unsigned long addr, bool atomic, bool *async,</span>
 	if (vma == NULL)
 		pfn = KVM_PFN_ERR_FAULT;
 	else if (vma-&gt;vm_flags &amp; (VM_IO | VM_PFNMAP)) {
<span class="p_del">-		r = hva_to_pfn_remapped(vma, addr, async, write_fault, writable, &amp;pfn);</span>
<span class="p_add">+		r = hva_to_pfn_remapped(vma, addr, async, write_fault, writable,</span>
<span class="p_add">+					&amp;pfn, &amp;mmrange);</span>
 		if (r == -EAGAIN)
 			goto retry;
 		if (r &lt; 0)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



