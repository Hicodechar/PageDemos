
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,for,4.15,09/14] Provide cpu_opv system call - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,for,4.15,09/14] Provide cpu_opv system call</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 12, 2017, 11:03 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171012230326.19984-10-mathieu.desnoyers@efficios.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10003189/mbox/"
   >mbox</a>
|
   <a href="/patch/10003189/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10003189/">/patch/10003189/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	19EF7602BF for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Oct 2017 23:05:45 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0862C28C13
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Oct 2017 23:05:45 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id F07C228CFD; Thu, 12 Oct 2017 23:05:44 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DE11E28C13
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Oct 2017 23:05:42 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757884AbdJLXFF (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 12 Oct 2017 19:05:05 -0400
Received: from mail.efficios.com ([167.114.142.141]:40725 &quot;EHLO
	mail.efficios.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1757726AbdJLXEQ (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 12 Oct 2017 19:04:16 -0400
Received: from localhost (localhost.localdomain [127.0.0.1])
	by mail.efficios.com (Postfix) with ESMTP id B439D340372;
	Thu, 12 Oct 2017 23:06:07 +0000 (UTC)
Received: from mail.efficios.com ([127.0.0.1])
	by localhost (evm-mail-1.efficios.com [127.0.0.1]) (amavisd-new,
	port 10032)
	with ESMTP id ZsNeuae_SNg0; Thu, 12 Oct 2017 23:05:52 +0000 (UTC)
Received: from localhost (localhost.localdomain [127.0.0.1])
	by mail.efficios.com (Postfix) with ESMTP id CC6A1340215;
	Thu, 12 Oct 2017 23:05:39 +0000 (UTC)
X-Virus-Scanned: amavisd-new at efficios.com
Received: from mail.efficios.com ([127.0.0.1])
	by localhost (evm-mail-1.efficios.com [127.0.0.1]) (amavisd-new,
	port 10026)
	with ESMTP id Ac6k2af1fGAo; Thu, 12 Oct 2017 23:05:39 +0000 (UTC)
Received: from thinkos.internal.efficios.com
	(cable-192.222.218.157.electronicbox.net [192.222.218.157])
	by mail.efficios.com (Postfix) with ESMTPSA id 784AE34035C;
	Thu, 12 Oct 2017 23:05:39 +0000 (UTC)
From: Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;
To: &quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;,
	Boqun Feng &lt;boqun.feng@gmail.com&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Paul Turner &lt;pjt@google.com&gt;, Andrew Hunter &lt;ahh@google.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;, Dave Watson &lt;davejwatson@fb.com&gt;,
	Josh Triplett &lt;josh@joshtriplett.org&gt;, Will Deacon &lt;will.deacon@arm.com&gt;
Cc: linux-kernel@vger.kernel.org,
	Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Andi Kleen &lt;andi@firstfloor.org&gt;, Chris Lameter &lt;cl@linux.com&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Ben Maurer &lt;bmaurer@fb.com&gt;,
	Steven Rostedt &lt;rostedt@goodmis.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Russell King &lt;linux@arm.linux.org.uk&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;, linux-api@vger.kernel.org
Subject: [RFC PATCH for 4.15 09/14] Provide cpu_opv system call
Date: Thu, 12 Oct 2017 19:03:21 -0400
Message-Id: &lt;20171012230326.19984-10-mathieu.desnoyers@efficios.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20171012230326.19984-1-mathieu.desnoyers@efficios.com&gt;
References: &lt;20171012230326.19984-1-mathieu.desnoyers@efficios.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Oct. 12, 2017, 11:03 p.m.</div>
<pre class="content">
This new cpu_opv system call executes a vector of operations on behalf
of user-space on a specific CPU with preemption disabled. It is inspired
from readv() and writev() system calls which take a &quot;struct iovec&quot; array
as argument.

The operations available are: comparison, memcpy, add, or, and, xor,
left shift, and right shift. The system call receives a CPU number from
user-space as argument, which is the CPU on which those operations need
to be performed. All preparation steps such as loading pointers, and
applying offsets to arrays, need to be performed by user-space before
invoking the system call. The &quot;comparison&quot; operation can be used to
check that the data used in the preparation step did not change between
preparation of system call inputs and operation execution within the
preempt-off critical section.

The reason why we require all pointer offsets to be calculated by
user-space beforehand is because we need to use get_user_pages_fast() to
first pin all pages touched by each operation. This takes care of
faulting-in the pages. Then, preemption is disabled, and the operations
are performed atomically with respect to other thread execution on that
CPU, without generating any page fault.

A maximum limit of 16 operations per cpu_opv syscall invocation is
enforced, so user-space cannot generate a too long preempt-off critical
section. Each operation is also limited a length of PAGE_SIZE bytes,
meaning that an operation can touch a maximum of 4 pages (memcpy: 2
pages for source, 2 pages for destination if addresses are not aligned
on page boundaries).

If the thread is not running on the requested CPU, a new
push_task_to_cpu() is invoked to migrate the task to the requested CPU.
If the requested CPU is not part of the cpus allowed mask of the thread,
the system call fails with EINVAL. After the migration has been
performed, preemption is disabled, and the current CPU number is checked
again and compared to the requested CPU number. If it still differs, it
means the scheduler migrated us away from that CPU. Return EAGAIN to
user-space in that case, and let user-space retry (either requesting the
same CPU number, or a different one, depending on the user-space
algorithm constraints).
<span class="signed-off-by">
Signed-off-by: Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
CC: &quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;
CC: Peter Zijlstra &lt;peterz@infradead.org&gt;
CC: Paul Turner &lt;pjt@google.com&gt;
CC: Thomas Gleixner &lt;tglx@linutronix.de&gt;
CC: Andrew Hunter &lt;ahh@google.com&gt;
CC: Andy Lutomirski &lt;luto@amacapital.net&gt;
CC: Andi Kleen &lt;andi@firstfloor.org&gt;
CC: Dave Watson &lt;davejwatson@fb.com&gt;
CC: Chris Lameter &lt;cl@linux.com&gt;
CC: Ingo Molnar &lt;mingo@redhat.com&gt;
CC: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
CC: Ben Maurer &lt;bmaurer@fb.com&gt;
CC: Steven Rostedt &lt;rostedt@goodmis.org&gt;
CC: Josh Triplett &lt;josh@joshtriplett.org&gt;
CC: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
CC: Andrew Morton &lt;akpm@linux-foundation.org&gt;
CC: Russell King &lt;linux@arm.linux.org.uk&gt;
CC: Catalin Marinas &lt;catalin.marinas@arm.com&gt;
CC: Will Deacon &lt;will.deacon@arm.com&gt;
CC: Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;
CC: Boqun Feng &lt;boqun.feng@gmail.com&gt;
CC: linux-api@vger.kernel.org
---
 MAINTAINERS                  |    7 +
 include/uapi/linux/cpu_opv.h |   93 ++++
 init/Kconfig                 |   14 +
 kernel/Makefile              |    1 +
 kernel/cpu_opv.c             | 1000 ++++++++++++++++++++++++++++++++++++++++++
 kernel/sched/core.c          |   37 ++
 kernel/sched/sched.h         |    2 +
 kernel/sys_ni.c              |    1 +
 8 files changed, 1155 insertions(+)
 create mode 100644 include/uapi/linux/cpu_opv.h
 create mode 100644 kernel/cpu_opv.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80821">One Thousand Gnomes</a> - Oct. 13, 2017, 1:57 p.m.</div>
<pre class="content">
<span class="quote">&gt; A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="quote">&gt; enforced, so user-space cannot generate a too long preempt-off critical</span>
<span class="quote">&gt; section. </span>

Except that all the operations could be going to mmapped I/O space and if
I pick the right targets could take quite a long time to complete. It&#39;s
still only 16 operations - But 160ms is a lot worse than 10ms. In fact
with compare_iter I could make it much much worse still as I get 2 x
TMP_BUFLEN x 16 x worst case latency in my attack. That&#39;s enough to screw
up plenty of things.

So it seems to me at minimum it needs to be restricted to genuine RAM user
pages, and in fact would be far far simpler code as well if it were
limited to a single page for a given invocation or if like futexes you
had to specifically create a per_cpu_opv mapping.

Alan
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Oct. 13, 2017, 2:50 p.m.</div>
<pre class="content">
----- On Oct 13, 2017, at 9:57 AM, One Thousand Gnomes gnomes@lxorguk.ukuu.org.uk wrote:
<span class="quote">
&gt;&gt; A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="quote">&gt;&gt; enforced, so user-space cannot generate a too long preempt-off critical</span>
<span class="quote">&gt;&gt; section.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Except that all the operations could be going to mmapped I/O space and if</span>
<span class="quote">&gt; I pick the right targets could take quite a long time to complete.</span>

We could check whether a struct page belongs to mmapped I/O space, and return
EINVAL in that case.
<span class="quote">
&gt; It&#39;s</span>
<span class="quote">&gt; still only 16 operations - But 160ms is a lot worse than 10ms. In fact</span>
<span class="quote">&gt; with compare_iter I could make it much much worse still as I get 2 x</span>
<span class="quote">&gt; TMP_BUFLEN x 16 x worst case latency in my attack. That&#39;s enough to screw</span>
<span class="quote">&gt; up plenty of things.</span>

Would a check that ensures the page is not mmapped I/O space be sufficient
to take care of this ? If happen to know which API I need to look for, it
would be welcome.
<span class="quote">
&gt; </span>
<span class="quote">&gt; So it seems to me at minimum it needs to be restricted to genuine RAM user</span>
<span class="quote">&gt; pages, and in fact would be far far simpler code as well if it were</span>
<span class="quote">&gt; limited to a single page for a given invocation or if like futexes you</span>
<span class="quote">&gt; had to specifically create a per_cpu_opv mapping.</span>

I&#39;ve had requests to implement per-cpu ring buffers with memcpy + offset
pointer update restartable sequences. Having a memcpy operation which does not
require page-alignment allows cpu_opv() to be used as a single-stepping
fallback for those use-cases.

I&#39;m open to consider simplifying the other operations such as compare, add,
bitwise ops, and shift ops by requiring that they target aligned content,
which would therefore fit within a single page. However, given that we already
want to support the unaligned memcpy operation, it does not add much extra
complexity to support unaligned accesses for the other cases. We could also
limit the &quot;compare&quot; operation to 1, 2, 4, 8 aligned bytes rather than being an
up-to-PAGE_SIZE compare, but it would limit its usefulness in case of structure
content comparison.

Thanks,

Mathieu
<span class="quote">

&gt; </span>
<span class="quote">&gt; Alan</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Oct. 13, 2017, 5:20 p.m.</div>
<pre class="content">
On Thu, Oct 12, 2017 at 4:03 PM, Mathieu Desnoyers
&lt;mathieu.desnoyers@efficios.com&gt; wrote:
<span class="quote">&gt; This new cpu_opv system call executes a vector of operations on behalf</span>
<span class="quote">&gt; of user-space on a specific CPU with preemption disabled. It is inspired</span>
<span class="quote">&gt; from readv() and writev() system calls which take a &quot;struct iovec&quot; array</span>
<span class="quote">&gt; as argument.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="quote">&gt; left shift, and right shift. The system call receives a CPU number from</span>
<span class="quote">&gt; user-space as argument, which is the CPU on which those operations need</span>
<span class="quote">&gt; to be performed. All preparation steps such as loading pointers, and</span>
<span class="quote">&gt; applying offsets to arrays, need to be performed by user-space before</span>
<span class="quote">&gt; invoking the system call. The &quot;comparison&quot; operation can be used to</span>
<span class="quote">&gt; check that the data used in the preparation step did not change between</span>
<span class="quote">&gt; preparation of system call inputs and operation execution within the</span>
<span class="quote">&gt; preempt-off critical section.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The reason why we require all pointer offsets to be calculated by</span>
<span class="quote">&gt; user-space beforehand is because we need to use get_user_pages_fast() to</span>
<span class="quote">&gt; first pin all pages touched by each operation. This takes care of</span>
<span class="quote">&gt; faulting-in the pages. Then, preemption is disabled, and the operations</span>
<span class="quote">&gt; are performed atomically with respect to other thread execution on that</span>
<span class="quote">&gt; CPU, without generating any page fault.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="quote">&gt; enforced, so user-space cannot generate a too long preempt-off critical</span>
<span class="quote">&gt; section. Each operation is also limited a length of PAGE_SIZE bytes,</span>
<span class="quote">&gt; meaning that an operation can touch a maximum of 4 pages (memcpy: 2</span>
<span class="quote">&gt; pages for source, 2 pages for destination if addresses are not aligned</span>
<span class="quote">&gt; on page boundaries).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If the thread is not running on the requested CPU, a new</span>
<span class="quote">&gt; push_task_to_cpu() is invoked to migrate the task to the requested CPU.</span>
<span class="quote">&gt; If the requested CPU is not part of the cpus allowed mask of the thread,</span>
<span class="quote">&gt; the system call fails with EINVAL. After the migration has been</span>
<span class="quote">&gt; performed, preemption is disabled, and the current CPU number is checked</span>
<span class="quote">&gt; again and compared to the requested CPU number. If it still differs, it</span>
<span class="quote">&gt; means the scheduler migrated us away from that CPU. Return EAGAIN to</span>
<span class="quote">&gt; user-space in that case, and let user-space retry (either requesting the</span>
<span class="quote">&gt; same CPU number, or a different one, depending on the user-space</span>
<span class="quote">&gt; algorithm constraints).</span>

This series seems to get more complicated every time, and it&#39;s been so
long that I&#39;ve mostly forgetten all the details.  I would have sworn
we had a solution that got single-stepping right without any
complicated work like this in the kernel and had at most a minor
performance hit relative to the absolutely fastest solution.  I&#39;ll try
to dig it up.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=104">Andi Kleen</a> - Oct. 14, 2017, 2:50 a.m.</div>
<pre class="content">
<span class="quote">&gt; +	pagefault_disable();</span>
<span class="quote">&gt; +	switch (len) {</span>
<span class="quote">&gt; +	case 1:</span>
<span class="quote">&gt; +		if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		tmp._u8 += (uint8_t)count;</span>
<span class="quote">&gt; +		if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="quote">&gt; +			goto end;</span>
<span class="quote">&gt; +		break;</span>

It seems the code size could be dramatically shrunk by using a function
pointer callback for the actual operation, and then avoiding all the
copies. Since this is only a fallback this shouldn&#39;t be a problem
(and modern branch predictors are fairly good in such situations anyways)

If you really want to keep the code it would be better if you used
some macros -- i bet there&#39;s a typo in here somewhere in this
forest.

-Andi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Oct. 14, 2017, 1:35 p.m.</div>
<pre class="content">
----- On Oct 13, 2017, at 10:50 PM, Andi Kleen andi@firstfloor.org wrote:
<span class="quote">
&gt;&gt; +	pagefault_disable();</span>
<span class="quote">&gt;&gt; +	switch (len) {</span>
<span class="quote">&gt;&gt; +	case 1:</span>
<span class="quote">&gt;&gt; +		if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		tmp._u8 += (uint8_t)count;</span>
<span class="quote">&gt;&gt; +		if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="quote">&gt;&gt; +			goto end;</span>
<span class="quote">&gt;&gt; +		break;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It seems the code size could be dramatically shrunk by using a function</span>
<span class="quote">&gt; pointer callback for the actual operation, and then avoiding all the</span>
<span class="quote">&gt; copies. Since this is only a fallback this shouldn&#39;t be a problem</span>
<span class="quote">&gt; (and modern branch predictors are fairly good in such situations anyways)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you really want to keep the code it would be better if you used</span>
<span class="quote">&gt; some macros -- i bet there&#39;s a typo in here somewhere in this</span>
<span class="quote">&gt; forest.</span>

Good point!

I&#39;ll add this in the next round. Meanwhile, here is the resulting
commit: https://github.com/compudj/linux-percpu-dev/commit/864b444f64f4c227ddc587d12631ff0d3440796c

Thanks,

Mathieu
<span class="quote">

&gt; </span>
<span class="quote">&gt; -Andi</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=6835">Mathieu Desnoyers</a> - Oct. 14, 2017, 2:22 p.m.</div>
<pre class="content">
----- On Oct 13, 2017, at 10:50 AM, Mathieu Desnoyers mathieu.desnoyers@efficios.com wrote:
<span class="quote">
&gt; ----- On Oct 13, 2017, at 9:57 AM, One Thousand Gnomes</span>
<span class="quote">&gt; gnomes@lxorguk.ukuu.org.uk wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="quote">&gt;&gt;&gt; enforced, so user-space cannot generate a too long preempt-off critical</span>
<span class="quote">&gt;&gt;&gt; section.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Except that all the operations could be going to mmapped I/O space and if</span>
<span class="quote">&gt;&gt; I pick the right targets could take quite a long time to complete.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We could check whether a struct page belongs to mmapped I/O space, and return</span>
<span class="quote">&gt; EINVAL in that case.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; It&#39;s</span>
<span class="quote">&gt;&gt; still only 16 operations - But 160ms is a lot worse than 10ms. In fact</span>
<span class="quote">&gt;&gt; with compare_iter I could make it much much worse still as I get 2 x</span>
<span class="quote">&gt;&gt; TMP_BUFLEN x 16 x worst case latency in my attack. That&#39;s enough to screw</span>
<span class="quote">&gt;&gt; up plenty of things.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Would a check that ensures the page is not mmapped I/O space be sufficient</span>
<span class="quote">&gt; to take care of this ? If happen to know which API I need to look for, it</span>
<span class="quote">&gt; would be welcome.</span>

I think is_zone_device_page() is what I was looking for.

Let me know if I missed something,

Thanks,

Mathieu
<span class="quote">
&gt; Thanks,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Mathieu</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Alan</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; Mathieu Desnoyers</span>
<span class="quote">&gt; EfficiOS Inc.</span>
<span class="quote">&gt; http://www.efficios.com</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="p_header">index 9d6a830a8c32..6a5f3afb2ea4 100644</span>
<span class="p_header">--- a/MAINTAINERS</span>
<span class="p_header">+++ b/MAINTAINERS</span>
<span class="p_chunk">@@ -3611,6 +3611,13 @@</span> <span class="p_context"> B:	https://bugzilla.kernel.org</span>
 F:	drivers/cpuidle/*
 F:	include/linux/cpuidle.h
 
<span class="p_add">+CPU NON-PREEMPTIBLE OPERATION VECTOR SUPPORT</span>
<span class="p_add">+M:	Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="p_add">+L:	linux-kernel@vger.kernel.org</span>
<span class="p_add">+S:	Supported</span>
<span class="p_add">+F:	kernel/cpu_opv.c</span>
<span class="p_add">+F:	include/uapi/linux/cpu_opv.h</span>
<span class="p_add">+</span>
 CRAMFS FILESYSTEM
 W:	http://sourceforge.net/projects/cramfs/
 S:	Orphan / Obsolete
<span class="p_header">diff --git a/include/uapi/linux/cpu_opv.h b/include/uapi/linux/cpu_opv.h</span>
new file mode 100644
<span class="p_header">index 000000000000..a3fcdebd063b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/uapi/linux/cpu_opv.h</span>
<span class="p_chunk">@@ -0,0 +1,93 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _UAPI_LINUX_CPU_OPV_H</span>
<span class="p_add">+#define _UAPI_LINUX_CPU_OPV_H</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * linux/cpu_opv.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * CPU preempt-off operation vector system call API</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (c) 2017 Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Permission is hereby granted, free of charge, to any person obtaining a copy</span>
<span class="p_add">+ * of this software and associated documentation files (the &quot;Software&quot;), to deal</span>
<span class="p_add">+ * in the Software without restriction, including without limitation the rights</span>
<span class="p_add">+ * to use, copy, modify, merge, publish, distribute, sublicense, and/or sell</span>
<span class="p_add">+ * copies of the Software, and to permit persons to whom the Software is</span>
<span class="p_add">+ * furnished to do so, subject to the following conditions:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The above copyright notice and this permission notice shall be included in</span>
<span class="p_add">+ * all copies or substantial portions of the Software.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR</span>
<span class="p_add">+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,</span>
<span class="p_add">+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE</span>
<span class="p_add">+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER</span>
<span class="p_add">+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,</span>
<span class="p_add">+ * OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE</span>
<span class="p_add">+ * SOFTWARE.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+# include &lt;linux/types.h&gt;</span>
<span class="p_add">+#else	/* #ifdef __KERNEL__ */</span>
<span class="p_add">+# include &lt;stdint.h&gt;</span>
<span class="p_add">+#endif	/* #else #ifdef __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __LP64__</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64(field)	uint64_t field</span>
<span class="p_add">+#elif defined(__BYTE_ORDER) ? \</span>
<span class="p_add">+	__BYTE_ORDER == __BIG_ENDIAN : defined(__BIG_ENDIAN)</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64(field)	uint32_t _padding ## field, field</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define CPU_OP_FIELD_u32_u64(field)	uint32_t field, _padding ## field</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define CPU_OP_VEC_LEN_MAX		16</span>
<span class="p_add">+#define CPU_OP_ARG_LEN_MAX		24</span>
<span class="p_add">+#define CPU_OP_DATA_LEN_MAX		PAGE_SIZE</span>
<span class="p_add">+#define CPU_OP_MAX_PAGES		4	/* Max. pages per op. */</span>
<span class="p_add">+</span>
<span class="p_add">+enum cpu_op_type {</span>
<span class="p_add">+	CPU_COMPARE_EQ_OP,	/* compare */</span>
<span class="p_add">+	CPU_COMPARE_NE_OP,	/* compare */</span>
<span class="p_add">+	CPU_MEMCPY_OP,		/* memcpy */</span>
<span class="p_add">+	CPU_ADD_OP,		/* arithmetic */</span>
<span class="p_add">+	CPU_OR_OP,		/* bitwise */</span>
<span class="p_add">+	CPU_AND_OP,		/* bitwise */</span>
<span class="p_add">+	CPU_XOR_OP,		/* bitwise */</span>
<span class="p_add">+	CPU_LSHIFT_OP,		/* shift */</span>
<span class="p_add">+	CPU_RSHIFT_OP,		/* shift */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* Vector of operations to perform. Limited to 16. */</span>
<span class="p_add">+struct cpu_op {</span>
<span class="p_add">+	int32_t op;	/* enum cpu_op_type. */</span>
<span class="p_add">+	uint32_t len;	/* data length, in bytes. */</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(a);</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(b);</span>
<span class="p_add">+		} compare_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(dst);</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(src);</span>
<span class="p_add">+		} memcpy_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(p);</span>
<span class="p_add">+			int64_t count;</span>
<span class="p_add">+		} arithmetic_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(p);</span>
<span class="p_add">+			uint64_t mask;</span>
<span class="p_add">+		} bitwise_op;</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			CPU_OP_FIELD_u32_u64(p);</span>
<span class="p_add">+			uint32_t bits;</span>
<span class="p_add">+		} shift_op;</span>
<span class="p_add">+		char __padding[CPU_OP_ARG_LEN_MAX];</span>
<span class="p_add">+	} u;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_LINUX_CPU_OPV_H */</span>
<span class="p_header">diff --git a/init/Kconfig b/init/Kconfig</span>
<span class="p_header">index b8aa41bd4f4f..98b79eb9020e 100644</span>
<span class="p_header">--- a/init/Kconfig</span>
<span class="p_header">+++ b/init/Kconfig</span>
<span class="p_chunk">@@ -1399,6 +1399,7 @@</span> <span class="p_context"> config RSEQ</span>
 	bool &quot;Enable rseq() system call&quot; if EXPERT
 	default y
 	depends on HAVE_RSEQ
<span class="p_add">+	select CPU_OPV</span>
 	help
 	  Enable the restartable sequences system call. It provides a
 	  user-space cache for the current CPU number value, which
<span class="p_chunk">@@ -1408,6 +1409,19 @@</span> <span class="p_context"> config RSEQ</span>
 
 	  If unsure, say Y.
 
<span class="p_add">+config CPU_OPV</span>
<span class="p_add">+	bool &quot;Enable cpu_opv() system call&quot; if EXPERT</span>
<span class="p_add">+	default y</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Enable the CPU preempt-off operation vector system call.</span>
<span class="p_add">+	  It allows user-space to perform a sequence of operations on</span>
<span class="p_add">+	  per-cpu data with preemption disabled. Useful as</span>
<span class="p_add">+	  single-stepping fall-back for restartable sequences, and for</span>
<span class="p_add">+	  performing more complex operations on per-cpu data that would</span>
<span class="p_add">+	  not be otherwise possible to do with restartable sequences.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If unsure, say Y.</span>
<span class="p_add">+</span>
 config EMBEDDED
 	bool &quot;Embedded system&quot;
 	option allnoconfig_y
<span class="p_header">diff --git a/kernel/Makefile b/kernel/Makefile</span>
<span class="p_header">index 5c09592b3b9f..8301e454c2a8 100644</span>
<span class="p_header">--- a/kernel/Makefile</span>
<span class="p_header">+++ b/kernel/Makefile</span>
<span class="p_chunk">@@ -112,6 +112,7 @@</span> <span class="p_context"> obj-$(CONFIG_MEMBARRIER) += membarrier.o</span>
 
 obj-$(CONFIG_HAS_IOMEM) += memremap.o
 obj-$(CONFIG_RSEQ) += rseq.o
<span class="p_add">+obj-$(CONFIG_CPU_OPV) += cpu_opv.o</span>
 
 $(obj)/configs.o: $(obj)/config_data.h
 
<span class="p_header">diff --git a/kernel/cpu_opv.c b/kernel/cpu_opv.c</span>
new file mode 100644
<span class="p_header">index 000000000000..2e615612acb1</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/kernel/cpu_opv.c</span>
<span class="p_chunk">@@ -0,0 +1,1000 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * CPU preempt-off operation vector system call</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * It allows user-space to perform a sequence of operations on per-cpu</span>
<span class="p_add">+ * data with preemption disabled. Useful as single-stepping fall-back</span>
<span class="p_add">+ * for restartable sequences, and for performing more complex operations</span>
<span class="p_add">+ * on per-cpu data that would not be otherwise possible to do with</span>
<span class="p_add">+ * restartable sequences.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2017, EfficiOS Inc.,</span>
<span class="p_add">+ * Mathieu Desnoyers &lt;mathieu.desnoyers@efficios.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;linux/syscalls.h&gt;</span>
<span class="p_add">+#include &lt;linux/cpu_opv.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;sched/sched.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#define TMP_BUFLEN			64</span>
<span class="p_add">+#define NR_PINNED_PAGES_ON_STACK	8</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The cpu_opv system call executes a vector of operations on behalf of</span>
<span class="p_add">+ * user-space on a specific CPU with preemption disabled. It is inspired</span>
<span class="p_add">+ * from readv() and writev() system calls which take a &quot;struct iovec&quot;</span>
<span class="p_add">+ * array as argument.</span>
<span class="p_add">+ * </span>
<span class="p_add">+ * The operations available are: comparison, memcpy, add, or, and, xor,</span>
<span class="p_add">+ * left shift, and right shift. The system call receives a CPU number</span>
<span class="p_add">+ * from user-space as argument, which is the CPU on which those</span>
<span class="p_add">+ * operations need to be performed. All preparation steps such as</span>
<span class="p_add">+ * loading pointers, and applying offsets to arrays, need to be</span>
<span class="p_add">+ * performed by user-space before invoking the system call. The</span>
<span class="p_add">+ * &quot;comparison&quot; operation can be used to check that the data used in the</span>
<span class="p_add">+ * preparation step did not change between preparation of system call</span>
<span class="p_add">+ * inputs and operation execution within the preempt-off critical</span>
<span class="p_add">+ * section.</span>
<span class="p_add">+ * </span>
<span class="p_add">+ * The reason why we require all pointer offsets to be calculated by</span>
<span class="p_add">+ * user-space beforehand is because we need to use get_user_pages_fast()</span>
<span class="p_add">+ * to first pin all pages touched by each operation. This takes care of</span>
<span class="p_add">+ * faulting-in the pages. Then, preemption is disabled, and the</span>
<span class="p_add">+ * operations are performed atomically with respect to other thread</span>
<span class="p_add">+ * execution on that CPU, without generating any page fault.</span>
<span class="p_add">+ * </span>
<span class="p_add">+ * A maximum limit of 16 operations per cpu_opv syscall invocation is</span>
<span class="p_add">+ * enforced, so user-space cannot generate a too long preempt-off</span>
<span class="p_add">+ * critical section. Each operation is also limited a length of</span>
<span class="p_add">+ * PAGE_SIZE bytes, meaning that an operation can touch a maximum of 4</span>
<span class="p_add">+ * pages (memcpy: 2 pages for source, 2 pages for destination if</span>
<span class="p_add">+ * addresses are not aligned on page boundaries).</span>
<span class="p_add">+ * </span>
<span class="p_add">+ * If the thread is not running on the requested CPU, a new</span>
<span class="p_add">+ * push_task_to_cpu() is invoked to migrate the task to the requested</span>
<span class="p_add">+ * CPU.  If the requested CPU is not part of the cpus allowed mask of</span>
<span class="p_add">+ * the thread, the system call fails with EINVAL. After the migration</span>
<span class="p_add">+ * has been performed, preemption is disabled, and the current CPU</span>
<span class="p_add">+ * number is checked again and compared to the requested CPU number. If</span>
<span class="p_add">+ * it still differs, it means the scheduler migrated us away from that</span>
<span class="p_add">+ * CPU. Return EAGAIN to user-space in that case, and let user-space</span>
<span class="p_add">+ * retry (either requesting the same CPU number, or a different one,</span>
<span class="p_add">+ * depending on the user-space algorithm constraints).</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Check operation types and length parameters.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int cpu_opv_check(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="p_add">+		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_COMPARE_EQ_OP:</span>
<span class="p_add">+		case CPU_COMPARE_NE_OP:</span>
<span class="p_add">+		case CPU_MEMCPY_OP:</span>
<span class="p_add">+			if (op-&gt;len &gt; CPU_OP_DATA_LEN_MAX)</span>
<span class="p_add">+				return -EINVAL;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_ADD_OP:</span>
<span class="p_add">+		case CPU_OR_OP:</span>
<span class="p_add">+		case CPU_AND_OP:</span>
<span class="p_add">+		case CPU_XOR_OP:</span>
<span class="p_add">+			switch (op-&gt;len) {</span>
<span class="p_add">+			case 1:</span>
<span class="p_add">+			case 2:</span>
<span class="p_add">+			case 4:</span>
<span class="p_add">+			case 8:</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			default:</span>
<span class="p_add">+				return -EINVAL;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_LSHIFT_OP:</span>
<span class="p_add">+		case CPU_RSHIFT_OP:</span>
<span class="p_add">+			switch (op-&gt;len) {</span>
<span class="p_add">+			case 1:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 7)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			case 2:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 15)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			case 4:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 31)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			case 8:</span>
<span class="p_add">+				if (op-&gt;u.shift_op.bits &gt; 63)</span>
<span class="p_add">+					return -EINVAL;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			default:</span>
<span class="p_add">+				return -EINVAL;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long cpu_op_range_nr_pages(unsigned long addr,</span>
<span class="p_add">+		unsigned long len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ((addr + len - 1) &gt;&gt; PAGE_SHIFT) - (addr &gt;&gt; PAGE_SHIFT) + 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpu_op_pin_pages(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		struct page ***pinned_pages_ptr, size_t *nr_pinned)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long nr_pages;</span>
<span class="p_add">+	struct page *pages[2];</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!len)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	nr_pages = cpu_op_range_nr_pages(addr, len);</span>
<span class="p_add">+	BUG_ON(nr_pages &gt; 2);</span>
<span class="p_add">+	if (*nr_pinned + nr_pages &gt; NR_PINNED_PAGES_ON_STACK) {</span>
<span class="p_add">+		struct page **pinned_pages =</span>
<span class="p_add">+			kzalloc(CPU_OP_VEC_LEN_MAX * CPU_OP_MAX_PAGES</span>
<span class="p_add">+				* sizeof(struct page *), GFP_KERNEL);</span>
<span class="p_add">+		if (!pinned_pages)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		memcpy(pinned_pages, *pinned_pages_ptr,</span>
<span class="p_add">+			*nr_pinned * sizeof(struct page *));</span>
<span class="p_add">+		*pinned_pages_ptr = pinned_pages;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = get_user_pages_fast(addr, nr_pages, 0, pages);</span>
<span class="p_add">+	if (ret &lt; nr_pages) {</span>
<span class="p_add">+		if (ret &gt; 0)</span>
<span class="p_add">+			put_page(pages[0]);</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	(*pinned_pages_ptr)[(*nr_pinned)++] = pages[0];</span>
<span class="p_add">+	if (nr_pages &gt; 1)</span>
<span class="p_add">+		(*pinned_pages_ptr)[(*nr_pinned)++] = pages[1];</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int cpu_opv_pin_pages(struct cpu_op *cpuop, int cpuopcnt,</span>
<span class="p_add">+		struct page ***pinned_pages_ptr, size_t *nr_pinned)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret, i;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check access, pin pages. */</span>
<span class="p_add">+	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="p_add">+		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_COMPARE_EQ_OP:</span>
<span class="p_add">+		case CPU_COMPARE_NE_OP:</span>
<span class="p_add">+			if (!access_ok(VERIFY_READ, op-&gt;u.compare_op.a,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.compare_op.a,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			if (!access_ok(VERIFY_READ, op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MEMCPY_OP:</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE, op-&gt;u.memcpy_op.dst,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.memcpy_op.dst,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			if (!access_ok(VERIFY_READ, op-&gt;u.memcpy_op.src,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.memcpy_op.src,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_ADD_OP:</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE, op-&gt;u.arithmetic_op.p,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.arithmetic_op.p,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_OR_OP:</span>
<span class="p_add">+		case CPU_AND_OP:</span>
<span class="p_add">+		case CPU_XOR_OP:</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE, op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_LSHIFT_OP:</span>
<span class="p_add">+		case CPU_RSHIFT_OP:</span>
<span class="p_add">+			if (!access_ok(VERIFY_WRITE, op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;len))</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			ret = cpu_op_pin_pages(</span>
<span class="p_add">+					(unsigned long)op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;len, pinned_pages_ptr, nr_pinned);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				goto error;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+error:</span>
<span class="p_add">+	for (i = 0; i &lt; *nr_pinned; i++)</span>
<span class="p_add">+		put_page((*pinned_pages_ptr)[i]);</span>
<span class="p_add">+	*nr_pinned = 0;</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_compare_iter(void __user *a, void __user *b, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char bufa[TMP_BUFLEN], bufb[TMP_BUFLEN];</span>
<span class="p_add">+	uint32_t compared = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (compared != len) {</span>
<span class="p_add">+		unsigned long to_compare;</span>
<span class="p_add">+</span>
<span class="p_add">+		to_compare = min_t(uint32_t, TMP_BUFLEN, len - compared);</span>
<span class="p_add">+		if (__copy_from_user_inatomic(bufa, a + compared, to_compare))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		if (__copy_from_user_inatomic(bufb, b + compared, to_compare))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		if (memcmp(bufa, bufb, to_compare))</span>
<span class="p_add">+			return 1;	/* different */</span>
<span class="p_add">+		compared += to_compare;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;	/* same */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 if same, &gt; 0 if different, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_compare(void __user *a, void __user *b, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp[2];</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp[0]._u8, (uint8_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u8, (uint8_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		ret = !!(tmp[0]._u8 != tmp[1]._u8);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp[0]._u16, (uint16_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u16, (uint16_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		ret = !!(tmp[0]._u16 != tmp[1]._u16);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp[0]._u32, (uint32_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u32, (uint32_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		ret = !!(tmp[0]._u32 != tmp[1]._u32);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp[0]._u64, (uint64_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u64, (uint64_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp[0]._u64_split[0], (uint32_t __user *)a))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[0]._u64_split[1], (uint32_t __user *)a + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u64_split[0], (uint32_t __user *)b))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp[1]._u64_split[1], (uint32_t __user *)b + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		ret = !!(tmp[0]._u64 != tmp[1]._u64);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		pagefault_enable();</span>
<span class="p_add">+		return do_cpu_op_compare_iter(a, b, len);</span>
<span class="p_add">+	}</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_memcpy_iter(void __user *dst, void __user *src,</span>
<span class="p_add">+		uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char buf[TMP_BUFLEN];</span>
<span class="p_add">+	uint32_t copied = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (copied != len) {</span>
<span class="p_add">+		unsigned long to_copy;</span>
<span class="p_add">+</span>
<span class="p_add">+		to_copy = min_t(uint32_t, TMP_BUFLEN, len - copied);</span>
<span class="p_add">+		if (__copy_from_user_inatomic(buf, src + copied, to_copy))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		if (__copy_to_user_inatomic(dst + copied, buf, to_copy))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+		copied += to_copy;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_memcpy(void __user *dst, void __user *src, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)src))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)src + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)dst))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)dst + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		pagefault_enable();</span>
<span class="p_add">+		return do_cpu_op_memcpy_iter(dst, src, len);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_add(void __user *p, int64_t count, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u8 += (uint8_t)count;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u16 += (uint16_t)count;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u32 += (uint32_t)count;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		tmp._u64 += (uint64_t)count;</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_or(void __user *p, uint64_t mask, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u8 |= (uint8_t)mask;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u16 |= (uint16_t)mask;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u32 |= (uint32_t)mask;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		tmp._u64 |= (uint64_t)mask;</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_and(void __user *p, uint64_t mask, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u8 &amp;= (uint8_t)mask;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u16 &amp;= (uint16_t)mask;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u32 &amp;= (uint32_t)mask;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		tmp._u64 &amp;= (uint64_t)mask;</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_xor(void __user *p, uint64_t mask, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u8 ^= (uint8_t)mask;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u16 ^= (uint16_t)mask;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u32 ^= (uint32_t)mask;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		tmp._u64 ^= (uint64_t)mask;</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_lshift(void __user *p, uint32_t bits, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u8 &lt;&lt;= bits;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u16 &lt;&lt;= bits;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u32 &lt;&lt;= bits;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		tmp._u64 &lt;&lt;= bits;</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return 0 on success, &lt; 0 on error. */</span>
<span class="p_add">+static int do_cpu_op_rshift(void __user *p, uint32_t bits, uint32_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		uint8_t _u8;</span>
<span class="p_add">+		uint16_t _u16;</span>
<span class="p_add">+		uint32_t _u32;</span>
<span class="p_add">+		uint64_t _u64;</span>
<span class="p_add">+#if (BITS_PER_LONG &lt; 64)</span>
<span class="p_add">+		uint32_t _u64_split[2];</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	} tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagefault_disable();</span>
<span class="p_add">+	switch (len) {</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		if (__get_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u8 &gt;&gt;= bits;</span>
<span class="p_add">+		if (__put_user(tmp._u8, (uint8_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		if (__get_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u16 &gt;&gt;= bits;</span>
<span class="p_add">+		if (__put_user(tmp._u16, (uint16_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		if (__get_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		tmp._u32 &gt;&gt;= bits;</span>
<span class="p_add">+		if (__put_user(tmp._u32, (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 8:</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__get_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__get_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		tmp._u64 &gt;&gt;= bits;</span>
<span class="p_add">+#if (BITS_PER_LONG &gt;= 64)</span>
<span class="p_add">+		if (__put_user(tmp._u64, (uint64_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[0], (uint32_t __user *)p))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+		if (__put_user(tmp._u64_split[1], (uint32_t __user *)p + 1))</span>
<span class="p_add">+			goto end;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+end:</span>
<span class="p_add">+	pagefault_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; cpuopcnt; i++) {</span>
<span class="p_add">+		struct cpu_op *op = &amp;cpuop[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (op-&gt;op) {</span>
<span class="p_add">+		case CPU_COMPARE_EQ_OP:</span>
<span class="p_add">+			ret = do_cpu_op_compare(</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret &lt; 0)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Stop execution, return op index + 1 if comparison</span>
<span class="p_add">+			 * differs.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (ret &gt; 0)</span>
<span class="p_add">+				return i + 1;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_COMPARE_NE_OP:</span>
<span class="p_add">+			ret = do_cpu_op_compare(</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.a,</span>
<span class="p_add">+					(void __user *)op-&gt;u.compare_op.b,</span>
<span class="p_add">+					op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret &lt; 0)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Stop execution, return op index + 1 if comparison</span>
<span class="p_add">+			 * is identical.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (ret == 0)</span>
<span class="p_add">+				return i + 1;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_MEMCPY_OP:</span>
<span class="p_add">+			ret = do_cpu_op_memcpy(</span>
<span class="p_add">+					(void __user *)op-&gt;u.memcpy_op.dst,</span>
<span class="p_add">+					(void __user *)op-&gt;u.memcpy_op.src,</span>
<span class="p_add">+					op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_ADD_OP:</span>
<span class="p_add">+			ret = do_cpu_op_add((void __user *)op-&gt;u.arithmetic_op.p,</span>
<span class="p_add">+					op-&gt;u.arithmetic_op.count, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_OR_OP:</span>
<span class="p_add">+			ret = do_cpu_op_or((void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_AND_OP:</span>
<span class="p_add">+			ret = do_cpu_op_and((void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_XOR_OP:</span>
<span class="p_add">+			ret = do_cpu_op_xor((void __user *)op-&gt;u.bitwise_op.p,</span>
<span class="p_add">+					op-&gt;u.bitwise_op.mask, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_LSHIFT_OP:</span>
<span class="p_add">+			ret = do_cpu_op_lshift((void __user *)op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;u.shift_op.bits, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case CPU_RSHIFT_OP:</span>
<span class="p_add">+			ret = do_cpu_op_rshift((void __user *)op-&gt;u.shift_op.p,</span>
<span class="p_add">+					op-&gt;u.shift_op.bits, op-&gt;len);</span>
<span class="p_add">+			/* Stop execution on error. */</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int do_cpu_opv(struct cpu_op *cpuop, int cpuopcnt, int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cpu != raw_smp_processor_id()) {</span>
<span class="p_add">+		ret = push_task_to_cpu(current, cpu);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	if (cpu != smp_processor_id()) {</span>
<span class="p_add">+		ret = -EAGAIN;</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = __do_cpu_opv(cpuop, cpuopcnt);</span>
<span class="p_add">+end:</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * cpu_opv - execute operation vector on a given CPU with preempt off.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Userspace should pass current CPU number as parameter. May fail with</span>
<span class="p_add">+ * -EAGAIN if currently executing on the wrong CPU.</span>
<span class="p_add">+ */</span>
<span class="p_add">+SYSCALL_DEFINE4(cpu_opv, struct cpu_op __user *, ucpuopv, int, cpuopcnt,</span>
<span class="p_add">+		int, cpu, int, flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpu_op cpuopv[CPU_OP_VEC_LEN_MAX];</span>
<span class="p_add">+	struct page *pinned_pages_on_stack[NR_PINNED_PAGES_ON_STACK];</span>
<span class="p_add">+	struct page **pinned_pages = pinned_pages_on_stack;</span>
<span class="p_add">+	int ret, i;</span>
<span class="p_add">+	size_t nr_pinned = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(flags))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (unlikely(cpu &lt; 0))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (cpuopcnt &lt; 0 || cpuopcnt &gt; CPU_OP_VEC_LEN_MAX)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (copy_from_user(cpuopv, ucpuopv, cpuopcnt * sizeof(struct cpu_op)))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	ret = cpu_opv_check(cpuopv, cpuopcnt);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	ret = cpu_opv_pin_pages(cpuopv, cpuopcnt,</span>
<span class="p_add">+				&amp;pinned_pages, &amp;nr_pinned);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		goto end;</span>
<span class="p_add">+	ret = do_cpu_opv(cpuopv, cpuopcnt, cpu);</span>
<span class="p_add">+	for (i = 0; i &lt; nr_pinned; i++)</span>
<span class="p_add">+		put_page(pinned_pages[i]);</span>
<span class="p_add">+end:</span>
<span class="p_add">+	if (pinned_pages != pinned_pages_on_stack)</span>
<span class="p_add">+		kfree(pinned_pages);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index 12da0f771d73..db50984f7535 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -1047,6 +1047,43 @@</span> <span class="p_context"> void do_set_cpus_allowed(struct task_struct *p, const struct cpumask *new_mask)</span>
 		set_curr_task(rq, p);
 }
 
<span class="p_add">+int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct rq *rq;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
<span class="p_add">+	update_rq_clock(rq);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!cpumask_test_cpu(dest_cpu, &amp;p-&gt;cpus_allowed)) {</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (task_cpu(p) == dest_cpu)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (task_running(rq, p) || p-&gt;state == TASK_WAKING) {</span>
<span class="p_add">+		struct migration_arg arg = { p, dest_cpu };</span>
<span class="p_add">+		/* Need help from migration thread: drop lock and wait. */</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
<span class="p_add">+		stop_one_cpu(cpu_of(rq), migration_cpu_stop, &amp;arg);</span>
<span class="p_add">+		tlb_migrate_finish(p-&gt;mm);</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	} else if (task_on_rq_queued(p)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * OK, since we&#39;re going to drop the lock immediately</span>
<span class="p_add">+		 * afterwards anyway.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		rq = move_queued_task(rq, &amp;rf, p, dest_cpu);</span>
<span class="p_add">+	}</span>
<span class="p_add">+out:</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Change a given task&#39;s CPU affinity. Migrate the thread to a
  * proper CPU and schedule it away if the CPU it&#39;s executing on
<span class="p_header">diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="p_header">index eeef1a3086d1..a1c0e60006f8 100644</span>
<span class="p_header">--- a/kernel/sched/sched.h</span>
<span class="p_header">+++ b/kernel/sched/sched.h</span>
<span class="p_chunk">@@ -1207,6 +1207,8 @@</span> <span class="p_context"> static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)</span>
 #endif
 }
 
<span class="p_add">+int push_task_to_cpu(struct task_struct *p, unsigned int dest_cpu);</span>
<span class="p_add">+</span>
 /*
  * Tunables that become constants when CONFIG_SCHED_DEBUG is off:
  */
<span class="p_header">diff --git a/kernel/sys_ni.c b/kernel/sys_ni.c</span>
<span class="p_header">index c7b366ccf39c..044808ac8197 100644</span>
<span class="p_header">--- a/kernel/sys_ni.c</span>
<span class="p_header">+++ b/kernel/sys_ni.c</span>
<span class="p_chunk">@@ -261,3 +261,4 @@</span> <span class="p_context"> cond_syscall(sys_pkey_free);</span>
 
 /* restartable sequence */
 cond_syscall(sys_rseq);
<span class="p_add">+cond_syscall(sys_cpu_opv);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



