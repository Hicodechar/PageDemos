
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v2,1/6] x86: Skip PTI when disable indication is set - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v2,1/6] x86: Skip PTI when disable indication is set</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 15, 2018, 4:35 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180215163602.61162-2-namit@vmware.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10222373/mbox/"
   >mbox</a>
|
   <a href="/patch/10222373/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10222373/">/patch/10222373/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	3A9516055C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Feb 2018 16:36:39 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2C0CE29489
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Feb 2018 16:36:39 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 209BD29490; Thu, 15 Feb 2018 16:36:39 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4FD1229489
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Feb 2018 16:36:38 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1426091AbeBOQgU (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 15 Feb 2018 11:36:20 -0500
Received: from ex13-edg-ou-002.vmware.com ([208.91.0.190]:37394 &quot;EHLO
	EX13-EDG-OU-002.vmware.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1424024AbeBOQgQ (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 15 Feb 2018 11:36:16 -0500
Received: from sc9-mailhost3.vmware.com (10.113.161.73) by
	EX13-EDG-OU-002.vmware.com (10.113.208.156) with Microsoft SMTP
	Server id 15.0.1156.6; Thu, 15 Feb 2018 08:36:07 -0800
Received: from ubuntu.localdomain (unknown [10.2.101.129])
	by sc9-mailhost3.vmware.com (Postfix) with ESMTP id 83FE440B8E;
	Thu, 15 Feb 2018 08:36:15 -0800 (PST)
From: Nadav Amit &lt;namit@vmware.com&gt;
To: Ingo Molnar &lt;mingo@redhat.com&gt;
CC: Thomas Gleixner &lt;tglx@linutronix.de&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Willy Tarreau &lt;w@1wt.eu&gt;, Nadav Amit &lt;nadav.amit@gmail.com&gt;,
	&lt;x86@kernel.org&gt;, &lt;linux-kernel@vger.kernel.org&gt;,
	Nadav Amit &lt;namit@vmware.com&gt;
Subject: [PATCH RFC v2 1/6] x86: Skip PTI when disable indication is set
Date: Thu, 15 Feb 2018 08:35:57 -0800
Message-ID: &lt;20180215163602.61162-2-namit@vmware.com&gt;
X-Mailer: git-send-email 2.14.1
In-Reply-To: &lt;20180215163602.61162-1-namit@vmware.com&gt;
References: &lt;20180215163602.61162-1-namit@vmware.com&gt;
MIME-Version: 1.0
Content-Type: text/plain
Received-SPF: None (EX13-EDG-OU-002.vmware.com: namit@vmware.com does not
	designate permitted sender hosts)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Feb. 15, 2018, 4:35 p.m.</div>
<pre class="content">
If PTI is disabled, we do not want to switch page-tables. On entry to
the kernel, this is done based on CR3 value. On return, do it according
to per core indication.

To be on the safe side, avoid speculative skipping of page-tables
switching when returning the userspace. This can be avoided if the CPU
cannot execute speculatively code without the proper permissions. When
switching to the kernel page-tables, this is anyhow not an issue: if PTI
is enabled and page-tables were not switched, the kernel part of the
user page-tables would not be set.
<span class="signed-off-by">
Signed-off-by: Nadav Amit &lt;namit@vmware.com&gt;</span>
---
 arch/x86/entry/calling.h        | 33 +++++++++++++++++++++++++++++++++
 arch/x86/include/asm/tlbflush.h | 17 +++++++++++++++--
 arch/x86/kernel/asm-offsets.c   |  1 +
 3 files changed, 49 insertions(+), 2 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=64071">Dave Hansen</a> - Feb. 15, 2018, 6:10 p.m.</div>
<pre class="content">
On 02/15/2018 08:35 AM, Nadav Amit wrote:
<span class="quote">&gt; +.Lno_spec_\@:</span>
<span class="quote">&gt; +	lfence</span>
<span class="quote">&gt; +	jmp	.Lend_\@</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  .Lnoflush_\@:</span>
<span class="quote">&gt;  	SET_NOFLUSH_BIT \save_reg</span>
<span class="quote">&gt;  </span>

How expensive is this?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Feb. 15, 2018, 7:51 p.m.</div>
<pre class="content">
On Thu, Feb 15, 2018 at 4:35 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:
<span class="quote">&gt; If PTI is disabled, we do not want to switch page-tables. On entry to</span>
<span class="quote">&gt; the kernel, this is done based on CR3 value. On return, do it according</span>
<span class="quote">&gt; to per core indication.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To be on the safe side, avoid speculative skipping of page-tables</span>
<span class="quote">&gt; switching when returning the userspace. This can be avoided if the CPU</span>
<span class="quote">&gt; cannot execute speculatively code without the proper permissions. When</span>
<span class="quote">&gt; switching to the kernel page-tables, this is anyhow not an issue: if PTI</span>
<span class="quote">&gt; is enabled and page-tables were not switched, the kernel part of the</span>
<span class="quote">&gt; user page-tables would not be set.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Nadav Amit &lt;namit@vmware.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/entry/calling.h        | 33 +++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  arch/x86/include/asm/tlbflush.h | 17 +++++++++++++++--</span>
<span class="quote">&gt;  arch/x86/kernel/asm-offsets.c   |  1 +</span>
<span class="quote">&gt;  3 files changed, 49 insertions(+), 2 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="quote">&gt; index 3f48f695d5e6..5e9895f44d11 100644</span>
<span class="quote">&gt; --- a/arch/x86/entry/calling.h</span>
<span class="quote">&gt; +++ b/arch/x86/entry/calling.h</span>
<span class="quote">&gt; @@ -216,7 +216,14 @@ For 32-bit we have the following conventions - kernel is built with</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  .macro SWITCH_TO_KERNEL_CR3 scratch_reg:req</span>
<span class="quote">&gt;         ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * Do not switch on compatibility mode.</span>
<span class="quote">&gt; +        */</span>

That comment should just say &quot;if we&#39;re already using kernel CR3, don&#39;t
switch&quot; or something like that.
<span class="quote">
&gt;         mov     %cr3, \scratch_reg</span>
<span class="quote">&gt; +       testq   $PTI_USER_PGTABLE_MASK, \scratch_reg</span>
<span class="quote">&gt; +       jz      .Lend_\@</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;         ADJUST_KERNEL_CR3 \scratch_reg</span>
<span class="quote">&gt;         mov     \scratch_reg, %cr3</span>
<span class="quote">&gt;  .Lend_\@:</span>
<span class="quote">&gt; @@ -225,8 +232,20 @@ For 32-bit we have the following conventions - kernel is built with</span>
<span class="quote">&gt;  #define THIS_CPU_user_pcid_flush_mask   \</span>
<span class="quote">&gt;         PER_CPU_VAR(cpu_tlbstate) + TLB_STATE_user_pcid_flush_mask</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +#define THIS_CPU_pti_disable \</span>
<span class="quote">&gt; +       PER_CPU_VAR(cpu_tlbstate) + TLB_STATE_pti_disable</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  .macro SWITCH_TO_USER_CR3_NOSTACK scratch_reg:req scratch_reg2:req</span>
<span class="quote">&gt;         ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * Do not switch on compatibility mode. If there is no need for a</span>
<span class="quote">&gt; +        * flush, run lfence to avoid speculative execution returning to user</span>
<span class="quote">&gt; +        * with the wrong CR3.</span>
<span class="quote">&gt; +        */</span>

Nix the &quot;compatibility mode&quot; stuff please.  Also, can someone confirm
whether the affected CPUs actually speculate through SYSRET?  Because
your LFENCE might be so expensive that it negates a decent chunk of
the benefit.
<span class="quote">
&gt; +       /*</span>
<span class="quote">&gt; +        * Cached value of mm.pti_enable to simplify and speed up kernel entry</span>
<span class="quote">&gt; +        * code.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       unsigned short pti_disable;</span>

Why unsigned short?

IIRC a lot of CPUs use a slow path when decoding instructions with
16-bit operands like cmpw, so u8 or u32 could be waaaay faster than
u16.
<span class="quote">
&gt; +/* Return whether page-table isolation is disabled on this CPU */</span>
<span class="quote">&gt; +static inline unsigned short cpu_pti_disable(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return this_cpu_read(cpu_tlbstate.pti_disable);</span>
<span class="quote">&gt; +}</span>

This should return bool regardless of what type lives in the struct.
<span class="quote">
&gt; -       invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
<span class="quote">&gt; +       if (!cpu_pti_disable())</span>
<span class="quote">&gt; +               invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>

This will go badly wrong if pti_disable becomes dynamic.  Can you just
leave the code as it was?
<span class="quote">
&gt;</span>
<span class="quote">&gt;         /* If current-&gt;mm == NULL then the read_cr3() &quot;borrows&quot; an mm */</span>
<span class="quote">&gt;         native_write_cr3(__native_read_cr3());</span>
<span class="quote">&gt; @@ -404,7 +417,7 @@ static inline void __native_flush_tlb_single(unsigned long addr)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt; +       if (!static_cpu_has(X86_FEATURE_PTI) || cpu_pti_disable())</span>
<span class="quote">&gt;                 return;</span>

Ditto.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Feb. 15, 2018, 8:51 p.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@kernel.org&gt; wrote:
<span class="quote">
&gt; On Thu, Feb 15, 2018 at 4:35 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt; If PTI is disabled, we do not want to switch page-tables. On entry to</span>
<span class="quote">&gt;&gt; the kernel, this is done based on CR3 value. On return, do it according</span>
<span class="quote">&gt;&gt; to per core indication.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; To be on the safe side, avoid speculative skipping of page-tables</span>
<span class="quote">&gt;&gt; switching when returning the userspace. This can be avoided if the CPU</span>
<span class="quote">&gt;&gt; cannot execute speculatively code without the proper permissions. When</span>
<span class="quote">&gt;&gt; switching to the kernel page-tables, this is anyhow not an issue: if PTI</span>
<span class="quote">&gt;&gt; is enabled and page-tables were not switched, the kernel part of the</span>
<span class="quote">&gt;&gt; user page-tables would not be set.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Signed-off-by: Nadav Amit &lt;namit@vmware.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt; arch/x86/entry/calling.h        | 33 +++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt; arch/x86/include/asm/tlbflush.h | 17 +++++++++++++++--</span>
<span class="quote">&gt;&gt; arch/x86/kernel/asm-offsets.c   |  1 +</span>
<span class="quote">&gt;&gt; 3 files changed, 49 insertions(+), 2 deletions(-)</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="quote">&gt;&gt; index 3f48f695d5e6..5e9895f44d11 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/entry/calling.h</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/entry/calling.h</span>
<span class="quote">&gt;&gt; @@ -216,7 +216,14 @@ For 32-bit we have the following conventions - kernel is built with</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; .macro SWITCH_TO_KERNEL_CR3 scratch_reg:req</span>
<span class="quote">&gt;&gt;        ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt; +        * Do not switch on compatibility mode.</span>
<span class="quote">&gt;&gt; +        */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That comment should just say &quot;if we&#39;re already using kernel CR3, don&#39;t</span>
<span class="quote">&gt; switch&quot; or something like that.</span>

ok.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt;        mov     %cr3, \scratch_reg</span>
<span class="quote">&gt;&gt; +       testq   $PTI_USER_PGTABLE_MASK, \scratch_reg</span>
<span class="quote">&gt;&gt; +       jz      .Lend_\@</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;        ADJUST_KERNEL_CR3 \scratch_reg</span>
<span class="quote">&gt;&gt;        mov     \scratch_reg, %cr3</span>
<span class="quote">&gt;&gt; .Lend_\@:</span>
<span class="quote">&gt;&gt; @@ -225,8 +232,20 @@ For 32-bit we have the following conventions - kernel is built with</span>
<span class="quote">&gt;&gt; #define THIS_CPU_user_pcid_flush_mask   \</span>
<span class="quote">&gt;&gt;        PER_CPU_VAR(cpu_tlbstate) + TLB_STATE_user_pcid_flush_mask</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; +#define THIS_CPU_pti_disable \</span>
<span class="quote">&gt;&gt; +       PER_CPU_VAR(cpu_tlbstate) + TLB_STATE_pti_disable</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; .macro SWITCH_TO_USER_CR3_NOSTACK scratch_reg:req scratch_reg2:req</span>
<span class="quote">&gt;&gt;        ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt; +        * Do not switch on compatibility mode. If there is no need for a</span>
<span class="quote">&gt;&gt; +        * flush, run lfence to avoid speculative execution returning to user</span>
<span class="quote">&gt;&gt; +        * with the wrong CR3.</span>
<span class="quote">&gt;&gt; +        */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Nix the &quot;compatibility mode&quot; stuff please.  Also, can someone confirm</span>
<span class="quote">&gt; whether the affected CPUs actually speculate through SYSRET?  Because</span>
<span class="quote">&gt; your LFENCE might be so expensive that it negates a decent chunk of</span>
<span class="quote">&gt; the benefit.</span>

I will send performance numbers with in the next iteration. The LFENCE did
not introduce high overheads. Anyhow, it would surely be nice to remove it.
<span class="quote">
&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt; +        * Cached value of mm.pti_enable to simplify and speed up kernel entry</span>
<span class="quote">&gt;&gt; +        * code.</span>
<span class="quote">&gt;&gt; +        */</span>
<span class="quote">&gt;&gt; +       unsigned short pti_disable;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why unsigned short?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; IIRC a lot of CPUs use a slow path when decoding instructions with</span>
<span class="quote">&gt; 16-bit operands like cmpw, so u8 or u32 could be waaaay faster than</span>
<span class="quote">&gt; u16.</span>

Will do.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +/* Return whether page-table isolation is disabled on this CPU */</span>
<span class="quote">&gt;&gt; +static inline unsigned short cpu_pti_disable(void)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       return this_cpu_read(cpu_tlbstate.pti_disable);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This should return bool regardless of what type lives in the struct.</span>

Ok. I think that it was so because I tried to support both CS64 and CS32.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; -       invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
<span class="quote">&gt;&gt; +       if (!cpu_pti_disable())</span>
<span class="quote">&gt;&gt; +               invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This will go badly wrong if pti_disable becomes dynamic.  Can you just</span>
<span class="quote">&gt; leave the code as it was?</span>

Will do.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt;        /* If current-&gt;mm == NULL then the read_cr3() &quot;borrows&quot; an mm */</span>
<span class="quote">&gt;&gt;        native_write_cr3(__native_read_cr3());</span>
<span class="quote">&gt;&gt; @@ -404,7 +417,7 @@ static inline void __native_flush_tlb_single(unsigned long addr)</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;        asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; -       if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt;&gt; +       if (!static_cpu_has(X86_FEATURE_PTI) || cpu_pti_disable())</span>
<span class="quote">&gt;&gt;                return;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ditto.</span>

As for this last one - I don’t see why. Can you please explain? If you are
only worried about enabling/disabling PTI dynamically, I can address this
specific issue by flushing the TLB when it happens.

Thanks,
Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Feb. 15, 2018, 11:35 p.m.</div>
<pre class="content">
On Thu, Feb 15, 2018 at 8:51 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:
<span class="quote">&gt; Andy Lutomirski &lt;luto@kernel.org&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Thu, Feb 15, 2018 at 4:35 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; If PTI is disabled, we do not want to switch page-tables. On entry to</span>
<span class="quote">&gt;&gt;&gt; the kernel, this is done based on CR3 value. On return, do it according</span>
<span class="quote">&gt;&gt;&gt; to per core indication.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; To be on the safe side, avoid speculative skipping of page-tables</span>
<span class="quote">&gt;&gt;&gt; switching when returning the userspace. This can be avoided if the CPU</span>
<span class="quote">&gt;&gt;&gt; cannot execute speculatively code without the proper permissions. When</span>
<span class="quote">&gt;&gt;&gt; switching to the kernel page-tables, this is anyhow not an issue: if PTI</span>
<span class="quote">&gt;&gt;&gt; is enabled and page-tables were not switched, the kernel part of the</span>
<span class="quote">&gt;&gt;&gt; user page-tables would not be set.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Nadav Amit &lt;namit@vmware.com&gt;</span>
<span class="quote">&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt; arch/x86/entry/calling.h        | 33 +++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;&gt; arch/x86/include/asm/tlbflush.h | 17 +++++++++++++++--</span>
<span class="quote">&gt;&gt;&gt; arch/x86/kernel/asm-offsets.c   |  1 +</span>
<span class="quote">&gt;&gt;&gt; 3 files changed, 49 insertions(+), 2 deletions(-)</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="quote">&gt;&gt;&gt; index 3f48f695d5e6..5e9895f44d11 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/x86/entry/calling.h</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/x86/entry/calling.h</span>
<span class="quote">&gt;&gt;&gt; @@ -216,7 +216,14 @@ For 32-bit we have the following conventions - kernel is built with</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; .macro SWITCH_TO_KERNEL_CR3 scratch_reg:req</span>
<span class="quote">&gt;&gt;&gt;        ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt;&gt; +        * Do not switch on compatibility mode.</span>
<span class="quote">&gt;&gt;&gt; +        */</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; That comment should just say &quot;if we&#39;re already using kernel CR3, don&#39;t</span>
<span class="quote">&gt;&gt; switch&quot; or something like that.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ok.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;        mov     %cr3, \scratch_reg</span>
<span class="quote">&gt;&gt;&gt; +       testq   $PTI_USER_PGTABLE_MASK, \scratch_reg</span>
<span class="quote">&gt;&gt;&gt; +       jz      .Lend_\@</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;        ADJUST_KERNEL_CR3 \scratch_reg</span>
<span class="quote">&gt;&gt;&gt;        mov     \scratch_reg, %cr3</span>
<span class="quote">&gt;&gt;&gt; .Lend_\@:</span>
<span class="quote">&gt;&gt;&gt; @@ -225,8 +232,20 @@ For 32-bit we have the following conventions - kernel is built with</span>
<span class="quote">&gt;&gt;&gt; #define THIS_CPU_user_pcid_flush_mask   \</span>
<span class="quote">&gt;&gt;&gt;        PER_CPU_VAR(cpu_tlbstate) + TLB_STATE_user_pcid_flush_mask</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; +#define THIS_CPU_pti_disable \</span>
<span class="quote">&gt;&gt;&gt; +       PER_CPU_VAR(cpu_tlbstate) + TLB_STATE_pti_disable</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; .macro SWITCH_TO_USER_CR3_NOSTACK scratch_reg:req scratch_reg2:req</span>
<span class="quote">&gt;&gt;&gt;        ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt;&gt; +        * Do not switch on compatibility mode. If there is no need for a</span>
<span class="quote">&gt;&gt;&gt; +        * flush, run lfence to avoid speculative execution returning to user</span>
<span class="quote">&gt;&gt;&gt; +        * with the wrong CR3.</span>
<span class="quote">&gt;&gt;&gt; +        */</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Nix the &quot;compatibility mode&quot; stuff please.  Also, can someone confirm</span>
<span class="quote">&gt;&gt; whether the affected CPUs actually speculate through SYSRET?  Because</span>
<span class="quote">&gt;&gt; your LFENCE might be so expensive that it negates a decent chunk of</span>
<span class="quote">&gt;&gt; the benefit.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I will send performance numbers with in the next iteration. The LFENCE did</span>
<span class="quote">&gt; not introduce high overheads. Anyhow, it would surely be nice to remove it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt;&gt; +        * Cached value of mm.pti_enable to simplify and speed up kernel entry</span>
<span class="quote">&gt;&gt;&gt; +        * code.</span>
<span class="quote">&gt;&gt;&gt; +        */</span>
<span class="quote">&gt;&gt;&gt; +       unsigned short pti_disable;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Why unsigned short?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; IIRC a lot of CPUs use a slow path when decoding instructions with</span>
<span class="quote">&gt;&gt; 16-bit operands like cmpw, so u8 or u32 could be waaaay faster than</span>
<span class="quote">&gt;&gt; u16.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Will do.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; +/* Return whether page-table isolation is disabled on this CPU */</span>
<span class="quote">&gt;&gt;&gt; +static inline unsigned short cpu_pti_disable(void)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +       return this_cpu_read(cpu_tlbstate.pti_disable);</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This should return bool regardless of what type lives in the struct.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ok. I think that it was so because I tried to support both CS64 and CS32.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; -       invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
<span class="quote">&gt;&gt;&gt; +       if (!cpu_pti_disable())</span>
<span class="quote">&gt;&gt;&gt; +               invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This will go badly wrong if pti_disable becomes dynamic.  Can you just</span>
<span class="quote">&gt;&gt; leave the code as it was?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Will do.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;        /* If current-&gt;mm == NULL then the read_cr3() &quot;borrows&quot; an mm */</span>
<span class="quote">&gt;&gt;&gt;        native_write_cr3(__native_read_cr3());</span>
<span class="quote">&gt;&gt;&gt; @@ -404,7 +417,7 @@ static inline void __native_flush_tlb_single(unsigned long addr)</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;        asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; -       if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt;&gt;&gt; +       if (!static_cpu_has(X86_FEATURE_PTI) || cpu_pti_disable())</span>
<span class="quote">&gt;&gt;&gt;                return;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Ditto.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; As for this last one - I don’t see why. Can you please explain? If you are</span>
<span class="quote">&gt; only worried about enabling/disabling PTI dynamically, I can address this</span>
<span class="quote">&gt; specific issue by flushing the TLB when it happens.</span>
<span class="quote">&gt;</span>

Simplicity.  But if the code is careful to get all the flushing right
when the mode switches, I&#39;m okay with keeping the optimization.

--Andy
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index 3f48f695d5e6..5e9895f44d11 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -216,7 +216,14 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 
 .macro SWITCH_TO_KERNEL_CR3 scratch_reg:req
 	ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Do not switch on compatibility mode.</span>
<span class="p_add">+	 */</span>
 	mov	%cr3, \scratch_reg
<span class="p_add">+	testq	$PTI_USER_PGTABLE_MASK, \scratch_reg</span>
<span class="p_add">+	jz	.Lend_\@</span>
<span class="p_add">+</span>
 	ADJUST_KERNEL_CR3 \scratch_reg
 	mov	\scratch_reg, %cr3
 .Lend_\@:
<span class="p_chunk">@@ -225,8 +232,20 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 #define THIS_CPU_user_pcid_flush_mask   \
 	PER_CPU_VAR(cpu_tlbstate) + TLB_STATE_user_pcid_flush_mask
 
<span class="p_add">+#define THIS_CPU_pti_disable \</span>
<span class="p_add">+	PER_CPU_VAR(cpu_tlbstate) + TLB_STATE_pti_disable</span>
<span class="p_add">+</span>
 .macro SWITCH_TO_USER_CR3_NOSTACK scratch_reg:req scratch_reg2:req
 	ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Do not switch on compatibility mode. If there is no need for a</span>
<span class="p_add">+	 * flush, run lfence to avoid speculative execution returning to user</span>
<span class="p_add">+	 * with the wrong CR3.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	cmpw    $(0), THIS_CPU_pti_disable</span>
<span class="p_add">+	jnz     .Lno_spec_\@</span>
<span class="p_add">+</span>
 	mov	%cr3, \scratch_reg
 
 	ALTERNATIVE &quot;jmp .Lwrcr3_\@&quot;, &quot;&quot;, X86_FEATURE_PCID
<span class="p_chunk">@@ -244,6 +263,10 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	movq	\scratch_reg2, \scratch_reg
 	jmp	.Lwrcr3_pcid_\@
 
<span class="p_add">+.Lno_spec_\@:</span>
<span class="p_add">+	lfence</span>
<span class="p_add">+	jmp	.Lend_\@</span>
<span class="p_add">+</span>
 .Lnoflush_\@:
 	movq	\scratch_reg2, \scratch_reg
 	SET_NOFLUSH_BIT \scratch_reg
<span class="p_chunk">@@ -288,6 +311,12 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 
 	ALTERNATIVE &quot;jmp .Lwrcr3_\@&quot;, &quot;&quot;, X86_FEATURE_PCID
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Do not restore if PTI is disabled.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	cmpw    $(0), THIS_CPU_pti_disable</span>
<span class="p_add">+	jnz     .Lno_spec_\@</span>
<span class="p_add">+</span>
 	/*
 	 * KERNEL pages can always resume with NOFLUSH as we do
 	 * explicit flushes.
<span class="p_chunk">@@ -307,6 +336,10 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 	btr	\scratch_reg, THIS_CPU_user_pcid_flush_mask
 	jmp	.Lwrcr3_\@
 
<span class="p_add">+.Lno_spec_\@:</span>
<span class="p_add">+	lfence</span>
<span class="p_add">+	jmp	.Lend_\@</span>
<span class="p_add">+</span>
 .Lnoflush_\@:
 	SET_NOFLUSH_BIT \save_reg
 
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index d33e4a26dc7e..cf91a484bb41 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -216,6 +216,12 @@</span> <span class="p_context"> struct tlb_state {</span>
 	 */
 	unsigned long cr4;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Cached value of mm.pti_enable to simplify and speed up kernel entry</span>
<span class="p_add">+	 * code.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	unsigned short pti_disable;</span>
<span class="p_add">+</span>
 	/*
 	 * This is a list of all contexts that might exist in the TLB.
 	 * There is one per ASID that we use, and the ASID (what the
<span class="p_chunk">@@ -298,6 +304,12 @@</span> <span class="p_context"> static inline void invalidate_other_asid(void)</span>
 	this_cpu_write(cpu_tlbstate.invalidate_other, true);
 }
 
<span class="p_add">+/* Return whether page-table isolation is disabled on this CPU */</span>
<span class="p_add">+static inline unsigned short cpu_pti_disable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return this_cpu_read(cpu_tlbstate.pti_disable);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Save some of cr4 feature set we&#39;re using (e.g.  Pentium 4MB
  * enable and PPro Global page enable), so that any CPU&#39;s that boot
<span class="p_chunk">@@ -355,7 +367,8 @@</span> <span class="p_context"> static inline void __native_flush_tlb(void)</span>
 	 */
 	WARN_ON_ONCE(preemptible());
 
<span class="p_del">-	invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
<span class="p_add">+	if (!cpu_pti_disable())</span>
<span class="p_add">+		invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
 
 	/* If current-&gt;mm == NULL then the read_cr3() &quot;borrows&quot; an mm */
 	native_write_cr3(__native_read_cr3());
<span class="p_chunk">@@ -404,7 +417,7 @@</span> <span class="p_context"> static inline void __native_flush_tlb_single(unsigned long addr)</span>
 
 	asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);
 
<span class="p_del">-	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI) || cpu_pti_disable())</span>
 		return;
 
 	/*
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">index 76417a9aab73..435bb5cdfd66 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_chunk">@@ -97,6 +97,7 @@</span> <span class="p_context"> void common(void) {</span>
 
 	/* TLB state for the entry code */
 	OFFSET(TLB_STATE_user_pcid_flush_mask, tlb_state, user_pcid_flush_mask);
<span class="p_add">+	OFFSET(TLB_STATE_pti_disable, tlb_state, pti_disable);</span>
 
 	/* Layout info for cpu_entry_area */
 	OFFSET(CPU_ENTRY_AREA_tss, cpu_entry_area, tss);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



