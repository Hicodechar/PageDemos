
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v5,08/10] arm64/mm: Add support for XPFO to swiotlb - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v5,08/10] arm64/mm: Add support for XPFO to swiotlb</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 9, 2017, 8:07 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170809200755.11234-9-tycho@docker.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9891881/mbox/"
   >mbox</a>
|
   <a href="/patch/9891881/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9891881/">/patch/9891881/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D040160363 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  9 Aug 2017 20:09:54 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BE9932896D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  9 Aug 2017 20:09:54 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B0BD6289E2; Wed,  9 Aug 2017 20:09:54 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, RCVD_IN_DNSWL_HI,
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EFAB32896D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  9 Aug 2017 20:09:53 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752357AbdHIUJt (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 9 Aug 2017 16:09:49 -0400
Received: from mail-it0-f47.google.com ([209.85.214.47]:36269 &quot;EHLO
	mail-it0-f47.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751884AbdHIUJF (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 9 Aug 2017 16:09:05 -0400
Received: by mail-it0-f47.google.com with SMTP id 77so3383905itj.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 09 Aug 2017 13:09:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=docker.com; s=google;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=X5htgiEd87KA3juTj1GcJ2YAJVutgZuUIozWmdnXBdg=;
	b=gWpVIY5IE6amdUBMQ/VAiWr+OLlPgj1/dxZsSL7ITGiD5/VbYEcUmPwC4M0E+RO66s
	LbWvDWysGdThfSp4mwn+IY5FozZzaxBTqkAwae8wU5RiEJqacpEsAS0mVGcakh0mqUu2
	e3xqaxTlECMrRQNCAV8GUZ6VeAEwXGaq5AuDQ=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=X5htgiEd87KA3juTj1GcJ2YAJVutgZuUIozWmdnXBdg=;
	b=tj8/S2RvReXmubReLI0q804sL3OGKz2+xNk4hJ5QCvxBF6QFQNBjte9yHjF+M0R4Ty
	TjfyTDWXdhwnKdzA8AaZLjGAj6bWvPP1L6uUY8DYuRvN5E9XL+XJSLp1cAwbl0pnz/GF
	hZ+kZE325UYi+U93wxgZNpF0JCo3JCSRubSQnWJftGyDUhd1dOmR1qTi4qa9y+1+3LCr
	1D0W4aWdfOXKNvxkrhsJM4FxRlQd/rx62rQAtk/q7vWPWC5L/RRPEY4Eix8dmi9Vrxvx
	+mfa8X46DSFzMTChaVyN3qAQHbb6FL0Z4l0H7DCX9SlCUWcDYkULx4X8R36cQa9beR0y
	jnSw==
X-Gm-Message-State: AHYfb5j+UKlrE0edPjrl1vQDplhk1wRuGQpIEXHyUOWwZkjNCTxqGszV
	pgPxBWwsaG43XfAkh16Tiw==
X-Received: by 10.36.80.201 with SMTP id m192mr8316599itb.31.1502309344094; 
	Wed, 09 Aug 2017 13:09:04 -0700 (PDT)
Received: from localhost.localdomain ([8.24.24.129])
	by smtp.gmail.com with ESMTPSA id
	p63sm2324422itg.32.2017.08.09.13.09.02
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Wed, 09 Aug 2017 13:09:03 -0700 (PDT)
From: Tycho Andersen &lt;tycho@docker.com&gt;
To: linux-kernel@vger.kernel.org
Cc: linux-mm@kvack.org, kernel-hardening@lists.openwall.com,
	Marco Benatto &lt;marco.antonio.780@gmail.com&gt;,
	Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;,
	Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;,
	Tycho Andersen &lt;tycho@docker.com&gt;
Subject: [PATCH v5 08/10] arm64/mm: Add support for XPFO to swiotlb
Date: Wed,  9 Aug 2017 14:07:53 -0600
Message-Id: &lt;20170809200755.11234-9-tycho@docker.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170809200755.11234-1-tycho@docker.com&gt;
References: &lt;20170809200755.11234-1-tycho@docker.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Aug. 9, 2017, 8:07 p.m.</div>
<pre class="content">
<span class="from">From: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>

Pages that are unmapped by XPFO need to be mapped before and unmapped
again after (to restore the original state) the __dma_{map,unmap}_area()
operations to prevent fatal page faults.
<span class="signed-off-by">
Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Tycho Andersen &lt;tycho@docker.com&gt;</span>
---
 arch/arm64/include/asm/cacheflush.h | 11 +++++++++
 arch/arm64/mm/dma-mapping.c         | 32 +++++++++++++-------------
 arch/arm64/mm/xpfo.c                | 45 +++++++++++++++++++++++++++++++++++++
 3 files changed, 72 insertions(+), 16 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=3565">Konrad Rzeszutek</a> - Aug. 10, 2017, 1:11 p.m.</div>
<pre class="content">
On Wed, Aug 09, 2017 at 02:07:53PM -0600, Tycho Andersen wrote:
<span class="quote">&gt; From: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Pages that are unmapped by XPFO need to be mapped before and unmapped</span>
<span class="quote">&gt; again after (to restore the original state) the __dma_{map,unmap}_area()</span>
<span class="quote">&gt; operations to prevent fatal page faults.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Tycho Andersen &lt;tycho@docker.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm64/include/asm/cacheflush.h | 11 +++++++++</span>
<span class="quote">&gt;  arch/arm64/mm/dma-mapping.c         | 32 +++++++++++++-------------</span>
<span class="quote">&gt;  arch/arm64/mm/xpfo.c                | 45 +++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  3 files changed, 72 insertions(+), 16 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h</span>
<span class="quote">&gt; index d74a284abdc2..b6a462e3b2f9 100644</span>
<span class="quote">&gt; --- a/arch/arm64/include/asm/cacheflush.h</span>
<span class="quote">&gt; +++ b/arch/arm64/include/asm/cacheflush.h</span>
<span class="quote">&gt; @@ -93,6 +93,17 @@ extern void __dma_map_area(const void *, size_t, int);</span>
<span class="quote">&gt;  extern void __dma_unmap_area(const void *, size_t, int);</span>
<span class="quote">&gt;  extern void __dma_flush_area(const void *, size_t);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_XPFO</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt; +#define _dma_map_area(addr, size, dir) \</span>
<span class="quote">&gt; +	xpfo_dma_map_unmap_area(true, addr, size, dir)</span>
<span class="quote">&gt; +#define _dma_unmap_area(addr, size, dir) \</span>
<span class="quote">&gt; +	xpfo_dma_map_unmap_area(false, addr, size, dir)</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +#define _dma_map_area(addr, size, dir) __dma_map_area(addr, size, dir)</span>
<span class="quote">&gt; +#define _dma_unmap_area(addr, size, dir) __dma_unmap_area(addr, size, dir)</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Copy user data from/to a page which is mapped into a different</span>
<span class="quote">&gt;   * processes address space.  Really, we want to allow our &quot;user</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; index f27d4dd04384..a79f200786ab 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -204,7 +204,7 @@ static dma_addr_t __swiotlb_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt;  	dev_addr = swiotlb_map_page(dev, page, offset, size, dir, attrs);</span>
<span class="quote">&gt;  	if (!is_device_dma_coherent(dev) &amp;&amp;</span>
<span class="quote">&gt;  	    (attrs &amp; DMA_ATTR_SKIP_CPU_SYNC) == 0)</span>
<span class="quote">&gt; -		__dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="quote">&gt; +		_dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return dev_addr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -216,7 +216,7 @@ static void __swiotlb_unmap_page(struct device *dev, dma_addr_t dev_addr,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (!is_device_dma_coherent(dev) &amp;&amp;</span>
<span class="quote">&gt;  	    (attrs &amp; DMA_ATTR_SKIP_CPU_SYNC) == 0)</span>
<span class="quote">&gt; -		__dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="quote">&gt; +		_dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="quote">&gt;  	swiotlb_unmap_page(dev, dev_addr, size, dir, attrs);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -231,8 +231,8 @@ static int __swiotlb_map_sg_attrs(struct device *dev, struct scatterlist *sgl,</span>
<span class="quote">&gt;  	if (!is_device_dma_coherent(dev) &amp;&amp;</span>
<span class="quote">&gt;  	    (attrs &amp; DMA_ATTR_SKIP_CPU_SYNC) == 0)</span>
<span class="quote">&gt;  		for_each_sg(sgl, sg, ret, i)</span>
<span class="quote">&gt; -			__dma_map_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="quote">&gt; -				       sg-&gt;length, dir);</span>
<span class="quote">&gt; +			_dma_map_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="quote">&gt; +				      sg-&gt;length, dir);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -248,8 +248,8 @@ static void __swiotlb_unmap_sg_attrs(struct device *dev,</span>
<span class="quote">&gt;  	if (!is_device_dma_coherent(dev) &amp;&amp;</span>
<span class="quote">&gt;  	    (attrs &amp; DMA_ATTR_SKIP_CPU_SYNC) == 0)</span>
<span class="quote">&gt;  		for_each_sg(sgl, sg, nelems, i)</span>
<span class="quote">&gt; -			__dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="quote">&gt; -					 sg-&gt;length, dir);</span>
<span class="quote">&gt; +			_dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="quote">&gt; +					sg-&gt;length, dir);</span>
<span class="quote">&gt;  	swiotlb_unmap_sg_attrs(dev, sgl, nelems, dir, attrs);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -258,7 +258,7 @@ static void __swiotlb_sync_single_for_cpu(struct device *dev,</span>
<span class="quote">&gt;  					  enum dma_data_direction dir)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (!is_device_dma_coherent(dev))</span>
<span class="quote">&gt; -		__dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="quote">&gt; +		_dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="quote">&gt;  	swiotlb_sync_single_for_cpu(dev, dev_addr, size, dir);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -268,7 +268,7 @@ static void __swiotlb_sync_single_for_device(struct device *dev,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	swiotlb_sync_single_for_device(dev, dev_addr, size, dir);</span>
<span class="quote">&gt;  	if (!is_device_dma_coherent(dev))</span>
<span class="quote">&gt; -		__dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="quote">&gt; +		_dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void __swiotlb_sync_sg_for_cpu(struct device *dev,</span>
<span class="quote">&gt; @@ -280,8 +280,8 @@ static void __swiotlb_sync_sg_for_cpu(struct device *dev,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!is_device_dma_coherent(dev))</span>
<span class="quote">&gt;  		for_each_sg(sgl, sg, nelems, i)</span>
<span class="quote">&gt; -			__dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="quote">&gt; -					 sg-&gt;length, dir);</span>
<span class="quote">&gt; +			_dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="quote">&gt; +					sg-&gt;length, dir);</span>
<span class="quote">&gt;  	swiotlb_sync_sg_for_cpu(dev, sgl, nelems, dir);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -295,8 +295,8 @@ static void __swiotlb_sync_sg_for_device(struct device *dev,</span>
<span class="quote">&gt;  	swiotlb_sync_sg_for_device(dev, sgl, nelems, dir);</span>
<span class="quote">&gt;  	if (!is_device_dma_coherent(dev))</span>
<span class="quote">&gt;  		for_each_sg(sgl, sg, nelems, i)</span>
<span class="quote">&gt; -			__dma_map_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="quote">&gt; -				       sg-&gt;length, dir);</span>
<span class="quote">&gt; +			_dma_map_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="quote">&gt; +				      sg-&gt;length, dir);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int __swiotlb_mmap_pfn(struct vm_area_struct *vma,</span>
<span class="quote">&gt; @@ -758,7 +758,7 @@ static void __iommu_sync_single_for_cpu(struct device *dev,</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	phys = iommu_iova_to_phys(iommu_get_domain_for_dev(dev), dev_addr);</span>
<span class="quote">&gt; -	__dma_unmap_area(phys_to_virt(phys), size, dir);</span>
<span class="quote">&gt; +	_dma_unmap_area(phys_to_virt(phys), size, dir);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void __iommu_sync_single_for_device(struct device *dev,</span>
<span class="quote">&gt; @@ -771,7 +771,7 @@ static void __iommu_sync_single_for_device(struct device *dev,</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	phys = iommu_iova_to_phys(iommu_get_domain_for_dev(dev), dev_addr);</span>
<span class="quote">&gt; -	__dma_map_area(phys_to_virt(phys), size, dir);</span>
<span class="quote">&gt; +	_dma_map_area(phys_to_virt(phys), size, dir);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static dma_addr_t __iommu_map_page(struct device *dev, struct page *page,</span>
<span class="quote">&gt; @@ -811,7 +811,7 @@ static void __iommu_sync_sg_for_cpu(struct device *dev,</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_sg(sgl, sg, nelems, i)</span>
<span class="quote">&gt; -		__dma_unmap_area(sg_virt(sg), sg-&gt;length, dir);</span>
<span class="quote">&gt; +		_dma_unmap_area(sg_virt(sg), sg-&gt;length, dir);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void __iommu_sync_sg_for_device(struct device *dev,</span>
<span class="quote">&gt; @@ -825,7 +825,7 @@ static void __iommu_sync_sg_for_device(struct device *dev,</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_sg(sgl, sg, nelems, i)</span>
<span class="quote">&gt; -		__dma_map_area(sg_virt(sg), sg-&gt;length, dir);</span>
<span class="quote">&gt; +		_dma_map_area(sg_virt(sg), sg-&gt;length, dir);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int __iommu_map_sg_attrs(struct device *dev, struct scatterlist *sgl,</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/xpfo.c b/arch/arm64/mm/xpfo.c</span>
<span class="quote">&gt; index de03a652d48a..c4deb2b720cf 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/xpfo.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/xpfo.c</span>
<span class="quote">&gt; @@ -11,8 +11,10 @@</span>
<span class="quote">&gt;   * the Free Software Foundation.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#include &lt;linux/highmem.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/module.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -62,3 +64,46 @@ inline void xpfo_flush_kernel_page(struct page *page, int order)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) * size);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +inline void xpfo_dma_map_unmap_area(bool map, const void *addr, size_t size,</span>

And inline? You sure about that? It is quite a lot of code to duplicate
in all of those call-sites.
<span class="quote">
&gt; +				    int dir)</span>

Not enum dma_data_direction ?
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +	struct page *page = virt_to_page(addr);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * +2 here because we really want</span>
<span class="quote">&gt; +	 * ceil(size / PAGE_SIZE), not floor(), and one extra in case things are</span>
<span class="quote">&gt; +	 * not page aligned</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	int i, possible_pages = size / PAGE_SIZE + 2;</span>

Could you use the PAGE_SHIFT macro instead? Or PFN_UP ?

And there is also the PAGE_ALIGN macro...
<span class="quote">
&gt; +	void *buf[possible_pages];</span>

What if you just did &#39;void *buf[possible_pages] = { };&#39;

Wouldn&#39;t that eliminate the need for the memset?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	memset(buf, 0, sizeof(void *) * possible_pages);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	local_irq_save(flags);</span>

?? Why?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	/* Map the first page */</span>
<span class="quote">&gt; +	if (xpfo_page_is_unmapped(page))</span>
<span class="quote">&gt; +		buf[0] = kmap_atomic(page);</span>

Especially in context of the lookup and kmap_atomic which can take
a bit of time.
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Map the remaining pages */</span>
<span class="quote">&gt; +	for (i = 1; i &lt; possible_pages; i++) {</span>
<span class="quote">&gt; +		if (page_to_virt(page + i) &gt;= addr + size)</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (xpfo_page_is_unmapped(page + i))</span>
<span class="quote">&gt; +			buf[i] = kmap_atomic(page + i);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (map)</span>
<span class="quote">&gt; +		__dma_map_area(addr, size, dir);</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		__dma_unmap_area(addr, size, dir);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; possible_pages; i++)</span>
<span class="quote">&gt; +		if (buf[i])</span>
<span class="quote">&gt; +			kunmap_atomic(buf[i]);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	local_irq_restore(flags);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.11.0</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Aug. 10, 2017, 4:35 p.m.</div>
<pre class="content">
Hi Konrad,

Thanks for taking a look!

On Thu, Aug 10, 2017 at 09:11:12AM -0400, Konrad Rzeszutek Wilk wrote:
<span class="quote">&gt; On Wed, Aug 09, 2017 at 02:07:53PM -0600, Tycho Andersen wrote:</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +inline void xpfo_dma_map_unmap_area(bool map, const void *addr, size_t size,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And inline? You sure about that? It is quite a lot of code to duplicate</span>
<span class="quote">&gt; in all of those call-sites.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &gt; +				    int dir)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Not enum dma_data_direction ?</span>

I&#39;ll fix both of these, thanks.
<span class="quote">
&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned long flags;</span>
<span class="quote">&gt; &gt; +	struct page *page = virt_to_page(addr);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * +2 here because we really want</span>
<span class="quote">&gt; &gt; +	 * ceil(size / PAGE_SIZE), not floor(), and one extra in case things are</span>
<span class="quote">&gt; &gt; +	 * not page aligned</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	int i, possible_pages = size / PAGE_SIZE + 2;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Could you use the PAGE_SHIFT macro instead? Or PFN_UP ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And there is also the PAGE_ALIGN macro...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	void *buf[possible_pages];</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What if you just did &#39;void *buf[possible_pages] = { };&#39;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Wouldn&#39;t that eliminate the need for the memset?</span>

gcc doesn&#39;t seem to like that:

arch/arm64//mm/xpfo.c: In function ‘xpfo_dma_map_unmap_area’:
arch/arm64//mm/xpfo.c:80:2: error: variable-sized object may not be initialized
  void *buf[possible_pages] = {};
  ^~~~

I thought about putting this on the heap, but there&#39;s no real way to return
errors here if e.g. the kmalloc fails. I&#39;m open to suggestions though, because
this is ugly.
<span class="quote">
&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	memset(buf, 0, sizeof(void *) * possible_pages);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	local_irq_save(flags);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ?? Why?</span>

I&#39;m afraid I don&#39;t really know. I&#39;ll drop it for the next version, thanks!

Tycho
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/include/asm/cacheflush.h b/arch/arm64/include/asm/cacheflush.h</span>
<span class="p_header">index d74a284abdc2..b6a462e3b2f9 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/cacheflush.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/cacheflush.h</span>
<span class="p_chunk">@@ -93,6 +93,17 @@</span> <span class="p_context"> extern void __dma_map_area(const void *, size_t, int);</span>
 extern void __dma_unmap_area(const void *, size_t, int);
 extern void __dma_flush_area(const void *, size_t);
 
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
<span class="p_add">+#define _dma_map_area(addr, size, dir) \</span>
<span class="p_add">+	xpfo_dma_map_unmap_area(true, addr, size, dir)</span>
<span class="p_add">+#define _dma_unmap_area(addr, size, dir) \</span>
<span class="p_add">+	xpfo_dma_map_unmap_area(false, addr, size, dir)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define _dma_map_area(addr, size, dir) __dma_map_area(addr, size, dir)</span>
<span class="p_add">+#define _dma_unmap_area(addr, size, dir) __dma_unmap_area(addr, size, dir)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * Copy user data from/to a page which is mapped into a different
  * processes address space.  Really, we want to allow our &quot;user
<span class="p_header">diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">index f27d4dd04384..a79f200786ab 100644</span>
<span class="p_header">--- a/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -204,7 +204,7 @@</span> <span class="p_context"> static dma_addr_t __swiotlb_map_page(struct device *dev, struct page *page,</span>
 	dev_addr = swiotlb_map_page(dev, page, offset, size, dir, attrs);
 	if (!is_device_dma_coherent(dev) &amp;&amp;
 	    (attrs &amp; DMA_ATTR_SKIP_CPU_SYNC) == 0)
<span class="p_del">-		__dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="p_add">+		_dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
 
 	return dev_addr;
 }
<span class="p_chunk">@@ -216,7 +216,7 @@</span> <span class="p_context"> static void __swiotlb_unmap_page(struct device *dev, dma_addr_t dev_addr,</span>
 {
 	if (!is_device_dma_coherent(dev) &amp;&amp;
 	    (attrs &amp; DMA_ATTR_SKIP_CPU_SYNC) == 0)
<span class="p_del">-		__dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="p_add">+		_dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
 	swiotlb_unmap_page(dev, dev_addr, size, dir, attrs);
 }
 
<span class="p_chunk">@@ -231,8 +231,8 @@</span> <span class="p_context"> static int __swiotlb_map_sg_attrs(struct device *dev, struct scatterlist *sgl,</span>
 	if (!is_device_dma_coherent(dev) &amp;&amp;
 	    (attrs &amp; DMA_ATTR_SKIP_CPU_SYNC) == 0)
 		for_each_sg(sgl, sg, ret, i)
<span class="p_del">-			__dma_map_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="p_del">-				       sg-&gt;length, dir);</span>
<span class="p_add">+			_dma_map_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="p_add">+				      sg-&gt;length, dir);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -248,8 +248,8 @@</span> <span class="p_context"> static void __swiotlb_unmap_sg_attrs(struct device *dev,</span>
 	if (!is_device_dma_coherent(dev) &amp;&amp;
 	    (attrs &amp; DMA_ATTR_SKIP_CPU_SYNC) == 0)
 		for_each_sg(sgl, sg, nelems, i)
<span class="p_del">-			__dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="p_del">-					 sg-&gt;length, dir);</span>
<span class="p_add">+			_dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="p_add">+					sg-&gt;length, dir);</span>
 	swiotlb_unmap_sg_attrs(dev, sgl, nelems, dir, attrs);
 }
 
<span class="p_chunk">@@ -258,7 +258,7 @@</span> <span class="p_context"> static void __swiotlb_sync_single_for_cpu(struct device *dev,</span>
 					  enum dma_data_direction dir)
 {
 	if (!is_device_dma_coherent(dev))
<span class="p_del">-		__dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="p_add">+		_dma_unmap_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
 	swiotlb_sync_single_for_cpu(dev, dev_addr, size, dir);
 }
 
<span class="p_chunk">@@ -268,7 +268,7 @@</span> <span class="p_context"> static void __swiotlb_sync_single_for_device(struct device *dev,</span>
 {
 	swiotlb_sync_single_for_device(dev, dev_addr, size, dir);
 	if (!is_device_dma_coherent(dev))
<span class="p_del">-		__dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
<span class="p_add">+		_dma_map_area(phys_to_virt(dma_to_phys(dev, dev_addr)), size, dir);</span>
 }
 
 static void __swiotlb_sync_sg_for_cpu(struct device *dev,
<span class="p_chunk">@@ -280,8 +280,8 @@</span> <span class="p_context"> static void __swiotlb_sync_sg_for_cpu(struct device *dev,</span>
 
 	if (!is_device_dma_coherent(dev))
 		for_each_sg(sgl, sg, nelems, i)
<span class="p_del">-			__dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="p_del">-					 sg-&gt;length, dir);</span>
<span class="p_add">+			_dma_unmap_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="p_add">+					sg-&gt;length, dir);</span>
 	swiotlb_sync_sg_for_cpu(dev, sgl, nelems, dir);
 }
 
<span class="p_chunk">@@ -295,8 +295,8 @@</span> <span class="p_context"> static void __swiotlb_sync_sg_for_device(struct device *dev,</span>
 	swiotlb_sync_sg_for_device(dev, sgl, nelems, dir);
 	if (!is_device_dma_coherent(dev))
 		for_each_sg(sgl, sg, nelems, i)
<span class="p_del">-			__dma_map_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="p_del">-				       sg-&gt;length, dir);</span>
<span class="p_add">+			_dma_map_area(phys_to_virt(dma_to_phys(dev, sg-&gt;dma_address)),</span>
<span class="p_add">+				      sg-&gt;length, dir);</span>
 }
 
 static int __swiotlb_mmap_pfn(struct vm_area_struct *vma,
<span class="p_chunk">@@ -758,7 +758,7 @@</span> <span class="p_context"> static void __iommu_sync_single_for_cpu(struct device *dev,</span>
 		return;
 
 	phys = iommu_iova_to_phys(iommu_get_domain_for_dev(dev), dev_addr);
<span class="p_del">-	__dma_unmap_area(phys_to_virt(phys), size, dir);</span>
<span class="p_add">+	_dma_unmap_area(phys_to_virt(phys), size, dir);</span>
 }
 
 static void __iommu_sync_single_for_device(struct device *dev,
<span class="p_chunk">@@ -771,7 +771,7 @@</span> <span class="p_context"> static void __iommu_sync_single_for_device(struct device *dev,</span>
 		return;
 
 	phys = iommu_iova_to_phys(iommu_get_domain_for_dev(dev), dev_addr);
<span class="p_del">-	__dma_map_area(phys_to_virt(phys), size, dir);</span>
<span class="p_add">+	_dma_map_area(phys_to_virt(phys), size, dir);</span>
 }
 
 static dma_addr_t __iommu_map_page(struct device *dev, struct page *page,
<span class="p_chunk">@@ -811,7 +811,7 @@</span> <span class="p_context"> static void __iommu_sync_sg_for_cpu(struct device *dev,</span>
 		return;
 
 	for_each_sg(sgl, sg, nelems, i)
<span class="p_del">-		__dma_unmap_area(sg_virt(sg), sg-&gt;length, dir);</span>
<span class="p_add">+		_dma_unmap_area(sg_virt(sg), sg-&gt;length, dir);</span>
 }
 
 static void __iommu_sync_sg_for_device(struct device *dev,
<span class="p_chunk">@@ -825,7 +825,7 @@</span> <span class="p_context"> static void __iommu_sync_sg_for_device(struct device *dev,</span>
 		return;
 
 	for_each_sg(sgl, sg, nelems, i)
<span class="p_del">-		__dma_map_area(sg_virt(sg), sg-&gt;length, dir);</span>
<span class="p_add">+		_dma_map_area(sg_virt(sg), sg-&gt;length, dir);</span>
 }
 
 static int __iommu_map_sg_attrs(struct device *dev, struct scatterlist *sgl,
<span class="p_header">diff --git a/arch/arm64/mm/xpfo.c b/arch/arm64/mm/xpfo.c</span>
<span class="p_header">index de03a652d48a..c4deb2b720cf 100644</span>
<span class="p_header">--- a/arch/arm64/mm/xpfo.c</span>
<span class="p_header">+++ b/arch/arm64/mm/xpfo.c</span>
<span class="p_chunk">@@ -11,8 +11,10 @@</span> <span class="p_context"></span>
  * the Free Software Foundation.
  */
 
<span class="p_add">+#include &lt;linux/highmem.h&gt;</span>
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/module.h&gt;
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
 
 #include &lt;asm/tlbflush.h&gt;
 
<span class="p_chunk">@@ -62,3 +64,46 @@</span> <span class="p_context"> inline void xpfo_flush_kernel_page(struct page *page, int order)</span>
 
 	flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) * size);
 }
<span class="p_add">+</span>
<span class="p_add">+inline void xpfo_dma_map_unmap_area(bool map, const void *addr, size_t size,</span>
<span class="p_add">+				    int dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	struct page *page = virt_to_page(addr);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * +2 here because we really want</span>
<span class="p_add">+	 * ceil(size / PAGE_SIZE), not floor(), and one extra in case things are</span>
<span class="p_add">+	 * not page aligned</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int i, possible_pages = size / PAGE_SIZE + 2;</span>
<span class="p_add">+	void *buf[possible_pages];</span>
<span class="p_add">+</span>
<span class="p_add">+	memset(buf, 0, sizeof(void *) * possible_pages);</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_save(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Map the first page */</span>
<span class="p_add">+	if (xpfo_page_is_unmapped(page))</span>
<span class="p_add">+		buf[0] = kmap_atomic(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Map the remaining pages */</span>
<span class="p_add">+	for (i = 1; i &lt; possible_pages; i++) {</span>
<span class="p_add">+		if (page_to_virt(page + i) &gt;= addr + size)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (xpfo_page_is_unmapped(page + i))</span>
<span class="p_add">+			buf[i] = kmap_atomic(page + i);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (map)</span>
<span class="p_add">+		__dma_map_area(addr, size, dir);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		__dma_unmap_area(addr, size, dir);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; possible_pages; i++)</span>
<span class="p_add">+		if (buf[i])</span>
<span class="p_add">+			kunmap_atomic(buf[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_restore(flags);</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



