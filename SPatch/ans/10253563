
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[-mm] mm: Fix races between swapoff and flush dcache - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [-mm] mm: Fix races between swapoff and flush dcache</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=294">Huang Ying</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 2, 2018, 8:04 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180302080426.14588-1-ying.huang@intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10253563/mbox/"
   >mbox</a>
|
   <a href="/patch/10253563/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10253563/">/patch/10253563/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6C0F660211 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  2 Mar 2018 08:04:47 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4EFDF2882E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  2 Mar 2018 08:04:47 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 408AD288C6; Fri,  2 Mar 2018 08:04:47 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9B12F288C4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  2 Mar 2018 08:04:45 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S936208AbeCBIEm (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 2 Mar 2018 03:04:42 -0500
Received: from mga14.intel.com ([192.55.52.115]:5438 &quot;EHLO mga14.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S936005AbeCBIEk (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 2 Mar 2018 03:04:40 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from fmsmga006.fm.intel.com ([10.253.24.20])
	by fmsmga103.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	02 Mar 2018 00:04:39 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.47,411,1515484800&quot;; d=&quot;scan&#39;208&quot;;a=&quot;208305544&quot;
Received: from yhuang-dev.sh.intel.com ([10.239.13.10])
	by fmsmga006.fm.intel.com with ESMTP; 02 Mar 2018 00:04:35 -0800
From: &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	Huang Ying &lt;ying.huang@intel.com&gt;, Minchan Kim &lt;minchan@kernel.org&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;, Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;, Arnd Bergmann &lt;arnd@arndb.de&gt;,
	Chen Liqin &lt;liqin.linux@gmail.com&gt;, Russell King &lt;linux@armlinux.org.uk&gt;,
	Yoshinori Sato &lt;ysato@users.sourceforge.jp&gt;,
	&quot;James E.J. Bottomley&quot; &lt;jejb@parisc-linux.org&gt;,
	Guan Xuetao &lt;gxt@mprc.pku.edu.cn&gt;,
	&quot;David S. Miller&quot; &lt;davem@davemloft.net&gt;, Chris Zankel &lt;chris@zankel.net&gt;,
	Vineet Gupta &lt;vgupta@synopsys.com&gt;, Ley Foon Tan &lt;lftan@altera.com&gt;,
	Ralf Baechle &lt;ralf@linux-mips.org&gt;, Andi Kleen &lt;ak@linux.intel.com&gt;
Subject: [PATCH -mm] mm: Fix races between swapoff and flush dcache
Date: Fri,  2 Mar 2018 16:04:26 +0800
Message-Id: &lt;20180302080426.14588-1-ying.huang@intel.com&gt;
X-Mailer: git-send-email 2.15.1
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - March 2, 2018, 8:04 a.m.</div>
<pre class="content">
<span class="from">From: Huang Ying &lt;ying.huang@intel.com&gt;</span>

From commit 4b3ef9daa4fc (&quot;mm/swap: split swap cache into 64MB
trunks&quot;) on, after swapoff, the address_space associated with the swap
device will be freed.  So page_mapping() users which may touch the
address_space need some kind of mechanism to prevent the address_space
from being freed during accessing.

The dcache flushing functions (flush_dcache_page(), etc) in
architecture specific code may access the address_space of swap device
for anonymous pages in swap cache via page_mapping() function.  But in
some cases there are no mechanisms to prevent the swap device from
being swapoff, for example,

CPU1					CPU2
__get_user_pages()			swapoff()
  flush_dcache_page()
    mapping = page_mapping()
      ...				  exit_swap_address_space()
      ...				    kvfree(spaces)
      mapping_mapped(mapping)

The address space may be accessed after being freed.

But from cachetlb.txt and Russell King, flush_dcache_page() only care
about file cache pages, for anonymous pages, flush_anon_page() should
be used.  The implementation of flush_dcache_page() in all
architectures follows this too.  They will check whether
page_mapping() is NULL and whether mapping_mapped() is true to
determine whether to flush the dcache immediately.  And they will use
interval tree (mapping-&gt;i_mmap) to find all user space mappings.
While mapping_mapped() and mapping-&gt;i_mmap isn&#39;t used by anonymous
pages in swap cache at all.

So, to fix the race between swapoff and flush dcache, __page_mapping()
is add to return the address_space for file cache pages and NULL
otherwise.  All page_mapping() invoking in flush dcache functions are
replaced with __page_mapping().

The patch is only build tested, because I have no machine with
architecture other than x86.
<span class="signed-off-by">
Signed-off-by: &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt;</span>
Cc: Minchan Kim &lt;minchan@kernel.org&gt;
Cc: Michal Hocko &lt;mhocko@suse.com&gt;
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;
Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;
Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;
Cc: Arnd Bergmann &lt;arnd@arndb.de&gt;
Cc: Chen Liqin &lt;liqin.linux@gmail.com&gt;
Cc: Russell King &lt;linux@armlinux.org.uk&gt;
Cc: Yoshinori Sato &lt;ysato@users.sourceforge.jp&gt;
Cc: &quot;James E.J. Bottomley&quot; &lt;jejb@parisc-linux.org&gt;
Cc: Guan Xuetao &lt;gxt@mprc.pku.edu.cn&gt;
Cc: &quot;David S. Miller&quot; &lt;davem@davemloft.net&gt;
Cc: Chris Zankel &lt;chris@zankel.net&gt;
Cc: Vineet Gupta &lt;vgupta@synopsys.com&gt;
Cc: Ley Foon Tan &lt;lftan@altera.com&gt;
Cc: Ralf Baechle &lt;ralf@linux-mips.org&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;
---
 arch/arc/mm/cache.c           |  2 +-
 arch/arm/mm/copypage-v4mc.c   |  2 +-
 arch/arm/mm/copypage-v6.c     |  2 +-
 arch/arm/mm/copypage-xscale.c |  2 +-
 arch/arm/mm/fault-armv.c      |  2 +-
 arch/arm/mm/flush.c           |  6 +++---
 arch/mips/mm/cache.c          |  2 +-
 arch/nios2/mm/cacheflush.c    |  4 ++--
 arch/parisc/kernel/cache.c    |  4 ++--
 arch/score/mm/cache.c         |  4 ++--
 arch/sh/mm/cache-sh4.c        |  2 +-
 arch/sh/mm/cache-sh7705.c     |  2 +-
 arch/sparc/kernel/smp_64.c    |  8 ++++----
 arch/sparc/mm/init_64.c       |  6 +++---
 arch/sparc/mm/tlb.c           |  2 +-
 arch/unicore32/mm/flush.c     |  2 +-
 arch/unicore32/mm/mmu.c       |  2 +-
 arch/xtensa/mm/cache.c        |  2 +-
 include/linux/mm.h            |  1 +
 mm/util.c                     | 20 ++++++++++++++++++++
 20 files changed, 49 insertions(+), 28 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - March 2, 2018, 9:18 p.m.</div>
<pre class="content">
On Fri,  2 Mar 2018 16:04:26 +0800 &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt; wrote:
<span class="quote">
&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;From commit 4b3ef9daa4fc (&quot;mm/swap: split swap cache into 64MB</span>
<span class="quote">&gt; trunks&quot;) on, after swapoff, the address_space associated with the swap</span>
<span class="quote">&gt; device will be freed.  So page_mapping() users which may touch the</span>
<span class="quote">&gt; address_space need some kind of mechanism to prevent the address_space</span>
<span class="quote">&gt; from being freed during accessing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The dcache flushing functions (flush_dcache_page(), etc) in</span>
<span class="quote">&gt; architecture specific code may access the address_space of swap device</span>
<span class="quote">&gt; for anonymous pages in swap cache via page_mapping() function.  But in</span>
<span class="quote">&gt; some cases there are no mechanisms to prevent the swap device from</span>
<span class="quote">&gt; being swapoff, for example,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CPU1					CPU2</span>
<span class="quote">&gt; __get_user_pages()			swapoff()</span>
<span class="quote">&gt;   flush_dcache_page()</span>
<span class="quote">&gt;     mapping = page_mapping()</span>
<span class="quote">&gt;       ...				  exit_swap_address_space()</span>
<span class="quote">&gt;       ...				    kvfree(spaces)</span>
<span class="quote">&gt;       mapping_mapped(mapping)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The address space may be accessed after being freed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But from cachetlb.txt and Russell King, flush_dcache_page() only care</span>
<span class="quote">&gt; about file cache pages, for anonymous pages, flush_anon_page() should</span>
<span class="quote">&gt; be used.  The implementation of flush_dcache_page() in all</span>
<span class="quote">&gt; architectures follows this too.  They will check whether</span>
<span class="quote">&gt; page_mapping() is NULL and whether mapping_mapped() is true to</span>
<span class="quote">&gt; determine whether to flush the dcache immediately.  And they will use</span>
<span class="quote">&gt; interval tree (mapping-&gt;i_mmap) to find all user space mappings.</span>
<span class="quote">&gt; While mapping_mapped() and mapping-&gt;i_mmap isn&#39;t used by anonymous</span>
<span class="quote">&gt; pages in swap cache at all.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So, to fix the race between swapoff and flush dcache, __page_mapping()</span>
<span class="quote">&gt; is add to return the address_space for file cache pages and NULL</span>
<span class="quote">&gt; otherwise.  All page_mapping() invoking in flush dcache functions are</span>
<span class="quote">&gt; replaced with __page_mapping().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The patch is only build tested, because I have no machine with</span>
<span class="quote">&gt; architecture other than x86.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * For file cache pages, return the address_space, otherwise return NULL</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct address_space *__page_mapping(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct address_space *mapping;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	page = compound_head(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* This happens if someone calls flush_dcache_page on slab page */</span>
<span class="quote">&gt; +	if (unlikely(PageSlab(page)))</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mapping = page-&gt;mapping;</span>
<span class="quote">&gt; +	if ((unsigned long)mapping &amp; PAGE_MAPPING_ANON)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return (void *)((unsigned long)mapping &amp; ~PAGE_MAPPING_FLAGS);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>

I think page_mapping_file() would be a better name.

And do we really need to duplicate page_mapping()?  Could it be

struct address_space *page_mapping_file(struct page *page)
{
	if (PageSwapCache(page))
		return NULL;
	return page_mapping(page);
}

(We don&#39;t need to run compound_head() here, do we?)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=294">Huang Ying</a> - March 5, 2018, 1:07 a.m.</div>
<pre class="content">
Andrew Morton &lt;akpm@linux-foundation.org&gt; writes:
<span class="quote">
&gt; On Fri,  2 Mar 2018 16:04:26 +0800 &quot;Huang, Ying&quot; &lt;ying.huang@intel.com&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; From: Huang Ying &lt;ying.huang@intel.com&gt;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;From commit 4b3ef9daa4fc (&quot;mm/swap: split swap cache into 64MB</span>
<span class="quote">&gt;&gt; trunks&quot;) on, after swapoff, the address_space associated with the swap</span>
<span class="quote">&gt;&gt; device will be freed.  So page_mapping() users which may touch the</span>
<span class="quote">&gt;&gt; address_space need some kind of mechanism to prevent the address_space</span>
<span class="quote">&gt;&gt; from being freed during accessing.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The dcache flushing functions (flush_dcache_page(), etc) in</span>
<span class="quote">&gt;&gt; architecture specific code may access the address_space of swap device</span>
<span class="quote">&gt;&gt; for anonymous pages in swap cache via page_mapping() function.  But in</span>
<span class="quote">&gt;&gt; some cases there are no mechanisms to prevent the swap device from</span>
<span class="quote">&gt;&gt; being swapoff, for example,</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; CPU1					CPU2</span>
<span class="quote">&gt;&gt; __get_user_pages()			swapoff()</span>
<span class="quote">&gt;&gt;   flush_dcache_page()</span>
<span class="quote">&gt;&gt;     mapping = page_mapping()</span>
<span class="quote">&gt;&gt;       ...				  exit_swap_address_space()</span>
<span class="quote">&gt;&gt;       ...				    kvfree(spaces)</span>
<span class="quote">&gt;&gt;       mapping_mapped(mapping)</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The address space may be accessed after being freed.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; But from cachetlb.txt and Russell King, flush_dcache_page() only care</span>
<span class="quote">&gt;&gt; about file cache pages, for anonymous pages, flush_anon_page() should</span>
<span class="quote">&gt;&gt; be used.  The implementation of flush_dcache_page() in all</span>
<span class="quote">&gt;&gt; architectures follows this too.  They will check whether</span>
<span class="quote">&gt;&gt; page_mapping() is NULL and whether mapping_mapped() is true to</span>
<span class="quote">&gt;&gt; determine whether to flush the dcache immediately.  And they will use</span>
<span class="quote">&gt;&gt; interval tree (mapping-&gt;i_mmap) to find all user space mappings.</span>
<span class="quote">&gt;&gt; While mapping_mapped() and mapping-&gt;i_mmap isn&#39;t used by anonymous</span>
<span class="quote">&gt;&gt; pages in swap cache at all.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; So, to fix the race between swapoff and flush dcache, __page_mapping()</span>
<span class="quote">&gt;&gt; is add to return the address_space for file cache pages and NULL</span>
<span class="quote">&gt;&gt; otherwise.  All page_mapping() invoking in flush dcache functions are</span>
<span class="quote">&gt;&gt; replaced with __page_mapping().</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The patch is only build tested, because I have no machine with</span>
<span class="quote">&gt;&gt; architecture other than x86.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; ...</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * For file cache pages, return the address_space, otherwise return NULL</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +struct address_space *__page_mapping(struct page *page)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct address_space *mapping;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	page = compound_head(page);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/* This happens if someone calls flush_dcache_page on slab page */</span>
<span class="quote">&gt;&gt; +	if (unlikely(PageSlab(page)))</span>
<span class="quote">&gt;&gt; +		return NULL;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	mapping = page-&gt;mapping;</span>
<span class="quote">&gt;&gt; +	if ((unsigned long)mapping &amp; PAGE_MAPPING_ANON)</span>
<span class="quote">&gt;&gt; +		return NULL;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	return (void *)((unsigned long)mapping &amp; ~PAGE_MAPPING_FLAGS);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think page_mapping_file() would be a better name.</span>

Thanks!  I will use that name.
<span class="quote">
&gt; And do we really need to duplicate page_mapping()?  Could it be</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; struct address_space *page_mapping_file(struct page *page)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	if (PageSwapCache(page))</span>
<span class="quote">&gt; 		return NULL;</span>
<span class="quote">&gt; 	return page_mapping(page);</span>
<span class="quote">&gt; }</span>

Yes.  This looks better.
<span class="quote">
&gt; (We don&#39;t need to run compound_head() here, do we?)</span>

Yes.  I think so.

Best Regards,
Huang, Ying
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arc/mm/cache.c b/arch/arc/mm/cache.c</span>
<span class="p_header">index 2072f3451e9c..0f607d5a85da 100644</span>
<span class="p_header">--- a/arch/arc/mm/cache.c</span>
<span class="p_header">+++ b/arch/arc/mm/cache.c</span>
<span class="p_chunk">@@ -833,7 +833,7 @@</span> <span class="p_context"> void flush_dcache_page(struct page *page)</span>
 	}
 
 	/* don&#39;t handle anon pages here */
<span class="p_del">-	mapping = page_mapping(page);</span>
<span class="p_add">+	mapping = __page_mapping(page);</span>
 	if (!mapping)
 		return;
 
<span class="p_header">diff --git a/arch/arm/mm/copypage-v4mc.c b/arch/arm/mm/copypage-v4mc.c</span>
<span class="p_header">index 1267e64133b9..6d9e632ca43b 100644</span>
<span class="p_header">--- a/arch/arm/mm/copypage-v4mc.c</span>
<span class="p_header">+++ b/arch/arm/mm/copypage-v4mc.c</span>
<span class="p_chunk">@@ -70,7 +70,7 @@</span> <span class="p_context"> void v4_mc_copy_user_highpage(struct page *to, struct page *from,</span>
 	void *kto = kmap_atomic(to);
 
 	if (!test_and_set_bit(PG_dcache_clean, &amp;from-&gt;flags))
<span class="p_del">-		__flush_dcache_page(page_mapping(from), from);</span>
<span class="p_add">+		__flush_dcache_page(__page_mapping(from), from);</span>
 
 	raw_spin_lock(&amp;minicache_lock);
 
<span class="p_header">diff --git a/arch/arm/mm/copypage-v6.c b/arch/arm/mm/copypage-v6.c</span>
<span class="p_header">index 70423345da26..2f13ffd847a6 100644</span>
<span class="p_header">--- a/arch/arm/mm/copypage-v6.c</span>
<span class="p_header">+++ b/arch/arm/mm/copypage-v6.c</span>
<span class="p_chunk">@@ -76,7 +76,7 @@</span> <span class="p_context"> static void v6_copy_user_highpage_aliasing(struct page *to,</span>
 	unsigned long kfrom, kto;
 
 	if (!test_and_set_bit(PG_dcache_clean, &amp;from-&gt;flags))
<span class="p_del">-		__flush_dcache_page(page_mapping(from), from);</span>
<span class="p_add">+		__flush_dcache_page(__page_mapping(from), from);</span>
 
 	/* FIXME: not highmem safe */
 	discard_old_kernel_data(page_address(to));
<span class="p_header">diff --git a/arch/arm/mm/copypage-xscale.c b/arch/arm/mm/copypage-xscale.c</span>
<span class="p_header">index 0fb85025344d..221129649627 100644</span>
<span class="p_header">--- a/arch/arm/mm/copypage-xscale.c</span>
<span class="p_header">+++ b/arch/arm/mm/copypage-xscale.c</span>
<span class="p_chunk">@@ -90,7 +90,7 @@</span> <span class="p_context"> void xscale_mc_copy_user_highpage(struct page *to, struct page *from,</span>
 	void *kto = kmap_atomic(to);
 
 	if (!test_and_set_bit(PG_dcache_clean, &amp;from-&gt;flags))
<span class="p_del">-		__flush_dcache_page(page_mapping(from), from);</span>
<span class="p_add">+		__flush_dcache_page(__page_mapping(from), from);</span>
 
 	raw_spin_lock(&amp;minicache_lock);
 
<span class="p_header">diff --git a/arch/arm/mm/fault-armv.c b/arch/arm/mm/fault-armv.c</span>
<span class="p_header">index d9e0d00a6699..593bd1549ce0 100644</span>
<span class="p_header">--- a/arch/arm/mm/fault-armv.c</span>
<span class="p_header">+++ b/arch/arm/mm/fault-armv.c</span>
<span class="p_chunk">@@ -195,7 +195,7 @@</span> <span class="p_context"> void update_mmu_cache(struct vm_area_struct *vma, unsigned long addr,</span>
 	if (page == ZERO_PAGE(0))
 		return;
 
<span class="p_del">-	mapping = page_mapping(page);</span>
<span class="p_add">+	mapping = __page_mapping(page);</span>
 	if (!test_and_set_bit(PG_dcache_clean, &amp;page-&gt;flags))
 		__flush_dcache_page(mapping, page);
 	if (mapping) {
<span class="p_header">diff --git a/arch/arm/mm/flush.c b/arch/arm/mm/flush.c</span>
<span class="p_header">index f1e6190aa7ea..2e4a478c6f02 100644</span>
<span class="p_header">--- a/arch/arm/mm/flush.c</span>
<span class="p_header">+++ b/arch/arm/mm/flush.c</span>
<span class="p_chunk">@@ -285,7 +285,7 @@</span> <span class="p_context"> void __sync_icache_dcache(pte_t pteval)</span>
 
 	page = pfn_to_page(pfn);
 	if (cache_is_vipt_aliasing())
<span class="p_del">-		mapping = page_mapping(page);</span>
<span class="p_add">+		mapping = __page_mapping(page);</span>
 	else
 		mapping = NULL;
 
<span class="p_chunk">@@ -333,7 +333,7 @@</span> <span class="p_context"> void flush_dcache_page(struct page *page)</span>
 		return;
 	}
 
<span class="p_del">-	mapping = page_mapping(page);</span>
<span class="p_add">+	mapping = __page_mapping(page);</span>
 
 	if (!cache_ops_need_broadcast() &amp;&amp;
 	    mapping &amp;&amp; !page_mapcount(page))
<span class="p_chunk">@@ -363,7 +363,7 @@</span> <span class="p_context"> void flush_kernel_dcache_page(struct page *page)</span>
 	if (cache_is_vivt() || cache_is_vipt_aliasing()) {
 		struct address_space *mapping;
 
<span class="p_del">-		mapping = page_mapping(page);</span>
<span class="p_add">+		mapping = __page_mapping(page);</span>
 
 		if (!mapping || mapping_mapped(mapping)) {
 			void *addr;
<span class="p_header">diff --git a/arch/mips/mm/cache.c b/arch/mips/mm/cache.c</span>
<span class="p_header">index 44ac64d51827..8a21c0345516 100644</span>
<span class="p_header">--- a/arch/mips/mm/cache.c</span>
<span class="p_header">+++ b/arch/mips/mm/cache.c</span>
<span class="p_chunk">@@ -86,7 +86,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(cacheflush, unsigned long, addr, unsigned long, bytes,</span>
 
 void __flush_dcache_page(struct page *page)
 {
<span class="p_del">-	struct address_space *mapping = page_mapping(page);</span>
<span class="p_add">+	struct address_space *mapping = __page_mapping(page);</span>
 	unsigned long addr;
 
 	if (mapping &amp;&amp; !mapping_mapped(mapping)) {
<span class="p_header">diff --git a/arch/nios2/mm/cacheflush.c b/arch/nios2/mm/cacheflush.c</span>
<span class="p_header">index 87bf88ed04c6..117bece3eb80 100644</span>
<span class="p_header">--- a/arch/nios2/mm/cacheflush.c</span>
<span class="p_header">+++ b/arch/nios2/mm/cacheflush.c</span>
<span class="p_chunk">@@ -180,7 +180,7 @@</span> <span class="p_context"> void flush_dcache_page(struct page *page)</span>
 	if (page == ZERO_PAGE(0))
 		return;
 
<span class="p_del">-	mapping = page_mapping(page);</span>
<span class="p_add">+	mapping = __page_mapping(page);</span>
 
 	/* Flush this page if there are aliases. */
 	if (mapping &amp;&amp; !mapping_mapped(mapping)) {
<span class="p_chunk">@@ -215,7 +215,7 @@</span> <span class="p_context"> void update_mmu_cache(struct vm_area_struct *vma,</span>
 	if (page == ZERO_PAGE(0))
 		return;
 
<span class="p_del">-	mapping = page_mapping(page);</span>
<span class="p_add">+	mapping = __page_mapping(page);</span>
 	if (!test_and_set_bit(PG_dcache_clean, &amp;page-&gt;flags))
 		__flush_dcache_page(mapping, page);
 
<span class="p_header">diff --git a/arch/parisc/kernel/cache.c b/arch/parisc/kernel/cache.c</span>
<span class="p_header">index 7c1bde80ada4..2150dd193654 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/cache.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/cache.c</span>
<span class="p_chunk">@@ -88,7 +88,7 @@</span> <span class="p_context"> update_mmu_cache(struct vm_area_struct *vma, unsigned long address, pte_t *ptep)</span>
 		return;
 
 	page = pfn_to_page(pfn);
<span class="p_del">-	if (page_mapping(page) &amp;&amp; test_bit(PG_dcache_dirty, &amp;page-&gt;flags)) {</span>
<span class="p_add">+	if (__page_mapping(page) &amp;&amp; test_bit(PG_dcache_dirty, &amp;page-&gt;flags)) {</span>
 		flush_kernel_dcache_page_addr(pfn_va(pfn));
 		clear_bit(PG_dcache_dirty, &amp;page-&gt;flags);
 	} else if (parisc_requires_coherency())
<span class="p_chunk">@@ -304,7 +304,7 @@</span> <span class="p_context"> __flush_cache_page(struct vm_area_struct *vma, unsigned long vmaddr,</span>
 
 void flush_dcache_page(struct page *page)
 {
<span class="p_del">-	struct address_space *mapping = page_mapping(page);</span>
<span class="p_add">+	struct address_space *mapping = __page_mapping(page);</span>
 	struct vm_area_struct *mpnt;
 	unsigned long offset;
 	unsigned long addr, old_addr = 0;
<span class="p_header">diff --git a/arch/score/mm/cache.c b/arch/score/mm/cache.c</span>
<span class="p_header">index b4bcfd3e8393..a74967a396e6 100644</span>
<span class="p_header">--- a/arch/score/mm/cache.c</span>
<span class="p_header">+++ b/arch/score/mm/cache.c</span>
<span class="p_chunk">@@ -54,7 +54,7 @@</span> <span class="p_context"> static void flush_data_cache_page(unsigned long addr)</span>
 
 void flush_dcache_page(struct page *page)
 {
<span class="p_del">-	struct address_space *mapping = page_mapping(page);</span>
<span class="p_add">+	struct address_space *mapping = __page_mapping(page);</span>
 	unsigned long addr;
 
 	if (PageHighMem(page))
<span class="p_chunk">@@ -86,7 +86,7 @@</span> <span class="p_context"> void __update_cache(struct vm_area_struct *vma, unsigned long address,</span>
 	if (unlikely(!pfn_valid(pfn)))
 		return;
 	page = pfn_to_page(pfn);
<span class="p_del">-	if (page_mapping(page) &amp;&amp; test_bit(PG_dcache_dirty, &amp;(page)-&gt;flags)) {</span>
<span class="p_add">+	if (__page_mapping(page) &amp;&amp; test_bit(PG_dcache_dirty, &amp;(page)-&gt;flags)) {</span>
 		addr = (unsigned long) page_address(page);
 		if (exec)
 			flush_data_cache_page(addr);
<span class="p_header">diff --git a/arch/sh/mm/cache-sh4.c b/arch/sh/mm/cache-sh4.c</span>
<span class="p_header">index 58aaa4f33b81..281e33daaa98 100644</span>
<span class="p_header">--- a/arch/sh/mm/cache-sh4.c</span>
<span class="p_header">+++ b/arch/sh/mm/cache-sh4.c</span>
<span class="p_chunk">@@ -112,7 +112,7 @@</span> <span class="p_context"> static void sh4_flush_dcache_page(void *arg)</span>
 	struct page *page = arg;
 	unsigned long addr = (unsigned long)page_address(page);
 #ifndef CONFIG_SMP
<span class="p_del">-	struct address_space *mapping = page_mapping(page);</span>
<span class="p_add">+	struct address_space *mapping = __page_mapping(page);</span>
 
 	if (mapping &amp;&amp; !mapping_mapped(mapping))
 		clear_bit(PG_dcache_clean, &amp;page-&gt;flags);
<span class="p_header">diff --git a/arch/sh/mm/cache-sh7705.c b/arch/sh/mm/cache-sh7705.c</span>
<span class="p_header">index 6cd2aa395817..92db57ccc35a 100644</span>
<span class="p_header">--- a/arch/sh/mm/cache-sh7705.c</span>
<span class="p_header">+++ b/arch/sh/mm/cache-sh7705.c</span>
<span class="p_chunk">@@ -136,7 +136,7 @@</span> <span class="p_context"> static void __flush_dcache_page(unsigned long phys)</span>
 static void sh7705_flush_dcache_page(void *arg)
 {
 	struct page *page = arg;
<span class="p_del">-	struct address_space *mapping = page_mapping(page);</span>
<span class="p_add">+	struct address_space *mapping = __page_mapping(page);</span>
 
 	if (mapping &amp;&amp; !mapping_mapped(mapping))
 		clear_bit(PG_dcache_clean, &amp;page-&gt;flags);
<span class="p_header">diff --git a/arch/sparc/kernel/smp_64.c b/arch/sparc/kernel/smp_64.c</span>
<span class="p_header">index c50182cd2f64..5bd8b67e66f2 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/smp_64.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/smp_64.c</span>
<span class="p_chunk">@@ -929,9 +929,9 @@</span> <span class="p_context"> static inline void __local_flush_dcache_page(struct page *page)</span>
 #ifdef DCACHE_ALIASING_POSSIBLE
 	__flush_dcache_page(page_address(page),
 			    ((tlb_type == spitfire) &amp;&amp;
<span class="p_del">-			     page_mapping(page) != NULL));</span>
<span class="p_add">+			     __page_mapping(page) != NULL));</span>
 #else
<span class="p_del">-	if (page_mapping(page) != NULL &amp;&amp;</span>
<span class="p_add">+	if (__page_mapping(page) != NULL &amp;&amp;</span>
 	    tlb_type == spitfire)
 		__flush_icache_page(__pa(page_address(page)));
 #endif
<span class="p_chunk">@@ -958,7 +958,7 @@</span> <span class="p_context"> void smp_flush_dcache_page_impl(struct page *page, int cpu)</span>
 
 		if (tlb_type == spitfire) {
 			data0 = ((u64)&amp;xcall_flush_dcache_page_spitfire);
<span class="p_del">-			if (page_mapping(page) != NULL)</span>
<span class="p_add">+			if (__page_mapping(page) != NULL)</span>
 				data0 |= ((u64)1 &lt;&lt; 32);
 		} else if (tlb_type == cheetah || tlb_type == cheetah_plus) {
 #ifdef DCACHE_ALIASING_POSSIBLE
<span class="p_chunk">@@ -994,7 +994,7 @@</span> <span class="p_context"> void flush_dcache_page_all(struct mm_struct *mm, struct page *page)</span>
 	pg_addr = page_address(page);
 	if (tlb_type == spitfire) {
 		data0 = ((u64)&amp;xcall_flush_dcache_page_spitfire);
<span class="p_del">-		if (page_mapping(page) != NULL)</span>
<span class="p_add">+		if (__page_mapping(page) != NULL)</span>
 			data0 |= ((u64)1 &lt;&lt; 32);
 	} else if (tlb_type == cheetah || tlb_type == cheetah_plus) {
 #ifdef DCACHE_ALIASING_POSSIBLE
<span class="p_header">diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c</span>
<span class="p_header">index a9f94e911e0a..6f0f3f2b04a9 100644</span>
<span class="p_header">--- a/arch/sparc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/init_64.c</span>
<span class="p_chunk">@@ -206,9 +206,9 @@</span> <span class="p_context"> inline void flush_dcache_page_impl(struct page *page)</span>
 #ifdef DCACHE_ALIASING_POSSIBLE
 	__flush_dcache_page(page_address(page),
 			    ((tlb_type == spitfire) &amp;&amp;
<span class="p_del">-			     page_mapping(page) != NULL));</span>
<span class="p_add">+			     __page_mapping(page) != NULL));</span>
 #else
<span class="p_del">-	if (page_mapping(page) != NULL &amp;&amp;</span>
<span class="p_add">+	if (__page_mapping(page) != NULL &amp;&amp;</span>
 	    tlb_type == spitfire)
 		__flush_icache_page(__pa(page_address(page)));
 #endif
<span class="p_chunk">@@ -490,7 +490,7 @@</span> <span class="p_context"> void flush_dcache_page(struct page *page)</span>
 
 	this_cpu = get_cpu();
 
<span class="p_del">-	mapping = page_mapping(page);</span>
<span class="p_add">+	mapping = __page_mapping(page);</span>
 	if (mapping &amp;&amp; !mapping_mapped(mapping)) {
 		int dirty = test_bit(PG_dcache_dirty, &amp;page-&gt;flags);
 		if (dirty) {
<span class="p_header">diff --git a/arch/sparc/mm/tlb.c b/arch/sparc/mm/tlb.c</span>
<span class="p_header">index 847ddffbf38a..5d7c13b88a42 100644</span>
<span class="p_header">--- a/arch/sparc/mm/tlb.c</span>
<span class="p_header">+++ b/arch/sparc/mm/tlb.c</span>
<span class="p_chunk">@@ -128,7 +128,7 @@</span> <span class="p_context"> void tlb_batch_add(struct mm_struct *mm, unsigned long vaddr,</span>
 			goto no_cache_flush;
 
 		/* A real file page? */
<span class="p_del">-		mapping = page_mapping(page);</span>
<span class="p_add">+		mapping = __page_mapping(page);</span>
 		if (!mapping)
 			goto no_cache_flush;
 
<span class="p_header">diff --git a/arch/unicore32/mm/flush.c b/arch/unicore32/mm/flush.c</span>
<span class="p_header">index 6d4c096ffa2a..def5142d16ea 100644</span>
<span class="p_header">--- a/arch/unicore32/mm/flush.c</span>
<span class="p_header">+++ b/arch/unicore32/mm/flush.c</span>
<span class="p_chunk">@@ -83,7 +83,7 @@</span> <span class="p_context"> void flush_dcache_page(struct page *page)</span>
 	if (page == ZERO_PAGE(0))
 		return;
 
<span class="p_del">-	mapping = page_mapping(page);</span>
<span class="p_add">+	mapping = __page_mapping(page);</span>
 
 	if (mapping &amp;&amp; !mapping_mapped(mapping))
 		clear_bit(PG_dcache_clean, &amp;page-&gt;flags);
<span class="p_header">diff --git a/arch/unicore32/mm/mmu.c b/arch/unicore32/mm/mmu.c</span>
<span class="p_header">index 4f5a532bee13..2bd143b7aca3 100644</span>
<span class="p_header">--- a/arch/unicore32/mm/mmu.c</span>
<span class="p_header">+++ b/arch/unicore32/mm/mmu.c</span>
<span class="p_chunk">@@ -503,7 +503,7 @@</span> <span class="p_context"> void update_mmu_cache(struct vm_area_struct *vma, unsigned long addr,</span>
 	if (page == ZERO_PAGE(0))
 		return;
 
<span class="p_del">-	mapping = page_mapping(page);</span>
<span class="p_add">+	mapping = __page_mapping(page);</span>
 	if (!test_and_set_bit(PG_dcache_clean, &amp;page-&gt;flags))
 		__flush_dcache_page(mapping, page);
 	if (mapping)
<span class="p_header">diff --git a/arch/xtensa/mm/cache.c b/arch/xtensa/mm/cache.c</span>
<span class="p_header">index 57dc231a0709..17485819d03b 100644</span>
<span class="p_header">--- a/arch/xtensa/mm/cache.c</span>
<span class="p_header">+++ b/arch/xtensa/mm/cache.c</span>
<span class="p_chunk">@@ -127,7 +127,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(copy_user_highpage);</span>
 
 void flush_dcache_page(struct page *page)
 {
<span class="p_del">-	struct address_space *mapping = page_mapping(page);</span>
<span class="p_add">+	struct address_space *mapping = __page_mapping(page);</span>
 
 	/*
 	 * If we have a mapping but the page is not mapped to user-space
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index c500bdfadf79..16d5e27b4438 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -1146,6 +1146,7 @@</span> <span class="p_context"> static inline pgoff_t page_index(struct page *page)</span>
 }
 
 bool page_mapped(struct page *page);
<span class="p_add">+struct address_space *__page_mapping(struct page *page);</span>
 struct address_space *page_mapping(struct page *page);
 
 /*
<span class="p_header">diff --git a/mm/util.c b/mm/util.c</span>
<span class="p_header">index d800ce40816c..f47f3a1cc3f2 100644</span>
<span class="p_header">--- a/mm/util.c</span>
<span class="p_header">+++ b/mm/util.c</span>
<span class="p_chunk">@@ -490,6 +490,26 @@</span> <span class="p_context"> struct anon_vma *page_anon_vma(struct page *page)</span>
 	return __page_rmapping(page);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * For file cache pages, return the address_space, otherwise return NULL</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct address_space *__page_mapping(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct address_space *mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+	page = compound_head(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* This happens if someone calls flush_dcache_page on slab page */</span>
<span class="p_add">+	if (unlikely(PageSlab(page)))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	mapping = page-&gt;mapping;</span>
<span class="p_add">+	if ((unsigned long)mapping &amp; PAGE_MAPPING_ANON)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (void *)((unsigned long)mapping &amp; ~PAGE_MAPPING_FLAGS);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 struct address_space *page_mapping(struct page *page)
 {
 	struct address_space *mapping;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



