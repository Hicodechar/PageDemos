
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[13/17] kvm-arm: Add stage2 page table modifiers - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [13/17] kvm-arm: Add stage2 page table modifiers</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=114661">Suzuki K. Poulose</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 4, 2016, 4:26 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1459787177-12767-14-git-send-email-suzuki.poulose@arm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8742431/mbox/"
   >mbox</a>
|
   <a href="/patch/8742431/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8742431/">/patch/8742431/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id D311BC0553
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  4 Apr 2016 16:28:42 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 03DA12027D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  4 Apr 2016 16:28:42 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 0D2672028D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  4 Apr 2016 16:28:41 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756136AbcDDQ2i (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 4 Apr 2016 12:28:38 -0400
Received: from foss.arm.com ([217.140.101.70]:46462 &quot;EHLO foss.arm.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932506AbcDDQ1J (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 4 Apr 2016 12:27:09 -0400
Received: from usa-sjc-imap-foss1.foss.arm.com (unknown [10.72.51.249])
	by usa-sjc-mx-foss1.foss.arm.com (Postfix) with ESMTP id 83900254;
	Mon,  4 Apr 2016 09:25:58 -0700 (PDT)
Received: from e106634-lin.cambridge.arm.com (e106634-lin.cambridge.arm.com
	[10.1.209.136])
	by usa-sjc-imap-foss1.foss.arm.com (Postfix) with ESMTPA id
	D41D03F459; Mon,  4 Apr 2016 09:27:06 -0700 (PDT)
From: Suzuki K Poulose &lt;suzuki.poulose@arm.com&gt;
To: linux-arm-kernel@lists.infradead.org
Cc: linux-kernel@vger.kernel.org, kvmarm@lists.cs.columbia.edu,
	kvm@vger.kernel.org, marc.zyngier@arm.com,
	christoffer.dall@linaro.org, mark.rutland@arm.com,
	will.deacon@arm.com, catalin.marinas@arm.com,
	Suzuki K Poulose &lt;suzuki.poulose@arm.com&gt;
Subject: [PATCH 13/17] kvm-arm: Add stage2 page table modifiers
Date: Mon,  4 Apr 2016 17:26:13 +0100
Message-Id: &lt;1459787177-12767-14-git-send-email-suzuki.poulose@arm.com&gt;
X-Mailer: git-send-email 1.7.9.5
In-Reply-To: &lt;1459787177-12767-1-git-send-email-suzuki.poulose@arm.com&gt;
References: &lt;1459787177-12767-1-git-send-email-suzuki.poulose@arm.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=114661">Suzuki K. Poulose</a> - April 4, 2016, 4:26 p.m.</div>
<pre class="content">
Now that the hyp page table is handled by different set of
routines, rename the original shared routines to stage2 handlers.
Also make explicit use of the stage2 page table helpers.

unmap_range has been merged to existing unmap_stage2_range.

Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;
Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;
<span class="signed-off-by">Signed-off-by: Suzuki K Poulose &lt;suzuki.poulose@arm.com&gt;</span>
---
 arch/arm/kvm/mmu.c |   97 ++++++++++++++++++++++++----------------------------
 1 file changed, 44 insertions(+), 53 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=68151">Christoffer Dall</a> - April 8, 2016, 1:42 p.m.</div>
<pre class="content">
On Mon, Apr 04, 2016 at 05:26:13PM +0100, Suzuki K Poulose wrote:
<span class="quote">&gt; Now that the hyp page table is handled by different set of</span>
<span class="quote">&gt; routines, rename the original shared routines to stage2 handlers.</span>
<span class="quote">&gt; Also make explicit use of the stage2 page table helpers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; unmap_range has been merged to existing unmap_stage2_range.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;</span>
<span class="quote">&gt; Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Suzuki K Poulose &lt;suzuki.poulose@arm.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm/kvm/mmu.c |   97 ++++++++++++++++++++++++----------------------------</span>
<span class="quote">&gt;  1 file changed, 44 insertions(+), 53 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; index 2b491e5..0009a24 100644</span>
<span class="quote">&gt; --- a/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; @@ -152,26 +152,26 @@ static void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)</span>
<span class="quote">&gt;  	return p;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void clear_pgd_entry(struct kvm *kvm, pgd_t *pgd, phys_addr_t addr)</span>
<span class="quote">&gt; +static void clear_stage2_pgd_entry(struct kvm *kvm, pgd_t *pgd, phys_addr_t addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pud_t *pud_table __maybe_unused = pud_offset(pgd, 0);</span>
<span class="quote">&gt; -	pgd_clear(pgd);</span>
<span class="quote">&gt; +	pud_t *pud_table __maybe_unused = stage2_pud_offset(pgd, 0UL);</span>
<span class="quote">&gt; +	stage2_pgd_clear(pgd);</span>
<span class="quote">&gt;  	kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="quote">&gt; -	pud_free(NULL, pud_table);</span>
<span class="quote">&gt; +	stage2_pud_free(NULL, pud_table);</span>
<span class="quote">&gt;  	put_page(virt_to_page(pgd));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void clear_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="quote">&gt; +static void clear_stage2_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	pmd_t *pmd_table = pmd_offset(pud, 0);</span>
<span class="quote">&gt; -	VM_BUG_ON(pud_huge(*pud));</span>
<span class="quote">&gt; -	pud_clear(pud);</span>
<span class="quote">&gt; +	pmd_t *pmd_table __maybe_unused = stage2_pmd_offset(pud, 0);</span>

The __maybe_unused are slightly ugly, so it may be nicer to create the
stage2_pmd_free() as static inline&#39;s if they&#39;re defined to do nothing
instead.
<span class="quote">

&gt; +	VM_BUG_ON(stage2_pud_huge(*pud));</span>
<span class="quote">&gt; +	stage2_pud_clear(pud);</span>
<span class="quote">&gt;  	kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="quote">&gt; -	pmd_free(NULL, pmd_table);</span>
<span class="quote">&gt; +	stage2_pmd_free(NULL, pmd_table);</span>
<span class="quote">&gt;  	put_page(virt_to_page(pud));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void clear_pmd_entry(struct kvm *kvm, pmd_t *pmd, phys_addr_t addr)</span>
<span class="quote">&gt; +static void clear_stage2_pmd_entry(struct kvm *kvm, pmd_t *pmd, phys_addr_t addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pte_t *pte_table = pte_offset_kernel(pmd, 0);</span>
<span class="quote">&gt;  	VM_BUG_ON(pmd_thp_or_huge(*pmd));</span>
<span class="quote">&gt; @@ -201,7 +201,7 @@ static void clear_pmd_entry(struct kvm *kvm, pmd_t *pmd, phys_addr_t addr)</span>
<span class="quote">&gt;   * the corresponding TLBs, we call kvm_flush_dcache_p*() to make sure</span>
<span class="quote">&gt;   * the IO subsystem will never hit in the cache.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static void unmap_ptes(struct kvm *kvm, pmd_t *pmd,</span>
<span class="quote">&gt; +static void unmap_stage2_ptes(struct kvm *kvm, pmd_t *pmd,</span>
<span class="quote">&gt;  		       phys_addr_t addr, phys_addr_t end)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	phys_addr_t start_addr = addr;</span>
<span class="quote">&gt; @@ -223,19 +223,19 @@ static void unmap_ptes(struct kvm *kvm, pmd_t *pmd,</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	} while (pte++, addr += PAGE_SIZE, addr != end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (kvm_pte_table_empty(kvm, start_pte))</span>
<span class="quote">&gt; -		clear_pmd_entry(kvm, pmd, start_addr);</span>
<span class="quote">&gt; +	if (stage2_pte_table_empty(start_pte))</span>
<span class="quote">&gt; +		clear_stage2_pmd_entry(kvm, pmd, start_addr);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void unmap_pmds(struct kvm *kvm, pud_t *pud,</span>
<span class="quote">&gt; +static void unmap_stage2_pmds(struct kvm *kvm, pud_t *pud,</span>
<span class="quote">&gt;  		       phys_addr_t addr, phys_addr_t end)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	phys_addr_t next, start_addr = addr;</span>
<span class="quote">&gt;  	pmd_t *pmd, *start_pmd;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	start_pmd = pmd = pmd_offset(pud, addr);</span>
<span class="quote">&gt; +	start_pmd = pmd = stage2_pmd_offset(pud, addr);</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt; -		next = kvm_pmd_addr_end(addr, end);</span>
<span class="quote">&gt; +		next = stage2_pmd_addr_end(addr, end);</span>
<span class="quote">&gt;  		if (!pmd_none(*pmd)) {</span>
<span class="quote">&gt;  			if (pmd_thp_or_huge(*pmd)) {</span>
<span class="quote">&gt;  				pmd_t old_pmd = *pmd;</span>
<span class="quote">&gt; @@ -247,57 +247,64 @@ static void unmap_pmds(struct kvm *kvm, pud_t *pud,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  				put_page(virt_to_page(pmd));</span>
<span class="quote">&gt;  			} else {</span>
<span class="quote">&gt; -				unmap_ptes(kvm, pmd, addr, next);</span>
<span class="quote">&gt; +				unmap_stage2_ptes(kvm, pmd, addr, next);</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	} while (pmd++, addr = next, addr != end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (kvm_pmd_table_empty(kvm, start_pmd))</span>
<span class="quote">&gt; -		clear_pud_entry(kvm, pud, start_addr);</span>
<span class="quote">&gt; +	if (stage2_pmd_table_empty(start_pmd))</span>
<span class="quote">&gt; +		clear_stage2_pud_entry(kvm, pud, start_addr);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void unmap_puds(struct kvm *kvm, pgd_t *pgd,</span>
<span class="quote">&gt; +static void unmap_stage2_puds(struct kvm *kvm, pgd_t *pgd,</span>
<span class="quote">&gt;  		       phys_addr_t addr, phys_addr_t end)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	phys_addr_t next, start_addr = addr;</span>
<span class="quote">&gt;  	pud_t *pud, *start_pud;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	start_pud = pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; +	start_pud = pud = stage2_pud_offset(pgd, addr);</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt; -		next = kvm_pud_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (!pud_none(*pud)) {</span>
<span class="quote">&gt; -			if (pud_huge(*pud)) {</span>
<span class="quote">&gt; +		next = stage2_pud_addr_end(addr, end);</span>
<span class="quote">&gt; +		if (!stage2_pud_none(*pud)) {</span>
<span class="quote">&gt; +			if (stage2_pud_huge(*pud)) {</span>
<span class="quote">&gt;  				pud_t old_pud = *pud;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -				pud_clear(pud);</span>
<span class="quote">&gt; +				stage2_pud_clear(pud);</span>
<span class="quote">&gt;  				kvm_tlb_flush_vmid_ipa(kvm, addr);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  				kvm_flush_dcache_pud(old_pud);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  				put_page(virt_to_page(pud));</span>
<span class="quote">&gt;  			} else {</span>
<span class="quote">&gt; -				unmap_pmds(kvm, pud, addr, next);</span>
<span class="quote">&gt; +				unmap_stage2_pmds(kvm, pud, addr, next);</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	} while (pud++, addr = next, addr != end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (kvm_pud_table_empty(kvm, start_pud))</span>
<span class="quote">&gt; -		clear_pgd_entry(kvm, pgd, start_addr);</span>
<span class="quote">&gt; +	if (stage2_pud_table_empty(start_pud))</span>
<span class="quote">&gt; +		clear_stage2_pgd_entry(kvm, pgd, start_addr);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static void unmap_range(struct kvm *kvm, pgd_t *pgdp,</span>
<span class="quote">&gt; -			phys_addr_t start, u64 size)</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * unmap_stage2_range -- Clear stage2 page table entries to unmap a range</span>
<span class="quote">&gt; + * @kvm:   The VM pointer</span>
<span class="quote">&gt; + * @start: The intermediate physical base address of the range to unmap</span>
<span class="quote">&gt; + * @size:  The size of the area to unmap</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Clear a range of stage-2 mappings, lowering the various ref-counts.  Must</span>
<span class="quote">&gt; + * be called while holding mmu_lock (unless for freeing the stage2 pgd before</span>
<span class="quote">&gt; + * destroying the VM), otherwise another faulting VCPU may come in and mess</span>
<span class="quote">&gt; + * with things behind our backs.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static void unmap_stage2_range(struct kvm *kvm, phys_addr_t start, u64 size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pgd_t *pgd;</span>
<span class="quote">&gt;  	phys_addr_t addr = start, end = start + size;</span>
<span class="quote">&gt;  	phys_addr_t next;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	pgd = pgdp + kvm_pgd_index(addr);</span>
<span class="quote">&gt; +	pgd = kvm-&gt;arch.pgd + stage2_pgd_index(addr);</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt; -		next = kvm_pgd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (!pgd_none(*pgd))</span>
<span class="quote">&gt; -			unmap_puds(kvm, pgd, addr, next);</span>
<span class="quote">&gt; +		next = stage2_pgd_addr_end(addr, end);</span>
<span class="quote">&gt; +		if (!stage2_pgd_none(*pgd))</span>
<span class="quote">&gt; +			unmap_stage2_puds(kvm, pgd, addr, next);</span>
<span class="quote">&gt;  	} while (pgd++, addr = next, addr != end);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -811,22 +818,6 @@ int kvm_alloc_stage2_pgd(struct kvm *kvm)</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/**</span>
<span class="quote">&gt; - * unmap_stage2_range -- Clear stage2 page table entries to unmap a range</span>
<span class="quote">&gt; - * @kvm:   The VM pointer</span>
<span class="quote">&gt; - * @start: The intermediate physical base address of the range to unmap</span>
<span class="quote">&gt; - * @size:  The size of the area to unmap</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * Clear a range of stage-2 mappings, lowering the various ref-counts.  Must</span>
<span class="quote">&gt; - * be called while holding mmu_lock (unless for freeing the stage2 pgd before</span>
<span class="quote">&gt; - * destroying the VM), otherwise another faulting VCPU may come in and mess</span>
<span class="quote">&gt; - * with things behind our backs.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -static void unmap_stage2_range(struct kvm *kvm, phys_addr_t start, u64 size)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	unmap_range(kvm, kvm-&gt;arch.pgd, start, size);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static void stage2_unmap_memslot(struct kvm *kvm,</span>
<span class="quote">&gt;  				 struct kvm_memory_slot *memslot)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 1.7.9.5</span>
<span class="quote">&gt; </span>
<span class="acked-by">
Acked-by: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=114661">Suzuki K. Poulose</a> - April 8, 2016, 3:37 p.m.</div>
<pre class="content">
On 08/04/16 14:42, Christoffer Dall wrote:
<span class="quote">&gt; On Mon, Apr 04, 2016 at 05:26:13PM +0100, Suzuki K Poulose wrote:</span>
<span class="quote">
&gt;&gt;</span>
<span class="quote">&gt;&gt; -static void clear_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="quote">&gt;&gt; +static void clear_stage2_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt; -	pmd_t *pmd_table = pmd_offset(pud, 0);</span>
<span class="quote">&gt;&gt; -	VM_BUG_ON(pud_huge(*pud));</span>
<span class="quote">&gt;&gt; -	pud_clear(pud);</span>
<span class="quote">&gt;&gt; +	pmd_t *pmd_table __maybe_unused = stage2_pmd_offset(pud, 0);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The __maybe_unused are slightly ugly, so it may be nicer to create the</span>
<span class="quote">&gt; stage2_pmd_free() as static inline&#39;s if they&#39;re defined to do nothing</span>
<span class="quote">&gt; instead.</span>
<span class="quote">&gt;</span>

Sure, we could do that for stage2. However, we will need to fix the host helpers
as well for making such a change in the _hyp version (for 16K + 36bit VA).
<span class="quote">
&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Acked-by: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="quote">&gt;</span>


Thanks
Suzuki
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=68151">Christoffer Dall</a> - April 8, 2016, 5:03 p.m.</div>
<pre class="content">
On Fri, Apr 08, 2016 at 04:37:02PM +0100, Suzuki K Poulose wrote:
<span class="quote">&gt; On 08/04/16 14:42, Christoffer Dall wrote:</span>
<span class="quote">&gt; &gt;On Mon, Apr 04, 2016 at 05:26:13PM +0100, Suzuki K Poulose wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;-static void clear_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="quote">&gt; &gt;&gt;+static void clear_stage2_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="quote">&gt; &gt;&gt;  {</span>
<span class="quote">&gt; &gt;&gt;-	pmd_t *pmd_table = pmd_offset(pud, 0);</span>
<span class="quote">&gt; &gt;&gt;-	VM_BUG_ON(pud_huge(*pud));</span>
<span class="quote">&gt; &gt;&gt;-	pud_clear(pud);</span>
<span class="quote">&gt; &gt;&gt;+	pmd_t *pmd_table __maybe_unused = stage2_pmd_offset(pud, 0);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;The __maybe_unused are slightly ugly, so it may be nicer to create the</span>
<span class="quote">&gt; &gt;stage2_pmd_free() as static inline&#39;s if they&#39;re defined to do nothing</span>
<span class="quote">&gt; &gt;instead.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure, we could do that for stage2. However, we will need to fix the host helpers</span>
<span class="quote">&gt; as well for making such a change in the _hyp version (for 16K + 36bit VA).</span>
<span class="quote">&gt; </span>

I thought the host helpers were already done like that, since we don&#39;t
need the __maybe_unused currently.  If it involves changing core code
etc. then don&#39;t bother.

Thanks,
-Christoffer
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=114661">Suzuki K. Poulose</a> - April 8, 2016, 5:07 p.m.</div>
<pre class="content">
On 08/04/16 18:03, Christoffer Dall wrote:
<span class="quote">&gt; On Fri, Apr 08, 2016 at 04:37:02PM +0100, Suzuki K Poulose wrote:</span>
<span class="quote">&gt;&gt; On 08/04/16 14:42, Christoffer Dall wrote:</span>
<span class="quote">&gt;&gt;&gt; On Mon, Apr 04, 2016 at 05:26:13PM +0100, Suzuki K Poulose wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; -static void clear_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="quote">&gt;&gt;&gt;&gt; +static void clear_stage2_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="quote">&gt;&gt;&gt;&gt;   {</span>
<span class="quote">&gt;&gt;&gt;&gt; -	pmd_t *pmd_table = pmd_offset(pud, 0);</span>
<span class="quote">&gt;&gt;&gt;&gt; -	VM_BUG_ON(pud_huge(*pud));</span>
<span class="quote">&gt;&gt;&gt;&gt; -	pud_clear(pud);</span>
<span class="quote">&gt;&gt;&gt;&gt; +	pmd_t *pmd_table __maybe_unused = stage2_pmd_offset(pud, 0);</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The __maybe_unused are slightly ugly, so it may be nicer to create the</span>
<span class="quote">&gt;&gt;&gt; stage2_pmd_free() as static inline&#39;s if they&#39;re defined to do nothing</span>
<span class="quote">&gt;&gt;&gt; instead.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Sure, we could do that for stage2. However, we will need to fix the host helpers</span>
<span class="quote">&gt;&gt; as well for making such a change in the _hyp version (for 16K + 36bit VA).</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I thought the host helpers were already done like that, since we don&#39;t</span>
<span class="quote">&gt; need the __maybe_unused currently.  If it involves changing core code</span>
<span class="quote">&gt; etc. then don&#39;t bother.</span>

Unfortunately no, e.g,

include/asm-generic/pgtable-nopud.h defines:

#define pud_free(mm, x)                         do { } while (0)

Cheers
Suzuki
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=68151">Christoffer Dall</a> - April 8, 2016, 5:25 p.m.</div>
<pre class="content">
On Fri, Apr 08, 2016 at 06:07:13PM +0100, Suzuki K Poulose wrote:
<span class="quote">&gt; On 08/04/16 18:03, Christoffer Dall wrote:</span>
<span class="quote">&gt; &gt;On Fri, Apr 08, 2016 at 04:37:02PM +0100, Suzuki K Poulose wrote:</span>
<span class="quote">&gt; &gt;&gt;On 08/04/16 14:42, Christoffer Dall wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;On Mon, Apr 04, 2016 at 05:26:13PM +0100, Suzuki K Poulose wrote:</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;-static void clear_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;+static void clear_stage2_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;  {</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;-	pmd_t *pmd_table = pmd_offset(pud, 0);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;-	VM_BUG_ON(pud_huge(*pud));</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;-	pud_clear(pud);</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;+	pmd_t *pmd_table __maybe_unused = stage2_pmd_offset(pud, 0);</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;The __maybe_unused are slightly ugly, so it may be nicer to create the</span>
<span class="quote">&gt; &gt;&gt;&gt;stage2_pmd_free() as static inline&#39;s if they&#39;re defined to do nothing</span>
<span class="quote">&gt; &gt;&gt;&gt;instead.</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;Sure, we could do that for stage2. However, we will need to fix the host helpers</span>
<span class="quote">&gt; &gt;&gt;as well for making such a change in the _hyp version (for 16K + 36bit VA).</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;I thought the host helpers were already done like that, since we don&#39;t</span>
<span class="quote">&gt; &gt;need the __maybe_unused currently.  If it involves changing core code</span>
<span class="quote">&gt; &gt;etc. then don&#39;t bother.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Unfortunately no, e.g,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; include/asm-generic/pgtable-nopud.h defines:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; #define pud_free(mm, x)                         do { } while (0)</span>
<span class="quote">&gt; </span>

Leave it then :)

Thanks,
-Christoffer
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c</span>
<span class="p_header">index 2b491e5..0009a24 100644</span>
<span class="p_header">--- a/arch/arm/kvm/mmu.c</span>
<span class="p_header">+++ b/arch/arm/kvm/mmu.c</span>
<span class="p_chunk">@@ -152,26 +152,26 @@</span> <span class="p_context"> static void *mmu_memory_cache_alloc(struct kvm_mmu_memory_cache *mc)</span>
 	return p;
 }
 
<span class="p_del">-static void clear_pgd_entry(struct kvm *kvm, pgd_t *pgd, phys_addr_t addr)</span>
<span class="p_add">+static void clear_stage2_pgd_entry(struct kvm *kvm, pgd_t *pgd, phys_addr_t addr)</span>
 {
<span class="p_del">-	pud_t *pud_table __maybe_unused = pud_offset(pgd, 0);</span>
<span class="p_del">-	pgd_clear(pgd);</span>
<span class="p_add">+	pud_t *pud_table __maybe_unused = stage2_pud_offset(pgd, 0UL);</span>
<span class="p_add">+	stage2_pgd_clear(pgd);</span>
 	kvm_tlb_flush_vmid_ipa(kvm, addr);
<span class="p_del">-	pud_free(NULL, pud_table);</span>
<span class="p_add">+	stage2_pud_free(NULL, pud_table);</span>
 	put_page(virt_to_page(pgd));
 }
 
<span class="p_del">-static void clear_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
<span class="p_add">+static void clear_stage2_pud_entry(struct kvm *kvm, pud_t *pud, phys_addr_t addr)</span>
 {
<span class="p_del">-	pmd_t *pmd_table = pmd_offset(pud, 0);</span>
<span class="p_del">-	VM_BUG_ON(pud_huge(*pud));</span>
<span class="p_del">-	pud_clear(pud);</span>
<span class="p_add">+	pmd_t *pmd_table __maybe_unused = stage2_pmd_offset(pud, 0);</span>
<span class="p_add">+	VM_BUG_ON(stage2_pud_huge(*pud));</span>
<span class="p_add">+	stage2_pud_clear(pud);</span>
 	kvm_tlb_flush_vmid_ipa(kvm, addr);
<span class="p_del">-	pmd_free(NULL, pmd_table);</span>
<span class="p_add">+	stage2_pmd_free(NULL, pmd_table);</span>
 	put_page(virt_to_page(pud));
 }
 
<span class="p_del">-static void clear_pmd_entry(struct kvm *kvm, pmd_t *pmd, phys_addr_t addr)</span>
<span class="p_add">+static void clear_stage2_pmd_entry(struct kvm *kvm, pmd_t *pmd, phys_addr_t addr)</span>
 {
 	pte_t *pte_table = pte_offset_kernel(pmd, 0);
 	VM_BUG_ON(pmd_thp_or_huge(*pmd));
<span class="p_chunk">@@ -201,7 +201,7 @@</span> <span class="p_context"> static void clear_pmd_entry(struct kvm *kvm, pmd_t *pmd, phys_addr_t addr)</span>
  * the corresponding TLBs, we call kvm_flush_dcache_p*() to make sure
  * the IO subsystem will never hit in the cache.
  */
<span class="p_del">-static void unmap_ptes(struct kvm *kvm, pmd_t *pmd,</span>
<span class="p_add">+static void unmap_stage2_ptes(struct kvm *kvm, pmd_t *pmd,</span>
 		       phys_addr_t addr, phys_addr_t end)
 {
 	phys_addr_t start_addr = addr;
<span class="p_chunk">@@ -223,19 +223,19 @@</span> <span class="p_context"> static void unmap_ptes(struct kvm *kvm, pmd_t *pmd,</span>
 		}
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 
<span class="p_del">-	if (kvm_pte_table_empty(kvm, start_pte))</span>
<span class="p_del">-		clear_pmd_entry(kvm, pmd, start_addr);</span>
<span class="p_add">+	if (stage2_pte_table_empty(start_pte))</span>
<span class="p_add">+		clear_stage2_pmd_entry(kvm, pmd, start_addr);</span>
 }
 
<span class="p_del">-static void unmap_pmds(struct kvm *kvm, pud_t *pud,</span>
<span class="p_add">+static void unmap_stage2_pmds(struct kvm *kvm, pud_t *pud,</span>
 		       phys_addr_t addr, phys_addr_t end)
 {
 	phys_addr_t next, start_addr = addr;
 	pmd_t *pmd, *start_pmd;
 
<span class="p_del">-	start_pmd = pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+	start_pmd = pmd = stage2_pmd_offset(pud, addr);</span>
 	do {
<span class="p_del">-		next = kvm_pmd_addr_end(addr, end);</span>
<span class="p_add">+		next = stage2_pmd_addr_end(addr, end);</span>
 		if (!pmd_none(*pmd)) {
 			if (pmd_thp_or_huge(*pmd)) {
 				pmd_t old_pmd = *pmd;
<span class="p_chunk">@@ -247,57 +247,64 @@</span> <span class="p_context"> static void unmap_pmds(struct kvm *kvm, pud_t *pud,</span>
 
 				put_page(virt_to_page(pmd));
 			} else {
<span class="p_del">-				unmap_ptes(kvm, pmd, addr, next);</span>
<span class="p_add">+				unmap_stage2_ptes(kvm, pmd, addr, next);</span>
 			}
 		}
 	} while (pmd++, addr = next, addr != end);
 
<span class="p_del">-	if (kvm_pmd_table_empty(kvm, start_pmd))</span>
<span class="p_del">-		clear_pud_entry(kvm, pud, start_addr);</span>
<span class="p_add">+	if (stage2_pmd_table_empty(start_pmd))</span>
<span class="p_add">+		clear_stage2_pud_entry(kvm, pud, start_addr);</span>
 }
 
<span class="p_del">-static void unmap_puds(struct kvm *kvm, pgd_t *pgd,</span>
<span class="p_add">+static void unmap_stage2_puds(struct kvm *kvm, pgd_t *pgd,</span>
 		       phys_addr_t addr, phys_addr_t end)
 {
 	phys_addr_t next, start_addr = addr;
 	pud_t *pud, *start_pud;
 
<span class="p_del">-	start_pud = pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	start_pud = pud = stage2_pud_offset(pgd, addr);</span>
 	do {
<span class="p_del">-		next = kvm_pud_addr_end(addr, end);</span>
<span class="p_del">-		if (!pud_none(*pud)) {</span>
<span class="p_del">-			if (pud_huge(*pud)) {</span>
<span class="p_add">+		next = stage2_pud_addr_end(addr, end);</span>
<span class="p_add">+		if (!stage2_pud_none(*pud)) {</span>
<span class="p_add">+			if (stage2_pud_huge(*pud)) {</span>
 				pud_t old_pud = *pud;
 
<span class="p_del">-				pud_clear(pud);</span>
<span class="p_add">+				stage2_pud_clear(pud);</span>
 				kvm_tlb_flush_vmid_ipa(kvm, addr);
<span class="p_del">-</span>
 				kvm_flush_dcache_pud(old_pud);
<span class="p_del">-</span>
 				put_page(virt_to_page(pud));
 			} else {
<span class="p_del">-				unmap_pmds(kvm, pud, addr, next);</span>
<span class="p_add">+				unmap_stage2_pmds(kvm, pud, addr, next);</span>
 			}
 		}
 	} while (pud++, addr = next, addr != end);
 
<span class="p_del">-	if (kvm_pud_table_empty(kvm, start_pud))</span>
<span class="p_del">-		clear_pgd_entry(kvm, pgd, start_addr);</span>
<span class="p_add">+	if (stage2_pud_table_empty(start_pud))</span>
<span class="p_add">+		clear_stage2_pgd_entry(kvm, pgd, start_addr);</span>
 }
 
<span class="p_del">-</span>
<span class="p_del">-static void unmap_range(struct kvm *kvm, pgd_t *pgdp,</span>
<span class="p_del">-			phys_addr_t start, u64 size)</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * unmap_stage2_range -- Clear stage2 page table entries to unmap a range</span>
<span class="p_add">+ * @kvm:   The VM pointer</span>
<span class="p_add">+ * @start: The intermediate physical base address of the range to unmap</span>
<span class="p_add">+ * @size:  The size of the area to unmap</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Clear a range of stage-2 mappings, lowering the various ref-counts.  Must</span>
<span class="p_add">+ * be called while holding mmu_lock (unless for freeing the stage2 pgd before</span>
<span class="p_add">+ * destroying the VM), otherwise another faulting VCPU may come in and mess</span>
<span class="p_add">+ * with things behind our backs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void unmap_stage2_range(struct kvm *kvm, phys_addr_t start, u64 size)</span>
 {
 	pgd_t *pgd;
 	phys_addr_t addr = start, end = start + size;
 	phys_addr_t next;
 
<span class="p_del">-	pgd = pgdp + kvm_pgd_index(addr);</span>
<span class="p_add">+	pgd = kvm-&gt;arch.pgd + stage2_pgd_index(addr);</span>
 	do {
<span class="p_del">-		next = kvm_pgd_addr_end(addr, end);</span>
<span class="p_del">-		if (!pgd_none(*pgd))</span>
<span class="p_del">-			unmap_puds(kvm, pgd, addr, next);</span>
<span class="p_add">+		next = stage2_pgd_addr_end(addr, end);</span>
<span class="p_add">+		if (!stage2_pgd_none(*pgd))</span>
<span class="p_add">+			unmap_stage2_puds(kvm, pgd, addr, next);</span>
 	} while (pgd++, addr = next, addr != end);
 }
 
<span class="p_chunk">@@ -811,22 +818,6 @@</span> <span class="p_context"> int kvm_alloc_stage2_pgd(struct kvm *kvm)</span>
 	return 0;
 }
 
<span class="p_del">-/**</span>
<span class="p_del">- * unmap_stage2_range -- Clear stage2 page table entries to unmap a range</span>
<span class="p_del">- * @kvm:   The VM pointer</span>
<span class="p_del">- * @start: The intermediate physical base address of the range to unmap</span>
<span class="p_del">- * @size:  The size of the area to unmap</span>
<span class="p_del">- *</span>
<span class="p_del">- * Clear a range of stage-2 mappings, lowering the various ref-counts.  Must</span>
<span class="p_del">- * be called while holding mmu_lock (unless for freeing the stage2 pgd before</span>
<span class="p_del">- * destroying the VM), otherwise another faulting VCPU may come in and mess</span>
<span class="p_del">- * with things behind our backs.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void unmap_stage2_range(struct kvm *kvm, phys_addr_t start, u64 size)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unmap_range(kvm, kvm-&gt;arch.pgd, start, size);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void stage2_unmap_memslot(struct kvm *kvm,
 				 struct kvm_memory_slot *memslot)
 {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



