
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4/5] KVM: x86: Add a return value to skip_emulated_instruction and propagate it. - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4/5] KVM: x86: Add a return value to skip_emulated_instruction and propagate it.</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=134881">Kyle Huey</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 28, 2016, 4:18 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20161128041856.11420-5-khuey@kylehuey.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9449087/mbox/"
   >mbox</a>
|
   <a href="/patch/9449087/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9449087/">/patch/9449087/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1BC356071E for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 28 Nov 2016 04:20:16 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 06A4320499
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 28 Nov 2016 04:20:16 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id EF96B20952; Mon, 28 Nov 2016 04:20:15 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,DKIM_VALID_AU,RCVD_IN_DNSWL_HI,RCVD_IN_SORBS_SPAM
	autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 23F33204C1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 28 Nov 2016 04:20:14 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754105AbcK1ETw (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sun, 27 Nov 2016 23:19:52 -0500
Received: from mail-pg0-f66.google.com ([74.125.83.66]:35751 &quot;EHLO
	mail-pg0-f66.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753972AbcK1ETW (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sun, 27 Nov 2016 23:19:22 -0500
Received: by mail-pg0-f66.google.com with SMTP id p66so11886946pga.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Sun, 27 Nov 2016 20:19:21 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=kylehuey.com; s=google;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=HJQV9O7aVKtroOsaLDwGeIvqsj5eRhek1ZQKfmcqxj8=;
	b=AKM4L06Iu1g2vugEQleYYmKZkHCG3iYn1m1TmzsCEb+d7EglNE/PWu6P1myM/ZGgO/
	zERqjx3tsCVGw1Y0yaPVbjyVvtwtfONjdBkpYluYimM9R8t51+TzrWtFGPm8ozBCYFOb
	xus3g8NWdln199XffJwY0qE1qkeo9kHJVP7cZ5dIv+htd6AntnJjTXexPN34H273xhKV
	6ElegQ8Cz5l+MY9cUvFyZsQPtDOcgiN2qNJL2hJUuRIbY5uDCQB84BFfjEOjGeIkilku
	PhcIvhT1mS806jkwingyXqzvqMyIFWaEfviJPAcdQRmNE1DkkZBVZlk52n0A/lCzUjHa
	o0qA==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=HJQV9O7aVKtroOsaLDwGeIvqsj5eRhek1ZQKfmcqxj8=;
	b=k1liMkzBRgX2u9A97S661AMX9wu5Ax8ZEX6RyhzHojMWPs4vhNvHlE9jGkmnBAr/By
	+HfWnywUPgbdfSnq+Z0RgMC9TMr0T/JJExUxbGwRBglDKHp4h2u3Vg7v2qBoqf3zP7lA
	4gUzQx/W+3kHNOEYQCMKwzEvAoBb6A/24De2SfOuYw6OPkLv4l5KmugbdO500d315uVM
	p9Xapu0KZF3m9iS2BNkRbRHPV10v+MsvYrxm7+CGNOTPeETSXdv+wpRMYOqJ8ysymScO
	nj51vmyhYwB8N0eKuGe8kEIEqASX7ffjCWBLxjU5SWJ0u185bRvsxxhbpluMXc4q0uZr
	QH8g==
X-Gm-Message-State: AKaTC02PuHLwkPTfty9GK3UhR201+mxgoROZIJF5SK67YsTDWgkDEyNlitMdsFqvwZtb8w==
X-Received: by 10.84.209.161 with SMTP id y30mr45769698plh.163.1480306760687;
	Sun, 27 Nov 2016 20:19:20 -0800 (PST)
Received: from minbar.hsd1.ca.comcast.net
	(c-73-162-102-141.hsd1.ca.comcast.net. [73.162.102.141])
	by smtp.gmail.com with ESMTPSA id
	t89sm83037711pfe.50.2016.11.27.20.19.19
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Sun, 27 Nov 2016 20:19:20 -0800 (PST)
From: Kyle Huey &lt;me@kylehuey.com&gt;
X-Google-Original-From: Kyle Huey &lt;khuey@kylehuey.com&gt;
To: Paolo Bonzini &lt;pbonzini@redhat.com&gt;,
	=?UTF-8?q?Radim=20Kr=C4=8Dm=C3=A1=C5=99?= &lt;rkrcmar@redhat.com&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, x86@kernel.org,
	Joerg Roedel &lt;joro@8bytes.org&gt;
Cc: kvm@vger.kernel.org, linux-kernel@vger.kernel.org
Subject: [PATCH 4/5] KVM: x86: Add a return value to
	skip_emulated_instruction and propagate it.
Date: Sun, 27 Nov 2016 20:18:55 -0800
Message-Id: &lt;20161128041856.11420-5-khuey@kylehuey.com&gt;
X-Mailer: git-send-email 2.10.2
In-Reply-To: &lt;20161128041856.11420-1-khuey@kylehuey.com&gt;
References: &lt;20161128041856.11420-1-khuey@kylehuey.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=134881">Kyle Huey</a> - Nov. 28, 2016, 4:18 a.m.</div>
<pre class="content">
Return 1 (for the moment) from skip_emulated_instruction and propagate it
through. This is straightforward, except for:

- ICEBP, which is already inside a trap, so avoid triggering another trap.
- Instructions that can trigger exits to userspace, such as the IO insns,
  MOVs to CR8, and HALT. If skip_emulated_instruction does trigger a
  KVM_GUESTDBG_SINGLESTEP exit, and the handling code for
  IN/OUT/MOV CR8/HALT also triggers an exit to userspace, the latter will
  take precedence. The singlestep will be triggered again on the next
  instruction, which is the current behavior.
- XSAVES/XRSTORS, which should never be called.
- Task switch instructions which would require additional handling (e.g.
  the task switch bit) and are instead modified to use the no_trap
  variant.
<span class="signed-off-by">
Signed-off-by: Kyle Huey &lt;khuey@kylehuey.com&gt;</span>
---
 arch/x86/include/asm/kvm_host.h |   4 +-
 arch/x86/kvm/cpuid.c            |   3 +-
 arch/x86/kvm/svm.c              |  16 ++--
 arch/x86/kvm/vmx.c              | 175 ++++++++++++++++------------------------
 arch/x86/kvm/x86.c              |  22 +++--
 5 files changed, 92 insertions(+), 128 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/kvm_host.h b/arch/x86/include/asm/kvm_host.h</span>
<span class="p_header">index 80bad5c..f846610 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/kvm_host.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/kvm_host.h</span>
<span class="p_chunk">@@ -919,17 +919,17 @@</span> <span class="p_context"> struct kvm_x86_ops {</span>
 	u32 (*get_pkru)(struct kvm_vcpu *vcpu);
 	void (*fpu_activate)(struct kvm_vcpu *vcpu);
 	void (*fpu_deactivate)(struct kvm_vcpu *vcpu);
 
 	void (*tlb_flush)(struct kvm_vcpu *vcpu);
 
 	void (*run)(struct kvm_vcpu *vcpu);
 	int (*handle_exit)(struct kvm_vcpu *vcpu);
<span class="p_del">-	void (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);</span>
<span class="p_add">+	int (*skip_emulated_instruction)(struct kvm_vcpu *vcpu);</span>
 	void (*set_interrupt_shadow)(struct kvm_vcpu *vcpu, int mask);
 	u32 (*get_interrupt_shadow)(struct kvm_vcpu *vcpu);
 	void (*patch_hypercall)(struct kvm_vcpu *vcpu,
 				unsigned char *hypercall_addr);
 	void (*set_irq)(struct kvm_vcpu *vcpu);
 	void (*set_nmi)(struct kvm_vcpu *vcpu);
 	void (*queue_exception)(struct kvm_vcpu *vcpu, unsigned nr,
 				bool has_error_code, u32 error_code,
<span class="p_chunk">@@ -1363,17 +1363,17 @@</span> <span class="p_context"> void kvm_arch_async_page_not_present(struct kvm_vcpu *vcpu,</span>
 				     struct kvm_async_pf *work);
 void kvm_arch_async_page_present(struct kvm_vcpu *vcpu,
 				 struct kvm_async_pf *work);
 void kvm_arch_async_page_ready(struct kvm_vcpu *vcpu,
 			       struct kvm_async_pf *work);
 bool kvm_arch_can_inject_async_page_present(struct kvm_vcpu *vcpu);
 extern bool kvm_find_async_pf_gfn(struct kvm_vcpu *vcpu, gfn_t gfn);
 
<span class="p_del">-void kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);</span>
<span class="p_add">+int kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err);</span>
 
 int kvm_is_in_guest(void);
 
 int __x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size);
 int x86_set_memory_region(struct kvm *kvm, int id, gpa_t gpa, u32 size);
 bool kvm_vcpu_is_reset_bsp(struct kvm_vcpu *vcpu);
 bool kvm_vcpu_is_bsp(struct kvm_vcpu *vcpu);
 
<span class="p_header">diff --git a/arch/x86/kvm/cpuid.c b/arch/x86/kvm/cpuid.c</span>
<span class="p_header">index 07cc629..d733ac2 100644</span>
<span class="p_header">--- a/arch/x86/kvm/cpuid.c</span>
<span class="p_header">+++ b/arch/x86/kvm/cpuid.c</span>
<span class="p_chunk">@@ -885,12 +885,11 @@</span> <span class="p_context"> int kvm_emulate_cpuid(struct kvm_vcpu *vcpu)</span>
 
 	eax = kvm_register_read(vcpu, VCPU_REGS_RAX);
 	ecx = kvm_register_read(vcpu, VCPU_REGS_RCX);
 	kvm_cpuid(vcpu, &amp;eax, &amp;ebx, &amp;ecx, &amp;edx);
 	kvm_register_write(vcpu, VCPU_REGS_RAX, eax);
 	kvm_register_write(vcpu, VCPU_REGS_RBX, ebx);
 	kvm_register_write(vcpu, VCPU_REGS_RCX, ecx);
 	kvm_register_write(vcpu, VCPU_REGS_RDX, edx);
<span class="p_del">-	kvm_x86_ops-&gt;skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return kvm_x86_ops-&gt;skip_emulated_instruction(vcpu);</span>
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_cpuid);
<span class="p_header">diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="p_header">index 5bdffcd..ed28be0 100644</span>
<span class="p_header">--- a/arch/x86/kvm/svm.c</span>
<span class="p_header">+++ b/arch/x86/kvm/svm.c</span>
<span class="p_chunk">@@ -603,37 +603,38 @@</span> <span class="p_context"> static void svm_set_interrupt_shadow(struct kvm_vcpu *vcpu, int mask)</span>
 
 	if (mask == 0)
 		svm-&gt;vmcb-&gt;control.int_state &amp;= ~SVM_INTERRUPT_SHADOW_MASK;
 	else
 		svm-&gt;vmcb-&gt;control.int_state |= SVM_INTERRUPT_SHADOW_MASK;
 
 }
 
<span class="p_del">-static void skip_emulated_instruction(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+static int skip_emulated_instruction(struct kvm_vcpu *vcpu)</span>
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 
 	if (svm-&gt;vmcb-&gt;control.next_rip != 0) {
 		WARN_ON_ONCE(!static_cpu_has(X86_FEATURE_NRIPS));
 		svm-&gt;next_rip = svm-&gt;vmcb-&gt;control.next_rip;
 	}
 
 	if (!svm-&gt;next_rip) {
 		if (emulate_instruction(vcpu, EMULTYPE_SKIP) !=
 				EMULATE_DONE)
 			printk(KERN_DEBUG &quot;%s: NOP\n&quot;, __func__);
<span class="p_del">-		return;</span>
<span class="p_add">+		return 1;</span>
 	}
 	if (svm-&gt;next_rip - kvm_rip_read(vcpu) &gt; MAX_INST_SIZE)
 		printk(KERN_ERR &quot;%s: ip 0x%lx next 0x%llx\n&quot;,
 		       __func__, kvm_rip_read(vcpu), svm-&gt;next_rip);
 
 	kvm_rip_write(vcpu, svm-&gt;next_rip);
 	svm_set_interrupt_shadow(vcpu, 0);
<span class="p_add">+	return 1;</span>
 }
 
 static void svm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr,
 				bool has_error_code, u32 error_code,
 				bool reinject)
 {
 	struct vcpu_svm *svm = to_svm(vcpu);
 
<span class="p_chunk">@@ -3146,18 +3147,17 @@</span> <span class="p_context"> static int skinit_interception(struct vcpu_svm *svm)</span>
 	trace_kvm_skinit(svm-&gt;vmcb-&gt;save.rip, kvm_register_read(&amp;svm-&gt;vcpu, VCPU_REGS_RAX));
 
 	kvm_queue_exception(&amp;svm-&gt;vcpu, UD_VECTOR);
 	return 1;
 }
 
 static int wbinvd_interception(struct vcpu_svm *svm)
 {
<span class="p_del">-	kvm_emulate_wbinvd(&amp;svm-&gt;vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return kvm_emulate_wbinvd(&amp;svm-&gt;vcpu);</span>
 }
 
 static int xsetbv_interception(struct vcpu_svm *svm)
 {
 	u64 new_bv = kvm_read_edx_eax(&amp;svm-&gt;vcpu);
 	u32 index = kvm_register_read(&amp;svm-&gt;vcpu, VCPU_REGS_RCX);
 
 	if (kvm_set_xcr(&amp;svm-&gt;vcpu, index, new_bv) == 0) {
<span class="p_chunk">@@ -3270,19 +3270,17 @@</span> <span class="p_context"> static int emulate_on_interception(struct vcpu_svm *svm)</span>
 static int rdpmc_interception(struct vcpu_svm *svm)
 {
 	int err;
 
 	if (!static_cpu_has(X86_FEATURE_NRIPS))
 		return emulate_on_interception(svm);
 
 	err = kvm_rdpmc(&amp;svm-&gt;vcpu);
<span class="p_del">-	kvm_complete_insn_gp(&amp;svm-&gt;vcpu, err);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return kvm_complete_insn_gp(&amp;svm-&gt;vcpu, err);</span>
 }
 
 static bool check_selective_cr0_intercepted(struct vcpu_svm *svm,
 					    unsigned long val)
 {
 	unsigned long cr0 = svm-&gt;vcpu.arch.cr0;
 	bool ret = false;
 	u64 intercept;
<span class="p_chunk">@@ -3369,19 +3367,17 @@</span> <span class="p_context"> static int cr_interception(struct vcpu_svm *svm)</span>
 			break;
 		default:
 			WARN(1, &quot;unhandled read from CR%d&quot;, cr);
 			kvm_queue_exception(&amp;svm-&gt;vcpu, UD_VECTOR);
 			return 1;
 		}
 		kvm_register_write(&amp;svm-&gt;vcpu, reg, val);
 	}
<span class="p_del">-	kvm_complete_insn_gp(&amp;svm-&gt;vcpu, err);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return kvm_complete_insn_gp(&amp;svm-&gt;vcpu, err);</span>
 }
 
 static int dr_interception(struct vcpu_svm *svm)
 {
 	int reg, dr;
 	unsigned long val;
 
 	if (svm-&gt;vcpu.guest_debug == 0) {
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index 9cc4c41..f404aef 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -2467,19 +2467,20 @@</span> <span class="p_context"> static void skip_emulated_instruction_no_trap(struct kvm_vcpu *vcpu)</span>
 	rip = kvm_rip_read(vcpu);
 	rip += vmcs_read32(VM_EXIT_INSTRUCTION_LEN);
 	kvm_rip_write(vcpu, rip);
 
 	/* skipping an emulated instruction also counts */
 	vmx_set_interrupt_shadow(vcpu, 0);
 }
 
<span class="p_del">-static void skip_emulated_instruction(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+static int skip_emulated_instruction(struct kvm_vcpu *vcpu)</span>
 {
 	skip_emulated_instruction_no_trap(vcpu);
<span class="p_add">+	return 1;</span>
 }
 
 /*
  * KVM wants to inject page-faults which it got to the guest. This function
  * checks whether in a nested guest, we need to inject them to L1 or L2.
  */
 static int nested_vmx_check_exception(struct kvm_vcpu *vcpu, unsigned nr)
 {
<span class="p_chunk">@@ -5511,17 +5512,17 @@</span> <span class="p_context"> static int handle_exception(struct kvm_vcpu *vcpu)</span>
 		return 1;
 	case DB_VECTOR:
 		dr6 = vmcs_readl(EXIT_QUALIFICATION);
 		if (!(vcpu-&gt;guest_debug &amp;
 		      (KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP))) {
 			vcpu-&gt;arch.dr6 &amp;= ~15;
 			vcpu-&gt;arch.dr6 |= dr6 | DR6_RTM;
 			if (!(dr6 &amp; ~DR6_RESERVED)) /* icebp */
<span class="p_del">-				skip_emulated_instruction(vcpu);</span>
<span class="p_add">+				skip_emulated_instruction_no_trap(vcpu);</span>
 
 			kvm_queue_exception(vcpu, DB_VECTOR);
 			return 1;
 		}
 		kvm_run-&gt;debug.arch.dr6 = dr6 | DR6_FIXED_1;
 		kvm_run-&gt;debug.arch.dr7 = vmcs_readl(GUEST_DR7);
 		/* fall through */
 	case BP_VECTOR:
<span class="p_chunk">@@ -5556,33 +5557,38 @@</span> <span class="p_context"> static int handle_triple_fault(struct kvm_vcpu *vcpu)</span>
 {
 	vcpu-&gt;run-&gt;exit_reason = KVM_EXIT_SHUTDOWN;
 	return 0;
 }
 
 static int handle_io(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
<span class="p_del">-	int size, in, string;</span>
<span class="p_add">+	int size, in, string, ret;</span>
 	unsigned port;
 
 	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 	string = (exit_qualification &amp; 16) != 0;
 	in = (exit_qualification &amp; 8) != 0;
 
 	++vcpu-&gt;stat.io_exits;
 
 	if (string || in)
 		return emulate_instruction(vcpu, 0) == EMULATE_DONE;
 
 	port = exit_qualification &gt;&gt; 16;
 	size = (exit_qualification &amp; 7) + 1;
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
 
<span class="p_del">-	return kvm_fast_pio_out(vcpu, size, port);</span>
<span class="p_add">+	ret = skip_emulated_instruction(vcpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * TODO: we might be squashing a KVM_GUESTDBG_SINGLESTEP-triggered</span>
<span class="p_add">+	 * KVM_EXIT_DEBUG here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return kvm_fast_pio_out(vcpu, size, port) &amp;&amp; ret;</span>
 }
 
 static void
 vmx_patch_hypercall(struct kvm_vcpu *vcpu, unsigned char *hypercall)
 {
 	/*
 	 * Patch in the VMCALL instruction:
 	 */
<span class="p_chunk">@@ -5670,80 +5676,79 @@</span> <span class="p_context"> static void handle_clts(struct kvm_vcpu *vcpu)</span>
 }
 
 static int handle_cr(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification, val;
 	int cr;
 	int reg;
 	int err;
<span class="p_add">+	int ret;</span>
 
 	exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 	cr = exit_qualification &amp; 15;
 	reg = (exit_qualification &gt;&gt; 8) &amp; 15;
 	switch ((exit_qualification &gt;&gt; 4) &amp; 3) {
 	case 0: /* mov to cr */
 		val = kvm_register_readl(vcpu, reg);
 		trace_kvm_cr_write(cr, val);
 		switch (cr) {
 		case 0:
 			err = handle_set_cr0(vcpu, val);
<span class="p_del">-			kvm_complete_insn_gp(vcpu, err);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return kvm_complete_insn_gp(vcpu, err);</span>
 		case 3:
 			err = kvm_set_cr3(vcpu, val);
<span class="p_del">-			kvm_complete_insn_gp(vcpu, err);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return kvm_complete_insn_gp(vcpu, err);</span>
 		case 4:
 			err = handle_set_cr4(vcpu, val);
<span class="p_del">-			kvm_complete_insn_gp(vcpu, err);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return kvm_complete_insn_gp(vcpu, err);</span>
 		case 8: {
 				u8 cr8_prev = kvm_get_cr8(vcpu);
 				u8 cr8 = (u8)val;
 				err = kvm_set_cr8(vcpu, cr8);
<span class="p_del">-				kvm_complete_insn_gp(vcpu, err);</span>
<span class="p_add">+				ret = kvm_complete_insn_gp(vcpu, err);</span>
 				if (lapic_in_kernel(vcpu))
<span class="p_del">-					return 1;</span>
<span class="p_add">+					return ret;</span>
 				if (cr8_prev &lt;= cr8)
<span class="p_del">-					return 1;</span>
<span class="p_add">+					return ret;</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * TODO: we might be squashing a</span>
<span class="p_add">+				 * KVM_GUESTDBG_SINGLESTEP-triggered</span>
<span class="p_add">+				 * KVM_EXIT_DEBUG here.</span>
<span class="p_add">+				 */</span>
 				vcpu-&gt;run-&gt;exit_reason = KVM_EXIT_SET_TPR;
 				return 0;
 			}
 		}
 		break;
 	case 2: /* clts */
 		handle_clts(vcpu);
 		trace_kvm_cr_write(0, kvm_read_cr0(vcpu));
 		vmx_fpu_activate(vcpu);
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	case 1: /*mov from cr*/
 		switch (cr) {
 		case 3:
 			val = kvm_read_cr3(vcpu);
 			kvm_register_write(vcpu, reg, val);
 			trace_kvm_cr_read(cr, val);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		case 8:
 			val = kvm_get_cr8(vcpu);
 			kvm_register_write(vcpu, reg, val);
 			trace_kvm_cr_read(cr, val);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 		break;
 	case 3: /* lmsw */
 		val = (exit_qualification &gt;&gt; LMSW_SOURCE_DATA_SHIFT) &amp; 0x0f;
 		trace_kvm_cr_write(0, (kvm_read_cr0(vcpu) &amp; ~0xful) | val);
 		kvm_lmsw(vcpu, val);
 
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	default:
 		break;
 	}
 	vcpu-&gt;run-&gt;exit_reason = 0;
 	vcpu_unimpl(vcpu, &quot;unhandled control register: op %d cr %d\n&quot;,
 	       (int)(exit_qualification &gt;&gt; 4) &amp; 3, cr);
 	return 0;
 }
<span class="p_chunk">@@ -5804,18 +5809,17 @@</span> <span class="p_context"> static int handle_dr(struct kvm_vcpu *vcpu)</span>
 
 		if (kvm_get_dr(vcpu, dr, &amp;val))
 			return 1;
 		kvm_register_write(vcpu, reg, val);
 	} else
 		if (kvm_set_dr(vcpu, dr, kvm_register_readl(vcpu, reg)))
 			return 1;
 
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 static u64 vmx_get_dr6(struct kvm_vcpu *vcpu)
 {
 	return vcpu-&gt;arch.dr6;
 }
 
 static void vmx_set_dr6(struct kvm_vcpu *vcpu, unsigned long val)
<span class="p_chunk">@@ -5858,18 +5862,17 @@</span> <span class="p_context"> static int handle_rdmsr(struct kvm_vcpu *vcpu)</span>
 		return 1;
 	}
 
 	trace_kvm_msr_read(ecx, msr_info.data);
 
 	/* FIXME: handling of bits 32:63 of rax, rdx */
 	vcpu-&gt;arch.regs[VCPU_REGS_RAX] = msr_info.data &amp; -1u;
 	vcpu-&gt;arch.regs[VCPU_REGS_RDX] = (msr_info.data &gt;&gt; 32) &amp; -1u;
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 static int handle_wrmsr(struct kvm_vcpu *vcpu)
 {
 	struct msr_data msr;
 	u32 ecx = vcpu-&gt;arch.regs[VCPU_REGS_RCX];
 	u64 data = (vcpu-&gt;arch.regs[VCPU_REGS_RAX] &amp; -1u)
 		| ((u64)(vcpu-&gt;arch.regs[VCPU_REGS_RDX] &amp; -1u) &lt;&lt; 32);
<span class="p_chunk">@@ -5879,18 +5882,17 @@</span> <span class="p_context"> static int handle_wrmsr(struct kvm_vcpu *vcpu)</span>
 	msr.host_initiated = false;
 	if (kvm_set_msr(vcpu, &amp;msr) != 0) {
 		trace_kvm_msr_write_ex(ecx, data);
 		kvm_inject_gp(vcpu, 0);
 		return 1;
 	}
 
 	trace_kvm_msr_write(ecx, data);
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 static int handle_tpr_below_threshold(struct kvm_vcpu *vcpu)
 {
 	kvm_make_request(KVM_REQ_EVENT, vcpu);
 	return 1;
 }
 
<span class="p_chunk">@@ -5924,43 +5926,39 @@</span> <span class="p_context"> static int handle_invd(struct kvm_vcpu *vcpu)</span>
 	return emulate_instruction(vcpu, 0) == EMULATE_DONE;
 }
 
 static int handle_invlpg(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 
 	kvm_mmu_invlpg(vcpu, exit_qualification);
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 static int handle_rdpmc(struct kvm_vcpu *vcpu)
 {
 	int err;
 
 	err = kvm_rdpmc(vcpu);
<span class="p_del">-	kvm_complete_insn_gp(vcpu, err);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return kvm_complete_insn_gp(vcpu, err);</span>
 }
 
 static int handle_wbinvd(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	kvm_emulate_wbinvd(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return kvm_emulate_wbinvd(vcpu);</span>
 }
 
 static int handle_xsetbv(struct kvm_vcpu *vcpu)
 {
 	u64 new_bv = kvm_read_edx_eax(vcpu);
 	u32 index = kvm_register_read(vcpu, VCPU_REGS_RCX);
 
 	if (kvm_set_xcr(vcpu, index, new_bv) == 0)
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	return 1;
 }
 
 static int handle_xsaves(struct kvm_vcpu *vcpu)
 {
 	skip_emulated_instruction(vcpu);
 	WARN(1, &quot;this should never happen\n&quot;);
 	return 1;
<span class="p_chunk">@@ -5984,18 +5982,17 @@</span> <span class="p_context"> static int handle_apic_access(struct kvm_vcpu *vcpu)</span>
 		/*
 		 * Sane guest uses MOV to write EOI, with written value
 		 * not cared. So make a short-circuit here by avoiding
 		 * heavy instruction emulation.
 		 */
 		if ((access_type == TYPE_LINEAR_APIC_INST_WRITE) &amp;&amp;
 		    (offset == APIC_EOI)) {
 			kvm_lapic_set_eoi(vcpu);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 	}
 	return emulate_instruction(vcpu, 0) == EMULATE_DONE;
 }
 
 static int handle_apic_eoi_induced(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
<span class="p_chunk">@@ -6057,17 +6054,17 @@</span> <span class="p_context"> static int handle_task_switch(struct kvm_vcpu *vcpu)</span>
 			break;
 		}
 	}
 	tss_selector = exit_qualification;
 
 	if (!idt_v || (type != INTR_TYPE_HARD_EXCEPTION &amp;&amp;
 		       type != INTR_TYPE_EXT_INTR &amp;&amp;
 		       type != INTR_TYPE_NMI_INTR))
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_add">+		skip_emulated_instruction_no_trap(vcpu);</span>
 
 	if (kvm_task_switch(vcpu, tss_selector,
 			    type == INTR_TYPE_SOFT_INTR ? idt_index : -1, reason,
 			    has_error_code, error_code) == EMULATE_FAIL) {
 		vcpu-&gt;run-&gt;exit_reason = KVM_EXIT_INTERNAL_ERROR;
 		vcpu-&gt;run-&gt;internal.suberror = KVM_INTERNAL_ERROR_EMULATION;
 		vcpu-&gt;run-&gt;internal.ndata = 0;
 		return 0;
<span class="p_chunk">@@ -6134,18 +6131,17 @@</span> <span class="p_context"> static int handle_ept_violation(struct kvm_vcpu *vcpu)</span>
 static int handle_ept_misconfig(struct kvm_vcpu *vcpu)
 {
 	int ret;
 	gpa_t gpa;
 
 	gpa = vmcs_read64(GUEST_PHYSICAL_ADDRESS);
 	if (!kvm_io_bus_write(vcpu, KVM_FAST_MMIO_BUS, gpa, 0, NULL)) {
 		trace_kvm_fast_mmio(gpa);
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	}
 
 	ret = handle_mmio_page_fault(vcpu, gpa, true);
 	if (likely(ret == RET_MMIO_PF_EMULATE))
 		return x86_emulate_instruction(vcpu, gpa, 0, NULL, 0) ==
 					      EMULATE_DONE;
 
 	if (unlikely(ret == RET_MMIO_PF_INVALID))
<span class="p_chunk">@@ -6508,25 +6504,22 @@</span> <span class="p_context"> static __exit void hardware_unsetup(void)</span>
  * exiting, so only get here on cpu with PAUSE-Loop-Exiting.
  */
 static int handle_pause(struct kvm_vcpu *vcpu)
 {
 	if (ple_gap)
 		grow_ple_window(vcpu);
 
 	kvm_vcpu_on_spin(vcpu);
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 static int handle_nop(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 static int handle_mwait(struct kvm_vcpu *vcpu)
 {
 	printk_once(KERN_WARNING &quot;kvm: MWAIT instruction emulated as NOP!\n&quot;);
 	return handle_nop(vcpu);
 }
 
<span class="p_chunk">@@ -6823,59 +6816,53 @@</span> <span class="p_context"> static int nested_vmx_check_vmptr(struct kvm_vcpu *vcpu, int exit_reason,</span>
 		 *
 		 * Note - IA32_VMX_BASIC[48] will never be 1
 		 * for the nested case;
 		 * which replaces physical address width with 32
 		 *
 		 */
 		if (!PAGE_ALIGNED(vmptr) || (vmptr &gt;&gt; maxphyaddr)) {
 			nested_vmx_failInvalid(vcpu);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 
 		page = nested_get_page(vcpu, vmptr);
 		if (page == NULL ||
 		    *(u32 *)kmap(page) != VMCS12_REVISION) {
 			nested_vmx_failInvalid(vcpu);
 			kunmap(page);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 		kunmap(page);
 		vmx-&gt;nested.vmxon_ptr = vmptr;
 		break;
 	case EXIT_REASON_VMCLEAR:
 		if (!PAGE_ALIGNED(vmptr) || (vmptr &gt;&gt; maxphyaddr)) {
 			nested_vmx_failValid(vcpu,
 					     VMXERR_VMCLEAR_INVALID_ADDRESS);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 
 		if (vmptr == vmx-&gt;nested.vmxon_ptr) {
 			nested_vmx_failValid(vcpu,
 					     VMXERR_VMCLEAR_VMXON_POINTER);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 		break;
 	case EXIT_REASON_VMPTRLD:
 		if (!PAGE_ALIGNED(vmptr) || (vmptr &gt;&gt; maxphyaddr)) {
 			nested_vmx_failValid(vcpu,
 					     VMXERR_VMPTRLD_INVALID_ADDRESS);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 
 		if (vmptr == vmx-&gt;nested.vmxon_ptr) {
 			nested_vmx_failValid(vcpu,
 					     VMXERR_VMCLEAR_VMXON_POINTER);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 		break;
 	default:
 		return 1; /* shouldn&#39;t happen */
 	}
 
 	if (vmpointer)
 		*vmpointer = vmptr;
<span class="p_chunk">@@ -6921,18 +6908,17 @@</span> <span class="p_context"> static int handle_vmon(struct kvm_vcpu *vcpu)</span>
 		return 1;
 	}
 
 	if (nested_vmx_check_vmptr(vcpu, EXIT_REASON_VMON, NULL))
 		return 1;
 
 	if (vmx-&gt;nested.vmxon) {
 		nested_vmx_failValid(vcpu, VMXERR_VMXON_IN_VMX_ROOT_OPERATION);
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	}
 
 	if ((vmx-&gt;msr_ia32_feature_control &amp; VMXON_NEEDED_FEATURES)
 			!= VMXON_NEEDED_FEATURES) {
 		kvm_inject_gp(vcpu, 0);
 		return 1;
 	}
 
<span class="p_chunk">@@ -6963,18 +6949,17 @@</span> <span class="p_context"> static int handle_vmon(struct kvm_vcpu *vcpu)</span>
 
 	hrtimer_init(&amp;vmx-&gt;nested.preemption_timer, CLOCK_MONOTONIC,
 		     HRTIMER_MODE_REL_PINNED);
 	vmx-&gt;nested.preemption_timer.function = vmx_preemption_timer_fn;
 
 	vmx-&gt;nested.vmxon = true;
 
 	nested_vmx_succeed(vcpu);
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 
 out_shadow_vmcs:
 	kfree(vmx-&gt;nested.cached_vmcs12);
 
 out_cached_vmcs12:
 	free_page((unsigned long)vmx-&gt;nested.msr_bitmap);
 
 out_msr_bitmap:
<span class="p_chunk">@@ -7084,18 +7069,17 @@</span> <span class="p_context"> static void free_nested(struct vcpu_vmx *vmx)</span>
 
 /* Emulate the VMXOFF instruction */
 static int handle_vmoff(struct kvm_vcpu *vcpu)
 {
 	if (!nested_vmx_check_permission(vcpu))
 		return 1;
 	free_nested(to_vmx(vcpu));
 	nested_vmx_succeed(vcpu);
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 /* Emulate the VMCLEAR instruction */
 static int handle_vmclear(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	gpa_t vmptr;
 	struct vmcs12 *vmcs12;
<span class="p_chunk">@@ -7125,18 +7109,17 @@</span> <span class="p_context"> static int handle_vmclear(struct kvm_vcpu *vcpu)</span>
 	vmcs12 = kmap(page);
 	vmcs12-&gt;launch_state = 0;
 	kunmap(page);
 	nested_release_page(page);
 
 	nested_free_vmcs02(vmx, vmptr);
 
 	nested_vmx_succeed(vcpu);
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch);
 
 /* Emulate the VMLAUNCH instruction */
 static int handle_vmlaunch(struct kvm_vcpu *vcpu)
 {
 	return nested_vmx_run(vcpu, true);
<span class="p_chunk">@@ -7340,28 +7323,25 @@</span> <span class="p_context"> static int handle_vmread(struct kvm_vcpu *vcpu)</span>
 	u64 field_value;
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 	u32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 	gva_t gva = 0;
 
 	if (!nested_vmx_check_permission(vcpu))
 		return 1;
 
<span class="p_del">-	if (!nested_vmx_check_vmcs12(vcpu)) {</span>
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (!nested_vmx_check_vmcs12(vcpu))</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 
 	/* Decode instruction info and find the field to read */
 	field = kvm_register_readl(vcpu, (((vmx_instruction_info) &gt;&gt; 28) &amp; 0xf));
 	/* Read the field, zero-extended to a u64 field_value */
 	if (vmcs12_read_any(vcpu, field, &amp;field_value) &lt; 0) {
 		nested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	}
 	/*
 	 * Now copy part of this value to register or memory, as requested.
 	 * Note that the number of bits actually copied is 32 or 64 depending
 	 * on the guest&#39;s mode (32 or 64 bit), not on the given field&#39;s length.
 	 */
 	if (vmx_instruction_info &amp; (1u &lt;&lt; 10)) {
 		kvm_register_writel(vcpu, (((vmx_instruction_info) &gt;&gt; 3) &amp; 0xf),
<span class="p_chunk">@@ -7371,18 +7351,17 @@</span> <span class="p_context"> static int handle_vmread(struct kvm_vcpu *vcpu)</span>
 				vmx_instruction_info, true, &amp;gva))
 			return 1;
 		/* _system ok, as nested_vmx_check_permission verified cpl=0 */
 		kvm_write_guest_virt_system(&amp;vcpu-&gt;arch.emulate_ctxt, gva,
 			     &amp;field_value, (is_long_mode(vcpu) ? 8 : 4), NULL);
 	}
 
 	nested_vmx_succeed(vcpu);
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 
 static int handle_vmwrite(struct kvm_vcpu *vcpu)
 {
 	unsigned long field;
 	gva_t gva;
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
<span class="p_chunk">@@ -7394,20 +7373,18 @@</span> <span class="p_context"> static int handle_vmwrite(struct kvm_vcpu *vcpu)</span>
 	 * bits into the vmcs12 field.
 	 */
 	u64 field_value = 0;
 	struct x86_exception e;
 
 	if (!nested_vmx_check_permission(vcpu))
 		return 1;
 
<span class="p_del">-	if (!nested_vmx_check_vmcs12(vcpu)) {</span>
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (!nested_vmx_check_vmcs12(vcpu))</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 
 	if (vmx_instruction_info &amp; (1u &lt;&lt; 10))
 		field_value = kvm_register_readl(vcpu,
 			(((vmx_instruction_info) &gt;&gt; 3) &amp; 0xf));
 	else {
 		if (get_vmx_mem_address(vcpu, exit_qualification,
 				vmx_instruction_info, false, &amp;gva))
 			return 1;
<span class="p_chunk">@@ -7418,29 +7395,26 @@</span> <span class="p_context"> static int handle_vmwrite(struct kvm_vcpu *vcpu)</span>
 		}
 	}
 
 
 	field = kvm_register_readl(vcpu, (((vmx_instruction_info) &gt;&gt; 28) &amp; 0xf));
 	if (vmcs_field_readonly(field)) {
 		nested_vmx_failValid(vcpu,
 			VMXERR_VMWRITE_READ_ONLY_VMCS_COMPONENT);
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	}
 
 	if (vmcs12_write_any(vcpu, field, field_value) &lt; 0) {
 		nested_vmx_failValid(vcpu, VMXERR_UNSUPPORTED_VMCS_COMPONENT);
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	}
 
 	nested_vmx_succeed(vcpu);
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 /* Emulate the VMPTRLD instruction */
 static int handle_vmptrld(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	gpa_t vmptr;
 
<span class="p_chunk">@@ -7451,27 +7425,25 @@</span> <span class="p_context"> static int handle_vmptrld(struct kvm_vcpu *vcpu)</span>
 		return 1;
 
 	if (vmx-&gt;nested.current_vmptr != vmptr) {
 		struct vmcs12 *new_vmcs12;
 		struct page *page;
 		page = nested_get_page(vcpu, vmptr);
 		if (page == NULL) {
 			nested_vmx_failInvalid(vcpu);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 		new_vmcs12 = kmap(page);
 		if (new_vmcs12-&gt;revision_id != VMCS12_REVISION) {
 			kunmap(page);
 			nested_release_page_clean(page);
 			nested_vmx_failValid(vcpu,
 				VMXERR_VMPTRLD_INCORRECT_VMCS_REVISION_ID);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 
 		nested_release_vmcs12(vmx);
 		vmx-&gt;nested.current_vmptr = vmptr;
 		vmx-&gt;nested.current_vmcs12 = new_vmcs12;
 		vmx-&gt;nested.current_vmcs12_page = page;
 		/*
 		 * Load VMCS12 from guest memory since it is not already
<span class="p_chunk">@@ -7485,18 +7457,17 @@</span> <span class="p_context"> static int handle_vmptrld(struct kvm_vcpu *vcpu)</span>
 				      SECONDARY_EXEC_SHADOW_VMCS);
 			vmcs_write64(VMCS_LINK_POINTER,
 				     __pa(vmx-&gt;vmcs01.shadow_vmcs));
 			vmx-&gt;nested.sync_shadow_vmcs = true;
 		}
 	}
 
 	nested_vmx_succeed(vcpu);
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 /* Emulate the VMPTRST instruction */
 static int handle_vmptrst(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification = vmcs_readl(EXIT_QUALIFICATION);
 	u32 vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 	gva_t vmcs_gva;
<span class="p_chunk">@@ -7511,18 +7482,17 @@</span> <span class="p_context"> static int handle_vmptrst(struct kvm_vcpu *vcpu)</span>
 	/* ok to use *_system, as nested_vmx_check_permission verified cpl=0 */
 	if (kvm_write_guest_virt_system(&amp;vcpu-&gt;arch.emulate_ctxt, vmcs_gva,
 				 (void *)&amp;to_vmx(vcpu)-&gt;nested.current_vmptr,
 				 sizeof(u64), &amp;e)) {
 		kvm_inject_page_fault(vcpu, &amp;e);
 		return 1;
 	}
 	nested_vmx_succeed(vcpu);
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 /* Emulate the INVEPT instruction */
 static int handle_invept(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	u32 vmx_instruction_info, types;
 	unsigned long type;
<span class="p_chunk">@@ -7550,18 +7520,17 @@</span> <span class="p_context"> static int handle_invept(struct kvm_vcpu *vcpu)</span>
 	vmx_instruction_info = vmcs_read32(VMX_INSTRUCTION_INFO);
 	type = kvm_register_readl(vcpu, (vmx_instruction_info &gt;&gt; 28) &amp; 0xf);
 
 	types = (vmx-&gt;nested.nested_vmx_ept_caps &gt;&gt; VMX_EPT_EXTENT_SHIFT) &amp; 6;
 
 	if (type &gt;= 32 || !(types &amp; (1 &lt;&lt; type))) {
 		nested_vmx_failValid(vcpu,
 				VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	}
 
 	/* According to the Intel VMX instruction reference, the memory
 	 * operand is read even if it isn&#39;t needed (e.g., for type==global)
 	 */
 	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
 			vmx_instruction_info, false, &amp;gva))
 		return 1;
<span class="p_chunk">@@ -7582,18 +7551,17 @@</span> <span class="p_context"> static int handle_invept(struct kvm_vcpu *vcpu)</span>
 		kvm_make_request(KVM_REQ_TLB_FLUSH, vcpu);
 		nested_vmx_succeed(vcpu);
 		break;
 	default:
 		BUG_ON(1);
 		break;
 	}
 
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 static int handle_invvpid(struct kvm_vcpu *vcpu)
 {
 	struct vcpu_vmx *vmx = to_vmx(vcpu);
 	u32 vmx_instruction_info;
 	unsigned long type, types;
 	gva_t gva;
<span class="p_chunk">@@ -7614,18 +7582,17 @@</span> <span class="p_context"> static int handle_invvpid(struct kvm_vcpu *vcpu)</span>
 	type = kvm_register_readl(vcpu, (vmx_instruction_info &gt;&gt; 28) &amp; 0xf);
 
 	types = (vmx-&gt;nested.nested_vmx_vpid_caps &amp;
 			VMX_VPID_EXTENT_SUPPORTED_MASK) &gt;&gt; 8;
 
 	if (type &gt;= 32 || !(types &amp; (1 &lt;&lt; type))) {
 		nested_vmx_failValid(vcpu,
 			VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	}
 
 	/* according to the intel vmx instruction reference, the memory
 	 * operand is read even if it isn&#39;t needed (e.g., for type==global)
 	 */
 	if (get_vmx_mem_address(vcpu, vmcs_readl(EXIT_QUALIFICATION),
 			vmx_instruction_info, false, &amp;gva))
 		return 1;
<span class="p_chunk">@@ -7637,33 +7604,30 @@</span> <span class="p_context"> static int handle_invvpid(struct kvm_vcpu *vcpu)</span>
 
 	switch (type) {
 	case VMX_VPID_EXTENT_INDIVIDUAL_ADDR:
 	case VMX_VPID_EXTENT_SINGLE_CONTEXT:
 	case VMX_VPID_EXTENT_SINGLE_NON_GLOBAL:
 		if (!vpid) {
 			nested_vmx_failValid(vcpu,
 				VMXERR_INVALID_OPERAND_TO_INVEPT_INVVPID);
<span class="p_del">-			skip_emulated_instruction(vcpu);</span>
<span class="p_del">-			return 1;</span>
<span class="p_add">+			return skip_emulated_instruction(vcpu);</span>
 		}
 		break;
 	case VMX_VPID_EXTENT_ALL_CONTEXT:
 		break;
 	default:
 		WARN_ON_ONCE(1);
<span class="p_del">-		skip_emulated_instruction(vcpu);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		return skip_emulated_instruction(vcpu);</span>
 	}
 
 	__vmx_flush_tlb(vcpu, vmx-&gt;nested.vpid02);
 	nested_vmx_succeed(vcpu);
 
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 static int handle_pml_full(struct kvm_vcpu *vcpu)
 {
 	unsigned long exit_qualification;
 
 	trace_kvm_pml_full(vcpu-&gt;vcpu_id);
 
<span class="p_chunk">@@ -10238,18 +10202,17 @@</span> <span class="p_context"> static int nested_vmx_run(struct kvm_vcpu *vcpu, bool launch)</span>
 	 * Note no nested_vmx_succeed or nested_vmx_fail here. At this point
 	 * we are no longer running L1, and VMLAUNCH/VMRESUME has not yet
 	 * returned as far as L1 is concerned. It will only return (and set
 	 * the success flag) when L2 exits (see nested_vmx_vmexit()).
 	 */
 	return 1;
 
 out:
<span class="p_del">-	skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return 1;</span>
<span class="p_add">+	return skip_emulated_instruction(vcpu);</span>
 }
 
 /*
  * On a nested exit from L2 to L1, vmcs12.guest_cr0 might not be up-to-date
  * because L2 may have changed some cr0 bits directly (CRO_GUEST_HOST_MASK).
  * This function returns the new value we should put in vmcs12.guest_cr0.
  * It&#39;s not enough to just return the vmcs02 GUEST_CR0. Rather,
  *  1. Bits that neither L0 nor L1 trapped, were set directly by L2 and are now
<span class="p_header">diff --git a/arch/x86/kvm/x86.c b/arch/x86/kvm/x86.c</span>
<span class="p_header">index ec59301..fbcacf8 100644</span>
<span class="p_header">--- a/arch/x86/kvm/x86.c</span>
<span class="p_header">+++ b/arch/x86/kvm/x86.c</span>
<span class="p_chunk">@@ -420,22 +420,24 @@</span> <span class="p_context"> void kvm_queue_exception(struct kvm_vcpu *vcpu, unsigned nr)</span>
 EXPORT_SYMBOL_GPL(kvm_queue_exception);
 
 void kvm_requeue_exception(struct kvm_vcpu *vcpu, unsigned nr)
 {
 	kvm_multiple_exception(vcpu, nr, false, 0, true);
 }
 EXPORT_SYMBOL_GPL(kvm_requeue_exception);
 
<span class="p_del">-void kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err)</span>
<span class="p_add">+int kvm_complete_insn_gp(struct kvm_vcpu *vcpu, int err)</span>
 {
 	if (err)
 		kvm_inject_gp(vcpu, 0);
 	else
<span class="p_del">-		kvm_x86_ops-&gt;skip_emulated_instruction(vcpu);</span>
<span class="p_add">+		return kvm_x86_ops-&gt;skip_emulated_instruction(vcpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 1;</span>
 }
 EXPORT_SYMBOL_GPL(kvm_complete_insn_gp);
 
 void kvm_inject_page_fault(struct kvm_vcpu *vcpu, struct x86_exception *fault)
 {
 	++vcpu-&gt;stat.pf_guest;
 	vcpu-&gt;arch.cr2 = fault-&gt;address;
 	kvm_queue_exception_e(vcpu, PF_VECTOR, fault-&gt;error_code);
<span class="p_chunk">@@ -4808,18 +4810,18 @@</span> <span class="p_context"> static int kvm_emulate_wbinvd_noskip(struct kvm_vcpu *vcpu)</span>
 		cpumask_clear(vcpu-&gt;arch.wbinvd_dirty_mask);
 	} else
 		wbinvd();
 	return X86EMUL_CONTINUE;
 }
 
 int kvm_emulate_wbinvd(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	kvm_x86_ops-&gt;skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return kvm_emulate_wbinvd_noskip(vcpu);</span>
<span class="p_add">+	kvm_emulate_wbinvd_noskip(vcpu);</span>
<span class="p_add">+	return kvm_x86_ops-&gt;skip_emulated_instruction(vcpu);</span>
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_wbinvd);
 
 
 
 static void emulator_wbinvd(struct x86_emulate_ctxt *ctxt)
 {
 	kvm_emulate_wbinvd_noskip(emul_to_vcpu(ctxt));
<span class="p_chunk">@@ -6002,18 +6004,22 @@</span> <span class="p_context"> int kvm_vcpu_halt(struct kvm_vcpu *vcpu)</span>
 		vcpu-&gt;run-&gt;exit_reason = KVM_EXIT_HLT;
 		return 0;
 	}
 }
 EXPORT_SYMBOL_GPL(kvm_vcpu_halt);
 
 int kvm_emulate_halt(struct kvm_vcpu *vcpu)
 {
<span class="p_del">-	kvm_x86_ops-&gt;skip_emulated_instruction(vcpu);</span>
<span class="p_del">-	return kvm_vcpu_halt(vcpu);</span>
<span class="p_add">+	int ret = kvm_x86_ops-&gt;skip_emulated_instruction(vcpu);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * TODO: we might be squashing a GUESTDBG_SINGLESTEP-triggered</span>
<span class="p_add">+	 * KVM_EXIT_DEBUG here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return kvm_vcpu_halt(vcpu) &amp;&amp; ret;</span>
 }
 EXPORT_SYMBOL_GPL(kvm_emulate_halt);
 
 /*
  * kvm_pv_kick_cpu_op:  Kick a vcpu.
  *
  * @apicid - apicid of vcpu to be kicked.
  */
<span class="p_chunk">@@ -6034,19 +6040,19 @@</span> <span class="p_context"> void kvm_vcpu_deactivate_apicv(struct kvm_vcpu *vcpu)</span>
 {
 	vcpu-&gt;arch.apicv_active = false;
 	kvm_x86_ops-&gt;refresh_apicv_exec_ctrl(vcpu);
 }
 
 int kvm_emulate_hypercall(struct kvm_vcpu *vcpu)
 {
 	unsigned long nr, a0, a1, a2, a3, ret;
<span class="p_del">-	int op_64_bit, r = 1;</span>
<span class="p_add">+	int op_64_bit, r;</span>
 
<span class="p_del">-	kvm_x86_ops-&gt;skip_emulated_instruction(vcpu);</span>
<span class="p_add">+	r = kvm_x86_ops-&gt;skip_emulated_instruction(vcpu);</span>
 
 	if (kvm_hv_hypercall_enabled(vcpu-&gt;kvm))
 		return kvm_hv_hypercall(vcpu);
 
 	nr = kvm_register_read(vcpu, VCPU_REGS_RAX);
 	a0 = kvm_register_read(vcpu, VCPU_REGS_RBX);
 	a1 = kvm_register_read(vcpu, VCPU_REGS_RCX);
 	a2 = kvm_register_read(vcpu, VCPU_REGS_RDX);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



