
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,3/8] mm: Isolate coherent device memory nodes from HugeTLB allocation paths - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,3/8] mm: Isolate coherent device memory nodes from HugeTLB allocation paths</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 24, 2016, 4:31 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1477283517-2504-4-git-send-email-khandual@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9391377/mbox/"
   >mbox</a>
|
   <a href="/patch/9391377/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9391377/">/patch/9391377/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	F2D796086B for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 24 Oct 2016 04:32:56 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EA10428C25
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 24 Oct 2016 04:32:56 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id DED7C28C2C; Mon, 24 Oct 2016 04:32:56 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DFEB028C33
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 24 Oct 2016 04:32:54 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1757218AbcJXEck (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 24 Oct 2016 00:32:40 -0400
Received: from mx0b-001b2d01.pphosted.com ([148.163.158.5]:50032 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S1757149AbcJXEca (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 24 Oct 2016 00:32:30 -0400
Received: from pps.filterd (m0098414.ppops.net [127.0.0.1])
	by mx0b-001b2d01.pphosted.com (8.16.0.17/8.16.0.17) with SMTP id
	u9O4SuYa016880
	for &lt;linux-kernel@vger.kernel.org&gt;; Mon, 24 Oct 2016 00:32:29 -0400
Received: from e28smtp04.in.ibm.com (e28smtp04.in.ibm.com [125.16.236.4])
	by mx0b-001b2d01.pphosted.com with ESMTP id 268qn3x2aw-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Mon, 24 Oct 2016 00:32:29 -0400
Received: from localhost
	by e28smtp04.in.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;khandual@linux.vnet.ibm.com&gt;;
	Mon, 24 Oct 2016 10:02:25 +0530
Received: from d28dlp01.in.ibm.com (9.184.220.126)
	by e28smtp04.in.ibm.com (192.168.1.134) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Mon, 24 Oct 2016 10:02:23 +0530
Received: from d28relay06.in.ibm.com (d28relay06.in.ibm.com [9.184.220.150])
	by d28dlp01.in.ibm.com (Postfix) with ESMTP id D66ABE0063
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 24 Oct 2016 10:02:13 +0530 (IST)
Received: from d28av05.in.ibm.com (d28av05.in.ibm.com [9.184.220.67])
	by d28relay06.in.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id
	u9O4WNRV23068866
	for &lt;linux-kernel@vger.kernel.org&gt;; Mon, 24 Oct 2016 10:02:23 +0530
Received: from d28av05.in.ibm.com (localhost [127.0.0.1])
	by d28av05.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id
	u9O4WIKO020894
	for &lt;linux-kernel@vger.kernel.org&gt;; Mon, 24 Oct 2016 10:02:22 +0530
Received: from localhost.in.ibm.com ([9.124.158.175])
	by d28av05.in.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id
	u9O4Vvm8019926; Mon, 24 Oct 2016 10:02:16 +0530
From: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: mhocko@suse.com, js1304@gmail.com, vbabka@suse.cz, mgorman@suse.de,
	minchan@kernel.org, akpm@linux-foundation.org,
	aneesh.kumar@linux.vnet.ibm.com, bsingharora@gmail.com
Subject: [RFC 3/8] mm: Isolate coherent device memory nodes from HugeTLB
	allocation paths
Date: Mon, 24 Oct 2016 10:01:52 +0530
X-Mailer: git-send-email 2.1.0
In-Reply-To: &lt;1477283517-2504-1-git-send-email-khandual@linux.vnet.ibm.com&gt;
References: &lt;1477283517-2504-1-git-send-email-khandual@linux.vnet.ibm.com&gt;
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 16102404-0012-0000-0000-000003438AB7
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 16102404-0013-0000-0000-0000198A5ED6
Message-Id: &lt;1477283517-2504-4-git-send-email-khandual@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2016-10-23_18:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=2
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1609300000
	definitions=main-1610240081
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - Oct. 24, 2016, 4:31 a.m.</div>
<pre class="content">
This change is part of the isolation requiring coherent device memory nodes
implementation.

Isolation seeking coherent device memory node requires allocation isolation
from implicit memory allocations from user space. Towards that effect, the
memory should not be used for generic HugeTLB page pool allocations. This
modifies relevant functions to skip all coherent memory nodes present on
the system during allocation, freeing and auditing for HugeTLB pages.
<span class="signed-off-by">
Signed-off-by: Anshuman Khandual &lt;khandual@linux.vnet.ibm.com&gt;</span>
---
 mm/hugetlb.c | 38 ++++++++++++++++++++++++++++++++++++--
 1 file changed, 36 insertions(+), 2 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Oct. 24, 2016, 5:16 p.m.</div>
<pre class="content">
On 10/23/2016 09:31 PM, Anshuman Khandual wrote:
<span class="quote">&gt; This change is part of the isolation requiring coherent device memory nodes</span>
<span class="quote">&gt; implementation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Isolation seeking coherent device memory node requires allocation isolation</span>
<span class="quote">&gt; from implicit memory allocations from user space. Towards that effect, the</span>
<span class="quote">&gt; memory should not be used for generic HugeTLB page pool allocations. This</span>
<span class="quote">&gt; modifies relevant functions to skip all coherent memory nodes present on</span>
<span class="quote">&gt; the system during allocation, freeing and auditing for HugeTLB pages.</span>

This seems really fragile.  You had to hit, what, 18 call sites?  What
are the odds that this is going to stay working?
<span class="quote">
&gt; @@ -2666,6 +2688,10 @@ static void __init hugetlb_register_all_nodes(void)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_node_state(nid, N_MEMORY) {</span>
<span class="quote">&gt;  		struct node *node = node_devices[nid];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (isolated_cdm_node(nid))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		if (node-&gt;dev.id == nid)</span>
<span class="quote">&gt;  			hugetlb_register_node(node);</span>
<span class="quote">&gt;  	}</span>

This looks to be completely kneecapping hugetlbfs on these cdm nodes.
Is that really what you want?
<span class="quote">
&gt; @@ -2819,8 +2845,12 @@ static unsigned int cpuset_mems_nr(unsigned int *array)</span>
<span class="quote">&gt;  	int node;</span>
<span class="quote">&gt;  	unsigned int nr = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	for_each_node_mask(node, cpuset_current_mems_allowed)</span>
<span class="quote">&gt; +	for_each_node_mask(node, cpuset_current_mems_allowed) {</span>
<span class="quote">&gt; +		if (isolated_cdm_node(node))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		nr += array[node];</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return nr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2940,7 +2970,10 @@ void hugetlb_show_meminfo(void)</span>
<span class="quote">&gt;  	if (!hugepages_supported())</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	for_each_node_state(nid, N_MEMORY)</span>
<span class="quote">&gt; +	for_each_node_state(nid, N_MEMORY) {</span>
<span class="quote">&gt; +		if (isolated_cdm_node(nid))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		for_each_hstate(h)</span>
<span class="quote">&gt;  			pr_info(&quot;Node %d hugepages_total=%u hugepages_free=%u hugepages_surp=%u hugepages_size=%lukB\n&quot;,</span>
<span class="quote">&gt;  				nid,</span>
<span class="quote">&gt; @@ -2948,6 +2981,7 @@ void hugetlb_show_meminfo(void)</span>
<span class="quote">&gt;  				h-&gt;free_huge_pages_node[nid],</span>
<span class="quote">&gt;  				h-&gt;surplus_huge_pages_node[nid],</span>
<span class="quote">&gt;  				1UL &lt;&lt; (huge_page_order(h) + PAGE_SHIFT - 10));</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  }</span>

Your patch description talks about removing *implicit* memory
allocations.  But, this removes even the ability to gather *stats* about
huge pages sitting on one of these nodes.  That&#39;s a lot more drastic
than just changing implicit policies.

Is that patch description accurate?

It looks to me like you just went through all the for_each_node*() loops
in hugetlb.c and hacked your node check into them indiscriminately.
This totally removes the ability to *do* hugetlb on this nodes.

Isn&#39;t there some simpler way to do all this, like maybe changing the
root cpuset to disallow allocations to these nodes?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - Oct. 25, 2016, 4:15 a.m.</div>
<pre class="content">
Dave Hansen &lt;dave.hansen@intel.com&gt; writes:
<span class="quote">
&gt; On 10/23/2016 09:31 PM, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt; This change is part of the isolation requiring coherent device memory nodes</span>
<span class="quote">&gt;&gt; implementation.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Isolation seeking coherent device memory node requires allocation isolation</span>
<span class="quote">&gt;&gt; from implicit memory allocations from user space. Towards that effect, the</span>
<span class="quote">&gt;&gt; memory should not be used for generic HugeTLB page pool allocations. This</span>
<span class="quote">&gt;&gt; modifies relevant functions to skip all coherent memory nodes present on</span>
<span class="quote">&gt;&gt; the system during allocation, freeing and auditing for HugeTLB pages.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This seems really fragile.  You had to hit, what, 18 call sites?  What</span>
<span class="quote">&gt; are the odds that this is going to stay working?</span>


I guess a better approach is to introduce new node_states entry such
that we have one that excludes coherent device memory numa nodes. One
possibility is to add N_SYSTEM_MEMORY and N_MEMORY.

Current N_MEMORY becomes N_SYSTEM_MEMORY and N_MEMORY includes
system and device/any other memory which is coherent.

All the isolation can then be achieved based on the nodemask_t used for
allocation. So for allocations we want to avoid from coherent device we
use N_SYSTEM_MEMORY mask or a derivative of that and where we are ok to
allocate from CDM with fallbacks we use N_MEMORY.

All nodes zonelist will have zones from the coherent device nodes but we
will not end up allocating from coherent device node zone due to the
node mask used.


This will also make sure we end up allocating from the correct coherent
device numa node in the presence of multiple of them based on the
distance of the coherent device node from the current executing numa
node.
<span class="quote">


&gt;</span>
<span class="quote">&gt;&gt; @@ -2666,6 +2688,10 @@ static void __init hugetlb_register_all_nodes(void)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	for_each_node_state(nid, N_MEMORY) {</span>
<span class="quote">&gt;&gt;  		struct node *node = node_devices[nid];</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +		if (isolated_cdm_node(nid))</span>
<span class="quote">&gt;&gt; +			continue;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  		if (node-&gt;dev.id == nid)</span>
<span class="quote">&gt;&gt;  			hugetlb_register_node(node);</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This looks to be completely kneecapping hugetlbfs on these cdm nodes.</span>
<span class="quote">&gt; Is that really what you want?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; @@ -2819,8 +2845,12 @@ static unsigned int cpuset_mems_nr(unsigned int *array)</span>
<span class="quote">&gt;&gt;  	int node;</span>
<span class="quote">&gt;&gt;  	unsigned int nr = 0;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	for_each_node_mask(node, cpuset_current_mems_allowed)</span>
<span class="quote">&gt;&gt; +	for_each_node_mask(node, cpuset_current_mems_allowed) {</span>
<span class="quote">&gt;&gt; +		if (isolated_cdm_node(node))</span>
<span class="quote">&gt;&gt; +			continue;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  		nr += array[node];</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	return nr;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; @@ -2940,7 +2970,10 @@ void hugetlb_show_meminfo(void)</span>
<span class="quote">&gt;&gt;  	if (!hugepages_supported())</span>
<span class="quote">&gt;&gt;  		return;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; -	for_each_node_state(nid, N_MEMORY)</span>
<span class="quote">&gt;&gt; +	for_each_node_state(nid, N_MEMORY) {</span>
<span class="quote">&gt;&gt; +		if (isolated_cdm_node(nid))</span>
<span class="quote">&gt;&gt; +			continue;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  		for_each_hstate(h)</span>
<span class="quote">&gt;&gt;  			pr_info(&quot;Node %d hugepages_total=%u hugepages_free=%u hugepages_surp=%u hugepages_size=%lukB\n&quot;,</span>
<span class="quote">&gt;&gt;  				nid,</span>
<span class="quote">&gt;&gt; @@ -2948,6 +2981,7 @@ void hugetlb_show_meminfo(void)</span>
<span class="quote">&gt;&gt;  				h-&gt;free_huge_pages_node[nid],</span>
<span class="quote">&gt;&gt;  				h-&gt;surplus_huge_pages_node[nid],</span>
<span class="quote">&gt;&gt;  				1UL &lt;&lt; (huge_page_order(h) + PAGE_SHIFT - 10));</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Your patch description talks about removing *implicit* memory</span>
<span class="quote">&gt; allocations.  But, this removes even the ability to gather *stats* about</span>
<span class="quote">&gt; huge pages sitting on one of these nodes.  That&#39;s a lot more drastic</span>
<span class="quote">&gt; than just changing implicit policies.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Is that patch description accurate?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It looks to me like you just went through all the for_each_node*() loops</span>
<span class="quote">&gt; in hugetlb.c and hacked your node check into them indiscriminately.</span>
<span class="quote">&gt; This totally removes the ability to *do* hugetlb on this nodes.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Isn&#39;t there some simpler way to do all this, like maybe changing the</span>
<span class="quote">&gt; root cpuset to disallow allocations to these nodes?</span>

-aneesh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - Oct. 25, 2016, 7:17 a.m.</div>
<pre class="content">
On 25/10/16 15:15, Aneesh Kumar K.V wrote:
<span class="quote">&gt; Dave Hansen &lt;dave.hansen@intel.com&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; On 10/23/2016 09:31 PM, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt;&gt; This change is part of the isolation requiring coherent device memory nodes</span>
<span class="quote">&gt;&gt;&gt; implementation.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Isolation seeking coherent device memory node requires allocation isolation</span>
<span class="quote">&gt;&gt;&gt; from implicit memory allocations from user space. Towards that effect, the</span>
<span class="quote">&gt;&gt;&gt; memory should not be used for generic HugeTLB page pool allocations. This</span>
<span class="quote">&gt;&gt;&gt; modifies relevant functions to skip all coherent memory nodes present on</span>
<span class="quote">&gt;&gt;&gt; the system during allocation, freeing and auditing for HugeTLB pages.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This seems really fragile.  You had to hit, what, 18 call sites?  What</span>
<span class="quote">&gt;&gt; are the odds that this is going to stay working?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess a better approach is to introduce new node_states entry such</span>
<span class="quote">&gt; that we have one that excludes coherent device memory numa nodes. One</span>
<span class="quote">&gt; possibility is to add N_SYSTEM_MEMORY and N_MEMORY.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Current N_MEMORY becomes N_SYSTEM_MEMORY and N_MEMORY includes</span>
<span class="quote">&gt; system and device/any other memory which is coherent.</span>
<span class="quote">&gt; </span>

I thought of this as well, but I would rather see N_COHERENT_MEMORY
as a flag. The idea being that some device memory is a part of
N_MEMORY, but N_COHERENT_MEMORY gives it additional attributes
<span class="quote">
&gt; All the isolation can then be achieved based on the nodemask_t used for</span>
<span class="quote">&gt; allocation. So for allocations we want to avoid from coherent device we</span>
<span class="quote">&gt; use N_SYSTEM_MEMORY mask or a derivative of that and where we are ok to</span>
<span class="quote">&gt; allocate from CDM with fallbacks we use N_MEMORY.</span>
<span class="quote">&gt; </span>

I suspect its going to be easier to exclude N_COHERENT_MEMORY.
<span class="quote">
&gt; All nodes zonelist will have zones from the coherent device nodes but we</span>
<span class="quote">&gt; will not end up allocating from coherent device node zone due to the</span>
<span class="quote">&gt; node mask used.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This will also make sure we end up allocating from the correct coherent</span>
<span class="quote">&gt; device numa node in the presence of multiple of them based on the</span>
<span class="quote">&gt; distance of the coherent device node from the current executing numa</span>
<span class="quote">&gt; node.</span>
<span class="quote">&gt; </span>

The idea is good overall, but I think its going to be good to document
the exclusions with the flags

Balbir Singh.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - Oct. 25, 2016, 7:25 a.m.</div>
<pre class="content">
On 25/10/16 18:17, Balbir Singh wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 25/10/16 15:15, Aneesh Kumar K.V wrote:</span>
<span class="quote">&gt;&gt; Dave Hansen &lt;dave.hansen@intel.com&gt; writes:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On 10/23/2016 09:31 PM, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; This change is part of the isolation requiring coherent device memory nodes</span>
<span class="quote">&gt;&gt;&gt;&gt; implementation.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Isolation seeking coherent device memory node requires allocation isolation</span>
<span class="quote">&gt;&gt;&gt;&gt; from implicit memory allocations from user space. Towards that effect, the</span>
<span class="quote">&gt;&gt;&gt;&gt; memory should not be used for generic HugeTLB page pool allocations. This</span>
<span class="quote">&gt;&gt;&gt;&gt; modifies relevant functions to skip all coherent memory nodes present on</span>
<span class="quote">&gt;&gt;&gt;&gt; the system during allocation, freeing and auditing for HugeTLB pages.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This seems really fragile.  You had to hit, what, 18 call sites?  What</span>
<span class="quote">&gt;&gt;&gt; are the odds that this is going to stay working?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I guess a better approach is to introduce new node_states entry such</span>
<span class="quote">&gt;&gt; that we have one that excludes coherent device memory numa nodes. One</span>
<span class="quote">&gt;&gt; possibility is to add N_SYSTEM_MEMORY and N_MEMORY.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Current N_MEMORY becomes N_SYSTEM_MEMORY and N_MEMORY includes</span>
<span class="quote">&gt;&gt; system and device/any other memory which is coherent.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I thought of this as well, but I would rather see N_COHERENT_MEMORY</span>
<span class="quote">&gt; as a flag. The idea being that some device memory is a part of</span>
<span class="quote">&gt; N_MEMORY, but N_COHERENT_MEMORY gives it additional attributes</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; All the isolation can then be achieved based on the nodemask_t used for</span>
<span class="quote">&gt;&gt; allocation. So for allocations we want to avoid from coherent device we</span>
<span class="quote">&gt;&gt; use N_SYSTEM_MEMORY mask or a derivative of that and where we are ok to</span>
<span class="quote">&gt;&gt; allocate from CDM with fallbacks we use N_MEMORY.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I suspect its going to be easier to exclude N_COHERENT_MEMORY.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; All nodes zonelist will have zones from the coherent device nodes but we</span>
<span class="quote">&gt;&gt; will not end up allocating from coherent device node zone due to the</span>
<span class="quote">&gt;&gt; node mask used.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This will also make sure we end up allocating from the correct coherent</span>
<span class="quote">&gt;&gt; device numa node in the presence of multiple of them based on the</span>
<span class="quote">&gt;&gt; distance of the coherent device node from the current executing numa</span>
<span class="quote">&gt;&gt; node.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The idea is good overall, but I think its going to be good to document</span>
<span class="quote">&gt; the exclusions with the flags</span>
<span class="quote">&gt; </span>

FWIW,, some of this is present in 8/8

Balbir
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index ec49d9e..466a44c 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1147,6 +1147,9 @@</span> <span class="p_context"> static int alloc_fresh_gigantic_page(struct hstate *h,</span>
 	int nr_nodes, node;
 
 	for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {
<span class="p_add">+		if (isolated_cdm_node(node))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		page = alloc_fresh_gigantic_page_node(h, node);
 		if (page)
 			return 1;
<span class="p_chunk">@@ -1382,6 +1385,9 @@</span> <span class="p_context"> static int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)</span>
 	int ret = 0;
 
 	for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {
<span class="p_add">+		if (isolated_cdm_node(node))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		page = alloc_fresh_huge_page_node(h, node);
 		if (page) {
 			ret = 1;
<span class="p_chunk">@@ -1410,6 +1416,9 @@</span> <span class="p_context"> static int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,</span>
 	int ret = 0;
 
 	for_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {
<span class="p_add">+		if (isolated_cdm_node(node))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		/*
 		 * If we&#39;re returning unused surplus pages, only examine
 		 * nodes with surplus pages.
<span class="p_chunk">@@ -2028,6 +2037,9 @@</span> <span class="p_context"> int __weak alloc_bootmem_huge_page(struct hstate *h)</span>
 	for_each_node_mask_to_alloc(h, nr_nodes, node, &amp;node_states[N_MEMORY]) {
 		void *addr;
 
<span class="p_add">+		if (isolated_cdm_node(node))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		addr = memblock_virt_alloc_try_nid_nopanic(
 				huge_page_size(h), huge_page_size(h),
 				0, BOOTMEM_ALLOC_ACCESSIBLE, node);
<span class="p_chunk">@@ -2156,6 +2168,10 @@</span> <span class="p_context"> static void try_to_free_low(struct hstate *h, unsigned long count,</span>
 	for_each_node_mask(i, *nodes_allowed) {
 		struct page *page, *next;
 		struct list_head *freel = &amp;h-&gt;hugepage_freelists[i];
<span class="p_add">+</span>
<span class="p_add">+		if (isolated_cdm_node(i))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		list_for_each_entry_safe(page, next, freel, lru) {
 			if (count &gt;= h-&gt;nr_huge_pages)
 				return;
<span class="p_chunk">@@ -2189,11 +2205,17 @@</span> <span class="p_context"> static int adjust_pool_surplus(struct hstate *h, nodemask_t *nodes_allowed,</span>
 
 	if (delta &lt; 0) {
 		for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {
<span class="p_add">+			if (isolated_cdm_node(node))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
 			if (h-&gt;surplus_huge_pages_node[node])
 				goto found;
 		}
 	} else {
 		for_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {
<span class="p_add">+			if (isolated_cdm_node(node))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
 			if (h-&gt;surplus_huge_pages_node[node] &lt;
 					h-&gt;nr_huge_pages_node[node])
 				goto found;
<span class="p_chunk">@@ -2666,6 +2688,10 @@</span> <span class="p_context"> static void __init hugetlb_register_all_nodes(void)</span>
 
 	for_each_node_state(nid, N_MEMORY) {
 		struct node *node = node_devices[nid];
<span class="p_add">+</span>
<span class="p_add">+		if (isolated_cdm_node(nid))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		if (node-&gt;dev.id == nid)
 			hugetlb_register_node(node);
 	}
<span class="p_chunk">@@ -2819,8 +2845,12 @@</span> <span class="p_context"> static unsigned int cpuset_mems_nr(unsigned int *array)</span>
 	int node;
 	unsigned int nr = 0;
 
<span class="p_del">-	for_each_node_mask(node, cpuset_current_mems_allowed)</span>
<span class="p_add">+	for_each_node_mask(node, cpuset_current_mems_allowed) {</span>
<span class="p_add">+		if (isolated_cdm_node(node))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		nr += array[node];
<span class="p_add">+	}</span>
 
 	return nr;
 }
<span class="p_chunk">@@ -2940,7 +2970,10 @@</span> <span class="p_context"> void hugetlb_show_meminfo(void)</span>
 	if (!hugepages_supported())
 		return;
 
<span class="p_del">-	for_each_node_state(nid, N_MEMORY)</span>
<span class="p_add">+	for_each_node_state(nid, N_MEMORY) {</span>
<span class="p_add">+		if (isolated_cdm_node(nid))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
 		for_each_hstate(h)
 			pr_info(&quot;Node %d hugepages_total=%u hugepages_free=%u hugepages_surp=%u hugepages_size=%lukB\n&quot;,
 				nid,
<span class="p_chunk">@@ -2948,6 +2981,7 @@</span> <span class="p_context"> void hugetlb_show_meminfo(void)</span>
 				h-&gt;free_huge_pages_node[nid],
 				h-&gt;surplus_huge_pages_node[nid],
 				1UL &lt;&lt; (huge_page_order(h) + PAGE_SHIFT - 10));
<span class="p_add">+	}</span>
 }
 
 void hugetlb_report_usage(struct seq_file *m, struct mm_struct *mm)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



