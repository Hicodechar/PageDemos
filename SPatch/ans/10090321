
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,5/5] mm, hugetlb: further simplify hugetlb allocation API - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,5/5] mm, hugetlb: further simplify hugetlb allocation API</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 4, 2017, 2:01 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171204140117.7191-6-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10090321/mbox/"
   >mbox</a>
|
   <a href="/patch/10090321/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10090321/">/patch/10090321/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	F32BD60329 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  4 Dec 2017 14:02:28 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D539D287DE
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  4 Dec 2017 14:02:28 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id C8F932899A; Mon,  4 Dec 2017 14:02:28 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id CF9F2287DE
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  4 Dec 2017 14:02:27 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753093AbdLDOCX (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 4 Dec 2017 09:02:23 -0500
Received: from mail-wm0-f68.google.com ([74.125.82.68]:35983 &quot;EHLO
	mail-wm0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752689AbdLDOBf (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 4 Dec 2017 09:01:35 -0500
Received: by mail-wm0-f68.google.com with SMTP id b76so14523614wmg.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 04 Dec 2017 06:01:34 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=u/jT5LyLe/KajnfulUeZf3w90Es1jgYGf08+pm3Ohxc=;
	b=Yyq4NNDzTpufH2g6n82lfzzdnHTnSGu5D8p+Ge6u/2aDCYQmPH/s2eYBjDOM9K1UcD
	jNxK7D9PTV2+88vNzOKdwuZRFuHsCbtlVMAQrIRz85XbtJiqMJwuG/lqU1zOxQAndgX3
	Yuy/VFB/Hqpb8GK1lRyEBH/UnKZoE7iMPW1C/mmiiSVQkSFexH1mMIhMQJOgBJdQfa8t
	FtySvde80bm2NoDoeHPv/Eyy56vv/45O6QLkOjQ0DVeuKGLFyHHXmEn4BeT82CJX0xbJ
	sugX+T1DzczYprcq/WAhn62DTmv1NlKLl8dLyYbG7cS9oDk0qWTUf3E4IJoNQtR3DM6C
	xHUQ==
X-Gm-Message-State: AJaThX7473TqOVhOJSWHzEiPm9YjGky1Rh7+cGJeuCEZVDujs7Sv/hqG
	XXjVkkarRTpucluWKFN9QdPmHQ==
X-Google-Smtp-Source: AGs4zMYS+Ar/ET/ey0tP+F0v1Ef3BnVzjFvVeXoOoXzdKVEhL+0aKjw0bwsNgxp+48n6I55LyuDNpQ==
X-Received: by 10.28.48.150 with SMTP id w144mr7137313wmw.23.1512396093640; 
	Mon, 04 Dec 2017 06:01:33 -0800 (PST)
Received: from tiehlicka.suse.cz (ip-89-177-66-30.net.upcbroadband.cz.
	[89.177.66.30]) by smtp.gmail.com with ESMTPSA id
	73sm9549983wrb.64.2017.12.04.06.01.32
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 04 Dec 2017 06:01:32 -0800 (PST)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: &lt;linux-mm@kvack.org&gt;
Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [RFC PATCH 5/5] mm, hugetlb: further simplify hugetlb allocation API
Date: Mon,  4 Dec 2017 15:01:17 +0100
Message-Id: &lt;20171204140117.7191-6-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.15.0
In-Reply-To: &lt;20171204140117.7191-1-mhocko@kernel.org&gt;
References: &lt;20171204140117.7191-1-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Dec. 4, 2017, 2:01 p.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

Hugetlb allocator has several layer of allocation functions depending
and the purpose of the allocation. There are two allocators depending
on whether the page can be allocated from the page allocator or we need
a contiguous allocator. This is currently opencoded in alloc_fresh_huge_page
which is the only path that might allocate giga pages which require the
later allocator. Create alloc_fresh_huge_page which hides this
implementation detail and use it in all callers which hardcoded the
buddy allocator path (__hugetlb_alloc_buddy_huge_page). This shouldn&#39;t
introduce any funtional change because both migration and surplus
allocators exlude giga pages explicitly.

While we are at it let&#39;s do some renaming. The current scheme is not
consistent and overly painfull to read and understand. Get rid of prefix
underscores from most functions. There is no real reason to make names
longer.
* alloc_fresh_huge_page is the new layer to abstract underlying
  allocator
* __hugetlb_alloc_buddy_huge_page becomes shorter and neater
  alloc_buddy_huge_page.
* Former alloc_fresh_huge_page becomes alloc_pool_huge_page because we put
  the new page directly to the pool
* alloc_surplus_huge_page can drop the opencoded prep_new_huge_page code
  as it uses alloc_fresh_huge_page now
* others lose their excessive prefix underscores to make names shorter
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 mm/hugetlb.c | 74 +++++++++++++++++++++++++++++++++---------------------------
 1 file changed, 41 insertions(+), 33 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - Dec. 14, 2017, 9:01 p.m.</div>
<pre class="content">
On 12/04/2017 06:01 AM, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hugetlb allocator has several layer of allocation functions depending</span>
<span class="quote">&gt; and the purpose of the allocation. There are two allocators depending</span>
<span class="quote">&gt; on whether the page can be allocated from the page allocator or we need</span>
<span class="quote">&gt; a contiguous allocator. This is currently opencoded in alloc_fresh_huge_page</span>
<span class="quote">&gt; which is the only path that might allocate giga pages which require the</span>
<span class="quote">&gt; later allocator. Create alloc_fresh_huge_page which hides this</span>
<span class="quote">&gt; implementation detail and use it in all callers which hardcoded the</span>
<span class="quote">&gt; buddy allocator path (__hugetlb_alloc_buddy_huge_page). This shouldn&#39;t</span>
<span class="quote">&gt; introduce any funtional change because both migration and surplus</span>
<span class="quote">&gt; allocators exlude giga pages explicitly.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; While we are at it let&#39;s do some renaming. The current scheme is not</span>
<span class="quote">&gt; consistent and overly painfull to read and understand. Get rid of prefix</span>
<span class="quote">&gt; underscores from most functions. There is no real reason to make names</span>
<span class="quote">&gt; longer.</span>
<span class="quote">&gt; * alloc_fresh_huge_page is the new layer to abstract underlying</span>
<span class="quote">&gt;   allocator</span>
<span class="quote">&gt; * __hugetlb_alloc_buddy_huge_page becomes shorter and neater</span>
<span class="quote">&gt;   alloc_buddy_huge_page.</span>
<span class="quote">&gt; * Former alloc_fresh_huge_page becomes alloc_pool_huge_page because we put</span>
<span class="quote">&gt;   the new page directly to the pool</span>
<span class="quote">&gt; * alloc_surplus_huge_page can drop the opencoded prep_new_huge_page code</span>
<span class="quote">&gt;   as it uses alloc_fresh_huge_page now</span>
<span class="quote">&gt; * others lose their excessive prefix underscores to make names shorter</span>
<span class="quote">&gt; </span>

This patch will need to be modified to take into account the incremental
diff to patch 4 in this series.  Other than that, the changes look good.
<span class="reviewed-by">
Reviewed-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 0c7dc269b6c0..73c74851a304 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1376,7 +1376,7 @@</span> <span class="p_context"> pgoff_t __basepage_index(struct page *page)</span>
 	return (index &lt;&lt; compound_order(page_head)) + compound_idx;
 }
 
<span class="p_del">-static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,</span>
<span class="p_add">+static struct page *alloc_buddy_huge_page(struct hstate *h,</span>
 		gfp_t gfp_mask, int nid, nodemask_t *nmask)
 {
 	int order = huge_page_order(h);
<span class="p_chunk">@@ -1394,34 +1394,49 @@</span> <span class="p_context"> static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,</span>
 	return page;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Common helper to allocate a fresh hugetlb page. All specific allocators</span>
<span class="p_add">+ * should use this function to get new hugetlb pages</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct page *alloc_fresh_huge_page(struct hstate *h,</span>
<span class="p_add">+		gfp_t gfp_mask, int nid, nodemask_t *nmask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hstate_is_gigantic(h))</span>
<span class="p_add">+		page = alloc_gigantic_page(h, gfp_mask, nid, nmask);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		page = alloc_buddy_huge_page(h, gfp_mask,</span>
<span class="p_add">+				nid, nmask);</span>
<span class="p_add">+	if (!page)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hstate_is_gigantic(h))</span>
<span class="p_add">+		prep_compound_gigantic_page(page, huge_page_order(h));</span>
<span class="p_add">+	prep_new_huge_page(h, page, page_to_nid(page));</span>
<span class="p_add">+</span>
<span class="p_add">+	return page;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Allocates a fresh page to the hugetlb allocator pool in the node interleaved
  * manner.
  */
<span class="p_del">-static int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)</span>
<span class="p_add">+static int alloc_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed)</span>
 {
 	struct page *page;
 	int nr_nodes, node;
 	gfp_t gfp_mask = htlb_alloc_mask(h) | __GFP_THISNODE;
 
 	for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {
<span class="p_del">-		if (hstate_is_gigantic(h))</span>
<span class="p_del">-			page = alloc_gigantic_page(h, gfp_mask,</span>
<span class="p_del">-					node, nodes_allowed);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask,</span>
<span class="p_del">-					node, nodes_allowed);</span>
<span class="p_add">+		page = alloc_fresh_huge_page(h, gfp_mask, node, nodes_allowed);</span>
 		if (page)
 			break;
<span class="p_del">-</span>
 	}
 
 	if (!page)
 		return 0;
 
<span class="p_del">-	if (hstate_is_gigantic(h))</span>
<span class="p_del">-		prep_compound_gigantic_page(page, huge_page_order(h));</span>
<span class="p_del">-	prep_new_huge_page(h, page, page_to_nid(page));</span>
 	put_page(page); /* free it into the hugepage allocator */
 
 	return 1;
<span class="p_chunk">@@ -1535,7 +1550,7 @@</span> <span class="p_context"> int dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
 /*
  * Allocates a fresh surplus page from the page allocator.
  */
<span class="p_del">-static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="p_add">+static struct page *alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 		int nid, nodemask_t *nmask)
 {
 	struct page *page = NULL;
<span class="p_chunk">@@ -1548,7 +1563,7 @@</span> <span class="p_context"> static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 		goto out_unlock;
 	spin_unlock(&amp;hugetlb_lock);
 
<span class="p_del">-	page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask, nid, nmask);</span>
<span class="p_add">+	page = alloc_fresh_huge_page(h, gfp_mask, nid, nmask);</span>
 	if (!page)
 		goto out_unlock;
 
<span class="p_chunk">@@ -1567,12 +1582,6 @@</span> <span class="p_context"> static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 	} else {
 		h-&gt;surplus_huge_pages_node[page_to_nid(page)]++;
 		h-&gt;surplus_huge_pages++;
<span class="p_del">-		INIT_LIST_HEAD(&amp;page-&gt;lru);</span>
<span class="p_del">-		r_nid = page_to_nid(page);</span>
<span class="p_del">-		set_compound_page_dtor(page, HUGETLB_PAGE_DTOR);</span>
<span class="p_del">-		set_hugetlb_cgroup(page, NULL);</span>
<span class="p_del">-		h-&gt;nr_huge_pages_node[r_nid]++;</span>
<span class="p_del">-		h-&gt;surplus_huge_pages_node[r_nid]++;</span>
 	}
 
 out_unlock:
<span class="p_chunk">@@ -1581,7 +1590,7 @@</span> <span class="p_context"> static struct page *__alloc_surplus_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 	return page;
 }
 
<span class="p_del">-static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
<span class="p_add">+static struct page *alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 		int nid, nodemask_t *nmask)
 {
 	struct page *page;
<span class="p_chunk">@@ -1589,7 +1598,7 @@</span> <span class="p_context"> static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 	if (hstate_is_gigantic(h))
 		return NULL;
 
<span class="p_del">-	page = __hugetlb_alloc_buddy_huge_page(h, gfp_mask, nid, nmask);</span>
<span class="p_add">+	page = alloc_fresh_huge_page(h, gfp_mask, nid, nmask);</span>
 	if (!page)
 		return NULL;
 
<span class="p_chunk">@@ -1597,7 +1606,6 @@</span> <span class="p_context"> static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 	 * We do not account these pages as surplus because they are only
 	 * temporary and will be released properly on the last reference
 	 */
<span class="p_del">-	prep_new_huge_page(h, page, page_to_nid(page));</span>
 	SetPageHugeTemporary(page);
 
 	return page;
<span class="p_chunk">@@ -1607,7 +1615,7 @@</span> <span class="p_context"> static struct page *__alloc_migrate_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
  * Use the VMA&#39;s mpolicy to allocate a huge page from the buddy.
  */
 static
<span class="p_del">-struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
<span class="p_add">+struct page *alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
 		struct vm_area_struct *vma, unsigned long addr)
 {
 	struct page *page;
<span class="p_chunk">@@ -1617,7 +1625,7 @@</span> <span class="p_context"> struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
 	nodemask_t *nodemask;
 
 	nid = huge_node(vma, addr, gfp_mask, &amp;mpol, &amp;nodemask);
<span class="p_del">-	page = __alloc_surplus_huge_page(h, gfp_mask, nid, nodemask);</span>
<span class="p_add">+	page = alloc_surplus_huge_page(h, gfp_mask, nid, nodemask);</span>
 	mpol_cond_put(mpol);
 
 	return page;
<span class="p_chunk">@@ -1638,7 +1646,7 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
 	spin_unlock(&amp;hugetlb_lock);
 
 	if (!page)
<span class="p_del">-		page = __alloc_migrate_huge_page(h, gfp_mask, nid, NULL);</span>
<span class="p_add">+		page = alloc_migrate_huge_page(h, gfp_mask, nid, NULL);</span>
 
 	return page;
 }
<span class="p_chunk">@@ -1661,7 +1669,7 @@</span> <span class="p_context"> struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
 	}
 	spin_unlock(&amp;hugetlb_lock);
 
<span class="p_del">-	return __alloc_migrate_huge_page(h, gfp_mask, preferred_nid, nmask);</span>
<span class="p_add">+	return alloc_migrate_huge_page(h, gfp_mask, preferred_nid, nmask);</span>
 }
 
 /*
<span class="p_chunk">@@ -1689,7 +1697,7 @@</span> <span class="p_context"> static int gather_surplus_pages(struct hstate *h, int delta)</span>
 retry:
 	spin_unlock(&amp;hugetlb_lock);
 	for (i = 0; i &lt; needed; i++) {
<span class="p_del">-		page = __alloc_surplus_huge_page(h, htlb_alloc_mask(h),</span>
<span class="p_add">+		page = alloc_surplus_huge_page(h, htlb_alloc_mask(h),</span>
 				NUMA_NO_NODE, NULL);
 		if (!page) {
 			alloc_ok = false;
<span class="p_chunk">@@ -2026,7 +2034,7 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 	page = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);
 	if (!page) {
 		spin_unlock(&amp;hugetlb_lock);
<span class="p_del">-		page = __alloc_buddy_huge_page_with_mpol(h, vma, addr);</span>
<span class="p_add">+		page = alloc_buddy_huge_page_with_mpol(h, vma, addr);</span>
 		if (!page)
 			goto out_uncharge_cgroup;
 		if (!avoid_reserve &amp;&amp; vma_has_reserves(vma, gbl_chg)) {
<span class="p_chunk">@@ -2166,7 +2174,7 @@</span> <span class="p_context"> static void __init hugetlb_hstate_alloc_pages(struct hstate *h)</span>
 		if (hstate_is_gigantic(h)) {
 			if (!alloc_bootmem_huge_page(h))
 				break;
<span class="p_del">-		} else if (!alloc_fresh_huge_page(h,</span>
<span class="p_add">+		} else if (!alloc_pool_huge_page(h,</span>
 					 &amp;node_states[N_MEMORY]))
 			break;
 		cond_resched();
<span class="p_chunk">@@ -2286,7 +2294,7 @@</span> <span class="p_context"> static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
 	 * First take pages out of surplus state.  Then make up the
 	 * remaining difference by allocating fresh huge pages.
 	 *
<span class="p_del">-	 * We might race with __alloc_surplus_huge_page() here and be unable</span>
<span class="p_add">+	 * We might race with alloc_surplus_huge_page() here and be unable</span>
 	 * to convert a surplus huge page to a normal huge page. That is
 	 * not critical, though, it just means the overall size of the
 	 * pool might be one hugepage larger than it needs to be, but
<span class="p_chunk">@@ -2309,7 +2317,7 @@</span> <span class="p_context"> static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
 		/* yield cpu to avoid soft lockup */
 		cond_resched();
 
<span class="p_del">-		ret = alloc_fresh_huge_page(h, nodes_allowed);</span>
<span class="p_add">+		ret = alloc_pool_huge_page(h, nodes_allowed);</span>
 		spin_lock(&amp;hugetlb_lock);
 		if (!ret)
 			goto out;
<span class="p_chunk">@@ -2329,7 +2337,7 @@</span> <span class="p_context"> static unsigned long set_max_huge_pages(struct hstate *h, unsigned long count,</span>
 	 * By placing pages into the surplus state independent of the
 	 * overcommit value, we are allowing the surplus pool size to
 	 * exceed overcommit. There are few sane options here. Since
<span class="p_del">-	 * __alloc_surplus_huge_page() is checking the global counter,</span>
<span class="p_add">+	 * alloc_surplus_huge_page() is checking the global counter,</span>
 	 * though, we&#39;ll note that we&#39;re not allowed to exceed surplus
 	 * and won&#39;t grow the pool anywhere else. Not until one of the
 	 * sysctls are changed, or the surplus pages go out of use.

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



