
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4.1,005/123] parisc: Fix some PTE/TLB race conditions and optimize __flush_tlb_range based on timing results - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4.1,005/123] parisc: Fix some PTE/TLB race conditions and optimize __flush_tlb_range based on timing results</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 8, 2015, 10:08 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20150808220718.009823870@linuxfoundation.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6977471/mbox/"
   >mbox</a>
|
   <a href="/patch/6977471/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6977471/">/patch/6977471/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 0D3F29F358
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat,  8 Aug 2015 22:39:57 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 38652204F6
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat,  8 Aug 2015 22:39:55 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 2C75E204EB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat,  8 Aug 2015 22:39:53 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S2994341AbbHHWjm (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sat, 8 Aug 2015 18:39:42 -0400
Received: from mail.linuxfoundation.org ([140.211.169.12]:36242 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S2993077AbbHHWMq (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sat, 8 Aug 2015 18:12:46 -0400
Received: from localhost (c-50-170-35-168.hsd1.wa.comcast.net
	[50.170.35.168])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id A003487A;
	Sat,  8 Aug 2015 22:12:45 +0000 (UTC)
From: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org
Cc: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;,
	stable@vger.kernel.org, John David Anglin &lt;dave.anglin@bell.net&gt;,
	Helge Deller &lt;deller@gmx.de&gt;
Subject: [PATCH 4.1 005/123] parisc: Fix some PTE/TLB race conditions and
	optimize __flush_tlb_range based on timing results
Date: Sat,  8 Aug 2015 15:08:03 -0700
Message-Id: &lt;20150808220718.009823870@linuxfoundation.org&gt;
X-Mailer: git-send-email 2.5.0
In-Reply-To: &lt;20150808220717.771230091@linuxfoundation.org&gt;
References: &lt;20150808220717.771230091@linuxfoundation.org&gt;
User-Agent: quilt/0.64
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.3 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Aug. 8, 2015, 10:08 p.m.</div>
<pre class="content">
4.1-stable review patch.  If anyone has any objections, please let me know.

------------------
<span class="from">
From: John David Anglin &lt;dave.anglin@bell.net&gt;</span>

commit 01ab60570427caa24b9debc369e452e86cd9beb4 upstream.

The increased use of pdtlb/pitlb instructions seemed to increase the
frequency of random segmentation faults building packages. Further, we
had a number of cases where TLB inserts would repeatedly fail and all
forward progress would stop. The Haskell ghc package caused a lot of
trouble in this area. The final indication of a race in pte handling was
this syslog entry on sibaris (C8000):

 swap_free: Unused swap offset entry 00000004
 BUG: Bad page map in process mysqld  pte:00000100 pmd:019bbec5
 addr:00000000ec464000 vm_flags:00100073 anon_vma:0000000221023828 mapping: (null) index:ec464
 CPU: 1 PID: 9176 Comm: mysqld Not tainted 4.0.0-2-parisc64-smp #1 Debian 4.0.5-1
 Backtrace:
  [&lt;0000000040173eb0&gt;] show_stack+0x20/0x38
  [&lt;0000000040444424&gt;] dump_stack+0x9c/0x110
  [&lt;00000000402a0d38&gt;] print_bad_pte+0x1a8/0x278
  [&lt;00000000402a28b8&gt;] unmap_single_vma+0x3d8/0x770
  [&lt;00000000402a4090&gt;] zap_page_range+0xf0/0x198
  [&lt;00000000402ba2a4&gt;] SyS_madvise+0x404/0x8c0

Note that the pte value is 0 except for the accessed bit 0x100. This bit
shouldn&#39;t be set without the present bit.

It should be noted that the madvise system call is probably a trigger for many
of the random segmentation faults.

In looking at the kernel code, I found the following problems:

1) The pte_clear define didn&#39;t take TLB lock when clearing a pte.
2) We didn&#39;t test pte present bit inside lock in exception support.
3) The pte and tlb locks needed to merged in order to ensure consistency
between page table and TLB. This also has the effect of serializing TLB
broadcasts on SMP systems.

The attached change implements the above and a few other tweaks to try
to improve performance. Based on the timing code, TLB purges are very
slow (e.g., ~ 209 cycles per page on rp3440). Thus, I think it
beneficial to test the split_tlb variable to avoid duplicate purges.
Probably, all PA 2.0 machines have combined TLBs.

I dropped using __flush_tlb_range in flush_tlb_mm as I realized all
applications and most threads have a stack size that is too large to
make this useful. I added some comments to this effect.

Since implementing 1 through 3, I haven&#39;t had any random segmentation
faults on mx3210 (rp3440) in about one week of building code and running
as a Debian buildd.
<span class="signed-off-by">
Signed-off-by: John David Anglin &lt;dave.anglin@bell.net&gt;</span>
<span class="signed-off-by">Signed-off-by: Helge Deller &lt;deller@gmx.de&gt;</span>
<span class="signed-off-by">Signed-off-by: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;</span>

---
 arch/parisc/include/asm/pgtable.h  |   57 ++++++++----
 arch/parisc/include/asm/tlbflush.h |   53 ++++++------
 arch/parisc/kernel/cache.c         |  105 +++++++++++++++--------
 arch/parisc/kernel/entry.S         |  163 +++++++++++++++++--------------------
 arch/parisc/kernel/traps.c         |    4 
 5 files changed, 213 insertions(+), 169 deletions(-)



--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/arch/parisc/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/parisc/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -16,7 +16,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/cache.h&gt;
 
<span class="p_del">-extern spinlock_t pa_dbit_lock;</span>
<span class="p_add">+extern spinlock_t pa_tlb_lock;</span>
 
 /*
  * kern_addr_valid(ADDR) tests if ADDR is pointing to valid kernel
<span class="p_chunk">@@ -33,6 +33,19 @@</span> <span class="p_context"> extern spinlock_t pa_dbit_lock;</span>
  */
 #define kern_addr_valid(addr)	(1)
 
<span class="p_add">+/* Purge data and instruction TLB entries.  Must be called holding</span>
<span class="p_add">+ * the pa_tlb_lock.  The TLB purge instructions are slow on SMP</span>
<span class="p_add">+ * machines since the purge must be broadcast to all CPUs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void purge_tlb_entries(struct mm_struct *mm, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mtsp(mm-&gt;context, 1);</span>
<span class="p_add">+	pdtlb(addr);</span>
<span class="p_add">+	if (unlikely(split_tlb))</span>
<span class="p_add">+		pitlb(addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Certain architectures need to do special things when PTEs
  * within a page table are directly modified.  Thus, the following
  * hook is made available.
<span class="p_chunk">@@ -42,15 +55,20 @@</span> <span class="p_context"> extern spinlock_t pa_dbit_lock;</span>
                 *(pteptr) = (pteval);                           \
         } while(0)
 
<span class="p_del">-extern void purge_tlb_entries(struct mm_struct *, unsigned long);</span>
<span class="p_del">-</span>
<span class="p_del">-#define set_pte_at(mm, addr, ptep, pteval)                      \</span>
<span class="p_del">-	do {                                                    \</span>
<span class="p_add">+#define pte_inserted(x)						\</span>
<span class="p_add">+	((pte_val(x) &amp; (_PAGE_PRESENT|_PAGE_ACCESSED))		\</span>
<span class="p_add">+	 == (_PAGE_PRESENT|_PAGE_ACCESSED))</span>
<span class="p_add">+</span>
<span class="p_add">+#define set_pte_at(mm, addr, ptep, pteval)			\</span>
<span class="p_add">+	do {							\</span>
<span class="p_add">+		pte_t old_pte;					\</span>
 		unsigned long flags;				\
<span class="p_del">-		spin_lock_irqsave(&amp;pa_dbit_lock, flags);	\</span>
<span class="p_del">-		set_pte(ptep, pteval);                          \</span>
<span class="p_del">-		purge_tlb_entries(mm, addr);                    \</span>
<span class="p_del">-		spin_unlock_irqrestore(&amp;pa_dbit_lock, flags);	\</span>
<span class="p_add">+		spin_lock_irqsave(&amp;pa_tlb_lock, flags);		\</span>
<span class="p_add">+		old_pte = *ptep;				\</span>
<span class="p_add">+		set_pte(ptep, pteval);				\</span>
<span class="p_add">+		if (pte_inserted(old_pte))			\</span>
<span class="p_add">+			purge_tlb_entries(mm, addr);		\</span>
<span class="p_add">+		spin_unlock_irqrestore(&amp;pa_tlb_lock, flags);	\</span>
 	} while (0)
 
 #endif /* !__ASSEMBLY__ */
<span class="p_chunk">@@ -268,7 +286,7 @@</span> <span class="p_context"> extern unsigned long *empty_zero_page;</span>
 
 #define pte_none(x)     (pte_val(x) == 0)
 #define pte_present(x)	(pte_val(x) &amp; _PAGE_PRESENT)
<span class="p_del">-#define pte_clear(mm,addr,xp)	do { pte_val(*(xp)) = 0; } while (0)</span>
<span class="p_add">+#define pte_clear(mm, addr, xp)  set_pte_at(mm, addr, xp, __pte(0))</span>
 
 #define pmd_flag(x)	(pmd_val(x) &amp; PxD_FLAG_MASK)
 #define pmd_address(x)	((unsigned long)(pmd_val(x) &amp;~ PxD_FLAG_MASK) &lt;&lt; PxD_VALUE_SHIFT)
<span class="p_chunk">@@ -435,15 +453,15 @@</span> <span class="p_context"> static inline int ptep_test_and_clear_yo</span>
 	if (!pte_young(*ptep))
 		return 0;
 
<span class="p_del">-	spin_lock_irqsave(&amp;pa_dbit_lock, flags);</span>
<span class="p_add">+	spin_lock_irqsave(&amp;pa_tlb_lock, flags);</span>
 	pte = *ptep;
 	if (!pte_young(pte)) {
<span class="p_del">-		spin_unlock_irqrestore(&amp;pa_dbit_lock, flags);</span>
<span class="p_add">+		spin_unlock_irqrestore(&amp;pa_tlb_lock, flags);</span>
 		return 0;
 	}
 	set_pte(ptep, pte_mkold(pte));
 	purge_tlb_entries(vma-&gt;vm_mm, addr);
<span class="p_del">-	spin_unlock_irqrestore(&amp;pa_dbit_lock, flags);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;pa_tlb_lock, flags);</span>
 	return 1;
 }
 
<span class="p_chunk">@@ -453,11 +471,12 @@</span> <span class="p_context"> static inline pte_t ptep_get_and_clear(s</span>
 	pte_t old_pte;
 	unsigned long flags;
 
<span class="p_del">-	spin_lock_irqsave(&amp;pa_dbit_lock, flags);</span>
<span class="p_add">+	spin_lock_irqsave(&amp;pa_tlb_lock, flags);</span>
 	old_pte = *ptep;
<span class="p_del">-	pte_clear(mm,addr,ptep);</span>
<span class="p_del">-	purge_tlb_entries(mm, addr);</span>
<span class="p_del">-	spin_unlock_irqrestore(&amp;pa_dbit_lock, flags);</span>
<span class="p_add">+	set_pte(ptep, __pte(0));</span>
<span class="p_add">+	if (pte_inserted(old_pte))</span>
<span class="p_add">+		purge_tlb_entries(mm, addr);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;pa_tlb_lock, flags);</span>
 
 	return old_pte;
 }
<span class="p_chunk">@@ -465,10 +484,10 @@</span> <span class="p_context"> static inline pte_t ptep_get_and_clear(s</span>
 static inline void ptep_set_wrprotect(struct mm_struct *mm, unsigned long addr, pte_t *ptep)
 {
 	unsigned long flags;
<span class="p_del">-	spin_lock_irqsave(&amp;pa_dbit_lock, flags);</span>
<span class="p_add">+	spin_lock_irqsave(&amp;pa_tlb_lock, flags);</span>
 	set_pte(ptep, pte_wrprotect(*ptep));
 	purge_tlb_entries(mm, addr);
<span class="p_del">-	spin_unlock_irqrestore(&amp;pa_dbit_lock, flags);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;pa_tlb_lock, flags);</span>
 }
 
 #define pte_same(A,B)	(pte_val(A) == pte_val(B))
<span class="p_header">--- a/arch/parisc/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/parisc/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -13,6 +13,9 @@</span> <span class="p_context"></span>
  * active at any one time on the Merced bus.  This tlb purge
  * synchronisation is fairly lightweight and harmless so we activate
  * it on all systems not just the N class.
<span class="p_add">+</span>
<span class="p_add">+ * It is also used to ensure PTE updates are atomic and consistent</span>
<span class="p_add">+ * with the TLB.</span>
  */
 extern spinlock_t pa_tlb_lock;
 
<span class="p_chunk">@@ -24,20 +27,24 @@</span> <span class="p_context"> extern void flush_tlb_all_local(void *);</span>
 
 #define smp_flush_tlb_all()	flush_tlb_all()
 
<span class="p_add">+int __flush_tlb_range(unsigned long sid,</span>
<span class="p_add">+	unsigned long start, unsigned long end);</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) \</span>
<span class="p_add">+	__flush_tlb_range((vma)-&gt;vm_mm-&gt;context, start, end)</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_kernel_range(start, end) \</span>
<span class="p_add">+	__flush_tlb_range(0, start, end)</span>
<span class="p_add">+</span>
 /*
  * flush_tlb_mm()
  *
<span class="p_del">- * XXX This code is NOT valid for HP-UX compatibility processes,</span>
<span class="p_del">- * (although it will probably work 99% of the time). HP-UX</span>
<span class="p_del">- * processes are free to play with the space id&#39;s and save them</span>
<span class="p_del">- * over long periods of time, etc. so we have to preserve the</span>
<span class="p_del">- * space and just flush the entire tlb. We need to check the</span>
<span class="p_del">- * personality in order to do that, but the personality is not</span>
<span class="p_del">- * currently being set correctly.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Of course, Linux processes could do the same thing, but</span>
<span class="p_del">- * we don&#39;t support that (and the compilers, dynamic linker,</span>
<span class="p_del">- * etc. do not do that).</span>
<span class="p_add">+ * The code to switch to a new context is NOT valid for processes</span>
<span class="p_add">+ * which play with the space id&#39;s.  Thus, we have to preserve the</span>
<span class="p_add">+ * space and just flush the entire tlb.  However, the compilers,</span>
<span class="p_add">+ * dynamic linker, etc, do not manipulate space id&#39;s, so there</span>
<span class="p_add">+ * could be a significant performance benefit in switching contexts</span>
<span class="p_add">+ * and not flushing the whole tlb.</span>
  */
 
 static inline void flush_tlb_mm(struct mm_struct *mm)
<span class="p_chunk">@@ -45,10 +52,18 @@</span> <span class="p_context"> static inline void flush_tlb_mm(struct m</span>
 	BUG_ON(mm == &amp;init_mm); /* Should never happen */
 
 #if 1 || defined(CONFIG_SMP)
<span class="p_add">+	/* Except for very small threads, flushing the whole TLB is</span>
<span class="p_add">+	 * faster than using __flush_tlb_range.  The pdtlb and pitlb</span>
<span class="p_add">+	 * instructions are very slow because of the TLB broadcast.</span>
<span class="p_add">+	 * It might be faster to do local range flushes on all CPUs</span>
<span class="p_add">+	 * on PA 2.0 systems.</span>
<span class="p_add">+	 */</span>
 	flush_tlb_all();
 #else
 	/* FIXME: currently broken, causing space id and protection ids
<span class="p_del">-	 *  to go out of sync, resulting in faults on userspace accesses.</span>
<span class="p_add">+	 * to go out of sync, resulting in faults on userspace accesses.</span>
<span class="p_add">+	 * This approach needs further investigation since running many</span>
<span class="p_add">+	 * small applications (e.g., GCC testsuite) is faster on HP-UX.</span>
 	 */
 	if (mm) {
 		if (mm-&gt;context != 0)
<span class="p_chunk">@@ -65,22 +80,12 @@</span> <span class="p_context"> static inline void flush_tlb_page(struct</span>
 {
 	unsigned long flags, sid;
 
<span class="p_del">-	/* For one page, it&#39;s not worth testing the split_tlb variable */</span>
<span class="p_del">-</span>
<span class="p_del">-	mb();</span>
 	sid = vma-&gt;vm_mm-&gt;context;
 	purge_tlb_start(flags);
 	mtsp(sid, 1);
 	pdtlb(addr);
<span class="p_del">-	pitlb(addr);</span>
<span class="p_add">+	if (unlikely(split_tlb))</span>
<span class="p_add">+		pitlb(addr);</span>
 	purge_tlb_end(flags);
 }
<span class="p_del">-</span>
<span class="p_del">-void __flush_tlb_range(unsigned long sid,</span>
<span class="p_del">-	unsigned long start, unsigned long end);</span>
<span class="p_del">-</span>
<span class="p_del">-#define flush_tlb_range(vma,start,end) __flush_tlb_range((vma)-&gt;vm_mm-&gt;context,start,end)</span>
<span class="p_del">-</span>
<span class="p_del">-#define flush_tlb_kernel_range(start, end) __flush_tlb_range(0,start,end)</span>
<span class="p_del">-</span>
 #endif
<span class="p_header">--- a/arch/parisc/kernel/cache.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/cache.c</span>
<span class="p_chunk">@@ -342,12 +342,15 @@</span> <span class="p_context"> EXPORT_SYMBOL(flush_data_cache_local);</span>
 EXPORT_SYMBOL(flush_kernel_icache_range_asm);
 
 #define FLUSH_THRESHOLD 0x80000 /* 0.5MB */
<span class="p_del">-int parisc_cache_flush_threshold __read_mostly = FLUSH_THRESHOLD;</span>
<span class="p_add">+static unsigned long parisc_cache_flush_threshold __read_mostly = FLUSH_THRESHOLD;</span>
<span class="p_add">+</span>
<span class="p_add">+#define FLUSH_TLB_THRESHOLD (2*1024*1024) /* 2MB initial TLB threshold */</span>
<span class="p_add">+static unsigned long parisc_tlb_flush_threshold __read_mostly = FLUSH_TLB_THRESHOLD;</span>
 
 void __init parisc_setup_cache_timing(void)
 {
 	unsigned long rangetime, alltime;
<span class="p_del">-	unsigned long size;</span>
<span class="p_add">+	unsigned long size, start;</span>
 
 	alltime = mfctl(16);
 	flush_data_cache();
<span class="p_chunk">@@ -364,14 +367,43 @@</span> <span class="p_context"> void __init parisc_setup_cache_timing(vo</span>
 	/* Racy, but if we see an intermediate value, it&#39;s ok too... */
 	parisc_cache_flush_threshold = size * alltime / rangetime;
 
<span class="p_del">-	parisc_cache_flush_threshold = (parisc_cache_flush_threshold + L1_CACHE_BYTES - 1) &amp;~ (L1_CACHE_BYTES - 1); </span>
<span class="p_add">+	parisc_cache_flush_threshold = L1_CACHE_ALIGN(parisc_cache_flush_threshold);</span>
 	if (!parisc_cache_flush_threshold)
 		parisc_cache_flush_threshold = FLUSH_THRESHOLD;
 
 	if (parisc_cache_flush_threshold &gt; cache_info.dc_size)
 		parisc_cache_flush_threshold = cache_info.dc_size;
 
<span class="p_del">-	printk(KERN_INFO &quot;Setting cache flush threshold to %x (%d CPUs online)\n&quot;, parisc_cache_flush_threshold, num_online_cpus());</span>
<span class="p_add">+	printk(KERN_INFO &quot;Setting cache flush threshold to %lu kB\n&quot;,</span>
<span class="p_add">+		parisc_cache_flush_threshold/1024);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* calculate TLB flush threshold */</span>
<span class="p_add">+</span>
<span class="p_add">+	alltime = mfctl(16);</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+	alltime = mfctl(16) - alltime;</span>
<span class="p_add">+</span>
<span class="p_add">+	size = PAGE_SIZE;</span>
<span class="p_add">+	start = (unsigned long) _text;</span>
<span class="p_add">+	rangetime = mfctl(16);</span>
<span class="p_add">+	while (start &lt; (unsigned long) _end) {</span>
<span class="p_add">+		flush_tlb_kernel_range(start, start + PAGE_SIZE);</span>
<span class="p_add">+		start += PAGE_SIZE;</span>
<span class="p_add">+		size += PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	rangetime = mfctl(16) - rangetime;</span>
<span class="p_add">+</span>
<span class="p_add">+	printk(KERN_DEBUG &quot;Whole TLB flush %lu cycles, flushing %lu bytes %lu cycles\n&quot;,</span>
<span class="p_add">+		alltime, size, rangetime);</span>
<span class="p_add">+</span>
<span class="p_add">+	parisc_tlb_flush_threshold = size * alltime / rangetime;</span>
<span class="p_add">+	parisc_tlb_flush_threshold *= num_online_cpus();</span>
<span class="p_add">+	parisc_tlb_flush_threshold = PAGE_ALIGN(parisc_tlb_flush_threshold);</span>
<span class="p_add">+	if (!parisc_tlb_flush_threshold)</span>
<span class="p_add">+		parisc_tlb_flush_threshold = FLUSH_TLB_THRESHOLD;</span>
<span class="p_add">+</span>
<span class="p_add">+	printk(KERN_INFO &quot;Setting TLB flush threshold to %lu kB\n&quot;,</span>
<span class="p_add">+		parisc_tlb_flush_threshold/1024);</span>
 }
 
 extern void purge_kernel_dcache_page_asm(unsigned long);
<span class="p_chunk">@@ -403,48 +435,45 @@</span> <span class="p_context"> void copy_user_page(void *vto, void *vfr</span>
 }
 EXPORT_SYMBOL(copy_user_page);
 
<span class="p_del">-void purge_tlb_entries(struct mm_struct *mm, unsigned long addr)</span>
<span class="p_add">+/* __flush_tlb_range()</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * returns 1 if all TLBs were flushed.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int __flush_tlb_range(unsigned long sid, unsigned long start,</span>
<span class="p_add">+		      unsigned long end)</span>
 {
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Note: purge_tlb_entries can be called at startup with</span>
<span class="p_del">-	   no context.  */</span>
<span class="p_add">+	unsigned long flags, size;</span>
 
<span class="p_del">-	purge_tlb_start(flags);</span>
<span class="p_del">-	mtsp(mm-&gt;context, 1);</span>
<span class="p_del">-	pdtlb(addr);</span>
<span class="p_del">-	pitlb(addr);</span>
<span class="p_del">-	purge_tlb_end(flags);</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL(purge_tlb_entries);</span>
<span class="p_del">-</span>
<span class="p_del">-void __flush_tlb_range(unsigned long sid, unsigned long start,</span>
<span class="p_del">-		       unsigned long end)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long npages;</span>
<span class="p_del">-</span>
<span class="p_del">-	npages = ((end - (start &amp; PAGE_MASK)) + (PAGE_SIZE - 1)) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_del">-	if (npages &gt;= 512)  /* 2MB of space: arbitrary, should be tuned */</span>
<span class="p_add">+	size = (end - start);</span>
<span class="p_add">+	if (size &gt;= parisc_tlb_flush_threshold) {</span>
 		flush_tlb_all();
<span class="p_del">-	else {</span>
<span class="p_del">-		unsigned long flags;</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Purge TLB entries for small ranges using the pdtlb and</span>
<span class="p_add">+	   pitlb instructions.  These instructions execute locally</span>
<span class="p_add">+	   but cause a purge request to be broadcast to other TLBs.  */</span>
<span class="p_add">+	if (likely(!split_tlb)) {</span>
<span class="p_add">+		while (start &lt; end) {</span>
<span class="p_add">+			purge_tlb_start(flags);</span>
<span class="p_add">+			mtsp(sid, 1);</span>
<span class="p_add">+			pdtlb(start);</span>
<span class="p_add">+			purge_tlb_end(flags);</span>
<span class="p_add">+			start += PAGE_SIZE;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
 
<span class="p_add">+	/* split TLB case */</span>
<span class="p_add">+	while (start &lt; end) {</span>
 		purge_tlb_start(flags);
 		mtsp(sid, 1);
<span class="p_del">-		if (split_tlb) {</span>
<span class="p_del">-			while (npages--) {</span>
<span class="p_del">-				pdtlb(start);</span>
<span class="p_del">-				pitlb(start);</span>
<span class="p_del">-				start += PAGE_SIZE;</span>
<span class="p_del">-			}</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			while (npages--) {</span>
<span class="p_del">-				pdtlb(start);</span>
<span class="p_del">-				start += PAGE_SIZE;</span>
<span class="p_del">-			}</span>
<span class="p_del">-		}</span>
<span class="p_add">+		pdtlb(start);</span>
<span class="p_add">+		pitlb(start);</span>
 		purge_tlb_end(flags);
<span class="p_add">+		start += PAGE_SIZE;</span>
 	}
<span class="p_add">+	return 0;</span>
 }
 
 static void cacheflush_h_tmp_function(void *dummy)
<span class="p_header">--- a/arch/parisc/kernel/entry.S</span>
<span class="p_header">+++ b/arch/parisc/kernel/entry.S</span>
<span class="p_chunk">@@ -45,7 +45,7 @@</span> <span class="p_context"></span>
 	.level 2.0
 #endif
 
<span class="p_del">-	.import         pa_dbit_lock,data</span>
<span class="p_add">+	.import		pa_tlb_lock,data</span>
 
 	/* space_to_prot macro creates a prot id from a space id */
 
<span class="p_chunk">@@ -420,8 +420,8 @@</span> <span class="p_context"></span>
 	SHLREG		%r9,PxD_VALUE_SHIFT,\pmd
 	extru		\va,31-PAGE_SHIFT,ASM_BITS_PER_PTE,\index
 	dep		%r0,31,PAGE_SHIFT,\pmd  /* clear offset */
<span class="p_del">-	shladd		\index,BITS_PER_PTE_ENTRY,\pmd,\pmd</span>
<span class="p_del">-	LDREG		%r0(\pmd),\pte		/* pmd is now pte */</span>
<span class="p_add">+	shladd		\index,BITS_PER_PTE_ENTRY,\pmd,\pmd /* pmd is now pte */</span>
<span class="p_add">+	LDREG		%r0(\pmd),\pte</span>
 	bb,&gt;=,n		\pte,_PAGE_PRESENT_BIT,\fault
 	.endm
 
<span class="p_chunk">@@ -453,57 +453,53 @@</span> <span class="p_context"></span>
 	L2_ptep		\pgd,\pte,\index,\va,\fault
 	.endm
 
<span class="p_del">-	/* Acquire pa_dbit_lock lock. */</span>
<span class="p_del">-	.macro		dbit_lock	spc,tmp,tmp1</span>
<span class="p_add">+	/* Acquire pa_tlb_lock lock and recheck page is still present. */</span>
<span class="p_add">+	.macro		tlb_lock	spc,ptp,pte,tmp,tmp1,fault</span>
 #ifdef CONFIG_SMP
 	cmpib,COND(=),n	0,\spc,2f
<span class="p_del">-	load32		PA(pa_dbit_lock),\tmp</span>
<span class="p_add">+	load32		PA(pa_tlb_lock),\tmp</span>
 1:	LDCW		0(\tmp),\tmp1
 	cmpib,COND(=)	0,\tmp1,1b
 	nop
<span class="p_add">+	LDREG		0(\ptp),\pte</span>
<span class="p_add">+	bb,&lt;,n		\pte,_PAGE_PRESENT_BIT,2f</span>
<span class="p_add">+	b		\fault</span>
<span class="p_add">+	stw		 \spc,0(\tmp)</span>
 2:
 #endif
 	.endm
 
<span class="p_del">-	/* Release pa_dbit_lock lock without reloading lock address. */</span>
<span class="p_del">-	.macro		dbit_unlock0	spc,tmp</span>
<span class="p_add">+	/* Release pa_tlb_lock lock without reloading lock address. */</span>
<span class="p_add">+	.macro		tlb_unlock0	spc,tmp</span>
 #ifdef CONFIG_SMP
 	or,COND(=)	%r0,\spc,%r0
 	stw             \spc,0(\tmp)
 #endif
 	.endm
 
<span class="p_del">-	/* Release pa_dbit_lock lock. */</span>
<span class="p_del">-	.macro		dbit_unlock1	spc,tmp</span>
<span class="p_add">+	/* Release pa_tlb_lock lock. */</span>
<span class="p_add">+	.macro		tlb_unlock1	spc,tmp</span>
 #ifdef CONFIG_SMP
<span class="p_del">-	load32		PA(pa_dbit_lock),\tmp</span>
<span class="p_del">-	dbit_unlock0	\spc,\tmp</span>
<span class="p_add">+	load32		PA(pa_tlb_lock),\tmp</span>
<span class="p_add">+	tlb_unlock0	\spc,\tmp</span>
 #endif
 	.endm
 
 	/* Set the _PAGE_ACCESSED bit of the PTE.  Be clever and
 	 * don&#39;t needlessly dirty the cache line if it was already set */
<span class="p_del">-	.macro		update_ptep	spc,ptep,pte,tmp,tmp1</span>
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_del">-	or,COND(=)	%r0,\spc,%r0</span>
<span class="p_del">-	LDREG		0(\ptep),\pte</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	.macro		update_accessed	ptp,pte,tmp,tmp1</span>
 	ldi		_PAGE_ACCESSED,\tmp1
 	or		\tmp1,\pte,\tmp
 	and,COND(&lt;&gt;)	\tmp1,\pte,%r0
<span class="p_del">-	STREG		\tmp,0(\ptep)</span>
<span class="p_add">+	STREG		\tmp,0(\ptp)</span>
 	.endm
 
 	/* Set the dirty bit (and accessed bit).  No need to be
 	 * clever, this is only used from the dirty fault */
<span class="p_del">-	.macro		update_dirty	spc,ptep,pte,tmp</span>
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_del">-	or,COND(=)	%r0,\spc,%r0</span>
<span class="p_del">-	LDREG		0(\ptep),\pte</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	.macro		update_dirty	ptp,pte,tmp</span>
 	ldi		_PAGE_ACCESSED|_PAGE_DIRTY,\tmp
 	or		\tmp,\pte,\pte
<span class="p_del">-	STREG		\pte,0(\ptep)</span>
<span class="p_add">+	STREG		\pte,0(\ptp)</span>
 	.endm
 
 	/* bitshift difference between a PFN (based on kernel&#39;s PAGE_SIZE)
<span class="p_chunk">@@ -1148,14 +1144,14 @@</span> <span class="p_context"> dtlb_miss_20w:</span>
 
 	L3_ptep		ptp,pte,t0,va,dtlb_check_alias_20w
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,dtlb_check_alias_20w</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb	spc,pte,prot
 	
 	idtlbt          pte,prot
<span class="p_del">-	dbit_unlock1	spc,t0</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1174,14 +1170,14 @@</span> <span class="p_context"> nadtlb_miss_20w:</span>
 
 	L3_ptep		ptp,pte,t0,va,nadtlb_check_alias_20w
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,nadtlb_check_alias_20w</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb	spc,pte,prot
 
 	idtlbt          pte,prot
<span class="p_del">-	dbit_unlock1	spc,t0</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1202,20 +1198,20 @@</span> <span class="p_context"> dtlb_miss_11:</span>
 
 	L2_ptep		ptp,pte,t0,va,dtlb_check_alias_11
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,dtlb_check_alias_11</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb_11	spc,pte,prot
 
<span class="p_del">-	mfsp		%sr1,t0  /* Save sr1 so we can use it in tlb inserts */</span>
<span class="p_add">+	mfsp		%sr1,t1  /* Save sr1 so we can use it in tlb inserts */</span>
 	mtsp		spc,%sr1
 
 	idtlba		pte,(%sr1,va)
 	idtlbp		prot,(%sr1,va)
 
<span class="p_del">-	mtsp		t0, %sr1	/* Restore sr1 */</span>
<span class="p_del">-	dbit_unlock1	spc,t0</span>
<span class="p_add">+	mtsp		t1, %sr1	/* Restore sr1 */</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1235,21 +1231,20 @@</span> <span class="p_context"> nadtlb_miss_11:</span>
 
 	L2_ptep		ptp,pte,t0,va,nadtlb_check_alias_11
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,nadtlb_check_alias_11</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb_11	spc,pte,prot
 
<span class="p_del">-</span>
<span class="p_del">-	mfsp		%sr1,t0  /* Save sr1 so we can use it in tlb inserts */</span>
<span class="p_add">+	mfsp		%sr1,t1  /* Save sr1 so we can use it in tlb inserts */</span>
 	mtsp		spc,%sr1
 
 	idtlba		pte,(%sr1,va)
 	idtlbp		prot,(%sr1,va)
 
<span class="p_del">-	mtsp		t0, %sr1	/* Restore sr1 */</span>
<span class="p_del">-	dbit_unlock1	spc,t0</span>
<span class="p_add">+	mtsp		t1, %sr1	/* Restore sr1 */</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1269,16 +1264,16 @@</span> <span class="p_context"> dtlb_miss_20:</span>
 
 	L2_ptep		ptp,pte,t0,va,dtlb_check_alias_20
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,dtlb_check_alias_20</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb	spc,pte,prot
 
<span class="p_del">-	f_extend	pte,t0</span>
<span class="p_add">+	f_extend	pte,t1</span>
 
 	idtlbt          pte,prot
<span class="p_del">-	dbit_unlock1	spc,t0</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1297,16 +1292,16 @@</span> <span class="p_context"> nadtlb_miss_20:</span>
 
 	L2_ptep		ptp,pte,t0,va,nadtlb_check_alias_20
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,nadtlb_check_alias_20</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb	spc,pte,prot
 
<span class="p_del">-	f_extend	pte,t0</span>
<span class="p_add">+	f_extend	pte,t1</span>
 	
<span class="p_del">-        idtlbt          pte,prot</span>
<span class="p_del">-	dbit_unlock1	spc,t0</span>
<span class="p_add">+	idtlbt		pte,prot</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1406,14 +1401,14 @@</span> <span class="p_context"> itlb_miss_20w:</span>
 
 	L3_ptep		ptp,pte,t0,va,itlb_fault
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,itlb_fault</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb	spc,pte,prot
 	
 	iitlbt          pte,prot
<span class="p_del">-	dbit_unlock1	spc,t0</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1430,14 +1425,14 @@</span> <span class="p_context"> naitlb_miss_20w:</span>
 
 	L3_ptep		ptp,pte,t0,va,naitlb_check_alias_20w
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,naitlb_check_alias_20w</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb	spc,pte,prot
 
 	iitlbt          pte,prot
<span class="p_del">-	dbit_unlock1	spc,t0</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1458,20 +1453,20 @@</span> <span class="p_context"> itlb_miss_11:</span>
 
 	L2_ptep		ptp,pte,t0,va,itlb_fault
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,itlb_fault</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb_11	spc,pte,prot
 
<span class="p_del">-	mfsp		%sr1,t0  /* Save sr1 so we can use it in tlb inserts */</span>
<span class="p_add">+	mfsp		%sr1,t1  /* Save sr1 so we can use it in tlb inserts */</span>
 	mtsp		spc,%sr1
 
 	iitlba		pte,(%sr1,va)
 	iitlbp		prot,(%sr1,va)
 
<span class="p_del">-	mtsp		t0, %sr1	/* Restore sr1 */</span>
<span class="p_del">-	dbit_unlock1	spc,t0</span>
<span class="p_add">+	mtsp		t1, %sr1	/* Restore sr1 */</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1482,20 +1477,20 @@</span> <span class="p_context"> naitlb_miss_11:</span>
 
 	L2_ptep		ptp,pte,t0,va,naitlb_check_alias_11
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,naitlb_check_alias_11</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb_11	spc,pte,prot
 
<span class="p_del">-	mfsp		%sr1,t0  /* Save sr1 so we can use it in tlb inserts */</span>
<span class="p_add">+	mfsp		%sr1,t1  /* Save sr1 so we can use it in tlb inserts */</span>
 	mtsp		spc,%sr1
 
 	iitlba		pte,(%sr1,va)
 	iitlbp		prot,(%sr1,va)
 
<span class="p_del">-	mtsp		t0, %sr1	/* Restore sr1 */</span>
<span class="p_del">-	dbit_unlock1	spc,t0</span>
<span class="p_add">+	mtsp		t1, %sr1	/* Restore sr1 */</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1516,16 +1511,16 @@</span> <span class="p_context"> itlb_miss_20:</span>
 
 	L2_ptep		ptp,pte,t0,va,itlb_fault
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,itlb_fault</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb	spc,pte,prot
 
<span class="p_del">-	f_extend	pte,t0	</span>
<span class="p_add">+	f_extend	pte,t1</span>
 
 	iitlbt          pte,prot
<span class="p_del">-	dbit_unlock1	spc,t0</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1536,16 +1531,16 @@</span> <span class="p_context"> naitlb_miss_20:</span>
 
 	L2_ptep		ptp,pte,t0,va,naitlb_check_alias_20
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_ptep	spc,ptp,pte,t0,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,naitlb_check_alias_20</span>
<span class="p_add">+	update_accessed	ptp,pte,t0,t1</span>
 
 	make_insert_tlb	spc,pte,prot
 
<span class="p_del">-	f_extend	pte,t0</span>
<span class="p_add">+	f_extend	pte,t1</span>
 
 	iitlbt          pte,prot
<span class="p_del">-	dbit_unlock1	spc,t0</span>
 
<span class="p_add">+	tlb_unlock1	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1568,14 +1563,14 @@</span> <span class="p_context"> dbit_trap_20w:</span>
 
 	L3_ptep		ptp,pte,t0,va,dbit_fault
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_dirty	spc,ptp,pte,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,dbit_fault</span>
<span class="p_add">+	update_dirty	ptp,pte,t1</span>
 
 	make_insert_tlb	spc,pte,prot
 		
 	idtlbt          pte,prot
<span class="p_del">-	dbit_unlock0	spc,t0</span>
 
<span class="p_add">+	tlb_unlock0	spc,t0</span>
 	rfir
 	nop
 #else
<span class="p_chunk">@@ -1588,8 +1583,8 @@</span> <span class="p_context"> dbit_trap_11:</span>
 
 	L2_ptep		ptp,pte,t0,va,dbit_fault
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_dirty	spc,ptp,pte,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,dbit_fault</span>
<span class="p_add">+	update_dirty	ptp,pte,t1</span>
 
 	make_insert_tlb_11	spc,pte,prot
 
<span class="p_chunk">@@ -1600,8 +1595,8 @@</span> <span class="p_context"> dbit_trap_11:</span>
 	idtlbp		prot,(%sr1,va)
 
 	mtsp            t1, %sr1     /* Restore sr1 */
<span class="p_del">-	dbit_unlock0	spc,t0</span>
 
<span class="p_add">+	tlb_unlock0	spc,t0</span>
 	rfir
 	nop
 
<span class="p_chunk">@@ -1612,16 +1607,16 @@</span> <span class="p_context"> dbit_trap_20:</span>
 
 	L2_ptep		ptp,pte,t0,va,dbit_fault
 
<span class="p_del">-	dbit_lock	spc,t0,t1</span>
<span class="p_del">-	update_dirty	spc,ptp,pte,t1</span>
<span class="p_add">+	tlb_lock	spc,ptp,pte,t0,t1,dbit_fault</span>
<span class="p_add">+	update_dirty	ptp,pte,t1</span>
 
 	make_insert_tlb	spc,pte,prot
 
 	f_extend	pte,t1
 	
<span class="p_del">-        idtlbt          pte,prot</span>
<span class="p_del">-	dbit_unlock0	spc,t0</span>
<span class="p_add">+	idtlbt		pte,prot</span>
 
<span class="p_add">+	tlb_unlock0	spc,t0</span>
 	rfir
 	nop
 #endif
<span class="p_header">--- a/arch/parisc/kernel/traps.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/traps.c</span>
<span class="p_chunk">@@ -43,10 +43,6 @@</span> <span class="p_context"></span>
 
 #include &quot;../math-emu/math-emu.h&quot;	/* for handle_fpe() */
 
<span class="p_del">-#if defined(CONFIG_SMP) || defined(CONFIG_DEBUG_SPINLOCK)</span>
<span class="p_del">-DEFINE_SPINLOCK(pa_dbit_lock);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 static void parisc_show_stack(struct task_struct *task, unsigned long *sp,
 	struct pt_regs *regs);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



