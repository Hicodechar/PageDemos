
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[13/31] mm, memcg: move memcg limit enforcement from zones to nodes - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [13/31] mm, memcg: move memcg limit enforcement from zones to nodes</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 1, 2016, 3:37 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1467387466-10022-14-git-send-email-mgorman@techsingularity.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9210051/mbox/"
   >mbox</a>
|
   <a href="/patch/9210051/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9210051/">/patch/9210051/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	55DBF607D6 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  1 Jul 2016 15:40:30 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 44C1F28383
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  1 Jul 2016 15:40:30 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 395D42868E; Fri,  1 Jul 2016 15:40:30 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B2E86286AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  1 Jul 2016 15:40:28 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752635AbcGAPkP (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 1 Jul 2016 11:40:15 -0400
Received: from outbound-smtp04.blacknight.com ([81.17.249.35]:51266 &quot;EHLO
	outbound-smtp04.blacknight.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752199AbcGAPkM (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 1 Jul 2016 11:40:12 -0400
Received: from mail.blacknight.com (pemlinmail06.blacknight.ie
	[81.17.255.152])
	by outbound-smtp04.blacknight.com (Postfix) with ESMTPS id 617B798E49
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Fri,  1 Jul 2016 15:40:10 +0000 (UTC)
Received: (qmail 22126 invoked from network); 1 Jul 2016 15:40:10 -0000
Received: from unknown (HELO stampy.163woodhaven.lan)
	(mgorman@techsingularity.net@[37.228.231.136])
	by 81.17.254.9 with ESMTPA; 1 Jul 2016 15:40:10 -0000
From: Mel Gorman &lt;mgorman@techsingularity.net&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;, Linux-MM &lt;linux-mm@kvack.org&gt;
Cc: Rik van Riel &lt;riel@surriel.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;
Subject: [PATCH 13/31] mm,
	memcg: move memcg limit enforcement from zones to nodes
Date: Fri,  1 Jul 2016 16:37:28 +0100
Message-Id: &lt;1467387466-10022-14-git-send-email-mgorman@techsingularity.net&gt;
X-Mailer: git-send-email 2.6.4
In-Reply-To: &lt;1467387466-10022-1-git-send-email-mgorman@techsingularity.net&gt;
References: &lt;1467387466-10022-1-git-send-email-mgorman@techsingularity.net&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - July 1, 2016, 3:37 p.m.</div>
<pre class="content">
Memcg needs adjustment after moving LRUs to the node. Limits are tracked
per memcg but the soft-limit excess is tracked per zone. As global page
reclaim is based on the node, it is easy to imagine a situation where
a zone soft limit is exceeded even though the memcg limit is fine.

This patch moves the soft limit tree the node.  Technically, all the variable
names should also change but people are already familiar by the meaning of
&quot;mz&quot; even if &quot;mn&quot; would be a more appropriate name now.

Link: http://lkml.kernel.org/r/1466518566-30034-14-git-send-email-mgorman@techsingularity.net
<span class="signed-off-by">Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="acked-by">Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;
Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;
Cc: Rik van Riel &lt;riel@surriel.com&gt;
<span class="signed-off-by">Signed-off-by: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
---
 include/linux/memcontrol.h |  38 ++++-----
 include/linux/swap.h       |   2 +-
 mm/memcontrol.c            | 190 ++++++++++++++++++++-------------------------
 mm/vmscan.c                |  19 +++--
 mm/workingset.c            |   6 +-
 5 files changed, 111 insertions(+), 144 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="p_header">index 48b43c709ed7..65e472f48f6c 100644</span>
<span class="p_header">--- a/include/linux/memcontrol.h</span>
<span class="p_header">+++ b/include/linux/memcontrol.h</span>
<span class="p_chunk">@@ -61,7 +61,7 @@</span> <span class="p_context"> enum mem_cgroup_stat_index {</span>
 };
 
 struct mem_cgroup_reclaim_cookie {
<span class="p_del">-	struct zone *zone;</span>
<span class="p_add">+	pg_data_t *pgdat;</span>
 	int priority;
 	unsigned int generation;
 };
<span class="p_chunk">@@ -119,7 +119,7 @@</span> <span class="p_context"> struct mem_cgroup_reclaim_iter {</span>
 /*
  * per-zone information in memory controller.
  */
<span class="p_del">-struct mem_cgroup_per_zone {</span>
<span class="p_add">+struct mem_cgroup_per_node {</span>
 	struct lruvec		lruvec;
 	unsigned long		lru_size[NR_LRU_LISTS];
 
<span class="p_chunk">@@ -133,10 +133,6 @@</span> <span class="p_context"> struct mem_cgroup_per_zone {</span>
 						/* use container_of	   */
 };
 
<span class="p_del">-struct mem_cgroup_per_node {</span>
<span class="p_del">-	struct mem_cgroup_per_zone zoneinfo[MAX_NR_ZONES];</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 struct mem_cgroup_threshold {
 	struct eventfd_ctx *eventfd;
 	unsigned long threshold;
<span class="p_chunk">@@ -315,19 +311,15 @@</span> <span class="p_context"> void mem_cgroup_uncharge_list(struct list_head *page_list);</span>
 
 void mem_cgroup_migrate(struct page *oldpage, struct page *newpage);
 
<span class="p_del">-static inline struct mem_cgroup_per_zone *</span>
<span class="p_del">-mem_cgroup_zone_zoneinfo(struct mem_cgroup *memcg, struct zone *zone)</span>
<span class="p_add">+static struct mem_cgroup_per_node *</span>
<span class="p_add">+mem_cgroup_nodeinfo(struct mem_cgroup *memcg, int nid)</span>
 {
<span class="p_del">-	int nid = zone_to_nid(zone);</span>
<span class="p_del">-	int zid = zone_idx(zone);</span>
<span class="p_del">-</span>
<span class="p_del">-	return &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_add">+	return memcg-&gt;nodeinfo[nid];</span>
 }
 
 /**
  * mem_cgroup_lruvec - get the lru list vector for a node or a memcg zone
  * @node: node of the wanted lruvec
<span class="p_del">- * @zone: zone of the wanted lruvec</span>
  * @memcg: memcg of the wanted lruvec
  *
  * Returns the lru list vector holding pages for a given @node or a given
<span class="p_chunk">@@ -335,9 +327,9 @@</span> <span class="p_context"> mem_cgroup_zone_zoneinfo(struct mem_cgroup *memcg, struct zone *zone)</span>
  * is disabled.
  */
 static inline struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,
<span class="p_del">-				struct zone *zone, struct mem_cgroup *memcg)</span>
<span class="p_add">+				struct mem_cgroup *memcg)</span>
 {
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 	struct lruvec *lruvec;
 
 	if (mem_cgroup_disabled()) {
<span class="p_chunk">@@ -345,7 +337,7 @@</span> <span class="p_context"> static inline struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
 		goto out;
 	}
 
<span class="p_del">-	mz = mem_cgroup_zone_zoneinfo(memcg, zone);</span>
<span class="p_add">+	mz = mem_cgroup_nodeinfo(memcg, pgdat-&gt;node_id);</span>
 	lruvec = &amp;mz-&gt;lruvec;
 out:
 	/*
<span class="p_chunk">@@ -353,8 +345,8 @@</span> <span class="p_context"> static inline struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
 	 * we have to be prepared to initialize lruvec-&gt;zone here;
 	 * and if offlined then reonlined, we need to reinitialize it.
 	 */
<span class="p_del">-	if (unlikely(lruvec-&gt;pgdat != zone-&gt;zone_pgdat))</span>
<span class="p_del">-		lruvec-&gt;pgdat = zone-&gt;zone_pgdat;</span>
<span class="p_add">+	if (unlikely(lruvec-&gt;pgdat != pgdat))</span>
<span class="p_add">+		lruvec-&gt;pgdat = pgdat;</span>
 	return lruvec;
 }
 
<span class="p_chunk">@@ -447,9 +439,9 @@</span> <span class="p_context"> unsigned long mem_cgroup_node_nr_lru_pages(struct mem_cgroup *memcg,</span>
 static inline
 unsigned long mem_cgroup_get_lru_size(struct lruvec *lruvec, enum lru_list lru)
 {
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 
<span class="p_del">-	mz = container_of(lruvec, struct mem_cgroup_per_zone, lruvec);</span>
<span class="p_add">+	mz = container_of(lruvec, struct mem_cgroup_per_node, lruvec);</span>
 	return mz-&gt;lru_size[lru];
 }
 
<span class="p_chunk">@@ -520,7 +512,7 @@</span> <span class="p_context"> static inline void mem_cgroup_dec_page_stat(struct page *page,</span>
 	mem_cgroup_update_page_stat(page, idx, -1);
 }
 
<span class="p_del">-unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
<span class="p_add">+unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,</span>
 						gfp_t gfp_mask,
 						unsigned long *total_scanned);
 
<span class="p_chunk">@@ -612,7 +604,7 @@</span> <span class="p_context"> static inline void mem_cgroup_migrate(struct page *old, struct page *new)</span>
 }
 
 static inline struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,
<span class="p_del">-				struct zone *zone, struct mem_cgroup *memcg)</span>
<span class="p_add">+				struct mem_cgroup *memcg)</span>
 {
 	return node_lruvec(pgdat);
 }
<span class="p_chunk">@@ -724,7 +716,7 @@</span> <span class="p_context"> static inline void mem_cgroup_dec_page_stat(struct page *page,</span>
 }
 
 static inline
<span class="p_del">-unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
<span class="p_add">+unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,</span>
 					    gfp_t gfp_mask,
 					    unsigned long *total_scanned)
 {
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index 0ad616d7c381..2a23ddc96edd 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -318,7 +318,7 @@</span> <span class="p_context"> extern unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,</span>
 						  bool may_swap);
 extern unsigned long mem_cgroup_shrink_node(struct mem_cgroup *mem,
 						gfp_t gfp_mask, bool noswap,
<span class="p_del">-						struct zone *zone,</span>
<span class="p_add">+						pg_data_t *pgdat,</span>
 						unsigned long *nr_scanned);
 extern unsigned long shrink_all_memory(unsigned long nr_pages);
 extern int vm_swappiness;
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index c9ebec98e92a..9cbd40ebccd1 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -132,15 +132,11 @@</span> <span class="p_context"> static const char * const mem_cgroup_lru_names[] = {</span>
  * their hierarchy representation
  */
 
<span class="p_del">-struct mem_cgroup_tree_per_zone {</span>
<span class="p_add">+struct mem_cgroup_tree_per_node {</span>
 	struct rb_root rb_root;
 	spinlock_t lock;
 };
 
<span class="p_del">-struct mem_cgroup_tree_per_node {</span>
<span class="p_del">-	struct mem_cgroup_tree_per_zone rb_tree_per_zone[MAX_NR_ZONES];</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 struct mem_cgroup_tree {
 	struct mem_cgroup_tree_per_node *rb_tree_per_node[MAX_NUMNODES];
 };
<span class="p_chunk">@@ -374,37 +370,35 @@</span> <span class="p_context"> ino_t page_cgroup_ino(struct page *page)</span>
 	return ino;
 }
 
<span class="p_del">-static struct mem_cgroup_per_zone *</span>
<span class="p_del">-mem_cgroup_page_zoneinfo(struct mem_cgroup *memcg, struct page *page)</span>
<span class="p_add">+static struct mem_cgroup_per_node *</span>
<span class="p_add">+mem_cgroup_page_nodeinfo(struct mem_cgroup *memcg, struct page *page)</span>
 {
 	int nid = page_to_nid(page);
<span class="p_del">-	int zid = page_zonenum(page);</span>
 
<span class="p_del">-	return &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_add">+	return memcg-&gt;nodeinfo[nid];</span>
 }
 
<span class="p_del">-static struct mem_cgroup_tree_per_zone *</span>
<span class="p_del">-soft_limit_tree_node_zone(int nid, int zid)</span>
<span class="p_add">+static struct mem_cgroup_tree_per_node *</span>
<span class="p_add">+soft_limit_tree_node(int nid)</span>
 {
<span class="p_del">-	return &amp;soft_limit_tree.rb_tree_per_node[nid]-&gt;rb_tree_per_zone[zid];</span>
<span class="p_add">+	return soft_limit_tree.rb_tree_per_node[nid];</span>
 }
 
<span class="p_del">-static struct mem_cgroup_tree_per_zone *</span>
<span class="p_add">+static struct mem_cgroup_tree_per_node *</span>
 soft_limit_tree_from_page(struct page *page)
 {
 	int nid = page_to_nid(page);
<span class="p_del">-	int zid = page_zonenum(page);</span>
 
<span class="p_del">-	return &amp;soft_limit_tree.rb_tree_per_node[nid]-&gt;rb_tree_per_zone[zid];</span>
<span class="p_add">+	return soft_limit_tree.rb_tree_per_node[nid];</span>
 }
 
<span class="p_del">-static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="p_del">-					 struct mem_cgroup_tree_per_zone *mctz,</span>
<span class="p_add">+static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_node *mz,</span>
<span class="p_add">+					 struct mem_cgroup_tree_per_node *mctz,</span>
 					 unsigned long new_usage_in_excess)
 {
 	struct rb_node **p = &amp;mctz-&gt;rb_root.rb_node;
 	struct rb_node *parent = NULL;
<span class="p_del">-	struct mem_cgroup_per_zone *mz_node;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz_node;</span>
 
 	if (mz-&gt;on_tree)
 		return;
<span class="p_chunk">@@ -414,7 +408,7 @@</span> <span class="p_context"> static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_zone *mz,</span>
 		return;
 	while (*p) {
 		parent = *p;
<span class="p_del">-		mz_node = rb_entry(parent, struct mem_cgroup_per_zone,</span>
<span class="p_add">+		mz_node = rb_entry(parent, struct mem_cgroup_per_node,</span>
 					tree_node);
 		if (mz-&gt;usage_in_excess &lt; mz_node-&gt;usage_in_excess)
 			p = &amp;(*p)-&gt;rb_left;
<span class="p_chunk">@@ -430,8 +424,8 @@</span> <span class="p_context"> static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_zone *mz,</span>
 	mz-&gt;on_tree = true;
 }
 
<span class="p_del">-static void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="p_del">-					 struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="p_add">+static void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_node *mz,</span>
<span class="p_add">+					 struct mem_cgroup_tree_per_node *mctz)</span>
 {
 	if (!mz-&gt;on_tree)
 		return;
<span class="p_chunk">@@ -439,8 +433,8 @@</span> <span class="p_context"> static void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_zone *mz,</span>
 	mz-&gt;on_tree = false;
 }
 
<span class="p_del">-static void mem_cgroup_remove_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="p_del">-				       struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="p_add">+static void mem_cgroup_remove_exceeded(struct mem_cgroup_per_node *mz,</span>
<span class="p_add">+				       struct mem_cgroup_tree_per_node *mctz)</span>
 {
 	unsigned long flags;
 
<span class="p_chunk">@@ -464,8 +458,8 @@</span> <span class="p_context"> static unsigned long soft_limit_excess(struct mem_cgroup *memcg)</span>
 static void mem_cgroup_update_tree(struct mem_cgroup *memcg, struct page *page)
 {
 	unsigned long excess;
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_del">-	struct mem_cgroup_tree_per_zone *mctz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
<span class="p_add">+	struct mem_cgroup_tree_per_node *mctz;</span>
 
 	mctz = soft_limit_tree_from_page(page);
 	/*
<span class="p_chunk">@@ -473,7 +467,7 @@</span> <span class="p_context"> static void mem_cgroup_update_tree(struct mem_cgroup *memcg, struct page *page)</span>
 	 * because their event counter is not touched.
 	 */
 	for (; memcg; memcg = parent_mem_cgroup(memcg)) {
<span class="p_del">-		mz = mem_cgroup_page_zoneinfo(memcg, page);</span>
<span class="p_add">+		mz = mem_cgroup_page_nodeinfo(memcg, page);</span>
 		excess = soft_limit_excess(memcg);
 		/*
 		 * We have to update the tree if mz is on RB-tree or
<span class="p_chunk">@@ -498,24 +492,22 @@</span> <span class="p_context"> static void mem_cgroup_update_tree(struct mem_cgroup *memcg, struct page *page)</span>
 
 static void mem_cgroup_remove_from_trees(struct mem_cgroup *memcg)
 {
<span class="p_del">-	struct mem_cgroup_tree_per_zone *mctz;</span>
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_del">-	int nid, zid;</span>
<span class="p_add">+	struct mem_cgroup_tree_per_node *mctz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
<span class="p_add">+	int nid;</span>
 
 	for_each_node(nid) {
<span class="p_del">-		for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="p_del">-			mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_del">-			mctz = soft_limit_tree_node_zone(nid, zid);</span>
<span class="p_del">-			mem_cgroup_remove_exceeded(mz, mctz);</span>
<span class="p_del">-		}</span>
<span class="p_add">+		mz = mem_cgroup_nodeinfo(memcg, nid);</span>
<span class="p_add">+		mctz = soft_limit_tree_node(nid);</span>
<span class="p_add">+		mem_cgroup_remove_exceeded(mz, mctz);</span>
 	}
 }
 
<span class="p_del">-static struct mem_cgroup_per_zone *</span>
<span class="p_del">-__mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="p_add">+static struct mem_cgroup_per_node *</span>
<span class="p_add">+__mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_node *mctz)</span>
 {
 	struct rb_node *rightmost = NULL;
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 
 retry:
 	mz = NULL;
<span class="p_chunk">@@ -523,7 +515,7 @@</span> <span class="p_context"> __mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
 	if (!rightmost)
 		goto done;		/* Nothing to reclaim from */
 
<span class="p_del">-	mz = rb_entry(rightmost, struct mem_cgroup_per_zone, tree_node);</span>
<span class="p_add">+	mz = rb_entry(rightmost, struct mem_cgroup_per_node, tree_node);</span>
 	/*
 	 * Remove the node now but someone else can add it back,
 	 * we will to add it back at the end of reclaim to its correct
<span class="p_chunk">@@ -537,10 +529,10 @@</span> <span class="p_context"> __mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
 	return mz;
 }
 
<span class="p_del">-static struct mem_cgroup_per_zone *</span>
<span class="p_del">-mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="p_add">+static struct mem_cgroup_per_node *</span>
<span class="p_add">+mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_node *mctz)</span>
 {
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 
 	spin_lock_irq(&amp;mctz-&gt;lock);
 	mz = __mem_cgroup_largest_soft_limit_node(mctz);
<span class="p_chunk">@@ -634,20 +626,16 @@</span> <span class="p_context"> unsigned long mem_cgroup_node_nr_lru_pages(struct mem_cgroup *memcg,</span>
 					   int nid, unsigned int lru_mask)
 {
 	unsigned long nr = 0;
<span class="p_del">-	int zid;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
<span class="p_add">+	enum lru_list lru;</span>
 
 	VM_BUG_ON((unsigned)nid &gt;= nr_node_ids);
 
<span class="p_del">-	for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="p_del">-		struct mem_cgroup_per_zone *mz;</span>
<span class="p_del">-		enum lru_list lru;</span>
<span class="p_del">-</span>
<span class="p_del">-		for_each_lru(lru) {</span>
<span class="p_del">-			if (!(BIT(lru) &amp; lru_mask))</span>
<span class="p_del">-				continue;</span>
<span class="p_del">-			mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_del">-			nr += mz-&gt;lru_size[lru];</span>
<span class="p_del">-		}</span>
<span class="p_add">+	for_each_lru(lru) {</span>
<span class="p_add">+		if (!(BIT(lru) &amp; lru_mask))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		mz = mem_cgroup_nodeinfo(memcg, nid);</span>
<span class="p_add">+		nr += mz-&gt;lru_size[lru];</span>
 	}
 	return nr;
 }
<span class="p_chunk">@@ -800,9 +788,9 @@</span> <span class="p_context"> struct mem_cgroup *mem_cgroup_iter(struct mem_cgroup *root,</span>
 	rcu_read_lock();
 
 	if (reclaim) {
<span class="p_del">-		struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+		struct mem_cgroup_per_node *mz;</span>
 
<span class="p_del">-		mz = mem_cgroup_zone_zoneinfo(root, reclaim-&gt;zone);</span>
<span class="p_add">+		mz = mem_cgroup_nodeinfo(root, reclaim-&gt;pgdat-&gt;node_id);</span>
 		iter = &amp;mz-&gt;iter[reclaim-&gt;priority];
 
 		if (prev &amp;&amp; reclaim-&gt;generation != iter-&gt;generation)
<span class="p_chunk">@@ -901,19 +889,17 @@</span> <span class="p_context"> static void invalidate_reclaim_iterators(struct mem_cgroup *dead_memcg)</span>
 {
 	struct mem_cgroup *memcg = dead_memcg;
 	struct mem_cgroup_reclaim_iter *iter;
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_del">-	int nid, zid;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
<span class="p_add">+	int nid;</span>
 	int i;
 
 	while ((memcg = parent_mem_cgroup(memcg))) {
 		for_each_node(nid) {
<span class="p_del">-			for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="p_del">-				mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_del">-				for (i = 0; i &lt;= DEF_PRIORITY; i++) {</span>
<span class="p_del">-					iter = &amp;mz-&gt;iter[i];</span>
<span class="p_del">-					cmpxchg(&amp;iter-&gt;position,</span>
<span class="p_del">-						dead_memcg, NULL);</span>
<span class="p_del">-				}</span>
<span class="p_add">+			mz = mem_cgroup_nodeinfo(memcg, nid);</span>
<span class="p_add">+			for (i = 0; i &lt;= DEF_PRIORITY; i++) {</span>
<span class="p_add">+				iter = &amp;mz-&gt;iter[i];</span>
<span class="p_add">+				cmpxchg(&amp;iter-&gt;position,</span>
<span class="p_add">+					dead_memcg, NULL);</span>
 			}
 		}
 	}
<span class="p_chunk">@@ -945,7 +931,7 @@</span> <span class="p_context"> static void invalidate_reclaim_iterators(struct mem_cgroup *dead_memcg)</span>
  */
 struct lruvec *mem_cgroup_page_lruvec(struct page *page, struct pglist_data *pgdat)
 {
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 	struct mem_cgroup *memcg;
 	struct lruvec *lruvec;
 
<span class="p_chunk">@@ -962,7 +948,7 @@</span> <span class="p_context"> struct lruvec *mem_cgroup_page_lruvec(struct page *page, struct pglist_data *pgd</span>
 	if (!memcg)
 		memcg = root_mem_cgroup;
 
<span class="p_del">-	mz = mem_cgroup_page_zoneinfo(memcg, page);</span>
<span class="p_add">+	mz = mem_cgroup_page_nodeinfo(memcg, page);</span>
 	lruvec = &amp;mz-&gt;lruvec;
 out:
 	/*
<span class="p_chunk">@@ -989,7 +975,7 @@</span> <span class="p_context"> struct lruvec *mem_cgroup_page_lruvec(struct page *page, struct pglist_data *pgd</span>
 void mem_cgroup_update_lru_size(struct lruvec *lruvec, enum lru_list lru,
 				enum zone_type zid, int nr_pages)
 {
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 	unsigned long *lru_size;
 	long size;
 	bool empty;
<span class="p_chunk">@@ -999,7 +985,7 @@</span> <span class="p_context"> void mem_cgroup_update_lru_size(struct lruvec *lruvec, enum lru_list lru,</span>
 	if (mem_cgroup_disabled())
 		return;
 
<span class="p_del">-	mz = container_of(lruvec, struct mem_cgroup_per_zone, lruvec);</span>
<span class="p_add">+	mz = container_of(lruvec, struct mem_cgroup_per_node, lruvec);</span>
 	lru_size = mz-&gt;lru_size + lru;
 	empty = list_empty(lruvec-&gt;lists + lru);
 
<span class="p_chunk">@@ -1392,7 +1378,7 @@</span> <span class="p_context"> int mem_cgroup_select_victim_node(struct mem_cgroup *memcg)</span>
 #endif
 
 static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,
<span class="p_del">-				   struct zone *zone,</span>
<span class="p_add">+				   pg_data_t *pgdat,</span>
 				   gfp_t gfp_mask,
 				   unsigned long *total_scanned)
 {
<span class="p_chunk">@@ -1402,7 +1388,7 @@</span> <span class="p_context"> static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,</span>
 	unsigned long excess;
 	unsigned long nr_scanned;
 	struct mem_cgroup_reclaim_cookie reclaim = {
<span class="p_del">-		.zone = zone,</span>
<span class="p_add">+		.pgdat = pgdat,</span>
 		.priority = 0,
 	};
 
<span class="p_chunk">@@ -1433,7 +1419,7 @@</span> <span class="p_context"> static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,</span>
 			continue;
 		}
 		total += mem_cgroup_shrink_node(victim, gfp_mask, false,
<span class="p_del">-					zone, &amp;nr_scanned);</span>
<span class="p_add">+					pgdat, &amp;nr_scanned);</span>
 		*total_scanned += nr_scanned;
 		if (!soft_limit_excess(root_memcg))
 			break;
<span class="p_chunk">@@ -2560,22 +2546,22 @@</span> <span class="p_context"> static int mem_cgroup_resize_memsw_limit(struct mem_cgroup *memcg,</span>
 	return ret;
 }
 
<span class="p_del">-unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
<span class="p_add">+unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,</span>
 					    gfp_t gfp_mask,
 					    unsigned long *total_scanned)
 {
 	unsigned long nr_reclaimed = 0;
<span class="p_del">-	struct mem_cgroup_per_zone *mz, *next_mz = NULL;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz, *next_mz = NULL;</span>
 	unsigned long reclaimed;
 	int loop = 0;
<span class="p_del">-	struct mem_cgroup_tree_per_zone *mctz;</span>
<span class="p_add">+	struct mem_cgroup_tree_per_node *mctz;</span>
 	unsigned long excess;
 	unsigned long nr_scanned;
 
 	if (order &gt; 0)
 		return 0;
 
<span class="p_del">-	mctz = soft_limit_tree_node_zone(zone_to_nid(zone), zone_idx(zone));</span>
<span class="p_add">+	mctz = soft_limit_tree_node(pgdat-&gt;node_id);</span>
 	/*
 	 * This loop can run a while, specially if mem_cgroup&#39;s continuously
 	 * keep exceeding their soft limit and putting the system under
<span class="p_chunk">@@ -2590,7 +2576,7 @@</span> <span class="p_context"> unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
 			break;
 
 		nr_scanned = 0;
<span class="p_del">-		reclaimed = mem_cgroup_soft_reclaim(mz-&gt;memcg, zone,</span>
<span class="p_add">+		reclaimed = mem_cgroup_soft_reclaim(mz-&gt;memcg, pgdat,</span>
 						    gfp_mask, &amp;nr_scanned);
 		nr_reclaimed += reclaimed;
 		*total_scanned += nr_scanned;
<span class="p_chunk">@@ -3211,22 +3197,21 @@</span> <span class="p_context"> static int memcg_stat_show(struct seq_file *m, void *v)</span>
 
 #ifdef CONFIG_DEBUG_VM
 	{
<span class="p_del">-		int nid, zid;</span>
<span class="p_del">-		struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+		pg_data_t *pgdat;</span>
<span class="p_add">+		struct mem_cgroup_per_node *mz;</span>
 		struct zone_reclaim_stat *rstat;
 		unsigned long recent_rotated[2] = {0, 0};
 		unsigned long recent_scanned[2] = {0, 0};
 
<span class="p_del">-		for_each_online_node(nid)</span>
<span class="p_del">-			for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="p_del">-				mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_del">-				rstat = &amp;mz-&gt;lruvec.reclaim_stat;</span>
<span class="p_add">+		for_each_online_pgdat(pgdat) {</span>
<span class="p_add">+			mz = mem_cgroup_nodeinfo(memcg, pgdat-&gt;node_id);</span>
<span class="p_add">+			rstat = &amp;mz-&gt;lruvec.reclaim_stat;</span>
 
<span class="p_del">-				recent_rotated[0] += rstat-&gt;recent_rotated[0];</span>
<span class="p_del">-				recent_rotated[1] += rstat-&gt;recent_rotated[1];</span>
<span class="p_del">-				recent_scanned[0] += rstat-&gt;recent_scanned[0];</span>
<span class="p_del">-				recent_scanned[1] += rstat-&gt;recent_scanned[1];</span>
<span class="p_del">-			}</span>
<span class="p_add">+			recent_rotated[0] += rstat-&gt;recent_rotated[0];</span>
<span class="p_add">+			recent_rotated[1] += rstat-&gt;recent_rotated[1];</span>
<span class="p_add">+			recent_scanned[0] += rstat-&gt;recent_scanned[0];</span>
<span class="p_add">+			recent_scanned[1] += rstat-&gt;recent_scanned[1];</span>
<span class="p_add">+		}</span>
 		seq_printf(m, &quot;recent_rotated_anon %lu\n&quot;, recent_rotated[0]);
 		seq_printf(m, &quot;recent_rotated_file %lu\n&quot;, recent_rotated[1]);
 		seq_printf(m, &quot;recent_scanned_anon %lu\n&quot;, recent_scanned[0]);
<span class="p_chunk">@@ -4106,11 +4091,10 @@</span> <span class="p_context"> struct mem_cgroup *mem_cgroup_from_id(unsigned short id)</span>
 	return idr_find(&amp;mem_cgroup_idr, id);
 }
 
<span class="p_del">-static int alloc_mem_cgroup_per_zone_info(struct mem_cgroup *memcg, int node)</span>
<span class="p_add">+static int alloc_mem_cgroup_per_node_info(struct mem_cgroup *memcg, int node)</span>
 {
 	struct mem_cgroup_per_node *pn;
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_del">-	int zone, tmp = node;</span>
<span class="p_add">+	int tmp = node;</span>
 	/*
 	 * This routine is called against possible nodes.
 	 * But it&#39;s BUG to call kmalloc() against offline node.
<span class="p_chunk">@@ -4125,18 +4109,16 @@</span> <span class="p_context"> static int alloc_mem_cgroup_per_zone_info(struct mem_cgroup *memcg, int node)</span>
 	if (!pn)
 		return 1;
 
<span class="p_del">-	for (zone = 0; zone &lt; MAX_NR_ZONES; zone++) {</span>
<span class="p_del">-		mz = &amp;pn-&gt;zoneinfo[zone];</span>
<span class="p_del">-		lruvec_init(&amp;mz-&gt;lruvec);</span>
<span class="p_del">-		mz-&gt;usage_in_excess = 0;</span>
<span class="p_del">-		mz-&gt;on_tree = false;</span>
<span class="p_del">-		mz-&gt;memcg = memcg;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	lruvec_init(&amp;pn-&gt;lruvec);</span>
<span class="p_add">+	pn-&gt;usage_in_excess = 0;</span>
<span class="p_add">+	pn-&gt;on_tree = false;</span>
<span class="p_add">+	pn-&gt;memcg = memcg;</span>
<span class="p_add">+</span>
 	memcg-&gt;nodeinfo[node] = pn;
 	return 0;
 }
 
<span class="p_del">-static void free_mem_cgroup_per_zone_info(struct mem_cgroup *memcg, int node)</span>
<span class="p_add">+static void free_mem_cgroup_per_node_info(struct mem_cgroup *memcg, int node)</span>
 {
 	kfree(memcg-&gt;nodeinfo[node]);
 }
<span class="p_chunk">@@ -4147,7 +4129,7 @@</span> <span class="p_context"> static void mem_cgroup_free(struct mem_cgroup *memcg)</span>
 
 	memcg_wb_domain_exit(memcg);
 	for_each_node(node)
<span class="p_del">-		free_mem_cgroup_per_zone_info(memcg, node);</span>
<span class="p_add">+		free_mem_cgroup_per_node_info(memcg, node);</span>
 	free_percpu(memcg-&gt;stat);
 	kfree(memcg);
 }
<span class="p_chunk">@@ -4176,7 +4158,7 @@</span> <span class="p_context"> static struct mem_cgroup *mem_cgroup_alloc(void)</span>
 		goto fail;
 
 	for_each_node(node)
<span class="p_del">-		if (alloc_mem_cgroup_per_zone_info(memcg, node))</span>
<span class="p_add">+		if (alloc_mem_cgroup_per_node_info(memcg, node))</span>
 			goto fail;
 
 	if (memcg_wb_domain_init(memcg, GFP_KERNEL))
<span class="p_chunk">@@ -5779,18 +5761,12 @@</span> <span class="p_context"> static int __init mem_cgroup_init(void)</span>
 
 	for_each_node(node) {
 		struct mem_cgroup_tree_per_node *rtpn;
<span class="p_del">-		int zone;</span>
 
 		rtpn = kzalloc_node(sizeof(*rtpn), GFP_KERNEL,
 				    node_online(node) ? node : NUMA_NO_NODE);
 
<span class="p_del">-		for (zone = 0; zone &lt; MAX_NR_ZONES; zone++) {</span>
<span class="p_del">-			struct mem_cgroup_tree_per_zone *rtpz;</span>
<span class="p_del">-</span>
<span class="p_del">-			rtpz = &amp;rtpn-&gt;rb_tree_per_zone[zone];</span>
<span class="p_del">-			rtpz-&gt;rb_root = RB_ROOT;</span>
<span class="p_del">-			spin_lock_init(&amp;rtpz-&gt;lock);</span>
<span class="p_del">-		}</span>
<span class="p_add">+		rtpn-&gt;rb_root = RB_ROOT;</span>
<span class="p_add">+		spin_lock_init(&amp;rtpn-&gt;lock);</span>
 		soft_limit_tree.rb_tree_per_node[node] = rtpn;
 	}
 
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index b8e0f76b6e00..82b59b63b481 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -2229,8 +2229,7 @@</span> <span class="p_context"> static inline void init_tlb_ubc(void)</span>
 static void shrink_node_memcg(struct pglist_data *pgdat, struct mem_cgroup *memcg,
 			      struct scan_control *sc, unsigned long *lru_pages)
 {
<span class="p_del">-	struct zone *zone = &amp;pgdat-&gt;node_zones[sc-&gt;reclaim_idx];</span>
<span class="p_del">-	struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, zone, memcg);</span>
<span class="p_add">+	struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, memcg);</span>
 	unsigned long nr[NR_LRU_LISTS];
 	unsigned long targets[NR_LRU_LISTS];
 	unsigned long nr_to_scan;
<span class="p_chunk">@@ -2437,7 +2436,7 @@</span> <span class="p_context"> static bool shrink_node(pg_data_t *pgdat, struct scan_control *sc,</span>
 	do {
 		struct mem_cgroup *root = sc-&gt;target_mem_cgroup;
 		struct mem_cgroup_reclaim_cookie reclaim = {
<span class="p_del">-			.zone = &amp;pgdat-&gt;node_zones[classzone_idx],</span>
<span class="p_add">+			.pgdat = pgdat,</span>
 			.priority = sc-&gt;priority,
 		};
 		unsigned long node_lru_pages = 0;
<span class="p_chunk">@@ -2646,7 +2645,7 @@</span> <span class="p_context"> static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc)</span>
 			 * and balancing, not for a memcg&#39;s limit.
 			 */
 			nr_soft_scanned = 0;
<span class="p_del">-			nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone,</span>
<span class="p_add">+			nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone-&gt;zone_pgdat,</span>
 						sc-&gt;order, sc-&gt;gfp_mask,
 						&amp;nr_soft_scanned);
 			sc-&gt;nr_reclaimed += nr_soft_reclaimed;
<span class="p_chunk">@@ -2912,7 +2911,7 @@</span> <span class="p_context"> unsigned long try_to_free_pages(struct zonelist *zonelist, int order,</span>
 
 unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,
 						gfp_t gfp_mask, bool noswap,
<span class="p_del">-						struct zone *zone,</span>
<span class="p_add">+						pg_data_t *pgdat,</span>
 						unsigned long *nr_scanned)
 {
 	struct scan_control sc = {
<span class="p_chunk">@@ -2939,7 +2938,7 @@</span> <span class="p_context"> unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,</span>
 	 * will pick up pages from other mem cgroup&#39;s as well. We hack
 	 * the priority and make it zero.
 	 */
<span class="p_del">-	shrink_node_memcg(zone-&gt;zone_pgdat, memcg, &amp;sc, &amp;lru_pages);</span>
<span class="p_add">+	shrink_node_memcg(pgdat, memcg, &amp;sc, &amp;lru_pages);</span>
 
 	trace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);
 
<span class="p_chunk">@@ -2989,7 +2988,7 @@</span> <span class="p_context"> unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,</span>
 #endif
 
 static void age_active_anon(struct pglist_data *pgdat,
<span class="p_del">-				struct zone *zone, struct scan_control *sc)</span>
<span class="p_add">+				struct scan_control *sc)</span>
 {
 	struct mem_cgroup *memcg;
 
<span class="p_chunk">@@ -2998,7 +2997,7 @@</span> <span class="p_context"> static void age_active_anon(struct pglist_data *pgdat,</span>
 
 	memcg = mem_cgroup_iter(NULL, NULL, NULL);
 	do {
<span class="p_del">-		struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, zone, memcg);</span>
<span class="p_add">+		struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, memcg);</span>
 
 		if (inactive_list_is_low(lruvec, false))
 			shrink_active_list(SWAP_CLUSTER_MAX, lruvec,
<span class="p_chunk">@@ -3185,7 +3184,7 @@</span> <span class="p_context"> static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)</span>
 		 * pages are rotated regardless of classzone as this is
 		 * about consistent aging.
 		 */
<span class="p_del">-		age_active_anon(pgdat, &amp;pgdat-&gt;node_zones[MAX_NR_ZONES - 1], &amp;sc);</span>
<span class="p_add">+		age_active_anon(pgdat, &amp;sc);</span>
 
 		/*
 		 * If we&#39;re getting trouble reclaiming, start doing writepage
<span class="p_chunk">@@ -3197,7 +3196,7 @@</span> <span class="p_context"> static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)</span>
 		/* Call soft limit reclaim before calling shrink_node. */
 		sc.nr_scanned = 0;
 		nr_soft_scanned = 0;
<span class="p_del">-		nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone, sc.order,</span>
<span class="p_add">+		nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(pgdat, sc.order,</span>
 						sc.gfp_mask, &amp;nr_soft_scanned);
 		sc.nr_reclaimed += nr_soft_reclaimed;
 
<span class="p_header">diff --git a/mm/workingset.c b/mm/workingset.c</span>
<span class="p_header">index de68ad681585..9a1016f5d500 100644</span>
<span class="p_header">--- a/mm/workingset.c</span>
<span class="p_header">+++ b/mm/workingset.c</span>
<span class="p_chunk">@@ -218,7 +218,7 @@</span> <span class="p_context"> void *workingset_eviction(struct address_space *mapping, struct page *page)</span>
 	VM_BUG_ON_PAGE(page_count(page), page);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 
<span class="p_del">-	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, zone, memcg);</span>
<span class="p_add">+	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, memcg);</span>
 	eviction = atomic_long_inc_return(&amp;lruvec-&gt;inactive_age);
 	return pack_shadow(memcgid, zone, eviction);
 }
<span class="p_chunk">@@ -267,7 +267,7 @@</span> <span class="p_context"> bool workingset_refault(void *shadow)</span>
 		rcu_read_unlock();
 		return false;
 	}
<span class="p_del">-	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, zone, memcg);</span>
<span class="p_add">+	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, memcg);</span>
 	refault = atomic_long_read(&amp;lruvec-&gt;inactive_age);
 	active_file = lruvec_lru_size(lruvec, LRU_ACTIVE_FILE);
 	rcu_read_unlock();
<span class="p_chunk">@@ -319,7 +319,7 @@</span> <span class="p_context"> void workingset_activation(struct page *page)</span>
 	memcg = page_memcg_rcu(page);
 	if (!mem_cgroup_disabled() &amp;&amp; !memcg)
 		goto out;
<span class="p_del">-	lruvec = mem_cgroup_lruvec(page_pgdat(page), page_zone(page), memcg);</span>
<span class="p_add">+	lruvec = mem_cgroup_lruvec(page_pgdat(page), memcg);</span>
 	atomic_long_inc(&amp;lruvec-&gt;inactive_age);
 out:
 	rcu_read_unlock();

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



