
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[05/16] mm: Protect VMA modifications using VMA sequence count - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [05/16] mm: Protect VMA modifications using VMA sequence count</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 8, 2017, 2:35 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1502202949-8138-6-git-send-email-ldufour@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9888445/mbox/"
   >mbox</a>
|
   <a href="/patch/9888445/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9888445/">/patch/9888445/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	16F6360363 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  8 Aug 2017 14:39:31 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 08D0F28769
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  8 Aug 2017 14:39:31 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id F1255288A9; Tue,  8 Aug 2017 14:39:30 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 50E5428769
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  8 Aug 2017 14:39:30 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752693AbdHHOj1 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 8 Aug 2017 10:39:27 -0400
Received: from mx0b-001b2d01.pphosted.com ([148.163.158.5]:43032 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S1752010AbdHHOgK (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 8 Aug 2017 10:36:10 -0400
Received: from pps.filterd (m0098421.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.21/8.16.0.21) with SMTP id
	v78EXaVV075841
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 8 Aug 2017 10:36:10 -0400
Received: from e06smtp15.uk.ibm.com (e06smtp15.uk.ibm.com [195.75.94.111])
	by mx0a-001b2d01.pphosted.com with ESMTP id 2c7evh1sdq-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 08 Aug 2017 10:36:10 -0400
Received: from localhost
	by e06smtp15.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from &lt;ldufour@linux.vnet.ibm.com&gt;; 
	Tue, 8 Aug 2017 15:36:07 +0100
Received: from b06cxnps3074.portsmouth.uk.ibm.com (9.149.109.194)
	by e06smtp15.uk.ibm.com (192.168.101.145) with IBM ESMTP SMTP
	Gateway: Authorized Use Only! Violators will be prosecuted; 
	Tue, 8 Aug 2017 15:36:00 +0100
Received: from d06av22.portsmouth.uk.ibm.com (d06av22.portsmouth.uk.ibm.com
	[9.149.105.58])
	by b06cxnps3074.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id v78Ea0cU18808984; Tue, 8 Aug 2017 14:36:00 GMT
Received: from d06av22.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 52ADB4C04E;
	Tue,  8 Aug 2017 15:33:20 +0100 (BST)
Received: from d06av22.portsmouth.uk.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 6452A4C04A;
	Tue,  8 Aug 2017 15:33:19 +0100 (BST)
Received: from nimbus.lab.toulouse-stg.fr.ibm.com (unknown [9.101.4.33])
	by d06av22.portsmouth.uk.ibm.com (Postfix) with ESMTP;
	Tue,  8 Aug 2017 15:33:19 +0100 (BST)
From: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;
To: paulmck@linux.vnet.ibm.com, peterz@infradead.org,
	akpm@linux-foundation.org, kirill@shutemov.name,
	ak@linux.intel.com, mhocko@kernel.org, dave@stgolabs.net,
	jack@suse.cz, Matthew Wilcox &lt;willy@infradead.org&gt;,
	benh@kernel.crashing.org, mpe@ellerman.id.au, paulus@samba.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, hpa@zytor.com,
	Will Deacon &lt;will.deacon@arm.com&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	haren@linux.vnet.ibm.com, khandual@linux.vnet.ibm.com,
	npiggin@gmail.com, bsingharora@gmail.com,
	Tim Chen &lt;tim.c.chen@linux.intel.com&gt;,
	linuxppc-dev@lists.ozlabs.org, x86@kernel.org
Subject: [PATCH 05/16] mm: Protect VMA modifications using VMA sequence count
Date: Tue,  8 Aug 2017 16:35:38 +0200
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1502202949-8138-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
References: &lt;1502202949-8138-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-TM-AS-GCONF: 00
x-cbid: 17080814-0020-0000-0000-000003AC4BAF
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17080814-0021-0000-0000-00004239848B
Message-Id: &lt;1502202949-8138-6-git-send-email-ldufour@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-08-08_07:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=2
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1706020000
	definitions=main-1708080235
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Aug. 8, 2017, 2:35 p.m.</div>
<pre class="content">
The VMA sequence count has been introduced to allow fast detection of
VMA modification when running a page fault handler without holding
the mmap_sem.

This patch provides protection agains the VMA modification done in :
	- madvise()
	- mremap()
	- mpol_rebind_policy()
	- vma_replace_policy()
	- change_prot_numa()
	- mlock(), munlock()
	- mprotect()
	- mmap_region()
	- collapse_huge_page()
<span class="signed-off-by">
Signed-off-by: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;</span>
---
 fs/proc/task_mmu.c |  2 ++
 mm/khugepaged.c    |  3 +++
 mm/madvise.c       |  4 ++++
 mm/mempolicy.c     | 10 +++++++++-
 mm/mlock.c         |  9 ++++++---
 mm/mmap.c          |  2 ++
 mm/mprotect.c      |  2 ++
 mm/mremap.c        |  7 +++++++
 8 files changed, 35 insertions(+), 4 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Aug. 9, 2017, 10:12 a.m.</div>
<pre class="content">
On Tue, Aug 08, 2017 at 04:35:38PM +0200, Laurent Dufour wrote:
<span class="quote">&gt; The VMA sequence count has been introduced to allow fast detection of</span>
<span class="quote">&gt; VMA modification when running a page fault handler without holding</span>
<span class="quote">&gt; the mmap_sem.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch provides protection agains the VMA modification done in :</span>
<span class="quote">&gt; 	- madvise()</span>
<span class="quote">&gt; 	- mremap()</span>
<span class="quote">&gt; 	- mpol_rebind_policy()</span>
<span class="quote">&gt; 	- vma_replace_policy()</span>
<span class="quote">&gt; 	- change_prot_numa()</span>
<span class="quote">&gt; 	- mlock(), munlock()</span>
<span class="quote">&gt; 	- mprotect()</span>
<span class="quote">&gt; 	- mmap_region()</span>
<span class="quote">&gt; 	- collapse_huge_page()</span>

I don&#39;t thinks it&#39;s anywhere near complete list of places where we touch
vm_flags. What is your plan for the rest?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Aug. 9, 2017, 10:43 a.m.</div>
<pre class="content">
On 09/08/2017 12:12, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Tue, Aug 08, 2017 at 04:35:38PM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt; The VMA sequence count has been introduced to allow fast detection of</span>
<span class="quote">&gt;&gt; VMA modification when running a page fault handler without holding</span>
<span class="quote">&gt;&gt; the mmap_sem.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch provides protection agains the VMA modification done in :</span>
<span class="quote">&gt;&gt; 	- madvise()</span>
<span class="quote">&gt;&gt; 	- mremap()</span>
<span class="quote">&gt;&gt; 	- mpol_rebind_policy()</span>
<span class="quote">&gt;&gt; 	- vma_replace_policy()</span>
<span class="quote">&gt;&gt; 	- change_prot_numa()</span>
<span class="quote">&gt;&gt; 	- mlock(), munlock()</span>
<span class="quote">&gt;&gt; 	- mprotect()</span>
<span class="quote">&gt;&gt; 	- mmap_region()</span>
<span class="quote">&gt;&gt; 	- collapse_huge_page()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t thinks it&#39;s anywhere near complete list of places where we touch</span>
<span class="quote">&gt; vm_flags. What is your plan for the rest?</span>

The goal is only to protect places where change to the VMA is impacting the
page fault handling. If you think I missed one, please advise.

Thanks,
Laurent.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Aug. 10, 2017, 12:58 a.m.</div>
<pre class="content">
On Wed, Aug 09, 2017 at 12:43:33PM +0200, Laurent Dufour wrote:
<span class="quote">&gt; On 09/08/2017 12:12, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Tue, Aug 08, 2017 at 04:35:38PM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt; &gt;&gt; The VMA sequence count has been introduced to allow fast detection of</span>
<span class="quote">&gt; &gt;&gt; VMA modification when running a page fault handler without holding</span>
<span class="quote">&gt; &gt;&gt; the mmap_sem.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; This patch provides protection agains the VMA modification done in :</span>
<span class="quote">&gt; &gt;&gt; 	- madvise()</span>
<span class="quote">&gt; &gt;&gt; 	- mremap()</span>
<span class="quote">&gt; &gt;&gt; 	- mpol_rebind_policy()</span>
<span class="quote">&gt; &gt;&gt; 	- vma_replace_policy()</span>
<span class="quote">&gt; &gt;&gt; 	- change_prot_numa()</span>
<span class="quote">&gt; &gt;&gt; 	- mlock(), munlock()</span>
<span class="quote">&gt; &gt;&gt; 	- mprotect()</span>
<span class="quote">&gt; &gt;&gt; 	- mmap_region()</span>
<span class="quote">&gt; &gt;&gt; 	- collapse_huge_page()</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I don&#39;t thinks it&#39;s anywhere near complete list of places where we touch</span>
<span class="quote">&gt; &gt; vm_flags. What is your plan for the rest?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The goal is only to protect places where change to the VMA is impacting the</span>
<span class="quote">&gt; page fault handling. If you think I missed one, please advise.</span>

That&#39;s very fragile approach. We rely here too much on specific compiler behaviour.

Any write access to vm_flags can, in theory, be translated to several
write accesses. For instance with setting vm_flags to 0 in the middle,
which would result in sigfault on page fault to the vma.

Nothing (apart from common sense) prevents compiler from generating this
kind of pattern.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Aug. 10, 2017, 8:27 a.m.</div>
<pre class="content">
On 10/08/2017 02:58, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Wed, Aug 09, 2017 at 12:43:33PM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt; On 09/08/2017 12:12, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt;&gt; On Tue, Aug 08, 2017 at 04:35:38PM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; The VMA sequence count has been introduced to allow fast detection of</span>
<span class="quote">&gt;&gt;&gt;&gt; VMA modification when running a page fault handler without holding</span>
<span class="quote">&gt;&gt;&gt;&gt; the mmap_sem.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; This patch provides protection agains the VMA modification done in :</span>
<span class="quote">&gt;&gt;&gt;&gt; 	- madvise()</span>
<span class="quote">&gt;&gt;&gt;&gt; 	- mremap()</span>
<span class="quote">&gt;&gt;&gt;&gt; 	- mpol_rebind_policy()</span>
<span class="quote">&gt;&gt;&gt;&gt; 	- vma_replace_policy()</span>
<span class="quote">&gt;&gt;&gt;&gt; 	- change_prot_numa()</span>
<span class="quote">&gt;&gt;&gt;&gt; 	- mlock(), munlock()</span>
<span class="quote">&gt;&gt;&gt;&gt; 	- mprotect()</span>
<span class="quote">&gt;&gt;&gt;&gt; 	- mmap_region()</span>
<span class="quote">&gt;&gt;&gt;&gt; 	- collapse_huge_page()</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I don&#39;t thinks it&#39;s anywhere near complete list of places where we touch</span>
<span class="quote">&gt;&gt;&gt; vm_flags. What is your plan for the rest?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The goal is only to protect places where change to the VMA is impacting the</span>
<span class="quote">&gt;&gt; page fault handling. If you think I missed one, please advise.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s very fragile approach. We rely here too much on specific compiler behaviour.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Any write access to vm_flags can, in theory, be translated to several</span>
<span class="quote">&gt; write accesses. For instance with setting vm_flags to 0 in the middle,</span>
<span class="quote">&gt; which would result in sigfault on page fault to the vma.</span>

Indeed, just setting vm_flags to 0 will not result in sigfault, the real
job is done when the pte are updated and the bits allowing access are
cleared. Access to the pte is controlled by the pte lock.
Page fault handler is triggered based on the pte bits, not the content of
vm_flags and the speculative page fault is checking for the vma again once
the pte lock is held. So there is no concurrency when dealing with the pte
bits.

Regarding the compiler behaviour, there are memory barriers and locking
which should prevent that.

Thanks,
Laurent.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Aug. 10, 2017, 1:43 p.m.</div>
<pre class="content">
On Thu, Aug 10, 2017 at 10:27:50AM +0200, Laurent Dufour wrote:
<span class="quote">&gt; On 10/08/2017 02:58, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On Wed, Aug 09, 2017 at 12:43:33PM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt; &gt;&gt; On 09/08/2017 12:12, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt; On Tue, Aug 08, 2017 at 04:35:38PM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; The VMA sequence count has been introduced to allow fast detection of</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; VMA modification when running a page fault handler without holding</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; the mmap_sem.</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; This patch provides protection agains the VMA modification done in :</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; 	- madvise()</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; 	- mremap()</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; 	- mpol_rebind_policy()</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; 	- vma_replace_policy()</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; 	- change_prot_numa()</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; 	- mlock(), munlock()</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; 	- mprotect()</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; 	- mmap_region()</span>
<span class="quote">&gt; &gt;&gt;&gt;&gt; 	- collapse_huge_page()</span>
<span class="quote">&gt; &gt;&gt;&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; I don&#39;t thinks it&#39;s anywhere near complete list of places where we touch</span>
<span class="quote">&gt; &gt;&gt;&gt; vm_flags. What is your plan for the rest?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; The goal is only to protect places where change to the VMA is impacting the</span>
<span class="quote">&gt; &gt;&gt; page fault handling. If you think I missed one, please advise.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That&#39;s very fragile approach. We rely here too much on specific compiler behaviour.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Any write access to vm_flags can, in theory, be translated to several</span>
<span class="quote">&gt; &gt; write accesses. For instance with setting vm_flags to 0 in the middle,</span>
<span class="quote">&gt; &gt; which would result in sigfault on page fault to the vma.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Indeed, just setting vm_flags to 0 will not result in sigfault, the real</span>
<span class="quote">&gt; job is done when the pte are updated and the bits allowing access are</span>
<span class="quote">&gt; cleared. Access to the pte is controlled by the pte lock.</span>
<span class="quote">&gt; Page fault handler is triggered based on the pte bits, not the content of</span>
<span class="quote">&gt; vm_flags and the speculative page fault is checking for the vma again once</span>
<span class="quote">&gt; the pte lock is held. So there is no concurrency when dealing with the pte</span>
<span class="quote">&gt; bits.</span>

Suppose we are getting page fault to readable VMA, pte is clear at the
time of page fault. In this case we need to consult vm_flags to check if
the vma is read-accessible.

If by the time of check vm_flags happend to be &#39;0&#39; we would get SIGSEGV as
the vma appears to be non-readable.

Where is my logic faulty?
<span class="quote">
&gt; Regarding the compiler behaviour, there are memory barriers and locking</span>
<span class="quote">&gt; which should prevent that.</span>

Which locks barriers are you talking about?

We need at least READ_ONCE/WRITE_ONCE to access vm_flags everywhere.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Aug. 10, 2017, 6:16 p.m.</div>
<pre class="content">
On 10/08/2017 15:43, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Thu, Aug 10, 2017 at 10:27:50AM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt; On 10/08/2017 02:58, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt;&gt; On Wed, Aug 09, 2017 at 12:43:33PM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On 09/08/2017 12:12, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On Tue, Aug 08, 2017 at 04:35:38PM +0200, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; The VMA sequence count has been introduced to allow fast detection of</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; VMA modification when running a page fault handler without holding</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; the mmap_sem.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; This patch provides protection agains the VMA modification done in :</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; 	- madvise()</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; 	- mremap()</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; 	- mpol_rebind_policy()</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; 	- vma_replace_policy()</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; 	- change_prot_numa()</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; 	- mlock(), munlock()</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; 	- mprotect()</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; 	- mmap_region()</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; 	- collapse_huge_page()</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; I don&#39;t thinks it&#39;s anywhere near complete list of places where we touch</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; vm_flags. What is your plan for the rest?</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; The goal is only to protect places where change to the VMA is impacting the</span>
<span class="quote">&gt;&gt;&gt;&gt; page fault handling. If you think I missed one, please advise.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; That&#39;s very fragile approach. We rely here too much on specific compiler behaviour.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Any write access to vm_flags can, in theory, be translated to several</span>
<span class="quote">&gt;&gt;&gt; write accesses. For instance with setting vm_flags to 0 in the middle,</span>
<span class="quote">&gt;&gt;&gt; which would result in sigfault on page fault to the vma.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Indeed, just setting vm_flags to 0 will not result in sigfault, the real</span>
<span class="quote">&gt;&gt; job is done when the pte are updated and the bits allowing access are</span>
<span class="quote">&gt;&gt; cleared. Access to the pte is controlled by the pte lock.</span>
<span class="quote">&gt;&gt; Page fault handler is triggered based on the pte bits, not the content of</span>
<span class="quote">&gt;&gt; vm_flags and the speculative page fault is checking for the vma again once</span>
<span class="quote">&gt;&gt; the pte lock is held. So there is no concurrency when dealing with the pte</span>
<span class="quote">&gt;&gt; bits.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Suppose we are getting page fault to readable VMA, pte is clear at the</span>
<span class="quote">&gt; time of page fault. In this case we need to consult vm_flags to check if</span>
<span class="quote">&gt; the vma is read-accessible.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If by the time of check vm_flags happend to be &#39;0&#39; we would get SIGSEGV as</span>
<span class="quote">&gt; the vma appears to be non-readable.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Where is my logic faulty?</span>

The speculative page fault handler will not deliver the signal, if the page
fault can&#39;t be done in the speculative path for instance because the
vm_flags are not matching the required one, the speculative page fault is
aborted and the *classic* page fault handler is run which will do the job
again grabbing the mmap_sem.
<span class="quote">
&gt;&gt; Regarding the compiler behaviour, there are memory barriers and locking</span>
<span class="quote">&gt;&gt; which should prevent that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Which locks barriers are you talking about?</span>

When the VMA is modified and that the changes will impact the speculative
page fault handler the sequence count is touch using write_seqcount_begin()
and write_seqcount_end(). These 2 services contains calls to smp_wmb().
On the speculative path side, the calls to *_read_seqcount() contains also
memory barriers calls.
<span class="quote">
&gt; We need at least READ_ONCE/WRITE_ONCE to access vm_flags everywhere.</span>

I don&#39;t think READ_ONCE/WRITE_ONCE would help here, as they would not
prevent reading transcient state as the vm_flags example you mentioned.

That said, there are not so much VMA&#39;s fields used in the SPF&#39;s path and
caching them into the vmf structure under the control of the VMA&#39;s sequence
count would solve this.
I&#39;ll try to move in that direction unless anyone has a better idea.


Cheers,
Laurent.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index b836fd61ed87..5c0c3ab10f3c 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -1064,8 +1064,10 @@</span> <span class="p_context"> static ssize_t clear_refs_write(struct file *file, const char __user *buf,</span>
 					goto out_mm;
 				}
 				for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {
<span class="p_add">+					write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 					vma-&gt;vm_flags &amp;= ~VM_SOFTDIRTY;
 					vma_set_page_prot(vma);
<span class="p_add">+					write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 				}
 				downgrade_write(&amp;mm-&gt;mmap_sem);
 				break;
<span class="p_header">diff --git a/mm/khugepaged.c b/mm/khugepaged.c</span>
<span class="p_header">index c01f177a1120..56dd994c05d0 100644</span>
<span class="p_header">--- a/mm/khugepaged.c</span>
<span class="p_header">+++ b/mm/khugepaged.c</span>
<span class="p_chunk">@@ -1005,6 +1005,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	if (mm_find_pmd(mm, address) != pmd)
 		goto out;
 
<span class="p_add">+	write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 	anon_vma_lock_write(vma-&gt;anon_vma);
 
 	pte = pte_offset_map(pmd, address);
<span class="p_chunk">@@ -1040,6 +1041,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 		pmd_populate(mm, pmd, pmd_pgtable(_pmd));
 		spin_unlock(pmd_ptl);
 		anon_vma_unlock_write(vma-&gt;anon_vma);
<span class="p_add">+		write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 		result = SCAN_FAIL;
 		goto out;
 	}
<span class="p_chunk">@@ -1074,6 +1076,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 	set_pmd_at(mm, address, pmd, _pmd);
 	update_mmu_cache_pmd(vma, address, pmd);
 	spin_unlock(pmd_ptl);
<span class="p_add">+	write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 
 	*hpage = NULL;
 
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 47d8d8a25eae..4f73ecaa0961 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -172,7 +172,9 @@</span> <span class="p_context"> static long madvise_behavior(struct vm_area_struct *vma,</span>
 	/*
 	 * vm_flags is protected by the mmap_sem held in write mode.
 	 */
<span class="p_add">+	write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 	vma-&gt;vm_flags = new_flags;
<span class="p_add">+	write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 out:
 	return error;
 }
<span class="p_chunk">@@ -440,9 +442,11 @@</span> <span class="p_context"> static void madvise_free_page_range(struct mmu_gather *tlb,</span>
 		.private = tlb,
 	};
 
<span class="p_add">+	write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 	tlb_start_vma(tlb, vma);
 	walk_page_range(addr, end, &amp;free_walk);
 	tlb_end_vma(tlb, vma);
<span class="p_add">+	write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 }
 
 static int madvise_free_single_vma(struct vm_area_struct *vma,
<span class="p_header">diff --git a/mm/mempolicy.c b/mm/mempolicy.c</span>
<span class="p_header">index d911fa5cb2a7..32ed50c0d4b2 100644</span>
<span class="p_header">--- a/mm/mempolicy.c</span>
<span class="p_header">+++ b/mm/mempolicy.c</span>
<span class="p_chunk">@@ -378,8 +378,11 @@</span> <span class="p_context"> void mpol_rebind_mm(struct mm_struct *mm, nodemask_t *new)</span>
 	struct vm_area_struct *vma;
 
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_del">-	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next)</span>
<span class="p_add">+	for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+		write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 		mpol_rebind_policy(vma-&gt;vm_policy, new);
<span class="p_add">+		write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
<span class="p_add">+	}</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 }
 
<span class="p_chunk">@@ -537,9 +540,11 @@</span> <span class="p_context"> unsigned long change_prot_numa(struct vm_area_struct *vma,</span>
 {
 	int nr_updated;
 
<span class="p_add">+	write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 	nr_updated = change_protection(vma, addr, end, PAGE_NONE, 0, 1);
 	if (nr_updated)
 		count_vm_numa_events(NUMA_PTE_UPDATES, nr_updated);
<span class="p_add">+	write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 
 	return nr_updated;
 }
<span class="p_chunk">@@ -640,6 +645,7 @@</span> <span class="p_context"> static int vma_replace_policy(struct vm_area_struct *vma,</span>
 	if (IS_ERR(new))
 		return PTR_ERR(new);
 
<span class="p_add">+	write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 	if (vma-&gt;vm_ops &amp;&amp; vma-&gt;vm_ops-&gt;set_policy) {
 		err = vma-&gt;vm_ops-&gt;set_policy(vma, new);
 		if (err)
<span class="p_chunk">@@ -648,10 +654,12 @@</span> <span class="p_context"> static int vma_replace_policy(struct vm_area_struct *vma,</span>
 
 	old = vma-&gt;vm_policy;
 	vma-&gt;vm_policy = new; /* protected by mmap_sem */
<span class="p_add">+	write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 	mpol_put(old);
 
 	return 0;
  err_out:
<span class="p_add">+	write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 	mpol_put(new);
 	return err;
 }
<span class="p_header">diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="p_header">index b562b5523a65..30d9bfc61929 100644</span>
<span class="p_header">--- a/mm/mlock.c</span>
<span class="p_header">+++ b/mm/mlock.c</span>
<span class="p_chunk">@@ -438,7 +438,9 @@</span> <span class="p_context"> static unsigned long __munlock_pagevec_fill(struct pagevec *pvec,</span>
 void munlock_vma_pages_range(struct vm_area_struct *vma,
 			     unsigned long start, unsigned long end)
 {
<span class="p_add">+	write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 	vma-&gt;vm_flags &amp;= VM_LOCKED_CLEAR_MASK;
<span class="p_add">+	write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 
 	while (start &lt; end) {
 		struct page *page;
<span class="p_chunk">@@ -563,10 +565,11 @@</span> <span class="p_context"> static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
 	 * It&#39;s okay if try_to_unmap_one unmaps a page just after we
 	 * set VM_LOCKED, populate_vma_page_range will bring it back.
 	 */
<span class="p_del">-</span>
<span class="p_del">-	if (lock)</span>
<span class="p_add">+	if (lock) {</span>
<span class="p_add">+		write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 		vma-&gt;vm_flags = newflags;
<span class="p_del">-	else</span>
<span class="p_add">+		write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
<span class="p_add">+	} else</span>
 		munlock_vma_pages_range(vma, start, end);
 
 out:
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index 140b22136cb7..221b1f3e966a 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -1734,6 +1734,7 @@</span> <span class="p_context"> unsigned long mmap_region(struct file *file, unsigned long addr,</span>
 out:
 	perf_event_mmap(vma);
 
<span class="p_add">+	write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 	vm_stat_account(mm, vm_flags, len &gt;&gt; PAGE_SHIFT);
 	if (vm_flags &amp; VM_LOCKED) {
 		if (!((vm_flags &amp; VM_SPECIAL) || is_vm_hugetlb_page(vma) ||
<span class="p_chunk">@@ -1756,6 +1757,7 @@</span> <span class="p_context"> unsigned long mmap_region(struct file *file, unsigned long addr,</span>
 	vma-&gt;vm_flags |= VM_SOFTDIRTY;
 
 	vma_set_page_prot(vma);
<span class="p_add">+	write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 
 	return addr;
 
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index 4180ad8cc9c5..297f0f1e7560 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -344,6 +344,7 @@</span> <span class="p_context"> mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
 	 * vm_flags and vm_page_prot are protected by the mmap_sem
 	 * held in write mode.
 	 */
<span class="p_add">+	write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
 	vma-&gt;vm_flags = newflags;
 	dirty_accountable = vma_wants_writenotify(vma, vma-&gt;vm_page_prot);
 	vma_set_page_prot(vma);
<span class="p_chunk">@@ -359,6 +360,7 @@</span> <span class="p_context"> mprotect_fixup(struct vm_area_struct *vma, struct vm_area_struct **pprev,</span>
 			(newflags &amp; VM_WRITE)) {
 		populate_vma_page_range(vma, start, end, NULL);
 	}
<span class="p_add">+	write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 
 	vm_stat_account(mm, oldflags, -nrpages);
 	vm_stat_account(mm, newflags, nrpages);
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index 3f23715d3c69..1abadea8ab84 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -301,6 +301,10 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 	if (!new_vma)
 		return -ENOMEM;
 
<span class="p_add">+	write_seqcount_begin(&amp;vma-&gt;vm_sequence);</span>
<span class="p_add">+	write_seqcount_begin_nested(&amp;new_vma-&gt;vm_sequence,</span>
<span class="p_add">+				    SINGLE_DEPTH_NESTING);</span>
<span class="p_add">+</span>
 	moved_len = move_page_tables(vma, old_addr, new_vma, new_addr, old_len,
 				     need_rmap_locks);
 	if (moved_len &lt; old_len) {
<span class="p_chunk">@@ -317,6 +321,7 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 		 */
 		move_page_tables(new_vma, new_addr, vma, old_addr, moved_len,
 				 true);
<span class="p_add">+		write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 		vma = new_vma;
 		old_len = new_len;
 		old_addr = new_addr;
<span class="p_chunk">@@ -325,7 +330,9 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 		mremap_userfaultfd_prep(new_vma, uf);
 		arch_remap(mm, old_addr, old_addr + old_len,
 			   new_addr, new_addr + new_len);
<span class="p_add">+		write_seqcount_end(&amp;vma-&gt;vm_sequence);</span>
 	}
<span class="p_add">+	write_seqcount_end(&amp;new_vma-&gt;vm_sequence);</span>
 
 	/* Conceal VM_ACCOUNT so old reservation is not undone */
 	if (vm_flags &amp; VM_ACCOUNT) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



