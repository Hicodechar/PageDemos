
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,07/12] mm: thp: check pmd migration entry in common path - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,07/12] mm: thp: check pmd migration entry in common path</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 7, 2016, 11:31 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1478561517-4317-8-git-send-email-n-horiguchi@ah.jp.nec.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9416251/mbox/"
   >mbox</a>
|
   <a href="/patch/9416251/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9416251/">/patch/9416251/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	226686022E for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  7 Nov 2016 23:49:50 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1062528C01
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  7 Nov 2016 23:49:50 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 01DB028E98; Mon,  7 Nov 2016 23:49:49 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E2EA728C01
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  7 Nov 2016 23:49:48 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752220AbcKGXtl (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 7 Nov 2016 18:49:41 -0500
Received: from mail-pf0-f194.google.com ([209.85.192.194]:35624 &quot;EHLO
	mail-pf0-f194.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751359AbcKGXtj (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 7 Nov 2016 18:49:39 -0500
Received: by mail-pf0-f194.google.com with SMTP id i88so17349459pfk.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 07 Nov 2016 15:49:38 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=eHEcCY5fLlgSLZw7649CCg1wlG/6LfYfq1IDtLTa104=;
	b=qf+Da9COU0kM49+95W63xic+aojzwuIkjrDiyvAWqmJBN/b4Oh0KZW/MpnjDG3lA9k
	ke3VbuQV57n7lxBbdtFKbryLqBVxkBW5wU1gYOIRwZhprN2l9viSnJzvWdixK385Pt5K
	Y45oHR69NYqL2XgJKKNsN4phzgkpsZwR9U2i5zlpzXOhB2x+Ts8wuuPgYnck90PFtB1l
	9tlUzhMTG0W89bEs7OrCONDqr2R17A2FRLpQf4lGUmudj8MVaiLlBqO8zps2VwVA/4kZ
	iZyiVjtqTGleoOY652vh8c40juDK1wp7O3HgCx9xUozuPtfdpPB0zyGwWCp7G638w43N
	061Q==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:from:to:cc:subject:date:message-id
	:in-reply-to:references;
	bh=eHEcCY5fLlgSLZw7649CCg1wlG/6LfYfq1IDtLTa104=;
	b=nIo2r5WIam7BbGjIDusc/DPfZSNewV0MxDSn1HiLCt6rJWVG0EzdT139JK9GvM9Dy7
	jIpCTo9ceRs0WT7OFA+CMyE6IBfebipzTVo9kIrGGU0qLkg/EGvu80+UCpczrT7SOSt6
	J80a1QLftlm/CfclXrRQbQ4uileXvS3CaY27CODDXDYKYDrTIc7NI3MP6J8qnk/j2tpa
	L2esrfNnX/7+KR1j6q8JU+R2UfnDpoMt9+QWf9Y0BzRFaRHbkxaPTB2rWxK9tXiM7KhM
	fW0xo3FVhnw4+5zEtw0/qYDWk6s7k8uQgwvbrNeMF1hcqVxFTQmVfZo9OH7sZUZZc6FS
	X4ig==
X-Gm-Message-State: ABUngve/fQ31kzG1jpD5vCViNio99vdNv4RAPg/ZELsxdrpW0IzIJbDFXnQ40NLRaqSsJg==
X-Received: by 10.98.21.148 with SMTP id 142mr17697100pfv.38.1478561547354; 
	Mon, 07 Nov 2016 15:32:27 -0800 (PST)
Received: from www9186uo.sakura.ne.jp (www9186uo.sakura.ne.jp.
	[153.121.56.200]) by smtp.gmail.com with ESMTPSA id
	w125sm40952pfb.8.2016.11.07.15.32.23
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 07 Nov 2016 15:32:26 -0800 (PST)
From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;
To: linux-mm@kvack.org
Cc: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Hugh Dickins &lt;hughd@google.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Pavel Emelyanov &lt;xemul@parallels.com&gt;, Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;,
	Balbir Singh &lt;bsingharora@gmail.com&gt;, linux-kernel@vger.kernel.org,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Naoya Horiguchi &lt;nao.horiguchi@gmail.com&gt;
Subject: [PATCH v2 07/12] mm: thp: check pmd migration entry in common path
Date: Tue,  8 Nov 2016 08:31:52 +0900
Message-Id: &lt;1478561517-4317-8-git-send-email-n-horiguchi@ah.jp.nec.com&gt;
X-Mailer: git-send-email 2.7.0
In-Reply-To: &lt;1478561517-4317-1-git-send-email-n-horiguchi@ah.jp.nec.com&gt;
References: &lt;1478561517-4317-1-git-send-email-n-horiguchi@ah.jp.nec.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Nov. 7, 2016, 11:31 p.m.</div>
<pre class="content">
If one of callers of page migration starts to handle thp, memory management code
start to see pmd migration entry, so we need to prepare for it before enabling.
This patch changes various code point which checks the status of given pmds in
order to prevent race between thp migration and the pmd-related works.
<span class="signed-off-by">
Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
---
ChangeLog v1 -&gt; v2:
- introduce pmd_related() (I know the naming is not good, but can&#39;t think up
  no better name. Any suggesntion is welcomed.)
---
 arch/x86/mm/gup.c       |  4 +--
 fs/proc/task_mmu.c      | 23 +++++++------
 include/linux/huge_mm.h |  9 ++++-
 mm/gup.c                | 10 ++++--
 mm/huge_memory.c        | 88 ++++++++++++++++++++++++++++++++++++++++---------
 mm/madvise.c            |  2 +-
 mm/memcontrol.c         |  2 ++
 mm/memory.c             |  6 +++-
 mm/mprotect.c           |  2 ++
 mm/mremap.c             |  2 +-
 10 files changed, 114 insertions(+), 34 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - Nov. 8, 2016, 12:23 a.m.</div>
<pre class="content">
Hi Naoya,

[auto build test ERROR on mmotm/master]
[also build test ERROR on v4.9-rc4 next-20161028]
[if your patch is applied to the wrong git tree, please drop us a note to help improve the system]

url:    https://github.com/0day-ci/linux/commits/Naoya-Horiguchi/mm-x86-move-_PAGE_SWP_SOFT_DIRTY-from-bit-7-to-bit-6/20161108-080615
base:   git://git.cmpxchg.org/linux-mmotm.git master
config: i386-randconfig-x007-201645 (attached as .config)
compiler: gcc-6 (Debian 6.2.0-3) 6.2.0 20160901
reproduce:
        # save the attached .config to linux build tree
        make ARCH=i386 

All error/warnings (new ones prefixed by &gt;&gt;):

   mm/memory.c: In function &#39;copy_pmd_range&#39;:
<span class="quote">&gt;&gt; mm/memory.c:1002:7: error: implicit declaration of function &#39;pmd_related&#39; [-Werror=implicit-function-declaration]</span>
      if (pmd_related(*src_pmd)) {
          ^~~~~~~~~~~
   cc1: some warnings being treated as errors
--
   mm/mremap.c: In function &#39;move_page_tables&#39;:
<span class="quote">&gt;&gt; mm/mremap.c:197:7: error: implicit declaration of function &#39;pmd_related&#39; [-Werror=implicit-function-declaration]</span>
      if (pmd_related(*old_pmd)) {
          ^~~~~~~~~~~
   In file included from include/asm-generic/bug.h:4:0,
                    from arch/x86/include/asm/bug.h:35,
                    from include/linux/bug.h:4,
                    from include/linux/mmdebug.h:4,
                    from include/linux/mm.h:8,
                    from mm/mremap.c:10:
<span class="quote">&gt;&gt; include/linux/compiler.h:518:38: error: call to &#39;__compiletime_assert_198&#39; declared with attribute error: BUILD_BUG failed</span>
     _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
                                         ^
   include/linux/compiler.h:501:4: note: in definition of macro &#39;__compiletime_assert&#39;
       prefix ## suffix();    \
       ^~~~~~
   include/linux/compiler.h:518:2: note: in expansion of macro &#39;_compiletime_assert&#39;
     _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
     ^~~~~~~~~~~~~~~~~~~
   include/linux/bug.h:54:37: note: in expansion of macro &#39;compiletime_assert&#39;
    #define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)
                                        ^~~~~~~~~~~~~~~~~~
   include/linux/bug.h:88:21: note: in expansion of macro &#39;BUILD_BUG_ON_MSG&#39;
    #define BUILD_BUG() BUILD_BUG_ON_MSG(1, &quot;BUILD_BUG failed&quot;)
                        ^~~~~~~~~~~~~~~~
<span class="quote">&gt;&gt; include/linux/huge_mm.h:183:27: note: in expansion of macro &#39;BUILD_BUG&#39;</span>
    #define HPAGE_PMD_SIZE ({ BUILD_BUG(); 0; })
                              ^~~~~~~~~
<span class="quote">&gt;&gt; mm/mremap.c:198:18: note: in expansion of macro &#39;HPAGE_PMD_SIZE&#39;</span>
       if (extent == HPAGE_PMD_SIZE) {
                     ^~~~~~~~~~~~~~
   cc1: some warnings being treated as errors
--
   mm/madvise.c: In function &#39;madvise_free_pte_range&#39;:
<span class="quote">&gt;&gt; mm/madvise.c:277:6: error: implicit declaration of function &#39;pmd_related&#39; [-Werror=implicit-function-declaration]</span>
     if (pmd_related(*pmd))
         ^~~~~~~~~~~
   cc1: some warnings being treated as errors

vim +/pmd_related +1002 mm/memory.c

   996		dst_pmd = pmd_alloc(dst_mm, dst_pud, addr);
   997		if (!dst_pmd)
   998			return -ENOMEM;
   999		src_pmd = pmd_offset(src_pud, addr);
  1000		do {
  1001			next = pmd_addr_end(addr, end);
<span class="quote">&gt; 1002			if (pmd_related(*src_pmd)) {</span>
  1003				int err;
  1004				VM_BUG_ON(next-addr != HPAGE_PMD_SIZE);
  1005				err = copy_huge_pmd(dst_mm, src_mm,

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - Nov. 10, 2016, 8:36 a.m.</div>
<pre class="content">
On 11/08/2016 05:01 AM, Naoya Horiguchi wrote:
<span class="quote">&gt; If one of callers of page migration starts to handle thp, memory management code</span>
<span class="quote">&gt; start to see pmd migration entry, so we need to prepare for it before enabling.</span>
<span class="quote">&gt; This patch changes various code point which checks the status of given pmds in</span>
<span class="quote">&gt; order to prevent race between thp migration and the pmd-related works.</span>

There are lot of changes in this one patch. Should not we split
this up into multiple patches and explain them in a bit detail
through their commit messages ?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=113021">Hillf Danton</a> - Nov. 10, 2016, 9:08 a.m.</div>
<pre class="content">
On Tuesday, November 08, 2016 7:32 AM Naoya Horiguchi wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; @@ -1013,6 +1027,9 @@ int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd)</span>
<span class="quote">&gt;  	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; +		goto out_unlock;</span>
<span class="quote">&gt; +</span>

Can we encounter a migration entry after acquiring ptl ?
<span class="quote">
&gt;  	page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageCompound(page) || !PageHead(page), page);</span>
<span class="quote">&gt;  	/*</span>
[...]
<span class="quote">&gt; @@ -3591,6 +3591,10 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  		int ret;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		barrier();</span>
<span class="quote">&gt; +		if (unlikely(is_pmd_migration_entry(orig_pmd))) {</span>
<span class="quote">&gt; +			pmd_migration_entry_wait(mm, fe.pmd);</span>
<span class="quote">&gt; +			return 0;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {</span>
<span class="quote">&gt;  			if (pmd_protnone(orig_pmd) &amp;&amp; vma_is_accessible(vma))</span>
<span class="quote">&gt;  				return do_huge_pmd_numa_page(&amp;fe, orig_pmd);</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Nov. 10, 2016, 9:12 a.m.</div>
<pre class="content">
On Thu, Nov 10, 2016 at 02:06:14PM +0530, Anshuman Khandual wrote:
<span class="quote">&gt; On 11/08/2016 05:01 AM, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; If one of callers of page migration starts to handle thp, memory management code</span>
<span class="quote">&gt; &gt; start to see pmd migration entry, so we need to prepare for it before enabling.</span>
<span class="quote">&gt; &gt; This patch changes various code point which checks the status of given pmds in</span>
<span class="quote">&gt; &gt; order to prevent race between thp migration and the pmd-related works.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There are lot of changes in this one patch. Should not we split</span>
<span class="quote">&gt; this up into multiple patches and explain them in a bit detail</span>
<span class="quote">&gt; through their commit messages ?</span>

Yes, and I admit that I might change more than necessary, if the context
never encounters migration entry for any reason, no change is needed.
I&#39;ll dig more detail.

- Naoya Horiguchi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Nov. 10, 2016, 9:21 a.m.</div>
<pre class="content">
Hi Hillf,

On Thu, Nov 10, 2016 at 05:08:07PM +0800, Hillf Danton wrote:
<span class="quote">&gt; On Tuesday, November 08, 2016 7:32 AM Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; @@ -1013,6 +1027,9 @@ int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd)</span>
<span class="quote">&gt; &gt;  	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))</span>
<span class="quote">&gt; &gt;  		goto out_unlock;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; &gt; +		goto out_unlock;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can we encounter a migration entry after acquiring ptl ?</span>

I think we can. thp migration code releases ptl after converting pmd into
migration entry, so other code can see it even within ptl.

Thanks,
Naoya Horiguchi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=113021">Hillf Danton</a> - Nov. 10, 2016, 9:28 a.m.</div>
<pre class="content">
On Thursday, November 10, 2016 5:22 PM Naoya Horiguchi wrote:
<span class="quote">&gt; On Thu, Nov 10, 2016 at 05:08:07PM +0800, Hillf Danton wrote:</span>
<span class="quote">&gt; &gt; On Tuesday, November 08, 2016 7:32 AM Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; @@ -1013,6 +1027,9 @@ int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd)</span>
<span class="quote">&gt; &gt; &gt;  	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))</span>
<span class="quote">&gt; &gt; &gt;  		goto out_unlock;</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; &gt; &gt; +		goto out_unlock;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Can we encounter a migration entry after acquiring ptl ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think we can. thp migration code releases ptl after converting pmd into</span>
<span class="quote">&gt; migration entry, so other code can see it even within ptl.</span>
<span class="quote">&gt; </span>
But we have a pmd_same check there, you see.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Nov. 10, 2016, 9:34 a.m.</div>
<pre class="content">
On Thu, Nov 10, 2016 at 05:28:20PM +0800, Hillf Danton wrote:
<span class="quote">&gt; On Thursday, November 10, 2016 5:22 PM Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; On Thu, Nov 10, 2016 at 05:08:07PM +0800, Hillf Danton wrote:</span>
<span class="quote">&gt; &gt; &gt; On Tuesday, November 08, 2016 7:32 AM Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -1013,6 +1027,9 @@ int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd)</span>
<span class="quote">&gt; &gt; &gt; &gt;  	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))</span>
<span class="quote">&gt; &gt; &gt; &gt;  		goto out_unlock;</span>
<span class="quote">&gt; &gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; &gt; &gt; &gt; +		goto out_unlock;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Can we encounter a migration entry after acquiring ptl ?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think we can. thp migration code releases ptl after converting pmd into</span>
<span class="quote">&gt; &gt; migration entry, so other code can see it even within ptl.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; But we have a pmd_same check there, you see. </span>

You&#39;re right. So we can omit this pmd_present check.

Thanks,
Naoya Horiguchi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Nov. 17, 2016, 11:56 p.m.</div>
<pre class="content">
On Tue, Nov 08, 2016 at 08:31:52AM +0900, Naoya Horiguchi wrote:
<span class="quote">&gt; If one of callers of page migration starts to handle thp, memory management code</span>
<span class="quote">&gt; start to see pmd migration entry, so we need to prepare for it before enabling.</span>
<span class="quote">&gt; This patch changes various code point which checks the status of given pmds in</span>
<span class="quote">&gt; order to prevent race between thp migration and the pmd-related works.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt; - introduce pmd_related() (I know the naming is not good, but can&#39;t think up</span>
<span class="quote">&gt;   no better name. Any suggesntion is welcomed.)</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/mm/gup.c       |  4 +--</span>
<span class="quote">&gt;  fs/proc/task_mmu.c      | 23 +++++++------</span>
<span class="quote">&gt;  include/linux/huge_mm.h |  9 ++++-</span>
<span class="quote">&gt;  mm/gup.c                | 10 ++++--</span>
<span class="quote">&gt;  mm/huge_memory.c        | 88 ++++++++++++++++++++++++++++++++++++++++---------</span>
<span class="quote">&gt;  mm/madvise.c            |  2 +-</span>
<span class="quote">&gt;  mm/memcontrol.c         |  2 ++</span>
<span class="quote">&gt;  mm/memory.c             |  6 +++-</span>
<span class="quote">&gt;  mm/mprotect.c           |  2 ++</span>
<span class="quote">&gt;  mm/mremap.c             |  2 +-</span>
<span class="quote">&gt;  10 files changed, 114 insertions(+), 34 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/arch/x86/mm/gup.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/arch/x86/mm/gup.c</span>
<span class="quote">&gt; index 0d4fb3e..78a153d 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/arch/x86/mm/gup.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/arch/x86/mm/gup.c</span>
<span class="quote">&gt; @@ -222,9 +222,9 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		pmd_t pmd = *pmdp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (pmd_none(pmd))</span>
<span class="quote">&gt; +		if (!pmd_present(pmd))</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt; -		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="quote">&gt; +		if (unlikely(pmd_large(pmd))) {</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt;  			 * NUMA hinting faults need to be handled in the GUP</span>
<span class="quote">&gt;  			 * slowpath for accounting purposes and so that they</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/fs/proc/task_mmu.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/fs/proc/task_mmu.c</span>
<span class="quote">&gt; index 35b92d8..c1f9cf4 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/fs/proc/task_mmu.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/fs/proc/task_mmu.c</span>
<span class="quote">&gt; @@ -596,7 +596,8 @@ static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ptl = pmd_trans_huge_lock(pmd, vma);</span>
<span class="quote">&gt;  	if (ptl) {</span>
<span class="quote">&gt; -		smaps_pmd_entry(pmd, addr, walk);</span>
<span class="quote">&gt; +		if (pmd_present(*pmd))</span>
<span class="quote">&gt; +			smaps_pmd_entry(pmd, addr, walk);</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -929,6 +930,9 @@ static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt; +</span>

Hm. Looks like clear_soft_dirty_pmd() should handle !present. It doesn&#39;t.

Ah.. Found it in 08/12.
<span class="quote">
&gt;  		page = pmd_page(*pmd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* Clear accessed and referenced bits. */</span>
<span class="quote">&gt; @@ -1208,19 +1212,18 @@ static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  	if (ptl) {</span>
<span class="quote">&gt;  		u64 flags = 0, frame = 0;</span>
<span class="quote">&gt;  		pmd_t pmd = *pmdp;</span>
<span class="quote">&gt; +		struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(pmd))</span>
<span class="quote">&gt;  			flags |= PM_SOFT_DIRTY;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * Currently pmd for thp is always present because thp</span>
<span class="quote">&gt; -		 * can not be swapped-out, migrated, or HWPOISONed</span>
<span class="quote">&gt; -		 * (split in such cases instead.)</span>
<span class="quote">&gt; -		 * This if-check is just to prepare for future implementation.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		if (pmd_present(pmd)) {</span>
<span class="quote">&gt; -			struct page *page = pmd_page(pmd);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +		if (is_pmd_migration_entry(pmd)) {</span>
<span class="quote">&gt; +			swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; +			frame = swp_type(entry) |</span>
<span class="quote">&gt; +				(swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);</span>
<span class="quote">&gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; +		} else if (pmd_present(pmd)) {</span>
<span class="quote">&gt; +			page = pmd_page(pmd);</span>
<span class="quote">&gt;  			if (page_mapcount(page) == 1)</span>
<span class="quote">&gt;  				flags |= PM_MMAP_EXCLUSIVE;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/include/linux/huge_mm.h v4.9-rc2-mmotm-2016-10-27-18-27_patched/include/linux/huge_mm.h</span>
<span class="quote">&gt; index fcbca51..3c252cd 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/include/linux/huge_mm.h</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/include/linux/huge_mm.h</span>
<span class="quote">&gt; @@ -125,12 +125,19 @@ extern void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				    long adjust_next);</span>
<span class="quote">&gt;  extern spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt;  		struct vm_area_struct *vma);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int pmd_related(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return !pmd_none(pmd) &amp;&amp;</span>
<span class="quote">&gt; +		(!pmd_present(pmd) || pmd_trans_huge(pmd) || pmd_devmap(pmd));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>

I would rather create is_swap_pmd() -- (!none &amp;&amp; !present) and leave the
reset open-codded.
<span class="quote">

&gt;  /* mmap_sem must be held on entry */</span>
<span class="quote">&gt;  static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt;  		struct vm_area_struct *vma)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	VM_BUG_ON_VMA(!rwsem_is_locked(&amp;vma-&gt;vm_mm-&gt;mmap_sem), vma);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd))</span>
<span class="quote">&gt; +	if (pmd_related(*pmd))</span>
<span class="quote">&gt;  		return __pmd_trans_huge_lock(pmd, vma);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/gup.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/gup.c</span>
<span class="quote">&gt; index e50178c..2dc4978 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/gup.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/gup.c</span>
<span class="quote">&gt; @@ -267,6 +267,8 @@ struct page *follow_page_mask(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))</span>
<span class="quote">&gt;  		return no_page_table(vma, flags);</span>
<span class="quote">&gt; +	if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +		return no_page_table(vma, flags);</span>

Don&#39;t we need FOLL_MIGRATION, like on pte side?
<span class="quote">
&gt;  	if (pmd_devmap(*pmd)) {</span>
<span class="quote">&gt;  		ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt;  		page = follow_devmap_pmd(vma, address, pmd, flags);</span>
<span class="quote">&gt; @@ -278,6 +280,10 @@ struct page *follow_page_mask(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		return follow_page_pte(vma, address, pmd, flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(*pmd))) {</span>
<span class="quote">&gt; +		spin_unlock(ptl);</span>
<span class="quote">&gt; +		return no_page_table(vma, flags);</span>
<span class="quote">&gt; +	}</span>

Ditto.
<span class="quote">
&gt;  	if (unlikely(!pmd_trans_huge(*pmd))) {</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  		return follow_page_pte(vma, address, pmd, flags);</span>
<span class="quote">&gt; @@ -333,7 +339,7 @@ static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt;  	pud = pud_offset(pgd, address);</span>
<span class="quote">&gt;  	BUG_ON(pud_none(*pud));</span>
<span class="quote">&gt;  	pmd = pmd_offset(pud, address);</span>
<span class="quote">&gt; -	if (pmd_none(*pmd))</span>
<span class="quote">&gt; +	if (!pmd_present(*pmd))</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt;  	VM_BUG_ON(pmd_trans_huge(*pmd));</span>
<span class="quote">&gt;  	pte = pte_offset_map(pmd, address);</span>
<span class="quote">&gt; @@ -1357,7 +1363,7 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		pmd_t pmd = READ_ONCE(*pmdp);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (pmd_none(pmd))</span>
<span class="quote">&gt; +		if (!pmd_present(pmd))</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/huge_memory.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/huge_memory.c</span>
<span class="quote">&gt; index b3022b3..4e9090c 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/huge_memory.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -825,6 +825,20 @@ int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ret = -EAGAIN;</span>
<span class="quote">&gt;  	pmd = *src_pmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(is_pmd_migration_entry(pmd))) {</span>
<span class="quote">&gt; +		swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (is_write_migration_entry(entry)) {</span>
<span class="quote">&gt; +			make_migration_entry_read(&amp;entry);</span>
<span class="quote">&gt; +			pmd = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; +			set_pmd_at(src_mm, addr, src_pmd, pmd);</span>
<span class="quote">&gt; +		}</span>

I think we should put at least WARN_ONCE() in &#39;else&#39; here. We don&#39;t want
to miss such places when swap will be supported (or other swap entry type).
<span class="quote">
&gt; +		set_pmd_at(dst_mm, addr, dst_pmd, pmd);</span>
<span class="quote">&gt; +		ret = 0;</span>
<span class="quote">&gt; +		goto out_unlock;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (unlikely(!pmd_trans_huge(pmd))) {</span>
<span class="quote">&gt;  		pte_free(dst_mm, pgtable);</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt; @@ -1013,6 +1027,9 @@ int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd)</span>
<span class="quote">&gt;  	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; +		goto out_unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageCompound(page) || !PageHead(page), page);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -1137,7 +1154,14 @@ struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page = pmd_page(*pmd);</span>
<span class="quote">&gt; +	if (is_pmd_migration_entry(*pmd)) {</span>
<span class="quote">&gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; +		entry = pmd_to_swp_entry(*pmd);</span>
<span class="quote">&gt; +		page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; +		if (!is_migration_entry(entry))</span>
<span class="quote">&gt; +			goto out;</span>

follow_page_pte() does different thing: wait for page to be migrated and
retry. Any reason you don&#39;t do the same?
<span class="quote">
&gt; +	} else</span>
<span class="quote">&gt; +		page = pmd_page(*pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageHead(page) &amp;&amp; !is_zone_device_page(page), page);</span>
<span class="quote">&gt;  	if (flags &amp; FOLL_TOUCH)</span>
<span class="quote">&gt;  		touch_pmd(vma, addr, pmd);</span>
<span class="quote">&gt; @@ -1332,6 +1356,9 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if (is_huge_zero_pmd(orig_pmd))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * If other processes are mapping this page, we couldn&#39;t discard</span>
<span class="quote">&gt; @@ -1410,20 +1437,35 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt;  		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; -		if (PageAnon(page)) {</span>
<span class="quote">&gt; -			pgtable_t pgtable;</span>
<span class="quote">&gt; -			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; -			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; -			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; -			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +		int migration = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="quote">&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt; +								      pmd);</span>
<span class="quote">&gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt; -			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt; +			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="quote">&gt; +			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			migration = 1;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; -		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +		if (!migration)</span>
<span class="quote">&gt; +			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return 1;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1496,14 +1538,27 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  		bool preserve_write = prot_numa &amp;&amp; pmd_write(*pmd);</span>
<span class="quote">&gt;  		ret = 1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Avoid trapping faults against the zero page. The read-only</span>
<span class="quote">&gt;  		 * data is likely to be read-cached on the local CPU and</span>
<span class="quote">&gt;  		 * local/remote hits to the zero page are not interesting.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd)) {</span>
<span class="quote">&gt; -			spin_unlock(ptl);</span>
<span class="quote">&gt; -			return ret;</span>
<span class="quote">&gt; +		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd))</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (is_pmd_migration_entry(*pmd)) {</span>

Hm? But we filtered out !present above?
<span class="quote">
&gt; +			swp_entry_t entry = pmd_to_swp_entry(*pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_write_migration_entry(entry)) {</span>
<span class="quote">&gt; +				pmd_t newpmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				make_migration_entry_read(&amp;entry);</span>
<span class="quote">&gt; +				newpmd = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; +				set_pmd_at(mm, addr, pmd, newpmd);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (!prot_numa || !pmd_protnone(*pmd)) {</span>
<span class="quote">&gt; @@ -1516,6 +1571,7 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  			BUG_ON(vma_is_anonymous(vma) &amp;&amp; !preserve_write &amp;&amp;</span>
<span class="quote">&gt;  					pmd_write(entry));</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +unlock:</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1532,7 +1588,7 @@ spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	spinlock_t *ptl;</span>
<span class="quote">&gt;  	ptl = pmd_lock(vma-&gt;vm_mm, pmd);</span>
<span class="quote">&gt; -	if (likely(pmd_trans_huge(*pmd) || pmd_devmap(*pmd)))</span>
<span class="quote">&gt; +	if (likely(pmd_related(*pmd)))</span>
<span class="quote">&gt;  		return ptl;</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt;  	return NULL;</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/madvise.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/madvise.c</span>
<span class="quote">&gt; index 0e3828e..eaa2b02 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/madvise.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/madvise.c</span>
<span class="quote">&gt; @@ -274,7 +274,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  	unsigned long next;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pmd))</span>
<span class="quote">&gt; +	if (pmd_related(*pmd))</span>

I don&#39;t see a point going for madvise_free_huge_pmd(), just to fall off on
!present inside.

And is it safe for devmap?
<span class="quote">
&gt;  		if (madvise_free_huge_pmd(tlb, vma, pmd, addr, next))</span>
<span class="quote">&gt;  			goto next;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/memcontrol.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memcontrol.c</span>
<span class="quote">&gt; index 91dfc7c..ebc2c42 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/memcontrol.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memcontrol.c</span>
<span class="quote">&gt; @@ -4635,6 +4635,8 @@ static enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt;  	enum mc_target_type ret = MC_TARGET_NONE;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(pmd)))</span>
<span class="quote">&gt; +		return ret;</span>
<span class="quote">&gt;  	page = pmd_page(pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!page || !PageHead(page), page);</span>
<span class="quote">&gt;  	if (!(mc.flags &amp; MOVE_ANON))</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/memory.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memory.c</span>
<span class="quote">&gt; index 94b5e2c..33fa439 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/memory.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memory.c</span>
<span class="quote">&gt; @@ -999,7 +999,7 @@ static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
<span class="quote">&gt;  	src_pmd = pmd_offset(src_pud, addr);</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (pmd_trans_huge(*src_pmd) || pmd_devmap(*src_pmd)) {</span>
<span class="quote">&gt; +		if (pmd_related(*src_pmd)) {</span>
<span class="quote">&gt;  			int err;</span>
<span class="quote">&gt;  			VM_BUG_ON(next-addr != HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  			err = copy_huge_pmd(dst_mm, src_mm,</span>
<span class="quote">&gt; @@ -3591,6 +3591,10 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  		int ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		barrier();</span>
<span class="quote">&gt; +		if (unlikely(is_pmd_migration_entry(orig_pmd))) {</span>
<span class="quote">&gt; +			pmd_migration_entry_wait(mm, fe.pmd);</span>
<span class="quote">&gt; +			return 0;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {</span>
<span class="quote">&gt;  			if (pmd_protnone(orig_pmd) &amp;&amp; vma_is_accessible(vma))</span>
<span class="quote">&gt;  				return do_huge_pmd_numa_page(&amp;fe, orig_pmd);</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/mprotect.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mprotect.c</span>
<span class="quote">&gt; index c5ba2aa..81186e3 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/mprotect.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mprotect.c</span>
<span class="quote">&gt; @@ -164,6 +164,8 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		unsigned long this_pages;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt;  		if (!pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd)</span>
<span class="quote">&gt;  				&amp;&amp; pmd_none_or_clear_bad(pmd))</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/mremap.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mremap.c</span>
<span class="quote">&gt; index da22ad2..a94a698 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/mremap.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mremap.c</span>
<span class="quote">&gt; @@ -194,7 +194,7 @@ unsigned long move_page_tables(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		new_pmd = alloc_new_pmd(vma-&gt;vm_mm, vma, new_addr);</span>
<span class="quote">&gt;  		if (!new_pmd)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; -		if (pmd_trans_huge(*old_pmd)) {</span>
<span class="quote">&gt; +		if (pmd_related(*old_pmd)) {</span>
<span class="quote">&gt;  			if (extent == HPAGE_PMD_SIZE) {</span>
<span class="quote">&gt;  				bool moved;</span>
<span class="quote">&gt;  				/* See comment in move_ptes() */</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.7.0</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 28, 2016, 2:35 p.m.</div>
<pre class="content">
On Tue 08-11-16 08:31:52, Naoya Horiguchi wrote:
<span class="quote">&gt; If one of callers of page migration starts to handle thp, memory management code</span>
<span class="quote">&gt; start to see pmd migration entry, so we need to prepare for it before enabling.</span>
<span class="quote">&gt; This patch changes various code point which checks the status of given pmds in</span>
<span class="quote">&gt; order to prevent race between thp migration and the pmd-related works.</span>

Please be much more verbose about which code paths those are and why do
we care. The above wording didn&#39;t really help me to understand both the
problem and the solution until I started to stare into the code. And
even then I am not sure how to know that all places have been handled
properly.
<span class="quote">
&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt; - introduce pmd_related() (I know the naming is not good, but can&#39;t think up</span>
<span class="quote">&gt;   no better name. Any suggesntion is welcomed.)</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/mm/gup.c       |  4 +--</span>
<span class="quote">&gt;  fs/proc/task_mmu.c      | 23 +++++++------</span>
<span class="quote">&gt;  include/linux/huge_mm.h |  9 ++++-</span>
<span class="quote">&gt;  mm/gup.c                | 10 ++++--</span>
<span class="quote">&gt;  mm/huge_memory.c        | 88 ++++++++++++++++++++++++++++++++++++++++---------</span>
<span class="quote">&gt;  mm/madvise.c            |  2 +-</span>
<span class="quote">&gt;  mm/memcontrol.c         |  2 ++</span>
<span class="quote">&gt;  mm/memory.c             |  6 +++-</span>
<span class="quote">&gt;  mm/mprotect.c           |  2 ++</span>
<span class="quote">&gt;  mm/mremap.c             |  2 +-</span>
<span class="quote">&gt;  10 files changed, 114 insertions(+), 34 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/arch/x86/mm/gup.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/arch/x86/mm/gup.c</span>
<span class="quote">&gt; index 0d4fb3e..78a153d 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/arch/x86/mm/gup.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/arch/x86/mm/gup.c</span>
<span class="quote">&gt; @@ -222,9 +222,9 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		pmd_t pmd = *pmdp;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (pmd_none(pmd))</span>
<span class="quote">&gt; +		if (!pmd_present(pmd))</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt; -		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="quote">&gt; +		if (unlikely(pmd_large(pmd))) {</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt;  			 * NUMA hinting faults need to be handled in the GUP</span>
<span class="quote">&gt;  			 * slowpath for accounting purposes and so that they</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/fs/proc/task_mmu.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/fs/proc/task_mmu.c</span>
<span class="quote">&gt; index 35b92d8..c1f9cf4 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/fs/proc/task_mmu.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/fs/proc/task_mmu.c</span>
<span class="quote">&gt; @@ -596,7 +596,8 @@ static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ptl = pmd_trans_huge_lock(pmd, vma);</span>
<span class="quote">&gt;  	if (ptl) {</span>
<span class="quote">&gt; -		smaps_pmd_entry(pmd, addr, walk);</span>
<span class="quote">&gt; +		if (pmd_present(*pmd))</span>
<span class="quote">&gt; +			smaps_pmd_entry(pmd, addr, walk);</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -929,6 +930,9 @@ static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  			goto out;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		page = pmd_page(*pmd);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* Clear accessed and referenced bits. */</span>
<span class="quote">&gt; @@ -1208,19 +1212,18 @@ static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  	if (ptl) {</span>
<span class="quote">&gt;  		u64 flags = 0, frame = 0;</span>
<span class="quote">&gt;  		pmd_t pmd = *pmdp;</span>
<span class="quote">&gt; +		struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(pmd))</span>
<span class="quote">&gt;  			flags |= PM_SOFT_DIRTY;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * Currently pmd for thp is always present because thp</span>
<span class="quote">&gt; -		 * can not be swapped-out, migrated, or HWPOISONed</span>
<span class="quote">&gt; -		 * (split in such cases instead.)</span>
<span class="quote">&gt; -		 * This if-check is just to prepare for future implementation.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		if (pmd_present(pmd)) {</span>
<span class="quote">&gt; -			struct page *page = pmd_page(pmd);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +		if (is_pmd_migration_entry(pmd)) {</span>
<span class="quote">&gt; +			swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; +			frame = swp_type(entry) |</span>
<span class="quote">&gt; +				(swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);</span>
<span class="quote">&gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; +		} else if (pmd_present(pmd)) {</span>
<span class="quote">&gt; +			page = pmd_page(pmd);</span>
<span class="quote">&gt;  			if (page_mapcount(page) == 1)</span>
<span class="quote">&gt;  				flags |= PM_MMAP_EXCLUSIVE;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/include/linux/huge_mm.h v4.9-rc2-mmotm-2016-10-27-18-27_patched/include/linux/huge_mm.h</span>
<span class="quote">&gt; index fcbca51..3c252cd 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/include/linux/huge_mm.h</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/include/linux/huge_mm.h</span>
<span class="quote">&gt; @@ -125,12 +125,19 @@ extern void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				    long adjust_next);</span>
<span class="quote">&gt;  extern spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt;  		struct vm_area_struct *vma);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int pmd_related(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return !pmd_none(pmd) &amp;&amp;</span>
<span class="quote">&gt; +		(!pmd_present(pmd) || pmd_trans_huge(pmd) || pmd_devmap(pmd));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /* mmap_sem must be held on entry */</span>
<span class="quote">&gt;  static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt;  		struct vm_area_struct *vma)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	VM_BUG_ON_VMA(!rwsem_is_locked(&amp;vma-&gt;vm_mm-&gt;mmap_sem), vma);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd))</span>
<span class="quote">&gt; +	if (pmd_related(*pmd))</span>
<span class="quote">&gt;  		return __pmd_trans_huge_lock(pmd, vma);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/gup.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/gup.c</span>
<span class="quote">&gt; index e50178c..2dc4978 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/gup.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/gup.c</span>
<span class="quote">&gt; @@ -267,6 +267,8 @@ struct page *follow_page_mask(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))</span>
<span class="quote">&gt;  		return no_page_table(vma, flags);</span>
<span class="quote">&gt; +	if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +		return no_page_table(vma, flags);</span>
<span class="quote">&gt;  	if (pmd_devmap(*pmd)) {</span>
<span class="quote">&gt;  		ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt;  		page = follow_devmap_pmd(vma, address, pmd, flags);</span>
<span class="quote">&gt; @@ -278,6 +280,10 @@ struct page *follow_page_mask(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		return follow_page_pte(vma, address, pmd, flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(*pmd))) {</span>
<span class="quote">&gt; +		spin_unlock(ptl);</span>
<span class="quote">&gt; +		return no_page_table(vma, flags);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	if (unlikely(!pmd_trans_huge(*pmd))) {</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  		return follow_page_pte(vma, address, pmd, flags);</span>
<span class="quote">&gt; @@ -333,7 +339,7 @@ static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt;  	pud = pud_offset(pgd, address);</span>
<span class="quote">&gt;  	BUG_ON(pud_none(*pud));</span>
<span class="quote">&gt;  	pmd = pmd_offset(pud, address);</span>
<span class="quote">&gt; -	if (pmd_none(*pmd))</span>
<span class="quote">&gt; +	if (!pmd_present(*pmd))</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt;  	VM_BUG_ON(pmd_trans_huge(*pmd));</span>
<span class="quote">&gt;  	pte = pte_offset_map(pmd, address);</span>
<span class="quote">&gt; @@ -1357,7 +1363,7 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt;  		pmd_t pmd = READ_ONCE(*pmdp);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (pmd_none(pmd))</span>
<span class="quote">&gt; +		if (!pmd_present(pmd))</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/huge_memory.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/huge_memory.c</span>
<span class="quote">&gt; index b3022b3..4e9090c 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/huge_memory.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -825,6 +825,20 @@ int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ret = -EAGAIN;</span>
<span class="quote">&gt;  	pmd = *src_pmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unlikely(is_pmd_migration_entry(pmd))) {</span>
<span class="quote">&gt; +		swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (is_write_migration_entry(entry)) {</span>
<span class="quote">&gt; +			make_migration_entry_read(&amp;entry);</span>
<span class="quote">&gt; +			pmd = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; +			set_pmd_at(src_mm, addr, src_pmd, pmd);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		set_pmd_at(dst_mm, addr, dst_pmd, pmd);</span>
<span class="quote">&gt; +		ret = 0;</span>
<span class="quote">&gt; +		goto out_unlock;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (unlikely(!pmd_trans_huge(pmd))) {</span>
<span class="quote">&gt;  		pte_free(dst_mm, pgtable);</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt; @@ -1013,6 +1027,9 @@ int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd)</span>
<span class="quote">&gt;  	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; +		goto out_unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageCompound(page) || !PageHead(page), page);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -1137,7 +1154,14 @@ struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page = pmd_page(*pmd);</span>
<span class="quote">&gt; +	if (is_pmd_migration_entry(*pmd)) {</span>
<span class="quote">&gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; +		entry = pmd_to_swp_entry(*pmd);</span>
<span class="quote">&gt; +		page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; +		if (!is_migration_entry(entry))</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt; +	} else</span>
<span class="quote">&gt; +		page = pmd_page(*pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageHead(page) &amp;&amp; !is_zone_device_page(page), page);</span>
<span class="quote">&gt;  	if (flags &amp; FOLL_TOUCH)</span>
<span class="quote">&gt;  		touch_pmd(vma, addr, pmd);</span>
<span class="quote">&gt; @@ -1332,6 +1356,9 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if (is_huge_zero_pmd(orig_pmd))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	page = pmd_page(orig_pmd);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * If other processes are mapping this page, we couldn&#39;t discard</span>
<span class="quote">&gt; @@ -1410,20 +1437,35 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt;  		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; -		if (PageAnon(page)) {</span>
<span class="quote">&gt; -			pgtable_t pgtable;</span>
<span class="quote">&gt; -			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; -			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; -			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; -			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +		int migration = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="quote">&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt; +								      pmd);</span>
<span class="quote">&gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			} else {</span>
<span class="quote">&gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt; -			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt; +			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="quote">&gt; +			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			migration = 1;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; -		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +		if (!migration)</span>
<span class="quote">&gt; +			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return 1;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1496,14 +1538,27 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  		bool preserve_write = prot_numa &amp;&amp; pmd_write(*pmd);</span>
<span class="quote">&gt;  		ret = 1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Avoid trapping faults against the zero page. The read-only</span>
<span class="quote">&gt;  		 * data is likely to be read-cached on the local CPU and</span>
<span class="quote">&gt;  		 * local/remote hits to the zero page are not interesting.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd)) {</span>
<span class="quote">&gt; -			spin_unlock(ptl);</span>
<span class="quote">&gt; -			return ret;</span>
<span class="quote">&gt; +		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd))</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (is_pmd_migration_entry(*pmd)) {</span>
<span class="quote">&gt; +			swp_entry_t entry = pmd_to_swp_entry(*pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_write_migration_entry(entry)) {</span>
<span class="quote">&gt; +				pmd_t newpmd;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				make_migration_entry_read(&amp;entry);</span>
<span class="quote">&gt; +				newpmd = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; +				set_pmd_at(mm, addr, pmd, newpmd);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (!prot_numa || !pmd_protnone(*pmd)) {</span>
<span class="quote">&gt; @@ -1516,6 +1571,7 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  			BUG_ON(vma_is_anonymous(vma) &amp;&amp; !preserve_write &amp;&amp;</span>
<span class="quote">&gt;  					pmd_write(entry));</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +unlock:</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1532,7 +1588,7 @@ spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	spinlock_t *ptl;</span>
<span class="quote">&gt;  	ptl = pmd_lock(vma-&gt;vm_mm, pmd);</span>
<span class="quote">&gt; -	if (likely(pmd_trans_huge(*pmd) || pmd_devmap(*pmd)))</span>
<span class="quote">&gt; +	if (likely(pmd_related(*pmd)))</span>
<span class="quote">&gt;  		return ptl;</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt;  	return NULL;</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/madvise.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/madvise.c</span>
<span class="quote">&gt; index 0e3828e..eaa2b02 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/madvise.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/madvise.c</span>
<span class="quote">&gt; @@ -274,7 +274,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  	unsigned long next;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pmd))</span>
<span class="quote">&gt; +	if (pmd_related(*pmd))</span>
<span class="quote">&gt;  		if (madvise_free_huge_pmd(tlb, vma, pmd, addr, next))</span>
<span class="quote">&gt;  			goto next;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/memcontrol.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memcontrol.c</span>
<span class="quote">&gt; index 91dfc7c..ebc2c42 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/memcontrol.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memcontrol.c</span>
<span class="quote">&gt; @@ -4635,6 +4635,8 @@ static enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt;  	enum mc_target_type ret = MC_TARGET_NONE;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (unlikely(!pmd_present(pmd)))</span>
<span class="quote">&gt; +		return ret;</span>
<span class="quote">&gt;  	page = pmd_page(pmd);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!page || !PageHead(page), page);</span>
<span class="quote">&gt;  	if (!(mc.flags &amp; MOVE_ANON))</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/memory.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memory.c</span>
<span class="quote">&gt; index 94b5e2c..33fa439 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/memory.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memory.c</span>
<span class="quote">&gt; @@ -999,7 +999,7 @@ static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
<span class="quote">&gt;  	src_pmd = pmd_offset(src_pud, addr);</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; -		if (pmd_trans_huge(*src_pmd) || pmd_devmap(*src_pmd)) {</span>
<span class="quote">&gt; +		if (pmd_related(*src_pmd)) {</span>
<span class="quote">&gt;  			int err;</span>
<span class="quote">&gt;  			VM_BUG_ON(next-addr != HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  			err = copy_huge_pmd(dst_mm, src_mm,</span>
<span class="quote">&gt; @@ -3591,6 +3591,10 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  		int ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		barrier();</span>
<span class="quote">&gt; +		if (unlikely(is_pmd_migration_entry(orig_pmd))) {</span>
<span class="quote">&gt; +			pmd_migration_entry_wait(mm, fe.pmd);</span>
<span class="quote">&gt; +			return 0;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {</span>
<span class="quote">&gt;  			if (pmd_protnone(orig_pmd) &amp;&amp; vma_is_accessible(vma))</span>
<span class="quote">&gt;  				return do_huge_pmd_numa_page(&amp;fe, orig_pmd);</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/mprotect.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mprotect.c</span>
<span class="quote">&gt; index c5ba2aa..81186e3 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/mprotect.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mprotect.c</span>
<span class="quote">&gt; @@ -164,6 +164,8 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		unsigned long this_pages;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt;  		if (!pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd)</span>
<span class="quote">&gt;  				&amp;&amp; pmd_none_or_clear_bad(pmd))</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/mremap.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mremap.c</span>
<span class="quote">&gt; index da22ad2..a94a698 100644</span>
<span class="quote">&gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/mremap.c</span>
<span class="quote">&gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mremap.c</span>
<span class="quote">&gt; @@ -194,7 +194,7 @@ unsigned long move_page_tables(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		new_pmd = alloc_new_pmd(vma-&gt;vm_mm, vma, new_addr);</span>
<span class="quote">&gt;  		if (!new_pmd)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; -		if (pmd_trans_huge(*old_pmd)) {</span>
<span class="quote">&gt; +		if (pmd_related(*old_pmd)) {</span>
<span class="quote">&gt;  			if (extent == HPAGE_PMD_SIZE) {</span>
<span class="quote">&gt;  				bool moved;</span>
<span class="quote">&gt;  				/* See comment in move_ptes() */</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.7.0</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Nov. 29, 2016, 6:46 a.m.</div>
<pre class="content">
# sorry for late reply ...

On Fri, Nov 18, 2016 at 02:56:24AM +0300, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Tue, Nov 08, 2016 at 08:31:52AM +0900, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; If one of callers of page migration starts to handle thp, memory management code</span>
<span class="quote">&gt; &gt; start to see pmd migration entry, so we need to prepare for it before enabling.</span>
<span class="quote">&gt; &gt; This patch changes various code point which checks the status of given pmds in</span>
<span class="quote">&gt; &gt; order to prevent race between thp migration and the pmd-related works.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt; &gt; - introduce pmd_related() (I know the naming is not good, but can&#39;t think up</span>
<span class="quote">&gt; &gt;   no better name. Any suggesntion is welcomed.)</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  arch/x86/mm/gup.c       |  4 +--</span>
<span class="quote">&gt; &gt;  fs/proc/task_mmu.c      | 23 +++++++------</span>
<span class="quote">&gt; &gt;  include/linux/huge_mm.h |  9 ++++-</span>
<span class="quote">&gt; &gt;  mm/gup.c                | 10 ++++--</span>
<span class="quote">&gt; &gt;  mm/huge_memory.c        | 88 ++++++++++++++++++++++++++++++++++++++++---------</span>
<span class="quote">&gt; &gt;  mm/madvise.c            |  2 +-</span>
<span class="quote">&gt; &gt;  mm/memcontrol.c         |  2 ++</span>
<span class="quote">&gt; &gt;  mm/memory.c             |  6 +++-</span>
<span class="quote">&gt; &gt;  mm/mprotect.c           |  2 ++</span>
<span class="quote">&gt; &gt;  mm/mremap.c             |  2 +-</span>
<span class="quote">&gt; &gt;  10 files changed, 114 insertions(+), 34 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/arch/x86/mm/gup.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/arch/x86/mm/gup.c</span>
<span class="quote">&gt; &gt; index 0d4fb3e..78a153d 100644</span>
<span class="quote">&gt; &gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/arch/x86/mm/gup.c</span>
<span class="quote">&gt; &gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/arch/x86/mm/gup.c</span>
<span class="quote">&gt; &gt; @@ -222,9 +222,9 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; &gt;  		pmd_t pmd = *pmdp;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt; -		if (pmd_none(pmd))</span>
<span class="quote">&gt; &gt; +		if (!pmd_present(pmd))</span>
<span class="quote">&gt; &gt;  			return 0;</span>
<span class="quote">&gt; &gt; -		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="quote">&gt; &gt; +		if (unlikely(pmd_large(pmd))) {</span>
<span class="quote">&gt; &gt;  			/*</span>
<span class="quote">&gt; &gt;  			 * NUMA hinting faults need to be handled in the GUP</span>
<span class="quote">&gt; &gt;  			 * slowpath for accounting purposes and so that they</span>
<span class="quote">&gt; &gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/fs/proc/task_mmu.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt; index 35b92d8..c1f9cf4 100644</span>
<span class="quote">&gt; &gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt; @@ -596,7 +596,8 @@ static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	ptl = pmd_trans_huge_lock(pmd, vma);</span>
<span class="quote">&gt; &gt;  	if (ptl) {</span>
<span class="quote">&gt; &gt; -		smaps_pmd_entry(pmd, addr, walk);</span>
<span class="quote">&gt; &gt; +		if (pmd_present(*pmd))</span>
<span class="quote">&gt; &gt; +			smaps_pmd_entry(pmd, addr, walk);</span>
<span class="quote">&gt; &gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; &gt;  		return 0;</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt; @@ -929,6 +930,9 @@ static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; &gt;  			goto out;</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; &gt; +			goto out;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hm. Looks like clear_soft_dirty_pmd() should handle !present. It doesn&#39;t.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ah.. Found it in 08/12.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;  		page = pmd_page(*pmd);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		/* Clear accessed and referenced bits. */</span>
<span class="quote">&gt; &gt; @@ -1208,19 +1212,18 @@ static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; &gt;  	if (ptl) {</span>
<span class="quote">&gt; &gt;  		u64 flags = 0, frame = 0;</span>
<span class="quote">&gt; &gt;  		pmd_t pmd = *pmdp;</span>
<span class="quote">&gt; &gt; +		struct page *page;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(pmd))</span>
<span class="quote">&gt; &gt;  			flags |= PM_SOFT_DIRTY;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -		/*</span>
<span class="quote">&gt; &gt; -		 * Currently pmd for thp is always present because thp</span>
<span class="quote">&gt; &gt; -		 * can not be swapped-out, migrated, or HWPOISONed</span>
<span class="quote">&gt; &gt; -		 * (split in such cases instead.)</span>
<span class="quote">&gt; &gt; -		 * This if-check is just to prepare for future implementation.</span>
<span class="quote">&gt; &gt; -		 */</span>
<span class="quote">&gt; &gt; -		if (pmd_present(pmd)) {</span>
<span class="quote">&gt; &gt; -			struct page *page = pmd_page(pmd);</span>
<span class="quote">&gt; &gt; -</span>
<span class="quote">&gt; &gt; +		if (is_pmd_migration_entry(pmd)) {</span>
<span class="quote">&gt; &gt; +			swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; &gt; +			frame = swp_type(entry) |</span>
<span class="quote">&gt; &gt; +				(swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);</span>
<span class="quote">&gt; &gt; +			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; &gt; +		} else if (pmd_present(pmd)) {</span>
<span class="quote">&gt; &gt; +			page = pmd_page(pmd);</span>
<span class="quote">&gt; &gt;  			if (page_mapcount(page) == 1)</span>
<span class="quote">&gt; &gt;  				flags |= PM_MMAP_EXCLUSIVE;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/include/linux/huge_mm.h v4.9-rc2-mmotm-2016-10-27-18-27_patched/include/linux/huge_mm.h</span>
<span class="quote">&gt; &gt; index fcbca51..3c252cd 100644</span>
<span class="quote">&gt; &gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/include/linux/huge_mm.h</span>
<span class="quote">&gt; &gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/include/linux/huge_mm.h</span>
<span class="quote">&gt; &gt; @@ -125,12 +125,19 @@ extern void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  				    long adjust_next);</span>
<span class="quote">&gt; &gt;  extern spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt; &gt;  		struct vm_area_struct *vma);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline int pmd_related(pmd_t pmd)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return !pmd_none(pmd) &amp;&amp;</span>
<span class="quote">&gt; &gt; +		(!pmd_present(pmd) || pmd_trans_huge(pmd) || pmd_devmap(pmd));</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would rather create is_swap_pmd() -- (!none &amp;&amp; !present) and leave the</span>
<span class="quote">&gt; reset open-codded.</span>

OK, I do this.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;  /* mmap_sem must be held on entry */</span>
<span class="quote">&gt; &gt;  static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,</span>
<span class="quote">&gt; &gt;  		struct vm_area_struct *vma)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;  	VM_BUG_ON_VMA(!rwsem_is_locked(&amp;vma-&gt;vm_mm-&gt;mmap_sem), vma);</span>
<span class="quote">&gt; &gt; -	if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd))</span>
<span class="quote">&gt; &gt; +	if (pmd_related(*pmd))</span>
<span class="quote">&gt; &gt;  		return __pmd_trans_huge_lock(pmd, vma);</span>
<span class="quote">&gt; &gt;  	else</span>
<span class="quote">&gt; &gt;  		return NULL;</span>
<span class="quote">&gt; &gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/gup.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/gup.c</span>
<span class="quote">&gt; &gt; index e50178c..2dc4978 100644</span>
<span class="quote">&gt; &gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/gup.c</span>
<span class="quote">&gt; &gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/gup.c</span>
<span class="quote">&gt; &gt; @@ -267,6 +267,8 @@ struct page *follow_page_mask(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))</span>
<span class="quote">&gt; &gt;  		return no_page_table(vma, flags);</span>
<span class="quote">&gt; &gt; +	if (!pmd_present(*pmd))</span>
<span class="quote">&gt; &gt; +		return no_page_table(vma, flags);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Don&#39;t we need FOLL_MIGRATION, like on pte side?</span>

That&#39;s better, applied.
<span class="quote">
&gt; &gt;  	if (pmd_devmap(*pmd)) {</span>
<span class="quote">&gt; &gt;  		ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; &gt;  		page = follow_devmap_pmd(vma, address, pmd, flags);</span>
<span class="quote">&gt; &gt; @@ -278,6 +280,10 @@ struct page *follow_page_mask(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  		return follow_page_pte(vma, address, pmd, flags);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; &gt; +	if (unlikely(!pmd_present(*pmd))) {</span>
<span class="quote">&gt; &gt; +		spin_unlock(ptl);</span>
<span class="quote">&gt; &gt; +		return no_page_table(vma, flags);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ditto.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;  	if (unlikely(!pmd_trans_huge(*pmd))) {</span>
<span class="quote">&gt; &gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; &gt;  		return follow_page_pte(vma, address, pmd, flags);</span>
<span class="quote">&gt; &gt; @@ -333,7 +339,7 @@ static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
<span class="quote">&gt; &gt;  	pud = pud_offset(pgd, address);</span>
<span class="quote">&gt; &gt;  	BUG_ON(pud_none(*pud));</span>
<span class="quote">&gt; &gt;  	pmd = pmd_offset(pud, address);</span>
<span class="quote">&gt; &gt; -	if (pmd_none(*pmd))</span>
<span class="quote">&gt; &gt; +	if (!pmd_present(*pmd))</span>
<span class="quote">&gt; &gt;  		return -EFAULT;</span>
<span class="quote">&gt; &gt;  	VM_BUG_ON(pmd_trans_huge(*pmd));</span>
<span class="quote">&gt; &gt;  	pte = pte_offset_map(pmd, address);</span>
<span class="quote">&gt; &gt; @@ -1357,7 +1363,7 @@ static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
<span class="quote">&gt; &gt;  		pmd_t pmd = READ_ONCE(*pmdp);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt; -		if (pmd_none(pmd))</span>
<span class="quote">&gt; &gt; +		if (!pmd_present(pmd))</span>
<span class="quote">&gt; &gt;  			return 0;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {</span>
<span class="quote">&gt; &gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/huge_memory.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; index b3022b3..4e9090c 100644</span>
<span class="quote">&gt; &gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; @@ -825,6 +825,20 @@ int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	ret = -EAGAIN;</span>
<span class="quote">&gt; &gt;  	pmd = *src_pmd;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (unlikely(is_pmd_migration_entry(pmd))) {</span>
<span class="quote">&gt; &gt; +		swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (is_write_migration_entry(entry)) {</span>
<span class="quote">&gt; &gt; +			make_migration_entry_read(&amp;entry);</span>
<span class="quote">&gt; &gt; +			pmd = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; &gt; +			set_pmd_at(src_mm, addr, src_pmd, pmd);</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think we should put at least WARN_ONCE() in &#39;else&#39; here. We don&#39;t want</span>
<span class="quote">&gt; to miss such places when swap will be supported (or other swap entry type).</span>

I guess that you mean &#39;else&#39; branch of outer if-block, right?
<span class="quote">
&gt; &gt; +		set_pmd_at(dst_mm, addr, dst_pmd, pmd);</span>
<span class="quote">&gt; &gt; +		ret = 0;</span>
<span class="quote">&gt; &gt; +		goto out_unlock;</span>
<span class="quote">&gt; &gt; +	}</span>

Maybe inserting the below here seems OK.

        WARN_ONCE(!pmd_present(pmd), &quot;Unknown non-present format on pmd.\n&quot;);
<span class="quote">
&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	if (unlikely(!pmd_trans_huge(pmd))) {</span>
<span class="quote">&gt; &gt;  		pte_free(dst_mm, pgtable);</span>
<span class="quote">&gt; &gt;  		goto out_unlock;</span>
<span class="quote">&gt; &gt; @@ -1013,6 +1027,9 @@ int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd)</span>
<span class="quote">&gt; &gt;  	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))</span>
<span class="quote">&gt; &gt;  		goto out_unlock;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; &gt; +		goto out_unlock;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; &gt;  	VM_BUG_ON_PAGE(!PageCompound(page) || !PageHead(page), page);</span>
<span class="quote">&gt; &gt;  	/*</span>
<span class="quote">&gt; &gt; @@ -1137,7 +1154,14 @@ struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))</span>
<span class="quote">&gt; &gt;  		goto out;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	page = pmd_page(*pmd);</span>
<span class="quote">&gt; &gt; +	if (is_pmd_migration_entry(*pmd)) {</span>
<span class="quote">&gt; &gt; +		swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +		entry = pmd_to_swp_entry(*pmd);</span>
<span class="quote">&gt; &gt; +		page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; &gt; +		if (!is_migration_entry(entry))</span>
<span class="quote">&gt; &gt; +			goto out;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; follow_page_pte() does different thing: wait for page to be migrated and</span>
<span class="quote">&gt; retry. Any reason you don&#39;t do the same?</span>

No, just my self-check wasn&#39;t enough.
<span class="quote">
&gt; &gt; +	} else</span>
<span class="quote">&gt; &gt; +		page = pmd_page(*pmd);</span>
<span class="quote">&gt; &gt;  	VM_BUG_ON_PAGE(!PageHead(page) &amp;&amp; !is_zone_device_page(page), page);</span>
<span class="quote">&gt; &gt;  	if (flags &amp; FOLL_TOUCH)</span>
<span class="quote">&gt; &gt;  		touch_pmd(vma, addr, pmd);</span>
<span class="quote">&gt; &gt; @@ -1332,6 +1356,9 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  	if (is_huge_zero_pmd(orig_pmd))</span>
<span class="quote">&gt; &gt;  		goto out;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="quote">&gt; &gt; +		goto out;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  	page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; &gt;  	/*</span>
<span class="quote">&gt; &gt;  	 * If other processes are mapping this page, we couldn&#39;t discard</span>
<span class="quote">&gt; &gt; @@ -1410,20 +1437,35 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; &gt;  	} else {</span>
<span class="quote">&gt; &gt;  		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; &gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt; &gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; &gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt; -		if (PageAnon(page)) {</span>
<span class="quote">&gt; &gt; -			pgtable_t pgtable;</span>
<span class="quote">&gt; &gt; -			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; &gt; -			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; &gt; -			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; &gt; -			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt; +		int migration = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="quote">&gt; &gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt; &gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; &gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt; +			if (PageAnon(page)) {</span>
<span class="quote">&gt; &gt; +				pgtable_t pgtable;</span>
<span class="quote">&gt; &gt; +				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt; &gt; +								      pmd);</span>
<span class="quote">&gt; &gt; +				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; &gt; +				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; &gt; +				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt; &gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt; +			} else {</span>
<span class="quote">&gt; &gt; +				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt; &gt; +					       -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt;  		} else {</span>
<span class="quote">&gt; &gt; -			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt; &gt; +			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="quote">&gt; &gt; +			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; &gt; +			migration = 1;</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; &gt; -		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; &gt; +		if (!migration)</span>
<span class="quote">&gt; &gt; +			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  	return 1;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt; @@ -1496,14 +1538,27 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt; &gt;  		bool preserve_write = prot_numa &amp;&amp; pmd_write(*pmd);</span>
<span class="quote">&gt; &gt;  		ret = 1;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; &gt; +			goto unlock;</span>
<span class="quote">&gt; &gt;  		/*</span>
<span class="quote">&gt; &gt;  		 * Avoid trapping faults against the zero page. The read-only</span>
<span class="quote">&gt; &gt;  		 * data is likely to be read-cached on the local CPU and</span>
<span class="quote">&gt; &gt;  		 * local/remote hits to the zero page are not interesting.</span>
<span class="quote">&gt; &gt;  		 */</span>
<span class="quote">&gt; &gt; -		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd)) {</span>
<span class="quote">&gt; &gt; -			spin_unlock(ptl);</span>
<span class="quote">&gt; &gt; -			return ret;</span>
<span class="quote">&gt; &gt; +		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd))</span>
<span class="quote">&gt; &gt; +			goto unlock;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (is_pmd_migration_entry(*pmd)) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hm? But we filtered out !present above?</span>

the !present check must come after this if-block with WARN_ONCE().
<span class="quote">
&gt; &gt; +			swp_entry_t entry = pmd_to_swp_entry(*pmd);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			if (is_write_migration_entry(entry)) {</span>
<span class="quote">&gt; &gt; +				pmd_t newpmd;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +				make_migration_entry_read(&amp;entry);</span>
<span class="quote">&gt; &gt; +				newpmd = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; &gt; +				set_pmd_at(mm, addr, pmd, newpmd);</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +			goto unlock;</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		if (!prot_numa || !pmd_protnone(*pmd)) {</span>
<span class="quote">&gt; &gt; @@ -1516,6 +1571,7 @@ int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt; &gt;  			BUG_ON(vma_is_anonymous(vma) &amp;&amp; !preserve_write &amp;&amp;</span>
<span class="quote">&gt; &gt;  					pmd_write(entry));</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt; +unlock:</span>
<span class="quote">&gt; &gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -1532,7 +1588,7 @@ spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;  	spinlock_t *ptl;</span>
<span class="quote">&gt; &gt;  	ptl = pmd_lock(vma-&gt;vm_mm, pmd);</span>
<span class="quote">&gt; &gt; -	if (likely(pmd_trans_huge(*pmd) || pmd_devmap(*pmd)))</span>
<span class="quote">&gt; &gt; +	if (likely(pmd_related(*pmd)))</span>
<span class="quote">&gt; &gt;  		return ptl;</span>
<span class="quote">&gt; &gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; &gt;  	return NULL;</span>
<span class="quote">&gt; &gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/madvise.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/madvise.c</span>
<span class="quote">&gt; &gt; index 0e3828e..eaa2b02 100644</span>
<span class="quote">&gt; &gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/madvise.c</span>
<span class="quote">&gt; &gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/madvise.c</span>
<span class="quote">&gt; &gt; @@ -274,7 +274,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt; &gt;  	unsigned long next;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt; -	if (pmd_trans_huge(*pmd))</span>
<span class="quote">&gt; &gt; +	if (pmd_related(*pmd))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t see a point going for madvise_free_huge_pmd(), just to fall off on</span>
<span class="quote">&gt; !present inside.</span>

Sorry, this code was wrong. I should&#39;ve done like below simply:

+	if (!pmd_present(*pmd))
+		return 0;
 	if (pmd_trans_huge(*pmd))
 		...
<span class="quote">
&gt; And is it safe for devmap?</span>

I&#39;m not sure, so let&#39;s keep it as-is.

Thanks,
Naoya Horiguchi
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;  		if (madvise_free_huge_pmd(tlb, vma, pmd, addr, next))</span>
<span class="quote">&gt; &gt;  			goto next;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/memcontrol.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memcontrol.c</span>
<span class="quote">&gt; &gt; index 91dfc7c..ebc2c42 100644</span>
<span class="quote">&gt; &gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/memcontrol.c</span>
<span class="quote">&gt; &gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memcontrol.c</span>
<span class="quote">&gt; &gt; @@ -4635,6 +4635,8 @@ static enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  	struct page *page = NULL;</span>
<span class="quote">&gt; &gt;  	enum mc_target_type ret = MC_TARGET_NONE;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	if (unlikely(!pmd_present(pmd)))</span>
<span class="quote">&gt; &gt; +		return ret;</span>
<span class="quote">&gt; &gt;  	page = pmd_page(pmd);</span>
<span class="quote">&gt; &gt;  	VM_BUG_ON_PAGE(!page || !PageHead(page), page);</span>
<span class="quote">&gt; &gt;  	if (!(mc.flags &amp; MOVE_ANON))</span>
<span class="quote">&gt; &gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/memory.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memory.c</span>
<span class="quote">&gt; &gt; index 94b5e2c..33fa439 100644</span>
<span class="quote">&gt; &gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/memory.c</span>
<span class="quote">&gt; &gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memory.c</span>
<span class="quote">&gt; &gt; @@ -999,7 +999,7 @@ static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
<span class="quote">&gt; &gt;  	src_pmd = pmd_offset(src_pud, addr);</span>
<span class="quote">&gt; &gt;  	do {</span>
<span class="quote">&gt; &gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt; -		if (pmd_trans_huge(*src_pmd) || pmd_devmap(*src_pmd)) {</span>
<span class="quote">&gt; &gt; +		if (pmd_related(*src_pmd)) {</span>
<span class="quote">&gt; &gt;  			int err;</span>
<span class="quote">&gt; &gt;  			VM_BUG_ON(next-addr != HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; &gt;  			err = copy_huge_pmd(dst_mm, src_mm,</span>
<span class="quote">&gt; &gt; @@ -3591,6 +3591,10 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; &gt;  		int ret;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		barrier();</span>
<span class="quote">&gt; &gt; +		if (unlikely(is_pmd_migration_entry(orig_pmd))) {</span>
<span class="quote">&gt; &gt; +			pmd_migration_entry_wait(mm, fe.pmd);</span>
<span class="quote">&gt; &gt; +			return 0;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt;  		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {</span>
<span class="quote">&gt; &gt;  			if (pmd_protnone(orig_pmd) &amp;&amp; vma_is_accessible(vma))</span>
<span class="quote">&gt; &gt;  				return do_huge_pmd_numa_page(&amp;fe, orig_pmd);</span>
<span class="quote">&gt; &gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/mprotect.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mprotect.c</span>
<span class="quote">&gt; &gt; index c5ba2aa..81186e3 100644</span>
<span class="quote">&gt; &gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/mprotect.c</span>
<span class="quote">&gt; &gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mprotect.c</span>
<span class="quote">&gt; &gt; @@ -164,6 +164,8 @@ static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  		unsigned long this_pages;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  		next = pmd_addr_end(addr, end);</span>
<span class="quote">&gt; &gt; +		if (!pmd_present(*pmd))</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt;  		if (!pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd)</span>
<span class="quote">&gt; &gt;  				&amp;&amp; pmd_none_or_clear_bad(pmd))</span>
<span class="quote">&gt; &gt;  			continue;</span>
<span class="quote">&gt; &gt; diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/mremap.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mremap.c</span>
<span class="quote">&gt; &gt; index da22ad2..a94a698 100644</span>
<span class="quote">&gt; &gt; --- v4.9-rc2-mmotm-2016-10-27-18-27/mm/mremap.c</span>
<span class="quote">&gt; &gt; +++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mremap.c</span>
<span class="quote">&gt; &gt; @@ -194,7 +194,7 @@ unsigned long move_page_tables(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  		new_pmd = alloc_new_pmd(vma-&gt;vm_mm, vma, new_addr);</span>
<span class="quote">&gt; &gt;  		if (!new_pmd)</span>
<span class="quote">&gt; &gt;  			break;</span>
<span class="quote">&gt; &gt; -		if (pmd_trans_huge(*old_pmd)) {</span>
<span class="quote">&gt; &gt; +		if (pmd_related(*old_pmd)) {</span>
<span class="quote">&gt; &gt;  			if (extent == HPAGE_PMD_SIZE) {</span>
<span class="quote">&gt; &gt;  				bool moved;</span>
<span class="quote">&gt; &gt;  				/* See comment in move_ptes() */</span>
<span class="quote">&gt; &gt; -- </span>
<span class="quote">&gt; &gt; 2.7.0</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; --</span>
<span class="quote">&gt; &gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; &gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; &gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; &gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt;  Kirill A. Shutemov</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git v4.9-rc2-mmotm-2016-10-27-18-27/arch/x86/mm/gup.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/arch/x86/mm/gup.c</span>
<span class="p_header">index 0d4fb3e..78a153d 100644</span>
<span class="p_header">--- v4.9-rc2-mmotm-2016-10-27-18-27/arch/x86/mm/gup.c</span>
<span class="p_header">+++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/arch/x86/mm/gup.c</span>
<span class="p_chunk">@@ -222,9 +222,9 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 		pmd_t pmd = *pmdp;
 
 		next = pmd_addr_end(addr, end);
<span class="p_del">-		if (pmd_none(pmd))</span>
<span class="p_add">+		if (!pmd_present(pmd))</span>
 			return 0;
<span class="p_del">-		if (unlikely(pmd_large(pmd) || !pmd_present(pmd))) {</span>
<span class="p_add">+		if (unlikely(pmd_large(pmd))) {</span>
 			/*
 			 * NUMA hinting faults need to be handled in the GUP
 			 * slowpath for accounting purposes and so that they
<span class="p_header">diff --git v4.9-rc2-mmotm-2016-10-27-18-27/fs/proc/task_mmu.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/fs/proc/task_mmu.c</span>
<span class="p_header">index 35b92d8..c1f9cf4 100644</span>
<span class="p_header">--- v4.9-rc2-mmotm-2016-10-27-18-27/fs/proc/task_mmu.c</span>
<span class="p_header">+++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -596,7 +596,8 @@</span> <span class="p_context"> static int smaps_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end,</span>
 
 	ptl = pmd_trans_huge_lock(pmd, vma);
 	if (ptl) {
<span class="p_del">-		smaps_pmd_entry(pmd, addr, walk);</span>
<span class="p_add">+		if (pmd_present(*pmd))</span>
<span class="p_add">+			smaps_pmd_entry(pmd, addr, walk);</span>
 		spin_unlock(ptl);
 		return 0;
 	}
<span class="p_chunk">@@ -929,6 +930,9 @@</span> <span class="p_context"> static int clear_refs_pte_range(pmd_t *pmd, unsigned long addr,</span>
 			goto out;
 		}
 
<span class="p_add">+		if (!pmd_present(*pmd))</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+</span>
 		page = pmd_page(*pmd);
 
 		/* Clear accessed and referenced bits. */
<span class="p_chunk">@@ -1208,19 +1212,18 @@</span> <span class="p_context"> static int pagemap_pmd_range(pmd_t *pmdp, unsigned long addr, unsigned long end,</span>
 	if (ptl) {
 		u64 flags = 0, frame = 0;
 		pmd_t pmd = *pmdp;
<span class="p_add">+		struct page *page;</span>
 
 		if ((vma-&gt;vm_flags &amp; VM_SOFTDIRTY) || pmd_soft_dirty(pmd))
 			flags |= PM_SOFT_DIRTY;
 
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Currently pmd for thp is always present because thp</span>
<span class="p_del">-		 * can not be swapped-out, migrated, or HWPOISONed</span>
<span class="p_del">-		 * (split in such cases instead.)</span>
<span class="p_del">-		 * This if-check is just to prepare for future implementation.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (pmd_present(pmd)) {</span>
<span class="p_del">-			struct page *page = pmd_page(pmd);</span>
<span class="p_del">-</span>
<span class="p_add">+		if (is_pmd_migration_entry(pmd)) {</span>
<span class="p_add">+			swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="p_add">+			frame = swp_type(entry) |</span>
<span class="p_add">+				(swp_offset(entry) &lt;&lt; MAX_SWAPFILES_SHIFT);</span>
<span class="p_add">+			page = migration_entry_to_page(entry);</span>
<span class="p_add">+		} else if (pmd_present(pmd)) {</span>
<span class="p_add">+			page = pmd_page(pmd);</span>
 			if (page_mapcount(page) == 1)
 				flags |= PM_MMAP_EXCLUSIVE;
 
<span class="p_header">diff --git v4.9-rc2-mmotm-2016-10-27-18-27/include/linux/huge_mm.h v4.9-rc2-mmotm-2016-10-27-18-27_patched/include/linux/huge_mm.h</span>
<span class="p_header">index fcbca51..3c252cd 100644</span>
<span class="p_header">--- v4.9-rc2-mmotm-2016-10-27-18-27/include/linux/huge_mm.h</span>
<span class="p_header">+++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/include/linux/huge_mm.h</span>
<span class="p_chunk">@@ -125,12 +125,19 @@</span> <span class="p_context"> extern void vma_adjust_trans_huge(struct vm_area_struct *vma,</span>
 				    long adjust_next);
 extern spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd,
 		struct vm_area_struct *vma);
<span class="p_add">+</span>
<span class="p_add">+static inline int pmd_related(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !pmd_none(pmd) &amp;&amp;</span>
<span class="p_add">+		(!pmd_present(pmd) || pmd_trans_huge(pmd) || pmd_devmap(pmd));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* mmap_sem must be held on entry */
 static inline spinlock_t *pmd_trans_huge_lock(pmd_t *pmd,
 		struct vm_area_struct *vma)
 {
 	VM_BUG_ON_VMA(!rwsem_is_locked(&amp;vma-&gt;vm_mm-&gt;mmap_sem), vma);
<span class="p_del">-	if (pmd_trans_huge(*pmd) || pmd_devmap(*pmd))</span>
<span class="p_add">+	if (pmd_related(*pmd))</span>
 		return __pmd_trans_huge_lock(pmd, vma);
 	else
 		return NULL;
<span class="p_header">diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/gup.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/gup.c</span>
<span class="p_header">index e50178c..2dc4978 100644</span>
<span class="p_header">--- v4.9-rc2-mmotm-2016-10-27-18-27/mm/gup.c</span>
<span class="p_header">+++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/gup.c</span>
<span class="p_chunk">@@ -267,6 +267,8 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 	}
 	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))
 		return no_page_table(vma, flags);
<span class="p_add">+	if (!pmd_present(*pmd))</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
 	if (pmd_devmap(*pmd)) {
 		ptl = pmd_lock(mm, pmd);
 		page = follow_devmap_pmd(vma, address, pmd, flags);
<span class="p_chunk">@@ -278,6 +280,10 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 		return follow_page_pte(vma, address, pmd, flags);
 
 	ptl = pmd_lock(mm, pmd);
<span class="p_add">+	if (unlikely(!pmd_present(*pmd))) {</span>
<span class="p_add">+		spin_unlock(ptl);</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	}</span>
 	if (unlikely(!pmd_trans_huge(*pmd))) {
 		spin_unlock(ptl);
 		return follow_page_pte(vma, address, pmd, flags);
<span class="p_chunk">@@ -333,7 +339,7 @@</span> <span class="p_context"> static int get_gate_page(struct mm_struct *mm, unsigned long address,</span>
 	pud = pud_offset(pgd, address);
 	BUG_ON(pud_none(*pud));
 	pmd = pmd_offset(pud, address);
<span class="p_del">-	if (pmd_none(*pmd))</span>
<span class="p_add">+	if (!pmd_present(*pmd))</span>
 		return -EFAULT;
 	VM_BUG_ON(pmd_trans_huge(*pmd));
 	pte = pte_offset_map(pmd, address);
<span class="p_chunk">@@ -1357,7 +1363,7 @@</span> <span class="p_context"> static int gup_pmd_range(pud_t pud, unsigned long addr, unsigned long end,</span>
 		pmd_t pmd = READ_ONCE(*pmdp);
 
 		next = pmd_addr_end(addr, end);
<span class="p_del">-		if (pmd_none(pmd))</span>
<span class="p_add">+		if (!pmd_present(pmd))</span>
 			return 0;
 
 		if (unlikely(pmd_trans_huge(pmd) || pmd_huge(pmd))) {
<span class="p_header">diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/huge_memory.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/huge_memory.c</span>
<span class="p_header">index b3022b3..4e9090c 100644</span>
<span class="p_header">--- v4.9-rc2-mmotm-2016-10-27-18-27/mm/huge_memory.c</span>
<span class="p_header">+++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/huge_memory.c</span>
<span class="p_chunk">@@ -825,6 +825,20 @@</span> <span class="p_context"> int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 
 	ret = -EAGAIN;
 	pmd = *src_pmd;
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(is_pmd_migration_entry(pmd))) {</span>
<span class="p_add">+		swp_entry_t entry = pmd_to_swp_entry(pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (is_write_migration_entry(entry)) {</span>
<span class="p_add">+			make_migration_entry_read(&amp;entry);</span>
<span class="p_add">+			pmd = swp_entry_to_pmd(entry);</span>
<span class="p_add">+			set_pmd_at(src_mm, addr, src_pmd, pmd);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		set_pmd_at(dst_mm, addr, dst_pmd, pmd);</span>
<span class="p_add">+		ret = 0;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	if (unlikely(!pmd_trans_huge(pmd))) {
 		pte_free(dst_mm, pgtable);
 		goto out_unlock;
<span class="p_chunk">@@ -1013,6 +1027,9 @@</span> <span class="p_context"> int do_huge_pmd_wp_page(struct fault_env *fe, pmd_t orig_pmd)</span>
 	if (unlikely(!pmd_same(*fe-&gt;pmd, orig_pmd)))
 		goto out_unlock;
 
<span class="p_add">+	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+</span>
 	page = pmd_page(orig_pmd);
 	VM_BUG_ON_PAGE(!PageCompound(page) || !PageHead(page), page);
 	/*
<span class="p_chunk">@@ -1137,7 +1154,14 @@</span> <span class="p_context"> struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
 	if ((flags &amp; FOLL_NUMA) &amp;&amp; pmd_protnone(*pmd))
 		goto out;
 
<span class="p_del">-	page = pmd_page(*pmd);</span>
<span class="p_add">+	if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+		swp_entry_t entry;</span>
<span class="p_add">+		entry = pmd_to_swp_entry(*pmd);</span>
<span class="p_add">+		page = pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+		if (!is_migration_entry(entry))</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+	} else</span>
<span class="p_add">+		page = pmd_page(*pmd);</span>
 	VM_BUG_ON_PAGE(!PageHead(page) &amp;&amp; !is_zone_device_page(page), page);
 	if (flags &amp; FOLL_TOUCH)
 		touch_pmd(vma, addr, pmd);
<span class="p_chunk">@@ -1332,6 +1356,9 @@</span> <span class="p_context"> bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 	if (is_huge_zero_pmd(orig_pmd))
 		goto out;
 
<span class="p_add">+	if (unlikely(!pmd_present(orig_pmd)))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
 	page = pmd_page(orig_pmd);
 	/*
 	 * If other processes are mapping this page, we couldn&#39;t discard
<span class="p_chunk">@@ -1410,20 +1437,35 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);
 	} else {
 		struct page *page = pmd_page(orig_pmd);
<span class="p_del">-		page_remove_rmap(page, true);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_del">-		if (PageAnon(page)) {</span>
<span class="p_del">-			pgtable_t pgtable;</span>
<span class="p_del">-			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="p_del">-			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="p_del">-			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="p_del">-			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+		int migration = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="p_add">+			page_remove_rmap(page, true);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+			if (PageAnon(page)) {</span>
<span class="p_add">+				pgtable_t pgtable;</span>
<span class="p_add">+				pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="p_add">+								      pmd);</span>
<span class="p_add">+				pte_free(tlb-&gt;mm, pgtable);</span>
<span class="p_add">+				atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="p_add">+				add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="p_add">+					       -HPAGE_PMD_NR);</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="p_add">+					       -HPAGE_PMD_NR);</span>
<span class="p_add">+			}</span>
 		} else {
<span class="p_del">-			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="p_add">+			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="p_add">+			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+			migration = 1;</span>
 		}
 		spin_unlock(ptl);
<span class="p_del">-		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="p_add">+		if (!migration)</span>
<span class="p_add">+			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
 	}
 	return 1;
 }
<span class="p_chunk">@@ -1496,14 +1538,27 @@</span> <span class="p_context"> int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 		bool preserve_write = prot_numa &amp;&amp; pmd_write(*pmd);
 		ret = 1;
 
<span class="p_add">+		if (!pmd_present(*pmd))</span>
<span class="p_add">+			goto unlock;</span>
 		/*
 		 * Avoid trapping faults against the zero page. The read-only
 		 * data is likely to be read-cached on the local CPU and
 		 * local/remote hits to the zero page are not interesting.
 		 */
<span class="p_del">-		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd)) {</span>
<span class="p_del">-			spin_unlock(ptl);</span>
<span class="p_del">-			return ret;</span>
<span class="p_add">+		if (prot_numa &amp;&amp; is_huge_zero_pmd(*pmd))</span>
<span class="p_add">+			goto unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (is_pmd_migration_entry(*pmd)) {</span>
<span class="p_add">+			swp_entry_t entry = pmd_to_swp_entry(*pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (is_write_migration_entry(entry)) {</span>
<span class="p_add">+				pmd_t newpmd;</span>
<span class="p_add">+</span>
<span class="p_add">+				make_migration_entry_read(&amp;entry);</span>
<span class="p_add">+				newpmd = swp_entry_to_pmd(entry);</span>
<span class="p_add">+				set_pmd_at(mm, addr, pmd, newpmd);</span>
<span class="p_add">+			}</span>
<span class="p_add">+			goto unlock;</span>
 		}
 
 		if (!prot_numa || !pmd_protnone(*pmd)) {
<span class="p_chunk">@@ -1516,6 +1571,7 @@</span> <span class="p_context"> int change_huge_pmd(struct vm_area_struct *vma, pmd_t *pmd,</span>
 			BUG_ON(vma_is_anonymous(vma) &amp;&amp; !preserve_write &amp;&amp;
 					pmd_write(entry));
 		}
<span class="p_add">+unlock:</span>
 		spin_unlock(ptl);
 	}
 
<span class="p_chunk">@@ -1532,7 +1588,7 @@</span> <span class="p_context"> spinlock_t *__pmd_trans_huge_lock(pmd_t *pmd, struct vm_area_struct *vma)</span>
 {
 	spinlock_t *ptl;
 	ptl = pmd_lock(vma-&gt;vm_mm, pmd);
<span class="p_del">-	if (likely(pmd_trans_huge(*pmd) || pmd_devmap(*pmd)))</span>
<span class="p_add">+	if (likely(pmd_related(*pmd)))</span>
 		return ptl;
 	spin_unlock(ptl);
 	return NULL;
<span class="p_header">diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/madvise.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/madvise.c</span>
<span class="p_header">index 0e3828e..eaa2b02 100644</span>
<span class="p_header">--- v4.9-rc2-mmotm-2016-10-27-18-27/mm/madvise.c</span>
<span class="p_header">+++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/madvise.c</span>
<span class="p_chunk">@@ -274,7 +274,7 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 	unsigned long next;
 
 	next = pmd_addr_end(addr, end);
<span class="p_del">-	if (pmd_trans_huge(*pmd))</span>
<span class="p_add">+	if (pmd_related(*pmd))</span>
 		if (madvise_free_huge_pmd(tlb, vma, pmd, addr, next))
 			goto next;
 
<span class="p_header">diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/memcontrol.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memcontrol.c</span>
<span class="p_header">index 91dfc7c..ebc2c42 100644</span>
<span class="p_header">--- v4.9-rc2-mmotm-2016-10-27-18-27/mm/memcontrol.c</span>
<span class="p_header">+++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memcontrol.c</span>
<span class="p_chunk">@@ -4635,6 +4635,8 @@</span> <span class="p_context"> static enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
 	struct page *page = NULL;
 	enum mc_target_type ret = MC_TARGET_NONE;
 
<span class="p_add">+	if (unlikely(!pmd_present(pmd)))</span>
<span class="p_add">+		return ret;</span>
 	page = pmd_page(pmd);
 	VM_BUG_ON_PAGE(!page || !PageHead(page), page);
 	if (!(mc.flags &amp; MOVE_ANON))
<span class="p_header">diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/memory.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memory.c</span>
<span class="p_header">index 94b5e2c..33fa439 100644</span>
<span class="p_header">--- v4.9-rc2-mmotm-2016-10-27-18-27/mm/memory.c</span>
<span class="p_header">+++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/memory.c</span>
<span class="p_chunk">@@ -999,7 +999,7 @@</span> <span class="p_context"> static inline int copy_pmd_range(struct mm_struct *dst_mm, struct mm_struct *src</span>
 	src_pmd = pmd_offset(src_pud, addr);
 	do {
 		next = pmd_addr_end(addr, end);
<span class="p_del">-		if (pmd_trans_huge(*src_pmd) || pmd_devmap(*src_pmd)) {</span>
<span class="p_add">+		if (pmd_related(*src_pmd)) {</span>
 			int err;
 			VM_BUG_ON(next-addr != HPAGE_PMD_SIZE);
 			err = copy_huge_pmd(dst_mm, src_mm,
<span class="p_chunk">@@ -3591,6 +3591,10 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 		int ret;
 
 		barrier();
<span class="p_add">+		if (unlikely(is_pmd_migration_entry(orig_pmd))) {</span>
<span class="p_add">+			pmd_migration_entry_wait(mm, fe.pmd);</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		}</span>
 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
 			if (pmd_protnone(orig_pmd) &amp;&amp; vma_is_accessible(vma))
 				return do_huge_pmd_numa_page(&amp;fe, orig_pmd);
<span class="p_header">diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/mprotect.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mprotect.c</span>
<span class="p_header">index c5ba2aa..81186e3 100644</span>
<span class="p_header">--- v4.9-rc2-mmotm-2016-10-27-18-27/mm/mprotect.c</span>
<span class="p_header">+++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mprotect.c</span>
<span class="p_chunk">@@ -164,6 +164,8 @@</span> <span class="p_context"> static inline unsigned long change_pmd_range(struct vm_area_struct *vma,</span>
 		unsigned long this_pages;
 
 		next = pmd_addr_end(addr, end);
<span class="p_add">+		if (!pmd_present(*pmd))</span>
<span class="p_add">+			continue;</span>
 		if (!pmd_trans_huge(*pmd) &amp;&amp; !pmd_devmap(*pmd)
 				&amp;&amp; pmd_none_or_clear_bad(pmd))
 			continue;
<span class="p_header">diff --git v4.9-rc2-mmotm-2016-10-27-18-27/mm/mremap.c v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mremap.c</span>
<span class="p_header">index da22ad2..a94a698 100644</span>
<span class="p_header">--- v4.9-rc2-mmotm-2016-10-27-18-27/mm/mremap.c</span>
<span class="p_header">+++ v4.9-rc2-mmotm-2016-10-27-18-27_patched/mm/mremap.c</span>
<span class="p_chunk">@@ -194,7 +194,7 @@</span> <span class="p_context"> unsigned long move_page_tables(struct vm_area_struct *vma,</span>
 		new_pmd = alloc_new_pmd(vma-&gt;vm_mm, vma, new_addr);
 		if (!new_pmd)
 			break;
<span class="p_del">-		if (pmd_trans_huge(*old_pmd)) {</span>
<span class="p_add">+		if (pmd_related(*old_pmd)) {</span>
 			if (extent == HPAGE_PMD_SIZE) {
 				bool moved;
 				/* See comment in move_ptes() */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



