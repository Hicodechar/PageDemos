
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v6,22/24] mm: Speculative page fault handler return VMA - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v6,22/24] mm: Speculative page fault handler return VMA</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=172187">willy@infradead.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 13, 2018, 4:23 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180113042354.GA24241@bombadil.infradead.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10162113/mbox/"
   >mbox</a>
|
   <a href="/patch/10162113/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10162113/">/patch/10162113/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C346B602D8 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 13 Jan 2018 04:24:33 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id ABBCC28A46
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 13 Jan 2018 04:24:33 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 9EB3028A52; Sat, 13 Jan 2018 04:24:33 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 028BB28A46
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat, 13 Jan 2018 04:24:33 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S965472AbeAMEY2 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 12 Jan 2018 23:24:28 -0500
Received: from bombadil.infradead.org ([65.50.211.133]:56578 &quot;EHLO
	bombadil.infradead.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S965407AbeAMEY0 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 12 Jan 2018 23:24:26 -0500
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
	d=infradead.org; s=bombadil.20170209;
	h=In-Reply-To:Content-Type:MIME-Version
	:References:Message-ID:Subject:Cc:To:From:Date:Sender:Reply-To:
	Content-Transfer-Encoding:Content-ID:Content-Description:Resent-Date:
	Resent-From:Resent-Sender:Resent-To:Resent-Cc:Resent-Message-ID:List-Id:
	List-Help:List-Unsubscribe:List-Subscribe:List-Post:List-Owner:List-Archive;
	bh=Ihg9MeNr56tIh0Ovm77A9oW7dRrq1/CxRbV8Pa8A3gA=;
	b=RUjVzyZDh1F+SNFRma6bvHV9U
	SgGnmRA5R9cDoTYPKUR1tOj/BJUZ/Qmpa9GYVh5V2I7pHHXYMpwxEIc1Xv1RGTQwm49ZWRF5DT/Iy
	nU8azEIu1YHLxRsBDauhh6q5TTZS3FPTRDf0AlHGISIf25H18pTHJZyVqm/WsvYKYidwUMXyMX0pj
	+YviwKqJs6AF384RMG8FRsrjRyyNIT/j2wQCQlLa/5MQxI29LqSnAooA26SdJZwm1vBRxJt3HZiut
	rgxKWLZzP8BcTUEtP34UlUaEys/UH2FaIw6NysDzd7a7wePd55xc2f6sLY1VRKR9Xyd5tMaZX36VA
	x3NZeyEkA==;
Received: from willy by bombadil.infradead.org with local (Exim 4.89 #1 (Red
	Hat Linux)) id 1eaDMI-0005Da-Aa; Sat, 13 Jan 2018 04:23:54 +0000
Date: Fri, 12 Jan 2018 20:23:54 -0800
From: Matthew Wilcox &lt;willy@infradead.org&gt;
To: Laurent Dufour &lt;ldufour@linux.vnet.ibm.com&gt;
Cc: paulmck@linux.vnet.ibm.com, peterz@infradead.org,
	akpm@linux-foundation.org, kirill@shutemov.name,
	ak@linux.intel.com, mhocko@kernel.org, dave@stgolabs.net,
	jack@suse.cz, benh@kernel.crashing.org, mpe@ellerman.id.au,
	paulus@samba.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Ingo Molnar &lt;mingo@redhat.com&gt;, hpa@zytor.com,
	Will Deacon &lt;will.deacon@arm.com&gt;,
	Sergey Senozhatsky &lt;sergey.senozhatsky@gmail.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Alexei Starovoitov &lt;alexei.starovoitov@gmail.com&gt;,
	kemi.wang@intel.com, sergey.senozhatsky.work@gmail.com,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	haren@linux.vnet.ibm.com, khandual@linux.vnet.ibm.com,
	npiggin@gmail.com, bsingharora@gmail.com,
	Tim Chen &lt;tim.c.chen@linux.intel.com&gt;,
	linuxppc-dev@lists.ozlabs.org, x86@kernel.org
Subject: Re: [PATCH v6 22/24] mm: Speculative page fault handler return VMA
Message-ID: &lt;20180113042354.GA24241@bombadil.infradead.org&gt;
References: &lt;1515777968-867-1-git-send-email-ldufour@linux.vnet.ibm.com&gt;
	&lt;1515777968-867-23-git-send-email-ldufour@linux.vnet.ibm.com&gt;
	&lt;20180112190251.GC7590@bombadil.infradead.org&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20180112190251.GC7590@bombadil.infradead.org&gt;
User-Agent: Mutt/1.9.1 (2017-09-22)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=172187">willy@infradead.org</a> - Jan. 13, 2018, 4:23 a.m.</div>
<pre class="content">
On Fri, Jan 12, 2018 at 11:02:51AM -0800, Matthew Wilcox wrote:
<span class="quote">&gt; On Fri, Jan 12, 2018 at 06:26:06PM +0100, Laurent Dufour wrote:</span>
<span class="quote">&gt; &gt; @@ -1354,7 +1354,10 @@ extern int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; &gt;  		unsigned int flags);</span>
<span class="quote">&gt; &gt;  #ifdef CONFIG_SPF</span>
<span class="quote">&gt; &gt;  extern int handle_speculative_fault(struct mm_struct *mm,</span>
<span class="quote">&gt; &gt; +				    unsigned long address, unsigned int flags,</span>
<span class="quote">&gt; &gt; +				    struct vm_area_struct **vma);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think this shows that we need to create &#39;struct vm_fault&#39; on the stack</span>
<span class="quote">&gt; in the arch code and then pass it to handle_speculative_fault(), followed</span>
<span class="quote">&gt; by handle_mm_fault().  That should be quite a nice cleanup actually.</span>
<span class="quote">&gt; I know that&#39;s only 30+ architectures to change ;-)</span>

Of course, we don&#39;t need to change them all.  Try this:

Subject: [PATCH] Add vm_handle_fault

For the speculative fault handler, we want to create the struct vm_fault
on the stack in the arch code and pass it into the generic mm code.
To avoid changing 30+ architectures, leave handle_mm_fault with its
current function signature and move its guts into the new vm_handle_fault
function.  Even this saves a nice 172 bytes on the random x86-64 .config
I happen to have around.
<span class="signed-off-by">
Signed-off-by: Matthew Wilcox &lt;mawilcox@microsoft.com&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=89031">Laurent Dufour</a> - Jan. 16, 2018, 2:47 p.m.</div>
<pre class="content">
On 13/01/2018 05:23, Matthew Wilcox wrote:
<span class="quote">&gt; On Fri, Jan 12, 2018 at 11:02:51AM -0800, Matthew Wilcox wrote:</span>
<span class="quote">&gt;&gt; On Fri, Jan 12, 2018 at 06:26:06PM +0100, Laurent Dufour wrote:</span>
<span class="quote">&gt;&gt;&gt; @@ -1354,7 +1354,10 @@ extern int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;&gt;&gt;  		unsigned int flags);</span>
<span class="quote">&gt;&gt;&gt;  #ifdef CONFIG_SPF</span>
<span class="quote">&gt;&gt;&gt;  extern int handle_speculative_fault(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt;&gt; +				    unsigned long address, unsigned int flags,</span>
<span class="quote">&gt;&gt;&gt; +				    struct vm_area_struct **vma);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think this shows that we need to create &#39;struct vm_fault&#39; on the stack</span>
<span class="quote">&gt;&gt; in the arch code and then pass it to handle_speculative_fault(), followed</span>
<span class="quote">&gt;&gt; by handle_mm_fault().  That should be quite a nice cleanup actually.</span>
<span class="quote">&gt;&gt; I know that&#39;s only 30+ architectures to change ;-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Of course, we don&#39;t need to change them all.  Try this:</span>

That would be good candidate for a clean up but I&#39;m not sure this should be
part of this already too long series.

If you don&#39;t mind, unless a global agreement is stated on that, I&#39;d prefer
to postpone such a change once the initial series is accepted.

Cheers,
Laurent.
<span class="quote">
&gt; Subject: [PATCH] Add vm_handle_fault</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For the speculative fault handler, we want to create the struct vm_fault</span>
<span class="quote">&gt; on the stack in the arch code and pass it into the generic mm code.</span>
<span class="quote">&gt; To avoid changing 30+ architectures, leave handle_mm_fault with its</span>
<span class="quote">&gt; current function signature and move its guts into the new vm_handle_fault</span>
<span class="quote">&gt; function.  Even this saves a nice 172 bytes on the random x86-64 .config</span>
<span class="quote">&gt; I happen to have around.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Matthew Wilcox &lt;mawilcox@microsoft.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="quote">&gt; index 5eb3d2524bdc..403934297a3d 100644</span>
<span class="quote">&gt; --- a/mm/memory.c</span>
<span class="quote">&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt; @@ -3977,36 +3977,28 @@ static int handle_pte_fault(struct vm_fault *vmf)</span>
<span class="quote">&gt;   * The mmap_sem may have been released depending on flags and our</span>
<span class="quote">&gt;   * return value.  See filemap_fault() and __lock_page_or_retry().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; -		unsigned int flags)</span>
<span class="quote">&gt; +static int __handle_mm_fault(struct vm_fault *vmf)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct vm_fault vmf = {</span>
<span class="quote">&gt; -		.vma = vma,</span>
<span class="quote">&gt; -		.address = address &amp; PAGE_MASK,</span>
<span class="quote">&gt; -		.flags = flags,</span>
<span class="quote">&gt; -		.pgoff = linear_page_index(vma, address),</span>
<span class="quote">&gt; -		.gfp_mask = __get_fault_gfp_mask(vma),</span>
<span class="quote">&gt; -	};</span>
<span class="quote">&gt; -	unsigned int dirty = flags &amp; FAULT_FLAG_WRITE;</span>
<span class="quote">&gt; -	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned int dirty = vmf-&gt;flags &amp; FAULT_FLAG_WRITE;</span>
<span class="quote">&gt; +	struct mm_struct *mm = vmf-&gt;vma-&gt;vm_mm;</span>
<span class="quote">&gt;  	pgd_t *pgd;</span>
<span class="quote">&gt;  	p4d_t *p4d;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	pgd = pgd_offset(mm, address);</span>
<span class="quote">&gt; -	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="quote">&gt; +	pgd = pgd_offset(mm, vmf-&gt;address);</span>
<span class="quote">&gt; +	p4d = p4d_alloc(mm, pgd, vmf-&gt;address);</span>
<span class="quote">&gt;  	if (!p4d)</span>
<span class="quote">&gt;  		return VM_FAULT_OOM;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	vmf.pud = pud_alloc(mm, p4d, address);</span>
<span class="quote">&gt; -	if (!vmf.pud)</span>
<span class="quote">&gt; +	vmf-&gt;pud = pud_alloc(mm, p4d, vmf-&gt;address);</span>
<span class="quote">&gt; +	if (!vmf-&gt;pud)</span>
<span class="quote">&gt;  		return VM_FAULT_OOM;</span>
<span class="quote">&gt; -	if (pud_none(*vmf.pud) &amp;&amp; transparent_hugepage_enabled(vma)) {</span>
<span class="quote">&gt; -		ret = create_huge_pud(&amp;vmf);</span>
<span class="quote">&gt; +	if (pud_none(*vmf-&gt;pud) &amp;&amp; transparent_hugepage_enabled(vmf-&gt;vma)) {</span>
<span class="quote">&gt; +		ret = create_huge_pud(vmf);</span>
<span class="quote">&gt;  		if (!(ret &amp; VM_FAULT_FALLBACK))</span>
<span class="quote">&gt;  			return ret;</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt; -		pud_t orig_pud = *vmf.pud;</span>
<span class="quote">&gt; +		pud_t orig_pud = *vmf-&gt;pud;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		barrier();</span>
<span class="quote">&gt;  		if (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {</span>
<span class="quote">&gt; @@ -4014,50 +4006,51 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  			/* NUMA case for anonymous PUDs would go here */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  			if (dirty &amp;&amp; !pud_access_permitted(orig_pud, WRITE)) {</span>
<span class="quote">&gt; -				ret = wp_huge_pud(&amp;vmf, orig_pud);</span>
<span class="quote">&gt; +				ret = wp_huge_pud(vmf, orig_pud);</span>
<span class="quote">&gt;  				if (!(ret &amp; VM_FAULT_FALLBACK))</span>
<span class="quote">&gt;  					return ret;</span>
<span class="quote">&gt;  			} else {</span>
<span class="quote">&gt; -				huge_pud_set_accessed(&amp;vmf, orig_pud);</span>
<span class="quote">&gt; +				huge_pud_set_accessed(vmf, orig_pud);</span>
<span class="quote">&gt;  				return 0;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	vmf.pmd = pmd_alloc(mm, vmf.pud, address);</span>
<span class="quote">&gt; -	if (!vmf.pmd)</span>
<span class="quote">&gt; +	vmf-&gt;pmd = pmd_alloc(mm, vmf-&gt;pud, vmf-&gt;address);</span>
<span class="quote">&gt; +	if (!vmf-&gt;pmd)</span>
<span class="quote">&gt;  		return VM_FAULT_OOM;</span>
<span class="quote">&gt; -	if (pmd_none(*vmf.pmd) &amp;&amp; transparent_hugepage_enabled(vma)) {</span>
<span class="quote">&gt; -		ret = create_huge_pmd(&amp;vmf);</span>
<span class="quote">&gt; +	if (pmd_none(*vmf-&gt;pmd) &amp;&amp; transparent_hugepage_enabled(vmf-&gt;vma)) {</span>
<span class="quote">&gt; +		ret = create_huge_pmd(vmf);</span>
<span class="quote">&gt;  		if (!(ret &amp; VM_FAULT_FALLBACK))</span>
<span class="quote">&gt;  			return ret;</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt; -		pmd_t orig_pmd = *vmf.pmd;</span>
<span class="quote">&gt; +		pmd_t orig_pmd = *vmf-&gt;pmd;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  		barrier();</span>
<span class="quote">&gt;  		if (unlikely(is_swap_pmd(orig_pmd))) {</span>
<span class="quote">&gt;  			VM_BUG_ON(thp_migration_supported() &amp;&amp;</span>
<span class="quote">&gt;  					  !is_pmd_migration_entry(orig_pmd));</span>
<span class="quote">&gt;  			if (is_pmd_migration_entry(orig_pmd))</span>
<span class="quote">&gt; -				pmd_migration_entry_wait(mm, vmf.pmd);</span>
<span class="quote">&gt; +				pmd_migration_entry_wait(mm, vmf-&gt;pmd);</span>
<span class="quote">&gt;  			return 0;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {</span>
<span class="quote">&gt; -			if (pmd_protnone(orig_pmd) &amp;&amp; vma_is_accessible(vma))</span>
<span class="quote">&gt; -				return do_huge_pmd_numa_page(&amp;vmf, orig_pmd);</span>
<span class="quote">&gt; +			if (pmd_protnone(orig_pmd) &amp;&amp;</span>
<span class="quote">&gt; +						vma_is_accessible(vmf-&gt;vma))</span>
<span class="quote">&gt; +				return do_huge_pmd_numa_page(vmf, orig_pmd);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  			if (dirty &amp;&amp; !pmd_access_permitted(orig_pmd, WRITE)) {</span>
<span class="quote">&gt; -				ret = wp_huge_pmd(&amp;vmf, orig_pmd);</span>
<span class="quote">&gt; +				ret = wp_huge_pmd(vmf, orig_pmd);</span>
<span class="quote">&gt;  				if (!(ret &amp; VM_FAULT_FALLBACK))</span>
<span class="quote">&gt;  					return ret;</span>
<span class="quote">&gt;  			} else {</span>
<span class="quote">&gt; -				huge_pmd_set_accessed(&amp;vmf, orig_pmd);</span>
<span class="quote">&gt; +				huge_pmd_set_accessed(vmf, orig_pmd);</span>
<span class="quote">&gt;  				return 0;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; -	return handle_pte_fault(&amp;vmf);</span>
<span class="quote">&gt; +	return handle_pte_fault(vmf);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -4066,9 +4059,10 @@ static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;   * The mmap_sem may have been released depending on flags and our</span>
<span class="quote">&gt;   * return value.  See filemap_fault() and __lock_page_or_retry().</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; -		unsigned int flags)</span>
<span class="quote">&gt; +int vm_handle_fault(struct vm_fault *vmf)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	unsigned int flags = vmf-&gt;flags;</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = vmf-&gt;vma;</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	__set_current_state(TASK_RUNNING);</span>
<span class="quote">&gt; @@ -4092,9 +4086,9 @@ int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  		mem_cgroup_oom_enable();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (unlikely(is_vm_hugetlb_page(vma)))</span>
<span class="quote">&gt; -		ret = hugetlb_fault(vma-&gt;vm_mm, vma, address, flags);</span>
<span class="quote">&gt; +		ret = hugetlb_fault(vma-&gt;vm_mm, vma, vmf-&gt;address, flags);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; -		ret = __handle_mm_fault(vma, address, flags);</span>
<span class="quote">&gt; +		ret = __handle_mm_fault(vmf);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	if (flags &amp; FAULT_FLAG_USER) {</span>
<span class="quote">&gt;  		mem_cgroup_oom_disable();</span>
<span class="quote">&gt; @@ -4110,6 +4104,26 @@ int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * By the time we get here, we already hold the mm semaphore</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The mmap_sem may have been released depending on flags and our</span>
<span class="quote">&gt; + * return value.  See filemap_fault() and __lock_page_or_retry().</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; +		unsigned int flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_fault vmf = {</span>
<span class="quote">&gt; +		.vma = vma,</span>
<span class="quote">&gt; +		.address = address &amp; PAGE_MASK,</span>
<span class="quote">&gt; +		.flags = flags,</span>
<span class="quote">&gt; +		.pgoff = linear_page_index(vma, address),</span>
<span class="quote">&gt; +		.gfp_mask = __get_fault_gfp_mask(vma),</span>
<span class="quote">&gt; +	};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return vm_handle_fault(&amp;vmf);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(handle_mm_fault);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  #ifndef __PAGETABLE_P4D_FOLDED</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=172187">willy@infradead.org</a> - Jan. 16, 2018, 2:58 p.m.</div>
<pre class="content">
On Tue, Jan 16, 2018 at 03:47:51PM +0100, Laurent Dufour wrote:
<span class="quote">&gt; On 13/01/2018 05:23, Matthew Wilcox wrote:</span>
<span class="quote">&gt; &gt; Of course, we don&#39;t need to change them all.  Try this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That would be good candidate for a clean up but I&#39;m not sure this should be</span>
<span class="quote">&gt; part of this already too long series.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you don&#39;t mind, unless a global agreement is stated on that, I&#39;d prefer</span>
<span class="quote">&gt; to postpone such a change once the initial series is accepted.</span>

Actually, I think this can go in first, independently of the speculative
fault series.  It&#39;s a win in memory savings, and probably shaves a
cycle or two off the fault handler due to less argument marshalling in
the call-stack.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 5eb3d2524bdc..403934297a3d 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -3977,36 +3977,28 @@</span> <span class="p_context"> static int handle_pte_fault(struct vm_fault *vmf)</span>
  * The mmap_sem may have been released depending on flags and our
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
<span class="p_del">-static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-		unsigned int flags)</span>
<span class="p_add">+static int __handle_mm_fault(struct vm_fault *vmf)</span>
 {
<span class="p_del">-	struct vm_fault vmf = {</span>
<span class="p_del">-		.vma = vma,</span>
<span class="p_del">-		.address = address &amp; PAGE_MASK,</span>
<span class="p_del">-		.flags = flags,</span>
<span class="p_del">-		.pgoff = linear_page_index(vma, address),</span>
<span class="p_del">-		.gfp_mask = __get_fault_gfp_mask(vma),</span>
<span class="p_del">-	};</span>
<span class="p_del">-	unsigned int dirty = flags &amp; FAULT_FLAG_WRITE;</span>
<span class="p_del">-	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned int dirty = vmf-&gt;flags &amp; FAULT_FLAG_WRITE;</span>
<span class="p_add">+	struct mm_struct *mm = vmf-&gt;vma-&gt;vm_mm;</span>
 	pgd_t *pgd;
 	p4d_t *p4d;
 	int ret;
 
<span class="p_del">-	pgd = pgd_offset(mm, address);</span>
<span class="p_del">-	p4d = p4d_alloc(mm, pgd, address);</span>
<span class="p_add">+	pgd = pgd_offset(mm, vmf-&gt;address);</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, vmf-&gt;address);</span>
 	if (!p4d)
 		return VM_FAULT_OOM;
 
<span class="p_del">-	vmf.pud = pud_alloc(mm, p4d, address);</span>
<span class="p_del">-	if (!vmf.pud)</span>
<span class="p_add">+	vmf-&gt;pud = pud_alloc(mm, p4d, vmf-&gt;address);</span>
<span class="p_add">+	if (!vmf-&gt;pud)</span>
 		return VM_FAULT_OOM;
<span class="p_del">-	if (pud_none(*vmf.pud) &amp;&amp; transparent_hugepage_enabled(vma)) {</span>
<span class="p_del">-		ret = create_huge_pud(&amp;vmf);</span>
<span class="p_add">+	if (pud_none(*vmf-&gt;pud) &amp;&amp; transparent_hugepage_enabled(vmf-&gt;vma)) {</span>
<span class="p_add">+		ret = create_huge_pud(vmf);</span>
 		if (!(ret &amp; VM_FAULT_FALLBACK))
 			return ret;
 	} else {
<span class="p_del">-		pud_t orig_pud = *vmf.pud;</span>
<span class="p_add">+		pud_t orig_pud = *vmf-&gt;pud;</span>
 
 		barrier();
 		if (pud_trans_huge(orig_pud) || pud_devmap(orig_pud)) {
<span class="p_chunk">@@ -4014,50 +4006,51 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 			/* NUMA case for anonymous PUDs would go here */
 
 			if (dirty &amp;&amp; !pud_access_permitted(orig_pud, WRITE)) {
<span class="p_del">-				ret = wp_huge_pud(&amp;vmf, orig_pud);</span>
<span class="p_add">+				ret = wp_huge_pud(vmf, orig_pud);</span>
 				if (!(ret &amp; VM_FAULT_FALLBACK))
 					return ret;
 			} else {
<span class="p_del">-				huge_pud_set_accessed(&amp;vmf, orig_pud);</span>
<span class="p_add">+				huge_pud_set_accessed(vmf, orig_pud);</span>
 				return 0;
 			}
 		}
 	}
 
<span class="p_del">-	vmf.pmd = pmd_alloc(mm, vmf.pud, address);</span>
<span class="p_del">-	if (!vmf.pmd)</span>
<span class="p_add">+	vmf-&gt;pmd = pmd_alloc(mm, vmf-&gt;pud, vmf-&gt;address);</span>
<span class="p_add">+	if (!vmf-&gt;pmd)</span>
 		return VM_FAULT_OOM;
<span class="p_del">-	if (pmd_none(*vmf.pmd) &amp;&amp; transparent_hugepage_enabled(vma)) {</span>
<span class="p_del">-		ret = create_huge_pmd(&amp;vmf);</span>
<span class="p_add">+	if (pmd_none(*vmf-&gt;pmd) &amp;&amp; transparent_hugepage_enabled(vmf-&gt;vma)) {</span>
<span class="p_add">+		ret = create_huge_pmd(vmf);</span>
 		if (!(ret &amp; VM_FAULT_FALLBACK))
 			return ret;
 	} else {
<span class="p_del">-		pmd_t orig_pmd = *vmf.pmd;</span>
<span class="p_add">+		pmd_t orig_pmd = *vmf-&gt;pmd;</span>
 
 		barrier();
 		if (unlikely(is_swap_pmd(orig_pmd))) {
 			VM_BUG_ON(thp_migration_supported() &amp;&amp;
 					  !is_pmd_migration_entry(orig_pmd));
 			if (is_pmd_migration_entry(orig_pmd))
<span class="p_del">-				pmd_migration_entry_wait(mm, vmf.pmd);</span>
<span class="p_add">+				pmd_migration_entry_wait(mm, vmf-&gt;pmd);</span>
 			return 0;
 		}
 		if (pmd_trans_huge(orig_pmd) || pmd_devmap(orig_pmd)) {
<span class="p_del">-			if (pmd_protnone(orig_pmd) &amp;&amp; vma_is_accessible(vma))</span>
<span class="p_del">-				return do_huge_pmd_numa_page(&amp;vmf, orig_pmd);</span>
<span class="p_add">+			if (pmd_protnone(orig_pmd) &amp;&amp;</span>
<span class="p_add">+						vma_is_accessible(vmf-&gt;vma))</span>
<span class="p_add">+				return do_huge_pmd_numa_page(vmf, orig_pmd);</span>
 
 			if (dirty &amp;&amp; !pmd_access_permitted(orig_pmd, WRITE)) {
<span class="p_del">-				ret = wp_huge_pmd(&amp;vmf, orig_pmd);</span>
<span class="p_add">+				ret = wp_huge_pmd(vmf, orig_pmd);</span>
 				if (!(ret &amp; VM_FAULT_FALLBACK))
 					return ret;
 			} else {
<span class="p_del">-				huge_pmd_set_accessed(&amp;vmf, orig_pmd);</span>
<span class="p_add">+				huge_pmd_set_accessed(vmf, orig_pmd);</span>
 				return 0;
 			}
 		}
 	}
 
<span class="p_del">-	return handle_pte_fault(&amp;vmf);</span>
<span class="p_add">+	return handle_pte_fault(vmf);</span>
 }
 
 /*
<span class="p_chunk">@@ -4066,9 +4059,10 @@</span> <span class="p_context"> static int __handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
  * The mmap_sem may have been released depending on flags and our
  * return value.  See filemap_fault() and __lock_page_or_retry().
  */
<span class="p_del">-int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-		unsigned int flags)</span>
<span class="p_add">+int vm_handle_fault(struct vm_fault *vmf)</span>
 {
<span class="p_add">+	unsigned int flags = vmf-&gt;flags;</span>
<span class="p_add">+	struct vm_area_struct *vma = vmf-&gt;vma;</span>
 	int ret;
 
 	__set_current_state(TASK_RUNNING);
<span class="p_chunk">@@ -4092,9 +4086,9 @@</span> <span class="p_context"> int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 		mem_cgroup_oom_enable();
 
 	if (unlikely(is_vm_hugetlb_page(vma)))
<span class="p_del">-		ret = hugetlb_fault(vma-&gt;vm_mm, vma, address, flags);</span>
<span class="p_add">+		ret = hugetlb_fault(vma-&gt;vm_mm, vma, vmf-&gt;address, flags);</span>
 	else
<span class="p_del">-		ret = __handle_mm_fault(vma, address, flags);</span>
<span class="p_add">+		ret = __handle_mm_fault(vmf);</span>
 
 	if (flags &amp; FAULT_FLAG_USER) {
 		mem_cgroup_oom_disable();
<span class="p_chunk">@@ -4110,6 +4104,26 @@</span> <span class="p_context"> int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
 
 	return ret;
 }
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * By the time we get here, we already hold the mm semaphore</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The mmap_sem may have been released depending on flags and our</span>
<span class="p_add">+ * return value.  See filemap_fault() and __lock_page_or_retry().</span>
<span class="p_add">+ */</span>
<span class="p_add">+int handle_mm_fault(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_add">+		unsigned int flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_fault vmf = {</span>
<span class="p_add">+		.vma = vma,</span>
<span class="p_add">+		.address = address &amp; PAGE_MASK,</span>
<span class="p_add">+		.flags = flags,</span>
<span class="p_add">+		.pgoff = linear_page_index(vma, address),</span>
<span class="p_add">+		.gfp_mask = __get_fault_gfp_mask(vma),</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	return vm_handle_fault(&amp;vmf);</span>
<span class="p_add">+}</span>
 EXPORT_SYMBOL_GPL(handle_mm_fault);
 
 #ifndef __PAGETABLE_P4D_FOLDED

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



