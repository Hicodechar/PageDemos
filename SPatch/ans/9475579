
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[03/11] powerpc/kvm: Gather HPT related variables into sub-structure - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [03/11] powerpc/kvm: Gather HPT related variables into sub-structure</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=1741">David Gibson</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 15, 2016, 5:53 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20161215055404.29351-4-david@gibson.dropbear.id.au&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9475579/mbox/"
   >mbox</a>
|
   <a href="/patch/9475579/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9475579/">/patch/9475579/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	62A246047D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Dec 2016 06:02:11 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 511F5286AA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Dec 2016 06:02:11 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 459D6286B5; Thu, 15 Dec 2016 06:02:11 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id ED527286AD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Dec 2016 06:02:09 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S935281AbcLOGBh (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 15 Dec 2016 01:01:37 -0500
Received: from ozlabs.org ([103.22.144.67]:59869 &quot;EHLO ozlabs.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1755197AbcLOF6v (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 15 Dec 2016 00:58:51 -0500
Received: by ozlabs.org (Postfix, from userid 1007)
	id 3tfN6Y1BvWz9t3K; Thu, 15 Dec 2016 16:58:44 +1100 (AEDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple;
	d=gibson.dropbear.id.au; s=201602; t=1481781525;
	bh=ZqTpk8gNZHqRv6WpzEr2Aa17ID8PF9GtncIKiqs3H7w=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=pMGCrs1bStavJDLlfMEteYubuGbIWapSvZQi78XJNOE3RmnhKKzU2DLIyk1E0m3dt
	TdlhTwCy4/ZCNr9B3BYG4Tvq/vJE+mvJI9SXYE9sue5+LEKPAio6LkBN7JLqTS5wZf
	iHpCeXPXQJ11SIy1Q9P+8CEirmKG7gBKodpXRDtU=
From: David Gibson &lt;david@gibson.dropbear.id.au&gt;
To: paulus@samba.org
Cc: michael@ellerman.id.au, benh@kernel.crashing.org,
	sjitindarsingh@gmail.com, thuth@redhat.com, lvivier@redhat.com,
	linuxppc-dev@lists.ozlabs.org, linux-kernel@vger.kernel.org,
	kvm@vger.kernel.org, David Gibson &lt;david@gibson.dropbear.id.au&gt;
Subject: [PATCH 03/11] powerpc/kvm: Gather HPT related variables into
	sub-structure
Date: Thu, 15 Dec 2016 16:53:56 +1100
Message-Id: &lt;20161215055404.29351-4-david@gibson.dropbear.id.au&gt;
X-Mailer: git-send-email 2.9.3
In-Reply-To: &lt;20161215055404.29351-1-david@gibson.dropbear.id.au&gt;
References: &lt;20161215055404.29351-1-david@gibson.dropbear.id.au&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1741">David Gibson</a> - Dec. 15, 2016, 5:53 a.m.</div>
<pre class="content">
Currently, the powerpc kvm_arch structure contains a number of variables
tracking the state of the guest&#39;s hashed page table (HPT) in KVM HV.  This
patch gathers them all together into a single kvm_hpt_info substructure.
This makes life more convenient for the upcoming HPT resizing
implementation.
<span class="signed-off-by">
Signed-off-by: David Gibson &lt;david@gibson.dropbear.id.au&gt;</span>
---
 arch/powerpc/include/asm/kvm_host.h | 16 ++++---
 arch/powerpc/kvm/book3s_64_mmu_hv.c | 90 ++++++++++++++++++-------------------
 arch/powerpc/kvm/book3s_hv.c        |  2 +-
 arch/powerpc/kvm/book3s_hv_rm_mmu.c | 62 ++++++++++++-------------
 4 files changed, 87 insertions(+), 83 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=129511">Thomas Huth</a> - Dec. 16, 2016, 9:24 a.m.</div>
<pre class="content">
On 15.12.2016 06:53, David Gibson wrote:
<span class="quote">&gt; Currently, the powerpc kvm_arch structure contains a number of variables</span>
<span class="quote">&gt; tracking the state of the guest&#39;s hashed page table (HPT) in KVM HV.  This</span>
<span class="quote">&gt; patch gathers them all together into a single kvm_hpt_info substructure.</span>
<span class="quote">&gt; This makes life more convenient for the upcoming HPT resizing</span>
<span class="quote">&gt; implementation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: David Gibson &lt;david@gibson.dropbear.id.au&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/powerpc/include/asm/kvm_host.h | 16 ++++---</span>
<span class="quote">&gt;  arch/powerpc/kvm/book3s_64_mmu_hv.c | 90 ++++++++++++++++++-------------------</span>
<span class="quote">&gt;  arch/powerpc/kvm/book3s_hv.c        |  2 +-</span>
<span class="quote">&gt;  arch/powerpc/kvm/book3s_hv_rm_mmu.c | 62 ++++++++++++-------------</span>
<span class="quote">&gt;  4 files changed, 87 insertions(+), 83 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h</span>
<span class="quote">&gt; index e59b172..2673271 100644</span>
<span class="quote">&gt; --- a/arch/powerpc/include/asm/kvm_host.h</span>
<span class="quote">&gt; +++ b/arch/powerpc/include/asm/kvm_host.h</span>
<span class="quote">&gt; @@ -241,12 +241,20 @@ struct kvm_arch_memory_slot {</span>
<span class="quote">&gt;  #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +struct kvm_hpt_info {</span>
<span class="quote">&gt; +	unsigned long virt;</span>
<span class="quote">&gt; +	struct revmap_entry *rev;</span>
<span class="quote">&gt; +	unsigned long npte;</span>
<span class="quote">&gt; +	unsigned long mask;</span>
<span class="quote">&gt; +	u32 order;</span>
<span class="quote">&gt; +	int cma;</span>
<span class="quote">&gt; +};</span>

While you&#39;re at it, it would be really great if you could add a comment
at the end of each line with a short description of what the variables
are about. E.g. if I just read &quot;virt&quot; and do not have much clue of the
code yet, I have a hard time to figure out what this means...

 Thomas
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1741">David Gibson</a> - Dec. 19, 2016, 12:03 a.m.</div>
<pre class="content">
On Fri, Dec 16, 2016 at 10:24:17AM +0100, Thomas Huth wrote:
<span class="quote">&gt; On 15.12.2016 06:53, David Gibson wrote:</span>
<span class="quote">&gt; &gt; Currently, the powerpc kvm_arch structure contains a number of variables</span>
<span class="quote">&gt; &gt; tracking the state of the guest&#39;s hashed page table (HPT) in KVM HV.  This</span>
<span class="quote">&gt; &gt; patch gathers them all together into a single kvm_hpt_info substructure.</span>
<span class="quote">&gt; &gt; This makes life more convenient for the upcoming HPT resizing</span>
<span class="quote">&gt; &gt; implementation.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: David Gibson &lt;david@gibson.dropbear.id.au&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  arch/powerpc/include/asm/kvm_host.h | 16 ++++---</span>
<span class="quote">&gt; &gt;  arch/powerpc/kvm/book3s_64_mmu_hv.c | 90 ++++++++++++++++++-------------------</span>
<span class="quote">&gt; &gt;  arch/powerpc/kvm/book3s_hv.c        |  2 +-</span>
<span class="quote">&gt; &gt;  arch/powerpc/kvm/book3s_hv_rm_mmu.c | 62 ++++++++++++-------------</span>
<span class="quote">&gt; &gt;  4 files changed, 87 insertions(+), 83 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h</span>
<span class="quote">&gt; &gt; index e59b172..2673271 100644</span>
<span class="quote">&gt; &gt; --- a/arch/powerpc/include/asm/kvm_host.h</span>
<span class="quote">&gt; &gt; +++ b/arch/powerpc/include/asm/kvm_host.h</span>
<span class="quote">&gt; &gt; @@ -241,12 +241,20 @@ struct kvm_arch_memory_slot {</span>
<span class="quote">&gt; &gt;  #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +struct kvm_hpt_info {</span>
<span class="quote">&gt; &gt; +	unsigned long virt;</span>
<span class="quote">&gt; &gt; +	struct revmap_entry *rev;</span>
<span class="quote">&gt; &gt; +	unsigned long npte;</span>
<span class="quote">&gt; &gt; +	unsigned long mask;</span>
<span class="quote">&gt; &gt; +	u32 order;</span>
<span class="quote">&gt; &gt; +	int cma;</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; While you&#39;re at it, it would be really great if you could add a comment</span>
<span class="quote">&gt; at the end of each line with a short description of what the variables</span>
<span class="quote">&gt; are about. E.g. if I just read &quot;virt&quot; and do not have much clue of the</span>
<span class="quote">&gt; code yet, I have a hard time to figure out what this means...</span>

Good idea, done.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/powerpc/include/asm/kvm_host.h b/arch/powerpc/include/asm/kvm_host.h</span>
<span class="p_header">index e59b172..2673271 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/kvm_host.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/kvm_host.h</span>
<span class="p_chunk">@@ -241,12 +241,20 @@</span> <span class="p_context"> struct kvm_arch_memory_slot {</span>
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
 };
 
<span class="p_add">+struct kvm_hpt_info {</span>
<span class="p_add">+	unsigned long virt;</span>
<span class="p_add">+	struct revmap_entry *rev;</span>
<span class="p_add">+	unsigned long npte;</span>
<span class="p_add">+	unsigned long mask;</span>
<span class="p_add">+	u32 order;</span>
<span class="p_add">+	int cma;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 struct kvm_arch {
 	unsigned int lpid;
 #ifdef CONFIG_KVM_BOOK3S_HV_POSSIBLE
 	unsigned int tlb_sets;
<span class="p_del">-	unsigned long hpt_virt;</span>
<span class="p_del">-	struct revmap_entry *revmap;</span>
<span class="p_add">+	struct kvm_hpt_info hpt;</span>
 	atomic64_t mmio_update;
 	unsigned int host_lpid;
 	unsigned long host_lpcr;
<span class="p_chunk">@@ -256,14 +264,10 @@</span> <span class="p_context"> struct kvm_arch {</span>
 	unsigned long lpcr;
 	unsigned long vrma_slb_v;
 	int hpte_setup_done;
<span class="p_del">-	u32 hpt_order;</span>
 	atomic_t vcpus_running;
 	u32 online_vcores;
<span class="p_del">-	unsigned long hpt_npte;</span>
<span class="p_del">-	unsigned long hpt_mask;</span>
 	atomic_t hpte_mod_interest;
 	cpumask_t need_tlb_flush;
<span class="p_del">-	int hpt_cma_alloc;</span>
 	struct dentry *debugfs_dir;
 	struct dentry *htab_dentry;
 #endif /* CONFIG_KVM_BOOK3S_HV_POSSIBLE */
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">index ae17cdd..b5799d1 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_chunk">@@ -61,12 +61,12 @@</span> <span class="p_context"> long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp)</span>
 			order = PPC_MIN_HPT_ORDER;
 	}
 
<span class="p_del">-	kvm-&gt;arch.hpt_cma_alloc = 0;</span>
<span class="p_add">+	kvm-&gt;arch.hpt.cma = 0;</span>
 	page = kvm_alloc_hpt_cma(1ul &lt;&lt; (order - PAGE_SHIFT));
 	if (page) {
 		hpt = (unsigned long)pfn_to_kaddr(page_to_pfn(page));
 		memset((void *)hpt, 0, (1ul &lt;&lt; order));
<span class="p_del">-		kvm-&gt;arch.hpt_cma_alloc = 1;</span>
<span class="p_add">+		kvm-&gt;arch.hpt.cma = 1;</span>
 	}
 
 	/* Lastly try successively smaller sizes from the page allocator */
<span class="p_chunk">@@ -81,22 +81,22 @@</span> <span class="p_context"> long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp)</span>
 	if (!hpt)
 		return -ENOMEM;
 
<span class="p_del">-	kvm-&gt;arch.hpt_virt = hpt;</span>
<span class="p_del">-	kvm-&gt;arch.hpt_order = order;</span>
<span class="p_add">+	kvm-&gt;arch.hpt.virt = hpt;</span>
<span class="p_add">+	kvm-&gt;arch.hpt.order = order;</span>
 	/* HPTEs are 2**4 bytes long */
<span class="p_del">-	kvm-&gt;arch.hpt_npte = 1ul &lt;&lt; (order - 4);</span>
<span class="p_add">+	kvm-&gt;arch.hpt.npte = 1ul &lt;&lt; (order - 4);</span>
 	/* 128 (2**7) bytes in each HPTEG */
<span class="p_del">-	kvm-&gt;arch.hpt_mask = (1ul &lt;&lt; (order - 7)) - 1;</span>
<span class="p_add">+	kvm-&gt;arch.hpt.mask = (1ul &lt;&lt; (order - 7)) - 1;</span>
 
 	atomic64_set(&amp;kvm-&gt;arch.mmio_update, 0);
 
 	/* Allocate reverse map array */
<span class="p_del">-	rev = vmalloc(sizeof(struct revmap_entry) * kvm-&gt;arch.hpt_npte);</span>
<span class="p_add">+	rev = vmalloc(sizeof(struct revmap_entry) * kvm-&gt;arch.hpt.npte);</span>
 	if (!rev) {
 		pr_err(&quot;kvmppc_alloc_hpt: Couldn&#39;t alloc reverse map array\n&quot;);
 		goto out_freehpt;
 	}
<span class="p_del">-	kvm-&gt;arch.revmap = rev;</span>
<span class="p_add">+	kvm-&gt;arch.hpt.rev = rev;</span>
 	kvm-&gt;arch.sdr1 = __pa(hpt) | (order - 18);
 
 	pr_info(&quot;KVM guest htab at %lx (order %ld), LPID %x\n&quot;,
<span class="p_chunk">@@ -107,7 +107,7 @@</span> <span class="p_context"> long kvmppc_alloc_hpt(struct kvm *kvm, u32 *htab_orderp)</span>
 	return 0;
 
  out_freehpt:
<span class="p_del">-	if (kvm-&gt;arch.hpt_cma_alloc)</span>
<span class="p_add">+	if (kvm-&gt;arch.hpt.cma)</span>
 		kvm_free_hpt_cma(page, 1 &lt;&lt; (order - PAGE_SHIFT));
 	else
 		free_pages(hpt, order - PAGE_SHIFT);
<span class="p_chunk">@@ -129,10 +129,10 @@</span> <span class="p_context"> long kvmppc_alloc_reset_hpt(struct kvm *kvm, u32 *htab_orderp)</span>
 			goto out;
 		}
 	}
<span class="p_del">-	if (kvm-&gt;arch.hpt_virt) {</span>
<span class="p_del">-		order = kvm-&gt;arch.hpt_order;</span>
<span class="p_add">+	if (kvm-&gt;arch.hpt.virt) {</span>
<span class="p_add">+		order = kvm-&gt;arch.hpt.order;</span>
 		/* Set the entire HPT to 0, i.e. invalid HPTEs */
<span class="p_del">-		memset((void *)kvm-&gt;arch.hpt_virt, 0, 1ul &lt;&lt; order);</span>
<span class="p_add">+		memset((void *)kvm-&gt;arch.hpt.virt, 0, 1ul &lt;&lt; order);</span>
 		/*
 		 * Reset all the reverse-mapping chains for all memslots
 		 */
<span class="p_chunk">@@ -153,13 +153,13 @@</span> <span class="p_context"> long kvmppc_alloc_reset_hpt(struct kvm *kvm, u32 *htab_orderp)</span>
 void kvmppc_free_hpt(struct kvm *kvm)
 {
 	kvmppc_free_lpid(kvm-&gt;arch.lpid);
<span class="p_del">-	vfree(kvm-&gt;arch.revmap);</span>
<span class="p_del">-	if (kvm-&gt;arch.hpt_cma_alloc)</span>
<span class="p_del">-		kvm_free_hpt_cma(virt_to_page(kvm-&gt;arch.hpt_virt),</span>
<span class="p_del">-				 1 &lt;&lt; (kvm-&gt;arch.hpt_order - PAGE_SHIFT));</span>
<span class="p_add">+	vfree(kvm-&gt;arch.hpt.rev);</span>
<span class="p_add">+	if (kvm-&gt;arch.hpt.cma)</span>
<span class="p_add">+		kvm_free_hpt_cma(virt_to_page(kvm-&gt;arch.hpt.virt),</span>
<span class="p_add">+				 1 &lt;&lt; (kvm-&gt;arch.hpt.order - PAGE_SHIFT));</span>
 	else
<span class="p_del">-		free_pages(kvm-&gt;arch.hpt_virt,</span>
<span class="p_del">-			   kvm-&gt;arch.hpt_order - PAGE_SHIFT);</span>
<span class="p_add">+		free_pages(kvm-&gt;arch.hpt.virt,</span>
<span class="p_add">+			   kvm-&gt;arch.hpt.order - PAGE_SHIFT);</span>
 }
 
 /* Bits in first HPTE dword for pagesize 4k, 64k or 16M */
<span class="p_chunk">@@ -194,8 +194,8 @@</span> <span class="p_context"> void kvmppc_map_vrma(struct kvm_vcpu *vcpu, struct kvm_memory_slot *memslot,</span>
 	if (npages &gt; 1ul &lt;&lt; (40 - porder))
 		npages = 1ul &lt;&lt; (40 - porder);
 	/* Can&#39;t use more than 1 HPTE per HPTEG */
<span class="p_del">-	if (npages &gt; kvm-&gt;arch.hpt_mask + 1)</span>
<span class="p_del">-		npages = kvm-&gt;arch.hpt_mask + 1;</span>
<span class="p_add">+	if (npages &gt; kvm-&gt;arch.hpt.mask + 1)</span>
<span class="p_add">+		npages = kvm-&gt;arch.hpt.mask + 1;</span>
 
 	hp0 = HPTE_V_1TB_SEG | (VRMA_VSID &lt;&lt; (40 - 16)) |
 		HPTE_V_BOLTED | hpte0_pgsize_encoding(psize);
<span class="p_chunk">@@ -205,7 +205,7 @@</span> <span class="p_context"> void kvmppc_map_vrma(struct kvm_vcpu *vcpu, struct kvm_memory_slot *memslot,</span>
 	for (i = 0; i &lt; npages; ++i) {
 		addr = i &lt;&lt; porder;
 		/* can&#39;t use hpt_hash since va &gt; 64 bits */
<span class="p_del">-		hash = (i ^ (VRMA_VSID ^ (VRMA_VSID &lt;&lt; 25))) &amp; kvm-&gt;arch.hpt_mask;</span>
<span class="p_add">+		hash = (i ^ (VRMA_VSID ^ (VRMA_VSID &lt;&lt; 25))) &amp; kvm-&gt;arch.hpt.mask;</span>
 		/*
 		 * We assume that the hash table is empty and no
 		 * vcpus are using it at this stage.  Since we create
<span class="p_chunk">@@ -338,11 +338,11 @@</span> <span class="p_context"> static int kvmppc_mmu_book3s_64_hv_xlate(struct kvm_vcpu *vcpu, gva_t eaddr,</span>
 		preempt_enable();
 		return -ENOENT;
 	}
<span class="p_del">-	hptep = (__be64 *)(kvm-&gt;arch.hpt_virt + (index &lt;&lt; 4));</span>
<span class="p_add">+	hptep = (__be64 *)(kvm-&gt;arch.hpt.virt + (index &lt;&lt; 4));</span>
 	v = orig_v = be64_to_cpu(hptep[0]) &amp; ~HPTE_V_HVLOCK;
 	if (cpu_has_feature(CPU_FTR_ARCH_300))
 		v = hpte_new_to_old_v(v, be64_to_cpu(hptep[1]));
<span class="p_del">-	gr = kvm-&gt;arch.revmap[index].guest_rpte;</span>
<span class="p_add">+	gr = kvm-&gt;arch.hpt.rev[index].guest_rpte;</span>
 
 	unlock_hpte(hptep, orig_v);
 	preempt_enable();
<span class="p_chunk">@@ -480,8 +480,8 @@</span> <span class="p_context"> int kvmppc_book3s_hv_page_fault(struct kvm_run *run, struct kvm_vcpu *vcpu,</span>
 		}
 	}
 	index = vcpu-&gt;arch.pgfault_index;
<span class="p_del">-	hptep = (__be64 *)(kvm-&gt;arch.hpt_virt + (index &lt;&lt; 4));</span>
<span class="p_del">-	rev = &amp;kvm-&gt;arch.revmap[index];</span>
<span class="p_add">+	hptep = (__be64 *)(kvm-&gt;arch.hpt.virt + (index &lt;&lt; 4));</span>
<span class="p_add">+	rev = &amp;kvm-&gt;arch.hpt.rev[index];</span>
 	preempt_disable();
 	while (!try_lock_hpte(hptep, HPTE_V_HVLOCK))
 		cpu_relax();
<span class="p_chunk">@@ -745,7 +745,7 @@</span> <span class="p_context"> static int kvm_handle_hva(struct kvm *kvm, unsigned long hva,</span>
 static int kvm_unmap_rmapp(struct kvm *kvm, unsigned long *rmapp,
 			   unsigned long gfn)
 {
<span class="p_del">-	struct revmap_entry *rev = kvm-&gt;arch.revmap;</span>
<span class="p_add">+	struct revmap_entry *rev = kvm-&gt;arch.hpt.rev;</span>
 	unsigned long h, i, j;
 	__be64 *hptep;
 	unsigned long ptel, psize, rcbits;
<span class="p_chunk">@@ -763,7 +763,7 @@</span> <span class="p_context"> static int kvm_unmap_rmapp(struct kvm *kvm, unsigned long *rmapp,</span>
 		 * rmap chain lock.
 		 */
 		i = *rmapp &amp; KVMPPC_RMAP_INDEX;
<span class="p_del">-		hptep = (__be64 *) (kvm-&gt;arch.hpt_virt + (i &lt;&lt; 4));</span>
<span class="p_add">+		hptep = (__be64 *) (kvm-&gt;arch.hpt.virt + (i &lt;&lt; 4));</span>
 		if (!try_lock_hpte(hptep, HPTE_V_HVLOCK)) {
 			/* unlock rmap before spinning on the HPTE lock */
 			unlock_rmap(rmapp);
<span class="p_chunk">@@ -846,7 +846,7 @@</span> <span class="p_context"> void kvmppc_core_flush_memslot_hv(struct kvm *kvm,</span>
 static int kvm_age_rmapp(struct kvm *kvm, unsigned long *rmapp,
 			 unsigned long gfn)
 {
<span class="p_del">-	struct revmap_entry *rev = kvm-&gt;arch.revmap;</span>
<span class="p_add">+	struct revmap_entry *rev = kvm-&gt;arch.hpt.rev;</span>
 	unsigned long head, i, j;
 	__be64 *hptep;
 	int ret = 0;
<span class="p_chunk">@@ -864,7 +864,7 @@</span> <span class="p_context"> static int kvm_age_rmapp(struct kvm *kvm, unsigned long *rmapp,</span>
 
 	i = head = *rmapp &amp; KVMPPC_RMAP_INDEX;
 	do {
<span class="p_del">-		hptep = (__be64 *) (kvm-&gt;arch.hpt_virt + (i &lt;&lt; 4));</span>
<span class="p_add">+		hptep = (__be64 *) (kvm-&gt;arch.hpt.virt + (i &lt;&lt; 4));</span>
 		j = rev[i].forw;
 
 		/* If this HPTE isn&#39;t referenced, ignore it */
<span class="p_chunk">@@ -904,7 +904,7 @@</span> <span class="p_context"> int kvm_age_hva_hv(struct kvm *kvm, unsigned long start, unsigned long end)</span>
 static int kvm_test_age_rmapp(struct kvm *kvm, unsigned long *rmapp,
 			      unsigned long gfn)
 {
<span class="p_del">-	struct revmap_entry *rev = kvm-&gt;arch.revmap;</span>
<span class="p_add">+	struct revmap_entry *rev = kvm-&gt;arch.hpt.rev;</span>
 	unsigned long head, i, j;
 	unsigned long *hp;
 	int ret = 1;
<span class="p_chunk">@@ -919,7 +919,7 @@</span> <span class="p_context"> static int kvm_test_age_rmapp(struct kvm *kvm, unsigned long *rmapp,</span>
 	if (*rmapp &amp; KVMPPC_RMAP_PRESENT) {
 		i = head = *rmapp &amp; KVMPPC_RMAP_INDEX;
 		do {
<span class="p_del">-			hp = (unsigned long *)(kvm-&gt;arch.hpt_virt + (i &lt;&lt; 4));</span>
<span class="p_add">+			hp = (unsigned long *)(kvm-&gt;arch.hpt.virt + (i &lt;&lt; 4));</span>
 			j = rev[i].forw;
 			if (be64_to_cpu(hp[1]) &amp; HPTE_R_R)
 				goto out;
<span class="p_chunk">@@ -953,7 +953,7 @@</span> <span class="p_context"> static int vcpus_running(struct kvm *kvm)</span>
  */
 static int kvm_test_clear_dirty_npages(struct kvm *kvm, unsigned long *rmapp)
 {
<span class="p_del">-	struct revmap_entry *rev = kvm-&gt;arch.revmap;</span>
<span class="p_add">+	struct revmap_entry *rev = kvm-&gt;arch.hpt.rev;</span>
 	unsigned long head, i, j;
 	unsigned long n;
 	unsigned long v, r;
<span class="p_chunk">@@ -978,7 +978,7 @@</span> <span class="p_context"> static int kvm_test_clear_dirty_npages(struct kvm *kvm, unsigned long *rmapp)</span>
 	i = head = *rmapp &amp; KVMPPC_RMAP_INDEX;
 	do {
 		unsigned long hptep1;
<span class="p_del">-		hptep = (__be64 *) (kvm-&gt;arch.hpt_virt + (i &lt;&lt; 4));</span>
<span class="p_add">+		hptep = (__be64 *) (kvm-&gt;arch.hpt.virt + (i &lt;&lt; 4));</span>
 		j = rev[i].forw;
 
 		/*
<span class="p_chunk">@@ -1290,8 +1290,8 @@</span> <span class="p_context"> static ssize_t kvm_htab_read(struct file *file, char __user *buf,</span>
 	flags = ctx-&gt;flags;
 
 	i = ctx-&gt;index;
<span class="p_del">-	hptp = (__be64 *)(kvm-&gt;arch.hpt_virt + (i * HPTE_SIZE));</span>
<span class="p_del">-	revp = kvm-&gt;arch.revmap + i;</span>
<span class="p_add">+	hptp = (__be64 *)(kvm-&gt;arch.hpt.virt + (i * HPTE_SIZE));</span>
<span class="p_add">+	revp = kvm-&gt;arch.hpt.rev + i;</span>
 	lbuf = (unsigned long __user *)buf;
 
 	nb = 0;
<span class="p_chunk">@@ -1306,7 +1306,7 @@</span> <span class="p_context"> static ssize_t kvm_htab_read(struct file *file, char __user *buf,</span>
 
 		/* Skip uninteresting entries, i.e. clean on not-first pass */
 		if (!first_pass) {
<span class="p_del">-			while (i &lt; kvm-&gt;arch.hpt_npte &amp;&amp;</span>
<span class="p_add">+			while (i &lt; kvm-&gt;arch.hpt.npte &amp;&amp;</span>
 			       !hpte_dirty(revp, hptp)) {
 				++i;
 				hptp += 2;
<span class="p_chunk">@@ -1316,7 +1316,7 @@</span> <span class="p_context"> static ssize_t kvm_htab_read(struct file *file, char __user *buf,</span>
 		hdr.index = i;
 
 		/* Grab a series of valid entries */
<span class="p_del">-		while (i &lt; kvm-&gt;arch.hpt_npte &amp;&amp;</span>
<span class="p_add">+		while (i &lt; kvm-&gt;arch.hpt.npte &amp;&amp;</span>
 		       hdr.n_valid &lt; 0xffff &amp;&amp;
 		       nb + HPTE_SIZE &lt; count &amp;&amp;
 		       record_hpte(flags, hptp, hpte, revp, 1, first_pass)) {
<span class="p_chunk">@@ -1332,7 +1332,7 @@</span> <span class="p_context"> static ssize_t kvm_htab_read(struct file *file, char __user *buf,</span>
 			++revp;
 		}
 		/* Now skip invalid entries while we can */
<span class="p_del">-		while (i &lt; kvm-&gt;arch.hpt_npte &amp;&amp;</span>
<span class="p_add">+		while (i &lt; kvm-&gt;arch.hpt.npte &amp;&amp;</span>
 		       hdr.n_invalid &lt; 0xffff &amp;&amp;
 		       record_hpte(flags, hptp, hpte, revp, 0, first_pass)) {
 			/* found an invalid entry */
<span class="p_chunk">@@ -1353,7 +1353,7 @@</span> <span class="p_context"> static ssize_t kvm_htab_read(struct file *file, char __user *buf,</span>
 		}
 
 		/* Check if we&#39;ve wrapped around the hash table */
<span class="p_del">-		if (i &gt;= kvm-&gt;arch.hpt_npte) {</span>
<span class="p_add">+		if (i &gt;= kvm-&gt;arch.hpt.npte) {</span>
 			i = 0;
 			ctx-&gt;first_pass = 0;
 			break;
<span class="p_chunk">@@ -1412,11 +1412,11 @@</span> <span class="p_context"> static ssize_t kvm_htab_write(struct file *file, const char __user *buf,</span>
 
 		err = -EINVAL;
 		i = hdr.index;
<span class="p_del">-		if (i &gt;= kvm-&gt;arch.hpt_npte ||</span>
<span class="p_del">-		    i + hdr.n_valid + hdr.n_invalid &gt; kvm-&gt;arch.hpt_npte)</span>
<span class="p_add">+		if (i &gt;= kvm-&gt;arch.hpt.npte ||</span>
<span class="p_add">+		    i + hdr.n_valid + hdr.n_invalid &gt; kvm-&gt;arch.hpt.npte)</span>
 			break;
 
<span class="p_del">-		hptp = (__be64 *)(kvm-&gt;arch.hpt_virt + (i * HPTE_SIZE));</span>
<span class="p_add">+		hptp = (__be64 *)(kvm-&gt;arch.hpt.virt + (i * HPTE_SIZE));</span>
 		lbuf = (unsigned long __user *)buf;
 		for (j = 0; j &lt; hdr.n_valid; ++j) {
 			__be64 hpte_v;
<span class="p_chunk">@@ -1603,8 +1603,8 @@</span> <span class="p_context"> static ssize_t debugfs_htab_read(struct file *file, char __user *buf,</span>
 
 	kvm = p-&gt;kvm;
 	i = p-&gt;hpt_index;
<span class="p_del">-	hptp = (__be64 *)(kvm-&gt;arch.hpt_virt + (i * HPTE_SIZE));</span>
<span class="p_del">-	for (; len != 0 &amp;&amp; i &lt; kvm-&gt;arch.hpt_npte; ++i, hptp += 2) {</span>
<span class="p_add">+	hptp = (__be64 *)(kvm-&gt;arch.hpt.virt + (i * HPTE_SIZE));</span>
<span class="p_add">+	for (; len != 0 &amp;&amp; i &lt; kvm-&gt;arch.hpt.npte; ++i, hptp += 2) {</span>
 		if (!(be64_to_cpu(hptp[0]) &amp; (HPTE_V_VALID | HPTE_V_ABSENT)))
 			continue;
 
<span class="p_chunk">@@ -1614,7 +1614,7 @@</span> <span class="p_context"> static ssize_t debugfs_htab_read(struct file *file, char __user *buf,</span>
 			cpu_relax();
 		v = be64_to_cpu(hptp[0]) &amp; ~HPTE_V_HVLOCK;
 		hr = be64_to_cpu(hptp[1]);
<span class="p_del">-		gr = kvm-&gt;arch.revmap[i].guest_rpte;</span>
<span class="p_add">+		gr = kvm-&gt;arch.hpt.rev[i].guest_rpte;</span>
 		unlock_hpte(hptp, v);
 		preempt_enable();
 
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_hv.c b/arch/powerpc/kvm/book3s_hv.c</span>
<span class="p_header">index 8dcbe37..78baa2b 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_hv.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_hv.c</span>
<span class="p_chunk">@@ -3114,7 +3114,7 @@</span> <span class="p_context"> static int kvmppc_hv_setup_htab_rma(struct kvm_vcpu *vcpu)</span>
 		goto out;	/* another vcpu beat us to it */
 
 	/* Allocate hashed page table (if not done already) and reset it */
<span class="p_del">-	if (!kvm-&gt;arch.hpt_virt) {</span>
<span class="p_add">+	if (!kvm-&gt;arch.hpt.virt) {</span>
 		err = kvmppc_alloc_hpt(kvm, NULL);
 		if (err) {
 			pr_err(&quot;KVM: Couldn&#39;t alloc HPT\n&quot;);
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_hv_rm_mmu.c b/arch/powerpc/kvm/book3s_hv_rm_mmu.c</span>
<span class="p_header">index 9ef3c4b..7e2b048 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_hv_rm_mmu.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_hv_rm_mmu.c</span>
<span class="p_chunk">@@ -79,10 +79,10 @@</span> <span class="p_context"> void kvmppc_add_revmap_chain(struct kvm *kvm, struct revmap_entry *rev,</span>
 
 	if (*rmap &amp; KVMPPC_RMAP_PRESENT) {
 		i = *rmap &amp; KVMPPC_RMAP_INDEX;
<span class="p_del">-		head = &amp;kvm-&gt;arch.revmap[i];</span>
<span class="p_add">+		head = &amp;kvm-&gt;arch.hpt.rev[i];</span>
 		if (realmode)
 			head = real_vmalloc_addr(head);
<span class="p_del">-		tail = &amp;kvm-&gt;arch.revmap[head-&gt;back];</span>
<span class="p_add">+		tail = &amp;kvm-&gt;arch.hpt.rev[head-&gt;back];</span>
 		if (realmode)
 			tail = real_vmalloc_addr(tail);
 		rev-&gt;forw = i;
<span class="p_chunk">@@ -147,8 +147,8 @@</span> <span class="p_context"> static void remove_revmap_chain(struct kvm *kvm, long pte_index,</span>
 	lock_rmap(rmap);
 
 	head = *rmap &amp; KVMPPC_RMAP_INDEX;
<span class="p_del">-	next = real_vmalloc_addr(&amp;kvm-&gt;arch.revmap[rev-&gt;forw]);</span>
<span class="p_del">-	prev = real_vmalloc_addr(&amp;kvm-&gt;arch.revmap[rev-&gt;back]);</span>
<span class="p_add">+	next = real_vmalloc_addr(&amp;kvm-&gt;arch.hpt.rev[rev-&gt;forw]);</span>
<span class="p_add">+	prev = real_vmalloc_addr(&amp;kvm-&gt;arch.hpt.rev[rev-&gt;back]);</span>
 	next-&gt;back = rev-&gt;back;
 	prev-&gt;forw = rev-&gt;forw;
 	if (head == pte_index) {
<span class="p_chunk">@@ -283,11 +283,11 @@</span> <span class="p_context"> long kvmppc_do_h_enter(struct kvm *kvm, unsigned long flags,</span>
 
 	/* Find and lock the HPTEG slot to use */
  do_insert:
<span class="p_del">-	if (pte_index &gt;= kvm-&gt;arch.hpt_npte)</span>
<span class="p_add">+	if (pte_index &gt;= kvm-&gt;arch.hpt.npte)</span>
 		return H_PARAMETER;
 	if (likely((flags &amp; H_EXACT) == 0)) {
 		pte_index &amp;= ~7UL;
<span class="p_del">-		hpte = (__be64 *)(kvm-&gt;arch.hpt_virt + (pte_index &lt;&lt; 4));</span>
<span class="p_add">+		hpte = (__be64 *)(kvm-&gt;arch.hpt.virt + (pte_index &lt;&lt; 4));</span>
 		for (i = 0; i &lt; 8; ++i) {
 			if ((be64_to_cpu(*hpte) &amp; HPTE_V_VALID) == 0 &amp;&amp;
 			    try_lock_hpte(hpte, HPTE_V_HVLOCK | HPTE_V_VALID |
<span class="p_chunk">@@ -318,7 +318,7 @@</span> <span class="p_context"> long kvmppc_do_h_enter(struct kvm *kvm, unsigned long flags,</span>
 		}
 		pte_index += i;
 	} else {
<span class="p_del">-		hpte = (__be64 *)(kvm-&gt;arch.hpt_virt + (pte_index &lt;&lt; 4));</span>
<span class="p_add">+		hpte = (__be64 *)(kvm-&gt;arch.hpt.virt + (pte_index &lt;&lt; 4));</span>
 		if (!try_lock_hpte(hpte, HPTE_V_HVLOCK | HPTE_V_VALID |
 				   HPTE_V_ABSENT)) {
 			/* Lock the slot and check again */
<span class="p_chunk">@@ -335,7 +335,7 @@</span> <span class="p_context"> long kvmppc_do_h_enter(struct kvm *kvm, unsigned long flags,</span>
 	}
 
 	/* Save away the guest&#39;s idea of the second HPTE dword */
<span class="p_del">-	rev = &amp;kvm-&gt;arch.revmap[pte_index];</span>
<span class="p_add">+	rev = &amp;kvm-&gt;arch.hpt.rev[pte_index];</span>
 	if (realmode)
 		rev = real_vmalloc_addr(rev);
 	if (rev) {
<span class="p_chunk">@@ -458,9 +458,9 @@</span> <span class="p_context"> long kvmppc_do_h_remove(struct kvm *kvm, unsigned long flags,</span>
 	struct revmap_entry *rev;
 	u64 pte, orig_pte, pte_r;
 
<span class="p_del">-	if (pte_index &gt;= kvm-&gt;arch.hpt_npte)</span>
<span class="p_add">+	if (pte_index &gt;= kvm-&gt;arch.hpt.npte)</span>
 		return H_PARAMETER;
<span class="p_del">-	hpte = (__be64 *)(kvm-&gt;arch.hpt_virt + (pte_index &lt;&lt; 4));</span>
<span class="p_add">+	hpte = (__be64 *)(kvm-&gt;arch.hpt.virt + (pte_index &lt;&lt; 4));</span>
 	while (!try_lock_hpte(hpte, HPTE_V_HVLOCK))
 		cpu_relax();
 	pte = orig_pte = be64_to_cpu(hpte[0]);
<span class="p_chunk">@@ -476,7 +476,7 @@</span> <span class="p_context"> long kvmppc_do_h_remove(struct kvm *kvm, unsigned long flags,</span>
 		return H_NOT_FOUND;
 	}
 
<span class="p_del">-	rev = real_vmalloc_addr(&amp;kvm-&gt;arch.revmap[pte_index]);</span>
<span class="p_add">+	rev = real_vmalloc_addr(&amp;kvm-&gt;arch.hpt.rev[pte_index]);</span>
 	v = pte &amp; ~HPTE_V_HVLOCK;
 	if (v &amp; HPTE_V_VALID) {
 		hpte[0] &amp;= ~cpu_to_be64(HPTE_V_VALID);
<span class="p_chunk">@@ -544,13 +544,13 @@</span> <span class="p_context"> long kvmppc_h_bulk_remove(struct kvm_vcpu *vcpu)</span>
 				break;
 			}
 			if (req != 1 || flags == 3 ||
<span class="p_del">-			    pte_index &gt;= kvm-&gt;arch.hpt_npte) {</span>
<span class="p_add">+			    pte_index &gt;= kvm-&gt;arch.hpt.npte) {</span>
 				/* parameter error */
 				args[j] = ((0xa0 | flags) &lt;&lt; 56) + pte_index;
 				ret = H_PARAMETER;
 				break;
 			}
<span class="p_del">-			hp = (__be64 *) (kvm-&gt;arch.hpt_virt + (pte_index &lt;&lt; 4));</span>
<span class="p_add">+			hp = (__be64 *) (kvm-&gt;arch.hpt.virt + (pte_index &lt;&lt; 4));</span>
 			/* to avoid deadlock, don&#39;t spin except for first */
 			if (!try_lock_hpte(hp, HPTE_V_HVLOCK)) {
 				if (n)
<span class="p_chunk">@@ -587,7 +587,7 @@</span> <span class="p_context"> long kvmppc_h_bulk_remove(struct kvm_vcpu *vcpu)</span>
 			}
 
 			args[j] = ((0x80 | flags) &lt;&lt; 56) + pte_index;
<span class="p_del">-			rev = real_vmalloc_addr(&amp;kvm-&gt;arch.revmap[pte_index]);</span>
<span class="p_add">+			rev = real_vmalloc_addr(&amp;kvm-&gt;arch.hpt.rev[pte_index]);</span>
 			note_hpte_modification(kvm, rev);
 
 			if (!(hp0 &amp; HPTE_V_VALID)) {
<span class="p_chunk">@@ -642,10 +642,10 @@</span> <span class="p_context"> long kvmppc_h_protect(struct kvm_vcpu *vcpu, unsigned long flags,</span>
 	unsigned long v, r, rb, mask, bits;
 	u64 pte_v, pte_r;
 
<span class="p_del">-	if (pte_index &gt;= kvm-&gt;arch.hpt_npte)</span>
<span class="p_add">+	if (pte_index &gt;= kvm-&gt;arch.hpt.npte)</span>
 		return H_PARAMETER;
 
<span class="p_del">-	hpte = (__be64 *)(kvm-&gt;arch.hpt_virt + (pte_index &lt;&lt; 4));</span>
<span class="p_add">+	hpte = (__be64 *)(kvm-&gt;arch.hpt.virt + (pte_index &lt;&lt; 4));</span>
 	while (!try_lock_hpte(hpte, HPTE_V_HVLOCK))
 		cpu_relax();
 	v = pte_v = be64_to_cpu(hpte[0]);
<span class="p_chunk">@@ -665,7 +665,7 @@</span> <span class="p_context"> long kvmppc_h_protect(struct kvm_vcpu *vcpu, unsigned long flags,</span>
 	/* Update guest view of 2nd HPTE dword */
 	mask = HPTE_R_PP0 | HPTE_R_PP | HPTE_R_N |
 		HPTE_R_KEY_HI | HPTE_R_KEY_LO;
<span class="p_del">-	rev = real_vmalloc_addr(&amp;kvm-&gt;arch.revmap[pte_index]);</span>
<span class="p_add">+	rev = real_vmalloc_addr(&amp;kvm-&gt;arch.hpt.rev[pte_index]);</span>
 	if (rev) {
 		r = (rev-&gt;guest_rpte &amp; ~mask) | bits;
 		rev-&gt;guest_rpte = r;
<span class="p_chunk">@@ -711,15 +711,15 @@</span> <span class="p_context"> long kvmppc_h_read(struct kvm_vcpu *vcpu, unsigned long flags,</span>
 	int i, n = 1;
 	struct revmap_entry *rev = NULL;
 
<span class="p_del">-	if (pte_index &gt;= kvm-&gt;arch.hpt_npte)</span>
<span class="p_add">+	if (pte_index &gt;= kvm-&gt;arch.hpt.npte)</span>
 		return H_PARAMETER;
 	if (flags &amp; H_READ_4) {
 		pte_index &amp;= ~3;
 		n = 4;
 	}
<span class="p_del">-	rev = real_vmalloc_addr(&amp;kvm-&gt;arch.revmap[pte_index]);</span>
<span class="p_add">+	rev = real_vmalloc_addr(&amp;kvm-&gt;arch.hpt.rev[pte_index]);</span>
 	for (i = 0; i &lt; n; ++i, ++pte_index) {
<span class="p_del">-		hpte = (__be64 *)(kvm-&gt;arch.hpt_virt + (pte_index &lt;&lt; 4));</span>
<span class="p_add">+		hpte = (__be64 *)(kvm-&gt;arch.hpt.virt + (pte_index &lt;&lt; 4));</span>
 		v = be64_to_cpu(hpte[0]) &amp; ~HPTE_V_HVLOCK;
 		r = be64_to_cpu(hpte[1]);
 		if (cpu_has_feature(CPU_FTR_ARCH_300)) {
<span class="p_chunk">@@ -750,11 +750,11 @@</span> <span class="p_context"> long kvmppc_h_clear_ref(struct kvm_vcpu *vcpu, unsigned long flags,</span>
 	unsigned long *rmap;
 	long ret = H_NOT_FOUND;
 
<span class="p_del">-	if (pte_index &gt;= kvm-&gt;arch.hpt_npte)</span>
<span class="p_add">+	if (pte_index &gt;= kvm-&gt;arch.hpt.npte)</span>
 		return H_PARAMETER;
 
<span class="p_del">-	rev = real_vmalloc_addr(&amp;kvm-&gt;arch.revmap[pte_index]);</span>
<span class="p_del">-	hpte = (__be64 *)(kvm-&gt;arch.hpt_virt + (pte_index &lt;&lt; 4));</span>
<span class="p_add">+	rev = real_vmalloc_addr(&amp;kvm-&gt;arch.hpt.rev[pte_index]);</span>
<span class="p_add">+	hpte = (__be64 *)(kvm-&gt;arch.hpt.virt + (pte_index &lt;&lt; 4));</span>
 	while (!try_lock_hpte(hpte, HPTE_V_HVLOCK))
 		cpu_relax();
 	v = be64_to_cpu(hpte[0]);
<span class="p_chunk">@@ -796,11 +796,11 @@</span> <span class="p_context"> long kvmppc_h_clear_mod(struct kvm_vcpu *vcpu, unsigned long flags,</span>
 	unsigned long *rmap;
 	long ret = H_NOT_FOUND;
 
<span class="p_del">-	if (pte_index &gt;= kvm-&gt;arch.hpt_npte)</span>
<span class="p_add">+	if (pte_index &gt;= kvm-&gt;arch.hpt.npte)</span>
 		return H_PARAMETER;
 
<span class="p_del">-	rev = real_vmalloc_addr(&amp;kvm-&gt;arch.revmap[pte_index]);</span>
<span class="p_del">-	hpte = (__be64 *)(kvm-&gt;arch.hpt_virt + (pte_index &lt;&lt; 4));</span>
<span class="p_add">+	rev = real_vmalloc_addr(&amp;kvm-&gt;arch.hpt.rev[pte_index]);</span>
<span class="p_add">+	hpte = (__be64 *)(kvm-&gt;arch.hpt.virt + (pte_index &lt;&lt; 4));</span>
 	while (!try_lock_hpte(hpte, HPTE_V_HVLOCK))
 		cpu_relax();
 	v = be64_to_cpu(hpte[0]);
<span class="p_chunk">@@ -949,7 +949,7 @@</span> <span class="p_context"> long kvmppc_hv_find_lock_hpte(struct kvm *kvm, gva_t eaddr, unsigned long slb_v,</span>
 		somask = (1UL &lt;&lt; 28) - 1;
 		vsid = (slb_v &amp; ~SLB_VSID_B) &gt;&gt; SLB_VSID_SHIFT;
 	}
<span class="p_del">-	hash = (vsid ^ ((eaddr &amp; somask) &gt;&gt; pshift)) &amp; kvm-&gt;arch.hpt_mask;</span>
<span class="p_add">+	hash = (vsid ^ ((eaddr &amp; somask) &gt;&gt; pshift)) &amp; kvm-&gt;arch.hpt.mask;</span>
 	avpn = slb_v &amp; ~(somask &gt;&gt; 16);	/* also includes B */
 	avpn |= (eaddr &amp; somask) &gt;&gt; 16;
 
<span class="p_chunk">@@ -960,7 +960,7 @@</span> <span class="p_context"> long kvmppc_hv_find_lock_hpte(struct kvm *kvm, gva_t eaddr, unsigned long slb_v,</span>
 	val |= avpn;
 
 	for (;;) {
<span class="p_del">-		hpte = (__be64 *)(kvm-&gt;arch.hpt_virt + (hash &lt;&lt; 7));</span>
<span class="p_add">+		hpte = (__be64 *)(kvm-&gt;arch.hpt.virt + (hash &lt;&lt; 7));</span>
 
 		for (i = 0; i &lt; 16; i += 2) {
 			/* Read the PTE racily */
<span class="p_chunk">@@ -996,7 +996,7 @@</span> <span class="p_context"> long kvmppc_hv_find_lock_hpte(struct kvm *kvm, gva_t eaddr, unsigned long slb_v,</span>
 		if (val &amp; HPTE_V_SECONDARY)
 			break;
 		val |= HPTE_V_SECONDARY;
<span class="p_del">-		hash = hash ^ kvm-&gt;arch.hpt_mask;</span>
<span class="p_add">+		hash = hash ^ kvm-&gt;arch.hpt.mask;</span>
 	}
 	return -1;
 }
<span class="p_chunk">@@ -1045,14 +1045,14 @@</span> <span class="p_context"> long kvmppc_hpte_hv_fault(struct kvm_vcpu *vcpu, unsigned long addr,</span>
 				return status;	/* there really was no HPTE */
 			return 0;	/* for prot fault, HPTE disappeared */
 		}
<span class="p_del">-		hpte = (__be64 *)(kvm-&gt;arch.hpt_virt + (index &lt;&lt; 4));</span>
<span class="p_add">+		hpte = (__be64 *)(kvm-&gt;arch.hpt.virt + (index &lt;&lt; 4));</span>
 		v = orig_v = be64_to_cpu(hpte[0]) &amp; ~HPTE_V_HVLOCK;
 		r = be64_to_cpu(hpte[1]);
 		if (cpu_has_feature(CPU_FTR_ARCH_300)) {
 			v = hpte_new_to_old_v(v, r);
 			r = hpte_new_to_old_r(r);
 		}
<span class="p_del">-		rev = real_vmalloc_addr(&amp;kvm-&gt;arch.revmap[index]);</span>
<span class="p_add">+		rev = real_vmalloc_addr(&amp;kvm-&gt;arch.hpt.rev[index]);</span>
 		gr = rev-&gt;guest_rpte;
 
 		unlock_hpte(hpte, orig_v);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



