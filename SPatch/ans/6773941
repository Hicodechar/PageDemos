
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,09/10] hugetlbfs: add hugetlbfs_fallocate() - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,09/10] hugetlbfs: add hugetlbfs_fallocate()</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 13, 2015, 4:21 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1436761268-6397-10-git-send-email-mike.kravetz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6773941/mbox/"
   >mbox</a>
|
   <a href="/patch/6773941/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6773941/">/patch/6773941/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 5FE329F2E8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Jul 2015 04:22:30 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 410E82055C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Jul 2015 04:22:29 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 0BB292056E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Jul 2015 04:22:28 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751908AbbGMEWV (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 13 Jul 2015 00:22:21 -0400
Received: from aserp1040.oracle.com ([141.146.126.69]:30063 &quot;EHLO
	aserp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751096AbbGMEWR (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 13 Jul 2015 00:22:17 -0400
Received: from userv0022.oracle.com (userv0022.oracle.com [156.151.31.74])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with
	ESMTP id t6D4LhKX020704
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Mon, 13 Jul 2015 04:21:44 GMT
Received: from userv0121.oracle.com (userv0121.oracle.com [156.151.31.72])
	by userv0022.oracle.com (8.13.8/8.13.8) with ESMTP id t6D4Lhhh019649
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL); 
	Mon, 13 Jul 2015 04:21:43 GMT
Received: from abhmp0013.oracle.com (abhmp0013.oracle.com [141.146.116.19])
	by userv0121.oracle.com (8.13.8/8.13.8) with ESMTP id
	t6D4LhvL017904; Mon, 13 Jul 2015 04:21:43 GMT
Received: from monkey.oracle.com (/50.53.81.168)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Sun, 12 Jul 2015 21:21:42 -0700
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	linux-api@vger.kernel.org
Cc: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	David Rientjes &lt;rientjes@google.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Davidlohr Bueso &lt;dave@stgolabs.net&gt;,
	Aneesh Kumar &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;,
	Christoph Hellwig &lt;hch@infradead.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Subject: [PATCH v3 09/10] hugetlbfs: add hugetlbfs_fallocate()
Date: Sun, 12 Jul 2015 21:21:07 -0700
Message-Id: &lt;1436761268-6397-10-git-send-email-mike.kravetz@oracle.com&gt;
X-Mailer: git-send-email 2.1.0
In-Reply-To: &lt;1436761268-6397-1-git-send-email-mike.kravetz@oracle.com&gt;
References: &lt;1436761268-6397-1-git-send-email-mike.kravetz@oracle.com&gt;
X-Source-IP: userv0022.oracle.com [156.151.31.74]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.3 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - July 13, 2015, 4:21 a.m.</div>
<pre class="content">
This is based on the shmem version, but it has diverged quite
a bit.  We have no swap to worry about, nor the new file sealing.
Add synchronication via the fault mutex table to coordinate
page faults,  fallocate allocation and fallocate hole punch.

What this allows us to do is move physical memory in and out of
a hugetlbfs file without having it mapped.  This also gives us
the ability to support MADV_REMOVE since it is currently
implemented using fallocate().  MADV_REMOVE lets madvise() remove
pages from the middle of a hugetlbfs file, which wasn&#39;t possible
before.

hugetlbfs fallocate only operates on whole huge pages.

Based-on code-by: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;
<span class="signed-off-by">Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
---
 fs/hugetlbfs/inode.c    | 158 +++++++++++++++++++++++++++++++++++++++++++++++-
 include/linux/hugetlb.h |   3 +
 mm/hugetlb.c            |   2 +-
 3 files changed, 161 insertions(+), 2 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index a974e4b..6e565a4 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/thread_info.h&gt;
 #include &lt;asm/current.h&gt;
 #include &lt;linux/sched.h&gt;		/* remove ASAP */
<span class="p_add">+#include &lt;linux/falloc.h&gt;</span>
 #include &lt;linux/fs.h&gt;
 #include &lt;linux/mount.h&gt;
 #include &lt;linux/file.h&gt;
<span class="p_chunk">@@ -479,6 +480,160 @@</span> <span class="p_context"> static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)</span>
 	return 0;
 }
 
<span class="p_add">+static long hugetlbfs_punch_hole(struct inode *inode, loff_t offset, loff_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hstate *h = hstate_inode(inode);</span>
<span class="p_add">+	loff_t hpage_size = huge_page_size(h);</span>
<span class="p_add">+	loff_t hole_start, hole_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * For hole punch round up the beginning offset of the hole and</span>
<span class="p_add">+	 * round down the end.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	hole_start = round_up(offset, hpage_size);</span>
<span class="p_add">+	hole_end = round_down(offset + len, hpage_size);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hole_end &gt; hole_start) {</span>
<span class="p_add">+		struct address_space *mapping = inode-&gt;i_mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+		mutex_lock(&amp;inode-&gt;i_mutex);</span>
<span class="p_add">+		i_mmap_lock_write(mapping);</span>
<span class="p_add">+		if (!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap))</span>
<span class="p_add">+			hugetlb_vmdelete_list(&amp;mapping-&gt;i_mmap,</span>
<span class="p_add">+						hole_start &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						hole_end  &gt;&gt; PAGE_SHIFT);</span>
<span class="p_add">+		i_mmap_unlock_write(mapping);</span>
<span class="p_add">+		remove_inode_hugepages(inode, hole_start, hole_end);</span>
<span class="p_add">+		mutex_unlock(&amp;inode-&gt;i_mutex);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static long hugetlbfs_fallocate(struct file *file, int mode, loff_t offset,</span>
<span class="p_add">+				loff_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct inode *inode = file_inode(file);</span>
<span class="p_add">+	struct address_space *mapping = inode-&gt;i_mapping;</span>
<span class="p_add">+	struct hstate *h = hstate_inode(inode);</span>
<span class="p_add">+	struct vm_area_struct pseudo_vma;</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	loff_t hpage_size = huge_page_size(h);</span>
<span class="p_add">+	unsigned long hpage_shift = huge_page_shift(h);</span>
<span class="p_add">+	pgoff_t start, index, end;</span>
<span class="p_add">+	int error;</span>
<span class="p_add">+	u32 hash;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mode &amp; ~(FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE))</span>
<span class="p_add">+		return -EOPNOTSUPP;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mode &amp; FALLOC_FL_PUNCH_HOLE)</span>
<span class="p_add">+		return hugetlbfs_punch_hole(inode, offset, len);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Default preallocate case.</span>
<span class="p_add">+	 * For this range, start is rounded down and end is rounded up</span>
<span class="p_add">+	 * as well as being converted to page offsets.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	start = offset &gt;&gt; hpage_shift;</span>
<span class="p_add">+	end = (offset + len + hpage_size - 1) &gt;&gt; hpage_shift;</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;inode-&gt;i_mutex);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We need to check rlimit even when FALLOC_FL_KEEP_SIZE */</span>
<span class="p_add">+	error = inode_newsize_ok(inode, offset + len);</span>
<span class="p_add">+	if (error)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Initialize a pseudo vma that just contains the policy used</span>
<span class="p_add">+	 * when allocating the huge pages.  The actual policy field</span>
<span class="p_add">+	 * (vm_policy) is determined based on the index in the loop below.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	memset(&amp;pseudo_vma, 0, sizeof(struct vm_area_struct));</span>
<span class="p_add">+	pseudo_vma.vm_flags = (VM_HUGETLB | VM_MAYSHARE | VM_SHARED);</span>
<span class="p_add">+	pseudo_vma.vm_file = file;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (index = start; index &lt; end; index++) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This is supposed to be the vaddr where the page is being</span>
<span class="p_add">+		 * faulted in, but we have no vaddr here.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		unsigned long addr;</span>
<span class="p_add">+		int avoid_reserve = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		cond_resched();</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * fallocate(2) manpage permits EINTR; we may have been</span>
<span class="p_add">+		 * interrupted because we are using up too much memory.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (signal_pending(current)) {</span>
<span class="p_add">+			error = -EINTR;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Get policy based on index */</span>
<span class="p_add">+		pseudo_vma.vm_policy =</span>
<span class="p_add">+			mpol_shared_policy_lookup(&amp;HUGETLBFS_I(inode)-&gt;policy,</span>
<span class="p_add">+							index);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* addr is the offset within the file (zero based) */</span>
<span class="p_add">+		addr = index * hpage_size;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* mutex taken here, fault path and hole punch */</span>
<span class="p_add">+		hash = hugetlb_fault_mutex_hash(h, mm, &amp;pseudo_vma, mapping,</span>
<span class="p_add">+						index, addr);</span>
<span class="p_add">+		mutex_lock(&amp;hugetlb_fault_mutex_table[hash]);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* See if already present in mapping to avoid alloc/free */</span>
<span class="p_add">+		page = find_get_page(mapping, index);</span>
<span class="p_add">+		if (page) {</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			mutex_unlock(&amp;hugetlb_fault_mutex_table[hash]);</span>
<span class="p_add">+			mpol_cond_put(pseudo_vma.vm_policy);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Allocate page and add to page cache */</span>
<span class="p_add">+		page = alloc_huge_page(&amp;pseudo_vma, addr, avoid_reserve);</span>
<span class="p_add">+		mpol_cond_put(pseudo_vma.vm_policy);</span>
<span class="p_add">+		if (IS_ERR(page)) {</span>
<span class="p_add">+			mutex_unlock(&amp;hugetlb_fault_mutex_table[hash]);</span>
<span class="p_add">+			error = PTR_ERR(page);</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		clear_huge_page(page, addr, pages_per_huge_page(h));</span>
<span class="p_add">+		__SetPageUptodate(page);</span>
<span class="p_add">+		error = huge_add_to_page_cache(page, mapping, index);</span>
<span class="p_add">+		if (unlikely(error)) {</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			mutex_unlock(&amp;hugetlb_fault_mutex_table[hash]);</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		mutex_unlock(&amp;hugetlb_fault_mutex_table[hash]);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * page_put due to reference from alloc_huge_page()</span>
<span class="p_add">+		 * unlock_page because locked by add_to_page_cache()</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		put_page(page);</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(mode &amp; FALLOC_FL_KEEP_SIZE) &amp;&amp; offset + len &gt; inode-&gt;i_size)</span>
<span class="p_add">+		i_size_write(inode, offset + len);</span>
<span class="p_add">+	inode-&gt;i_ctime = CURRENT_TIME;</span>
<span class="p_add">+	spin_lock(&amp;inode-&gt;i_lock);</span>
<span class="p_add">+	inode-&gt;i_private = NULL;</span>
<span class="p_add">+	spin_unlock(&amp;inode-&gt;i_lock);</span>
<span class="p_add">+out:</span>
<span class="p_add">+	mutex_unlock(&amp;inode-&gt;i_mutex);</span>
<span class="p_add">+	return error;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int hugetlbfs_setattr(struct dentry *dentry, struct iattr *attr)
 {
 	struct inode *inode = d_inode(dentry);
<span class="p_chunk">@@ -790,7 +945,8 @@</span> <span class="p_context"> const struct file_operations hugetlbfs_file_operations = {</span>
 	.mmap			= hugetlbfs_file_mmap,
 	.fsync			= noop_fsync,
 	.get_unmapped_area	= hugetlb_get_unmapped_area,
<span class="p_del">-	.llseek		= default_llseek,</span>
<span class="p_add">+	.llseek			= default_llseek,</span>
<span class="p_add">+	.fallocate		= hugetlbfs_fallocate,</span>
 };
 
 static const struct inode_operations hugetlbfs_dir_inode_operations = {
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 657ef26..386dcbf 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -330,6 +330,8 @@</span> <span class="p_context"> struct huge_bootmem_page {</span>
 #endif
 };
 
<span class="p_add">+struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
<span class="p_add">+				unsigned long addr, int avoid_reserve);</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_chunk">@@ -483,6 +485,7 @@</span> <span class="p_context"> static inline spinlock_t *huge_pte_lockptr(struct hstate *h,</span>
 
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
<span class="p_add">+#define alloc_huge_page(v, a, r) NULL</span>
 #define alloc_huge_page_node(h, nid) NULL
 #define alloc_huge_page_noerr(v, a, r) NULL
 #define alloc_bootmem_huge_page(h) NULL
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index aedc5e7..a775a65 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1726,7 +1726,7 @@</span> <span class="p_context"> static void vma_abort_reservation(struct hstate *h,</span>
 	(void)__vma_reservation_common(h, vma, addr, VMA_ABORT_RESV);
 }
 
<span class="p_del">-static struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
<span class="p_add">+struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 				    unsigned long addr, int avoid_reserve)
 {
 	struct hugepage_subpool *spool = subpool_vma(vma);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



