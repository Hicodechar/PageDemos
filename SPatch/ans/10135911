
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,pull] x86/pti: The real thing - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,pull] x86/pti: The real thing</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 28, 2017, 8:34 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;alpine.DEB.2.20.1712282122580.1899@nanos&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10135911/mbox/"
   >mbox</a>
|
   <a href="/patch/10135911/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10135911/">/patch/10135911/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	0369F60318 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 28 Dec 2017 20:35:15 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E1CCD2A23A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 28 Dec 2017 20:35:14 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id D5B262B1F6; Thu, 28 Dec 2017 20:35:14 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1E2E22B3EB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 28 Dec 2017 20:35:10 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755379AbdL1UfD (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 28 Dec 2017 15:35:03 -0500
Received: from Galois.linutronix.de ([146.0.238.70]:33305 &quot;EHLO
	Galois.linutronix.de&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1755320AbdL1Uef (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 28 Dec 2017 15:34:35 -0500
Received: from p4fea5f09.dip0.t-ipconnect.de ([79.234.95.9] helo=nanos)
	by Galois.linutronix.de with esmtpsa
	(TLS1.2:DHE_RSA_AES_256_CBC_SHA256:256) (Exim 4.80)
	(envelope-from &lt;tglx@linutronix.de&gt;)
	id 1eUeqv-0001Aq-2W; Thu, 28 Dec 2017 21:32:34 +0100
Date: Thu, 28 Dec 2017 21:34:29 +0100 (CET)
From: Thomas Gleixner &lt;tglx@linutronix.de&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
cc: LKML &lt;linux-kernel@vger.kernel.org&gt;, Ingo Molnar &lt;mingo@kernel.org&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;, Borislav Petkov &lt;bp@alien8.de&gt;
Subject: [GIT pull] x86/pti: The real thing
Message-ID: &lt;alpine.DEB.2.20.1712282122580.1899@nanos&gt;
User-Agent: Alpine 2.20 (DEB 67 2015-01-07)
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
X-Linutronix-Spam-Score: -1.0
X-Linutronix-Spam-Level: -
X-Linutronix-Spam-Status: No , -1.0 points, 5.0 required, ALL_TRUSTED=-1,
	SHORTCIRCUIT=-0.0001
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Dec. 28, 2017, 8:34 p.m.</div>
<pre class="content">
Linus,

please pull the latest x86-pti-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus

This is the final set of enabling page table isolation on x86:

  - Infrastructure patches for handling the extra page tables.

  - Patches which map the various bits and pieces which are required to get
    in and out of user space into the user space visible page tables.

  - The required changes to have CR3 switching in the entry/exit code.

  - Optimizations for the CR3 switching along with documentation how the
    ASID/PCID mechanism works.

  - Updates to dump pagetables to cover the user space page tables for W+X
    scans and extra debugfs files to analyze both the kernel and the user
    space visible page tables

  The whole functionality is compile time controlled via a config switch
  and can be turned on/off on the command line as well.

There is a trivial conflict in efi_alloc_page_tables() because the x86/pti
branch still has the __GFP_NOTRACK allocation flag as it&#39;s based on 4.14
for known reasons.

Thanks,

	tglx

------------------&gt;
Andy Lutomirski (6):
      x86/mm/pti: Add functions to clone kernel PMDs
      x86/mm/pti: Share cpu_entry_area with user space page tables
      x86/mm/pti: Map ESPFIX into user space
      x86/mm/64: Make a full PGD-entry size hole in the memory map
      x86/pti: Put the LDT in its own PGD if PTI is on
      x86/pti: Map the vsyscall page if needed

Borislav Petkov (2):
      x86/pti: Add the pti= cmdline option and documentation
      x86/mm/dump_pagetables: Add page table directory to the debugfs VFS hierarchy

Dave Hansen (10):
      x86/mm/pti: Disable global pages if PAGE_TABLE_ISOLATION=y
      x86/mm/pti: Prepare the x86/entry assembly code for entry/exit CR3 switching
      x86/mm/pti: Add mapping helper functions
      x86/mm/pti: Allow NX poison to be set in p4d/pgd
      x86/mm/pti: Allocate a separate user PGD
      x86/mm/pti: Populate user PGD
      x86/mm: Allow flushing for future ASID switches
      x86/mm: Abstract switching CR3
      x86/mm: Use INVPCID for __native_flush_tlb_single()
      x86/mm/pti: Add Kconfig

Hugh Dickins (1):
      x86/events/intel/ds: Map debug buffers in cpu_entry_area

Peter Zijlstra (3):
      x86/mm: Use/Fix PCID to optimize user/kernel switches
      x86/mm: Optimize RESTORE_CR3
      x86/mm: Clarify the whole ASID/kernel PCID/user PCID naming

Thomas Gleixner (9):
      x86/cpufeatures: Add X86_BUG_CPU_INSECURE
      x86/mm/pti: Add infrastructure for page table isolation
      x86/mm/pti: Force entry through trampoline when PTI active
      x86/entry: Align entry text section to PMD boundary
      x86/mm/pti: Share entry text PMD
      x86/cpu_entry_area: Add debugstore entries to cpu_entry_area
      x86/mm/dump_pagetables: Check user space page table for WX pages
      x86/mm/dump_pagetables: Allow dumping current pagetables
      x86/ldt: Make the LDT mapping RO

Vlastimil Babka (1):
      x86/dumpstack: Indicate in Oops whether PTI is configured and enabled


 Documentation/admin-guide/kernel-parameters.txt |   8 +
 Documentation/x86/x86_64/mm.txt                 |   5 +-
 arch/x86/boot/compressed/pagetable.c            |   3 +
 arch/x86/entry/calling.h                        | 145 +++++++++
 arch/x86/entry/entry_64.S                       |  48 ++-
 arch/x86/entry/entry_64_compat.S                |  24 +-
 arch/x86/entry/vsyscall/vsyscall_64.c           |   6 +-
 arch/x86/events/intel/ds.c                      | 130 +++++---
 arch/x86/events/perf_event.h                    |  23 +-
 arch/x86/include/asm/cpu_entry_area.h           |  13 +
 arch/x86/include/asm/cpufeatures.h              |   4 +-
 arch/x86/include/asm/desc.h                     |   2 +
 arch/x86/include/asm/disabled-features.h        |   8 +-
 arch/x86/include/asm/intel_ds.h                 |  36 +++
 arch/x86/include/asm/mmu_context.h              |  59 +++-
 arch/x86/include/asm/pgalloc.h                  |  11 +
 arch/x86/include/asm/pgtable.h                  |  30 +-
 arch/x86/include/asm/pgtable_64.h               |  92 ++++++
 arch/x86/include/asm/pgtable_64_types.h         |   8 +-
 arch/x86/include/asm/processor-flags.h          |   5 +
 arch/x86/include/asm/processor.h                |  23 +-
 arch/x86/include/asm/pti.h                      |  14 +
 arch/x86/include/asm/tlbflush.h                 | 202 +++++++++++--
 arch/x86/include/asm/vsyscall.h                 |   1 +
 arch/x86/include/uapi/asm/processor-flags.h     |   7 +-
 arch/x86/kernel/asm-offsets.c                   |   4 +
 arch/x86/kernel/cpu/common.c                    |   9 +-
 arch/x86/kernel/dumpstack.c                     |   6 +-
 arch/x86/kernel/head_64.S                       |  30 +-
 arch/x86/kernel/ldt.c                           | 144 ++++++++-
 arch/x86/kernel/tls.c                           |  11 +-
 arch/x86/kernel/vmlinux.lds.S                   |   8 +
 arch/x86/mm/Makefile                            |   7 +-
 arch/x86/mm/cpu_entry_area.c                    |  27 ++
 arch/x86/mm/debug_pagetables.c                  |  80 ++++-
 arch/x86/mm/dump_pagetables.c                   |  43 ++-
 arch/x86/mm/init.c                              |  80 +++--
 arch/x86/mm/pgtable.c                           |   5 +-
 arch/x86/mm/pti.c                               | 387 ++++++++++++++++++++++++
 arch/x86/mm/tlb.c                               |  58 +++-
 arch/x86/platform/efi/efi_64.c                  |   5 +-
 include/linux/pti.h                             |  11 +
 init/main.c                                     |   3 +
 security/Kconfig                                |  10 +
 tools/testing/selftests/x86/ldt_gdt.c           |   3 +-
 45 files changed, 1636 insertions(+), 202 deletions(-)
 create mode 100644 arch/x86/include/asm/intel_ds.h
 create mode 100644 arch/x86/include/asm/pti.h
 create mode 100644 arch/x86/mm/pti.c
 create mode 100644 include/linux/pti.h
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101">Christoph Hellwig</a> - Dec. 29, 2017, 9:24 a.m.</div>
<pre class="content">
So we have an entirely new memory model and this is sent after -rc5?

That&#39;s not really how things are supposed to work, are they?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Dec. 30, 2017, 1:42 a.m.</div>
<pre class="content">
On Thu, Dec 28, 2017 at 12:34 PM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; please pull the latest x86-pti-for-linus git tree from:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;    git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is the final set of enabling page table isolation on x86:</span>

Ok, after that late MCORE2 scare that held things up, this is now
merged in my tree and pushed out. &quot;WorksForMe(tm)&quot;.

                 Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174">Mike Galbraith</a> - Dec. 30, 2017, 9:04 a.m.</div>
<pre class="content">
On Fri, 2017-12-29 at 17:42 -0800, Linus Torvalds wrote:
<span class="quote">&gt; On Thu, Dec 28, 2017 at 12:34 PM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; please pull the latest x86-pti-for-linus git tree from:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;    git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This is the final set of enabling page table isolation on x86:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok, after that late MCORE2 scare that held things up, this is now</span>
<span class="quote">&gt; merged in my tree and pushed out. &quot;WorksForMe(tm)&quot;.</span>

But it definitely ain&#39;t free.  NFS hanging out in the kernel for 82ms
at a whack may not be such a bad idea after all ;-)

i4790+tbench

CONFIG_PAGE_TABLE_ISOLATION=y
Throughput 3467.72 MB/sec  8 clients  8 procs  max_latency=3.499 ms

CONFIG_PAGE_TABLE_ISOLATION=n
Throughput 3700.81 MB/sec  8 clients  8 procs  max_latency=4.893 ms

CONFIG_PAGE_TABLE_ISOLATION=y                CONFIG_PAGE_TABLE_ISOLATION=n
4.48%  [k] syscall_return_via_sysret         3.56%  [k] copy_user_enhanced_fast_string
3.41%  [k] copy_user_enhanced_fast_string    2.54%  [.] child_run
2.48%  [.] child_run                         1.76%  [k] tcp_transmit_skb
1.64%  [.] __strchr_sse2                     1.74%  [k] tcp_recvmsg
1.60%  [k] tcp_transmit_skb                  1.69%  [.] __strchr_sse2
1.55%  [k] tcp_recvmsg                       1.56%  [.] __GI_____strtoll_l_internal
1.46%  [k] ipt_do_table                      1.44%  [k] ipt_do_table
1.45%  [.] __GI_____strtoll_l_internal       1.42%  [k] tcp_sendmsg_locked
1.25%  [k] tcp_in_window                     1.34%  [k] tcp_write_xmit
1.21%  [.] next_token                        1.34%  [k] tcp_clean_rtx_queue
1.17%  [k] tcp_clean_rtx_queue               1.26%  [.] next_token
1.16%  [k] tcp_sendmsg_locked                1.24%  [k] tcp_in_window
1.15%  [k] __switch_to                       1.19%  [k] nf_conntrack_in
1.10%  [k] switch_mm_irqs_off                1.17%  [k] __switch_to
1.09%  [k] tcp_write_xmit                    1.13%  [.] __strcasecmp_l_avx
1.09%  [k] tcp_ack                           1.10%  [k] switch_mm_irqs_off
1.08%  [k] nf_hook_slow                      1.08%  [k] tcp_v4_rcv
1.04%  [k] __sched_text_start                1.06%  [k] nf_hook_slow
0.99%  [.] __strcasecmp_l_avx                1.05%  [k] tcp_ack
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=178165">François Valenduc</a> - Dec. 31, 2017, 8:40 a.m.</div>
<pre class="content">
Le 30/12/17 à 10:04, Mike Galbraith a écrit :
<span class="quote">&gt; On Fri, 2017-12-29 at 17:42 -0800, Linus Torvalds wrote:</span>
<span class="quote">&gt;&gt; On Thu, Dec 28, 2017 at 12:34 PM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; please pull the latest x86-pti-for-linus git tree from:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;    git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This is the final set of enabling page table isolation on x86:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Ok, after that late MCORE2 scare that held things up, this is now</span>
<span class="quote">&gt;&gt; merged in my tree and pushed out. &quot;WorksForMe(tm)&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But it definitely ain&#39;t free.  NFS hanging out in the kernel for 82ms</span>
<span class="quote">&gt; at a whack may not be such a bad idea after all ;-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; i4790+tbench</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CONFIG_PAGE_TABLE_ISOLATION=y</span>
<span class="quote">&gt; Throughput 3467.72 MB/sec  8 clients  8 procs  max_latency=3.499 ms</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CONFIG_PAGE_TABLE_ISOLATION=n</span>
<span class="quote">&gt; Throughput 3700.81 MB/sec  8 clients  8 procs  max_latency=4.893 ms</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; CONFIG_PAGE_TABLE_ISOLATION=y                CONFIG_PAGE_TABLE_ISOLATION=n</span>
<span class="quote">&gt; 4.48%  [k] syscall_return_via_sysret         3.56%  [k] copy_user_enhanced_fast_string</span>
<span class="quote">&gt; 3.41%  [k] copy_user_enhanced_fast_string    2.54%  [.] child_run</span>
<span class="quote">&gt; 2.48%  [.] child_run                         1.76%  [k] tcp_transmit_skb</span>
<span class="quote">&gt; 1.64%  [.] __strchr_sse2                     1.74%  [k] tcp_recvmsg</span>
<span class="quote">&gt; 1.60%  [k] tcp_transmit_skb                  1.69%  [.] __strchr_sse2</span>
<span class="quote">&gt; 1.55%  [k] tcp_recvmsg                       1.56%  [.] __GI_____strtoll_l_internal</span>
<span class="quote">&gt; 1.46%  [k] ipt_do_table                      1.44%  [k] ipt_do_table</span>
<span class="quote">&gt; 1.45%  [.] __GI_____strtoll_l_internal       1.42%  [k] tcp_sendmsg_locked</span>
<span class="quote">&gt; 1.25%  [k] tcp_in_window                     1.34%  [k] tcp_write_xmit</span>
<span class="quote">&gt; 1.21%  [.] next_token                        1.34%  [k] tcp_clean_rtx_queue</span>
<span class="quote">&gt; 1.17%  [k] tcp_clean_rtx_queue               1.26%  [.] next_token</span>
<span class="quote">&gt; 1.16%  [k] tcp_sendmsg_locked                1.24%  [k] tcp_in_window</span>
<span class="quote">&gt; 1.15%  [k] __switch_to                       1.19%  [k] nf_conntrack_in</span>
<span class="quote">&gt; 1.10%  [k] switch_mm_irqs_off                1.17%  [k] __switch_to</span>
<span class="quote">&gt; 1.09%  [k] tcp_write_xmit                    1.13%  [.] __strcasecmp_l_avx</span>
<span class="quote">&gt; 1.09%  [k] tcp_ack                           1.10%  [k] switch_mm_irqs_off</span>
<span class="quote">&gt; 1.08%  [k] nf_hook_slow                      1.08%  [k] tcp_v4_rcv</span>
<span class="quote">&gt; 1.04%  [k] __sched_text_start                1.06%  [k] nf_hook_slow</span>
<span class="quote">&gt; 0.99%  [.] __strcasecmp_l_avx                1.05%  [k] tcp_ack</span>
<span class="quote">&gt; </span>

The KConfig text refers to Documentation/x86/pagetable-isolation.txt,
which doesn&#39;t exists...

François Valenduc
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">index 05496622b4ef..520fdec15bbb 100644</span>
<span class="p_header">--- a/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2685,6 +2685,8 @@</span> <span class="p_context"></span>
 			steal time is computed, but won&#39;t influence scheduler
 			behaviour
 
<span class="p_add">+	nopti		[X86-64] Disable kernel page table isolation</span>
<span class="p_add">+</span>
 	nolapic		[X86-32,APIC] Do not enable or use the local APIC.
 
 	nolapic_timer	[X86-32,APIC] Do not use the local APIC timer.
<span class="p_chunk">@@ -3253,6 +3255,12 @@</span> <span class="p_context"></span>
 	pt.		[PARIDE]
 			See Documentation/blockdev/paride.txt.
 
<span class="p_add">+	pti=		[X86_64]</span>
<span class="p_add">+			Control user/kernel address space isolation:</span>
<span class="p_add">+			on - enable</span>
<span class="p_add">+			off - disable</span>
<span class="p_add">+			auto - default setting</span>
<span class="p_add">+</span>
 	pty.legacy_count=
 			[KNL] Number of legacy pty&#39;s. Overwrites compiled-in
 			default number.
<span class="p_header">diff --git a/Documentation/x86/x86_64/mm.txt b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">index 51101708a03a..ad41b3813f0a 100644</span>
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_add">+fffffe0000000000 - fffffe7fffffffff (=39 bits) LDT remap for PTI</span>
 fffffe8000000000 - fffffeffffffffff (=39 bits) cpu_entry_area mapping
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
<span class="p_chunk">@@ -29,8 +30,8 @@</span> <span class="p_context"> Virtual memory map with 5 level page tables:</span>
 hole caused by [56:63] sign extension
 ff00000000000000 - ff0fffffffffffff (=52 bits) guard hole, reserved for hypervisor
 ff10000000000000 - ff8fffffffffffff (=55 bits) direct mapping of all phys. memory
<span class="p_del">-ff90000000000000 - ff91ffffffffffff (=49 bits) hole</span>
<span class="p_del">-ff92000000000000 - ffd1ffffffffffff (=54 bits) vmalloc/ioremap space</span>
<span class="p_add">+ff90000000000000 - ff9fffffffffffff (=52 bits) LDT remap for PTI</span>
<span class="p_add">+ffa0000000000000 - ffd1ffffffffffff (=54 bits) vmalloc/ioremap space (12800 TB)</span>
 ffd2000000000000 - ffd3ffffffffffff (=49 bits) hole
 ffd4000000000000 - ffd5ffffffffffff (=49 bits) virtual memory map (512TB)
 ... unused hole ...
<span class="p_header">diff --git a/arch/x86/boot/compressed/pagetable.c b/arch/x86/boot/compressed/pagetable.c</span>
<span class="p_header">index 972319ff5b01..e691ff734cb5 100644</span>
<span class="p_header">--- a/arch/x86/boot/compressed/pagetable.c</span>
<span class="p_header">+++ b/arch/x86/boot/compressed/pagetable.c</span>
<span class="p_chunk">@@ -23,6 +23,9 @@</span> <span class="p_context"></span>
  */
 #undef CONFIG_AMD_MEM_ENCRYPT
 
<span class="p_add">+/* No PAGE_TABLE_ISOLATION support needed either: */</span>
<span class="p_add">+#undef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+</span>
 #include &quot;misc.h&quot;
 
 /* These actually do the work of building the kernel identity maps. */
<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index 3fd8bc560fae..45a63e00a6af 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -1,6 +1,11 @@</span> <span class="p_context"></span>
 /* SPDX-License-Identifier: GPL-2.0 */
 #include &lt;linux/jump_label.h&gt;
 #include &lt;asm/unwind_hints.h&gt;
<span class="p_add">+#include &lt;asm/cpufeatures.h&gt;</span>
<span class="p_add">+#include &lt;asm/page_types.h&gt;</span>
<span class="p_add">+#include &lt;asm/percpu.h&gt;</span>
<span class="p_add">+#include &lt;asm/asm-offsets.h&gt;</span>
<span class="p_add">+#include &lt;asm/processor-flags.h&gt;</span>
 
 /*
 
<span class="p_chunk">@@ -187,6 +192,146 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 #endif
 .endm
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PAGE_TABLE_ISOLATION PGDs are 8k.  Flip bit 12 to switch between the two</span>
<span class="p_add">+ * halves:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PTI_SWITCH_PGTABLES_MASK	(1&lt;&lt;PAGE_SHIFT)</span>
<span class="p_add">+#define PTI_SWITCH_MASK		(PTI_SWITCH_PGTABLES_MASK|(1&lt;&lt;X86_CR3_PTI_SWITCH_BIT))</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SET_NOFLUSH_BIT	reg:req</span>
<span class="p_add">+	bts	$X86_CR3_PCID_NOFLUSH_BIT, \reg</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro ADJUST_KERNEL_CR3 reg:req</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;SET_NOFLUSH_BIT \reg&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+	/* Clear PCID and &quot;PAGE_TABLE_ISOLATION bit&quot;, point CR3 at kernel pagetables: */</span>
<span class="p_add">+	andq    $(~PTI_SWITCH_MASK), \reg</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_TO_KERNEL_CR3 scratch_reg:req</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="p_add">+	mov	%cr3, \scratch_reg</span>
<span class="p_add">+	ADJUST_KERNEL_CR3 \scratch_reg</span>
<span class="p_add">+	mov	\scratch_reg, %cr3</span>
<span class="p_add">+.Lend_\@:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#define THIS_CPU_user_pcid_flush_mask   \</span>
<span class="p_add">+	PER_CPU_VAR(cpu_tlbstate) + TLB_STATE_user_pcid_flush_mask</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_TO_USER_CR3_NOSTACK scratch_reg:req scratch_reg2:req</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="p_add">+	mov	%cr3, \scratch_reg</span>
<span class="p_add">+</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lwrcr3_\@&quot;, &quot;&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Test if the ASID needs a flush.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq	\scratch_reg, \scratch_reg2</span>
<span class="p_add">+	andq	$(0x7FF), \scratch_reg		/* mask ASID */</span>
<span class="p_add">+	bt	\scratch_reg, THIS_CPU_user_pcid_flush_mask</span>
<span class="p_add">+	jnc	.Lnoflush_\@</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Flush needed, clear the bit */</span>
<span class="p_add">+	btr	\scratch_reg, THIS_CPU_user_pcid_flush_mask</span>
<span class="p_add">+	movq	\scratch_reg2, \scratch_reg</span>
<span class="p_add">+	jmp	.Lwrcr3_\@</span>
<span class="p_add">+</span>
<span class="p_add">+.Lnoflush_\@:</span>
<span class="p_add">+	movq	\scratch_reg2, \scratch_reg</span>
<span class="p_add">+	SET_NOFLUSH_BIT \scratch_reg</span>
<span class="p_add">+</span>
<span class="p_add">+.Lwrcr3_\@:</span>
<span class="p_add">+	/* Flip the PGD and ASID to the user version */</span>
<span class="p_add">+	orq     $(PTI_SWITCH_MASK), \scratch_reg</span>
<span class="p_add">+	mov	\scratch_reg, %cr3</span>
<span class="p_add">+.Lend_\@:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_TO_USER_CR3_STACK	scratch_reg:req</span>
<span class="p_add">+	pushq	%rax</span>
<span class="p_add">+	SWITCH_TO_USER_CR3_NOSTACK scratch_reg=\scratch_reg scratch_reg2=%rax</span>
<span class="p_add">+	popq	%rax</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SAVE_AND_SWITCH_TO_KERNEL_CR3 scratch_reg:req save_reg:req</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Ldone_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="p_add">+	movq	%cr3, \scratch_reg</span>
<span class="p_add">+	movq	\scratch_reg, \save_reg</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Is the &quot;switch mask&quot; all zero?  That means that both of</span>
<span class="p_add">+	 * these are zero:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *	1. The user/kernel PCID bit, and</span>
<span class="p_add">+	 *	2. The user/kernel &quot;bit&quot; that points CR3 to the</span>
<span class="p_add">+	 *	   bottom half of the 8k PGD</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * That indicates a kernel CR3 value, not a user CR3.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	testq	$(PTI_SWITCH_MASK), \scratch_reg</span>
<span class="p_add">+	jz	.Ldone_\@</span>
<span class="p_add">+</span>
<span class="p_add">+	ADJUST_KERNEL_CR3 \scratch_reg</span>
<span class="p_add">+	movq	\scratch_reg, %cr3</span>
<span class="p_add">+</span>
<span class="p_add">+.Ldone_\@:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro RESTORE_CR3 scratch_reg:req save_reg:req</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="p_add">+</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lwrcr3_\@&quot;, &quot;&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * KERNEL pages can always resume with NOFLUSH as we do</span>
<span class="p_add">+	 * explicit flushes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	bt	$X86_CR3_PTI_SWITCH_BIT, \save_reg</span>
<span class="p_add">+	jnc	.Lnoflush_\@</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Check if there&#39;s a pending flush for the user ASID we&#39;re</span>
<span class="p_add">+	 * about to set.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq	\save_reg, \scratch_reg</span>
<span class="p_add">+	andq	$(0x7FF), \scratch_reg</span>
<span class="p_add">+	bt	\scratch_reg, THIS_CPU_user_pcid_flush_mask</span>
<span class="p_add">+	jnc	.Lnoflush_\@</span>
<span class="p_add">+</span>
<span class="p_add">+	btr	\scratch_reg, THIS_CPU_user_pcid_flush_mask</span>
<span class="p_add">+	jmp	.Lwrcr3_\@</span>
<span class="p_add">+</span>
<span class="p_add">+.Lnoflush_\@:</span>
<span class="p_add">+	SET_NOFLUSH_BIT \save_reg</span>
<span class="p_add">+</span>
<span class="p_add">+.Lwrcr3_\@:</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The CR3 write could be avoided when not changing its value,</span>
<span class="p_add">+	 * but would require a CR3 read *and* a scratch register.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq	\save_reg, %cr3</span>
<span class="p_add">+.Lend_\@:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_PAGE_TABLE_ISOLATION=n: */</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_TO_KERNEL_CR3 scratch_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SWITCH_TO_USER_CR3_NOSTACK scratch_reg:req scratch_reg2:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SWITCH_TO_USER_CR3_STACK scratch_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SAVE_AND_SWITCH_TO_KERNEL_CR3 scratch_reg:req save_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro RESTORE_CR3 scratch_reg:req save_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif /* CONFIG_X86_64 */
 
 /*
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index 87cebe78bbef..ed31d00dc5ee 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -23,7 +23,6 @@</span> <span class="p_context"></span>
 #include &lt;asm/segment.h&gt;
 #include &lt;asm/cache.h&gt;
 #include &lt;asm/errno.h&gt;
<span class="p_del">-#include &quot;calling.h&quot;</span>
 #include &lt;asm/asm-offsets.h&gt;
 #include &lt;asm/msr.h&gt;
 #include &lt;asm/unistd.h&gt;
<span class="p_chunk">@@ -40,6 +39,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/frame.h&gt;
 #include &lt;linux/err.h&gt;
 
<span class="p_add">+#include &quot;calling.h&quot;</span>
<span class="p_add">+</span>
 .code64
 .section .entry.text, &quot;ax&quot;
 
<span class="p_chunk">@@ -164,6 +165,9 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_64_trampoline)</span>
 	/* Stash the user RSP. */
 	movq	%rsp, RSP_SCRATCH
 
<span class="p_add">+	/* Note: using %rsp as a scratch reg. */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp</span>
<span class="p_add">+</span>
 	/* Load the top of the task stack into RSP */
 	movq	CPU_ENTRY_AREA_tss + TSS_sp1 + CPU_ENTRY_AREA, %rsp
 
<span class="p_chunk">@@ -203,6 +207,10 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_64)</span>
 	 */
 
 	swapgs
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This path is not taken when PAGE_TABLE_ISOLATION is disabled so it</span>
<span class="p_add">+	 * is not required to switch CR3.</span>
<span class="p_add">+	 */</span>
 	movq	%rsp, PER_CPU_VAR(rsp_scratch)
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 
<span class="p_chunk">@@ -399,6 +407,7 @@</span> <span class="p_context"> syscall_return_via_sysret:</span>
 	 * We are on the trampoline stack.  All regs except RDI are live.
 	 * We can do future final exit work right here.
 	 */
<span class="p_add">+	SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi</span>
 
 	popq	%rdi
 	popq	%rsp
<span class="p_chunk">@@ -736,6 +745,8 @@</span> <span class="p_context"> GLOBAL(swapgs_restore_regs_and_return_to_usermode)</span>
 	 * We can do future final exit work right here.
 	 */
 
<span class="p_add">+	SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi</span>
<span class="p_add">+</span>
 	/* Restore RDI. */
 	popq	%rdi
 	SWAPGS
<span class="p_chunk">@@ -818,7 +829,9 @@</span> <span class="p_context"> native_irq_return_ldt:</span>
 	 */
 
 	pushq	%rdi				/* Stash user RDI */
<span class="p_del">-	SWAPGS</span>
<span class="p_add">+	SWAPGS					/* to kernel GS */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdi	/* to kernel CR3 */</span>
<span class="p_add">+</span>
 	movq	PER_CPU_VAR(espfix_waddr), %rdi
 	movq	%rax, (0*8)(%rdi)		/* user RAX */
 	movq	(1*8)(%rsp), %rax		/* user RIP */
<span class="p_chunk">@@ -834,7 +847,6 @@</span> <span class="p_context"> native_irq_return_ldt:</span>
 	/* Now RAX == RSP. */
 
 	andl	$0xffff0000, %eax		/* RAX = (RSP &amp; 0xffff0000) */
<span class="p_del">-	popq	%rdi				/* Restore user RDI */</span>
 
 	/*
 	 * espfix_stack[31:16] == 0.  The page tables are set up such that
<span class="p_chunk">@@ -845,7 +857,11 @@</span> <span class="p_context"> native_irq_return_ldt:</span>
 	 * still points to an RO alias of the ESPFIX stack.
 	 */
 	orq	PER_CPU_VAR(espfix_stack), %rax
<span class="p_del">-	SWAPGS</span>
<span class="p_add">+</span>
<span class="p_add">+	SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi</span>
<span class="p_add">+	SWAPGS					/* to user GS */</span>
<span class="p_add">+	popq	%rdi				/* Restore user RDI */</span>
<span class="p_add">+</span>
 	movq	%rax, %rsp
 	UNWIND_HINT_IRET_REGS offset=8
 
<span class="p_chunk">@@ -945,6 +961,8 @@</span> <span class="p_context"> ENTRY(switch_to_thread_stack)</span>
 	UNWIND_HINT_FUNC
 
 	pushq	%rdi
<span class="p_add">+	/* Need to switch before accessing the thread stack. */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdi</span>
 	movq	%rsp, %rdi
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 	UNWIND_HINT sp_offset=16 sp_reg=ORC_REG_DI
<span class="p_chunk">@@ -1244,7 +1262,11 @@</span> <span class="p_context"> ENTRY(paranoid_entry)</span>
 	js	1f				/* negative -&gt; in kernel */
 	SWAPGS
 	xorl	%ebx, %ebx
<span class="p_del">-1:	ret</span>
<span class="p_add">+</span>
<span class="p_add">+1:</span>
<span class="p_add">+	SAVE_AND_SWITCH_TO_KERNEL_CR3 scratch_reg=%rax save_reg=%r14</span>
<span class="p_add">+</span>
<span class="p_add">+	ret</span>
 END(paranoid_entry)
 
 /*
<span class="p_chunk">@@ -1266,6 +1288,7 @@</span> <span class="p_context"> ENTRY(paranoid_exit)</span>
 	testl	%ebx, %ebx			/* swapgs needed? */
 	jnz	.Lparanoid_exit_no_swapgs
 	TRACE_IRQS_IRETQ
<span class="p_add">+	RESTORE_CR3	scratch_reg=%rbx save_reg=%r14</span>
 	SWAPGS_UNSAFE_STACK
 	jmp	.Lparanoid_exit_restore
 .Lparanoid_exit_no_swapgs:
<span class="p_chunk">@@ -1293,6 +1316,8 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 	 * from user mode due to an IRET fault.
 	 */
 	SWAPGS
<span class="p_add">+	/* We have user CR3.  Change to kernel CR3. */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax</span>
 
 .Lerror_entry_from_usermode_after_swapgs:
 	/* Put us onto the real thread stack. */
<span class="p_chunk">@@ -1339,6 +1364,7 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 	 * .Lgs_change&#39;s error handler with kernel gsbase.
 	 */
 	SWAPGS
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax</span>
 	jmp .Lerror_entry_done
 
 .Lbstep_iret:
<span class="p_chunk">@@ -1348,10 +1374,11 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 
 .Lerror_bad_iret:
 	/*
<span class="p_del">-	 * We came from an IRET to user mode, so we have user gsbase.</span>
<span class="p_del">-	 * Switch to kernel gsbase:</span>
<span class="p_add">+	 * We came from an IRET to user mode, so we have user</span>
<span class="p_add">+	 * gsbase and CR3.  Switch to kernel gsbase and CR3:</span>
 	 */
 	SWAPGS
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax</span>
 
 	/*
 	 * Pretend that the exception came from user mode: set up pt_regs
<span class="p_chunk">@@ -1383,6 +1410,10 @@</span> <span class="p_context"> END(error_exit)</span>
 /*
  * Runs on exception stack.  Xen PV does not go through this path at all,
  * so we can use real assembly here.
<span class="p_add">+ *</span>
<span class="p_add">+ * Registers:</span>
<span class="p_add">+ *	%r14: Used to save/restore the CR3 of the interrupted context</span>
<span class="p_add">+ *	      when PAGE_TABLE_ISOLATION is in use.  Do not clobber.</span>
  */
 ENTRY(nmi)
 	UNWIND_HINT_IRET_REGS
<span class="p_chunk">@@ -1446,6 +1477,7 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 
 	swapgs
 	cld
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdx</span>
 	movq	%rsp, %rdx
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 	UNWIND_HINT_IRET_REGS base=%rdx offset=8
<span class="p_chunk">@@ -1698,6 +1730,8 @@</span> <span class="p_context"> end_repeat_nmi:</span>
 	movq	$-1, %rsi
 	call	do_nmi
 
<span class="p_add">+	RESTORE_CR3 scratch_reg=%r15 save_reg=%r14</span>
<span class="p_add">+</span>
 	testl	%ebx, %ebx			/* swapgs needed? */
 	jnz	nmi_restore
 nmi_swapgs:
<span class="p_header">diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">index 95ad40eb7eff..40f17009ec20 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_chunk">@@ -49,6 +49,10 @@</span> <span class="p_context"></span>
 ENTRY(entry_SYSENTER_compat)
 	/* Interrupts are off on entry. */
 	SWAPGS
<span class="p_add">+</span>
<span class="p_add">+	/* We are about to clobber %rsp anyway, clobbering here is OK */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp</span>
<span class="p_add">+</span>
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 
 	/*
<span class="p_chunk">@@ -216,6 +220,12 @@</span> <span class="p_context"> GLOBAL(entry_SYSCALL_compat_after_hwframe)</span>
 	pushq   $0			/* pt_regs-&gt;r15 = 0 */
 
 	/*
<span class="p_add">+	 * We just saved %rdi so it is safe to clobber.  It is not</span>
<span class="p_add">+	 * preserved during the C calls inside TRACE_IRQS_OFF anyway.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdi</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
 	 * User mode is traced as though IRQs are on, and SYSENTER
 	 * turned them off.
 	 */
<span class="p_chunk">@@ -256,10 +266,22 @@</span> <span class="p_context"> sysret32_from_system_call:</span>
 	 * when the system call started, which is already known to user
 	 * code.  We zero R8-R10 to avoid info leaks.
          */
<span class="p_add">+	movq	RSP-ORIG_RAX(%rsp), %rsp</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The original userspace %rsp (RSP-ORIG_RAX(%rsp)) is stored</span>
<span class="p_add">+	 * on the process stack which is not mapped to userspace and</span>
<span class="p_add">+	 * not readable after we SWITCH_TO_USER_CR3.  Delay the CR3</span>
<span class="p_add">+	 * switch until after after the last reference to the process</span>
<span class="p_add">+	 * stack.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * %r8/%r9 are zeroed before the sysret, thus safe to clobber.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_TO_USER_CR3_NOSTACK scratch_reg=%r8 scratch_reg2=%r9</span>
<span class="p_add">+</span>
 	xorq	%r8, %r8
 	xorq	%r9, %r9
 	xorq	%r10, %r10
<span class="p_del">-	movq	RSP-ORIG_RAX(%rsp), %rsp</span>
 	swapgs
 	sysretl
 END(entry_SYSCALL_compat)
<span class="p_header">diff --git a/arch/x86/entry/vsyscall/vsyscall_64.c b/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_header">index 1faf40f2dda9..577fa8adb785 100644</span>
<span class="p_header">--- a/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_header">+++ b/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_chunk">@@ -344,14 +344,14 @@</span> <span class="p_context"> int in_gate_area_no_mm(unsigned long addr)</span>
  * vsyscalls but leave the page not present.  If so, we skip calling
  * this.
  */
<span class="p_del">-static void __init set_vsyscall_pgtable_user_bits(void)</span>
<span class="p_add">+void __init set_vsyscall_pgtable_user_bits(pgd_t *root)</span>
 {
 	pgd_t *pgd;
 	p4d_t *p4d;
 	pud_t *pud;
 	pmd_t *pmd;
 
<span class="p_del">-	pgd = pgd_offset_k(VSYSCALL_ADDR);</span>
<span class="p_add">+	pgd = pgd_offset_pgd(root, VSYSCALL_ADDR);</span>
 	set_pgd(pgd, __pgd(pgd_val(*pgd) | _PAGE_USER));
 	p4d = p4d_offset(pgd, VSYSCALL_ADDR);
 #if CONFIG_PGTABLE_LEVELS &gt;= 5
<span class="p_chunk">@@ -373,7 +373,7 @@</span> <span class="p_context"> void __init map_vsyscall(void)</span>
 			     vsyscall_mode == NATIVE
 			     ? PAGE_KERNEL_VSYSCALL
 			     : PAGE_KERNEL_VVAR);
<span class="p_del">-		set_vsyscall_pgtable_user_bits();</span>
<span class="p_add">+		set_vsyscall_pgtable_user_bits(swapper_pg_dir);</span>
 	}
 
 	BUILD_BUG_ON((unsigned long)__fix_to_virt(VSYSCALL_PAGE) !=
<span class="p_header">diff --git a/arch/x86/events/intel/ds.c b/arch/x86/events/intel/ds.c</span>
<span class="p_header">index 3674a4b6f8bd..8f0aace08b87 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/ds.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/ds.c</span>
<span class="p_chunk">@@ -3,16 +3,18 @@</span> <span class="p_context"></span>
 #include &lt;linux/types.h&gt;
 #include &lt;linux/slab.h&gt;
 
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 #include &lt;asm/perf_event.h&gt;
 #include &lt;asm/insn.h&gt;
 
 #include &quot;../perf_event.h&quot;
 
<span class="p_add">+/* Waste a full page so it can be mapped into the cpu_entry_area */</span>
<span class="p_add">+DEFINE_PER_CPU_PAGE_ALIGNED(struct debug_store, cpu_debug_store);</span>
<span class="p_add">+</span>
 /* The size of a BTS record in bytes: */
 #define BTS_RECORD_SIZE		24
 
<span class="p_del">-#define BTS_BUFFER_SIZE		(PAGE_SIZE &lt;&lt; 4)</span>
<span class="p_del">-#define PEBS_BUFFER_SIZE	(PAGE_SIZE &lt;&lt; 4)</span>
 #define PEBS_FIXUP_SIZE		PAGE_SIZE
 
 /*
<span class="p_chunk">@@ -279,17 +281,52 @@</span> <span class="p_context"> void fini_debug_store_on_cpu(int cpu)</span>
 
 static DEFINE_PER_CPU(void *, insn_buffer);
 
<span class="p_del">-static int alloc_pebs_buffer(int cpu)</span>
<span class="p_add">+static void ds_update_cea(void *cea, void *addr, size_t size, pgprot_t prot)</span>
 {
<span class="p_del">-	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;</span>
<span class="p_add">+	phys_addr_t pa;</span>
<span class="p_add">+	size_t msz = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pa = virt_to_phys(addr);</span>
<span class="p_add">+	for (; msz &lt; size; msz += PAGE_SIZE, pa += PAGE_SIZE, cea += PAGE_SIZE)</span>
<span class="p_add">+		cea_set_pte(cea, pa, prot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void ds_clear_cea(void *cea, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	size_t msz = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; msz &lt; size; msz += PAGE_SIZE, cea += PAGE_SIZE)</span>
<span class="p_add">+		cea_set_pte(cea, 0, PAGE_NONE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void *dsalloc_pages(size_t size, gfp_t flags, int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int order = get_order(size);</span>
 	int node = cpu_to_node(cpu);
<span class="p_del">-	int max;</span>
<span class="p_del">-	void *buffer, *ibuffer;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	page = __alloc_pages_node(node, flags | __GFP_ZERO, order);</span>
<span class="p_add">+	return page ? page_address(page) : NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dsfree_pages(const void *buffer, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (buffer)</span>
<span class="p_add">+		free_pages((unsigned long)buffer, get_order(size));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int alloc_pebs_buffer(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpu_hw_events *hwev = per_cpu_ptr(&amp;cpu_hw_events, cpu);</span>
<span class="p_add">+	struct debug_store *ds = hwev-&gt;ds;</span>
<span class="p_add">+	size_t bsiz = x86_pmu.pebs_buffer_size;</span>
<span class="p_add">+	int max, node = cpu_to_node(cpu);</span>
<span class="p_add">+	void *buffer, *ibuffer, *cea;</span>
 
 	if (!x86_pmu.pebs)
 		return 0;
 
<span class="p_del">-	buffer = kzalloc_node(x86_pmu.pebs_buffer_size, GFP_KERNEL, node);</span>
<span class="p_add">+	buffer = dsalloc_pages(bsiz, GFP_KERNEL, cpu);</span>
 	if (unlikely(!buffer))
 		return -ENOMEM;
 
<span class="p_chunk">@@ -300,25 +337,27 @@</span> <span class="p_context"> static int alloc_pebs_buffer(int cpu)</span>
 	if (x86_pmu.intel_cap.pebs_format &lt; 2) {
 		ibuffer = kzalloc_node(PEBS_FIXUP_SIZE, GFP_KERNEL, node);
 		if (!ibuffer) {
<span class="p_del">-			kfree(buffer);</span>
<span class="p_add">+			dsfree_pages(buffer, bsiz);</span>
 			return -ENOMEM;
 		}
 		per_cpu(insn_buffer, cpu) = ibuffer;
 	}
<span class="p_del">-</span>
<span class="p_del">-	max = x86_pmu.pebs_buffer_size / x86_pmu.pebs_record_size;</span>
<span class="p_del">-</span>
<span class="p_del">-	ds-&gt;pebs_buffer_base = (u64)(unsigned long)buffer;</span>
<span class="p_add">+	hwev-&gt;ds_pebs_vaddr = buffer;</span>
<span class="p_add">+	/* Update the cpu entry area mapping */</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_buffers.pebs_buffer;</span>
<span class="p_add">+	ds-&gt;pebs_buffer_base = (unsigned long) cea;</span>
<span class="p_add">+	ds_update_cea(cea, buffer, bsiz, PAGE_KERNEL);</span>
 	ds-&gt;pebs_index = ds-&gt;pebs_buffer_base;
<span class="p_del">-	ds-&gt;pebs_absolute_maximum = ds-&gt;pebs_buffer_base +</span>
<span class="p_del">-		max * x86_pmu.pebs_record_size;</span>
<span class="p_del">-</span>
<span class="p_add">+	max = x86_pmu.pebs_record_size * (bsiz / x86_pmu.pebs_record_size);</span>
<span class="p_add">+	ds-&gt;pebs_absolute_maximum = ds-&gt;pebs_buffer_base + max;</span>
 	return 0;
 }
 
 static void release_pebs_buffer(int cpu)
 {
<span class="p_del">-	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;</span>
<span class="p_add">+	struct cpu_hw_events *hwev = per_cpu_ptr(&amp;cpu_hw_events, cpu);</span>
<span class="p_add">+	struct debug_store *ds = hwev-&gt;ds;</span>
<span class="p_add">+	void *cea;</span>
 
 	if (!ds || !x86_pmu.pebs)
 		return;
<span class="p_chunk">@@ -326,73 +365,70 @@</span> <span class="p_context"> static void release_pebs_buffer(int cpu)</span>
 	kfree(per_cpu(insn_buffer, cpu));
 	per_cpu(insn_buffer, cpu) = NULL;
 
<span class="p_del">-	kfree((void *)(unsigned long)ds-&gt;pebs_buffer_base);</span>
<span class="p_add">+	/* Clear the fixmap */</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_buffers.pebs_buffer;</span>
<span class="p_add">+	ds_clear_cea(cea, x86_pmu.pebs_buffer_size);</span>
 	ds-&gt;pebs_buffer_base = 0;
<span class="p_add">+	dsfree_pages(hwev-&gt;ds_pebs_vaddr, x86_pmu.pebs_buffer_size);</span>
<span class="p_add">+	hwev-&gt;ds_pebs_vaddr = NULL;</span>
 }
 
 static int alloc_bts_buffer(int cpu)
 {
<span class="p_del">-	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;</span>
<span class="p_del">-	int node = cpu_to_node(cpu);</span>
<span class="p_del">-	int max, thresh;</span>
<span class="p_del">-	void *buffer;</span>
<span class="p_add">+	struct cpu_hw_events *hwev = per_cpu_ptr(&amp;cpu_hw_events, cpu);</span>
<span class="p_add">+	struct debug_store *ds = hwev-&gt;ds;</span>
<span class="p_add">+	void *buffer, *cea;</span>
<span class="p_add">+	int max;</span>
 
 	if (!x86_pmu.bts)
 		return 0;
 
<span class="p_del">-	buffer = kzalloc_node(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_NOWARN, node);</span>
<span class="p_add">+	buffer = dsalloc_pages(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_NOWARN, cpu);</span>
 	if (unlikely(!buffer)) {
 		WARN_ONCE(1, &quot;%s: BTS buffer allocation failure\n&quot;, __func__);
 		return -ENOMEM;
 	}
<span class="p_del">-</span>
<span class="p_del">-	max = BTS_BUFFER_SIZE / BTS_RECORD_SIZE;</span>
<span class="p_del">-	thresh = max / 16;</span>
<span class="p_del">-</span>
<span class="p_del">-	ds-&gt;bts_buffer_base = (u64)(unsigned long)buffer;</span>
<span class="p_add">+	hwev-&gt;ds_bts_vaddr = buffer;</span>
<span class="p_add">+	/* Update the fixmap */</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_buffers.bts_buffer;</span>
<span class="p_add">+	ds-&gt;bts_buffer_base = (unsigned long) cea;</span>
<span class="p_add">+	ds_update_cea(cea, buffer, BTS_BUFFER_SIZE, PAGE_KERNEL);</span>
 	ds-&gt;bts_index = ds-&gt;bts_buffer_base;
<span class="p_del">-	ds-&gt;bts_absolute_maximum = ds-&gt;bts_buffer_base +</span>
<span class="p_del">-		max * BTS_RECORD_SIZE;</span>
<span class="p_del">-	ds-&gt;bts_interrupt_threshold = ds-&gt;bts_absolute_maximum -</span>
<span class="p_del">-		thresh * BTS_RECORD_SIZE;</span>
<span class="p_del">-</span>
<span class="p_add">+	max = BTS_RECORD_SIZE * (BTS_BUFFER_SIZE / BTS_RECORD_SIZE);</span>
<span class="p_add">+	ds-&gt;bts_absolute_maximum = ds-&gt;bts_buffer_base + max;</span>
<span class="p_add">+	ds-&gt;bts_interrupt_threshold = ds-&gt;bts_absolute_maximum - (max / 16);</span>
 	return 0;
 }
 
 static void release_bts_buffer(int cpu)
 {
<span class="p_del">-	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;</span>
<span class="p_add">+	struct cpu_hw_events *hwev = per_cpu_ptr(&amp;cpu_hw_events, cpu);</span>
<span class="p_add">+	struct debug_store *ds = hwev-&gt;ds;</span>
<span class="p_add">+	void *cea;</span>
 
 	if (!ds || !x86_pmu.bts)
 		return;
 
<span class="p_del">-	kfree((void *)(unsigned long)ds-&gt;bts_buffer_base);</span>
<span class="p_add">+	/* Clear the fixmap */</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_buffers.bts_buffer;</span>
<span class="p_add">+	ds_clear_cea(cea, BTS_BUFFER_SIZE);</span>
 	ds-&gt;bts_buffer_base = 0;
<span class="p_add">+	dsfree_pages(hwev-&gt;ds_bts_vaddr, BTS_BUFFER_SIZE);</span>
<span class="p_add">+	hwev-&gt;ds_bts_vaddr = NULL;</span>
 }
 
 static int alloc_ds_buffer(int cpu)
 {
<span class="p_del">-	int node = cpu_to_node(cpu);</span>
<span class="p_del">-	struct debug_store *ds;</span>
<span class="p_del">-</span>
<span class="p_del">-	ds = kzalloc_node(sizeof(*ds), GFP_KERNEL, node);</span>
<span class="p_del">-	if (unlikely(!ds))</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_add">+	struct debug_store *ds = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_store;</span>
 
<span class="p_add">+	memset(ds, 0, sizeof(*ds));</span>
 	per_cpu(cpu_hw_events, cpu).ds = ds;
<span class="p_del">-</span>
 	return 0;
 }
 
 static void release_ds_buffer(int cpu)
 {
<span class="p_del">-	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!ds)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
 	per_cpu(cpu_hw_events, cpu).ds = NULL;
<span class="p_del">-	kfree(ds);</span>
 }
 
 void release_ds_buffers(void)
<span class="p_header">diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h</span>
<span class="p_header">index f7aaadf9331f..8e4ea143ed96 100644</span>
<span class="p_header">--- a/arch/x86/events/perf_event.h</span>
<span class="p_header">+++ b/arch/x86/events/perf_event.h</span>
<span class="p_chunk">@@ -14,6 +14,8 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/perf_event.h&gt;
 
<span class="p_add">+#include &lt;asm/intel_ds.h&gt;</span>
<span class="p_add">+</span>
 /* To enable MSR tracing please use the generic trace points. */
 
 /*
<span class="p_chunk">@@ -77,8 +79,6 @@</span> <span class="p_context"> struct amd_nb {</span>
 	struct event_constraint event_constraints[X86_PMC_IDX_MAX];
 };
 
<span class="p_del">-/* The maximal number of PEBS events: */</span>
<span class="p_del">-#define MAX_PEBS_EVENTS		8</span>
 #define PEBS_COUNTER_MASK	((1ULL &lt;&lt; MAX_PEBS_EVENTS) - 1)
 
 /*
<span class="p_chunk">@@ -95,23 +95,6 @@</span> <span class="p_context"> struct amd_nb {</span>
 	PERF_SAMPLE_TRANSACTION | PERF_SAMPLE_PHYS_ADDR | \
 	PERF_SAMPLE_REGS_INTR | PERF_SAMPLE_REGS_USER)
 
<span class="p_del">-/*</span>
<span class="p_del">- * A debug store configuration.</span>
<span class="p_del">- *</span>
<span class="p_del">- * We only support architectures that use 64bit fields.</span>
<span class="p_del">- */</span>
<span class="p_del">-struct debug_store {</span>
<span class="p_del">-	u64	bts_buffer_base;</span>
<span class="p_del">-	u64	bts_index;</span>
<span class="p_del">-	u64	bts_absolute_maximum;</span>
<span class="p_del">-	u64	bts_interrupt_threshold;</span>
<span class="p_del">-	u64	pebs_buffer_base;</span>
<span class="p_del">-	u64	pebs_index;</span>
<span class="p_del">-	u64	pebs_absolute_maximum;</span>
<span class="p_del">-	u64	pebs_interrupt_threshold;</span>
<span class="p_del">-	u64	pebs_event_reset[MAX_PEBS_EVENTS];</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 #define PEBS_REGS \
 	(PERF_REG_X86_AX | \
 	 PERF_REG_X86_BX | \
<span class="p_chunk">@@ -216,6 +199,8 @@</span> <span class="p_context"> struct cpu_hw_events {</span>
 	 * Intel DebugStore bits
 	 */
 	struct debug_store	*ds;
<span class="p_add">+	void			*ds_pebs_vaddr;</span>
<span class="p_add">+	void			*ds_bts_vaddr;</span>
 	u64			pebs_enabled;
 	int			n_pebs;
 	int			n_large_pebs;
<span class="p_header">diff --git a/arch/x86/include/asm/cpu_entry_area.h b/arch/x86/include/asm/cpu_entry_area.h</span>
<span class="p_header">index 2fbc69a0916e..4a7884b8dca5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpu_entry_area.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpu_entry_area.h</span>
<span class="p_chunk">@@ -5,6 +5,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/percpu-defs.h&gt;
 #include &lt;asm/processor.h&gt;
<span class="p_add">+#include &lt;asm/intel_ds.h&gt;</span>
 
 /*
  * cpu_entry_area is a percpu region that contains things needed by the CPU
<span class="p_chunk">@@ -40,6 +41,18 @@</span> <span class="p_context"> struct cpu_entry_area {</span>
 	 */
 	char exception_stacks[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ];
 #endif
<span class="p_add">+#ifdef CONFIG_CPU_SUP_INTEL</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Per CPU debug store for Intel performance monitoring. Wastes a</span>
<span class="p_add">+	 * full page at the moment.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct debug_store cpu_debug_store;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The actual PEBS/BTS buffers must be mapped to user space</span>
<span class="p_add">+	 * Reserve enough fixmap PTEs.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct debug_store_buffers cpu_debug_buffers;</span>
<span class="p_add">+#endif</span>
 };
 
 #define CPU_ENTRY_AREA_SIZE	(sizeof(struct cpu_entry_area))
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 800104c8a3ed..07cdd1715705 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -197,11 +197,12 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_CAT_L3		( 7*32+ 4) /* Cache Allocation Technology L3 */
 #define X86_FEATURE_CAT_L2		( 7*32+ 5) /* Cache Allocation Technology L2 */
 #define X86_FEATURE_CDP_L3		( 7*32+ 6) /* Code and Data Prioritization L3 */
<span class="p_add">+#define X86_FEATURE_INVPCID_SINGLE	( 7*32+ 7) /* Effectively INVPCID &amp;&amp; CR4.PCIDE=1 */</span>
 
 #define X86_FEATURE_HW_PSTATE		( 7*32+ 8) /* AMD HW-PState */
 #define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
 #define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
<span class="p_del">-</span>
<span class="p_add">+#define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */</span>
 #define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
 #define X86_FEATURE_INTEL_PT		( 7*32+15) /* Intel Processor Trace */
 #define X86_FEATURE_AVX512_4VNNIW	( 7*32+16) /* AVX-512 Neural Network Instructions */
<span class="p_chunk">@@ -340,5 +341,6 @@</span> <span class="p_context"></span>
 #define X86_BUG_SWAPGS_FENCE		X86_BUG(11) /* SWAPGS without input dep on GS */
 #define X86_BUG_MONITOR			X86_BUG(12) /* IPI required to wake up remote CPU */
 #define X86_BUG_AMD_E400		X86_BUG(13) /* CPU is among the affected by Erratum 400 */
<span class="p_add">+#define X86_BUG_CPU_INSECURE		X86_BUG(14) /* CPU is insecure and needs kernel page table isolation */</span>
 
 #endif /* _ASM_X86_CPUFEATURES_H */
<span class="p_header">diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="p_header">index bc359dd2f7f6..85e23bb7b34e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -21,6 +21,8 @@</span> <span class="p_context"> static inline void fill_ldt(struct desc_struct *desc, const struct user_desc *in</span>
 
 	desc-&gt;type		= (info-&gt;read_exec_only ^ 1) &lt;&lt; 1;
 	desc-&gt;type	       |= info-&gt;contents &lt;&lt; 2;
<span class="p_add">+	/* Set the ACCESS bit so it can be mapped RO */</span>
<span class="p_add">+	desc-&gt;type	       |= 1;</span>
 
 	desc-&gt;s			= 1;
 	desc-&gt;dpl		= 0x3;
<span class="p_header">diff --git a/arch/x86/include/asm/disabled-features.h b/arch/x86/include/asm/disabled-features.h</span>
<span class="p_header">index c10c9128f54e..e428e16dd822 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/disabled-features.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/disabled-features.h</span>
<span class="p_chunk">@@ -44,6 +44,12 @@</span> <span class="p_context"></span>
 # define DISABLE_LA57	(1&lt;&lt;(X86_FEATURE_LA57 &amp; 31))
 #endif
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+# define DISABLE_PTI		0</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define DISABLE_PTI		(1 &lt;&lt; (X86_FEATURE_PTI &amp; 31))</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * Make sure to add features to the correct mask
  */
<span class="p_chunk">@@ -54,7 +60,7 @@</span> <span class="p_context"></span>
 #define DISABLED_MASK4	(DISABLE_PCID)
 #define DISABLED_MASK5	0
 #define DISABLED_MASK6	0
<span class="p_del">-#define DISABLED_MASK7	0</span>
<span class="p_add">+#define DISABLED_MASK7	(DISABLE_PTI)</span>
 #define DISABLED_MASK8	0
 #define DISABLED_MASK9	(DISABLE_MPX)
 #define DISABLED_MASK10	0
<span class="p_header">diff --git a/arch/x86/include/asm/intel_ds.h b/arch/x86/include/asm/intel_ds.h</span>
new file mode 100644
<span class="p_header">index 000000000000..62a9f4966b42</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/intel_ds.h</span>
<span class="p_chunk">@@ -0,0 +1,36 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_INTEL_DS_H</span>
<span class="p_add">+#define _ASM_INTEL_DS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/percpu-defs.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define BTS_BUFFER_SIZE		(PAGE_SIZE &lt;&lt; 4)</span>
<span class="p_add">+#define PEBS_BUFFER_SIZE	(PAGE_SIZE &lt;&lt; 4)</span>
<span class="p_add">+</span>
<span class="p_add">+/* The maximal number of PEBS events: */</span>
<span class="p_add">+#define MAX_PEBS_EVENTS		8</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * A debug store configuration.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We only support architectures that use 64bit fields.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct debug_store {</span>
<span class="p_add">+	u64	bts_buffer_base;</span>
<span class="p_add">+	u64	bts_index;</span>
<span class="p_add">+	u64	bts_absolute_maximum;</span>
<span class="p_add">+	u64	bts_interrupt_threshold;</span>
<span class="p_add">+	u64	pebs_buffer_base;</span>
<span class="p_add">+	u64	pebs_index;</span>
<span class="p_add">+	u64	pebs_absolute_maximum;</span>
<span class="p_add">+	u64	pebs_interrupt_threshold;</span>
<span class="p_add">+	u64	pebs_event_reset[MAX_PEBS_EVENTS];</span>
<span class="p_add">+} __aligned(PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+DECLARE_PER_CPU_PAGE_ALIGNED(struct debug_store, cpu_debug_store);</span>
<span class="p_add">+</span>
<span class="p_add">+struct debug_store_buffers {</span>
<span class="p_add">+	char	bts_buffer[BTS_BUFFER_SIZE];</span>
<span class="p_add">+	char	pebs_buffer[PEBS_BUFFER_SIZE];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 5ede7cae1d67..c931b88982a0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -50,10 +50,33 @@</span> <span class="p_context"> struct ldt_struct {</span>
 	 * call gates.  On native, we could merge the ldt_struct and LDT
 	 * allocations, but it&#39;s not worth trying to optimize.
 	 */
<span class="p_del">-	struct desc_struct *entries;</span>
<span class="p_del">-	unsigned int nr_entries;</span>
<span class="p_add">+	struct desc_struct	*entries;</span>
<span class="p_add">+	unsigned int		nr_entries;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PTI is in use, then the entries array is not mapped while we&#39;re</span>
<span class="p_add">+	 * in user mode.  The whole array will be aliased at the addressed</span>
<span class="p_add">+	 * given by ldt_slot_va(slot).  We use two slots so that we can allocate</span>
<span class="p_add">+	 * and map, and enable a new LDT without invalidating the mapping</span>
<span class="p_add">+	 * of an older, still-in-use LDT.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * slot will be -1 if this LDT doesn&#39;t have an alias mapping.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int			slot;</span>
 };
 
<span class="p_add">+/* This is a multiple of PAGE_SIZE. */</span>
<span class="p_add">+#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void *ldt_slot_va(int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	return (void *)(LDT_BASE_ADDR + LDT_SLOT_STRIDE * slot);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Used for LDT copy/destruction.
  */
<span class="p_chunk">@@ -64,6 +87,7 @@</span> <span class="p_context"> static inline void init_new_context_ldt(struct mm_struct *mm)</span>
 }
 int ldt_dup_context(struct mm_struct *oldmm, struct mm_struct *mm);
 void destroy_context_ldt(struct mm_struct *mm);
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm);</span>
 #else	/* CONFIG_MODIFY_LDT_SYSCALL */
 static inline void init_new_context_ldt(struct mm_struct *mm) { }
 static inline int ldt_dup_context(struct mm_struct *oldmm,
<span class="p_chunk">@@ -71,7 +95,8 @@</span> <span class="p_context"> static inline int ldt_dup_context(struct mm_struct *oldmm,</span>
 {
 	return 0;
 }
<span class="p_del">-static inline void destroy_context_ldt(struct mm_struct *mm) {}</span>
<span class="p_add">+static inline void destroy_context_ldt(struct mm_struct *mm) { }</span>
<span class="p_add">+static inline void ldt_arch_exit_mmap(struct mm_struct *mm) { }</span>
 #endif
 
 static inline void load_mm_ldt(struct mm_struct *mm)
<span class="p_chunk">@@ -96,10 +121,31 @@</span> <span class="p_context"> static inline void load_mm_ldt(struct mm_struct *mm)</span>
 	 * that we can see.
 	 */
 
<span class="p_del">-	if (unlikely(ldt))</span>
<span class="p_del">-		set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (unlikely(ldt)) {</span>
<span class="p_add">+		if (static_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="p_add">+			if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * Whoops -- either the new LDT isn&#39;t mapped</span>
<span class="p_add">+				 * (if slot == -1) or is mapped into a bogus</span>
<span class="p_add">+				 * slot (if slot &gt; 1).</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				clear_LDT();</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If page table isolation is enabled, ldt-&gt;entries</span>
<span class="p_add">+			 * will not be mapped in the userspace pagetables.</span>
<span class="p_add">+			 * Tell the CPU to access the LDT through the alias</span>
<span class="p_add">+			 * at ldt_slot_va(ldt-&gt;slot).</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			set_ldt(ldt_slot_va(ldt-&gt;slot), ldt-&gt;nr_entries);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
 		clear_LDT();
<span class="p_add">+	}</span>
 #else
 	clear_LDT();
 #endif
<span class="p_chunk">@@ -194,6 +240,7 @@</span> <span class="p_context"> static inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)</span>
 static inline void arch_exit_mmap(struct mm_struct *mm)
 {
 	paravirt_arch_exit_mmap(mm);
<span class="p_add">+	ldt_arch_exit_mmap(mm);</span>
 }
 
 #ifdef CONFIG_X86_64
<span class="p_header">diff --git a/arch/x86/include/asm/pgalloc.h b/arch/x86/include/asm/pgalloc.h</span>
<span class="p_header">index 4b5e1eafada7..aff42e1da6ee 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -30,6 +30,17 @@</span> <span class="p_context"> static inline void paravirt_release_p4d(unsigned long pfn) {}</span>
  */
 extern gfp_t __userpte_alloc_gfp;
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Instead of one PGD, we acquire two PGDs.  Being order-1, it is</span>
<span class="p_add">+ * both 8k in size and 8k-aligned.  That lets us just flip bit 12</span>
<span class="p_add">+ * in a pointer to swap between the two 4k halves.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PGD_ALLOCATION_ORDER 1</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define PGD_ALLOCATION_ORDER 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * Allocate and free page tables.
  */
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index f735c3016325..6b43d677f8ca 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -28,6 +28,7 @@</span> <span class="p_context"> extern pgd_t early_top_pgt[PTRS_PER_PGD];</span>
 int __init __early_make_pgtable(unsigned long address, pmdval_t pmd);
 
 void ptdump_walk_pgd_level(struct seq_file *m, pgd_t *pgd);
<span class="p_add">+void ptdump_walk_pgd_level_debugfs(struct seq_file *m, pgd_t *pgd, bool user);</span>
 void ptdump_walk_pgd_level_checkwx(void);
 
 #ifdef CONFIG_DEBUG_WX
<span class="p_chunk">@@ -846,7 +847,12 @@</span> <span class="p_context"> static inline pud_t *pud_offset(p4d_t *p4d, unsigned long address)</span>
 
 static inline int p4d_bad(p4d_t p4d)
 {
<span class="p_del">-	return (p4d_flags(p4d) &amp; ~(_KERNPG_TABLE | _PAGE_USER)) != 0;</span>
<span class="p_add">+	unsigned long ignore_flags = _KERNPG_TABLE | _PAGE_USER;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))</span>
<span class="p_add">+		ignore_flags |= _PAGE_NX;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (p4d_flags(p4d) &amp; ~ignore_flags) != 0;</span>
 }
 #endif  /* CONFIG_PGTABLE_LEVELS &gt; 3 */
 
<span class="p_chunk">@@ -880,7 +886,12 @@</span> <span class="p_context"> static inline p4d_t *p4d_offset(pgd_t *pgd, unsigned long address)</span>
 
 static inline int pgd_bad(pgd_t pgd)
 {
<span class="p_del">-	return (pgd_flags(pgd) &amp; ~_PAGE_USER) != _KERNPG_TABLE;</span>
<span class="p_add">+	unsigned long ignore_flags = _PAGE_USER;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))</span>
<span class="p_add">+		ignore_flags |= _PAGE_NX;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (pgd_flags(pgd) &amp; ~ignore_flags) != _KERNPG_TABLE;</span>
 }
 
 static inline int pgd_none(pgd_t pgd)
<span class="p_chunk">@@ -909,7 +920,11 @@</span> <span class="p_context"> static inline int pgd_none(pgd_t pgd)</span>
  * pgd_offset() returns a (pgd_t *)
  * pgd_index() is used get the offset into the pgd page&#39;s array of pgd_t&#39;s;
  */
<span class="p_del">-#define pgd_offset(mm, address) ((mm)-&gt;pgd + pgd_index((address)))</span>
<span class="p_add">+#define pgd_offset_pgd(pgd, address) (pgd + pgd_index((address)))</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * a shortcut to get a pgd_t in a given mm</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define pgd_offset(mm, address) pgd_offset_pgd((mm)-&gt;pgd, (address))</span>
 /*
  * a shortcut which implies the use of the kernel&#39;s pgd, instead
  * of a process&#39;s
<span class="p_chunk">@@ -1105,7 +1120,14 @@</span> <span class="p_context"> static inline void pmdp_set_wrprotect(struct mm_struct *mm,</span>
  */
 static inline void clone_pgd_range(pgd_t *dst, pgd_t *src, int count)
 {
<span class="p_del">-       memcpy(dst, src, count * sizeof(pgd_t));</span>
<span class="p_add">+	memcpy(dst, src, count * sizeof(pgd_t));</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	/* Clone the user space pgd as well */</span>
<span class="p_add">+	memcpy(kernel_to_user_pgdp(dst), kernel_to_user_pgdp(src),</span>
<span class="p_add">+	       count * sizeof(pgd_t));</span>
<span class="p_add">+#endif</span>
 }
 
 #define PTE_SHIFT ilog2(PTRS_PER_PTE)
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index e9f05331e732..81462e9a34f6 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -131,9 +131,97 @@</span> <span class="p_context"> static inline pud_t native_pudp_get_and_clear(pud_t *xp)</span>
 #endif
 }
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * All top-level PAGE_TABLE_ISOLATION page tables are order-1 pages</span>
<span class="p_add">+ * (8k-aligned and 8k in size).  The kernel one is at the beginning 4k and</span>
<span class="p_add">+ * the user one is in the last 4k.  To switch between them, you</span>
<span class="p_add">+ * just need to flip the 12th bit in their addresses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PTI_PGTABLE_SWITCH_BIT	PAGE_SHIFT</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This generates better code than the inline assembly in</span>
<span class="p_add">+ * __set_bit().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void *ptr_set_bit(void *ptr, int bit)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long __ptr = (unsigned long)ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	__ptr |= BIT(bit);</span>
<span class="p_add">+	return (void *)__ptr;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void *ptr_clear_bit(void *ptr, int bit)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long __ptr = (unsigned long)ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	__ptr &amp;= ~BIT(bit);</span>
<span class="p_add">+	return (void *)__ptr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *kernel_to_user_pgdp(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_set_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *user_to_kernel_pgdp(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_clear_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline p4d_t *kernel_to_user_p4dp(p4d_t *p4dp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_set_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline p4d_t *user_to_kernel_p4dp(p4d_t *p4dp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_clear_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Page table pages are page-aligned.  The lower half of the top</span>
<span class="p_add">+ * level is used for userspace and the top half for the kernel.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns true for parts of the PGD that map userspace and</span>
<span class="p_add">+ * false for the parts that map the kernel.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool pgdp_maps_userspace(void *__ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long ptr = (unsigned long)__ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (ptr &amp; ~PAGE_MASK) &lt; (PAGE_SIZE / 2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+pgd_t __pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Take a PGD location (pgdp) and a pgd value that needs to be set there.</span>
<span class="p_add">+ * Populates the user and returns the resulting PGD that must be set in</span>
<span class="p_add">+ * the kernel copy of the page tables.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline pgd_t pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return pgd;</span>
<span class="p_add">+	return __pti_set_user_pgd(pgdp, pgd);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline pgd_t pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 static inline void native_set_p4d(p4d_t *p4dp, p4d_t p4d)
 {
<span class="p_add">+#if defined(CONFIG_PAGE_TABLE_ISOLATION) &amp;&amp; !defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	p4dp-&gt;pgd = pti_set_user_pgd(&amp;p4dp-&gt;pgd, p4d.pgd);</span>
<span class="p_add">+#else</span>
 	*p4dp = p4d;
<span class="p_add">+#endif</span>
 }
 
 static inline void native_p4d_clear(p4d_t *p4d)
<span class="p_chunk">@@ -147,7 +235,11 @@</span> <span class="p_context"> static inline void native_p4d_clear(p4d_t *p4d)</span>
 
 static inline void native_set_pgd(pgd_t *pgdp, pgd_t pgd)
 {
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	*pgdp = pti_set_user_pgd(pgdp, pgd);</span>
<span class="p_add">+#else</span>
 	*pgdp = pgd;
<span class="p_add">+#endif</span>
 }
 
 static inline void native_pgd_clear(pgd_t *pgd)
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64_types.h b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">index 3d27831bc58d..b97a539bcdee 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -79,13 +79,17 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define MAXMEM			_AC(__AC(1, UL) &lt;&lt; MAX_PHYSMEM_BITS, UL)
 
 #ifdef CONFIG_X86_5LEVEL
<span class="p_del">-# define VMALLOC_SIZE_TB	_AC(16384, UL)</span>
<span class="p_del">-# define __VMALLOC_BASE		_AC(0xff92000000000000, UL)</span>
<span class="p_add">+# define VMALLOC_SIZE_TB	_AC(12800, UL)</span>
<span class="p_add">+# define __VMALLOC_BASE		_AC(0xffa0000000000000, UL)</span>
 # define __VMEMMAP_BASE		_AC(0xffd4000000000000, UL)
<span class="p_add">+# define LDT_PGD_ENTRY		_AC(-112, UL)</span>
<span class="p_add">+# define LDT_BASE_ADDR		(LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #else
 # define VMALLOC_SIZE_TB	_AC(32, UL)
 # define __VMALLOC_BASE		_AC(0xffffc90000000000, UL)
 # define __VMEMMAP_BASE		_AC(0xffffea0000000000, UL)
<span class="p_add">+# define LDT_PGD_ENTRY		_AC(-4, UL)</span>
<span class="p_add">+# define LDT_BASE_ADDR		(LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #endif
 
 #ifdef CONFIG_RANDOMIZE_MEMORY
<span class="p_header">diff --git a/arch/x86/include/asm/processor-flags.h b/arch/x86/include/asm/processor-flags.h</span>
<span class="p_header">index 43212a43ee69..6a60fea90b9d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor-flags.h</span>
<span class="p_chunk">@@ -38,6 +38,11 @@</span> <span class="p_context"></span>
 #define CR3_ADDR_MASK	__sme_clr(0x7FFFFFFFFFFFF000ull)
 #define CR3_PCID_MASK	0xFFFull
 #define CR3_NOFLUSH	BIT_ULL(63)
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+# define X86_CR3_PTI_SWITCH_BIT	11</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #else
 /*
  * CR3_ADDR_MASK needs at least bits 31:5 set on PAE systems, and we save
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 9e482d8b0b97..9c18da64daa9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -851,13 +851,22 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 
 #else
 /*
<span class="p_del">- * User space process size. 47bits minus one guard page.  The guard</span>
<span class="p_del">- * page is necessary on Intel CPUs: if a SYSCALL instruction is at</span>
<span class="p_del">- * the highest possible canonical userspace address, then that</span>
<span class="p_del">- * syscall will enter the kernel with a non-canonical return</span>
<span class="p_del">- * address, and SYSRET will explode dangerously.  We avoid this</span>
<span class="p_del">- * particular problem by preventing anything from being mapped</span>
<span class="p_del">- * at the maximum canonical address.</span>
<span class="p_add">+ * User space process size.  This is the first address outside the user range.</span>
<span class="p_add">+ * There are a few constraints that determine this:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On Intel CPUs, if a SYSCALL instruction is at the highest canonical</span>
<span class="p_add">+ * address, then that syscall will enter the kernel with a</span>
<span class="p_add">+ * non-canonical return address, and SYSRET will explode dangerously.</span>
<span class="p_add">+ * We avoid this particular problem by preventing anything executable</span>
<span class="p_add">+ * from being mapped at the maximum canonical address.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On AMD CPUs in the Ryzen family, there&#39;s a nasty bug in which the</span>
<span class="p_add">+ * CPUs malfunction if they execute code from the highest canonical page.</span>
<span class="p_add">+ * They&#39;ll speculate right off the end of the canonical space, and</span>
<span class="p_add">+ * bad things happen.  This is worked around in the same way as the</span>
<span class="p_add">+ * Intel problem.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With page table isolation enabled, we map the LDT in ... [stay tuned]</span>
  */
 #define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
 
<span class="p_header">diff --git a/arch/x86/include/asm/pti.h b/arch/x86/include/asm/pti.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0b5ef05b2d2d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/pti.h</span>
<span class="p_chunk">@@ -0,0 +1,14 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+#ifndef _ASM_X86_PTI_H</span>
<span class="p_add">+#define _ASM_X86_PTI_H</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+extern void pti_init(void);</span>
<span class="p_add">+extern void pti_check_boottime_disable(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void pti_check_boottime_disable(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+#endif /* _ASM_X86_PTI_H */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 171b429f43a2..b519da4fc03c 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -10,38 +10,90 @@</span> <span class="p_context"></span>
 #include &lt;asm/special_insns.h&gt;
 #include &lt;asm/smp.h&gt;
 #include &lt;asm/invpcid.h&gt;
<span class="p_add">+#include &lt;asm/pti.h&gt;</span>
<span class="p_add">+#include &lt;asm/processor-flags.h&gt;</span>
 
<span class="p_del">-static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Bump the generation count.  This also serves as a full barrier</span>
<span class="p_del">-	 * that synchronizes with switch_mm(): callers are required to order</span>
<span class="p_del">-	 * their read of mm_cpumask after their writes to the paging</span>
<span class="p_del">-	 * structures.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	return atomic64_inc_return(&amp;mm-&gt;context.tlb_gen);</span>
<span class="p_del">-}</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The x86 feature is called PCID (Process Context IDentifier). It is similar</span>
<span class="p_add">+ * to what is traditionally called ASID on the RISC processors.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We don&#39;t use the traditional ASID implementation, where each process/mm gets</span>
<span class="p_add">+ * its own ASID and flush/restart when we run out of ASID space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Instead we have a small per-cpu array of ASIDs and cache the last few mm&#39;s</span>
<span class="p_add">+ * that came by on this CPU, allowing cheaper switch_mm between processes on</span>
<span class="p_add">+ * this CPU.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We end up with different spaces for different things. To avoid confusion we</span>
<span class="p_add">+ * use different names for each of them:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * ASID  - [0, TLB_NR_DYN_ASIDS-1]</span>
<span class="p_add">+ *         the canonical identifier for an mm</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * kPCID - [1, TLB_NR_DYN_ASIDS]</span>
<span class="p_add">+ *         the value we write into the PCID part of CR3; corresponds to the</span>
<span class="p_add">+ *         ASID+1, because PCID 0 is special.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * uPCID - [2048 + 1, 2048 + TLB_NR_DYN_ASIDS]</span>
<span class="p_add">+ *         for KPTI each mm has two address spaces and thus needs two</span>
<span class="p_add">+ *         PCID values, but we can still do with a single ASID denomination</span>
<span class="p_add">+ *         for each mm. Corresponds to kPCID + 2048.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
 
 /* There are 12 bits of space for ASIDS in CR3 */
 #define CR3_HW_ASID_BITS		12
<span class="p_add">+</span>
 /*
  * When enabled, PAGE_TABLE_ISOLATION consumes a single bit for
  * user/kernel switches
  */
<span class="p_del">-#define PTI_CONSUMED_ASID_BITS		0</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+# define PTI_CONSUMED_PCID_BITS	1</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define PTI_CONSUMED_PCID_BITS	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define CR3_AVAIL_PCID_BITS (X86_CR3_PCID_BITS - PTI_CONSUMED_PCID_BITS)</span>
 
<span class="p_del">-#define CR3_AVAIL_ASID_BITS (CR3_HW_ASID_BITS - PTI_CONSUMED_ASID_BITS)</span>
 /*
  * ASIDs are zero-based: 0-&gt;MAX_AVAIL_ASID are valid.  -1 below to account
<span class="p_del">- * for them being zero-based.  Another -1 is because ASID 0 is reserved for</span>
<span class="p_add">+ * for them being zero-based.  Another -1 is because PCID 0 is reserved for</span>
  * use by non-PCID-aware users.
  */
<span class="p_del">-#define MAX_ASID_AVAILABLE ((1 &lt;&lt; CR3_AVAIL_ASID_BITS) - 2)</span>
<span class="p_add">+#define MAX_ASID_AVAILABLE ((1 &lt;&lt; CR3_AVAIL_PCID_BITS) - 2)</span>
 
<span class="p_add">+/*</span>
<span class="p_add">+ * 6 because 6 should be plenty and struct tlb_state will fit in two cache</span>
<span class="p_add">+ * lines.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define TLB_NR_DYN_ASIDS	6</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Given @asid, compute kPCID</span>
<span class="p_add">+ */</span>
 static inline u16 kern_pcid(u16 asid)
 {
 	VM_WARN_ON_ONCE(asid &gt; MAX_ASID_AVAILABLE);
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Make sure that the dynamic ASID space does not confict with the</span>
<span class="p_add">+	 * bit we are using to switch between user and kernel ASIDs.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUILD_BUG_ON(TLB_NR_DYN_ASIDS &gt;= (1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT));</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The ASID being passed in here should have respected the</span>
<span class="p_add">+	 * MAX_ASID_AVAILABLE and thus never have the switch bit set.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	VM_WARN_ON_ONCE(asid &amp; (1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT));</span>
<span class="p_add">+#endif</span>
 	/*
<span class="p_add">+	 * The dynamically-assigned ASIDs that get passed in are small</span>
<span class="p_add">+	 * (&lt;TLB_NR_DYN_ASIDS).  They never have the high switch bit set,</span>
<span class="p_add">+	 * so do not bother to clear it.</span>
<span class="p_add">+	 *</span>
 	 * If PCID is on, ASID-aware code paths put the ASID+1 into the
 	 * PCID bits.  This serves two purposes.  It prevents a nasty
 	 * situation in which PCID-unaware code saves CR3, loads some other
<span class="p_chunk">@@ -53,6 +105,18 @@</span> <span class="p_context"> static inline u16 kern_pcid(u16 asid)</span>
 	return asid + 1;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Given @asid, compute uPCID</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline u16 user_pcid(u16 asid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u16 ret = kern_pcid(asid);</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	ret |= 1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 struct pgd_t;
 static inline unsigned long build_cr3(pgd_t *pgd, u16 asid)
 {
<span class="p_chunk">@@ -95,12 +159,6 @@</span> <span class="p_context"> static inline bool tlb_defer_switch_to_init_mm(void)</span>
 	return !static_cpu_has(X86_FEATURE_PCID);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * 6 because 6 should be plenty and struct tlb_state will fit in</span>
<span class="p_del">- * two cache lines.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define TLB_NR_DYN_ASIDS 6</span>
<span class="p_del">-</span>
 struct tlb_context {
 	u64 ctx_id;
 	u64 tlb_gen;
<span class="p_chunk">@@ -135,6 +193,24 @@</span> <span class="p_context"> struct tlb_state {</span>
 	bool is_lazy;
 
 	/*
<span class="p_add">+	 * If set we changed the page tables in such a way that we</span>
<span class="p_add">+	 * needed an invalidation of all contexts (aka. PCIDs / ASIDs).</span>
<span class="p_add">+	 * This tells us to go invalidate all the non-loaded ctxs[]</span>
<span class="p_add">+	 * on the next context switch.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The current ctx was kept up-to-date as it ran and does not</span>
<span class="p_add">+	 * need to be invalidated.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	bool invalidate_other;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Mask that contains TLB_NR_DYN_ASIDS+1 bits to indicate</span>
<span class="p_add">+	 * the corresponding user PCID needs a flush next time we</span>
<span class="p_add">+	 * switch to it; see SWITCH_TO_USER_CR3.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	unsigned short user_pcid_flush_mask;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
 	 * Access to this CR4 shadow and to H/W CR4 is protected by
 	 * disabling interrupts when modifying either one.
 	 */
<span class="p_chunk">@@ -212,6 +288,14 @@</span> <span class="p_context"> static inline unsigned long cr4_read_shadow(void)</span>
 }
 
 /*
<span class="p_add">+ * Mark all other ASIDs as invalid, preserves the current.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void invalidate_other_asid(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.invalidate_other, true);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Save some of cr4 feature set we&#39;re using (e.g.  Pentium 4MB
  * enable and PPro Global page enable), so that any CPU&#39;s that boot
  * up after us can get the correct flags.  This should only be used
<span class="p_chunk">@@ -231,14 +315,41 @@</span> <span class="p_context"> static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
 extern void initialize_tlbstate_and_flush(void);
 
 /*
<span class="p_add">+ * Given an ASID, flush the corresponding user ASID.  We can delay this</span>
<span class="p_add">+ * until the next time we switch to it.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * See SWITCH_TO_USER_CR3.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void invalidate_user_asid(u16 asid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* There is no user ASID if address space separation is off */</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We only have a single ASID if PCID is off and the CR3</span>
<span class="p_add">+	 * write will have flushed it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!cpu_feature_enabled(X86_FEATURE_PCID))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	__set_bit(kern_pcid(asid),</span>
<span class="p_add">+		  (unsigned long *)this_cpu_ptr(&amp;cpu_tlbstate.user_pcid_flush_mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * flush the entire current user mapping
  */
 static inline void __native_flush_tlb(void)
 {
<span class="p_add">+	invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
 	/*
<span class="p_del">-	 * If current-&gt;mm == NULL then we borrow a mm which may change during a</span>
<span class="p_del">-	 * task switch and therefore we must not be preempted while we write CR3</span>
<span class="p_del">-	 * back:</span>
<span class="p_add">+	 * If current-&gt;mm == NULL then we borrow a mm which may change</span>
<span class="p_add">+	 * during a task switch and therefore we must not be preempted</span>
<span class="p_add">+	 * while we write CR3 back:</span>
 	 */
 	preempt_disable();
 	native_write_cr3(__native_read_cr3());
<span class="p_chunk">@@ -256,6 +367,8 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
 		/*
 		 * Using INVPCID is considerably faster than a pair of writes
 		 * to CR4 sandwiched inside an IRQ flag save/restore.
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Note, this works with CR4.PCIDE=0 or 1.</span>
 		 */
 		invpcid_flush_all();
 		return;
<span class="p_chunk">@@ -282,7 +395,21 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
  */
 static inline void __native_flush_tlb_single(unsigned long addr)
 {
<span class="p_add">+	u32 loaded_mm_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);</span>
<span class="p_add">+</span>
 	asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Some platforms #GP if we call invpcid(type=1/2) before CR4.PCIDE=1.</span>
<span class="p_add">+	 * Just use invalidate_user_asid() in case we are called early.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!this_cpu_has(X86_FEATURE_INVPCID_SINGLE))</span>
<span class="p_add">+		invalidate_user_asid(loaded_mm_asid);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		invpcid_flush_one(user_pcid(loaded_mm_asid), addr);</span>
 }
 
 /*
<span class="p_chunk">@@ -298,14 +425,6 @@</span> <span class="p_context"> static inline void __flush_tlb_all(void)</span>
 		 */
 		__flush_tlb();
 	}
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Note: if we somehow had PCID but not PGE, then this wouldn&#39;t work --</span>
<span class="p_del">-	 * we&#39;d end up flushing kernel translations for the current ASID but</span>
<span class="p_del">-	 * we might fail to flush kernel translations for other cached ASIDs.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * To avoid this issue, we force PCID off if PGE is off.</span>
<span class="p_del">-	 */</span>
 }
 
 /*
<span class="p_chunk">@@ -315,6 +434,16 @@</span> <span class="p_context"> static inline void __flush_tlb_one(unsigned long addr)</span>
 {
 	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);
 	__flush_tlb_single(addr);
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * __flush_tlb_single() will have cleared the TLB entry for this ASID,</span>
<span class="p_add">+	 * but since kernel space is replicated across all, we must also</span>
<span class="p_add">+	 * invalidate all others.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	invalidate_other_asid();</span>
 }
 
 #define TLB_FLUSH_ALL	-1UL
<span class="p_chunk">@@ -375,6 +504,17 @@</span> <span class="p_context"> static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long a)</span>
 void native_flush_tlb_others(const struct cpumask *cpumask,
 			     const struct flush_tlb_info *info);
 
<span class="p_add">+static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Bump the generation count.  This also serves as a full barrier</span>
<span class="p_add">+	 * that synchronizes with switch_mm(): callers are required to order</span>
<span class="p_add">+	 * their read of mm_cpumask after their writes to the paging</span>
<span class="p_add">+	 * structures.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return atomic64_inc_return(&amp;mm-&gt;context.tlb_gen);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void arch_tlbbatch_add_mm(struct arch_tlbflush_unmap_batch *batch,
 					struct mm_struct *mm)
 {
<span class="p_header">diff --git a/arch/x86/include/asm/vsyscall.h b/arch/x86/include/asm/vsyscall.h</span>
<span class="p_header">index d9a7c659009c..b986b2ca688a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/vsyscall.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/vsyscall.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 
 #ifdef CONFIG_X86_VSYSCALL_EMULATION
 extern void map_vsyscall(void);
<span class="p_add">+extern void set_vsyscall_pgtable_user_bits(pgd_t *root);</span>
 
 /*
  * Called on instruction fetch fault in vsyscall page.
<span class="p_header">diff --git a/arch/x86/include/uapi/asm/processor-flags.h b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">index 53b4ca55ebb6..97abdaab9535 100644</span>
<span class="p_header">--- a/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_chunk">@@ -78,7 +78,12 @@</span> <span class="p_context"></span>
 #define X86_CR3_PWT		_BITUL(X86_CR3_PWT_BIT)
 #define X86_CR3_PCD_BIT		4 /* Page Cache Disable */
 #define X86_CR3_PCD		_BITUL(X86_CR3_PCD_BIT)
<span class="p_del">-#define X86_CR3_PCID_MASK	_AC(0x00000fff,UL) /* PCID Mask */</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_CR3_PCID_BITS	12</span>
<span class="p_add">+#define X86_CR3_PCID_MASK	(_AC((1UL &lt;&lt; X86_CR3_PCID_BITS) - 1, UL))</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH_BIT 63 /* Preserve old PCID */</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH    _BITULL(X86_CR3_PCID_NOFLUSH_BIT)</span>
 
 /*
  * Intel CPU features in CR4
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">index 676b7cf4b62b..76417a9aab73 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_chunk">@@ -17,6 +17,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/sigframe.h&gt;
 #include &lt;asm/bootparam.h&gt;
 #include &lt;asm/suspend.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 
 #ifdef CONFIG_XEN
 #include &lt;xen/interface/xen.h&gt;
<span class="p_chunk">@@ -94,6 +95,9 @@</span> <span class="p_context"> void common(void) {</span>
 	BLANK();
 	DEFINE(PTREGS_SIZE, sizeof(struct pt_regs));
 
<span class="p_add">+	/* TLB state for the entry code */</span>
<span class="p_add">+	OFFSET(TLB_STATE_user_pcid_flush_mask, tlb_state, user_pcid_flush_mask);</span>
<span class="p_add">+</span>
 	/* Layout info for cpu_entry_area */
 	OFFSET(CPU_ENTRY_AREA_tss, cpu_entry_area, tss);
 	OFFSET(CPU_ENTRY_AREA_entry_trampoline, cpu_entry_area, entry_trampoline);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 8ddcfa4d4165..f2a94dfb434e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -898,6 +898,10 @@</span> <span class="p_context"> static void __init early_identify_cpu(struct cpuinfo_x86 *c)</span>
 	}
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
<span class="p_add">+</span>
<span class="p_add">+	/* Assume for now that ALL x86 CPUs are insecure */</span>
<span class="p_add">+	setup_force_cpu_bug(X86_BUG_CPU_INSECURE);</span>
<span class="p_add">+</span>
 	fpu__init_system(c);
 
 #ifdef CONFIG_X86_32
<span class="p_chunk">@@ -1335,7 +1339,10 @@</span> <span class="p_context"> void syscall_init(void)</span>
 		(entry_SYSCALL_64_trampoline - _entry_trampoline);
 
 	wrmsr(MSR_STAR, 0, (__USER32_CS &lt;&lt; 16) | __KERNEL_CS);
<span class="p_del">-	wrmsrl(MSR_LSTAR, SYSCALL64_entry_trampoline);</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		wrmsrl(MSR_LSTAR, SYSCALL64_entry_trampoline);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);</span>
 
 #ifdef CONFIG_IA32_EMULATION
 	wrmsrl(MSR_CSTAR, (unsigned long)entry_SYSCALL_compat);
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack.c b/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">index 36b17e0febe8..5fa110699ed2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack.c</span>
<span class="p_chunk">@@ -297,11 +297,13 @@</span> <span class="p_context"> int __die(const char *str, struct pt_regs *regs, long err)</span>
 	unsigned long sp;
 #endif
 	printk(KERN_DEFAULT
<span class="p_del">-	       &quot;%s: %04lx [#%d]%s%s%s%s\n&quot;, str, err &amp; 0xffff, ++die_counter,</span>
<span class="p_add">+	       &quot;%s: %04lx [#%d]%s%s%s%s%s\n&quot;, str, err &amp; 0xffff, ++die_counter,</span>
 	       IS_ENABLED(CONFIG_PREEMPT) ? &quot; PREEMPT&quot;         : &quot;&quot;,
 	       IS_ENABLED(CONFIG_SMP)     ? &quot; SMP&quot;             : &quot;&quot;,
 	       debug_pagealloc_enabled()  ? &quot; DEBUG_PAGEALLOC&quot; : &quot;&quot;,
<span class="p_del">-	       IS_ENABLED(CONFIG_KASAN)   ? &quot; KASAN&quot;           : &quot;&quot;);</span>
<span class="p_add">+	       IS_ENABLED(CONFIG_KASAN)   ? &quot; KASAN&quot;           : &quot;&quot;,</span>
<span class="p_add">+	       IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION) ?</span>
<span class="p_add">+	       (boot_cpu_has(X86_FEATURE_PTI) ? &quot; PTI&quot; : &quot; NOPTI&quot;) : &quot;&quot;);</span>
 
 	if (notify_die(DIE_OOPS, str, regs, err,
 			current-&gt;thread.trap_nr, SIGSEGV) == NOTIFY_STOP)
<span class="p_header">diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S</span>
<span class="p_header">index 7dca675fe78d..04a625f0fcda 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S</span>
<span class="p_chunk">@@ -341,6 +341,27 @@</span> <span class="p_context"> GLOBAL(early_recursion_flag)</span>
 	.balign	PAGE_SIZE; \
 GLOBAL(name)
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Each PGD needs to be 8k long and 8k aligned.  We do not</span>
<span class="p_add">+ * ever go out to userspace with these, so we do not</span>
<span class="p_add">+ * strictly *need* the second page, but this allows us to</span>
<span class="p_add">+ * have a single set_pgd() implementation that does not</span>
<span class="p_add">+ * need to worry about whether it has 4k or 8k to work</span>
<span class="p_add">+ * with.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This ensures PGDs are 8k long:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PTI_USER_PGD_FILL	512</span>
<span class="p_add">+/* This ensures they are 8k-aligned: */</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) \</span>
<span class="p_add">+	.balign 2 * PAGE_SIZE; \</span>
<span class="p_add">+GLOBAL(name)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) NEXT_PAGE(name)</span>
<span class="p_add">+#define PTI_USER_PGD_FILL	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* Automate the creation of 1 to 1 mapping pmd entries */
 #define PMDS(START, PERM, COUNT)			\
 	i = 0 ;						\
<span class="p_chunk">@@ -350,13 +371,14 @@</span> <span class="p_context"> GLOBAL(name)</span>
 	.endr
 
 	__INITDATA
<span class="p_del">-NEXT_PAGE(early_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(early_top_pgt)</span>
 	.fill	511,8,0
 #ifdef CONFIG_X86_5LEVEL
 	.quad	level4_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC
 #else
 	.quad	level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC
 #endif
<span class="p_add">+	.fill	PTI_USER_PGD_FILL,8,0</span>
 
 NEXT_PAGE(early_dynamic_pgts)
 	.fill	512*EARLY_DYNAMIC_PAGE_TABLES,8,0
<span class="p_chunk">@@ -364,13 +386,14 @@</span> <span class="p_context"> NEXT_PAGE(early_dynamic_pgts)</span>
 	.data
 
 #if defined(CONFIG_XEN_PV) || defined(CONFIG_XEN_PVH)
<span class="p_del">-NEXT_PAGE(init_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_top_pgt)</span>
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
 	.org    init_top_pgt + PGD_PAGE_OFFSET*8, 0
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
 	.org    init_top_pgt + PGD_START_KERNEL*8, 0
 	/* (2^48-(2*1024*1024*1024))/(2^39) = 511 */
 	.quad   level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC
<span class="p_add">+	.fill	PTI_USER_PGD_FILL,8,0</span>
 
 NEXT_PAGE(level3_ident_pgt)
 	.quad	level2_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
<span class="p_chunk">@@ -381,8 +404,9 @@</span> <span class="p_context"> NEXT_PAGE(level2_ident_pgt)</span>
 	 */
 	PMDS(0, __PAGE_KERNEL_IDENT_LARGE_EXEC, PTRS_PER_PMD)
 #else
<span class="p_del">-NEXT_PAGE(init_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_top_pgt)</span>
 	.fill	512,8,0
<span class="p_add">+	.fill	PTI_USER_PGD_FILL,8,0</span>
 #endif
 
 #ifdef CONFIG_X86_5LEVEL
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index a6b5d62f45a7..579cc4a66fdf 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -24,6 +24,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/uaccess.h&gt;
 
 #include &lt;asm/ldt.h&gt;
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_chunk">@@ -51,13 +52,11 @@</span> <span class="p_context"> static void refresh_ldt_segments(void)</span>
 static void flush_ldt(void *__mm)
 {
 	struct mm_struct *mm = __mm;
<span class="p_del">-	mm_context_t *pc;</span>
 
 	if (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)
 		return;
 
<span class="p_del">-	pc = &amp;mm-&gt;context;</span>
<span class="p_del">-	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;nr_entries);</span>
<span class="p_add">+	load_mm_ldt(mm);</span>
 
 	refresh_ldt_segments();
 }
<span class="p_chunk">@@ -94,10 +93,126 @@</span> <span class="p_context"> static struct ldt_struct *alloc_ldt_struct(unsigned int num_entries)</span>
 		return NULL;
 	}
 
<span class="p_add">+	/* The new LDT isn&#39;t aliased for PTI yet. */</span>
<span class="p_add">+	new_ldt-&gt;slot = -1;</span>
<span class="p_add">+</span>
 	new_ldt-&gt;nr_entries = num_entries;
 	return new_ldt;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * If PTI is enabled, this maps the LDT into the kernelmode and</span>
<span class="p_add">+ * usermode tables for the given mm.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * There is no corresponding unmap function.  Even if the LDT is freed, we</span>
<span class="p_add">+ * leave the PTEs around until the slot is reused or the mm is destroyed.</span>
<span class="p_add">+ * This is harmless: the LDT is always in ordinary memory, and no one will</span>
<span class="p_add">+ * access the freed slot.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If we wanted to unmap freed LDTs, we&#39;d also need to do a flush to make</span>
<span class="p_add">+ * it useful, and the flush would slow down modify_ldt().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int</span>
<span class="p_add">+map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	bool is_vmalloc, had_top_level_entry;</span>
<span class="p_add">+	unsigned long va;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Any given ldt_struct should have map_ldt_struct() called at most</span>
<span class="p_add">+	 * once.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WARN_ON(ldt-&gt;slot != -1);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Did we already have the top level entry allocated?  We can&#39;t</span>
<span class="p_add">+	 * use pgd_none() for this because it doens&#39;t do anything on</span>
<span class="p_add">+	 * 4-level page table kernels.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="p_add">+	had_top_level_entry = (pgd-&gt;pgd != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i * PAGE_SIZE &lt; ldt-&gt;nr_entries * LDT_ENTRY_SIZE; i++) {</span>
<span class="p_add">+		unsigned long offset = i &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		const void *src = (char *)ldt-&gt;entries + offset;</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+		pte_t pte, *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		va = (unsigned long)ldt_slot_va(slot) + offset;</span>
<span class="p_add">+		pfn = is_vmalloc ? vmalloc_to_pfn(src) :</span>
<span class="p_add">+			page_to_pfn(virt_to_page(src));</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Treat the PTI LDT range as a *userspace* range.</span>
<span class="p_add">+		 * get_locked_pte() will allocate all needed pagetables</span>
<span class="p_add">+		 * and account for them in this mm.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ptep = get_locked_pte(mm, va, &amp;ptl);</span>
<span class="p_add">+		if (!ptep)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Map it RO so the easy to find address is not a primary</span>
<span class="p_add">+		 * target via some kernel interface which misses a</span>
<span class="p_add">+		 * permission check.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pte = pfn_pte(pfn, __pgprot(__PAGE_KERNEL_RO &amp; ~_PAGE_GLOBAL));</span>
<span class="p_add">+		set_pte_at(mm, va, ptep, pte);</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mm-&gt;context.ldt) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We already had an LDT.  The top-level entry should already</span>
<span class="p_add">+		 * have been allocated and synchronized with the usermode</span>
<span class="p_add">+		 * tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(!had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+			WARN_ON(!kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This is the first time we&#39;re mapping an LDT for this process.</span>
<span class="p_add">+		 * Sync the pgd to the usermode tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="p_add">+			WARN_ON(kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+			set_pgd(kernel_to_user_pgdp(pgd), *pgd);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	va = (unsigned long)ldt_slot_va(slot);</span>
<span class="p_add">+	flush_tlb_mm_range(mm, va, va + LDT_SLOT_STRIDE, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	ldt-&gt;slot = slot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_ldt_pgtables(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	struct mmu_gather tlb;</span>
<span class="p_add">+	unsigned long start = LDT_BASE_ADDR;</span>
<span class="p_add">+	unsigned long end = start + (1UL &lt;&lt; PGDIR_SHIFT);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="p_add">+	free_pgd_range(&amp;tlb, start, end, start, end);</span>
<span class="p_add">+	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* After calling this, the LDT is immutable. */
 static void finalize_ldt_struct(struct ldt_struct *ldt)
 {
<span class="p_chunk">@@ -156,6 +271,12 @@</span> <span class="p_context"> int ldt_dup_context(struct mm_struct *old_mm, struct mm_struct *mm)</span>
 	       new_ldt-&gt;nr_entries * LDT_ENTRY_SIZE);
 	finalize_ldt_struct(new_ldt);
 
<span class="p_add">+	retval = map_ldt_struct(mm, new_ldt, 0);</span>
<span class="p_add">+	if (retval) {</span>
<span class="p_add">+		free_ldt_pgtables(mm);</span>
<span class="p_add">+		free_ldt_struct(new_ldt);</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
 	mm-&gt;context.ldt = new_ldt;
 
 out_unlock:
<span class="p_chunk">@@ -174,6 +295,11 @@</span> <span class="p_context"> void destroy_context_ldt(struct mm_struct *mm)</span>
 	mm-&gt;context.ldt = NULL;
 }
 
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int read_ldt(void __user *ptr, unsigned long bytecount)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_chunk">@@ -287,6 +413,18 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 	new_ldt-&gt;entries[ldt_info.entry_number] = ldt;
 	finalize_ldt_struct(new_ldt);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we are using PTI, map the new LDT into the userspace pagetables.</span>
<span class="p_add">+	 * If there is already an LDT, use the other slot so that other CPUs</span>
<span class="p_add">+	 * will continue to use the old LDT until install_ldt() switches</span>
<span class="p_add">+	 * them over to the new LDT.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	error = map_ldt_struct(mm, new_ldt, old_ldt ? !old_ldt-&gt;slot : 0);</span>
<span class="p_add">+	if (error) {</span>
<span class="p_add">+		free_ldt_struct(old_ldt);</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	install_ldt(mm, new_ldt);
 	free_ldt_struct(old_ldt);
 	error = 0;
<span class="p_header">diff --git a/arch/x86/kernel/tls.c b/arch/x86/kernel/tls.c</span>
<span class="p_header">index 9a9c9b076955..a5b802a12212 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tls.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tls.c</span>
<span class="p_chunk">@@ -93,17 +93,10 @@</span> <span class="p_context"> static void set_tls_desc(struct task_struct *p, int idx,</span>
 	cpu = get_cpu();
 
 	while (n-- &gt; 0) {
<span class="p_del">-		if (LDT_empty(info) || LDT_zero(info)) {</span>
<span class="p_add">+		if (LDT_empty(info) || LDT_zero(info))</span>
 			memset(desc, 0, sizeof(*desc));
<span class="p_del">-		} else {</span>
<span class="p_add">+		else</span>
 			fill_ldt(desc, info);
<span class="p_del">-</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * Always set the accessed bit so that the CPU</span>
<span class="p_del">-			 * doesn&#39;t try to write to the (read-only) GDT.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			desc-&gt;type |= 1;</span>
<span class="p_del">-		}</span>
 		++info;
 		++desc;
 	}
<span class="p_header">diff --git a/arch/x86/kernel/vmlinux.lds.S b/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_header">index d2a8b5a24a44..1e413a9326aa 100644</span>
<span class="p_header">--- a/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_header">+++ b/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_chunk">@@ -61,11 +61,17 @@</span> <span class="p_context"> jiffies_64 = jiffies;</span>
 		. = ALIGN(HPAGE_SIZE);				\
 		__end_rodata_hpage_align = .;
 
<span class="p_add">+#define ALIGN_ENTRY_TEXT_BEGIN	. = ALIGN(PMD_SIZE);</span>
<span class="p_add">+#define ALIGN_ENTRY_TEXT_END	. = ALIGN(PMD_SIZE);</span>
<span class="p_add">+</span>
 #else
 
 #define X64_ALIGN_RODATA_BEGIN
 #define X64_ALIGN_RODATA_END
 
<span class="p_add">+#define ALIGN_ENTRY_TEXT_BEGIN</span>
<span class="p_add">+#define ALIGN_ENTRY_TEXT_END</span>
<span class="p_add">+</span>
 #endif
 
 PHDRS {
<span class="p_chunk">@@ -102,8 +108,10 @@</span> <span class="p_context"> SECTIONS</span>
 		CPUIDLE_TEXT
 		LOCK_TEXT
 		KPROBES_TEXT
<span class="p_add">+		ALIGN_ENTRY_TEXT_BEGIN</span>
 		ENTRY_TEXT
 		IRQENTRY_TEXT
<span class="p_add">+		ALIGN_ENTRY_TEXT_END</span>
 		SOFTIRQENTRY_TEXT
 		*(.fixup)
 		*(.gnu.warning)
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 2e0017af8f9b..52906808e277 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -43,9 +43,10 @@</span> <span class="p_context"> obj-$(CONFIG_AMD_NUMA)		+= amdtopology.o</span>
 obj-$(CONFIG_ACPI_NUMA)		+= srat.o
 obj-$(CONFIG_NUMA_EMU)		+= numa_emulation.o
 
<span class="p_del">-obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o</span>
<span class="p_del">-obj-$(CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS) += pkeys.o</span>
<span class="p_del">-obj-$(CONFIG_RANDOMIZE_MEMORY) += kaslr.o</span>
<span class="p_add">+obj-$(CONFIG_X86_INTEL_MPX)			+= mpx.o</span>
<span class="p_add">+obj-$(CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS)	+= pkeys.o</span>
<span class="p_add">+obj-$(CONFIG_RANDOMIZE_MEMORY)			+= kaslr.o</span>
<span class="p_add">+obj-$(CONFIG_PAGE_TABLE_ISOLATION)		+= pti.o</span>
 
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt.o
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_boot.o
<span class="p_header">diff --git a/arch/x86/mm/cpu_entry_area.c b/arch/x86/mm/cpu_entry_area.c</span>
<span class="p_header">index fe814fd5e014..b9283cc27622 100644</span>
<span class="p_header">--- a/arch/x86/mm/cpu_entry_area.c</span>
<span class="p_header">+++ b/arch/x86/mm/cpu_entry_area.c</span>
<span class="p_chunk">@@ -38,6 +38,32 @@</span> <span class="p_context"> cea_map_percpu_pages(void *cea_vaddr, void *ptr, int pages, pgprot_t prot)</span>
 		cea_set_pte(cea_vaddr, per_cpu_ptr_to_phys(ptr), prot);
 }
 
<span class="p_add">+static void percpu_setup_debug_store(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_CPU_SUP_INTEL</span>
<span class="p_add">+	int npages;</span>
<span class="p_add">+	void *cea;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_store;</span>
<span class="p_add">+	npages = sizeof(struct debug_store) / PAGE_SIZE;</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(struct debug_store) % PAGE_SIZE != 0);</span>
<span class="p_add">+	cea_map_percpu_pages(cea, &amp;per_cpu(cpu_debug_store, cpu), npages,</span>
<span class="p_add">+			     PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_buffers;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Force the population of PMDs for not yet allocated per cpu</span>
<span class="p_add">+	 * memory like debug store buffers.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	npages = sizeof(struct debug_store_buffers) / PAGE_SIZE;</span>
<span class="p_add">+	for (; npages; npages--, cea += PAGE_SIZE)</span>
<span class="p_add">+		cea_set_pte(cea, 0, PAGE_NONE);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Setup the fixmap mappings only once per-processor */
 static void __init setup_cpu_entry_area(int cpu)
 {
<span class="p_chunk">@@ -109,6 +135,7 @@</span> <span class="p_context"> static void __init setup_cpu_entry_area(int cpu)</span>
 	cea_set_pte(&amp;get_cpu_entry_area(cpu)-&gt;entry_trampoline,
 		     __pa_symbol(_entry_trampoline), PAGE_KERNEL_RX);
 #endif
<span class="p_add">+	percpu_setup_debug_store(cpu);</span>
 }
 
 static __init void setup_cpu_entry_area_ptes(void)
<span class="p_header">diff --git a/arch/x86/mm/debug_pagetables.c b/arch/x86/mm/debug_pagetables.c</span>
<span class="p_header">index bfcffdf6c577..421f2664ffa0 100644</span>
<span class="p_header">--- a/arch/x86/mm/debug_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/debug_pagetables.c</span>
<span class="p_chunk">@@ -5,7 +5,7 @@</span> <span class="p_context"></span>
 
 static int ptdump_show(struct seq_file *m, void *v)
 {
<span class="p_del">-	ptdump_walk_pgd_level(m, NULL);</span>
<span class="p_add">+	ptdump_walk_pgd_level_debugfs(m, NULL, false);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -22,21 +22,89 @@</span> <span class="p_context"> static const struct file_operations ptdump_fops = {</span>
 	.release	= single_release,
 };
 
<span class="p_del">-static struct dentry *pe;</span>
<span class="p_add">+static int ptdump_show_curknl(struct seq_file *m, void *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (current-&gt;mm-&gt;pgd) {</span>
<span class="p_add">+		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		ptdump_walk_pgd_level_debugfs(m, current-&gt;mm-&gt;pgd, false);</span>
<span class="p_add">+		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int ptdump_open_curknl(struct inode *inode, struct file *filp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return single_open(filp, ptdump_show_curknl, NULL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct file_operations ptdump_curknl_fops = {</span>
<span class="p_add">+	.owner		= THIS_MODULE,</span>
<span class="p_add">+	.open		= ptdump_open_curknl,</span>
<span class="p_add">+	.read		= seq_read,</span>
<span class="p_add">+	.llseek		= seq_lseek,</span>
<span class="p_add">+	.release	= single_release,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+static struct dentry *pe_curusr;</span>
<span class="p_add">+</span>
<span class="p_add">+static int ptdump_show_curusr(struct seq_file *m, void *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (current-&gt;mm-&gt;pgd) {</span>
<span class="p_add">+		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		ptdump_walk_pgd_level_debugfs(m, current-&gt;mm-&gt;pgd, true);</span>
<span class="p_add">+		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int ptdump_open_curusr(struct inode *inode, struct file *filp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return single_open(filp, ptdump_show_curusr, NULL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct file_operations ptdump_curusr_fops = {</span>
<span class="p_add">+	.owner		= THIS_MODULE,</span>
<span class="p_add">+	.open		= ptdump_open_curusr,</span>
<span class="p_add">+	.read		= seq_read,</span>
<span class="p_add">+	.llseek		= seq_lseek,</span>
<span class="p_add">+	.release	= single_release,</span>
<span class="p_add">+};</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static struct dentry *dir, *pe_knl, *pe_curknl;</span>
 
 static int __init pt_dump_debug_init(void)
 {
<span class="p_del">-	pe = debugfs_create_file(&quot;kernel_page_tables&quot;, S_IRUSR, NULL, NULL,</span>
<span class="p_del">-				 &amp;ptdump_fops);</span>
<span class="p_del">-	if (!pe)</span>
<span class="p_add">+	dir = debugfs_create_dir(&quot;page_tables&quot;, NULL);</span>
<span class="p_add">+	if (!dir)</span>
 		return -ENOMEM;
 
<span class="p_add">+	pe_knl = debugfs_create_file(&quot;kernel&quot;, 0400, dir, NULL,</span>
<span class="p_add">+				     &amp;ptdump_fops);</span>
<span class="p_add">+	if (!pe_knl)</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+</span>
<span class="p_add">+	pe_curknl = debugfs_create_file(&quot;current_kernel&quot;, 0400,</span>
<span class="p_add">+					dir, NULL, &amp;ptdump_curknl_fops);</span>
<span class="p_add">+	if (!pe_curknl)</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	pe_curusr = debugfs_create_file(&quot;current_user&quot;, 0400,</span>
<span class="p_add">+					dir, NULL, &amp;ptdump_curusr_fops);</span>
<span class="p_add">+	if (!pe_curusr)</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+#endif</span>
 	return 0;
<span class="p_add">+err:</span>
<span class="p_add">+	debugfs_remove_recursive(dir);</span>
<span class="p_add">+	return -ENOMEM;</span>
 }
 
 static void __exit pt_dump_debug_exit(void)
 {
<span class="p_del">-	debugfs_remove_recursive(pe);</span>
<span class="p_add">+	debugfs_remove_recursive(dir);</span>
 }
 
 module_init(pt_dump_debug_init);
<span class="p_header">diff --git a/arch/x86/mm/dump_pagetables.c b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">index 43dedbfb7257..f56902c1f04b 100644</span>
<span class="p_header">--- a/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_chunk">@@ -52,12 +52,18 @@</span> <span class="p_context"> enum address_markers_idx {</span>
 	USER_SPACE_NR = 0,
 	KERNEL_SPACE_NR,
 	LOW_KERNEL_NR,
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	LDT_NR,</span>
<span class="p_add">+#endif</span>
 	VMALLOC_START_NR,
 	VMEMMAP_START_NR,
 #ifdef CONFIG_KASAN
 	KASAN_SHADOW_START_NR,
 	KASAN_SHADOW_END_NR,
 #endif
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; !defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	LDT_NR,</span>
<span class="p_add">+#endif</span>
 	CPU_ENTRY_AREA_NR,
 #ifdef CONFIG_X86_ESPFIX64
 	ESPFIX_START_NR,
<span class="p_chunk">@@ -82,6 +88,9 @@</span> <span class="p_context"> static struct addr_marker address_markers[] = {</span>
 	[KASAN_SHADOW_START_NR]	= { KASAN_SHADOW_START,	&quot;KASAN shadow&quot; },
 	[KASAN_SHADOW_END_NR]	= { KASAN_SHADOW_END,	&quot;KASAN shadow end&quot; },
 #endif
<span class="p_add">+#ifdef CONFIG_MODIFY_LDT_SYSCALL</span>
<span class="p_add">+	[LDT_NR]		= { LDT_BASE_ADDR,	&quot;LDT remap&quot; },</span>
<span class="p_add">+#endif</span>
 	[CPU_ENTRY_AREA_NR]	= { CPU_ENTRY_AREA_BASE,&quot;CPU entry Area&quot; },
 #ifdef CONFIG_X86_ESPFIX64
 	[ESPFIX_START_NR]	= { ESPFIX_BASE_ADDR,	&quot;ESPfix Area&quot;, 16 },
<span class="p_chunk">@@ -467,7 +476,7 @@</span> <span class="p_context"> static inline bool is_hypervisor_range(int idx)</span>
 }
 
 static void ptdump_walk_pgd_level_core(struct seq_file *m, pgd_t *pgd,
<span class="p_del">-				       bool checkwx)</span>
<span class="p_add">+				       bool checkwx, bool dmesg)</span>
 {
 #ifdef CONFIG_X86_64
 	pgd_t *start = (pgd_t *) &amp;init_top_pgt;
<span class="p_chunk">@@ -480,7 +489,7 @@</span> <span class="p_context"> static void ptdump_walk_pgd_level_core(struct seq_file *m, pgd_t *pgd,</span>
 
 	if (pgd) {
 		start = pgd;
<span class="p_del">-		st.to_dmesg = true;</span>
<span class="p_add">+		st.to_dmesg = dmesg;</span>
 	}
 
 	st.check_wx = checkwx;
<span class="p_chunk">@@ -518,13 +527,37 @@</span> <span class="p_context"> static void ptdump_walk_pgd_level_core(struct seq_file *m, pgd_t *pgd,</span>
 
 void ptdump_walk_pgd_level(struct seq_file *m, pgd_t *pgd)
 {
<span class="p_del">-	ptdump_walk_pgd_level_core(m, pgd, false);</span>
<span class="p_add">+	ptdump_walk_pgd_level_core(m, pgd, false, true);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void ptdump_walk_pgd_level_debugfs(struct seq_file *m, pgd_t *pgd, bool user)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	if (user &amp;&amp; static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		pgd = kernel_to_user_pgdp(pgd);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	ptdump_walk_pgd_level_core(m, pgd, false, false);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(ptdump_walk_pgd_level_debugfs);</span>
<span class="p_add">+</span>
<span class="p_add">+static void ptdump_walk_user_pgd_level_checkwx(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	pgd_t *pgd = (pgd_t *) &amp;init_top_pgt;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;x86/mm: Checking user space page tables\n&quot;);</span>
<span class="p_add">+	pgd = kernel_to_user_pgdp(pgd);</span>
<span class="p_add">+	ptdump_walk_pgd_level_core(NULL, pgd, true, false);</span>
<span class="p_add">+#endif</span>
 }
<span class="p_del">-EXPORT_SYMBOL_GPL(ptdump_walk_pgd_level);</span>
 
 void ptdump_walk_pgd_level_checkwx(void)
 {
<span class="p_del">-	ptdump_walk_pgd_level_core(NULL, NULL, true);</span>
<span class="p_add">+	ptdump_walk_pgd_level_core(NULL, NULL, true, false);</span>
<span class="p_add">+	ptdump_walk_user_pgd_level_checkwx();</span>
 }
 
 static int __init pt_dump_init(void)
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index a22c2b95e513..80259ad8c386 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -20,6 +20,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/kaslr.h&gt;
 #include &lt;asm/hypervisor.h&gt;
 #include &lt;asm/cpufeature.h&gt;
<span class="p_add">+#include &lt;asm/pti.h&gt;</span>
 
 /*
  * We need to define the tracepoints somewhere, and tlb.c
<span class="p_chunk">@@ -161,6 +162,12 @@</span> <span class="p_context"> struct map_range {</span>
 
 static int page_size_mask;
 
<span class="p_add">+static void enable_global_pages(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		__supported_pte_mask |= _PAGE_GLOBAL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void __init probe_page_size_mask(void)
 {
 	/*
<span class="p_chunk">@@ -179,11 +186,11 @@</span> <span class="p_context"> static void __init probe_page_size_mask(void)</span>
 		cr4_set_bits_and_update_boot(X86_CR4_PSE);
 
 	/* Enable PGE if available */
<span class="p_add">+	__supported_pte_mask &amp;= ~_PAGE_GLOBAL;</span>
 	if (boot_cpu_has(X86_FEATURE_PGE)) {
 		cr4_set_bits_and_update_boot(X86_CR4_PGE);
<span class="p_del">-		__supported_pte_mask |= _PAGE_GLOBAL;</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		__supported_pte_mask &amp;= ~_PAGE_GLOBAL;</span>
<span class="p_add">+		enable_global_pages();</span>
<span class="p_add">+	}</span>
 
 	/* Enable 1 GB linear kernel mappings if available: */
 	if (direct_gbpages &amp;&amp; boot_cpu_has(X86_FEATURE_GBPAGES)) {
<span class="p_chunk">@@ -196,34 +203,44 @@</span> <span class="p_context"> static void __init probe_page_size_mask(void)</span>
 
 static void setup_pcid(void)
 {
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	if (boot_cpu_has(X86_FEATURE_PCID)) {</span>
<span class="p_del">-		if (boot_cpu_has(X86_FEATURE_PGE)) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * This can&#39;t be cr4_set_bits_and_update_boot() --</span>
<span class="p_del">-			 * the trampoline code can&#39;t handle CR4.PCIDE and</span>
<span class="p_del">-			 * it wouldn&#39;t do any good anyway.  Despite the name,</span>
<span class="p_del">-			 * cr4_set_bits_and_update_boot() doesn&#39;t actually</span>
<span class="p_del">-			 * cause the bits in question to remain set all the</span>
<span class="p_del">-			 * way through the secondary boot asm.</span>
<span class="p_del">-			 *</span>
<span class="p_del">-			 * Instead, we brute-force it and set CR4.PCIDE</span>
<span class="p_del">-			 * manually in start_secondary().</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			cr4_set_bits(X86_CR4_PCIDE);</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * flush_tlb_all(), as currently implemented, won&#39;t</span>
<span class="p_del">-			 * work if PCID is on but PGE is not.  Since that</span>
<span class="p_del">-			 * combination doesn&#39;t exist on real hardware, there&#39;s</span>
<span class="p_del">-			 * no reason to try to fully support it, but it&#39;s</span>
<span class="p_del">-			 * polite to avoid corrupting data if we&#39;re on</span>
<span class="p_del">-			 * an improperly configured VM.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			setup_clear_cpu_cap(X86_FEATURE_PCID);</span>
<span class="p_del">-		}</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_X86_64))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PGE)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This can&#39;t be cr4_set_bits_and_update_boot() -- the</span>
<span class="p_add">+		 * trampoline code can&#39;t handle CR4.PCIDE and it wouldn&#39;t</span>
<span class="p_add">+		 * do any good anyway.  Despite the name,</span>
<span class="p_add">+		 * cr4_set_bits_and_update_boot() doesn&#39;t actually cause</span>
<span class="p_add">+		 * the bits in question to remain set all the way through</span>
<span class="p_add">+		 * the secondary boot asm.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Instead, we brute-force it and set CR4.PCIDE manually in</span>
<span class="p_add">+		 * start_secondary().</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		cr4_set_bits(X86_CR4_PCIDE);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * INVPCID&#39;s single-context modes (2/3) only work if we set</span>
<span class="p_add">+		 * X86_CR4_PCIDE, *and* we INVPCID support.  It&#39;s unusable</span>
<span class="p_add">+		 * on systems that have X86_CR4_PCIDE clear, or that have</span>
<span class="p_add">+		 * no INVPCID support at all.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_INVPCID))</span>
<span class="p_add">+			setup_force_cpu_cap(X86_FEATURE_INVPCID_SINGLE);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * flush_tlb_all(), as currently implemented, won&#39;t work if</span>
<span class="p_add">+		 * PCID is on but PGE is not.  Since that combination</span>
<span class="p_add">+		 * doesn&#39;t exist on real hardware, there&#39;s no reason to try</span>
<span class="p_add">+		 * to fully support it, but it&#39;s polite to avoid corrupting</span>
<span class="p_add">+		 * data if we&#39;re on an improperly configured VM.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_PCID);</span>
 	}
<span class="p_del">-#endif</span>
 }
 
 #ifdef CONFIG_X86_32
<span class="p_chunk">@@ -624,6 +641,7 @@</span> <span class="p_context"> void __init init_mem_mapping(void)</span>
 {
 	unsigned long end;
 
<span class="p_add">+	pti_check_boottime_disable();</span>
 	probe_page_size_mask();
 	setup_pcid();
 
<span class="p_chunk">@@ -847,7 +865,7 @@</span> <span class="p_context"> void __init zone_sizes_init(void)</span>
 	free_area_init_nodes(max_zone_pfns);
 }
 
<span class="p_del">-DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {</span>
<span class="p_add">+__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {</span>
 	.loaded_mm = &amp;init_mm,
 	.next_asid = 1,
 	.cr4 = ~0UL,	/* fail hard if we screw up cr4 shadow initialization */
<span class="p_header">diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c</span>
<span class="p_header">index 17ebc5a978cc..9b7bcbd33cc2 100644</span>
<span class="p_header">--- a/arch/x86/mm/pgtable.c</span>
<span class="p_header">+++ b/arch/x86/mm/pgtable.c</span>
<span class="p_chunk">@@ -355,14 +355,15 @@</span> <span class="p_context"> static inline void _pgd_free(pgd_t *pgd)</span>
 		kmem_cache_free(pgd_cache, pgd);
 }
 #else
<span class="p_add">+</span>
 static inline pgd_t *_pgd_alloc(void)
 {
<span class="p_del">-	return (pgd_t *)__get_free_page(PGALLOC_GFP);</span>
<span class="p_add">+	return (pgd_t *)__get_free_pages(PGALLOC_GFP, PGD_ALLOCATION_ORDER);</span>
 }
 
 static inline void _pgd_free(pgd_t *pgd)
 {
<span class="p_del">-	free_page((unsigned long)pgd);</span>
<span class="p_add">+	free_pages((unsigned long)pgd, PGD_ALLOCATION_ORDER);</span>
 }
 #endif /* CONFIG_X86_PAE */
 
<span class="p_header">diff --git a/arch/x86/mm/pti.c b/arch/x86/mm/pti.c</span>
new file mode 100644
<span class="p_header">index 000000000000..bce8aea65606</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/pti.c</span>
<span class="p_chunk">@@ -0,0 +1,387 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright(c) 2017 Intel Corporation. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of version 2 of the GNU General Public License as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ * WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU</span>
<span class="p_add">+ * General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This code is based in part on work published here:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	https://github.com/IAIK/KAISER</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The original work was written by and and signed off by for the Linux</span>
<span class="p_add">+ * kernel by:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   Signed-off-by: Richard Fellner &lt;richard.fellner@student.tugraz.at&gt;</span>
<span class="p_add">+ *   Signed-off-by: Moritz Lipp &lt;moritz.lipp@iaik.tugraz.at&gt;</span>
<span class="p_add">+ *   Signed-off-by: Daniel Gruss &lt;daniel.gruss@iaik.tugraz.at&gt;</span>
<span class="p_add">+ *   Signed-off-by: Michael Schwarz &lt;michael.schwarz@iaik.tugraz.at&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Major changes to the original code by: Dave Hansen &lt;dave.hansen@intel.com&gt;</span>
<span class="p_add">+ * Mostly rewritten by Thomas Gleixner &lt;tglx@linutronix.de&gt; and</span>
<span class="p_add">+ *		       Andy Lutomirsky &lt;luto@amacapital.net&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &lt;linux/string.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cpufeature.h&gt;</span>
<span class="p_add">+#include &lt;asm/hypervisor.h&gt;</span>
<span class="p_add">+#include &lt;asm/vsyscall.h&gt;</span>
<span class="p_add">+#include &lt;asm/cmdline.h&gt;</span>
<span class="p_add">+#include &lt;asm/pti.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/desc.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#undef pr_fmt</span>
<span class="p_add">+#define pr_fmt(fmt)     &quot;Kernel/User page tables isolation: &quot; fmt</span>
<span class="p_add">+</span>
<span class="p_add">+/* Backporting helper */</span>
<span class="p_add">+#ifndef __GFP_NOTRACK</span>
<span class="p_add">+#define __GFP_NOTRACK	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init pti_print_if_insecure(const char *reason)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+		pr_info(&quot;%s\n&quot;, reason);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init pti_print_if_secure(const char *reason)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+		pr_info(&quot;%s\n&quot;, reason);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init pti_check_boottime_disable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char arg[5];</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hypervisor_is_type(X86_HYPER_XEN_PV)) {</span>
<span class="p_add">+		pti_print_if_insecure(&quot;disabled on XEN PV.&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = cmdline_find_option(boot_command_line, &quot;pti&quot;, arg, sizeof(arg));</span>
<span class="p_add">+	if (ret &gt; 0)  {</span>
<span class="p_add">+		if (ret == 3 &amp;&amp; !strncmp(arg, &quot;off&quot;, 3)) {</span>
<span class="p_add">+			pti_print_if_insecure(&quot;disabled on command line.&quot;);</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (ret == 2 &amp;&amp; !strncmp(arg, &quot;on&quot;, 2)) {</span>
<span class="p_add">+			pti_print_if_secure(&quot;force enabled on command line.&quot;);</span>
<span class="p_add">+			goto enable;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (ret == 4 &amp;&amp; !strncmp(arg, &quot;auto&quot;, 4))</span>
<span class="p_add">+			goto autosel;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;nopti&quot;)) {</span>
<span class="p_add">+		pti_print_if_insecure(&quot;disabled on command line.&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+autosel:</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+enable:</span>
<span class="p_add">+	setup_force_cpu_cap(X86_FEATURE_PTI);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+pgd_t __pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Changes to the high (kernel) portion of the kernelmode page</span>
<span class="p_add">+	 * tables are not automatically propagated to the usermode tables.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Users should keep in mind that, unlike the kernelmode tables,</span>
<span class="p_add">+	 * there is no vmalloc_fault equivalent for the usermode tables.</span>
<span class="p_add">+	 * Top-level entries added to init_mm&#39;s usermode pgd after boot</span>
<span class="p_add">+	 * will not be automatically propagated to other mms.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!pgdp_maps_userspace(pgdp))</span>
<span class="p_add">+		return pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The user page tables get the full PGD, accessible from</span>
<span class="p_add">+	 * userspace:</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kernel_to_user_pgdp(pgdp)-&gt;pgd = pgd.pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If this is normal user memory, make it NX in the kernel</span>
<span class="p_add">+	 * pagetables so that, if we somehow screw up and return to</span>
<span class="p_add">+	 * usermode with the kernel CR3 loaded, we&#39;ll get a page fault</span>
<span class="p_add">+	 * instead of allowing user code to execute with the wrong CR3.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * As exceptions, we don&#39;t set NX if:</span>
<span class="p_add">+	 *  - _PAGE_USER is not set.  This could be an executable</span>
<span class="p_add">+	 *     EFI runtime mapping or something similar, and the kernel</span>
<span class="p_add">+	 *     may execute from it</span>
<span class="p_add">+	 *  - we don&#39;t have NX support</span>
<span class="p_add">+	 *  - we&#39;re clearing the PGD (i.e. the new pgd is not present).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((pgd.pgd &amp; (_PAGE_USER|_PAGE_PRESENT)) == (_PAGE_USER|_PAGE_PRESENT) &amp;&amp;</span>
<span class="p_add">+	    (__supported_pte_mask &amp; _PAGE_NX))</span>
<span class="p_add">+		pgd.pgd |= _PAGE_NX;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* return the copy of the PGD we want the kernel to use: */</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Walk the user copy of the page tables (optionally) trying to allocate</span>
<span class="p_add">+ * page table pages on the way down.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns a pointer to a P4D on success, or NULL on failure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static p4d_t *pti_user_pagetable_walk_p4d(unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd = kernel_to_user_pgdp(pgd_offset_k(address));</span>
<span class="p_add">+	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (address &lt; PAGE_OFFSET) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;attempt to walk user address\n&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd_none(*pgd)) {</span>
<span class="p_add">+		unsigned long new_p4d_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_p4d_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pgd_none(*pgd)) {</span>
<span class="p_add">+			set_pgd(pgd, __pgd(_KERNPG_TABLE | __pa(new_p4d_page)));</span>
<span class="p_add">+			new_p4d_page = 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (new_p4d_page)</span>
<span class="p_add">+			free_page(new_p4d_page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	BUILD_BUG_ON(pgd_large(*pgd) != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	return p4d_offset(pgd, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Walk the user copy of the page tables (optionally) trying to allocate</span>
<span class="p_add">+ * page table pages on the way down.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns a pointer to a PMD on success, or NULL on failure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static pmd_t *pti_user_pagetable_walk_pmd(unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
<span class="p_add">+	p4d_t *p4d = pti_user_pagetable_walk_p4d(address);</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(p4d_large(*p4d) != 0);</span>
<span class="p_add">+	if (p4d_none(*p4d)) {</span>
<span class="p_add">+		unsigned long new_pud_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pud_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (p4d_none(*p4d)) {</span>
<span class="p_add">+			set_p4d(p4d, __p4d(_KERNPG_TABLE | __pa(new_pud_page)));</span>
<span class="p_add">+			new_pud_page = 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (new_pud_page)</span>
<span class="p_add">+			free_page(new_pud_page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
<span class="p_add">+	/* The user page tables do not use large mappings: */</span>
<span class="p_add">+	if (pud_large(*pud)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		unsigned long new_pmd_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pmd_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pud_none(*pud)) {</span>
<span class="p_add">+			set_pud(pud, __pud(_KERNPG_TABLE | __pa(new_pmd_page)));</span>
<span class="p_add">+			new_pmd_page = 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (new_pmd_page)</span>
<span class="p_add">+			free_page(new_pmd_page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return pmd_offset(pud, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_VSYSCALL_EMULATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Walk the shadow copy of the page tables (optionally) trying to allocate</span>
<span class="p_add">+ * page table pages on the way down.  Does not support large pages.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: this is only used when mapping *new* kernel data into the</span>
<span class="p_add">+ * user/shadow page tables.  It is never used for userspace data.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns a pointer to a PTE on success, or NULL on failure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __init pte_t *pti_user_pagetable_walk_pte(unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
<span class="p_add">+	pmd_t *pmd = pti_user_pagetable_walk_pmd(address);</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We can&#39;t do anything sensible if we hit a large mapping. */</span>
<span class="p_add">+	if (pmd_large(*pmd)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		unsigned long new_pte_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pte_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pmd_none(*pmd)) {</span>
<span class="p_add">+			set_pmd(pmd, __pmd(_KERNPG_TABLE | __pa(new_pte_page)));</span>
<span class="p_add">+			new_pte_page = 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (new_pte_page)</span>
<span class="p_add">+			free_page(new_pte_page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_kernel(pmd, address);</span>
<span class="p_add">+	if (pte_flags(*pte) &amp; _PAGE_USER) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;attempt to walk to user pte\n&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init pti_setup_vsyscall(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t *pte, *target_pte;</span>
<span class="p_add">+	unsigned int level;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = lookup_address(VSYSCALL_ADDR, &amp;level);</span>
<span class="p_add">+	if (!pte || WARN_ON(level != PG_LEVEL_4K) || pte_none(*pte))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	target_pte = pti_user_pagetable_walk_pte(VSYSCALL_ADDR);</span>
<span class="p_add">+	if (WARN_ON(!target_pte))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	*target_pte = *pte;</span>
<span class="p_add">+	set_vsyscall_pgtable_user_bits(kernel_to_user_pgdp(swapper_pg_dir));</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static void __init pti_setup_vsyscall(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init</span>
<span class="p_add">+pti_clone_pmds(unsigned long start, unsigned long end, pmdval_t clear)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Clone the populated PMDs which cover start to end. These PMD areas</span>
<span class="p_add">+	 * can have holes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (addr = start; addr &lt; end; addr += PMD_SIZE) {</span>
<span class="p_add">+		pmd_t *pmd, *target_pmd;</span>
<span class="p_add">+		pgd_t *pgd;</span>
<span class="p_add">+		p4d_t *p4d;</span>
<span class="p_add">+		pud_t *pud;</span>
<span class="p_add">+</span>
<span class="p_add">+		pgd = pgd_offset_k(addr);</span>
<span class="p_add">+		if (WARN_ON(pgd_none(*pgd)))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+		if (WARN_ON(p4d_none(*p4d)))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		pud = pud_offset(p4d, addr);</span>
<span class="p_add">+		if (pud_none(*pud))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+		if (pmd_none(*pmd))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		target_pmd = pti_user_pagetable_walk_pmd(addr);</span>
<span class="p_add">+		if (WARN_ON(!target_pmd))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Copy the PMD.  That is, the kernelmode and usermode</span>
<span class="p_add">+		 * tables will share the last-level page tables of this</span>
<span class="p_add">+		 * address range</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		*target_pmd = pmd_clear_flags(*pmd, clear);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Clone a single p4d (i.e. a top-level entry on 4-level systems and a</span>
<span class="p_add">+ * next-level entry on 5-level systems.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init pti_clone_p4d(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *kernel_p4d, *user_p4d;</span>
<span class="p_add">+	pgd_t *kernel_pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	user_p4d = pti_user_pagetable_walk_p4d(addr);</span>
<span class="p_add">+	kernel_pgd = pgd_offset_k(addr);</span>
<span class="p_add">+	kernel_p4d = p4d_offset(kernel_pgd, addr);</span>
<span class="p_add">+	*user_p4d = *kernel_p4d;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Clone the CPU_ENTRY_AREA into the user space visible page table.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init pti_clone_user_shared(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pti_clone_p4d(CPU_ENTRY_AREA_BASE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Clone the ESPFIX P4D into the user space visinble page table</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init pti_setup_espfix64(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_ESPFIX64</span>
<span class="p_add">+	pti_clone_p4d(ESPFIX_BASE_ADDR);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Clone the populated PMDs of the entry and irqentry text and force it RO.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init pti_clone_entry_text(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pti_clone_pmds((unsigned long) __entry_text_start,</span>
<span class="p_add">+			(unsigned long) __irqentry_text_end, _PAGE_RW);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Initialize kernel page table isolation</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init pti_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;enabled\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	pti_clone_user_shared();</span>
<span class="p_add">+	pti_clone_entry_text();</span>
<span class="p_add">+	pti_setup_espfix64();</span>
<span class="p_add">+	pti_setup_vsyscall();</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 0a1be3adc97e..a1561957dccb 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -28,6 +28,38 @@</span> <span class="p_context"></span>
  *	Implement flush IPI by CALL_FUNCTION_VECTOR, Alex Shi
  */
 
<span class="p_add">+/*</span>
<span class="p_add">+ * We get here when we do something requiring a TLB invalidation</span>
<span class="p_add">+ * but could not go invalidate all of the contexts.  We do the</span>
<span class="p_add">+ * necessary invalidation by clearing out the &#39;ctx_id&#39; which</span>
<span class="p_add">+ * forces a TLB flush when the context is loaded.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void clear_asid_other(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u16 asid;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This is only expected to be set if we have disabled</span>
<span class="p_add">+	 * kernel _PAGE_GLOBAL pages.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	for (asid = 0; asid &lt; TLB_NR_DYN_ASIDS; asid++) {</span>
<span class="p_add">+		/* Do not need to flush the current asid */</span>
<span class="p_add">+		if (asid == this_cpu_read(cpu_tlbstate.loaded_mm_asid))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Make sure the next time we go to switch to</span>
<span class="p_add">+		 * this asid, we do a flush:</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.ctxs[asid].ctx_id, 0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.invalidate_other, false);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 atomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);
 
 
<span class="p_chunk">@@ -42,6 +74,9 @@</span> <span class="p_context"> static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,</span>
 		return;
 	}
 
<span class="p_add">+	if (this_cpu_read(cpu_tlbstate.invalidate_other))</span>
<span class="p_add">+		clear_asid_other();</span>
<span class="p_add">+</span>
 	for (asid = 0; asid &lt; TLB_NR_DYN_ASIDS; asid++) {
 		if (this_cpu_read(cpu_tlbstate.ctxs[asid].ctx_id) !=
 		    next-&gt;context.ctx_id)
<span class="p_chunk">@@ -65,6 +100,25 @@</span> <span class="p_context"> static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,</span>
 	*need_flush = true;
 }
 
<span class="p_add">+static void load_new_mm_cr3(pgd_t *pgdir, u16 new_asid, bool need_flush)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long new_mm_cr3;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (need_flush) {</span>
<span class="p_add">+		invalidate_user_asid(new_asid);</span>
<span class="p_add">+		new_mm_cr3 = build_cr3(pgdir, new_asid);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		new_mm_cr3 = build_cr3_noflush(pgdir, new_asid);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Caution: many callers of this function expect</span>
<span class="p_add">+	 * that load_cr3() is serializing and orders TLB</span>
<span class="p_add">+	 * fills with respect to the mm_cpumask writes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	write_cr3(new_mm_cr3);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void leave_mm(int cpu)
 {
 	struct mm_struct *loaded_mm = this_cpu_read(cpu_tlbstate.loaded_mm);
<span class="p_chunk">@@ -195,7 +249,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		if (need_flush) {
 			this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next-&gt;context.ctx_id);
 			this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
<span class="p_del">-			write_cr3(build_cr3(next-&gt;pgd, new_asid));</span>
<span class="p_add">+			load_new_mm_cr3(next-&gt;pgd, new_asid, true);</span>
 
 			/*
 			 * NB: This gets called via leave_mm() in the idle path
<span class="p_chunk">@@ -208,7 +262,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 		} else {
 			/* The new ASID is already up to date. */
<span class="p_del">-			write_cr3(build_cr3_noflush(next-&gt;pgd, new_asid));</span>
<span class="p_add">+			load_new_mm_cr3(next-&gt;pgd, new_asid, false);</span>
 
 			/* See above wrt _rcuidle. */
 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, 0);
<span class="p_header">diff --git a/arch/x86/platform/efi/efi_64.c b/arch/x86/platform/efi/efi_64.c</span>
<span class="p_header">index 20fb31579b69..39c4b35ac7a4 100644</span>
<span class="p_header">--- a/arch/x86/platform/efi/efi_64.c</span>
<span class="p_header">+++ b/arch/x86/platform/efi/efi_64.c</span>
<span class="p_chunk">@@ -195,6 +195,9 @@</span> <span class="p_context"> static pgd_t *efi_pgd;</span>
  * because we want to avoid inserting EFI region mappings (EFI_VA_END
  * to EFI_VA_START) into the standard kernel page tables. Everything
  * else can be shared, see efi_sync_low_kernel_mappings().
<span class="p_add">+ *</span>
<span class="p_add">+ * We don&#39;t want the pgd on the pgd_list and cannot use pgd_alloc() for the</span>
<span class="p_add">+ * allocation.</span>
  */
 int __init efi_alloc_page_tables(void)
 {
<span class="p_chunk">@@ -207,7 +210,7 @@</span> <span class="p_context"> int __init efi_alloc_page_tables(void)</span>
 		return 0;
 
 	gfp_mask = GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO;
<span class="p_del">-	efi_pgd = (pgd_t *)__get_free_page(gfp_mask);</span>
<span class="p_add">+	efi_pgd = (pgd_t *)__get_free_pages(gfp_mask, PGD_ALLOCATION_ORDER);</span>
 	if (!efi_pgd)
 		return -ENOMEM;
 
<span class="p_header">diff --git a/include/linux/pti.h b/include/linux/pti.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0174883a935a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/pti.h</span>
<span class="p_chunk">@@ -0,0 +1,11 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+#ifndef _INCLUDE_PTI_H</span>
<span class="p_add">+#define _INCLUDE_PTI_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+#include &lt;asm/pti.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void pti_init(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/init/main.c b/init/main.c</span>
<span class="p_header">index 8a390f60ec81..b32ec72cdf3d 100644</span>
<span class="p_header">--- a/init/main.c</span>
<span class="p_header">+++ b/init/main.c</span>
<span class="p_chunk">@@ -75,6 +75,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/perf_event.h&gt;
 #include &lt;linux/ptrace.h&gt;
<span class="p_add">+#include &lt;linux/pti.h&gt;</span>
 #include &lt;linux/blkdev.h&gt;
 #include &lt;linux/elevator.h&gt;
 #include &lt;linux/sched_clock.h&gt;
<span class="p_chunk">@@ -506,6 +507,8 @@</span> <span class="p_context"> static void __init mm_init(void)</span>
 	ioremap_huge_init();
 	/* Should be run before the first non-init thread is created */
 	init_espfix_bsp();
<span class="p_add">+	/* Should be run after espfix64 is set up. */</span>
<span class="p_add">+	pti_init();</span>
 }
 
 asmlinkage __visible void __init start_kernel(void)
<span class="p_header">diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="p_header">index e8e449444e65..a623d13bf288 100644</span>
<span class="p_header">--- a/security/Kconfig</span>
<span class="p_header">+++ b/security/Kconfig</span>
<span class="p_chunk">@@ -54,6 +54,16 @@</span> <span class="p_context"> config SECURITY_NETWORK</span>
 	  implement socket and networking access controls.
 	  If you are unsure how to answer this question, answer N.
 
<span class="p_add">+config PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	bool &quot;Remove the kernel mapping in user mode&quot;</span>
<span class="p_add">+	depends on X86_64 &amp;&amp; !UML</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  This feature reduces the number of hardware side channels by</span>
<span class="p_add">+	  ensuring that the majority of kernel addresses are not mapped</span>
<span class="p_add">+	  into userspace.</span>
<span class="p_add">+</span>
<span class="p_add">+	  See Documentation/x86/pagetable-isolation.txt for more details.</span>
<span class="p_add">+</span>
 config SECURITY_INFINIBAND
 	bool &quot;Infiniband Security Hooks&quot;
 	depends on SECURITY &amp;&amp; INFINIBAND
<span class="p_header">diff --git a/tools/testing/selftests/x86/ldt_gdt.c b/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_header">index 0304ffb714f2..1aef72df20a1 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_chunk">@@ -122,8 +122,7 @@</span> <span class="p_context"> static void check_valid_segment(uint16_t index, int ldt,</span>
 	 * NB: Different Linux versions do different things with the
 	 * accessed bit in set_thread_area().
 	 */
<span class="p_del">-	if (ar != expected_ar &amp;&amp;</span>
<span class="p_del">-	    (ldt || ar != (expected_ar | AR_ACCESSED))) {</span>
<span class="p_add">+	if (ar != expected_ar &amp;&amp; ar != (expected_ar | AR_ACCESSED)) {</span>
 		printf(&quot;[FAIL]\t%s entry %hu has AR 0x%08X but expected 0x%08X\n&quot;,
 		       (ldt ? &quot;LDT&quot; : &quot;GDT&quot;), index, ar, expected_ar);
 		nerrs++;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



