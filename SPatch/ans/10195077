
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v5] vfio/type1: Adopt fast IOTLB flush interface when unmap IOVAs - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v5] vfio/type1: Adopt fast IOTLB flush interface when unmap IOVAs</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2908">Suthikulpanit, Suravee</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 1, 2018, 6:27 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1517466458-3523-1-git-send-email-suravee.suthikulpanit@amd.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10195077/mbox/"
   >mbox</a>
|
   <a href="/patch/10195077/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10195077/">/patch/10195077/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C1B3F60247 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  1 Feb 2018 06:28:23 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B8944289AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  1 Feb 2018 06:28:23 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id AB52B289B8; Thu,  1 Feb 2018 06:28:23 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F2F68289AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  1 Feb 2018 06:28:22 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751516AbeBAG2A (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 1 Feb 2018 01:28:00 -0500
Received: from mail-by2nam01on0084.outbound.protection.outlook.com
	([104.47.34.84]:26796
	&quot;EHLO NAM01-BY2-obe.outbound.protection.outlook.com&quot;
	rhost-flags-OK-OK-OK-FAIL) by vger.kernel.org with ESMTP
	id S1750998AbeBAG16 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 1 Feb 2018 01:27:58 -0500
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=amdcloud.onmicrosoft.com; s=selector1-amd-com;
	h=From:Date:Subject:Message-ID:Content-Type:MIME-Version;
	bh=qPJh46uLbP87LMViY55LqARFIUKA57rvTaUbLUdHgpc=;
	b=COsU+C+gAja34Yp8oBKkXdxbc2lohGajzkderH0ezr304CdutyxXRSTxY5WkwCTfzisumkmMxs8BBWvVBzciI8gdh2IC2ns+vbxF1Lr6uSZRV4DvgNcYMoKPaapEScTCPByRQXysShwteH9aUXr9PK61erSiLllWpQOzh0pCZmk=
Received: from ssuthiku-rhel74.localdomain (114.109.128.54) by
	DM5PR12MB1737.namprd12.prod.outlook.com (10.175.89.142) with
	Microsoft SMTP Server (version=TLS1_2,
	cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P256) id
	15.20.444.14; Thu, 1 Feb 2018 06:27:54 +0000
From: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;
To: kvm@vger.kernel.org, linux-kernel@vger.kernel.org
Cc: alex.williamson@redhat.com, joro@8bytes.org, jroedel@suse.de,
	Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;
Subject: [PATCH v5] vfio/type1: Adopt fast IOTLB flush interface when unmap
	IOVAs
Date: Thu,  1 Feb 2018 01:27:38 -0500
Message-Id: &lt;1517466458-3523-1-git-send-email-suravee.suthikulpanit@amd.com&gt;
X-Mailer: git-send-email 1.8.3.1
MIME-Version: 1.0
Content-Type: text/plain
X-Originating-IP: [114.109.128.54]
X-ClientProxiedBy: HK2PR04CA0086.apcprd04.prod.outlook.com (10.170.154.158)
	To DM5PR12MB1737.namprd12.prod.outlook.com (10.175.89.142)
X-MS-PublicTrafficType: Email
X-MS-Office365-Filtering-HT: Tenant
X-MS-Office365-Filtering-Correlation-Id: 9690fec6-1487-4f2e-e3de-08d5693cee90
X-Microsoft-Antispam: UriScan:; BCL:0; PCL:0;
	RULEID:(7020095)(4652020)(4534165)(4627221)(201703031133081)(201702281549075)(48565401081)(5600026)(4604075)(2017052603307)(7153060)(7193020);
	SRVR:DM5PR12MB1737; 
X-Microsoft-Exchange-Diagnostics: 1; DM5PR12MB1737;
	3:JDlpMUKDh3A4iY1+LxubCjENPR0s6LfMLienYOQaTjw9+iRC8fZWe5lIgKcjxJZq9FT8Bzb5K8kqxbojAnwtLmN0e+fSk/L6A4ce7UXpGNmxcDUOGL5aYvBO9EKmxwlWYBiXTQkme3Kduiilvw/wZKOrr2KPx3SKbhQs+CzmTzBYGHEBAB/PEz+XZ1pHUU5t6JpWPl8FGyzgs0e8k/a8uoKLGagUR/ZTxWYkzd5dWn/VzFbRvFmN48uOqUnXg2zW;
	25:uorai1oBbktzOh8QSRkHfRIVi1g6rhYYbYnLllwxfQstlfYkrnoZptlcCVQ9pGg31YhwdkGcgomYVfM3xuwH43F9m6iv0xperUY/63D5RvxqrWJUzHvW4MHcLRJaUMQTcMHleo2B+H4tGyGfX8AJDjY3XgzLpZcAnGJ4t8X42O2JUYnSGPTFn/fElV0e7gSxp3R07QT5DnmH4JCMMc4D2sC6nvAk/xtqrIGGAEzzNm1PQutT5ZPWJy3Nq/fB1g0LDtQr3GX+hDUXaPqzTg+LCGrEu+p0K0hLpKu5Gusr1P1Chokn3aj/ZhlnLrCkkYN2RkfpUgNr3xjsjt2jHgLbng==;
	31:KTqSnaOaSVaCKnvVIOHhwAiNgGB/+PCve0jbsmTc6/qkzzzLbFJ/t8h//nQtPR10BLruyBiVyYigRV6LStJp0fW9dyB2hY9kAbr6Hn/eyr2VjaXNPylrwAbFOpimq85ZRNIkt5BOk9Ndq+o3Y84X5TKXBJMxnsqfjOidFUUhqnOZ8NlemZKz6+A4HBuC1m7Ro9Xh/U/GGvOB7RLjyxFYyLcbrfq10rOKX0UamjrqUBk=
X-MS-TrafficTypeDiagnostic: DM5PR12MB1737:
Authentication-Results: spf=none (sender IP is )
	smtp.mailfrom=Suravee.Suthikulpanit@amd.com; 
X-Microsoft-Exchange-Diagnostics: 1; DM5PR12MB1737;
	20:MC4fGBYz/fQY7g3lDlHojiBIItBgVgNUkuLg2yLN8EkLPYHEUzYGjYnTViaEBI7OVN6rQ/vmPOZX8RBblXI/XmQ14jo+Z79fAosRqJu6uzCJs354A53uxeQyijK0UHVnlo/8FwvdEV3BHuqm28bqzfDs7cuxaOAw0PbgWMET+Ek6pLxDLBgAM3QxeUmuSApHF90y1Q1iOo1wC73oUOnpxh+aLqhwq/C4vnKkhPxlmMbtWXP7HKNsUQp0M8HcsocEHiNZPiP8wkWPCaANpm5N5cz623+ODCe3EAIibt8obQS+vClhyHF116TcCOxCc7kA95K2lHTvzneO2hBwhEYZ9m/YiQsjwBFvLnB0i8VDrnkb7F7g6TdKpe8GSjvoRypCzWcGSPeFiOJIZb4Xw2PCNXjj4UlKQjUKv0hwWy+6YYNtwdmn00fw2Gq/3iCN1d+SkMNUukij+dVTJWk8jHTKyVj0O4OW5RFFfJ80AJ21nyZT0xY+f2peu7OV3oQGdxCW;
	4:QfRIz7N6nu648prTOky9HpLfPIo+C7QXvKbmwoTJKtRshR194nL6KowZe6ZCj1AmxkgZW6eKRlnRVH6+LAFyr5iWhmITttVeB7Clw03JC4bKQ2cntjR7PYhkbj1uC3V3FlzkMJR78Q/ZVxc7Yx0P1vkzoix/3jlnJ8xKjXiRUOSJ1p3seiLY5M195akj4U1rPVDTOGzPVVYC8XD2QcsfhUtPJ0nbcg9ZnD11D6Es5JZxvA5UtLdP+l84iPYcaOrtDPC6Y/ubwI4mpPh2eAjRbT3WHW0C6Wx9vRuoybq8/KPGmdj5Jtv/zaKWJjZymVAV
X-Microsoft-Antispam-PRVS: &lt;DM5PR12MB1737DF750B8B059B20401B16F3FA0@DM5PR12MB1737.namprd12.prod.outlook.com&gt;
X-Exchange-Antispam-Report-Test: UriScan:(767451399110);
X-Exchange-Antispam-Report-CFA-Test: BCL:0; PCL:0;
	RULEID:(6040501)(2401047)(5005006)(8121501046)(10201501046)(3231101)(2400082)(944501161)(93006095)(93001095)(3002001)(6055026)(6041288)(201703131423095)(201702281528075)(20161123555045)(201703061421075)(201703061406153)(20161123564045)(20161123560045)(20161123562045)(20161123558120)(6072148)(201708071742011);
	SRVR:DM5PR12MB1737; BCL:0; PCL:0; RULEID:; SRVR:DM5PR12MB1737;
X-Forefront-PRVS: 0570F1F193
X-Forefront-Antispam-Report: SFV:NSPM;
	SFS:(10009020)(39860400002)(39380400002)(366004)(346002)(376002)(396003)(189003)(199004)(81166006)(25786009)(2906002)(386003)(47776003)(305945005)(6666003)(72206003)(478600001)(59450400001)(16586007)(66066001)(7736002)(6506007)(6116002)(3846002)(316002)(4326008)(52116002)(5660300001)(51416003)(4720700003)(186003)(86362001)(68736007)(26005)(53936002)(97736004)(50226002)(6486002)(8936002)(81156014)(6512007)(106356001)(48376002)(105586002)(36756003)(50466002)(6306002)(16526019)(8676002);
	DIR:OUT; SFP:1101; SCL:1; SRVR:DM5PR12MB1737;
	H:ssuthiku-rhel74.localdomain; FPR:; SPF:None;
	PTR:InfoNoRecords; MX:1; A:1; LANG:en; 
Received-SPF: None (protection.outlook.com: amd.com does not designate
	permitted sender hosts)
X-Microsoft-Exchange-Diagnostics: =?us-ascii?Q?1; DM5PR12MB1737;
	23:zeO5Ri3GdwdKvowwftb75n0hPA4Uz36xos+eeHDVs?=
	=?us-ascii?Q?vCi3CyZiLm2n+DymNV5GF98JgJl7ePwKB2NMBWVuDyYmX0BlVHUVmZ48DQKe?=
	=?us-ascii?Q?SGZZSuBamoNg6YyPuASrDqlB62oPkx9g94je6TSW1nWHR7XIRf+oAd/tYpGd?=
	=?us-ascii?Q?Ldc9ocJ6Mqz+4rSMSag3c0PkjG4C2ihqPF4BCjK6Y7m0UxpgIgjcS3oJiiFx?=
	=?us-ascii?Q?kiFpf7VVcL4OUPIEDUtpQF218fWdoit9O9Y/wDJfWjGg/oCoSFS9P7sJ+Elw?=
	=?us-ascii?Q?Zjoixd8MvxACVaI7Jj9z8zKGgmYePr9UOn0h+fjWDI3B+3WhB3+XwqtwRHZN?=
	=?us-ascii?Q?uBX44VogcKT5/4RjsTO4zARdBnfBkK1AjJMomnIRfHWODvKud67OErOXqHsZ?=
	=?us-ascii?Q?GiBAoeQKdakKLS1o+h0nXWcQULipMx/npOeK3AI+PpG55nB55JTsJ12/4mru?=
	=?us-ascii?Q?UbwKw/ydFI6UH+w55JhzXI43r+jWNdzOdL4EUyQhzGsS2t0spM5BbupHJ7da?=
	=?us-ascii?Q?XbGRIPC1/nxaghtZH7XnIVoh2zIlbDnskn29ggQW/OFn9MyUXqajbVprSJNe?=
	=?us-ascii?Q?RnQ8A3chiY9y7kLSv/C2n7So15DXWh1+34yOAuBiP0kDN/DFdv1fdYNp+RyC?=
	=?us-ascii?Q?rm/Ua+jb9x1QJEyjuhGMNMKVLELmHX/X2FGeODYhM8NKSKZ9fJcC4zwKGOQL?=
	=?us-ascii?Q?LKgX/hZ5DGwdu9Mz3YFdMk3yGUZixguw/CtKRNDnwDCv4lJprw7ejCPJd5s4?=
	=?us-ascii?Q?k9PYdmPLE8gAAYdVj4HiXOidTCo3RgEl1rXSzPko5IXE49u2gyGfWekFZsjz?=
	=?us-ascii?Q?s7kQAvxVPj2/+vOfAhYfLm1X3kSuvHESsgpGf+GhL/LOEID6xOX7g99jhUg2?=
	=?us-ascii?Q?K/l6b6eqJ6XJtOhx22Nkpqd++1tDwQYdanBc2Oblk2ENzV4Mz7L0r7zL5qt7?=
	=?us-ascii?Q?vf5CUD4Qu5hqpJXWAuNBYNpgZnmnyKa/6kmIVBofB43nGqQ1XBu8Nm1W8XbO?=
	=?us-ascii?Q?R02BhxaDvDJuaJrOrtIbnzp4Zw7x31mMdSURP6cQXxU8Cuo4PiXYsOj3DfwI?=
	=?us-ascii?Q?4/tLsm7wwlSDnnfvzkVZ4MkavHFk3S1KjPbh65Kj/n5Mux+1tXxV4ukA0IJf?=
	=?us-ascii?Q?V0vOR2wn7A=3D?=
X-Microsoft-Exchange-Diagnostics: 1; DM5PR12MB1737;
	6:q6/ufsrR+PsOcwtMS6a09lnULCKKm8USC4yGqZk3w3lHt+/iISd+nxtFNOUHiGNTL1m9ZndzH4iGSeATZ/wvpwZAeZQAFZ6dXYgNQiOV65vAlldYr5A+JMLfKn00GZEe4q2H+elLMfvkVCA8M1UGD1gSIYNnZneVDEPmX1f4fffJz1Cv8Jdf+mFP+/q/z9xlFciTqermdGg/tAERg9l9DWhR8icSrxltxSKnsP0fXcVEz5ySJJtXhY5Aq3EVfsLYssUqVQOkA8j8AjL875/mA7PCX9kaVF/aH7klp3I0VEH8+MrJ6miqAtbyW9y3GjdwJUQh/vUiJ+hz8cctmKZ4QOkyxBncknn2ZU9lHlQMhDE=;
	5:Fbs/X7HqQqn4g89Ed/GJ8WxsE5bMHqwktDkbfZJag5dTbxLRfS6L1AdPR/ewsGmff+XUEuNkn5nnfoKtpO1ALnXcC+vty56T+Da7uyhvXlEizlyt0kCPekmF1ZMTdf/xDL6v1w+3RVQreRqJh0khJTzhIiD8EXmV8GEg6XSiitw=;
	24:5v7zLO9FRHfWFIaa3889/WiLHcGBktt06pVAP3DfJsta5mfxIk0qxq012Zpb4EPDea+Q3jNkodMIkYCXzqxqLBaAI4FxwSUCey2ynmFwHK8=;
	7:xpHVfKGCnQrUtWvGqNPy9XlvJyOJgh0PGCswOwfZlEFxDA+VbRfeHtSDM2Ozee0Du7UmsfjXFwlrgerAIFBXMgpOnCoKc/WQYN2MUHbkGaBsojOeogXOSI16oqJW0Ta3YbwJx26OZUwKeryjhzSJDWK7MgkhNpV8ElYY9Uw9j8Vsf1+6t9zqod2PEFlYFtYFJZdHQYli/eT92ksSKgCd2UGsyDFfiMV63fyHTL1OcuLp0BTtYXCgXR8iFKJpJcvn
SpamDiagnosticOutput: 1:99
SpamDiagnosticMetadata: NSPM
X-Microsoft-Exchange-Diagnostics: 1; DM5PR12MB1737;
	20:SAMlbgjekszx3ufwl1yHB8SJTZJtxKCWuqUEByZGFTiM8GRoLxtlZRBxW9rHMv+7hREA8MwDzuxYwwngc6UCba4t8PYYn0xi69PaNbBp31EwEN+fp34JlsvVUJNFy7qoSycbmIUMvC7iaC66iS3cAEFJRa/95YgrCWmn7PySLuI2X9HBTmkH0HWciYV1OP8BquYVXRZ4Kp4PkmVvWC3z7lBbHt7OScgVIcP/mx815kxA21FFFWxJmZPU8B7CIcDO
X-OriginatorOrg: amd.com
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 01 Feb 2018 06:27:54.0314
	(UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: 9690fec6-1487-4f2e-e3de-08d5693cee90
X-MS-Exchange-CrossTenant-FromEntityHeader: Hosted
X-MS-Exchange-CrossTenant-Id: 3dd8961f-e488-4e60-8e11-a82d994e183d
X-MS-Exchange-Transport-CrossTenantHeadersStamped: DM5PR12MB1737
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2908">Suthikulpanit, Suravee</a> - Feb. 1, 2018, 6:27 a.m.</div>
<pre class="content">
VFIO IOMMU type1 currently upmaps IOVA pages synchronously, which requires
IOTLB flushing for every unmapping. This results in large IOTLB flushing
overhead when handling pass-through devices has a large number of mapped
IOVAs. This can be avoided by using the new IOTLB flushing interface.

Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;
Cc: Joerg Roedel &lt;joro@8bytes.org&gt;
<span class="signed-off-by">Signed-off-by: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;</span>
---

Changes from v4 (https://lkml.org/lkml/2018/1/31/153)
 * Change return type from ssize_t back to size_t since we no longer
   changing IOMMU API. Also update error handling logic accordingly.
 * In unmap_unpin_fast(), also sync when failing to allocate entry.
 * Some code restructuring and variable renaming.

 drivers/vfio/vfio_iommu_type1.c | 128 ++++++++++++++++++++++++++++++++++++----
 1 file changed, 117 insertions(+), 11 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7781">Alex Williamson</a> - Feb. 22, 2018, 10:59 p.m.</div>
<pre class="content">
On Thu,  1 Feb 2018 01:27:38 -0500
Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt; wrote:
<span class="quote">
&gt; VFIO IOMMU type1 currently upmaps IOVA pages synchronously, which requires</span>
<span class="quote">&gt; IOTLB flushing for every unmapping. This results in large IOTLB flushing</span>
<span class="quote">&gt; overhead when handling pass-through devices has a large number of mapped</span>
<span class="quote">&gt; IOVAs. This can be avoided by using the new IOTLB flushing interface.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Joerg Roedel &lt;joro@8bytes.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changes from v4 (https://lkml.org/lkml/2018/1/31/153)</span>
<span class="quote">&gt;  * Change return type from ssize_t back to size_t since we no longer</span>
<span class="quote">&gt;    changing IOMMU API. Also update error handling logic accordingly.</span>
<span class="quote">&gt;  * In unmap_unpin_fast(), also sync when failing to allocate entry.</span>
<span class="quote">&gt;  * Some code restructuring and variable renaming.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  drivers/vfio/vfio_iommu_type1.c | 128 ++++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt;  1 file changed, 117 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; index e30e29a..6041530 100644</span>
<span class="quote">&gt; --- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; +++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; @@ -102,6 +102,13 @@ struct vfio_pfn {</span>
<span class="quote">&gt;  	atomic_t		ref_count;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +struct vfio_regions {</span>
<span class="quote">&gt; +	struct list_head list;</span>
<span class="quote">&gt; +	dma_addr_t iova;</span>
<span class="quote">&gt; +	phys_addr_t phys;</span>
<span class="quote">&gt; +	size_t len;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #define IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)	\</span>
<span class="quote">&gt;  					(!list_empty(&amp;iommu-&gt;domain_list))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -648,11 +655,102 @@ static int vfio_iommu_type1_unpin_pages(void *iommu_data,</span>
<span class="quote">&gt;  	return i &gt; npage ? npage : (i &gt; 0 ? i : -EINVAL);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static long vfio_sync_unpin(struct vfio_dma *dma, struct vfio_domain *domain,</span>
<span class="quote">&gt; +				struct list_head *regions)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	long unlocked = 0;</span>
<span class="quote">&gt; +	struct vfio_regions *entry, *next;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	iommu_tlb_sync(domain-&gt;domain);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	list_for_each_entry_safe(entry, next, regions, list) {</span>
<span class="quote">&gt; +		unlocked += vfio_unpin_pages_remote(dma,</span>
<span class="quote">&gt; +						    entry-&gt;iova,</span>
<span class="quote">&gt; +						    entry-&gt;phys &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						    entry-&gt;len &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						    false);</span>
<span class="quote">&gt; +		list_del(&amp;entry-&gt;list);</span>
<span class="quote">&gt; +		kfree(entry);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	cond_resched();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return unlocked;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="quote">&gt; + * Therefore, when using IOTLB flush sync interface, VFIO need to keep track</span>
<span class="quote">&gt; + * of these regions (currently using a list).</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This value specifies maximum number of regions for each IOTLB flush sync.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define VFIO_IOMMU_TLB_SYNC_MAX		512</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static size_t unmap_unpin_fast(struct vfio_domain *domain,</span>
<span class="quote">&gt; +			       struct vfio_dma *dma, dma_addr_t *iova,</span>
<span class="quote">&gt; +			       size_t len, phys_addr_t phys, long *unlocked,</span>
<span class="quote">&gt; +			       struct list_head *unmapped_list,</span>
<span class="quote">&gt; +			       int *unmapped_cnt)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	size_t unmapped = 0;</span>
<span class="quote">&gt; +	struct vfio_regions *entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (entry) {</span>
<span class="quote">&gt; +		unmapped = iommu_unmap_fast(domain-&gt;domain, *iova, len);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!unmapped) {</span>
<span class="quote">&gt; +			kfree(entry);</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			iommu_tlb_range_add(domain-&gt;domain, *iova, unmapped);</span>
<span class="quote">&gt; +			entry-&gt;iova = *iova;</span>
<span class="quote">&gt; +			entry-&gt;phys = phys;</span>
<span class="quote">&gt; +			entry-&gt;len  = unmapped;</span>
<span class="quote">&gt; +			list_add_tail(&amp;entry-&gt;list, unmapped_list);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			*iova += unmapped;</span>
<span class="quote">&gt; +			(*unmapped_cnt)++;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Sync if the number of fast-unmap regions hits the limit</span>
<span class="quote">&gt; +	 * or in case of errors.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (*unmapped_cnt &gt;= VFIO_IOMMU_TLB_SYNC_MAX || !unmapped) {</span>
<span class="quote">&gt; +		*unlocked += vfio_sync_unpin(dma, domain,</span>
<span class="quote">&gt; +					     unmapped_list);</span>
<span class="quote">&gt; +		*unmapped_cnt = 0;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return unmapped;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static size_t unmap_unpin_slow(struct vfio_domain *domain,</span>
<span class="quote">&gt; +			       struct vfio_dma *dma, dma_addr_t *iova,</span>
<span class="quote">&gt; +			       size_t len, phys_addr_t phys,</span>
<span class="quote">&gt; +			       long *unlocked)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	size_t unmapped = iommu_unmap(domain-&gt;domain, *iova, len);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (unmapped) {</span>
<span class="quote">&gt; +		*unlocked += vfio_unpin_pages_remote(dma, *iova,</span>
<span class="quote">&gt; +						     phys &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						     unmapped &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; +						     false);</span>
<span class="quote">&gt; +		*iova += unmapped;</span>
<span class="quote">&gt; +		cond_resched();</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	return unmapped;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  			     bool do_accounting)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	dma_addr_t iova = dma-&gt;iova, end = dma-&gt;iova + dma-&gt;size;</span>
<span class="quote">&gt;  	struct vfio_domain *domain, *d;</span>
<span class="quote">&gt; +	struct list_head unmapped_region_list;</span>
<span class="quote">&gt; +	int unmapped_region_cnt = 0;</span>
<span class="quote">&gt;  	long unlocked = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!dma-&gt;size)</span>
<span class="quote">&gt; @@ -661,6 +759,8 @@ static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
<span class="quote">&gt;  	if (!IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu))</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	INIT_LIST_HEAD(&amp;unmapped_region_list);</span>

Since I harassed Shameer about using LIST_HEAD() for the iova list
extension, I feel obligated to note that it can also be used here.  If
you approve I&#39;ll just remove the above INIT_LIST_HEAD() and declare
unmapped_region_list as LIST_HEAD(unmapped_region_list);, no need to
re-send.  Otherwise looks fine to me.  Thanks,

Alex
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1631">Tian, Kevin</a> - Feb. 23, 2018, 8:20 a.m.</div>
<pre class="content">
<span class="quote">&gt; From: Alex Williamson</span>
<span class="quote">&gt; Sent: Friday, February 23, 2018 6:59 AM</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu,  1 Feb 2018 01:27:38 -0500</span>
<span class="quote">&gt; Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; VFIO IOMMU type1 currently upmaps IOVA pages synchronously, which</span>
<span class="quote">&gt; requires</span>
<span class="quote">&gt; &gt; IOTLB flushing for every unmapping. This results in large IOTLB flushing</span>
<span class="quote">&gt; &gt; overhead when handling pass-through devices has a large number of</span>
<span class="quote">&gt; mapped</span>
<span class="quote">&gt; &gt; IOVAs. This can be avoided by using the new IOTLB flushing interface.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Joerg Roedel &lt;joro@8bytes.org&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Changes from v4 (https://lkml.org/lkml/2018/1/31/153)</span>
<span class="quote">&gt; &gt;  * Change return type from ssize_t back to size_t since we no longer</span>
<span class="quote">&gt; &gt;    changing IOMMU API. Also update error handling logic accordingly.</span>
<span class="quote">&gt; &gt;  * In unmap_unpin_fast(), also sync when failing to allocate entry.</span>
<span class="quote">&gt; &gt;  * Some code restructuring and variable renaming.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  drivers/vfio/vfio_iommu_type1.c | 128</span>
<span class="quote">&gt; ++++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt; &gt;  1 file changed, 117 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; diff --git a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt; index e30e29a..6041530 100644</span>
<span class="quote">&gt; &gt; --- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt; +++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt; @@ -102,6 +102,13 @@ struct vfio_pfn {</span>
<span class="quote">&gt; &gt;  	atomic_t		ref_count;</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; +struct vfio_regions {</span>
<span class="quote">&gt; &gt; +	struct list_head list;</span>
<span class="quote">&gt; &gt; +	dma_addr_t iova;</span>
<span class="quote">&gt; &gt; +	phys_addr_t phys;</span>
<span class="quote">&gt; &gt; +	size_t len;</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  #define IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)	\</span>
<span class="quote">&gt; &gt;  					(!list_empty(&amp;iommu-&gt;domain_list))</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; @@ -648,11 +655,102 @@ static int</span>
<span class="quote">&gt; vfio_iommu_type1_unpin_pages(void *iommu_data,</span>
<span class="quote">&gt; &gt;  	return i &gt; npage ? npage : (i &gt; 0 ? i : -EINVAL);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; +static long vfio_sync_unpin(struct vfio_dma *dma, struct vfio_domain</span>
<span class="quote">&gt; *domain,</span>
<span class="quote">&gt; &gt; +				struct list_head *regions)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	long unlocked = 0;</span>
<span class="quote">&gt; &gt; +	struct vfio_regions *entry, *next;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	iommu_tlb_sync(domain-&gt;domain);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	list_for_each_entry_safe(entry, next, regions, list) {</span>
<span class="quote">&gt; &gt; +		unlocked += vfio_unpin_pages_remote(dma,</span>
<span class="quote">&gt; &gt; +						    entry-&gt;iova,</span>
<span class="quote">&gt; &gt; +						    entry-&gt;phys &gt;&gt;</span>
<span class="quote">&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; &gt; +						    entry-&gt;len &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; &gt; +						    false);</span>
<span class="quote">&gt; &gt; +		list_del(&amp;entry-&gt;list);</span>
<span class="quote">&gt; &gt; +		kfree(entry);</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	cond_resched();</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return unlocked;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="quote">&gt; &gt; + * Therefore, when using IOTLB flush sync interface, VFIO need to keep</span>
<span class="quote">&gt; track</span>
<span class="quote">&gt; &gt; + * of these regions (currently using a list).</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * This value specifies maximum number of regions for each IOTLB flush</span>
<span class="quote">&gt; sync.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +#define VFIO_IOMMU_TLB_SYNC_MAX		512</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static size_t unmap_unpin_fast(struct vfio_domain *domain,</span>
<span class="quote">&gt; &gt; +			       struct vfio_dma *dma, dma_addr_t *iova,</span>
<span class="quote">&gt; &gt; +			       size_t len, phys_addr_t phys, long *unlocked,</span>
<span class="quote">&gt; &gt; +			       struct list_head *unmapped_list,</span>
<span class="quote">&gt; &gt; +			       int *unmapped_cnt)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	size_t unmapped = 0;</span>
<span class="quote">&gt; &gt; +	struct vfio_regions *entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (entry) {</span>
<span class="quote">&gt; &gt; +		unmapped = iommu_unmap_fast(domain-&gt;domain, *iova,</span>
<span class="quote">&gt; len);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		if (!unmapped) {</span>
<span class="quote">&gt; &gt; +			kfree(entry);</span>
<span class="quote">&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			iommu_tlb_range_add(domain-&gt;domain, *iova,</span>
<span class="quote">&gt; unmapped);</span>
<span class="quote">&gt; &gt; +			entry-&gt;iova = *iova;</span>
<span class="quote">&gt; &gt; +			entry-&gt;phys = phys;</span>
<span class="quote">&gt; &gt; +			entry-&gt;len  = unmapped;</span>
<span class="quote">&gt; &gt; +			list_add_tail(&amp;entry-&gt;list, unmapped_list);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			*iova += unmapped;</span>
<span class="quote">&gt; &gt; +			(*unmapped_cnt)++;</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Sync if the number of fast-unmap regions hits the limit</span>
<span class="quote">&gt; &gt; +	 * or in case of errors.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (*unmapped_cnt &gt;= VFIO_IOMMU_TLB_SYNC_MAX</span>
<span class="quote">&gt; || !unmapped) {</span>
<span class="quote">&gt; &gt; +		*unlocked += vfio_sync_unpin(dma, domain,</span>
<span class="quote">&gt; &gt; +					     unmapped_list);</span>
<span class="quote">&gt; &gt; +		*unmapped_cnt = 0;</span>
<span class="quote">&gt; &gt; +	}</span>

I&#39;m not sure why returning ZERO is treated as only unmap error 
here, but if looking at __iommu_unmap clearly there are other
error codes returned also. I know it&#39;s not introduced by this
patch but Alex, was it deliberately implemented such way under 
any assumption or a typo?

Thanks
Kevin
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7781">Alex Williamson</a> - Feb. 23, 2018, 3:15 p.m.</div>
<pre class="content">
On Fri, 23 Feb 2018 08:20:51 +0000
&quot;Tian, Kevin&quot; &lt;kevin.tian@intel.com&gt; wrote:
<span class="quote">
&gt; &gt; From: Alex Williamson</span>
<span class="quote">&gt; &gt; Sent: Friday, February 23, 2018 6:59 AM</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On Thu,  1 Feb 2018 01:27:38 -0500</span>
<span class="quote">&gt; &gt; Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt; wrote:</span>
<span class="quote">&gt; &gt;   </span>
<span class="quote">&gt; &gt; &gt; VFIO IOMMU type1 currently upmaps IOVA pages synchronously, which  </span>
<span class="quote">&gt; &gt; requires  </span>
<span class="quote">&gt; &gt; &gt; IOTLB flushing for every unmapping. This results in large IOTLB flushing</span>
<span class="quote">&gt; &gt; &gt; overhead when handling pass-through devices has a large number of  </span>
<span class="quote">&gt; &gt; mapped  </span>
<span class="quote">&gt; &gt; &gt; IOVAs. This can be avoided by using the new IOTLB flushing interface.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Cc: Alex Williamson &lt;alex.williamson@redhat.com&gt;</span>
<span class="quote">&gt; &gt; &gt; Cc: Joerg Roedel &lt;joro@8bytes.org&gt;</span>
<span class="quote">&gt; &gt; &gt; Signed-off-by: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;</span>
<span class="quote">&gt; &gt; &gt; ---</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; Changes from v4 (https://lkml.org/lkml/2018/1/31/153)</span>
<span class="quote">&gt; &gt; &gt;  * Change return type from ssize_t back to size_t since we no longer</span>
<span class="quote">&gt; &gt; &gt;    changing IOMMU API. Also update error handling logic accordingly.</span>
<span class="quote">&gt; &gt; &gt;  * In unmap_unpin_fast(), also sync when failing to allocate entry.</span>
<span class="quote">&gt; &gt; &gt;  * Some code restructuring and variable renaming.</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;  drivers/vfio/vfio_iommu_type1.c | 128  </span>
<span class="quote">&gt; &gt; ++++++++++++++++++++++++++++++++++++----  </span>
<span class="quote">&gt; &gt; &gt;  1 file changed, 117 insertions(+), 11 deletions(-)</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; diff --git a/drivers/vfio/vfio_iommu_type1.c  </span>
<span class="quote">&gt; &gt; b/drivers/vfio/vfio_iommu_type1.c  </span>
<span class="quote">&gt; &gt; &gt; index e30e29a..6041530 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt; &gt; +++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="quote">&gt; &gt; &gt; @@ -102,6 +102,13 @@ struct vfio_pfn {</span>
<span class="quote">&gt; &gt; &gt;  	atomic_t		ref_count;</span>
<span class="quote">&gt; &gt; &gt;  };</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; +struct vfio_regions {</span>
<span class="quote">&gt; &gt; &gt; +	struct list_head list;</span>
<span class="quote">&gt; &gt; &gt; +	dma_addr_t iova;</span>
<span class="quote">&gt; &gt; &gt; +	phys_addr_t phys;</span>
<span class="quote">&gt; &gt; &gt; +	size_t len;</span>
<span class="quote">&gt; &gt; &gt; +};</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt;  #define IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)	\</span>
<span class="quote">&gt; &gt; &gt;  					(!list_empty(&amp;iommu-&gt;domain_list))</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; @@ -648,11 +655,102 @@ static int  </span>
<span class="quote">&gt; &gt; vfio_iommu_type1_unpin_pages(void *iommu_data,  </span>
<span class="quote">&gt; &gt; &gt;  	return i &gt; npage ? npage : (i &gt; 0 ? i : -EINVAL);</span>
<span class="quote">&gt; &gt; &gt;  }</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt; +static long vfio_sync_unpin(struct vfio_dma *dma, struct vfio_domain  </span>
<span class="quote">&gt; &gt; *domain,  </span>
<span class="quote">&gt; &gt; &gt; +				struct list_head *regions)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +	long unlocked = 0;</span>
<span class="quote">&gt; &gt; &gt; +	struct vfio_regions *entry, *next;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	iommu_tlb_sync(domain-&gt;domain);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	list_for_each_entry_safe(entry, next, regions, list) {</span>
<span class="quote">&gt; &gt; &gt; +		unlocked += vfio_unpin_pages_remote(dma,</span>
<span class="quote">&gt; &gt; &gt; +						    entry-&gt;iova,</span>
<span class="quote">&gt; &gt; &gt; +						    entry-&gt;phys &gt;&gt;  </span>
<span class="quote">&gt; &gt; PAGE_SHIFT,  </span>
<span class="quote">&gt; &gt; &gt; +						    entry-&gt;len &gt;&gt; PAGE_SHIFT,</span>
<span class="quote">&gt; &gt; &gt; +						    false);</span>
<span class="quote">&gt; &gt; &gt; +		list_del(&amp;entry-&gt;list);</span>
<span class="quote">&gt; &gt; &gt; +		kfree(entry);</span>
<span class="quote">&gt; &gt; &gt; +	}</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	cond_resched();</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	return unlocked;</span>
<span class="quote">&gt; &gt; &gt; +}</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +/*</span>
<span class="quote">&gt; &gt; &gt; + * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="quote">&gt; &gt; &gt; + * Therefore, when using IOTLB flush sync interface, VFIO need to keep  </span>
<span class="quote">&gt; &gt; track  </span>
<span class="quote">&gt; &gt; &gt; + * of these regions (currently using a list).</span>
<span class="quote">&gt; &gt; &gt; + *</span>
<span class="quote">&gt; &gt; &gt; + * This value specifies maximum number of regions for each IOTLB flush  </span>
<span class="quote">&gt; &gt; sync.  </span>
<span class="quote">&gt; &gt; &gt; + */</span>
<span class="quote">&gt; &gt; &gt; +#define VFIO_IOMMU_TLB_SYNC_MAX		512</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +static size_t unmap_unpin_fast(struct vfio_domain *domain,</span>
<span class="quote">&gt; &gt; &gt; +			       struct vfio_dma *dma, dma_addr_t *iova,</span>
<span class="quote">&gt; &gt; &gt; +			       size_t len, phys_addr_t phys, long *unlocked,</span>
<span class="quote">&gt; &gt; &gt; +			       struct list_head *unmapped_list,</span>
<span class="quote">&gt; &gt; &gt; +			       int *unmapped_cnt)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +	size_t unmapped = 0;</span>
<span class="quote">&gt; &gt; &gt; +	struct vfio_regions *entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	if (entry) {</span>
<span class="quote">&gt; &gt; &gt; +		unmapped = iommu_unmap_fast(domain-&gt;domain, *iova,  </span>
<span class="quote">&gt; &gt; len);  </span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +		if (!unmapped) {</span>
<span class="quote">&gt; &gt; &gt; +			kfree(entry);</span>
<span class="quote">&gt; &gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; &gt; +			iommu_tlb_range_add(domain-&gt;domain, *iova,  </span>
<span class="quote">&gt; &gt; unmapped);  </span>
<span class="quote">&gt; &gt; &gt; +			entry-&gt;iova = *iova;</span>
<span class="quote">&gt; &gt; &gt; +			entry-&gt;phys = phys;</span>
<span class="quote">&gt; &gt; &gt; +			entry-&gt;len  = unmapped;</span>
<span class="quote">&gt; &gt; &gt; +			list_add_tail(&amp;entry-&gt;list, unmapped_list);</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +			*iova += unmapped;</span>
<span class="quote">&gt; &gt; &gt; +			(*unmapped_cnt)++;</span>
<span class="quote">&gt; &gt; &gt; +		}</span>
<span class="quote">&gt; &gt; &gt; +	}</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; +	 * Sync if the number of fast-unmap regions hits the limit</span>
<span class="quote">&gt; &gt; &gt; +	 * or in case of errors.</span>
<span class="quote">&gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; +	if (*unmapped_cnt &gt;= VFIO_IOMMU_TLB_SYNC_MAX  </span>
<span class="quote">&gt; &gt; || !unmapped) {  </span>
<span class="quote">&gt; &gt; &gt; +		*unlocked += vfio_sync_unpin(dma, domain,</span>
<span class="quote">&gt; &gt; &gt; +					     unmapped_list);</span>
<span class="quote">&gt; &gt; &gt; +		*unmapped_cnt = 0;</span>
<span class="quote">&gt; &gt; &gt; +	}  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure why returning ZERO is treated as only unmap error </span>
<span class="quote">&gt; here, but if looking at __iommu_unmap clearly there are other</span>
<span class="quote">&gt; error codes returned also. I know it&#39;s not introduced by this</span>
<span class="quote">&gt; patch but Alex, was it deliberately implemented such way under </span>
<span class="quote">&gt; any assumption or a typo?</span>

iommu_unmap() returns a size_t, an unsigned type.  Suravee has another
patch in the iommu space to correct that function from trying to return
-errno.  Thanks,

Alex
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/vfio/vfio_iommu_type1.c b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">index e30e29a..6041530 100644</span>
<span class="p_header">--- a/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_header">+++ b/drivers/vfio/vfio_iommu_type1.c</span>
<span class="p_chunk">@@ -102,6 +102,13 @@</span> <span class="p_context"> struct vfio_pfn {</span>
 	atomic_t		ref_count;
 };
 
<span class="p_add">+struct vfio_regions {</span>
<span class="p_add">+	struct list_head list;</span>
<span class="p_add">+	dma_addr_t iova;</span>
<span class="p_add">+	phys_addr_t phys;</span>
<span class="p_add">+	size_t len;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #define IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu)	\
 					(!list_empty(&amp;iommu-&gt;domain_list))
 
<span class="p_chunk">@@ -648,11 +655,102 @@</span> <span class="p_context"> static int vfio_iommu_type1_unpin_pages(void *iommu_data,</span>
 	return i &gt; npage ? npage : (i &gt; 0 ? i : -EINVAL);
 }
 
<span class="p_add">+static long vfio_sync_unpin(struct vfio_dma *dma, struct vfio_domain *domain,</span>
<span class="p_add">+				struct list_head *regions)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long unlocked = 0;</span>
<span class="p_add">+	struct vfio_regions *entry, *next;</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu_tlb_sync(domain-&gt;domain);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry_safe(entry, next, regions, list) {</span>
<span class="p_add">+		unlocked += vfio_unpin_pages_remote(dma,</span>
<span class="p_add">+						    entry-&gt;iova,</span>
<span class="p_add">+						    entry-&gt;phys &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						    entry-&gt;len &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						    false);</span>
<span class="p_add">+		list_del(&amp;entry-&gt;list);</span>
<span class="p_add">+		kfree(entry);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	cond_resched();</span>
<span class="p_add">+</span>
<span class="p_add">+	return unlocked;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Generally, VFIO needs to unpin remote pages after each IOTLB flush.</span>
<span class="p_add">+ * Therefore, when using IOTLB flush sync interface, VFIO need to keep track</span>
<span class="p_add">+ * of these regions (currently using a list).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This value specifies maximum number of regions for each IOTLB flush sync.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define VFIO_IOMMU_TLB_SYNC_MAX		512</span>
<span class="p_add">+</span>
<span class="p_add">+static size_t unmap_unpin_fast(struct vfio_domain *domain,</span>
<span class="p_add">+			       struct vfio_dma *dma, dma_addr_t *iova,</span>
<span class="p_add">+			       size_t len, phys_addr_t phys, long *unlocked,</span>
<span class="p_add">+			       struct list_head *unmapped_list,</span>
<span class="p_add">+			       int *unmapped_cnt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	size_t unmapped = 0;</span>
<span class="p_add">+	struct vfio_regions *entry = kzalloc(sizeof(*entry), GFP_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (entry) {</span>
<span class="p_add">+		unmapped = iommu_unmap_fast(domain-&gt;domain, *iova, len);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!unmapped) {</span>
<span class="p_add">+			kfree(entry);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			iommu_tlb_range_add(domain-&gt;domain, *iova, unmapped);</span>
<span class="p_add">+			entry-&gt;iova = *iova;</span>
<span class="p_add">+			entry-&gt;phys = phys;</span>
<span class="p_add">+			entry-&gt;len  = unmapped;</span>
<span class="p_add">+			list_add_tail(&amp;entry-&gt;list, unmapped_list);</span>
<span class="p_add">+</span>
<span class="p_add">+			*iova += unmapped;</span>
<span class="p_add">+			(*unmapped_cnt)++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Sync if the number of fast-unmap regions hits the limit</span>
<span class="p_add">+	 * or in case of errors.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (*unmapped_cnt &gt;= VFIO_IOMMU_TLB_SYNC_MAX || !unmapped) {</span>
<span class="p_add">+		*unlocked += vfio_sync_unpin(dma, domain,</span>
<span class="p_add">+					     unmapped_list);</span>
<span class="p_add">+		*unmapped_cnt = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return unmapped;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static size_t unmap_unpin_slow(struct vfio_domain *domain,</span>
<span class="p_add">+			       struct vfio_dma *dma, dma_addr_t *iova,</span>
<span class="p_add">+			       size_t len, phys_addr_t phys,</span>
<span class="p_add">+			       long *unlocked)</span>
<span class="p_add">+{</span>
<span class="p_add">+	size_t unmapped = iommu_unmap(domain-&gt;domain, *iova, len);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unmapped) {</span>
<span class="p_add">+		*unlocked += vfio_unpin_pages_remote(dma, *iova,</span>
<span class="p_add">+						     phys &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						     unmapped &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+						     false);</span>
<span class="p_add">+		*iova += unmapped;</span>
<span class="p_add">+		cond_resched();</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return unmapped;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,
 			     bool do_accounting)
 {
 	dma_addr_t iova = dma-&gt;iova, end = dma-&gt;iova + dma-&gt;size;
 	struct vfio_domain *domain, *d;
<span class="p_add">+	struct list_head unmapped_region_list;</span>
<span class="p_add">+	int unmapped_region_cnt = 0;</span>
 	long unlocked = 0;
 
 	if (!dma-&gt;size)
<span class="p_chunk">@@ -661,6 +759,8 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 	if (!IS_IOMMU_CAP_DOMAIN_IN_CONTAINER(iommu))
 		return 0;
 
<span class="p_add">+	INIT_LIST_HEAD(&amp;unmapped_region_list);</span>
<span class="p_add">+</span>
 	/*
 	 * We use the IOMMU to track the physical addresses, otherwise we&#39;d
 	 * need a much more complicated tracking system.  Unfortunately that
<span class="p_chunk">@@ -698,20 +798,26 @@</span> <span class="p_context"> static long vfio_unmap_unpin(struct vfio_iommu *iommu, struct vfio_dma *dma,</span>
 				break;
 		}
 
<span class="p_del">-		unmapped = iommu_unmap(domain-&gt;domain, iova, len);</span>
<span class="p_del">-		if (WARN_ON(!unmapped))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-</span>
<span class="p_del">-		unlocked += vfio_unpin_pages_remote(dma, iova,</span>
<span class="p_del">-						    phys &gt;&gt; PAGE_SHIFT,</span>
<span class="p_del">-						    unmapped &gt;&gt; PAGE_SHIFT,</span>
<span class="p_del">-						    false);</span>
<span class="p_del">-		iova += unmapped;</span>
<span class="p_del">-</span>
<span class="p_del">-		cond_resched();</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * First, try to use fast unmap/unpin. In case of failure,</span>
<span class="p_add">+		 * switch to slow unmap/unpin path.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		unmapped = unmap_unpin_fast(domain, dma, &amp;iova, len, phys,</span>
<span class="p_add">+					    &amp;unlocked, &amp;unmapped_region_list,</span>
<span class="p_add">+					    &amp;unmapped_region_cnt);</span>
<span class="p_add">+		if (!unmapped) {</span>
<span class="p_add">+			unmapped = unmap_unpin_slow(domain, dma, &amp;iova, len,</span>
<span class="p_add">+						    phys, &amp;unlocked);</span>
<span class="p_add">+			if (WARN_ON(!unmapped))</span>
<span class="p_add">+				break;</span>
<span class="p_add">+		}</span>
 	}
 
 	dma-&gt;iommu_mapped = false;
<span class="p_add">+</span>
<span class="p_add">+	if (unmapped_region_cnt)</span>
<span class="p_add">+		unlocked += vfio_sync_unpin(dma, domain, &amp;unmapped_region_list);</span>
<span class="p_add">+</span>
 	if (do_accounting) {
 		vfio_lock_acct(dma-&gt;task, -unlocked, NULL);
 		return 0;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



