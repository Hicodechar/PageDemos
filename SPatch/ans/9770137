
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[15/17] RISC-V: Add mm subdirectory - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [15/17] RISC-V: Add mm subdirectory</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 6, 2017, 11 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170606230007.19101-16-palmer@dabbelt.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9770137/mbox/"
   >mbox</a>
|
   <a href="/patch/9770137/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9770137/">/patch/9770137/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	D47776035D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 23:03:49 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B664A284E4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 23:03:49 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A9FD8284F5; Tue,  6 Jun 2017 23:03:49 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID,RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 58759284E4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  6 Jun 2017 23:03:48 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751814AbdFFXDb (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 6 Jun 2017 19:03:31 -0400
Received: from mail-pf0-f193.google.com ([209.85.192.193]:36820 &quot;EHLO
	mail-pf0-f193.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751633AbdFFXBk (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 6 Jun 2017 19:01:40 -0400
Received: by mail-pf0-f193.google.com with SMTP id y7so7021449pfd.3
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 06 Jun 2017 16:01:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=dabbelt-com.20150623.gappssmtp.com; s=20150623;
	h=from:to:to:to:to:cc:cc:cc:subject:date:message-id:in-reply-to
	:references; bh=+UuPCIOojvm3is+ghah94wevy51ZwUDKIaRem+PlgNE=;
	b=e6ijtwrYSVV2iTpmEyssunRNixVOY4++vP4B1Fi6MuXHqKRifWWpUUXDn2CHVbt9zd
	kt1RhNRa1laBNZfN1EfhQOgyc5rr256M/HUwAt1H25dZu2f0uohQmQxP5xHOsbJg2FsQ
	w7vYWQpS0s0PlIHL91N2YZUEzN8p35lGvunsdBB/ja/V/DP45uNekmjsvkTFo5YDcDhw
	cXZXYB+c4DpwBvcpy9K987oaIehhbxgvunGVXJLOXNaoDjJA5macS2IxPDf207Wo/vud
	2imcuANWMEe8hHQvV5+RpC2Z+ubeaVPOe9CU5iy8cgMfpoY7zcS1ZJkLItGsfY27HSdM
	ko/w==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:to:to:to:cc:cc:cc:subject:date
	:message-id:in-reply-to:references;
	bh=+UuPCIOojvm3is+ghah94wevy51ZwUDKIaRem+PlgNE=;
	b=lG/LLs+LXsUeHWZmal2uKOxnjIfXbOjoIpRbA5ezMPt7wvBltTiFBSPf004Auo9/17
	nSjE3cJjDTAG7Br8GQ59EDLTv/ZGGHatIqsSzvQ/yMOCLUlfZob6ustcnNAqbJu9d79w
	X96RS7hxMrn5ObuSQqKd0jLzIsqn7HHce88V//EZD7vyJNRKq3TgA7Jx/mYppLJQEUML
	XwFzS4g8RgkU6lO70WQ+FLGMw1CS6q5B3bRdYRfVzA9XhoJ53gxy2X7clkSRramADbYB
	u0+uvC6Olusz8oMqObrhp3YVFeFhiFX9TXnb4GIuV01X0E0NCb0u/NohjLTVBVFZnyVJ
	CmNw==
X-Gm-Message-State: AODbwcCAoNnyfrITZ+yuxgtnHXhc/J66osNh4g/8LLzYut+T2W6wJQzi
	BhV2Lo9BZGGot7vq
X-Received: by 10.84.132.98 with SMTP id 89mr23743179ple.29.1496790099401;
	Tue, 06 Jun 2017 16:01:39 -0700 (PDT)
Received: from localhost ([216.38.154.21]) by smtp.gmail.com with ESMTPSA id
	x73sm71214245pfa.71.2017.06.06.16.01.38
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 06 Jun 2017 16:01:38 -0700 (PDT)
From: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
To: linux-arch@vger.kernel.org
To: linux-kernel@vger.kernel.org
To: Arnd Bergmann &lt;arnd@arndb.de&gt;
To: olof@lixom.net
Cc: albert@sifive.com
Cc: patches@groups.riscv.org
Cc: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
Subject: [PATCH 15/17] RISC-V: Add mm subdirectory
Date: Tue,  6 Jun 2017 16:00:05 -0700
Message-Id: &lt;20170606230007.19101-16-palmer@dabbelt.com&gt;
X-Mailer: git-send-email 2.13.0
In-Reply-To: &lt;20170606230007.19101-1-palmer@dabbelt.com&gt;
References: &lt;20170523004107.536-1-palmer@dabbelt.com&gt;
	&lt;20170606230007.19101-1-palmer@dabbelt.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 6, 2017, 11 p.m.</div>
<pre class="content">
These files are mostly based on the score port, but as all the non-stub
functions are very ISA specific they&#39;ve been heavily modified.
<span class="signed-off-by">
Signed-off-by: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;</span>
---
 arch/riscv/mm/Makefile  |   1 +
 arch/riscv/mm/extable.c |  37 +++++++
 arch/riscv/mm/fault.c   | 280 ++++++++++++++++++++++++++++++++++++++++++++++++
 arch/riscv/mm/init.c    |  72 +++++++++++++
 arch/riscv/mm/ioremap.c |  93 ++++++++++++++++
 5 files changed, 483 insertions(+)
 create mode 100644 arch/riscv/mm/Makefile
 create mode 100644 arch/riscv/mm/extable.c
 create mode 100644 arch/riscv/mm/fault.c
 create mode 100644 arch/riscv/mm/init.c
 create mode 100644 arch/riscv/mm/ioremap.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/riscv/mm/Makefile b/arch/riscv/mm/Makefile</span>
new file mode 100644
<span class="p_header">index 000000000000..36ebe6feb5d6</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/mm/Makefile</span>
<span class="p_chunk">@@ -0,0 +1 @@</span> <span class="p_context"></span>
<span class="p_add">+obj-y := init.o fault.o extable.o ioremap.o</span>
<span class="p_header">diff --git a/arch/riscv/mm/extable.c b/arch/riscv/mm/extable.c</span>
new file mode 100644
<span class="p_header">index 000000000000..11bb9417123b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/mm/extable.c</span>
<span class="p_chunk">@@ -0,0 +1,37 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Sunplus Core Technology Co., Ltd.</span>
<span class="p_add">+ *  Lennox Wu &lt;lennox.wu@sunplusct.com&gt;</span>
<span class="p_add">+ *  Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2013 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program; if not, see the file COPYING, or write</span>
<span class="p_add">+ * to the Free Software Foundation, Inc.,</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/extable.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+int fixup_exception(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const struct exception_table_entry *fixup;</span>
<span class="p_add">+</span>
<span class="p_add">+	fixup = search_exception_tables(regs-&gt;sepc);</span>
<span class="p_add">+	if (fixup) {</span>
<span class="p_add">+		regs-&gt;sepc = fixup-&gt;fixup;</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/riscv/mm/fault.c b/arch/riscv/mm/fault.c</span>
new file mode 100644
<span class="p_header">index 000000000000..b2a431c7f233</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/mm/fault.c</span>
<span class="p_chunk">@@ -0,0 +1,280 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Sunplus Core Technology Co., Ltd.</span>
<span class="p_add">+ *  Lennox Wu &lt;lennox.wu@sunplusct.com&gt;</span>
<span class="p_add">+ *  Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program; if not, see the file COPYING, or write</span>
<span class="p_add">+ * to the Free Software Foundation, Inc.,</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/interrupt.h&gt;</span>
<span class="p_add">+#include &lt;linux/perf_event.h&gt;</span>
<span class="p_add">+#include &lt;linux/signal.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/uaccess.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This routine handles page faults.  It determines the address and the</span>
<span class="p_add">+ * problem, and then passes it off to one of the appropriate routines.</span>
<span class="p_add">+ */</span>
<span class="p_add">+asmlinkage void do_page_fault(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct task_struct *tsk;</span>
<span class="p_add">+	struct vm_area_struct *vma;</span>
<span class="p_add">+	struct mm_struct *mm;</span>
<span class="p_add">+	unsigned long addr, cause;</span>
<span class="p_add">+	unsigned int flags = FAULT_FLAG_ALLOW_RETRY | FAULT_FLAG_KILLABLE;</span>
<span class="p_add">+	int fault, code = SEGV_MAPERR;</span>
<span class="p_add">+</span>
<span class="p_add">+	cause = regs-&gt;scause;</span>
<span class="p_add">+	addr = regs-&gt;sbadaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	tsk = current;</span>
<span class="p_add">+	mm = tsk-&gt;mm;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Fault-in kernel-space virtual memory on-demand.</span>
<span class="p_add">+	 * The &#39;reference&#39; page table is init_mm.pgd.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * NOTE! We MUST NOT take any locks for this case. We may</span>
<span class="p_add">+	 * be in an interrupt or a critical region, and should</span>
<span class="p_add">+	 * only copy the information from the master page table,</span>
<span class="p_add">+	 * nothing more.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (unlikely((addr &gt;= VMALLOC_START) &amp;&amp; (addr &lt;= VMALLOC_END)))</span>
<span class="p_add">+		goto vmalloc_fault;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Enable interrupts if they were enabled in the parent context. */</span>
<span class="p_add">+	if (likely(regs-&gt;sstatus &amp; SR_PIE))</span>
<span class="p_add">+		local_irq_enable();</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we&#39;re in an interrupt, have no user context, or are running</span>
<span class="p_add">+	 * in an atomic region, then we must not take the fault.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (unlikely(faulthandler_disabled() || !mm))</span>
<span class="p_add">+		goto no_context;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (user_mode(regs))</span>
<span class="p_add">+		flags |= FAULT_FLAG_USER;</span>
<span class="p_add">+</span>
<span class="p_add">+	perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS, 1, regs, addr);</span>
<span class="p_add">+</span>
<span class="p_add">+retry:</span>
<span class="p_add">+	down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	vma = find_vma(mm, addr);</span>
<span class="p_add">+	if (unlikely(!vma))</span>
<span class="p_add">+		goto bad_area;</span>
<span class="p_add">+	if (likely(vma-&gt;vm_start &lt;= addr))</span>
<span class="p_add">+		goto good_area;</span>
<span class="p_add">+	if (unlikely(!(vma-&gt;vm_flags &amp; VM_GROWSDOWN)))</span>
<span class="p_add">+		goto bad_area;</span>
<span class="p_add">+	if (unlikely(expand_stack(vma, addr)))</span>
<span class="p_add">+		goto bad_area;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Ok, we have a good vm_area for this memory access, so</span>
<span class="p_add">+	 * we can handle it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+good_area:</span>
<span class="p_add">+	code = SEGV_ACCERR;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (cause) {</span>
<span class="p_add">+	case EXC_INST_PAGE_FAULT:</span>
<span class="p_add">+		if (!(vma-&gt;vm_flags &amp; VM_EXEC))</span>
<span class="p_add">+			goto bad_area;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case EXC_LOAD_PAGE_FAULT:</span>
<span class="p_add">+		if (!(vma-&gt;vm_flags &amp; VM_READ))</span>
<span class="p_add">+			goto bad_area;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case EXC_STORE_PAGE_FAULT:</span>
<span class="p_add">+		if (!(vma-&gt;vm_flags &amp; VM_WRITE))</span>
<span class="p_add">+			goto bad_area;</span>
<span class="p_add">+		flags |= FAULT_FLAG_WRITE;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		panic(&quot;%s: unhandled cause %lu&quot;, __func__, cause);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If for any reason at all we could not handle the fault,</span>
<span class="p_add">+	 * make sure we exit gracefully rather than endlessly redo</span>
<span class="p_add">+	 * the fault.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	fault = handle_mm_fault(vma, addr, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we need to retry but a fatal signal is pending, handle the</span>
<span class="p_add">+	 * signal first. We do not need to release the mmap_sem because it</span>
<span class="p_add">+	 * would already be released in __lock_page_or_retry in mm/filemap.c.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((fault &amp; VM_FAULT_RETRY) &amp;&amp; fatal_signal_pending(tsk))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(fault &amp; VM_FAULT_ERROR)) {</span>
<span class="p_add">+		if (fault &amp; VM_FAULT_OOM)</span>
<span class="p_add">+			goto out_of_memory;</span>
<span class="p_add">+		else if (fault &amp; VM_FAULT_SIGBUS)</span>
<span class="p_add">+			goto do_sigbus;</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Major/minor page fault accounting is only done on the</span>
<span class="p_add">+	 * initial attempt. If we go through a retry, it is extremely</span>
<span class="p_add">+	 * likely that the page will be found in page cache at that point.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (flags &amp; FAULT_FLAG_ALLOW_RETRY) {</span>
<span class="p_add">+		if (fault &amp; VM_FAULT_MAJOR) {</span>
<span class="p_add">+			tsk-&gt;maj_flt++;</span>
<span class="p_add">+			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MAJ,</span>
<span class="p_add">+				      1, regs, addr);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			tsk-&gt;min_flt++;</span>
<span class="p_add">+			perf_sw_event(PERF_COUNT_SW_PAGE_FAULTS_MIN,</span>
<span class="p_add">+				      1, regs, addr);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (fault &amp; VM_FAULT_RETRY) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Clear FAULT_FLAG_ALLOW_RETRY to avoid any risk</span>
<span class="p_add">+			 * of starvation.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			flags &amp;= ~(FAULT_FLAG_ALLOW_RETRY);</span>
<span class="p_add">+			flags |= FAULT_FLAG_TRIED;</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * No need to up_read(&amp;mm-&gt;mmap_sem) as we would</span>
<span class="p_add">+			 * have already released it in __lock_page_or_retry</span>
<span class="p_add">+			 * in mm/filemap.c.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			goto retry;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Something tried to access memory that isn&#39;t in our memory map.</span>
<span class="p_add">+	 * Fix it, but check if it&#39;s kernel or user first.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+bad_area:</span>
<span class="p_add">+	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	/* User mode accesses just cause a SIGSEGV */</span>
<span class="p_add">+	if (user_mode(regs)) {</span>
<span class="p_add">+		do_trap(regs, SIGSEGV, code, addr, tsk);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+no_context:</span>
<span class="p_add">+	/* Are we prepared to handle this kernel fault? */</span>
<span class="p_add">+	if (fixup_exception(regs))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Oops. The kernel tried to access some bad page. We&#39;ll have to</span>
<span class="p_add">+	 * terminate things with extreme prejudice.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	bust_spinlocks(1);</span>
<span class="p_add">+	pr_alert(&quot;Unable to handle kernel %s at virtual address &quot; REG_FMT &quot;\n&quot;,</span>
<span class="p_add">+		(addr &lt; PAGE_SIZE) ? &quot;NULL pointer dereference&quot; :</span>
<span class="p_add">+		&quot;paging request&quot;, addr);</span>
<span class="p_add">+	die(regs, &quot;Oops&quot;);</span>
<span class="p_add">+	do_exit(SIGKILL);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We ran out of memory, call the OOM killer, and return the userspace</span>
<span class="p_add">+	 * (which will retry the fault, or kill us if we got oom-killed).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+out_of_memory:</span>
<span class="p_add">+	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	if (!user_mode(regs))</span>
<span class="p_add">+		goto no_context;</span>
<span class="p_add">+	pagefault_out_of_memory();</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+do_sigbus:</span>
<span class="p_add">+	up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	/* Kernel mode? Handle exceptions or die */</span>
<span class="p_add">+	if (!user_mode(regs))</span>
<span class="p_add">+		goto no_context;</span>
<span class="p_add">+	do_trap(regs, SIGBUS, BUS_ADRERR, addr, tsk);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+vmalloc_fault:</span>
<span class="p_add">+	{</span>
<span class="p_add">+		pgd_t *pgd, *pgd_k;</span>
<span class="p_add">+		pud_t *pud, *pud_k;</span>
<span class="p_add">+		p4d_t *p4d, *p4d_k;</span>
<span class="p_add">+		pmd_t *pmd, *pmd_k;</span>
<span class="p_add">+		pte_t *pte_k;</span>
<span class="p_add">+		int index;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (user_mode(regs))</span>
<span class="p_add">+			goto bad_area;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Synchronize this task&#39;s top level page-table</span>
<span class="p_add">+		 * with the &#39;reference&#39; page table.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Do _not_ use &quot;tsk-&gt;active_mm-&gt;pgd&quot; here.</span>
<span class="p_add">+		 * We might be inside an interrupt in the middle</span>
<span class="p_add">+		 * of a task switch.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		index = pgd_index(addr);</span>
<span class="p_add">+		pgd = (pgd_t *)pfn_to_virt(csr_read(sptbr)) + index;</span>
<span class="p_add">+		pgd_k = init_mm.pgd + index;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pgd_present(*pgd_k))</span>
<span class="p_add">+			goto no_context;</span>
<span class="p_add">+		set_pgd(pgd, *pgd_k);</span>
<span class="p_add">+</span>
<span class="p_add">+		p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+		p4d_k = p4d_offset(pgd_k, addr);</span>
<span class="p_add">+		if (!p4d_present(*p4d_k))</span>
<span class="p_add">+			goto no_context;</span>
<span class="p_add">+</span>
<span class="p_add">+		pud = pud_offset(p4d, addr);</span>
<span class="p_add">+		pud_k = pud_offset(p4d_k, addr);</span>
<span class="p_add">+		if (!pud_present(*pud_k))</span>
<span class="p_add">+			goto no_context;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Since the vmalloc area is global, it is unnecessary</span>
<span class="p_add">+		 * to copy individual PTEs</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+		pmd_k = pmd_offset(pud_k, addr);</span>
<span class="p_add">+		if (!pmd_present(*pmd_k))</span>
<span class="p_add">+			goto no_context;</span>
<span class="p_add">+		set_pmd(pmd, *pmd_k);</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Make sure the actual PTE exists as well to</span>
<span class="p_add">+		 * catch kernel vmalloc-area accesses to non-mapped</span>
<span class="p_add">+		 * addresses. If we don&#39;t do this, this will just</span>
<span class="p_add">+		 * silently loop forever.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pte_k = pte_offset_kernel(pmd_k, addr);</span>
<span class="p_add">+		if (!pte_present(*pte_k))</span>
<span class="p_add">+			goto no_context;</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/riscv/mm/init.c b/arch/riscv/mm/init.c</span>
new file mode 100644
<span class="p_header">index 000000000000..8ad464ce4a4c</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/mm/init.c</span>
<span class="p_chunk">@@ -0,0 +1,72 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/bootmem.h&gt;</span>
<span class="p_add">+#include &lt;linux/initrd.h&gt;</span>
<span class="p_add">+#include &lt;linux/memblock.h&gt;</span>
<span class="p_add">+#include &lt;linux/swap.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/sections.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/io.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init zone_sizes_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long zones_size[MAX_NR_ZONES];</span>
<span class="p_add">+</span>
<span class="p_add">+	memset(zones_size, 0, sizeof(zones_size));</span>
<span class="p_add">+	zones_size[ZONE_NORMAL] = pfn_base + max_mapnr;</span>
<span class="p_add">+	free_area_init_node(0, zones_size, pfn_base, NULL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void setup_zero_page(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	memset((void *)empty_zero_page, 0, PAGE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init paging_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	init_mm.pgd = (pgd_t *)pfn_to_virt(csr_read(sptbr));</span>
<span class="p_add">+</span>
<span class="p_add">+	setup_zero_page();</span>
<span class="p_add">+	local_flush_tlb_all();</span>
<span class="p_add">+	zone_sizes_init();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init mem_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_FLATMEM</span>
<span class="p_add">+	BUG_ON(!mem_map);</span>
<span class="p_add">+#endif /* CONFIG_FLATMEM */</span>
<span class="p_add">+</span>
<span class="p_add">+	high_memory = (void *)(__va(PFN_PHYS(max_low_pfn)));</span>
<span class="p_add">+	free_all_bootmem();</span>
<span class="p_add">+</span>
<span class="p_add">+	mem_init_print_info(NULL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void free_initmem(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_initmem_default(0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_BLK_DEV_INITRD</span>
<span class="p_add">+void free_initrd_mem(unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+//	free_reserved_area(start, end, 0, &quot;initrd&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_BLK_DEV_INITRD */</span>
<span class="p_add">+</span>
<span class="p_header">diff --git a/arch/riscv/mm/ioremap.c b/arch/riscv/mm/ioremap.c</span>
new file mode 100644
<span class="p_header">index 000000000000..c5cc0935096d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/mm/ioremap.c</span>
<span class="p_chunk">@@ -0,0 +1,93 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * (C) Copyright 1995 1996 Linus Torvalds</span>
<span class="p_add">+ * (C) Copyright 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ *   GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/export.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/vmalloc.h&gt;</span>
<span class="p_add">+#include &lt;linux/io.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Remap an arbitrary physical address space into the kernel virtual</span>
<span class="p_add">+ * address space. Needed when the kernel wants to access high addresses</span>
<span class="p_add">+ * directly.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * NOTE! We need to allow non-page-aligned mappings too: we will obviously</span>
<span class="p_add">+ * have to convert them into an offset in a page-aligned mapping, but the</span>
<span class="p_add">+ * caller shouldn&#39;t need to know that small detail.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __iomem *__ioremap_caller(phys_addr_t addr, size_t size,</span>
<span class="p_add">+	pgprot_t prot, void *caller)</span>
<span class="p_add">+{</span>
<span class="p_add">+	phys_addr_t last_addr;</span>
<span class="p_add">+	unsigned long offset, vaddr;</span>
<span class="p_add">+	struct vm_struct *area;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Disallow wrap-around or zero size */</span>
<span class="p_add">+	last_addr = addr + size - 1;</span>
<span class="p_add">+	if (!size || last_addr &lt; addr)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Page-align mappings */</span>
<span class="p_add">+	offset = addr &amp; (~PAGE_MASK);</span>
<span class="p_add">+	addr &amp;= PAGE_MASK;</span>
<span class="p_add">+	size = PAGE_ALIGN(size + offset);</span>
<span class="p_add">+</span>
<span class="p_add">+	area = get_vm_area_caller(size, VM_IOREMAP, caller);</span>
<span class="p_add">+	if (!area)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	vaddr = (unsigned long)area-&gt;addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ioremap_page_range(vaddr, vaddr + size, addr, prot)) {</span>
<span class="p_add">+		free_vm_area(area);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return (void __iomem *)(vaddr + offset);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * ioremap     -   map bus memory into CPU space</span>
<span class="p_add">+ * @offset:    bus address of the memory</span>
<span class="p_add">+ * @size:      size of the resource to map</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * ioremap performs a platform specific sequence of operations to</span>
<span class="p_add">+ * make bus memory CPU accessible via the readb/readw/readl/writeb/</span>
<span class="p_add">+ * writew/writel functions and the other mmio helpers. The returned</span>
<span class="p_add">+ * address is not guaranteed to be usable directly as a virtual</span>
<span class="p_add">+ * address.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Must be freed with iounmap.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __iomem *ioremap(phys_addr_t offset, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __ioremap_caller(offset, size, PAGE_KERNEL,</span>
<span class="p_add">+		__builtin_return_address(0));</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(ioremap);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * iounmap - Free a IO remapping</span>
<span class="p_add">+ * @addr: virtual address from ioremap_*</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Caller must ensure there is only one unmapping for the same pointer.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void iounmap(void __iomem *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	vunmap((void *)((unsigned long)addr &amp; PAGE_MASK));</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(iounmap);</span>
<span class="p_add">+</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



