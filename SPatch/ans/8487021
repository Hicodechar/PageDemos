
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>RFC: [PATCH] x86/kmmio: fix mmiotrace for hugepages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    RFC: [PATCH] x86/kmmio: fix mmiotrace for hugepages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=151631">Karol Herbst</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 3, 2016, 1:03 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1456966991-6861-1-git-send-email-nouveau@karolherbst.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8487021/mbox/"
   >mbox</a>
|
   <a href="/patch/8487021/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8487021/">/patch/8487021/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 0D0E49F372
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Mar 2016 01:03:31 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 2DC9E201FE
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Mar 2016 01:03:30 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 2F9322021F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Mar 2016 01:03:29 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754702AbcCCBD0 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 2 Mar 2016 20:03:26 -0500
Received: from smtp01.udag.de ([62.146.106.17]:35426 &quot;EHLO smtp01.udag.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751270AbcCCBDY (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 2 Mar 2016 20:03:24 -0500
Received: from pingu.lan (unknown [85.182.145.90])
	by smtp01.udag.de (Postfix) with ESMTPA id 46399D2;
	Thu,  3 Mar 2016 02:03:20 +0100 (CET)
From: Karol Herbst &lt;nouveau@karolherbst.de&gt;
To: linux-kernel@vger.kernel.org
Cc: linux-mm@kvack.org, linux-x86_64@vger.kernel.org,
	rostedt@goodmis.org, pq@iki.fi, mingo@redhat.com,
	nouveau@lists.freedesktop.org
Subject: RFC: [PATCH] x86/kmmio: fix mmiotrace for hugepages
Date: Thu,  3 Mar 2016 02:03:11 +0100
Message-Id: &lt;1456966991-6861-1-git-send-email-nouveau@karolherbst.de&gt;
X-Mailer: git-send-email 2.7.2
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=151631">Karol Herbst</a> - March 3, 2016, 1:03 a.m.</div>
<pre class="content">
Because Linux might use bigger pages than the 4K pages to handle those mmio
ioremaps, the kmmio code shouldn&#39;t rely on the pade id as it currently does.

Using the memory address instead of the page id let&#39;s us lookup how big the
page is and what it&#39;s base address is, so that we won&#39;t get a page fault
within the same page twice anymore.

I don&#39;t know if I got this right though, so please read this change with
great care

v2: use page_level macros
<span class="signed-off-by">
Signed-off-by: Karol Herbst &lt;nouveau@karolherbst.de&gt;</span>
---
 arch/x86/mm/kmmio.c | 89 ++++++++++++++++++++++++++++++++++++-----------------
 1 file changed, 60 insertions(+), 29 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99951">Pierre Moreau</a> - March 3, 2016, 10:49 p.m.</div>
<pre class="content">
The secondary hit exception thrown while MMIOtracing NVIDIA&#39;s driver is gone
with this patch.
<span class="tested-by">
Tested-by: Pierre Moreau &lt;pierre.morrow@free.fr&gt;</span>

On 02:03 AM - Mar 03 2016, Karol Herbst wrote:
<span class="quote">&gt; Because Linux might use bigger pages than the 4K pages to handle those mmio</span>
<span class="quote">&gt; ioremaps, the kmmio code shouldn&#39;t rely on the pade id as it currently does.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Using the memory address instead of the page id let&#39;s us lookup how big the</span>
<span class="quote">&gt; page is and what it&#39;s base address is, so that we won&#39;t get a page fault</span>
<span class="quote">&gt; within the same page twice anymore.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t know if I got this right though, so please read this change with</span>
<span class="quote">&gt; great care</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; v2: use page_level macros</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Karol Herbst &lt;nouveau@karolherbst.de&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/mm/kmmio.c | 89 ++++++++++++++++++++++++++++++++++++-----------------</span>
<span class="quote">&gt;  1 file changed, 60 insertions(+), 29 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/mm/kmmio.c b/arch/x86/mm/kmmio.c</span>
<span class="quote">&gt; index 637ab34..d203beb 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/kmmio.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/kmmio.c</span>
<span class="quote">&gt; @@ -33,7 +33,7 @@</span>
<span class="quote">&gt;  struct kmmio_fault_page {</span>
<span class="quote">&gt;  	struct list_head list;</span>
<span class="quote">&gt;  	struct kmmio_fault_page *release_next;</span>
<span class="quote">&gt; -	unsigned long page; /* location of the fault page */</span>
<span class="quote">&gt; +	unsigned long addr; /* the requested address */</span>
<span class="quote">&gt;  	pteval_t old_presence; /* page presence prior to arming */</span>
<span class="quote">&gt;  	bool armed;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -70,9 +70,16 @@ unsigned int kmmio_count;</span>
<span class="quote">&gt;  static struct list_head kmmio_page_table[KMMIO_PAGE_TABLE_SIZE];</span>
<span class="quote">&gt;  static LIST_HEAD(kmmio_probes);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct list_head *kmmio_page_list(unsigned long page)</span>
<span class="quote">&gt; +static struct list_head *kmmio_page_list(unsigned long addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return &amp;kmmio_page_table[hash_long(page, KMMIO_PAGE_HASH_BITS)];</span>
<span class="quote">&gt; +	unsigned int l;</span>
<span class="quote">&gt; +	pte_t *pte = lookup_address(addr, &amp;l);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!pte)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +	addr &amp;= page_level_mask(l);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return &amp;kmmio_page_table[hash_long(addr, KMMIO_PAGE_HASH_BITS)];</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* Accessed per-cpu */</span>
<span class="quote">&gt; @@ -98,15 +105,19 @@ static struct kmmio_probe *get_kmmio_probe(unsigned long addr)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* You must be holding RCU read lock. */</span>
<span class="quote">&gt; -static struct kmmio_fault_page *get_kmmio_fault_page(unsigned long page)</span>
<span class="quote">&gt; +static struct kmmio_fault_page *get_kmmio_fault_page(unsigned long addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct list_head *head;</span>
<span class="quote">&gt;  	struct kmmio_fault_page *f;</span>
<span class="quote">&gt; +	unsigned int l;</span>
<span class="quote">&gt; +	pte_t *pte = lookup_address(addr, &amp;l);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page &amp;= PAGE_MASK;</span>
<span class="quote">&gt; -	head = kmmio_page_list(page);</span>
<span class="quote">&gt; +	if (!pte)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +	addr &amp;= page_level_mask(l);</span>
<span class="quote">&gt; +	head = kmmio_page_list(addr);</span>
<span class="quote">&gt;  	list_for_each_entry_rcu(f, head, list) {</span>
<span class="quote">&gt; -		if (f-&gt;page == page)</span>
<span class="quote">&gt; +		if (f-&gt;addr == addr)</span>
<span class="quote">&gt;  			return f;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return NULL;</span>
<span class="quote">&gt; @@ -137,10 +148,10 @@ static void clear_pte_presence(pte_t *pte, bool clear, pteval_t *old)</span>
<span class="quote">&gt;  static int clear_page_presence(struct kmmio_fault_page *f, bool clear)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned int level;</span>
<span class="quote">&gt; -	pte_t *pte = lookup_address(f-&gt;page, &amp;level);</span>
<span class="quote">&gt; +	pte_t *pte = lookup_address(f-&gt;addr, &amp;level);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!pte) {</span>
<span class="quote">&gt; -		pr_err(&quot;no pte for page 0x%08lx\n&quot;, f-&gt;page);</span>
<span class="quote">&gt; +		pr_err(&quot;no pte for addr 0x%08lx\n&quot;, f-&gt;addr);</span>
<span class="quote">&gt;  		return -1;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -156,7 +167,7 @@ static int clear_page_presence(struct kmmio_fault_page *f, bool clear)</span>
<span class="quote">&gt;  		return -1;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	__flush_tlb_one(f-&gt;page);</span>
<span class="quote">&gt; +	__flush_tlb_one(f-&gt;addr);</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -176,12 +187,12 @@ static int arm_kmmio_fault_page(struct kmmio_fault_page *f)</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  	WARN_ONCE(f-&gt;armed, KERN_ERR pr_fmt(&quot;kmmio page already armed.\n&quot;));</span>
<span class="quote">&gt;  	if (f-&gt;armed) {</span>
<span class="quote">&gt; -		pr_warning(&quot;double-arm: page 0x%08lx, ref %d, old %d\n&quot;,</span>
<span class="quote">&gt; -			   f-&gt;page, f-&gt;count, !!f-&gt;old_presence);</span>
<span class="quote">&gt; +		pr_warning(&quot;double-arm: addr 0x%08lx, ref %d, old %d\n&quot;,</span>
<span class="quote">&gt; +			   f-&gt;addr, f-&gt;count, !!f-&gt;old_presence);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	ret = clear_page_presence(f, true);</span>
<span class="quote">&gt; -	WARN_ONCE(ret &lt; 0, KERN_ERR pr_fmt(&quot;arming 0x%08lx failed.\n&quot;),</span>
<span class="quote">&gt; -		  f-&gt;page);</span>
<span class="quote">&gt; +	WARN_ONCE(ret &lt; 0, KERN_ERR pr_fmt(&quot;arming at 0x%08lx failed.\n&quot;),</span>
<span class="quote">&gt; +		  f-&gt;addr);</span>
<span class="quote">&gt;  	f-&gt;armed = true;</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -191,7 +202,7 @@ static void disarm_kmmio_fault_page(struct kmmio_fault_page *f)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret = clear_page_presence(f, false);</span>
<span class="quote">&gt;  	WARN_ONCE(ret &lt; 0,</span>
<span class="quote">&gt; -			KERN_ERR &quot;kmmio disarming 0x%08lx failed.\n&quot;, f-&gt;page);</span>
<span class="quote">&gt; +			KERN_ERR &quot;kmmio disarming at 0x%08lx failed.\n&quot;, f-&gt;addr);</span>
<span class="quote">&gt;  	f-&gt;armed = false;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -215,6 +226,12 @@ int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
<span class="quote">&gt;  	struct kmmio_context *ctx;</span>
<span class="quote">&gt;  	struct kmmio_fault_page *faultpage;</span>
<span class="quote">&gt;  	int ret = 0; /* default to fault not handled */</span>
<span class="quote">&gt; +	unsigned long page_base = addr;</span>
<span class="quote">&gt; +	unsigned int l;</span>
<span class="quote">&gt; +	pte_t *pte = lookup_address(addr, &amp;l);</span>
<span class="quote">&gt; +	if (!pte)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +	page_base &amp;= page_level_mask(l);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Preemption is now disabled to prevent process switch during</span>
<span class="quote">&gt; @@ -227,7 +244,7 @@ int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
<span class="quote">&gt;  	preempt_disable();</span>
<span class="quote">&gt;  	rcu_read_lock();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	faultpage = get_kmmio_fault_page(addr);</span>
<span class="quote">&gt; +	faultpage = get_kmmio_fault_page(page_base);</span>
<span class="quote">&gt;  	if (!faultpage) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Either this page fault is not caused by kmmio, or</span>
<span class="quote">&gt; @@ -239,7 +256,7 @@ int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ctx = &amp;get_cpu_var(kmmio_ctx);</span>
<span class="quote">&gt;  	if (ctx-&gt;active) {</span>
<span class="quote">&gt; -		if (addr == ctx-&gt;addr) {</span>
<span class="quote">&gt; +		if (page_base == ctx-&gt;addr) {</span>
<span class="quote">&gt;  			/*</span>
<span class="quote">&gt;  			 * A second fault on the same page means some other</span>
<span class="quote">&gt;  			 * condition needs handling by do_page_fault(), the</span>
<span class="quote">&gt; @@ -267,9 +284,9 @@ int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
<span class="quote">&gt;  	ctx-&gt;active++;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ctx-&gt;fpage = faultpage;</span>
<span class="quote">&gt; -	ctx-&gt;probe = get_kmmio_probe(addr);</span>
<span class="quote">&gt; +	ctx-&gt;probe = get_kmmio_probe(page_base);</span>
<span class="quote">&gt;  	ctx-&gt;saved_flags = (regs-&gt;flags &amp; (X86_EFLAGS_TF | X86_EFLAGS_IF));</span>
<span class="quote">&gt; -	ctx-&gt;addr = addr;</span>
<span class="quote">&gt; +	ctx-&gt;addr = page_base;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (ctx-&gt;probe &amp;&amp; ctx-&gt;probe-&gt;pre_handler)</span>
<span class="quote">&gt;  		ctx-&gt;probe-&gt;pre_handler(ctx-&gt;probe, regs, addr);</span>
<span class="quote">&gt; @@ -354,12 +371,11 @@ out:</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* You must be holding kmmio_lock. */</span>
<span class="quote">&gt; -static int add_kmmio_fault_page(unsigned long page)</span>
<span class="quote">&gt; +static int add_kmmio_fault_page(unsigned long addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kmmio_fault_page *f;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page &amp;= PAGE_MASK;</span>
<span class="quote">&gt; -	f = get_kmmio_fault_page(page);</span>
<span class="quote">&gt; +	f = get_kmmio_fault_page(addr);</span>
<span class="quote">&gt;  	if (f) {</span>
<span class="quote">&gt;  		if (!f-&gt;count)</span>
<span class="quote">&gt;  			arm_kmmio_fault_page(f);</span>
<span class="quote">&gt; @@ -372,26 +388,25 @@ static int add_kmmio_fault_page(unsigned long page)</span>
<span class="quote">&gt;  		return -1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	f-&gt;count = 1;</span>
<span class="quote">&gt; -	f-&gt;page = page;</span>
<span class="quote">&gt; +	f-&gt;addr = addr;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (arm_kmmio_fault_page(f)) {</span>
<span class="quote">&gt;  		kfree(f);</span>
<span class="quote">&gt;  		return -1;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	list_add_rcu(&amp;f-&gt;list, kmmio_page_list(f-&gt;page));</span>
<span class="quote">&gt; +	list_add_rcu(&amp;f-&gt;list, kmmio_page_list(f-&gt;addr));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* You must be holding kmmio_lock. */</span>
<span class="quote">&gt; -static void release_kmmio_fault_page(unsigned long page,</span>
<span class="quote">&gt; +static void release_kmmio_fault_page(unsigned long addr,</span>
<span class="quote">&gt;  				struct kmmio_fault_page **release_list)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct kmmio_fault_page *f;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	page &amp;= PAGE_MASK;</span>
<span class="quote">&gt; -	f = get_kmmio_fault_page(page);</span>
<span class="quote">&gt; +	f = get_kmmio_fault_page(addr);</span>
<span class="quote">&gt;  	if (!f)</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -420,18 +435,27 @@ int register_kmmio_probe(struct kmmio_probe *p)</span>
<span class="quote">&gt;  	int ret = 0;</span>
<span class="quote">&gt;  	unsigned long size = 0;</span>
<span class="quote">&gt;  	const unsigned long size_lim = p-&gt;len + (p-&gt;addr &amp; ~PAGE_MASK);</span>
<span class="quote">&gt; +	unsigned int l;</span>
<span class="quote">&gt; +	pte_t *pte;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock_irqsave(&amp;kmmio_lock, flags);</span>
<span class="quote">&gt;  	if (get_kmmio_probe(p-&gt;addr)) {</span>
<span class="quote">&gt;  		ret = -EEXIST;</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pte = lookup_address(p-&gt;addr, &amp;l);</span>
<span class="quote">&gt; +	if (!pte) {</span>
<span class="quote">&gt; +		ret = -EINVAL;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	kmmio_count++;</span>
<span class="quote">&gt;  	list_add_rcu(&amp;p-&gt;list, &amp;kmmio_probes);</span>
<span class="quote">&gt;  	while (size &lt; size_lim) {</span>
<span class="quote">&gt;  		if (add_kmmio_fault_page(p-&gt;addr + size))</span>
<span class="quote">&gt;  			pr_err(&quot;Unable to set page fault.\n&quot;);</span>
<span class="quote">&gt; -		size += PAGE_SIZE;</span>
<span class="quote">&gt; +		size += page_level_size(l);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	spin_unlock_irqrestore(&amp;kmmio_lock, flags);</span>
<span class="quote">&gt; @@ -506,11 +530,18 @@ void unregister_kmmio_probe(struct kmmio_probe *p)</span>
<span class="quote">&gt;  	const unsigned long size_lim = p-&gt;len + (p-&gt;addr &amp; ~PAGE_MASK);</span>
<span class="quote">&gt;  	struct kmmio_fault_page *release_list = NULL;</span>
<span class="quote">&gt;  	struct kmmio_delayed_release *drelease;</span>
<span class="quote">&gt; +	unsigned int l;</span>
<span class="quote">&gt; +	pte_t *pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pte = lookup_address(p-&gt;addr, &amp;l);</span>
<span class="quote">&gt; +	if (!pte) {</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock_irqsave(&amp;kmmio_lock, flags);</span>
<span class="quote">&gt;  	while (size &lt; size_lim) {</span>
<span class="quote">&gt;  		release_kmmio_fault_page(p-&gt;addr + size, &amp;release_list);</span>
<span class="quote">&gt; -		size += PAGE_SIZE;</span>
<span class="quote">&gt; +		size += page_level_size(l);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	list_del_rcu(&amp;p-&gt;list);</span>
<span class="quote">&gt;  	kmmio_count--;</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.7.2</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; _______________________________________________</span>
<span class="quote">&gt; Nouveau mailing list</span>
<span class="quote">&gt; Nouveau@lists.freedesktop.org</span>
<span class="quote">&gt; https://lists.freedesktop.org/mailman/listinfo/nouveau</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/mm/kmmio.c b/arch/x86/mm/kmmio.c</span>
<span class="p_header">index 637ab34..d203beb 100644</span>
<span class="p_header">--- a/arch/x86/mm/kmmio.c</span>
<span class="p_header">+++ b/arch/x86/mm/kmmio.c</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"></span>
 struct kmmio_fault_page {
 	struct list_head list;
 	struct kmmio_fault_page *release_next;
<span class="p_del">-	unsigned long page; /* location of the fault page */</span>
<span class="p_add">+	unsigned long addr; /* the requested address */</span>
 	pteval_t old_presence; /* page presence prior to arming */
 	bool armed;
 
<span class="p_chunk">@@ -70,9 +70,16 @@</span> <span class="p_context"> unsigned int kmmio_count;</span>
 static struct list_head kmmio_page_table[KMMIO_PAGE_TABLE_SIZE];
 static LIST_HEAD(kmmio_probes);
 
<span class="p_del">-static struct list_head *kmmio_page_list(unsigned long page)</span>
<span class="p_add">+static struct list_head *kmmio_page_list(unsigned long addr)</span>
 {
<span class="p_del">-	return &amp;kmmio_page_table[hash_long(page, KMMIO_PAGE_HASH_BITS)];</span>
<span class="p_add">+	unsigned int l;</span>
<span class="p_add">+	pte_t *pte = lookup_address(addr, &amp;l);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pte)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	addr &amp;= page_level_mask(l);</span>
<span class="p_add">+</span>
<span class="p_add">+	return &amp;kmmio_page_table[hash_long(addr, KMMIO_PAGE_HASH_BITS)];</span>
 }
 
 /* Accessed per-cpu */
<span class="p_chunk">@@ -98,15 +105,19 @@</span> <span class="p_context"> static struct kmmio_probe *get_kmmio_probe(unsigned long addr)</span>
 }
 
 /* You must be holding RCU read lock. */
<span class="p_del">-static struct kmmio_fault_page *get_kmmio_fault_page(unsigned long page)</span>
<span class="p_add">+static struct kmmio_fault_page *get_kmmio_fault_page(unsigned long addr)</span>
 {
 	struct list_head *head;
 	struct kmmio_fault_page *f;
<span class="p_add">+	unsigned int l;</span>
<span class="p_add">+	pte_t *pte = lookup_address(addr, &amp;l);</span>
 
<span class="p_del">-	page &amp;= PAGE_MASK;</span>
<span class="p_del">-	head = kmmio_page_list(page);</span>
<span class="p_add">+	if (!pte)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	addr &amp;= page_level_mask(l);</span>
<span class="p_add">+	head = kmmio_page_list(addr);</span>
 	list_for_each_entry_rcu(f, head, list) {
<span class="p_del">-		if (f-&gt;page == page)</span>
<span class="p_add">+		if (f-&gt;addr == addr)</span>
 			return f;
 	}
 	return NULL;
<span class="p_chunk">@@ -137,10 +148,10 @@</span> <span class="p_context"> static void clear_pte_presence(pte_t *pte, bool clear, pteval_t *old)</span>
 static int clear_page_presence(struct kmmio_fault_page *f, bool clear)
 {
 	unsigned int level;
<span class="p_del">-	pte_t *pte = lookup_address(f-&gt;page, &amp;level);</span>
<span class="p_add">+	pte_t *pte = lookup_address(f-&gt;addr, &amp;level);</span>
 
 	if (!pte) {
<span class="p_del">-		pr_err(&quot;no pte for page 0x%08lx\n&quot;, f-&gt;page);</span>
<span class="p_add">+		pr_err(&quot;no pte for addr 0x%08lx\n&quot;, f-&gt;addr);</span>
 		return -1;
 	}
 
<span class="p_chunk">@@ -156,7 +167,7 @@</span> <span class="p_context"> static int clear_page_presence(struct kmmio_fault_page *f, bool clear)</span>
 		return -1;
 	}
 
<span class="p_del">-	__flush_tlb_one(f-&gt;page);</span>
<span class="p_add">+	__flush_tlb_one(f-&gt;addr);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -176,12 +187,12 @@</span> <span class="p_context"> static int arm_kmmio_fault_page(struct kmmio_fault_page *f)</span>
 	int ret;
 	WARN_ONCE(f-&gt;armed, KERN_ERR pr_fmt(&quot;kmmio page already armed.\n&quot;));
 	if (f-&gt;armed) {
<span class="p_del">-		pr_warning(&quot;double-arm: page 0x%08lx, ref %d, old %d\n&quot;,</span>
<span class="p_del">-			   f-&gt;page, f-&gt;count, !!f-&gt;old_presence);</span>
<span class="p_add">+		pr_warning(&quot;double-arm: addr 0x%08lx, ref %d, old %d\n&quot;,</span>
<span class="p_add">+			   f-&gt;addr, f-&gt;count, !!f-&gt;old_presence);</span>
 	}
 	ret = clear_page_presence(f, true);
<span class="p_del">-	WARN_ONCE(ret &lt; 0, KERN_ERR pr_fmt(&quot;arming 0x%08lx failed.\n&quot;),</span>
<span class="p_del">-		  f-&gt;page);</span>
<span class="p_add">+	WARN_ONCE(ret &lt; 0, KERN_ERR pr_fmt(&quot;arming at 0x%08lx failed.\n&quot;),</span>
<span class="p_add">+		  f-&gt;addr);</span>
 	f-&gt;armed = true;
 	return ret;
 }
<span class="p_chunk">@@ -191,7 +202,7 @@</span> <span class="p_context"> static void disarm_kmmio_fault_page(struct kmmio_fault_page *f)</span>
 {
 	int ret = clear_page_presence(f, false);
 	WARN_ONCE(ret &lt; 0,
<span class="p_del">-			KERN_ERR &quot;kmmio disarming 0x%08lx failed.\n&quot;, f-&gt;page);</span>
<span class="p_add">+			KERN_ERR &quot;kmmio disarming at 0x%08lx failed.\n&quot;, f-&gt;addr);</span>
 	f-&gt;armed = false;
 }
 
<span class="p_chunk">@@ -215,6 +226,12 @@</span> <span class="p_context"> int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
 	struct kmmio_context *ctx;
 	struct kmmio_fault_page *faultpage;
 	int ret = 0; /* default to fault not handled */
<span class="p_add">+	unsigned long page_base = addr;</span>
<span class="p_add">+	unsigned int l;</span>
<span class="p_add">+	pte_t *pte = lookup_address(addr, &amp;l);</span>
<span class="p_add">+	if (!pte)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	page_base &amp;= page_level_mask(l);</span>
 
 	/*
 	 * Preemption is now disabled to prevent process switch during
<span class="p_chunk">@@ -227,7 +244,7 @@</span> <span class="p_context"> int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
 	preempt_disable();
 	rcu_read_lock();
 
<span class="p_del">-	faultpage = get_kmmio_fault_page(addr);</span>
<span class="p_add">+	faultpage = get_kmmio_fault_page(page_base);</span>
 	if (!faultpage) {
 		/*
 		 * Either this page fault is not caused by kmmio, or
<span class="p_chunk">@@ -239,7 +256,7 @@</span> <span class="p_context"> int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
 
 	ctx = &amp;get_cpu_var(kmmio_ctx);
 	if (ctx-&gt;active) {
<span class="p_del">-		if (addr == ctx-&gt;addr) {</span>
<span class="p_add">+		if (page_base == ctx-&gt;addr) {</span>
 			/*
 			 * A second fault on the same page means some other
 			 * condition needs handling by do_page_fault(), the
<span class="p_chunk">@@ -267,9 +284,9 @@</span> <span class="p_context"> int kmmio_handler(struct pt_regs *regs, unsigned long addr)</span>
 	ctx-&gt;active++;
 
 	ctx-&gt;fpage = faultpage;
<span class="p_del">-	ctx-&gt;probe = get_kmmio_probe(addr);</span>
<span class="p_add">+	ctx-&gt;probe = get_kmmio_probe(page_base);</span>
 	ctx-&gt;saved_flags = (regs-&gt;flags &amp; (X86_EFLAGS_TF | X86_EFLAGS_IF));
<span class="p_del">-	ctx-&gt;addr = addr;</span>
<span class="p_add">+	ctx-&gt;addr = page_base;</span>
 
 	if (ctx-&gt;probe &amp;&amp; ctx-&gt;probe-&gt;pre_handler)
 		ctx-&gt;probe-&gt;pre_handler(ctx-&gt;probe, regs, addr);
<span class="p_chunk">@@ -354,12 +371,11 @@</span> <span class="p_context"> out:</span>
 }
 
 /* You must be holding kmmio_lock. */
<span class="p_del">-static int add_kmmio_fault_page(unsigned long page)</span>
<span class="p_add">+static int add_kmmio_fault_page(unsigned long addr)</span>
 {
 	struct kmmio_fault_page *f;
 
<span class="p_del">-	page &amp;= PAGE_MASK;</span>
<span class="p_del">-	f = get_kmmio_fault_page(page);</span>
<span class="p_add">+	f = get_kmmio_fault_page(addr);</span>
 	if (f) {
 		if (!f-&gt;count)
 			arm_kmmio_fault_page(f);
<span class="p_chunk">@@ -372,26 +388,25 @@</span> <span class="p_context"> static int add_kmmio_fault_page(unsigned long page)</span>
 		return -1;
 
 	f-&gt;count = 1;
<span class="p_del">-	f-&gt;page = page;</span>
<span class="p_add">+	f-&gt;addr = addr;</span>
 
 	if (arm_kmmio_fault_page(f)) {
 		kfree(f);
 		return -1;
 	}
 
<span class="p_del">-	list_add_rcu(&amp;f-&gt;list, kmmio_page_list(f-&gt;page));</span>
<span class="p_add">+	list_add_rcu(&amp;f-&gt;list, kmmio_page_list(f-&gt;addr));</span>
 
 	return 0;
 }
 
 /* You must be holding kmmio_lock. */
<span class="p_del">-static void release_kmmio_fault_page(unsigned long page,</span>
<span class="p_add">+static void release_kmmio_fault_page(unsigned long addr,</span>
 				struct kmmio_fault_page **release_list)
 {
 	struct kmmio_fault_page *f;
 
<span class="p_del">-	page &amp;= PAGE_MASK;</span>
<span class="p_del">-	f = get_kmmio_fault_page(page);</span>
<span class="p_add">+	f = get_kmmio_fault_page(addr);</span>
 	if (!f)
 		return;
 
<span class="p_chunk">@@ -420,18 +435,27 @@</span> <span class="p_context"> int register_kmmio_probe(struct kmmio_probe *p)</span>
 	int ret = 0;
 	unsigned long size = 0;
 	const unsigned long size_lim = p-&gt;len + (p-&gt;addr &amp; ~PAGE_MASK);
<span class="p_add">+	unsigned int l;</span>
<span class="p_add">+	pte_t *pte;</span>
 
 	spin_lock_irqsave(&amp;kmmio_lock, flags);
 	if (get_kmmio_probe(p-&gt;addr)) {
 		ret = -EEXIST;
 		goto out;
 	}
<span class="p_add">+</span>
<span class="p_add">+	pte = lookup_address(p-&gt;addr, &amp;l);</span>
<span class="p_add">+	if (!pte) {</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	kmmio_count++;
 	list_add_rcu(&amp;p-&gt;list, &amp;kmmio_probes);
 	while (size &lt; size_lim) {
 		if (add_kmmio_fault_page(p-&gt;addr + size))
 			pr_err(&quot;Unable to set page fault.\n&quot;);
<span class="p_del">-		size += PAGE_SIZE;</span>
<span class="p_add">+		size += page_level_size(l);</span>
 	}
 out:
 	spin_unlock_irqrestore(&amp;kmmio_lock, flags);
<span class="p_chunk">@@ -506,11 +530,18 @@</span> <span class="p_context"> void unregister_kmmio_probe(struct kmmio_probe *p)</span>
 	const unsigned long size_lim = p-&gt;len + (p-&gt;addr &amp; ~PAGE_MASK);
 	struct kmmio_fault_page *release_list = NULL;
 	struct kmmio_delayed_release *drelease;
<span class="p_add">+	unsigned int l;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = lookup_address(p-&gt;addr, &amp;l);</span>
<span class="p_add">+	if (!pte) {</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
 
 	spin_lock_irqsave(&amp;kmmio_lock, flags);
 	while (size &lt; size_lim) {
 		release_kmmio_fault_page(p-&gt;addr + size, &amp;release_list);
<span class="p_del">-		size += PAGE_SIZE;</span>
<span class="p_add">+		size += page_level_size(l);</span>
 	}
 	list_del_rcu(&amp;p-&gt;list);
 	kmmio_count--;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



