
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,1/4] mm, hugetlb: unclutter hugetlb allocation layers - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,1/4] mm, hugetlb: unclutter hugetlb allocation layers</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 13, 2017, 9 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170613090039.14393-2-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9783375/mbox/"
   >mbox</a>
|
   <a href="/patch/9783375/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9783375/">/patch/9783375/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	76E0060325 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 13 Jun 2017 09:01:47 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6AEAE274D1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 13 Jun 2017 09:01:47 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5FB0627F92; Tue, 13 Jun 2017 09:01:47 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 78FF0274D1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 13 Jun 2017 09:01:46 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752706AbdFMJBn (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 13 Jun 2017 05:01:43 -0400
Received: from mail-wr0-f193.google.com ([209.85.128.193]:35294 &quot;EHLO
	mail-wr0-f193.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751884AbdFMJAv (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 13 Jun 2017 05:00:51 -0400
Received: by mail-wr0-f193.google.com with SMTP id g76so27856171wrd.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 13 Jun 2017 02:00:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=1mRw1yR6HFdMOVFnbKovuDw6UUdGHlB01+uYMbU804w=;
	b=tREvH35RJrRnPTc3PRSSMhNS906smUPyfvgRFornz7M50Wu2hC1ss7KbfXbIJYx5wj
	J5EkKWO1/18nzoIHZN6v1I190VIaCjxuVdxOCm6HMAitzpILkiB2Pz7Pp6oM3vA2/jCs
	WFaLbYzwowMaQOK0HkA+bTHAVzKAcDzmfbkvIh6J3fVIEaSdaJDbmkNhkdZtGP/eyrFI
	UfcVxet95F37C5JwnVLy0Dqu1IVOCPIQtUTZTqWA9893jDiGGwXp3DHPsXRsM5IS/1NZ
	V7cs4udR5vXicDwRX89QY5WXel9EAQLa8sFIdF8/pDZYy84XyKL7FfURblhrqlbVEIar
	e7QA==
X-Gm-Message-State: AKS2vOzKNqTUpth2YvTVExBuOl71QE3OJ1Abh2hbk+Abripc661e0/du
	AnasCrdAE1Sv4Q==
X-Received: by 10.223.144.69 with SMTP id h63mr1860012wrh.187.1497344450052; 
	Tue, 13 Jun 2017 02:00:50 -0700 (PDT)
Received: from tiehlicka.suse.cz (prg-ext-pat.suse.com. [213.151.95.130])
	by smtp.gmail.com with ESMTPSA id
	c11sm6041078wrb.58.2017.06.13.02.00.49
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 13 Jun 2017 02:00:49 -0700 (PDT)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: &lt;linux-mm@kvack.org&gt;
Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [RFC PATCH 1/4] mm, hugetlb: unclutter hugetlb allocation layers
Date: Tue, 13 Jun 2017 11:00:36 +0200
Message-Id: &lt;20170613090039.14393-2-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170613090039.14393-1-mhocko@kernel.org&gt;
References: &lt;20170613090039.14393-1-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 13, 2017, 9 a.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

Hugetlb allocation path for fresh huge pages is unnecessarily complex
and it mixes different interfaces between layers. __alloc_buddy_huge_page
is the central place to perform a new allocation. It checks for the
hugetlb overcommit and then relies on __hugetlb_alloc_buddy_huge_page to
invoke the page allocator. This is all good except that
__alloc_buddy_huge_page pushes vma and address down the callchain and
so __hugetlb_alloc_buddy_huge_page has to deal with two different
allocation modes - one for memory policy and other node specific (or to
make it more obscure node non-specific) requests. This just screams for a
reorganization.

This patch pulls out all the vma specific handling up to
__alloc_buddy_huge_page_with_mpol where it belongs.
__alloc_buddy_huge_page will get nodemask argument and
__hugetlb_alloc_buddy_huge_page will become a trivial wrapper over the
page allocator.

In short:
__alloc_buddy_huge_page_with_mpol - memory policy handling
  __alloc_buddy_huge_page - overcommit handling and accounting
    __hugetlb_alloc_buddy_huge_page - page allocator layer

Also note that __hugetlb_alloc_buddy_huge_page and its cpuset retry loop
is not really needed because the page allocator already handles the
cpusets update.

Finally __hugetlb_alloc_buddy_huge_page had a special case for node
specific allocations (when no policy is applied and there is a node
given). This has relied on __GFP_THISNODE to not fallback to a different
node. alloc_huge_page_node is the only caller which relies on this
behavior. Keep it for now and emulate it by a proper nodemask.

Not only this removes quite some code it also should make those layers
easier to follow and clear wrt responsibilities.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 include/linux/hugetlb.h |   2 +-
 mm/hugetlb.c            | 134 +++++++++++-------------------------------------
 2 files changed, 31 insertions(+), 105 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - June 14, 2017, 1:18 p.m.</div>
<pre class="content">
On 06/13/2017 11:00 AM, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hugetlb allocation path for fresh huge pages is unnecessarily complex</span>
<span class="quote">&gt; and it mixes different interfaces between layers. __alloc_buddy_huge_page</span>
<span class="quote">&gt; is the central place to perform a new allocation. It checks for the</span>
<span class="quote">&gt; hugetlb overcommit and then relies on __hugetlb_alloc_buddy_huge_page to</span>
<span class="quote">&gt; invoke the page allocator. This is all good except that</span>
<span class="quote">&gt; __alloc_buddy_huge_page pushes vma and address down the callchain and</span>
<span class="quote">&gt; so __hugetlb_alloc_buddy_huge_page has to deal with two different</span>
<span class="quote">&gt; allocation modes - one for memory policy and other node specific (or to</span>
<span class="quote">&gt; make it more obscure node non-specific) requests. This just screams for a</span>
<span class="quote">&gt; reorganization.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch pulls out all the vma specific handling up to</span>
<span class="quote">&gt; __alloc_buddy_huge_page_with_mpol where it belongs.</span>
<span class="quote">&gt; __alloc_buddy_huge_page will get nodemask argument and</span>
<span class="quote">&gt; __hugetlb_alloc_buddy_huge_page will become a trivial wrapper over the</span>
<span class="quote">&gt; page allocator.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In short:</span>
<span class="quote">&gt; __alloc_buddy_huge_page_with_mpol - memory policy handling</span>
<span class="quote">&gt;   __alloc_buddy_huge_page - overcommit handling and accounting</span>
<span class="quote">&gt;     __hugetlb_alloc_buddy_huge_page - page allocator layer</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also note that __hugetlb_alloc_buddy_huge_page and its cpuset retry loop</span>
<span class="quote">&gt; is not really needed because the page allocator already handles the</span>
<span class="quote">&gt; cpusets update.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Finally __hugetlb_alloc_buddy_huge_page had a special case for node</span>
<span class="quote">&gt; specific allocations (when no policy is applied and there is a node</span>
<span class="quote">&gt; given). This has relied on __GFP_THISNODE to not fallback to a different</span>
<span class="quote">&gt; node. alloc_huge_page_node is the only caller which relies on this</span>
<span class="quote">&gt; behavior. Keep it for now and emulate it by a proper nodemask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Not only this removes quite some code it also should make those layers</span>
<span class="quote">&gt; easier to follow and clear wrt responsibilities.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/hugetlb.h |   2 +-</span>
<span class="quote">&gt;  mm/hugetlb.c            | 134 +++++++++++-------------------------------------</span>
<span class="quote">&gt;  2 files changed, 31 insertions(+), 105 deletions(-)</span>

Very nice cleanup indeed!
<span class="quote">
&gt; @@ -1717,13 +1640,22 @@ struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  		page = dequeue_huge_page_node(h, nid);</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (!page)</span>
<span class="quote">&gt; -		page = __alloc_buddy_huge_page_no_mpol(h, nid);</span>
<span class="quote">&gt; +	if (!page) {</span>
<span class="quote">&gt; +		nodemask_t nmask;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (nid != NUMA_NO_NODE) {</span>
<span class="quote">&gt; +			nmask = NODE_MASK_NONE;</span>
<span class="quote">&gt; +			node_set(nid, nmask);</span>

TBH I don&#39;t like this hack too much, and would rather see __GFP_THISNODE
involved, which picks a different (short) zonelist. Also it&#39;s allocating
nodemask on stack, which we generally avoid? Although the callers
currently seem to be shallow.
<span class="quote">
&gt; +		} else {</span>
<span class="quote">&gt; +			nmask = node_states[N_MEMORY];</span>

If nothing, this case could pass NULL? Although that would lead to
uglier code too...
<span class="quote">
&gt; +		}</span>
<span class="quote">&gt; +		page = __alloc_buddy_huge_page(h, nid, &amp;nmask);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="quote">&gt; +struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt;  	int node;</span>
<span class="quote">&gt; @@ -1741,13 +1673,7 @@ struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="quote">&gt;  		return page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* No reservations, try to overcommit */</span>
<span class="quote">&gt; -	for_each_node_mask(node, *nmask) {</span>
<span class="quote">&gt; -		page = __alloc_buddy_huge_page_no_mpol(h, node);</span>
<span class="quote">&gt; -		if (page)</span>
<span class="quote">&gt; -			return page;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return NULL;</span>
<span class="quote">&gt; +	return __alloc_buddy_huge_page(h, NUMA_NO_NODE, nmask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -1775,7 +1701,7 @@ static int gather_surplus_pages(struct hstate *h, int delta)</span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  	for (i = 0; i &lt; needed; i++) {</span>
<span class="quote">&gt; -		page = __alloc_buddy_huge_page_no_mpol(h, NUMA_NO_NODE);</span>
<span class="quote">&gt; +		page = __alloc_buddy_huge_page(h, NUMA_NO_NODE, NULL);</span>
<span class="quote">&gt;  		if (!page) {</span>
<span class="quote">&gt;  			alloc_ok = false;</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index c469191bb13b..016831fcdca1 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -349,7 +349,7 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask);</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask);</span>
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 01c11ceb47d6..3d5f25d589b3 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1531,82 +1531,18 @@</span> <span class="p_context"> int dissolve_free_huge_pages(unsigned long start_pfn, unsigned long end_pfn)</span>
 	return rc;
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * There are 3 ways this can get called:</span>
<span class="p_del">- * 1. With vma+addr: we use the VMA&#39;s memory policy</span>
<span class="p_del">- * 2. With !vma, but nid=NUMA_NO_NODE:  We try to allocate a huge</span>
<span class="p_del">- *    page from any node, and let the buddy allocator itself figure</span>
<span class="p_del">- *    it out.</span>
<span class="p_del">- * 3. With !vma, but nid!=NUMA_NO_NODE.  We allocate a huge page</span>
<span class="p_del">- *    strictly from &#39;nid&#39;</span>
<span class="p_del">- */</span>
 static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,
<span class="p_del">-		struct vm_area_struct *vma, unsigned long addr, int nid)</span>
<span class="p_add">+		int nid, nodemask_t *nmask)</span>
 {
 	int order = huge_page_order(h);
 	gfp_t gfp = htlb_alloc_mask(h)|__GFP_COMP|__GFP_REPEAT|__GFP_NOWARN;
<span class="p_del">-	unsigned int cpuset_mems_cookie;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We need a VMA to get a memory policy.  If we do not</span>
<span class="p_del">-	 * have one, we use the &#39;nid&#39; argument.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * The mempolicy stuff below has some non-inlined bits</span>
<span class="p_del">-	 * and calls -&gt;vm_ops.  That makes it hard to optimize at</span>
<span class="p_del">-	 * compile-time, even when NUMA is off and it does</span>
<span class="p_del">-	 * nothing.  This helps the compiler optimize it out.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!IS_ENABLED(CONFIG_NUMA) || !vma) {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * If a specific node is requested, make sure to</span>
<span class="p_del">-		 * get memory from there, but only when a node</span>
<span class="p_del">-		 * is explicitly specified.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (nid != NUMA_NO_NODE)</span>
<span class="p_del">-			gfp |= __GFP_THISNODE;</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Make sure to call something that can handle</span>
<span class="p_del">-		 * nid=NUMA_NO_NODE</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		return alloc_pages_node(nid, gfp, order);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * OK, so we have a VMA.  Fetch the mempolicy and try to</span>
<span class="p_del">-	 * allocate a huge page with it.  We will only reach this</span>
<span class="p_del">-	 * when CONFIG_NUMA=y.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		struct page *page;</span>
<span class="p_del">-		struct mempolicy *mpol;</span>
<span class="p_del">-		int nid;</span>
<span class="p_del">-		nodemask_t *nodemask;</span>
<span class="p_del">-</span>
<span class="p_del">-		cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="p_del">-		nid = huge_node(vma, addr, gfp, &amp;mpol, &amp;nodemask);</span>
<span class="p_del">-		mpol_cond_put(mpol);</span>
<span class="p_del">-		page = __alloc_pages_nodemask(gfp, order, nid, nodemask);</span>
<span class="p_del">-		if (page)</span>
<span class="p_del">-			return page;</span>
<span class="p_del">-	} while (read_mems_allowed_retry(cpuset_mems_cookie));</span>
 
<span class="p_del">-	return NULL;</span>
<span class="p_add">+	if (nid == NUMA_NO_NODE)</span>
<span class="p_add">+		nid = numa_mem_id();</span>
<span class="p_add">+	return __alloc_pages_nodemask(gfp, order, nid, nmask);</span>
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * There are two ways to allocate a huge page:</span>
<span class="p_del">- * 1. When you have a VMA and an address (like a fault)</span>
<span class="p_del">- * 2. When you have no VMA (like when setting /proc/.../nr_hugepages)</span>
<span class="p_del">- *</span>
<span class="p_del">- * &#39;vma&#39; and &#39;addr&#39; are only for (1).  &#39;nid&#39; is always NUMA_NO_NODE in</span>
<span class="p_del">- * this case which signifies that the allocation should be done with</span>
<span class="p_del">- * respect for the VMA&#39;s memory policy.</span>
<span class="p_del">- *</span>
<span class="p_del">- * For (2), we ignore &#39;vma&#39; and &#39;addr&#39; and use &#39;nid&#39; exclusively. This</span>
<span class="p_del">- * implies that memory policies will not be taken in to account.</span>
<span class="p_del">- */</span>
<span class="p_del">-static struct page *__alloc_buddy_huge_page(struct hstate *h,</span>
<span class="p_del">-		struct vm_area_struct *vma, unsigned long addr, int nid)</span>
<span class="p_add">+static struct page *__alloc_buddy_huge_page(struct hstate *h, int nid, nodemask_t *nmask)</span>
 {
 	struct page *page;
 	unsigned int r_nid;
<span class="p_chunk">@@ -1615,15 +1551,6 @@</span> <span class="p_context"> static struct page *__alloc_buddy_huge_page(struct hstate *h,</span>
 		return NULL;
 
 	/*
<span class="p_del">-	 * Make sure that anyone specifying &#39;nid&#39; is not also specifying a VMA.</span>
<span class="p_del">-	 * This makes sure the caller is picking _one_ of the modes with which</span>
<span class="p_del">-	 * we can call this function, not both.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (vma || (addr != -1)) {</span>
<span class="p_del">-		VM_WARN_ON_ONCE(addr == -1);</span>
<span class="p_del">-		VM_WARN_ON_ONCE(nid != NUMA_NO_NODE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	/*</span>
 	 * Assume we will successfully allocate the surplus page to
 	 * prevent racing processes from causing the surplus to exceed
 	 * overcommit
<span class="p_chunk">@@ -1656,7 +1583,7 @@</span> <span class="p_context"> static struct page *__alloc_buddy_huge_page(struct hstate *h,</span>
 	}
 	spin_unlock(&amp;hugetlb_lock);
 
<span class="p_del">-	page = __hugetlb_alloc_buddy_huge_page(h, vma, addr, nid);</span>
<span class="p_add">+	page = __hugetlb_alloc_buddy_huge_page(h, nid, nmask);</span>
 
 	spin_lock(&amp;hugetlb_lock);
 	if (page) {
<span class="p_chunk">@@ -1681,26 +1608,22 @@</span> <span class="p_context"> static struct page *__alloc_buddy_huge_page(struct hstate *h,</span>
 }
 
 /*
<span class="p_del">- * Allocate a huge page from &#39;nid&#39;.  Note, &#39;nid&#39; may be</span>
<span class="p_del">- * NUMA_NO_NODE, which means that it may be allocated</span>
<span class="p_del">- * anywhere.</span>
<span class="p_del">- */</span>
<span class="p_del">-static</span>
<span class="p_del">-struct page *__alloc_buddy_huge_page_no_mpol(struct hstate *h, int nid)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long addr = -1;</span>
<span class="p_del">-</span>
<span class="p_del">-	return __alloc_buddy_huge_page(h, NULL, addr, nid);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * Use the VMA&#39;s mpolicy to allocate a huge page from the buddy.
  */
 static
 struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,
 		struct vm_area_struct *vma, unsigned long addr)
 {
<span class="p_del">-	return __alloc_buddy_huge_page(h, vma, addr, NUMA_NO_NODE);</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	struct mempolicy *mpol;</span>
<span class="p_add">+	int nid;</span>
<span class="p_add">+	nodemask_t *nodemask;</span>
<span class="p_add">+</span>
<span class="p_add">+	nid = huge_node(vma, addr, htlb_alloc_mask(h), &amp;mpol, &amp;nodemask);</span>
<span class="p_add">+	page = __alloc_buddy_huge_page(h, nid, nodemask);</span>
<span class="p_add">+	mpol_cond_put(mpol);</span>
<span class="p_add">+</span>
<span class="p_add">+	return page;</span>
 }
 
 /*
<span class="p_chunk">@@ -1717,13 +1640,22 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
 		page = dequeue_huge_page_node(h, nid);
 	spin_unlock(&amp;hugetlb_lock);
 
<span class="p_del">-	if (!page)</span>
<span class="p_del">-		page = __alloc_buddy_huge_page_no_mpol(h, nid);</span>
<span class="p_add">+	if (!page) {</span>
<span class="p_add">+		nodemask_t nmask;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (nid != NUMA_NO_NODE) {</span>
<span class="p_add">+			nmask = NODE_MASK_NONE;</span>
<span class="p_add">+			node_set(nid, nmask);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			nmask = node_states[N_MEMORY];</span>
<span class="p_add">+		}</span>
<span class="p_add">+		page = __alloc_buddy_huge_page(h, nid, &amp;nmask);</span>
<span class="p_add">+	}</span>
 
 	return page;
 }
 
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask)</span>
 {
 	struct page *page = NULL;
 	int node;
<span class="p_chunk">@@ -1741,13 +1673,7 @@</span> <span class="p_context"> struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
 		return page;
 
 	/* No reservations, try to overcommit */
<span class="p_del">-	for_each_node_mask(node, *nmask) {</span>
<span class="p_del">-		page = __alloc_buddy_huge_page_no_mpol(h, node);</span>
<span class="p_del">-		if (page)</span>
<span class="p_del">-			return page;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return NULL;</span>
<span class="p_add">+	return __alloc_buddy_huge_page(h, NUMA_NO_NODE, nmask);</span>
 }
 
 /*
<span class="p_chunk">@@ -1775,7 +1701,7 @@</span> <span class="p_context"> static int gather_surplus_pages(struct hstate *h, int delta)</span>
 retry:
 	spin_unlock(&amp;hugetlb_lock);
 	for (i = 0; i &lt; needed; i++) {
<span class="p_del">-		page = __alloc_buddy_huge_page_no_mpol(h, NUMA_NO_NODE);</span>
<span class="p_add">+		page = __alloc_buddy_huge_page(h, NUMA_NO_NODE, NULL);</span>
 		if (!page) {
 			alloc_ok = false;
 			break;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



