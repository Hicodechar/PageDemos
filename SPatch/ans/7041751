
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v5,2/2] mm: hugetlb: proc: add HugetlbPages field to /proc/PID/status - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v5,2/2] mm: hugetlb: proc: add HugetlbPages field to /proc/PID/status</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 20, 2015, 8:26 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1440059182-19798-3-git-send-email-n-horiguchi@ah.jp.nec.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7041751/mbox/"
   >mbox</a>
|
   <a href="/patch/7041751/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7041751/">/patch/7041751/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 954C1C05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Aug 2015 08:44:08 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id B1004204FF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Aug 2015 08:44:07 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id A006A204FB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 20 Aug 2015 08:44:06 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752177AbbHTIn5 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 20 Aug 2015 04:43:57 -0400
Received: from TYO201.gate.nec.co.jp ([210.143.35.51]:43603 &quot;EHLO
	tyo201.gate.nec.co.jp&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751819AbbHTInu (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 20 Aug 2015 04:43:50 -0400
Received: from mailgate3.nec.co.jp ([10.7.69.195])
	by tyo201.gate.nec.co.jp (8.13.8/8.13.4) with ESMTP id t7K8hbGl009004;
	Thu, 20 Aug 2015 17:43:37 +0900 (JST)
Received: from mailsv3.nec.co.jp (imss62.nec.co.jp [10.7.69.157]) by
	mailgate3.nec.co.jp (8.11.7/3.7W-MAILGATE-NEC) with ESMTP
	id t7K8hae13755; Thu, 20 Aug 2015 17:43:36 +0900 (JST)
Received: from mail01b.kamome.nec.co.jp (mail01b.kamome.nec.co.jp
	[10.25.43.2])
	by mailsv3.nec.co.jp (8.13.8/8.13.4) with ESMTP id t7K8haGH017194;
	Thu, 20 Aug 2015 17:43:36 +0900 (JST)
Received: from bpxc99gp.gisp.nec.co.jp ([10.38.151.147] [10.38.151.147]) by
	mail02.kamome.nec.co.jp with ESMTP id BT-MMP-1329793;
	Thu, 20 Aug 2015 17:26:28 +0900
Received: from BPXM23GP.gisp.nec.co.jp ([169.254.2.176]) by
	BPXC19GP.gisp.nec.co.jp ([10.38.151.147]) with mapi id 14.03.0224.002;
	Thu, 20 Aug 2015 17:26:27 +0900
From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
CC: David Rientjes &lt;rientjes@google.com&gt;,
	=?utf-8?B?SsO2cm4gRW5nZWw=?= &lt;joern@purestorage.com&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	&quot;linux-mm@kvack.org&quot; &lt;linux-mm@kvack.org&gt;,
	&quot;linux-kernel@vger.kernel.org&quot; &lt;linux-kernel@vger.kernel.org&gt;,
	Naoya Horiguchi &lt;nao.horiguchi@gmail.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;
Subject: [PATCH v5 2/2] mm: hugetlb: proc: add HugetlbPages field to
	/proc/PID/status
Thread-Topic: [PATCH v5 2/2] mm: hugetlb: proc: add HugetlbPages field to
	/proc/PID/status
Thread-Index: AQHQ2yHoV0QXX6xP10iLFfDQGG5gpA==
Date: Thu, 20 Aug 2015 08:26:27 +0000
Message-ID: &lt;1440059182-19798-3-git-send-email-n-horiguchi@ah.jp.nec.com&gt;
References: &lt;20150812000336.GB32192@hori1.linux.bs1.fc.nec.co.jp&gt;
	&lt;1440059182-19798-1-git-send-email-n-horiguchi@ah.jp.nec.com&gt;
In-Reply-To: &lt;1440059182-19798-1-git-send-email-n-horiguchi@ah.jp.nec.com&gt;
Accept-Language: ja-JP, en-US
Content-Language: ja-JP
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.128.101.21]
Content-Type: text/plain; charset=&quot;utf-8&quot;
Content-Transfer-Encoding: base64
MIME-Version: 1.0
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.5 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Aug. 20, 2015, 8:26 a.m.</div>
<pre class="content">
Currently there&#39;s no easy way to get per-process usage of hugetlb pages, which
is inconvenient because userspace applications which use hugetlb typically want
to control their processes on the basis of how much memory (including hugetlb)
they use. So this patch simply provides easy access to the info via
/proc/PID/status.

With this patch, for example, /proc/PID/status shows a line like this:

  HugetlbPages:      20480 kB (10*2048kB)

If your system supports and enables multiple hugepage sizes, the line looks
like this:

  HugetlbPages:    1069056 kB (1*1048576kB 10*2048kB)

, so you can easily know how many hugepages in which pagesize are used by a
process.
<span class="signed-off-by">
Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="acked-by">Acked-by: Joern Engel &lt;joern@logfs.org&gt;</span>
<span class="acked-by">Acked-by: David Rientjes &lt;rientjes@google.com&gt;</span>
---
v4 -&gt; v5:
- add (struct hugetlb_usage *) to struct mm_struct
- use %lu instead of %d for seq_printf()
- introduce hugetlb_fork

v3 -&gt; v4:
- rename field (VmHugetlbRSS is not the best name)
- introduce struct hugetlb_usage in struct mm_struct (no invasion to struct
  mm_rss_stat)
- introduce hugetlb_report_usage()
- merged documentation update

v2 -&gt; v3:
- use inline functions instead of macros for !CONFIG_HUGETLB_PAGE
---
 Documentation/filesystems/proc.txt |  3 +++
 fs/hugetlbfs/inode.c               | 12 ++++++++++
 fs/proc/task_mmu.c                 |  1 +
 include/linux/hugetlb.h            | 36 +++++++++++++++++++++++++++++
 include/linux/mm_types.h           |  7 ++++++
 kernel/fork.c                      |  3 +++
 mm/hugetlb.c                       | 46 ++++++++++++++++++++++++++++++++++++++
 mm/mmap.c                          |  1 +
 mm/rmap.c                          |  4 +++-
 9 files changed, 112 insertions(+), 1 deletion(-)

-- 
2.4.3
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 20, 2015, 11 a.m.</div>
<pre class="content">
On Thu 20-08-15 08:26:27, Naoya Horiguchi wrote:
<span class="quote">&gt; Currently there&#39;s no easy way to get per-process usage of hugetlb pages,</span>

Is this really the case after your previous patch? You have both 
HugetlbPages and KernelPageSize which should be sufficient no?

Reading a single file is, of course, easier but is it really worth the
additional code? I haven&#39;t really looked at the patch so I might be
missing something but what would be an advantage over reading
/proc/&lt;pid&gt;/smaps and extracting the information from there?

[...]
<span class="quote">&gt;  Documentation/filesystems/proc.txt |  3 +++</span>
<span class="quote">&gt;  fs/hugetlbfs/inode.c               | 12 ++++++++++</span>
<span class="quote">&gt;  fs/proc/task_mmu.c                 |  1 +</span>
<span class="quote">&gt;  include/linux/hugetlb.h            | 36 +++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  include/linux/mm_types.h           |  7 ++++++</span>
<span class="quote">&gt;  kernel/fork.c                      |  3 +++</span>
<span class="quote">&gt;  mm/hugetlb.c                       | 46 ++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  mm/mmap.c                          |  1 +</span>
<span class="quote">&gt;  mm/rmap.c                          |  4 +++-</span>
<span class="quote">&gt;  9 files changed, 112 insertions(+), 1 deletion(-)</span>
[...]
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - Aug. 20, 2015, 7:49 p.m.</div>
<pre class="content">
On Thu, 20 Aug 2015, Michal Hocko wrote:
<span class="quote">
&gt; On Thu 20-08-15 08:26:27, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; Currently there&#39;s no easy way to get per-process usage of hugetlb pages,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this really the case after your previous patch? You have both </span>
<span class="quote">&gt; HugetlbPages and KernelPageSize which should be sufficient no?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reading a single file is, of course, easier but is it really worth the</span>
<span class="quote">&gt; additional code? I haven&#39;t really looked at the patch so I might be</span>
<span class="quote">&gt; missing something but what would be an advantage over reading</span>
<span class="quote">&gt; /proc/&lt;pid&gt;/smaps and extracting the information from there?</span>
<span class="quote">&gt; </span>

/proc/pid/smaps requires root, /proc/pid/status doesn&#39;t.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - Aug. 20, 2015, 11:34 p.m.</div>
<pre class="content">
On Thu, Aug 20, 2015 at 01:00:05PM +0200, Michal Hocko wrote:
<span class="quote">&gt; On Thu 20-08-15 08:26:27, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; Currently there&#39;s no easy way to get per-process usage of hugetlb pages,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is this really the case after your previous patch? You have both </span>
<span class="quote">&gt; HugetlbPages and KernelPageSize which should be sufficient no?</span>

We can calcurate it from these info, so saying &quot;no easy way&quot; was incorrect :(
<span class="quote">
&gt; Reading a single file is, of course, easier but is it really worth the</span>
<span class="quote">&gt; additional code? I haven&#39;t really looked at the patch so I might be</span>
<span class="quote">&gt; missing something but what would be an advantage over reading</span>
<span class="quote">&gt; /proc/&lt;pid&gt;/smaps and extracting the information from there?</span>

My first idea was just &quot;users should feel it useful&quot;, but permission as David
commented sounds a good technical reason to me.

Thanks,
Naoya Horiguchi
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 21, 2015, 6:32 a.m.</div>
<pre class="content">
On Thu 20-08-15 12:49:59, David Rientjes wrote:
<span class="quote">&gt; On Thu, 20 Aug 2015, Michal Hocko wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Thu 20-08-15 08:26:27, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; &gt; Currently there&#39;s no easy way to get per-process usage of hugetlb pages,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Is this really the case after your previous patch? You have both </span>
<span class="quote">&gt; &gt; HugetlbPages and KernelPageSize which should be sufficient no?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Reading a single file is, of course, easier but is it really worth the</span>
<span class="quote">&gt; &gt; additional code? I haven&#39;t really looked at the patch so I might be</span>
<span class="quote">&gt; &gt; missing something but what would be an advantage over reading</span>
<span class="quote">&gt; &gt; /proc/&lt;pid&gt;/smaps and extracting the information from there?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; /proc/pid/smaps requires root, /proc/pid/status doesn&#39;t.</span>

Both mmotm and linus tree have
        REG(&quot;smaps&quot;,      S_IRUGO, proc_pid_smaps_operations),

and opening the file requires PTRACE_MODE_READ. So I do not see any
requirement for root here. Or did you mean that you need root to examine
all processes? That would be true but I am wondering why would be a regular
user interested in this break out numbers. Hugetlb management sounds
pretty much like an administrative or very specialized thing.

From my understanding of the discussion there is no usecase to have this
information world readable. Is this correct?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 21, 2015, 6:53 a.m.</div>
<pre class="content">
On Thu 20-08-15 23:34:51, Naoya Horiguchi wrote:
[...]
<span class="quote">&gt; &gt; Reading a single file is, of course, easier but is it really worth the</span>
<span class="quote">&gt; &gt; additional code? I haven&#39;t really looked at the patch so I might be</span>
<span class="quote">&gt; &gt; missing something but what would be an advantage over reading</span>
<span class="quote">&gt; &gt; /proc/&lt;pid&gt;/smaps and extracting the information from there?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; My first idea was just &quot;users should feel it useful&quot;, but permission as David</span>
<span class="quote">&gt; commented sounds a good technical reason to me.</span>

9 files changed, 112 insertions(+), 1 deletion(-)

is quite a lot especially when it touches hot paths like fork so it
better should have a good usecase. I have already asked in the other
email but is actually anybody requesting this? Nice to have is not
a good justification IMO.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137441">Jörn Engel</a> - Aug. 21, 2015, 4:30 p.m.</div>
<pre class="content">
On Fri, Aug 21, 2015 at 08:53:21AM +0200, Michal Hocko wrote:
<span class="quote">&gt; On Thu 20-08-15 23:34:51, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; Reading a single file is, of course, easier but is it really worth the</span>
<span class="quote">&gt; &gt; &gt; additional code? I haven&#39;t really looked at the patch so I might be</span>
<span class="quote">&gt; &gt; &gt; missing something but what would be an advantage over reading</span>
<span class="quote">&gt; &gt; &gt; /proc/&lt;pid&gt;/smaps and extracting the information from there?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; My first idea was just &quot;users should feel it useful&quot;, but permission as David</span>
<span class="quote">&gt; &gt; commented sounds a good technical reason to me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 9 files changed, 112 insertions(+), 1 deletion(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; is quite a lot especially when it touches hot paths like fork so it</span>
<span class="quote">&gt; better should have a good usecase. I have already asked in the other</span>
<span class="quote">&gt; email but is actually anybody requesting this? Nice to have is not</span>
<span class="quote">&gt; a good justification IMO.</span>

I need some way to judge the real rss of a process, including huge
pages.  No strong opinion on implementation details, but something is
clearly needed.

If you have processes with 99% huge pages, you are currently reduced to
guesswork.

Jörn

--
Journalism is printing what someone else does not want printed;
everything else is public relations.
-- George Orwell
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137441">Jörn Engel</a> - Aug. 21, 2015, 4:38 p.m.</div>
<pre class="content">
On Fri, Aug 21, 2015 at 08:32:33AM +0200, Michal Hocko wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; Both mmotm and linus tree have</span>
<span class="quote">&gt;         REG(&quot;smaps&quot;,      S_IRUGO, proc_pid_smaps_operations),</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; and opening the file requires PTRACE_MODE_READ. So I do not see any</span>
<span class="quote">&gt; requirement for root here. Or did you mean that you need root to examine</span>
<span class="quote">&gt; all processes? That would be true but I am wondering why would be a regular</span>
<span class="quote">&gt; user interested in this break out numbers. Hugetlb management sounds</span>
<span class="quote">&gt; pretty much like an administrative or very specialized thing.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; From my understanding of the discussion there is no usecase to have this</span>
<span class="quote">&gt; information world readable. Is this correct?</span>

Well, tools like top currently display rss.  Once we have some
interface, I would like a version of top that displays the true rss
including hugepages (hrss maybe?).

If we make such a tool impossible today, someone will complain about it
in the future and we created a new mess for ourselves.  I think it is
trouble enough to deal with the old one.

Jörn

--
Denying any reality for any laudable political goal is a bad strategy.
When the facts come out, the discovery of the facts will undermine the
laudable political goals.
-- Jared Diamond
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 24, 2015, 8:51 a.m.</div>
<pre class="content">
On Fri 21-08-15 09:30:33, Jörn Engel wrote:
<span class="quote">&gt; On Fri, Aug 21, 2015 at 08:53:21AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Thu 20-08-15 23:34:51, Naoya Horiguchi wrote:</span>
<span class="quote">&gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; Reading a single file is, of course, easier but is it really worth the</span>
<span class="quote">&gt; &gt; &gt; &gt; additional code? I haven&#39;t really looked at the patch so I might be</span>
<span class="quote">&gt; &gt; &gt; &gt; missing something but what would be an advantage over reading</span>
<span class="quote">&gt; &gt; &gt; &gt; /proc/&lt;pid&gt;/smaps and extracting the information from there?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; My first idea was just &quot;users should feel it useful&quot;, but permission as David</span>
<span class="quote">&gt; &gt; &gt; commented sounds a good technical reason to me.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 9 files changed, 112 insertions(+), 1 deletion(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; is quite a lot especially when it touches hot paths like fork so it</span>
<span class="quote">&gt; &gt; better should have a good usecase. I have already asked in the other</span>
<span class="quote">&gt; &gt; email but is actually anybody requesting this? Nice to have is not</span>
<span class="quote">&gt; &gt; a good justification IMO.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I need some way to judge the real rss of a process, including huge</span>
<span class="quote">&gt; pages.  No strong opinion on implementation details, but something is</span>
<span class="quote">&gt; clearly needed.</span>

The current implementation makes me worry. Is the per hstate break down
really needed? The implementation would be much more easier without it.
<span class="quote">
&gt; If you have processes with 99% huge pages, you are currently reduced to</span>
<span class="quote">&gt; guesswork.</span>

If you have 99% of hugetlb pages then your load is rather specific and I
would argue that /proc/&lt;pid&gt;/smaps (after patch 1) is a much better way to
get what you want.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - Aug. 25, 2015, 11:23 p.m.</div>
<pre class="content">
On Mon, 24 Aug 2015, Michal Hocko wrote:
<span class="quote">
&gt; The current implementation makes me worry. Is the per hstate break down</span>
<span class="quote">&gt; really needed? The implementation would be much more easier without it.</span>
<span class="quote">&gt; </span>

Yes, it&#39;s needed.  It provides a complete picture of what statically 
reserved hugepages are in use and we&#39;re not going to change the 
implementation when it is needed to differentiate between variable hugetlb 
page sizes that risk breaking existing userspace parsers.
<span class="quote">
&gt; If you have 99% of hugetlb pages then your load is rather specific and I</span>
<span class="quote">&gt; would argue that /proc/&lt;pid&gt;/smaps (after patch 1) is a much better way to</span>
<span class="quote">&gt; get what you want.</span>
<span class="quote">&gt; </span>

Some distributions change the permissions of smaps, as already stated, for 
pretty clear security reasons since it can be used to defeat existing 
protection.  There&#39;s no reason why hugetlb page usage should not be 
exported in the same manner and location as memory usage.

Sheesh.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 26, 2015, 6:38 a.m.</div>
<pre class="content">
On Tue 25-08-15 16:23:34, David Rientjes wrote:
<span class="quote">&gt; On Mon, 24 Aug 2015, Michal Hocko wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; The current implementation makes me worry. Is the per hstate break down</span>
<span class="quote">&gt; &gt; really needed? The implementation would be much more easier without it.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, it&#39;s needed.  It provides a complete picture of what statically </span>
<span class="quote">&gt; reserved hugepages are in use and we&#39;re not going to change the </span>
<span class="quote">&gt; implementation when it is needed to differentiate between variable hugetlb </span>
<span class="quote">&gt; page sizes that risk breaking existing userspace parsers.</span>

I thought the purpose was to give the amount of hugetlb based
resident memory. At least this is what Jörn was asking for AFAIU.
/proc/&lt;pid&gt;/status should be as lightweight as possible. The current
implementation is quite heavy as already pointed out. So I am really
curious whether this is _really_ needed. I haven&#39;t heard about a real
usecase except for top displaying HRss which doesn&#39;t need the break
down values. You have brought that up already
http://marc.info/?l=linux-mm&amp;m=143941143109335&amp;w=2 and nobody actually
asked for it. &quot;I do not mind having it&quot; is not an argument for inclusion
especially when the implementation is more costly and touches hot paths.
<span class="quote">
&gt; &gt; If you have 99% of hugetlb pages then your load is rather specific and I</span>
<span class="quote">&gt; &gt; would argue that /proc/&lt;pid&gt;/smaps (after patch 1) is a much better way to</span>
<span class="quote">&gt; &gt; get what you want.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Some distributions change the permissions of smaps, as already stated, for </span>
<span class="quote">&gt; pretty clear security reasons since it can be used to defeat existing </span>
<span class="quote">&gt; protection.  There&#39;s no reason why hugetlb page usage should not be </span>
<span class="quote">&gt; exported in the same manner and location as memory usage.</span>

/proc/&lt;pid&gt;/status provides only per-memory-type break down information
(locked, data, stack, etc...). Different hugetlb sizes are still a
hugetlb memory. So I am not sure I understand you argument here.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - Aug. 26, 2015, 10:02 p.m.</div>
<pre class="content">
On Wed, 26 Aug 2015, Michal Hocko wrote:
<span class="quote">
&gt; I thought the purpose was to give the amount of hugetlb based</span>
<span class="quote">&gt; resident memory.</span>

Persistent hugetlb memory is always resident, the goal is to show what is 
currently mapped.
<span class="quote">
&gt; At least this is what Jörn was asking for AFAIU.</span>
<span class="quote">&gt; /proc/&lt;pid&gt;/status should be as lightweight as possible. The current</span>
<span class="quote">&gt; implementation is quite heavy as already pointed out. So I am really</span>
<span class="quote">&gt; curious whether this is _really_ needed. I haven&#39;t heard about a real</span>
<span class="quote">&gt; usecase except for top displaying HRss which doesn&#39;t need the break</span>
<span class="quote">&gt; down values. You have brought that up already</span>
<span class="quote">&gt; http://marc.info/?l=linux-mm&amp;m=143941143109335&amp;w=2 and nobody actually</span>
<span class="quote">&gt; asked for it. &quot;I do not mind having it&quot; is not an argument for inclusion</span>
<span class="quote">&gt; especially when the implementation is more costly and touches hot paths.</span>
<span class="quote">&gt; </span>

It iterates over HUGE_MAX_HSTATE and reads atomic usage counters twice.  
On x86, HUGE_MAX_HSTATE == 2.  I don&#39;t consider that to be expensive.

If you are concerned about the memory allocation of struct hugetlb_usage, 
it could easily be embedded directly in struct mm_struct.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 27, 2015, 6:48 a.m.</div>
<pre class="content">
On Wed 26-08-15 15:02:49, David Rientjes wrote:
<span class="quote">&gt; On Wed, 26 Aug 2015, Michal Hocko wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; I thought the purpose was to give the amount of hugetlb based</span>
<span class="quote">&gt; &gt; resident memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Persistent hugetlb memory is always resident, the goal is to show what is </span>
<span class="quote">&gt; currently mapped.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; At least this is what Jörn was asking for AFAIU.</span>
<span class="quote">&gt; &gt; /proc/&lt;pid&gt;/status should be as lightweight as possible. The current</span>
<span class="quote">&gt; &gt; implementation is quite heavy as already pointed out. So I am really</span>
<span class="quote">&gt; &gt; curious whether this is _really_ needed. I haven&#39;t heard about a real</span>
<span class="quote">&gt; &gt; usecase except for top displaying HRss which doesn&#39;t need the break</span>
<span class="quote">&gt; &gt; down values. You have brought that up already</span>
<span class="quote">&gt; &gt; http://marc.info/?l=linux-mm&amp;m=143941143109335&amp;w=2 and nobody actually</span>
<span class="quote">&gt; &gt; asked for it. &quot;I do not mind having it&quot; is not an argument for inclusion</span>
<span class="quote">&gt; &gt; especially when the implementation is more costly and touches hot paths.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It iterates over HUGE_MAX_HSTATE and reads atomic usage counters twice.  </span>

I am not worried about /proc/&lt;pid&gt;/status read path. That one is indeed
trivial.
<span class="quote">
&gt; On x86, HUGE_MAX_HSTATE == 2.  I don&#39;t consider that to be expensive.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If you are concerned about the memory allocation of struct hugetlb_usage, </span>
<span class="quote">&gt; it could easily be embedded directly in struct mm_struct.</span>

Yes I am concerned about that and
9 files changed, 112 insertions(+), 1 deletion(-)
for something that is even not clear to be really required. And I still
haven&#39;t heard any strong usecase to justify it.

Can we go with the single and much simpler cumulative number first and
only add the break down list if it is _really_ required? We can even
document that the future version of /proc/&lt;pid&gt;/status might add an
additional information to prepare all the parsers to be more careful.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137441">Jörn Engel</a> - Aug. 27, 2015, 5:23 p.m.</div>
<pre class="content">
On Thu, Aug 27, 2015 at 08:48:18AM +0200, Michal Hocko wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On x86, HUGE_MAX_HSTATE == 2.  I don&#39;t consider that to be expensive.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If you are concerned about the memory allocation of struct hugetlb_usage, </span>
<span class="quote">&gt; &gt; it could easily be embedded directly in struct mm_struct.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes I am concerned about that and</span>
<span class="quote">&gt; 9 files changed, 112 insertions(+), 1 deletion(-)</span>
<span class="quote">&gt; for something that is even not clear to be really required. And I still</span>
<span class="quote">&gt; haven&#39;t heard any strong usecase to justify it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can we go with the single and much simpler cumulative number first and</span>
<span class="quote">&gt; only add the break down list if it is _really_ required? We can even</span>
<span class="quote">&gt; document that the future version of /proc/&lt;pid&gt;/status might add an</span>
<span class="quote">&gt; additional information to prepare all the parsers to be more careful.</span>

I don&#39;t care much which way we decide.  But I find your reasoning a bit
worrying.  If someone asks for a by-size breakup of hugepages in a few
years, you might have existing binaries that depend on the _absence_ of
those extra characters on the line.

Compare:
  HugetlbPages:      18432 kB
  HugetlbPages:    1069056 kB (1*1048576kB 10*2048kB)

Once someone has written a script that greps for &#39;HugetlbPages:.*kB$&#39;,
you have lost the option of adding anything else to the line.  You have
created yet another ABI compatibility headache today in order to save
112 lines of code.

That may be a worthwhile tradeoff, I don&#39;t know.  But at least I realize
there is a cost, while you seem to ignore that component.  There is
value in not painting yourself into a corner.

Jörn

--
A quarrel is quickly settled when deserted by one party; there is
no battle unless there be two.
-- Seneca
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=579">David Rientjes</a> - Aug. 27, 2015, 8:44 p.m.</div>
<pre class="content">
On Thu, 27 Aug 2015, Jörn Engel wrote:
<span class="quote">
&gt; On Thu, Aug 27, 2015 at 08:48:18AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; Can we go with the single and much simpler cumulative number first and</span>
<span class="quote">&gt; &gt; only add the break down list if it is _really_ required? We can even</span>
<span class="quote">&gt; &gt; document that the future version of /proc/&lt;pid&gt;/status might add an</span>
<span class="quote">&gt; &gt; additional information to prepare all the parsers to be more careful.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t care much which way we decide.  But I find your reasoning a bit</span>
<span class="quote">&gt; worrying.  If someone asks for a by-size breakup of hugepages in a few</span>
<span class="quote">&gt; years, you might have existing binaries that depend on the _absence_ of</span>
<span class="quote">&gt; those extra characters on the line.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Compare:</span>
<span class="quote">&gt;   HugetlbPages:      18432 kB</span>
<span class="quote">&gt;   HugetlbPages:    1069056 kB (1*1048576kB 10*2048kB)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Once someone has written a script that greps for &#39;HugetlbPages:.*kB$&#39;,</span>
<span class="quote">&gt; you have lost the option of adding anything else to the line.  You have</span>
<span class="quote">&gt; created yet another ABI compatibility headache today in order to save</span>
<span class="quote">&gt; 112 lines of code.</span>
<span class="quote">&gt; </span>

This is exactly the concern that I have brought up in this thread.  We 
have no other way to sanely export the breakdown in hugepage size without 
new fields being added later with the hstate size being embedded in the 
name itself.

I agree with the code as it stands in -mm today and I&#39;m thankful to Naoya 
that a long-term maintainable API has been established.  Respectfully, I 
have no idea why we are still talking about this and I&#39;m not going to be 
responding further unless something changes in -mm.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 31, 2015, 9:12 a.m.</div>
<pre class="content">
On Thu 27-08-15 10:23:51, Jörn Engel wrote:
<span class="quote">&gt; On Thu, Aug 27, 2015 at 08:48:18AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; On x86, HUGE_MAX_HSTATE == 2.  I don&#39;t consider that to be expensive.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; If you are concerned about the memory allocation of struct hugetlb_usage, </span>
<span class="quote">&gt; &gt; &gt; it could easily be embedded directly in struct mm_struct.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yes I am concerned about that and</span>
<span class="quote">&gt; &gt; 9 files changed, 112 insertions(+), 1 deletion(-)</span>
<span class="quote">&gt; &gt; for something that is even not clear to be really required. And I still</span>
<span class="quote">&gt; &gt; haven&#39;t heard any strong usecase to justify it.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Can we go with the single and much simpler cumulative number first and</span>
<span class="quote">&gt; &gt; only add the break down list if it is _really_ required? We can even</span>
<span class="quote">&gt; &gt; document that the future version of /proc/&lt;pid&gt;/status might add an</span>
<span class="quote">&gt; &gt; additional information to prepare all the parsers to be more careful.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t care much which way we decide.  But I find your reasoning a bit</span>
<span class="quote">&gt; worrying.  If someone asks for a by-size breakup of hugepages in a few</span>
<span class="quote">&gt; years, you might have existing binaries that depend on the _absence_ of</span>
<span class="quote">&gt; those extra characters on the line.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Compare:</span>
<span class="quote">&gt;   HugetlbPages:      18432 kB</span>
<span class="quote">&gt;   HugetlbPages:    1069056 kB (1*1048576kB 10*2048kB)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Once someone has written a script that greps for &#39;HugetlbPages:.*kB$&#39;,</span>
<span class="quote">&gt; you have lost the option of adding anything else to the line. </span>

If you think that an explicit note in the documentation is
not sufficient then I believe we can still handle it backward
compatible. Like separate entries for each existing hugetlb page:
HugetlbPages:	     1069056 kB
Hugetlb2MPages:	     20480 kB
Hugetlb1GPages:	     1048576 kB

or something similar. I would even argue this would be slightly easier
to parse. So it is not like we would be locked into anything.
<span class="quote">
&gt; You have</span>
<span class="quote">&gt; created yet another ABI compatibility headache today in order to save</span>
<span class="quote">&gt; 112 lines of code.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That may be a worthwhile tradeoff, I don&#39;t know.  But at least I realize</span>
<span class="quote">&gt; there is a cost, while you seem to ignore that component.  There is</span>
<span class="quote">&gt; value in not painting yourself into a corner.</span>

My primary point was that we are adding a code for a feature nobody
actually asked for just because somebody might ask for it in future.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git v4.2-rc4/Documentation/filesystems/proc.txt v4.2-rc4_patched/Documentation/filesystems/proc.txt</span>
<span class="p_header">index 22e40211ef64..f561fc46e41b 100644</span>
<span class="p_header">--- v4.2-rc4/Documentation/filesystems/proc.txt</span>
<span class="p_header">+++ v4.2-rc4_patched/Documentation/filesystems/proc.txt</span>
<span class="p_chunk">@@ -174,6 +174,7 @@</span> <span class="p_context"> For example, to get the status information of a process, all you have to do is</span>
   VmLib:      1412 kB
   VmPTE:        20 kb
   VmSwap:        0 kB
<span class="p_add">+  HugetlbPages:          0 kB (0*2048kB)</span>
   Threads:        1
   SigQ:   0/28578
   SigPnd: 0000000000000000
<span class="p_chunk">@@ -237,6 +238,8 @@</span> <span class="p_context"> Table 1-2: Contents of the status files (as of 4.1)</span>
  VmPTE                       size of page table entries
  VmPMD                       size of second level page tables
  VmSwap                      size of swap usage (the number of referred swapents)
<span class="p_add">+ HugetlbPages                size of hugetlb memory portions (with additional info</span>
<span class="p_add">+                             about number of mapped hugepages for each page size)</span>
  Threads                     number of threads
  SigQ                        number of signals queued/max. number for queue
  SigPnd                      bitmap of pending signals for the thread
<span class="p_header">diff --git v4.2-rc4/fs/hugetlbfs/inode.c v4.2-rc4_patched/fs/hugetlbfs/inode.c</span>
<span class="p_header">index 0cf74df68617..bf6ea2645d35 100644</span>
<span class="p_header">--- v4.2-rc4/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ v4.2-rc4_patched/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -115,6 +115,13 @@</span> <span class="p_context"> static int hugetlbfs_file_mmap(struct file *file, struct vm_area_struct *vma)</span>
 	if (vma-&gt;vm_pgoff &amp; (~huge_page_mask(h) &gt;&gt; PAGE_SHIFT))
 		return -EINVAL;
 
<span class="p_add">+	if (!vma-&gt;vm_mm-&gt;hugetlb_usage) {</span>
<span class="p_add">+		vma-&gt;vm_mm-&gt;hugetlb_usage = kzalloc(sizeof(struct hugetlb_usage),</span>
<span class="p_add">+							GFP_KERNEL);</span>
<span class="p_add">+		if (!vma-&gt;vm_mm-&gt;hugetlb_usage)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	vma_len = (loff_t)(vma-&gt;vm_end - vma-&gt;vm_start);
 
 	mutex_lock(&amp;inode-&gt;i_mutex);
<span class="p_chunk">@@ -138,6 +145,11 @@</span> <span class="p_context"> static int hugetlbfs_file_mmap(struct file *file, struct vm_area_struct *vma)</span>
 	return ret;
 }
 
<span class="p_add">+void exit_hugetlb_mmap(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	kfree(mm-&gt;hugetlb_usage);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Called under down_write(mmap_sem).
  */
<span class="p_header">diff --git v4.2-rc4/fs/proc/task_mmu.c v4.2-rc4_patched/fs/proc/task_mmu.c</span>
<span class="p_header">index 2c37938b82ee..b3cf7fa9ef6c 100644</span>
<span class="p_header">--- v4.2-rc4/fs/proc/task_mmu.c</span>
<span class="p_header">+++ v4.2-rc4_patched/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -69,6 +69,7 @@</span> <span class="p_context"> void task_mem(struct seq_file *m, struct mm_struct *mm)</span>
 		ptes &gt;&gt; 10,
 		pmds &gt;&gt; 10,
 		swap &lt;&lt; (PAGE_SHIFT-10));
<span class="p_add">+	hugetlb_report_usage(m, mm);</span>
 }
 
 unsigned long task_vsize(struct mm_struct *mm)
<span class="p_header">diff --git v4.2-rc4/include/linux/hugetlb.h v4.2-rc4_patched/include/linux/hugetlb.h</span>
<span class="p_header">index d891f949466a..db642ad0b847 100644</span>
<span class="p_header">--- v4.2-rc4/include/linux/hugetlb.h</span>
<span class="p_header">+++ v4.2-rc4_patched/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -469,6 +469,25 @@</span> <span class="p_context"> static inline spinlock_t *huge_pte_lockptr(struct hstate *h,</span>
 #define hugepages_supported() (HPAGE_SHIFT != 0)
 #endif
 
<span class="p_add">+struct hugetlb_usage {</span>
<span class="p_add">+	atomic_long_t count[HUGE_MAX_HSTATE];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+void hugetlb_report_usage(struct seq_file *m, struct mm_struct *mm);</span>
<span class="p_add">+void exit_hugetlb_mmap(struct mm_struct *mm);</span>
<span class="p_add">+int hugetlb_fork(struct mm_struct *new, struct mm_struct *old);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void inc_hugetlb_count(struct mm_struct *mm, struct hstate *h)</span>
<span class="p_add">+{</span>
<span class="p_add">+	VM_BUG_ON_MM(!mm-&gt;hugetlb_usage, mm);</span>
<span class="p_add">+	atomic_long_inc(&amp;mm-&gt;hugetlb_usage-&gt;count[hstate_index(h)]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void dec_hugetlb_count(struct mm_struct *mm, struct hstate *h)</span>
<span class="p_add">+{</span>
<span class="p_add">+	VM_BUG_ON_MM(!mm-&gt;hugetlb_usage, mm);</span>
<span class="p_add">+	atomic_long_dec(&amp;mm-&gt;hugetlb_usage-&gt;count[hstate_index(h)]);</span>
<span class="p_add">+}</span>
 #else	/* CONFIG_HUGETLB_PAGE */
 struct hstate {};
 #define alloc_huge_page_node(h, nid) NULL
<span class="p_chunk">@@ -504,6 +523,23 @@</span> <span class="p_context"> static inline spinlock_t *huge_pte_lockptr(struct hstate *h,</span>
 {
 	return &amp;mm-&gt;page_table_lock;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline void hugetlb_report_usage(struct seq_file *f, struct mm_struct *m)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void exit_hugetlb_mmap(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int hugetlb_fork(struct mm_struct *new, struct mm_struct *old)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void dec_hugetlb_count(struct mm_struct *mm, struct hstate *h)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
 #endif	/* CONFIG_HUGETLB_PAGE */
 
 static inline spinlock_t *huge_pte_lock(struct hstate *h,
<span class="p_header">diff --git v4.2-rc4/include/linux/mm_types.h v4.2-rc4_patched/include/linux/mm_types.h</span>
<span class="p_header">index 0038ac7466fd..851e964ee8d6 100644</span>
<span class="p_header">--- v4.2-rc4/include/linux/mm_types.h</span>
<span class="p_header">+++ v4.2-rc4_patched/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -364,6 +364,10 @@</span> <span class="p_context"> struct mm_rss_stat {</span>
 	atomic_long_t count[NR_MM_COUNTERS];
 };
 
<span class="p_add">+#ifdef CONFIG_HUGETLB_PAGE</span>
<span class="p_add">+struct hugetlb_usage;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 struct kioctx_table;
 struct mm_struct {
 	struct vm_area_struct *mmap;		/* list of VMAs */
<span class="p_chunk">@@ -484,6 +488,9 @@</span> <span class="p_context"> struct mm_struct {</span>
 	/* address of the bounds directory */
 	void __user *bd_addr;
 #endif
<span class="p_add">+#ifdef CONFIG_HUGETLB_PAGE</span>
<span class="p_add">+	struct hugetlb_usage *hugetlb_usage;</span>
<span class="p_add">+#endif</span>
 };
 
 static inline void mm_init_cpumask(struct mm_struct *mm)
<span class="p_header">diff --git v4.2-rc4/kernel/fork.c v4.2-rc4_patched/kernel/fork.c</span>
<span class="p_header">index dbd9b8d7b7cc..d43baa91d48c 100644</span>
<span class="p_header">--- v4.2-rc4/kernel/fork.c</span>
<span class="p_header">+++ v4.2-rc4_patched/kernel/fork.c</span>
<span class="p_chunk">@@ -425,6 +425,9 @@</span> <span class="p_context"> static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)</span>
 	retval = khugepaged_fork(mm, oldmm);
 	if (retval)
 		goto out;
<span class="p_add">+	retval = hugetlb_fork(mm, oldmm);</span>
<span class="p_add">+	if (retval)</span>
<span class="p_add">+		goto out;</span>
 
 	prev = NULL;
 	for (mpnt = oldmm-&gt;mmap; mpnt; mpnt = mpnt-&gt;vm_next) {
<span class="p_header">diff --git v4.2-rc4/mm/hugetlb.c v4.2-rc4_patched/mm/hugetlb.c</span>
<span class="p_header">index a8c3087089d8..3aa8c7919364 100644</span>
<span class="p_header">--- v4.2-rc4/mm/hugetlb.c</span>
<span class="p_header">+++ v4.2-rc4_patched/mm/hugetlb.c</span>
<span class="p_chunk">@@ -2562,6 +2562,49 @@</span> <span class="p_context"> void hugetlb_show_meminfo(void)</span>
 				1UL &lt;&lt; (huge_page_order(h) + PAGE_SHIFT - 10));
 }
 
<span class="p_add">+static unsigned long mm_hstate_usage(struct mm_struct *mm, int hs_idx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!mm-&gt;hugetlb_usage)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	return atomic_long_read(&amp;mm-&gt;hugetlb_usage-&gt;count[hs_idx]);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void hugetlb_report_usage(struct seq_file *m, struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	unsigned long total_usage = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; HUGE_MAX_HSTATE; i++) {</span>
<span class="p_add">+		total_usage += mm_hstate_usage(mm, i) *</span>
<span class="p_add">+			(huge_page_size(&amp;hstates[i]) &gt;&gt; 10);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	seq_printf(m, &quot;HugetlbPages:\t%8lu kB (&quot;, total_usage);</span>
<span class="p_add">+	for (i = 0; i &lt; HUGE_MAX_HSTATE; i++) {</span>
<span class="p_add">+		if (huge_page_order(&amp;hstates[i]) == 0)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		if (i &gt; 0)</span>
<span class="p_add">+			seq_puts(m, &quot; &quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+		seq_printf(m, &quot;%ld*%lukB&quot;, mm_hstate_usage(mm, i),</span>
<span class="p_add">+			huge_page_size(&amp;hstates[i]) &gt;&gt; 10);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	seq_puts(m, &quot;)\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int hugetlb_fork(struct mm_struct *new, struct mm_struct *old)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (old-&gt;hugetlb_usage) {</span>
<span class="p_add">+		new-&gt;hugetlb_usage = kmalloc(sizeof(struct hugetlb_usage),</span>
<span class="p_add">+							GFP_KERNEL);</span>
<span class="p_add">+		if (!new-&gt;hugetlb_usage)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		memcpy(new-&gt;hugetlb_usage, old-&gt;hugetlb_usage,</span>
<span class="p_add">+			sizeof(struct hugetlb_usage));</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Return the number pages of memory we physically have, in PAGE_SIZE units. */
 unsigned long hugetlb_total_pages(void)
 {
<span class="p_chunk">@@ -2797,6 +2840,7 @@</span> <span class="p_context"> int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
 			get_page(ptepage);
 			page_dup_rmap(ptepage);
 			set_huge_pte_at(dst, addr, dst_pte, entry);
<span class="p_add">+			inc_hugetlb_count(dst, h);</span>
 		}
 		spin_unlock(src_ptl);
 		spin_unlock(dst_ptl);
<span class="p_chunk">@@ -2877,6 +2921,7 @@</span> <span class="p_context"> void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		if (huge_pte_dirty(pte))
 			set_page_dirty(page);
 
<span class="p_add">+		dec_hugetlb_count(mm, h);</span>
 		page_remove_rmap(page);
 		force_flush = !__tlb_remove_page(tlb, page);
 		if (force_flush) {
<span class="p_chunk">@@ -3261,6 +3306,7 @@</span> <span class="p_context"> static int hugetlb_no_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 				&amp;&amp; (vma-&gt;vm_flags &amp; VM_SHARED)));
 	set_huge_pte_at(mm, address, ptep, new_pte);
 
<span class="p_add">+	inc_hugetlb_count(mm, h);</span>
 	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED)) {
 		/* Optimization, do the COW without a second fault */
 		ret = hugetlb_cow(mm, vma, address, ptep, new_pte, page, ptl);
<span class="p_header">diff --git v4.2-rc4/mm/mmap.c v4.2-rc4_patched/mm/mmap.c</span>
<span class="p_header">index aa632ade2be7..9d9562bc79a8 100644</span>
<span class="p_header">--- v4.2-rc4/mm/mmap.c</span>
<span class="p_header">+++ v4.2-rc4_patched/mm/mmap.c</span>
<span class="p_chunk">@@ -2847,6 +2847,7 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 			nr_accounted += vma_pages(vma);
 		vma = remove_vma(vma);
 	}
<span class="p_add">+	exit_hugetlb_mmap(mm);</span>
 	vm_unacct_memory(nr_accounted);
 }
 
<span class="p_header">diff --git v4.2-rc4/mm/rmap.c v4.2-rc4_patched/mm/rmap.c</span>
<span class="p_header">index 171b68768df1..b33278bc4ddb 100644</span>
<span class="p_header">--- v4.2-rc4/mm/rmap.c</span>
<span class="p_header">+++ v4.2-rc4_patched/mm/rmap.c</span>
<span class="p_chunk">@@ -1230,7 +1230,9 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	update_hiwater_rss(mm);
 
 	if (PageHWPoison(page) &amp;&amp; !(flags &amp; TTU_IGNORE_HWPOISON)) {
<span class="p_del">-		if (!PageHuge(page)) {</span>
<span class="p_add">+		if (PageHuge(page)) {</span>
<span class="p_add">+			dec_hugetlb_count(mm, page_hstate(page));</span>
<span class="p_add">+		} else {</span>
 			if (PageAnon(page))
 				dec_mm_counter(mm, MM_ANONPAGES);
 			else

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



