
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,15/17] mm: introduce lazyfree LRU list - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,15/17] mm: introduce lazyfree LRU list</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 12, 2015, 4:33 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1447302793-5376-16-git-send-email-minchan@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7599231/mbox/"
   >mbox</a>
|
   <a href="/patch/7599231/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7599231/">/patch/7599231/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id C5F11BF90C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Nov 2015 04:33:56 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id A68EC207DD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Nov 2015 04:33:54 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 39463207DB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Nov 2015 04:33:52 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753454AbbKLEds (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 11 Nov 2015 23:33:48 -0500
Received: from LGEAMRELO11.lge.com ([156.147.23.51]:39117 &quot;EHLO
	lgeamrelo11.lge.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753336AbbKLEcq (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 11 Nov 2015 23:32:46 -0500
Received: from unknown (HELO lgeamrelo02.lge.com) (156.147.1.126)
	by 156.147.23.51 with ESMTP; 12 Nov 2015 13:32:44 +0900
X-Original-SENDERIP: 156.147.1.126
X-Original-MAILFROM: minchan@kernel.org
Received: from unknown (HELO localhost.localdomain) (10.177.223.161)
	by 156.147.1.126 with ESMTP; 12 Nov 2015 13:32:44 +0900
X-Original-SENDERIP: 10.177.223.161
X-Original-MAILFROM: minchan@kernel.org
From: Minchan Kim &lt;minchan@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;,
	linux-api@vger.kernel.org, Hugh Dickins &lt;hughd@google.com&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	KOSAKI Motohiro &lt;kosaki.motohiro@jp.fujitsu.com&gt;,
	Jason Evans &lt;je@fb.com&gt;, Daniel Micay &lt;danielmicay@gmail.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Shaohua Li &lt;shli@kernel.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	yalin.wang2010@gmail.com, Minchan Kim &lt;minchan@kernel.org&gt;
Subject: [PATCH v3 15/17] mm: introduce lazyfree LRU list
Date: Thu, 12 Nov 2015 13:33:11 +0900
Message-Id: &lt;1447302793-5376-16-git-send-email-minchan@kernel.org&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1447302793-5376-1-git-send-email-minchan@kernel.org&gt;
References: &lt;1447302793-5376-1-git-send-email-minchan@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.2 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 12, 2015, 4:33 a.m.</div>
<pre class="content">
There are issues to support MADV_FREE.

* MADV_FREE pages&#39;s hotness

It&#39;s really arguable. Someone think it&#39;s cold while others are not.
It&#39;s matter of workload dependent so I think no one could have
a one way. IOW, we need tunable knob.

* MADV_FREE on swapless system

Now, we instantly free MADV_FREEed pages on swapless system
because we don&#39;t have aged anonymous LRU list on swapless system
so there is no chance to discard them.

I tried to solve it with inactive anonymous LRU list without
introducing new LRU list but it needs a few hooks in reclaim
path to fix old behavior witch was not good to me. Moreover,
it makes implement tuning konb hard.

For addressing issues, this patch adds new LazyFree LRU list and
functions for the stat. Pages on the list have PG_lazyfree flag
which overrides PG_mappedtodisk(It should be safe because
no anonymous page can have the flag).

If user calls madvise(start, len, MADV_FREE), pages in the range
moves to lazyfree LRU from anonymous LRU. When memory pressure
happens, they can be discarded since there is no more store
opeartion since then. If there is store operation, they can move
to active anonymous LRU list.

In this patch, How to age lazyfree pages is very basic, which just
discards all pages in the list whenever memory pressure happens.
It&#39;s enough to prove working. Later patch will implement the policy.
<span class="signed-off-by">
Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
---
 drivers/base/node.c                       |  2 +
 drivers/staging/android/lowmemorykiller.c |  3 +-
 fs/proc/meminfo.c                         |  2 +
 include/linux/mm_inline.h                 | 25 +++++++++--
 include/linux/mmzone.h                    | 11 +++--
 include/linux/page-flags.h                |  5 +++
 include/linux/rmap.h                      |  2 +-
 include/linux/swap.h                      |  1 +
 include/linux/vm_event_item.h             |  4 +-
 include/trace/events/vmscan.h             | 18 +++++---
 mm/compaction.c                           | 12 ++++--
 mm/huge_memory.c                          |  4 +-
 mm/madvise.c                              |  3 +-
 mm/memcontrol.c                           | 14 +++++-
 mm/migrate.c                              |  2 +
 mm/page_alloc.c                           |  3 ++
 mm/rmap.c                                 | 15 +++++--
 mm/swap.c                                 | 48 +++++++++++++++++++++
 mm/vmscan.c                               | 71 +++++++++++++++++++++++++------
 mm/vmstat.c                               |  3 ++
 20 files changed, 203 insertions(+), 45 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="p_header">index 560751bad294..f7a1f2107b43 100644</span>
<span class="p_header">--- a/drivers/base/node.c</span>
<span class="p_header">+++ b/drivers/base/node.c</span>
<span class="p_chunk">@@ -70,6 +70,7 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       &quot;Node %d Active(file):   %8lu kB\n&quot;
 		       &quot;Node %d Inactive(file): %8lu kB\n&quot;
 		       &quot;Node %d Unevictable:    %8lu kB\n&quot;
<span class="p_add">+		       &quot;Node %d LazyFree:	%8lu kB\n&quot;</span>
 		       &quot;Node %d Mlocked:        %8lu kB\n&quot;,
 		       nid, K(i.totalram),
 		       nid, K(i.freeram),
<span class="p_chunk">@@ -83,6 +84,7 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       nid, K(node_page_state(nid, NR_ACTIVE_FILE)),
 		       nid, K(node_page_state(nid, NR_INACTIVE_FILE)),
 		       nid, K(node_page_state(nid, NR_UNEVICTABLE)),
<span class="p_add">+		       nid, K(node_page_state(nid, NR_LZFREE)),</span>
 		       nid, K(node_page_state(nid, NR_MLOCK)));
 
 #ifdef CONFIG_HIGHMEM
<span class="p_header">diff --git a/drivers/staging/android/lowmemorykiller.c b/drivers/staging/android/lowmemorykiller.c</span>
<span class="p_header">index 872bd603fd0d..658c16a653c2 100644</span>
<span class="p_header">--- a/drivers/staging/android/lowmemorykiller.c</span>
<span class="p_header">+++ b/drivers/staging/android/lowmemorykiller.c</span>
<span class="p_chunk">@@ -72,7 +72,8 @@</span> <span class="p_context"> static unsigned long lowmem_count(struct shrinker *s,</span>
 	return global_page_state(NR_ACTIVE_ANON) +
 		global_page_state(NR_ACTIVE_FILE) +
 		global_page_state(NR_INACTIVE_ANON) +
<span class="p_del">-		global_page_state(NR_INACTIVE_FILE);</span>
<span class="p_add">+		global_page_state(NR_INACTIVE_FILE) +</span>
<span class="p_add">+		global_page_state(NR_LZFREE);</span>
 }
 
 static unsigned long lowmem_scan(struct shrinker *s, struct shrink_control *sc)
<span class="p_header">diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c</span>
<span class="p_header">index d3ebf2e61853..3444f7c4e0b6 100644</span>
<span class="p_header">--- a/fs/proc/meminfo.c</span>
<span class="p_header">+++ b/fs/proc/meminfo.c</span>
<span class="p_chunk">@@ -102,6 +102,7 @@</span> <span class="p_context"> static int meminfo_proc_show(struct seq_file *m, void *v)</span>
 		&quot;Active(file):   %8lu kB\n&quot;
 		&quot;Inactive(file): %8lu kB\n&quot;
 		&quot;Unevictable:    %8lu kB\n&quot;
<span class="p_add">+		&quot;LazyFree:	 %8lu kB\n&quot;</span>
 		&quot;Mlocked:        %8lu kB\n&quot;
 #ifdef CONFIG_HIGHMEM
 		&quot;HighTotal:      %8lu kB\n&quot;
<span class="p_chunk">@@ -159,6 +160,7 @@</span> <span class="p_context"> static int meminfo_proc_show(struct seq_file *m, void *v)</span>
 		K(pages[LRU_ACTIVE_FILE]),
 		K(pages[LRU_INACTIVE_FILE]),
 		K(pages[LRU_UNEVICTABLE]),
<span class="p_add">+		K(pages[LRU_LZFREE]),</span>
 		K(global_page_state(NR_MLOCK)),
 #ifdef CONFIG_HIGHMEM
 		K(i.totalhigh),
<span class="p_header">diff --git a/include/linux/mm_inline.h b/include/linux/mm_inline.h</span>
<span class="p_header">index 5e08a354f936..7342400f434d 100644</span>
<span class="p_header">--- a/include/linux/mm_inline.h</span>
<span class="p_header">+++ b/include/linux/mm_inline.h</span>
<span class="p_chunk">@@ -26,6 +26,10 @@</span> <span class="p_context"> static __always_inline void add_page_to_lru_list(struct page *page,</span>
 				struct lruvec *lruvec, enum lru_list lru)
 {
 	int nr_pages = hpage_nr_pages(page);
<span class="p_add">+</span>
<span class="p_add">+	if (lru == LRU_LZFREE)</span>
<span class="p_add">+		VM_BUG_ON_PAGE(PageActive(page), page);</span>
<span class="p_add">+</span>
 	mem_cgroup_update_lru_size(lruvec, lru, nr_pages);
 	list_add(&amp;page-&gt;lru, &amp;lruvec-&gt;lists[lru]);
 	__mod_zone_page_state(lruvec_zone(lruvec), NR_LRU_BASE + lru, nr_pages);
<span class="p_chunk">@@ -35,6 +39,10 @@</span> <span class="p_context"> static __always_inline void del_page_from_lru_list(struct page *page,</span>
 				struct lruvec *lruvec, enum lru_list lru)
 {
 	int nr_pages = hpage_nr_pages(page);
<span class="p_add">+</span>
<span class="p_add">+	if (lru == LRU_LZFREE)</span>
<span class="p_add">+		VM_BUG_ON_PAGE(!PageLazyFree(page), page);</span>
<span class="p_add">+</span>
 	mem_cgroup_update_lru_size(lruvec, lru, -nr_pages);
 	list_del(&amp;page-&gt;lru);
 	__mod_zone_page_state(lruvec_zone(lruvec), NR_LRU_BASE + lru, -nr_pages);
<span class="p_chunk">@@ -46,12 +54,14 @@</span> <span class="p_context"> static __always_inline void del_page_from_lru_list(struct page *page,</span>
  *
  * Used for LRU list index arithmetic.
  *
<span class="p_del">- * Returns the base LRU type - file or anon - @page should be on.</span>
<span class="p_add">+ * Returns the base LRU type - file or anon or lazyfree - @page should be on.</span>
  */
 static inline enum lru_list page_lru_base_type(struct page *page)
 {
 	if (page_is_file_cache(page))
 		return LRU_INACTIVE_FILE;
<span class="p_add">+	if (PageLazyFree(page))</span>
<span class="p_add">+		return LRU_LZFREE;</span>
 	return LRU_INACTIVE_ANON;
 }
 
<span class="p_chunk">@@ -60,7 +70,7 @@</span> <span class="p_context"> static inline enum lru_list page_lru_base_type(struct page *page)</span>
  *
  * Used for LRU list index arithmetic.
  *
<span class="p_del">- * Returns 0 if @lru is anon, 1 if it is file.</span>
<span class="p_add">+ * Returns 0 if @lru is anon, 1 if it is file, 2 if it is lazyfree</span>
  */
 static inline int lru_index(enum lru_list lru)
 {
<span class="p_chunk">@@ -75,6 +85,9 @@</span> <span class="p_context"> static inline int lru_index(enum lru_list lru)</span>
 	case LRU_ACTIVE_FILE:
 		base = 1;
 		break;
<span class="p_add">+	case LRU_LZFREE:</span>
<span class="p_add">+		base = 2;</span>
<span class="p_add">+		break;</span>
 	default:
 		BUG();
 	}
<span class="p_chunk">@@ -90,10 +103,12 @@</span> <span class="p_context"> static inline int lru_index(enum lru_list lru)</span>
  */
 static inline int page_off_isolate(struct page *page)
 {
<span class="p_del">-	int lru = NR_ISOLATED_ANON;</span>
<span class="p_add">+	int lru = NR_ISOLATED_LZFREE;</span>
 
 	if (!PageSwapBacked(page))
 		lru = NR_ISOLATED_FILE;
<span class="p_add">+	else if (PageLazyFree(page))</span>
<span class="p_add">+		lru = NR_ISOLATED_LZFREE;</span>
 	return lru;
 }
 
<span class="p_chunk">@@ -106,10 +121,12 @@</span> <span class="p_context"> static inline int page_off_isolate(struct page *page)</span>
  */
 static inline int lru_off_isolate(enum lru_list lru)
 {
<span class="p_del">-	int base = NR_ISOLATED_FILE;</span>
<span class="p_add">+	int base = NR_ISOLATED_LZFREE;</span>
 
 	if (lru &lt;= LRU_ACTIVE_ANON)
 		base = NR_ISOLATED_ANON;
<span class="p_add">+	else if (lru &lt;= LRU_ACTIVE_FILE)</span>
<span class="p_add">+		base = NR_ISOLATED_FILE;</span>
 	return base;
 }
 
<span class="p_header">diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="p_header">index d94347737292..1aaa436da0d5 100644</span>
<span class="p_header">--- a/include/linux/mmzone.h</span>
<span class="p_header">+++ b/include/linux/mmzone.h</span>
<span class="p_chunk">@@ -121,6 +121,7 @@</span> <span class="p_context"> enum zone_stat_item {</span>
 	NR_INACTIVE_FILE,	/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
 	NR_ACTIVE_FILE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
 	NR_UNEVICTABLE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
<span class="p_add">+	NR_LZFREE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
<span class="p_chunk">@@ -140,6 +141,7 @@</span> <span class="p_context"> enum zone_stat_item {</span>
 	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
<span class="p_add">+	NR_ISOLATED_LZFREE,	/* Temporary isolated pages from lzfree lru */</span>
 	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
 	NR_DIRTIED,		/* page dirtyings since bootup */
 	NR_WRITTEN,		/* page writings since bootup */
<span class="p_chunk">@@ -178,6 +180,7 @@</span> <span class="p_context"> enum lru_list {</span>
 	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
 	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
 	LRU_UNEVICTABLE,
<span class="p_add">+	LRU_LZFREE,</span>
 	NR_LRU_LISTS
 };
 
<span class="p_chunk">@@ -207,10 +210,11 @@</span> <span class="p_context"> struct zone_reclaim_stat {</span>
 	 * The higher the rotated/scanned ratio, the more valuable
 	 * that cache is.
 	 *
<span class="p_del">-	 * The anon LRU stats live in [0], file LRU stats in [1]</span>
<span class="p_add">+	 * The anon LRU stats live in [0], file LRU stats in [1],</span>
<span class="p_add">+	 * lazyfree LRU stats in [2]</span>
 	 */
<span class="p_del">-	unsigned long		recent_rotated[2];</span>
<span class="p_del">-	unsigned long		recent_scanned[2];</span>
<span class="p_add">+	unsigned long		recent_rotated[3];</span>
<span class="p_add">+	unsigned long		recent_scanned[3];</span>
 };
 
 struct lruvec {
<span class="p_chunk">@@ -224,6 +228,7 @@</span> <span class="p_context"> struct lruvec {</span>
 /* Mask used at gathering information at once (see memcontrol.c) */
 #define LRU_ALL_FILE (BIT(LRU_INACTIVE_FILE) | BIT(LRU_ACTIVE_FILE))
 #define LRU_ALL_ANON (BIT(LRU_INACTIVE_ANON) | BIT(LRU_ACTIVE_ANON))
<span class="p_add">+#define LRU_ALL_LZFREE (BIT(LRU_LZFREE))</span>
 #define LRU_ALL	     ((1 &lt;&lt; NR_LRU_LISTS) - 1)
 
 /* Isolate clean file */
<span class="p_header">diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h</span>
<span class="p_header">index 416509e26d6d..14f0643af5c4 100644</span>
<span class="p_header">--- a/include/linux/page-flags.h</span>
<span class="p_header">+++ b/include/linux/page-flags.h</span>
<span class="p_chunk">@@ -115,6 +115,9 @@</span> <span class="p_context"> enum pageflags {</span>
 #endif
 	__NR_PAGEFLAGS,
 
<span class="p_add">+	/* MADV_FREE */</span>
<span class="p_add">+	PG_lazyfree = PG_mappedtodisk,</span>
<span class="p_add">+</span>
 	/* Filesystems */
 	PG_checked = PG_owner_priv_1,
 
<span class="p_chunk">@@ -343,6 +346,8 @@</span> <span class="p_context"> TESTPAGEFLAG_FALSE(Ksm)</span>
 
 u64 stable_page_flags(struct page *page);
 
<span class="p_add">+PAGEFLAG(LazyFree, lazyfree);</span>
<span class="p_add">+</span>
 static inline int PageUptodate(struct page *page)
 {
 	int ret = test_bit(PG_uptodate, &amp;(page)-&gt;flags);
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index f4c992826242..edace84b45d5 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -85,7 +85,7 @@</span> <span class="p_context"> enum ttu_flags {</span>
 	TTU_UNMAP = 1,			/* unmap mode */
 	TTU_MIGRATION = 2,		/* migration mode */
 	TTU_MUNLOCK = 4,		/* munlock mode */
<span class="p_del">-	TTU_FREE = 8,			/* free mode */</span>
<span class="p_add">+	TTU_LZFREE = 8,			/* lazyfree mode */</span>
 
 	TTU_IGNORE_MLOCK = (1 &lt;&lt; 8),	/* ignore mlock */
 	TTU_IGNORE_ACCESS = (1 &lt;&lt; 9),	/* don&#39;t age */
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index 8e944c0cedea..f0310eeab3ec 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -308,6 +308,7 @@</span> <span class="p_context"> extern void lru_add_drain_cpu(int cpu);</span>
 extern void lru_add_drain_all(void);
 extern void rotate_reclaimable_page(struct page *page);
 extern void deactivate_page(struct page *page);
<span class="p_add">+extern void add_page_to_lazyfree_list(struct page *page);</span>
 extern void swap_setup(void);
 
 extern void add_page_to_unevictable_list(struct page *page);
<span class="p_header">diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="p_header">index 2b1cef88b827..7ebfd7ca992d 100644</span>
<span class="p_header">--- a/include/linux/vm_event_item.h</span>
<span class="p_header">+++ b/include/linux/vm_event_item.h</span>
<span class="p_chunk">@@ -23,9 +23,9 @@</span> <span class="p_context"></span>
 
 enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGALLOC),
<span class="p_del">-		PGFREE, PGACTIVATE, PGDEACTIVATE,</span>
<span class="p_add">+		PGFREE, PGACTIVATE, PGDEACTIVATE, PGLZFREE,</span>
 		PGFAULT, PGMAJFAULT,
<span class="p_del">-		PGLAZYFREED,</span>
<span class="p_add">+		PGLZFREED,</span>
 		FOR_ALL_ZONES(PGREFILL),
 		FOR_ALL_ZONES(PGSTEAL_KSWAPD),
 		FOR_ALL_ZONES(PGSTEAL_DIRECT),
<span class="p_header">diff --git a/include/trace/events/vmscan.h b/include/trace/events/vmscan.h</span>
<span class="p_header">index 4e9e86733849..a7ce9169b0fa 100644</span>
<span class="p_header">--- a/include/trace/events/vmscan.h</span>
<span class="p_header">+++ b/include/trace/events/vmscan.h</span>
<span class="p_chunk">@@ -12,28 +12,32 @@</span> <span class="p_context"></span>
 
 #define RECLAIM_WB_ANON		0x0001u
 #define RECLAIM_WB_FILE		0x0002u
<span class="p_add">+#define RECLAIM_WB_LZFREE	0x0004u</span>
 #define RECLAIM_WB_MIXED	0x0010u
<span class="p_del">-#define RECLAIM_WB_SYNC		0x0004u /* Unused, all reclaim async */</span>
<span class="p_del">-#define RECLAIM_WB_ASYNC	0x0008u</span>
<span class="p_add">+#define RECLAIM_WB_SYNC		0x0040u /* Unused, all reclaim async */</span>
<span class="p_add">+#define RECLAIM_WB_ASYNC	0x0080u</span>
 
 #define show_reclaim_flags(flags)				\
 	(flags) ? __print_flags(flags, &quot;|&quot;,			\
 		{RECLAIM_WB_ANON,	&quot;RECLAIM_WB_ANON&quot;},	\
 		{RECLAIM_WB_FILE,	&quot;RECLAIM_WB_FILE&quot;},	\
<span class="p_add">+		{RECLAIM_WB_LZFREE,	&quot;RECLAIM_WB_LZFREE&quot;},	\</span>
 		{RECLAIM_WB_MIXED,	&quot;RECLAIM_WB_MIXED&quot;},	\
 		{RECLAIM_WB_SYNC,	&quot;RECLAIM_WB_SYNC&quot;},	\
 		{RECLAIM_WB_ASYNC,	&quot;RECLAIM_WB_ASYNC&quot;}	\
 		) : &quot;RECLAIM_WB_NONE&quot;
 
 #define trace_reclaim_flags(page) ( \
<span class="p_del">-	(page_is_file_cache(page) ? RECLAIM_WB_FILE : RECLAIM_WB_ANON) | \</span>
<span class="p_del">-	(RECLAIM_WB_ASYNC) \</span>
<span class="p_add">+	(page_is_file_cache(page) ? RECLAIM_WB_FILE : \</span>
<span class="p_add">+		(PageLazyFree(page) ? RECLAIM_WB_LZFREE : \</span>
<span class="p_add">+		RECLAIM_WB_ANON)) | (RECLAIM_WB_ASYNC) \</span>
 	)
 
<span class="p_del">-#define trace_shrink_flags(lru) \</span>
<span class="p_add">+#define trace_shrink_flags(lru_idx) \</span>
 	( \
<span class="p_del">-		(lru ? RECLAIM_WB_FILE : RECLAIM_WB_ANON) | \</span>
<span class="p_del">-		(RECLAIM_WB_ASYNC) \</span>
<span class="p_add">+		(lru_idx == 1 ? RECLAIM_WB_FILE : (lru_idx == 0 ? \</span>
<span class="p_add">+			RECLAIM_WB_ANON : RECLAIM_WB_LZFREE)) | \</span>
<span class="p_add">+			(RECLAIM_WB_ASYNC) \</span>
 	)
 
 TRACE_EVENT(mm_vmscan_kswapd_sleep,
<span class="p_header">diff --git a/mm/compaction.c b/mm/compaction.c</span>
<span class="p_header">index d888fa248ebb..cc40c766de38 100644</span>
<span class="p_header">--- a/mm/compaction.c</span>
<span class="p_header">+++ b/mm/compaction.c</span>
<span class="p_chunk">@@ -626,7 +626,7 @@</span> <span class="p_context"> isolate_freepages_range(struct compact_control *cc,</span>
 static void acct_isolated(struct zone *zone, struct compact_control *cc)
 {
 	struct page *page;
<span class="p_del">-	unsigned int count[2] = { 0, };</span>
<span class="p_add">+	unsigned int count[3] = { 0, };</span>
 
 	if (list_empty(&amp;cc-&gt;migratepages))
 		return;
<span class="p_chunk">@@ -636,21 +636,25 @@</span> <span class="p_context"> static void acct_isolated(struct zone *zone, struct compact_control *cc)</span>
 
 	mod_zone_page_state(zone, NR_ISOLATED_ANON, count[0]);
 	mod_zone_page_state(zone, NR_ISOLATED_FILE, count[1]);
<span class="p_add">+	mod_zone_page_state(zone, NR_ISOLATED_LZFREE, count[2]);</span>
 }
 
 /* Similar to reclaim, but different enough that they don&#39;t share logic */
 static bool too_many_isolated(struct zone *zone)
 {
<span class="p_del">-	unsigned long active, inactive, isolated;</span>
<span class="p_add">+	unsigned long active, inactive, lzfree, isolated;</span>
 
 	inactive = zone_page_state(zone, NR_INACTIVE_FILE) +
 					zone_page_state(zone, NR_INACTIVE_ANON);
 	active = zone_page_state(zone, NR_ACTIVE_FILE) +
 					zone_page_state(zone, NR_ACTIVE_ANON);
<span class="p_add">+	lzfree = zone_page_state(zone, NR_LZFREE);</span>
<span class="p_add">+</span>
 	isolated = zone_page_state(zone, NR_ISOLATED_FILE) +
<span class="p_del">-					zone_page_state(zone, NR_ISOLATED_ANON);</span>
<span class="p_add">+			zone_page_state(zone, NR_ISOLATED_ANON) +</span>
<span class="p_add">+			zone_page_state(zone, NR_ISOLATED_LZFREE);</span>
 
<span class="p_del">-	return isolated &gt; (inactive + active) / 2;</span>
<span class="p_add">+	return isolated &gt; (inactive + active + lzfree) / 2;</span>
 }
 
 /**
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index d020aec63717..6da441618548 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1470,8 +1470,7 @@</span> <span class="p_context"> int madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		goto out;
 
 	page = pmd_page(orig_pmd);
<span class="p_del">-	if (PageActive(page))</span>
<span class="p_del">-		deactivate_page(page);</span>
<span class="p_add">+	add_page_to_lazyfree_list(page);</span>
 
 	if (pmd_young(orig_pmd) || pmd_dirty(orig_pmd)) {
 		orig_pmd = pmdp_huge_get_and_clear_full(tlb-&gt;mm, addr, pmd,
<span class="p_chunk">@@ -1787,6 +1786,7 @@</span> <span class="p_context"> static void __split_huge_page_refcount(struct page *page,</span>
 				      (1L &lt;&lt; PG_mlocked) |
 				      (1L &lt;&lt; PG_uptodate) |
 				      (1L &lt;&lt; PG_active) |
<span class="p_add">+				      (1L &lt;&lt; PG_lazyfree) |</span>
 				      (1L &lt;&lt; PG_unevictable) |
 				      (1L &lt;&lt; PG_dirty)));
 
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 27ed057c0bd7..7c88c6cfe300 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -334,8 +334,7 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 			unlock_page(page);
 		}
 
<span class="p_del">-		if (PageActive(page))</span>
<span class="p_del">-			deactivate_page(page);</span>
<span class="p_add">+		add_page_to_lazyfree_list(page);</span>
 
 		if (pte_young(ptent) || pte_dirty(ptent)) {
 			/*
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index c57c4423c688..1dc599ce1bcb 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -109,6 +109,7 @@</span> <span class="p_context"> static const char * const mem_cgroup_lru_names[] = {</span>
 	&quot;inactive_file&quot;,
 	&quot;active_file&quot;,
 	&quot;unevictable&quot;,
<span class="p_add">+	&quot;lazyfree&quot;,</span>
 };
 
 #define THRESHOLDS_EVENTS_TARGET 128
<span class="p_chunk">@@ -1402,6 +1403,8 @@</span> <span class="p_context"> static void mem_cgroup_out_of_memory(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
 static bool test_mem_cgroup_node_reclaimable(struct mem_cgroup *memcg,
 		int nid, bool noswap)
 {
<span class="p_add">+	if (mem_cgroup_node_nr_lru_pages(memcg, nid, LRU_ALL_LZFREE))</span>
<span class="p_add">+		return true;</span>
 	if (mem_cgroup_node_nr_lru_pages(memcg, nid, LRU_ALL_FILE))
 		return true;
 	if (noswap || !total_swap_pages)
<span class="p_chunk">@@ -3120,6 +3123,7 @@</span> <span class="p_context"> static int memcg_numa_stat_show(struct seq_file *m, void *v)</span>
 		{ &quot;total&quot;, LRU_ALL },
 		{ &quot;file&quot;, LRU_ALL_FILE },
 		{ &quot;anon&quot;, LRU_ALL_ANON },
<span class="p_add">+		{ &quot;lazyfree&quot;, LRU_ALL_LZFREE },</span>
 		{ &quot;unevictable&quot;, BIT(LRU_UNEVICTABLE) },
 	};
 	const struct numa_stat *stat;
<span class="p_chunk">@@ -3231,8 +3235,8 @@</span> <span class="p_context"> static int memcg_stat_show(struct seq_file *m, void *v)</span>
 		int nid, zid;
 		struct mem_cgroup_per_zone *mz;
 		struct zone_reclaim_stat *rstat;
<span class="p_del">-		unsigned long recent_rotated[2] = {0, 0};</span>
<span class="p_del">-		unsigned long recent_scanned[2] = {0, 0};</span>
<span class="p_add">+		unsigned long recent_rotated[3] = {0, 0};</span>
<span class="p_add">+		unsigned long recent_scanned[3] = {0, 0};</span>
 
 		for_each_online_node(nid)
 			for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {
<span class="p_chunk">@@ -3241,13 +3245,19 @@</span> <span class="p_context"> static int memcg_stat_show(struct seq_file *m, void *v)</span>
 
 				recent_rotated[0] += rstat-&gt;recent_rotated[0];
 				recent_rotated[1] += rstat-&gt;recent_rotated[1];
<span class="p_add">+				recent_rotated[2] += rstat-&gt;recent_rotated[2];</span>
 				recent_scanned[0] += rstat-&gt;recent_scanned[0];
 				recent_scanned[1] += rstat-&gt;recent_scanned[1];
<span class="p_add">+				recent_scanned[2] += rstat-&gt;recent_scanned[2];</span>
 			}
 		seq_printf(m, &quot;recent_rotated_anon %lu\n&quot;, recent_rotated[0]);
 		seq_printf(m, &quot;recent_rotated_file %lu\n&quot;, recent_rotated[1]);
<span class="p_add">+		seq_printf(m, &quot;recent_rotated_lzfree %lu\n&quot;,</span>
<span class="p_add">+						recent_rotated[2]);</span>
 		seq_printf(m, &quot;recent_scanned_anon %lu\n&quot;, recent_scanned[0]);
 		seq_printf(m, &quot;recent_scanned_file %lu\n&quot;, recent_scanned[1]);
<span class="p_add">+		seq_printf(m, &quot;recent_scanned_lzfree %lu\n&quot;,</span>
<span class="p_add">+						recent_scanned[2]);</span>
 	}
 #endif
 
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 87ebf0833b84..945e5655cd69 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -508,6 +508,8 @@</span> <span class="p_context"> void migrate_page_copy(struct page *newpage, struct page *page)</span>
 		SetPageChecked(newpage);
 	if (PageMappedToDisk(page))
 		SetPageMappedToDisk(newpage);
<span class="p_add">+	if (PageLazyFree(page))</span>
<span class="p_add">+		SetPageLazyFree(newpage);</span>
 
 	if (PageDirty(page)) {
 		clear_page_dirty_for_io(page);
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 48aaf7b9f253..5d0321c3bc82 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -3712,6 +3712,7 @@</span> <span class="p_context"> void show_free_areas(unsigned int filter)</span>
 
 	printk(&quot;active_anon:%lu inactive_anon:%lu isolated_anon:%lu\n&quot;
 		&quot; active_file:%lu inactive_file:%lu isolated_file:%lu\n&quot;
<span class="p_add">+		&quot; lazy_free:%lu isolated_lazyfree:%lu\n&quot;</span>
 		&quot; unevictable:%lu dirty:%lu writeback:%lu unstable:%lu\n&quot;
 		&quot; slab_reclaimable:%lu slab_unreclaimable:%lu\n&quot;
 		&quot; mapped:%lu shmem:%lu pagetables:%lu bounce:%lu\n&quot;
<span class="p_chunk">@@ -3722,6 +3723,8 @@</span> <span class="p_context"> void show_free_areas(unsigned int filter)</span>
 		global_page_state(NR_ACTIVE_FILE),
 		global_page_state(NR_INACTIVE_FILE),
 		global_page_state(NR_ISOLATED_FILE),
<span class="p_add">+		global_page_state(NR_LZFREE),</span>
<span class="p_add">+		global_page_state(NR_ISOLATED_LZFREE),</span>
 		global_page_state(NR_UNEVICTABLE),
 		global_page_state(NR_FILE_DIRTY),
 		global_page_state(NR_WRITEBACK),
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 9449e91839ab..75bd68bc8abc 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1374,10 +1374,17 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 		swp_entry_t entry = { .val = page_private(page) };
 		pte_t swp_pte;
 
<span class="p_del">-		if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_FREE)) {</span>
<span class="p_del">-			/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="p_del">-			dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="p_del">-			goto discard;</span>
<span class="p_add">+		if ((flags &amp; TTU_LZFREE)) {</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageLazyFree(page), page);</span>
<span class="p_add">+			if (!PageDirty(page)) {</span>
<span class="p_add">+				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="p_add">+				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="p_add">+				goto discard;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				set_pte_at(mm, address, pte, pteval);</span>
<span class="p_add">+				ret = SWAP_FAIL;</span>
<span class="p_add">+				goto out_unmap;</span>
<span class="p_add">+			}</span>
 		}
 
 		if (PageSwapCache(page)) {
<span class="p_header">diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="p_header">index 367940d093ad..11c1eb147fd4 100644</span>
<span class="p_header">--- a/mm/swap.c</span>
<span class="p_header">+++ b/mm/swap.c</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"> int page_cluster;</span>
 static DEFINE_PER_CPU(struct pagevec, lru_add_pvec);
 static DEFINE_PER_CPU(struct pagevec, lru_rotate_pvecs);
 static DEFINE_PER_CPU(struct pagevec, lru_deactivate_pvecs);
<span class="p_add">+static DEFINE_PER_CPU(struct pagevec, lru_lazyfree_pvecs);</span>
 
 /*
  * This path almost never happens for VM activity - pages are normally
<span class="p_chunk">@@ -507,6 +508,10 @@</span> <span class="p_context"> static void __activate_page(struct page *page, struct lruvec *lruvec,</span>
 
 		del_page_from_lru_list(page, lruvec, lru);
 		SetPageActive(page);
<span class="p_add">+		if (lru == LRU_LZFREE) {</span>
<span class="p_add">+			ClearPageLazyFree(page);</span>
<span class="p_add">+			lru = LRU_INACTIVE_ANON;</span>
<span class="p_add">+		}</span>
 		lru += LRU_ACTIVE;
 		add_page_to_lru_list(page, lruvec, lru);
 		trace_mm_lru_activate(page);
<span class="p_chunk">@@ -767,6 +772,9 @@</span> <span class="p_context"> static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec,</span>
 	active = PageActive(page);
 	lru = page_lru_base_type(page);
 
<span class="p_add">+	if (lru == LRU_LZFREE)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	if (!file &amp;&amp; !active)
 		return;
 
<span class="p_chunk">@@ -803,6 +811,29 @@</span> <span class="p_context"> static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec,</span>
 	update_page_reclaim_stat(lruvec, lru_index(lru), 0);
 }
 
<span class="p_add">+static void lru_lazyfree_fn(struct page *page, struct lruvec *lruvec,</span>
<span class="p_add">+		void *arg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageAnon(page), page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (PageLRU(page) &amp;&amp; !PageLazyFree(page) &amp;&amp;</span>
<span class="p_add">+				!PageUnevictable(page)) {</span>
<span class="p_add">+		unsigned int nr_pages = 1;</span>
<span class="p_add">+		bool active = PageActive(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		del_page_from_lru_list(page, lruvec,</span>
<span class="p_add">+				LRU_INACTIVE_ANON + active);</span>
<span class="p_add">+		ClearPageActive(page);</span>
<span class="p_add">+		SetPageLazyFree(page);</span>
<span class="p_add">+		add_page_to_lru_list(page, lruvec, LRU_LZFREE);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (PageTransHuge(page))</span>
<span class="p_add">+			nr_pages = HPAGE_PMD_NR;</span>
<span class="p_add">+		count_vm_events(PGLZFREE, nr_pages);</span>
<span class="p_add">+		update_page_reclaim_stat(lruvec, 2, 0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Drain pages out of the cpu&#39;s pagevecs.
  * Either &quot;cpu&quot; is the current CPU, and preemption has already been
<span class="p_chunk">@@ -829,9 +860,25 @@</span> <span class="p_context"> void lru_add_drain_cpu(int cpu)</span>
 	if (pagevec_count(pvec))
 		pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);
 
<span class="p_add">+	pvec = &amp;per_cpu(lru_lazyfree_pvecs, cpu);</span>
<span class="p_add">+	if (pagevec_count(pvec))</span>
<span class="p_add">+		pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
<span class="p_add">+</span>
 	activate_page_drain(cpu);
 }
 
<span class="p_add">+void add_page_to_lazyfree_list(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (PageLRU(page) &amp;&amp; !PageLazyFree(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="p_add">+		struct pagevec *pvec = &amp;get_cpu_var(lru_lazyfree_pvecs);</span>
<span class="p_add">+</span>
<span class="p_add">+		page_cache_get(page);</span>
<span class="p_add">+		if (!pagevec_add(pvec, page))</span>
<span class="p_add">+			pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
<span class="p_add">+		put_cpu_var(lru_lazyfree_pvecs);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /**
  * deactivate_page - forcefully deactivate a page
  * @page: page to deactivate
<span class="p_chunk">@@ -890,6 +937,7 @@</span> <span class="p_context"> void lru_add_drain_all(void)</span>
 		if (pagevec_count(&amp;per_cpu(lru_add_pvec, cpu)) ||
 		    pagevec_count(&amp;per_cpu(lru_rotate_pvecs, cpu)) ||
 		    pagevec_count(&amp;per_cpu(lru_deactivate_pvecs, cpu)) ||
<span class="p_add">+		    pagevec_count(&amp;per_cpu(lru_lazyfree_pvecs, cpu)) ||</span>
 		    need_activate_page_drain(cpu)) {
 			INIT_WORK(work, lru_add_drain_per_cpu);
 			schedule_work_on(cpu, work);
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index f731084c3a23..3a7d57cbceb3 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -197,7 +197,8 @@</span> <span class="p_context"> static unsigned long zone_reclaimable_pages(struct zone *zone)</span>
 	int nr;
 
 	nr = zone_page_state(zone, NR_ACTIVE_FILE) +
<span class="p_del">-	     zone_page_state(zone, NR_INACTIVE_FILE);</span>
<span class="p_add">+		zone_page_state(zone, NR_INACTIVE_FILE) +</span>
<span class="p_add">+		zone_page_state(zone, NR_LZFREE);</span>
 
 	if (get_nr_swap_pages() &gt; 0)
 		nr += zone_page_state(zone, NR_ACTIVE_ANON) +
<span class="p_chunk">@@ -918,6 +919,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 
 		VM_BUG_ON_PAGE(PageActive(page), page);
 		VM_BUG_ON_PAGE(page_zone(page) != zone, page);
<span class="p_add">+		VM_BUG_ON_PAGE((ttu_flags &amp; TTU_LZFREE) &amp;&amp;</span>
<span class="p_add">+				!PageLazyFree(page), page);</span>
 
 		sc-&gt;nr_scanned++;
 
<span class="p_chunk">@@ -1050,7 +1053,16 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 				goto keep_locked;
 			if (!add_to_swap(page, page_list))
 				goto activate_locked;
<span class="p_del">-			freeable = true;</span>
<span class="p_add">+			if (ttu_flags &amp; TTU_LZFREE) {</span>
<span class="p_add">+				freeable = true;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * anon-LRU list can have !PG_dirty &amp;&amp;</span>
<span class="p_add">+				 * !PG_swapcache &amp;&amp; clean pte until</span>
<span class="p_add">+				 * lru_lazyfree_pvec is flushed.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				SetPageDirty(page);</span>
<span class="p_add">+			}</span>
 			may_enter_fs = 1;
 
 			/* Adding to swap updated mapping */
<span class="p_chunk">@@ -1063,8 +1075,9 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 */
 		if (page_mapped(page) &amp;&amp; mapping) {
 			switch (try_to_unmap(page, freeable ?
<span class="p_del">-				(ttu_flags | TTU_BATCH_FLUSH | TTU_FREE) :</span>
<span class="p_del">-				(ttu_flags | TTU_BATCH_FLUSH))) {</span>
<span class="p_add">+				(ttu_flags | TTU_BATCH_FLUSH) :</span>
<span class="p_add">+				((ttu_flags &amp; ~TTU_LZFREE) |</span>
<span class="p_add">+						TTU_BATCH_FLUSH))) {</span>
 			case SWAP_FAIL:
 				goto activate_locked;
 			case SWAP_AGAIN:
<span class="p_chunk">@@ -1190,7 +1203,7 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		__clear_page_locked(page);
 free_it:
 		if (freeable &amp;&amp; !PageDirty(page))
<span class="p_del">-			count_vm_event(PGLAZYFREED);</span>
<span class="p_add">+			count_vm_event(PGLZFREED);</span>
 
 		nr_reclaimed++;
 
<span class="p_chunk">@@ -1458,7 +1471,7 @@</span> <span class="p_context"> int isolate_lru_page(struct page *page)</span>
  * the LRU list will go small and be scanned faster than necessary, leading to
  * unnecessary swapping, thrashing and OOM.
  */
<span class="p_del">-static int too_many_isolated(struct zone *zone, int file,</span>
<span class="p_add">+static int too_many_isolated(struct zone *zone, int lru_index,</span>
 		struct scan_control *sc)
 {
 	unsigned long inactive, isolated;
<span class="p_chunk">@@ -1469,12 +1482,21 @@</span> <span class="p_context"> static int too_many_isolated(struct zone *zone, int file,</span>
 	if (!sane_reclaim(sc))
 		return 0;
 
<span class="p_del">-	if (file) {</span>
<span class="p_del">-		inactive = zone_page_state(zone, NR_INACTIVE_FILE);</span>
<span class="p_del">-		isolated = zone_page_state(zone, NR_ISOLATED_FILE);</span>
<span class="p_del">-	} else {</span>
<span class="p_add">+	switch (lru_index) {</span>
<span class="p_add">+	case 0:</span>
 		inactive = zone_page_state(zone, NR_INACTIVE_ANON);
 		isolated = zone_page_state(zone, NR_ISOLATED_ANON);
<span class="p_add">+		break;</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		inactive = zone_page_state(zone, NR_INACTIVE_FILE);</span>
<span class="p_add">+		isolated = zone_page_state(zone, NR_ISOLATED_FILE);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		inactive = zone_page_state(zone, NR_LZFREE);</span>
<span class="p_add">+		isolated = zone_page_state(zone, NR_ISOLATED_LZFREE);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		BUG();</span>
 	}
 
 	/*
<span class="p_chunk">@@ -1515,6 +1537,10 @@</span> <span class="p_context"> putback_inactive_pages(struct lruvec *lruvec, struct list_head *page_list)</span>
 
 		SetPageLRU(page);
 		lru = page_lru(page);
<span class="p_add">+		if (lru == LRU_LZFREE + LRU_ACTIVE) {</span>
<span class="p_add">+			ClearPageLazyFree(page);</span>
<span class="p_add">+			lru = LRU_ACTIVE_ANON;</span>
<span class="p_add">+		}</span>
 		add_page_to_lru_list(page, lruvec, lru);
 
 		if (is_active_lru(lru)) {
<span class="p_chunk">@@ -1578,7 +1604,7 @@</span> <span class="p_context"> shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,</span>
 	struct zone *zone = lruvec_zone(lruvec);
 	struct zone_reclaim_stat *reclaim_stat = &amp;lruvec-&gt;reclaim_stat;
 
<span class="p_del">-	while (unlikely(too_many_isolated(zone, file, sc))) {</span>
<span class="p_add">+	while (unlikely(too_many_isolated(zone, lru_index(lru), sc))) {</span>
 		congestion_wait(BLK_RW_ASYNC, HZ/10);
 
 		/* We are about to die and free our memory. Return now. */
<span class="p_chunk">@@ -1613,7 +1639,10 @@</span> <span class="p_context"> shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,</span>
 	if (nr_taken == 0)
 		return 0;
 
<span class="p_del">-	nr_reclaimed = shrink_page_list(&amp;page_list, zone, sc, TTU_UNMAP,</span>
<span class="p_add">+	nr_reclaimed = shrink_page_list(&amp;page_list, zone, sc,</span>
<span class="p_add">+				(lru != LRU_LZFREE) ?</span>
<span class="p_add">+				TTU_UNMAP :</span>
<span class="p_add">+				TTU_UNMAP|TTU_LZFREE,</span>
 				&amp;nr_dirty, &amp;nr_unqueued_dirty, &amp;nr_congested,
 				&amp;nr_writeback, &amp;nr_immediate,
 				false);
<span class="p_chunk">@@ -1701,7 +1730,7 @@</span> <span class="p_context"> shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,</span>
 		zone_idx(zone),
 		nr_scanned, nr_reclaimed,
 		sc-&gt;priority,
<span class="p_del">-		trace_shrink_flags(lru));</span>
<span class="p_add">+		trace_shrink_flags(lru_index(lru)));</span>
 	return nr_reclaimed;
 }
 
<span class="p_chunk">@@ -2194,6 +2223,7 @@</span> <span class="p_context"> static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
 	unsigned long nr[NR_LRU_LISTS];
 	unsigned long targets[NR_LRU_LISTS];
 	unsigned long nr_to_scan;
<span class="p_add">+	unsigned long nr_to_scan_lzfree;</span>
 	enum lru_list lru;
 	unsigned long nr_reclaimed = 0;
 	unsigned long nr_to_reclaim = sc-&gt;nr_to_reclaim;
<span class="p_chunk">@@ -2204,6 +2234,7 @@</span> <span class="p_context"> static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
 
 	/* Record the original scan target for proportional adjustments later */
 	memcpy(targets, nr, sizeof(nr));
<span class="p_add">+	nr_to_scan_lzfree = get_lru_size(lruvec, LRU_LZFREE);</span>
 
 	/*
 	 * Global reclaiming within direct reclaim at DEF_PRIORITY is a normal
<span class="p_chunk">@@ -2221,6 +2252,19 @@</span> <span class="p_context"> static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
 
 	init_tlb_ubc();
 
<span class="p_add">+	while (nr_to_scan_lzfree) {</span>
<span class="p_add">+		nr_to_scan = min(nr_to_scan_lzfree, SWAP_CLUSTER_MAX);</span>
<span class="p_add">+		nr_to_scan_lzfree -= nr_to_scan;</span>
<span class="p_add">+</span>
<span class="p_add">+		nr_reclaimed += shrink_inactive_list(nr_to_scan, lruvec,</span>
<span class="p_add">+						sc, LRU_LZFREE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (nr_reclaimed &gt;= nr_to_reclaim) {</span>
<span class="p_add">+		sc-&gt;nr_reclaimed += nr_reclaimed;</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	blk_start_plug(&amp;plug);
 	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
 					nr[LRU_INACTIVE_FILE]) {
<span class="p_chunk">@@ -2364,6 +2408,7 @@</span> <span class="p_context"> static inline bool should_continue_reclaim(struct zone *zone,</span>
 	 */
 	pages_for_compaction = (2UL &lt;&lt; sc-&gt;order);
 	inactive_lru_pages = zone_page_state(zone, NR_INACTIVE_FILE);
<span class="p_add">+	inactive_lru_pages += zone_page_state(zone, NR_LZFREE);</span>
 	if (get_nr_swap_pages() &gt; 0)
 		inactive_lru_pages += zone_page_state(zone, NR_INACTIVE_ANON);
 	if (sc-&gt;nr_reclaimed &lt; pages_for_compaction &amp;&amp;
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index 59d45b22355f..df95d9473bba 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -704,6 +704,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;nr_inactive_file&quot;,
 	&quot;nr_active_file&quot;,
 	&quot;nr_unevictable&quot;,
<span class="p_add">+	&quot;nr_lazyfree&quot;,</span>
 	&quot;nr_mlock&quot;,
 	&quot;nr_anon_pages&quot;,
 	&quot;nr_mapped&quot;,
<span class="p_chunk">@@ -721,6 +722,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;nr_writeback_temp&quot;,
 	&quot;nr_isolated_anon&quot;,
 	&quot;nr_isolated_file&quot;,
<span class="p_add">+	&quot;nr_isolated_lazyfree&quot;,</span>
 	&quot;nr_shmem&quot;,
 	&quot;nr_dirtied&quot;,
 	&quot;nr_written&quot;,
<span class="p_chunk">@@ -756,6 +758,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;pgfree&quot;,
 	&quot;pgactivate&quot;,
 	&quot;pgdeactivate&quot;,
<span class="p_add">+	&quot;pglazyfree&quot;,</span>
 
 	&quot;pgfault&quot;,
 	&quot;pgmajfault&quot;,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



