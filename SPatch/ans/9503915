
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,43/55] KVM: arm/arm64: Handle shadow stage 2 page faults - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,43/55] KVM: arm/arm64: Handle shadow stage 2 page faults</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=171407">Jintack Lim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 9, 2017, 6:24 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1483943091-1364-44-git-send-email-jintack@cs.columbia.edu&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9503915/mbox/"
   >mbox</a>
|
   <a href="/patch/9503915/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9503915/">/patch/9503915/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	B1FAD60762 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 Jan 2017 06:31:18 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9EFCB2815E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 Jan 2017 06:31:18 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 93CC52811C; Mon,  9 Jan 2017 06:31:18 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RCVD_IN_SORBS_SPAM autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3BE282824F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 Jan 2017 06:31:17 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1163590AbdAIGbE (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 9 Jan 2017 01:31:04 -0500
Received: from outprodmail02.cc.columbia.edu ([128.59.72.51]:52368 &quot;EHLO
	outprodmail02.cc.columbia.edu&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S939784AbdAIG0k (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 9 Jan 2017 01:26:40 -0500
Received: from hazelnut (hazelnut.cc.columbia.edu [128.59.213.250])
	by outprodmail02.cc.columbia.edu (8.14.4/8.14.4) with ESMTP id
	v096PWub005289
	for &lt;linux-kernel@vger.kernel.org&gt;; Mon, 9 Jan 2017 01:26:26 -0500
Received: from hazelnut (localhost.localdomain [127.0.0.1])
	by hazelnut (Postfix) with ESMTP id C28298D
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon,  9 Jan 2017 01:26:26 -0500 (EST)
Received: from sendprodmail02.cc.columbia.edu
	(sendprodmail02.cc.columbia.edu [128.59.72.14])
	by hazelnut (Postfix) with ESMTP id A5E0B7E
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon,  9 Jan 2017 01:26:26 -0500 (EST)
Received: from mail-qt0-f197.google.com (mail-qt0-f197.google.com
	[209.85.216.197])
	by sendprodmail02.cc.columbia.edu (8.14.4/8.14.4) with ESMTP id
	v096QQ33042879
	(version=TLSv1/SSLv3 cipher=AES128-GCM-SHA256 bits=128 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Mon, 9 Jan 2017 01:26:26 -0500
Received: by mail-qt0-f197.google.com with SMTP id x49so82679633qtc.7
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Sun, 08 Jan 2017 22:26:26 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=qblQqy16PrAtvaF33AxE4NerDY/pAs5IwSex4zUjaBs=;
	b=iUhWj6wqglaO3mabbHGdDOMLl0z2kyVRa1yQ6RXwyoDC0nsP8j24yDASxdUCMlixix
	Z6xnOqfCUdeLmJuBz4Hy+1/DjDa9eGkNCNgG/lrHg2dJKKSShTVr1cP2oUy6p032ePlU
	7ubdzx+Bp0c10j9sS/9wm0aeCnYHuQMbY2VpFPNbyk0kl3NJL2uYGVGMEN8iG8TbBKzy
	7JPTaUq8FfUp/IPi0ltLl3+cfkFpWFrZJQDtN3zY99kiI2Bknxf5ED0pPu4es1SOFbmA
	nYog9BBx2wre/jiZD8ggs1QLo2g220VLtHwCrfoKpHR3kd/loBxqoJki5FxMaTsVhxBB
	mKOw==
X-Gm-Message-State: AIkVDXJ4ESNwAjtXor4lPh8iBrd5VjSvmRXiFakk0IuqmF0S6XUTxWu2FFGqq2VmweLmUDGNWtFkIyqxXJ0BAL9YL06EQkS5ON6a5P7AOQbthuyRyZvSfc44uC9wBEC8PhL2VEwbAkHrralbVKr0YD13pl8=
X-Received: by 10.55.76.82 with SMTP id z79mr6564895qka.20.1483943186222;
	Sun, 08 Jan 2017 22:26:26 -0800 (PST)
X-Received: by 10.55.76.82 with SMTP id z79mr6564871qka.20.1483943185992;
	Sun, 08 Jan 2017 22:26:25 -0800 (PST)
Received: from jintack.cs.columbia.edu
	([2001:18d8:ffff:16:21a:4aff:feaa:f900])
	by smtp.gmail.com with ESMTPSA id
	h3sm8623257qtc.6.2017.01.08.22.26.24
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Sun, 08 Jan 2017 22:26:25 -0800 (PST)
From: Jintack Lim &lt;jintack@cs.columbia.edu&gt;
To: christoffer.dall@linaro.org, marc.zyngier@arm.com,
	pbonzini@redhat.com, rkrcmar@redhat.com, linux@armlinux.org.uk,
	catalin.marinas@arm.com, will.deacon@arm.com,
	vladimir.murzin@arm.com, suzuki.poulose@arm.com,
	mark.rutland@arm.com, james.morse@arm.com,
	lorenzo.pieralisi@arm.com, kevin.brodsky@arm.com,
	wcohen@redhat.com, shankerd@codeaurora.org, geoff@infradead.org,
	andre.przywara@arm.com, eric.auger@redhat.com,
	anna-maria@linutronix.de, shihwei@cs.columbia.edu,
	linux-arm-kernel@lists.infradead.org, kvmarm@lists.cs.columbia.edu,
	kvm@vger.kernel.org, linux-kernel@vger.kernel.org
Cc: jintack@cs.columbia.edu
Subject: [RFC 43/55] KVM: arm/arm64: Handle shadow stage 2 page faults
Date: Mon,  9 Jan 2017 01:24:39 -0500
Message-Id: &lt;1483943091-1364-44-git-send-email-jintack@cs.columbia.edu&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1483943091-1364-1-git-send-email-jintack@cs.columbia.edu&gt;
References: &lt;1483943091-1364-1-git-send-email-jintack@cs.columbia.edu&gt;
X-No-Spam-Score: Local
X-Scanned-By: MIMEDefang 2.78 on 128.59.72.14
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171407">Jintack Lim</a> - Jan. 9, 2017, 6:24 a.m.</div>
<pre class="content">
<span class="from">From: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>

If we are faulting on a shadow stage 2 translation, we have to take
extra care in faulting in a page, because we have to collapse the two
levels of stage 2 paging by walking the L2-to-L1 stage 2 page tables in
software.

This approach tries to integrate as much as possible with the existing
code.
<span class="signed-off-by">
Signed-off-by: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="signed-off-by">Signed-off-by: Jintack Lim &lt;jintack@cs.columbia.edu&gt;</span>
---
 arch/arm/include/asm/kvm_emulate.h   |  7 ++++
 arch/arm/kvm/mmio.c                  | 12 +++---
 arch/arm/kvm/mmu.c                   | 75 ++++++++++++++++++++++++++++--------
 arch/arm64/include/asm/kvm_emulate.h |  9 +++++
 4 files changed, 82 insertions(+), 21 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/kvm_emulate.h b/arch/arm/include/asm/kvm_emulate.h</span>
<span class="p_header">index 6285f4f..dfc53ce 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/kvm_emulate.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/kvm_emulate.h</span>
<span class="p_chunk">@@ -309,4 +309,11 @@</span> <span class="p_context"> static inline struct kvm_s2_vmid *vcpu_get_active_vmid(struct kvm_vcpu *vcpu)</span>
 {
 	return &amp;vcpu-&gt;kvm-&gt;arch.mmu.vmid;
 }
<span class="p_add">+</span>
<span class="p_add">+/* arm architecture doesn&#39;t support the nesting */</span>
<span class="p_add">+static inline bool kvm_is_shadow_s2_fault(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* __ARM_KVM_EMULATE_H__ */
<span class="p_header">diff --git a/arch/arm/kvm/mmio.c b/arch/arm/kvm/mmio.c</span>
<span class="p_header">index b6e715f..a1009c2 100644</span>
<span class="p_header">--- a/arch/arm/kvm/mmio.c</span>
<span class="p_header">+++ b/arch/arm/kvm/mmio.c</span>
<span class="p_chunk">@@ -153,7 +153,7 @@</span> <span class="p_context"> static int decode_hsr(struct kvm_vcpu *vcpu, bool *is_write, int *len)</span>
 }
 
 int io_mem_abort(struct kvm_vcpu *vcpu, struct kvm_run *run,
<span class="p_del">-		 phys_addr_t fault_ipa)</span>
<span class="p_add">+		 phys_addr_t ipa)</span>
 {
 	unsigned long data;
 	unsigned long rt;
<span class="p_chunk">@@ -182,22 +182,22 @@</span> <span class="p_context"> int io_mem_abort(struct kvm_vcpu *vcpu, struct kvm_run *run,</span>
 		data = vcpu_data_guest_to_host(vcpu, vcpu_get_reg(vcpu, rt),
 					       len);
 
<span class="p_del">-		trace_kvm_mmio(KVM_TRACE_MMIO_WRITE, len, fault_ipa, data);</span>
<span class="p_add">+		trace_kvm_mmio(KVM_TRACE_MMIO_WRITE, len, ipa, data);</span>
 		kvm_mmio_write_buf(data_buf, len, data);
 
<span class="p_del">-		ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, fault_ipa, len,</span>
<span class="p_add">+		ret = kvm_io_bus_write(vcpu, KVM_MMIO_BUS, ipa, len,</span>
 				       data_buf);
 	} else {
 		trace_kvm_mmio(KVM_TRACE_MMIO_READ_UNSATISFIED, len,
<span class="p_del">-			       fault_ipa, 0);</span>
<span class="p_add">+			       ipa, 0);</span>
 
<span class="p_del">-		ret = kvm_io_bus_read(vcpu, KVM_MMIO_BUS, fault_ipa, len,</span>
<span class="p_add">+		ret = kvm_io_bus_read(vcpu, KVM_MMIO_BUS, ipa, len,</span>
 				      data_buf);
 	}
 
 	/* Now prepare kvm_run for the potential return to userland. */
 	run-&gt;mmio.is_write	= is_write;
<span class="p_del">-	run-&gt;mmio.phys_addr	= fault_ipa;</span>
<span class="p_add">+	run-&gt;mmio.phys_addr	= ipa;</span>
 	run-&gt;mmio.len		= len;
 
 	if (!ret) {
<span class="p_header">diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c</span>
<span class="p_header">index 1677a87..710ae60 100644</span>
<span class="p_header">--- a/arch/arm/kvm/mmu.c</span>
<span class="p_header">+++ b/arch/arm/kvm/mmu.c</span>
<span class="p_chunk">@@ -1072,10 +1072,10 @@</span> <span class="p_context"> int kvm_phys_addr_ioremap(struct kvm *kvm, phys_addr_t guest_ipa,</span>
 	return ret;
 }
 
<span class="p_del">-static bool transparent_hugepage_adjust(kvm_pfn_t *pfnp, phys_addr_t *ipap)</span>
<span class="p_add">+static bool transparent_hugepage_adjust(kvm_pfn_t *pfnp, gfn_t gfn,</span>
<span class="p_add">+					phys_addr_t *ipap)</span>
 {
 	kvm_pfn_t pfn = *pfnp;
<span class="p_del">-	gfn_t gfn = *ipap &gt;&gt; PAGE_SHIFT;</span>
 
 	if (PageTransCompoundMap(pfn_to_page(pfn))) {
 		unsigned long mask;
<span class="p_chunk">@@ -1291,13 +1291,15 @@</span> <span class="p_context"> static void coherent_cache_guest_page(struct kvm_vcpu *vcpu, kvm_pfn_t pfn,</span>
 }
 
 static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,
<span class="p_del">-			  struct kvm_memory_slot *memslot, unsigned long hva,</span>
<span class="p_del">-			  unsigned long fault_status)</span>
<span class="p_add">+			  struct kvm_s2_trans *nested,</span>
<span class="p_add">+			  struct kvm_memory_slot *memslot,</span>
<span class="p_add">+			  unsigned long hva, unsigned long fault_status)</span>
 {
 	int ret;
 	bool write_fault, writable, hugetlb = false, force_pte = false;
 	unsigned long mmu_seq;
<span class="p_del">-	gfn_t gfn = fault_ipa &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	phys_addr_t ipa = fault_ipa;</span>
<span class="p_add">+	gfn_t gfn;</span>
 	struct kvm *kvm = vcpu-&gt;kvm;
 	struct kvm_mmu_memory_cache *memcache = &amp;vcpu-&gt;arch.mmu_page_cache;
 	struct vm_area_struct *vma;
<span class="p_chunk">@@ -1323,9 +1325,23 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 		return -EFAULT;
 	}
 
<span class="p_del">-	if (is_vm_hugetlb_page(vma) &amp;&amp; !logging_active) {</span>
<span class="p_add">+	if (kvm_is_shadow_s2_fault(vcpu)) {</span>
<span class="p_add">+		ipa = nested-&gt;output;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If we&#39;re about to create a shadow stage 2 entry, then we</span>
<span class="p_add">+		 * can only create huge mapings if the guest hypervior also</span>
<span class="p_add">+		 * uses a huge mapping.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (nested-&gt;block_size != PMD_SIZE)</span>
<span class="p_add">+			force_pte = true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	gfn = ipa &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!force_pte &amp;&amp; is_vm_hugetlb_page(vma) &amp;&amp; !logging_active) {</span>
 		hugetlb = true;
<span class="p_del">-		gfn = (fault_ipa &amp; PMD_MASK) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+		gfn = (ipa &amp; PMD_MASK) &gt;&gt; PAGE_SHIFT;</span>
 	} else {
 		/*
 		 * Pages belonging to memslots that don&#39;t have the same
<span class="p_chunk">@@ -1389,7 +1405,7 @@</span> <span class="p_context"> static int user_mem_abort(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa,</span>
 		goto out_unlock;
 
 	if (!hugetlb &amp;&amp; !force_pte)
<span class="p_del">-		hugetlb = transparent_hugepage_adjust(&amp;pfn, &amp;fault_ipa);</span>
<span class="p_add">+		hugetlb = transparent_hugepage_adjust(&amp;pfn, gfn, &amp;fault_ipa);</span>
 
 	fault_ipa_uncached = memslot-&gt;flags &amp; KVM_MEMSLOT_INCOHERENT;
 
<span class="p_chunk">@@ -1435,6 +1451,12 @@</span> <span class="p_context"> static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)</span>
 	kvm_pfn_t pfn;
 	bool pfn_valid = false;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * TODO: Lookup nested S2 pgtable entry and if the access flag is set,</span>
<span class="p_add">+	 * then inject an access fault to the guest and invalidate the shadow</span>
<span class="p_add">+	 * entry.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
 	trace_kvm_access_fault(fault_ipa);
 
 	spin_lock(&amp;vcpu-&gt;kvm-&gt;mmu_lock);
<span class="p_chunk">@@ -1478,8 +1500,10 @@</span> <span class="p_context"> static void handle_access_fault(struct kvm_vcpu *vcpu, phys_addr_t fault_ipa)</span>
 int kvm_handle_guest_abort(struct kvm_vcpu *vcpu, struct kvm_run *run)
 {
 	unsigned long fault_status;
<span class="p_del">-	phys_addr_t fault_ipa;</span>
<span class="p_add">+	phys_addr_t fault_ipa; /* The address we faulted on */</span>
<span class="p_add">+	phys_addr_t ipa; /* Always the IPA in the L1 guest phys space */</span>
 	struct kvm_memory_slot *memslot;
<span class="p_add">+	struct kvm_s2_trans nested_trans;</span>
 	unsigned long hva;
 	bool is_iabt, write_fault, writable;
 	gfn_t gfn;
<span class="p_chunk">@@ -1491,7 +1515,7 @@</span> <span class="p_context"> int kvm_handle_guest_abort(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 		return 1;
 	}
 
<span class="p_del">-	fault_ipa = kvm_vcpu_get_fault_ipa(vcpu);</span>
<span class="p_add">+	ipa = fault_ipa = kvm_vcpu_get_fault_ipa(vcpu);</span>
 
 	trace_kvm_guest_fault(*vcpu_pc(vcpu), kvm_vcpu_get_hsr(vcpu),
 			      kvm_vcpu_get_hfar(vcpu), fault_ipa);
<span class="p_chunk">@@ -1500,6 +1524,10 @@</span> <span class="p_context"> int kvm_handle_guest_abort(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 	fault_status = kvm_vcpu_trap_get_fault_type(vcpu);
 	if (fault_status != FSC_FAULT &amp;&amp; fault_status != FSC_PERM &amp;&amp;
 	    fault_status != FSC_ACCESS) {
<span class="p_add">+		/*</span>
<span class="p_add">+		 * TODO: Report address size faults from an L2 IPA which</span>
<span class="p_add">+		 * exceeds KVM_PHYS_SIZE to the L1 hypervisor.</span>
<span class="p_add">+		 */</span>
 		kvm_err(&quot;Unsupported FSC: EC=%#x xFSC=%#lx ESR_EL2=%#lx\n&quot;,
 			kvm_vcpu_trap_get_class(vcpu),
 			(unsigned long)kvm_vcpu_trap_get_fault(vcpu),
<span class="p_chunk">@@ -1509,7 +1537,23 @@</span> <span class="p_context"> int kvm_handle_guest_abort(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 
 	idx = srcu_read_lock(&amp;vcpu-&gt;kvm-&gt;srcu);
 
<span class="p_del">-	gfn = fault_ipa &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We may have faulted on a shadow stage 2 page table if we are</span>
<span class="p_add">+	 * running a nested guest.  In this case, we have to resovle the L2</span>
<span class="p_add">+	 * IPA to the L1 IPA first, before knowing what kind of memory should</span>
<span class="p_add">+	 * back the L1 IPA.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * If the shadow stage 2 page table walk faults, then we simply inject</span>
<span class="p_add">+	 * this to the guest and carry on.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (kvm_is_shadow_s2_fault(vcpu)) {</span>
<span class="p_add">+		ret = kvm_walk_nested_s2(vcpu, fault_ipa, &amp;nested_trans);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto out_unlock;</span>
<span class="p_add">+		ipa = nested_trans.output;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	gfn = ipa &gt;&gt; PAGE_SHIFT;</span>
 	memslot = gfn_to_memslot(vcpu-&gt;kvm, gfn);
 	hva = gfn_to_hva_memslot_prot(memslot, gfn, &amp;writable);
 	write_fault = kvm_is_write_fault(vcpu);
<span class="p_chunk">@@ -1543,13 +1587,13 @@</span> <span class="p_context"> int kvm_handle_guest_abort(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 		 * faulting VA. This is always 12 bits, irrespective
 		 * of the page size.
 		 */
<span class="p_del">-		fault_ipa |= kvm_vcpu_get_hfar(vcpu) &amp; ((1 &lt;&lt; 12) - 1);</span>
<span class="p_del">-		ret = io_mem_abort(vcpu, run, fault_ipa);</span>
<span class="p_add">+		ipa |= kvm_vcpu_get_hfar(vcpu) &amp; ((1 &lt;&lt; 12) - 1);</span>
<span class="p_add">+		ret = io_mem_abort(vcpu, run, ipa);</span>
 		goto out_unlock;
 	}
 
 	/* Userspace should not be able to register out-of-bounds IPAs */
<span class="p_del">-	VM_BUG_ON(fault_ipa &gt;= KVM_PHYS_SIZE);</span>
<span class="p_add">+	VM_BUG_ON(ipa &gt;= KVM_PHYS_SIZE);</span>
 
 	if (fault_status == FSC_ACCESS) {
 		handle_access_fault(vcpu, fault_ipa);
<span class="p_chunk">@@ -1557,7 +1601,8 @@</span> <span class="p_context"> int kvm_handle_guest_abort(struct kvm_vcpu *vcpu, struct kvm_run *run)</span>
 		goto out_unlock;
 	}
 
<span class="p_del">-	ret = user_mem_abort(vcpu, fault_ipa, memslot, hva, fault_status);</span>
<span class="p_add">+	ret = user_mem_abort(vcpu, fault_ipa, &amp;nested_trans,</span>
<span class="p_add">+			     memslot, hva, fault_status);</span>
 	if (ret == 0)
 		ret = 1;
 out_unlock:
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_emulate.h b/arch/arm64/include/asm/kvm_emulate.h</span>
<span class="p_header">index abad676..2994410 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_emulate.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_emulate.h</span>
<span class="p_chunk">@@ -368,4 +368,13 @@</span> <span class="p_context"> static inline unsigned long vcpu_data_host_to_guest(struct kvm_vcpu *vcpu,</span>
 	return data;		/* Leave LE untouched */
 }
 
<span class="p_add">+static inline bool kvm_is_shadow_s2_fault(struct kvm_vcpu *vcpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_KVM_ARM_NESTED_HYP</span>
<span class="p_add">+	return (!vcpu_mode_el2(vcpu)) &amp;&amp; vcpu_nested_stage2_enabled(vcpu);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif /* __ARM64_KVM_EMULATE_H__ */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



