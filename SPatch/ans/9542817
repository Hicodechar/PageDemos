
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,v17,06/14] mm/migrate: new memory migration helper for use with device memory v3 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,v17,06/14] mm/migrate: new memory migration helper for use with device memory v3</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 27, 2017, 10:52 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1485557541-7806-7-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9542817/mbox/"
   >mbox</a>
|
   <a href="/patch/9542817/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9542817/">/patch/9542817/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	8AF6960415 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 27 Jan 2017 21:58:40 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 79C93204C1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 27 Jan 2017 21:58:40 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 6B678276D6; Fri, 27 Jan 2017 21:58:40 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 455AE204C1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 27 Jan 2017 21:58:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751850AbdA0V6h (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 27 Jan 2017 16:58:37 -0500
Received: from mx1.redhat.com ([209.132.183.28]:54140 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751596AbdA0Vuy (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 27 Jan 2017 16:50:54 -0500
Received: from int-mx13.intmail.prod.int.phx2.redhat.com
	(int-mx13.intmail.prod.int.phx2.redhat.com [10.5.11.26])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 4B6B96AAE2;
	Fri, 27 Jan 2017 21:50:55 +0000 (UTC)
Received: from xgl-cortex.ml2.eng.bos.redhat.com
	(xgl-cortex.ml2.eng.bos.redhat.com [10.19.160.80])
	by int-mx13.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id v0RLoj1A020725; Fri, 27 Jan 2017 16:50:54 -0500
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	David Nellans &lt;dnellans@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;
Subject: [HMM v17 06/14] mm/migrate: new memory migration helper for use
	with device memory v3
Date: Fri, 27 Jan 2017 17:52:13 -0500
Message-Id: &lt;1485557541-7806-7-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1485557541-7806-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1485557541-7806-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.26
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.26]);
	Fri, 27 Jan 2017 21:50:55 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Jan. 27, 2017, 10:52 p.m.</div>
<pre class="content">
This patch add a new memory migration helpers, which migrate memory
backing a range of virtual address of a process to different memory
(which can be allocated through special allocator). It differs from
numa migration by working on a range of virtual address and thus by
doing migration in chunk that can be large enough to use DMA engine
or special copy offloading engine.

Expected users are any one with heterogeneous memory where different
memory have different characteristics (latency, bandwidth, ...). As
an example IBM platform with CAPI bus can make use of this feature
to migrate between regular memory and CAPI device memory. New CPU
architecture with a pool of high performance memory not manage as
cache but presented as regular memory (while being faster and with
lower latency than DDR) will also be prime user of this patch.

Migration to private device memory will be usefull for device that
have large pool of such like GPU, NVidia plans to use HMM for that.

Changed since v2:
  - droped HMM prefix and HMM specific code
Changed since v1:
  - typos fix
  - split early unmap optimization for page with single mapping
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
---
 include/linux/migrate.h |  74 ++++++++
 mm/migrate.c            | 449 ++++++++++++++++++++++++++++++++++++++++++++++++
 2 files changed, 523 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=165331">Reza Arbab</a> - Feb. 22, 2017, 3:31 p.m.</div>
<pre class="content">
On Fri, Jan 27, 2017 at 05:52:13PM -0500, Jérôme Glisse wrote:
<span class="quote">&gt;This patch add a new memory migration helpers, which migrate memory</span>
<span class="quote">&gt;backing a range of virtual address of a process to different memory</span>
<span class="quote">&gt;(which can be allocated through special allocator). It differs from</span>
<span class="quote">&gt;numa migration by working on a range of virtual address and thus by</span>
<span class="quote">&gt;doing migration in chunk that can be large enough to use DMA engine</span>
<span class="quote">&gt;or special copy offloading engine.</span>

Just wanted to say I&#39;ve found these migration helpers quite useful. I&#39;ve 
been prototyping some driver code which uses them, rebasing on each HMM 
revision since v14. So for what it&#39;s worth, 
<span class="acked-by">
Acked-by: Reza Arbab &lt;arbab@linux.vnet.ibm.com&gt;</span>
<span class="tested-by">Tested-by: Reza Arbab &lt;arbab@linux.vnet.ibm.com&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="p_header">index 37b77ba..cd56e41 100644</span>
<span class="p_header">--- a/include/linux/migrate.h</span>
<span class="p_header">+++ b/include/linux/migrate.h</span>
<span class="p_chunk">@@ -122,4 +122,78 @@</span> <span class="p_context"> static inline int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 }
 #endif /* CONFIG_NUMA_BALANCING &amp;&amp; CONFIG_TRANSPARENT_HUGEPAGE*/
 
<span class="p_add">+</span>
<span class="p_add">+#define MIGRATE_PFN_VALID	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 1))</span>
<span class="p_add">+#define MIGRATE_PFN_MIGRATE	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 2))</span>
<span class="p_add">+#define MIGRATE_PFN_HUGE	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 3))</span>
<span class="p_add">+#define MIGRATE_PFN_LOCKED	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 4))</span>
<span class="p_add">+#define MIGRATE_PFN_WRITE	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 5))</span>
<span class="p_add">+#define MIGRATE_PFN_ZERO	(1UL &lt;&lt; (BITS_PER_LONG_LONG - 6))</span>
<span class="p_add">+#define MIGRATE_PFN_MASK	((1UL &lt;&lt; (BITS_PER_LONG_LONG - PAGE_SHIFT)) - 1)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *migrate_pfn_to_page(unsigned long mpfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!(mpfn &amp; MIGRATE_PFN_VALID))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	return pfn_to_page(mpfn &amp; MIGRATE_PFN_MASK);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long migrate_pfn_size(unsigned long mpfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return mpfn &amp; MIGRATE_PFN_HUGE ? PMD_SIZE : PAGE_SIZE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * struct migrate_vma_ops - migrate operation callback</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @alloc_and_copy: alloc destination memoiry and copy source to it</span>
<span class="p_add">+ * @finalize_and_map: allow caller to inspect successfull migrated page</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * migrate_vma() allow memory migration to use DMA  engine to perform copy from</span>
<span class="p_add">+ * source to destination memory it also allow caller to use its own memory</span>
<span class="p_add">+ * allocator for destination memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that in alloc_and_copy device driver can decide not to migrate some of</span>
<span class="p_add">+ * the entry by simply setting corresponding dst entry 0.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Destination page must locked and MIGRATE_PFN_LOCKED set in the corresponding</span>
<span class="p_add">+ * entry of dstarray. It is expected that page allocated will have an elevated</span>
<span class="p_add">+ * refcount and that a put_page() will free the page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Device driver might want to allocate with an extra-refcount if they want to</span>
<span class="p_add">+ * control deallocation of failed migration inside finalize_and_map() callback.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The finalize_and_map() callback must use the MIGRATE_PFN_MIGRATE flag to</span>
<span class="p_add">+ * determine which page have been successfully migrated (it is set in the src</span>
<span class="p_add">+ * array for each entry that have been successfully migrated).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For migration from device memory to system memory device driver must set any</span>
<span class="p_add">+ * dst entry to MIGRATE_PFN_ERROR for any entry it can not migrate back due to</span>
<span class="p_add">+ * hardware fatal failure that can not be recovered. Such failure will trigger</span>
<span class="p_add">+ * a SIGBUS for the process trying to access such memory.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct migrate_vma_ops {</span>
<span class="p_add">+	void (*alloc_and_copy)(struct vm_area_struct *vma,</span>
<span class="p_add">+			       const unsigned long *src,</span>
<span class="p_add">+			       unsigned long *dst,</span>
<span class="p_add">+			       unsigned long start,</span>
<span class="p_add">+			       unsigned long end,</span>
<span class="p_add">+			       void *private);</span>
<span class="p_add">+	void (*finalize_and_map)(struct vm_area_struct *vma,</span>
<span class="p_add">+				 const unsigned long *src,</span>
<span class="p_add">+				 unsigned long *dst,</span>
<span class="p_add">+				 unsigned long start,</span>
<span class="p_add">+				 unsigned long end,</span>
<span class="p_add">+				 void *private);</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="p_add">+		struct vm_area_struct *vma,</span>
<span class="p_add">+		unsigned long mentries,</span>
<span class="p_add">+		unsigned long start,</span>
<span class="p_add">+		unsigned long end,</span>
<span class="p_add">+		unsigned long *src,</span>
<span class="p_add">+		unsigned long *dst,</span>
<span class="p_add">+		void *private);</span>
<span class="p_add">+</span>
 #endif /* _LINUX_MIGRATE_H */
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 567674d..150fc4d 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -412,6 +412,14 @@</span> <span class="p_context"> int migrate_page_move_mapping(struct address_space *mapping,</span>
 	int expected_count = 1 + extra_count;
 	void **pslot;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * ZONE_DEVICE pages have 1 refcount always held by their device</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that DAX memory will never reach that point as it does not have</span>
<span class="p_add">+	 * the MEMORY_DEVICE_ALLOW_MIGRATE flag set (see memory_hotplug.h).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	expected_count += is_zone_device_page(page);</span>
<span class="p_add">+</span>
 	if (!mapping) {
 		/* Anonymous page without mapping */
 		if (page_count(page) != expected_count)
<span class="p_chunk">@@ -2078,3 +2086,444 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 #endif /* CONFIG_NUMA_BALANCING */
 
 #endif /* CONFIG_NUMA */
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+struct migrate_vma {</span>
<span class="p_add">+	struct vm_area_struct	*vma;</span>
<span class="p_add">+	unsigned long		*dst;</span>
<span class="p_add">+	unsigned long		*src;</span>
<span class="p_add">+	unsigned long		cpages;</span>
<span class="p_add">+	unsigned long		npages;</span>
<span class="p_add">+	unsigned long		mpages;</span>
<span class="p_add">+	unsigned long		start;</span>
<span class="p_add">+	unsigned long		end;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int migrate_vma_array_full(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return migrate-&gt;npages &gt;= migrate-&gt;mpages ? -ENOSPC : 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int migrate_vma_collect_hole(unsigned long start,</span>
<span class="p_add">+				    unsigned long end,</span>
<span class="p_add">+				    struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct migrate_vma *migrate = walk-&gt;private;</span>
<span class="p_add">+	unsigned long addr, next;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (addr = start &amp; PAGE_MASK; addr &lt; end; addr = next) {</span>
<span class="p_add">+		unsigned long npages, i;</span>
<span class="p_add">+		int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+		next = pmd_addr_end(addr, end);</span>
<span class="p_add">+		npages = (next - addr) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+		if (npages == (PMD_SIZE &gt;&gt; PAGE_SHIFT)) {</span>
<span class="p_add">+			migrate-&gt;src[migrate-&gt;npages++] = MIGRATE_PFN_HUGE;</span>
<span class="p_add">+			ret = migrate_vma_array_full(migrate);</span>
<span class="p_add">+			if (ret)</span>
<span class="p_add">+				return ret;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			for (i = 0; i &lt; npages; ++i) {</span>
<span class="p_add">+				migrate-&gt;src[migrate-&gt;npages++] = 0;</span>
<span class="p_add">+				ret = migrate_vma_array_full(migrate);</span>
<span class="p_add">+				if (ret)</span>
<span class="p_add">+					return ret;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int migrate_vma_collect_pmd(pmd_t *pmdp,</span>
<span class="p_add">+				   unsigned long start,</span>
<span class="p_add">+				   unsigned long end,</span>
<span class="p_add">+				   struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct migrate_vma *migrate = walk-&gt;private;</span>
<span class="p_add">+	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long addr = start;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_none(*pmdp) || pmd_trans_unstable(pmdp)) {</span>
<span class="p_add">+		/* FIXME support THP */</span>
<span class="p_add">+		return migrate_vma_collect_hole(start, end, walk);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="p_add">+	for (; addr &lt; end; addr += PAGE_SIZE, ptep++) {</span>
<span class="p_add">+		unsigned long flags, pfn;</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		pte_t pte;</span>
<span class="p_add">+		int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = *ptep;</span>
<span class="p_add">+		pfn = pte_pfn(pte);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_present(pte)) {</span>
<span class="p_add">+			flags = pfn = 0;</span>
<span class="p_add">+			goto next;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* FIXME support THP */</span>
<span class="p_add">+		page = vm_normal_page(migrate-&gt;vma, addr, pte);</span>
<span class="p_add">+		if (!page || !page-&gt;mapping || PageTransCompound(page)) {</span>
<span class="p_add">+			flags = pfn = 0;</span>
<span class="p_add">+			goto next;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * By getting a reference on the page we pin it and blocks any</span>
<span class="p_add">+		 * kind of migration. Side effect is that it &quot;freeze&quot; the pte.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * We drop this reference after isolating the page from the lru</span>
<span class="p_add">+		 * for non device page (device page are not on the lru and thus</span>
<span class="p_add">+		 * can&#39;t be drop from it).</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		get_page(page);</span>
<span class="p_add">+		migrate-&gt;cpages++;</span>
<span class="p_add">+		flags = MIGRATE_PFN_VALID | MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+		flags |= pte_write(pte) ? MIGRATE_PFN_WRITE : 0;</span>
<span class="p_add">+</span>
<span class="p_add">+next:</span>
<span class="p_add">+		migrate-&gt;src[migrate-&gt;npages++] = pfn | flags;</span>
<span class="p_add">+		ret = migrate_vma_array_full(migrate);</span>
<span class="p_add">+		if (ret) {</span>
<span class="p_add">+			pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_collect() - collect page over range of virtual address</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This will go over the CPU page table and for each virtual address back by a</span>
<span class="p_add">+ * valid page it update the src array and take a reference on the page in</span>
<span class="p_add">+ * order to pin the page until we lock it and unmap it.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void migrate_vma_collect(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_walk mm_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+	mm_walk.pmd_entry = migrate_vma_collect_pmd;</span>
<span class="p_add">+	mm_walk.pte_entry = NULL;</span>
<span class="p_add">+	mm_walk.pte_hole = migrate_vma_collect_hole;</span>
<span class="p_add">+	mm_walk.hugetlb_entry = NULL;</span>
<span class="p_add">+	mm_walk.test_walk = NULL;</span>
<span class="p_add">+	mm_walk.vma = migrate-&gt;vma;</span>
<span class="p_add">+	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+	mm_walk.private = migrate;</span>
<span class="p_add">+</span>
<span class="p_add">+	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_check_page() - check if page is pin or not</span>
<span class="p_add">+ * @page: struct page to check</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Pinned page can not be migrated. Same test in migrate_page_move_mapping()</span>
<span class="p_add">+ * except that here we allow migration of ZONE_DEVICE page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static bool migrate_vma_check_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * One extra ref because caller hold an extra reference either from</span>
<span class="p_add">+	 * either isolate_lru_page() for regular page or migrate_vma_collect()</span>
<span class="p_add">+	 * for device page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int extra = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * FIXME support THP (transparent huge page), it is bit more complex to</span>
<span class="p_add">+	 * check them then regular page because they can be map with a pmd or</span>
<span class="p_add">+	 * with a pte (split pte mapping).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (PageCompound(page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((page_count(page) - extra) &gt; page_mapcount(page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_prepare() - lock pages and isolate them from the lru</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This lock pages that have been collected by migrate_vma_collect(). Once page</span>
<span class="p_add">+ * is locked it is isolated from the lru (for non device page). Finaly the ref</span>
<span class="p_add">+ * taken by migrate_vma_collect() is drop as locked page can not be migrated by</span>
<span class="p_add">+ * concurrent kernel thread.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void migrate_vma_prepare(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0, size;</span>
<span class="p_add">+	bool allow_drain = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	lru_add_drain();</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; i &lt; migrate-&gt;npages &amp;&amp; migrate-&gt;cpages; i++, addr += size) {</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		lock_page(page);</span>
<span class="p_add">+		migrate-&gt;src[i] |= MIGRATE_PFN_LOCKED;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!PageLRU(page) &amp;&amp; allow_drain) {</span>
<span class="p_add">+			/* Drain CPU&#39;s pagevec */</span>
<span class="p_add">+			lru_add_drain_all();</span>
<span class="p_add">+			allow_drain = false;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (isolate_lru_page(page)) {</span>
<span class="p_add">+			migrate-&gt;src[i] = 0;</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			migrate-&gt;cpages--;</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Drop the reference we took in collect */</span>
<span class="p_add">+		put_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!migrate_vma_check_page(page)) {</span>
<span class="p_add">+			migrate-&gt;src[i] = 0;</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			migrate-&gt;cpages--;</span>
<span class="p_add">+</span>
<span class="p_add">+			putback_lru_page(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_unmap() - replace page mapping with special migration pte entry</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Replace page mapping (CPU page table pte) with special migration pte entry</span>
<span class="p_add">+ * and check again if it has be pin. Pin page are restore because we can not</span>
<span class="p_add">+ * migrate them.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is the last step before we call the device driver callback to allocate</span>
<span class="p_add">+ * destination memory and copy content of original page over to new page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void migrate_vma_unmap(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int flags = TTU_MIGRATION | TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS;</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0, restore = 0, size;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; migrate-&gt;end &amp;&amp; migrate-&gt;cpages; addr += size, i++) {</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || !(migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		try_to_unmap(page, flags);</span>
<span class="p_add">+		if (page_mapped(page) || !migrate_vma_check_page(page)) {</span>
<span class="p_add">+			migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+			migrate-&gt;cpages--;</span>
<span class="p_add">+			restore++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; migrate-&gt;end &amp;&amp; restore; addr += size, i++) {</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || (migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		remove_migration_ptes(page, page, false);</span>
<span class="p_add">+</span>
<span class="p_add">+		migrate-&gt;src[i] = 0;</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		restore--;</span>
<span class="p_add">+</span>
<span class="p_add">+		putback_lru_page(page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_pages() - migrate meta-data from src page to dst page</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This migrate struct page meta-data from source struct page to destination</span>
<span class="p_add">+ * struct page. This effectively finish the migration from source page to the</span>
<span class="p_add">+ * destination page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void migrate_vma_pages(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0, size;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; migrate-&gt;end; addr += size, i++) {</span>
<span class="p_add">+		struct page *newpage = migrate_pfn_to_page(migrate-&gt;dst[i]);</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		struct address_space *mapping;</span>
<span class="p_add">+		int r;</span>
<span class="p_add">+</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || !newpage)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (!(migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		mapping = page_mapping(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		r = migrate_page(mapping, newpage, page, MIGRATE_SYNC, false);</span>
<span class="p_add">+		if (r != MIGRATEPAGE_SUCCESS)</span>
<span class="p_add">+			migrate-&gt;src[i] &amp;= ~MIGRATE_PFN_MIGRATE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma_finalize() - restore CPU page table entry</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This replace the special migration pte entry with either a mapping to the</span>
<span class="p_add">+ * new page if migration was successful for that page or to the original page</span>
<span class="p_add">+ * otherwise.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This also unlock the page and put them back on the lru or drop the extra</span>
<span class="p_add">+ * ref for device page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void migrate_vma_finalize(struct migrate_vma *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0, size;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr&lt;migrate-&gt;end &amp;&amp; migrate-&gt;cpages; addr += size, i++) {</span>
<span class="p_add">+		struct page *newpage = migrate_pfn_to_page(migrate-&gt;dst[i]);</span>
<span class="p_add">+		struct page *page = migrate_pfn_to_page(migrate-&gt;src[i]);</span>
<span class="p_add">+		size = migrate_pfn_size(migrate-&gt;src[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (!(migrate-&gt;src[i] &amp; MIGRATE_PFN_MIGRATE) || !newpage) {</span>
<span class="p_add">+			if (newpage)</span>
<span class="p_add">+				put_page(newpage);</span>
<span class="p_add">+			newpage = page;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		remove_migration_ptes(page, newpage, false);</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		migrate-&gt;cpages--;</span>
<span class="p_add">+</span>
<span class="p_add">+		putback_lru_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (newpage != page) {</span>
<span class="p_add">+			unlock_page(newpage);</span>
<span class="p_add">+			putback_lru_page(newpage);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * migrate_vma() - migrate a range of memory inside vma using accel copy</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ops: migration callback for allocating destination memory and copying</span>
<span class="p_add">+ * @vma: virtual memory area containing the range to be migrated</span>
<span class="p_add">+ * @mentries: maximum number of entry in src or dst pfns array</span>
<span class="p_add">+ * @start: start address of the range to migrate (inclusive)</span>
<span class="p_add">+ * @end: end address of the range to migrate (exclusive)</span>
<span class="p_add">+ * @src: array of hmm_pfn_t containing source pfns</span>
<span class="p_add">+ * @dst: array of hmm_pfn_t containing destination pfns</span>
<span class="p_add">+ * @private: pointer passed back to each of the callback</span>
<span class="p_add">+ * Returns: 0 on success, error code otherwise</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This will try to migrate a range of memory using callback to allocate and</span>
<span class="p_add">+ * copy memory from source to destination. This function will first collect,</span>
<span class="p_add">+ * lock and unmap pages in the range and then call alloc_and_copy() callback</span>
<span class="p_add">+ * for device driver to allocate destination memory and copy from source.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Then it will proceed and try to effectively migrate the page (struct page</span>
<span class="p_add">+ * metadata) a step that can fail for various reasons. Before updating CPU page</span>
<span class="p_add">+ * table it will call finalize_and_map() callback so that device driver can</span>
<span class="p_add">+ * inspect what have been successfully migrated and update its own page table</span>
<span class="p_add">+ * (this latter aspect is not mandatory and only make sense for some user of</span>
<span class="p_add">+ * this API).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Finaly the function update CPU page table and unlock the pages before</span>
<span class="p_add">+ * returning 0.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * It will return an error code only if one of the argument is invalid.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int migrate_vma(const struct migrate_vma_ops *ops,</span>
<span class="p_add">+		struct vm_area_struct *vma,</span>
<span class="p_add">+		unsigned long mentries,</span>
<span class="p_add">+		unsigned long start,</span>
<span class="p_add">+		unsigned long end,</span>
<span class="p_add">+		unsigned long *src,</span>
<span class="p_add">+		unsigned long *dst,</span>
<span class="p_add">+		void *private)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct migrate_vma migrate;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Sanity check the arguments */</span>
<span class="p_add">+	start &amp;= PAGE_MASK;</span>
<span class="p_add">+	end &amp;= PAGE_MASK;</span>
<span class="p_add">+	if (is_vm_hugetlb_page(vma) || (vma-&gt;vm_flags &amp; VM_SPECIAL))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (!vma || !ops || !src || !dst || start &gt;= end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (start &lt; vma-&gt;vm_start || start &gt;= vma-&gt;vm_end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (end &lt;= vma-&gt;vm_start || end &gt; vma-&gt;vm_end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	memset(src, 0, sizeof(*src) * ((end - start) &gt;&gt; PAGE_SHIFT));</span>
<span class="p_add">+	migrate.src = src;</span>
<span class="p_add">+	migrate.dst = dst;</span>
<span class="p_add">+	migrate.start = start;</span>
<span class="p_add">+	migrate.npages = 0;</span>
<span class="p_add">+	migrate.cpages = 0;</span>
<span class="p_add">+	migrate.mpages = mentries;</span>
<span class="p_add">+	migrate.end = end;</span>
<span class="p_add">+	migrate.vma = vma;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Collect, and try to unmap source pages */</span>
<span class="p_add">+	migrate_vma_collect(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.cpages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Lock and isolate page */</span>
<span class="p_add">+	migrate_vma_prepare(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.cpages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unmap pages */</span>
<span class="p_add">+	migrate_vma_unmap(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.cpages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At this point pages are lock and unmap and thus they have stable</span>
<span class="p_add">+	 * content and can safely be copied to destination memory that is</span>
<span class="p_add">+	 * allocated by the callback.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that migration can fail in migrate_vma_struct_page() for each</span>
<span class="p_add">+	 * individual page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ops-&gt;alloc_and_copy(vma, src, dst, start, end, private);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* This does the real migration of struct page */</span>
<span class="p_add">+	migrate_vma_pages(&amp;migrate);</span>
<span class="p_add">+</span>
<span class="p_add">+	ops-&gt;finalize_and_map(vma, src, dst, start, end, private);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unlock and remap pages */</span>
<span class="p_add">+	migrate_vma_finalize(&amp;migrate);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(migrate_vma);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



