
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V2,3/3] vhost: device IOTLB API - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V2,3/3] vhost: device IOTLB API</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2154">Jason Wang</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 23, 2016, 6:04 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1466661872-29165-4-git-send-email-jasowang@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9194565/mbox/"
   >mbox</a>
|
   <a href="/patch/9194565/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9194565/">/patch/9194565/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	DD913608A0 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 23 Jun 2016 06:05:13 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id CCC1E2842D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 23 Jun 2016 06:05:13 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id C0EC92843C; Thu, 23 Jun 2016 06:05:13 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 41FA42843A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 23 Jun 2016 06:05:12 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752104AbcFWGEv (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 23 Jun 2016 02:04:51 -0400
Received: from mx1.redhat.com ([209.132.183.28]:33384 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751255AbcFWGEr (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 23 Jun 2016 02:04:47 -0400
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 1685783F45;
	Thu, 23 Jun 2016 06:04:47 +0000 (UTC)
Received: from hp-dl380pg8-02.lab.eng.pek2.redhat.com
	(hp-dl380pg8-02.lab.eng.pek2.redhat.com [10.73.8.12])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id u5N64YPA012562; Thu, 23 Jun 2016 02:04:44 -0400
From: Jason Wang &lt;jasowang@redhat.com&gt;
To: mst@redhat.com, kvm@vger.kernel.org,
	virtualization@lists.linux-foundation.org, netdev@vger.kernel.org,
	linux-kernel@vger.kernel.org
Cc: peterx@redhat.com, vkaplans@redhat.com, wexu@redhat.com,
	Jason Wang &lt;jasowang@redhat.com&gt;
Subject: [PATCH V2 3/3] vhost: device IOTLB API
Date: Thu, 23 Jun 2016 02:04:32 -0400
Message-Id: &lt;1466661872-29165-4-git-send-email-jasowang@redhat.com&gt;
In-Reply-To: &lt;1466661872-29165-1-git-send-email-jasowang@redhat.com&gt;
References: &lt;1466661872-29165-1-git-send-email-jasowang@redhat.com&gt;
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.27]);
	Thu, 23 Jun 2016 06:04:47 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2154">Jason Wang</a> - June 23, 2016, 6:04 a.m.</div>
<pre class="content">
This patch tries to implement an device IOTLB for vhost. This could be
used with for co-operation with userspace(qemu) implementation of DMA
remapping.

The idea is simple, cache the translation in a software device IOTLB
(which was implemented as interval tree) in vhost and use vhost_net
file descriptor for reporting IOTLB miss and IOTLB
update/invalidation. When vhost meets an IOTLB miss, the fault
address, size and access could be read from the file. After userspace
finishes the translation, it write the translated address to the
vhost_net file to update the device IOTLB.

When device IOTLB (VHOST_F_DEVICE_IOTLB) is enabled all vq address
set by ioctl were treated as iova instead of virtual address and the
accessing could only be done through IOTLB instead of direct
userspace memory access. Before each rounds or vq processing, all vq
metadata were prefetched in device IOTLB to make sure no translation
fault happens during vq processing.

In most cases, virtqueue were mapped contiguous even in virtual
address. So the IOTLB translation for virtqueue itself maybe a little
bit slower. We can add fast path on top of this patch.
<span class="signed-off-by">
Signed-off-by: Jason Wang &lt;jasowang@redhat.com&gt;</span>
---
 drivers/vhost/net.c        |  50 +++-
 drivers/vhost/vhost.c      | 634 ++++++++++++++++++++++++++++++++++++++++++---
 drivers/vhost/vhost.h      |  32 ++-
 include/uapi/linux/vhost.h |  28 ++
 4 files changed, 697 insertions(+), 47 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c</span>
<span class="p_header">index fc66956..4ccebad 100644</span>
<span class="p_header">--- a/drivers/vhost/net.c</span>
<span class="p_header">+++ b/drivers/vhost/net.c</span>
<span class="p_chunk">@@ -61,7 +61,8 @@</span> <span class="p_context"> MODULE_PARM_DESC(experimental_zcopytx, &quot;Enable Zero Copy TX;&quot;</span>
 enum {
 	VHOST_NET_FEATURES = VHOST_FEATURES |
 			 (1ULL &lt;&lt; VHOST_NET_F_VIRTIO_NET_HDR) |
<span class="p_del">-			 (1ULL &lt;&lt; VIRTIO_NET_F_MRG_RXBUF)</span>
<span class="p_add">+			 (1ULL &lt;&lt; VIRTIO_NET_F_MRG_RXBUF) |</span>
<span class="p_add">+			 (1ULL &lt;&lt; VHOST_F_DEVICE_IOTLB)</span>
 };
 
 enum {
<span class="p_chunk">@@ -334,7 +335,7 @@</span> <span class="p_context"> static int vhost_net_tx_get_vq_desc(struct vhost_net *net,</span>
 {
 	unsigned long uninitialized_var(endtime);
 	int r = vhost_get_vq_desc(vq, vq-&gt;iov, ARRAY_SIZE(vq-&gt;iov),
<span class="p_del">-				    out_num, in_num, NULL, NULL);</span>
<span class="p_add">+				  out_num, in_num, NULL, NULL);</span>
 
 	if (r == vq-&gt;num &amp;&amp; vq-&gt;busyloop_timeout) {
 		preempt_disable();
<span class="p_chunk">@@ -344,7 +345,7 @@</span> <span class="p_context"> static int vhost_net_tx_get_vq_desc(struct vhost_net *net,</span>
 			cpu_relax_lowlatency();
 		preempt_enable();
 		r = vhost_get_vq_desc(vq, vq-&gt;iov, ARRAY_SIZE(vq-&gt;iov),
<span class="p_del">-					out_num, in_num, NULL, NULL);</span>
<span class="p_add">+				      out_num, in_num, NULL, NULL);</span>
 	}
 
 	return r;
<span class="p_chunk">@@ -377,6 +378,9 @@</span> <span class="p_context"> static void handle_tx(struct vhost_net *net)</span>
 	if (!sock)
 		goto out;
 
<span class="p_add">+	if (!vq_iotlb_prefetch(vq))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
 	vhost_disable_notify(&amp;net-&gt;dev, vq);
 
 	hdr_size = nvq-&gt;vhost_hlen;
<span class="p_chunk">@@ -638,6 +642,10 @@</span> <span class="p_context"> static void handle_rx(struct vhost_net *net)</span>
 	sock = vq-&gt;private_data;
 	if (!sock)
 		goto out;
<span class="p_add">+</span>
<span class="p_add">+	if (!vq_iotlb_prefetch(vq))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
 	vhost_disable_notify(&amp;net-&gt;dev, vq);
 	vhost_net_disable_vq(net, vq);
 
<span class="p_chunk">@@ -1086,6 +1094,11 @@</span> <span class="p_context"> static int vhost_net_set_features(struct vhost_net *n, u64 features)</span>
 		mutex_unlock(&amp;n-&gt;dev.mutex);
 		return -EFAULT;
 	}
<span class="p_add">+	if ((features &amp; (1ULL &lt;&lt; VHOST_F_DEVICE_IOTLB))) {</span>
<span class="p_add">+		if (vhost_init_device_iotlb(&amp;n-&gt;dev, true))</span>
<span class="p_add">+			return -EFAULT;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	for (i = 0; i &lt; VHOST_NET_VQ_MAX; ++i) {
 		mutex_lock(&amp;n-&gt;vqs[i].vq.mutex);
 		n-&gt;vqs[i].vq.acked_features = features;
<span class="p_chunk">@@ -1168,9 +1181,40 @@</span> <span class="p_context"> static long vhost_net_compat_ioctl(struct file *f, unsigned int ioctl,</span>
 }
 #endif
 
<span class="p_add">+static ssize_t vhost_net_chr_read_iter(struct kiocb *iocb, struct iov_iter *to)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct file *file = iocb-&gt;ki_filp;</span>
<span class="p_add">+	struct vhost_net *n = file-&gt;private_data;</span>
<span class="p_add">+	struct vhost_dev *dev = &amp;n-&gt;dev;</span>
<span class="p_add">+	int noblock = file-&gt;f_flags &amp; O_NONBLOCK;</span>
<span class="p_add">+</span>
<span class="p_add">+	return vhost_chr_read_iter(dev, to, noblock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static ssize_t vhost_net_chr_write_iter(struct kiocb *iocb,</span>
<span class="p_add">+					struct iov_iter *from)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct file *file = iocb-&gt;ki_filp;</span>
<span class="p_add">+	struct vhost_net *n = file-&gt;private_data;</span>
<span class="p_add">+	struct vhost_dev *dev = &amp;n-&gt;dev;</span>
<span class="p_add">+</span>
<span class="p_add">+	return vhost_chr_write_iter(dev, from);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned int vhost_net_chr_poll(struct file *file, poll_table *wait)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_net *n = file-&gt;private_data;</span>
<span class="p_add">+	struct vhost_dev *dev = &amp;n-&gt;dev;</span>
<span class="p_add">+</span>
<span class="p_add">+	return vhost_chr_poll(file, dev, wait);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static const struct file_operations vhost_net_fops = {
 	.owner          = THIS_MODULE,
 	.release        = vhost_net_release,
<span class="p_add">+	.read_iter      = vhost_net_chr_read_iter,</span>
<span class="p_add">+	.write_iter     = vhost_net_chr_write_iter,</span>
<span class="p_add">+	.poll           = vhost_net_chr_poll,</span>
 	.unlocked_ioctl = vhost_net_ioctl,
 #ifdef CONFIG_COMPAT
 	.compat_ioctl   = vhost_net_compat_ioctl,
<span class="p_header">diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="p_header">index 166e779..11d2f55 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.c</span>
<span class="p_header">+++ b/drivers/vhost/vhost.c</span>
<span class="p_chunk">@@ -35,6 +35,10 @@</span> <span class="p_context"> static ushort max_mem_regions = 64;</span>
 module_param(max_mem_regions, ushort, 0444);
 MODULE_PARM_DESC(max_mem_regions,
 	&quot;Maximum number of memory regions in memory map. (default: 64)&quot;);
<span class="p_add">+static int max_iotlb_entries = 2048;</span>
<span class="p_add">+module_param(max_iotlb_entries, int, 0444);</span>
<span class="p_add">+MODULE_PARM_DESC(max_iotlb_entries,</span>
<span class="p_add">+	&quot;Maximum number of iotlb entries. (default: 2048)&quot;);</span>
 
 enum {
 	VHOST_MEMORY_F_LOG = 0x1,
<span class="p_chunk">@@ -309,6 +313,7 @@</span> <span class="p_context"> static void vhost_vq_reset(struct vhost_dev *dev,</span>
 	vhost_disable_cross_endian(vq);
 	vq-&gt;busyloop_timeout = 0;
 	vq-&gt;umem = NULL;
<span class="p_add">+	vq-&gt;iotlb = NULL;</span>
 }
 
 static int vhost_worker(void *data)
<span class="p_chunk">@@ -413,9 +418,14 @@</span> <span class="p_context"> void vhost_dev_init(struct vhost_dev *dev,</span>
 	dev-&gt;log_ctx = NULL;
 	dev-&gt;log_file = NULL;
 	dev-&gt;umem = NULL;
<span class="p_add">+	dev-&gt;iotlb = NULL;</span>
 	dev-&gt;mm = NULL;
 	spin_lock_init(&amp;dev-&gt;work_lock);
 	INIT_LIST_HEAD(&amp;dev-&gt;work_list);
<span class="p_add">+	init_waitqueue_head(&amp;dev-&gt;wait);</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;dev-&gt;read_list);</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;dev-&gt;pending_list);</span>
<span class="p_add">+	spin_lock_init(&amp;dev-&gt;iotlb_lock);</span>
 	dev-&gt;worker = NULL;
 
 	for (i = 0; i &lt; dev-&gt;nvqs; ++i) {
<span class="p_chunk">@@ -563,6 +573,15 @@</span> <span class="p_context"> void vhost_dev_stop(struct vhost_dev *dev)</span>
 }
 EXPORT_SYMBOL_GPL(vhost_dev_stop);
 
<span class="p_add">+static void vhost_umem_free(struct vhost_umem *umem,</span>
<span class="p_add">+			    struct vhost_umem_node *node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	vhost_umem_interval_tree_remove(node, &amp;umem-&gt;umem_tree);</span>
<span class="p_add">+	list_del(&amp;node-&gt;link);</span>
<span class="p_add">+	kfree(node);</span>
<span class="p_add">+	umem-&gt;numem--;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void vhost_umem_clean(struct vhost_umem *umem)
 {
 	struct vhost_umem_node *node, *tmp;
<span class="p_chunk">@@ -570,14 +589,31 @@</span> <span class="p_context"> static void vhost_umem_clean(struct vhost_umem *umem)</span>
 	if (!umem)
 		return;
 
<span class="p_del">-	list_for_each_entry_safe(node, tmp, &amp;umem-&gt;umem_list, link) {</span>
<span class="p_del">-		vhost_umem_interval_tree_remove(node, &amp;umem-&gt;umem_tree);</span>
<span class="p_del">-		list_del(&amp;node-&gt;link);</span>
<span class="p_del">-		kvfree(node);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	list_for_each_entry_safe(node, tmp, &amp;umem-&gt;umem_list, link)</span>
<span class="p_add">+		vhost_umem_free(umem, node);</span>
<span class="p_add">+</span>
 	kvfree(umem);
 }
 
<span class="p_add">+static void vhost_clear_msg(struct vhost_dev *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_msg_node *node, *n;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;dev-&gt;iotlb_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry_safe(node, n, &amp;dev-&gt;read_list, node) {</span>
<span class="p_add">+		list_del(&amp;node-&gt;node);</span>
<span class="p_add">+		kfree(node);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry_safe(node, n, &amp;dev-&gt;pending_list, node) {</span>
<span class="p_add">+		list_del(&amp;node-&gt;node);</span>
<span class="p_add">+		kfree(node);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock(&amp;dev-&gt;iotlb_lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Caller should have device mutex if and only if locked is set */
 void vhost_dev_cleanup(struct vhost_dev *dev, bool locked)
 {
<span class="p_chunk">@@ -606,6 +642,10 @@</span> <span class="p_context"> void vhost_dev_cleanup(struct vhost_dev *dev, bool locked)</span>
 	/* No one will access memory at this point */
 	vhost_umem_clean(dev-&gt;umem);
 	dev-&gt;umem = NULL;
<span class="p_add">+	vhost_umem_clean(dev-&gt;iotlb);</span>
<span class="p_add">+	dev-&gt;iotlb = NULL;</span>
<span class="p_add">+	vhost_clear_msg(dev);</span>
<span class="p_add">+	wake_up_interruptible_poll(&amp;dev-&gt;wait, POLLIN | POLLRDNORM);</span>
 	WARN_ON(!list_empty(&amp;dev-&gt;work_list));
 	if (dev-&gt;worker) {
 		kthread_stop(dev-&gt;worker);
<span class="p_chunk">@@ -681,28 +721,379 @@</span> <span class="p_context"> static int memory_access_ok(struct vhost_dev *d, struct vhost_umem *umem,</span>
 	return 1;
 }
 
<span class="p_del">-#define vhost_put_user(vq, x, ptr)  __put_user(x, ptr)</span>
<span class="p_add">+static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,</span>
<span class="p_add">+			  struct iovec iov[], int iov_size, int access);</span>
 
 static int vhost_copy_to_user(struct vhost_virtqueue *vq, void *to,
 			      const void *from, unsigned size)
 {
<span class="p_del">-	return __copy_to_user(to, from, size);</span>
<span class="p_del">-}</span>
<span class="p_add">+	int ret;</span>
 
<span class="p_del">-#define vhost_get_user(vq, x, ptr) __get_user(x, ptr)</span>
<span class="p_add">+	if (!vq-&gt;iotlb)</span>
<span class="p_add">+		return __copy_to_user(to, from, size);</span>
<span class="p_add">+	else {</span>
<span class="p_add">+		/* This function should be called after iotlb</span>
<span class="p_add">+		 * prefetch, which means we&#39;re sure that all vq</span>
<span class="p_add">+		 * could be access through iotlb. So -EAGAIN should</span>
<span class="p_add">+		 * not happen in this case.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		/* TODO: more fast path */</span>
<span class="p_add">+		struct iov_iter t;</span>
<span class="p_add">+		ret = translate_desc(vq, (u64)(uintptr_t)to, size, vq-&gt;iotlb_iov,</span>
<span class="p_add">+				     ARRAY_SIZE(vq-&gt;iotlb_iov),</span>
<span class="p_add">+				     VHOST_ACCESS_WO);</span>
<span class="p_add">+		if (ret &lt; 0)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		iov_iter_init(&amp;t, WRITE, vq-&gt;iotlb_iov, ret, size);</span>
<span class="p_add">+		ret = copy_to_iter(from, size, &amp;t);</span>
<span class="p_add">+		if (ret == size)</span>
<span class="p_add">+			ret = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
 
 static int vhost_copy_from_user(struct vhost_virtqueue *vq, void *to,
 				void *from, unsigned size)
 {
<span class="p_del">-	return __copy_from_user(to, from, size);</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!vq-&gt;iotlb)</span>
<span class="p_add">+		return __copy_from_user(to, from, size);</span>
<span class="p_add">+	else {</span>
<span class="p_add">+		/* This function should be called after iotlb</span>
<span class="p_add">+		 * prefetch, which means we&#39;re sure that vq</span>
<span class="p_add">+		 * could be access through iotlb. So -EAGAIN should</span>
<span class="p_add">+		 * not happen in this case.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		/* TODO: more fast path */</span>
<span class="p_add">+		struct iov_iter f;</span>
<span class="p_add">+		ret = translate_desc(vq, (u64)(uintptr_t)from, size, vq-&gt;iotlb_iov,</span>
<span class="p_add">+				     ARRAY_SIZE(vq-&gt;iotlb_iov),</span>
<span class="p_add">+				     VHOST_ACCESS_RO);</span>
<span class="p_add">+		if (ret &lt; 0) {</span>
<span class="p_add">+			vq_err(vq, &quot;IOTLB translation failure: uaddr &quot;</span>
<span class="p_add">+			       &quot;%p size 0x%llx\n&quot;, from,</span>
<span class="p_add">+			       (unsigned long long) size);</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		iov_iter_init(&amp;f, READ, vq-&gt;iotlb_iov, ret, size);</span>
<span class="p_add">+		ret = copy_from_iter(to, size, &amp;f);</span>
<span class="p_add">+		if (ret == size)</span>
<span class="p_add">+			ret = 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __user *__vhost_get_user(struct vhost_virtqueue *vq,</span>
<span class="p_add">+				     void *addr, unsigned size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* This function should be called after iotlb</span>
<span class="p_add">+	 * prefetch, which means we&#39;re sure that vq</span>
<span class="p_add">+	 * could be access through iotlb. So -EAGAIN should</span>
<span class="p_add">+	 * not happen in this case.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	/* TODO: more fast path */</span>
<span class="p_add">+	ret = translate_desc(vq, (u64)(uintptr_t)addr, size, vq-&gt;iotlb_iov,</span>
<span class="p_add">+			     ARRAY_SIZE(vq-&gt;iotlb_iov),</span>
<span class="p_add">+			     VHOST_ACCESS_RO);</span>
<span class="p_add">+	if (ret &lt; 0) {</span>
<span class="p_add">+		vq_err(vq, &quot;IOTLB translation failure: uaddr &quot;</span>
<span class="p_add">+			&quot;%p size 0x%llx\n&quot;, addr,</span>
<span class="p_add">+			(unsigned long long) size);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ret != 1 || vq-&gt;iotlb_iov[0].iov_len != size) {</span>
<span class="p_add">+		vq_err(vq, &quot;Non atomic userspace memory access: uaddr &quot;</span>
<span class="p_add">+			&quot;%p size 0x%llx\n&quot;, addr,</span>
<span class="p_add">+			(unsigned long long) size);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return vq-&gt;iotlb_iov[0].iov_base;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define vhost_put_user(vq, x, ptr) \</span>
<span class="p_add">+({ \</span>
<span class="p_add">+	int ret = -EFAULT; \</span>
<span class="p_add">+	if (!vq-&gt;iotlb) { \</span>
<span class="p_add">+		ret = __put_user(x, ptr); \</span>
<span class="p_add">+	} else { \</span>
<span class="p_add">+		__typeof__(ptr) to = \</span>
<span class="p_add">+			(__typeof__(ptr)) __vhost_get_user(vq, ptr, sizeof(*ptr)); \</span>
<span class="p_add">+		if (to != NULL) \</span>
<span class="p_add">+			ret = __put_user(x, to); \</span>
<span class="p_add">+		else \</span>
<span class="p_add">+			ret = -EFAULT;	\</span>
<span class="p_add">+	} \</span>
<span class="p_add">+	ret; \</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define vhost_get_user(vq, x, ptr) \</span>
<span class="p_add">+({ \</span>
<span class="p_add">+	int ret; \</span>
<span class="p_add">+	if (!vq-&gt;iotlb) { \</span>
<span class="p_add">+		ret = __get_user(x, ptr); \</span>
<span class="p_add">+	} else { \</span>
<span class="p_add">+		__typeof__(ptr) from = \</span>
<span class="p_add">+			(__typeof__(ptr)) __vhost_get_user(vq, ptr, sizeof(*ptr)); \</span>
<span class="p_add">+		if (from != NULL) \</span>
<span class="p_add">+			ret = __get_user(x, from); \</span>
<span class="p_add">+		else \</span>
<span class="p_add">+			ret = -EFAULT; \</span>
<span class="p_add">+	} \</span>
<span class="p_add">+	ret; \</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+static void vhost_dev_lock_vqs(struct vhost_dev *d)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i = 0;</span>
<span class="p_add">+	for (i = 0; i &lt; d-&gt;nvqs; ++i)</span>
<span class="p_add">+		mutex_lock(&amp;d-&gt;vqs[i]-&gt;mutex);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void vhost_dev_unlock_vqs(struct vhost_dev *d)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i = 0;</span>
<span class="p_add">+	for (i = 0; i &lt; d-&gt;nvqs; ++i)</span>
<span class="p_add">+		mutex_unlock(&amp;d-&gt;vqs[i]-&gt;mutex);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int vhost_new_umem_range(struct vhost_umem *umem,</span>
<span class="p_add">+				u64 start, u64 size, u64 end,</span>
<span class="p_add">+				u64 userspace_addr, int perm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_umem_node *tmp, *node = kmalloc(sizeof(*node), GFP_ATOMIC);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!node)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (umem-&gt;numem == max_iotlb_entries) {</span>
<span class="p_add">+		tmp = list_first_entry(&amp;umem-&gt;umem_list, typeof(*tmp), link);</span>
<span class="p_add">+		vhost_umem_free(umem, tmp);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	node-&gt;start = start;</span>
<span class="p_add">+	node-&gt;size = size;</span>
<span class="p_add">+	node-&gt;last = end;</span>
<span class="p_add">+	node-&gt;userspace_addr = userspace_addr;</span>
<span class="p_add">+	node-&gt;perm = perm;</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;node-&gt;link);</span>
<span class="p_add">+	list_add_tail(&amp;node-&gt;link, &amp;umem-&gt;umem_list);</span>
<span class="p_add">+	vhost_umem_interval_tree_insert(node, &amp;umem-&gt;umem_tree);</span>
<span class="p_add">+	umem-&gt;numem++;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void vhost_del_umem_range(struct vhost_umem *umem,</span>
<span class="p_add">+				 u64 start, u64 end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_umem_node *node;</span>
<span class="p_add">+</span>
<span class="p_add">+	while ((node = vhost_umem_interval_tree_iter_first(&amp;umem-&gt;umem_tree,</span>
<span class="p_add">+							   start, end)))</span>
<span class="p_add">+		vhost_umem_free(umem, node);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void vhost_iotlb_notify_vq(struct vhost_dev *d,</span>
<span class="p_add">+				  struct vhost_iotlb_msg *msg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_msg_node *node, *n;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;d-&gt;iotlb_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry_safe(node, n, &amp;d-&gt;pending_list, node) {</span>
<span class="p_add">+		struct vhost_iotlb_msg *vq_msg = &amp;node-&gt;msg.iotlb;</span>
<span class="p_add">+		if (msg-&gt;iova &lt;= vq_msg-&gt;iova &amp;&amp;</span>
<span class="p_add">+		    msg-&gt;iova + msg-&gt;size - 1 &gt; vq_msg-&gt;iova &amp;&amp;</span>
<span class="p_add">+		    vq_msg-&gt;type == VHOST_IOTLB_MISS) {</span>
<span class="p_add">+			vhost_poll_queue(&amp;node-&gt;vq-&gt;poll);</span>
<span class="p_add">+			list_del(&amp;node-&gt;node);</span>
<span class="p_add">+			kfree(node);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock(&amp;d-&gt;iotlb_lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int umem_access_ok(u64 uaddr, u64 size, int access)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if ((access &amp; VHOST_ACCESS_RO) &amp;&amp;</span>
<span class="p_add">+	    !access_ok(VERIFY_READ, uaddr, size))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	if ((access &amp; VHOST_ACCESS_WO) &amp;&amp;</span>
<span class="p_add">+	    !access_ok(VERIFY_WRITE, uaddr, size))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int vhost_process_iotlb_msg(struct vhost_dev *dev,</span>
<span class="p_add">+			    struct vhost_iotlb_msg *msg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	vhost_dev_lock_vqs(dev);</span>
<span class="p_add">+	switch (msg-&gt;type) {</span>
<span class="p_add">+	case VHOST_IOTLB_UPDATE:</span>
<span class="p_add">+		if (!dev-&gt;iotlb) {</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (umem_access_ok(msg-&gt;uaddr, msg-&gt;size, msg-&gt;perm)) {</span>
<span class="p_add">+			ret = -EFAULT;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (vhost_new_umem_range(dev-&gt;iotlb, msg-&gt;iova, msg-&gt;size,</span>
<span class="p_add">+					 msg-&gt;iova + msg-&gt;size - 1,</span>
<span class="p_add">+					 msg-&gt;uaddr, msg-&gt;perm)) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		vhost_iotlb_notify_vq(dev, msg);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case VHOST_IOTLB_INVALIDATE:</span>
<span class="p_add">+		vhost_del_umem_range(dev-&gt;iotlb, msg-&gt;iova,</span>
<span class="p_add">+				     msg-&gt;iova + msg-&gt;size - 1);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	vhost_dev_unlock_vqs(dev);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+ssize_t vhost_chr_write_iter(struct vhost_dev *dev,</span>
<span class="p_add">+			     struct iov_iter *from)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_msg_node node;</span>
<span class="p_add">+	unsigned size = sizeof(struct vhost_msg);</span>
<span class="p_add">+	size_t ret;</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (iov_iter_count(from) &lt; size)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	ret = copy_from_iter(&amp;node.msg, size, from);</span>
<span class="p_add">+	if (ret != size)</span>
<span class="p_add">+		goto done;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (node.msg.type) {</span>
<span class="p_add">+	case VHOST_IOTLB_MSG:</span>
<span class="p_add">+		err = vhost_process_iotlb_msg(dev, &amp;node.msg.iotlb);</span>
<span class="p_add">+		if (err)</span>
<span class="p_add">+			ret = err;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ret = -EINVAL;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+done:</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(vhost_chr_write_iter);</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,</span>
<span class="p_add">+			    poll_table *wait)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int mask = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	poll_wait(file, &amp;dev-&gt;wait, wait);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!list_empty(&amp;dev-&gt;read_list))</span>
<span class="p_add">+		mask |= POLLIN | POLLRDNORM;</span>
<span class="p_add">+</span>
<span class="p_add">+	return mask;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(vhost_chr_poll);</span>
<span class="p_add">+</span>
<span class="p_add">+ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,</span>
<span class="p_add">+			    int noblock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	DEFINE_WAIT(wait);</span>
<span class="p_add">+	struct vhost_msg_node *node;</span>
<span class="p_add">+	ssize_t ret = 0;</span>
<span class="p_add">+	unsigned size = sizeof(struct vhost_msg);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (iov_iter_count(to) &lt; size)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (1) {</span>
<span class="p_add">+		if (!noblock)</span>
<span class="p_add">+			prepare_to_wait(&amp;dev-&gt;wait, &amp;wait,</span>
<span class="p_add">+					TASK_INTERRUPTIBLE);</span>
<span class="p_add">+</span>
<span class="p_add">+		node = vhost_dequeue_msg(dev, &amp;dev-&gt;read_list);</span>
<span class="p_add">+		if (node)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		if (noblock) {</span>
<span class="p_add">+			ret = -EAGAIN;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (signal_pending(current)) {</span>
<span class="p_add">+			ret = -ERESTARTSYS;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (!dev-&gt;iotlb) {</span>
<span class="p_add">+			ret = -EBADFD;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		schedule();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!noblock)</span>
<span class="p_add">+		finish_wait(&amp;dev-&gt;wait, &amp;wait);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (node) {</span>
<span class="p_add">+		ret = copy_to_iter(&amp;node-&gt;msg, size, to);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ret != size || node-&gt;msg.type != VHOST_IOTLB_MISS) {</span>
<span class="p_add">+			kfree(node);</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		vhost_enqueue_msg(dev, &amp;dev-&gt;pending_list, node);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(vhost_chr_read_iter);</span>
<span class="p_add">+</span>
<span class="p_add">+static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova, int access)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_dev *dev = vq-&gt;dev;</span>
<span class="p_add">+	struct vhost_msg_node *node;</span>
<span class="p_add">+	struct vhost_iotlb_msg *msg;</span>
<span class="p_add">+</span>
<span class="p_add">+	node = vhost_new_msg(vq, VHOST_IOTLB_MISS);</span>
<span class="p_add">+	if (!node)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	msg = &amp;node-&gt;msg.iotlb;</span>
<span class="p_add">+	msg-&gt;type = VHOST_IOTLB_MISS;</span>
<span class="p_add">+	msg-&gt;iova = iova;</span>
<span class="p_add">+	msg-&gt;perm = access;</span>
<span class="p_add">+</span>
<span class="p_add">+	vhost_enqueue_msg(dev, &amp;dev-&gt;read_list, node);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
 }
 
 static int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,
 			struct vring_desc __user *desc,
 			struct vring_avail __user *avail,
 			struct vring_used __user *used)
<span class="p_add">+</span>
 {
 	size_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;
<span class="p_add">+</span>
 	return access_ok(VERIFY_READ, desc, num * sizeof *desc) &amp;&amp;
 	       access_ok(VERIFY_READ, avail,
 			 sizeof *avail + num * sizeof *avail-&gt;ring + s) &amp;&amp;
<span class="p_chunk">@@ -710,6 +1101,54 @@</span> <span class="p_context"> static int vq_access_ok(struct vhost_virtqueue *vq, unsigned int num,</span>
 			sizeof *used + num * sizeof *used-&gt;ring + s);
 }
 
<span class="p_add">+static int iotlb_access_ok(struct vhost_virtqueue *vq,</span>
<span class="p_add">+			   int access, u64 addr, u64 len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const struct vhost_umem_node *node;</span>
<span class="p_add">+	struct vhost_umem *umem = vq-&gt;iotlb;</span>
<span class="p_add">+	u64 s = 0, size;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (len &gt; s) {</span>
<span class="p_add">+		node = vhost_umem_interval_tree_iter_first(&amp;umem-&gt;umem_tree,</span>
<span class="p_add">+							   addr,</span>
<span class="p_add">+							   addr + len - 1);</span>
<span class="p_add">+		if (node == NULL || node-&gt;start &gt; addr) {</span>
<span class="p_add">+			vhost_iotlb_miss(vq, addr, access);</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		} else if (!(node-&gt;perm &amp; access)) {</span>
<span class="p_add">+			/* Report the possible access violation by</span>
<span class="p_add">+			 * request another translation from userspace.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		size = node-&gt;size - addr + node-&gt;start;</span>
<span class="p_add">+		s += size;</span>
<span class="p_add">+		addr += size;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int vq_iotlb_prefetch(struct vhost_virtqueue *vq)</span>
<span class="p_add">+{</span>
<span class="p_add">+	size_t s = vhost_has_feature(vq, VIRTIO_RING_F_EVENT_IDX) ? 2 : 0;</span>
<span class="p_add">+	unsigned int num = vq-&gt;num;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!vq-&gt;iotlb)</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	return iotlb_access_ok(vq, VHOST_ACCESS_RO, (u64)(uintptr_t)vq-&gt;desc,</span>
<span class="p_add">+			       num * sizeof *vq-&gt;desc) &amp;&amp;</span>
<span class="p_add">+	       iotlb_access_ok(vq, VHOST_ACCESS_RO, (u64)(uintptr_t)vq-&gt;avail,</span>
<span class="p_add">+			       sizeof *vq-&gt;avail +</span>
<span class="p_add">+			       num * sizeof *vq-&gt;avail-&gt;ring + s) &amp;&amp;</span>
<span class="p_add">+	       iotlb_access_ok(vq, VHOST_ACCESS_WO, (u64)(uintptr_t)vq-&gt;used,</span>
<span class="p_add">+			       sizeof *vq-&gt;used +</span>
<span class="p_add">+			       num * sizeof *vq-&gt;used-&gt;ring + s);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(vq_iotlb_prefetch);</span>
<span class="p_add">+</span>
 /* Can we log writes? */
 /* Caller should have device mutex but not vq mutex */
 int vhost_log_access_ok(struct vhost_dev *dev)
<span class="p_chunk">@@ -736,16 +1175,35 @@</span> <span class="p_context"> static int vq_log_access_ok(struct vhost_virtqueue *vq,</span>
 /* Caller should have vq mutex and device mutex */
 int vhost_vq_access_ok(struct vhost_virtqueue *vq)
 {
<span class="p_add">+	if (vq-&gt;iotlb) {</span>
<span class="p_add">+		/* When device IOTLB was used, the access validation</span>
<span class="p_add">+		 * will be validated during prefetching.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
 	return vq_access_ok(vq, vq-&gt;num, vq-&gt;desc, vq-&gt;avail, vq-&gt;used) &amp;&amp;
 		vq_log_access_ok(vq, vq-&gt;log_base);
 }
 EXPORT_SYMBOL_GPL(vhost_vq_access_ok);
 
<span class="p_add">+static struct vhost_umem *vhost_umem_alloc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_umem *umem = vhost_kvzalloc(sizeof(*umem));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!umem)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	umem-&gt;umem_tree = RB_ROOT;</span>
<span class="p_add">+	umem-&gt;numem = 0;</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;umem-&gt;umem_list);</span>
<span class="p_add">+</span>
<span class="p_add">+	return umem;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 {
 	struct vhost_memory mem, *newmem;
 	struct vhost_memory_region *region;
<span class="p_del">-	struct vhost_umem_node *node;</span>
 	struct vhost_umem *newumem, *oldumem;
 	unsigned long size = offsetof(struct vhost_memory, regions);
 	int i;
<span class="p_chunk">@@ -767,28 +1225,23 @@</span> <span class="p_context"> static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)</span>
 		return -EFAULT;
 	}
 
<span class="p_del">-	newumem = vhost_kvzalloc(sizeof(*newumem));</span>
<span class="p_add">+	newumem = vhost_umem_alloc();</span>
 	if (!newumem) {
 		kvfree(newmem);
 		return -ENOMEM;
 	}
 
<span class="p_del">-	newumem-&gt;umem_tree = RB_ROOT;</span>
<span class="p_del">-	INIT_LIST_HEAD(&amp;newumem-&gt;umem_list);</span>
<span class="p_del">-</span>
 	for (region = newmem-&gt;regions;
 	     region &lt; newmem-&gt;regions + mem.nregions;
 	     region++) {
<span class="p_del">-		node = vhost_kvzalloc(sizeof(*node));</span>
<span class="p_del">-		if (!node)</span>
<span class="p_add">+		if (vhost_new_umem_range(newumem,</span>
<span class="p_add">+					 region-&gt;guest_phys_addr,</span>
<span class="p_add">+					 region-&gt;memory_size,</span>
<span class="p_add">+					 region-&gt;guest_phys_addr +</span>
<span class="p_add">+					 region-&gt;memory_size - 1,</span>
<span class="p_add">+					 region-&gt;userspace_addr,</span>
<span class="p_add">+					 VHOST_ACCESS_RW))</span>
 			goto err;
<span class="p_del">-		node-&gt;start = region-&gt;guest_phys_addr;</span>
<span class="p_del">-		node-&gt;size = region-&gt;memory_size;</span>
<span class="p_del">-		node-&gt;last = node-&gt;start + node-&gt;size - 1;</span>
<span class="p_del">-		node-&gt;userspace_addr = region-&gt;userspace_addr;</span>
<span class="p_del">-		INIT_LIST_HEAD(&amp;node-&gt;link);</span>
<span class="p_del">-		list_add_tail(&amp;node-&gt;link, &amp;newumem-&gt;umem_list);</span>
<span class="p_del">-		vhost_umem_interval_tree_insert(node, &amp;newumem-&gt;umem_tree);</span>
 	}
 
 	if (!memory_access_ok(d, newumem, 0))
<span class="p_chunk">@@ -1032,6 +1485,30 @@</span> <span class="p_context"> long vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)</span>
 }
 EXPORT_SYMBOL_GPL(vhost_vring_ioctl);
 
<span class="p_add">+int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_umem *niotlb, *oiotlb;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	niotlb = vhost_umem_alloc();</span>
<span class="p_add">+	if (!niotlb)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	oiotlb = d-&gt;iotlb;</span>
<span class="p_add">+	d-&gt;iotlb = niotlb;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; d-&gt;nvqs; ++i) {</span>
<span class="p_add">+		mutex_lock(&amp;d-&gt;vqs[i]-&gt;mutex);</span>
<span class="p_add">+		d-&gt;vqs[i]-&gt;iotlb = niotlb;</span>
<span class="p_add">+		mutex_unlock(&amp;d-&gt;vqs[i]-&gt;mutex);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	vhost_umem_clean(oiotlb);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(vhost_init_device_iotlb);</span>
<span class="p_add">+</span>
 /* Caller must have device mutex */
 long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 {
<span class="p_chunk">@@ -1246,15 +1723,20 @@</span> <span class="p_context"> int vhost_vq_init_access(struct vhost_virtqueue *vq)</span>
 	if (r)
 		goto err;
 	vq-&gt;signalled_used_valid = false;
<span class="p_del">-	if (!access_ok(VERIFY_READ, &amp;vq-&gt;used-&gt;idx, sizeof vq-&gt;used-&gt;idx)) {</span>
<span class="p_add">+	if (!vq-&gt;iotlb &amp;&amp;</span>
<span class="p_add">+	    !access_ok(VERIFY_READ, &amp;vq-&gt;used-&gt;idx, sizeof vq-&gt;used-&gt;idx)) {</span>
 		r = -EFAULT;
 		goto err;
 	}
 	r = vhost_get_user(vq, last_used_idx, &amp;vq-&gt;used-&gt;idx);
<span class="p_del">-	if (r)</span>
<span class="p_add">+	if (r) {</span>
<span class="p_add">+		vq_err(vq, &quot;Can&#39;t access used idx at %p\n&quot;,</span>
<span class="p_add">+		       &amp;vq-&gt;used-&gt;idx);</span>
 		goto err;
<span class="p_add">+	}</span>
 	vq-&gt;last_used_idx = vhost16_to_cpu(vq, last_used_idx);
 	return 0;
<span class="p_add">+</span>
 err:
 	vq-&gt;is_le = is_le;
 	return r;
<span class="p_chunk">@@ -1262,10 +1744,11 @@</span> <span class="p_context"> err:</span>
 EXPORT_SYMBOL_GPL(vhost_vq_init_access);
 
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
<span class="p_del">-			  struct iovec iov[], int iov_size)</span>
<span class="p_add">+			  struct iovec iov[], int iov_size, int access)</span>
 {
 	const struct vhost_umem_node *node;
<span class="p_del">-	struct vhost_umem *umem = vq-&gt;umem;</span>
<span class="p_add">+	struct vhost_dev *dev = vq-&gt;dev;</span>
<span class="p_add">+	struct vhost_umem *umem = dev-&gt;iotlb ? dev-&gt;iotlb : dev-&gt;umem;</span>
 	struct iovec *_iov;
 	u64 s = 0;
 	int ret = 0;
<span class="p_chunk">@@ -1276,12 +1759,21 @@</span> <span class="p_context"> static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,</span>
 			ret = -ENOBUFS;
 			break;
 		}
<span class="p_add">+</span>
 		node = vhost_umem_interval_tree_iter_first(&amp;umem-&gt;umem_tree,
 							addr, addr + len - 1);
 		if (node == NULL || node-&gt;start &gt; addr) {
<span class="p_del">-			ret = -EFAULT;</span>
<span class="p_add">+			if (umem != dev-&gt;iotlb) {</span>
<span class="p_add">+				ret = -EFAULT;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			ret = -EAGAIN;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		} else if (!(node-&gt;perm &amp; access)) {</span>
<span class="p_add">+			ret = -EPERM;</span>
 			break;
 		}
<span class="p_add">+</span>
 		_iov = iov + ret;
 		size = node-&gt;size - addr + node-&gt;start;
 		_iov-&gt;iov_len = min((u64)len - s, size);
<span class="p_chunk">@@ -1292,6 +1784,8 @@</span> <span class="p_context"> static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,</span>
 		++ret;
 	}
 
<span class="p_add">+	if (ret == -EAGAIN)</span>
<span class="p_add">+		vhost_iotlb_miss(vq, addr, access);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1326,7 +1820,7 @@</span> <span class="p_context"> static int get_indirect(struct vhost_virtqueue *vq,</span>
 	unsigned int i = 0, count, found = 0;
 	u32 len = vhost32_to_cpu(vq, indirect-&gt;len);
 	struct iov_iter from;
<span class="p_del">-	int ret;</span>
<span class="p_add">+	int ret, access;</span>
 
 	/* Sanity check */
 	if (unlikely(len % sizeof desc)) {
<span class="p_chunk">@@ -1338,9 +1832,10 @@</span> <span class="p_context"> static int get_indirect(struct vhost_virtqueue *vq,</span>
 	}
 
 	ret = translate_desc(vq, vhost64_to_cpu(vq, indirect-&gt;addr), len, vq-&gt;indirect,
<span class="p_del">-			     UIO_MAXIOV);</span>
<span class="p_add">+			     UIO_MAXIOV, VHOST_ACCESS_RO);</span>
 	if (unlikely(ret &lt; 0)) {
<span class="p_del">-		vq_err(vq, &quot;Translation failure %d in indirect.\n&quot;, ret);</span>
<span class="p_add">+		if (ret != -EAGAIN)</span>
<span class="p_add">+			vq_err(vq, &quot;Translation failure %d in indirect.\n&quot;, ret);</span>
 		return ret;
 	}
 	iov_iter_init(&amp;from, READ, vq-&gt;indirect, ret, len);
<span class="p_chunk">@@ -1378,16 +1873,22 @@</span> <span class="p_context"> static int get_indirect(struct vhost_virtqueue *vq,</span>
 			return -EINVAL;
 		}
 
<span class="p_add">+		if (desc.flags &amp; cpu_to_vhost16(vq, VRING_DESC_F_WRITE))</span>
<span class="p_add">+			access = VHOST_ACCESS_WO;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			access = VHOST_ACCESS_RO;</span>
<span class="p_add">+</span>
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
<span class="p_del">-				     iov_size - iov_count);</span>
<span class="p_add">+				     iov_size - iov_count, access);</span>
 		if (unlikely(ret &lt; 0)) {
<span class="p_del">-			vq_err(vq, &quot;Translation failure %d indirect idx %d\n&quot;,</span>
<span class="p_del">-			       ret, i);</span>
<span class="p_add">+			if (ret != -EAGAIN)</span>
<span class="p_add">+				vq_err(vq, &quot;Translation failure %d indirect idx %d\n&quot;,</span>
<span class="p_add">+					ret, i);</span>
 			return ret;
 		}
 		/* If this is an input descriptor, increment that count. */
<span class="p_del">-		if (desc.flags &amp; cpu_to_vhost16(vq, VRING_DESC_F_WRITE)) {</span>
<span class="p_add">+		if (access == VHOST_ACCESS_WO) {</span>
 			*in_num += ret;
 			if (unlikely(log)) {
 				log[*log_num].addr = vhost64_to_cpu(vq, desc.addr);
<span class="p_chunk">@@ -1426,7 +1927,7 @@</span> <span class="p_context"> int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
 	u16 last_avail_idx;
 	__virtio16 avail_idx;
 	__virtio16 ring_head;
<span class="p_del">-	int ret;</span>
<span class="p_add">+	int ret, access;</span>
 
 	/* Check it isn&#39;t doing very strange things with descriptor numbers. */
 	last_avail_idx = vq-&gt;last_avail_idx;
<span class="p_chunk">@@ -1500,22 +2001,28 @@</span> <span class="p_context"> int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
 					   out_num, in_num,
 					   log, log_num, &amp;desc);
 			if (unlikely(ret &lt; 0)) {
<span class="p_del">-				vq_err(vq, &quot;Failure detected &quot;</span>
<span class="p_del">-				       &quot;in indirect descriptor at idx %d\n&quot;, i);</span>
<span class="p_add">+				if (ret != -EAGAIN)</span>
<span class="p_add">+					vq_err(vq, &quot;Failure detected &quot;</span>
<span class="p_add">+						&quot;in indirect descriptor at idx %d\n&quot;, i);</span>
 				return ret;
 			}
 			continue;
 		}
 
<span class="p_add">+		if (desc.flags &amp; cpu_to_vhost16(vq, VRING_DESC_F_WRITE))</span>
<span class="p_add">+			access = VHOST_ACCESS_WO;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			access = VHOST_ACCESS_RO;</span>
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
<span class="p_del">-				     iov_size - iov_count);</span>
<span class="p_add">+				     iov_size - iov_count, access);</span>
 		if (unlikely(ret &lt; 0)) {
<span class="p_del">-			vq_err(vq, &quot;Translation failure %d descriptor idx %d\n&quot;,</span>
<span class="p_del">-			       ret, i);</span>
<span class="p_add">+			if (ret != -EAGAIN)</span>
<span class="p_add">+				vq_err(vq, &quot;Translation failure %d descriptor idx %d\n&quot;,</span>
<span class="p_add">+					ret, i);</span>
 			return ret;
 		}
<span class="p_del">-		if (desc.flags &amp; cpu_to_vhost16(vq, VRING_DESC_F_WRITE)) {</span>
<span class="p_add">+		if (access == VHOST_ACCESS_WO) {</span>
 			/* If this is an input descriptor,
 			 * increment that count. */
 			*in_num += ret;
<span class="p_chunk">@@ -1781,6 +2288,47 @@</span> <span class="p_context"> void vhost_disable_notify(struct vhost_dev *dev, struct vhost_virtqueue *vq)</span>
 }
 EXPORT_SYMBOL_GPL(vhost_disable_notify);
 
<span class="p_add">+/* Create a new message. */</span>
<span class="p_add">+struct vhost_msg_node *vhost_new_msg(struct vhost_virtqueue *vq, int type)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_msg_node *node = kmalloc(sizeof *node, GFP_KERNEL);</span>
<span class="p_add">+	if (!node)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	node-&gt;vq = vq;</span>
<span class="p_add">+	node-&gt;msg.type = type;</span>
<span class="p_add">+	return node;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(vhost_new_msg);</span>
<span class="p_add">+</span>
<span class="p_add">+void vhost_enqueue_msg(struct vhost_dev *dev, struct list_head *head,</span>
<span class="p_add">+		       struct vhost_msg_node *node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spin_lock(&amp;dev-&gt;iotlb_lock);</span>
<span class="p_add">+	list_add_tail(&amp;node-&gt;node, head);</span>
<span class="p_add">+	spin_unlock(&amp;dev-&gt;iotlb_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	wake_up_interruptible_poll(&amp;dev-&gt;wait, POLLIN | POLLRDNORM);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(vhost_enqueue_msg);</span>
<span class="p_add">+</span>
<span class="p_add">+struct vhost_msg_node *vhost_dequeue_msg(struct vhost_dev *dev,</span>
<span class="p_add">+					 struct list_head *head)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_msg_node *node = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;dev-&gt;iotlb_lock);</span>
<span class="p_add">+	if (!list_empty(head)) {</span>
<span class="p_add">+		node = list_first_entry(head, struct vhost_msg_node,</span>
<span class="p_add">+					node);</span>
<span class="p_add">+		list_del(&amp;node-&gt;node);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	spin_unlock(&amp;dev-&gt;iotlb_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	return node;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(vhost_dequeue_msg);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 static int __init vhost_init(void)
 {
 	return 0;
<span class="p_header">diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h</span>
<span class="p_header">index b93b6a0..8601fc6 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.h</span>
<span class="p_header">+++ b/drivers/vhost/vhost.h</span>
<span class="p_chunk">@@ -63,13 +63,15 @@</span> <span class="p_context"> struct vhost_umem_node {</span>
 	__u64 last;
 	__u64 size;
 	__u64 userspace_addr;
<span class="p_del">-	__u64 flags_padding;</span>
<span class="p_add">+	__u32 perm;</span>
<span class="p_add">+	__u32 flags_padding;</span>
 	__u64 __subtree_last;
 };
 
 struct vhost_umem {
 	struct rb_root umem_tree;
 	struct list_head umem_list;
<span class="p_add">+	int numem;</span>
 };
 
 /* The virtqueue structure describes a queue attached to a device. */
<span class="p_chunk">@@ -117,10 +119,12 @@</span> <span class="p_context"> struct vhost_virtqueue {</span>
 	u64 log_addr;
 
 	struct iovec iov[UIO_MAXIOV];
<span class="p_add">+	struct iovec iotlb_iov[64];</span>
 	struct iovec *indirect;
 	struct vring_used_elem *heads;
 	/* Protected by virtqueue mutex. */
 	struct vhost_umem *umem;
<span class="p_add">+	struct vhost_umem *iotlb;</span>
 	void *private_data;
 	u64 acked_features;
 	/* Log write descriptors */
<span class="p_chunk">@@ -137,6 +141,12 @@</span> <span class="p_context"> struct vhost_virtqueue {</span>
 	u32 busyloop_timeout;
 };
 
<span class="p_add">+struct vhost_msg_node {</span>
<span class="p_add">+  struct vhost_msg msg;</span>
<span class="p_add">+  struct vhost_virtqueue *vq;</span>
<span class="p_add">+  struct list_head node;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 struct vhost_dev {
 	struct mm_struct *mm;
 	struct mutex mutex;
<span class="p_chunk">@@ -148,6 +158,11 @@</span> <span class="p_context"> struct vhost_dev {</span>
 	struct list_head work_list;
 	struct task_struct *worker;
 	struct vhost_umem *umem;
<span class="p_add">+	struct vhost_umem *iotlb;</span>
<span class="p_add">+	spinlock_t iotlb_lock;</span>
<span class="p_add">+	struct list_head read_list;</span>
<span class="p_add">+	struct list_head pending_list;</span>
<span class="p_add">+	wait_queue_head_t wait;</span>
 };
 
 void vhost_dev_init(struct vhost_dev *, struct vhost_virtqueue **vqs, int nvqs);
<span class="p_chunk">@@ -184,6 +199,21 @@</span> <span class="p_context"> bool vhost_enable_notify(struct vhost_dev *, struct vhost_virtqueue *);</span>
 
 int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,
 		    unsigned int log_num, u64 len);
<span class="p_add">+int vq_iotlb_prefetch(struct vhost_virtqueue *vq);</span>
<span class="p_add">+</span>
<span class="p_add">+struct vhost_msg_node *vhost_new_msg(struct vhost_virtqueue *vq, int type);</span>
<span class="p_add">+void vhost_enqueue_msg(struct vhost_dev *dev,</span>
<span class="p_add">+		       struct list_head *head,</span>
<span class="p_add">+		       struct vhost_msg_node *node);</span>
<span class="p_add">+struct vhost_msg_node *vhost_dequeue_msg(struct vhost_dev *dev,</span>
<span class="p_add">+					 struct list_head *head);</span>
<span class="p_add">+unsigned int vhost_chr_poll(struct file *file, struct vhost_dev *dev,</span>
<span class="p_add">+			    poll_table *wait);</span>
<span class="p_add">+ssize_t vhost_chr_read_iter(struct vhost_dev *dev, struct iov_iter *to,</span>
<span class="p_add">+			    int noblock);</span>
<span class="p_add">+ssize_t vhost_chr_write_iter(struct vhost_dev *dev,</span>
<span class="p_add">+			     struct iov_iter *from);</span>
<span class="p_add">+int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled);</span>
 
 #define vq_err(vq, fmt, ...) do {                                  \
 		pr_debug(pr_fmt(fmt), ##__VA_ARGS__);       \
<span class="p_header">diff --git a/include/uapi/linux/vhost.h b/include/uapi/linux/vhost.h</span>
<span class="p_header">index 61a8777..8cb0a65 100644</span>
<span class="p_header">--- a/include/uapi/linux/vhost.h</span>
<span class="p_header">+++ b/include/uapi/linux/vhost.h</span>
<span class="p_chunk">@@ -47,6 +47,32 @@</span> <span class="p_context"> struct vhost_vring_addr {</span>
 	__u64 log_guest_addr;
 };
 
<span class="p_add">+/* no alignment requirement */</span>
<span class="p_add">+struct vhost_iotlb_msg {</span>
<span class="p_add">+	__u64 iova;</span>
<span class="p_add">+	__u64 size;</span>
<span class="p_add">+	__u64 uaddr;</span>
<span class="p_add">+#define VHOST_ACCESS_RO      0x1</span>
<span class="p_add">+#define VHOST_ACCESS_WO      0x2</span>
<span class="p_add">+#define VHOST_ACCESS_RW      0x3</span>
<span class="p_add">+	__u8 perm;</span>
<span class="p_add">+#define VHOST_IOTLB_MISS           1</span>
<span class="p_add">+#define VHOST_IOTLB_UPDATE         2</span>
<span class="p_add">+#define VHOST_IOTLB_INVALIDATE     3</span>
<span class="p_add">+#define VHOST_IOTLB_ACCESS_FAIL    4</span>
<span class="p_add">+	__u8 type;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define VHOST_IOTLB_MSG 0x1</span>
<span class="p_add">+</span>
<span class="p_add">+struct vhost_msg {</span>
<span class="p_add">+	int type;</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		struct vhost_iotlb_msg iotlb;</span>
<span class="p_add">+		__u8 padding[64];</span>
<span class="p_add">+	};</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 struct vhost_memory_region {
 	__u64 guest_phys_addr;
 	__u64 memory_size; /* bytes */
<span class="p_chunk">@@ -146,6 +172,8 @@</span> <span class="p_context"> struct vhost_memory {</span>
 #define VHOST_F_LOG_ALL 26
 /* vhost-net should add virtio_net_hdr for RX, and strip for TX packets. */
 #define VHOST_NET_F_VIRTIO_NET_HDR 27
<span class="p_add">+/* Vhost have device IOTLB */</span>
<span class="p_add">+#define VHOST_F_DEVICE_IOTLB 63</span>
 
 /* VHOST_SCSI specific definitions */
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



