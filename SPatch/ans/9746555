
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,07/15] mm/ZONE_DEVICE: new type of ZONE_DEVICE for unaddressable memory v3 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,07/15] mm/ZONE_DEVICE: new type of ZONE_DEVICE for unaddressable memory v3</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 24, 2017, 5:20 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170524172024.30810-8-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9746555/mbox/"
   >mbox</a>
|
   <a href="/patch/9746555/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9746555/">/patch/9746555/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	54B2C601C2 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 May 2017 17:24:00 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3B0BC289A1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 May 2017 17:24:00 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 2D6CC289C4; Wed, 24 May 2017 17:24:00 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B3E4E289A1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 May 2017 17:23:58 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753791AbdEXRXu (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 24 May 2017 13:23:50 -0400
Received: from mx1.redhat.com ([209.132.183.28]:59712 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752721AbdEXRUi (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 24 May 2017 13:20:38 -0400
Received: from smtp.corp.redhat.com
	(int-mx05.intmail.prod.int.phx2.redhat.com [10.5.11.15])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 3418317AC64;
	Wed, 24 May 2017 17:20:38 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com 3418317AC64
Authentication-Results: ext-mx10.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx10.extmail.prod.ext.phx2.redhat.com;
	spf=pass smtp.mailfrom=jglisse@redhat.com
DKIM-Filter: OpenDKIM Filter v2.11.0 mx1.redhat.com 3418317AC64
Received: from localhost.localdomain.com (ovpn-122-80.rdu2.redhat.com
	[10.10.122.80])
	by smtp.corp.redhat.com (Postfix) with ESMTP id 342821710F;
	Wed, 24 May 2017 17:20:37 +0000 (UTC)
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, linux-kernel@vger.kernel.org,
	linux-mm@kvack.org
Cc: Dan Williams &lt;dan.j.williams@intel.com&gt;,
	&quot;Kirill A . Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	John Hubbard &lt;jhubbard@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;
Subject: [HMM 07/15] mm/ZONE_DEVICE: new type of ZONE_DEVICE for
	unaddressable memory v3
Date: Wed, 24 May 2017 13:20:16 -0400
Message-Id: &lt;20170524172024.30810-8-jglisse@redhat.com&gt;
In-Reply-To: &lt;20170524172024.30810-1-jglisse@redhat.com&gt;
References: &lt;20170524172024.30810-1-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.15
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.39]);
	Wed, 24 May 2017 17:20:38 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - May 24, 2017, 5:20 p.m.</div>
<pre class="content">
HMM (heterogeneous memory management) need struct page to support migration
from system main memory to device memory.  Reasons for HMM and migration to
device memory is explained with HMM core patch.

This patch deals with device memory that is un-addressable memory (ie CPU
can not access it). Hence we do not want those struct page to be manage
like regular memory. That is why we extend ZONE_DEVICE to support different
types of memory.

A persistent memory type is define for existing user of ZONE_DEVICE and a
new device un-addressable type is added for the un-addressable memory type.
There is a clear separation between what is expected from each memory type
and existing user of ZONE_DEVICE are un-affected by new requirement and new
use of the un-addressable type. All specific code path are protect with
test against the memory type.

Because memory is un-addressable we use a new special swap type for when
a page is migrated to device memory (this reduces the number of maximum
swap file).

The main two additions beside memory type to ZONE_DEVICE is two callbacks.
First one, page_free() is call whenever page refcount reach 1 (which means
the page is free as ZONE_DEVICE page never reach a refcount of 0). This
allow device driver to manage its memory and associated struct page.

The second callback page_fault() happens when there is a CPU access to
an address that is back by a device page (which are un-addressable by the
CPU). This callback is responsible to migrate the page back to system
main memory. Device driver can not block migration back to system memory,
HMM make sure that such page can not be pin into device memory.

If device is in some error condition and can not migrate memory back then
a CPU page fault to device memory should end with SIGBUS.

Changed since v2:
  - s/DEVICE_UNADDRESSABLE/DEVICE_PRIVATE
Changed since v1:
  - rename to device private memory (from device unaddressable)
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="acked-by">Acked-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;
---
 fs/proc/task_mmu.c       |  7 +++++
 include/linux/ioport.h   |  1 +
 include/linux/memremap.h | 72 ++++++++++++++++++++++++++++++++++++++++++++++++
 include/linux/mm.h       | 12 ++++++++
 include/linux/swap.h     | 24 ++++++++++++++--
 include/linux/swapops.h  | 68 +++++++++++++++++++++++++++++++++++++++++++++
 kernel/memremap.c        | 34 +++++++++++++++++++++++
 mm/Kconfig               | 13 +++++++++
 mm/memory.c              | 61 ++++++++++++++++++++++++++++++++++++++++
 mm/memory_hotplug.c      | 10 +++++--
 mm/mprotect.c            | 14 ++++++++++
 11 files changed, 311 insertions(+), 5 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=83981">Ross Zwisler</a> - May 30, 2017, 4:43 p.m.</div>
<pre class="content">
On Wed, May 24, 2017 at 01:20:16PM -0400, Jérôme Glisse wrote:
<span class="quote">&gt; HMM (heterogeneous memory management) need struct page to support migration</span>
<span class="quote">&gt; from system main memory to device memory.  Reasons for HMM and migration to</span>
<span class="quote">&gt; device memory is explained with HMM core patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch deals with device memory that is un-addressable memory (ie CPU</span>
<span class="quote">&gt; can not access it). Hence we do not want those struct page to be manage</span>
<span class="quote">&gt; like regular memory. That is why we extend ZONE_DEVICE to support different</span>
<span class="quote">&gt; types of memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A persistent memory type is define for existing user of ZONE_DEVICE and a</span>
<span class="quote">&gt; new device un-addressable type is added for the un-addressable memory type.</span>
<span class="quote">&gt; There is a clear separation between what is expected from each memory type</span>
<span class="quote">&gt; and existing user of ZONE_DEVICE are un-affected by new requirement and new</span>
<span class="quote">&gt; use of the un-addressable type. All specific code path are protect with</span>
<span class="quote">&gt; test against the memory type.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Because memory is un-addressable we use a new special swap type for when</span>
<span class="quote">&gt; a page is migrated to device memory (this reduces the number of maximum</span>
<span class="quote">&gt; swap file).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The main two additions beside memory type to ZONE_DEVICE is two callbacks.</span>
<span class="quote">&gt; First one, page_free() is call whenever page refcount reach 1 (which means</span>
<span class="quote">&gt; the page is free as ZONE_DEVICE page never reach a refcount of 0). This</span>
<span class="quote">&gt; allow device driver to manage its memory and associated struct page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The second callback page_fault() happens when there is a CPU access to</span>
<span class="quote">&gt; an address that is back by a device page (which are un-addressable by the</span>
<span class="quote">&gt; CPU). This callback is responsible to migrate the page back to system</span>
<span class="quote">&gt; main memory. Device driver can not block migration back to system memory,</span>
<span class="quote">&gt; HMM make sure that such page can not be pin into device memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If device is in some error condition and can not migrate memory back then</span>
<span class="quote">&gt; a CPU page fault to device memory should end with SIGBUS.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changed since v2:</span>
<span class="quote">&gt;   - s/DEVICE_UNADDRESSABLE/DEVICE_PRIVATE</span>
<span class="quote">&gt; Changed since v1:</span>
<span class="quote">&gt;   - rename to device private memory (from device unaddressable)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Acked-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
<span class="quote">&gt; Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;</span>
<span class="quote">&gt; ---</span>
&lt;&gt;
<span class="quote">&gt; @@ -35,18 +37,88 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Specialize ZONE_DEVICE memory into multiple types each having differents</span>
<span class="quote">&gt; + * usage.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * MEMORY_DEVICE_PUBLIC:</span>
<span class="quote">&gt; + * Persistent device memory (pmem): struct page might be allocated in different</span>
<span class="quote">&gt; + * memory and architecture might want to perform special actions. It is similar</span>
<span class="quote">&gt; + * to regular memory, in that the CPU can access it transparently. However,</span>
<span class="quote">&gt; + * it is likely to have different bandwidth and latency than regular memory.</span>
<span class="quote">&gt; + * See Documentation/nvdimm/nvdimm.txt for more information.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * MEMORY_DEVICE_PRIVATE:</span>
<span class="quote">&gt; + * Device memory that is not directly addressable by the CPU: CPU can neither</span>
<span class="quote">&gt; + * read nor write _UNADDRESSABLE memory. In this case, we do still have struct</span>
		     _PRIVATE

Just noticed that one holdover from the DEVICE_UNADDRESSABLE naming.
<span class="quote">
&gt; + * pages backing the device memory. Doing so simplifies the implementation, but</span>
<span class="quote">&gt; + * it is important to remember that there are certain points at which the struct</span>
<span class="quote">&gt; + * page must be treated as an opaque object, rather than a &quot;normal&quot; struct page.</span>
<span class="quote">&gt; + * A more complete discussion of unaddressable memory may be found in</span>
<span class="quote">&gt; + * include/linux/hmm.h and Documentation/vm/hmm.txt.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +enum memory_type {</span>
<span class="quote">&gt; +	MEMORY_DEVICE_PUBLIC = 0,</span>
<span class="quote">&gt; +	MEMORY_DEVICE_PRIVATE,</span>
<span class="quote">&gt; +};</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - May 30, 2017, 9:43 p.m.</div>
<pre class="content">
On Tue, May 30, 2017 at 10:43:55AM -0600, Ross Zwisler wrote:
<span class="quote">&gt; On Wed, May 24, 2017 at 01:20:16PM -0400, Jérôme Glisse wrote:</span>
<span class="quote">&gt; &gt; HMM (heterogeneous memory management) need struct page to support migration</span>
<span class="quote">&gt; &gt; from system main memory to device memory.  Reasons for HMM and migration to</span>
<span class="quote">&gt; &gt; device memory is explained with HMM core patch.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This patch deals with device memory that is un-addressable memory (ie CPU</span>
<span class="quote">&gt; &gt; can not access it). Hence we do not want those struct page to be manage</span>
<span class="quote">&gt; &gt; like regular memory. That is why we extend ZONE_DEVICE to support different</span>
<span class="quote">&gt; &gt; types of memory.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; A persistent memory type is define for existing user of ZONE_DEVICE and a</span>
<span class="quote">&gt; &gt; new device un-addressable type is added for the un-addressable memory type.</span>
<span class="quote">&gt; &gt; There is a clear separation between what is expected from each memory type</span>
<span class="quote">&gt; &gt; and existing user of ZONE_DEVICE are un-affected by new requirement and new</span>
<span class="quote">&gt; &gt; use of the un-addressable type. All specific code path are protect with</span>
<span class="quote">&gt; &gt; test against the memory type.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Because memory is un-addressable we use a new special swap type for when</span>
<span class="quote">&gt; &gt; a page is migrated to device memory (this reduces the number of maximum</span>
<span class="quote">&gt; &gt; swap file).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The main two additions beside memory type to ZONE_DEVICE is two callbacks.</span>
<span class="quote">&gt; &gt; First one, page_free() is call whenever page refcount reach 1 (which means</span>
<span class="quote">&gt; &gt; the page is free as ZONE_DEVICE page never reach a refcount of 0). This</span>
<span class="quote">&gt; &gt; allow device driver to manage its memory and associated struct page.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The second callback page_fault() happens when there is a CPU access to</span>
<span class="quote">&gt; &gt; an address that is back by a device page (which are un-addressable by the</span>
<span class="quote">&gt; &gt; CPU). This callback is responsible to migrate the page back to system</span>
<span class="quote">&gt; &gt; main memory. Device driver can not block migration back to system memory,</span>
<span class="quote">&gt; &gt; HMM make sure that such page can not be pin into device memory.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If device is in some error condition and can not migrate memory back then</span>
<span class="quote">&gt; &gt; a CPU page fault to device memory should end with SIGBUS.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Changed since v2:</span>
<span class="quote">&gt; &gt;   - s/DEVICE_UNADDRESSABLE/DEVICE_PRIVATE</span>
<span class="quote">&gt; &gt; Changed since v1:</span>
<span class="quote">&gt; &gt;   - rename to device private memory (from device unaddressable)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Acked-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &lt;&gt;</span>
<span class="quote">&gt; &gt; @@ -35,18 +37,88 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt; + * Specialize ZONE_DEVICE memory into multiple types each having differents</span>
<span class="quote">&gt; &gt; + * usage.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * MEMORY_DEVICE_PUBLIC:</span>
<span class="quote">&gt; &gt; + * Persistent device memory (pmem): struct page might be allocated in different</span>
<span class="quote">&gt; &gt; + * memory and architecture might want to perform special actions. It is similar</span>
<span class="quote">&gt; &gt; + * to regular memory, in that the CPU can access it transparently. However,</span>
<span class="quote">&gt; &gt; + * it is likely to have different bandwidth and latency than regular memory.</span>
<span class="quote">&gt; &gt; + * See Documentation/nvdimm/nvdimm.txt for more information.</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt; + * MEMORY_DEVICE_PRIVATE:</span>
<span class="quote">&gt; &gt; + * Device memory that is not directly addressable by the CPU: CPU can neither</span>
<span class="quote">&gt; &gt; + * read nor write _UNADDRESSABLE memory. In this case, we do still have struct</span>
<span class="quote">&gt; 		     _PRIVATE</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just noticed that one holdover from the DEVICE_UNADDRESSABLE naming.</span>
<span class="quote">&gt; </span>

Thanks for catching that, Andrew can you change it yourself to _PRIVATE
s/_UNADDRESSABLE/_PRIVATE

Or should i repost fixed patch ?

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - May 31, 2017, 1:23 a.m.</div>
<pre class="content">
On Wed, 24 May 2017 13:20:16 -0400
Jérôme Glisse &lt;jglisse@redhat.com&gt; wrote:
<span class="quote">
&gt; HMM (heterogeneous memory management) need struct page to support migration</span>
<span class="quote">&gt; from system main memory to device memory.  Reasons for HMM and migration to</span>
<span class="quote">&gt; device memory is explained with HMM core patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch deals with device memory that is un-addressable memory (ie CPU</span>
<span class="quote">&gt; can not access it). Hence we do not want those struct page to be manage</span>
<span class="quote">&gt; like regular memory. That is why we extend ZONE_DEVICE to support different</span>
<span class="quote">&gt; types of memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A persistent memory type is define for existing user of ZONE_DEVICE and a</span>
<span class="quote">&gt; new device un-addressable type is added for the un-addressable memory type.</span>
<span class="quote">&gt; There is a clear separation between what is expected from each memory type</span>
<span class="quote">&gt; and existing user of ZONE_DEVICE are un-affected by new requirement and new</span>
<span class="quote">&gt; use of the un-addressable type. All specific code path are protect with</span>
<span class="quote">&gt; test against the memory type.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Because memory is un-addressable we use a new special swap type for when</span>
<span class="quote">&gt; a page is migrated to device memory (this reduces the number of maximum</span>
<span class="quote">&gt; swap file).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The main two additions beside memory type to ZONE_DEVICE is two callbacks.</span>
<span class="quote">&gt; First one, page_free() is call whenever page refcount reach 1 (which means</span>
<span class="quote">&gt; the page is free as ZONE_DEVICE page never reach a refcount of 0). This</span>
<span class="quote">&gt; allow device driver to manage its memory and associated struct page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The second callback page_fault() happens when there is a CPU access to</span>
<span class="quote">&gt; an address that is back by a device page (which are un-addressable by the</span>
<span class="quote">&gt; CPU). This callback is responsible to migrate the page back to system</span>
<span class="quote">&gt; main memory. Device driver can not block migration back to system memory,</span>
<span class="quote">&gt; HMM make sure that such page can not be pin into device memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If device is in some error condition and can not migrate memory back then</span>
<span class="quote">&gt; a CPU page fault to device memory should end with SIGBUS.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changed since v2:</span>
<span class="quote">&gt;   - s/DEVICE_UNADDRESSABLE/DEVICE_PRIVATE</span>
<span class="quote">&gt; Changed since v1:</span>
<span class="quote">&gt;   - rename to device private memory (from device unaddressable)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Acked-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
<span class="quote">&gt; Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  fs/proc/task_mmu.c       |  7 +++++</span>
<span class="quote">&gt;  include/linux/ioport.h   |  1 +</span>
<span class="quote">&gt;  include/linux/memremap.h | 72 ++++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  include/linux/mm.h       | 12 ++++++++</span>
<span class="quote">&gt;  include/linux/swap.h     | 24 ++++++++++++++--</span>
<span class="quote">&gt;  include/linux/swapops.h  | 68 +++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  kernel/memremap.c        | 34 +++++++++++++++++++++++</span>
<span class="quote">&gt;  mm/Kconfig               | 13 +++++++++</span>
<span class="quote">&gt;  mm/memory.c              | 61 ++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  mm/memory_hotplug.c      | 10 +++++--</span>
<span class="quote">&gt;  mm/mprotect.c            | 14 ++++++++++</span>
<span class="quote">&gt;  11 files changed, 311 insertions(+), 5 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; index f0c8b33..90b2fa4 100644</span>
<span class="quote">&gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; @@ -542,6 +542,8 @@ static void smaps_pte_entry(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  		} else if (is_migration_entry(swpent))</span>
<span class="quote">&gt;  			page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt; +		else if (is_device_private_entry(swpent))</span>
<span class="quote">&gt; +			page = device_private_entry_to_page(swpent);</span>
<span class="quote">&gt;  	} else if (unlikely(IS_ENABLED(CONFIG_SHMEM) &amp;&amp; mss-&gt;check_shmem_swap</span>
<span class="quote">&gt;  							&amp;&amp; pte_none(*pte))) {</span>
<span class="quote">&gt;  		page = find_get_entry(vma-&gt;vm_file-&gt;f_mapping,</span>
<span class="quote">&gt; @@ -704,6 +706,8 @@ static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (is_migration_entry(swpent))</span>
<span class="quote">&gt;  			page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt; +		else if (is_device_private_entry(swpent))</span>
<span class="quote">&gt; +			page = device_private_entry_to_page(swpent);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if (page) {</span>
<span class="quote">&gt;  		int mapcount = page_mapcount(page);</span>
<span class="quote">&gt; @@ -1196,6 +1200,9 @@ static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt;  		flags |= PM_SWAP;</span>
<span class="quote">&gt;  		if (is_migration_entry(entry))</span>
<span class="quote">&gt;  			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (is_device_private_entry(entry))</span>
<span class="quote">&gt; +			page = device_private_entry_to_page(entry);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (page &amp;&amp; !PageAnon(page))</span>
<span class="quote">&gt; diff --git a/include/linux/ioport.h b/include/linux/ioport.h</span>
<span class="quote">&gt; index 6230064..3a4f691 100644</span>
<span class="quote">&gt; --- a/include/linux/ioport.h</span>
<span class="quote">&gt; +++ b/include/linux/ioport.h</span>
<span class="quote">&gt; @@ -130,6 +130,7 @@ enum {</span>
<span class="quote">&gt;  	IORES_DESC_ACPI_NV_STORAGE		= 3,</span>
<span class="quote">&gt;  	IORES_DESC_PERSISTENT_MEMORY		= 4,</span>
<span class="quote">&gt;  	IORES_DESC_PERSISTENT_MEMORY_LEGACY	= 5,</span>
<span class="quote">&gt; +	IORES_DESC_DEVICE_PRIVATE_MEMORY	= 6,</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* helpers to define resources */</span>
<span class="quote">&gt; diff --git a/include/linux/memremap.h b/include/linux/memremap.h</span>
<span class="quote">&gt; index 9341619..0fcf840 100644</span>
<span class="quote">&gt; --- a/include/linux/memremap.h</span>
<span class="quote">&gt; +++ b/include/linux/memremap.h</span>
<span class="quote">&gt; @@ -4,6 +4,8 @@</span>
<span class="quote">&gt;  #include &lt;linux/ioport.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/percpu-refcount.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  struct resource;</span>
<span class="quote">&gt;  struct device;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -35,18 +37,88 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Specialize ZONE_DEVICE memory into multiple types each having differents</span>
<span class="quote">&gt; + * usage.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * MEMORY_DEVICE_PUBLIC:</span>
<span class="quote">&gt; + * Persistent device memory (pmem): struct page might be allocated in different</span>
<span class="quote">&gt; + * memory and architecture might want to perform special actions. It is similar</span>
<span class="quote">&gt; + * to regular memory, in that the CPU can access it transparently. However,</span>
<span class="quote">&gt; + * it is likely to have different bandwidth and latency than regular memory.</span>
<span class="quote">&gt; + * See Documentation/nvdimm/nvdimm.txt for more information.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * MEMORY_DEVICE_PRIVATE:</span>
<span class="quote">&gt; + * Device memory that is not directly addressable by the CPU: CPU can neither</span>
<span class="quote">&gt; + * read nor write _UNADDRESSABLE memory. In this case, we do still have struct</span>
<span class="quote">&gt; + * pages backing the device memory. Doing so simplifies the implementation, but</span>
<span class="quote">&gt; + * it is important to remember that there are certain points at which the struct</span>
<span class="quote">&gt; + * page must be treated as an opaque object, rather than a &quot;normal&quot; struct page.</span>
<span class="quote">&gt; + * A more complete discussion of unaddressable memory may be found in</span>
<span class="quote">&gt; + * include/linux/hmm.h and Documentation/vm/hmm.txt.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +enum memory_type {</span>
<span class="quote">&gt; +	MEMORY_DEVICE_PUBLIC = 0,</span>
<span class="quote">&gt; +	MEMORY_DEVICE_PRIVATE,</span>

I guess CDM falls somewhere in between PUBLIC and PRIVATE, I wonder how we&#39;ll represent
it?
<span class="quote">
&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * For MEMORY_DEVICE_PRIVATE we use ZONE_DEVICE and extend it with two</span>
<span class="quote">&gt; + * callbacks:</span>
<span class="quote">&gt; + *   page_fault()</span>
<span class="quote">&gt; + *   page_free()</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Additional notes about MEMORY_DEVICE_PRIVATE may be found in</span>
<span class="quote">&gt; + * include/linux/hmm.h and Documentation/vm/hmm.txt. There is also a brief</span>
<span class="quote">&gt; + * explanation in include/linux/memory_hotplug.h.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The page_fault() callback must migrate page back, from device memory to</span>
<span class="quote">&gt; + * system memory, so that the CPU can access it. This might fail for various</span>
<span class="quote">&gt; + * reasons (device issues,  device have been unplugged, ...). When such error</span>
<span class="quote">&gt; + * conditions happen, the page_fault() callback must return VM_FAULT_SIGBUS and</span>
<span class="quote">&gt; + * set the CPU page table entry to &quot;poisoned&quot;.</span>

I would expect this would become optional with HMM-CDM.
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + * Note that because memory cgroup charges are transferred to the device memory,</span>

What does transferred mean? I presume you mean since device memory is not visible
and private, it implies that we charge on the system and migrate to the device?
<span class="quote">
&gt; + * this should never fail due to memory restrictions. However, allocation</span>
<span class="quote">&gt; + * of a regular system page might still fail because we are out of memory. If</span>
<span class="quote">&gt; + * that happens, the page_fault() callback must return VM_FAULT_OOM.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The page_fault() callback can also try to migrate back multiple pages in one</span>
<span class="quote">&gt; + * chunk, as an optimization. It must, however, prioritize the faulting address</span>
<span class="quote">&gt; + * over all the others.</span>
<span class="quote">&gt; + *</span>

I think the page_fault() is good to have for CDM, it should not be made mandatory
<span class="quote">
&gt; + *</span>
<span class="quote">&gt; + * The page_free() callback is called once the page refcount reaches 1</span>
<span class="quote">&gt; + * (ZONE_DEVICE pages never reach 0 refcount unless there is a refcount bug.</span>
<span class="quote">&gt; + * This allows the device driver to implement its own memory management.)</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				unsigned long addr,</span>
<span class="quote">&gt; +				struct page *page,</span>
<span class="quote">&gt; +				unsigned int flags,</span>
<span class="quote">&gt; +				pmd_t *pmdp);</span>
<span class="quote">&gt; +typedef void (*dev_page_free_t)(struct page *page, void *data);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt;   * struct dev_pagemap - metadata for ZONE_DEVICE mappings</span>
<span class="quote">&gt; + * @page_fault: callback when CPU fault on an unaddressable device page</span>
<span class="quote">&gt; + * @page_free: free page callback when page refcount reaches 1</span>
<span class="quote">&gt;   * @altmap: pre-allocated/reserved memory for vmemmap allocations</span>
<span class="quote">&gt;   * @res: physical address range covered by @ref</span>
<span class="quote">&gt;   * @ref: reference count that pins the devm_memremap_pages() mapping</span>
<span class="quote">&gt;   * @dev: host device of the mapping for debug</span>
<span class="quote">&gt; + * @data: private data pointer for page_free()</span>
<span class="quote">&gt; + * @type: memory type: see MEMORY_* in memory_hotplug.h</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  struct dev_pagemap {</span>
<span class="quote">&gt; +	dev_page_fault_t page_fault;</span>
<span class="quote">&gt; +	dev_page_free_t page_free;</span>
<span class="quote">&gt;  	struct vmem_altmap *altmap;</span>
<span class="quote">&gt;  	const struct resource *res;</span>
<span class="quote">&gt;  	struct percpu_ref *ref;</span>
<span class="quote">&gt;  	struct device *dev;</span>
<span class="quote">&gt; +	void *data;</span>
<span class="quote">&gt; +	enum memory_type type;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_ZONE_DEVICE</span>
<span class="quote">&gt; diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="quote">&gt; index 7cb17c6..a825dab 100644</span>
<span class="quote">&gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt; @@ -788,11 +788,23 @@ static inline bool is_zone_device_page(const struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return page_zonenum(page) == ZONE_DEVICE;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_device_private_page(const struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/* See MEMORY_DEVICE_PRIVATE in include/linux/memory_hotplug.h */</span>
<span class="quote">&gt; +	return ((page_zonenum(page) == ZONE_DEVICE) &amp;&amp;</span>
<span class="quote">&gt; +		(page-&gt;pgmap-&gt;type == MEMORY_DEVICE_PRIVATE));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline bool is_zone_device_page(const struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return false;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_device_private_page(const struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return false;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void get_page(struct page *page)</span>
<span class="quote">&gt; diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="quote">&gt; index 5ab1c98..ab6c20b 100644</span>
<span class="quote">&gt; --- a/include/linux/swap.h</span>
<span class="quote">&gt; +++ b/include/linux/swap.h</span>
<span class="quote">&gt; @@ -51,6 +51,23 @@ static inline int current_is_kswapd(void)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * Unaddressable device memory support. See include/linux/hmm.h and</span>
<span class="quote">&gt; + * Documentation/vm/hmm.txt. Short description is we need struct pages for</span>
<span class="quote">&gt; + * device memory that is unaddressable (inaccessible) by CPU, so that we can</span>
<span class="quote">&gt; + * migrate part of a process memory to device memory.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * When a page is migrated from CPU to device, we set the CPU page table entry</span>
<span class="quote">&gt; + * to a special SWP_DEVICE_* entry.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#ifdef CONFIG_DEVICE_PRIVATE</span>
<span class="quote">&gt; +#define SWP_DEVICE_NUM 2</span>
<span class="quote">&gt; +#define SWP_DEVICE_WRITE (MAX_SWAPFILES+SWP_HWPOISON_NUM+SWP_MIGRATION_NUM)</span>
<span class="quote">&gt; +#define SWP_DEVICE_READ (MAX_SWAPFILES+SWP_HWPOISON_NUM+SWP_MIGRATION_NUM+1)</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +#define SWP_DEVICE_NUM 0</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;   * NUMA node memory migration support</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt; @@ -72,7 +89,8 @@ static inline int current_is_kswapd(void)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define MAX_SWAPFILES \</span>
<span class="quote">&gt; -	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
<span class="quote">&gt; +	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_DEVICE_NUM - \</span>
<span class="quote">&gt; +	SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Magic header for a swap area. The first part of the union is</span>
<span class="quote">&gt; @@ -432,8 +450,8 @@ static inline void show_swap_cache_info(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define free_swap_and_cache(swp)	is_migration_entry(swp)</span>
<span class="quote">&gt; -#define swapcache_prepare(swp)		is_migration_entry(swp)</span>
<span class="quote">&gt; +#define free_swap_and_cache(e) (is_migration_entry(e) || is_device_private_entry(e))</span>
<span class="quote">&gt; +#define swapcache_prepare(e) (is_migration_entry(e) || is_device_private_entry(e))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline int add_swap_count_continuation(swp_entry_t swp, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt; index 5c3a5f3..361090c 100644</span>
<span class="quote">&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt; @@ -100,6 +100,74 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
<span class="quote">&gt;  	return (void *)(value | RADIX_TREE_EXCEPTIONAL_ENTRY);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_DEVICE_PRIVATE)</span>
<span class="quote">&gt; +static inline swp_entry_t make_device_private_entry(struct page *page, bool write)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return swp_entry(write ? SWP_DEVICE_WRITE : SWP_DEVICE_READ,</span>
<span class="quote">&gt; +			 page_to_pfn(page));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_device_private_entry(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int type = swp_type(entry);</span>
<span class="quote">&gt; +	return type == SWP_DEVICE_READ || type == SWP_DEVICE_WRITE;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void make_device_private_entry_read(swp_entry_t *entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	*entry = swp_entry(SWP_DEVICE_READ, swp_offset(*entry));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_write_device_private_entry(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return unlikely(swp_type(entry) == SWP_DEVICE_WRITE);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline struct page *device_private_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +int device_private_entry_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		       unsigned long addr,</span>
<span class="quote">&gt; +		       swp_entry_t entry,</span>
<span class="quote">&gt; +		       unsigned int flags,</span>
<span class="quote">&gt; +		       pmd_t *pmdp);</span>
<span class="quote">&gt; +#else /* CONFIG_DEVICE_PRIVATE */</span>
<span class="quote">&gt; +static inline swp_entry_t make_device_private_entry(struct page *page, bool write)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return swp_entry(0, 0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void make_device_private_entry_read(swp_entry_t *entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_device_private_entry(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return false;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_write_device_private_entry(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return false;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline struct page *device_private_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return NULL;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int device_private_entry_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				     unsigned long addr,</span>
<span class="quote">&gt; +				     swp_entry_t entry,</span>
<span class="quote">&gt; +				     unsigned int flags,</span>
<span class="quote">&gt; +				     pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return VM_FAULT_SIGBUS;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif /* CONFIG_DEVICE_PRIVATE */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt;  static inline swp_entry_t make_migration_entry(struct page *page, int write)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/kernel/memremap.c b/kernel/memremap.c</span>
<span class="quote">&gt; index 124bed7..cd596d4 100644</span>
<span class="quote">&gt; --- a/kernel/memremap.c</span>
<span class="quote">&gt; +++ b/kernel/memremap.c</span>
<span class="quote">&gt; @@ -18,6 +18,8 @@</span>
<span class="quote">&gt;  #include &lt;linux/io.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/memory_hotplug.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swapops.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifndef ioremap_cache</span>
<span class="quote">&gt;  /* temporary while we convert existing ioremap_cache users to memremap */</span>
<span class="quote">&gt; @@ -182,6 +184,34 @@ struct page_map {</span>
<span class="quote">&gt;  	struct vmem_altmap altmap;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_DEVICE_PRIVATE)</span>
<span class="quote">&gt; +int device_private_entry_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		       unsigned long addr,</span>
<span class="quote">&gt; +		       swp_entry_t entry,</span>
<span class="quote">&gt; +		       unsigned int flags,</span>
<span class="quote">&gt; +		       pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page *page = device_private_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page_fault() callback must migrate page back to system memory</span>
<span class="quote">&gt; +	 * so that CPU can access it. This might fail for various reasons</span>
<span class="quote">&gt; +	 * (device issue, device was unsafely unplugged, ...). When such</span>
<span class="quote">&gt; +	 * error conditions happen, the callback must return VM_FAULT_SIGBUS.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * Note that because memory cgroup charges are accounted to the device</span>
<span class="quote">&gt; +	 * memory, this should never fail because of memory restrictions (but</span>
<span class="quote">&gt; +	 * allocation of regular system page might still fail because we are</span>
<span class="quote">&gt; +	 * out of memory).</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * There is a more in-depth description of what that callback can and</span>
<span class="quote">&gt; +	 * cannot do, in include/linux/memremap.h</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	return page-&gt;pgmap-&gt;page_fault(vma, addr, page, flags, pmdp);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(device_private_entry_fault);</span>
<span class="quote">&gt; +#endif /* CONFIG_DEVICE_PRIVATE */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static void pgmap_radix_release(struct resource *res)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	resource_size_t key, align_start, align_size, align_end;</span>
<span class="quote">&gt; @@ -321,6 +351,10 @@ void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	pgmap-&gt;ref = ref;</span>
<span class="quote">&gt;  	pgmap-&gt;res = &amp;page_map-&gt;res;</span>
<span class="quote">&gt; +	pgmap-&gt;type = MEMORY_DEVICE_PUBLIC;</span>
<span class="quote">&gt; +	pgmap-&gt;page_fault = NULL;</span>
<span class="quote">&gt; +	pgmap-&gt;page_free = NULL;</span>
<span class="quote">&gt; +	pgmap-&gt;data = NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mutex_lock(&amp;pgmap_lock);</span>
<span class="quote">&gt;  	error = 0;</span>
<span class="quote">&gt; diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="quote">&gt; index d744cff..f5357ff 100644</span>
<span class="quote">&gt; --- a/mm/Kconfig</span>
<span class="quote">&gt; +++ b/mm/Kconfig</span>
<span class="quote">&gt; @@ -736,6 +736,19 @@ config ZONE_DEVICE</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	  If FS_DAX is enabled, then say Y.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config DEVICE_PRIVATE</span>
<span class="quote">&gt; +	bool &quot;Unaddressable device memory (GPU memory, ...)&quot;</span>
<span class="quote">&gt; +	depends on X86_64</span>
<span class="quote">&gt; +	depends on ZONE_DEVICE</span>
<span class="quote">&gt; +	depends on MEMORY_HOTPLUG</span>
<span class="quote">&gt; +	depends on MEMORY_HOTREMOVE</span>
<span class="quote">&gt; +	depends on SPARSEMEM_VMEMMAP</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	help</span>
<span class="quote">&gt; +	  Allows creation of struct pages to represent unaddressable device</span>
<span class="quote">&gt; +	  memory; i.e., memory that is only accessible from the device (or</span>
<span class="quote">&gt; +	  group of devices).</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config FRAME_VECTOR</span>
<span class="quote">&gt;  	bool</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="quote">&gt; index d320b4e..eba61dd 100644</span>
<span class="quote">&gt; --- a/mm/memory.c</span>
<span class="quote">&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt; @@ -49,6 +49,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/highmem.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/pagemap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/memremap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/ksm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/rmap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/export.h&gt;</span>
<span class="quote">&gt; @@ -927,6 +928,35 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="quote">&gt;  					pte = pte_swp_mksoft_dirty(pte);</span>
<span class="quote">&gt;  				set_pte_at(src_mm, addr, src_pte, pte);</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; +		} else if (is_device_private_entry(entry)) {</span>
<span class="quote">&gt; +			page = device_private_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * Update rss count even for unaddressable pages, as</span>
<span class="quote">&gt; +			 * they should treated just like normal pages in this</span>
<span class="quote">&gt; +			 * respect.</span>
<span class="quote">&gt; +			 *</span>
<span class="quote">&gt; +			 * We will likely want to have some new rss counters</span>
<span class="quote">&gt; +			 * for unaddressable pages, at some point. But for now</span>
<span class="quote">&gt; +			 * keep things as they are.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			get_page(page);</span>
<span class="quote">&gt; +			rss[mm_counter(page)]++;</span>
<span class="quote">&gt; +			page_dup_rmap(page, false);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * We do not preserve soft-dirty information, because so</span>
<span class="quote">&gt; +			 * far, checkpoint/restore is the only feature that</span>
<span class="quote">&gt; +			 * requires that. And checkpoint/restore does not work</span>
<span class="quote">&gt; +			 * when a device driver is involved (you cannot easily</span>
<span class="quote">&gt; +			 * save and restore device driver state).</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (is_write_device_private_entry(entry) &amp;&amp;</span>
<span class="quote">&gt; +			    is_cow_mapping(vm_flags)) {</span>
<span class="quote">&gt; +				make_device_private_entry_read(&amp;entry);</span>
<span class="quote">&gt; +				pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +				set_pte_at(src_mm, addr, src_pte, pte);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		goto out_set_pte;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1243,6 +1273,29 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		entry = pte_to_swp_entry(ptent);</span>
<span class="quote">&gt; +		if (non_swap_entry(entry) &amp;&amp; is_device_private_entry(entry)) {</span>
<span class="quote">&gt; +			struct page *page = device_private_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (unlikely(details &amp;&amp; details-&gt;check_mapping)) {</span>
<span class="quote">&gt; +				/*</span>
<span class="quote">&gt; +				 * unmap_shared_mapping_pages() wants to</span>
<span class="quote">&gt; +				 * invalidate cache without truncating:</span>
<span class="quote">&gt; +				 * unmap shared but keep private pages.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +				if (details-&gt;check_mapping !=</span>
<span class="quote">&gt; +				    page_rmapping(page))</span>
<span class="quote">&gt; +					continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			pte_clear_not_present_full(mm, addr, pte, tlb-&gt;fullmm);</span>
<span class="quote">&gt; +			rss[mm_counter(page)]--;</span>
<span class="quote">&gt; +			page_remove_rmap(page, false);</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		/* If details-&gt;check_mapping, we leave swap entries. */</span>
<span class="quote">&gt;  		if (unlikely(details))</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; @@ -2690,6 +2743,14 @@ int do_swap_page(struct vm_fault *vmf)</span>
<span class="quote">&gt;  		if (is_migration_entry(entry)) {</span>
<span class="quote">&gt;  			migration_entry_wait(vma-&gt;vm_mm, vmf-&gt;pmd,</span>
<span class="quote">&gt;  					     vmf-&gt;address);</span>
<span class="quote">&gt; +		} else if (is_device_private_entry(entry)) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * For un-addressable device memory we call the pgmap</span>
<span class="quote">&gt; +			 * fault handler callback. The callback must migrate</span>
<span class="quote">&gt; +			 * the page back to some CPU accessible page.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			ret = device_private_entry_fault(vma, vmf-&gt;address, entry,</span>
<span class="quote">&gt; +						 vmf-&gt;flags, vmf-&gt;pmd);</span>
<span class="quote">&gt;  		} else if (is_hwpoison_entry(entry)) {</span>
<span class="quote">&gt;  			ret = VM_FAULT_HWPOISON;</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt; diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="quote">&gt; index 599c675..0a9f690 100644</span>
<span class="quote">&gt; --- a/mm/memory_hotplug.c</span>
<span class="quote">&gt; +++ b/mm/memory_hotplug.c</span>
<span class="quote">&gt; @@ -156,7 +156,7 @@ void mem_hotplug_done(void)</span>
<span class="quote">&gt;  /* add this memory to iomem resource */</span>
<span class="quote">&gt;  static struct resource *register_memory_resource(u64 start, u64 size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct resource *res;</span>
<span class="quote">&gt; +	struct resource *res, *conflict;</span>
<span class="quote">&gt;  	res = kzalloc(sizeof(struct resource), GFP_KERNEL);</span>
<span class="quote">&gt;  	if (!res)</span>
<span class="quote">&gt;  		return ERR_PTR(-ENOMEM);</span>
<span class="quote">&gt; @@ -165,7 +165,13 @@ static struct resource *register_memory_resource(u64 start, u64 size)</span>
<span class="quote">&gt;  	res-&gt;start = start;</span>
<span class="quote">&gt;  	res-&gt;end = start + size - 1;</span>
<span class="quote">&gt;  	res-&gt;flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;</span>
<span class="quote">&gt; -	if (request_resource(&amp;iomem_resource, res) &lt; 0) {</span>
<span class="quote">&gt; +	conflict =  request_resource_conflict(&amp;iomem_resource, res);</span>
<span class="quote">&gt; +	if (conflict) {</span>
<span class="quote">&gt; +		if (conflict-&gt;desc == IORES_DESC_DEVICE_PRIVATE_MEMORY) {</span>
<span class="quote">&gt; +			pr_debug(&quot;Device unaddressable memory block &quot;</span>
<span class="quote">&gt; +				 &quot;memory hotplug at %#010llx !\n&quot;,</span>
<span class="quote">&gt; +				 (unsigned long long)start);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		pr_debug(&quot;System RAM resource %pR cannot be added\n&quot;, res);</span>
<span class="quote">&gt;  		kfree(res);</span>
<span class="quote">&gt;  		return ERR_PTR(-EEXIST);</span>
<span class="quote">&gt; diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="quote">&gt; index 1a8c9ca..868d0ed 100644</span>
<span class="quote">&gt; --- a/mm/mprotect.c</span>
<span class="quote">&gt; +++ b/mm/mprotect.c</span>
<span class="quote">&gt; @@ -124,6 +124,20 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  				pages++;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_write_device_private_entry(entry)) {</span>
<span class="quote">&gt; +				pte_t newpte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				/*</span>
<span class="quote">&gt; +				 * We do not preserve soft-dirtiness. See</span>
<span class="quote">&gt; +				 * copy_one_pte() for explanation.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +				make_device_private_entry_read(&amp;entry);</span>
<span class="quote">&gt; +				newpte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +				set_pte_at(mm, addr, pte, newpte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				pages++;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	} while (pte++, addr += PAGE_SIZE, addr != end);</span>
<span class="quote">&gt;  	arch_leave_lazy_mmu_mode();</span>

The patch looks good overall, but I&#39;m trying to evaluate it from a HMM-CDM perspective
to make sure we have no missing bits for extension


Balbir Singh.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=146341">zhong jiang</a> - June 15, 2017, 3:41 a.m.</div>
<pre class="content">
On 2017/5/25 1:20, Jérôme Glisse wrote:
<span class="quote">&gt; HMM (heterogeneous memory management) need struct page to support migration</span>
<span class="quote">&gt; from system main memory to device memory.  Reasons for HMM and migration to</span>
<span class="quote">&gt; device memory is explained with HMM core patch.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch deals with device memory that is un-addressable memory (ie CPU</span>
<span class="quote">&gt; can not access it). Hence we do not want those struct page to be manage</span>
<span class="quote">&gt; like regular memory. That is why we extend ZONE_DEVICE to support different</span>
<span class="quote">&gt; types of memory.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; A persistent memory type is define for existing user of ZONE_DEVICE and a</span>
<span class="quote">&gt; new device un-addressable type is added for the un-addressable memory type.</span>
<span class="quote">&gt; There is a clear separation between what is expected from each memory type</span>
<span class="quote">&gt; and existing user of ZONE_DEVICE are un-affected by new requirement and new</span>
<span class="quote">&gt; use of the un-addressable type. All specific code path are protect with</span>
<span class="quote">&gt; test against the memory type.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Because memory is un-addressable we use a new special swap type for when</span>
<span class="quote">&gt; a page is migrated to device memory (this reduces the number of maximum</span>
<span class="quote">&gt; swap file).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The main two additions beside memory type to ZONE_DEVICE is two callbacks.</span>
<span class="quote">&gt; First one, page_free() is call whenever page refcount reach 1 (which means</span>
<span class="quote">&gt; the page is free as ZONE_DEVICE page never reach a refcount of 0). This</span>
<span class="quote">&gt; allow device driver to manage its memory and associated struct page.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The second callback page_fault() happens when there is a CPU access to</span>
<span class="quote">&gt; an address that is back by a device page (which are un-addressable by the</span>
<span class="quote">&gt; CPU). This callback is responsible to migrate the page back to system</span>
<span class="quote">&gt; main memory. Device driver can not block migration back to system memory,</span>
<span class="quote">&gt; HMM make sure that such page can not be pin into device memory.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If device is in some error condition and can not migrate memory back then</span>
<span class="quote">&gt; a CPU page fault to device memory should end with SIGBUS.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Changed since v2:</span>
<span class="quote">&gt;   - s/DEVICE_UNADDRESSABLE/DEVICE_PRIVATE</span>
<span class="quote">&gt; Changed since v1:</span>
<span class="quote">&gt;   - rename to device private memory (from device unaddressable)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Acked-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
<span class="quote">&gt; Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  fs/proc/task_mmu.c       |  7 +++++</span>
<span class="quote">&gt;  include/linux/ioport.h   |  1 +</span>
<span class="quote">&gt;  include/linux/memremap.h | 72 ++++++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  include/linux/mm.h       | 12 ++++++++</span>
<span class="quote">&gt;  include/linux/swap.h     | 24 ++++++++++++++--</span>
<span class="quote">&gt;  include/linux/swapops.h  | 68 +++++++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  kernel/memremap.c        | 34 +++++++++++++++++++++++</span>
<span class="quote">&gt;  mm/Kconfig               | 13 +++++++++</span>
<span class="quote">&gt;  mm/memory.c              | 61 ++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  mm/memory_hotplug.c      | 10 +++++--</span>
<span class="quote">&gt;  mm/mprotect.c            | 14 ++++++++++</span>
<span class="quote">&gt;  11 files changed, 311 insertions(+), 5 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; index f0c8b33..90b2fa4 100644</span>
<span class="quote">&gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; @@ -542,6 +542,8 @@ static void smaps_pte_entry(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  		} else if (is_migration_entry(swpent))</span>
<span class="quote">&gt;  			page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt; +		else if (is_device_private_entry(swpent))</span>
<span class="quote">&gt; +			page = device_private_entry_to_page(swpent);</span>
<span class="quote">&gt;  	} else if (unlikely(IS_ENABLED(CONFIG_SHMEM) &amp;&amp; mss-&gt;check_shmem_swap</span>
<span class="quote">&gt;  							&amp;&amp; pte_none(*pte))) {</span>
<span class="quote">&gt;  		page = find_get_entry(vma-&gt;vm_file-&gt;f_mapping,</span>
<span class="quote">&gt; @@ -704,6 +706,8 @@ static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (is_migration_entry(swpent))</span>
<span class="quote">&gt;  			page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt; +		else if (is_device_private_entry(swpent))</span>
<span class="quote">&gt; +			page = device_private_entry_to_page(swpent);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	if (page) {</span>
<span class="quote">&gt;  		int mapcount = page_mapcount(page);</span>
<span class="quote">&gt; @@ -1196,6 +1200,9 @@ static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt;  		flags |= PM_SWAP;</span>
<span class="quote">&gt;  		if (is_migration_entry(entry))</span>
<span class="quote">&gt;  			page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (is_device_private_entry(entry))</span>
<span class="quote">&gt; +			page = device_private_entry_to_page(entry);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (page &amp;&amp; !PageAnon(page))</span>
<span class="quote">&gt; diff --git a/include/linux/ioport.h b/include/linux/ioport.h</span>
<span class="quote">&gt; index 6230064..3a4f691 100644</span>
<span class="quote">&gt; --- a/include/linux/ioport.h</span>
<span class="quote">&gt; +++ b/include/linux/ioport.h</span>
<span class="quote">&gt; @@ -130,6 +130,7 @@ enum {</span>
<span class="quote">&gt;  	IORES_DESC_ACPI_NV_STORAGE		= 3,</span>
<span class="quote">&gt;  	IORES_DESC_PERSISTENT_MEMORY		= 4,</span>
<span class="quote">&gt;  	IORES_DESC_PERSISTENT_MEMORY_LEGACY	= 5,</span>
<span class="quote">&gt; +	IORES_DESC_DEVICE_PRIVATE_MEMORY	= 6,</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* helpers to define resources */</span>
<span class="quote">&gt; diff --git a/include/linux/memremap.h b/include/linux/memremap.h</span>
<span class="quote">&gt; index 9341619..0fcf840 100644</span>
<span class="quote">&gt; --- a/include/linux/memremap.h</span>
<span class="quote">&gt; +++ b/include/linux/memremap.h</span>
<span class="quote">&gt; @@ -4,6 +4,8 @@</span>
<span class="quote">&gt;  #include &lt;linux/ioport.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/percpu-refcount.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  struct resource;</span>
<span class="quote">&gt;  struct device;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -35,18 +37,88 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Specialize ZONE_DEVICE memory into multiple types each having differents</span>
<span class="quote">&gt; + * usage.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * MEMORY_DEVICE_PUBLIC:</span>
<span class="quote">&gt; + * Persistent device memory (pmem): struct page might be allocated in different</span>
<span class="quote">&gt; + * memory and architecture might want to perform special actions. It is similar</span>
<span class="quote">&gt; + * to regular memory, in that the CPU can access it transparently. However,</span>
<span class="quote">&gt; + * it is likely to have different bandwidth and latency than regular memory.</span>
<span class="quote">&gt; + * See Documentation/nvdimm/nvdimm.txt for more information.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * MEMORY_DEVICE_PRIVATE:</span>
<span class="quote">&gt; + * Device memory that is not directly addressable by the CPU: CPU can neither</span>
<span class="quote">&gt; + * read nor write _UNADDRESSABLE memory. In this case, we do still have struct</span>
<span class="quote">&gt; + * pages backing the device memory. Doing so simplifies the implementation, but</span>
<span class="quote">&gt; + * it is important to remember that there are certain points at which the struct</span>
<span class="quote">&gt; + * page must be treated as an opaque object, rather than a &quot;normal&quot; struct page.</span>
<span class="quote">&gt; + * A more complete discussion of unaddressable memory may be found in</span>
<span class="quote">&gt; + * include/linux/hmm.h and Documentation/vm/hmm.txt.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +enum memory_type {</span>
<span class="quote">&gt; +	MEMORY_DEVICE_PUBLIC = 0,</span>
<span class="quote">&gt; +	MEMORY_DEVICE_PRIVATE,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * For MEMORY_DEVICE_PRIVATE we use ZONE_DEVICE and extend it with two</span>
<span class="quote">&gt; + * callbacks:</span>
<span class="quote">&gt; + *   page_fault()</span>
<span class="quote">&gt; + *   page_free()</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Additional notes about MEMORY_DEVICE_PRIVATE may be found in</span>
<span class="quote">&gt; + * include/linux/hmm.h and Documentation/vm/hmm.txt. There is also a brief</span>
<span class="quote">&gt; + * explanation in include/linux/memory_hotplug.h.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The page_fault() callback must migrate page back, from device memory to</span>
<span class="quote">&gt; + * system memory, so that the CPU can access it. This might fail for various</span>
<span class="quote">&gt; + * reasons (device issues,  device have been unplugged, ...). When such error</span>
<span class="quote">&gt; + * conditions happen, the page_fault() callback must return VM_FAULT_SIGBUS and</span>
<span class="quote">&gt; + * set the CPU page table entry to &quot;poisoned&quot;.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Note that because memory cgroup charges are transferred to the device memory,</span>
<span class="quote">&gt; + * this should never fail due to memory restrictions. However, allocation</span>
<span class="quote">&gt; + * of a regular system page might still fail because we are out of memory. If</span>
<span class="quote">&gt; + * that happens, the page_fault() callback must return VM_FAULT_OOM.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The page_fault() callback can also try to migrate back multiple pages in one</span>
<span class="quote">&gt; + * chunk, as an optimization. It must, however, prioritize the faulting address</span>
<span class="quote">&gt; + * over all the others.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The page_free() callback is called once the page refcount reaches 1</span>
<span class="quote">&gt; + * (ZONE_DEVICE pages never reach 0 refcount unless there is a refcount bug.</span>
<span class="quote">&gt; + * This allows the device driver to implement its own memory management.)</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				unsigned long addr,</span>
<span class="quote">&gt; +				struct page *page,</span>
<span class="quote">&gt; +				unsigned int flags,</span>
<span class="quote">&gt; +				pmd_t *pmdp);</span>
<span class="quote">&gt; +typedef void (*dev_page_free_t)(struct page *page, void *data);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt;   * struct dev_pagemap - metadata for ZONE_DEVICE mappings</span>
<span class="quote">&gt; + * @page_fault: callback when CPU fault on an unaddressable device page</span>
<span class="quote">&gt; + * @page_free: free page callback when page refcount reaches 1</span>
<span class="quote">&gt;   * @altmap: pre-allocated/reserved memory for vmemmap allocations</span>
<span class="quote">&gt;   * @res: physical address range covered by @ref</span>
<span class="quote">&gt;   * @ref: reference count that pins the devm_memremap_pages() mapping</span>
<span class="quote">&gt;   * @dev: host device of the mapping for debug</span>
<span class="quote">&gt; + * @data: private data pointer for page_free()</span>
<span class="quote">&gt; + * @type: memory type: see MEMORY_* in memory_hotplug.h</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  struct dev_pagemap {</span>
<span class="quote">&gt; +	dev_page_fault_t page_fault;</span>
<span class="quote">&gt; +	dev_page_free_t page_free;</span>
<span class="quote">&gt;  	struct vmem_altmap *altmap;</span>
<span class="quote">&gt;  	const struct resource *res;</span>
<span class="quote">&gt;  	struct percpu_ref *ref;</span>
<span class="quote">&gt;  	struct device *dev;</span>
<span class="quote">&gt; +	void *data;</span>
<span class="quote">&gt; +	enum memory_type type;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_ZONE_DEVICE</span>
<span class="quote">&gt; diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="quote">&gt; index 7cb17c6..a825dab 100644</span>
<span class="quote">&gt; --- a/include/linux/mm.h</span>
<span class="quote">&gt; +++ b/include/linux/mm.h</span>
<span class="quote">&gt; @@ -788,11 +788,23 @@ static inline bool is_zone_device_page(const struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return page_zonenum(page) == ZONE_DEVICE;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_device_private_page(const struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/* See MEMORY_DEVICE_PRIVATE in include/linux/memory_hotplug.h */</span>
<span class="quote">&gt; +	return ((page_zonenum(page) == ZONE_DEVICE) &amp;&amp;</span>
<span class="quote">&gt; +		(page-&gt;pgmap-&gt;type == MEMORY_DEVICE_PRIVATE));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline bool is_zone_device_page(const struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return false;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_device_private_page(const struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return false;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline void get_page(struct page *page)</span>
<span class="quote">&gt; diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="quote">&gt; index 5ab1c98..ab6c20b 100644</span>
<span class="quote">&gt; --- a/include/linux/swap.h</span>
<span class="quote">&gt; +++ b/include/linux/swap.h</span>
<span class="quote">&gt; @@ -51,6 +51,23 @@ static inline int current_is_kswapd(void)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * Unaddressable device memory support. See include/linux/hmm.h and</span>
<span class="quote">&gt; + * Documentation/vm/hmm.txt. Short description is we need struct pages for</span>
<span class="quote">&gt; + * device memory that is unaddressable (inaccessible) by CPU, so that we can</span>
<span class="quote">&gt; + * migrate part of a process memory to device memory.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * When a page is migrated from CPU to device, we set the CPU page table entry</span>
<span class="quote">&gt; + * to a special SWP_DEVICE_* entry.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#ifdef CONFIG_DEVICE_PRIVATE</span>
<span class="quote">&gt; +#define SWP_DEVICE_NUM 2</span>
<span class="quote">&gt; +#define SWP_DEVICE_WRITE (MAX_SWAPFILES+SWP_HWPOISON_NUM+SWP_MIGRATION_NUM)</span>
<span class="quote">&gt; +#define SWP_DEVICE_READ (MAX_SWAPFILES+SWP_HWPOISON_NUM+SWP_MIGRATION_NUM+1)</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +#define SWP_DEVICE_NUM 0</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;   * NUMA node memory migration support</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt; @@ -72,7 +89,8 @@ static inline int current_is_kswapd(void)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define MAX_SWAPFILES \</span>
<span class="quote">&gt; -	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
<span class="quote">&gt; +	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_DEVICE_NUM - \</span>
<span class="quote">&gt; +	SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Magic header for a swap area. The first part of the union is</span>
<span class="quote">&gt; @@ -432,8 +450,8 @@ static inline void show_swap_cache_info(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define free_swap_and_cache(swp)	is_migration_entry(swp)</span>
<span class="quote">&gt; -#define swapcache_prepare(swp)		is_migration_entry(swp)</span>
<span class="quote">&gt; +#define free_swap_and_cache(e) (is_migration_entry(e) || is_device_private_entry(e))</span>
<span class="quote">&gt; +#define swapcache_prepare(e) (is_migration_entry(e) || is_device_private_entry(e))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline int add_swap_count_continuation(swp_entry_t swp, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt; index 5c3a5f3..361090c 100644</span>
<span class="quote">&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt; @@ -100,6 +100,74 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
<span class="quote">&gt;  	return (void *)(value | RADIX_TREE_EXCEPTIONAL_ENTRY);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_DEVICE_PRIVATE)</span>
<span class="quote">&gt; +static inline swp_entry_t make_device_private_entry(struct page *page, bool write)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return swp_entry(write ? SWP_DEVICE_WRITE : SWP_DEVICE_READ,</span>
<span class="quote">&gt; +			 page_to_pfn(page));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_device_private_entry(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int type = swp_type(entry);</span>
<span class="quote">&gt; +	return type == SWP_DEVICE_READ || type == SWP_DEVICE_WRITE;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void make_device_private_entry_read(swp_entry_t *entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	*entry = swp_entry(SWP_DEVICE_READ, swp_offset(*entry));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_write_device_private_entry(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return unlikely(swp_type(entry) == SWP_DEVICE_WRITE);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline struct page *device_private_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +int device_private_entry_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		       unsigned long addr,</span>
<span class="quote">&gt; +		       swp_entry_t entry,</span>
<span class="quote">&gt; +		       unsigned int flags,</span>
<span class="quote">&gt; +		       pmd_t *pmdp);</span>
<span class="quote">&gt; +#else /* CONFIG_DEVICE_PRIVATE */</span>
<span class="quote">&gt; +static inline swp_entry_t make_device_private_entry(struct page *page, bool write)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return swp_entry(0, 0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void make_device_private_entry_read(swp_entry_t *entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_device_private_entry(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return false;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_write_device_private_entry(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return false;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline struct page *device_private_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return NULL;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int device_private_entry_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +				     unsigned long addr,</span>
<span class="quote">&gt; +				     swp_entry_t entry,</span>
<span class="quote">&gt; +				     unsigned int flags,</span>
<span class="quote">&gt; +				     pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return VM_FAULT_SIGBUS;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif /* CONFIG_DEVICE_PRIVATE */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt;  static inline swp_entry_t make_migration_entry(struct page *page, int write)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/kernel/memremap.c b/kernel/memremap.c</span>
<span class="quote">&gt; index 124bed7..cd596d4 100644</span>
<span class="quote">&gt; --- a/kernel/memremap.c</span>
<span class="quote">&gt; +++ b/kernel/memremap.c</span>
<span class="quote">&gt; @@ -18,6 +18,8 @@</span>
<span class="quote">&gt;  #include &lt;linux/io.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/memory_hotplug.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swapops.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifndef ioremap_cache</span>
<span class="quote">&gt;  /* temporary while we convert existing ioremap_cache users to memremap */</span>
<span class="quote">&gt; @@ -182,6 +184,34 @@ struct page_map {</span>
<span class="quote">&gt;  	struct vmem_altmap altmap;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_DEVICE_PRIVATE)</span>
<span class="quote">&gt; +int device_private_entry_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		       unsigned long addr,</span>
<span class="quote">&gt; +		       swp_entry_t entry,</span>
<span class="quote">&gt; +		       unsigned int flags,</span>
<span class="quote">&gt; +		       pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page *page = device_private_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * The page_fault() callback must migrate page back to system memory</span>
<span class="quote">&gt; +	 * so that CPU can access it. This might fail for various reasons</span>
<span class="quote">&gt; +	 * (device issue, device was unsafely unplugged, ...). When such</span>
<span class="quote">&gt; +	 * error conditions happen, the callback must return VM_FAULT_SIGBUS.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * Note that because memory cgroup charges are accounted to the device</span>
<span class="quote">&gt; +	 * memory, this should never fail because of memory restrictions (but</span>
<span class="quote">&gt; +	 * allocation of regular system page might still fail because we are</span>
<span class="quote">&gt; +	 * out of memory).</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * There is a more in-depth description of what that callback can and</span>
<span class="quote">&gt; +	 * cannot do, in include/linux/memremap.h</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	return page-&gt;pgmap-&gt;page_fault(vma, addr, page, flags, pmdp);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +EXPORT_SYMBOL(device_private_entry_fault);</span>
<span class="quote">&gt; +#endif /* CONFIG_DEVICE_PRIVATE */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static void pgmap_radix_release(struct resource *res)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	resource_size_t key, align_start, align_size, align_end;</span>
<span class="quote">&gt; @@ -321,6 +351,10 @@ void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	pgmap-&gt;ref = ref;</span>
<span class="quote">&gt;  	pgmap-&gt;res = &amp;page_map-&gt;res;</span>
<span class="quote">&gt; +	pgmap-&gt;type = MEMORY_DEVICE_PUBLIC;</span>
<span class="quote">&gt; +	pgmap-&gt;page_fault = NULL;</span>
<span class="quote">&gt; +	pgmap-&gt;page_free = NULL;</span>
<span class="quote">&gt; +	pgmap-&gt;data = NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mutex_lock(&amp;pgmap_lock);</span>
<span class="quote">&gt;  	error = 0;</span>
<span class="quote">&gt; diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="quote">&gt; index d744cff..f5357ff 100644</span>
<span class="quote">&gt; --- a/mm/Kconfig</span>
<span class="quote">&gt; +++ b/mm/Kconfig</span>
<span class="quote">&gt; @@ -736,6 +736,19 @@ config ZONE_DEVICE</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	  If FS_DAX is enabled, then say Y.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config DEVICE_PRIVATE</span>
<span class="quote">&gt; +	bool &quot;Unaddressable device memory (GPU memory, ...)&quot;</span>
<span class="quote">&gt; +	depends on X86_64</span>
<span class="quote">&gt; +	depends on ZONE_DEVICE</span>
<span class="quote">&gt; +	depends on MEMORY_HOTPLUG</span>
<span class="quote">&gt; +	depends on MEMORY_HOTREMOVE</span>
<span class="quote">&gt; +	depends on SPARSEMEM_VMEMMAP</span>
<span class="quote">&gt; +</span>
 maybe just depends on ARCH_HAS_HMM is enough.
<span class="quote">&gt; +	help</span>
<span class="quote">&gt; +	  Allows creation of struct pages to represent unaddressable device</span>
<span class="quote">&gt; +	  memory; i.e., memory that is only accessible from the device (or</span>
<span class="quote">&gt; +	  group of devices).</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config FRAME_VECTOR</span>
<span class="quote">&gt;  	bool</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="quote">&gt; index d320b4e..eba61dd 100644</span>
<span class="quote">&gt; --- a/mm/memory.c</span>
<span class="quote">&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt; @@ -49,6 +49,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/highmem.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/pagemap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/memremap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/ksm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/rmap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/export.h&gt;</span>
<span class="quote">&gt; @@ -927,6 +928,35 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="quote">&gt;  					pte = pte_swp_mksoft_dirty(pte);</span>
<span class="quote">&gt;  				set_pte_at(src_mm, addr, src_pte, pte);</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; +		} else if (is_device_private_entry(entry)) {</span>
<span class="quote">&gt; +			page = device_private_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * Update rss count even for unaddressable pages, as</span>
<span class="quote">&gt; +			 * they should treated just like normal pages in this</span>
<span class="quote">&gt; +			 * respect.</span>
<span class="quote">&gt; +			 *</span>
<span class="quote">&gt; +			 * We will likely want to have some new rss counters</span>
<span class="quote">&gt; +			 * for unaddressable pages, at some point. But for now</span>
<span class="quote">&gt; +			 * keep things as they are.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			get_page(page);</span>
<span class="quote">&gt; +			rss[mm_counter(page)]++;</span>
<span class="quote">&gt; +			page_dup_rmap(page, false);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * We do not preserve soft-dirty information, because so</span>
<span class="quote">&gt; +			 * far, checkpoint/restore is the only feature that</span>
<span class="quote">&gt; +			 * requires that. And checkpoint/restore does not work</span>
<span class="quote">&gt; +			 * when a device driver is involved (you cannot easily</span>
<span class="quote">&gt; +			 * save and restore device driver state).</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (is_write_device_private_entry(entry) &amp;&amp;</span>
<span class="quote">&gt; +			    is_cow_mapping(vm_flags)) {</span>
<span class="quote">&gt; +				make_device_private_entry_read(&amp;entry);</span>
<span class="quote">&gt; +				pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +				set_pte_at(src_mm, addr, src_pte, pte);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		goto out_set_pte;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -1243,6 +1273,29 @@ static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		entry = pte_to_swp_entry(ptent);</span>
<span class="quote">&gt; +		if (non_swap_entry(entry) &amp;&amp; is_device_private_entry(entry)) {</span>
<span class="quote">&gt; +			struct page *page = device_private_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (unlikely(details &amp;&amp; details-&gt;check_mapping)) {</span>
<span class="quote">&gt; +				/*</span>
<span class="quote">&gt; +				 * unmap_shared_mapping_pages() wants to</span>
<span class="quote">&gt; +				 * invalidate cache without truncating:</span>
<span class="quote">&gt; +				 * unmap shared but keep private pages.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +				if (details-&gt;check_mapping !=</span>
<span class="quote">&gt; +				    page_rmapping(page))</span>
<span class="quote">&gt; +					continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			pte_clear_not_present_full(mm, addr, pte, tlb-&gt;fullmm);</span>
<span class="quote">&gt; +			rss[mm_counter(page)]--;</span>
<span class="quote">&gt; +			page_remove_rmap(page, false);</span>
<span class="quote">&gt; +			put_page(page);</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		/* If details-&gt;check_mapping, we leave swap entries. */</span>
<span class="quote">&gt;  		if (unlikely(details))</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; @@ -2690,6 +2743,14 @@ int do_swap_page(struct vm_fault *vmf)</span>
<span class="quote">&gt;  		if (is_migration_entry(entry)) {</span>
<span class="quote">&gt;  			migration_entry_wait(vma-&gt;vm_mm, vmf-&gt;pmd,</span>
<span class="quote">&gt;  					     vmf-&gt;address);</span>
<span class="quote">&gt; +		} else if (is_device_private_entry(entry)) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * For un-addressable device memory we call the pgmap</span>
<span class="quote">&gt; +			 * fault handler callback. The callback must migrate</span>
<span class="quote">&gt; +			 * the page back to some CPU accessible page.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			ret = device_private_entry_fault(vma, vmf-&gt;address, entry,</span>
<span class="quote">&gt; +						 vmf-&gt;flags, vmf-&gt;pmd);</span>
<span class="quote">&gt;  		} else if (is_hwpoison_entry(entry)) {</span>
<span class="quote">&gt;  			ret = VM_FAULT_HWPOISON;</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt; diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="quote">&gt; index 599c675..0a9f690 100644</span>
<span class="quote">&gt; --- a/mm/memory_hotplug.c</span>
<span class="quote">&gt; +++ b/mm/memory_hotplug.c</span>
<span class="quote">&gt; @@ -156,7 +156,7 @@ void mem_hotplug_done(void)</span>
<span class="quote">&gt;  /* add this memory to iomem resource */</span>
<span class="quote">&gt;  static struct resource *register_memory_resource(u64 start, u64 size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct resource *res;</span>
<span class="quote">&gt; +	struct resource *res, *conflict;</span>
<span class="quote">&gt;  	res = kzalloc(sizeof(struct resource), GFP_KERNEL);</span>
<span class="quote">&gt;  	if (!res)</span>
<span class="quote">&gt;  		return ERR_PTR(-ENOMEM);</span>
<span class="quote">&gt; @@ -165,7 +165,13 @@ static struct resource *register_memory_resource(u64 start, u64 size)</span>
<span class="quote">&gt;  	res-&gt;start = start;</span>
<span class="quote">&gt;  	res-&gt;end = start + size - 1;</span>
<span class="quote">&gt;  	res-&gt;flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;</span>
<span class="quote">&gt; -	if (request_resource(&amp;iomem_resource, res) &lt; 0) {</span>
<span class="quote">&gt; +	conflict =  request_resource_conflict(&amp;iomem_resource, res);</span>
<span class="quote">&gt; +	if (conflict) {</span>
<span class="quote">&gt; +		if (conflict-&gt;desc == IORES_DESC_DEVICE_PRIVATE_MEMORY) {</span>
<span class="quote">&gt; +			pr_debug(&quot;Device unaddressable memory block &quot;</span>
<span class="quote">&gt; +				 &quot;memory hotplug at %#010llx !\n&quot;,</span>
<span class="quote">&gt; +				 (unsigned long long)start);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		pr_debug(&quot;System RAM resource %pR cannot be added\n&quot;, res);</span>
<span class="quote">&gt;  		kfree(res);</span>
<span class="quote">&gt;  		return ERR_PTR(-EEXIST);</span>
<span class="quote">&gt; diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="quote">&gt; index 1a8c9ca..868d0ed 100644</span>
<span class="quote">&gt; --- a/mm/mprotect.c</span>
<span class="quote">&gt; +++ b/mm/mprotect.c</span>
<span class="quote">&gt; @@ -124,6 +124,20 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  				pages++;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_write_device_private_entry(entry)) {</span>
<span class="quote">&gt; +				pte_t newpte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				/*</span>
<span class="quote">&gt; +				 * We do not preserve soft-dirtiness. See</span>
<span class="quote">&gt; +				 * copy_one_pte() for explanation.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +				make_device_private_entry_read(&amp;entry);</span>
<span class="quote">&gt; +				newpte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +				set_pte_at(mm, addr, pte, newpte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				pages++;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	} while (pte++, addr += PAGE_SIZE, addr != end);</span>
<span class="quote">&gt;  	arch_leave_lazy_mmu_mode();</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - June 15, 2017, 5:43 p.m.</div>
<pre class="content">
<span class="quote">&gt; On 2017/5/25 1:20, Jérôme Glisse wrote:</span>

[...]
<span class="quote">
&gt; &gt; diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="quote">&gt; &gt; index d744cff..f5357ff 100644</span>
<span class="quote">&gt; &gt; --- a/mm/Kconfig</span>
<span class="quote">&gt; &gt; +++ b/mm/Kconfig</span>
<span class="quote">&gt; &gt; @@ -736,6 +736,19 @@ config ZONE_DEVICE</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	  If FS_DAX is enabled, then say Y.</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +config DEVICE_PRIVATE</span>
<span class="quote">&gt; &gt; +	bool &quot;Unaddressable device memory (GPU memory, ...)&quot;</span>
<span class="quote">&gt; &gt; +	depends on X86_64</span>
<span class="quote">&gt; &gt; +	depends on ZONE_DEVICE</span>
<span class="quote">&gt; &gt; +	depends on MEMORY_HOTPLUG</span>
<span class="quote">&gt; &gt; +	depends on MEMORY_HOTREMOVE</span>
<span class="quote">&gt; &gt; +	depends on SPARSEMEM_VMEMMAP</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt;  maybe just depends on ARCH_HAS_HMM is enough.</span>

I have updated that as part of HMM CDM patchset.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index f0c8b33..90b2fa4 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -542,6 +542,8 @@</span> <span class="p_context"> static void smaps_pte_entry(pte_t *pte, unsigned long addr,</span>
 			}
 		} else if (is_migration_entry(swpent))
 			page = migration_entry_to_page(swpent);
<span class="p_add">+		else if (is_device_private_entry(swpent))</span>
<span class="p_add">+			page = device_private_entry_to_page(swpent);</span>
 	} else if (unlikely(IS_ENABLED(CONFIG_SHMEM) &amp;&amp; mss-&gt;check_shmem_swap
 							&amp;&amp; pte_none(*pte))) {
 		page = find_get_entry(vma-&gt;vm_file-&gt;f_mapping,
<span class="p_chunk">@@ -704,6 +706,8 @@</span> <span class="p_context"> static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
 
 		if (is_migration_entry(swpent))
 			page = migration_entry_to_page(swpent);
<span class="p_add">+		else if (is_device_private_entry(swpent))</span>
<span class="p_add">+			page = device_private_entry_to_page(swpent);</span>
 	}
 	if (page) {
 		int mapcount = page_mapcount(page);
<span class="p_chunk">@@ -1196,6 +1200,9 @@</span> <span class="p_context"> static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
 		flags |= PM_SWAP;
 		if (is_migration_entry(entry))
 			page = migration_entry_to_page(entry);
<span class="p_add">+</span>
<span class="p_add">+		if (is_device_private_entry(entry))</span>
<span class="p_add">+			page = device_private_entry_to_page(entry);</span>
 	}
 
 	if (page &amp;&amp; !PageAnon(page))
<span class="p_header">diff --git a/include/linux/ioport.h b/include/linux/ioport.h</span>
<span class="p_header">index 6230064..3a4f691 100644</span>
<span class="p_header">--- a/include/linux/ioport.h</span>
<span class="p_header">+++ b/include/linux/ioport.h</span>
<span class="p_chunk">@@ -130,6 +130,7 @@</span> <span class="p_context"> enum {</span>
 	IORES_DESC_ACPI_NV_STORAGE		= 3,
 	IORES_DESC_PERSISTENT_MEMORY		= 4,
 	IORES_DESC_PERSISTENT_MEMORY_LEGACY	= 5,
<span class="p_add">+	IORES_DESC_DEVICE_PRIVATE_MEMORY	= 6,</span>
 };
 
 /* helpers to define resources */
<span class="p_header">diff --git a/include/linux/memremap.h b/include/linux/memremap.h</span>
<span class="p_header">index 9341619..0fcf840 100644</span>
<span class="p_header">--- a/include/linux/memremap.h</span>
<span class="p_header">+++ b/include/linux/memremap.h</span>
<span class="p_chunk">@@ -4,6 +4,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/ioport.h&gt;
 #include &lt;linux/percpu-refcount.h&gt;
 
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+</span>
 struct resource;
 struct device;
 
<span class="p_chunk">@@ -35,18 +37,88 @@</span> <span class="p_context"> static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
 }
 #endif
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Specialize ZONE_DEVICE memory into multiple types each having differents</span>
<span class="p_add">+ * usage.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * MEMORY_DEVICE_PUBLIC:</span>
<span class="p_add">+ * Persistent device memory (pmem): struct page might be allocated in different</span>
<span class="p_add">+ * memory and architecture might want to perform special actions. It is similar</span>
<span class="p_add">+ * to regular memory, in that the CPU can access it transparently. However,</span>
<span class="p_add">+ * it is likely to have different bandwidth and latency than regular memory.</span>
<span class="p_add">+ * See Documentation/nvdimm/nvdimm.txt for more information.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * MEMORY_DEVICE_PRIVATE:</span>
<span class="p_add">+ * Device memory that is not directly addressable by the CPU: CPU can neither</span>
<span class="p_add">+ * read nor write _UNADDRESSABLE memory. In this case, we do still have struct</span>
<span class="p_add">+ * pages backing the device memory. Doing so simplifies the implementation, but</span>
<span class="p_add">+ * it is important to remember that there are certain points at which the struct</span>
<span class="p_add">+ * page must be treated as an opaque object, rather than a &quot;normal&quot; struct page.</span>
<span class="p_add">+ * A more complete discussion of unaddressable memory may be found in</span>
<span class="p_add">+ * include/linux/hmm.h and Documentation/vm/hmm.txt.</span>
<span class="p_add">+ */</span>
<span class="p_add">+enum memory_type {</span>
<span class="p_add">+	MEMORY_DEVICE_PUBLIC = 0,</span>
<span class="p_add">+	MEMORY_DEVICE_PRIVATE,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * For MEMORY_DEVICE_PRIVATE we use ZONE_DEVICE and extend it with two</span>
<span class="p_add">+ * callbacks:</span>
<span class="p_add">+ *   page_fault()</span>
<span class="p_add">+ *   page_free()</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Additional notes about MEMORY_DEVICE_PRIVATE may be found in</span>
<span class="p_add">+ * include/linux/hmm.h and Documentation/vm/hmm.txt. There is also a brief</span>
<span class="p_add">+ * explanation in include/linux/memory_hotplug.h.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The page_fault() callback must migrate page back, from device memory to</span>
<span class="p_add">+ * system memory, so that the CPU can access it. This might fail for various</span>
<span class="p_add">+ * reasons (device issues,  device have been unplugged, ...). When such error</span>
<span class="p_add">+ * conditions happen, the page_fault() callback must return VM_FAULT_SIGBUS and</span>
<span class="p_add">+ * set the CPU page table entry to &quot;poisoned&quot;.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that because memory cgroup charges are transferred to the device memory,</span>
<span class="p_add">+ * this should never fail due to memory restrictions. However, allocation</span>
<span class="p_add">+ * of a regular system page might still fail because we are out of memory. If</span>
<span class="p_add">+ * that happens, the page_fault() callback must return VM_FAULT_OOM.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The page_fault() callback can also try to migrate back multiple pages in one</span>
<span class="p_add">+ * chunk, as an optimization. It must, however, prioritize the faulting address</span>
<span class="p_add">+ * over all the others.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The page_free() callback is called once the page refcount reaches 1</span>
<span class="p_add">+ * (ZONE_DEVICE pages never reach 0 refcount unless there is a refcount bug.</span>
<span class="p_add">+ * This allows the device driver to implement its own memory management.)</span>
<span class="p_add">+ */</span>
<span class="p_add">+typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,</span>
<span class="p_add">+				unsigned long addr,</span>
<span class="p_add">+				struct page *page,</span>
<span class="p_add">+				unsigned int flags,</span>
<span class="p_add">+				pmd_t *pmdp);</span>
<span class="p_add">+typedef void (*dev_page_free_t)(struct page *page, void *data);</span>
<span class="p_add">+</span>
 /**
  * struct dev_pagemap - metadata for ZONE_DEVICE mappings
<span class="p_add">+ * @page_fault: callback when CPU fault on an unaddressable device page</span>
<span class="p_add">+ * @page_free: free page callback when page refcount reaches 1</span>
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
  * @res: physical address range covered by @ref
  * @ref: reference count that pins the devm_memremap_pages() mapping
  * @dev: host device of the mapping for debug
<span class="p_add">+ * @data: private data pointer for page_free()</span>
<span class="p_add">+ * @type: memory type: see MEMORY_* in memory_hotplug.h</span>
  */
 struct dev_pagemap {
<span class="p_add">+	dev_page_fault_t page_fault;</span>
<span class="p_add">+	dev_page_free_t page_free;</span>
 	struct vmem_altmap *altmap;
 	const struct resource *res;
 	struct percpu_ref *ref;
 	struct device *dev;
<span class="p_add">+	void *data;</span>
<span class="p_add">+	enum memory_type type;</span>
 };
 
 #ifdef CONFIG_ZONE_DEVICE
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 7cb17c6..a825dab 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -788,11 +788,23 @@</span> <span class="p_context"> static inline bool is_zone_device_page(const struct page *page)</span>
 {
 	return page_zonenum(page) == ZONE_DEVICE;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_device_private_page(const struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* See MEMORY_DEVICE_PRIVATE in include/linux/memory_hotplug.h */</span>
<span class="p_add">+	return ((page_zonenum(page) == ZONE_DEVICE) &amp;&amp;</span>
<span class="p_add">+		(page-&gt;pgmap-&gt;type == MEMORY_DEVICE_PRIVATE));</span>
<span class="p_add">+}</span>
 #else
 static inline bool is_zone_device_page(const struct page *page)
 {
 	return false;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_device_private_page(const struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
 #endif
 
 static inline void get_page(struct page *page)
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index 5ab1c98..ab6c20b 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -51,6 +51,23 @@</span> <span class="p_context"> static inline int current_is_kswapd(void)</span>
  */
 
 /*
<span class="p_add">+ * Unaddressable device memory support. See include/linux/hmm.h and</span>
<span class="p_add">+ * Documentation/vm/hmm.txt. Short description is we need struct pages for</span>
<span class="p_add">+ * device memory that is unaddressable (inaccessible) by CPU, so that we can</span>
<span class="p_add">+ * migrate part of a process memory to device memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * When a page is migrated from CPU to device, we set the CPU page table entry</span>
<span class="p_add">+ * to a special SWP_DEVICE_* entry.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_DEVICE_PRIVATE</span>
<span class="p_add">+#define SWP_DEVICE_NUM 2</span>
<span class="p_add">+#define SWP_DEVICE_WRITE (MAX_SWAPFILES+SWP_HWPOISON_NUM+SWP_MIGRATION_NUM)</span>
<span class="p_add">+#define SWP_DEVICE_READ (MAX_SWAPFILES+SWP_HWPOISON_NUM+SWP_MIGRATION_NUM+1)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define SWP_DEVICE_NUM 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * NUMA node memory migration support
  */
 #ifdef CONFIG_MIGRATION
<span class="p_chunk">@@ -72,7 +89,8 @@</span> <span class="p_context"> static inline int current_is_kswapd(void)</span>
 #endif
 
 #define MAX_SWAPFILES \
<span class="p_del">-	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
<span class="p_add">+	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_DEVICE_NUM - \</span>
<span class="p_add">+	SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
 
 /*
  * Magic header for a swap area. The first part of the union is
<span class="p_chunk">@@ -432,8 +450,8 @@</span> <span class="p_context"> static inline void show_swap_cache_info(void)</span>
 {
 }
 
<span class="p_del">-#define free_swap_and_cache(swp)	is_migration_entry(swp)</span>
<span class="p_del">-#define swapcache_prepare(swp)		is_migration_entry(swp)</span>
<span class="p_add">+#define free_swap_and_cache(e) (is_migration_entry(e) || is_device_private_entry(e))</span>
<span class="p_add">+#define swapcache_prepare(e) (is_migration_entry(e) || is_device_private_entry(e))</span>
 
 static inline int add_swap_count_continuation(swp_entry_t swp, gfp_t gfp_mask)
 {
<span class="p_header">diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="p_header">index 5c3a5f3..361090c 100644</span>
<span class="p_header">--- a/include/linux/swapops.h</span>
<span class="p_header">+++ b/include/linux/swapops.h</span>
<span class="p_chunk">@@ -100,6 +100,74 @@</span> <span class="p_context"> static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
 	return (void *)(value | RADIX_TREE_EXCEPTIONAL_ENTRY);
 }
 
<span class="p_add">+#if IS_ENABLED(CONFIG_DEVICE_PRIVATE)</span>
<span class="p_add">+static inline swp_entry_t make_device_private_entry(struct page *page, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swp_entry(write ? SWP_DEVICE_WRITE : SWP_DEVICE_READ,</span>
<span class="p_add">+			 page_to_pfn(page));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_device_private_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int type = swp_type(entry);</span>
<span class="p_add">+	return type == SWP_DEVICE_READ || type == SWP_DEVICE_WRITE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void make_device_private_entry_read(swp_entry_t *entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*entry = swp_entry(SWP_DEVICE_READ, swp_offset(*entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_write_device_private_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return unlikely(swp_type(entry) == SWP_DEVICE_WRITE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *device_private_entry_to_page(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int device_private_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+		       unsigned long addr,</span>
<span class="p_add">+		       swp_entry_t entry,</span>
<span class="p_add">+		       unsigned int flags,</span>
<span class="p_add">+		       pmd_t *pmdp);</span>
<span class="p_add">+#else /* CONFIG_DEVICE_PRIVATE */</span>
<span class="p_add">+static inline swp_entry_t make_device_private_entry(struct page *page, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swp_entry(0, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void make_device_private_entry_read(swp_entry_t *entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_device_private_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_write_device_private_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *device_private_entry_to_page(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int device_private_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+				     unsigned long addr,</span>
<span class="p_add">+				     swp_entry_t entry,</span>
<span class="p_add">+				     unsigned int flags,</span>
<span class="p_add">+				     pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return VM_FAULT_SIGBUS;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_DEVICE_PRIVATE */</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MIGRATION
 static inline swp_entry_t make_migration_entry(struct page *page, int write)
 {
<span class="p_header">diff --git a/kernel/memremap.c b/kernel/memremap.c</span>
<span class="p_header">index 124bed7..cd596d4 100644</span>
<span class="p_header">--- a/kernel/memremap.c</span>
<span class="p_header">+++ b/kernel/memremap.c</span>
<span class="p_chunk">@@ -18,6 +18,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/io.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/memory_hotplug.h&gt;
<span class="p_add">+#include &lt;linux/swap.h&gt;</span>
<span class="p_add">+#include &lt;linux/swapops.h&gt;</span>
 
 #ifndef ioremap_cache
 /* temporary while we convert existing ioremap_cache users to memremap */
<span class="p_chunk">@@ -182,6 +184,34 @@</span> <span class="p_context"> struct page_map {</span>
 	struct vmem_altmap altmap;
 };
 
<span class="p_add">+#if IS_ENABLED(CONFIG_DEVICE_PRIVATE)</span>
<span class="p_add">+int device_private_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+		       unsigned long addr,</span>
<span class="p_add">+		       swp_entry_t entry,</span>
<span class="p_add">+		       unsigned int flags,</span>
<span class="p_add">+		       pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page = device_private_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page_fault() callback must migrate page back to system memory</span>
<span class="p_add">+	 * so that CPU can access it. This might fail for various reasons</span>
<span class="p_add">+	 * (device issue, device was unsafely unplugged, ...). When such</span>
<span class="p_add">+	 * error conditions happen, the callback must return VM_FAULT_SIGBUS.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that because memory cgroup charges are accounted to the device</span>
<span class="p_add">+	 * memory, this should never fail because of memory restrictions (but</span>
<span class="p_add">+	 * allocation of regular system page might still fail because we are</span>
<span class="p_add">+	 * out of memory).</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * There is a more in-depth description of what that callback can and</span>
<span class="p_add">+	 * cannot do, in include/linux/memremap.h</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return page-&gt;pgmap-&gt;page_fault(vma, addr, page, flags, pmdp);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(device_private_entry_fault);</span>
<span class="p_add">+#endif /* CONFIG_DEVICE_PRIVATE */</span>
<span class="p_add">+</span>
 static void pgmap_radix_release(struct resource *res)
 {
 	resource_size_t key, align_start, align_size, align_end;
<span class="p_chunk">@@ -321,6 +351,10 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 	}
 	pgmap-&gt;ref = ref;
 	pgmap-&gt;res = &amp;page_map-&gt;res;
<span class="p_add">+	pgmap-&gt;type = MEMORY_DEVICE_PUBLIC;</span>
<span class="p_add">+	pgmap-&gt;page_fault = NULL;</span>
<span class="p_add">+	pgmap-&gt;page_free = NULL;</span>
<span class="p_add">+	pgmap-&gt;data = NULL;</span>
 
 	mutex_lock(&amp;pgmap_lock);
 	error = 0;
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index d744cff..f5357ff 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -736,6 +736,19 @@</span> <span class="p_context"> config ZONE_DEVICE</span>
 
 	  If FS_DAX is enabled, then say Y.
 
<span class="p_add">+config DEVICE_PRIVATE</span>
<span class="p_add">+	bool &quot;Unaddressable device memory (GPU memory, ...)&quot;</span>
<span class="p_add">+	depends on X86_64</span>
<span class="p_add">+	depends on ZONE_DEVICE</span>
<span class="p_add">+	depends on MEMORY_HOTPLUG</span>
<span class="p_add">+	depends on MEMORY_HOTREMOVE</span>
<span class="p_add">+	depends on SPARSEMEM_VMEMMAP</span>
<span class="p_add">+</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Allows creation of struct pages to represent unaddressable device</span>
<span class="p_add">+	  memory; i.e., memory that is only accessible from the device (or</span>
<span class="p_add">+	  group of devices).</span>
<span class="p_add">+</span>
 config FRAME_VECTOR
 	bool
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index d320b4e..eba61dd 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -49,6 +49,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/swap.h&gt;
 #include &lt;linux/highmem.h&gt;
 #include &lt;linux/pagemap.h&gt;
<span class="p_add">+#include &lt;linux/memremap.h&gt;</span>
 #include &lt;linux/ksm.h&gt;
 #include &lt;linux/rmap.h&gt;
 #include &lt;linux/export.h&gt;
<span class="p_chunk">@@ -927,6 +928,35 @@</span> <span class="p_context"> copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 					pte = pte_swp_mksoft_dirty(pte);
 				set_pte_at(src_mm, addr, src_pte, pte);
 			}
<span class="p_add">+		} else if (is_device_private_entry(entry)) {</span>
<span class="p_add">+			page = device_private_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Update rss count even for unaddressable pages, as</span>
<span class="p_add">+			 * they should treated just like normal pages in this</span>
<span class="p_add">+			 * respect.</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * We will likely want to have some new rss counters</span>
<span class="p_add">+			 * for unaddressable pages, at some point. But for now</span>
<span class="p_add">+			 * keep things as they are.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			get_page(page);</span>
<span class="p_add">+			rss[mm_counter(page)]++;</span>
<span class="p_add">+			page_dup_rmap(page, false);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * We do not preserve soft-dirty information, because so</span>
<span class="p_add">+			 * far, checkpoint/restore is the only feature that</span>
<span class="p_add">+			 * requires that. And checkpoint/restore does not work</span>
<span class="p_add">+			 * when a device driver is involved (you cannot easily</span>
<span class="p_add">+			 * save and restore device driver state).</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (is_write_device_private_entry(entry) &amp;&amp;</span>
<span class="p_add">+			    is_cow_mapping(vm_flags)) {</span>
<span class="p_add">+				make_device_private_entry_read(&amp;entry);</span>
<span class="p_add">+				pte = swp_entry_to_pte(entry);</span>
<span class="p_add">+				set_pte_at(src_mm, addr, src_pte, pte);</span>
<span class="p_add">+			}</span>
 		}
 		goto out_set_pte;
 	}
<span class="p_chunk">@@ -1243,6 +1273,29 @@</span> <span class="p_context"> static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
 			}
 			continue;
 		}
<span class="p_add">+</span>
<span class="p_add">+		entry = pte_to_swp_entry(ptent);</span>
<span class="p_add">+		if (non_swap_entry(entry) &amp;&amp; is_device_private_entry(entry)) {</span>
<span class="p_add">+			struct page *page = device_private_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (unlikely(details &amp;&amp; details-&gt;check_mapping)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * unmap_shared_mapping_pages() wants to</span>
<span class="p_add">+				 * invalidate cache without truncating:</span>
<span class="p_add">+				 * unmap shared but keep private pages.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (details-&gt;check_mapping !=</span>
<span class="p_add">+				    page_rmapping(page))</span>
<span class="p_add">+					continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			pte_clear_not_present_full(mm, addr, pte, tlb-&gt;fullmm);</span>
<span class="p_add">+			rss[mm_counter(page)]--;</span>
<span class="p_add">+			page_remove_rmap(page, false);</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		/* If details-&gt;check_mapping, we leave swap entries. */
 		if (unlikely(details))
 			continue;
<span class="p_chunk">@@ -2690,6 +2743,14 @@</span> <span class="p_context"> int do_swap_page(struct vm_fault *vmf)</span>
 		if (is_migration_entry(entry)) {
 			migration_entry_wait(vma-&gt;vm_mm, vmf-&gt;pmd,
 					     vmf-&gt;address);
<span class="p_add">+		} else if (is_device_private_entry(entry)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * For un-addressable device memory we call the pgmap</span>
<span class="p_add">+			 * fault handler callback. The callback must migrate</span>
<span class="p_add">+			 * the page back to some CPU accessible page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			ret = device_private_entry_fault(vma, vmf-&gt;address, entry,</span>
<span class="p_add">+						 vmf-&gt;flags, vmf-&gt;pmd);</span>
 		} else if (is_hwpoison_entry(entry)) {
 			ret = VM_FAULT_HWPOISON;
 		} else {
<span class="p_header">diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="p_header">index 599c675..0a9f690 100644</span>
<span class="p_header">--- a/mm/memory_hotplug.c</span>
<span class="p_header">+++ b/mm/memory_hotplug.c</span>
<span class="p_chunk">@@ -156,7 +156,7 @@</span> <span class="p_context"> void mem_hotplug_done(void)</span>
 /* add this memory to iomem resource */
 static struct resource *register_memory_resource(u64 start, u64 size)
 {
<span class="p_del">-	struct resource *res;</span>
<span class="p_add">+	struct resource *res, *conflict;</span>
 	res = kzalloc(sizeof(struct resource), GFP_KERNEL);
 	if (!res)
 		return ERR_PTR(-ENOMEM);
<span class="p_chunk">@@ -165,7 +165,13 @@</span> <span class="p_context"> static struct resource *register_memory_resource(u64 start, u64 size)</span>
 	res-&gt;start = start;
 	res-&gt;end = start + size - 1;
 	res-&gt;flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;
<span class="p_del">-	if (request_resource(&amp;iomem_resource, res) &lt; 0) {</span>
<span class="p_add">+	conflict =  request_resource_conflict(&amp;iomem_resource, res);</span>
<span class="p_add">+	if (conflict) {</span>
<span class="p_add">+		if (conflict-&gt;desc == IORES_DESC_DEVICE_PRIVATE_MEMORY) {</span>
<span class="p_add">+			pr_debug(&quot;Device unaddressable memory block &quot;</span>
<span class="p_add">+				 &quot;memory hotplug at %#010llx !\n&quot;,</span>
<span class="p_add">+				 (unsigned long long)start);</span>
<span class="p_add">+		}</span>
 		pr_debug(&quot;System RAM resource %pR cannot be added\n&quot;, res);
 		kfree(res);
 		return ERR_PTR(-EEXIST);
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index 1a8c9ca..868d0ed 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -124,6 +124,20 @@</span> <span class="p_context"> static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,</span>
 
 				pages++;
 			}
<span class="p_add">+</span>
<span class="p_add">+			if (is_write_device_private_entry(entry)) {</span>
<span class="p_add">+				pte_t newpte;</span>
<span class="p_add">+</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We do not preserve soft-dirtiness. See</span>
<span class="p_add">+				 * copy_one_pte() for explanation.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				make_device_private_entry_read(&amp;entry);</span>
<span class="p_add">+				newpte = swp_entry_to_pte(entry);</span>
<span class="p_add">+				set_pte_at(mm, addr, pte, newpte);</span>
<span class="p_add">+</span>
<span class="p_add">+				pages++;</span>
<span class="p_add">+			}</span>
 		}
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	arch_leave_lazy_mmu_mode();

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



