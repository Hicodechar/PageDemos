
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v2,4/6] x86: Disable PTI on compatibility mode - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v2,4/6] x86: Disable PTI on compatibility mode</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 15, 2018, 4:36 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180215163602.61162-5-namit@vmware.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10222383/mbox/"
   >mbox</a>
|
   <a href="/patch/10222383/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10222383/">/patch/10222383/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	DF5986055C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Feb 2018 16:37:49 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id CEA8D2835B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Feb 2018 16:37:49 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id C376B283AF; Thu, 15 Feb 2018 16:37:49 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D64E72835B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 15 Feb 2018 16:37:48 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1426158AbeBOQhp (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 15 Feb 2018 11:37:45 -0500
Received: from ex13-edg-ou-001.vmware.com ([208.91.0.189]:57791 &quot;EHLO
	EX13-EDG-OU-001.vmware.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1426071AbeBOQgR (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 15 Feb 2018 11:36:17 -0500
Received: from sc9-mailhost3.vmware.com (10.113.161.73) by
	EX13-EDG-OU-001.vmware.com (10.113.208.155) with Microsoft SMTP
	Server id 15.0.1156.6; Thu, 15 Feb 2018 08:36:08 -0800
Received: from ubuntu.localdomain (unknown [10.2.101.129])
	by sc9-mailhost3.vmware.com (Postfix) with ESMTP id 52DB240B35;
	Thu, 15 Feb 2018 08:36:16 -0800 (PST)
From: Nadav Amit &lt;namit@vmware.com&gt;
To: Ingo Molnar &lt;mingo@redhat.com&gt;
CC: Thomas Gleixner &lt;tglx@linutronix.de&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Willy Tarreau &lt;w@1wt.eu&gt;, Nadav Amit &lt;nadav.amit@gmail.com&gt;,
	&lt;x86@kernel.org&gt;, &lt;linux-kernel@vger.kernel.org&gt;,
	Nadav Amit &lt;namit@vmware.com&gt;
Subject: [PATCH RFC v2 4/6] x86: Disable PTI on compatibility mode
Date: Thu, 15 Feb 2018 08:36:00 -0800
Message-ID: &lt;20180215163602.61162-5-namit@vmware.com&gt;
X-Mailer: git-send-email 2.14.1
In-Reply-To: &lt;20180215163602.61162-1-namit@vmware.com&gt;
References: &lt;20180215163602.61162-1-namit@vmware.com&gt;
MIME-Version: 1.0
Content-Type: text/plain
Received-SPF: None (EX13-EDG-OU-001.vmware.com: namit@vmware.com does not
	designate permitted sender hosts)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=159401">Nadav Amit</a> - Feb. 15, 2018, 4:36 p.m.</div>
<pre class="content">
Based on the understanding that there should be no way for userspace to
address the kernel-space from compatibility mode, disable it while
running in compatibility mode as long as the 64-bit code segment of the
user is not used.

Reenabling PTI is performed by restoring NX-bits to the userspace
mappings, flushing the TLBs, and notifying all the CPUs that use the
affected mm to disable PTI. Each core responds by removing the present
bit for the 64-bit code-segment, and marking that PTI is disabled on
that core.
<span class="signed-off-by">
Signed-off-by: Nadav Amit &lt;namit@vmware.com&gt;</span>
---
 arch/x86/include/asm/pti.h   |  39 +++++++++++++
 arch/x86/kernel/process_64.c |  13 ++++-
 arch/x86/kernel/traps.c      |  23 +++++++-
 arch/x86/mm/pti.c            | 130 +++++++++++++++++++++++++++++++++++++++++++
 4 files changed, 201 insertions(+), 4 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Feb. 15, 2018, 8:02 p.m.</div>
<pre class="content">
On Thu, Feb 15, 2018 at 4:36 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:
<span class="quote">&gt; Based on the understanding that there should be no way for userspace to</span>
<span class="quote">&gt; address the kernel-space from compatibility mode, disable it while</span>
<span class="quote">&gt; running in compatibility mode as long as the 64-bit code segment of the</span>
<span class="quote">&gt; user is not used.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Reenabling PTI is performed by restoring NX-bits to the userspace</span>
<span class="quote">&gt; mappings, flushing the TLBs, and notifying all the CPUs that use the</span>
<span class="quote">&gt; affected mm to disable PTI. Each core responds by removing the present</span>
<span class="quote">&gt; bit for the 64-bit code-segment, and marking that PTI is disabled on</span>
<span class="quote">&gt; that core.</span>
<span class="quote">&gt;</span>

I dislike this patch because it&#39;s conflating two things.  The patch
claims to merely disable PTI for compat tasks, whatever those are.
But it&#39;s also introducing a much stronger concept of what a compat
task is.  The kernel currently mostly doesn&#39;t care whether a task is
&quot;compat&quot; or not, and I think that most remaining code paths that do
care are buggy and should be removed.

I think the right way to approach this is to add a new arch_prctl()
that changes allowable bitness, like this:

arch_prctl(ARCH_SET_ALLOWED_GDT_CS, X86_ALLOW_CS32 | X86_ALLOW_CS64);

this would set the current task to work the normal way, where 32-bit
and 64-bit CS are available.  You could set just X86_ALLOW_CS32 to
deny 64-bit mode and just X86_ALLOW_CS64 to deny 32-bit mode.  This
would make nice attack surface reduction tools for the more paranoid
sandbox users to use.  Doing arch_prctl(ARCH_SET_ALLOWED_GDT_CS, 0)
would return -EINVAL.

A separate patch would turn PTI off if you set X86_ALLOW_CS32.

This has the downside that old code doesn&#39;t get the benefit without
some code change, but that&#39;s not the end of the world.
<span class="quote">
&gt; +static void pti_cpu_update_func(void *info)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct mm_struct *mm = (struct mm_struct *)info;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (mm != this_cpu_read(cpu_tlbstate.loaded_mm))</span>
<span class="quote">&gt; +               return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * Keep CS64 and CPU settings in sync despite potential concurrent</span>
<span class="quote">&gt; +        * updates.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       set_cpu_pti_disable(READ_ONCE(mm-&gt;context.pti_disable));</span>
<span class="quote">&gt; +}</span>

I don&#39;t like this at all.  IMO a sane implementation should never
change PTI status on a remote CPU.  Just track it per task.
<span class="quote">
&gt; +void __pti_reenable(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt; +       int cpu;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (!mm_pti_disable(mm))</span>
<span class="quote">&gt; +               return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * Prevent spurious page-fault storm while we set the NX-bit and have</span>
<span class="quote">&gt; +        * yet not updated the per-CPU pti_disable flag.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if (!mm_pti_disable(mm))</span>
<span class="quote">&gt; +               goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * First, mark the PTI is enabled. Although we do anything yet, we are</span>
<span class="quote">&gt; +        * safe as long as we do not reenable CS64. Since we did not update the</span>
<span class="quote">&gt; +        * page tables yet, this may lead to spurious page-faults, but we need</span>
<span class="quote">&gt; +        * the pti_disable in mm to be set for __pti_set_user_pgd() to do the</span>
<span class="quote">&gt; +        * right thing.  Holding mmap_sem would ensure matter we hold the</span>
<span class="quote">&gt; +        * mmap_sem to prevent them from swamping the system.</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       mm-&gt;context.pti_disable = PTI_DISABLE_OFF;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       /* Second, restore the NX bits. */</span>
<span class="quote">&gt; +       pti_update_user_pgds(mm, true);</span>

You&#39;re holding mmap_sem, but there are code paths that touch page
tables that don&#39;t hold mmap_sem, such as the stack extension code.
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +bool pti_handle_segment_not_present(long error_code)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt; +               return false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       if ((unsigned short)error_code != GDT_ENTRY_DEFAULT_USER_CS &lt;&lt; 3)</span>
<span class="quote">&gt; +               return false;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       pti_reenable();</span>
<span class="quote">&gt; +       return true;</span>
<span class="quote">&gt; +}</span>

Please don&#39;t.  You&#39;re trying to emulate the old behavior here, but
you&#39;re emulating it wrong.  In particular, you won&#39;t trap on LAR.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Feb. 15, 2018, 8:58 p.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@kernel.org&gt; wrote:
<span class="quote">
&gt; On Thu, Feb 15, 2018 at 4:36 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt; Based on the understanding that there should be no way for userspace to</span>
<span class="quote">&gt;&gt; address the kernel-space from compatibility mode, disable it while</span>
<span class="quote">&gt;&gt; running in compatibility mode as long as the 64-bit code segment of the</span>
<span class="quote">&gt;&gt; user is not used.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Reenabling PTI is performed by restoring NX-bits to the userspace</span>
<span class="quote">&gt;&gt; mappings, flushing the TLBs, and notifying all the CPUs that use the</span>
<span class="quote">&gt;&gt; affected mm to disable PTI. Each core responds by removing the present</span>
<span class="quote">&gt;&gt; bit for the 64-bit code-segment, and marking that PTI is disabled on</span>
<span class="quote">&gt;&gt; that core.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I dislike this patch because it&#39;s conflating two things.  The patch</span>
<span class="quote">&gt; claims to merely disable PTI for compat tasks, whatever those are.</span>
<span class="quote">&gt; But it&#39;s also introducing a much stronger concept of what a compat</span>
<span class="quote">&gt; task is.  The kernel currently mostly doesn&#39;t care whether a task is</span>
<span class="quote">&gt; &quot;compat&quot; or not, and I think that most remaining code paths that do</span>
<span class="quote">&gt; care are buggy and should be removed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think the right way to approach this is to add a new arch_prctl()</span>
<span class="quote">&gt; that changes allowable bitness, like this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; arch_prctl(ARCH_SET_ALLOWED_GDT_CS, X86_ALLOW_CS32 | X86_ALLOW_CS64);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; this would set the current task to work the normal way, where 32-bit</span>
<span class="quote">&gt; and 64-bit CS are available.  You could set just X86_ALLOW_CS32 to</span>
<span class="quote">&gt; deny 64-bit mode and just X86_ALLOW_CS64 to deny 32-bit mode.  This</span>
<span class="quote">&gt; would make nice attack surface reduction tools for the more paranoid</span>
<span class="quote">&gt; sandbox users to use.  Doing arch_prctl(ARCH_SET_ALLOWED_GDT_CS, 0)</span>
<span class="quote">&gt; would return -EINVAL.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A separate patch would turn PTI off if you set X86_ALLOW_CS32.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This has the downside that old code doesn&#39;t get the benefit without</span>
<span class="quote">&gt; some code change, but that&#39;s not the end of the world.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +static void pti_cpu_update_func(void *info)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       struct mm_struct *mm = (struct mm_struct *)info;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if (mm != this_cpu_read(cpu_tlbstate.loaded_mm))</span>
<span class="quote">&gt;&gt; +               return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt; +        * Keep CS64 and CPU settings in sync despite potential concurrent</span>
<span class="quote">&gt;&gt; +        * updates.</span>
<span class="quote">&gt;&gt; +        */</span>
<span class="quote">&gt;&gt; +       set_cpu_pti_disable(READ_ONCE(mm-&gt;context.pti_disable));</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t like this at all.  IMO a sane implementation should never</span>
<span class="quote">&gt; change PTI status on a remote CPU.  Just track it per task.</span>

From your comments in the past, I understood that this requirement
(enabling/disabling PTI per mm) came from Linus. I can do it per task, but
that means that the NX-bit will always be cleared on the kernel page-table
for user mappings.
<span class="quote">
&gt;&gt; +void __pti_reenable(void)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;&gt; +       int cpu;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if (!mm_pti_disable(mm))</span>
<span class="quote">&gt;&gt; +               return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt; +        * Prevent spurious page-fault storm while we set the NX-bit and have</span>
<span class="quote">&gt;&gt; +        * yet not updated the per-CPU pti_disable flag.</span>
<span class="quote">&gt;&gt; +        */</span>
<span class="quote">&gt;&gt; +       down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if (!mm_pti_disable(mm))</span>
<span class="quote">&gt;&gt; +               goto out;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt; +        * First, mark the PTI is enabled. Although we do anything yet, we are</span>
<span class="quote">&gt;&gt; +        * safe as long as we do not reenable CS64. Since we did not update the</span>
<span class="quote">&gt;&gt; +        * page tables yet, this may lead to spurious page-faults, but we need</span>
<span class="quote">&gt;&gt; +        * the pti_disable in mm to be set for __pti_set_user_pgd() to do the</span>
<span class="quote">&gt;&gt; +        * right thing.  Holding mmap_sem would ensure matter we hold the</span>
<span class="quote">&gt;&gt; +        * mmap_sem to prevent them from swamping the system.</span>
<span class="quote">&gt;&gt; +        */</span>
<span class="quote">&gt;&gt; +       mm-&gt;context.pti_disable = PTI_DISABLE_OFF;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       /* Second, restore the NX bits. */</span>
<span class="quote">&gt;&gt; +       pti_update_user_pgds(mm, true);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You&#39;re holding mmap_sem, but there are code paths that touch page</span>
<span class="quote">&gt; tables that don&#39;t hold mmap_sem, such as the stack extension code.</span>

Do they change user mappings? These are the only ones pti_update_user_pgds()
changes.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +bool pti_handle_segment_not_present(long error_code)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt;&gt; +               return false;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       if ((unsigned short)error_code != GDT_ENTRY_DEFAULT_USER_CS &lt;&lt; 3)</span>
<span class="quote">&gt;&gt; +               return false;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       pti_reenable();</span>
<span class="quote">&gt;&gt; +       return true;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please don&#39;t.  You&#39;re trying to emulate the old behavior here, but</span>
<span class="quote">&gt; you&#39;re emulating it wrong.  In particular, you won&#39;t trap on LAR.</span>

Yes, I thought I’ll manage to address LAR, but failed. I thought you said
this is not a “show-stopper”. I’ll adapt your approach of using prctl, although
it really limits the benefit of this mechanism.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Feb. 15, 2018, 11:29 p.m.</div>
<pre class="content">
On Thu, Feb 15, 2018 at 8:58 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:
<span class="quote">&gt; Andy Lutomirski &lt;luto@kernel.org&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On Thu, Feb 15, 2018 at 4:36 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; Based on the understanding that there should be no way for userspace to</span>
<span class="quote">&gt;&gt;&gt; address the kernel-space from compatibility mode, disable it while</span>
<span class="quote">&gt;&gt;&gt; running in compatibility mode as long as the 64-bit code segment of the</span>
<span class="quote">&gt;&gt;&gt; user is not used.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Reenabling PTI is performed by restoring NX-bits to the userspace</span>
<span class="quote">&gt;&gt;&gt; mappings, flushing the TLBs, and notifying all the CPUs that use the</span>
<span class="quote">&gt;&gt;&gt; affected mm to disable PTI. Each core responds by removing the present</span>
<span class="quote">&gt;&gt;&gt; bit for the 64-bit code-segment, and marking that PTI is disabled on</span>
<span class="quote">&gt;&gt;&gt; that core.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I dislike this patch because it&#39;s conflating two things.  The patch</span>
<span class="quote">&gt;&gt; claims to merely disable PTI for compat tasks, whatever those are.</span>
<span class="quote">&gt;&gt; But it&#39;s also introducing a much stronger concept of what a compat</span>
<span class="quote">&gt;&gt; task is.  The kernel currently mostly doesn&#39;t care whether a task is</span>
<span class="quote">&gt;&gt; &quot;compat&quot; or not, and I think that most remaining code paths that do</span>
<span class="quote">&gt;&gt; care are buggy and should be removed.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think the right way to approach this is to add a new arch_prctl()</span>
<span class="quote">&gt;&gt; that changes allowable bitness, like this:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; arch_prctl(ARCH_SET_ALLOWED_GDT_CS, X86_ALLOW_CS32 | X86_ALLOW_CS64);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; this would set the current task to work the normal way, where 32-bit</span>
<span class="quote">&gt;&gt; and 64-bit CS are available.  You could set just X86_ALLOW_CS32 to</span>
<span class="quote">&gt;&gt; deny 64-bit mode and just X86_ALLOW_CS64 to deny 32-bit mode.  This</span>
<span class="quote">&gt;&gt; would make nice attack surface reduction tools for the more paranoid</span>
<span class="quote">&gt;&gt; sandbox users to use.  Doing arch_prctl(ARCH_SET_ALLOWED_GDT_CS, 0)</span>
<span class="quote">&gt;&gt; would return -EINVAL.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; A separate patch would turn PTI off if you set X86_ALLOW_CS32.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This has the downside that old code doesn&#39;t get the benefit without</span>
<span class="quote">&gt;&gt; some code change, but that&#39;s not the end of the world.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; +static void pti_cpu_update_func(void *info)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +       struct mm_struct *mm = (struct mm_struct *)info;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       if (mm != this_cpu_read(cpu_tlbstate.loaded_mm))</span>
<span class="quote">&gt;&gt;&gt; +               return;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt;&gt; +        * Keep CS64 and CPU settings in sync despite potential concurrent</span>
<span class="quote">&gt;&gt;&gt; +        * updates.</span>
<span class="quote">&gt;&gt;&gt; +        */</span>
<span class="quote">&gt;&gt;&gt; +       set_cpu_pti_disable(READ_ONCE(mm-&gt;context.pti_disable));</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I don&#39;t like this at all.  IMO a sane implementation should never</span>
<span class="quote">&gt;&gt; change PTI status on a remote CPU.  Just track it per task.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; From your comments in the past, I understood that this requirement</span>
<span class="quote">&gt; (enabling/disabling PTI per mm) came from Linus. I can do it per task, but</span>
<span class="quote">&gt; that means that the NX-bit will always be cleared on the kernel page-table</span>
<span class="quote">&gt; for user mappings.</span>

I disagree with Linus here, although I could be convinced.  But
there&#39;s still no need for an IPI -- even if it were per-mm, PTI could
stay on until the next kernel entry.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;&gt; +void __pti_reenable(void)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +       struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt;&gt;&gt; +       int cpu;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       if (!mm_pti_disable(mm))</span>
<span class="quote">&gt;&gt;&gt; +               return;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt;&gt; +        * Prevent spurious page-fault storm while we set the NX-bit and have</span>
<span class="quote">&gt;&gt;&gt; +        * yet not updated the per-CPU pti_disable flag.</span>
<span class="quote">&gt;&gt;&gt; +        */</span>
<span class="quote">&gt;&gt;&gt; +       down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       if (!mm_pti_disable(mm))</span>
<span class="quote">&gt;&gt;&gt; +               goto out;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt;&gt; +        * First, mark the PTI is enabled. Although we do anything yet, we are</span>
<span class="quote">&gt;&gt;&gt; +        * safe as long as we do not reenable CS64. Since we did not update the</span>
<span class="quote">&gt;&gt;&gt; +        * page tables yet, this may lead to spurious page-faults, but we need</span>
<span class="quote">&gt;&gt;&gt; +        * the pti_disable in mm to be set for __pti_set_user_pgd() to do the</span>
<span class="quote">&gt;&gt;&gt; +        * right thing.  Holding mmap_sem would ensure matter we hold the</span>
<span class="quote">&gt;&gt;&gt; +        * mmap_sem to prevent them from swamping the system.</span>
<span class="quote">&gt;&gt;&gt; +        */</span>
<span class="quote">&gt;&gt;&gt; +       mm-&gt;context.pti_disable = PTI_DISABLE_OFF;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       /* Second, restore the NX bits. */</span>
<span class="quote">&gt;&gt;&gt; +       pti_update_user_pgds(mm, true);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; You&#39;re holding mmap_sem, but there are code paths that touch page</span>
<span class="quote">&gt;&gt; tables that don&#39;t hold mmap_sem, such as the stack extension code.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Do they change user mappings? These are the only ones pti_update_user_pgds()</span>
<span class="quote">&gt; changes.</span>

I think they do -- it&#39;s the user stack.  page_table_lock may be more
appropriate, although I&#39;m far from an expert in this.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +bool pti_handle_segment_not_present(long error_code)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +       if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt;&gt;&gt; +               return false;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       if ((unsigned short)error_code != GDT_ENTRY_DEFAULT_USER_CS &lt;&lt; 3)</span>
<span class="quote">&gt;&gt;&gt; +               return false;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +       pti_reenable();</span>
<span class="quote">&gt;&gt;&gt; +       return true;</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Please don&#39;t.  You&#39;re trying to emulate the old behavior here, but</span>
<span class="quote">&gt;&gt; you&#39;re emulating it wrong.  In particular, you won&#39;t trap on LAR.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yes, I thought I’ll manage to address LAR, but failed. I thought you said</span>
<span class="quote">&gt; this is not a “show-stopper”. I’ll adapt your approach of using prctl, although</span>
<span class="quote">&gt; it really limits the benefit of this mechanism.</span>
<span class="quote">&gt;</span>

It&#39;s possible we could get away with adding the prctl but making the
default be that only the bitness that matches the program being run is
allowed.  After all, it&#39;s possible that CRIU is literally the only
program that switches bitness using the GDT.  (DOSEMU2 definitely does
cross-bitness stuff, but it uses the LDT as far as I know.)  And I&#39;ve
never been entirely sure that CRIU fully counts toward the Linux
&quot;don&#39;t break ABI&quot; guarantee.

Linus, how would you feel about, by default, preventing 64-bit
programs from long-jumping to __USER32_CS and vice versa?  I think it
has some value as a hardening measure.  I&#39;ve certainly engaged in some
exploit shenanigans myself that took advantage of the ability to long
jump/ret to change bitness at will.  This wouldn&#39;t affect users of
modify_ldt() -- 64-bit programs could still create and use their own
private 32-bit segments with modify_ldt(), and seccomp can (and
should!) prevent that in sandboxed programs.

In general, I prefer an approach where everything is explicit to an
approach where we almost, but not quite, emulate the weird historical
behavior.

Pavel and Cyrill, how annoying would it be if CRIU had to do an extra
arch_prctl() to enable its cross-bitness shenanigans when
checkpointing and restoring a 32-bit program?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Feb. 16, 2018, 12:08 a.m.</div>
<pre class="content">
On Thu, Feb 15, 2018 at 3:29 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; It&#39;s possible we could get away with adding the prctl but making the</span>
<span class="quote">&gt; default be that only the bitness that matches the program being run is</span>
<span class="quote">&gt; allowed.  After all, it&#39;s possible that CRIU is literally the only</span>
<span class="quote">&gt; program that switches bitness using the GDT.  (DOSEMU2 definitely does</span>
<span class="quote">&gt; cross-bitness stuff, but it uses the LDT as far as I know.)  And I&#39;ve</span>
<span class="quote">&gt; never been entirely sure that CRIU fully counts toward the Linux</span>
<span class="quote">&gt; &quot;don&#39;t break ABI&quot; guarantee.</span>

Ugh.

There are just _so_ many reasons to dislike that.

It&#39;s not that I don&#39;t think we could try to encourage it, but this
whole &quot;security depends on it being in sync&quot; seems really like a
fundamentally bad design.
<span class="quote">
&gt; Linus, how would you feel about, by default, preventing 64-bit</span>
<span class="quote">&gt; programs from long-jumping to __USER32_CS and vice versa?</span>

How? It&#39;s a standard GDT entry. Are you going to start switching the
GDT around every context switch?

I *thought* that user space can just do a far jump on its own. But
it&#39;s so long since I had to care that I may have forgotten all the
requirements for going between &quot;compatibility mode&quot; and real long
mode.

I just feel this all is a nightmare. I can see how you would want to
think that compatibility mode doesn&#39;t need PTI, but at the same time
it feels like a really risky move to do this.

I can see one thread being in compatibiilty mode, and another being in
long mode, and sharing the address space. But even with just one
thread, I&#39;m not seeing how you keep user mode from going from
compatibility mode to L mode with just a far jump.

But maybe you have some clever scheme in mind that guarantees that
there are no issues, or maybe I&#39;ve just forgotten all the details of
long mode vs compat mode.

               Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Feb. 16, 2018, 12:22 a.m.</div>
<pre class="content">
Linus Torvalds &lt;torvalds@linux-foundation.org&gt; wrote:
<span class="quote">
&gt; On Thu, Feb 15, 2018 at 3:29 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:</span>
<span class="quote">&gt;&gt; It&#39;s possible we could get away with adding the prctl but making the</span>
<span class="quote">&gt;&gt; default be that only the bitness that matches the program being run is</span>
<span class="quote">&gt;&gt; allowed.  After all, it&#39;s possible that CRIU is literally the only</span>
<span class="quote">&gt;&gt; program that switches bitness using the GDT.  (DOSEMU2 definitely does</span>
<span class="quote">&gt;&gt; cross-bitness stuff, but it uses the LDT as far as I know.)  And I&#39;ve</span>
<span class="quote">&gt;&gt; never been entirely sure that CRIU fully counts toward the Linux</span>
<span class="quote">&gt;&gt; &quot;don&#39;t break ABI&quot; guarantee.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ugh.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There are just _so_ many reasons to dislike that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s not that I don&#39;t think we could try to encourage it, but this</span>
<span class="quote">&gt; whole &quot;security depends on it being in sync&quot; seems really like a</span>
<span class="quote">&gt; fundamentally bad design.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Linus, how would you feel about, by default, preventing 64-bit</span>
<span class="quote">&gt;&gt; programs from long-jumping to __USER32_CS and vice versa?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How? It&#39;s a standard GDT entry. Are you going to start switching the</span>
<span class="quote">&gt; GDT around every context switch?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I *thought* that user space can just do a far jump on its own. But</span>
<span class="quote">&gt; it&#39;s so long since I had to care that I may have forgotten all the</span>
<span class="quote">&gt; requirements for going between &quot;compatibility mode&quot; and real long</span>
<span class="quote">&gt; mode.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I just feel this all is a nightmare. I can see how you would want to</span>
<span class="quote">&gt; think that compatibility mode doesn&#39;t need PTI, but at the same time</span>
<span class="quote">&gt; it feels like a really risky move to do this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I can see one thread being in compatibiilty mode, and another being in</span>
<span class="quote">&gt; long mode, and sharing the address space. But even with just one</span>
<span class="quote">&gt; thread, I&#39;m not seeing how you keep user mode from going from</span>
<span class="quote">&gt; compatibility mode to L mode with just a far jump.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But maybe you have some clever scheme in mind that guarantees that</span>
<span class="quote">&gt; there are no issues, or maybe I&#39;ve just forgotten all the details of</span>
<span class="quote">&gt; long mode vs compat mode.</span>

It is not too pretty, I agree, but it should do the work. There is only one
problematic descriptor that can be used to switch from compatibility-mode to
long-mode in the GDT (LDT descriptors always have the L-bit cleared).
Changing the descriptor&#39;s present bit on context switch when needed can do
the work.

I tried to do it transparently, and if long-mode is entered, by any thread,
restore PTI. There is one corner case I did not cover (LAR) and Andy felt
this scheme is too complicated. Unfortunately, I don’t have a better scheme
in mind.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=53881">Andrew Cooper</a> - Feb. 16, 2018, 12:35 a.m.</div>
<pre class="content">
On 16/02/2018 00:08, Linus Torvalds wrote:
<span class="quote">&gt; On Thu, Feb 15, 2018 at 3:29 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:</span>
<span class="quote">&gt;&gt; Linus, how would you feel about, by default, preventing 64-bit</span>
<span class="quote">&gt;&gt; programs from long-jumping to __USER32_CS and vice versa?</span>
<span class="quote">&gt; How? It&#39;s a standard GDT entry. Are you going to start switching the</span>
<span class="quote">&gt; GDT around every context switch?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I *thought* that user space can just do a far jump on its own. But</span>
<span class="quote">&gt; it&#39;s so long since I had to care that I may have forgotten all the</span>
<span class="quote">&gt; requirements for going between &quot;compatibility mode&quot; and real long</span>
<span class="quote">&gt; mode.</span>

Yes - it is just a straight far jump to switch between compat and long mode.

A evil^W cunning programmer can use the 286 world view and disable
segments by clearing the present bit to yield #NP[sel] on use, which is
liable to be rather faster than LGDT on a context switch.

Alternatively, set both the L and D (code segments only), or playing
with DPL/type can all yield #GP[sel] on use, but these probably aren&#39;t
as good options.

~Andrew
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Feb. 16, 2018, 12:42 a.m.</div>
<pre class="content">
On Thu, Feb 15, 2018 at 4:22 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; It is not too pretty, I agree, but it should do the work. There is only one</span>
<span class="quote">&gt; problematic descriptor that can be used to switch from compatibility-mode to</span>
<span class="quote">&gt; long-mode in the GDT (LDT descriptors always have the L-bit cleared).</span>
<span class="quote">&gt; Changing the descriptor&#39;s present bit on context switch when needed can do</span>
<span class="quote">&gt; the work.</span>

Sure, I can see it working, but it&#39;s some really shady stuff, and now
the scheduler needs to save/restore/check one more subtle bit.

And if you get it wrong, things will happily work, except you&#39;ve now
defeated PTI. But you&#39;ll never notice, because you won&#39;t be testing
for it, and the only people who will are the black hats.

This is exactly the &quot;security depends on it being in sync&quot; thing that
makes me go &quot;eww&quot; about the whole model. Get one thing wrong, and
you&#39;ll blow all the PTI code out of the water.

So now you tried to optimize one small case that most people won&#39;t
use, but the downside is that you may make all our PTI work (and all
the overhead for all the _normal_ cases) pointless.

                 Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Feb. 16, 2018, 3:03 a.m.</div>
<pre class="content">
On Fri, Feb 16, 2018 at 12:42 AM, Linus Torvalds
&lt;torvalds@linux-foundation.org&gt; wrote:
<span class="quote">&gt; On Thu, Feb 15, 2018 at 4:22 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It is not too pretty, I agree, but it should do the work. There is only one</span>
<span class="quote">&gt;&gt; problematic descriptor that can be used to switch from compatibility-mode to</span>
<span class="quote">&gt;&gt; long-mode in the GDT (LDT descriptors always have the L-bit cleared).</span>
<span class="quote">&gt;&gt; Changing the descriptor&#39;s present bit on context switch when needed can do</span>
<span class="quote">&gt;&gt; the work.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Sure, I can see it working, but it&#39;s some really shady stuff, and now</span>
<span class="quote">&gt; the scheduler needs to save/restore/check one more subtle bit.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And if you get it wrong, things will happily work, except you&#39;ve now</span>
<span class="quote">&gt; defeated PTI. But you&#39;ll never notice, because you won&#39;t be testing</span>
<span class="quote">&gt; for it, and the only people who will are the black hats.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is exactly the &quot;security depends on it being in sync&quot; thing that</span>
<span class="quote">&gt; makes me go &quot;eww&quot; about the whole model. Get one thing wrong, and</span>
<span class="quote">&gt; you&#39;ll blow all the PTI code out of the water.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So now you tried to optimize one small case that most people won&#39;t</span>
<span class="quote">&gt; use, but the downside is that you may make all our PTI work (and all</span>
<span class="quote">&gt; the overhead for all the _normal_ cases) pointless.</span>
<span class="quote">&gt;</span>

There&#39;s also the fact that, if this stuff goes in, we&#39;ll be
encouraging people to deploy 32-bit binaries.  Then they&#39;ll buy
Meltdown-fixed CPUs (or AMD CPUs!) and they may well continue running
32-bit binaries.  Sigh.  I&#39;m not totally a fan of this.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Feb. 16, 2018, 4:55 a.m.</div>
<pre class="content">
Andy Lutomirski &lt;luto@kernel.org&gt; wrote:
<span class="quote">
&gt; On Fri, Feb 16, 2018 at 12:42 AM, Linus Torvalds</span>
<span class="quote">&gt; &lt;torvalds@linux-foundation.org&gt; wrote:</span>
<span class="quote">&gt;&gt; On Thu, Feb 15, 2018 at 4:22 PM, Nadav Amit &lt;nadav.amit@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; It is not too pretty, I agree, but it should do the work. There is only one</span>
<span class="quote">&gt;&gt;&gt; problematic descriptor that can be used to switch from compatibility-mode to</span>
<span class="quote">&gt;&gt;&gt; long-mode in the GDT (LDT descriptors always have the L-bit cleared).</span>
<span class="quote">&gt;&gt;&gt; Changing the descriptor&#39;s present bit on context switch when needed can do</span>
<span class="quote">&gt;&gt;&gt; the work.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Sure, I can see it working, but it&#39;s some really shady stuff, and now</span>
<span class="quote">&gt;&gt; the scheduler needs to save/restore/check one more subtle bit.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; And if you get it wrong, things will happily work, except you&#39;ve now</span>
<span class="quote">&gt;&gt; defeated PTI. But you&#39;ll never notice, because you won&#39;t be testing</span>
<span class="quote">&gt;&gt; for it, and the only people who will are the black hats.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; This is exactly the &quot;security depends on it being in sync&quot; thing that</span>
<span class="quote">&gt;&gt; makes me go &quot;eww&quot; about the whole model. Get one thing wrong, and</span>
<span class="quote">&gt;&gt; you&#39;ll blow all the PTI code out of the water.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; So now you tried to optimize one small case that most people won&#39;t</span>
<span class="quote">&gt;&gt; use, but the downside is that you may make all our PTI work (and all</span>
<span class="quote">&gt;&gt; the overhead for all the _normal_ cases) pointless.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There&#39;s also the fact that, if this stuff goes in, we&#39;ll be</span>
<span class="quote">&gt; encouraging people to deploy 32-bit binaries.  Then they&#39;ll buy</span>
<span class="quote">&gt; Meltdown-fixed CPUs (or AMD CPUs!) and they may well continue running</span>
<span class="quote">&gt; 32-bit binaries.  Sigh.  I&#39;m not totally a fan of this.</span>

Ok, ok. Stop kicking the dead body...
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7">Cyrill Gorcunov</a> - Feb. 16, 2018, 7:11 a.m.</div>
<pre class="content">
On Thu, Feb 15, 2018 at 11:29:42PM +0000, Andy Lutomirski wrote:
...
<span class="quote">&gt; &gt;&gt;&gt; +bool pti_handle_segment_not_present(long error_code)</span>
<span class="quote">&gt; &gt;&gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt;&gt; +       if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt; &gt;&gt;&gt; +               return false;</span>
<span class="quote">&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt; +       if ((unsigned short)error_code != GDT_ENTRY_DEFAULT_USER_CS &lt;&lt; 3)</span>
<span class="quote">&gt; &gt;&gt;&gt; +               return false;</span>
<span class="quote">&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt; +       pti_reenable();</span>
<span class="quote">&gt; &gt;&gt;&gt; +       return true;</span>
<span class="quote">&gt; &gt;&gt;&gt; +}</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Please don&#39;t.  You&#39;re trying to emulate the old behavior here, but</span>
<span class="quote">&gt; &gt;&gt; you&#39;re emulating it wrong.  In particular, you won&#39;t trap on LAR.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Yes, I thought I’ll manage to address LAR, but failed. I thought you said</span>
<span class="quote">&gt; &gt; this is not a “show-stopper”. I’ll adapt your approach of using prctl, although</span>
<span class="quote">&gt; &gt; it really limits the benefit of this mechanism.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s possible we could get away with adding the prctl but making the</span>
<span class="quote">&gt; default be that only the bitness that matches the program being run is</span>
<span class="quote">&gt; allowed.  After all, it&#39;s possible that CRIU is literally the only</span>
<span class="quote">&gt; program that switches bitness using the GDT.  (DOSEMU2 definitely does</span>
<span class="quote">&gt; cross-bitness stuff, but it uses the LDT as far as I know.)  And I&#39;ve</span>
<span class="quote">&gt; never been entirely sure that CRIU fully counts toward the Linux</span>
<span class="quote">&gt; &quot;don&#39;t break ABI&quot; guarantee.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Linus, how would you feel about, by default, preventing 64-bit</span>
<span class="quote">&gt; programs from long-jumping to __USER32_CS and vice versa?  I think it</span>
<span class="quote">&gt; has some value as a hardening measure.  I&#39;ve certainly engaged in some</span>
<span class="quote">&gt; exploit shenanigans myself that took advantage of the ability to long</span>
<span class="quote">&gt; jump/ret to change bitness at will.  This wouldn&#39;t affect users of</span>
<span class="quote">&gt; modify_ldt() -- 64-bit programs could still create and use their own</span>
<span class="quote">&gt; private 32-bit segments with modify_ldt(), and seccomp can (and</span>
<span class="quote">&gt; should!) prevent that in sandboxed programs.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In general, I prefer an approach where everything is explicit to an</span>
<span class="quote">&gt; approach where we almost, but not quite, emulate the weird historical</span>
<span class="quote">&gt; behavior.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Pavel and Cyrill, how annoying would it be if CRIU had to do an extra</span>
<span class="quote">&gt; arch_prctl() to enable its cross-bitness shenanigans when</span>
<span class="quote">&gt; checkpointing and restoring a 32-bit program?</span>

I think this should not be a problem for criu (CC&#39;ing Dima, who has
been working on compat mode support in criu). As far as I remember
we initiate restoring of 32 bit tasks in native 64 bit mode (well,
ia32e to be precise :) mode and then, once everything is ready,
we changing the mode by doing a return to __USER32_CS descriptor.
So this won&#39;t be painful to add additional prctl call here.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Feb. 16, 2018, 3:20 p.m.</div>
<pre class="content">
<span class="quote">&gt;&gt; On Feb 15, 2018, at 4:08 PM, Linus Torvalds &lt;torvalds@linux-foundation.org&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; On Thu, Feb 15, 2018 at 3:29 PM, Andy Lutomirski &lt;luto@kernel.org&gt; wrote:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; It&#39;s possible we could get away with adding the prctl but making the</span>
<span class="quote">&gt;&gt; default be that only the bitness that matches the program being run is</span>
<span class="quote">&gt;&gt; allowed.  After all, it&#39;s possible that CRIU is literally the only</span>
<span class="quote">&gt;&gt; program that switches bitness using the GDT.  (DOSEMU2 definitely does</span>
<span class="quote">&gt;&gt; cross-bitness stuff, but it uses the LDT as far as I know.)  And I&#39;ve</span>
<span class="quote">&gt;&gt; never been entirely sure that CRIU fully counts toward the Linux</span>
<span class="quote">&gt;&gt; &quot;don&#39;t break ABI&quot; guarantee.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ugh.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There are just _so_ many reasons to dislike that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s not that I don&#39;t think we could try to encourage it, but this</span>
<span class="quote">&gt; whole &quot;security depends on it being in sync&quot; seems really like a</span>
<span class="quote">&gt; fundamentally bad design.</span>

If we&#39;re going to do Nadav&#39;s thing, I think we have no choice.  We could say that Nadav&#39;s idea of turning off PTI for 32-bit is just too messy, though.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; Linus, how would you feel about, by default, preventing 64-bit</span>
<span class="quote">&gt;&gt; programs from long-jumping to __USER32_CS and vice versa?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How? It&#39;s a standard GDT entry. Are you going to start switching the</span>
<span class="quote">&gt; GDT around every context switch?</span>

That&#39;s the idea.  We already switch out three GDT entries for TLS.  Switching two more isn&#39;t going to kill us.
<span class="quote">
&gt; </span>
<span class="quote">&gt; I *thought* that user space can just do a far jump on its own. But</span>
<span class="quote">&gt; it&#39;s so long since I had to care that I may have forgotten all the</span>
<span class="quote">&gt; requirements for going between &quot;compatibility mode&quot; and real long</span>
<span class="quote">&gt; mode.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I just feel this all is a nightmare. I can see how you would want to</span>
<span class="quote">&gt; think that compatibility mode doesn&#39;t need PTI, but at the same time</span>
<span class="quote">&gt; it feels like a really risky move to do this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I can see one thread being in compatibiilty mode, and another being in</span>
<span class="quote">&gt; long mode, and sharing the address space. But even with just one</span>
<span class="quote">&gt; thread, I&#39;m not seeing how you keep user mode from going from</span>
<span class="quote">&gt; compatibility mode to L mode with just a far jump.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But maybe you have some clever scheme in mind that guarantees that</span>
<span class="quote">&gt; there are no issues, or maybe I&#39;ve just forgotten all the details of</span>
<span class="quote">&gt; long mode vs compat mode.</span>

The clever scheme is that we have a new (maybe default) compat-and-i-mean-it mode that removes the DPL=3 L code segment from the GDT and prevents opportunistic SYSRET.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137481">Dmitry Safonov</a> - Feb. 16, 2018, 4:25 p.m.</div>
<pre class="content">
2018-02-15 20:02 GMT+00:00 Andy Lutomirski &lt;luto@kernel.org&gt;:
<span class="quote">&gt; On Thu, Feb 15, 2018 at 4:36 PM, Nadav Amit &lt;namit@vmware.com&gt; wrote:</span>
<span class="quote">&gt;&gt; Based on the understanding that there should be no way for userspace to</span>
<span class="quote">&gt;&gt; address the kernel-space from compatibility mode, disable it while</span>
<span class="quote">&gt;&gt; running in compatibility mode as long as the 64-bit code segment of the</span>
<span class="quote">&gt;&gt; user is not used.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Reenabling PTI is performed by restoring NX-bits to the userspace</span>
<span class="quote">&gt;&gt; mappings, flushing the TLBs, and notifying all the CPUs that use the</span>
<span class="quote">&gt;&gt; affected mm to disable PTI. Each core responds by removing the present</span>
<span class="quote">&gt;&gt; bit for the 64-bit code-segment, and marking that PTI is disabled on</span>
<span class="quote">&gt;&gt; that core.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I dislike this patch because it&#39;s conflating two things.  The patch</span>
<span class="quote">&gt; claims to merely disable PTI for compat tasks, whatever those are.</span>
<span class="quote">&gt; But it&#39;s also introducing a much stronger concept of what a compat</span>
<span class="quote">&gt; task is.  The kernel currently mostly doesn&#39;t care whether a task is</span>
<span class="quote">&gt; &quot;compat&quot; or not, and I think that most remaining code paths that do</span>
<span class="quote">&gt; care are buggy and should be removed.</span>

Yes, please, don&#39;t do a stronger concept..
Speaking from CRIU side, it C/R ia32 tasks with x86_64 binaries.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137481">Dmitry Safonov</a> - Feb. 16, 2018, 10:07 p.m.</div>
<pre class="content">
2018-02-16 7:11 GMT+00:00 Cyrill Gorcunov &lt;gorcunov@gmail.com&gt;:
<span class="quote">&gt; On Thu, Feb 15, 2018 at 11:29:42PM +0000, Andy Lutomirski wrote:</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +bool pti_handle_segment_not_present(long error_code)</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +       if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +               return false;</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +       if ((unsigned short)error_code != GDT_ENTRY_DEFAULT_USER_CS &lt;&lt; 3)</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +               return false;</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +       pti_reenable();</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +       return true;</span>
<span class="quote">&gt;&gt; &gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; Please don&#39;t.  You&#39;re trying to emulate the old behavior here, but</span>
<span class="quote">&gt;&gt; &gt;&gt; you&#39;re emulating it wrong.  In particular, you won&#39;t trap on LAR.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Yes, I thought I’ll manage to address LAR, but failed. I thought you said</span>
<span class="quote">&gt;&gt; &gt; this is not a “show-stopper”. I’ll adapt your approach of using prctl, although</span>
<span class="quote">&gt;&gt; &gt; it really limits the benefit of this mechanism.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; It&#39;s possible we could get away with adding the prctl but making the</span>
<span class="quote">&gt;&gt; default be that only the bitness that matches the program being run is</span>
<span class="quote">&gt;&gt; allowed.  After all, it&#39;s possible that CRIU is literally the only</span>
<span class="quote">&gt;&gt; program that switches bitness using the GDT.  (DOSEMU2 definitely does</span>
<span class="quote">&gt;&gt; cross-bitness stuff, but it uses the LDT as far as I know.)  And I&#39;ve</span>
<span class="quote">&gt;&gt; never been entirely sure that CRIU fully counts toward the Linux</span>
<span class="quote">&gt;&gt; &quot;don&#39;t break ABI&quot; guarantee.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Linus, how would you feel about, by default, preventing 64-bit</span>
<span class="quote">&gt;&gt; programs from long-jumping to __USER32_CS and vice versa?  I think it</span>
<span class="quote">&gt;&gt; has some value as a hardening measure.  I&#39;ve certainly engaged in some</span>
<span class="quote">&gt;&gt; exploit shenanigans myself that took advantage of the ability to long</span>
<span class="quote">&gt;&gt; jump/ret to change bitness at will.  This wouldn&#39;t affect users of</span>
<span class="quote">&gt;&gt; modify_ldt() -- 64-bit programs could still create and use their own</span>
<span class="quote">&gt;&gt; private 32-bit segments with modify_ldt(), and seccomp can (and</span>
<span class="quote">&gt;&gt; should!) prevent that in sandboxed programs.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; In general, I prefer an approach where everything is explicit to an</span>
<span class="quote">&gt;&gt; approach where we almost, but not quite, emulate the weird historical</span>
<span class="quote">&gt;&gt; behavior.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Pavel and Cyrill, how annoying would it be if CRIU had to do an extra</span>
<span class="quote">&gt;&gt; arch_prctl() to enable its cross-bitness shenanigans when</span>
<span class="quote">&gt;&gt; checkpointing and restoring a 32-bit program?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think this should not be a problem for criu (CC&#39;ing Dima, who has</span>
<span class="quote">&gt; been working on compat mode support in criu). As far as I remember</span>
<span class="quote">&gt; we initiate restoring of 32 bit tasks in native 64 bit mode (well,</span>
<span class="quote">&gt; ia32e to be precise :) mode and then, once everything is ready,</span>
<span class="quote">&gt; we changing the mode by doing a return to __USER32_CS descriptor.</span>
<span class="quote">&gt; So this won&#39;t be painful to add additional prctl call here.</span>

Yeah, restoring will still be easy..
But checkpointing will be harder if we can&#39;t switch to 64-bit mode.
ATM we have one 64-bit parasite binary, which does all seizing job
for both 64 and 32 bit binaries.
So, if you can&#39;t switch back to 64-bit from 32-bit mode, we&#39;ll need
to keep two parasites.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - Feb. 16, 2018, 10:11 p.m.</div>
<pre class="content">
Dmitry Safonov &lt;0x7f454c46@gmail.com&gt; wrote:
<span class="quote">
&gt; 2018-02-16 7:11 GMT+00:00 Cyrill Gorcunov &lt;gorcunov@gmail.com&gt;:</span>
<span class="quote">&gt;&gt; On Thu, Feb 15, 2018 at 11:29:42PM +0000, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; ...</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +bool pti_handle_segment_not_present(long error_code)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +       if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +               return false;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +       if ((unsigned short)error_code != GDT_ENTRY_DEFAULT_USER_CS &lt;&lt; 3)</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +               return false;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +       pti_reenable();</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +       return true;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Please don&#39;t.  You&#39;re trying to emulate the old behavior here, but</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; you&#39;re emulating it wrong.  In particular, you won&#39;t trap on LAR.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; Yes, I thought I’ll manage to address LAR, but failed. I thought you said</span>
<span class="quote">&gt;&gt;&gt;&gt; this is not a “show-stopper”. I’ll adapt your approach of using prctl, although</span>
<span class="quote">&gt;&gt;&gt;&gt; it really limits the benefit of this mechanism.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; It&#39;s possible we could get away with adding the prctl but making the</span>
<span class="quote">&gt;&gt;&gt; default be that only the bitness that matches the program being run is</span>
<span class="quote">&gt;&gt;&gt; allowed.  After all, it&#39;s possible that CRIU is literally the only</span>
<span class="quote">&gt;&gt;&gt; program that switches bitness using the GDT.  (DOSEMU2 definitely does</span>
<span class="quote">&gt;&gt;&gt; cross-bitness stuff, but it uses the LDT as far as I know.)  And I&#39;ve</span>
<span class="quote">&gt;&gt;&gt; never been entirely sure that CRIU fully counts toward the Linux</span>
<span class="quote">&gt;&gt;&gt; &quot;don&#39;t break ABI&quot; guarantee.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Linus, how would you feel about, by default, preventing 64-bit</span>
<span class="quote">&gt;&gt;&gt; programs from long-jumping to __USER32_CS and vice versa?  I think it</span>
<span class="quote">&gt;&gt;&gt; has some value as a hardening measure.  I&#39;ve certainly engaged in some</span>
<span class="quote">&gt;&gt;&gt; exploit shenanigans myself that took advantage of the ability to long</span>
<span class="quote">&gt;&gt;&gt; jump/ret to change bitness at will.  This wouldn&#39;t affect users of</span>
<span class="quote">&gt;&gt;&gt; modify_ldt() -- 64-bit programs could still create and use their own</span>
<span class="quote">&gt;&gt;&gt; private 32-bit segments with modify_ldt(), and seccomp can (and</span>
<span class="quote">&gt;&gt;&gt; should!) prevent that in sandboxed programs.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; In general, I prefer an approach where everything is explicit to an</span>
<span class="quote">&gt;&gt;&gt; approach where we almost, but not quite, emulate the weird historical</span>
<span class="quote">&gt;&gt;&gt; behavior.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Pavel and Cyrill, how annoying would it be if CRIU had to do an extra</span>
<span class="quote">&gt;&gt;&gt; arch_prctl() to enable its cross-bitness shenanigans when</span>
<span class="quote">&gt;&gt;&gt; checkpointing and restoring a 32-bit program?</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I think this should not be a problem for criu (CC&#39;ing Dima, who has</span>
<span class="quote">&gt;&gt; been working on compat mode support in criu). As far as I remember</span>
<span class="quote">&gt;&gt; we initiate restoring of 32 bit tasks in native 64 bit mode (well,</span>
<span class="quote">&gt;&gt; ia32e to be precise :) mode and then, once everything is ready,</span>
<span class="quote">&gt;&gt; we changing the mode by doing a return to __USER32_CS descriptor.</span>
<span class="quote">&gt;&gt; So this won&#39;t be painful to add additional prctl call here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, restoring will still be easy..</span>
<span class="quote">&gt; But checkpointing will be harder if we can&#39;t switch to 64-bit mode.</span>
<span class="quote">&gt; ATM we have one 64-bit parasite binary, which does all seizing job</span>
<span class="quote">&gt; for both 64 and 32 bit binaries.</span>
<span class="quote">&gt; So, if you can&#39;t switch back to 64-bit from 32-bit mode, we&#39;ll need</span>
<span class="quote">&gt; to keep two parasites.</span>

I can allow to switch back and forth by dynamically enabling/disabling PTI.
Andy, Dave, do you think it makes it a viable option? Should I respin
another version of the patch-set?
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/pti.h b/arch/x86/include/asm/pti.h</span>
<span class="p_header">index 78a333699874..d04954ebb0d4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pti.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pti.h</span>
<span class="p_chunk">@@ -31,6 +31,42 @@</span> <span class="p_context"> static inline void pti_update_user_cs64(unsigned short prev_pti_disable,</span>
 	write_gdt_entry(d, GDT_ENTRY_DEFAULT_USER_CS, &amp;user_cs, DESCTYPE_S);
 }
 
<span class="p_add">+void __pti_reenable(void);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pti_reenable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI) || !mm_pti_disable(current-&gt;mm))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	__pti_reenable();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __pti_disable(unsigned short type);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pti_disable(unsigned short type)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * To allow PTI to be disabled, we must:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * 1. Have PTI enabled.</span>
<span class="p_add">+	 * 2. Have SMEP enabled, since the lack of NX-bit on user mappings</span>
<span class="p_add">+	 *    raises general security concerns.</span>
<span class="p_add">+	 * 3. Have NX-bit enabled, since reenabling PTI has a corner case in</span>
<span class="p_add">+	 *    which the kernel tables are restored instead of those of those of</span>
<span class="p_add">+	 *    the user. Having NX-bit causes this scenario to trigger a spurious</span>
<span class="p_add">+	 *    page-fault when control is returned to the user, and allow the</span>
<span class="p_add">+	 *    entry code to restore the page-tables to their correct state.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI) ||</span>
<span class="p_add">+	    !static_cpu_has(X86_FEATURE_SMEP) ||</span>
<span class="p_add">+	    !static_cpu_has(X86_FEATURE_NX))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	__pti_disable(type);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+bool pti_handle_segment_not_present(long error_code);</span>
<span class="p_add">+</span>
 extern void pti_init(void);
 extern void pti_check_boottime_disable(void);
 #else
<span class="p_chunk">@@ -38,6 +74,9 @@</span> <span class="p_context"> static inline unsigned short mm_pti_disable(struct mm_struct *mm) { return 0; }</span>
 static inline unsigned short mm_pti_disable(struct mm_struct *mm);
 static inline void pti_update_user_cs64(unsigned short prev_pti_disable,
 					unsigned short next_pti_disable) { }
<span class="p_add">+static inline void pti_disable(unsigned short type) { }</span>
<span class="p_add">+static inline void pti_reenable(void) { }</span>
<span class="p_add">+static inline bool pti_handle_segment_not_present(long error_code) { return false; }</span>
 static inline void pti_check_boottime_disable(void) { }
 #endif
 
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index c75466232016..24d3429b4191 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -54,6 +54,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/vdso.h&gt;
 #include &lt;asm/intel_rdt_sched.h&gt;
 #include &lt;asm/unistd.h&gt;
<span class="p_add">+#include &lt;asm/pti.h&gt;</span>
 #ifdef CONFIG_IA32_EMULATION
 /* Not included via unistd.h */
 #include &lt;asm/unistd_32_ia32.h&gt;
<span class="p_chunk">@@ -530,8 +531,10 @@</span> <span class="p_context"> void set_personality_64bit(void)</span>
 	task_pt_regs(current)-&gt;orig_ax = __NR_execve;
 
 	/* Ensure the corresponding mm is not marked. */
<span class="p_del">-	if (current-&gt;mm)</span>
<span class="p_add">+	if (current-&gt;mm) {</span>
 		current-&gt;mm-&gt;context.ia32_compat = 0;
<span class="p_add">+		pti_reenable();</span>
<span class="p_add">+	}</span>
 
 	/* TBD: overwrites user setup. Should have two bits.
 	   But 64bit processes have always behaved this way,
<span class="p_chunk">@@ -545,8 +548,10 @@</span> <span class="p_context"> static void __set_personality_x32(void)</span>
 #ifdef CONFIG_X86_X32
 	clear_thread_flag(TIF_IA32);
 	set_thread_flag(TIF_X32);
<span class="p_del">-	if (current-&gt;mm)</span>
<span class="p_add">+	if (current-&gt;mm) {</span>
 		current-&gt;mm-&gt;context.ia32_compat = TIF_X32;
<span class="p_add">+		pti_reenable();</span>
<span class="p_add">+	}</span>
 	current-&gt;personality &amp;= ~READ_IMPLIES_EXEC;
 	/*
 	 * in_compat_syscall() uses the presence of the x32 syscall bit
<span class="p_chunk">@@ -566,8 +571,10 @@</span> <span class="p_context"> static void __set_personality_ia32(void)</span>
 #ifdef CONFIG_IA32_EMULATION
 	set_thread_flag(TIF_IA32);
 	clear_thread_flag(TIF_X32);
<span class="p_del">-	if (current-&gt;mm)</span>
<span class="p_add">+	if (current-&gt;mm) {</span>
 		current-&gt;mm-&gt;context.ia32_compat = TIF_IA32;
<span class="p_add">+		pti_disable(PTI_DISABLE_IA32);</span>
<span class="p_add">+	}</span>
 	current-&gt;personality |= force_personality32;
 	/* Prepare the first &quot;return&quot; to user space */
 	task_pt_regs(current)-&gt;orig_ax = __NR_ia32_execve;
<span class="p_header">diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c</span>
<span class="p_header">index 446c9ef8cfc3..65d8ccb20175 100644</span>
<span class="p_header">--- a/arch/x86/kernel/traps.c</span>
<span class="p_header">+++ b/arch/x86/kernel/traps.c</span>
<span class="p_chunk">@@ -61,6 +61,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/mpx.h&gt;
 #include &lt;asm/vm86.h&gt;
 #include &lt;asm/umip.h&gt;
<span class="p_add">+#include &lt;asm/pti.h&gt;</span>
 
 #ifdef CONFIG_X86_64
 #include &lt;asm/x86_init.h&gt;
<span class="p_chunk">@@ -315,7 +316,6 @@</span> <span class="p_context"> DO_ERROR(X86_TRAP_OF,     SIGSEGV, &quot;overflow&quot;,			overflow)</span>
 DO_ERROR(X86_TRAP_UD,     SIGILL,  &quot;invalid opcode&quot;,		invalid_op)
 DO_ERROR(X86_TRAP_OLD_MF, SIGFPE,  &quot;coprocessor segment overrun&quot;,coprocessor_segment_overrun)
 DO_ERROR(X86_TRAP_TS,     SIGSEGV, &quot;invalid TSS&quot;,		invalid_TSS)
<span class="p_del">-DO_ERROR(X86_TRAP_NP,     SIGBUS,  &quot;segment not present&quot;,	segment_not_present)</span>
 DO_ERROR(X86_TRAP_SS,     SIGBUS,  &quot;stack segment&quot;,		stack_segment)
 DO_ERROR(X86_TRAP_AC,     SIGBUS,  &quot;alignment check&quot;,		alignment_check)
 
<span class="p_chunk">@@ -529,6 +529,27 @@</span> <span class="p_context"> dotraplinkage void do_bounds(struct pt_regs *regs, long error_code)</span>
 	do_trap(X86_TRAP_BR, SIGSEGV, &quot;bounds&quot;, regs, error_code, NULL);
 }
 
<span class="p_add">+dotraplinkage void</span>
<span class="p_add">+do_segment_not_present(struct pt_regs *regs, long error_code)</span>
<span class="p_add">+{</span>
<span class="p_add">+	RCU_LOCKDEP_WARN(!rcu_is_watching(), &quot;entry code didn&#39;t wake RCU&quot;);</span>
<span class="p_add">+	cond_local_irq_enable(regs);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * 64-bit mode was disabled to prevent unnecessary page table isolation.</span>
<span class="p_add">+	 * Enable it, and from now on page-tables will be switched on kernel</span>
<span class="p_add">+	 * entry. Due to potential race conditions, we check the error code to</span>
<span class="p_add">+	 * see whether it references the __USER_CS, and ensure we only handle a</span>
<span class="p_add">+	 * single event per thread.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (pti_handle_segment_not_present(error_code))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	do_trap(X86_TRAP_NP, SIGTRAP, &quot;segment not present&quot;, regs, error_code,</span>
<span class="p_add">+		NULL);</span>
<span class="p_add">+}</span>
<span class="p_add">+NOKPROBE_SYMBOL(do_segment_not_present);</span>
<span class="p_add">+</span>
 dotraplinkage void
 do_general_protection(struct pt_regs *regs, long error_code)
 {
<span class="p_header">diff --git a/arch/x86/mm/pti.c b/arch/x86/mm/pti.c</span>
<span class="p_header">index a973a291a34d..18d936c5aa31 100644</span>
<span class="p_header">--- a/arch/x86/mm/pti.c</span>
<span class="p_header">+++ b/arch/x86/mm/pti.c</span>
<span class="p_chunk">@@ -148,6 +148,136 @@</span> <span class="p_context"> pgd_t __pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd)</span>
 	return pgd;
 }
 
<span class="p_add">+static void pti_update_user_pgds(struct mm_struct *mm, bool pti_enable)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(__supported_pte_mask &amp; _PAGE_NX))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; PTRS_PER_PGD / 2; i++) {</span>
<span class="p_add">+		pgd_t pgd, *pgdp = &amp;mm-&gt;pgd[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		pgd = *pgdp;</span>
<span class="p_add">+</span>
<span class="p_add">+		if ((pgd.pgd &amp; (_PAGE_USER|_PAGE_PRESENT)) !=</span>
<span class="p_add">+			(_PAGE_USER|_PAGE_PRESENT))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pti_enable)</span>
<span class="p_add">+			pgd.pgd |= _PAGE_NX;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			pgd.pgd &amp;= ~_PAGE_NX;</span>
<span class="p_add">+</span>
<span class="p_add">+		*pgdp = pgd;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void pti_cpu_update_func(void *info)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = (struct mm_struct *)info;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mm != this_cpu_read(cpu_tlbstate.loaded_mm))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Keep CS64 and CPU settings in sync despite potential concurrent</span>
<span class="p_add">+	 * updates.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	set_cpu_pti_disable(READ_ONCE(mm-&gt;context.pti_disable));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Reenable PTI after it was selectively disabled. Since the mm is in use, and</span>
<span class="p_add">+ * the NX-bit of the PGD may be set while the user still uses the kernel PGD, it</span>
<span class="p_add">+ * may lead to spurious page-faults. The page-fault handler should be able to</span>
<span class="p_add">+ * handle them gracefully.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __pti_reenable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	int cpu;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mm_pti_disable(mm))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Prevent spurious page-fault storm while we set the NX-bit and have</span>
<span class="p_add">+	 * yet not updated the per-CPU pti_disable flag.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mm_pti_disable(mm))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * First, mark the PTI is enabled. Although we do anything yet, we are</span>
<span class="p_add">+	 * safe as long as we do not reenable CS64. Since we did not update the</span>
<span class="p_add">+	 * page tables yet, this may lead to spurious page-faults, but we need</span>
<span class="p_add">+	 * the pti_disable in mm to be set for __pti_set_user_pgd() to do the</span>
<span class="p_add">+	 * right thing.  Holding mmap_sem would ensure matter we hold the</span>
<span class="p_add">+	 * mmap_sem to prevent them from swamping the system.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	mm-&gt;context.pti_disable = PTI_DISABLE_OFF;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Second, restore the NX bits. */</span>
<span class="p_add">+	pti_update_user_pgds(mm, true);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Third, flush the entire mm. By doing so we also force the processes</span>
<span class="p_add">+	 * to reload the correct page-table on return. This also provides a</span>
<span class="p_add">+	 * barrier before we restore USER_CS, ensuring we see the update</span>
<span class="p_add">+	 * cpumask.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flush_tlb_mm(mm);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Finally, restore CS64 to its correct state and mark that PTI is</span>
<span class="p_add">+	 * reenabled.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	cpu = get_cpu();</span>
<span class="p_add">+	pti_cpu_update_func(mm);</span>
<span class="p_add">+	if (cpumask_any_but(mm_cpumask(mm), cpu) &lt; nr_cpu_ids)</span>
<span class="p_add">+		smp_call_function_many(mm_cpumask(mm), pti_cpu_update_func,</span>
<span class="p_add">+				       mm, 1);</span>
<span class="p_add">+	put_cpu();</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __pti_disable(unsigned short type)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Give disabling options with higher value higher priority, as they are</span>
<span class="p_add">+	 * permanent and not transient. This also avoids re-disabling.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (mm_pti_disable(mm) &gt;= type)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	mm-&gt;context.pti_disable = type;</span>
<span class="p_add">+</span>
<span class="p_add">+	pti_update_user_pgds(mm, false);</span>
<span class="p_add">+</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	set_cpu_pti_disable(type);</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+bool pti_handle_segment_not_present(long error_code)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((unsigned short)error_code != GDT_ENTRY_DEFAULT_USER_CS &lt;&lt; 3)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	pti_reenable();</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Walk the user copy of the page tables (optionally) trying to allocate
  * page table pages on the way down.

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



