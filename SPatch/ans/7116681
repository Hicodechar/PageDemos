
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv5,5/7] mm: make compound_head() robust - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv5,5/7] mm: make compound_head() robust</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 3, 2015, 12:35 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1441283758-92774-6-git-send-email-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7116681/mbox/"
   >mbox</a>
|
   <a href="/patch/7116681/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7116681/">/patch/7116681/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id E56EC9F1CD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Sep 2015 12:37:07 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 13EDF2039C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Sep 2015 12:37:06 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 0A2252084D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Sep 2015 12:37:04 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755468AbbICMgq (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 3 Sep 2015 08:36:46 -0400
Received: from mga14.intel.com ([192.55.52.115]:65043 &quot;EHLO mga14.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1755407AbbICMgm (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 3 Sep 2015 08:36:42 -0400
Received: from orsmga002.jf.intel.com ([10.7.209.21])
	by fmsmga103.fm.intel.com with ESMTP; 03 Sep 2015 05:36:42 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.17,461,1437462000&quot;; 
	d=&quot;scan&#39;208,217&quot;;a=&quot;797010895&quot;
Received: from black.fi.intel.com ([10.237.72.213])
	by orsmga002.jf.intel.com with ESMTP; 03 Sep 2015 05:36:38 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 6E0BB28E; Thu,  3 Sep 2015 15:36:03 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Hugh Dickins &lt;hughd@google.com&gt;
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	David Rientjes &lt;rientjes@google.com&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	&quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;
Subject: [PATCHv5 5/7] mm: make compound_head() robust
Date: Thu,  3 Sep 2015 15:35:56 +0300
Message-Id: &lt;1441283758-92774-6-git-send-email-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.5.0
In-Reply-To: &lt;1441283758-92774-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
References: &lt;1441283758-92774-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - Sept. 3, 2015, 12:35 p.m.</div>
<pre class="content">
Hugh has pointed that compound_head() call can be unsafe in some
context. There&#39;s one example:

	CPU0					CPU1

isolate_migratepages_block()
  page_count()
    compound_head()
      !!PageTail() == true
					put_page()
					  tail-&gt;first_page = NULL
      head = tail-&gt;first_page
					alloc_pages(__GFP_COMP)
					   prep_compound_page()
					     tail-&gt;first_page = head
					     __SetPageTail(p);
      !!PageTail() == true
    &lt;head == NULL dereferencing&gt;

The race is pure theoretical. I don&#39;t it&#39;s possible to trigger it in
practice. But who knows.

We can fix the race by changing how encode PageTail() and compound_head()
within struct page to be able to update them in one shot.

The patch introduces page-&gt;compound_head into third double word block in
front of compound_dtor and compound_order. Bit 0 encodes PageTail() and
the rest bits are pointer to head page if bit zero is set.

The patch moves page-&gt;pmd_huge_pte out of word, just in case if an
architecture defines pgtable_t into something what can have the bit 0
set.

hugetlb_cgroup uses page-&gt;lru.next in the second tail page to store
pointer struct hugetlb_cgroup. The patch switch it to use page-&gt;private
in the second tail page instead. The space is free since -&gt;first_page is
removed from the union.

The patch also opens possibility to remove HUGETLB_CGROUP_MIN_ORDER
limitation, since there&#39;s now space in first tail page to store struct
hugetlb_cgroup pointer. But that&#39;s out of scope of the patch.

That means page-&gt;compound_head shares storage space with:

 - page-&gt;lru.next;
 - page-&gt;next;
 - page-&gt;rcu_head.next;

That&#39;s too long list to be absolutely sure, but looks like nobody uses
bit 0 of the word.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="acked-by">Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: David Rientjes &lt;rientjes@google.com&gt;
Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;
Cc: &quot;Paul E. McKenney&quot; &lt;paulmck@linux.vnet.ibm.com&gt;
Cc: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
---
 Documentation/vm/split_page_table_lock |  4 +-
 arch/xtensa/configs/iss_defconfig      |  1 -
 include/linux/hugetlb_cgroup.h         |  4 +-
 include/linux/mm.h                     | 53 ++--------------------
 include/linux/mm_types.h               | 18 ++++++--
 include/linux/page-flags.h             | 80 ++++++++--------------------------
 mm/Kconfig                             | 12 -----
 mm/debug.c                             |  5 ---
 mm/huge_memory.c                       |  3 +-
 mm/hugetlb.c                           |  8 +---
 mm/hugetlb_cgroup.c                    |  2 +-
 mm/internal.h                          |  4 +-
 mm/memory-failure.c                    |  7 ---
 mm/page_alloc.c                        | 46 +++++++++++--------
 mm/swap.c                              |  4 +-
 15 files changed, 77 insertions(+), 174 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Sept. 10, 2015, 10:54 a.m.</div>
<pre class="content">
On 09/03/2015 02:35 PM, Kirill A. Shutemov wrote:
<span class="quote">&gt; Hugh has pointed that compound_head() call can be unsafe in some</span>
<span class="quote">&gt; context. There&#39;s one example:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	CPU0					CPU1</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; isolate_migratepages_block()</span>
<span class="quote">&gt;   page_count()</span>
<span class="quote">&gt;     compound_head()</span>
<span class="quote">&gt;       !!PageTail() == true</span>
<span class="quote">&gt; 					put_page()</span>
<span class="quote">&gt; 					  tail-&gt;first_page = NULL</span>
<span class="quote">&gt;       head = tail-&gt;first_page</span>
<span class="quote">&gt; 					alloc_pages(__GFP_COMP)</span>
<span class="quote">&gt; 					   prep_compound_page()</span>
<span class="quote">&gt; 					     tail-&gt;first_page = head</span>
<span class="quote">&gt; 					     __SetPageTail(p);</span>
<span class="quote">&gt;       !!PageTail() == true</span>
<span class="quote">&gt;     &lt;head == NULL dereferencing&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The race is pure theoretical. I don&#39;t it&#39;s possible to trigger it in</span>
<span class="quote">&gt; practice. But who knows.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We can fix the race by changing how encode PageTail() and compound_head()</span>
<span class="quote">&gt; within struct page to be able to update them in one shot.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The patch introduces page-&gt;compound_head into third double word block in</span>
<span class="quote">&gt; front of compound_dtor and compound_order. Bit 0 encodes PageTail() and</span>
<span class="quote">&gt; the rest bits are pointer to head page if bit zero is set.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The patch moves page-&gt;pmd_huge_pte out of word, just in case if an</span>
<span class="quote">&gt; architecture defines pgtable_t into something what can have the bit 0</span>
<span class="quote">&gt; set.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; hugetlb_cgroup uses page-&gt;lru.next in the second tail page to store</span>
<span class="quote">&gt; pointer struct hugetlb_cgroup. The patch switch it to use page-&gt;private</span>
<span class="quote">&gt; in the second tail page instead. The space is free since -&gt;first_page is</span>
<span class="quote">&gt; removed from the union.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The patch also opens possibility to remove HUGETLB_CGROUP_MIN_ORDER</span>
<span class="quote">&gt; limitation, since there&#39;s now space in first tail page to store struct</span>
<span class="quote">&gt; hugetlb_cgroup pointer. But that&#39;s out of scope of the patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That means page-&gt;compound_head shares storage space with:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - page-&gt;lru.next;</span>
<span class="quote">&gt;  - page-&gt;next;</span>
<span class="quote">&gt;  - page-&gt;rcu_head.next;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s too long list to be absolutely sure, but looks like nobody uses</span>
<span class="quote">&gt; bit 0 of the word.</span>

Given the discussion about rcu_head, that should warrant some summary here :)
<span class="quote">
&gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; @@ -120,7 +120,13 @@ struct page {</span>
<span class="quote">&gt;  		};</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	/* Third double word block */</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Third double word block</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * WARNING: bit 0 of the first word encode PageTail(). That means</span>
<span class="quote">&gt; +	 * the rest users of the storage space MUST NOT use the bit to</span>
<span class="quote">&gt; +	 * avoid collision and false-positive PageTail().</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt;  	union {</span>
<span class="quote">&gt;  		struct list_head lru;	/* Pageout list, eg. active_list</span>
<span class="quote">&gt;  					 * protected by zone-&gt;lru_lock !</span>
<span class="quote">&gt; @@ -143,12 +149,19 @@ struct page {</span>
<span class="quote">&gt;  						 */</span>
<span class="quote">&gt;  		/* First tail page of compound page */</span>

&quot;First tail&quot; doesn&#39;t apply for compound_head.
<span class="quote">
&gt; index 097c7a4bfbd9..330377f83ac7 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1686,8 +1686,7 @@ static void __split_huge_page_refcount(struct page *page,</span>
<span class="quote">&gt;  				      (1L &lt;&lt; PG_unevictable)));</span>
<span class="quote">&gt;  		page_tail-&gt;flags |= (1L &lt;&lt; PG_dirty);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		/* clear PageTail before overwriting first_page */</span>
<span class="quote">&gt; -		smp_wmb();</span>
<span class="quote">&gt; +		clear_compound_head(page_tail);</span>

I would sleep better if this was done before setting all the page-&gt;flags above,
previously, PageTail was cleared by the first operation which is
&quot;page_tail-&gt;flags &amp;= ~PAGE_FLAGS_CHECK_AT_PREP;&quot;
I do realize that it doesn&#39;t use WRITE_ONCE, so it might have been theoretically
broken already, if it does matter.
<span class="quote">
&gt; diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="quote">&gt; index 36b23f1e2ca6..89e21a07080a 100644</span>
<span class="quote">&gt; --- a/mm/internal.h</span>
<span class="quote">&gt; +++ b/mm/internal.h</span>
<span class="quote">&gt; @@ -61,9 +61,9 @@ static inline void __get_page_tail_foll(struct page *page,</span>
<span class="quote">&gt;  	 * speculative page access (like in</span>
<span class="quote">&gt;  	 * page_cache_get_speculative()) on tail pages.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;first_page-&gt;_count) &lt;= 0, page);</span>
<span class="quote">&gt; +	VM_BUG_ON_PAGE(atomic_read(&amp;compound_head(page)-&gt;_count) &lt;= 0, page);</span>
<span class="quote">&gt;  	if (get_page_head)</span>
<span class="quote">&gt; -		atomic_inc(&amp;page-&gt;first_page-&gt;_count);</span>
<span class="quote">&gt; +		atomic_inc(&amp;compound_head(page)-&gt;_count);</span>

Doing another compound_head() seems like overkill when this code already assumes
PageTail. All callers do it after if (PageTail()) which means they already did
READ_ONCE(page-&gt;compound_head) and here they do another one. Potentially with
different result in bit 0, which would be a subtle bug, that could be
interesting to catch with some VM_BUG_ON. I don&#39;t know if a direct plain
page-&gt;compound_head access here instead of compound_head() would also result in
a re-read, since the previous access did use READ_ONCE(). Maybe it would be best
to reorganize the code here and in the 3 call sites so that the READ_ONCE() used
to determine PageTail also obtains the compound head pointer.

Some of that is probably made moot by your other series, but better let&#39;s think
of this series as standalone first.
<span class="quote">
&gt;  	get_huge_page_tail(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="quote">&gt; index 1f4446a90cef..4d1a5de9653d 100644</span>
<span class="quote">&gt; --- a/mm/memory-failure.c</span>
<span class="quote">&gt; +++ b/mm/memory-failure.c</span>
<span class="quote">&gt; @@ -787,8 +787,6 @@ static int me_huge_page(struct page *p, unsigned long pfn)</span>
<span class="quote">&gt;  #define lru		(1UL &lt;&lt; PG_lru)</span>
<span class="quote">&gt;  #define swapbacked	(1UL &lt;&lt; PG_swapbacked)</span>
<span class="quote">&gt;  #define head		(1UL &lt;&lt; PG_head)</span>
<span class="quote">&gt; -#define tail		(1UL &lt;&lt; PG_tail)</span>
<span class="quote">&gt; -#define compound	(1UL &lt;&lt; PG_compound)</span>
<span class="quote">&gt;  #define slab		(1UL &lt;&lt; PG_slab)</span>
<span class="quote">&gt;  #define reserved	(1UL &lt;&lt; PG_reserved)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -811,12 +809,7 @@ static struct page_state {</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	{ slab,		slab,		MF_MSG_SLAB,	me_kernel },</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
<span class="quote">&gt;  	{ head,		head,		MF_MSG_HUGE,		me_huge_page },</span>
<span class="quote">&gt; -	{ tail,		tail,		MF_MSG_HUGE,		me_huge_page },</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -	{ compound,	compound,	MF_MSG_HUGE,		me_huge_page },</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	{ sc|dirty,	sc|dirty,	MF_MSG_DIRTY_SWAPCACHE,	me_swapcache_dirty },</span>
<span class="quote">&gt;  	{ sc|dirty,	sc,		MF_MSG_CLEAN_SWAPCACHE,	me_swapcache_clean },</span>
<span class="quote">&gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; index c6733cc3cbce..a56ad53ff553 100644</span>
<span class="quote">&gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; @@ -424,15 +424,15 @@ out:</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Higher-order pages are called &quot;compound pages&quot;.  They are structured thusly:</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * The first PAGE_SIZE page is called the &quot;head page&quot;.</span>
<span class="quote">&gt; + * The first PAGE_SIZE page is called the &quot;head page&quot; and have PG_head set.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;.</span>
<span class="quote">&gt; + * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;. PageTail() is encoded</span>
<span class="quote">&gt; + * in bit 0 of page-&gt;compound_head. The rest of bits is pointer to head page.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * All pages have PG_compound set.  All tail pages have their -&gt;first_page</span>
<span class="quote">&gt; - * pointing at the head page.</span>
<span class="quote">&gt; + * The first tail page&#39;s -&gt;compound_dtor holds the offset in array of compound</span>
<span class="quote">&gt; + * page destructors. See compound_page_dtors.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * The first tail page&#39;s -&gt;lru.next holds the address of the compound page&#39;s</span>
<span class="quote">&gt; - * put_page() function.  Its -&gt;lru.prev holds the order of allocation.</span>
<span class="quote">&gt; + * The first tail page&#39;s -&gt;compound_order holds the order of allocation.</span>
<span class="quote">&gt;   * This usage means that zero-order pages may not be compound.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -452,10 +452,7 @@ void prep_compound_page(struct page *page, unsigned long order)</span>
<span class="quote">&gt;  	for (i = 1; i &lt; nr_pages; i++) {</span>
<span class="quote">&gt;  		struct page *p = page + i;</span>
<span class="quote">&gt;  		set_page_count(p, 0);</span>
<span class="quote">&gt; -		p-&gt;first_page = page;</span>
<span class="quote">&gt; -		/* Make sure p-&gt;first_page is always valid for PageTail() */</span>
<span class="quote">&gt; -		smp_wmb();</span>
<span class="quote">&gt; -		__SetPageTail(p);</span>
<span class="quote">&gt; +		set_compound_head(p, page);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -830,17 +827,30 @@ static void free_one_page(struct zone *zone,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int free_tail_pages_check(struct page *head_page, struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (!IS_ENABLED(CONFIG_DEBUG_VM))</span>
<span class="quote">&gt; -		return 0;</span>
<span class="quote">&gt; +	int ret = 1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We rely page-&gt;lru.next never has bit 0 set, unless the page</span>
<span class="quote">&gt; +	 * is PageTail(). Let&#39;s make sure that&#39;s true even for poisoned -&gt;lru.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	BUILD_BUG_ON((unsigned long)LIST_POISON1 &amp; 1);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!IS_ENABLED(CONFIG_DEBUG_VM)) {</span>
<span class="quote">&gt; +		ret = 0;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	if (unlikely(!PageTail(page))) {</span>
<span class="quote">&gt;  		bad_page(page, &quot;PageTail not set&quot;, 0);</span>
<span class="quote">&gt; -		return 1;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	if (unlikely(page-&gt;first_page != head_page)) {</span>
<span class="quote">&gt; -		bad_page(page, &quot;first_page not consistent&quot;, 0);</span>
<span class="quote">&gt; -		return 1;</span>
<span class="quote">&gt; +	if (unlikely(compound_head(page) != head_page)) {</span>
<span class="quote">&gt; +		bad_page(page, &quot;compound_head not consistent&quot;, 0);</span>
<span class="quote">&gt; +		goto out;</span>

Same here, although for a DEBUG_VM config only it&#39;s not as important.
<span class="quote">
&gt;  	}</span>
<span class="quote">&gt; -	return 0;</span>
<span class="quote">&gt; +	ret = 0;</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt; +	clear_compound_head(page);</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void __meminit __init_single_page(struct page *page, unsigned long pfn,</span>
<span class="quote">&gt; @@ -888,6 +898,8 @@ static void init_reserved_page(unsigned long pfn)</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline void init_reserved_page(unsigned long pfn)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	/* Avoid false-positive PageTail() */</span>
<span class="quote">&gt; +	INIT_LIST_HEAD(&amp;pfn_to_page(pfn)-&gt;lru);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="quote">&gt; index a3a0a2f1f7c3..faa9e1687dea 100644</span>
<span class="quote">&gt; --- a/mm/swap.c</span>
<span class="quote">&gt; +++ b/mm/swap.c</span>
<span class="quote">&gt; @@ -200,7 +200,7 @@ out_put_single:</span>
<span class="quote">&gt;  				__put_single_page(page);</span>
<span class="quote">&gt;  			return;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(page_head != page-&gt;first_page, page);</span>
<span class="quote">&gt; +		VM_BUG_ON_PAGE(page_head != compound_head(page), page);</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * We can release the refcount taken by</span>
<span class="quote">&gt;  		 * get_page_unless_zero() now that</span>
<span class="quote">&gt; @@ -261,7 +261,7 @@ static void put_compound_page(struct page *page)</span>
<span class="quote">&gt;  	 *  Case 3 is possible, as we may race with</span>
<span class="quote">&gt;  	 *  __split_huge_page_refcount tearing down a THP page.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	page_head = compound_head_by_tail(page);</span>
<span class="quote">&gt; +	page_head = compound_head(page);</span>

This is also in a path after PageTail() is true.
<span class="quote">
&gt;  	if (!__compound_tail_refcounted(page_head))</span>
<span class="quote">&gt;  		put_unrefcounted_compound_page(page_head, page);</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt; </span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Sept. 11, 2015, 1:35 p.m.</div>
<pre class="content">
On Thu, Sep 10, 2015 at 12:54:08PM +0200, Vlastimil Babka wrote:
<span class="quote">&gt; On 09/03/2015 02:35 PM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; Hugh has pointed that compound_head() call can be unsafe in some</span>
<span class="quote">&gt; &gt; context. There&#39;s one example:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	CPU0					CPU1</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; isolate_migratepages_block()</span>
<span class="quote">&gt; &gt;   page_count()</span>
<span class="quote">&gt; &gt;     compound_head()</span>
<span class="quote">&gt; &gt;       !!PageTail() == true</span>
<span class="quote">&gt; &gt; 					put_page()</span>
<span class="quote">&gt; &gt; 					  tail-&gt;first_page = NULL</span>
<span class="quote">&gt; &gt;       head = tail-&gt;first_page</span>
<span class="quote">&gt; &gt; 					alloc_pages(__GFP_COMP)</span>
<span class="quote">&gt; &gt; 					   prep_compound_page()</span>
<span class="quote">&gt; &gt; 					     tail-&gt;first_page = head</span>
<span class="quote">&gt; &gt; 					     __SetPageTail(p);</span>
<span class="quote">&gt; &gt;       !!PageTail() == true</span>
<span class="quote">&gt; &gt;     &lt;head == NULL dereferencing&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The race is pure theoretical. I don&#39;t it&#39;s possible to trigger it in</span>
<span class="quote">&gt; &gt; practice. But who knows.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We can fix the race by changing how encode PageTail() and compound_head()</span>
<span class="quote">&gt; &gt; within struct page to be able to update them in one shot.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The patch introduces page-&gt;compound_head into third double word block in</span>
<span class="quote">&gt; &gt; front of compound_dtor and compound_order. Bit 0 encodes PageTail() and</span>
<span class="quote">&gt; &gt; the rest bits are pointer to head page if bit zero is set.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The patch moves page-&gt;pmd_huge_pte out of word, just in case if an</span>
<span class="quote">&gt; &gt; architecture defines pgtable_t into something what can have the bit 0</span>
<span class="quote">&gt; &gt; set.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; hugetlb_cgroup uses page-&gt;lru.next in the second tail page to store</span>
<span class="quote">&gt; &gt; pointer struct hugetlb_cgroup. The patch switch it to use page-&gt;private</span>
<span class="quote">&gt; &gt; in the second tail page instead. The space is free since -&gt;first_page is</span>
<span class="quote">&gt; &gt; removed from the union.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The patch also opens possibility to remove HUGETLB_CGROUP_MIN_ORDER</span>
<span class="quote">&gt; &gt; limitation, since there&#39;s now space in first tail page to store struct</span>
<span class="quote">&gt; &gt; hugetlb_cgroup pointer. But that&#39;s out of scope of the patch.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That means page-&gt;compound_head shares storage space with:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;  - page-&gt;lru.next;</span>
<span class="quote">&gt; &gt;  - page-&gt;next;</span>
<span class="quote">&gt; &gt;  - page-&gt;rcu_head.next;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; That&#39;s too long list to be absolutely sure, but looks like nobody uses</span>
<span class="quote">&gt; &gt; bit 0 of the word.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Given the discussion about rcu_head, that should warrant some summary here :)</span>

Agreed.
<span class="quote">
&gt; &gt; --- a/include/linux/mm_types.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/mm_types.h</span>
<span class="quote">&gt; &gt; @@ -120,7 +120,13 @@ struct page {</span>
<span class="quote">&gt; &gt;  		};</span>
<span class="quote">&gt; &gt;  	};</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	/* Third double word block */</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Third double word block</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * WARNING: bit 0 of the first word encode PageTail(). That means</span>
<span class="quote">&gt; &gt; +	 * the rest users of the storage space MUST NOT use the bit to</span>
<span class="quote">&gt; &gt; +	 * avoid collision and false-positive PageTail().</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt;  	union {</span>
<span class="quote">&gt; &gt;  		struct list_head lru;	/* Pageout list, eg. active_list</span>
<span class="quote">&gt; &gt;  					 * protected by zone-&gt;lru_lock !</span>
<span class="quote">&gt; &gt; @@ -143,12 +149,19 @@ struct page {</span>
<span class="quote">&gt; &gt;  						 */</span>
<span class="quote">&gt; &gt;  		/* First tail page of compound page */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &quot;First tail&quot; doesn&#39;t apply for compound_head.</span>

I&#39;ll adjust comments.
<span class="quote"> 
&gt; &gt; index 097c7a4bfbd9..330377f83ac7 100644</span>
<span class="quote">&gt; &gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; @@ -1686,8 +1686,7 @@ static void __split_huge_page_refcount(struct page *page,</span>
<span class="quote">&gt; &gt;  				      (1L &lt;&lt; PG_unevictable)));</span>
<span class="quote">&gt; &gt;  		page_tail-&gt;flags |= (1L &lt;&lt; PG_dirty);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -		/* clear PageTail before overwriting first_page */</span>
<span class="quote">&gt; &gt; -		smp_wmb();</span>
<span class="quote">&gt; &gt; +		clear_compound_head(page_tail);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would sleep better if this was done before setting all the page-&gt;flags above,</span>
<span class="quote">&gt; previously, PageTail was cleared by the first operation which is</span>
<span class="quote">&gt; &quot;page_tail-&gt;flags &amp;= ~PAGE_FLAGS_CHECK_AT_PREP;&quot;</span>
<span class="quote">&gt; I do realize that it doesn&#39;t use WRITE_ONCE, so it might have been theoretically</span>
<span class="quote">&gt; broken already, if it does matter.</span>

Right. Nothing enforces particular order. If we really need to have some
ordering on PageTail() vs. page-&gt;flags let&#39;s define it, but so far I
don&#39;t see a reason to change this part.
<span class="quote">
&gt; &gt; diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="quote">&gt; &gt; index 36b23f1e2ca6..89e21a07080a 100644</span>
<span class="quote">&gt; &gt; --- a/mm/internal.h</span>
<span class="quote">&gt; &gt; +++ b/mm/internal.h</span>
<span class="quote">&gt; &gt; @@ -61,9 +61,9 @@ static inline void __get_page_tail_foll(struct page *page,</span>
<span class="quote">&gt; &gt;  	 * speculative page access (like in</span>
<span class="quote">&gt; &gt;  	 * page_cache_get_speculative()) on tail pages.</span>
<span class="quote">&gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; -	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;first_page-&gt;_count) &lt;= 0, page);</span>
<span class="quote">&gt; &gt; +	VM_BUG_ON_PAGE(atomic_read(&amp;compound_head(page)-&gt;_count) &lt;= 0, page);</span>
<span class="quote">&gt; &gt;  	if (get_page_head)</span>
<span class="quote">&gt; &gt; -		atomic_inc(&amp;page-&gt;first_page-&gt;_count);</span>
<span class="quote">&gt; &gt; +		atomic_inc(&amp;compound_head(page)-&gt;_count);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Doing another compound_head() seems like overkill when this code already assumes</span>
<span class="quote">&gt; PageTail.</span>

&quot;Overkill&quot;? It&#39;s too strong wording for re-read hot cache line.
<span class="quote">
&gt; All callers do it after if (PageTail()) which means they already did</span>
<span class="quote">&gt; READ_ONCE(page-&gt;compound_head) and here they do another one. Potentially with</span>
<span class="quote">&gt; different result in bit 0, which would be a subtle bug, that could be</span>
<span class="quote">&gt; interesting to catch with some VM_BUG_ON. I don&#39;t know if a direct plain</span>
<span class="quote">&gt; page-&gt;compound_head access here instead of compound_head() would also result in</span>
<span class="quote">&gt; a re-read, since the previous access did use READ_ONCE(). Maybe it would be best</span>
<span class="quote">&gt; to reorganize the code here and in the 3 call sites so that the READ_ONCE() used</span>
<span class="quote">&gt; to determine PageTail also obtains the compound head pointer.</span>

All we would possbily win by the change is few bytes in code. Additional
READ_ONCE() only affect code generation. It doesn&#39;t introduce any cpu
barriers. The cache line with compound_head is in L1 anyway.

I don&#39;t see justification to change this part too. If you think we can
gain something by reworking this code, feel free to propose patch on top.
<span class="quote">
&gt; Some of that is probably made moot by your other series, but better let&#39;s think</span>
<span class="quote">&gt; of this series as standalone first.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;  	get_huge_page_tail(page);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="quote">&gt; &gt; index 1f4446a90cef..4d1a5de9653d 100644</span>
<span class="quote">&gt; &gt; --- a/mm/memory-failure.c</span>
<span class="quote">&gt; &gt; +++ b/mm/memory-failure.c</span>
<span class="quote">&gt; &gt; @@ -787,8 +787,6 @@ static int me_huge_page(struct page *p, unsigned long pfn)</span>
<span class="quote">&gt; &gt;  #define lru		(1UL &lt;&lt; PG_lru)</span>
<span class="quote">&gt; &gt;  #define swapbacked	(1UL &lt;&lt; PG_swapbacked)</span>
<span class="quote">&gt; &gt;  #define head		(1UL &lt;&lt; PG_head)</span>
<span class="quote">&gt; &gt; -#define tail		(1UL &lt;&lt; PG_tail)</span>
<span class="quote">&gt; &gt; -#define compound	(1UL &lt;&lt; PG_compound)</span>
<span class="quote">&gt; &gt;  #define slab		(1UL &lt;&lt; PG_slab)</span>
<span class="quote">&gt; &gt;  #define reserved	(1UL &lt;&lt; PG_reserved)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -811,12 +809,7 @@ static struct page_state {</span>
<span class="quote">&gt; &gt;  	 */</span>
<span class="quote">&gt; &gt;  	{ slab,		slab,		MF_MSG_SLAB,	me_kernel },</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
<span class="quote">&gt; &gt;  	{ head,		head,		MF_MSG_HUGE,		me_huge_page },</span>
<span class="quote">&gt; &gt; -	{ tail,		tail,		MF_MSG_HUGE,		me_huge_page },</span>
<span class="quote">&gt; &gt; -#else</span>
<span class="quote">&gt; &gt; -	{ compound,	compound,	MF_MSG_HUGE,		me_huge_page },</span>
<span class="quote">&gt; &gt; -#endif</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	{ sc|dirty,	sc|dirty,	MF_MSG_DIRTY_SWAPCACHE,	me_swapcache_dirty },</span>
<span class="quote">&gt; &gt;  	{ sc|dirty,	sc,		MF_MSG_CLEAN_SWAPCACHE,	me_swapcache_clean },</span>
<span class="quote">&gt; &gt; diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt; &gt; index c6733cc3cbce..a56ad53ff553 100644</span>
<span class="quote">&gt; &gt; --- a/mm/page_alloc.c</span>
<span class="quote">&gt; &gt; +++ b/mm/page_alloc.c</span>
<span class="quote">&gt; &gt; @@ -424,15 +424,15 @@ out:</span>
<span class="quote">&gt; &gt;  /*</span>
<span class="quote">&gt; &gt;   * Higher-order pages are called &quot;compound pages&quot;.  They are structured thusly:</span>
<span class="quote">&gt; &gt;   *</span>
<span class="quote">&gt; &gt; - * The first PAGE_SIZE page is called the &quot;head page&quot;.</span>
<span class="quote">&gt; &gt; + * The first PAGE_SIZE page is called the &quot;head page&quot; and have PG_head set.</span>
<span class="quote">&gt; &gt;   *</span>
<span class="quote">&gt; &gt; - * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;.</span>
<span class="quote">&gt; &gt; + * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;. PageTail() is encoded</span>
<span class="quote">&gt; &gt; + * in bit 0 of page-&gt;compound_head. The rest of bits is pointer to head page.</span>
<span class="quote">&gt; &gt;   *</span>
<span class="quote">&gt; &gt; - * All pages have PG_compound set.  All tail pages have their -&gt;first_page</span>
<span class="quote">&gt; &gt; - * pointing at the head page.</span>
<span class="quote">&gt; &gt; + * The first tail page&#39;s -&gt;compound_dtor holds the offset in array of compound</span>
<span class="quote">&gt; &gt; + * page destructors. See compound_page_dtors.</span>
<span class="quote">&gt; &gt;   *</span>
<span class="quote">&gt; &gt; - * The first tail page&#39;s -&gt;lru.next holds the address of the compound page&#39;s</span>
<span class="quote">&gt; &gt; - * put_page() function.  Its -&gt;lru.prev holds the order of allocation.</span>
<span class="quote">&gt; &gt; + * The first tail page&#39;s -&gt;compound_order holds the order of allocation.</span>
<span class="quote">&gt; &gt;   * This usage means that zero-order pages may not be compound.</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -452,10 +452,7 @@ void prep_compound_page(struct page *page, unsigned long order)</span>
<span class="quote">&gt; &gt;  	for (i = 1; i &lt; nr_pages; i++) {</span>
<span class="quote">&gt; &gt;  		struct page *p = page + i;</span>
<span class="quote">&gt; &gt;  		set_page_count(p, 0);</span>
<span class="quote">&gt; &gt; -		p-&gt;first_page = page;</span>
<span class="quote">&gt; &gt; -		/* Make sure p-&gt;first_page is always valid for PageTail() */</span>
<span class="quote">&gt; &gt; -		smp_wmb();</span>
<span class="quote">&gt; &gt; -		__SetPageTail(p);</span>
<span class="quote">&gt; &gt; +		set_compound_head(p, page);</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -830,17 +827,30 @@ static void free_one_page(struct zone *zone,</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static int free_tail_pages_check(struct page *head_page, struct page *page)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; -	if (!IS_ENABLED(CONFIG_DEBUG_VM))</span>
<span class="quote">&gt; &gt; -		return 0;</span>
<span class="quote">&gt; &gt; +	int ret = 1;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * We rely page-&gt;lru.next never has bit 0 set, unless the page</span>
<span class="quote">&gt; &gt; +	 * is PageTail(). Let&#39;s make sure that&#39;s true even for poisoned -&gt;lru.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	BUILD_BUG_ON((unsigned long)LIST_POISON1 &amp; 1);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!IS_ENABLED(CONFIG_DEBUG_VM)) {</span>
<span class="quote">&gt; &gt; +		ret = 0;</span>
<span class="quote">&gt; &gt; +		goto out;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt;  	if (unlikely(!PageTail(page))) {</span>
<span class="quote">&gt; &gt;  		bad_page(page, &quot;PageTail not set&quot;, 0);</span>
<span class="quote">&gt; &gt; -		return 1;</span>
<span class="quote">&gt; &gt; +		goto out;</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt; -	if (unlikely(page-&gt;first_page != head_page)) {</span>
<span class="quote">&gt; &gt; -		bad_page(page, &quot;first_page not consistent&quot;, 0);</span>
<span class="quote">&gt; &gt; -		return 1;</span>
<span class="quote">&gt; &gt; +	if (unlikely(compound_head(page) != head_page)) {</span>
<span class="quote">&gt; &gt; +		bad_page(page, &quot;compound_head not consistent&quot;, 0);</span>
<span class="quote">&gt; &gt; +		goto out;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Same here, although for a DEBUG_VM config only it&#39;s not as important.</span>

Ditto.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt; -	return 0;</span>
<span class="quote">&gt; &gt; +	ret = 0;</span>
<span class="quote">&gt; &gt; +out:</span>
<span class="quote">&gt; &gt; +	clear_compound_head(page);</span>
<span class="quote">&gt; &gt; +	return ret;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static void __meminit __init_single_page(struct page *page, unsigned long pfn,</span>
<span class="quote">&gt; &gt; @@ -888,6 +898,8 @@ static void init_reserved_page(unsigned long pfn)</span>
<span class="quote">&gt; &gt;  #else</span>
<span class="quote">&gt; &gt;  static inline void init_reserved_page(unsigned long pfn)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +	/* Avoid false-positive PageTail() */</span>
<span class="quote">&gt; &gt; +	INIT_LIST_HEAD(&amp;pfn_to_page(pfn)-&gt;lru);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="quote">&gt; &gt; index a3a0a2f1f7c3..faa9e1687dea 100644</span>
<span class="quote">&gt; &gt; --- a/mm/swap.c</span>
<span class="quote">&gt; &gt; +++ b/mm/swap.c</span>
<span class="quote">&gt; &gt; @@ -200,7 +200,7 @@ out_put_single:</span>
<span class="quote">&gt; &gt;  				__put_single_page(page);</span>
<span class="quote">&gt; &gt;  			return;</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt; -		VM_BUG_ON_PAGE(page_head != page-&gt;first_page, page);</span>
<span class="quote">&gt; &gt; +		VM_BUG_ON_PAGE(page_head != compound_head(page), page);</span>
<span class="quote">&gt; &gt;  		/*</span>
<span class="quote">&gt; &gt;  		 * We can release the refcount taken by</span>
<span class="quote">&gt; &gt;  		 * get_page_unless_zero() now that</span>
<span class="quote">&gt; &gt; @@ -261,7 +261,7 @@ static void put_compound_page(struct page *page)</span>
<span class="quote">&gt; &gt;  	 *  Case 3 is possible, as we may race with</span>
<span class="quote">&gt; &gt;  	 *  __split_huge_page_refcount tearing down a THP page.</span>
<span class="quote">&gt; &gt;  	 */</span>
<span class="quote">&gt; &gt; -	page_head = compound_head_by_tail(page);</span>
<span class="quote">&gt; &gt; +	page_head = compound_head(page);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is also in a path after PageTail() is true.</span>

We can only save one branch here: other PageTail() is most likely in other
compilation unit and compiler would not be able to eliminate additional
load.
Why bother?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Sept. 14, 2015, 1:31 p.m.</div>
<pre class="content">
On 09/11/2015 03:35 PM, Kirill A. Shutemov wrote:
<span class="quote">&gt;&gt;&gt; index 097c7a4bfbd9..330377f83ac7 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt;&gt;&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt;&gt; @@ -1686,8 +1686,7 @@ static void __split_huge_page_refcount(struct page *page,</span>
<span class="quote">&gt;&gt;&gt;   				      (1L &lt;&lt; PG_unevictable)));</span>
<span class="quote">&gt;&gt;&gt;   		page_tail-&gt;flags |= (1L &lt;&lt; PG_dirty);</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; -		/* clear PageTail before overwriting first_page */</span>
<span class="quote">&gt;&gt;&gt; -		smp_wmb();</span>
<span class="quote">&gt;&gt;&gt; +		clear_compound_head(page_tail);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I would sleep better if this was done before setting all the page-&gt;flags above,</span>
<span class="quote">&gt;&gt; previously, PageTail was cleared by the first operation which is</span>
<span class="quote">&gt;&gt; &quot;page_tail-&gt;flags &amp;= ~PAGE_FLAGS_CHECK_AT_PREP;&quot;</span>
<span class="quote">&gt;&gt; I do realize that it doesn&#39;t use WRITE_ONCE, so it might have been theoretically</span>
<span class="quote">&gt;&gt; broken already, if it does matter.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Right. Nothing enforces particular order. If we really need to have some</span>
<span class="quote">&gt; ordering on PageTail() vs. page-&gt;flags let&#39;s define it, but so far I</span>
<span class="quote">&gt; don&#39;t see a reason to change this part.</span>

OK.
<span class="quote">
&gt;&gt;&gt; diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="quote">&gt;&gt;&gt; index 36b23f1e2ca6..89e21a07080a 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/mm/internal.h</span>
<span class="quote">&gt;&gt;&gt; +++ b/mm/internal.h</span>
<span class="quote">&gt;&gt;&gt; @@ -61,9 +61,9 @@ static inline void __get_page_tail_foll(struct page *page,</span>
<span class="quote">&gt;&gt;&gt;   	 * speculative page access (like in</span>
<span class="quote">&gt;&gt;&gt;   	 * page_cache_get_speculative()) on tail pages.</span>
<span class="quote">&gt;&gt;&gt;   	 */</span>
<span class="quote">&gt;&gt;&gt; -	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;first_page-&gt;_count) &lt;= 0, page);</span>
<span class="quote">&gt;&gt;&gt; +	VM_BUG_ON_PAGE(atomic_read(&amp;compound_head(page)-&gt;_count) &lt;= 0, page);</span>
<span class="quote">&gt;&gt;&gt;   	if (get_page_head)</span>
<span class="quote">&gt;&gt;&gt; -		atomic_inc(&amp;page-&gt;first_page-&gt;_count);</span>
<span class="quote">&gt;&gt;&gt; +		atomic_inc(&amp;compound_head(page)-&gt;_count);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Doing another compound_head() seems like overkill when this code already assumes</span>
<span class="quote">&gt;&gt; PageTail.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &quot;Overkill&quot;? It&#39;s too strong wording for re-read hot cache line.</span>

Hm good point.
<span class="quote">
&gt;&gt; All callers do it after if (PageTail()) which means they already did</span>
<span class="quote">&gt;&gt; READ_ONCE(page-&gt;compound_head) and here they do another one. Potentially with</span>
<span class="quote">&gt;&gt; different result in bit 0, which would be a subtle bug, that could be</span>
<span class="quote">&gt;&gt; interesting to catch with some VM_BUG_ON. I don&#39;t know if a direct plain</span>
<span class="quote">&gt;&gt; page-&gt;compound_head access here instead of compound_head() would also result in</span>
<span class="quote">&gt;&gt; a re-read, since the previous access did use READ_ONCE(). Maybe it would be best</span>
<span class="quote">&gt;&gt; to reorganize the code here and in the 3 call sites so that the READ_ONCE() used</span>
<span class="quote">&gt;&gt; to determine PageTail also obtains the compound head pointer.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; All we would possbily win by the change is few bytes in code. Additional</span>
<span class="quote">&gt; READ_ONCE() only affect code generation. It doesn&#39;t introduce any cpu</span>
<span class="quote">&gt; barriers. The cache line with compound_head is in L1 anyway.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I don&#39;t see justification to change this part too. If you think we can</span>
<span class="quote">&gt; gain something by reworking this code, feel free to propose patch on top.</span>

OK, it&#39;s probably not worth:

add/remove: 0/0 grow/shrink: 1/3 up/down: 7/-66 (-59)
function                                     old     new   delta
follow_trans_huge_pmd                        516     523      +7
follow_page_pte                              905     893     -12
follow_hugetlb_page                          943     919     -24
__get_page_tail                              440     410     -30
<span class="quote">

&gt;&gt;&gt; diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="quote">&gt;&gt;&gt; index a3a0a2f1f7c3..faa9e1687dea 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/mm/swap.c</span>
<span class="quote">&gt;&gt;&gt; +++ b/mm/swap.c</span>
<span class="quote">&gt;&gt;&gt; @@ -200,7 +200,7 @@ out_put_single:</span>
<span class="quote">&gt;&gt;&gt;   				__put_single_page(page);</span>
<span class="quote">&gt;&gt;&gt;   			return;</span>
<span class="quote">&gt;&gt;&gt;   		}</span>
<span class="quote">&gt;&gt;&gt; -		VM_BUG_ON_PAGE(page_head != page-&gt;first_page, page);</span>
<span class="quote">&gt;&gt;&gt; +		VM_BUG_ON_PAGE(page_head != compound_head(page), page);</span>
<span class="quote">&gt;&gt;&gt;   		/*</span>
<span class="quote">&gt;&gt;&gt;   		 * We can release the refcount taken by</span>
<span class="quote">&gt;&gt;&gt;   		 * get_page_unless_zero() now that</span>
<span class="quote">&gt;&gt;&gt; @@ -261,7 +261,7 @@ static void put_compound_page(struct page *page)</span>
<span class="quote">&gt;&gt;&gt;   	 *  Case 3 is possible, as we may race with</span>
<span class="quote">&gt;&gt;&gt;   	 *  __split_huge_page_refcount tearing down a THP page.</span>
<span class="quote">&gt;&gt;&gt;   	 */</span>
<span class="quote">&gt;&gt;&gt; -	page_head = compound_head_by_tail(page);</span>
<span class="quote">&gt;&gt;&gt; +	page_head = compound_head(page);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This is also in a path after PageTail() is true.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We can only save one branch here: other PageTail() is most likely in other</span>
<span class="quote">&gt; compilation unit and compiler would not be able to eliminate additional</span>
<span class="quote">&gt; load.</span>
<span class="quote">&gt; Why bother?</span>

Hmm, right. Bah.

add/remove: 0/0 grow/shrink: 1/0 up/down: 8/0 (8)
function                                     old     new   delta
put_compound_page                            500     508      +8



--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/vm/split_page_table_lock b/Documentation/vm/split_page_table_lock</span>
<span class="p_header">index 6dea4fd5c961..62842a857dab 100644</span>
<span class="p_header">--- a/Documentation/vm/split_page_table_lock</span>
<span class="p_header">+++ b/Documentation/vm/split_page_table_lock</span>
<span class="p_chunk">@@ -54,8 +54,8 @@</span> <span class="p_context"> everything required is done by pgtable_page_ctor() and pgtable_page_dtor(),</span>
 which must be called on PTE table allocation / freeing.
 
 Make sure the architecture doesn&#39;t use slab allocator for page table
<span class="p_del">-allocation: slab uses page-&gt;slab_cache and page-&gt;first_page for its pages.</span>
<span class="p_del">-These fields share storage with page-&gt;ptl.</span>
<span class="p_add">+allocation: slab uses page-&gt;slab_cache for its pages.</span>
<span class="p_add">+This field shares storage with page-&gt;ptl.</span>
 
 PMD split lock only makes sense if you have more than two page table
 levels.
<span class="p_header">diff --git a/arch/xtensa/configs/iss_defconfig b/arch/xtensa/configs/iss_defconfig</span>
<span class="p_header">index e4d193e7a300..5c7c385f21c4 100644</span>
<span class="p_header">--- a/arch/xtensa/configs/iss_defconfig</span>
<span class="p_header">+++ b/arch/xtensa/configs/iss_defconfig</span>
<span class="p_chunk">@@ -169,7 +169,6 @@</span> <span class="p_context"> CONFIG_FLATMEM_MANUAL=y</span>
 # CONFIG_SPARSEMEM_MANUAL is not set
 CONFIG_FLATMEM=y
 CONFIG_FLAT_NODE_MEM_MAP=y
<span class="p_del">-CONFIG_PAGEFLAGS_EXTENDED=y</span>
 CONFIG_SPLIT_PTLOCK_CPUS=4
 # CONFIG_PHYS_ADDR_T_64BIT is not set
 CONFIG_ZONE_DMA_FLAG=1
<span class="p_header">diff --git a/include/linux/hugetlb_cgroup.h b/include/linux/hugetlb_cgroup.h</span>
<span class="p_header">index bcc853eccc85..75e34b900748 100644</span>
<span class="p_header">--- a/include/linux/hugetlb_cgroup.h</span>
<span class="p_header">+++ b/include/linux/hugetlb_cgroup.h</span>
<span class="p_chunk">@@ -32,7 +32,7 @@</span> <span class="p_context"> static inline struct hugetlb_cgroup *hugetlb_cgroup_from_page(struct page *page)</span>
 
 	if (compound_order(page) &lt; HUGETLB_CGROUP_MIN_ORDER)
 		return NULL;
<span class="p_del">-	return (struct hugetlb_cgroup *)page[2].lru.next;</span>
<span class="p_add">+	return (struct hugetlb_cgroup *)page[2].private;</span>
 }
 
 static inline
<span class="p_chunk">@@ -42,7 +42,7 @@</span> <span class="p_context"> int set_hugetlb_cgroup(struct page *page, struct hugetlb_cgroup *h_cg)</span>
 
 	if (compound_order(page) &lt; HUGETLB_CGROUP_MIN_ORDER)
 		return -1;
<span class="p_del">-	page[2].lru.next = (void *)h_cg;</span>
<span class="p_add">+	page[2].private	= (unsigned long)h_cg;</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 2cfe6051574c..9fc7dc8a49af 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -437,46 +437,6 @@</span> <span class="p_context"> static inline void compound_unlock_irqrestore(struct page *page,</span>
 #endif
 }
 
<span class="p_del">-static inline struct page *compound_head_by_tail(struct page *tail)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct page *head = tail-&gt;first_page;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * page-&gt;first_page may be a dangling pointer to an old</span>
<span class="p_del">-	 * compound page, so recheck that it is still a tail</span>
<span class="p_del">-	 * page before returning.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	smp_rmb();</span>
<span class="p_del">-	if (likely(PageTail(tail)))</span>
<span class="p_del">-		return head;</span>
<span class="p_del">-	return tail;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Since either compound page could be dismantled asynchronously in THP</span>
<span class="p_del">- * or we access asynchronously arbitrary positioned struct page, there</span>
<span class="p_del">- * would be tail flag race. To handle this race, we should call</span>
<span class="p_del">- * smp_rmb() before checking tail flag. compound_head_by_tail() did it.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline struct page *compound_head(struct page *page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (unlikely(PageTail(page)))</span>
<span class="p_del">-		return compound_head_by_tail(page);</span>
<span class="p_del">-	return page;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * If we access compound page synchronously such as access to</span>
<span class="p_del">- * allocated page, there is no need to handle tail flag race, so we can</span>
<span class="p_del">- * check tail flag directly without any synchronization primitive.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline struct page *compound_head_fast(struct page *page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (unlikely(PageTail(page)))</span>
<span class="p_del">-		return page-&gt;first_page;</span>
<span class="p_del">-	return page;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /*
  * The atomic page-&gt;_mapcount, starts from -1: so that transitions
  * both from it and to it can be tracked, using atomic_inc_and_test
<span class="p_chunk">@@ -525,7 +485,7 @@</span> <span class="p_context"> static inline void get_huge_page_tail(struct page *page)</span>
 	VM_BUG_ON_PAGE(!PageTail(page), page);
 	VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);
 	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) != 0, page);
<span class="p_del">-	if (compound_tail_refcounted(page-&gt;first_page))</span>
<span class="p_add">+	if (compound_tail_refcounted(compound_head(page)))</span>
 		atomic_inc(&amp;page-&gt;_mapcount);
 }
 
<span class="p_chunk">@@ -548,13 +508,7 @@</span> <span class="p_context"> static inline struct page *virt_to_head_page(const void *x)</span>
 {
 	struct page *page = virt_to_page(x);
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We don&#39;t need to worry about synchronization of tail flag</span>
<span class="p_del">-	 * when we call virt_to_head_page() since it is only called for</span>
<span class="p_del">-	 * already allocated page and this page won&#39;t be freed until</span>
<span class="p_del">-	 * this virt_to_head_page() is finished. So use _fast variant.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	return compound_head_fast(page);</span>
<span class="p_add">+	return compound_head(page);</span>
 }
 
 /*
<span class="p_chunk">@@ -1496,8 +1450,7 @@</span> <span class="p_context"> static inline bool ptlock_init(struct page *page)</span>
 	 * with 0. Make sure nobody took it in use in between.
 	 *
 	 * It can happen if arch try to use slab for page table allocation:
<span class="p_del">-	 * slab code uses page-&gt;slab_cache and page-&gt;first_page (for tail</span>
<span class="p_del">-	 * pages), which share storage with page-&gt;ptl.</span>
<span class="p_add">+	 * slab code uses page-&gt;slab_cache, which share storage with page-&gt;ptl.</span>
 	 */
 	VM_BUG_ON_PAGE(*(unsigned long *)&amp;page-&gt;ptl, page);
 	if (!ptlock_alloc(page))
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index 408a54563f65..ecaf3b1d0216 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -120,7 +120,13 @@</span> <span class="p_context"> struct page {</span>
 		};
 	};
 
<span class="p_del">-	/* Third double word block */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Third double word block</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * WARNING: bit 0 of the first word encode PageTail(). That means</span>
<span class="p_add">+	 * the rest users of the storage space MUST NOT use the bit to</span>
<span class="p_add">+	 * avoid collision and false-positive PageTail().</span>
<span class="p_add">+	 */</span>
 	union {
 		struct list_head lru;	/* Pageout list, eg. active_list
 					 * protected by zone-&gt;lru_lock !
<span class="p_chunk">@@ -143,12 +149,19 @@</span> <span class="p_context"> struct page {</span>
 						 */
 		/* First tail page of compound page */
 		struct {
<span class="p_add">+			unsigned long compound_head; /* If bit zero is set */</span>
 			unsigned short int compound_dtor;
 			unsigned short int compound_order;
 		};
 
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; USE_SPLIT_PMD_PTLOCKS
<span class="p_del">-		pgtable_t pmd_huge_pte; /* protected by page-&gt;ptl */</span>
<span class="p_add">+		struct {</span>
<span class="p_add">+			unsigned long __pad;	/* do not overlay pmd_huge_pte</span>
<span class="p_add">+						 * with compound_head to avoid</span>
<span class="p_add">+						 * possible bit 0 collision.</span>
<span class="p_add">+						 */</span>
<span class="p_add">+			pgtable_t pmd_huge_pte; /* protected by page-&gt;ptl */</span>
<span class="p_add">+		};</span>
 #endif
 	};
 
<span class="p_chunk">@@ -169,7 +182,6 @@</span> <span class="p_context"> struct page {</span>
 #endif
 #endif
 		struct kmem_cache *slab_cache;	/* SL[AU]B: Pointer to slab */
<span class="p_del">-		struct page *first_page;	/* Compound tail pages */</span>
 	};
 
 #ifdef CONFIG_MEMCG
<span class="p_header">diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h</span>
<span class="p_header">index 41c93844fb1d..9b865158e452 100644</span>
<span class="p_header">--- a/include/linux/page-flags.h</span>
<span class="p_header">+++ b/include/linux/page-flags.h</span>
<span class="p_chunk">@@ -86,12 +86,7 @@</span> <span class="p_context"> enum pageflags {</span>
 	PG_private,		/* If pagecache, has fs-private data */
 	PG_private_2,		/* If pagecache, has fs aux data */
 	PG_writeback,		/* Page is under writeback */
<span class="p_del">-#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
 	PG_head,		/* A head page */
<span class="p_del">-	PG_tail,		/* A tail page */</span>
<span class="p_del">-#else</span>
<span class="p_del">-	PG_compound,		/* A compound page */</span>
<span class="p_del">-#endif</span>
 	PG_swapcache,		/* Swap page: swp_entry_t in private */
 	PG_mappedtodisk,	/* Has blocks allocated on-disk */
 	PG_reclaim,		/* To be reclaimed asap */
<span class="p_chunk">@@ -387,85 +382,46 @@</span> <span class="p_context"> static inline void set_page_writeback_keepwrite(struct page *page)</span>
 	test_set_page_writeback_keepwrite(page);
 }
 
<span class="p_del">-#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
<span class="p_del">-/*</span>
<span class="p_del">- * System with lots of page flags available. This allows separate</span>
<span class="p_del">- * flags for PageHead() and PageTail() checks of compound pages so that bit</span>
<span class="p_del">- * tests can be used in performance sensitive paths. PageCompound is</span>
<span class="p_del">- * generally not used in hot code paths except arch/powerpc/mm/init_64.c</span>
<span class="p_del">- * and arch/powerpc/kvm/book3s_64_vio_hv.c which use it to detect huge pages</span>
<span class="p_del">- * and avoid handling those in real mode.</span>
<span class="p_del">- */</span>
 __PAGEFLAG(Head, head) CLEARPAGEFLAG(Head, head)
<span class="p_del">-__PAGEFLAG(Tail, tail)</span>
 
<span class="p_del">-static inline int PageCompound(struct page *page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return page-&gt;flags &amp; ((1L &lt;&lt; PG_head) | (1L &lt;&lt; PG_tail));</span>
<span class="p_del">-</span>
<span class="p_del">-}</span>
<span class="p_del">-#ifdef CONFIG_TRANSPARENT_HUGEPAGE</span>
<span class="p_del">-static inline void ClearPageCompound(struct page *page)</span>
<span class="p_add">+static inline int PageTail(struct page *page)</span>
 {
<span class="p_del">-	BUG_ON(!PageHead(page));</span>
<span class="p_del">-	ClearPageHead(page);</span>
<span class="p_add">+	return READ_ONCE(page-&gt;compound_head) &amp; 1;</span>
 }
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#define PG_head_mask ((1L &lt;&lt; PG_head))</span>
 
<span class="p_del">-#else</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Reduce page flag use as much as possible by overlapping</span>
<span class="p_del">- * compound page flags with the flags used for page cache pages. Possible</span>
<span class="p_del">- * because PageCompound is always set for compound pages and not for</span>
<span class="p_del">- * pages on the LRU and/or pagecache.</span>
<span class="p_del">- */</span>
<span class="p_del">-TESTPAGEFLAG(Compound, compound)</span>
<span class="p_del">-__SETPAGEFLAG(Head, compound)  __CLEARPAGEFLAG(Head, compound)</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * PG_reclaim is used in combination with PG_compound to mark the</span>
<span class="p_del">- * head and tail of a compound page. This saves one page flag</span>
<span class="p_del">- * but makes it impossible to use compound pages for the page cache.</span>
<span class="p_del">- * The PG_reclaim bit would have to be used for reclaim or readahead</span>
<span class="p_del">- * if compound pages enter the page cache.</span>
<span class="p_del">- *</span>
<span class="p_del">- * PG_compound &amp; PG_reclaim	=&gt; Tail page</span>
<span class="p_del">- * PG_compound &amp; ~PG_reclaim	=&gt; Head page</span>
<span class="p_del">- */</span>
<span class="p_del">-#define PG_head_mask ((1L &lt;&lt; PG_compound))</span>
<span class="p_del">-#define PG_head_tail_mask ((1L &lt;&lt; PG_compound) | (1L &lt;&lt; PG_reclaim))</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int PageHead(struct page *page)</span>
<span class="p_add">+static inline void set_compound_head(struct page *page, struct page *head)</span>
 {
<span class="p_del">-	return ((page-&gt;flags &amp; PG_head_tail_mask) == PG_head_mask);</span>
<span class="p_add">+	WRITE_ONCE(page-&gt;compound_head, (unsigned long)head + 1);</span>
 }
 
<span class="p_del">-static inline int PageTail(struct page *page)</span>
<span class="p_add">+static inline void clear_compound_head(struct page *page)</span>
 {
<span class="p_del">-	return ((page-&gt;flags &amp; PG_head_tail_mask) == PG_head_tail_mask);</span>
<span class="p_add">+	WRITE_ONCE(page-&gt;compound_head, 0);</span>
 }
 
<span class="p_del">-static inline void __SetPageTail(struct page *page)</span>
<span class="p_add">+static inline struct page *compound_head(struct page *page)</span>
 {
<span class="p_del">-	page-&gt;flags |= PG_head_tail_mask;</span>
<span class="p_add">+	unsigned long head = READ_ONCE(page-&gt;compound_head);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unlikely(head &amp; 1))</span>
<span class="p_add">+		return (struct page *) (head - 1);</span>
<span class="p_add">+	return page;</span>
 }
 
<span class="p_del">-static inline void __ClearPageTail(struct page *page)</span>
<span class="p_add">+static inline int PageCompound(struct page *page)</span>
 {
<span class="p_del">-	page-&gt;flags &amp;= ~PG_head_tail_mask;</span>
<span class="p_del">-}</span>
<span class="p_add">+	return PageHead(page) || PageTail(page);</span>
 
<span class="p_add">+}</span>
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
 static inline void ClearPageCompound(struct page *page)
 {
<span class="p_del">-	BUG_ON((page-&gt;flags &amp; PG_head_tail_mask) != (1 &lt;&lt; PG_compound));</span>
<span class="p_del">-	clear_bit(PG_compound, &amp;page-&gt;flags);</span>
<span class="p_add">+	BUG_ON(!PageHead(page));</span>
<span class="p_add">+	ClearPageHead(page);</span>
 }
 #endif
 
<span class="p_del">-#endif /* !PAGEFLAGS_EXTENDED */</span>
<span class="p_add">+#define PG_head_mask ((1L &lt;&lt; PG_head))</span>
 
 #ifdef CONFIG_HUGETLB_PAGE
 int PageHuge(struct page *page);
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index e79de2bd12cd..454579d31081 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -200,18 +200,6 @@</span> <span class="p_context"> config MEMORY_HOTREMOVE</span>
 	depends on MEMORY_HOTPLUG &amp;&amp; ARCH_ENABLE_MEMORY_HOTREMOVE
 	depends on MIGRATION
 
<span class="p_del">-#</span>
<span class="p_del">-# If we have space for more page flags then we can enable additional</span>
<span class="p_del">-# optimizations and functionality.</span>
<span class="p_del">-#</span>
<span class="p_del">-# Regular Sparsemem takes page flag bits for the sectionid if it does not</span>
<span class="p_del">-# use a virtual memmap. Disable extended page flags for 32 bit platforms</span>
<span class="p_del">-# that require the use of a sectionid in the page flags.</span>
<span class="p_del">-#</span>
<span class="p_del">-config PAGEFLAGS_EXTENDED</span>
<span class="p_del">-	def_bool y</span>
<span class="p_del">-	depends on 64BIT || SPARSEMEM_VMEMMAP || !SPARSEMEM</span>
<span class="p_del">-</span>
 # Heavily threaded applications may benefit from splitting the mm-wide
 # page_table_lock, so that faults on different parts of the user address
 # space can be handled with less contention: split it at this NR_CPUS.
<span class="p_header">diff --git a/mm/debug.c b/mm/debug.c</span>
<span class="p_header">index 76089ddf99ea..205e5ef957ab 100644</span>
<span class="p_header">--- a/mm/debug.c</span>
<span class="p_header">+++ b/mm/debug.c</span>
<span class="p_chunk">@@ -25,12 +25,7 @@</span> <span class="p_context"> static const struct trace_print_flags pageflag_names[] = {</span>
 	{1UL &lt;&lt; PG_private,		&quot;private&quot;	},
 	{1UL &lt;&lt; PG_private_2,		&quot;private_2&quot;	},
 	{1UL &lt;&lt; PG_writeback,		&quot;writeback&quot;	},
<span class="p_del">-#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
 	{1UL &lt;&lt; PG_head,		&quot;head&quot;		},
<span class="p_del">-	{1UL &lt;&lt; PG_tail,		&quot;tail&quot;		},</span>
<span class="p_del">-#else</span>
<span class="p_del">-	{1UL &lt;&lt; PG_compound,		&quot;compound&quot;	},</span>
<span class="p_del">-#endif</span>
 	{1UL &lt;&lt; PG_swapcache,		&quot;swapcache&quot;	},
 	{1UL &lt;&lt; PG_mappedtodisk,	&quot;mappedtodisk&quot;	},
 	{1UL &lt;&lt; PG_reclaim,		&quot;reclaim&quot;	},
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 097c7a4bfbd9..330377f83ac7 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1686,8 +1686,7 @@</span> <span class="p_context"> static void __split_huge_page_refcount(struct page *page,</span>
 				      (1L &lt;&lt; PG_unevictable)));
 		page_tail-&gt;flags |= (1L &lt;&lt; PG_dirty);
 
<span class="p_del">-		/* clear PageTail before overwriting first_page */</span>
<span class="p_del">-		smp_wmb();</span>
<span class="p_add">+		clear_compound_head(page_tail);</span>
 
 		/*
 		 * __split_huge_page_splitting() already set the
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 8ea74caa1fa8..53c0709fd87b 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -824,9 +824,8 @@</span> <span class="p_context"> static void destroy_compound_gigantic_page(struct page *page,</span>
 	struct page *p = page + 1;
 
 	for (i = 1; i &lt; nr_pages; i++, p = mem_map_next(p, page, i)) {
<span class="p_del">-		__ClearPageTail(p);</span>
<span class="p_add">+		clear_compound_head(p);</span>
 		set_page_refcounted(p);
<span class="p_del">-		p-&gt;first_page = NULL;</span>
 	}
 
 	set_compound_order(page, 0);
<span class="p_chunk">@@ -1099,10 +1098,7 @@</span> <span class="p_context"> static void prep_compound_gigantic_page(struct page *page, unsigned long order)</span>
 		 */
 		__ClearPageReserved(p);
 		set_page_count(p, 0);
<span class="p_del">-		p-&gt;first_page = page;</span>
<span class="p_del">-		/* Make sure p-&gt;first_page is always valid for PageTail() */</span>
<span class="p_del">-		smp_wmb();</span>
<span class="p_del">-		__SetPageTail(p);</span>
<span class="p_add">+		set_compound_head(p, page);</span>
 	}
 }
 
<span class="p_header">diff --git a/mm/hugetlb_cgroup.c b/mm/hugetlb_cgroup.c</span>
<span class="p_header">index 6e0057439a46..6a4426372698 100644</span>
<span class="p_header">--- a/mm/hugetlb_cgroup.c</span>
<span class="p_header">+++ b/mm/hugetlb_cgroup.c</span>
<span class="p_chunk">@@ -384,7 +384,7 @@</span> <span class="p_context"> void __init hugetlb_cgroup_file_init(void)</span>
 		/*
 		 * Add cgroup control files only if the huge page consists
 		 * of more than two normal pages. This is because we use
<span class="p_del">-		 * page[2].lru.next for storing cgroup details.</span>
<span class="p_add">+		 * page[2].private for storing cgroup details.</span>
 		 */
 		if (huge_page_order(h) &gt;= HUGETLB_CGROUP_MIN_ORDER)
 			__hugetlb_cgroup_file_init(hstate_index(h));
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 36b23f1e2ca6..89e21a07080a 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -61,9 +61,9 @@</span> <span class="p_context"> static inline void __get_page_tail_foll(struct page *page,</span>
 	 * speculative page access (like in
 	 * page_cache_get_speculative()) on tail pages.
 	 */
<span class="p_del">-	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;first_page-&gt;_count) &lt;= 0, page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(atomic_read(&amp;compound_head(page)-&gt;_count) &lt;= 0, page);</span>
 	if (get_page_head)
<span class="p_del">-		atomic_inc(&amp;page-&gt;first_page-&gt;_count);</span>
<span class="p_add">+		atomic_inc(&amp;compound_head(page)-&gt;_count);</span>
 	get_huge_page_tail(page);
 }
 
<span class="p_header">diff --git a/mm/memory-failure.c b/mm/memory-failure.c</span>
<span class="p_header">index 1f4446a90cef..4d1a5de9653d 100644</span>
<span class="p_header">--- a/mm/memory-failure.c</span>
<span class="p_header">+++ b/mm/memory-failure.c</span>
<span class="p_chunk">@@ -787,8 +787,6 @@</span> <span class="p_context"> static int me_huge_page(struct page *p, unsigned long pfn)</span>
 #define lru		(1UL &lt;&lt; PG_lru)
 #define swapbacked	(1UL &lt;&lt; PG_swapbacked)
 #define head		(1UL &lt;&lt; PG_head)
<span class="p_del">-#define tail		(1UL &lt;&lt; PG_tail)</span>
<span class="p_del">-#define compound	(1UL &lt;&lt; PG_compound)</span>
 #define slab		(1UL &lt;&lt; PG_slab)
 #define reserved	(1UL &lt;&lt; PG_reserved)
 
<span class="p_chunk">@@ -811,12 +809,7 @@</span> <span class="p_context"> static struct page_state {</span>
 	 */
 	{ slab,		slab,		MF_MSG_SLAB,	me_kernel },
 
<span class="p_del">-#ifdef CONFIG_PAGEFLAGS_EXTENDED</span>
 	{ head,		head,		MF_MSG_HUGE,		me_huge_page },
<span class="p_del">-	{ tail,		tail,		MF_MSG_HUGE,		me_huge_page },</span>
<span class="p_del">-#else</span>
<span class="p_del">-	{ compound,	compound,	MF_MSG_HUGE,		me_huge_page },</span>
<span class="p_del">-#endif</span>
 
 	{ sc|dirty,	sc|dirty,	MF_MSG_DIRTY_SWAPCACHE,	me_swapcache_dirty },
 	{ sc|dirty,	sc,		MF_MSG_CLEAN_SWAPCACHE,	me_swapcache_clean },
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index c6733cc3cbce..a56ad53ff553 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -424,15 +424,15 @@</span> <span class="p_context"> out:</span>
 /*
  * Higher-order pages are called &quot;compound pages&quot;.  They are structured thusly:
  *
<span class="p_del">- * The first PAGE_SIZE page is called the &quot;head page&quot;.</span>
<span class="p_add">+ * The first PAGE_SIZE page is called the &quot;head page&quot; and have PG_head set.</span>
  *
<span class="p_del">- * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;.</span>
<span class="p_add">+ * The remaining PAGE_SIZE pages are called &quot;tail pages&quot;. PageTail() is encoded</span>
<span class="p_add">+ * in bit 0 of page-&gt;compound_head. The rest of bits is pointer to head page.</span>
  *
<span class="p_del">- * All pages have PG_compound set.  All tail pages have their -&gt;first_page</span>
<span class="p_del">- * pointing at the head page.</span>
<span class="p_add">+ * The first tail page&#39;s -&gt;compound_dtor holds the offset in array of compound</span>
<span class="p_add">+ * page destructors. See compound_page_dtors.</span>
  *
<span class="p_del">- * The first tail page&#39;s -&gt;lru.next holds the address of the compound page&#39;s</span>
<span class="p_del">- * put_page() function.  Its -&gt;lru.prev holds the order of allocation.</span>
<span class="p_add">+ * The first tail page&#39;s -&gt;compound_order holds the order of allocation.</span>
  * This usage means that zero-order pages may not be compound.
  */
 
<span class="p_chunk">@@ -452,10 +452,7 @@</span> <span class="p_context"> void prep_compound_page(struct page *page, unsigned long order)</span>
 	for (i = 1; i &lt; nr_pages; i++) {
 		struct page *p = page + i;
 		set_page_count(p, 0);
<span class="p_del">-		p-&gt;first_page = page;</span>
<span class="p_del">-		/* Make sure p-&gt;first_page is always valid for PageTail() */</span>
<span class="p_del">-		smp_wmb();</span>
<span class="p_del">-		__SetPageTail(p);</span>
<span class="p_add">+		set_compound_head(p, page);</span>
 	}
 }
 
<span class="p_chunk">@@ -830,17 +827,30 @@</span> <span class="p_context"> static void free_one_page(struct zone *zone,</span>
 
 static int free_tail_pages_check(struct page *head_page, struct page *page)
 {
<span class="p_del">-	if (!IS_ENABLED(CONFIG_DEBUG_VM))</span>
<span class="p_del">-		return 0;</span>
<span class="p_add">+	int ret = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We rely page-&gt;lru.next never has bit 0 set, unless the page</span>
<span class="p_add">+	 * is PageTail(). Let&#39;s make sure that&#39;s true even for poisoned -&gt;lru.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUILD_BUG_ON((unsigned long)LIST_POISON1 &amp; 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_DEBUG_VM)) {</span>
<span class="p_add">+		ret = 0;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
 	if (unlikely(!PageTail(page))) {
 		bad_page(page, &quot;PageTail not set&quot;, 0);
<span class="p_del">-		return 1;</span>
<span class="p_add">+		goto out;</span>
 	}
<span class="p_del">-	if (unlikely(page-&gt;first_page != head_page)) {</span>
<span class="p_del">-		bad_page(page, &quot;first_page not consistent&quot;, 0);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+	if (unlikely(compound_head(page) != head_page)) {</span>
<span class="p_add">+		bad_page(page, &quot;compound_head not consistent&quot;, 0);</span>
<span class="p_add">+		goto out;</span>
 	}
<span class="p_del">-	return 0;</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	clear_compound_head(page);</span>
<span class="p_add">+	return ret;</span>
 }
 
 static void __meminit __init_single_page(struct page *page, unsigned long pfn,
<span class="p_chunk">@@ -888,6 +898,8 @@</span> <span class="p_context"> static void init_reserved_page(unsigned long pfn)</span>
 #else
 static inline void init_reserved_page(unsigned long pfn)
 {
<span class="p_add">+	/* Avoid false-positive PageTail() */</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;pfn_to_page(pfn)-&gt;lru);</span>
 }
 #endif /* CONFIG_DEFERRED_STRUCT_PAGE_INIT */
 
<span class="p_header">diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="p_header">index a3a0a2f1f7c3..faa9e1687dea 100644</span>
<span class="p_header">--- a/mm/swap.c</span>
<span class="p_header">+++ b/mm/swap.c</span>
<span class="p_chunk">@@ -200,7 +200,7 @@</span> <span class="p_context"> out_put_single:</span>
 				__put_single_page(page);
 			return;
 		}
<span class="p_del">-		VM_BUG_ON_PAGE(page_head != page-&gt;first_page, page);</span>
<span class="p_add">+		VM_BUG_ON_PAGE(page_head != compound_head(page), page);</span>
 		/*
 		 * We can release the refcount taken by
 		 * get_page_unless_zero() now that
<span class="p_chunk">@@ -261,7 +261,7 @@</span> <span class="p_context"> static void put_compound_page(struct page *page)</span>
 	 *  Case 3 is possible, as we may race with
 	 *  __split_huge_page_refcount tearing down a THP page.
 	 */
<span class="p_del">-	page_head = compound_head_by_tail(page);</span>
<span class="p_add">+	page_head = compound_head(page);</span>
 	if (!__compound_tail_refcounted(page_head))
 		put_unrefcounted_compound_page(page_head, page);
 	else

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



