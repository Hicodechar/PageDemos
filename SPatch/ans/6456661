
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v3,06/10] hugetlbfs: truncate_hugepages() takes a range of pages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v3,06/10] hugetlbfs: truncate_hugepages() takes a range of pages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 21, 2015, 3:47 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1432223264-4414-7-git-send-email-mike.kravetz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6456661/mbox/"
   >mbox</a>
|
   <a href="/patch/6456661/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6456661/">/patch/6456661/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 362D09F1CC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 21 May 2015 15:51:14 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 81C5220457
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 21 May 2015 15:51:10 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 55ECC20443
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 21 May 2015 15:51:09 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756605AbbEUPu7 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 21 May 2015 11:50:59 -0400
Received: from aserp1040.oracle.com ([141.146.126.69]:32409 &quot;EHLO
	aserp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1755942AbbEUPu4 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 21 May 2015 11:50:56 -0400
Received: from userv0021.oracle.com (userv0021.oracle.com [156.151.31.71])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with
	ESMTP id t4LFmNBh020300
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Thu, 21 May 2015 15:48:23 GMT
Received: from userv0121.oracle.com (userv0121.oracle.com [156.151.31.72])
	by userv0021.oracle.com (8.13.8/8.13.8) with ESMTP id t4LFmM9t014548
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL); 
	Thu, 21 May 2015 15:48:22 GMT
Received: from abhmp0018.oracle.com (abhmp0018.oracle.com [141.146.116.24])
	by userv0121.oracle.com (8.13.8/8.13.8) with ESMTP id
	t4LFmMwi010526; Thu, 21 May 2015 15:48:22 GMT
Received: from monkey.oracle.com (/50.53.81.168)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Thu, 21 May 2015 08:48:21 -0700
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org
Cc: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	David Rientjes &lt;rientjes@google.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Davidlohr Bueso &lt;dave@stgolabs.net&gt;,
	Aneesh Kumar &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;,
	Christoph Hellwig &lt;hch@infradead.org&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Subject: [RFC v3 PATCH 06/10] hugetlbfs: truncate_hugepages() takes a range
	of pages
Date: Thu, 21 May 2015 08:47:40 -0700
Message-Id: &lt;1432223264-4414-7-git-send-email-mike.kravetz@oracle.com&gt;
X-Mailer: git-send-email 2.1.0
In-Reply-To: &lt;1432223264-4414-1-git-send-email-mike.kravetz@oracle.com&gt;
References: &lt;1432223264-4414-1-git-send-email-mike.kravetz@oracle.com&gt;
X-Source-IP: userv0021.oracle.com [156.151.31.71]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - May 21, 2015, 3:47 p.m.</div>
<pre class="content">
Modify truncate_hugepages() to take a range of pages (start, end)
instead of simply start. If an end value of -1 is passed, the
current &quot;truncate&quot; functionality is maintained. Existing callers
are modified to pass -1 as end of range. By keying off end == -1,
the routine behaves differently for truncate and hole punch.
Page removal is now synchronized with page allocation via faults
by using the fault mutex table. The hole punch case can experience
the rare region_del error and must handle accordingly.

Since the routine handles more than just the truncate case, it is
renamed to remove_inode_hugepages().  To be consistent, the routine
truncate_huge_page() is renamed remove_huge_page().

Downstream of remove_inode_hugepages(), the routine
hugetlb_unreserve_pages() is also modified to take a range of pages.
hugetlb_unreserve_pages is modified to detect an error from
region_del and pass it back to the caller.
<span class="signed-off-by">
Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
---
 fs/hugetlbfs/inode.c    | 88 +++++++++++++++++++++++++++++++++++++++++++------
 include/linux/hugetlb.h |  3 +-
 mm/hugetlb.c            | 17 ++++++++--
 3 files changed, 94 insertions(+), 14 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - May 22, 2015, 8:08 a.m.</div>
<pre class="content">
On Thu, May 21, 2015 at 08:47:40AM -0700, Mike Kravetz wrote:
<span class="quote">&gt; Modify truncate_hugepages() to take a range of pages (start, end)</span>
<span class="quote">&gt; instead of simply start. If an end value of -1 is passed, the</span>
<span class="quote">&gt; current &quot;truncate&quot; functionality is maintained. Existing callers</span>
<span class="quote">&gt; are modified to pass -1 as end of range. By keying off end == -1,</span>
<span class="quote">&gt; the routine behaves differently for truncate and hole punch.</span>
<span class="quote">&gt; Page removal is now synchronized with page allocation via faults</span>
<span class="quote">&gt; by using the fault mutex table. The hole punch case can experience</span>
<span class="quote">&gt; the rare region_del error and must handle accordingly.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Since the routine handles more than just the truncate case, it is</span>
<span class="quote">&gt; renamed to remove_inode_hugepages().  To be consistent, the routine</span>
<span class="quote">&gt; truncate_huge_page() is renamed remove_huge_page().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Downstream of remove_inode_hugepages(), the routine</span>
<span class="quote">&gt; hugetlb_unreserve_pages() is also modified to take a range of pages.</span>
<span class="quote">&gt; hugetlb_unreserve_pages is modified to detect an error from</span>
<span class="quote">&gt; region_del and pass it back to the caller.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  fs/hugetlbfs/inode.c    | 88 +++++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;  include/linux/hugetlb.h |  3 +-</span>
<span class="quote">&gt;  mm/hugetlb.c            | 17 ++++++++--</span>
<span class="quote">&gt;  3 files changed, 94 insertions(+), 14 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; index dda529c..dfa88a5 100644</span>
<span class="quote">&gt; --- a/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; +++ b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt; @@ -317,26 +317,53 @@ static int hugetlbfs_write_end(struct file *file, struct address_space *mapping,</span>
<span class="quote">&gt;  	return -EINVAL;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void truncate_huge_page(struct page *page)</span>
<span class="quote">&gt; +static void remove_huge_page(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	cancel_dirty_page(page, /* No IO accounting for huge pages? */0);</span>
<span class="quote">&gt;  	ClearPageUptodate(page);</span>
<span class="quote">&gt;  	delete_from_page_cache(page);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void truncate_hugepages(struct inode *inode, loff_t lstart)</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * remove_inode_hugepages handles two distinct cases: truncation and hole punch</span>
<span class="quote">&gt; + * truncation is indicated by end of range being -1</span>
<span class="quote">&gt; + *	In this case, we first scan the range and release found pages.</span>
<span class="quote">&gt; + *	After releasing pages, hugetlb_unreserve_pages cleans up region/reserv</span>
<span class="quote">&gt; + *	maps and global counts.</span>
<span class="quote">&gt; + * hole punch is indicated if end is not -1</span>
<span class="quote">&gt; + *	In the hole punch case we scan the range and release found pages.</span>
<span class="quote">&gt; + *	Only when releasing a page is the associated region/reserv map</span>
<span class="quote">&gt; + *	deleted.  The region/reserv map for ranges without associated</span>
<span class="quote">&gt; + *	pages are not modified.</span>

If lend is not -1 but large enough to go beyond the end of file, which
should it be handled by truncate operation or hole punch operation?
If it makes no difference or never happens, it&#39;s OK with some comments.

Thanks,
Naoya Horiguchi
<span class="quote">
&gt; + */</span>
<span class="quote">&gt; +static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
<span class="quote">&gt; +				   loff_t lend)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct hstate *h = hstate_inode(inode);</span>
<span class="quote">&gt;  	struct address_space *mapping = &amp;inode-&gt;i_data;</span>
<span class="quote">&gt;  	const pgoff_t start = lstart &gt;&gt; huge_page_shift(h);</span>
<span class="quote">&gt; +	const pgoff_t end = lend &gt;&gt; huge_page_shift(h);</span>
<span class="quote">&gt;  	struct pagevec pvec;</span>
<span class="quote">&gt;  	pgoff_t next;</span>
<span class="quote">&gt;  	int i, freed = 0;</span>
<span class="quote">&gt; +	long lookup_nr = PAGEVEC_SIZE;</span>
<span class="quote">&gt; +	bool truncate_op = (lend == -1);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pagevec_init(&amp;pvec, 0);</span>
<span class="quote">&gt;  	next = start;</span>
<span class="quote">&gt; -	while (1) {</span>
<span class="quote">&gt; -		if (!pagevec_lookup(&amp;pvec, mapping, next, PAGEVEC_SIZE)) {</span>
<span class="quote">&gt; +	while (next &lt; end) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Make sure to never grab more pages that we</span>
<span class="quote">&gt; +		 * might possibly need.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (end - next &lt; lookup_nr)</span>
<span class="quote">&gt; +			lookup_nr = end - next;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * This pagevec_lookup() may return pages past &#39;end&#39;,</span>
<span class="quote">&gt; +		 * so we must check for page-&gt;index &gt; end.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (!pagevec_lookup(&amp;pvec, mapping, next, lookup_nr)) {</span>
<span class="quote">&gt;  			if (next == start)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  			next = start;</span>
<span class="quote">&gt; @@ -345,26 +372,67 @@ static void truncate_hugepages(struct inode *inode, loff_t lstart)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		for (i = 0; i &lt; pagevec_count(&amp;pvec); ++i) {</span>
<span class="quote">&gt;  			struct page *page = pvec.pages[i];</span>
<span class="quote">&gt; +			u32 hash;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			hash = hugetlb_fault_mutex_shared_hash(mapping, next);</span>
<span class="quote">&gt; +			hugetlb_fault_mutex_lock(hash);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			lock_page(page);</span>
<span class="quote">&gt; +			if (page-&gt;index &gt;= end) {</span>
<span class="quote">&gt; +				unlock_page(page);</span>
<span class="quote">&gt; +				hugetlb_fault_mutex_unlock(hash);</span>
<span class="quote">&gt; +				next = end;	/* we are done */</span>
<span class="quote">&gt; +				break;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * If page is mapped, it was faulted in after being</span>
<span class="quote">&gt; +			 * unmapped.  Do nothing in this race case.  In the</span>
<span class="quote">&gt; +			 * normal case page is not mapped.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (!page_mapped(page)) {</span>
<span class="quote">&gt; +				bool rsv_on_error = !PagePrivate(page);</span>
<span class="quote">&gt; +				/*</span>
<span class="quote">&gt; +				 * We must free the huge page and remove</span>
<span class="quote">&gt; +				 * from page cache (remove_huge_page) BEFORE</span>
<span class="quote">&gt; +				 * removing the region/reserve map</span>
<span class="quote">&gt; +				 * (hugetlb_unreserve_pages).  In rare out</span>
<span class="quote">&gt; +				 * of memory conditions, removal of the</span>
<span class="quote">&gt; +				 * region/reserve map could fail.  Before</span>
<span class="quote">&gt; +				 * free&#39;ing the page, note PagePrivate which</span>
<span class="quote">&gt; +				 * is used in case of error.</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt; +				remove_huge_page(page);</span>
<span class="quote">&gt; +				freed++;</span>
<span class="quote">&gt; +				if (!truncate_op) {</span>
<span class="quote">&gt; +					if (unlikely(hugetlb_unreserve_pages(</span>
<span class="quote">&gt; +							inode, next,</span>
<span class="quote">&gt; +							next + 1, 1)))</span>
<span class="quote">&gt; +						hugetlb_fix_reserve_counts(</span>
<span class="quote">&gt; +							inode, rsv_on_error);</span>
<span class="quote">&gt; +				}</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  			if (page-&gt;index &gt; next)</span>
<span class="quote">&gt;  				next = page-&gt;index;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  			++next;</span>
<span class="quote">&gt; -			truncate_huge_page(page);</span>
<span class="quote">&gt;  			unlock_page(page);</span>
<span class="quote">&gt; -			freed++;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			hugetlb_fault_mutex_unlock(hash);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		huge_pagevec_release(&amp;pvec);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	BUG_ON(!lstart &amp;&amp; mapping-&gt;nrpages);</span>
<span class="quote">&gt; -	hugetlb_unreserve_pages(inode, start, freed);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (truncate_op)</span>
<span class="quote">&gt; +		(void)hugetlb_unreserve_pages(inode, start, end, freed);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void hugetlbfs_evict_inode(struct inode *inode)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct resv_map *resv_map;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	truncate_hugepages(inode, 0);</span>
<span class="quote">&gt; +	remove_inode_hugepages(inode, 0, -1);</span>
<span class="quote">&gt;  	resv_map = (struct resv_map *)inode-&gt;i_mapping-&gt;private_data;</span>
<span class="quote">&gt;  	/* root inode doesn&#39;t have the resv_map, so we should check it */</span>
<span class="quote">&gt;  	if (resv_map)</span>
<span class="quote">&gt; @@ -421,7 +489,7 @@ static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)</span>
<span class="quote">&gt;  	if (!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap))</span>
<span class="quote">&gt;  		hugetlb_vmdelete_list(&amp;mapping-&gt;i_mmap, pgoff, 0);</span>
<span class="quote">&gt;  	i_mmap_unlock_write(mapping);</span>
<span class="quote">&gt; -	truncate_hugepages(inode, offset);</span>
<span class="quote">&gt; +	remove_inode_hugepages(inode, offset, -1);</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="quote">&gt; index d0d033e..4c2856e 100644</span>
<span class="quote">&gt; --- a/include/linux/hugetlb.h</span>
<span class="quote">&gt; +++ b/include/linux/hugetlb.h</span>
<span class="quote">&gt; @@ -75,7 +75,8 @@ int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  int hugetlb_reserve_pages(struct inode *inode, long from, long to,</span>
<span class="quote">&gt;  						struct vm_area_struct *vma,</span>
<span class="quote">&gt;  						vm_flags_t vm_flags);</span>
<span class="quote">&gt; -void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);</span>
<span class="quote">&gt; +long hugetlb_unreserve_pages(struct inode *inode, long start, long end,</span>
<span class="quote">&gt; +						long freed);</span>
<span class="quote">&gt;  int dequeue_hwpoisoned_huge_page(struct page *page);</span>
<span class="quote">&gt;  bool isolate_huge_page(struct page *page, struct list_head *list);</span>
<span class="quote">&gt;  void putback_active_hugepage(struct page *page);</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index df0d32a..0cf0622 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -3628,21 +3628,32 @@ out_err:</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed)</span>
<span class="quote">&gt; +long hugetlb_unreserve_pages(struct inode *inode, long start, long end,</span>
<span class="quote">&gt; +								long freed)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct hstate *h = hstate_inode(inode);</span>
<span class="quote">&gt;  	struct resv_map *resv_map = inode_resv_map(inode);</span>
<span class="quote">&gt;  	long chg = 0;</span>
<span class="quote">&gt;  	struct hugepage_subpool *spool = subpool_inode(inode);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (resv_map)</span>
<span class="quote">&gt; -		chg = region_del(resv_map, offset, -1);</span>
<span class="quote">&gt; +	if (resv_map) {</span>
<span class="quote">&gt; +		chg = region_del(resv_map, start, end);</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * region_del() can fail in the rare case where a region</span>
<span class="quote">&gt; +		 * must be split and another region descriptor can not be</span>
<span class="quote">&gt; +		 * allocated.  If end == -1, it will not fail.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (chg &lt; 0)</span>
<span class="quote">&gt; +			return chg;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	spin_lock(&amp;inode-&gt;i_lock);</span>
<span class="quote">&gt;  	inode-&gt;i_blocks -= (blocks_per_huge_page(h) * freed);</span>
<span class="quote">&gt;  	spin_unlock(&amp;inode-&gt;i_lock);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	hugepage_subpool_put_pages(spool, (chg - freed));</span>
<span class="quote">&gt;  	hugetlb_acct_memory(h, -(chg - freed));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.1.0</span>
<span class="quote">&gt; --</span>
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - May 22, 2015, 5:07 p.m.</div>
<pre class="content">
On 05/22/2015 01:08 AM, Naoya Horiguchi wrote:
<span class="quote">&gt; On Thu, May 21, 2015 at 08:47:40AM -0700, Mike Kravetz wrote:</span>
<span class="quote">&gt;&gt; Modify truncate_hugepages() to take a range of pages (start, end)</span>
<span class="quote">&gt;&gt; instead of simply start. If an end value of -1 is passed, the</span>
<span class="quote">&gt;&gt; current &quot;truncate&quot; functionality is maintained. Existing callers</span>
<span class="quote">&gt;&gt; are modified to pass -1 as end of range. By keying off end == -1,</span>
<span class="quote">&gt;&gt; the routine behaves differently for truncate and hole punch.</span>
<span class="quote">&gt;&gt; Page removal is now synchronized with page allocation via faults</span>
<span class="quote">&gt;&gt; by using the fault mutex table. The hole punch case can experience</span>
<span class="quote">&gt;&gt; the rare region_del error and must handle accordingly.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Since the routine handles more than just the truncate case, it is</span>
<span class="quote">&gt;&gt; renamed to remove_inode_hugepages().  To be consistent, the routine</span>
<span class="quote">&gt;&gt; truncate_huge_page() is renamed remove_huge_page().</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Downstream of remove_inode_hugepages(), the routine</span>
<span class="quote">&gt;&gt; hugetlb_unreserve_pages() is also modified to take a range of pages.</span>
<span class="quote">&gt;&gt; hugetlb_unreserve_pages is modified to detect an error from</span>
<span class="quote">&gt;&gt; region_del and pass it back to the caller.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;   fs/hugetlbfs/inode.c    | 88 +++++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;&gt;   include/linux/hugetlb.h |  3 +-</span>
<span class="quote">&gt;&gt;   mm/hugetlb.c            | 17 ++++++++--</span>
<span class="quote">&gt;&gt;   3 files changed, 94 insertions(+), 14 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt; index dda529c..dfa88a5 100644</span>
<span class="quote">&gt;&gt; --- a/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt; +++ b/fs/hugetlbfs/inode.c</span>
<span class="quote">&gt;&gt; @@ -317,26 +317,53 @@ static int hugetlbfs_write_end(struct file *file, struct address_space *mapping,</span>
<span class="quote">&gt;&gt;   	return -EINVAL;</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;   </span>
<span class="quote">&gt;&gt; -static void truncate_huge_page(struct page *page)</span>
<span class="quote">&gt;&gt; +static void remove_huge_page(struct page *page)</span>
<span class="quote">&gt;&gt;   {</span>
<span class="quote">&gt;&gt;   	cancel_dirty_page(page, /* No IO accounting for huge pages? */0);</span>
<span class="quote">&gt;&gt;   	ClearPageUptodate(page);</span>
<span class="quote">&gt;&gt;   	delete_from_page_cache(page);</span>
<span class="quote">&gt;&gt;   }</span>
<span class="quote">&gt;&gt;   </span>
<span class="quote">&gt;&gt; -static void truncate_hugepages(struct inode *inode, loff_t lstart)</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * remove_inode_hugepages handles two distinct cases: truncation and hole punch</span>
<span class="quote">&gt;&gt; + * truncation is indicated by end of range being -1</span>
<span class="quote">&gt;&gt; + *	In this case, we first scan the range and release found pages.</span>
<span class="quote">&gt;&gt; + *	After releasing pages, hugetlb_unreserve_pages cleans up region/reserv</span>
<span class="quote">&gt;&gt; + *	maps and global counts.</span>
<span class="quote">&gt;&gt; + * hole punch is indicated if end is not -1</span>
<span class="quote">&gt;&gt; + *	In the hole punch case we scan the range and release found pages.</span>
<span class="quote">&gt;&gt; + *	Only when releasing a page is the associated region/reserv map</span>
<span class="quote">&gt;&gt; + *	deleted.  The region/reserv map for ranges without associated</span>
<span class="quote">&gt;&gt; + *	pages are not modified.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If lend is not -1 but large enough to go beyond the end of file, which</span>
<span class="quote">&gt; should it be handled by truncate operation or hole punch operation?</span>
<span class="quote">&gt; If it makes no difference or never happens, it&#39;s OK with some comments.</span>

Interesting question.  I did not think of this case.

If lend is not -1, then it must have been called by fallocate hole punch.
The other use cases/callers pass in -1 to truncate the file or entirely
remove the file.  remove_inode_hugepages really wants to distinguish
between hole punch and other operations.  In the hole punch case, the
size of the file does not change.  Therefore, we need to handle races with
faults for pages in the &quot;hole&quot; while deleting the hole.

The case where lend is beyond end of file and not -1 will be handled as
a hole punch.  I believe this is the desired behavior.  I will try to
make the documentation more clear.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/hugetlbfs/inode.c b/fs/hugetlbfs/inode.c</span>
<span class="p_header">index dda529c..dfa88a5 100644</span>
<span class="p_header">--- a/fs/hugetlbfs/inode.c</span>
<span class="p_header">+++ b/fs/hugetlbfs/inode.c</span>
<span class="p_chunk">@@ -317,26 +317,53 @@</span> <span class="p_context"> static int hugetlbfs_write_end(struct file *file, struct address_space *mapping,</span>
 	return -EINVAL;
 }
 
<span class="p_del">-static void truncate_huge_page(struct page *page)</span>
<span class="p_add">+static void remove_huge_page(struct page *page)</span>
 {
 	cancel_dirty_page(page, /* No IO accounting for huge pages? */0);
 	ClearPageUptodate(page);
 	delete_from_page_cache(page);
 }
 
<span class="p_del">-static void truncate_hugepages(struct inode *inode, loff_t lstart)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * remove_inode_hugepages handles two distinct cases: truncation and hole punch</span>
<span class="p_add">+ * truncation is indicated by end of range being -1</span>
<span class="p_add">+ *	In this case, we first scan the range and release found pages.</span>
<span class="p_add">+ *	After releasing pages, hugetlb_unreserve_pages cleans up region/reserv</span>
<span class="p_add">+ *	maps and global counts.</span>
<span class="p_add">+ * hole punch is indicated if end is not -1</span>
<span class="p_add">+ *	In the hole punch case we scan the range and release found pages.</span>
<span class="p_add">+ *	Only when releasing a page is the associated region/reserv map</span>
<span class="p_add">+ *	deleted.  The region/reserv map for ranges without associated</span>
<span class="p_add">+ *	pages are not modified.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void remove_inode_hugepages(struct inode *inode, loff_t lstart,</span>
<span class="p_add">+				   loff_t lend)</span>
 {
 	struct hstate *h = hstate_inode(inode);
 	struct address_space *mapping = &amp;inode-&gt;i_data;
 	const pgoff_t start = lstart &gt;&gt; huge_page_shift(h);
<span class="p_add">+	const pgoff_t end = lend &gt;&gt; huge_page_shift(h);</span>
 	struct pagevec pvec;
 	pgoff_t next;
 	int i, freed = 0;
<span class="p_add">+	long lookup_nr = PAGEVEC_SIZE;</span>
<span class="p_add">+	bool truncate_op = (lend == -1);</span>
 
 	pagevec_init(&amp;pvec, 0);
 	next = start;
<span class="p_del">-	while (1) {</span>
<span class="p_del">-		if (!pagevec_lookup(&amp;pvec, mapping, next, PAGEVEC_SIZE)) {</span>
<span class="p_add">+	while (next &lt; end) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Make sure to never grab more pages that we</span>
<span class="p_add">+		 * might possibly need.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (end - next &lt; lookup_nr)</span>
<span class="p_add">+			lookup_nr = end - next;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This pagevec_lookup() may return pages past &#39;end&#39;,</span>
<span class="p_add">+		 * so we must check for page-&gt;index &gt; end.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!pagevec_lookup(&amp;pvec, mapping, next, lookup_nr)) {</span>
 			if (next == start)
 				break;
 			next = start;
<span class="p_chunk">@@ -345,26 +372,67 @@</span> <span class="p_context"> static void truncate_hugepages(struct inode *inode, loff_t lstart)</span>
 
 		for (i = 0; i &lt; pagevec_count(&amp;pvec); ++i) {
 			struct page *page = pvec.pages[i];
<span class="p_add">+			u32 hash;</span>
<span class="p_add">+</span>
<span class="p_add">+			hash = hugetlb_fault_mutex_shared_hash(mapping, next);</span>
<span class="p_add">+			hugetlb_fault_mutex_lock(hash);</span>
 
 			lock_page(page);
<span class="p_add">+			if (page-&gt;index &gt;= end) {</span>
<span class="p_add">+				unlock_page(page);</span>
<span class="p_add">+				hugetlb_fault_mutex_unlock(hash);</span>
<span class="p_add">+				next = end;	/* we are done */</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If page is mapped, it was faulted in after being</span>
<span class="p_add">+			 * unmapped.  Do nothing in this race case.  In the</span>
<span class="p_add">+			 * normal case page is not mapped.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (!page_mapped(page)) {</span>
<span class="p_add">+				bool rsv_on_error = !PagePrivate(page);</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * We must free the huge page and remove</span>
<span class="p_add">+				 * from page cache (remove_huge_page) BEFORE</span>
<span class="p_add">+				 * removing the region/reserve map</span>
<span class="p_add">+				 * (hugetlb_unreserve_pages).  In rare out</span>
<span class="p_add">+				 * of memory conditions, removal of the</span>
<span class="p_add">+				 * region/reserve map could fail.  Before</span>
<span class="p_add">+				 * free&#39;ing the page, note PagePrivate which</span>
<span class="p_add">+				 * is used in case of error.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				remove_huge_page(page);</span>
<span class="p_add">+				freed++;</span>
<span class="p_add">+				if (!truncate_op) {</span>
<span class="p_add">+					if (unlikely(hugetlb_unreserve_pages(</span>
<span class="p_add">+							inode, next,</span>
<span class="p_add">+							next + 1, 1)))</span>
<span class="p_add">+						hugetlb_fix_reserve_counts(</span>
<span class="p_add">+							inode, rsv_on_error);</span>
<span class="p_add">+				}</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
 			if (page-&gt;index &gt; next)
 				next = page-&gt;index;
<span class="p_add">+</span>
 			++next;
<span class="p_del">-			truncate_huge_page(page);</span>
 			unlock_page(page);
<span class="p_del">-			freed++;</span>
<span class="p_add">+</span>
<span class="p_add">+			hugetlb_fault_mutex_unlock(hash);</span>
 		}
 		huge_pagevec_release(&amp;pvec);
 	}
<span class="p_del">-	BUG_ON(!lstart &amp;&amp; mapping-&gt;nrpages);</span>
<span class="p_del">-	hugetlb_unreserve_pages(inode, start, freed);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (truncate_op)</span>
<span class="p_add">+		(void)hugetlb_unreserve_pages(inode, start, end, freed);</span>
 }
 
 static void hugetlbfs_evict_inode(struct inode *inode)
 {
 	struct resv_map *resv_map;
 
<span class="p_del">-	truncate_hugepages(inode, 0);</span>
<span class="p_add">+	remove_inode_hugepages(inode, 0, -1);</span>
 	resv_map = (struct resv_map *)inode-&gt;i_mapping-&gt;private_data;
 	/* root inode doesn&#39;t have the resv_map, so we should check it */
 	if (resv_map)
<span class="p_chunk">@@ -421,7 +489,7 @@</span> <span class="p_context"> static int hugetlb_vmtruncate(struct inode *inode, loff_t offset)</span>
 	if (!RB_EMPTY_ROOT(&amp;mapping-&gt;i_mmap))
 		hugetlb_vmdelete_list(&amp;mapping-&gt;i_mmap, pgoff, 0);
 	i_mmap_unlock_write(mapping);
<span class="p_del">-	truncate_hugepages(inode, offset);</span>
<span class="p_add">+	remove_inode_hugepages(inode, offset, -1);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index d0d033e..4c2856e 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -75,7 +75,8 @@</span> <span class="p_context"> int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 int hugetlb_reserve_pages(struct inode *inode, long from, long to,
 						struct vm_area_struct *vma,
 						vm_flags_t vm_flags);
<span class="p_del">-void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed);</span>
<span class="p_add">+long hugetlb_unreserve_pages(struct inode *inode, long start, long end,</span>
<span class="p_add">+						long freed);</span>
 int dequeue_hwpoisoned_huge_page(struct page *page);
 bool isolate_huge_page(struct page *page, struct list_head *list);
 void putback_active_hugepage(struct page *page);
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index df0d32a..0cf0622 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -3628,21 +3628,32 @@</span> <span class="p_context"> out_err:</span>
 	return ret;
 }
 
<span class="p_del">-void hugetlb_unreserve_pages(struct inode *inode, long offset, long freed)</span>
<span class="p_add">+long hugetlb_unreserve_pages(struct inode *inode, long start, long end,</span>
<span class="p_add">+								long freed)</span>
 {
 	struct hstate *h = hstate_inode(inode);
 	struct resv_map *resv_map = inode_resv_map(inode);
 	long chg = 0;
 	struct hugepage_subpool *spool = subpool_inode(inode);
 
<span class="p_del">-	if (resv_map)</span>
<span class="p_del">-		chg = region_del(resv_map, offset, -1);</span>
<span class="p_add">+	if (resv_map) {</span>
<span class="p_add">+		chg = region_del(resv_map, start, end);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * region_del() can fail in the rare case where a region</span>
<span class="p_add">+		 * must be split and another region descriptor can not be</span>
<span class="p_add">+		 * allocated.  If end == -1, it will not fail.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (chg &lt; 0)</span>
<span class="p_add">+			return chg;</span>
<span class="p_add">+	}</span>
 	spin_lock(&amp;inode-&gt;i_lock);
 	inode-&gt;i_blocks -= (blocks_per_huge_page(h) * freed);
 	spin_unlock(&amp;inode-&gt;i_lock);
 
 	hugepage_subpool_put_pages(spool, (chg - freed));
 	hugetlb_acct_memory(h, -(chg - freed));
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
 }
 
 #ifdef CONFIG_ARCH_WANT_HUGE_PMD_SHARE

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



