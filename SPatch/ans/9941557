
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/2] x86/mm: Reinitialize TLB state on hotplug and resume - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/2] x86/mm: Reinitialize TLB state on hotplug and resume</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 7, 2017, 2:54 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;f13c8e8c58ba3b535f1e4cb9e62b50ab37dd69bb.1504752689.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9941557/mbox/"
   >mbox</a>
|
   <a href="/patch/9941557/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9941557/">/patch/9941557/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7D9676038C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Sep 2017 02:55:07 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6EB0C27E71
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Sep 2017 02:55:07 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 63DD327F10; Thu,  7 Sep 2017 02:55:07 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E759827E71
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  7 Sep 2017 02:55:06 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753761AbdIGCzD (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 6 Sep 2017 22:55:03 -0400
Received: from mail.kernel.org ([198.145.29.99]:39610 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752643AbdIGCy6 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 6 Sep 2017 22:54:58 -0400
Received: from localhost (c-71-202-137-17.hsd1.ca.comcast.net
	[71.202.137.17])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id ABB4E21BF5;
	Thu,  7 Sep 2017 02:54:57 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mail.kernel.org ABB4E21BF5
Authentication-Results: mail.kernel.org;
	dmarc=none (p=none dis=none) header.from=kernel.org
Authentication-Results: mail.kernel.org;
	spf=none smtp.mailfrom=luto@kernel.org
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: X86 ML &lt;x86@kernel.org&gt;
Cc: Borislav Petkov &lt;bpetkov@suse.de&gt;,
	&quot;linux-kernel@vger.kernel.org&quot; &lt;linux-kernel@vger.kernel.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;
Subject: [PATCH 1/2] x86/mm: Reinitialize TLB state on hotplug and resume
Date: Wed,  6 Sep 2017 19:54:53 -0700
Message-Id: &lt;f13c8e8c58ba3b535f1e4cb9e62b50ab37dd69bb.1504752689.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.13.5
In-Reply-To: &lt;cover.1504752689.git.luto@kernel.org&gt;
References: &lt;cover.1504752689.git.luto@kernel.org&gt;
In-Reply-To: &lt;cover.1504752689.git.luto@kernel.org&gt;
References: &lt;cover.1504752689.git.luto@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Sept. 7, 2017, 2:54 a.m.</div>
<pre class="content">
When Linux brings a CPU down and back up, it switches to init_mm and then
loads swapper_pg_dir into CR3.  With PCID enabled, this has the side effect
of masking off the ASID bits in CR3.

This can result in some confusion in the TLB handling code.  If we
bring a CPU down and back up with any ASID other than 0, we end up
with the wrong ASID active on the CPU after resume.  This could
cause our internal state to become corrupt, although major
corruption is unlikely because init_mm doesn&#39;t have any user pages.
More obviously, if CONFIG_DEBUG_VM=y, we&#39;ll trip over an assertion
in the next context switch.  The result of *that* is a failure to
resume from suspend with probability 1 - 1/6^(cpus-1).

Fix it by reinitializing cpu_tlbstate on resume and CPU bringup.

Reported-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Reported-by: Jiri Kosina &lt;jikos@kernel.org&gt;
Fixes: 10af6235e0d3 (&quot;x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID&quot;)
<span class="signed-off-by">Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---
 arch/x86/include/asm/tlbflush.h |  2 ++
 arch/x86/kernel/cpu/common.c    |  2 ++
 arch/x86/mm/tlb.c               | 44 +++++++++++++++++++++++++++++++++++++++++
 arch/x86/power/cpu.c            |  1 +
 4 files changed, 49 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=139651">Jiri Kosina</a> - Sept. 7, 2017, 7:31 a.m.</div>
<pre class="content">
On Wed, 6 Sep 2017, Andy Lutomirski wrote:
<span class="quote">
&gt; When Linux brings a CPU down and back up, it switches to init_mm and then</span>
<span class="quote">&gt; loads swapper_pg_dir into CR3.  With PCID enabled, this has the side effect</span>
<span class="quote">&gt; of masking off the ASID bits in CR3.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This can result in some confusion in the TLB handling code.  If we</span>
<span class="quote">&gt; bring a CPU down and back up with any ASID other than 0, we end up</span>
<span class="quote">&gt; with the wrong ASID active on the CPU after resume.  This could</span>
<span class="quote">&gt; cause our internal state to become corrupt, although major</span>
<span class="quote">&gt; corruption is unlikely because init_mm doesn&#39;t have any user pages.</span>
<span class="quote">&gt; More obviously, if CONFIG_DEBUG_VM=y, we&#39;ll trip over an assertion</span>
<span class="quote">&gt; in the next context switch.  The result of *that* is a failure to</span>
<span class="quote">&gt; resume from suspend with probability 1 - 1/6^(cpus-1).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Fix it by reinitializing cpu_tlbstate on resume and CPU bringup.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reported-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;</span>
<span class="quote">&gt; Reported-by: Jiri Kosina &lt;jikos@kernel.org&gt;</span>
<span class="quote">&gt; Fixes: 10af6235e0d3 (&quot;x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID&quot;)</span>
<span class="quote">&gt; Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="tested-by">
Tested-by: Jiri Kosina &lt;jkosina@suse.cz&gt;</span>

Thanks,
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Sept. 7, 2017, 7:48 a.m.</div>
<pre class="content">
* Jiri Kosina &lt;jikos@kernel.org&gt; wrote:
<span class="quote">
&gt; On Wed, 6 Sep 2017, Andy Lutomirski wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; When Linux brings a CPU down and back up, it switches to init_mm and then</span>
<span class="quote">&gt; &gt; loads swapper_pg_dir into CR3.  With PCID enabled, this has the side effect</span>
<span class="quote">&gt; &gt; of masking off the ASID bits in CR3.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This can result in some confusion in the TLB handling code.  If we</span>
<span class="quote">&gt; &gt; bring a CPU down and back up with any ASID other than 0, we end up</span>
<span class="quote">&gt; &gt; with the wrong ASID active on the CPU after resume.  This could</span>
<span class="quote">&gt; &gt; cause our internal state to become corrupt, although major</span>
<span class="quote">&gt; &gt; corruption is unlikely because init_mm doesn&#39;t have any user pages.</span>
<span class="quote">&gt; &gt; More obviously, if CONFIG_DEBUG_VM=y, we&#39;ll trip over an assertion</span>
<span class="quote">&gt; &gt; in the next context switch.  The result of *that* is a failure to</span>
<span class="quote">&gt; &gt; resume from suspend with probability 1 - 1/6^(cpus-1).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Fix it by reinitializing cpu_tlbstate on resume and CPU bringup.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Reported-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;</span>
<span class="quote">&gt; &gt; Reported-by: Jiri Kosina &lt;jikos@kernel.org&gt;</span>
<span class="quote">&gt; &gt; Fixes: 10af6235e0d3 (&quot;x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID&quot;)</span>
<span class="quote">&gt; &gt; Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Tested-by: Jiri Kosina &lt;jkosina@suse.cz&gt;</span>

The fix should be upstream already, as of 1c9fe4409ce3 and later.

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=57321">Borislav Petkov</a> - Sept. 7, 2017, 9:54 a.m.</div>
<pre class="content">
Just nitpicks:

On Wed, Sep 06, 2017 at 07:54:53PM -0700, Andy Lutomirski wrote:
<span class="quote">&gt; When Linux brings a CPU down and back up, it switches to init_mm and then</span>
<span class="quote">&gt; loads swapper_pg_dir into CR3.  With PCID enabled, this has the side effect</span>
<span class="quote">&gt; of masking off the ASID bits in CR3.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This can result in some confusion in the TLB handling code.  If we</span>
<span class="quote">&gt; bring a CPU down and back up with any ASID other than 0, we end up</span>
<span class="quote">&gt; with the wrong ASID active on the CPU after resume.  This could</span>
<span class="quote">&gt; cause our internal state to become corrupt, although major</span>
<span class="quote">&gt; corruption is unlikely because init_mm doesn&#39;t have any user pages.</span>
<span class="quote">&gt; More obviously, if CONFIG_DEBUG_VM=y, we&#39;ll trip over an assertion</span>
<span class="quote">&gt; in the next context switch.  The result of *that* is a failure to</span>
<span class="quote">&gt; resume from suspend with probability 1 - 1/6^(cpus-1).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Fix it by reinitializing cpu_tlbstate on resume and CPU bringup.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reported-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;</span>
<span class="quote">&gt; Reported-by: Jiri Kosina &lt;jikos@kernel.org&gt;</span>
<span class="quote">&gt; Fixes: 10af6235e0d3 (&quot;x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID&quot;)</span>
<span class="quote">&gt; Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/tlbflush.h |  2 ++</span>
<span class="quote">&gt;  arch/x86/kernel/cpu/common.c    |  2 ++</span>
<span class="quote">&gt;  arch/x86/mm/tlb.c               | 44 +++++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  arch/x86/power/cpu.c            |  1 +</span>
<span class="quote">&gt;  4 files changed, 49 insertions(+)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; index d23e61dc0640..4893abf7f74f 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="quote">&gt; @@ -198,6 +198,8 @@ static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
<span class="quote">&gt;  	cr4_set_bits(mask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +extern void initialize_tlbstate_and_flush(void);</span>

Let&#39;s put that declaration at the end.
<span class="quote">
&gt;  static inline void __native_flush_tlb(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="quote">&gt; index efba8e3da3e2..40cb4d0a5982 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/cpu/common.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/cpu/common.c</span>
<span class="quote">&gt; @@ -1583,6 +1583,7 @@ void cpu_init(void)</span>
<span class="quote">&gt;  	mmgrab(&amp;init_mm);</span>
<span class="quote">&gt;  	me-&gt;active_mm = &amp;init_mm;</span>
<span class="quote">&gt;  	BUG_ON(me-&gt;mm);</span>
<span class="quote">&gt; +	initialize_tlbstate_and_flush();</span>
<span class="quote">&gt;  	enter_lazy_tlb(&amp;init_mm, me);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	load_sp0(t, &amp;current-&gt;thread);</span>
<span class="quote">&gt; @@ -1637,6 +1638,7 @@ void cpu_init(void)</span>
<span class="quote">&gt;  	mmgrab(&amp;init_mm);</span>
<span class="quote">&gt;  	curr-&gt;active_mm = &amp;init_mm;</span>
<span class="quote">&gt;  	BUG_ON(curr-&gt;mm);</span>
<span class="quote">&gt; +	initialize_tlbstate_and_flush();</span>
<span class="quote">&gt;  	enter_lazy_tlb(&amp;init_mm, curr);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	load_sp0(t, thread);</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; index ce104b962a17..dbbcfd59726a 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/tlb.c</span>
<span class="quote">&gt; @@ -214,6 +214,50 @@ void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * Call this when reinitializing a CPU.  It fixes the following potential</span>
<span class="quote">&gt; + * problems:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * - The ASID changed from what cpu_tlbstate thinks it is (most likely</span>
<span class="quote">&gt; + *   because the CPU was taken down and came back up with CR3&#39;s PCID</span>
<span class="quote">&gt; + *   bits clear.  CPU hotplug can do this.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * - The TLB contains junk in slots corresponding to inactive ASIDs.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * - The CPU went so far out to lunch that it may have missed a TLB</span>
<span class="quote">&gt; + *   flush.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +void initialize_tlbstate_and_flush(void)</span>

I think we should prefix all those visible, TLB-handling functions with
&quot;tlb_&quot;. So you&#39;d have tlb_init_state_and_flush().
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Sept. 7, 2017, 9:59 a.m.</div>
<pre class="content">
* Borislav Petkov &lt;bp@suse.de&gt; wrote:
<span class="quote">
&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +void initialize_tlbstate_and_flush(void)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think we should prefix all those visible, TLB-handling functions with</span>
<span class="quote">&gt; &quot;tlb_&quot;. So you&#39;d have tlb_init_state_and_flush().</span>

Agreed absolutely, but note that this affects more functions as well - for example 
enter_lazy_tlb() should probably be tlb_lazy_enter() - or at least 
lazy_tlb_enter() or such?

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=57321">Borislav Petkov</a> - Sept. 7, 2017, 10:10 a.m.</div>
<pre class="content">
On Thu, Sep 07, 2017 at 11:59:32AM +0200, Ingo Molnar wrote:
<span class="quote">&gt; Agreed absolutely, but note that this affects more functions as well - for example </span>
<span class="quote">&gt; enter_lazy_tlb() should probably be tlb_lazy_enter() - or at least </span>
<span class="quote">&gt; lazy_tlb_enter() or such?</span>

Yeah, or tlb_enter_lazy() or even tlb_enter_lazy_mode() or so. I could
give it a try when things get a bit quieter and see how it actually
looks and how much &quot;better&quot; it becomes, staring at that code...
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=139651">Jiri Kosina</a> - Sept. 7, 2017, 7:55 p.m.</div>
<pre class="content">
On Thu, 7 Sep 2017, Ingo Molnar wrote:
<span class="quote">
&gt; &gt; &gt; When Linux brings a CPU down and back up, it switches to init_mm and then</span>
<span class="quote">&gt; &gt; &gt; loads swapper_pg_dir into CR3.  With PCID enabled, this has the side effect</span>
<span class="quote">&gt; &gt; &gt; of masking off the ASID bits in CR3.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; This can result in some confusion in the TLB handling code.  If we</span>
<span class="quote">&gt; &gt; &gt; bring a CPU down and back up with any ASID other than 0, we end up</span>
<span class="quote">&gt; &gt; &gt; with the wrong ASID active on the CPU after resume.  This could</span>
<span class="quote">&gt; &gt; &gt; cause our internal state to become corrupt, although major</span>
<span class="quote">&gt; &gt; &gt; corruption is unlikely because init_mm doesn&#39;t have any user pages.</span>
<span class="quote">&gt; &gt; &gt; More obviously, if CONFIG_DEBUG_VM=y, we&#39;ll trip over an assertion</span>
<span class="quote">&gt; &gt; &gt; in the next context switch.  The result of *that* is a failure to</span>
<span class="quote">&gt; &gt; &gt; resume from suspend with probability 1 - 1/6^(cpus-1).</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Fix it by reinitializing cpu_tlbstate on resume and CPU bringup.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Reported-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;</span>
<span class="quote">&gt; &gt; &gt; Reported-by: Jiri Kosina &lt;jikos@kernel.org&gt;</span>
<span class="quote">&gt; &gt; &gt; Fixes: 10af6235e0d3 (&quot;x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID&quot;)</span>
<span class="quote">&gt; &gt; &gt; Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Tested-by: Jiri Kosina &lt;jkosina@suse.cz&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The fix should be upstream already, as of 1c9fe4409ce3 and later.</span>

Hm, so I&#39;ve just experienced two instances in a row of reboot just after 
reading hibernation image (i.e. exactly the same symptom as before) even 
with 3b9f8ed kernel (which contains the fix). Seems like the fix is either 
incomplete (just the probability of it happening is lower), or I&#39;m seeing 
something differet with the same symptom.

I&#39;ll try to figure out whether it&#39;s the same VM_BUG_ON() triggering, but 
probably will be able to do so only tomorrow.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41531">Andy Lutomirski</a> - Sept. 8, 2017, 1:23 a.m.</div>
<pre class="content">
<span class="quote">&gt; On Sep 7, 2017, at 12:55 PM, Jiri Kosina &lt;jikos@kernel.org&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Thu, 7 Sep 2017, Ingo Molnar wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; When Linux brings a CPU down and back up, it switches to init_mm and then</span>
<span class="quote">&gt;&gt;&gt;&gt; loads swapper_pg_dir into CR3.  With PCID enabled, this has the side effect</span>
<span class="quote">&gt;&gt;&gt;&gt; of masking off the ASID bits in CR3.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; This can result in some confusion in the TLB handling code.  If we</span>
<span class="quote">&gt;&gt;&gt;&gt; bring a CPU down and back up with any ASID other than 0, we end up</span>
<span class="quote">&gt;&gt;&gt;&gt; with the wrong ASID active on the CPU after resume.  This could</span>
<span class="quote">&gt;&gt;&gt;&gt; cause our internal state to become corrupt, although major</span>
<span class="quote">&gt;&gt;&gt;&gt; corruption is unlikely because init_mm doesn&#39;t have any user pages.</span>
<span class="quote">&gt;&gt;&gt;&gt; More obviously, if CONFIG_DEBUG_VM=y, we&#39;ll trip over an assertion</span>
<span class="quote">&gt;&gt;&gt;&gt; in the next context switch.  The result of *that* is a failure to</span>
<span class="quote">&gt;&gt;&gt;&gt; resume from suspend with probability 1 - 1/6^(cpus-1).</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; Fix it by reinitializing cpu_tlbstate on resume and CPU bringup.</span>
<span class="quote">&gt;&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt;&gt; Reported-by: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Reported-by: Jiri Kosina &lt;jikos@kernel.org&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Fixes: 10af6235e0d3 (&quot;x86/mm: Implement PCID based optimization: try to preserve old TLB entries using PCID&quot;)</span>
<span class="quote">&gt;&gt;&gt;&gt; Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Tested-by: Jiri Kosina &lt;jkosina@suse.cz&gt;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; The fix should be upstream already, as of 1c9fe4409ce3 and later.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hm, so I&#39;ve just experienced two instances in a row of reboot just after </span>
<span class="quote">&gt; reading hibernation image (i.e. exactly the same symptom as before) even </span>
<span class="quote">&gt; with 3b9f8ed kernel (which contains the fix). Seems like the fix is either </span>
<span class="quote">&gt; incomplete (just the probability of it happening is lower), or I&#39;m seeing </span>
<span class="quote">&gt; something differet with the same symptom.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ll try to figure out whether it&#39;s the same VM_BUG_ON() triggering, but </span>
<span class="quote">&gt; probably will be able to do so only tomorrow.</span>
<span class="quote">&gt; </span>

Nah, don&#39;t waste your time.  I think I see the bug, and it&#39;s a different bug.  It&#39;s an easy one-line fix, but I have to figure out how to test it.
<span class="quote">
&gt; -- </span>
<span class="quote">&gt; Jiri Kosina</span>
<span class="quote">&gt; SUSE Labs</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index d23e61dc0640..4893abf7f74f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -198,6 +198,8 @@</span> <span class="p_context"> static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
 	cr4_set_bits(mask);
 }
 
<span class="p_add">+extern void initialize_tlbstate_and_flush(void);</span>
<span class="p_add">+</span>
 static inline void __native_flush_tlb(void)
 {
 	/*
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index efba8e3da3e2..40cb4d0a5982 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -1583,6 +1583,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	mmgrab(&amp;init_mm);
 	me-&gt;active_mm = &amp;init_mm;
 	BUG_ON(me-&gt;mm);
<span class="p_add">+	initialize_tlbstate_and_flush();</span>
 	enter_lazy_tlb(&amp;init_mm, me);
 
 	load_sp0(t, &amp;current-&gt;thread);
<span class="p_chunk">@@ -1637,6 +1638,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	mmgrab(&amp;init_mm);
 	curr-&gt;active_mm = &amp;init_mm;
 	BUG_ON(curr-&gt;mm);
<span class="p_add">+	initialize_tlbstate_and_flush();</span>
 	enter_lazy_tlb(&amp;init_mm, curr);
 
 	load_sp0(t, thread);
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index ce104b962a17..dbbcfd59726a 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -214,6 +214,50 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 }
 
 /*
<span class="p_add">+ * Call this when reinitializing a CPU.  It fixes the following potential</span>
<span class="p_add">+ * problems:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * - The ASID changed from what cpu_tlbstate thinks it is (most likely</span>
<span class="p_add">+ *   because the CPU was taken down and came back up with CR3&#39;s PCID</span>
<span class="p_add">+ *   bits clear.  CPU hotplug can do this.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * - The TLB contains junk in slots corresponding to inactive ASIDs.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * - The CPU went so far out to lunch that it may have missed a TLB</span>
<span class="p_add">+ *   flush.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void initialize_tlbstate_and_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	struct mm_struct *mm = this_cpu_read(cpu_tlbstate.loaded_mm);</span>
<span class="p_add">+	u64 tlb_gen = atomic64_read(&amp;init_mm.context.tlb_gen);</span>
<span class="p_add">+	unsigned long cr3 = __read_cr3();</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Assert that CR3 already references the right mm. */</span>
<span class="p_add">+	WARN_ON((cr3 &amp; CR3_ADDR_MASK) != __pa(mm-&gt;pgd));</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Assert that CR4.PCIDE is set if needed.  (CR4.PCIDE initialization</span>
<span class="p_add">+	 * doesn&#39;t work like other CR4 bits because it can only be set from</span>
<span class="p_add">+	 * long mode.)</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WARN_ON(boot_cpu_has(X86_CR4_PCIDE) &amp;&amp;</span>
<span class="p_add">+		!(cr4_read_shadow() &amp; X86_CR4_PCIDE));</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Force ASID 0 and force a TLB flush. */</span>
<span class="p_add">+	write_cr3(cr3 &amp; ~CR3_PCID_MASK);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Reinitialize tlbstate. */</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.loaded_mm_asid, 0);</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.next_asid, 1);</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id, mm-&gt;context.ctx_id);</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen, tlb_gen);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 1; i &lt; TLB_NR_DYN_ASIDS; i++)</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.ctxs[i].ctx_id, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * flush_tlb_func_common()&#39;s memory ordering requirement is that any
  * TLB fills that happen after we flush the TLB are ordered after we
  * read active_mm&#39;s tlb_gen.  We don&#39;t need any explicit barriers
<span class="p_header">diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c</span>
<span class="p_header">index 78459a6d455a..4d68d59f457d 100644</span>
<span class="p_header">--- a/arch/x86/power/cpu.c</span>
<span class="p_header">+++ b/arch/x86/power/cpu.c</span>
<span class="p_chunk">@@ -181,6 +181,7 @@</span> <span class="p_context"> static void fix_processor_context(void)</span>
 #endif
 	load_TR_desc();				/* This does ltr */
 	load_mm_ldt(current-&gt;active_mm);	/* This does lldt */
<span class="p_add">+	initialize_tlbstate_and_flush();</span>
 
 	fpu__resume_cpu();
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



