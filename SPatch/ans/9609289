
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,2/4] mm, tree wide: replace __GFP_REPEAT by __GFP_RETRY_MAYFAIL with more useful semantic - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,2/4] mm, tree wide: replace __GFP_REPEAT by __GFP_RETRY_MAYFAIL with more useful semantic</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 7, 2017, 3:48 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170307154843.32516-3-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9609289/mbox/"
   >mbox</a>
|
   <a href="/patch/9609289/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9609289/">/patch/9609289/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	0F8636046A for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  7 Mar 2017 15:58:04 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0030927165
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  7 Mar 2017 15:58:04 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id E90FA27CF3; Tue,  7 Mar 2017 15:58:03 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6456028405
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  7 Mar 2017 15:58:02 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932827AbdCGP5d (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 7 Mar 2017 10:57:33 -0500
Received: from mail-wr0-f193.google.com ([209.85.128.193]:35187 &quot;EHLO
	mail-wr0-f193.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1755528AbdCGP4k (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 7 Mar 2017 10:56:40 -0500
Received: by mail-wr0-f193.google.com with SMTP id u108so765164wrb.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 07 Mar 2017 07:56:05 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=bbuRGdsHmtOsC7f9bo/954a0+lef4Pmvemt2hy+anAI=;
	b=iUxqFweRFNkVL7xqcGr0B7x/GDA7tbSiBKISSFQSUFJ86N/i9ESCWlqTnPtgOSjkFd
	xfcD/qcJosU53HNvPOWIFNOy8oHPZinQwXxQy+k1FYpHWV9hbC+SBZXKA9da/IkujVtn
	A9/Nkq55QM5UEfIEFrB+2J0jkTdc5bTPuS/n1Ah65RpU7rJ+amimoBtwuAemP9VL0xMw
	oGNKfbC3z7U7taJozT9c8nfbNmG8L+dKfYwIIcyZTZaiYEnwqdhmShDAPpeLttiAquy1
	V/Niz2f44/gmQP4nNQrgSd4ibLJzWoEOS0TN4szRSP9t1igT1g4axlYMAcXJ/zyLjwzl
	81mQ==
X-Gm-Message-State: AMke39nWZeWy7B86XJIocGoZ/ZeQuEq0V1u8Nwvn6jjAA36HGrXY4kX4IoRJkbFMc5Q1UA==
X-Received: by 10.223.135.153 with SMTP id b25mr856144wrb.169.1488901731405; 
	Tue, 07 Mar 2017 07:48:51 -0800 (PST)
Received: from tiehlicka.suse.cz (prg-ext-pat.suse.com. [213.151.95.130])
	by smtp.gmail.com with ESMTPSA id
	z70sm507889wrc.2.2017.03.07.07.48.50
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 07 Mar 2017 07:48:50 -0800 (PST)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: &lt;linux-mm@kvack.org&gt;
Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;, Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [RFC PATCH 2/4] mm,
	tree wide: replace __GFP_REPEAT by __GFP_RETRY_MAYFAIL with more
	useful semantic
Date: Tue,  7 Mar 2017 16:48:41 +0100
Message-Id: &lt;20170307154843.32516-3-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170307154843.32516-1-mhocko@kernel.org&gt;
References: &lt;20170307154843.32516-1-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - March 7, 2017, 3:48 p.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

__GFP_REPEAT was designed to allow retry-but-eventually-fail semantic to
the page allocator. This has been true but only for allocations requests
larger than PAGE_ALLOC_COSTLY_ORDER. It has been always ignored for
smaller sizes. This is a bit unfortunate because there is no way to
express the same semantic for those requests and they are considered too
important to fail so they might end up looping in the page allocator for
ever, similarly to GFP_NOFAIL requests.

Now that the whole tree has been cleaned up and accidental or misled
usage of __GFP_REPEAT flag has been removed for !costly requests we can
give the original flag a better name and more importantly a more useful
semantic. Let&#39;s rename it to __GFP_RETRY_MAYFAIL which tells the user that
the allocator would try really hard but there is no promise of a
success. This will work independent of the order and overrides the
default allocator behavior. Page allocator users have several levels of
guarantee vs. cost options (take GFP_KERNEL as an example)
- GFP_KERNEL &amp; ~__GFP_RECLAIM - optimistic allocation without _any_
  attempt to free memory at all. The most light weight mode which even
  doesn&#39;t kick the background reclaim. Should be used carefully because
  it might deplete the memory and the next user might hit the more
  aggressive reclaim
- GFP_KERNEL &amp; ~__GFP_DIRECT_RECLAIM (or GFP_NOWAIT)- optimistic
  allocation without any attempt to free memory from the current context
  but can wake kswapd to reclaim memory if the zone is below the low
  watermark. Can be used from either atomic contexts or when the request
  is a performance optimization and there is another fallback for a slow
  path.
- (GFP_KERNEL|__GFP_HIGH) &amp; ~__GFP_DIRECT_RECLAIM (aka GFP_ATOMIC) - non
  sleeping allocation with an expensive fallback so it can access some
  portion of memory reserves. Usually used from interrupt/bh context with
  an expensive slow path fallback.
- GFP_KERNEL - both background and direct reclaim are allowed and the
  _default_ page allocator behavior is used. That means that !costly
  allocation requests are basically nofail (unless the requesting task
  is killed by the OOM killer) and costly will fail early rather than
  cause disruptive reclaim.
- GFP_KERNEL | __GFP_NORETRY - overrides the default allocator behavior and
  all allocation requests fail early rather than cause disruptive
  reclaim (one round of reclaim in this implementation). The OOM killer
  is not invoked.
- GFP_KERNEL | __GFP_RETRY_MAYFAIL - overrides the default allocator behavior
  and all allocation requests try really hard. The request will fail if the
  reclaim cannot make any progress. The OOM killer won&#39;t be triggered.
- GFP_KERNEL | __GFP_NOFAIL - overrides the default allocator behavior
  and all allocation requests will loop endlessly until they
  succeed. This might be really dangerous especially for larger orders.

Existing users of __GFP_REPEAT are changed to __GFP_RETRY_MAYFAIL because
they already had their semantic. No new users are added.
__alloc_pages_slowpath is changed to bail out for __GFP_RETRY_MAYFAIL if
there is no progress and we have already passed the OOM point. This
means that all the reclaim opportunities have been exhausted except the
most disruptive one (the OOM killer) and a user defined fallback
behavior is more sensible than keep retrying in the page allocator.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 Documentation/DMA-ISA-LPC.txt                |  2 +-
 arch/powerpc/include/asm/book3s/64/pgalloc.h |  2 +-
 arch/powerpc/kvm/book3s_64_mmu_hv.c          |  2 +-
 drivers/mmc/host/wbsd.c                      |  2 +-
 drivers/s390/char/vmcp.c                     |  2 +-
 drivers/target/target_core_transport.c       |  2 +-
 drivers/vhost/net.c                          |  2 +-
 drivers/vhost/scsi.c                         |  2 +-
 drivers/vhost/vsock.c                        |  2 +-
 fs/btrfs/check-integrity.c                   |  2 +-
 fs/btrfs/raid56.c                            |  2 +-
 include/linux/gfp.h                          | 32 +++++++++++++++++++---------
 include/linux/slab.h                         |  3 ++-
 include/trace/events/mmflags.h               |  2 +-
 mm/hugetlb.c                                 |  4 ++--
 mm/internal.h                                |  2 +-
 mm/page_alloc.c                              | 14 +++++++++---
 mm/sparse-vmemmap.c                          |  4 ++--
 mm/util.c                                    |  6 +++---
 mm/vmalloc.c                                 |  2 +-
 mm/vmscan.c                                  |  8 +++----
 net/core/dev.c                               |  6 +++---
 net/core/skbuff.c                            |  2 +-
 net/sched/sch_fq.c                           |  2 +-
 tools/perf/builtin-kmem.c                    |  2 +-
 25 files changed, 66 insertions(+), 45 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=135691">NeilBrown</a> - May 25, 2017, 1:21 a.m.</div>
<pre class="content">
On Tue, Mar 07 2017, Michal Hocko wrote:
<span class="quote">
&gt; diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="quote">&gt; index 2bfcfd33e476..60af7937c6f2 100644</span>
<span class="quote">&gt; --- a/include/linux/gfp.h</span>
<span class="quote">&gt; +++ b/include/linux/gfp.h</span>
<span class="quote">&gt; @@ -25,7 +25,7 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  #define ___GFP_FS		0x80u</span>
<span class="quote">&gt;  #define ___GFP_COLD		0x100u</span>
<span class="quote">&gt;  #define ___GFP_NOWARN		0x200u</span>
<span class="quote">&gt; -#define ___GFP_REPEAT		0x400u</span>
<span class="quote">&gt; +#define ___GFP_RETRY_MAYFAIL		0x400u</span>
<span class="quote">&gt;  #define ___GFP_NOFAIL		0x800u</span>
<span class="quote">&gt;  #define ___GFP_NORETRY		0x1000u</span>
<span class="quote">&gt;  #define ___GFP_MEMALLOC		0x2000u</span>
<span class="quote">&gt; @@ -136,26 +136,38 @@ struct vm_area_struct;</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * __GFP_RECLAIM is shorthand to allow/forbid both direct and kswapd reclaim.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; - * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt</span>
<span class="quote">&gt; - *   _might_ fail.  This depends upon the particular VM implementation.</span>
<span class="quote">&gt; + * The default allocator behavior depends on the request size. We have a concept</span>
<span class="quote">&gt; + * of so called costly allocations (with order &gt; PAGE_ALLOC_COSTLY_ORDER).</span>

Boundary conditions is one of my pet peeves....
The description here suggests that an allocation of
&quot;1&lt;&lt;PAGE_ALLOC_COSTLY_ORDER&quot; pages is not &quot;costly&quot;, which is
inconsistent with how those words would normally be interpreted.

Looking at the code I see comparisons like:

   order &lt; PAGE_ALLOC_COSTLY_ORDER
or
   order &gt;= PAGE_ALLOC_COSTLY_ORDER

which supports the documented (but incoherent) meaning.

But I also see:

  order = max_t(int, PAGE_ALLOC_COSTLY_ORDER - 1, 0);

which looks like it is trying to perform the largest non-costly
allocation, but is making a smaller allocation than necessary.

I would *really* like it if the constant actually meant what its name
implied.

 PAGE_ALLOC_MAX_NON_COSTLY
??
<span class="quote">
&gt; + * !costly allocations are too essential to fail so they are implicitly</span>
<span class="quote">&gt; + * non-failing (with some exceptions like OOM victims might fail) by default while</span>
<span class="quote">&gt; + * costly requests try to be not disruptive and back off even without invoking</span>
<span class="quote">&gt; + * the OOM killer. The following three modifiers might be used to override some of</span>
<span class="quote">&gt; + * these implicit rules</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * __GFP_NORETRY: The VM implementation must not retry indefinitely and will</span>
<span class="quote">&gt; + *   return NULL when direct reclaim and memory compaction have failed to allow</span>
<span class="quote">&gt; + *   the allocation to succeed.  The OOM killer is not called with the current</span>
<span class="quote">&gt; + *   implementation. This is a default mode for costly allocations.</span>

The name here is &quot;NORETRY&quot;, but the text says &quot;not retry indefinitely&quot;.
So does it retry or not?
I would assuming it &quot;tried&quot; once, and only once.
However it could be that a &quot;try&quot; is not a simple well defined task.
Maybe some escalation happens on the 2nd or 3rd &quot;try&quot;, so they are really
trying different things?

The word &quot;indefinitely&quot; implies there is a definite limit.  It might
help to say what that is, or at least say that it is small.

Also, this documentation is phrased to tell the VM implementor what is,
or is not, allowed.  Most readers will be more interested is the
responsibilities of the caller.

  __GFP_NORETRY: The VM implementation will not retry after all
     reasonable avenues for finding free memory have been pursued.  The
     implementation may sleep (i.e. call &#39;schedule()&#39;), but only while
     waiting for another task to perform some specific action.
     The caller must handle failure.  This flag is suitable when failure can
     easily be handled at small cost, such as reduced throughput.
<span class="quote">  

&gt; + *</span>
<span class="quote">&gt; + * __GFP_RETRY_MAYFAIL: Try hard to allocate the memory, but the allocation attempt</span>
<span class="quote">&gt; + *   _might_ fail. All viable forms of memory reclaim are tried before the fail.</span>
<span class="quote">&gt; + *   The OOM killer is excluded because this would be too disruptive. This can be</span>
<span class="quote">&gt; + *   used to override non-failing default behavior for !costly requests as well as</span>
<span class="quote">&gt; + *   fortify costly requests.</span>

What does &quot;Try hard&quot; mean?
In part, it means &quot;retry everything a few more times&quot;, I guess in the
hope that something happened in the mean time.
It also seems to mean waiting for compaction to happen, which I
guess is only relevant for &gt;PAGE_SIZE allocations?
Maybe it also means waiting for page-out to complete.
So the summary would be that it waits for a little while, hoping for a
miracle.

   __GFP_RETRY_MAYFAIL:  The VM implementation will retry memory reclaim
     procedures that have previously failed if there is some indication
     that progress has been made else where.  It can wait for other
     tasks to attempt high level approaches to freeing memory such as
     compaction (which removed fragmentation) and page-out.
     There is still a definite limit to the number of retries, but it is
     a larger limit than with __GFP_NORERY.
     Allocations with this flag may fail, but only when there is
     genuinely little unused memory.  While these allocations do not
     directly trigger the OOM killer, their failure indicates that the
     system is likely to need to use the OOM killer soon.
     The caller must handle failure, but can reasonably do so by failing
     a higher-level request, or completing it only in a much less
     efficient manner.
     If the allocation does fail, and the caller is in a position to
     free some non-essential memory, doing so could benefit the system
     as a whole.
<span class="quote">    


&gt;   *</span>
<span class="quote">&gt;   * __GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller</span>
<span class="quote">&gt;   *   cannot handle allocation failures. New users should be evaluated carefully</span>
<span class="quote">&gt;   *   (and the flag should be used only when there is no reasonable failure</span>
<span class="quote">&gt;   *   policy) but it is definitely preferable to use the flag rather than</span>
<span class="quote">&gt; - *   opencode endless loop around allocator.</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * __GFP_NORETRY: The VM implementation must not retry indefinitely and will</span>
<span class="quote">&gt; - *   return NULL when direct reclaim and memory compaction have failed to allow</span>
<span class="quote">&gt; - *   the allocation to succeed.  The OOM killer is not called with the current</span>
<span class="quote">&gt; - *   implementation.</span>
<span class="quote">&gt; + *   opencode endless loop around allocator. Using this flag for costly allocations</span>
<span class="quote">&gt; + *   is _highly_ discouraged.</span>

Should this explicitly say that the OOM killer might be invoked in an attempt
to satisfy this allocation?  Is the OOM killer *only* invoked from
allocations with __GFP_NOFAIL ?
Maybe be extra explicit &quot;The allocation could block indefinitely but
will never return with failure.  Testing for failure is pointless.&quot;.


I&#39;ve probably got several specifics wrong.  I&#39;ve tried to answer the
questions that I would like to see answered by the documentation.   If
you can fix it up so that those questions are answered correctly, that
would be great.

Thanks,
NeilBrown
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=154191">Wei Yang</a> - June 3, 2017, 2:24 a.m.</div>
<pre class="content">
Hi, Michal

Just go through your patch.

I have one question and one suggestion as below.

One suggestion:

This patch does two things to me:
1. Replace __GFP_REPEAT with __GFP_RETRY_MAYFAIL
2. Adjust the logic in page_alloc to provide the middle semantic

My suggestion is to split these two task into two patches, so that readers
could catch your fundamental logic change easily.

On Tue, Mar 07, 2017 at 04:48:41PM +0100, Michal Hocko wrote:
<span class="quote">&gt;From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;__GFP_REPEAT was designed to allow retry-but-eventually-fail semantic to</span>
<span class="quote">&gt;the page allocator. This has been true but only for allocations requests</span>
<span class="quote">&gt;larger than PAGE_ALLOC_COSTLY_ORDER. It has been always ignored for</span>
<span class="quote">&gt;smaller sizes. This is a bit unfortunate because there is no way to</span>
<span class="quote">&gt;express the same semantic for those requests and they are considered too</span>
<span class="quote">&gt;important to fail so they might end up looping in the page allocator for</span>
<span class="quote">&gt;ever, similarly to GFP_NOFAIL requests.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Now that the whole tree has been cleaned up and accidental or misled</span>
<span class="quote">&gt;usage of __GFP_REPEAT flag has been removed for !costly requests we can</span>
<span class="quote">&gt;give the original flag a better name and more importantly a more useful</span>
<span class="quote">&gt;semantic. Let&#39;s rename it to __GFP_RETRY_MAYFAIL which tells the user that</span>
<span class="quote">&gt;the allocator would try really hard but there is no promise of a</span>
<span class="quote">&gt;success. This will work independent of the order and overrides the</span>
<span class="quote">&gt;default allocator behavior. Page allocator users have several levels of</span>
<span class="quote">&gt;guarantee vs. cost options (take GFP_KERNEL as an example)</span>
<span class="quote">&gt;- GFP_KERNEL &amp; ~__GFP_RECLAIM - optimistic allocation without _any_</span>
<span class="quote">&gt;  attempt to free memory at all. The most light weight mode which even</span>
<span class="quote">&gt;  doesn&#39;t kick the background reclaim. Should be used carefully because</span>
<span class="quote">&gt;  it might deplete the memory and the next user might hit the more</span>
<span class="quote">&gt;  aggressive reclaim</span>
<span class="quote">&gt;- GFP_KERNEL &amp; ~__GFP_DIRECT_RECLAIM (or GFP_NOWAIT)- optimistic</span>
<span class="quote">&gt;  allocation without any attempt to free memory from the current context</span>
<span class="quote">&gt;  but can wake kswapd to reclaim memory if the zone is below the low</span>
<span class="quote">&gt;  watermark. Can be used from either atomic contexts or when the request</span>
<span class="quote">&gt;  is a performance optimization and there is another fallback for a slow</span>
<span class="quote">&gt;  path.</span>
<span class="quote">&gt;- (GFP_KERNEL|__GFP_HIGH) &amp; ~__GFP_DIRECT_RECLAIM (aka GFP_ATOMIC) - non</span>
<span class="quote">&gt;  sleeping allocation with an expensive fallback so it can access some</span>
<span class="quote">&gt;  portion of memory reserves. Usually used from interrupt/bh context with</span>
<span class="quote">&gt;  an expensive slow path fallback.</span>
<span class="quote">&gt;- GFP_KERNEL - both background and direct reclaim are allowed and the</span>
<span class="quote">&gt;  _default_ page allocator behavior is used. That means that !costly</span>
<span class="quote">&gt;  allocation requests are basically nofail (unless the requesting task</span>
<span class="quote">&gt;  is killed by the OOM killer) and costly will fail early rather than</span>
<span class="quote">&gt;  cause disruptive reclaim.</span>
<span class="quote">&gt;- GFP_KERNEL | __GFP_NORETRY - overrides the default allocator behavior and</span>
<span class="quote">&gt;  all allocation requests fail early rather than cause disruptive</span>
<span class="quote">&gt;  reclaim (one round of reclaim in this implementation). The OOM killer</span>
<span class="quote">&gt;  is not invoked.</span>
<span class="quote">&gt;- GFP_KERNEL | __GFP_RETRY_MAYFAIL - overrides the default allocator behavior</span>
<span class="quote">&gt;  and all allocation requests try really hard. The request will fail if the</span>
<span class="quote">&gt;  reclaim cannot make any progress. The OOM killer won&#39;t be triggered.</span>
<span class="quote">&gt;- GFP_KERNEL | __GFP_NOFAIL - overrides the default allocator behavior</span>
<span class="quote">&gt;  and all allocation requests will loop endlessly until they</span>
<span class="quote">&gt;  succeed. This might be really dangerous especially for larger orders.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Existing users of __GFP_REPEAT are changed to __GFP_RETRY_MAYFAIL because</span>
<span class="quote">&gt;they already had their semantic. No new users are added.</span>
<span class="quote">&gt;__alloc_pages_slowpath is changed to bail out for __GFP_RETRY_MAYFAIL if</span>
<span class="quote">&gt;there is no progress and we have already passed the OOM point. This</span>
<span class="quote">&gt;means that all the reclaim opportunities have been exhausted except the</span>
<span class="quote">&gt;most disruptive one (the OOM killer) and a user defined fallback</span>
<span class="quote">&gt;behavior is more sensible than keep retrying in the page allocator.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt;---</span>
<span class="quote">&gt; Documentation/DMA-ISA-LPC.txt                |  2 +-</span>
<span class="quote">&gt; arch/powerpc/include/asm/book3s/64/pgalloc.h |  2 +-</span>
<span class="quote">&gt; arch/powerpc/kvm/book3s_64_mmu_hv.c          |  2 +-</span>
<span class="quote">&gt; drivers/mmc/host/wbsd.c                      |  2 +-</span>
<span class="quote">&gt; drivers/s390/char/vmcp.c                     |  2 +-</span>
<span class="quote">&gt; drivers/target/target_core_transport.c       |  2 +-</span>
<span class="quote">&gt; drivers/vhost/net.c                          |  2 +-</span>
<span class="quote">&gt; drivers/vhost/scsi.c                         |  2 +-</span>
<span class="quote">&gt; drivers/vhost/vsock.c                        |  2 +-</span>
<span class="quote">&gt; fs/btrfs/check-integrity.c                   |  2 +-</span>
<span class="quote">&gt; fs/btrfs/raid56.c                            |  2 +-</span>
<span class="quote">&gt; include/linux/gfp.h                          | 32 +++++++++++++++++++---------</span>
<span class="quote">&gt; include/linux/slab.h                         |  3 ++-</span>
<span class="quote">&gt; include/trace/events/mmflags.h               |  2 +-</span>
<span class="quote">&gt; mm/hugetlb.c                                 |  4 ++--</span>
<span class="quote">&gt; mm/internal.h                                |  2 +-</span>
<span class="quote">&gt; mm/page_alloc.c                              | 14 +++++++++---</span>
<span class="quote">&gt; mm/sparse-vmemmap.c                          |  4 ++--</span>
<span class="quote">&gt; mm/util.c                                    |  6 +++---</span>
<span class="quote">&gt; mm/vmalloc.c                                 |  2 +-</span>
<span class="quote">&gt; mm/vmscan.c                                  |  8 +++----</span>
<span class="quote">&gt; net/core/dev.c                               |  6 +++---</span>
<span class="quote">&gt; net/core/skbuff.c                            |  2 +-</span>
<span class="quote">&gt; net/sched/sch_fq.c                           |  2 +-</span>
<span class="quote">&gt; tools/perf/builtin-kmem.c                    |  2 +-</span>
<span class="quote">&gt; 25 files changed, 66 insertions(+), 45 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;diff --git a/Documentation/DMA-ISA-LPC.txt b/Documentation/DMA-ISA-LPC.txt</span>
<span class="quote">&gt;index c41331398752..7a065ac4a9d1 100644</span>
<span class="quote">&gt;--- a/Documentation/DMA-ISA-LPC.txt</span>
<span class="quote">&gt;+++ b/Documentation/DMA-ISA-LPC.txt</span>
<span class="quote">&gt;@@ -42,7 +42,7 @@ requirements you pass the flag GFP_DMA to kmalloc.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Unfortunately the memory available for ISA DMA is scarce so unless you</span>
<span class="quote">&gt; allocate the memory during boot-up it&#39;s a good idea to also pass</span>
<span class="quote">&gt;-__GFP_REPEAT and __GFP_NOWARN to make the allocator try a bit harder.</span>
<span class="quote">&gt;+__GFP_RETRY_MAYFAIL and __GFP_NOWARN to make the allocator try a bit harder.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; (This scarcity also means that you should allocate the buffer as</span>
<span class="quote">&gt; early as possible and not release it until the driver is unloaded.)</span>
<span class="quote">&gt;diff --git a/arch/powerpc/include/asm/book3s/64/pgalloc.h b/arch/powerpc/include/asm/book3s/64/pgalloc.h</span>
<span class="quote">&gt;index cd5e7aa8cc34..1b835aa5b4d1 100644</span>
<span class="quote">&gt;--- a/arch/powerpc/include/asm/book3s/64/pgalloc.h</span>
<span class="quote">&gt;+++ b/arch/powerpc/include/asm/book3s/64/pgalloc.h</span>
<span class="quote">&gt;@@ -56,7 +56,7 @@ static inline pgd_t *radix__pgd_alloc(struct mm_struct *mm)</span>
<span class="quote">&gt; 	return (pgd_t *)__get_free_page(PGALLOC_GFP);</span>
<span class="quote">&gt; #else</span>
<span class="quote">&gt; 	struct page *page;</span>
<span class="quote">&gt;-	page = alloc_pages(PGALLOC_GFP | __GFP_REPEAT, 4);</span>
<span class="quote">&gt;+	page = alloc_pages(PGALLOC_GFP | __GFP_RETRY_MAYFAIL, 4);</span>
<span class="quote">&gt; 	if (!page)</span>
<span class="quote">&gt; 		return NULL;</span>
<span class="quote">&gt; 	return (pgd_t *) page_address(page);</span>
<span class="quote">&gt;diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="quote">&gt;index 8c68145ba1bd..8ad2c309f14a 100644</span>
<span class="quote">&gt;--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="quote">&gt;+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="quote">&gt;@@ -93,7 +93,7 @@ int kvmppc_allocate_hpt(struct kvm_hpt_info *info, u32 order)</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (!hpt)</span>
<span class="quote">&gt;-		hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT</span>
<span class="quote">&gt;+		hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt; 				       |__GFP_NOWARN, order - PAGE_SHIFT);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (!hpt)</span>
<span class="quote">&gt;diff --git a/drivers/mmc/host/wbsd.c b/drivers/mmc/host/wbsd.c</span>
<span class="quote">&gt;index bd04e8bae010..b58fa5b5b972 100644</span>
<span class="quote">&gt;--- a/drivers/mmc/host/wbsd.c</span>
<span class="quote">&gt;+++ b/drivers/mmc/host/wbsd.c</span>
<span class="quote">&gt;@@ -1386,7 +1386,7 @@ static void wbsd_request_dma(struct wbsd_host *host, int dma)</span>
<span class="quote">&gt; 	 * order for ISA to be able to DMA to it.</span>
<span class="quote">&gt; 	 */</span>
<span class="quote">&gt; 	host-&gt;dma_buffer = kmalloc(WBSD_DMA_SIZE,</span>
<span class="quote">&gt;-		GFP_NOIO | GFP_DMA | __GFP_REPEAT | __GFP_NOWARN);</span>
<span class="quote">&gt;+		GFP_NOIO | GFP_DMA | __GFP_RETRY_MAYFAIL | __GFP_NOWARN);</span>
<span class="quote">&gt; 	if (!host-&gt;dma_buffer)</span>
<span class="quote">&gt; 		goto free;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;diff --git a/drivers/s390/char/vmcp.c b/drivers/s390/char/vmcp.c</span>
<span class="quote">&gt;index 65f5a794f26d..98749fa817da 100644</span>
<span class="quote">&gt;--- a/drivers/s390/char/vmcp.c</span>
<span class="quote">&gt;+++ b/drivers/s390/char/vmcp.c</span>
<span class="quote">&gt;@@ -98,7 +98,7 @@ vmcp_write(struct file *file, const char __user *buff, size_t count,</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt; 	if (!session-&gt;response)</span>
<span class="quote">&gt; 		session-&gt;response = (char *)__get_free_pages(GFP_KERNEL</span>
<span class="quote">&gt;-						| __GFP_REPEAT | GFP_DMA,</span>
<span class="quote">&gt;+						| __GFP_RETRY_MAYFAIL | GFP_DMA,</span>
<span class="quote">&gt; 						get_order(session-&gt;bufsize));</span>
<span class="quote">&gt; 	if (!session-&gt;response) {</span>
<span class="quote">&gt; 		mutex_unlock(&amp;session-&gt;mutex);</span>
<span class="quote">&gt;diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c</span>
<span class="quote">&gt;index 434d9d693989..e585d301c665 100644</span>
<span class="quote">&gt;--- a/drivers/target/target_core_transport.c</span>
<span class="quote">&gt;+++ b/drivers/target/target_core_transport.c</span>
<span class="quote">&gt;@@ -251,7 +251,7 @@ int transport_alloc_session_tags(struct se_session *se_sess,</span>
<span class="quote">&gt; 	int rc;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	se_sess-&gt;sess_cmd_map = kzalloc(tag_num * tag_size,</span>
<span class="quote">&gt;-					GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="quote">&gt;+					GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_MAYFAIL);</span>
<span class="quote">&gt; 	if (!se_sess-&gt;sess_cmd_map) {</span>
<span class="quote">&gt; 		se_sess-&gt;sess_cmd_map = vzalloc(tag_num * tag_size);</span>
<span class="quote">&gt; 		if (!se_sess-&gt;sess_cmd_map) {</span>
<span class="quote">&gt;diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c</span>
<span class="quote">&gt;index f61f852d6cfd..7d2c4ce6d8d1 100644</span>
<span class="quote">&gt;--- a/drivers/vhost/net.c</span>
<span class="quote">&gt;+++ b/drivers/vhost/net.c</span>
<span class="quote">&gt;@@ -817,7 +817,7 @@ static int vhost_net_open(struct inode *inode, struct file *f)</span>
<span class="quote">&gt; 	struct vhost_virtqueue **vqs;</span>
<span class="quote">&gt; 	int i;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;-	n = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_REPEAT);</span>
<span class="quote">&gt;+	n = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_RETRY_MAYFAIL);</span>
<span class="quote">&gt; 	if (!n)</span>
<span class="quote">&gt; 		return -ENOMEM;</span>
<span class="quote">&gt; 	vqs = kmalloc(VHOST_NET_VQ_MAX * sizeof(*vqs), GFP_KERNEL);</span>
<span class="quote">&gt;diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c</span>
<span class="quote">&gt;index fd6c8b66f06f..ff02a942c4d5 100644</span>
<span class="quote">&gt;--- a/drivers/vhost/scsi.c</span>
<span class="quote">&gt;+++ b/drivers/vhost/scsi.c</span>
<span class="quote">&gt;@@ -1404,7 +1404,7 @@ static int vhost_scsi_open(struct inode *inode, struct file *f)</span>
<span class="quote">&gt; 	struct vhost_virtqueue **vqs;</span>
<span class="quote">&gt; 	int r = -ENOMEM, i;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;-	vs = kzalloc(sizeof(*vs), GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="quote">&gt;+	vs = kzalloc(sizeof(*vs), GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_MAYFAIL);</span>
<span class="quote">&gt; 	if (!vs) {</span>
<span class="quote">&gt; 		vs = vzalloc(sizeof(*vs));</span>
<span class="quote">&gt; 		if (!vs)</span>
<span class="quote">&gt;diff --git a/drivers/vhost/vsock.c b/drivers/vhost/vsock.c</span>
<span class="quote">&gt;index d403c647ba56..5b76242d73e3 100644</span>
<span class="quote">&gt;--- a/drivers/vhost/vsock.c</span>
<span class="quote">&gt;+++ b/drivers/vhost/vsock.c</span>
<span class="quote">&gt;@@ -460,7 +460,7 @@ static int vhost_vsock_dev_open(struct inode *inode, struct file *file)</span>
<span class="quote">&gt; 	/* This struct is large and allocation could fail, fall back to vmalloc</span>
<span class="quote">&gt; 	 * if there is no other way.</span>
<span class="quote">&gt; 	 */</span>
<span class="quote">&gt;-	vsock = kvmalloc(sizeof(*vsock), GFP_KERNEL | __GFP_REPEAT);</span>
<span class="quote">&gt;+	vsock = kvmalloc(sizeof(*vsock), GFP_KERNEL | __GFP_RETRY_MAYFAIL);</span>
<span class="quote">&gt; 	if (!vsock)</span>
<span class="quote">&gt; 		return -ENOMEM;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;diff --git a/fs/btrfs/check-integrity.c b/fs/btrfs/check-integrity.c</span>
<span class="quote">&gt;index ab14c2e635ca..e334ed2b7e64 100644</span>
<span class="quote">&gt;--- a/fs/btrfs/check-integrity.c</span>
<span class="quote">&gt;+++ b/fs/btrfs/check-integrity.c</span>
<span class="quote">&gt;@@ -2923,7 +2923,7 @@ int btrfsic_mount(struct btrfs_fs_info *fs_info,</span>
<span class="quote">&gt; 		       fs_info-&gt;sectorsize, PAGE_SIZE);</span>
<span class="quote">&gt; 		return -1;</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt;-	state = kzalloc(sizeof(*state), GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="quote">&gt;+	state = kzalloc(sizeof(*state), GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_MAYFAIL);</span>
<span class="quote">&gt; 	if (!state) {</span>
<span class="quote">&gt; 		state = vzalloc(sizeof(*state));</span>
<span class="quote">&gt; 		if (!state) {</span>
<span class="quote">&gt;diff --git a/fs/btrfs/raid56.c b/fs/btrfs/raid56.c</span>
<span class="quote">&gt;index 1571bf26dc07..94af3db1d0e4 100644</span>
<span class="quote">&gt;--- a/fs/btrfs/raid56.c</span>
<span class="quote">&gt;+++ b/fs/btrfs/raid56.c</span>
<span class="quote">&gt;@@ -218,7 +218,7 @@ int btrfs_alloc_stripe_hash_table(struct btrfs_fs_info *info)</span>
<span class="quote">&gt; 	 * of a failing mount.</span>
<span class="quote">&gt; 	 */</span>
<span class="quote">&gt; 	table_size = sizeof(*table) + sizeof(*h) * num_entries;</span>
<span class="quote">&gt;-	table = kzalloc(table_size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="quote">&gt;+	table = kzalloc(table_size, GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_MAYFAIL);</span>
<span class="quote">&gt; 	if (!table) {</span>
<span class="quote">&gt; 		table = vzalloc(table_size);</span>
<span class="quote">&gt; 		if (!table)</span>
<span class="quote">&gt;diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="quote">&gt;index 2bfcfd33e476..60af7937c6f2 100644</span>
<span class="quote">&gt;--- a/include/linux/gfp.h</span>
<span class="quote">&gt;+++ b/include/linux/gfp.h</span>
<span class="quote">&gt;@@ -25,7 +25,7 @@ struct vm_area_struct;</span>
<span class="quote">&gt; #define ___GFP_FS		0x80u</span>
<span class="quote">&gt; #define ___GFP_COLD		0x100u</span>
<span class="quote">&gt; #define ___GFP_NOWARN		0x200u</span>
<span class="quote">&gt;-#define ___GFP_REPEAT		0x400u</span>
<span class="quote">&gt;+#define ___GFP_RETRY_MAYFAIL		0x400u</span>
<span class="quote">&gt; #define ___GFP_NOFAIL		0x800u</span>
<span class="quote">&gt; #define ___GFP_NORETRY		0x1000u</span>
<span class="quote">&gt; #define ___GFP_MEMALLOC		0x2000u</span>
<span class="quote">&gt;@@ -136,26 +136,38 @@ struct vm_area_struct;</span>
<span class="quote">&gt;  *</span>
<span class="quote">&gt;  * __GFP_RECLAIM is shorthand to allow/forbid both direct and kswapd reclaim.</span>
<span class="quote">&gt;  *</span>
<span class="quote">&gt;- * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt</span>
<span class="quote">&gt;- *   _might_ fail.  This depends upon the particular VM implementation.</span>
<span class="quote">&gt;+ * The default allocator behavior depends on the request size. We have a concept</span>
<span class="quote">&gt;+ * of so called costly allocations (with order &gt; PAGE_ALLOC_COSTLY_ORDER).</span>
<span class="quote">&gt;+ * !costly allocations are too essential to fail so they are implicitly</span>
<span class="quote">&gt;+ * non-failing (with some exceptions like OOM victims might fail) by default while</span>
<span class="quote">&gt;+ * costly requests try to be not disruptive and back off even without invoking</span>
<span class="quote">&gt;+ * the OOM killer. The following three modifiers might be used to override some of</span>
<span class="quote">&gt;+ * these implicit rules</span>
<span class="quote">&gt;+ *</span>
<span class="quote">&gt;+ * __GFP_NORETRY: The VM implementation must not retry indefinitely and will</span>
<span class="quote">&gt;+ *   return NULL when direct reclaim and memory compaction have failed to allow</span>
<span class="quote">&gt;+ *   the allocation to succeed.  The OOM killer is not called with the current</span>
<span class="quote">&gt;+ *   implementation. This is a default mode for costly allocations.</span>
<span class="quote">&gt;+ *</span>
<span class="quote">&gt;+ * __GFP_RETRY_MAYFAIL: Try hard to allocate the memory, but the allocation attempt</span>
<span class="quote">&gt;+ *   _might_ fail. All viable forms of memory reclaim are tried before the fail.</span>
<span class="quote">&gt;+ *   The OOM killer is excluded because this would be too disruptive. This can be</span>
<span class="quote">&gt;+ *   used to override non-failing default behavior for !costly requests as well as</span>
<span class="quote">&gt;+ *   fortify costly requests.</span>
<span class="quote">&gt;  *</span>
<span class="quote">&gt;  * __GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller</span>
<span class="quote">&gt;  *   cannot handle allocation failures. New users should be evaluated carefully</span>
<span class="quote">&gt;  *   (and the flag should be used only when there is no reasonable failure</span>
<span class="quote">&gt;  *   policy) but it is definitely preferable to use the flag rather than</span>
<span class="quote">&gt;- *   opencode endless loop around allocator.</span>
<span class="quote">&gt;- *</span>
<span class="quote">&gt;- * __GFP_NORETRY: The VM implementation must not retry indefinitely and will</span>
<span class="quote">&gt;- *   return NULL when direct reclaim and memory compaction have failed to allow</span>
<span class="quote">&gt;- *   the allocation to succeed.  The OOM killer is not called with the current</span>
<span class="quote">&gt;- *   implementation.</span>
<span class="quote">&gt;+ *   opencode endless loop around allocator. Using this flag for costly allocations</span>
<span class="quote">&gt;+ *   is _highly_ discouraged.</span>
<span class="quote">&gt;  */</span>
<span class="quote">&gt; #define __GFP_IO	((__force gfp_t)___GFP_IO)</span>
<span class="quote">&gt; #define __GFP_FS	((__force gfp_t)___GFP_FS)</span>
<span class="quote">&gt; #define __GFP_DIRECT_RECLAIM	((__force gfp_t)___GFP_DIRECT_RECLAIM) /* Caller can reclaim */</span>
<span class="quote">&gt; #define __GFP_KSWAPD_RECLAIM	((__force gfp_t)___GFP_KSWAPD_RECLAIM) /* kswapd can wake */</span>
<span class="quote">&gt; #define __GFP_RECLAIM ((__force gfp_t)(___GFP_DIRECT_RECLAIM|___GFP_KSWAPD_RECLAIM))</span>
<span class="quote">&gt;-#define __GFP_REPEAT	((__force gfp_t)___GFP_REPEAT)</span>
<span class="quote">&gt;+#define __GFP_RETRY_MAYFAIL	((__force gfp_t)___GFP_RETRY_MAYFAIL)</span>
<span class="quote">&gt; #define __GFP_NOFAIL	((__force gfp_t)___GFP_NOFAIL)</span>
<span class="quote">&gt; #define __GFP_NORETRY	((__force gfp_t)___GFP_NORETRY)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;diff --git a/include/linux/slab.h b/include/linux/slab.h</span>
<span class="quote">&gt;index 3c37a8c51921..064901ac3e37 100644</span>
<span class="quote">&gt;--- a/include/linux/slab.h</span>
<span class="quote">&gt;+++ b/include/linux/slab.h</span>
<span class="quote">&gt;@@ -469,7 +469,8 @@ static __always_inline void *kmalloc_large(size_t size, gfp_t flags)</span>
<span class="quote">&gt;  *</span>
<span class="quote">&gt;  * %__GFP_NOWARN - If allocation fails, don&#39;t issue any warnings.</span>
<span class="quote">&gt;  *</span>
<span class="quote">&gt;- * %__GFP_REPEAT - If allocation fails initially, try once more before failing.</span>
<span class="quote">&gt;+ * %__GFP_RETRY_MAYFAIL - Try really hard to succeed the allocation but fail</span>
<span class="quote">&gt;+ *   eventually.</span>
<span class="quote">&gt;  *</span>
<span class="quote">&gt;  * There are other flags available as well, but these are not intended</span>
<span class="quote">&gt;  * for general use, and so are not documented here. For a full list of</span>
<span class="quote">&gt;diff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h</span>
<span class="quote">&gt;index 304ff94363b2..418142a4efce 100644</span>
<span class="quote">&gt;--- a/include/trace/events/mmflags.h</span>
<span class="quote">&gt;+++ b/include/trace/events/mmflags.h</span>
<span class="quote">&gt;@@ -34,7 +34,7 @@</span>
<span class="quote">&gt; 	{(unsigned long)__GFP_FS,		&quot;__GFP_FS&quot;},		\</span>
<span class="quote">&gt; 	{(unsigned long)__GFP_COLD,		&quot;__GFP_COLD&quot;},		\</span>
<span class="quote">&gt; 	{(unsigned long)__GFP_NOWARN,		&quot;__GFP_NOWARN&quot;},	\</span>
<span class="quote">&gt;-	{(unsigned long)__GFP_REPEAT,		&quot;__GFP_REPEAT&quot;},	\</span>
<span class="quote">&gt;+	{(unsigned long)__GFP_RETRY_MAYFAIL,	&quot;__GFP_RETRY_MAYFAIL&quot;},	\</span>
<span class="quote">&gt; 	{(unsigned long)__GFP_NOFAIL,		&quot;__GFP_NOFAIL&quot;},	\</span>
<span class="quote">&gt; 	{(unsigned long)__GFP_NORETRY,		&quot;__GFP_NORETRY&quot;},	\</span>
<span class="quote">&gt; 	{(unsigned long)__GFP_COMP,		&quot;__GFP_COMP&quot;},		\</span>
<span class="quote">&gt;diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt;index a7aa811b7d14..dc598bfe4ce9 100644</span>
<span class="quote">&gt;--- a/mm/hugetlb.c</span>
<span class="quote">&gt;+++ b/mm/hugetlb.c</span>
<span class="quote">&gt;@@ -1369,7 +1369,7 @@ static struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	page = __alloc_pages_node(nid,</span>
<span class="quote">&gt; 		htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|</span>
<span class="quote">&gt;-						__GFP_REPEAT|__GFP_NOWARN,</span>
<span class="quote">&gt;+						__GFP_RETRY_MAYFAIL|__GFP_NOWARN,</span>
<span class="quote">&gt; 		huge_page_order(h));</span>
<span class="quote">&gt; 	if (page) {</span>
<span class="quote">&gt; 		prep_new_huge_page(h, page, nid);</span>
<span class="quote">&gt;@@ -1510,7 +1510,7 @@ static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,</span>
<span class="quote">&gt; 		struct vm_area_struct *vma, unsigned long addr, int nid)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	int order = huge_page_order(h);</span>
<span class="quote">&gt;-	gfp_t gfp = htlb_alloc_mask(h)|__GFP_COMP|__GFP_REPEAT|__GFP_NOWARN;</span>
<span class="quote">&gt;+	gfp_t gfp = htlb_alloc_mask(h)|__GFP_COMP|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;</span>
<span class="quote">&gt; 	unsigned int cpuset_mems_cookie;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	/*</span>
<span class="quote">&gt;diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="quote">&gt;index 823a7a89099b..8e6d347f70fb 100644</span>
<span class="quote">&gt;--- a/mm/internal.h</span>
<span class="quote">&gt;+++ b/mm/internal.h</span>
<span class="quote">&gt;@@ -23,7 +23,7 @@</span>
<span class="quote">&gt;  * hints such as HIGHMEM usage.</span>
<span class="quote">&gt;  */</span>
<span class="quote">&gt; #define GFP_RECLAIM_MASK (__GFP_RECLAIM|__GFP_HIGH|__GFP_IO|__GFP_FS|\</span>
<span class="quote">&gt;-			__GFP_NOWARN|__GFP_REPEAT|__GFP_NOFAIL|\</span>
<span class="quote">&gt;+			__GFP_NOWARN|__GFP_RETRY_MAYFAIL|__GFP_NOFAIL|\</span>
<span class="quote">&gt; 			__GFP_NORETRY|__GFP_MEMALLOC|__GFP_NOMEMALLOC|\</span>
<span class="quote">&gt; 			__GFP_ATOMIC)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="quote">&gt;index 5238b87aec91..bfe4a9bad0f8 100644</span>
<span class="quote">&gt;--- a/mm/page_alloc.c</span>
<span class="quote">&gt;+++ b/mm/page_alloc.c</span>
<span class="quote">&gt;@@ -3181,6 +3181,14 @@ __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt; 	/* The OOM killer will not help higher order allocs */</span>
<span class="quote">&gt; 	if (order &gt; PAGE_ALLOC_COSTLY_ORDER)</span>
<span class="quote">&gt; 		goto out;</span>
<span class="quote">&gt;+	/*</span>
<span class="quote">&gt;+	 * We have already exhausted all our reclaim opportunities without any</span>
<span class="quote">&gt;+	 * success so it is time to admit defeat. We will skip the OOM killer</span>
<span class="quote">&gt;+	 * because it is very likely that the caller has a more reasonable</span>
<span class="quote">&gt;+	 * fallback than shooting a random task.</span>
<span class="quote">&gt;+	 */</span>
<span class="quote">&gt;+	if (gfp_mask &amp; __GFP_RETRY_MAYFAIL)</span>
<span class="quote">&gt;+		goto out;</span>
<span class="quote">&gt; 	/* The OOM killer does not needlessly kill tasks for lowmem */</span>
<span class="quote">&gt; 	if (ac-&gt;high_zoneidx &lt; ZONE_NORMAL)</span>
<span class="quote">&gt; 		goto out;</span>
<span class="quote">&gt;@@ -3309,7 +3317,7 @@ should_compact_retry(struct alloc_context *ac, int order, int alloc_flags,</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	/*</span>
<span class="quote">&gt;-	 * !costly requests are much more important than __GFP_REPEAT</span>
<span class="quote">&gt;+	 * !costly requests are much more important than __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt; 	 * costly ones because they are de facto nofail and invoke OOM</span>
<span class="quote">&gt; 	 * killer to move on while costly can fail and users are ready</span>
<span class="quote">&gt; 	 * to cope with that. 1/4 retries is rather arbitrary but we</span>
<span class="quote">&gt;@@ -3776,9 +3784,9 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	/*</span>
<span class="quote">&gt; 	 * Do not retry costly high order allocations unless they are</span>
<span class="quote">&gt;-	 * __GFP_REPEAT</span>
<span class="quote">&gt;+	 * __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt; 	 */</span>
<span class="quote">&gt;-	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_REPEAT))</span>
<span class="quote">&gt;+	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_RETRY_MAYFAIL))</span>
<span class="quote">&gt; 		goto nopage;</span>

One question:

From your change log, it mentions will provide the same semantic for !costly
allocations. While the logic here is the same as before.

For a !costly allocation with __GFP_REPEAT flag, the difference after this
patch is no OOM will be invoked, while it will still continue in the loop.

Maybe I don&#39;t catch your point in this message:

  __GFP_REPEAT was designed to allow retry-but-eventually-fail semantic to
  the page allocator. This has been true but only for allocations requests
  larger than PAGE_ALLOC_COSTLY_ORDER. It has been always ignored for
  smaller sizes. This is a bit unfortunate because there is no way to
  express the same semantic for those requests and they are considered too
  important to fail so they might end up looping in the page allocator for
  ever, similarly to GFP_NOFAIL requests.

I thought you will provide the same semantic to !costly allocation, or I
misunderstand?
<span class="quote">
&gt; </span>
<span class="quote">&gt; 	if (should_reclaim_retry(gfp_mask, order, ac, alloc_flags,</span>
<span class="quote">&gt;diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c</span>
<span class="quote">&gt;index 574c67b663fe..b21ba0dfe102 100644</span>
<span class="quote">&gt;--- a/mm/sparse-vmemmap.c</span>
<span class="quote">&gt;+++ b/mm/sparse-vmemmap.c</span>
<span class="quote">&gt;@@ -56,11 +56,11 @@ void * __meminit vmemmap_alloc_block(unsigned long size, int node)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		if (node_state(node, N_HIGH_MEMORY))</span>
<span class="quote">&gt; 			page = alloc_pages_node(</span>
<span class="quote">&gt;-				node, GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,</span>
<span class="quote">&gt;+				node, GFP_KERNEL | __GFP_ZERO | __GFP_RETRY_MAYFAIL,</span>
<span class="quote">&gt; 				get_order(size));</span>
<span class="quote">&gt; 		else</span>
<span class="quote">&gt; 			page = alloc_pages(</span>
<span class="quote">&gt;-				GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,</span>
<span class="quote">&gt;+				GFP_KERNEL | __GFP_ZERO | __GFP_RETRY_MAYFAIL,</span>
<span class="quote">&gt; 				get_order(size));</span>
<span class="quote">&gt; 		if (page)</span>
<span class="quote">&gt; 			return page_address(page);</span>
<span class="quote">&gt;diff --git a/mm/util.c b/mm/util.c</span>
<span class="quote">&gt;index 6ed3e49bf1e5..885a78d1941b 100644</span>
<span class="quote">&gt;--- a/mm/util.c</span>
<span class="quote">&gt;+++ b/mm/util.c</span>
<span class="quote">&gt;@@ -339,7 +339,7 @@ EXPORT_SYMBOL(vm_mmap);</span>
<span class="quote">&gt;  * Uses kmalloc to get the memory but if the allocation fails then falls back</span>
<span class="quote">&gt;  * to the vmalloc allocator. Use kvfree for freeing the memory.</span>
<span class="quote">&gt;  *</span>
<span class="quote">&gt;- * Reclaim modifiers - __GFP_NORETRY and __GFP_NOFAIL are not supported. __GFP_REPEAT</span>
<span class="quote">&gt;+ * Reclaim modifiers - __GFP_NORETRY and __GFP_NOFAIL are not supported. __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt;  * is supported only for large (&gt;32kB) allocations, and it should be used only if</span>
<span class="quote">&gt;  * kmalloc is preferable to the vmalloc fallback, due to visible performance drawbacks.</span>
<span class="quote">&gt;  *</span>
<span class="quote">&gt;@@ -364,11 +364,11 @@ void *kvmalloc_node(size_t size, gfp_t flags, int node)</span>
<span class="quote">&gt; 		kmalloc_flags |= __GFP_NOWARN;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 		/*</span>
<span class="quote">&gt;-		 * We have to override __GFP_REPEAT by __GFP_NORETRY for !costly</span>
<span class="quote">&gt;+		 * We have to override __GFP_RETRY_MAYFAIL by __GFP_NORETRY for !costly</span>
<span class="quote">&gt; 		 * requests because there is no other way to tell the allocator</span>
<span class="quote">&gt; 		 * that we want to fail rather than retry endlessly.</span>
<span class="quote">&gt; 		 */</span>
<span class="quote">&gt;-		if (!(kmalloc_flags &amp; __GFP_REPEAT) ||</span>
<span class="quote">&gt;+		if (!(kmalloc_flags &amp; __GFP_RETRY_MAYFAIL) ||</span>
<span class="quote">&gt; 				(size &lt;= PAGE_SIZE &lt;&lt; PAGE_ALLOC_COSTLY_ORDER))</span>
<span class="quote">&gt; 			kmalloc_flags |= __GFP_NORETRY;</span>
<span class="quote">&gt; 	}</span>
<span class="quote">&gt;diff --git a/mm/vmalloc.c b/mm/vmalloc.c</span>
<span class="quote">&gt;index 32979d945766..c2fa2e1b79fc 100644</span>
<span class="quote">&gt;--- a/mm/vmalloc.c</span>
<span class="quote">&gt;+++ b/mm/vmalloc.c</span>
<span class="quote">&gt;@@ -1747,7 +1747,7 @@ void *__vmalloc_node_range(unsigned long size, unsigned long align,</span>
<span class="quote">&gt;  *	allocator with @gfp_mask flags.  Map them into contiguous</span>
<span class="quote">&gt;  *	kernel virtual space, using a pagetable protection of @prot.</span>
<span class="quote">&gt;  *</span>
<span class="quote">&gt;- *	Reclaim modifiers in @gfp_mask - __GFP_NORETRY, __GFP_REPEAT</span>
<span class="quote">&gt;+ *	Reclaim modifiers in @gfp_mask - __GFP_NORETRY, __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt;  *	and __GFP_NOFAIL are not supported</span>
<span class="quote">&gt;  *</span>
<span class="quote">&gt;  *	Any use of gfp flags outside of GFP_KERNEL should be consulted</span>
<span class="quote">&gt;diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt;index 4e0a828781e5..8f547176e02c 100644</span>
<span class="quote">&gt;--- a/mm/vmscan.c</span>
<span class="quote">&gt;+++ b/mm/vmscan.c</span>
<span class="quote">&gt;@@ -2435,18 +2435,18 @@ static inline bool should_continue_reclaim(struct pglist_data *pgdat,</span>
<span class="quote">&gt; 		return false;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	/* Consider stopping depending on scan and reclaim activity */</span>
<span class="quote">&gt;-	if (sc-&gt;gfp_mask &amp; __GFP_REPEAT) {</span>
<span class="quote">&gt;+	if (sc-&gt;gfp_mask &amp; __GFP_RETRY_MAYFAIL) {</span>
<span class="quote">&gt; 		/*</span>
<span class="quote">&gt;-		 * For __GFP_REPEAT allocations, stop reclaiming if the</span>
<span class="quote">&gt;+		 * For __GFP_RETRY_MAYFAIL allocations, stop reclaiming if the</span>
<span class="quote">&gt; 		 * full LRU list has been scanned and we are still failing</span>
<span class="quote">&gt; 		 * to reclaim pages. This full LRU scan is potentially</span>
<span class="quote">&gt;-		 * expensive but a __GFP_REPEAT caller really wants to succeed</span>
<span class="quote">&gt;+		 * expensive but a __GFP_RETRY_MAYFAIL caller really wants to succeed</span>
<span class="quote">&gt; 		 */</span>
<span class="quote">&gt; 		if (!nr_reclaimed &amp;&amp; !nr_scanned)</span>
<span class="quote">&gt; 			return false;</span>
<span class="quote">&gt; 	} else {</span>
<span class="quote">&gt; 		/*</span>
<span class="quote">&gt;-		 * For non-__GFP_REPEAT allocations which can presumably</span>
<span class="quote">&gt;+		 * For non-__GFP_RETRY_MAYFAIL allocations which can presumably</span>
<span class="quote">&gt; 		 * fail without consequence, stop if we failed to reclaim</span>
<span class="quote">&gt; 		 * any pages from the last SWAP_CLUSTER_MAX number of</span>
<span class="quote">&gt; 		 * pages that were scanned. This will return to the</span>
<span class="quote">&gt;diff --git a/net/core/dev.c b/net/core/dev.c</span>
<span class="quote">&gt;index d947308ee255..3e659ac9e0ed 100644</span>
<span class="quote">&gt;--- a/net/core/dev.c</span>
<span class="quote">&gt;+++ b/net/core/dev.c</span>
<span class="quote">&gt;@@ -7121,7 +7121,7 @@ static int netif_alloc_rx_queues(struct net_device *dev)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	BUG_ON(count &lt; 1);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;-	rx = kvzalloc(sz, GFP_KERNEL | __GFP_REPEAT);</span>
<span class="quote">&gt;+	rx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);</span>
<span class="quote">&gt; 	if (!rx)</span>
<span class="quote">&gt; 		return -ENOMEM;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;@@ -7161,7 +7161,7 @@ static int netif_alloc_netdev_queues(struct net_device *dev)</span>
<span class="quote">&gt; 	if (count &lt; 1 || count &gt; 0xffff)</span>
<span class="quote">&gt; 		return -EINVAL;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;-	tx = kvzalloc(sz, GFP_KERNEL | __GFP_REPEAT);</span>
<span class="quote">&gt;+	tx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);</span>
<span class="quote">&gt; 	if (!tx)</span>
<span class="quote">&gt; 		return -ENOMEM;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;@@ -7698,7 +7698,7 @@ struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,</span>
<span class="quote">&gt; 	/* ensure 32-byte alignment of whole construct */</span>
<span class="quote">&gt; 	alloc_size += NETDEV_ALIGN - 1;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;-	p = kvzalloc(alloc_size, GFP_KERNEL | __GFP_REPEAT);</span>
<span class="quote">&gt;+	p = kvzalloc(alloc_size, GFP_KERNEL | __GFP_RETRY_MAYFAIL);</span>
<span class="quote">&gt; 	if (!p)</span>
<span class="quote">&gt; 		return NULL;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;diff --git a/net/core/skbuff.c b/net/core/skbuff.c</span>
<span class="quote">&gt;index 9ccba86fa23d..26af038e27f0 100644</span>
<span class="quote">&gt;--- a/net/core/skbuff.c</span>
<span class="quote">&gt;+++ b/net/core/skbuff.c</span>
<span class="quote">&gt;@@ -4653,7 +4653,7 @@ struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	gfp_head = gfp_mask;</span>
<span class="quote">&gt; 	if (gfp_head &amp; __GFP_DIRECT_RECLAIM)</span>
<span class="quote">&gt;-		gfp_head |= __GFP_REPEAT;</span>
<span class="quote">&gt;+		gfp_head |= __GFP_RETRY_MAYFAIL;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	*errcode = -ENOBUFS;</span>
<span class="quote">&gt; 	skb = alloc_skb(header_len, gfp_head);</span>
<span class="quote">&gt;diff --git a/net/sched/sch_fq.c b/net/sched/sch_fq.c</span>
<span class="quote">&gt;index 594f77d89f6c..daebe062a2dd 100644</span>
<span class="quote">&gt;--- a/net/sched/sch_fq.c</span>
<span class="quote">&gt;+++ b/net/sched/sch_fq.c</span>
<span class="quote">&gt;@@ -640,7 +640,7 @@ static int fq_resize(struct Qdisc *sch, u32 log)</span>
<span class="quote">&gt; 		return 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	/* If XPS was setup, we can allocate memory on right NUMA node */</span>
<span class="quote">&gt;-	array = kvmalloc_node(sizeof(struct rb_root) &lt;&lt; log, GFP_KERNEL | __GFP_REPEAT,</span>
<span class="quote">&gt;+	array = kvmalloc_node(sizeof(struct rb_root) &lt;&lt; log, GFP_KERNEL | __GFP_RETRY_MAYFAIL,</span>
<span class="quote">&gt; 			      netdev_queue_numa_node_read(sch-&gt;dev_queue));</span>
<span class="quote">&gt; 	if (!array)</span>
<span class="quote">&gt; 		return -ENOMEM;</span>
<span class="quote">&gt;diff --git a/tools/perf/builtin-kmem.c b/tools/perf/builtin-kmem.c</span>
<span class="quote">&gt;index 6da8d083e4e5..01ca903fcdb9 100644</span>
<span class="quote">&gt;--- a/tools/perf/builtin-kmem.c</span>
<span class="quote">&gt;+++ b/tools/perf/builtin-kmem.c</span>
<span class="quote">&gt;@@ -638,7 +638,7 @@ static const struct {</span>
<span class="quote">&gt; 	{ &quot;__GFP_FS&quot;,			&quot;F&quot; },</span>
<span class="quote">&gt; 	{ &quot;__GFP_COLD&quot;,			&quot;CO&quot; },</span>
<span class="quote">&gt; 	{ &quot;__GFP_NOWARN&quot;,		&quot;NWR&quot; },</span>
<span class="quote">&gt;-	{ &quot;__GFP_REPEAT&quot;,		&quot;R&quot; },</span>
<span class="quote">&gt;+	{ &quot;__GFP_RETRY_MAYFAIL&quot;,	&quot;R&quot; },</span>
<span class="quote">&gt; 	{ &quot;__GFP_NOFAIL&quot;,		&quot;NF&quot; },</span>
<span class="quote">&gt; 	{ &quot;__GFP_NORETRY&quot;,		&quot;NR&quot; },</span>
<span class="quote">&gt; 	{ &quot;__GFP_COMP&quot;,			&quot;C&quot; },</span>
<span class="quote">&gt;-- </span>
<span class="quote">&gt;2.11.0</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 5, 2017, 6:43 a.m.</div>
<pre class="content">
On Sat 03-06-17 10:24:40, Wei Yang wrote:
<span class="quote">&gt; Hi, Michal</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just go through your patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have one question and one suggestion as below.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; One suggestion:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch does two things to me:</span>
<span class="quote">&gt; 1. Replace __GFP_REPEAT with __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt; 2. Adjust the logic in page_alloc to provide the middle semantic</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; My suggestion is to split these two task into two patches, so that readers</span>
<span class="quote">&gt; could catch your fundamental logic change easily.</span>

Well, the rename and the change is intentionally tight together. My
previous patches have removed all __GFP_REPEAT users for low order
requests which didn&#39;t have any implemented semantic. So as of now we
should only have those users which semantic will not change. I do not
add any new low order user in this patch so it in fact doesn&#39;t change
any existing semnatic.
<span class="quote">
&gt; </span>
<span class="quote">&gt; On Tue, Mar 07, 2017 at 04:48:41PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt;From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
[...]
<span class="quote">&gt; &gt;@@ -3776,9 +3784,9 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	/*</span>
<span class="quote">&gt; &gt; 	 * Do not retry costly high order allocations unless they are</span>
<span class="quote">&gt; &gt;-	 * __GFP_REPEAT</span>
<span class="quote">&gt; &gt;+	 * __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt; &gt; 	 */</span>
<span class="quote">&gt; &gt;-	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_REPEAT))</span>
<span class="quote">&gt; &gt;+	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_RETRY_MAYFAIL))</span>
<span class="quote">&gt; &gt; 		goto nopage;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; One question:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; From your change log, it mentions will provide the same semantic for !costly</span>
<span class="quote">&gt; allocations. While the logic here is the same as before.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For a !costly allocation with __GFP_REPEAT flag, the difference after this</span>
<span class="quote">&gt; patch is no OOM will be invoked, while it will still continue in the loop.</span>

Not really. There are two things. The above will shortcut retrying if
there is _no_ __GFP_RETRY_MAYFAIL. If the flags _is_ specified we will
back of in __alloc_pages_may_oom.
<span class="quote"> 
&gt; Maybe I don&#39;t catch your point in this message:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   __GFP_REPEAT was designed to allow retry-but-eventually-fail semantic to</span>
<span class="quote">&gt;   the page allocator. This has been true but only for allocations requests</span>
<span class="quote">&gt;   larger than PAGE_ALLOC_COSTLY_ORDER. It has been always ignored for</span>
<span class="quote">&gt;   smaller sizes. This is a bit unfortunate because there is no way to</span>
<span class="quote">&gt;   express the same semantic for those requests and they are considered too</span>
<span class="quote">&gt;   important to fail so they might end up looping in the page allocator for</span>
<span class="quote">&gt;   ever, similarly to GFP_NOFAIL requests.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I thought you will provide the same semantic to !costly allocation, or I</span>
<span class="quote">&gt; misunderstand?</span>

yes and that is the case. __alloc_pages_may_oom will back off before OOM
killer is invoked and the allocator slow path will fail because
did_some_progress == 0;
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=154191">Wei Yang</a> - June 6, 2017, 3:04 a.m.</div>
<pre class="content">
On Mon, Jun 05, 2017 at 08:43:43AM +0200, Michal Hocko wrote:
<span class="quote">&gt;On Sat 03-06-17 10:24:40, Wei Yang wrote:</span>
<span class="quote">&gt;&gt; Hi, Michal</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Just go through your patch.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I have one question and one suggestion as below.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; One suggestion:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; This patch does two things to me:</span>
<span class="quote">&gt;&gt; 1. Replace __GFP_REPEAT with __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt;&gt; 2. Adjust the logic in page_alloc to provide the middle semantic</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; My suggestion is to split these two task into two patches, so that readers</span>
<span class="quote">&gt;&gt; could catch your fundamental logic change easily.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Well, the rename and the change is intentionally tight together. My</span>
<span class="quote">&gt;previous patches have removed all __GFP_REPEAT users for low order</span>
<span class="quote">&gt;requests which didn&#39;t have any implemented semantic. So as of now we</span>
<span class="quote">&gt;should only have those users which semantic will not change. I do not</span>
<span class="quote">&gt;add any new low order user in this patch so it in fact doesn&#39;t change</span>
<span class="quote">&gt;any existing semnatic.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; On Tue, Mar 07, 2017 at 04:48:41PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt; &gt;From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt;[...]</span>
<span class="quote">&gt;&gt; &gt;@@ -3776,9 +3784,9 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; 	/*</span>
<span class="quote">&gt;&gt; &gt; 	 * Do not retry costly high order allocations unless they are</span>
<span class="quote">&gt;&gt; &gt;-	 * __GFP_REPEAT</span>
<span class="quote">&gt;&gt; &gt;+	 * __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt;&gt; &gt; 	 */</span>
<span class="quote">&gt;&gt; &gt;-	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_REPEAT))</span>
<span class="quote">&gt;&gt; &gt;+	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_RETRY_MAYFAIL))</span>
<span class="quote">&gt;&gt; &gt; 		goto nopage;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; One question:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; From your change log, it mentions will provide the same semantic for !costly</span>
<span class="quote">&gt;&gt; allocations. While the logic here is the same as before.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; For a !costly allocation with __GFP_REPEAT flag, the difference after this</span>
<span class="quote">&gt;&gt; patch is no OOM will be invoked, while it will still continue in the loop.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Not really. There are two things. The above will shortcut retrying if</span>
<span class="quote">&gt;there is _no_ __GFP_RETRY_MAYFAIL. If the flags _is_ specified we will</span>
<span class="quote">&gt;back of in __alloc_pages_may_oom.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Maybe I don&#39;t catch your point in this message:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt;   __GFP_REPEAT was designed to allow retry-but-eventually-fail semantic to</span>
<span class="quote">&gt;&gt;   the page allocator. This has been true but only for allocations requests</span>
<span class="quote">&gt;&gt;   larger than PAGE_ALLOC_COSTLY_ORDER. It has been always ignored for</span>
<span class="quote">&gt;&gt;   smaller sizes. This is a bit unfortunate because there is no way to</span>
<span class="quote">&gt;&gt;   express the same semantic for those requests and they are considered too</span>
<span class="quote">&gt;&gt;   important to fail so they might end up looping in the page allocator for</span>
<span class="quote">&gt;&gt;   ever, similarly to GFP_NOFAIL requests.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I thought you will provide the same semantic to !costly allocation, or I</span>
<span class="quote">&gt;&gt; misunderstand?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;yes and that is the case. __alloc_pages_may_oom will back off before OOM</span>
<span class="quote">&gt;killer is invoked and the allocator slow path will fail because</span>
<span class="quote">&gt;did_some_progress == 0;</span>

Thanks for your explanation.

So same &quot;semantic&quot; doesn&#39;t mean same &quot;behavior&quot;.
1. costly allocations will pick up the shut cut
2. !costly allocations will try something more but finally fail without
invoking OOM.

Hope this time I catch your point.

BTW, did_some_progress mostly means the OOM works to me. Are there some other
important situations when did_some_progress is set to 1?
<span class="quote">
&gt;-- </span>
<span class="quote">&gt;Michal Hocko</span>
<span class="quote">&gt;SUSE Labs</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 6, 2017, 12:03 p.m.</div>
<pre class="content">
On Tue 06-06-17 11:04:01, Wei Yang wrote:
<span class="quote">&gt; On Mon, Jun 05, 2017 at 08:43:43AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt;On Sat 03-06-17 10:24:40, Wei Yang wrote:</span>
<span class="quote">&gt; &gt;&gt; Hi, Michal</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; Just go through your patch.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; I have one question and one suggestion as below.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; One suggestion:</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; This patch does two things to me:</span>
<span class="quote">&gt; &gt;&gt; 1. Replace __GFP_REPEAT with __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt; &gt;&gt; 2. Adjust the logic in page_alloc to provide the middle semantic</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; My suggestion is to split these two task into two patches, so that readers</span>
<span class="quote">&gt; &gt;&gt; could catch your fundamental logic change easily.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Well, the rename and the change is intentionally tight together. My</span>
<span class="quote">&gt; &gt;previous patches have removed all __GFP_REPEAT users for low order</span>
<span class="quote">&gt; &gt;requests which didn&#39;t have any implemented semantic. So as of now we</span>
<span class="quote">&gt; &gt;should only have those users which semantic will not change. I do not</span>
<span class="quote">&gt; &gt;add any new low order user in this patch so it in fact doesn&#39;t change</span>
<span class="quote">&gt; &gt;any existing semnatic.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; On Tue, Mar 07, 2017 at 04:48:41PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt;From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt;[...]</span>
<span class="quote">&gt; &gt;&gt; &gt;@@ -3776,9 +3784,9 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; 	/*</span>
<span class="quote">&gt; &gt;&gt; &gt; 	 * Do not retry costly high order allocations unless they are</span>
<span class="quote">&gt; &gt;&gt; &gt;-	 * __GFP_REPEAT</span>
<span class="quote">&gt; &gt;&gt; &gt;+	 * __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt; &gt;&gt; &gt; 	 */</span>
<span class="quote">&gt; &gt;&gt; &gt;-	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_REPEAT))</span>
<span class="quote">&gt; &gt;&gt; &gt;+	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_RETRY_MAYFAIL))</span>
<span class="quote">&gt; &gt;&gt; &gt; 		goto nopage;</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; One question:</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; From your change log, it mentions will provide the same semantic for !costly</span>
<span class="quote">&gt; &gt;&gt; allocations. While the logic here is the same as before.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; For a !costly allocation with __GFP_REPEAT flag, the difference after this</span>
<span class="quote">&gt; &gt;&gt; patch is no OOM will be invoked, while it will still continue in the loop.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Not really. There are two things. The above will shortcut retrying if</span>
<span class="quote">&gt; &gt;there is _no_ __GFP_RETRY_MAYFAIL. If the flags _is_ specified we will</span>
<span class="quote">&gt; &gt;back of in __alloc_pages_may_oom.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; Maybe I don&#39;t catch your point in this message:</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt;   __GFP_REPEAT was designed to allow retry-but-eventually-fail semantic to</span>
<span class="quote">&gt; &gt;&gt;   the page allocator. This has been true but only for allocations requests</span>
<span class="quote">&gt; &gt;&gt;   larger than PAGE_ALLOC_COSTLY_ORDER. It has been always ignored for</span>
<span class="quote">&gt; &gt;&gt;   smaller sizes. This is a bit unfortunate because there is no way to</span>
<span class="quote">&gt; &gt;&gt;   express the same semantic for those requests and they are considered too</span>
<span class="quote">&gt; &gt;&gt;   important to fail so they might end up looping in the page allocator for</span>
<span class="quote">&gt; &gt;&gt;   ever, similarly to GFP_NOFAIL requests.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; I thought you will provide the same semantic to !costly allocation, or I</span>
<span class="quote">&gt; &gt;&gt; misunderstand?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;yes and that is the case. __alloc_pages_may_oom will back off before OOM</span>
<span class="quote">&gt; &gt;killer is invoked and the allocator slow path will fail because</span>
<span class="quote">&gt; &gt;did_some_progress == 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for your explanation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So same &quot;semantic&quot; doesn&#39;t mean same &quot;behavior&quot;.</span>
<span class="quote">&gt; 1. costly allocations will pick up the shut cut</span>

yes and there are no such allocations yet (based on my previous
cleanups)
<span class="quote">
&gt; 2. !costly allocations will try something more but finally fail without</span>
<span class="quote">&gt; invoking OOM.</span>

no, the behavior will not change for those.
<span class="quote"> 
&gt; Hope this time I catch your point.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; BTW, did_some_progress mostly means the OOM works to me. Are there some other</span>
<span class="quote">&gt; important situations when did_some_progress is set to 1?</span>

Yes e.g. for GFP_NOFS when we cannot really invoke the OOM killer yet we
cannot fail the allocation.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=154191">Wei Yang</a> - June 7, 2017, 2:10 a.m.</div>
<pre class="content">
On Tue, Jun 06, 2017 at 02:03:15PM +0200, Michal Hocko wrote:
<span class="quote">&gt;On Tue 06-06-17 11:04:01, Wei Yang wrote:</span>
<span class="quote">&gt;&gt; On Mon, Jun 05, 2017 at 08:43:43AM +0200, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt; &gt;On Sat 03-06-17 10:24:40, Wei Yang wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt; Hi, Michal</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; Just go through your patch.</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; I have one question and one suggestion as below.</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; One suggestion:</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; This patch does two things to me:</span>
<span class="quote">&gt;&gt; &gt;&gt; 1. Replace __GFP_REPEAT with __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt;&gt; &gt;&gt; 2. Adjust the logic in page_alloc to provide the middle semantic</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; My suggestion is to split these two task into two patches, so that readers</span>
<span class="quote">&gt;&gt; &gt;&gt; could catch your fundamental logic change easily.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;Well, the rename and the change is intentionally tight together. My</span>
<span class="quote">&gt;&gt; &gt;previous patches have removed all __GFP_REPEAT users for low order</span>
<span class="quote">&gt;&gt; &gt;requests which didn&#39;t have any implemented semantic. So as of now we</span>
<span class="quote">&gt;&gt; &gt;should only have those users which semantic will not change. I do not</span>
<span class="quote">&gt;&gt; &gt;add any new low order user in this patch so it in fact doesn&#39;t change</span>
<span class="quote">&gt;&gt; &gt;any existing semnatic.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; On Tue, Mar 07, 2017 at 04:48:41PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt;&gt; &gt;[...]</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;@@ -3776,9 +3784,9 @@ __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; 	/*</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; 	 * Do not retry costly high order allocations unless they are</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;-	 * __GFP_REPEAT</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;+	 * __GFP_RETRY_MAYFAIL</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; 	 */</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;-	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_REPEAT))</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;+	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_RETRY_MAYFAIL))</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; 		goto nopage;</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; One question:</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; From your change log, it mentions will provide the same semantic for !costly</span>
<span class="quote">&gt;&gt; &gt;&gt; allocations. While the logic here is the same as before.</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; For a !costly allocation with __GFP_REPEAT flag, the difference after this</span>
<span class="quote">&gt;&gt; &gt;&gt; patch is no OOM will be invoked, while it will still continue in the loop.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;Not really. There are two things. The above will shortcut retrying if</span>
<span class="quote">&gt;&gt; &gt;there is _no_ __GFP_RETRY_MAYFAIL. If the flags _is_ specified we will</span>
<span class="quote">&gt;&gt; &gt;back of in __alloc_pages_may_oom.</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; Maybe I don&#39;t catch your point in this message:</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt;   __GFP_REPEAT was designed to allow retry-but-eventually-fail semantic to</span>
<span class="quote">&gt;&gt; &gt;&gt;   the page allocator. This has been true but only for allocations requests</span>
<span class="quote">&gt;&gt; &gt;&gt;   larger than PAGE_ALLOC_COSTLY_ORDER. It has been always ignored for</span>
<span class="quote">&gt;&gt; &gt;&gt;   smaller sizes. This is a bit unfortunate because there is no way to</span>
<span class="quote">&gt;&gt; &gt;&gt;   express the same semantic for those requests and they are considered too</span>
<span class="quote">&gt;&gt; &gt;&gt;   important to fail so they might end up looping in the page allocator for</span>
<span class="quote">&gt;&gt; &gt;&gt;   ever, similarly to GFP_NOFAIL requests.</span>
<span class="quote">&gt;&gt; &gt;&gt; </span>
<span class="quote">&gt;&gt; &gt;&gt; I thought you will provide the same semantic to !costly allocation, or I</span>
<span class="quote">&gt;&gt; &gt;&gt; misunderstand?</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;yes and that is the case. __alloc_pages_may_oom will back off before OOM</span>
<span class="quote">&gt;&gt; &gt;killer is invoked and the allocator slow path will fail because</span>
<span class="quote">&gt;&gt; &gt;did_some_progress == 0;</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Thanks for your explanation.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; So same &quot;semantic&quot; doesn&#39;t mean same &quot;behavior&quot;.</span>
<span class="quote">&gt;&gt; 1. costly allocations will pick up the shut cut</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;yes and there are no such allocations yet (based on my previous</span>
<span class="quote">&gt;cleanups)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; 2. !costly allocations will try something more but finally fail without</span>
<span class="quote">&gt;&gt; invoking OOM.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;no, the behavior will not change for those.</span>
<span class="quote">&gt; </span>

Hmm... Let me be more specific. With two factors, costly or not, flag set or
not, we have four combinations. Here it is classified into two categories.

1. __GFP_RETRY_MAYFAIL not set

Brief description on behavior:
    costly: pick up the shortcut, so no OOM
    !costly: no shortcut and will OOM I think

Impact from this patch set:
    No.
    
My personal understanding:
    The allocation without __GFP_RETRY_MAYFAIL is not effected by this patch
    set.  Since !costly allocation will trigger OOM, this is the reason why
    &quot;small allocations never fail _practically_&quot;, as mentioned in
    https://lwn.net/Articles/723317/.


3. __GFP_RETRY_MAYFAIL set

Brief description on behavior:
    costly/!costly: no shortcut here and no OOM invoked

Impact from this patch set:
    For those allocations with __GFP_RETRY_MAYFAIL, OOM is not invoked for
    both.

My personal understanding:
    This is the semantic you are willing to introduce in this patch set. By
    cutting off the OOM invoke when __GFP_RETRY_MAYFAIL is set, you makes this
    a middle situation between NOFAIL and NORETRY.
    
    page_alloc will try some luck to get some free pages without disturb other
    part of the system. By doing so, the never fail allocation for !costly
    pages will be &quot;fixed&quot;. If I understand correctly, you are willing to make
    this the default behavior in the future?
<span class="quote">
&gt;&gt; Hope this time I catch your point.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; BTW, did_some_progress mostly means the OOM works to me. Are there some other</span>
<span class="quote">&gt;&gt; important situations when did_some_progress is set to 1?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Yes e.g. for GFP_NOFS when we cannot really invoke the OOM killer yet we</span>
<span class="quote">&gt;cannot fail the allocation.</span>


Thanks, currently I have a clear concept on this, while I will remember this :)
<span class="quote">
&gt;-- </span>
<span class="quote">&gt;Michal Hocko</span>
<span class="quote">&gt;SUSE Labs</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 9, 2017, 7:32 a.m.</div>
<pre class="content">
On Wed 07-06-17 10:10:36, Wei Yang wrote:
[...]
<span class="quote">&gt; Hmm... Let me be more specific. With two factors, costly or not, flag set or</span>
<span class="quote">&gt; not, we have four combinations. Here it is classified into two categories.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1. __GFP_RETRY_MAYFAIL not set</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Brief description on behavior:</span>
<span class="quote">&gt;     costly: pick up the shortcut, so no OOM</span>
<span class="quote">&gt;     !costly: no shortcut and will OOM I think</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Impact from this patch set:</span>
<span class="quote">&gt;     No.</span>

true
<span class="quote">
&gt; My personal understanding:</span>
<span class="quote">&gt;     The allocation without __GFP_RETRY_MAYFAIL is not effected by this patch</span>
<span class="quote">&gt;     set.  Since !costly allocation will trigger OOM, this is the reason why</span>
<span class="quote">&gt;     &quot;small allocations never fail _practically_&quot;, as mentioned in</span>
<span class="quote">&gt;     https://lwn.net/Articles/723317/.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 3. __GFP_RETRY_MAYFAIL set</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Brief description on behavior:</span>
<span class="quote">&gt;     costly/!costly: no shortcut here and no OOM invoked</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Impact from this patch set:</span>
<span class="quote">&gt;     For those allocations with __GFP_RETRY_MAYFAIL, OOM is not invoked for</span>
<span class="quote">&gt;     both.</span>

yes
<span class="quote">
&gt; My personal understanding:</span>
<span class="quote">&gt;     This is the semantic you are willing to introduce in this patch set. By</span>
<span class="quote">&gt;     cutting off the OOM invoke when __GFP_RETRY_MAYFAIL is set, you makes this</span>
<span class="quote">&gt;     a middle situation between NOFAIL and NORETRY.</span>

yes
<span class="quote">
&gt;     page_alloc will try some luck to get some free pages without disturb other</span>
<span class="quote">&gt;     part of the system. By doing so, the never fail allocation for !costly</span>
<span class="quote">&gt;     pages will be &quot;fixed&quot;. If I understand correctly, you are willing to make</span>
<span class="quote">&gt;     this the default behavior in the future?</span>

I do not think we can make this a default in a foreseeable future
unfortunately. That&#39;s why I&#39;ve made it a gfp modifier in the first
place. I assume many users will opt in by using the flag. In future we
can even help by adding a highlevel GFP_$FOO flag but I am worried that
this would just add to the explosion of existing highlevel gfp masks
(e.g. do we want GFP_NOFS_MAY_FAIL, GFP_USER_MAY_FAIL,
GFP_USER_HIGH_MOVABLE_MAYFAIL etc...)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/DMA-ISA-LPC.txt b/Documentation/DMA-ISA-LPC.txt</span>
<span class="p_header">index c41331398752..7a065ac4a9d1 100644</span>
<span class="p_header">--- a/Documentation/DMA-ISA-LPC.txt</span>
<span class="p_header">+++ b/Documentation/DMA-ISA-LPC.txt</span>
<span class="p_chunk">@@ -42,7 +42,7 @@</span> <span class="p_context"> requirements you pass the flag GFP_DMA to kmalloc.</span>
 
 Unfortunately the memory available for ISA DMA is scarce so unless you
 allocate the memory during boot-up it&#39;s a good idea to also pass
<span class="p_del">-__GFP_REPEAT and __GFP_NOWARN to make the allocator try a bit harder.</span>
<span class="p_add">+__GFP_RETRY_MAYFAIL and __GFP_NOWARN to make the allocator try a bit harder.</span>
 
 (This scarcity also means that you should allocate the buffer as
 early as possible and not release it until the driver is unloaded.)
<span class="p_header">diff --git a/arch/powerpc/include/asm/book3s/64/pgalloc.h b/arch/powerpc/include/asm/book3s/64/pgalloc.h</span>
<span class="p_header">index cd5e7aa8cc34..1b835aa5b4d1 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/book3s/64/pgalloc.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/book3s/64/pgalloc.h</span>
<span class="p_chunk">@@ -56,7 +56,7 @@</span> <span class="p_context"> static inline pgd_t *radix__pgd_alloc(struct mm_struct *mm)</span>
 	return (pgd_t *)__get_free_page(PGALLOC_GFP);
 #else
 	struct page *page;
<span class="p_del">-	page = alloc_pages(PGALLOC_GFP | __GFP_REPEAT, 4);</span>
<span class="p_add">+	page = alloc_pages(PGALLOC_GFP | __GFP_RETRY_MAYFAIL, 4);</span>
 	if (!page)
 		return NULL;
 	return (pgd_t *) page_address(page);
<span class="p_header">diff --git a/arch/powerpc/kvm/book3s_64_mmu_hv.c b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">index 8c68145ba1bd..8ad2c309f14a 100644</span>
<span class="p_header">--- a/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_header">+++ b/arch/powerpc/kvm/book3s_64_mmu_hv.c</span>
<span class="p_chunk">@@ -93,7 +93,7 @@</span> <span class="p_context"> int kvmppc_allocate_hpt(struct kvm_hpt_info *info, u32 order)</span>
 	}
 
 	if (!hpt)
<span class="p_del">-		hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_REPEAT</span>
<span class="p_add">+		hpt = __get_free_pages(GFP_KERNEL|__GFP_ZERO|__GFP_RETRY_MAYFAIL</span>
 				       |__GFP_NOWARN, order - PAGE_SHIFT);
 
 	if (!hpt)
<span class="p_header">diff --git a/drivers/mmc/host/wbsd.c b/drivers/mmc/host/wbsd.c</span>
<span class="p_header">index bd04e8bae010..b58fa5b5b972 100644</span>
<span class="p_header">--- a/drivers/mmc/host/wbsd.c</span>
<span class="p_header">+++ b/drivers/mmc/host/wbsd.c</span>
<span class="p_chunk">@@ -1386,7 +1386,7 @@</span> <span class="p_context"> static void wbsd_request_dma(struct wbsd_host *host, int dma)</span>
 	 * order for ISA to be able to DMA to it.
 	 */
 	host-&gt;dma_buffer = kmalloc(WBSD_DMA_SIZE,
<span class="p_del">-		GFP_NOIO | GFP_DMA | __GFP_REPEAT | __GFP_NOWARN);</span>
<span class="p_add">+		GFP_NOIO | GFP_DMA | __GFP_RETRY_MAYFAIL | __GFP_NOWARN);</span>
 	if (!host-&gt;dma_buffer)
 		goto free;
 
<span class="p_header">diff --git a/drivers/s390/char/vmcp.c b/drivers/s390/char/vmcp.c</span>
<span class="p_header">index 65f5a794f26d..98749fa817da 100644</span>
<span class="p_header">--- a/drivers/s390/char/vmcp.c</span>
<span class="p_header">+++ b/drivers/s390/char/vmcp.c</span>
<span class="p_chunk">@@ -98,7 +98,7 @@</span> <span class="p_context"> vmcp_write(struct file *file, const char __user *buff, size_t count,</span>
 	}
 	if (!session-&gt;response)
 		session-&gt;response = (char *)__get_free_pages(GFP_KERNEL
<span class="p_del">-						| __GFP_REPEAT | GFP_DMA,</span>
<span class="p_add">+						| __GFP_RETRY_MAYFAIL | GFP_DMA,</span>
 						get_order(session-&gt;bufsize));
 	if (!session-&gt;response) {
 		mutex_unlock(&amp;session-&gt;mutex);
<span class="p_header">diff --git a/drivers/target/target_core_transport.c b/drivers/target/target_core_transport.c</span>
<span class="p_header">index 434d9d693989..e585d301c665 100644</span>
<span class="p_header">--- a/drivers/target/target_core_transport.c</span>
<span class="p_header">+++ b/drivers/target/target_core_transport.c</span>
<span class="p_chunk">@@ -251,7 +251,7 @@</span> <span class="p_context"> int transport_alloc_session_tags(struct se_session *se_sess,</span>
 	int rc;
 
 	se_sess-&gt;sess_cmd_map = kzalloc(tag_num * tag_size,
<span class="p_del">-					GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+					GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_MAYFAIL);</span>
 	if (!se_sess-&gt;sess_cmd_map) {
 		se_sess-&gt;sess_cmd_map = vzalloc(tag_num * tag_size);
 		if (!se_sess-&gt;sess_cmd_map) {
<span class="p_header">diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c</span>
<span class="p_header">index f61f852d6cfd..7d2c4ce6d8d1 100644</span>
<span class="p_header">--- a/drivers/vhost/net.c</span>
<span class="p_header">+++ b/drivers/vhost/net.c</span>
<span class="p_chunk">@@ -817,7 +817,7 @@</span> <span class="p_context"> static int vhost_net_open(struct inode *inode, struct file *f)</span>
 	struct vhost_virtqueue **vqs;
 	int i;
 
<span class="p_del">-	n = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_REPEAT);</span>
<span class="p_add">+	n = kvmalloc(sizeof *n, GFP_KERNEL | __GFP_RETRY_MAYFAIL);</span>
 	if (!n)
 		return -ENOMEM;
 	vqs = kmalloc(VHOST_NET_VQ_MAX * sizeof(*vqs), GFP_KERNEL);
<span class="p_header">diff --git a/drivers/vhost/scsi.c b/drivers/vhost/scsi.c</span>
<span class="p_header">index fd6c8b66f06f..ff02a942c4d5 100644</span>
<span class="p_header">--- a/drivers/vhost/scsi.c</span>
<span class="p_header">+++ b/drivers/vhost/scsi.c</span>
<span class="p_chunk">@@ -1404,7 +1404,7 @@</span> <span class="p_context"> static int vhost_scsi_open(struct inode *inode, struct file *f)</span>
 	struct vhost_virtqueue **vqs;
 	int r = -ENOMEM, i;
 
<span class="p_del">-	vs = kzalloc(sizeof(*vs), GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	vs = kzalloc(sizeof(*vs), GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_MAYFAIL);</span>
 	if (!vs) {
 		vs = vzalloc(sizeof(*vs));
 		if (!vs)
<span class="p_header">diff --git a/drivers/vhost/vsock.c b/drivers/vhost/vsock.c</span>
<span class="p_header">index d403c647ba56..5b76242d73e3 100644</span>
<span class="p_header">--- a/drivers/vhost/vsock.c</span>
<span class="p_header">+++ b/drivers/vhost/vsock.c</span>
<span class="p_chunk">@@ -460,7 +460,7 @@</span> <span class="p_context"> static int vhost_vsock_dev_open(struct inode *inode, struct file *file)</span>
 	/* This struct is large and allocation could fail, fall back to vmalloc
 	 * if there is no other way.
 	 */
<span class="p_del">-	vsock = kvmalloc(sizeof(*vsock), GFP_KERNEL | __GFP_REPEAT);</span>
<span class="p_add">+	vsock = kvmalloc(sizeof(*vsock), GFP_KERNEL | __GFP_RETRY_MAYFAIL);</span>
 	if (!vsock)
 		return -ENOMEM;
 
<span class="p_header">diff --git a/fs/btrfs/check-integrity.c b/fs/btrfs/check-integrity.c</span>
<span class="p_header">index ab14c2e635ca..e334ed2b7e64 100644</span>
<span class="p_header">--- a/fs/btrfs/check-integrity.c</span>
<span class="p_header">+++ b/fs/btrfs/check-integrity.c</span>
<span class="p_chunk">@@ -2923,7 +2923,7 @@</span> <span class="p_context"> int btrfsic_mount(struct btrfs_fs_info *fs_info,</span>
 		       fs_info-&gt;sectorsize, PAGE_SIZE);
 		return -1;
 	}
<span class="p_del">-	state = kzalloc(sizeof(*state), GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	state = kzalloc(sizeof(*state), GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_MAYFAIL);</span>
 	if (!state) {
 		state = vzalloc(sizeof(*state));
 		if (!state) {
<span class="p_header">diff --git a/fs/btrfs/raid56.c b/fs/btrfs/raid56.c</span>
<span class="p_header">index 1571bf26dc07..94af3db1d0e4 100644</span>
<span class="p_header">--- a/fs/btrfs/raid56.c</span>
<span class="p_header">+++ b/fs/btrfs/raid56.c</span>
<span class="p_chunk">@@ -218,7 +218,7 @@</span> <span class="p_context"> int btrfs_alloc_stripe_hash_table(struct btrfs_fs_info *info)</span>
 	 * of a failing mount.
 	 */
 	table_size = sizeof(*table) + sizeof(*h) * num_entries;
<span class="p_del">-	table = kzalloc(table_size, GFP_KERNEL | __GFP_NOWARN | __GFP_REPEAT);</span>
<span class="p_add">+	table = kzalloc(table_size, GFP_KERNEL | __GFP_NOWARN | __GFP_RETRY_MAYFAIL);</span>
 	if (!table) {
 		table = vzalloc(table_size);
 		if (!table)
<span class="p_header">diff --git a/include/linux/gfp.h b/include/linux/gfp.h</span>
<span class="p_header">index 2bfcfd33e476..60af7937c6f2 100644</span>
<span class="p_header">--- a/include/linux/gfp.h</span>
<span class="p_header">+++ b/include/linux/gfp.h</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"> struct vm_area_struct;</span>
 #define ___GFP_FS		0x80u
 #define ___GFP_COLD		0x100u
 #define ___GFP_NOWARN		0x200u
<span class="p_del">-#define ___GFP_REPEAT		0x400u</span>
<span class="p_add">+#define ___GFP_RETRY_MAYFAIL		0x400u</span>
 #define ___GFP_NOFAIL		0x800u
 #define ___GFP_NORETRY		0x1000u
 #define ___GFP_MEMALLOC		0x2000u
<span class="p_chunk">@@ -136,26 +136,38 @@</span> <span class="p_context"> struct vm_area_struct;</span>
  *
  * __GFP_RECLAIM is shorthand to allow/forbid both direct and kswapd reclaim.
  *
<span class="p_del">- * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt</span>
<span class="p_del">- *   _might_ fail.  This depends upon the particular VM implementation.</span>
<span class="p_add">+ * The default allocator behavior depends on the request size. We have a concept</span>
<span class="p_add">+ * of so called costly allocations (with order &gt; PAGE_ALLOC_COSTLY_ORDER).</span>
<span class="p_add">+ * !costly allocations are too essential to fail so they are implicitly</span>
<span class="p_add">+ * non-failing (with some exceptions like OOM victims might fail) by default while</span>
<span class="p_add">+ * costly requests try to be not disruptive and back off even without invoking</span>
<span class="p_add">+ * the OOM killer. The following three modifiers might be used to override some of</span>
<span class="p_add">+ * these implicit rules</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * __GFP_NORETRY: The VM implementation must not retry indefinitely and will</span>
<span class="p_add">+ *   return NULL when direct reclaim and memory compaction have failed to allow</span>
<span class="p_add">+ *   the allocation to succeed.  The OOM killer is not called with the current</span>
<span class="p_add">+ *   implementation. This is a default mode for costly allocations.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * __GFP_RETRY_MAYFAIL: Try hard to allocate the memory, but the allocation attempt</span>
<span class="p_add">+ *   _might_ fail. All viable forms of memory reclaim are tried before the fail.</span>
<span class="p_add">+ *   The OOM killer is excluded because this would be too disruptive. This can be</span>
<span class="p_add">+ *   used to override non-failing default behavior for !costly requests as well as</span>
<span class="p_add">+ *   fortify costly requests.</span>
  *
  * __GFP_NOFAIL: The VM implementation _must_ retry infinitely: the caller
  *   cannot handle allocation failures. New users should be evaluated carefully
  *   (and the flag should be used only when there is no reasonable failure
  *   policy) but it is definitely preferable to use the flag rather than
<span class="p_del">- *   opencode endless loop around allocator.</span>
<span class="p_del">- *</span>
<span class="p_del">- * __GFP_NORETRY: The VM implementation must not retry indefinitely and will</span>
<span class="p_del">- *   return NULL when direct reclaim and memory compaction have failed to allow</span>
<span class="p_del">- *   the allocation to succeed.  The OOM killer is not called with the current</span>
<span class="p_del">- *   implementation.</span>
<span class="p_add">+ *   opencode endless loop around allocator. Using this flag for costly allocations</span>
<span class="p_add">+ *   is _highly_ discouraged.</span>
  */
 #define __GFP_IO	((__force gfp_t)___GFP_IO)
 #define __GFP_FS	((__force gfp_t)___GFP_FS)
 #define __GFP_DIRECT_RECLAIM	((__force gfp_t)___GFP_DIRECT_RECLAIM) /* Caller can reclaim */
 #define __GFP_KSWAPD_RECLAIM	((__force gfp_t)___GFP_KSWAPD_RECLAIM) /* kswapd can wake */
 #define __GFP_RECLAIM ((__force gfp_t)(___GFP_DIRECT_RECLAIM|___GFP_KSWAPD_RECLAIM))
<span class="p_del">-#define __GFP_REPEAT	((__force gfp_t)___GFP_REPEAT)</span>
<span class="p_add">+#define __GFP_RETRY_MAYFAIL	((__force gfp_t)___GFP_RETRY_MAYFAIL)</span>
 #define __GFP_NOFAIL	((__force gfp_t)___GFP_NOFAIL)
 #define __GFP_NORETRY	((__force gfp_t)___GFP_NORETRY)
 
<span class="p_header">diff --git a/include/linux/slab.h b/include/linux/slab.h</span>
<span class="p_header">index 3c37a8c51921..064901ac3e37 100644</span>
<span class="p_header">--- a/include/linux/slab.h</span>
<span class="p_header">+++ b/include/linux/slab.h</span>
<span class="p_chunk">@@ -469,7 +469,8 @@</span> <span class="p_context"> static __always_inline void *kmalloc_large(size_t size, gfp_t flags)</span>
  *
  * %__GFP_NOWARN - If allocation fails, don&#39;t issue any warnings.
  *
<span class="p_del">- * %__GFP_REPEAT - If allocation fails initially, try once more before failing.</span>
<span class="p_add">+ * %__GFP_RETRY_MAYFAIL - Try really hard to succeed the allocation but fail</span>
<span class="p_add">+ *   eventually.</span>
  *
  * There are other flags available as well, but these are not intended
  * for general use, and so are not documented here. For a full list of
<span class="p_header">diff --git a/include/trace/events/mmflags.h b/include/trace/events/mmflags.h</span>
<span class="p_header">index 304ff94363b2..418142a4efce 100644</span>
<span class="p_header">--- a/include/trace/events/mmflags.h</span>
<span class="p_header">+++ b/include/trace/events/mmflags.h</span>
<span class="p_chunk">@@ -34,7 +34,7 @@</span> <span class="p_context"></span>
 	{(unsigned long)__GFP_FS,		&quot;__GFP_FS&quot;},		\
 	{(unsigned long)__GFP_COLD,		&quot;__GFP_COLD&quot;},		\
 	{(unsigned long)__GFP_NOWARN,		&quot;__GFP_NOWARN&quot;},	\
<span class="p_del">-	{(unsigned long)__GFP_REPEAT,		&quot;__GFP_REPEAT&quot;},	\</span>
<span class="p_add">+	{(unsigned long)__GFP_RETRY_MAYFAIL,	&quot;__GFP_RETRY_MAYFAIL&quot;},	\</span>
 	{(unsigned long)__GFP_NOFAIL,		&quot;__GFP_NOFAIL&quot;},	\
 	{(unsigned long)__GFP_NORETRY,		&quot;__GFP_NORETRY&quot;},	\
 	{(unsigned long)__GFP_COMP,		&quot;__GFP_COMP&quot;},		\
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index a7aa811b7d14..dc598bfe4ce9 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1369,7 +1369,7 @@</span> <span class="p_context"> static struct page *alloc_fresh_huge_page_node(struct hstate *h, int nid)</span>
 
 	page = __alloc_pages_node(nid,
 		htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|
<span class="p_del">-						__GFP_REPEAT|__GFP_NOWARN,</span>
<span class="p_add">+						__GFP_RETRY_MAYFAIL|__GFP_NOWARN,</span>
 		huge_page_order(h));
 	if (page) {
 		prep_new_huge_page(h, page, nid);
<span class="p_chunk">@@ -1510,7 +1510,7 @@</span> <span class="p_context"> static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,</span>
 		struct vm_area_struct *vma, unsigned long addr, int nid)
 {
 	int order = huge_page_order(h);
<span class="p_del">-	gfp_t gfp = htlb_alloc_mask(h)|__GFP_COMP|__GFP_REPEAT|__GFP_NOWARN;</span>
<span class="p_add">+	gfp_t gfp = htlb_alloc_mask(h)|__GFP_COMP|__GFP_RETRY_MAYFAIL|__GFP_NOWARN;</span>
 	unsigned int cpuset_mems_cookie;
 
 	/*
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 823a7a89099b..8e6d347f70fb 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -23,7 +23,7 @@</span> <span class="p_context"></span>
  * hints such as HIGHMEM usage.
  */
 #define GFP_RECLAIM_MASK (__GFP_RECLAIM|__GFP_HIGH|__GFP_IO|__GFP_FS|\
<span class="p_del">-			__GFP_NOWARN|__GFP_REPEAT|__GFP_NOFAIL|\</span>
<span class="p_add">+			__GFP_NOWARN|__GFP_RETRY_MAYFAIL|__GFP_NOFAIL|\</span>
 			__GFP_NORETRY|__GFP_MEMALLOC|__GFP_NOMEMALLOC|\
 			__GFP_ATOMIC)
 
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 5238b87aec91..bfe4a9bad0f8 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -3181,6 +3181,14 @@</span> <span class="p_context"> __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,</span>
 	/* The OOM killer will not help higher order allocs */
 	if (order &gt; PAGE_ALLOC_COSTLY_ORDER)
 		goto out;
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We have already exhausted all our reclaim opportunities without any</span>
<span class="p_add">+	 * success so it is time to admit defeat. We will skip the OOM killer</span>
<span class="p_add">+	 * because it is very likely that the caller has a more reasonable</span>
<span class="p_add">+	 * fallback than shooting a random task.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (gfp_mask &amp; __GFP_RETRY_MAYFAIL)</span>
<span class="p_add">+		goto out;</span>
 	/* The OOM killer does not needlessly kill tasks for lowmem */
 	if (ac-&gt;high_zoneidx &lt; ZONE_NORMAL)
 		goto out;
<span class="p_chunk">@@ -3309,7 +3317,7 @@</span> <span class="p_context"> should_compact_retry(struct alloc_context *ac, int order, int alloc_flags,</span>
 	}
 
 	/*
<span class="p_del">-	 * !costly requests are much more important than __GFP_REPEAT</span>
<span class="p_add">+	 * !costly requests are much more important than __GFP_RETRY_MAYFAIL</span>
 	 * costly ones because they are de facto nofail and invoke OOM
 	 * killer to move on while costly can fail and users are ready
 	 * to cope with that. 1/4 retries is rather arbitrary but we
<span class="p_chunk">@@ -3776,9 +3784,9 @@</span> <span class="p_context"> __alloc_pages_slowpath(gfp_t gfp_mask, unsigned int order,</span>
 
 	/*
 	 * Do not retry costly high order allocations unless they are
<span class="p_del">-	 * __GFP_REPEAT</span>
<span class="p_add">+	 * __GFP_RETRY_MAYFAIL</span>
 	 */
<span class="p_del">-	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_REPEAT))</span>
<span class="p_add">+	if (order &gt; PAGE_ALLOC_COSTLY_ORDER &amp;&amp; !(gfp_mask &amp; __GFP_RETRY_MAYFAIL))</span>
 		goto nopage;
 
 	if (should_reclaim_retry(gfp_mask, order, ac, alloc_flags,
<span class="p_header">diff --git a/mm/sparse-vmemmap.c b/mm/sparse-vmemmap.c</span>
<span class="p_header">index 574c67b663fe..b21ba0dfe102 100644</span>
<span class="p_header">--- a/mm/sparse-vmemmap.c</span>
<span class="p_header">+++ b/mm/sparse-vmemmap.c</span>
<span class="p_chunk">@@ -56,11 +56,11 @@</span> <span class="p_context"> void * __meminit vmemmap_alloc_block(unsigned long size, int node)</span>
 
 		if (node_state(node, N_HIGH_MEMORY))
 			page = alloc_pages_node(
<span class="p_del">-				node, GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,</span>
<span class="p_add">+				node, GFP_KERNEL | __GFP_ZERO | __GFP_RETRY_MAYFAIL,</span>
 				get_order(size));
 		else
 			page = alloc_pages(
<span class="p_del">-				GFP_KERNEL | __GFP_ZERO | __GFP_REPEAT,</span>
<span class="p_add">+				GFP_KERNEL | __GFP_ZERO | __GFP_RETRY_MAYFAIL,</span>
 				get_order(size));
 		if (page)
 			return page_address(page);
<span class="p_header">diff --git a/mm/util.c b/mm/util.c</span>
<span class="p_header">index 6ed3e49bf1e5..885a78d1941b 100644</span>
<span class="p_header">--- a/mm/util.c</span>
<span class="p_header">+++ b/mm/util.c</span>
<span class="p_chunk">@@ -339,7 +339,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(vm_mmap);</span>
  * Uses kmalloc to get the memory but if the allocation fails then falls back
  * to the vmalloc allocator. Use kvfree for freeing the memory.
  *
<span class="p_del">- * Reclaim modifiers - __GFP_NORETRY and __GFP_NOFAIL are not supported. __GFP_REPEAT</span>
<span class="p_add">+ * Reclaim modifiers - __GFP_NORETRY and __GFP_NOFAIL are not supported. __GFP_RETRY_MAYFAIL</span>
  * is supported only for large (&gt;32kB) allocations, and it should be used only if
  * kmalloc is preferable to the vmalloc fallback, due to visible performance drawbacks.
  *
<span class="p_chunk">@@ -364,11 +364,11 @@</span> <span class="p_context"> void *kvmalloc_node(size_t size, gfp_t flags, int node)</span>
 		kmalloc_flags |= __GFP_NOWARN;
 
 		/*
<span class="p_del">-		 * We have to override __GFP_REPEAT by __GFP_NORETRY for !costly</span>
<span class="p_add">+		 * We have to override __GFP_RETRY_MAYFAIL by __GFP_NORETRY for !costly</span>
 		 * requests because there is no other way to tell the allocator
 		 * that we want to fail rather than retry endlessly.
 		 */
<span class="p_del">-		if (!(kmalloc_flags &amp; __GFP_REPEAT) ||</span>
<span class="p_add">+		if (!(kmalloc_flags &amp; __GFP_RETRY_MAYFAIL) ||</span>
 				(size &lt;= PAGE_SIZE &lt;&lt; PAGE_ALLOC_COSTLY_ORDER))
 			kmalloc_flags |= __GFP_NORETRY;
 	}
<span class="p_header">diff --git a/mm/vmalloc.c b/mm/vmalloc.c</span>
<span class="p_header">index 32979d945766..c2fa2e1b79fc 100644</span>
<span class="p_header">--- a/mm/vmalloc.c</span>
<span class="p_header">+++ b/mm/vmalloc.c</span>
<span class="p_chunk">@@ -1747,7 +1747,7 @@</span> <span class="p_context"> void *__vmalloc_node_range(unsigned long size, unsigned long align,</span>
  *	allocator with @gfp_mask flags.  Map them into contiguous
  *	kernel virtual space, using a pagetable protection of @prot.
  *
<span class="p_del">- *	Reclaim modifiers in @gfp_mask - __GFP_NORETRY, __GFP_REPEAT</span>
<span class="p_add">+ *	Reclaim modifiers in @gfp_mask - __GFP_NORETRY, __GFP_RETRY_MAYFAIL</span>
  *	and __GFP_NOFAIL are not supported
  *
  *	Any use of gfp flags outside of GFP_KERNEL should be consulted
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 4e0a828781e5..8f547176e02c 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -2435,18 +2435,18 @@</span> <span class="p_context"> static inline bool should_continue_reclaim(struct pglist_data *pgdat,</span>
 		return false;
 
 	/* Consider stopping depending on scan and reclaim activity */
<span class="p_del">-	if (sc-&gt;gfp_mask &amp; __GFP_REPEAT) {</span>
<span class="p_add">+	if (sc-&gt;gfp_mask &amp; __GFP_RETRY_MAYFAIL) {</span>
 		/*
<span class="p_del">-		 * For __GFP_REPEAT allocations, stop reclaiming if the</span>
<span class="p_add">+		 * For __GFP_RETRY_MAYFAIL allocations, stop reclaiming if the</span>
 		 * full LRU list has been scanned and we are still failing
 		 * to reclaim pages. This full LRU scan is potentially
<span class="p_del">-		 * expensive but a __GFP_REPEAT caller really wants to succeed</span>
<span class="p_add">+		 * expensive but a __GFP_RETRY_MAYFAIL caller really wants to succeed</span>
 		 */
 		if (!nr_reclaimed &amp;&amp; !nr_scanned)
 			return false;
 	} else {
 		/*
<span class="p_del">-		 * For non-__GFP_REPEAT allocations which can presumably</span>
<span class="p_add">+		 * For non-__GFP_RETRY_MAYFAIL allocations which can presumably</span>
 		 * fail without consequence, stop if we failed to reclaim
 		 * any pages from the last SWAP_CLUSTER_MAX number of
 		 * pages that were scanned. This will return to the
<span class="p_header">diff --git a/net/core/dev.c b/net/core/dev.c</span>
<span class="p_header">index d947308ee255..3e659ac9e0ed 100644</span>
<span class="p_header">--- a/net/core/dev.c</span>
<span class="p_header">+++ b/net/core/dev.c</span>
<span class="p_chunk">@@ -7121,7 +7121,7 @@</span> <span class="p_context"> static int netif_alloc_rx_queues(struct net_device *dev)</span>
 
 	BUG_ON(count &lt; 1);
 
<span class="p_del">-	rx = kvzalloc(sz, GFP_KERNEL | __GFP_REPEAT);</span>
<span class="p_add">+	rx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);</span>
 	if (!rx)
 		return -ENOMEM;
 
<span class="p_chunk">@@ -7161,7 +7161,7 @@</span> <span class="p_context"> static int netif_alloc_netdev_queues(struct net_device *dev)</span>
 	if (count &lt; 1 || count &gt; 0xffff)
 		return -EINVAL;
 
<span class="p_del">-	tx = kvzalloc(sz, GFP_KERNEL | __GFP_REPEAT);</span>
<span class="p_add">+	tx = kvzalloc(sz, GFP_KERNEL | __GFP_RETRY_MAYFAIL);</span>
 	if (!tx)
 		return -ENOMEM;
 
<span class="p_chunk">@@ -7698,7 +7698,7 @@</span> <span class="p_context"> struct net_device *alloc_netdev_mqs(int sizeof_priv, const char *name,</span>
 	/* ensure 32-byte alignment of whole construct */
 	alloc_size += NETDEV_ALIGN - 1;
 
<span class="p_del">-	p = kvzalloc(alloc_size, GFP_KERNEL | __GFP_REPEAT);</span>
<span class="p_add">+	p = kvzalloc(alloc_size, GFP_KERNEL | __GFP_RETRY_MAYFAIL);</span>
 	if (!p)
 		return NULL;
 
<span class="p_header">diff --git a/net/core/skbuff.c b/net/core/skbuff.c</span>
<span class="p_header">index 9ccba86fa23d..26af038e27f0 100644</span>
<span class="p_header">--- a/net/core/skbuff.c</span>
<span class="p_header">+++ b/net/core/skbuff.c</span>
<span class="p_chunk">@@ -4653,7 +4653,7 @@</span> <span class="p_context"> struct sk_buff *alloc_skb_with_frags(unsigned long header_len,</span>
 
 	gfp_head = gfp_mask;
 	if (gfp_head &amp; __GFP_DIRECT_RECLAIM)
<span class="p_del">-		gfp_head |= __GFP_REPEAT;</span>
<span class="p_add">+		gfp_head |= __GFP_RETRY_MAYFAIL;</span>
 
 	*errcode = -ENOBUFS;
 	skb = alloc_skb(header_len, gfp_head);
<span class="p_header">diff --git a/net/sched/sch_fq.c b/net/sched/sch_fq.c</span>
<span class="p_header">index 594f77d89f6c..daebe062a2dd 100644</span>
<span class="p_header">--- a/net/sched/sch_fq.c</span>
<span class="p_header">+++ b/net/sched/sch_fq.c</span>
<span class="p_chunk">@@ -640,7 +640,7 @@</span> <span class="p_context"> static int fq_resize(struct Qdisc *sch, u32 log)</span>
 		return 0;
 
 	/* If XPS was setup, we can allocate memory on right NUMA node */
<span class="p_del">-	array = kvmalloc_node(sizeof(struct rb_root) &lt;&lt; log, GFP_KERNEL | __GFP_REPEAT,</span>
<span class="p_add">+	array = kvmalloc_node(sizeof(struct rb_root) &lt;&lt; log, GFP_KERNEL | __GFP_RETRY_MAYFAIL,</span>
 			      netdev_queue_numa_node_read(sch-&gt;dev_queue));
 	if (!array)
 		return -ENOMEM;
<span class="p_header">diff --git a/tools/perf/builtin-kmem.c b/tools/perf/builtin-kmem.c</span>
<span class="p_header">index 6da8d083e4e5..01ca903fcdb9 100644</span>
<span class="p_header">--- a/tools/perf/builtin-kmem.c</span>
<span class="p_header">+++ b/tools/perf/builtin-kmem.c</span>
<span class="p_chunk">@@ -638,7 +638,7 @@</span> <span class="p_context"> static const struct {</span>
 	{ &quot;__GFP_FS&quot;,			&quot;F&quot; },
 	{ &quot;__GFP_COLD&quot;,			&quot;CO&quot; },
 	{ &quot;__GFP_NOWARN&quot;,		&quot;NWR&quot; },
<span class="p_del">-	{ &quot;__GFP_REPEAT&quot;,		&quot;R&quot; },</span>
<span class="p_add">+	{ &quot;__GFP_RETRY_MAYFAIL&quot;,	&quot;R&quot; },</span>
 	{ &quot;__GFP_NOFAIL&quot;,		&quot;NF&quot; },
 	{ &quot;__GFP_NORETRY&quot;,		&quot;NR&quot; },
 	{ &quot;__GFP_COMP&quot;,			&quot;C&quot; },

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



