
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,v2,14/32] x86: mm: Provide support to use memblock when spliting large pages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,v2,14/32] x86: mm: Provide support to use memblock when spliting large pages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2536">Paolo Bonzini</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 16, 2017, 1:15 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;c3b12452-9533-87c8-4543-8f12f5c90fe3@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9628113/mbox/"
   >mbox</a>
|
   <a href="/patch/9628113/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9628113/">/patch/9628113/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C263C60244 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 13:16:25 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B632F28644
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 13:16:25 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id AA33C28654; Thu, 16 Mar 2017 13:16:25 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E238828644
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 13:16:24 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751997AbdCPNQG (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 16 Mar 2017 09:16:06 -0400
Received: from mx1.redhat.com ([209.132.183.28]:52268 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751357AbdCPNQD (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 16 Mar 2017 09:16:03 -0400
Received: from int-mx11.intmail.prod.int.phx2.redhat.com
	(int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 807987E9D3;
	Thu, 16 Mar 2017 13:15:35 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com 807987E9D3
Authentication-Results: ext-mx02.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx02.extmail.prod.ext.phx2.redhat.com;
	spf=pass smtp.mailfrom=pbonzini@redhat.com
DKIM-Filter: OpenDKIM Filter v2.11.0 mx1.redhat.com 807987E9D3
Received: from [10.36.117.7] (ovpn-117-7.ams2.redhat.com [10.36.117.7])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id v2GDFJ58023994
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NO);
	Thu, 16 Mar 2017 09:15:21 -0400
Subject: Re: [RFC PATCH v2 14/32] x86: mm: Provide support to use memblock
	when spliting large pages
To: Brijesh Singh &lt;brijesh.singh@amd.com&gt;, Borislav Petkov &lt;bp@suse.de&gt;
References: &lt;148846752022.2349.13667498174822419498.stgit@brijesh-build-machine&gt;
	&lt;148846771545.2349.9373586041426414252.stgit@brijesh-build-machine&gt;
	&lt;20170310110657.hophlog2juw5hpzz@pd.tnic&gt;
	&lt;cb6a9a56-2c52-d98d-3ff6-3b61d0e5875e@amd.com&gt;
Cc: simon.guinot@sequanux.org, linux-efi@vger.kernel.org,
	kvm@vger.kernel.org, rkrcmar@redhat.com, matt@codeblueprint.co.uk,
	linux-pci@vger.kernel.org, linus.walleij@linaro.org,
	gary.hook@amd.com, linux-mm@kvack.org,
	paul.gortmaker@windriver.com, hpa@zytor.com, cl@linux.com,
	dan.j.williams@intel.com, aarcange@redhat.com,
	sfr@canb.auug.org.au, andriy.shevchenko@linux.intel.com,
	herbert@gondor.apana.org.au, bhe@redhat.com, xemul@parallels.com,
	joro@8bytes.org, x86@kernel.org, peterz@infradead.org,
	piotr.luc@intel.com, mingo@redhat.com, msalter@redhat.com,
	ross.zwisler@linux.intel.com, dyoung@redhat.com,
	thomas.lendacky@amd.com, jroedel@suse.de, keescook@chromium.org,
	arnd@arndb.de, toshi.kani@hpe.com, mathieu.desnoyers@efficios.com,
	luto@kernel.org, devel@linuxdriverproject.org, bhelgaas@google.com,
	tglx@linutronix.de, mchehab@kernel.org, iamjoonsoo.kim@lge.com,
	labbott@fedoraproject.org, tony.luck@intel.com,
	alexandre.bounine@idt.com, kuleshovmail@gmail.com,
	linux-kernel@vger.kernel.org, mcgrof@kernel.org, mst@redhat.com,
	linux-crypto@vger.kernel.org, tj@kernel.org,
	akpm@linux-foundation.org, davem@davemloft.net
From: Paolo Bonzini &lt;pbonzini@redhat.com&gt;
Message-ID: &lt;c3b12452-9533-87c8-4543-8f12f5c90fe3@redhat.com&gt;
Date: Thu, 16 Mar 2017 14:15:19 +0100
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:45.0) Gecko/20100101
	Thunderbird/45.7.0
MIME-Version: 1.0
In-Reply-To: &lt;cb6a9a56-2c52-d98d-3ff6-3b61d0e5875e@amd.com&gt;
Content-Type: multipart/mixed;
	boundary=&quot;------------7AC6D09ABF2C56474372EF15&quot;
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.26]);
	Thu, 16 Mar 2017 13:15:38 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2536">Paolo Bonzini</a> - March 16, 2017, 1:15 p.m.</div>
<pre class="content">
On 10/03/2017 23:41, Brijesh Singh wrote:
<span class="quote">&gt;&gt; Maybe there&#39;s a reason this fires:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; WARNING: modpost: Found 2 section mismatch(es).</span>
<span class="quote">&gt;&gt; To see full details build your kernel with:</span>
<span class="quote">&gt;&gt; &#39;make CONFIG_DEBUG_SECTION_MISMATCH=y&#39;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; WARNING: vmlinux.o(.text+0x48edc): Section mismatch in reference from</span>
<span class="quote">&gt;&gt; the function __change_page_attr() to the function</span>
<span class="quote">&gt;&gt; .init.text:memblock_alloc()</span>
<span class="quote">&gt;&gt; The function __change_page_attr() references</span>
<span class="quote">&gt;&gt; the function __init memblock_alloc().</span>
<span class="quote">&gt;&gt; This is often because __change_page_attr lacks a __init</span>
<span class="quote">&gt;&gt; annotation or the annotation of memblock_alloc is wrong.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; WARNING: vmlinux.o(.text+0x491d1): Section mismatch in reference from</span>
<span class="quote">&gt;&gt; the function __change_page_attr() to the function</span>
<span class="quote">&gt;&gt; .meminit.text:memblock_free()</span>
<span class="quote">&gt;&gt; The function __change_page_attr() references</span>
<span class="quote">&gt;&gt; the function __meminit memblock_free().</span>
<span class="quote">&gt;&gt; This is often because __change_page_attr lacks a __meminit</span>
<span class="quote">&gt;&gt; annotation or the annotation of memblock_free is wrong.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; But maybe Paolo might have an even better idea...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am sure he will have better idea :)</span>

Not sure if it&#39;s better or worse, but an alternative idea is to turn
__change_page_attr and __change_page_attr_set_clr inside out, so that:
1) the alloc_pages/__free_page happens in __change_page_attr_set_clr;
2) __change_page_attr_set_clr overall does not beocome more complex.

Then you can introduce __early_change_page_attr_set_clr and/or
early_kernel_map_pages_in_pgd, for use in your next patches.  They use
the memblock allocator instead of alloc/free_page

The attached patch is compile-tested only and almost certainly has some
thinko in it.  But it even skims a few lines from the code so the idea
might have some merit.

Paolo
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index 28d42130243c..953c8e697562 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -490,11 +490,12 @@</span> <span class="p_context"> static void __set_pmd_pte(pte_t *kpte, unsigned long address, pte_t pte)</span>
 }
 
 static int
<span class="p_del">-try_preserve_large_page(pte_t *kpte, unsigned long address,</span>
<span class="p_add">+try_preserve_large_page(pte_t **p_kpte, unsigned long address,</span>
 			struct cpa_data *cpa)
 {
 	unsigned long nextpage_addr, numpages, pmask, psize, addr, pfn, old_pfn;
<span class="p_del">-	pte_t new_pte, old_pte, *tmp;</span>
<span class="p_add">+	pte_t *kpte = *p_kpte;</span>
<span class="p_add">+	pte_t new_pte, old_pte;</span>
 	pgprot_t old_prot, new_prot, req_prot;
 	int i, do_split = 1;
 	enum pg_level level;
<span class="p_chunk">@@ -507,8 +508,8 @@</span> <span class="p_context"> try_preserve_large_page(pte_t *kpte, unsigned long address,</span>
 	 * Check for races, another CPU might have split this page
 	 * up already:
 	 */
<span class="p_del">-	tmp = _lookup_address_cpa(cpa, address, &amp;level);</span>
<span class="p_del">-	if (tmp != kpte)</span>
<span class="p_add">+	*p_kpte = _lookup_address_cpa(cpa, address, &amp;level);</span>
<span class="p_add">+	if (*p_kpte != kpte)</span>
 		goto out_unlock;
 
 	switch (level) {
<span class="p_chunk">@@ -634,17 +635,18 @@</span> <span class="p_context"> __split_large_page(struct cpa_data *cpa, pte_t *kpte, unsigned long address,</span>
 	unsigned int i, level;
 	pte_t *tmp;
 	pgprot_t ref_prot;
<span class="p_add">+	int retry = 1;</span>
 
<span class="p_add">+	if (!debug_pagealloc_enabled())</span>
<span class="p_add">+		spin_lock(&amp;cpa_lock);</span>
 	spin_lock(&amp;pgd_lock);
 	/*
 	 * Check for races, another CPU might have split this page
 	 * up for us already:
 	 */
 	tmp = _lookup_address_cpa(cpa, address, &amp;level);
<span class="p_del">-	if (tmp != kpte) {</span>
<span class="p_del">-		spin_unlock(&amp;pgd_lock);</span>
<span class="p_del">-		return 1;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (tmp != kpte)</span>
<span class="p_add">+		goto out;</span>
 
 	paravirt_alloc_pte(&amp;init_mm, page_to_pfn(base));
 
<span class="p_chunk">@@ -671,10 +673,11 @@</span> <span class="p_context"> __split_large_page(struct cpa_data *cpa, pte_t *kpte, unsigned long address,</span>
 		break;
 
 	default:
<span class="p_del">-		spin_unlock(&amp;pgd_lock);</span>
<span class="p_del">-		return 1;</span>
<span class="p_add">+		goto out;</span>
 	}
 
<span class="p_add">+	retry = 0;</span>
<span class="p_add">+</span>
 	/*
 	 * Set the GLOBAL flags only if the PRESENT flag is set
 	 * otherwise pmd/pte_present will return true even on a non
<span class="p_chunk">@@ -718,28 +721,34 @@</span> <span class="p_context"> __split_large_page(struct cpa_data *cpa, pte_t *kpte, unsigned long address,</span>
 	 * going on.
 	 */
 	__flush_tlb_all();
<span class="p_del">-	spin_unlock(&amp;pgd_lock);</span>
 
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int split_large_page(struct cpa_data *cpa, pte_t *kpte,</span>
<span class="p_del">-			    unsigned long address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct page *base;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	spin_unlock(&amp;pgd_lock);</span>
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Do a global flush tlb after splitting the large page</span>
<span class="p_add">+ 	 * and before we do the actual change page attribute in the PTE.</span>
<span class="p_add">+ 	 *</span>
<span class="p_add">+ 	 * With out this, we violate the TLB application note, that says</span>
<span class="p_add">+ 	 * &quot;The TLBs may contain both ordinary and large-page</span>
<span class="p_add">+	 *  translations for a 4-KByte range of linear addresses. This</span>
<span class="p_add">+	 *  may occur if software modifies the paging structures so that</span>
<span class="p_add">+	 *  the page size used for the address range changes. If the two</span>
<span class="p_add">+	 *  translations differ with respect to page frame or attributes</span>
<span class="p_add">+	 *  (e.g., permissions), processor behavior is undefined and may</span>
<span class="p_add">+	 *  be implementation-specific.&quot;</span>
<span class="p_add">+ 	 *</span>
<span class="p_add">+ 	 * We do this global tlb flush inside the cpa_lock, so that we</span>
<span class="p_add">+	 * don&#39;t allow any other cpu, with stale tlb entries change the</span>
<span class="p_add">+	 * page attribute in parallel, that also falls into the</span>
<span class="p_add">+	 * just split large page entry.</span>
<span class="p_add">+ 	 */</span>
<span class="p_add">+	if (!retry)</span>
<span class="p_add">+		flush_tlb_all();</span>
 	if (!debug_pagealloc_enabled())
 		spin_unlock(&amp;cpa_lock);
<span class="p_del">-	base = alloc_pages(GFP_KERNEL | __GFP_NOTRACK, 0);</span>
<span class="p_del">-	if (!debug_pagealloc_enabled())</span>
<span class="p_del">-		spin_lock(&amp;cpa_lock);</span>
<span class="p_del">-	if (!base)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (__split_large_page(cpa, kpte, address, base))</span>
<span class="p_del">-		__free_page(base);</span>
 
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return retry;</span>
 }
 
 static bool try_to_free_pte_page(pte_t *pte)
<span class="p_chunk">@@ -1166,30 +1175,26 @@</span> <span class="p_context"> static int __cpa_process_fault(struct cpa_data *cpa, unsigned long vaddr,</span>
 	}
 }
 
<span class="p_del">-static int __change_page_attr(struct cpa_data *cpa, int primary)</span>
<span class="p_add">+static int __change_page_attr(struct cpa_data *cpa, pte_t **p_kpte, unsigned long address,</span>
<span class="p_add">+			      int primary)</span>
 {
<span class="p_del">-	unsigned long address;</span>
<span class="p_del">-	int do_split, err;</span>
 	unsigned int level;
 	pte_t *kpte, old_pte;
<span class="p_add">+	int err = 0;</span>
 
<span class="p_del">-	if (cpa-&gt;flags &amp; CPA_PAGES_ARRAY) {</span>
<span class="p_del">-		struct page *page = cpa-&gt;pages[cpa-&gt;curpage];</span>
<span class="p_del">-		if (unlikely(PageHighMem(page)))</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-		address = (unsigned long)page_address(page);</span>
<span class="p_del">-	} else if (cpa-&gt;flags &amp; CPA_ARRAY)</span>
<span class="p_del">-		address = cpa-&gt;vaddr[cpa-&gt;curpage];</span>
<span class="p_del">-	else</span>
<span class="p_del">-		address = *cpa-&gt;vaddr;</span>
<span class="p_del">-repeat:</span>
<span class="p_del">-	kpte = _lookup_address_cpa(cpa, address, &amp;level);</span>
<span class="p_del">-	if (!kpte)</span>
<span class="p_del">-		return __cpa_process_fault(cpa, address, primary);</span>
<span class="p_add">+	if (!debug_pagealloc_enabled())</span>
<span class="p_add">+		spin_lock(&amp;cpa_lock);</span>
<span class="p_add">+	*p_kpte = kpte = _lookup_address_cpa(cpa, address, &amp;level);</span>
<span class="p_add">+	if (!kpte) {</span>
<span class="p_add">+		err = __cpa_process_fault(cpa, address, primary);</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
 
 	old_pte = *kpte;
<span class="p_del">-	if (pte_none(old_pte))</span>
<span class="p_del">-		return __cpa_process_fault(cpa, address, primary);</span>
<span class="p_add">+	if (pte_none(old_pte)) {</span>
<span class="p_add">+		err = __cpa_process_fault(cpa, address, primary);</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
 
 	if (level == PG_LEVEL_4K) {
 		pte_t new_pte;
<span class="p_chunk">@@ -1228,59 +1233,27 @@</span> <span class="p_context"> static int __change_page_attr(struct cpa_data *cpa, int primary)</span>
 			cpa-&gt;flags |= CPA_FLUSHTLB;
 		}
 		cpa-&gt;numpages = 1;
<span class="p_del">-		return 0;</span>
<span class="p_add">+		goto out;</span>
 	}
 
 	/*
 	 * Check, whether we can keep the large page intact
 	 * and just change the pte:
 	 */
<span class="p_del">-	do_split = try_preserve_large_page(kpte, address, cpa);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * When the range fits into the existing large page,</span>
<span class="p_del">-	 * return. cp-&gt;numpages and cpa-&gt;tlbflush have been updated in</span>
<span class="p_del">-	 * try_large_page:</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (do_split &lt;= 0)</span>
<span class="p_del">-		return do_split;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We have to split the large page:</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	err = split_large_page(cpa, kpte, address);</span>
<span class="p_del">-	if (!err) {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-	 	 * Do a global flush tlb after splitting the large page</span>
<span class="p_del">-	 	 * and before we do the actual change page attribute in the PTE.</span>
<span class="p_del">-	 	 *</span>
<span class="p_del">-	 	 * With out this, we violate the TLB application note, that says</span>
<span class="p_del">-	 	 * &quot;The TLBs may contain both ordinary and large-page</span>
<span class="p_del">-		 *  translations for a 4-KByte range of linear addresses. This</span>
<span class="p_del">-		 *  may occur if software modifies the paging structures so that</span>
<span class="p_del">-		 *  the page size used for the address range changes. If the two</span>
<span class="p_del">-		 *  translations differ with respect to page frame or attributes</span>
<span class="p_del">-		 *  (e.g., permissions), processor behavior is undefined and may</span>
<span class="p_del">-		 *  be implementation-specific.&quot;</span>
<span class="p_del">-	 	 *</span>
<span class="p_del">-	 	 * We do this global tlb flush inside the cpa_lock, so that we</span>
<span class="p_del">-		 * don&#39;t allow any other cpu, with stale tlb entries change the</span>
<span class="p_del">-		 * page attribute in parallel, that also falls into the</span>
<span class="p_del">-		 * just split large page entry.</span>
<span class="p_del">-	 	 */</span>
<span class="p_del">-		flush_tlb_all();</span>
<span class="p_del">-		goto repeat;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	err = try_preserve_large_page(p_kpte, address, cpa);</span>
 
<span class="p_add">+out:</span>
<span class="p_add">+	if (!debug_pagealloc_enabled())</span>
<span class="p_add">+		spin_unlock(&amp;cpa_lock);</span>
 	return err;
 }
 
 static int __change_page_attr_set_clr(struct cpa_data *cpa, int checkalias);
 
<span class="p_del">-static int cpa_process_alias(struct cpa_data *cpa)</span>
<span class="p_add">+static int cpa_process_alias(struct cpa_data *cpa, unsigned long vaddr)</span>
 {
 	struct cpa_data alias_cpa;
 	unsigned long laddr = (unsigned long)__va(cpa-&gt;pfn &lt;&lt; PAGE_SHIFT);
<span class="p_del">-	unsigned long vaddr;</span>
 	int ret;
 
 	if (!pfn_range_is_mapped(cpa-&gt;pfn, cpa-&gt;pfn + 1))
<span class="p_chunk">@@ -1290,16 +1263,6 @@</span> <span class="p_context"> static int cpa_process_alias(struct cpa_data *cpa)</span>
 	 * No need to redo, when the primary call touched the direct
 	 * mapping already:
 	 */
<span class="p_del">-	if (cpa-&gt;flags &amp; CPA_PAGES_ARRAY) {</span>
<span class="p_del">-		struct page *page = cpa-&gt;pages[cpa-&gt;curpage];</span>
<span class="p_del">-		if (unlikely(PageHighMem(page)))</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-		vaddr = (unsigned long)page_address(page);</span>
<span class="p_del">-	} else if (cpa-&gt;flags &amp; CPA_ARRAY)</span>
<span class="p_del">-		vaddr = cpa-&gt;vaddr[cpa-&gt;curpage];</span>
<span class="p_del">-	else</span>
<span class="p_del">-		vaddr = *cpa-&gt;vaddr;</span>
<span class="p_del">-</span>
 	if (!(within(vaddr, PAGE_OFFSET,
 		    PAGE_OFFSET + (max_pfn_mapped &lt;&lt; PAGE_SHIFT)))) {
 
<span class="p_chunk">@@ -1338,33 +1301,64 @@</span> <span class="p_context"> static int cpa_process_alias(struct cpa_data *cpa)</span>
 	return 0;
 }
 
<span class="p_add">+static unsigned long cpa_address(struct cpa_data *cpa, unsigned long numpages)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Store the remaining nr of pages for the large page</span>
<span class="p_add">+	 * preservation check.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	/* for array changes, we can&#39;t use large page */</span>
<span class="p_add">+	cpa-&gt;numpages = 1;</span>
<span class="p_add">+	if (cpa-&gt;flags &amp; CPA_PAGES_ARRAY) {</span>
<span class="p_add">+		struct page *page = cpa-&gt;pages[cpa-&gt;curpage];</span>
<span class="p_add">+		if (unlikely(PageHighMem(page)))</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		return (unsigned long)page_address(page);</span>
<span class="p_add">+	} else if (cpa-&gt;flags &amp; CPA_ARRAY) {</span>
<span class="p_add">+		return cpa-&gt;vaddr[cpa-&gt;curpage];</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		cpa-&gt;numpages = numpages;</span>
<span class="p_add">+		return *cpa-&gt;vaddr;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void cpa_advance(struct cpa_data *cpa)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (cpa-&gt;flags &amp; (CPA_PAGES_ARRAY | CPA_ARRAY))</span>
<span class="p_add">+		cpa-&gt;curpage++;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		*cpa-&gt;vaddr += cpa-&gt;numpages * PAGE_SIZE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int __change_page_attr_set_clr(struct cpa_data *cpa, int checkalias)
 {
 	unsigned long numpages = cpa-&gt;numpages;
<span class="p_add">+	unsigned long vaddr;</span>
<span class="p_add">+	struct page *base;</span>
<span class="p_add">+	pte_t *kpte;</span>
 	int ret;
 
 	while (numpages) {
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Store the remaining nr of pages for the large page</span>
<span class="p_del">-		 * preservation check.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		cpa-&gt;numpages = numpages;</span>
<span class="p_del">-		/* for array changes, we can&#39;t use large page */</span>
<span class="p_del">-		if (cpa-&gt;flags &amp; (CPA_ARRAY | CPA_PAGES_ARRAY))</span>
<span class="p_del">-			cpa-&gt;numpages = 1;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (!debug_pagealloc_enabled())</span>
<span class="p_del">-			spin_lock(&amp;cpa_lock);</span>
<span class="p_del">-		ret = __change_page_attr(cpa, checkalias);</span>
<span class="p_del">-		if (!debug_pagealloc_enabled())</span>
<span class="p_del">-			spin_unlock(&amp;cpa_lock);</span>
<span class="p_del">-		if (ret)</span>
<span class="p_del">-			return ret;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (checkalias) {</span>
<span class="p_del">-			ret = cpa_process_alias(cpa);</span>
<span class="p_del">-			if (ret)</span>
<span class="p_add">+		vaddr = cpa_address(cpa, numpages);</span>
<span class="p_add">+		if (!IS_ERR_VALUE(vaddr)) {</span>
<span class="p_add">+repeat:</span>
<span class="p_add">+			ret = __change_page_attr(cpa, &amp;kpte, vaddr, checkalias);</span>
<span class="p_add">+			if (ret &lt; 0)</span>
 				return ret;
<span class="p_add">+			if (ret) {</span>
<span class="p_add">+				base = alloc_page(GFP_KERNEL|__GFP_NOTRACK);</span>
<span class="p_add">+				if (!base)</span>
<span class="p_add">+					return -ENOMEM;</span>
<span class="p_add">+				if (__split_large_page(cpa, kpte, vaddr, base))</span>
<span class="p_add">+					__free_page(base);</span>
<span class="p_add">+				goto repeat;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (checkalias) {</span>
<span class="p_add">+				ret = cpa_process_alias(cpa, vaddr);</span>
<span class="p_add">+				if (ret &lt; 0)</span>
<span class="p_add">+					return ret;</span>
<span class="p_add">+			}</span>
 		}
 
 		/*
<span class="p_chunk">@@ -1374,11 +1368,7 @@</span> <span class="p_context"> static int __change_page_attr_set_clr(struct cpa_data *cpa, int checkalias)</span>
 		 */
 		BUG_ON(cpa-&gt;numpages &gt; numpages || !cpa-&gt;numpages);
 		numpages -= cpa-&gt;numpages;
<span class="p_del">-		if (cpa-&gt;flags &amp; (CPA_PAGES_ARRAY | CPA_ARRAY))</span>
<span class="p_del">-			cpa-&gt;curpage++;</span>
<span class="p_del">-		else</span>
<span class="p_del">-			*cpa-&gt;vaddr += cpa-&gt;numpages * PAGE_SIZE;</span>
<span class="p_del">-</span>
<span class="p_add">+		cpa_advance(cpa);</span>
 	}
 	return 0;
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



