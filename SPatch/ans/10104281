
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PTI,v2,6/6] x86/pti: Put the LDT in its own PGD if PTI is on - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PTI,v2,6/6] x86/pti: Put the LDT in its own PGD if PTI is on</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 11, 2017, 6:47 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;90c99eb32b56ea1767472d8b48a524545a551dc6.1512974667.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10104281/mbox/"
   >mbox</a>
|
   <a href="/patch/10104281/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10104281/">/patch/10104281/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6ED3E60235 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 11 Dec 2017 06:47:40 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4DB5728779
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 11 Dec 2017 06:47:40 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 4266F28ECB; Mon, 11 Dec 2017 06:47:40 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5E12828779
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 11 Dec 2017 06:47:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752447AbdLKGrf (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 11 Dec 2017 01:47:35 -0500
Received: from mail.kernel.org ([198.145.29.99]:44798 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752316AbdLKGr3 (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 11 Dec 2017 01:47:29 -0500
Received: from localhost (c-71-202-137-17.hsd1.ca.comcast.net
	[71.202.137.17])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id 67181219A2;
	Mon, 11 Dec 2017 06:47:28 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mail.kernel.org 67181219A2
Authentication-Results: mail.kernel.org;
	dmarc=none (p=none dis=none) header.from=kernel.org
Authentication-Results: mail.kernel.org;
	spf=none smtp.mailfrom=luto@kernel.org
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: x86@kernel.org
Cc: linux-kernel@vger.kernel.org, Borislav Petkov &lt;bp@alien8.de&gt;,
	Brian Gerst &lt;brgerst@gmail.com&gt;, David Laight &lt;David.Laight@aculab.com&gt;,
	Kees Cook &lt;keescook@chromium.org&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;
Subject: [PATCH PTI v2 6/6] x86/pti: Put the LDT in its own PGD if PTI is on
Date: Sun, 10 Dec 2017 22:47:22 -0800
Message-Id: &lt;90c99eb32b56ea1767472d8b48a524545a551dc6.1512974667.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.13.6
In-Reply-To: &lt;cover.1512974667.git.luto@kernel.org&gt;
References: &lt;cover.1512974667.git.luto@kernel.org&gt;
In-Reply-To: &lt;cover.1512974667.git.luto@kernel.org&gt;
References: &lt;cover.1512974667.git.luto@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 11, 2017, 6:47 a.m.</div>
<pre class="content">
With PTI on, we need the LDT to be in the usermode tables somewhere,
and the LDT is per-mm.

tglx had a hack to have a per-cpu LDT and context switch it, but it
was probably insanely slow due to the required TLB flushes.

Instead, take advantage of the fact that we have an address space
hole that gives us a completely unused pgd and make that pgd be
per-mm.  We can put the LDT in it.

This has a down side: the LDT isn&#39;t (currently) randomized, and an
attack that can write the LDT is instant root due to call gates
(thanks, AMD, for leaving call gates in AMD64 but designing them
wrong so they&#39;re only useful for exploits).  We could mitigate this
by making the LDT read-only or randomizing it, either of which is
strightforward on top of this patch.

XXX: The 5-level case needs testing and docs updates
<span class="signed-off-by">
Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---
 Documentation/x86/x86_64/mm.txt         |  11 ++-
 arch/x86/include/asm/mmu_context.h      |  33 +++++++-
 arch/x86/include/asm/pgtable_64_types.h |   2 +
 arch/x86/include/asm/processor.h        |  23 ++++--
 arch/x86/kernel/ldt.c                   | 138 +++++++++++++++++++++++++++++---
 5 files changed, 184 insertions(+), 23 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Dec. 11, 2017, 5:49 p.m.</div>
<pre class="content">
So, before this,

On 12/10/2017 10:47 PM, Andy Lutomirski wrote:
...&gt; +	if (unlikely(ldt)) {
<span class="quote">&gt; +		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="quote">&gt; +			if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {</span>
<span class="quote">&gt; +				clear_LDT();</span>
<span class="quote">&gt; +				return;</span>
<span class="quote">&gt; +			}</span>

I&#39;m missing the purpose of the slots.  Are you hoping to use those
eventually for randomization, but just punting on implementing it for now?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +			set_ldt(ldt_slot_va(ldt-&gt;slot), ldt-&gt;nr_entries);</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="quote">&gt; +		}</span>

This seems like a much better place to point out why the aliasing exists
and what it is doing than the place it is actually commented.

Maybe:

			/*
			 * ldt-&gt;entries is not mapped into the user page
			 * tables when page table isolation is enabled.
			 * Point the hardware to the alias we created.
			 */
			set_ldt(ldt_slot_va(ldt-&gt;slot), ...
		} else {
			/*
			 * Point the hardware at the normal kernel
			 * mapping when not isolated.
			 */
			set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);
		}
<span class="quote">
&gt;  /*</span>
<span class="quote">&gt; - * User space process size. 47bits minus one guard page.  The guard</span>
<span class="quote">&gt; - * page is necessary on Intel CPUs: if a SYSCALL instruction is at</span>
<span class="quote">&gt; - * the highest possible canonical userspace address, then that</span>
<span class="quote">&gt; - * syscall will enter the kernel with a non-canonical return</span>
<span class="quote">&gt; - * address, and SYSRET will explode dangerously.  We avoid this</span>
<span class="quote">&gt; - * particular problem by preventing anything from being mapped</span>
<span class="quote">&gt; - * at the maximum canonical address.</span>
<span class="quote">&gt; + * User space process size.  This is the first address outside the user range.</span>
<span class="quote">&gt; + * There are a few constraints that determine this:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * On Intel CPUs, if a SYSCALL instruction is at the highest canonical</span>
<span class="quote">&gt; + * address, then that syscall will enter the kernel with a</span>
<span class="quote">&gt; + * non-canonical return address, and SYSRET will explode dangerously.</span>
<span class="quote">&gt; + * We avoid this particular problem by preventing anything executable</span>
<span class="quote">&gt; + * from being mapped at the maximum canonical address.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * On AMD CPUs in the Ryzen family, there&#39;s a nasty bug in which the</span>
<span class="quote">&gt; + * CPUs malfunction if they execute code from the highest canonical page.</span>
<span class="quote">&gt; + * They&#39;ll speculate right off the end of the canonical space, and</span>
<span class="quote">&gt; + * bad things happen.  This is worked around in the same way as the</span>
<span class="quote">&gt; + * Intel problem.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * With page table isolation enabled, we map the LDT in ... [stay tuned]</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="quote">&gt; index ae5615b03def..46ad333ed797 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/ldt.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/ldt.c</span>
<span class="quote">&gt; @@ -19,6 +19,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/uaccess.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/ldt.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/tlb.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/desc.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/mmu_context.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/syscalls.h&gt;</span>
<span class="quote">&gt; @@ -46,13 +47,12 @@ static void refresh_ldt_segments(void)</span>
<span class="quote">&gt;  static void flush_ldt(void *__mm)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = __mm;</span>
<span class="quote">&gt; -	mm_context_t *pc;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	pc = &amp;mm-&gt;context;</span>
<span class="quote">&gt; -	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;nr_entries);</span>
<span class="quote">&gt; +	__flush_tlb_all();</span>
<span class="quote">&gt; +	load_mm_ldt(mm);</span>

Why the new TLB flush?

Shouldn&#39;t this be done in the more obvious place closer to the page
table manipulation?
<span class="quote">
&gt; @@ -90,9 +90,112 @@ static struct ldt_struct *alloc_ldt_struct(unsigned int num_entries)</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	new_ldt-&gt;nr_entries = num_entries;</span>
<span class="quote">&gt; +	new_ldt-&gt;slot = -1;</span>
<span class="quote">&gt;  	return new_ldt;</span>
<span class="quote">&gt;  }</span>

This seems a bit silly to do given that &#39;slot&#39; is an int and this patch
introduces warnings looking for positive values:

	if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {

Seems like a good idea to just have a single warning in there looking
for non-zero, probably covering the PTI and non-PTI cases (at least for
now until the slots get used).
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * If PTI is enabled, this maps the LDT into the kernelmode and</span>
<span class="quote">&gt; + * usermode tables for the given mm.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * There is no corresponding unmap function.  Even if the LDT is freed, we</span>
<span class="quote">&gt; + * leave the PTEs around until the slot is reused or the mm is destroyed.</span>
<span class="quote">&gt; + * This is harmless: the LDT is always in ordinary memory, and no one will</span>
<span class="quote">&gt; + * access the freed slot.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * If we wanted to unmap freed LDTs, we&#39;d also need to do a flush to make</span>
<span class="quote">&gt; + * it useful, and the flush would slow down modify_ldt().</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static int map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +	bool is_vmalloc;</span>
<span class="quote">&gt; +	bool had_top_level_entry;</span>
<span class="quote">&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	WARN_ON(ldt-&gt;slot != -1);</span>

Only allow mapping newly-allocated LDTs?
<span class="quote">
&gt; +	/*</span>
<span class="quote">&gt; +	 * Did we already have the top level entry allocated?  We can&#39;t</span>
<span class="quote">&gt; +	 * use pgd_none() for this because it doens&#39;t do anything on</span>
<span class="quote">&gt; +	 * 4-level page table kernels.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="quote">&gt; +	had_top_level_entry = (pgd-&gt;pgd != 0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i * PAGE_SIZE &lt; ldt-&gt;nr_entries * LDT_ENTRY_SIZE; i++) {</span>
<span class="quote">&gt; +		unsigned long offset = i &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; +		unsigned long va = (unsigned long)ldt_slot_va(slot) + offset;</span>
<span class="quote">&gt; +		const void *src = (char *)ldt-&gt;entries + offset;</span>
<span class="quote">&gt; +		unsigned long pfn = is_vmalloc ? vmalloc_to_pfn(src) :</span>
<span class="quote">&gt; +			page_to_pfn(virt_to_page(src));</span>
<span class="quote">&gt; +		pte_t pte, *ptep;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		ptep = get_locked_pte(mm, va, &amp;ptl);</span>

It&#39;s *probably* worth calling out that all the page table allocation
happens in there.  I went looking for it in this patch and it took me a
few minutes to find it.
<span class="quote">
&gt; +		if (!ptep)</span>
<span class="quote">&gt; +			return -ENOMEM;</span>
<span class="quote">&gt; +		pte = pfn_pte(pfn, __pgprot(__PAGE_KERNEL &amp; ~_PAGE_GLOBAL));</span>

This ~_PAGE_GLOBAL is for the same reason as all the other KPTI code,
right?  BTW, does this function deserve to be in the LDT code or kpti?
<span class="quote">
&gt; +			      set_pte_at(mm, va, ptep, pte);</span>
<span class="quote">&gt; +		pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt; +	}</span>

Might want to fix up the set_pte_at() whitespace damage.
<span class="quote">
&gt; +	if (mm-&gt;context.ldt) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * We already had an LDT.  The top-level entry should already</span>
<span class="quote">&gt; +		 * have been allocated and synchronized with the usermode</span>
<span class="quote">&gt; +		 * tables.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		WARN_ON(!had_top_level_entry);</span>
<span class="quote">&gt; +		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="quote">&gt; +			WARN_ON(!kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * This is the first time we&#39;re mapping an LDT for this process.</span>
<span class="quote">&gt; +		 * Sync the pgd to the usermode tables.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		WARN_ON(had_top_level_entry);</span>
<span class="quote">&gt; +		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="quote">&gt; +			WARN_ON(kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="quote">&gt; +			set_pgd(kernel_to_user_pgdp(pgd), *pgd);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	flush_tlb_mm_range(mm,</span>
<span class="quote">&gt; +			   (unsigned long)ldt_slot_va(slot),</span>
<span class="quote">&gt; +			   (unsigned long)ldt_slot_va(slot) + LDT_SLOT_STRIDE,</span>
<span class="quote">&gt; +			   0);</span>

Why wait until here to flush?  Isn&#39;t this primarily for the case where
set_pte_at() overwrote something?
<span class="quote">
&gt; +	ldt-&gt;slot = slot;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +	return -EINVAL;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void free_ldt_pgtables(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="quote">&gt; +	struct mmu_gather tlb;</span>
<span class="quote">&gt; +	unsigned long start = LDT_BASE_ADDR;</span>
<span class="quote">&gt; +	unsigned long end = start + (1UL &lt;&lt; PGDIR_SHIFT);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="quote">&gt; +	free_pgd_range(&amp;tlb, start, end, start, end);</span>
<span class="quote">&gt; +	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +}</span>

Isn&#39;t this primarily called at exit()?  Isn&#39;t it a bit of a shame we
can&#39;t combine this with the other exit()-time TLB flushing?
<span class="quote">
&gt;  /* After calling this, the LDT is immutable. */</span>
<span class="quote">&gt;  static void finalize_ldt_struct(struct ldt_struct *ldt)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -134,17 +237,15 @@ int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  	int retval = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mutex_init(&amp;mm-&gt;context.lock);</span>
<span class="quote">&gt; +	mm-&gt;context.ldt = NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	old_mm = current-&gt;mm;</span>
<span class="quote">&gt; -	if (!old_mm) {</span>
<span class="quote">&gt; -		mm-&gt;context.ldt = NULL;</span>
<span class="quote">&gt; +	if (!old_mm)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mutex_lock(&amp;old_mm-&gt;context.lock);</span>
<span class="quote">&gt; -	if (!old_mm-&gt;context.ldt) {</span>
<span class="quote">&gt; -		mm-&gt;context.ldt = NULL;</span>
<span class="quote">&gt; +	if (!old_mm-&gt;context.ldt)</span>
<span class="quote">&gt;  		goto out_unlock;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	new_ldt = alloc_ldt_struct(old_mm-&gt;context.ldt-&gt;nr_entries);</span>
<span class="quote">&gt;  	if (!new_ldt) {</span>
<span class="quote">&gt; @@ -155,8 +256,17 @@ int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
<span class="quote">&gt;  	memcpy(new_ldt-&gt;entries, old_mm-&gt;context.ldt-&gt;entries,</span>
<span class="quote">&gt;  	       new_ldt-&gt;nr_entries * LDT_ENTRY_SIZE);</span>
<span class="quote">&gt;  	finalize_ldt_struct(new_ldt);</span>
<span class="quote">&gt; +	retval = map_ldt_struct(mm, new_ldt, 0);</span>
<span class="quote">&gt; +	if (retval)</span>
<span class="quote">&gt; +		goto out_free;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mm-&gt;context.ldt = new_ldt;</span>
<span class="quote">&gt; +	goto out_unlock;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +out_free:</span>
<span class="quote">&gt; +	free_ldt_pgtables(mm);</span>
<span class="quote">&gt; +	free_ldt_struct(new_ldt);</span>
<span class="quote">&gt; +	return retval;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  out_unlock:</span>
<span class="quote">&gt;  	mutex_unlock(&amp;old_mm-&gt;context.lock);</span>
<span class="quote">&gt; @@ -174,6 +284,11 @@ void destroy_context_ldt(struct mm_struct *mm)</span>
<span class="quote">&gt;  	mm-&gt;context.ldt = NULL;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +void ldt_arch_exit_mmap(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	free_ldt_pgtables(mm);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static int read_ldt(void __user *ptr, unsigned long bytecount)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt; @@ -285,6 +400,11 @@ static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	new_ldt-&gt;entries[ldt_info.entry_number] = ldt;</span>
<span class="quote">&gt;  	finalize_ldt_struct(new_ldt);</span>
<span class="quote">&gt; +	error = map_ldt_struct(mm, new_ldt, old_ldt ? !old_ldt-&gt;slot : 0);</span>
<span class="quote">&gt; +	if (error) {</span>
<span class="quote">&gt; +		free_ldt_struct(old_ldt);</span>
<span class="quote">&gt; +		goto out_unlock;</span>
<span class="quote">&gt; +	}</span>

Ahh, and the slots finally get used in the last hunk!  That was a long
wait! :)

OK, so slots can be 0 or 1, and we need that so we *have* an LDT to use
while we&#39;re setting up the new one.  Sounds sane, but it was pretty
non-obvious from everything up to this point and it&#39;s still pretty hard
to spot with the !old_ldt-&gt;slot in there.

Might be worth commenting when slot is defined.

Also, from a high level, this does increase the overhead of KPTI in a
non-trivial way, right?  It costs us three more page table pages per
process allocated at fork() and freed at exit() and a new TLB flush.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 11, 2017, 6:40 p.m.</div>
<pre class="content">
On Mon, Dec 11, 2017 at 9:49 AM, Dave Hansen &lt;dave.hansen@intel.com&gt; wrote:
<span class="quote">&gt; So, before this,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On 12/10/2017 10:47 PM, Andy Lutomirski wrote:</span>
<span class="quote">&gt; ...&gt; +  if (unlikely(ldt)) {</span>
<span class="quote">&gt;&gt; +             if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="quote">&gt;&gt; +                     if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {</span>
<span class="quote">&gt;&gt; +                             clear_LDT();</span>
<span class="quote">&gt;&gt; +                             return;</span>
<span class="quote">&gt;&gt; +                     }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m missing the purpose of the slots.  Are you hoping to use those</span>
<span class="quote">&gt; eventually for randomization, but just punting on implementing it for now?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +                     set_ldt(ldt_slot_va(ldt-&gt;slot), ldt-&gt;nr_entries);</span>
<span class="quote">&gt;&gt; +             } else {</span>
<span class="quote">&gt;&gt; +                     set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="quote">&gt;&gt; +             }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This seems like a much better place to point out why the aliasing exists</span>
<span class="quote">&gt; and what it is doing than the place it is actually commented.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Maybe:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                         /*</span>
<span class="quote">&gt;                          * ldt-&gt;entries is not mapped into the user page</span>
<span class="quote">&gt;                          * tables when page table isolation is enabled.</span>
<span class="quote">&gt;                          * Point the hardware to the alias we created.</span>
<span class="quote">&gt;                          */</span>
<span class="quote">&gt;                         set_ldt(ldt_slot_va(ldt-&gt;slot), ...</span>
<span class="quote">&gt;                 } else {</span>
<span class="quote">&gt;                         /*</span>
<span class="quote">&gt;                          * Point the hardware at the normal kernel</span>
<span class="quote">&gt;                          * mapping when not isolated.</span>
<span class="quote">&gt;                          */</span>
<span class="quote">&gt;                         set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="quote">&gt;                 }</span>
<span class="quote">&gt;</span>

Good call.
<span class="quote">
&gt;&gt;  /*</span>
<span class="quote">&gt;&gt; - * User space process size. 47bits minus one guard page.  The guard</span>
<span class="quote">&gt;&gt; - * page is necessary on Intel CPUs: if a SYSCALL instruction is at</span>
<span class="quote">&gt;&gt; - * the highest possible canonical userspace address, then that</span>
<span class="quote">&gt;&gt; - * syscall will enter the kernel with a non-canonical return</span>
<span class="quote">&gt;&gt; - * address, and SYSRET will explode dangerously.  We avoid this</span>
<span class="quote">&gt;&gt; - * particular problem by preventing anything from being mapped</span>
<span class="quote">&gt;&gt; - * at the maximum canonical address.</span>
<span class="quote">&gt;&gt; + * User space process size.  This is the first address outside the user range.</span>
<span class="quote">&gt;&gt; + * There are a few constraints that determine this:</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * On Intel CPUs, if a SYSCALL instruction is at the highest canonical</span>
<span class="quote">&gt;&gt; + * address, then that syscall will enter the kernel with a</span>
<span class="quote">&gt;&gt; + * non-canonical return address, and SYSRET will explode dangerously.</span>
<span class="quote">&gt;&gt; + * We avoid this particular problem by preventing anything executable</span>
<span class="quote">&gt;&gt; + * from being mapped at the maximum canonical address.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * On AMD CPUs in the Ryzen family, there&#39;s a nasty bug in which the</span>
<span class="quote">&gt;&gt; + * CPUs malfunction if they execute code from the highest canonical page.</span>
<span class="quote">&gt;&gt; + * They&#39;ll speculate right off the end of the canonical space, and</span>
<span class="quote">&gt;&gt; + * bad things happen.  This is worked around in the same way as the</span>
<span class="quote">&gt;&gt; + * Intel problem.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * With page table isolation enabled, we map the LDT in ... [stay tuned]</span>
<span class="quote">&gt;&gt;   */</span>
<span class="quote">&gt;&gt;  #define TASK_SIZE_MAX        ((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="quote">&gt;&gt; index ae5615b03def..46ad333ed797 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/kernel/ldt.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/kernel/ldt.c</span>
<span class="quote">&gt;&gt; @@ -19,6 +19,7 @@</span>
<span class="quote">&gt;&gt;  #include &lt;linux/uaccess.h&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;asm/ldt.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/tlb.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;asm/desc.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;asm/mmu_context.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;asm/syscalls.h&gt;</span>
<span class="quote">&gt;&gt; @@ -46,13 +47,12 @@ static void refresh_ldt_segments(void)</span>
<span class="quote">&gt;&gt;  static void flush_ldt(void *__mm)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;       struct mm_struct *mm = __mm;</span>
<span class="quote">&gt;&gt; -     mm_context_t *pc;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;       if (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)</span>
<span class="quote">&gt;&gt;               return;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -     pc = &amp;mm-&gt;context;</span>
<span class="quote">&gt;&gt; -     set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;nr_entries);</span>
<span class="quote">&gt;&gt; +     __flush_tlb_all();</span>
<span class="quote">&gt;&gt; +     load_mm_ldt(mm);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Why the new TLB flush?</span>

It was an attempt to debug a bug and I forgot to delete it.
<span class="quote">
&gt;&gt; @@ -90,9 +90,112 @@ static struct ldt_struct *alloc_ldt_struct(unsigned int num_entries)</span>
<span class="quote">&gt;&gt;       }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;       new_ldt-&gt;nr_entries = num_entries;</span>
<span class="quote">&gt;&gt; +     new_ldt-&gt;slot = -1;</span>
<span class="quote">&gt;&gt;       return new_ldt;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This seems a bit silly to do given that &#39;slot&#39; is an int and this patch</span>
<span class="quote">&gt; introduces warnings looking for positive values:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Seems like a good idea to just have a single warning in there looking</span>
<span class="quote">&gt; for non-zero, probably covering the PTI and non-PTI cases (at least for</span>
<span class="quote">&gt; now until the slots get used).</span>

The idea is to warn if we haven&#39;t mapped it yet.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * If PTI is enabled, this maps the LDT into the kernelmode and</span>
<span class="quote">&gt;&gt; + * usermode tables for the given mm.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * There is no corresponding unmap function.  Even if the LDT is freed, we</span>
<span class="quote">&gt;&gt; + * leave the PTEs around until the slot is reused or the mm is destroyed.</span>
<span class="quote">&gt;&gt; + * This is harmless: the LDT is always in ordinary memory, and no one will</span>
<span class="quote">&gt;&gt; + * access the freed slot.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * If we wanted to unmap freed LDTs, we&#39;d also need to do a flush to make</span>
<span class="quote">&gt;&gt; + * it useful, and the flush would slow down modify_ldt().</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static int map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="quote">&gt;&gt; +     spinlock_t *ptl;</span>
<span class="quote">&gt;&gt; +     bool is_vmalloc;</span>
<span class="quote">&gt;&gt; +     bool had_top_level_entry;</span>
<span class="quote">&gt;&gt; +     pgd_t *pgd;</span>
<span class="quote">&gt;&gt; +     int i;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="quote">&gt;&gt; +             return 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     WARN_ON(ldt-&gt;slot != -1);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Only allow mapping newly-allocated LDTs?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +     /*</span>
<span class="quote">&gt;&gt; +      * Did we already have the top level entry allocated?  We can&#39;t</span>
<span class="quote">&gt;&gt; +      * use pgd_none() for this because it doens&#39;t do anything on</span>
<span class="quote">&gt;&gt; +      * 4-level page table kernels.</span>
<span class="quote">&gt;&gt; +      */</span>
<span class="quote">&gt;&gt; +     pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="quote">&gt;&gt; +     had_top_level_entry = (pgd-&gt;pgd != 0);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     for (i = 0; i * PAGE_SIZE &lt; ldt-&gt;nr_entries * LDT_ENTRY_SIZE; i++) {</span>
<span class="quote">&gt;&gt; +             unsigned long offset = i &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;&gt; +             unsigned long va = (unsigned long)ldt_slot_va(slot) + offset;</span>
<span class="quote">&gt;&gt; +             const void *src = (char *)ldt-&gt;entries + offset;</span>
<span class="quote">&gt;&gt; +             unsigned long pfn = is_vmalloc ? vmalloc_to_pfn(src) :</span>
<span class="quote">&gt;&gt; +                     page_to_pfn(virt_to_page(src));</span>
<span class="quote">&gt;&gt; +             pte_t pte, *ptep;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +             ptep = get_locked_pte(mm, va, &amp;ptl);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It&#39;s *probably* worth calling out that all the page table allocation</span>
<span class="quote">&gt; happens in there.  I went looking for it in this patch and it took me a</span>
<span class="quote">&gt; few minutes to find it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +             if (!ptep)</span>
<span class="quote">&gt;&gt; +                     return -ENOMEM;</span>
<span class="quote">&gt;&gt; +             pte = pfn_pte(pfn, __pgprot(__PAGE_KERNEL &amp; ~_PAGE_GLOBAL));</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This ~_PAGE_GLOBAL is for the same reason as all the other KPTI code,</span>
<span class="quote">&gt; right?  BTW, does this function deserve to be in the LDT code or kpti?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +                           set_pte_at(mm, va, ptep, pte);</span>
<span class="quote">&gt;&gt; +             pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt;&gt; +     }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Might want to fix up the set_pte_at() whitespace damage.</span>

It&#39;s not damaged -- it lines up nicely if you look at the file instead
of the patch :)
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +     if (mm-&gt;context.ldt) {</span>
<span class="quote">&gt;&gt; +             /*</span>
<span class="quote">&gt;&gt; +              * We already had an LDT.  The top-level entry should already</span>
<span class="quote">&gt;&gt; +              * have been allocated and synchronized with the usermode</span>
<span class="quote">&gt;&gt; +              * tables.</span>
<span class="quote">&gt;&gt; +              */</span>
<span class="quote">&gt;&gt; +             WARN_ON(!had_top_level_entry);</span>
<span class="quote">&gt;&gt; +             if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="quote">&gt;&gt; +                     WARN_ON(!kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="quote">&gt;&gt; +     } else {</span>
<span class="quote">&gt;&gt; +             /*</span>
<span class="quote">&gt;&gt; +              * This is the first time we&#39;re mapping an LDT for this process.</span>
<span class="quote">&gt;&gt; +              * Sync the pgd to the usermode tables.</span>
<span class="quote">&gt;&gt; +              */</span>
<span class="quote">&gt;&gt; +             WARN_ON(had_top_level_entry);</span>
<span class="quote">&gt;&gt; +             if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="quote">&gt;&gt; +                     WARN_ON(kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="quote">&gt;&gt; +                     set_pgd(kernel_to_user_pgdp(pgd), *pgd);</span>
<span class="quote">&gt;&gt; +             }</span>
<span class="quote">&gt;&gt; +     }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     flush_tlb_mm_range(mm,</span>
<span class="quote">&gt;&gt; +                        (unsigned long)ldt_slot_va(slot),</span>
<span class="quote">&gt;&gt; +                        (unsigned long)ldt_slot_va(slot) + LDT_SLOT_STRIDE,</span>
<span class="quote">&gt;&gt; +                        0);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Why wait until here to flush?  Isn&#39;t this primarily for the case where</span>
<span class="quote">&gt; set_pte_at() overwrote something?</span>

I think it would be okay if we did it sooner, but CPUs are allowed to
cache intermediate mappings, and we&#39;re changing the userspace tables
above it.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void free_ldt_pgtables(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="quote">&gt;&gt; +     struct mmu_gather tlb;</span>
<span class="quote">&gt;&gt; +     unsigned long start = LDT_BASE_ADDR;</span>
<span class="quote">&gt;&gt; +     unsigned long end = start + (1UL &lt;&lt; PGDIR_SHIFT);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="quote">&gt;&gt; +             return;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="quote">&gt;&gt; +     free_pgd_range(&amp;tlb, start, end, start, end);</span>
<span class="quote">&gt;&gt; +     tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Isn&#39;t this primarily called at exit()?  Isn&#39;t it a bit of a shame we</span>
<span class="quote">&gt; can&#39;t combine this with the other exit()-time TLB flushing?</span>

Yes.  In fact, we don&#39;t really need that flush at all since the mm is
totally dead.  But the free_pgd_range() API is too dumb.  And yes, we
have the same issue in the normal mm/memory.c code.  In general, exit
handling is seriously overengineered.
<span class="quote">
&gt; Also, from a high level, this does increase the overhead of KPTI in a</span>
<span class="quote">&gt; non-trivial way, right?  It costs us three more page table pages per</span>
<span class="quote">&gt; process allocated at fork() and freed at exit() and a new TLB flush.</span>

Yeah, but no one will care.  modify_ldt() is used for DOSEMU, Wine,
and really old 32-bit programs.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Dec. 11, 2017, 7:32 p.m.</div>
<pre class="content">
On 12/11/2017 10:40 AM, Andy Lutomirski wrote:
<span class="quote">&gt;&gt; Also, from a high level, this does increase the overhead of KPTI in a</span>
<span class="quote">&gt;&gt; non-trivial way, right?  It costs us three more page table pages per</span>
<span class="quote">&gt;&gt; process allocated at fork() and freed at exit() and a new TLB flush.</span>
<span class="quote">&gt; Yeah, but no one will care.  modify_ldt() is used for DOSEMU, Wine,</span>
<span class="quote">&gt; and really old 32-bit programs.</span>

The heavyweight part of map_ldt_struct() (and unmap) looks to run
whenever we have KPTI enabled.  I&#39;m missing how it gets avoided for the
non-DOSEMU cases.

I thought there would be a &quot;fast path&quot; where we just use the normal
clear_LDT() LDT from the cpu_entry_area and don&#39;t have to do any of
this, but I&#39;m missing where that happens.  Do we need a check in
(un)map_ldt_struct() for !mm-&gt;context.ldt?

Just to make sure I understand this: We now have two places that LDTs
live in virtual space:

1. The &quot;plain&quot; one that we get from clear_LDT() which lives in the
   cpu_entry_area.  (No additional overhead when doing this)
2. The new one under the special PGD that&#39;s only used for modify_ldt()
   and is fairly slow.  (plenty of overhead, but nobody cares).
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 11, 2017, 7:39 p.m.</div>
<pre class="content">
On Mon, Dec 11, 2017 at 11:32 AM, Dave Hansen &lt;dave.hansen@intel.com&gt; wrote:
<span class="quote">&gt; On 12/11/2017 10:40 AM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt; Also, from a high level, this does increase the overhead of KPTI in a</span>
<span class="quote">&gt;&gt;&gt; non-trivial way, right?  It costs us three more page table pages per</span>
<span class="quote">&gt;&gt;&gt; process allocated at fork() and freed at exit() and a new TLB flush.</span>
<span class="quote">&gt;&gt; Yeah, but no one will care.  modify_ldt() is used for DOSEMU, Wine,</span>
<span class="quote">&gt;&gt; and really old 32-bit programs.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The heavyweight part of map_ldt_struct() (and unmap) looks to run</span>
<span class="quote">&gt; whenever we have KPTI enabled.  I&#39;m missing how it gets avoided for the</span>
<span class="quote">&gt; non-DOSEMU cases.</span>

It doesn&#39;t get called unless modify_ldt() is used.
<span class="quote">
&gt;</span>
<span class="quote">&gt; I thought there would be a &quot;fast path&quot; where we just use the normal</span>
<span class="quote">&gt; clear_LDT() LDT from the cpu_entry_area and don&#39;t have to do any of</span>
<span class="quote">&gt; this, but I&#39;m missing where that happens.  Do we need a check in</span>
<span class="quote">&gt; (un)map_ldt_struct() for !mm-&gt;context.ldt?</span>

I&#39;m confused.

if (unlikely(ldt)) {
  do something slowish;
} else {
  clear_LD();
}
<span class="quote">
&gt;</span>
<span class="quote">&gt; Just to make sure I understand this: We now have two places that LDTs</span>
<span class="quote">&gt; live in virtual space:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 1. The &quot;plain&quot; one that we get from clear_LDT() which lives in the</span>
<span class="quote">&gt;    cpu_entry_area.  (No additional overhead when doing this)</span>
<span class="quote">&gt; 2. The new one under the special PGD that&#39;s only used for modify_ldt()</span>
<span class="quote">&gt;    and is fairly slow.  (plenty of overhead, but nobody cares).</span>

Yes.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=65121">Dave Hansen</a> - Dec. 11, 2017, 7:47 p.m.</div>
<pre class="content">
On 12/11/2017 11:39 AM, Andy Lutomirski wrote:
<span class="quote">&gt;&gt; I thought there would be a &quot;fast path&quot; where we just use the normal</span>
<span class="quote">&gt;&gt; clear_LDT() LDT from the cpu_entry_area and don&#39;t have to do any of</span>
<span class="quote">&gt;&gt; this, but I&#39;m missing where that happens.  Do we need a check in</span>
<span class="quote">&gt;&gt; (un)map_ldt_struct() for !mm-&gt;context.ldt?</span>
<span class="quote">&gt; I&#39;m confused.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; if (unlikely(ldt)) {</span>
<span class="quote">&gt;   do something slowish;</span>
<span class="quote">&gt; } else {</span>
<span class="quote">&gt;   clear_LD();</span>
<span class="quote">&gt; }</span>

I was looking at the map/unmap paths.  It looks to me like the cases
where there is map/unmap overhead, we *are* doing checking against
mm-&gt;context.ldt.  It just wasn&#39;t visible from the patch context.

In any case, it would be really nice to call that out if you revise
these in the patch description: none of these LDT acrobatics are used in
the common case.  Virtually every process uses the !ldt paths which
don&#39;t do any of this.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 11, 2017, 8:06 p.m.</div>
<pre class="content">
On Mon, Dec 11, 2017 at 11:47 AM, Dave Hansen &lt;dave.hansen@intel.com&gt; wrote:
<span class="quote">&gt; On 12/11/2017 11:39 AM, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt;&gt; I thought there would be a &quot;fast path&quot; where we just use the normal</span>
<span class="quote">&gt;&gt;&gt; clear_LDT() LDT from the cpu_entry_area and don&#39;t have to do any of</span>
<span class="quote">&gt;&gt;&gt; this, but I&#39;m missing where that happens.  Do we need a check in</span>
<span class="quote">&gt;&gt;&gt; (un)map_ldt_struct() for !mm-&gt;context.ldt?</span>
<span class="quote">&gt;&gt; I&#39;m confused.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; if (unlikely(ldt)) {</span>
<span class="quote">&gt;&gt;   do something slowish;</span>
<span class="quote">&gt;&gt; } else {</span>
<span class="quote">&gt;&gt;   clear_LD();</span>
<span class="quote">&gt;&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I was looking at the map/unmap paths.  It looks to me like the cases</span>
<span class="quote">&gt; where there is map/unmap overhead, we *are* doing checking against</span>
<span class="quote">&gt; mm-&gt;context.ldt.  It just wasn&#39;t visible from the patch context.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; In any case, it would be really nice to call that out if you revise</span>
<span class="quote">&gt; these in the patch description: none of these LDT acrobatics are used in</span>
<span class="quote">&gt; the common case.  Virtually every process uses the !ldt paths which</span>
<span class="quote">&gt; don&#39;t do any of this.</span>

Will do.

I&#39;m currently fighting with the 5 level case.  I need to reorganize
the memory map a bit, but it&#39;s blowing up, and I&#39;m not sure why yet.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/x86/x86_64/mm.txt b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">index 2d7d6590ade8..bfa44e1cb293 100644</span>
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -12,13 +12,15 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) LDT range</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
 ... unused hole ...
 ffffffff80000000 - ffffffff9fffffff (=512 MB)  kernel text mapping, from phys 0
<span class="p_del">-ffffffffa0000000 - ffffffffff5fffff (=1526 MB) module mapping space (variable)</span>
<span class="p_del">-ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls</span>
<span class="p_add">+ffffffffa0000000 - [fixmap start]   (~1526 MB) module mapping space (variable)</span>
<span class="p_add">+[fixmap start]   - ffffffffff5fffff kernel-internal fixmap range</span>
<span class="p_add">+ffffffffff600000 - ffffffffff600fff (=4 kB) legacy vsyscall ABI</span>
 ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole
 
 Virtual memory map with 5 level page tables:
<span class="p_chunk">@@ -39,8 +41,9 @@</span> <span class="p_context"> ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks</span>
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
 ... unused hole ...
 ffffffff80000000 - ffffffff9fffffff (=512 MB)  kernel text mapping, from phys 0
<span class="p_del">-ffffffffa0000000 - ffffffffff5fffff (=1526 MB) module mapping space</span>
<span class="p_del">-ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls</span>
<span class="p_add">+ffffffffa0000000 - [fixmap start]   (~1526 MB) module mapping space</span>
<span class="p_add">+[fixmap start]   - ffffffffff5fffff kernel-internal fixmap range</span>
<span class="p_add">+ffffffffff600000 - ffffffffff600fff (=4 kB) legacy vsyscall ABI</span>
 ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole
 
 Architecture defines a 64-bit virtual address. Implementations can support
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 5e1a1ecb65c6..eb87bbeddacc 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -52,13 +52,29 @@</span> <span class="p_context"> struct ldt_struct {</span>
 	 */
 	struct desc_struct *entries;
 	unsigned int nr_entries;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PTI is in use, then the entries array is not mapped while we&#39;re</span>
<span class="p_add">+	 * in user mode.  The whole array will be aliased at the addressed</span>
<span class="p_add">+	 * given by ldt_slot_va(slot).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int slot;</span>
 };
 
<span class="p_add">+/* This is a multiple of PAGE_SIZE. */</span>
<span class="p_add">+#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+static void *ldt_slot_va(int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (void *)(LDT_BASE_ADDR + LDT_SLOT_STRIDE * slot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Used for LDT copy/destruction.
  */
 int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm);
 void destroy_context_ldt(struct mm_struct *mm);
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm);</span>
 #else	/* CONFIG_MODIFY_LDT_SYSCALL */
 static inline int init_new_context_ldt(struct task_struct *tsk,
 				       struct mm_struct *mm)
<span class="p_chunk">@@ -90,10 +106,20 @@</span> <span class="p_context"> static inline void load_mm_ldt(struct mm_struct *mm)</span>
 	 * that we can see.
 	 */
 
<span class="p_del">-	if (unlikely(ldt))</span>
<span class="p_del">-		set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (unlikely(ldt)) {</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="p_add">+			if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {</span>
<span class="p_add">+				clear_LDT();</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			set_ldt(ldt_slot_va(ldt-&gt;slot), ldt-&gt;nr_entries);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
 		clear_LDT();
<span class="p_add">+	}</span>
 #else
 	clear_LDT();
 #endif
<span class="p_chunk">@@ -185,6 +211,7 @@</span> <span class="p_context"> static inline void arch_dup_mmap(struct mm_struct *oldmm,</span>
 static inline void arch_exit_mmap(struct mm_struct *mm)
 {
 	paravirt_arch_exit_mmap(mm);
<span class="p_add">+	ldt_arch_exit_mmap(mm);</span>
 }
 
 #ifdef CONFIG_X86_64
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64_types.h b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">index 6d5f45dcd4a1..130f575f8d1e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -100,6 +100,8 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define MODULES_LEN   (MODULES_END - MODULES_VADDR)
 #define ESPFIX_PGD_ENTRY _AC(-2, UL)
 #define ESPFIX_BASE_ADDR (ESPFIX_PGD_ENTRY &lt;&lt; P4D_SHIFT)
<span class="p_add">+#define LDT_PGD_ENTRY _AC(-3, UL)</span>
<span class="p_add">+#define LDT_BASE_ADDR (LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #define EFI_VA_START	 ( -4 * (_AC(1, UL) &lt;&lt; 30))
 #define EFI_VA_END	 (-68 * (_AC(1, UL) &lt;&lt; 30))
 
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 9e482d8b0b97..9c18da64daa9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -851,13 +851,22 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 
 #else
 /*
<span class="p_del">- * User space process size. 47bits minus one guard page.  The guard</span>
<span class="p_del">- * page is necessary on Intel CPUs: if a SYSCALL instruction is at</span>
<span class="p_del">- * the highest possible canonical userspace address, then that</span>
<span class="p_del">- * syscall will enter the kernel with a non-canonical return</span>
<span class="p_del">- * address, and SYSRET will explode dangerously.  We avoid this</span>
<span class="p_del">- * particular problem by preventing anything from being mapped</span>
<span class="p_del">- * at the maximum canonical address.</span>
<span class="p_add">+ * User space process size.  This is the first address outside the user range.</span>
<span class="p_add">+ * There are a few constraints that determine this:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On Intel CPUs, if a SYSCALL instruction is at the highest canonical</span>
<span class="p_add">+ * address, then that syscall will enter the kernel with a</span>
<span class="p_add">+ * non-canonical return address, and SYSRET will explode dangerously.</span>
<span class="p_add">+ * We avoid this particular problem by preventing anything executable</span>
<span class="p_add">+ * from being mapped at the maximum canonical address.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On AMD CPUs in the Ryzen family, there&#39;s a nasty bug in which the</span>
<span class="p_add">+ * CPUs malfunction if they execute code from the highest canonical page.</span>
<span class="p_add">+ * They&#39;ll speculate right off the end of the canonical space, and</span>
<span class="p_add">+ * bad things happen.  This is worked around in the same way as the</span>
<span class="p_add">+ * Intel problem.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With page table isolation enabled, we map the LDT in ... [stay tuned]</span>
  */
 #define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
 
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index ae5615b03def..46ad333ed797 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -19,6 +19,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/uaccess.h&gt;
 
 #include &lt;asm/ldt.h&gt;
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_chunk">@@ -46,13 +47,12 @@</span> <span class="p_context"> static void refresh_ldt_segments(void)</span>
 static void flush_ldt(void *__mm)
 {
 	struct mm_struct *mm = __mm;
<span class="p_del">-	mm_context_t *pc;</span>
 
 	if (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)
 		return;
 
<span class="p_del">-	pc = &amp;mm-&gt;context;</span>
<span class="p_del">-	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;nr_entries);</span>
<span class="p_add">+	__flush_tlb_all();</span>
<span class="p_add">+	load_mm_ldt(mm);</span>
 
 	refresh_ldt_segments();
 }
<span class="p_chunk">@@ -90,9 +90,112 @@</span> <span class="p_context"> static struct ldt_struct *alloc_ldt_struct(unsigned int num_entries)</span>
 	}
 
 	new_ldt-&gt;nr_entries = num_entries;
<span class="p_add">+	new_ldt-&gt;slot = -1;</span>
 	return new_ldt;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * If PTI is enabled, this maps the LDT into the kernelmode and</span>
<span class="p_add">+ * usermode tables for the given mm.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * There is no corresponding unmap function.  Even if the LDT is freed, we</span>
<span class="p_add">+ * leave the PTEs around until the slot is reused or the mm is destroyed.</span>
<span class="p_add">+ * This is harmless: the LDT is always in ordinary memory, and no one will</span>
<span class="p_add">+ * access the freed slot.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If we wanted to unmap freed LDTs, we&#39;d also need to do a flush to make</span>
<span class="p_add">+ * it useful, and the flush would slow down modify_ldt().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	bool is_vmalloc;</span>
<span class="p_add">+	bool had_top_level_entry;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	WARN_ON(ldt-&gt;slot != -1);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Did we already have the top level entry allocated?  We can&#39;t</span>
<span class="p_add">+	 * use pgd_none() for this because it doens&#39;t do anything on</span>
<span class="p_add">+	 * 4-level page table kernels.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="p_add">+	had_top_level_entry = (pgd-&gt;pgd != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i * PAGE_SIZE &lt; ldt-&gt;nr_entries * LDT_ENTRY_SIZE; i++) {</span>
<span class="p_add">+		unsigned long offset = i &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		unsigned long va = (unsigned long)ldt_slot_va(slot) + offset;</span>
<span class="p_add">+		const void *src = (char *)ldt-&gt;entries + offset;</span>
<span class="p_add">+		unsigned long pfn = is_vmalloc ? vmalloc_to_pfn(src) :</span>
<span class="p_add">+			page_to_pfn(virt_to_page(src));</span>
<span class="p_add">+		pte_t pte, *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		ptep = get_locked_pte(mm, va, &amp;ptl);</span>
<span class="p_add">+		if (!ptep)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		pte = pfn_pte(pfn, __pgprot(__PAGE_KERNEL &amp; ~_PAGE_GLOBAL));</span>
<span class="p_add">+			      set_pte_at(mm, va, ptep, pte);</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mm-&gt;context.ldt) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We already had an LDT.  The top-level entry should already</span>
<span class="p_add">+		 * have been allocated and synchronized with the usermode</span>
<span class="p_add">+		 * tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(!had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+			WARN_ON(!kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This is the first time we&#39;re mapping an LDT for this process.</span>
<span class="p_add">+		 * Sync the pgd to the usermode tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="p_add">+			WARN_ON(kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+			set_pgd(kernel_to_user_pgdp(pgd), *pgd);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_mm_range(mm,</span>
<span class="p_add">+			   (unsigned long)ldt_slot_va(slot),</span>
<span class="p_add">+			   (unsigned long)ldt_slot_va(slot) + LDT_SLOT_STRIDE,</span>
<span class="p_add">+			   0);</span>
<span class="p_add">+</span>
<span class="p_add">+	ldt-&gt;slot = slot;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return -EINVAL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_ldt_pgtables(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	struct mmu_gather tlb;</span>
<span class="p_add">+	unsigned long start = LDT_BASE_ADDR;</span>
<span class="p_add">+	unsigned long end = start + (1UL &lt;&lt; PGDIR_SHIFT);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="p_add">+	free_pgd_range(&amp;tlb, start, end, start, end);</span>
<span class="p_add">+	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* After calling this, the LDT is immutable. */
 static void finalize_ldt_struct(struct ldt_struct *ldt)
 {
<span class="p_chunk">@@ -134,17 +237,15 @@</span> <span class="p_context"> int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
 	int retval = 0;
 
 	mutex_init(&amp;mm-&gt;context.lock);
<span class="p_add">+	mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+</span>
 	old_mm = current-&gt;mm;
<span class="p_del">-	if (!old_mm) {</span>
<span class="p_del">-		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+	if (!old_mm)</span>
 		return 0;
<span class="p_del">-	}</span>
 
 	mutex_lock(&amp;old_mm-&gt;context.lock);
<span class="p_del">-	if (!old_mm-&gt;context.ldt) {</span>
<span class="p_del">-		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+	if (!old_mm-&gt;context.ldt)</span>
 		goto out_unlock;
<span class="p_del">-	}</span>
 
 	new_ldt = alloc_ldt_struct(old_mm-&gt;context.ldt-&gt;nr_entries);
 	if (!new_ldt) {
<span class="p_chunk">@@ -155,8 +256,17 @@</span> <span class="p_context"> int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
 	memcpy(new_ldt-&gt;entries, old_mm-&gt;context.ldt-&gt;entries,
 	       new_ldt-&gt;nr_entries * LDT_ENTRY_SIZE);
 	finalize_ldt_struct(new_ldt);
<span class="p_add">+	retval = map_ldt_struct(mm, new_ldt, 0);</span>
<span class="p_add">+	if (retval)</span>
<span class="p_add">+		goto out_free;</span>
 
 	mm-&gt;context.ldt = new_ldt;
<span class="p_add">+	goto out_unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+out_free:</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+	free_ldt_struct(new_ldt);</span>
<span class="p_add">+	return retval;</span>
 
 out_unlock:
 	mutex_unlock(&amp;old_mm-&gt;context.lock);
<span class="p_chunk">@@ -174,6 +284,11 @@</span> <span class="p_context"> void destroy_context_ldt(struct mm_struct *mm)</span>
 	mm-&gt;context.ldt = NULL;
 }
 
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int read_ldt(void __user *ptr, unsigned long bytecount)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_chunk">@@ -285,6 +400,11 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 
 	new_ldt-&gt;entries[ldt_info.entry_number] = ldt;
 	finalize_ldt_struct(new_ldt);
<span class="p_add">+	error = map_ldt_struct(mm, new_ldt, old_ldt ? !old_ldt-&gt;slot : 0);</span>
<span class="p_add">+	if (error) {</span>
<span class="p_add">+		free_ldt_struct(old_ldt);</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
 
 	install_ldt(mm, new_ldt);
 	free_ldt_struct(old_ldt);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



