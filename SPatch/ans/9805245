
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/3] hugetlb: add support for preferred node to alloc_huge_page_nodemask - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/3] hugetlb: add support for preferred node to alloc_huge_page_nodemask</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 22, 2017, 7:30 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170622193034.28972-3-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9805245/mbox/"
   >mbox</a>
|
   <a href="/patch/9805245/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9805245/">/patch/9805245/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	E3D1160329 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 19:31:17 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id CD2831FF22
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 19:31:17 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id C0EF328653; Thu, 22 Jun 2017 19:31:17 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C1F0A2864C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 22 Jun 2017 19:31:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753865AbdFVTbN (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 22 Jun 2017 15:31:13 -0400
Received: from mail-wr0-f195.google.com ([209.85.128.195]:36011 &quot;EHLO
	mail-wr0-f195.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753612AbdFVTaq (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 22 Jun 2017 15:30:46 -0400
Received: by mail-wr0-f195.google.com with SMTP id 77so7014967wrb.3
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 22 Jun 2017 12:30:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=9f4T31HQYW82Kt8wQwEmwU7bGP4hGOhNZeq1B9QH7PM=;
	b=nzN7/fQx0ZRXEje+5wdma4xKZFsMD/mbEGd1ITS0IrwcGGGlfNtB642NgTx0HNkcsQ
	Uice0/Flwokod82+c6DR6uLHO/ALdiwXtXxCQLQWM5ReTYm/ctg37Ls83YL25nGGwnBP
	ClFI1oXe5EHAiFdeTmKA59EuPpGKHKvmXdvwztYu6g8GG1M5u7JxP8fOsKsCgINnojrs
	4QsOja71goBCJZzNlWPA5OXYkdyY3kIPFHHqy9LUdWH9dgs39MQINwU00sIo+15He6hj
	kuipKTHJKRLSBkaNJx9tHoRiQVzSJXmWAHfqivj0k9+iM55biBQ/vL+XKTK+zwy/I9Fb
	nYvA==
X-Gm-Message-State: AKS2vOwaPhjjzBx+zZ+Xw41NLXVdsgOqdFEF4njMotk9aGS87MryuaxY
	Uq6d5e6QhGApOQ==
X-Received: by 10.28.146.208 with SMTP id u199mr3011899wmd.6.1498159845403; 
	Thu, 22 Jun 2017 12:30:45 -0700 (PDT)
Received: from tiehlicka.suse.cz (ip-89-177-66-30.net.upcbroadband.cz.
	[89.177.66.30]) by smtp.gmail.com with ESMTPSA id
	h16sm2595033wma.14.2017.06.22.12.30.44
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 22 Jun 2017 12:30:44 -0700 (PDT)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: &lt;linux-mm@kvack.org&gt;, Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [PATCH 2/3] hugetlb: add support for preferred node to
	alloc_huge_page_nodemask
Date: Thu, 22 Jun 2017 21:30:33 +0200
Message-Id: &lt;20170622193034.28972-3-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170622193034.28972-1-mhocko@kernel.org&gt;
References: &lt;20170622193034.28972-1-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 22, 2017, 7:30 p.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

alloc_huge_page_nodemask tries to allocate from any numa node in the
allowed node mask starting from lower numa nodes. This might lead to
filling up those low NUMA nodes while others are not used. We can reduce
this risk by introducing a concept of the preferred node similar to what
we have in the regular page allocator. We will start allocating from the
preferred nid and then iterate over all allowed nodes in the zonelist
order until we try them all.

This is mimicking the page allocator logic except it operates on
per-node mempools. dequeue_huge_page_vma already does this so distill
the zonelist logic into a more generic dequeue_huge_page_nodemask
and use it in alloc_huge_page_nodemask.

This will allow us to use proper per numa distance fallback also for
alloc_huge_page_node which can use alloc_huge_page_nodemask now and we
can get rid of alloc_huge_page_node helper which doesn&#39;t have any user
anymore.

Changes since v1
- get rid of dequeue_huge_page_node because it is not really needed
- simplify dequeue_huge_page_nodemask and alloc_huge_page_nodemask a bit
  as per Vlastimil
<span class="acked-by">
Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="signed-off-by">Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 include/linux/hugetlb.h |  5 +--
 include/linux/migrate.h |  2 +-
 mm/hugetlb.c            | 88 ++++++++++++++++++++++++-------------------------
 3 files changed, 48 insertions(+), 47 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - June 26, 2017, 6:31 p.m.</div>
<pre class="content">
On 06/22/2017 12:30 PM, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; alloc_huge_page_nodemask tries to allocate from any numa node in the</span>
<span class="quote">&gt; allowed node mask starting from lower numa nodes. This might lead to</span>
<span class="quote">&gt; filling up those low NUMA nodes while others are not used. We can reduce</span>
<span class="quote">&gt; this risk by introducing a concept of the preferred node similar to what</span>
<span class="quote">&gt; we have in the regular page allocator. We will start allocating from the</span>
<span class="quote">&gt; preferred nid and then iterate over all allowed nodes in the zonelist</span>
<span class="quote">&gt; order until we try them all.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is mimicking the page allocator logic except it operates on</span>
<span class="quote">&gt; per-node mempools. dequeue_huge_page_vma already does this so distill</span>
<span class="quote">&gt; the zonelist logic into a more generic dequeue_huge_page_nodemask</span>
<span class="quote">&gt; and use it in alloc_huge_page_nodemask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This will allow us to use proper per numa distance fallback also for</span>
<span class="quote">&gt; alloc_huge_page_node which can use alloc_huge_page_nodemask now and we</span>
<span class="quote">&gt; can get rid of alloc_huge_page_node helper which doesn&#39;t have any user</span>
<span class="quote">&gt; anymore.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changes since v1</span>
<span class="quote">&gt; - get rid of dequeue_huge_page_node because it is not really needed</span>
<span class="quote">&gt; - simplify dequeue_huge_page_nodemask and alloc_huge_page_nodemask a bit</span>
<span class="quote">&gt;   as per Vlastimil</span>
<span class="reviewed-by">
Reviewed-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="tested-by">Tested-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 66b621469f52..8d9fe131a240 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -349,7 +349,8 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask);</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="p_add">+				nodemask_t *nmask);</span>
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
<span class="p_chunk">@@ -525,7 +526,7 @@</span> <span class="p_context"> static inline void set_huge_swap_pte_at(struct mm_struct *mm, unsigned long addr</span>
 struct hstate {};
 #define alloc_huge_page(v, a, r) NULL
 #define alloc_huge_page_node(h, nid) NULL
<span class="p_del">-#define alloc_huge_page_nodemask(h, nmask) NULL</span>
<span class="p_add">+#define alloc_huge_page_nodemask(h, preferred_nid, nmask) NULL</span>
 #define alloc_huge_page_noerr(v, a, r) NULL
 #define alloc_bootmem_huge_page(h) NULL
 #define hstate_file(f) NULL
<span class="p_header">diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="p_header">index f80c9882403a..af3ccf93efaa 100644</span>
<span class="p_header">--- a/include/linux/migrate.h</span>
<span class="p_header">+++ b/include/linux/migrate.h</span>
<span class="p_chunk">@@ -38,7 +38,7 @@</span> <span class="p_context"> static inline struct page *new_page_nodemask(struct page *page, int preferred_ni</span>
 
 	if (PageHuge(page))
 		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),
<span class="p_del">-				nodemask);</span>
<span class="p_add">+				preferred_nid, nodemask);</span>
 
 	if (PageHighMem(page)
 	    || (zone_idx(page_zone(page)) == ZONE_MOVABLE))
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index fd6e0c50f949..1e516520433d 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -887,19 +887,39 @@</span> <span class="p_context"> static struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)</span>
 	return page;
 }
 
<span class="p_del">-static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
<span class="p_add">+static struct page *dequeue_huge_page_nodemask(struct hstate *h, gfp_t gfp_mask, int nid,</span>
<span class="p_add">+		nodemask_t *nmask)</span>
 {
<span class="p_del">-	struct page *page;</span>
<span class="p_del">-	int node;</span>
<span class="p_add">+	unsigned int cpuset_mems_cookie;</span>
<span class="p_add">+	struct zonelist *zonelist;</span>
<span class="p_add">+	struct zone *zone;</span>
<span class="p_add">+	struct zoneref *z;</span>
<span class="p_add">+	int node = -1;</span>
 
<span class="p_del">-	if (nid != NUMA_NO_NODE)</span>
<span class="p_del">-		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="p_add">+	zonelist = node_zonelist(nid, gfp_mask);</span>
<span class="p_add">+</span>
<span class="p_add">+retry_cpuset:</span>
<span class="p_add">+	cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="p_add">+	for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!cpuset_zone_allowed(zone, gfp_mask))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * no need to ask again on the same node. Pool is node rather than</span>
<span class="p_add">+		 * zone aware</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (zone_to_nid(zone) == node)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		node = zone_to_nid(zone);</span>
 
<span class="p_del">-	for_each_online_node(node) {</span>
 		page = dequeue_huge_page_node_exact(h, node);
 		if (page)
 			return page;
 	}
<span class="p_add">+	if (unlikely(read_mems_allowed_retry(cpuset_mems_cookie)))</span>
<span class="p_add">+		goto retry_cpuset;</span>
<span class="p_add">+</span>
 	return NULL;
 }
 
<span class="p_chunk">@@ -917,15 +937,11 @@</span> <span class="p_context"> static struct page *dequeue_huge_page_vma(struct hstate *h,</span>
 				unsigned long address, int avoid_reserve,
 				long chg)
 {
<span class="p_del">-	struct page *page = NULL;</span>
<span class="p_add">+	struct page *page;</span>
 	struct mempolicy *mpol;
<span class="p_del">-	nodemask_t *nodemask;</span>
 	gfp_t gfp_mask;
<span class="p_add">+	nodemask_t *nodemask;</span>
 	int nid;
<span class="p_del">-	struct zonelist *zonelist;</span>
<span class="p_del">-	struct zone *zone;</span>
<span class="p_del">-	struct zoneref *z;</span>
<span class="p_del">-	unsigned int cpuset_mems_cookie;</span>
 
 	/*
 	 * A child process with MAP_PRIVATE mappings created by their parent
<span class="p_chunk">@@ -940,32 +956,15 @@</span> <span class="p_context"> static struct page *dequeue_huge_page_vma(struct hstate *h,</span>
 	if (avoid_reserve &amp;&amp; h-&gt;free_huge_pages - h-&gt;resv_huge_pages == 0)
 		goto err;
 
<span class="p_del">-retry_cpuset:</span>
<span class="p_del">-	cpuset_mems_cookie = read_mems_allowed_begin();</span>
 	gfp_mask = htlb_alloc_mask(h);
 	nid = huge_node(vma, address, gfp_mask, &amp;mpol, &amp;nodemask);
<span class="p_del">-	zonelist = node_zonelist(nid, gfp_mask);</span>
<span class="p_del">-</span>
<span class="p_del">-	for_each_zone_zonelist_nodemask(zone, z, zonelist,</span>
<span class="p_del">-						MAX_NR_ZONES - 1, nodemask) {</span>
<span class="p_del">-		if (cpuset_zone_allowed(zone, gfp_mask)) {</span>
<span class="p_del">-			page = dequeue_huge_page_node(h, zone_to_nid(zone));</span>
<span class="p_del">-			if (page) {</span>
<span class="p_del">-				if (avoid_reserve)</span>
<span class="p_del">-					break;</span>
<span class="p_del">-				if (!vma_has_reserves(vma, chg))</span>
<span class="p_del">-					break;</span>
<span class="p_del">-</span>
<span class="p_del">-				SetPagePrivate(page);</span>
<span class="p_del">-				h-&gt;resv_huge_pages--;</span>
<span class="p_del">-				break;</span>
<span class="p_del">-			}</span>
<span class="p_del">-		}</span>
<span class="p_add">+	page = dequeue_huge_page_nodemask(h, gfp_mask, nid, nodemask);</span>
<span class="p_add">+	if (page &amp;&amp; !avoid_reserve &amp;&amp; vma_has_reserves(vma, chg)) {</span>
<span class="p_add">+		SetPagePrivate(page);</span>
<span class="p_add">+		h-&gt;resv_huge_pages--;</span>
 	}
 
 	mpol_cond_put(mpol);
<span class="p_del">-	if (unlikely(!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie)))</span>
<span class="p_del">-		goto retry_cpuset;</span>
 	return page;
 
 err:
<span class="p_chunk">@@ -1633,7 +1632,7 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
 
 	spin_lock(&amp;hugetlb_lock);
 	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0)
<span class="p_del">-		page = dequeue_huge_page_node(h, nid);</span>
<span class="p_add">+		page = dequeue_huge_page_nodemask(h, gfp_mask, nid, NULL);</span>
 	spin_unlock(&amp;hugetlb_lock);
 
 	if (!page)
<span class="p_chunk">@@ -1642,26 +1641,27 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
 	return page;
 }
 
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask)</span>
<span class="p_add">+</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="p_add">+		nodemask_t *nmask)</span>
 {
 	gfp_t gfp_mask = htlb_alloc_mask(h);
<span class="p_del">-	struct page *page = NULL;</span>
<span class="p_del">-	int node;</span>
 
 	spin_lock(&amp;hugetlb_lock);
 	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {
<span class="p_del">-		for_each_node_mask(node, *nmask) {</span>
<span class="p_del">-			page = dequeue_huge_page_node_exact(h, node);</span>
<span class="p_del">-			if (page)</span>
<span class="p_del">-				break;</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+		page = dequeue_huge_page_nodemask(h, gfp_mask, preferred_nid, nmask);</span>
<span class="p_add">+		if (page) {</span>
<span class="p_add">+			spin_unlock(&amp;hugetlb_lock);</span>
<span class="p_add">+			return page;</span>
 		}
 	}
 	spin_unlock(&amp;hugetlb_lock);
<span class="p_del">-	if (page)</span>
<span class="p_del">-		return page;</span>
 
 	/* No reservations, try to overcommit */
<span class="p_del">-	return __alloc_buddy_huge_page(h, gfp_mask, NUMA_NO_NODE, nmask);</span>
<span class="p_add">+</span>
<span class="p_add">+	return __alloc_buddy_huge_page(h, gfp_mask, preferred_nid, nmask);</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



