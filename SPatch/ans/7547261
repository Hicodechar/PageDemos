
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/2] arm64: Allow changing of attributes outside of modules - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/2] arm64: Allow changing of attributes outside of modules</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=130331">Laura Abbott</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 3, 2015, 9:48 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1446587315-18145-3-git-send-email-labbott@fedoraproject.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7547261/mbox/"
   >mbox</a>
|
   <a href="/patch/7547261/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7547261/">/patch/7547261/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id D6FC59F36A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Nov 2015 21:49:02 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id DF65620792
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Nov 2015 21:49:01 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id A78B120797
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Nov 2015 21:49:00 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S964916AbbKCVsq (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 3 Nov 2015 16:48:46 -0500
Received: from mx1.redhat.com ([209.132.183.28]:41835 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932727AbbKCVsn (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 3 Nov 2015 16:48:43 -0500
Received: from int-mx11.intmail.prod.int.phx2.redhat.com
	(int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	by mx1.redhat.com (Postfix) with ESMTPS id 432408E3E9;
	Tue,  3 Nov 2015 21:48:43 +0000 (UTC)
Received: from labbott-redhat-machine.redhat.com
	(ovpn-112-84.phx2.redhat.com [10.3.112.84])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id tA3LmaWY003751; Tue, 3 Nov 2015 16:48:41 -0500
From: Laura Abbott &lt;labbott@fedoraproject.org&gt;
To: Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;
Cc: Laura Abbott &lt;labbott@fedoraproject.org&gt;,
	linux-arm-kernel@lists.infradead.org, linux-kernel@vger.kernel.org,
	Kees Cook &lt;keescook@chromium.org&gt;, Xishi Qiu &lt;qiuxishi@huawei.com&gt;,
	Ard Biesheuvel &lt;ard.biesheuvel@linaro.org&gt;,
	Mark Rutland &lt;mark.rutland@arm.com&gt;
Subject: [PATCH 2/2] arm64: Allow changing of attributes outside of modules
Date: Tue,  3 Nov 2015 13:48:35 -0800
Message-Id: &lt;1446587315-18145-3-git-send-email-labbott@fedoraproject.org&gt;
In-Reply-To: &lt;1446587315-18145-1-git-send-email-labbott@fedoraproject.org&gt;
References: &lt;1446587315-18145-1-git-send-email-labbott@fedoraproject.org&gt;
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130331">Laura Abbott</a> - Nov. 3, 2015, 9:48 p.m.</div>
<pre class="content">
Currently, the set_memory_* functions that are implemented for arm64
are restricted to module addresses only. This was mostly done
because arm64 maps normal zone memory with larger page sizes to
improve TLB performance. This has the side effect though of making it
difficult to adjust attributes at the PAGE_SIZE granularity. There are
an increasing number of use cases related to security where it is
necessary to change the attributes of kernel memory. Add functionality
to the page attribute changing code under a Kconfig to let systems
designers decide if they want to make the trade off of security for TLB
pressure.
<span class="signed-off-by">
Signed-off-by: Laura Abbott &lt;labbott@fedoraproject.org&gt;</span>
---
 arch/arm64/Kconfig.debug | 11 +++++++
 arch/arm64/mm/mm.h       |  3 ++
 arch/arm64/mm/mmu.c      |  2 +-
 arch/arm64/mm/pageattr.c | 74 ++++++++++++++++++++++++++++++++++++++++++++----
 4 files changed, 84 insertions(+), 6 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=146341">zhong jiang</a> - Nov. 4, 2015, 3:17 a.m.</div>
<pre class="content">
On 2015/11/4 5:48, Laura Abbott wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; Currently, the set_memory_* functions that are implemented for arm64</span>
<span class="quote">&gt; are restricted to module addresses only. This was mostly done</span>
<span class="quote">&gt; because arm64 maps normal zone memory with larger page sizes to</span>
<span class="quote">&gt; improve TLB performance. This has the side effect though of making it</span>
<span class="quote">&gt; difficult to adjust attributes at the PAGE_SIZE granularity. There are</span>
<span class="quote">&gt; an increasing number of use cases related to security where it is</span>
<span class="quote">&gt; necessary to change the attributes of kernel memory. Add functionality</span>
<span class="quote">&gt; to the page attribute changing code under a Kconfig to let systems</span>
<span class="quote">&gt; designers decide if they want to make the trade off of security for TLB</span>
<span class="quote">&gt; pressure.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Laura Abbott &lt;labbott@fedoraproject.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm64/Kconfig.debug | 11 +++++++</span>
<span class="quote">&gt;  arch/arm64/mm/mm.h       |  3 ++</span>
<span class="quote">&gt;  arch/arm64/mm/mmu.c      |  2 +-</span>
<span class="quote">&gt;  arch/arm64/mm/pageattr.c | 74 ++++++++++++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt;  4 files changed, 84 insertions(+), 6 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm64/Kconfig.debug b/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt; index d6285ef..abc6922 100644</span>
<span class="quote">&gt; --- a/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt; +++ b/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt; @@ -89,6 +89,17 @@ config DEBUG_ALIGN_RODATA</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	  If in doubt, say N</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config DEBUG_CHANGE_PAGEATTR</span>
<span class="quote">&gt; +	bool &quot;Allow all kernel memory to have attributes changed&quot;</span>
<span class="quote">&gt; +	help</span>
<span class="quote">&gt; +	  If this option is selected, APIs that change page attributes</span>
<span class="quote">&gt; +	  (RW &lt;-&gt; RO, X &lt;-&gt; NX) will be valid for all memory mapped in</span>
<span class="quote">&gt; +	  the kernel space. The trade off is that there may be increased</span>
<span class="quote">&gt; +	  TLB pressure from finer grained page mapping. Turn on this option</span>
<span class="quote">&gt; +	  if performance is more important than security</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  If in doubt, say N</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  source &quot;drivers/hwtracing/coresight/Kconfig&quot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  endmenu</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/mm.h b/arch/arm64/mm/mm.h</span>
<span class="quote">&gt; index ef47d99..7b0dcc4 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/mm.h</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/mm.h</span>
<span class="quote">&gt; @@ -1,3 +1,6 @@</span>
<span class="quote">&gt;  extern void __init bootmem_init(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void fixup_init(void);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void split_pud(pud_t *old_pud, pmd_t *pmd);</span>
<span class="quote">&gt; +void split_pmd(pmd_t *pmd, pte_t *pte);</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; index ff41efa..cefad2d 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; @@ -72,7 +72,7 @@ static void __init *early_alloc(unsigned long sz)</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * remap a PMD into pages</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static void split_pmd(pmd_t *pmd, pte_t *pte)</span>
<span class="quote">&gt; +void split_pmd(pmd_t *pmd, pte_t *pte)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long pfn = pmd_pfn(*pmd);</span>
<span class="quote">&gt;  	unsigned long addr = pfn &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; index e47ed1c..48a4ce9 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; @@ -15,9 +15,12 @@</span>
<span class="quote">&gt;  #include &lt;linux/module.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#include &quot;mm.h&quot;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  struct page_change_data {</span>
<span class="quote">&gt;  	pgprot_t set_mask;</span>
<span class="quote">&gt;  	pgprot_t clear_mask;</span>
<span class="quote">&gt; @@ -36,6 +39,66 @@ static int change_page_range(pte_t *ptep, pgtable_t token, unsigned long addr,</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_DEBUG_CHANGE_PAGEATTR</span>
<span class="quote">&gt; +static int check_address(unsigned long addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pgd_t *pgd = pgd_offset_k(addr);</span>
<span class="quote">&gt; +	pud_t *pud;</span>
<span class="quote">&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt; +	pte_t *pte;</span>
<span class="quote">&gt; +	int ret = -EFAULT;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (pgd_none(*pgd))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; +	if (pud_none(*pud))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (pud_sect(*pud)) {</span>
<span class="quote">&gt; +		pmd = pmd_alloc_one(&amp;init_mm, addr);</span>
<span class="quote">&gt; +		if (!pmd) {</span>
<span class="quote">&gt; +			ret = -ENOMEM;</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		split_pud(pud, pmd);</span>
<span class="quote">&gt; +		pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pmd = pmd_offset(pud, addr);</span>
<span class="quote">&gt; +	if (pmd_none(*pmd))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (pmd_sect(*pmd)) {</span>
<span class="quote">&gt; +		pte = pte_alloc_one_kernel(&amp;init_mm, addr);</span>
<span class="quote">&gt; +		if (!pte) {</span>
<span class="quote">&gt; +			ret = -ENOMEM;</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		split_pmd(pmd, pte);</span>
<span class="quote">&gt; +		__pmd_populate(pmd, __pa(pte), PMD_TYPE_TABLE);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pte = pte_offset_kernel(pmd, addr);</span>
<span class="quote">&gt; +	if (pte_none(*pte))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	flush_tlb_all();</span>
<span class="quote">&gt; +	ret = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static int check_address(unsigned long addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (addr &lt; MODULES_VADDR || addr &gt;= MODULES_END)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;  				pgprot_t set_mask, pgprot_t clear_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -45,17 +108,18 @@ static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  	struct page_change_data data;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (addr &lt; PAGE_OFFSET &amp;&amp; !is_vmalloc_addr((void *)addr))</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (!IS_ALIGNED(addr, PAGE_SIZE)) {</span>
<span class="quote">&gt;  		start &amp;= PAGE_MASK;</span>
<span class="quote">&gt;  		end = start + size;</span>
<span class="quote">&gt;  		WARN_ON_ONCE(1);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (start &lt; MODULES_VADDR || start &gt;= MODULES_END)</span>
<span class="quote">&gt; -		return -EINVAL;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (end &lt; MODULES_VADDR || end &gt;= MODULES_END)</span>
<span class="quote">&gt; -		return -EINVAL;</span>
<span class="quote">&gt; +	ret = check_address(addr);</span>
<span class="quote">&gt; +	if (ret)</span>
<span class="quote">&gt; +		return ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	data.set_mask = set_mask;</span>
<span class="quote">&gt;  	data.clear_mask = clear_mask;</span>

Hi Laura

This patch seems vaild, but I didn&#39;t feel very reasonable.
Because of the large page to make TLB performance better, just
split it if it is necessary.therefore, I think the first thing
we try to keep it, if they fail ,and then to split.

thanks
zhongjiang

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=66681">Ard Biesheuvel</a> - Nov. 5, 2015, 7:44 a.m.</div>
<pre class="content">
On 3 November 2015 at 22:48, Laura Abbott &lt;labbott@fedoraproject.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; Currently, the set_memory_* functions that are implemented for arm64</span>
<span class="quote">&gt; are restricted to module addresses only. This was mostly done</span>
<span class="quote">&gt; because arm64 maps normal zone memory with larger page sizes to</span>
<span class="quote">&gt; improve TLB performance. This has the side effect though of making it</span>
<span class="quote">&gt; difficult to adjust attributes at the PAGE_SIZE granularity. There are</span>
<span class="quote">&gt; an increasing number of use cases related to security where it is</span>
<span class="quote">&gt; necessary to change the attributes of kernel memory. Add functionality</span>
<span class="quote">&gt; to the page attribute changing code under a Kconfig to let systems</span>
<span class="quote">&gt; designers decide if they want to make the trade off of security for TLB</span>
<span class="quote">&gt; pressure.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Laura Abbott &lt;labbott@fedoraproject.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm64/Kconfig.debug | 11 +++++++</span>
<span class="quote">&gt;  arch/arm64/mm/mm.h       |  3 ++</span>
<span class="quote">&gt;  arch/arm64/mm/mmu.c      |  2 +-</span>
<span class="quote">&gt;  arch/arm64/mm/pageattr.c | 74 ++++++++++++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt;  4 files changed, 84 insertions(+), 6 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/arm64/Kconfig.debug b/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt; index d6285ef..abc6922 100644</span>
<span class="quote">&gt; --- a/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt; +++ b/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt; @@ -89,6 +89,17 @@ config DEBUG_ALIGN_RODATA</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;           If in doubt, say N</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +config DEBUG_CHANGE_PAGEATTR</span>

I don&#39;t think this belongs in Kconfig.debug, and I don&#39;t think this
should default to off.

We know we currently have no users of set_memory_xx() in arch/arm64
that operate on linear mapping addresses, so we will not introduce any
performance regressions by adding this functionality now. By putting
this feature behind a debug option, every newly introduced call
set_memory_xx() that operates on linear or vmalloc() addresses needs
to deal with -EINVAL (or depend on DEBUG_CHANGE_PAGEATTR), or it may
error out at runtime if the feature is not enabled.
<span class="quote">
&gt; +       bool &quot;Allow all kernel memory to have attributes changed&quot;</span>
<span class="quote">&gt; +       help</span>
<span class="quote">&gt; +         If this option is selected, APIs that change page attributes</span>
<span class="quote">&gt; +         (RW &lt;-&gt; RO, X &lt;-&gt; NX) will be valid for all memory mapped in</span>
<span class="quote">&gt; +         the kernel space. The trade off is that there may be increased</span>
<span class="quote">&gt; +         TLB pressure from finer grained page mapping. Turn on this option</span>
<span class="quote">&gt; +         if performance is more important than security</span>
<span class="quote">&gt; +</span>

This is backwards
<span class="quote">
&gt; +         If in doubt, say N</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  source &quot;drivers/hwtracing/coresight/Kconfig&quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  endmenu</span>

[...]
<span class="quote">
&gt; diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; index e47ed1c..48a4ce9 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/pageattr.c</span>

[...]
<span class="quote">
&gt; @@ -45,17 +108,18 @@ static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;         int ret;</span>
<span class="quote">&gt;         struct page_change_data data;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +       if (addr &lt; PAGE_OFFSET &amp;&amp; !is_vmalloc_addr((void *)addr))</span>
<span class="quote">&gt; +               return -EINVAL;</span>
<span class="quote">&gt; +</span>

Doesn&#39;t this exclude the module area?
<span class="quote">
&gt;         if (!IS_ALIGNED(addr, PAGE_SIZE)) {</span>
<span class="quote">&gt;                 start &amp;= PAGE_MASK;</span>
<span class="quote">&gt;                 end = start + size;</span>
<span class="quote">&gt;                 WARN_ON_ONCE(1);</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       if (start &lt; MODULES_VADDR || start &gt;= MODULES_END)</span>
<span class="quote">&gt; -               return -EINVAL;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -       if (end &lt; MODULES_VADDR || end &gt;= MODULES_END)</span>
<span class="quote">&gt; -               return -EINVAL;</span>
<span class="quote">&gt; +       ret = check_address(addr);</span>
<span class="quote">&gt; +       if (ret)</span>
<span class="quote">&gt; +               return ret;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         data.set_mask = set_mask;</span>
<span class="quote">&gt;         data.clear_mask = clear_mask;</span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; 2.4.3</span>
<span class="quote">&gt;</span>
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55031">Xishi Qiu</a> - Nov. 5, 2015, 11:10 a.m.</div>
<pre class="content">
On 2015/11/5 2:12, Laura Abbott wrote:
<span class="quote">
&gt; On 11/03/2015 06:59 PM, zhong jiang wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Hi Laura</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch seems vaild, but I didn&#39;t feel very reasonable.</span>
<span class="quote">&gt;&gt; Because of the large page to make TLB performance better, just</span>
<span class="quote">&gt;&gt; split it if it is necessary.therefore, I think the first thing</span>
<span class="quote">&gt;&gt; we try to keep it, if they fail ,and then to split.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not quite sure I understand the request. We know we are going</span>
<span class="quote">&gt; to have to have something mapped at page size granularity so we</span>
<span class="quote">&gt; are going to have to break down the larger mappings no matter</span>
<span class="quote">&gt; what. Can you explain a bit more where you think we could try to</span>
<span class="quote">&gt; keep the larger mappings?</span>
<span class="quote">&gt; </span>

Hi Laura,

He means like this, if the range is aligned with large page, we
need not to split it, just change the flag.

I have one more question.

alloc_init_pud()
	...
	if (!pud_none(old_pud))
		...
		memblock_free(table, PAGE_SIZE);
		...

Here we will free the memory from pmd page, so why not free
more memory from 512 pte pages, if the 512 old pmds are not none?

Thanks,
Xishi Qiu
<span class="quote">
&gt; At least two things I noticed looking at this again though:</span>
<span class="quote">&gt; - This only splits the start address. If the range happens</span>
<span class="quote">&gt; to overlap a pud/pmd this won&#39;t work. I&#39;ll address that in v2</span>
<span class="quote">&gt; - We&#39;re always flushing the TLB even if nothing changed. Was</span>
<span class="quote">&gt; this what you were referring to?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;&gt; thanks</span>
<span class="quote">&gt;&gt; zhongjiang</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Laura</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; .</span>
<span class="quote">&gt; </span>



--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55031">Xishi Qiu</a> - Nov. 5, 2015, 11:29 a.m.</div>
<pre class="content">
On 2015/11/4 5:48, Laura Abbott wrote:
<span class="quote">
&gt; </span>
<span class="quote">&gt; Currently, the set_memory_* functions that are implemented for arm64</span>
<span class="quote">&gt; are restricted to module addresses only. This was mostly done</span>
<span class="quote">&gt; because arm64 maps normal zone memory with larger page sizes to</span>
<span class="quote">&gt; improve TLB performance. This has the side effect though of making it</span>
<span class="quote">&gt; difficult to adjust attributes at the PAGE_SIZE granularity. There are</span>
<span class="quote">&gt; an increasing number of use cases related to security where it is</span>
<span class="quote">&gt; necessary to change the attributes of kernel memory. Add functionality</span>
<span class="quote">&gt; to the page attribute changing code under a Kconfig to let systems</span>
<span class="quote">&gt; designers decide if they want to make the trade off of security for TLB</span>
<span class="quote">&gt; pressure.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Laura Abbott &lt;labbott@fedoraproject.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm64/Kconfig.debug | 11 +++++++</span>
<span class="quote">&gt;  arch/arm64/mm/mm.h       |  3 ++</span>
<span class="quote">&gt;  arch/arm64/mm/mmu.c      |  2 +-</span>
<span class="quote">&gt;  arch/arm64/mm/pageattr.c | 74 ++++++++++++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt;  4 files changed, 84 insertions(+), 6 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm64/Kconfig.debug b/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt; index d6285ef..abc6922 100644</span>
<span class="quote">&gt; --- a/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt; +++ b/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt; @@ -89,6 +89,17 @@ config DEBUG_ALIGN_RODATA</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	  If in doubt, say N</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config DEBUG_CHANGE_PAGEATTR</span>
<span class="quote">&gt; +	bool &quot;Allow all kernel memory to have attributes changed&quot;</span>
<span class="quote">&gt; +	help</span>
<span class="quote">&gt; +	  If this option is selected, APIs that change page attributes</span>
<span class="quote">&gt; +	  (RW &lt;-&gt; RO, X &lt;-&gt; NX) will be valid for all memory mapped in</span>
<span class="quote">&gt; +	  the kernel space. The trade off is that there may be increased</span>
<span class="quote">&gt; +	  TLB pressure from finer grained page mapping. Turn on this option</span>
<span class="quote">&gt; +	  if performance is more important than security</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  If in doubt, say N</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  source &quot;drivers/hwtracing/coresight/Kconfig&quot;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  endmenu</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/mm.h b/arch/arm64/mm/mm.h</span>
<span class="quote">&gt; index ef47d99..7b0dcc4 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/mm.h</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/mm.h</span>
<span class="quote">&gt; @@ -1,3 +1,6 @@</span>
<span class="quote">&gt;  extern void __init bootmem_init(void);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void fixup_init(void);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void split_pud(pud_t *old_pud, pmd_t *pmd);</span>
<span class="quote">&gt; +void split_pmd(pmd_t *pmd, pte_t *pte);</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; index ff41efa..cefad2d 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/mmu.c</span>
<span class="quote">&gt; @@ -72,7 +72,7 @@ static void __init *early_alloc(unsigned long sz)</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * remap a PMD into pages</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -static void split_pmd(pmd_t *pmd, pte_t *pte)</span>
<span class="quote">&gt; +void split_pmd(pmd_t *pmd, pte_t *pte)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long pfn = pmd_pfn(*pmd);</span>
<span class="quote">&gt;  	unsigned long addr = pfn &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; index e47ed1c..48a4ce9 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt; @@ -15,9 +15,12 @@</span>
<span class="quote">&gt;  #include &lt;linux/module.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/sched.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/pgtable.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#include &quot;mm.h&quot;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  struct page_change_data {</span>
<span class="quote">&gt;  	pgprot_t set_mask;</span>
<span class="quote">&gt;  	pgprot_t clear_mask;</span>
<span class="quote">&gt; @@ -36,6 +39,66 @@ static int change_page_range(pte_t *ptep, pgtable_t token, unsigned long addr,</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_DEBUG_CHANGE_PAGEATTR</span>
<span class="quote">&gt; +static int check_address(unsigned long addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	pgd_t *pgd = pgd_offset_k(addr);</span>
<span class="quote">&gt; +	pud_t *pud;</span>
<span class="quote">&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt; +	pte_t *pte;</span>
<span class="quote">&gt; +	int ret = -EFAULT;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (pgd_none(*pgd))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pud = pud_offset(pgd, addr);</span>
<span class="quote">&gt; +	if (pud_none(*pud))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (pud_sect(*pud)) {</span>
<span class="quote">&gt; +		pmd = pmd_alloc_one(&amp;init_mm, addr);</span>
<span class="quote">&gt; +		if (!pmd) {</span>
<span class="quote">&gt; +			ret = -ENOMEM;</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		split_pud(pud, pmd);</span>
<span class="quote">&gt; +		pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pmd = pmd_offset(pud, addr);</span>
<span class="quote">&gt; +	if (pmd_none(*pmd))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (pmd_sect(*pmd)) {</span>
<span class="quote">&gt; +		pte = pte_alloc_one_kernel(&amp;init_mm, addr);</span>
<span class="quote">&gt; +		if (!pte) {</span>
<span class="quote">&gt; +			ret = -ENOMEM;</span>
<span class="quote">&gt; +			goto out;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		split_pmd(pmd, pte);</span>
<span class="quote">&gt; +		__pmd_populate(pmd, __pa(pte), PMD_TYPE_TABLE);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pte = pte_offset_kernel(pmd, addr);</span>
<span class="quote">&gt; +	if (pte_none(*pte))</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	flush_tlb_all();</span>
<span class="quote">&gt; +	ret = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static int check_address(unsigned long addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (addr &lt; MODULES_VADDR || addr &gt;= MODULES_END)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;  				pgprot_t set_mask, pgprot_t clear_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -45,17 +108,18 @@ static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt;  	struct page_change_data data;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (addr &lt; PAGE_OFFSET &amp;&amp; !is_vmalloc_addr((void *)addr))</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (!IS_ALIGNED(addr, PAGE_SIZE)) {</span>
<span class="quote">&gt;  		start &amp;= PAGE_MASK;</span>
<span class="quote">&gt;  		end = start + size;</span>
<span class="quote">&gt;  		WARN_ON_ONCE(1);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (start &lt; MODULES_VADDR || start &gt;= MODULES_END)</span>
<span class="quote">&gt; -		return -EINVAL;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (end &lt; MODULES_VADDR || end &gt;= MODULES_END)</span>
<span class="quote">&gt; -		return -EINVAL;</span>
<span class="quote">&gt; +	ret = check_address(addr);</span>

Hi Laura,

We only split the first page here, if the numpages is not 1, it&#39;s
wrong, right?

Thanks,
Xishi Qiu
<span class="quote">
&gt; +	if (ret)</span>
<span class="quote">&gt; +		return ret;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	data.set_mask = set_mask;</span>
<span class="quote">&gt;  	data.clear_mask = clear_mask;</span>



--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - Nov. 6, 2015, 1:11 a.m.</div>
<pre class="content">
On 11/05/2015 03:10 AM, Xishi Qiu wrote:
<span class="quote">&gt; On 2015/11/5 2:12, Laura Abbott wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; On 11/03/2015 06:59 PM, zhong jiang wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Hi Laura</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This patch seems vaild, but I didn&#39;t feel very reasonable.</span>
<span class="quote">&gt;&gt;&gt; Because of the large page to make TLB performance better, just</span>
<span class="quote">&gt;&gt;&gt; split it if it is necessary.therefore, I think the first thing</span>
<span class="quote">&gt;&gt;&gt; we try to keep it, if they fail ,and then to split.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;m not quite sure I understand the request. We know we are going</span>
<span class="quote">&gt;&gt; to have to have something mapped at page size granularity so we</span>
<span class="quote">&gt;&gt; are going to have to break down the larger mappings no matter</span>
<span class="quote">&gt;&gt; what. Can you explain a bit more where you think we could try to</span>
<span class="quote">&gt;&gt; keep the larger mappings?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hi Laura,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; He means like this, if the range is aligned with large page, we</span>
<span class="quote">&gt; need not to split it, just change the flag.</span>
<span class="quote">&gt;</span>

This complicates the logic for doing the update. Apply to page range
nicely walks across all the 4K pages and does the update. It looks
like x86 does the check to keep the large pages though so I&#39;ll
give it some thought.
<span class="quote">
&gt; I have one more question.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; alloc_init_pud()</span>
<span class="quote">&gt; 	...</span>
<span class="quote">&gt; 	if (!pud_none(old_pud))</span>
<span class="quote">&gt; 		...</span>
<span class="quote">&gt; 		memblock_free(table, PAGE_SIZE);</span>
<span class="quote">&gt; 		...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Here we will free the memory from pmd page, so why not free</span>
<span class="quote">&gt; more memory from 512 pte pages, if the 512 old pmds are not none?</span>
<span class="quote">&gt;</span>

It would be nice to reclaim the memory but I&#39;m not sure if that will
work. The memory was allocated before any of the regular kernel data
structures were set up. It&#39;s not clear if giving the pages back to
the buddy allocator would actually work. I&#39;ll take a look though.
<span class="quote">  
&gt; Thanks,</span>
<span class="quote">&gt; Xishi Qiu</span>
<span class="quote">&gt;</span>

Thanks,
Laura

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - Nov. 6, 2015, 1:35 a.m.</div>
<pre class="content">
On 11/04/2015 11:44 PM, Ard Biesheuvel wrote:
<span class="quote">&gt; On 3 November 2015 at 22:48, Laura Abbott &lt;labbott@fedoraproject.org&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Currently, the set_memory_* functions that are implemented for arm64</span>
<span class="quote">&gt;&gt; are restricted to module addresses only. This was mostly done</span>
<span class="quote">&gt;&gt; because arm64 maps normal zone memory with larger page sizes to</span>
<span class="quote">&gt;&gt; improve TLB performance. This has the side effect though of making it</span>
<span class="quote">&gt;&gt; difficult to adjust attributes at the PAGE_SIZE granularity. There are</span>
<span class="quote">&gt;&gt; an increasing number of use cases related to security where it is</span>
<span class="quote">&gt;&gt; necessary to change the attributes of kernel memory. Add functionality</span>
<span class="quote">&gt;&gt; to the page attribute changing code under a Kconfig to let systems</span>
<span class="quote">&gt;&gt; designers decide if they want to make the trade off of security for TLB</span>
<span class="quote">&gt;&gt; pressure.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Laura Abbott &lt;labbott@fedoraproject.org&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;   arch/arm64/Kconfig.debug | 11 +++++++</span>
<span class="quote">&gt;&gt;   arch/arm64/mm/mm.h       |  3 ++</span>
<span class="quote">&gt;&gt;   arch/arm64/mm/mmu.c      |  2 +-</span>
<span class="quote">&gt;&gt;   arch/arm64/mm/pageattr.c | 74 ++++++++++++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt;&gt;   4 files changed, 84 insertions(+), 6 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/Kconfig.debug b/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt;&gt; index d6285ef..abc6922 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/Kconfig.debug</span>
<span class="quote">&gt;&gt; @@ -89,6 +89,17 @@ config DEBUG_ALIGN_RODATA</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;            If in doubt, say N</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +config DEBUG_CHANGE_PAGEATTR</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I don&#39;t think this belongs in Kconfig.debug, and I don&#39;t think this</span>
<span class="quote">&gt; should default to off.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We know we currently have no users of set_memory_xx() in arch/arm64</span>
<span class="quote">&gt; that operate on linear mapping addresses, so we will not introduce any</span>
<span class="quote">&gt; performance regressions by adding this functionality now. By putting</span>
<span class="quote">&gt; this feature behind a debug option, every newly introduced call</span>
<span class="quote">&gt; set_memory_xx() that operates on linear or vmalloc() addresses needs</span>
<span class="quote">&gt; to deal with -EINVAL (or depend on DEBUG_CHANGE_PAGEATTR), or it may</span>
<span class="quote">&gt; error out at runtime if the feature is not enabled.</span>
<span class="quote">&gt;</span>

I stuck it in Kconfig.debug to have it match with the rest of the
module and DEBUG_RODATA options. I&#39;ll pull it out.
<span class="quote">  
&gt;&gt; +       bool &quot;Allow all kernel memory to have attributes changed&quot;</span>
<span class="quote">&gt;&gt; +       help</span>
<span class="quote">&gt;&gt; +         If this option is selected, APIs that change page attributes</span>
<span class="quote">&gt;&gt; +         (RW &lt;-&gt; RO, X &lt;-&gt; NX) will be valid for all memory mapped in</span>
<span class="quote">&gt;&gt; +         the kernel space. The trade off is that there may be increased</span>
<span class="quote">&gt;&gt; +         TLB pressure from finer grained page mapping. Turn on this option</span>
<span class="quote">&gt;&gt; +         if performance is more important than security</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is backwards</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +         If in doubt, say N</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;   source &quot;drivers/hwtracing/coresight/Kconfig&quot;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   endmenu</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt;&gt; index e47ed1c..48a4ce9 100644</span>
<span class="quote">&gt;&gt; --- a/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt;&gt; +++ b/arch/arm64/mm/pageattr.c</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; @@ -45,17 +108,18 @@ static int change_memory_common(unsigned long addr, int numpages,</span>
<span class="quote">&gt;&gt;          int ret;</span>
<span class="quote">&gt;&gt;          struct page_change_data data;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +       if (addr &lt; PAGE_OFFSET &amp;&amp; !is_vmalloc_addr((void *)addr))</span>
<span class="quote">&gt;&gt; +               return -EINVAL;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Doesn&#39;t this exclude the module area?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;          if (!IS_ALIGNED(addr, PAGE_SIZE)) {</span>
<span class="quote">&gt;&gt;                  start &amp;= PAGE_MASK;</span>
<span class="quote">&gt;&gt;                  end = start + size;</span>
<span class="quote">&gt;&gt;                  WARN_ON_ONCE(1);</span>
<span class="quote">&gt;&gt;          }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -       if (start &lt; MODULES_VADDR || start &gt;= MODULES_END)</span>
<span class="quote">&gt;&gt; -               return -EINVAL;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -       if (end &lt; MODULES_VADDR || end &gt;= MODULES_END)</span>
<span class="quote">&gt;&gt; -               return -EINVAL;</span>
<span class="quote">&gt;&gt; +       ret = check_address(addr);</span>
<span class="quote">&gt;&gt; +       if (ret)</span>
<span class="quote">&gt;&gt; +               return ret;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;          data.set_mask = set_mask;</span>
<span class="quote">&gt;&gt;          data.clear_mask = clear_mask;</span>
<span class="quote">&gt;&gt; --</span>
<span class="quote">&gt;&gt; 2.4.3</span>
<span class="quote">&gt;&gt;</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/Kconfig.debug b/arch/arm64/Kconfig.debug</span>
<span class="p_header">index d6285ef..abc6922 100644</span>
<span class="p_header">--- a/arch/arm64/Kconfig.debug</span>
<span class="p_header">+++ b/arch/arm64/Kconfig.debug</span>
<span class="p_chunk">@@ -89,6 +89,17 @@</span> <span class="p_context"> config DEBUG_ALIGN_RODATA</span>
 
 	  If in doubt, say N
 
<span class="p_add">+config DEBUG_CHANGE_PAGEATTR</span>
<span class="p_add">+	bool &quot;Allow all kernel memory to have attributes changed&quot;</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  If this option is selected, APIs that change page attributes</span>
<span class="p_add">+	  (RW &lt;-&gt; RO, X &lt;-&gt; NX) will be valid for all memory mapped in</span>
<span class="p_add">+	  the kernel space. The trade off is that there may be increased</span>
<span class="p_add">+	  TLB pressure from finer grained page mapping. Turn on this option</span>
<span class="p_add">+	  if performance is more important than security</span>
<span class="p_add">+</span>
<span class="p_add">+	  If in doubt, say N</span>
<span class="p_add">+</span>
 source &quot;drivers/hwtracing/coresight/Kconfig&quot;
 
 endmenu
<span class="p_header">diff --git a/arch/arm64/mm/mm.h b/arch/arm64/mm/mm.h</span>
<span class="p_header">index ef47d99..7b0dcc4 100644</span>
<span class="p_header">--- a/arch/arm64/mm/mm.h</span>
<span class="p_header">+++ b/arch/arm64/mm/mm.h</span>
<span class="p_chunk">@@ -1,3 +1,6 @@</span> <span class="p_context"></span>
 extern void __init bootmem_init(void);
 
 void fixup_init(void);
<span class="p_add">+</span>
<span class="p_add">+void split_pud(pud_t *old_pud, pmd_t *pmd);</span>
<span class="p_add">+void split_pmd(pmd_t *pmd, pte_t *pte);</span>
<span class="p_header">diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="p_header">index ff41efa..cefad2d 100644</span>
<span class="p_header">--- a/arch/arm64/mm/mmu.c</span>
<span class="p_header">+++ b/arch/arm64/mm/mmu.c</span>
<span class="p_chunk">@@ -72,7 +72,7 @@</span> <span class="p_context"> static void __init *early_alloc(unsigned long sz)</span>
 /*
  * remap a PMD into pages
  */
<span class="p_del">-static void split_pmd(pmd_t *pmd, pte_t *pte)</span>
<span class="p_add">+void split_pmd(pmd_t *pmd, pte_t *pte)</span>
 {
 	unsigned long pfn = pmd_pfn(*pmd);
 	unsigned long addr = pfn &lt;&lt; PAGE_SHIFT;
<span class="p_header">diff --git a/arch/arm64/mm/pageattr.c b/arch/arm64/mm/pageattr.c</span>
<span class="p_header">index e47ed1c..48a4ce9 100644</span>
<span class="p_header">--- a/arch/arm64/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/arm64/mm/pageattr.c</span>
<span class="p_chunk">@@ -15,9 +15,12 @@</span> <span class="p_context"></span>
 #include &lt;linux/module.h&gt;
 #include &lt;linux/sched.h&gt;
 
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/tlbflush.h&gt;
 
<span class="p_add">+#include &quot;mm.h&quot;</span>
<span class="p_add">+</span>
 struct page_change_data {
 	pgprot_t set_mask;
 	pgprot_t clear_mask;
<span class="p_chunk">@@ -36,6 +39,66 @@</span> <span class="p_context"> static int change_page_range(pte_t *ptep, pgtable_t token, unsigned long addr,</span>
 	return 0;
 }
 
<span class="p_add">+#ifdef CONFIG_DEBUG_CHANGE_PAGEATTR</span>
<span class="p_add">+static int check_address(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd = pgd_offset_k(addr);</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	int ret = -EFAULT;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd_none(*pgd))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(pgd, addr);</span>
<span class="p_add">+	if (pud_none(*pud))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pud_sect(*pud)) {</span>
<span class="p_add">+		pmd = pmd_alloc_one(&amp;init_mm, addr);</span>
<span class="p_add">+		if (!pmd) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		split_pud(pud, pmd);</span>
<span class="p_add">+		pud_populate(&amp;init_mm, pud, pmd);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+	if (pmd_none(*pmd))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_sect(*pmd)) {</span>
<span class="p_add">+		pte = pte_alloc_one_kernel(&amp;init_mm, addr);</span>
<span class="p_add">+		if (!pte) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		split_pmd(pmd, pte);</span>
<span class="p_add">+		__pmd_populate(pmd, __pa(pte), PMD_TYPE_TABLE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_kernel(pmd, addr);</span>
<span class="p_add">+	if (pte_none(*pte))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+	ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static int check_address(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (addr &lt; MODULES_VADDR || addr &gt;= MODULES_END)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 static int change_memory_common(unsigned long addr, int numpages,
 				pgprot_t set_mask, pgprot_t clear_mask)
 {
<span class="p_chunk">@@ -45,17 +108,18 @@</span> <span class="p_context"> static int change_memory_common(unsigned long addr, int numpages,</span>
 	int ret;
 	struct page_change_data data;
 
<span class="p_add">+	if (addr &lt; PAGE_OFFSET &amp;&amp; !is_vmalloc_addr((void *)addr))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	if (!IS_ALIGNED(addr, PAGE_SIZE)) {
 		start &amp;= PAGE_MASK;
 		end = start + size;
 		WARN_ON_ONCE(1);
 	}
 
<span class="p_del">-	if (start &lt; MODULES_VADDR || start &gt;= MODULES_END)</span>
<span class="p_del">-		return -EINVAL;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (end &lt; MODULES_VADDR || end &gt;= MODULES_END)</span>
<span class="p_del">-		return -EINVAL;</span>
<span class="p_add">+	ret = check_address(addr);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
 
 	data.set_mask = set_mask;
 	data.clear_mask = clear_mask;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



