
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/3] tree wide: get rid of __GFP_REPEAT for order-0 allocations part I - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/3] tree wide: get rid of __GFP_REPEAT for order-0 allocations part I</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 5, 2015, 4:15 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1446740160-29094-2-git-send-email-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7562611/mbox/"
   >mbox</a>
|
   <a href="/patch/7562611/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7562611/">/patch/7562611/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 0B7EB9F65E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  5 Nov 2015 16:16:38 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 3BD51205EB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  5 Nov 2015 16:16:36 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 3D66A20828
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  5 Nov 2015 16:16:34 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756619AbbKEQQW (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 5 Nov 2015 11:16:22 -0500
Received: from mail-wm0-f52.google.com ([74.125.82.52]:36022 &quot;EHLO
	mail-wm0-f52.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1033092AbbKEQQS (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 5 Nov 2015 11:16:18 -0500
Received: by wmww144 with SMTP id w144so11027355wmw.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 05 Nov 2015 08:16:17 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=GefGpaTb8z/3O+TfIilcbAOyYDIX84Zy4QBeTAwyKSQ=;
	b=EsNa3ReX5jVzP+27FR96YwIKPa4x1lt9meWnC4+ZEQcqvf+ZrKgb1s/NxkXCqCquV+
	cRFCxwIgU2AhhZdd/aR+da3NFPoWKXCj+Kk0VFjIsMNf1A7pGpLLh/fdY6UIkmyFoXMI
	I3oSFdkNdMbYE/qJTO0gkP2PZk9ySGwzWZUbTMtsUUuRtvp1U5Y9aVTyvk0KAMuEr5Nl
	PcsGkIM/gNt7pSvf8GR2n+0s5l+RcPFqu/NahKgrcjdOa/EvYoe+L/iqEhtGqR5R/xGA
	9phwjF3rrZIOvMhShEo0UelleWQSrh7GwHPEAU/8EmwDAOS2UqSdCsV4EyRnLRFJdikl
	sAJg==
X-Received: by 10.28.21.134 with SMTP id 128mr4739517wmv.29.1446740177430;
	Thu, 05 Nov 2015 08:16:17 -0800 (PST)
Received: from tiehlicka.suse.cz (nat1.scz.suse.com. [213.151.88.250])
	by smtp.gmail.com with ESMTPSA id
	79sm9151592wmf.13.2015.11.05.08.16.16
	(version=TLSv1.2 cipher=ECDHE-RSA-AES128-SHA bits=128/128);
	Thu, 05 Nov 2015 08:16:16 -0800 (PST)
From: mhocko@kernel.org
To: &lt;linux-mm@kvack.org&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [PATCH 1/3] tree wide: get rid of __GFP_REPEAT for order-0
	allocations part I
Date: Thu,  5 Nov 2015 17:15:58 +0100
Message-Id: &lt;1446740160-29094-2-git-send-email-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.6.1
In-Reply-To: &lt;1446740160-29094-1-git-send-email-mhocko@kernel.org&gt;
References: &lt;1446740160-29094-1-git-send-email-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 5, 2015, 4:15 p.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

__GFP_REPEAT has a rather weak semantic but since it has been introduced
around 2.6.12 it has been ignored for low order allocations. Yet we have
the full kernel tree with its usage for apparently order-0 allocations.
This is really confusing because __GFP_REPEAT is explicitly documented
to allow allocation failures which is a weaker semantic than the current
order-0 has (basically nofail).

Let&#39;s simply reap out __GFP_REPEAT from those places. This would allow
to identify place which really need allocator to retry harder and
formulate a more specific semantic for what the flag is supposed to do
actually.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 arch/alpha/include/asm/pgalloc.h         | 4 ++--
 arch/arm/include/asm/pgalloc.h           | 2 +-
 arch/avr32/include/asm/pgalloc.h         | 6 +++---
 arch/cris/include/asm/pgalloc.h          | 4 ++--
 arch/frv/mm/pgalloc.c                    | 6 +++---
 arch/hexagon/include/asm/pgalloc.h       | 4 ++--
 arch/m68k/include/asm/mcf_pgalloc.h      | 4 ++--
 arch/m68k/include/asm/motorola_pgalloc.h | 4 ++--
 arch/m68k/include/asm/sun3_pgalloc.h     | 4 ++--
 arch/metag/include/asm/pgalloc.h         | 5 ++---
 arch/microblaze/include/asm/pgalloc.h    | 4 ++--
 arch/microblaze/mm/pgtable.c             | 3 +--
 arch/mn10300/mm/pgtable.c                | 6 +++---
 arch/openrisc/include/asm/pgalloc.h      | 2 +-
 arch/openrisc/mm/ioremap.c               | 2 +-
 arch/parisc/include/asm/pgalloc.h        | 4 ++--
 arch/powerpc/include/asm/pgalloc-64.h    | 2 +-
 arch/powerpc/mm/pgtable_32.c             | 4 ++--
 arch/powerpc/mm/pgtable_64.c             | 3 +--
 arch/s390/mm/pgtable.c                   | 2 +-
 arch/sh/include/asm/pgalloc.h            | 4 ++--
 arch/sparc/mm/init_64.c                  | 6 ++----
 arch/um/kernel/mem.c                     | 4 ++--
 arch/x86/include/asm/pgalloc.h           | 4 ++--
 arch/x86/xen/p2m.c                       | 2 +-
 arch/xtensa/include/asm/pgalloc.h        | 2 +-
 drivers/block/aoe/aoecmd.c               | 2 +-
 27 files changed, 47 insertions(+), 52 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Nov. 9, 2015, 10:04 p.m.</div>
<pre class="content">
On 5.11.2015 17:15, mhocko@kernel.org wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; __GFP_REPEAT has a rather weak semantic but since it has been introduced</span>
<span class="quote">&gt; around 2.6.12 it has been ignored for low order allocations. Yet we have</span>
<span class="quote">&gt; the full kernel tree with its usage for apparently order-0 allocations.</span>
<span class="quote">&gt; This is really confusing because __GFP_REPEAT is explicitly documented</span>
<span class="quote">&gt; to allow allocation failures which is a weaker semantic than the current</span>
<span class="quote">&gt; order-0 has (basically nofail).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Let&#39;s simply reap out __GFP_REPEAT from those places. This would allow</span>
<span class="quote">&gt; to identify place which really need allocator to retry harder and</span>
<span class="quote">&gt; formulate a more specific semantic for what the flag is supposed to do</span>
<span class="quote">&gt; actually.</span>

So at first I thought &quot;yeah that&#39;s obvious&quot;, but then after some more thinking,
I&#39;m not so sure anymore.

I think we should formulate the semantic first, then do any changes. Also, let&#39;s
look at the flag description (which comes from pre-git):

 * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt
 * _might_ fail.  This depends upon the particular VM implementation.

So we say it&#39;s implementation detail, and IIRC the same is said about which
orders are considered costly and which not, and the associated rules. So, can we
blame callers that happen to use __GFP_REPEAT essentially as a no-op in the
current implementation? And is it a problem that they do that?

So I think we should answer the following questions:

* What is the semantic of __GFP_REPEAT?
  - My suggestion would be something like &quot;I would really like this allocation
to succeed. I still have some fallback but it&#39;s so suboptimal I&#39;d rather wait
for this allocation.&quot; And then we could e.g. change some heuristics to take that
into account - e.g. direct compaction could ignore the deferred state and
pageblock skip bits, to make sure it&#39;s as thorough as possible. Right now, that
sort of happens, but not quite - given enough reclaim/compact attempts, the
compact attempts might break out of deferred state. But pages_reclaimed might
reach 1 &lt;&lt; order before compaction &quot;undefers&quot;, and then it breaks out of the
loop. Is any such heuristic change possible for reclaim as well?
As part of this question we should also keep in mind/rethink __GFP_NORETRY as
that&#39;s supposed to be the opposite flag to __GFP_REPEAT.

* Can it ever happen that __GFP_REPEAT could make some difference for order-0?
  - Certainly not wrt compaction, how about reclaim?
  - If it couldn&#39;t possibly affect order-0, then yeah proceed with Patch 1.

* Is PAGE_ALLOC_COSTLY_ORDER considered an implementation detail?
  - I would think so, and if yes, then we probably shouldn&#39;t remove
__GFP_NORETRY for order-1+ allocations that happen to be not costly in the
current implementation?




--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 10, 2015, 12:51 p.m.</div>
<pre class="content">
On Mon 09-11-15 23:04:15, Vlastimil Babka wrote:
<span class="quote">&gt; On 5.11.2015 17:15, mhocko@kernel.org wrote:</span>
<span class="quote">&gt; &gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; __GFP_REPEAT has a rather weak semantic but since it has been introduced</span>
<span class="quote">&gt; &gt; around 2.6.12 it has been ignored for low order allocations. Yet we have</span>
<span class="quote">&gt; &gt; the full kernel tree with its usage for apparently order-0 allocations.</span>
<span class="quote">&gt; &gt; This is really confusing because __GFP_REPEAT is explicitly documented</span>
<span class="quote">&gt; &gt; to allow allocation failures which is a weaker semantic than the current</span>
<span class="quote">&gt; &gt; order-0 has (basically nofail).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Let&#39;s simply reap out __GFP_REPEAT from those places. This would allow</span>
<span class="quote">&gt; &gt; to identify place which really need allocator to retry harder and</span>
<span class="quote">&gt; &gt; formulate a more specific semantic for what the flag is supposed to do</span>
<span class="quote">&gt; &gt; actually.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So at first I thought &quot;yeah that&#39;s obvious&quot;, but then after some more thinking,</span>
<span class="quote">&gt; I&#39;m not so sure anymore.</span>

Thanks for looking into this! The primary purpose of this patch series was
to start the discussion. I&#39;ve only now realized I forgot to add RFC, sorry
about that.
<span class="quote">
&gt; I think we should formulate the semantic first, then do any changes. Also, let&#39;s</span>
<span class="quote">&gt; look at the flag description (which comes from pre-git):</span>

It&#39;s rather hard to formulate one without examining the current users...
<span class="quote">
&gt;  * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt</span>
<span class="quote">&gt;  * _might_ fail.  This depends upon the particular VM implementation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So we say it&#39;s implementation detail, and IIRC the same is said about which</span>
<span class="quote">&gt; orders are considered costly and which not, and the associated rules. So, can we</span>
<span class="quote">&gt; blame callers that happen to use __GFP_REPEAT essentially as a no-op in the</span>
<span class="quote">&gt; current implementation? And is it a problem that they do that?</span>

Well, I think that many users simply copy&amp;pasted the code along with the
flag. I have failed to find any justification for adding this flag for
basically all the cases I&#39;ve checked.

My understanding is that the overal motivation for the flag was to
fortify the allocation requests rather than weaken them. But if we were
literal then __GFP_REPEAT is in fact weaker than GFP_KERNEL for lower
orders. It is true that the later one is so only implicitly - and as an
implementation detail.

Anyway I think that getting rid of those users which couldn&#39;t ever see a
difference is a good start.
<span class="quote">
&gt; So I think we should answer the following questions:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; * What is the semantic of __GFP_REPEAT?</span>
<span class="quote">&gt;   - My suggestion would be something like &quot;I would really like this allocation</span>
<span class="quote">&gt; to succeed. I still have some fallback but it&#39;s so suboptimal I&#39;d rather wait</span>
<span class="quote">&gt; for this allocation.&quot;</span>

This is very close to the current wording.
<span class="quote">
&gt; And then we could e.g. change some heuristics to take that</span>
<span class="quote">&gt; into account - e.g. direct compaction could ignore the deferred state and</span>
<span class="quote">&gt; pageblock skip bits, to make sure it&#39;s as thorough as possible. Right now, that</span>
<span class="quote">&gt; sort of happens, but not quite - given enough reclaim/compact attempts, the</span>
<span class="quote">&gt; compact attempts might break out of deferred state. But pages_reclaimed might</span>
<span class="quote">&gt; reach 1 &lt;&lt; order before compaction &quot;undefers&quot;, and then it breaks out of the</span>
<span class="quote">&gt; loop. Is any such heuristic change possible for reclaim as well?</span>

I am not familiar with the compaction code enough to comment on this but
the reclaim part is already having something in should_continue_reclaim.
For low order allocations this doesn&#39;t make too much of a difference
because the reclaim is retried anyway from the page allocator path.
<span class="quote">
&gt; As part of this question we should also keep in mind/rethink __GFP_NORETRY as</span>
<span class="quote">&gt; that&#39;s supposed to be the opposite flag to __GFP_REPEAT.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; * Can it ever happen that __GFP_REPEAT could make some difference for order-0?</span>

Yes, if you want to try hard but eventually allow to fail the request.
Why just not use __GFP_NORETRY for that purpose? Well, that context is
much weaker. We give up after the first round of the reclaim and we do
not trigger the OOM killer in that context. __GFP_REPEAT should be about
finit retrying.

I am pretty sure there are users who would like to have this semantic.
None of the current low-order users seem to fall into this cathegory
AFAICS though.
<span class="quote">
&gt;   - Certainly not wrt compaction, how about reclaim?</span>

We can decide to allow the allocation to fail if reclaim progress was
not sufficient _and_ the OOM killer haven&#39;t helped rather than start
looping again.
<span class="quote">
&gt;   - If it couldn&#39;t possibly affect order-0, then yeah proceed with Patch 1.</span>

I&#39;ve split up obviously order-0 from the rest because I think order-0
are really easy to understand. Patch 1 is a bit harder to grasp but I
think it should be safe as well. I am open to discussion of course.
<span class="quote">
&gt; * Is PAGE_ALLOC_COSTLY_ORDER considered an implementation detail?</span>

Yes, more or less, but I doubt we can change it much considering all the
legacy code which might rely on it. I think we should simply remove the
dependency on the order and act for all orders same semantically.
<span class="quote">
&gt;   - I would think so, and if yes, then we probably shouldn&#39;t remove</span>
<span class="quote">&gt; __GFP_NORETRY for order-1+ allocations that happen to be not costly in the</span>
<span class="quote">&gt; current implementation?</span>

I guess you meant __GFP_REPEAT here but even then we should really think
about the reason why the flag has been added. Is it to fortify the
request? If yes it never worked that way so it is hard to justify it
that way.

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Nov. 18, 2015, 2:15 p.m.</div>
<pre class="content">
On 11/10/2015 01:51 PM, Michal Hocko wrote:
<span class="quote">&gt; On Mon 09-11-15 23:04:15, Vlastimil Babka wrote:</span>
<span class="quote">&gt;&gt; On 5.11.2015 17:15, mhocko@kernel.org wrote:</span>
<span class="quote">&gt;&gt; &gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; __GFP_REPEAT has a rather weak semantic but since it has been introduced</span>
<span class="quote">&gt;&gt; &gt; around 2.6.12 it has been ignored for low order allocations. Yet we have</span>
<span class="quote">&gt;&gt; &gt; the full kernel tree with its usage for apparently order-0 allocations.</span>
<span class="quote">&gt;&gt; &gt; This is really confusing because __GFP_REPEAT is explicitly documented</span>
<span class="quote">&gt;&gt; &gt; to allow allocation failures which is a weaker semantic than the current</span>
<span class="quote">&gt;&gt; &gt; order-0 has (basically nofail).</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; Let&#39;s simply reap out __GFP_REPEAT from those places. This would allow</span>
<span class="quote">&gt;&gt; &gt; to identify place which really need allocator to retry harder and</span>
<span class="quote">&gt;&gt; &gt; formulate a more specific semantic for what the flag is supposed to do</span>
<span class="quote">&gt;&gt; &gt; actually.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; So at first I thought &quot;yeah that&#39;s obvious&quot;, but then after some more thinking,</span>
<span class="quote">&gt;&gt; I&#39;m not so sure anymore.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks for looking into this! The primary purpose of this patch series was</span>
<span class="quote">&gt; to start the discussion. I&#39;ve only now realized I forgot to add RFC, sorry</span>
<span class="quote">&gt; about that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; I think we should formulate the semantic first, then do any changes. Also, let&#39;s</span>
<span class="quote">&gt;&gt; look at the flag description (which comes from pre-git):</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s rather hard to formulate one without examining the current users...</span>

Sure, but changing existing users is a different thing :)
<span class="quote">
&gt;&gt;  * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt</span>
<span class="quote">&gt;&gt;  * _might_ fail.  This depends upon the particular VM implementation.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; So we say it&#39;s implementation detail, and IIRC the same is said about which</span>
<span class="quote">&gt;&gt; orders are considered costly and which not, and the associated rules. So, can we</span>
<span class="quote">&gt;&gt; blame callers that happen to use __GFP_REPEAT essentially as a no-op in the</span>
<span class="quote">&gt;&gt; current implementation? And is it a problem that they do that?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well, I think that many users simply copy&amp;pasted the code along with the</span>
<span class="quote">&gt; flag. I have failed to find any justification for adding this flag for</span>
<span class="quote">&gt; basically all the cases I&#39;ve checked.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; My understanding is that the overal motivation for the flag was to</span>
<span class="quote">&gt; fortify the allocation requests rather than weaken them. But if we were</span>
<span class="quote">&gt; literal then __GFP_REPEAT is in fact weaker than GFP_KERNEL for lower</span>
<span class="quote">&gt; orders. It is true that the later one is so only implicitly - and as an</span>
<span class="quote">&gt; implementation detail.</span>

OK I admit I didn&#39;t realize fully that __GFP_REPEAT is supposed to be weaker,
although you did write it quite explicitly in the changelog. It&#39;s just
completely counterintuitive given the name of the flag!
<span class="quote">
&gt; Anyway I think that getting rid of those users which couldn&#39;t ever see a</span>
<span class="quote">&gt; difference is a good start.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; So I think we should answer the following questions:</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; * What is the semantic of __GFP_REPEAT?</span>
<span class="quote">&gt;&gt;   - My suggestion would be something like &quot;I would really like this allocation</span>
<span class="quote">&gt;&gt; to succeed. I still have some fallback but it&#39;s so suboptimal I&#39;d rather wait</span>
<span class="quote">&gt;&gt; for this allocation.&quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is very close to the current wording.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; And then we could e.g. change some heuristics to take that</span>
<span class="quote">&gt;&gt; into account - e.g. direct compaction could ignore the deferred state and</span>
<span class="quote">&gt;&gt; pageblock skip bits, to make sure it&#39;s as thorough as possible. Right now, that</span>
<span class="quote">&gt;&gt; sort of happens, but not quite - given enough reclaim/compact attempts, the</span>
<span class="quote">&gt;&gt; compact attempts might break out of deferred state. But pages_reclaimed might</span>
<span class="quote">&gt;&gt; reach 1 &lt;&lt; order before compaction &quot;undefers&quot;, and then it breaks out of the</span>
<span class="quote">&gt;&gt; loop. Is any such heuristic change possible for reclaim as well?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not familiar with the compaction code enough to comment on this but</span>
<span class="quote">&gt; the reclaim part is already having something in should_continue_reclaim.</span>

Yeah in this function having the flag means to try harder. Which is
counterintuitive to the &quot;allow failure&quot; semantics.
<span class="quote">
&gt; For low order allocations this doesn&#39;t make too much of a difference</span>
<span class="quote">&gt; because the reclaim is retried anyway from the page allocator path.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; As part of this question we should also keep in mind/rethink __GFP_NORETRY as</span>
<span class="quote">&gt;&gt; that&#39;s supposed to be the opposite flag to __GFP_REPEAT.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; * Can it ever happen that __GFP_REPEAT could make some difference for order-0?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, if you want to try hard but eventually allow to fail the request.</span>

Right, I missed the &quot;eventually fail&quot; part.
<span class="quote">
&gt; Why just not use __GFP_NORETRY for that purpose? Well, that context is</span>
<span class="quote">&gt; much weaker. We give up after the first round of the reclaim and we do</span>
<span class="quote">&gt; not trigger the OOM killer in that context. __GFP_REPEAT should be about</span>
<span class="quote">&gt; finit retrying.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am pretty sure there are users who would like to have this semantic.</span>
<span class="quote">&gt; None of the current low-order users seem to fall into this cathegory</span>
<span class="quote">&gt; AFAICS though.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;   - Certainly not wrt compaction, how about reclaim?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We can decide to allow the allocation to fail if reclaim progress was</span>
<span class="quote">&gt; not sufficient _and_ the OOM killer haven&#39;t helped rather than start</span>
<span class="quote">&gt; looping again.</span>

Makes sense.
<span class="quote">
&gt;&gt;   - If it couldn&#39;t possibly affect order-0, then yeah proceed with Patch 1.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve split up obviously order-0 from the rest because I think order-0</span>
<span class="quote">&gt; are really easy to understand. Patch 1 is a bit harder to grasp but I</span>
<span class="quote">&gt; think it should be safe as well. I am open to discussion of course.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; * Is PAGE_ALLOC_COSTLY_ORDER considered an implementation detail?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, more or less, but I doubt we can change it much considering all the</span>
<span class="quote">&gt; legacy code which might rely on it. I think we should simply remove the</span>
<span class="quote">&gt; dependency on the order and act for all orders same semantically.</span>

That&#39;s a nice goal, but probably a long-term one? Looks like making failure
possibility the default isn&#39;t viable. So that leaves us with making non-failure
the default. Which means checking all currently-costly-oder allocating sites to
pass some of the flags allowing failure.
<span class="quote">
&gt;&gt;   - I would think so, and if yes, then we probably shouldn&#39;t remove</span>
<span class="quote">&gt;&gt; __GFP_NORETRY for order-1+ allocations that happen to be not costly in the</span>
<span class="quote">&gt;&gt; current implementation?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I guess you meant __GFP_REPEAT here</span>

Ah, yes.
<span class="quote">
&gt; but even then we should really think</span>
<span class="quote">&gt; about the reason why the flag has been added. Is it to fortify the</span>
<span class="quote">&gt; request? If yes it never worked that way so it is hard to justify it</span>
<span class="quote">&gt; that way.</span>

Yep, we should definitely think of a less misleading name.
<span class="quote">
&gt; Thanks!</span>
<span class="quote">&gt; </span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 27, 2015, 9:38 a.m.</div>
<pre class="content">
On Wed 18-11-15 15:15:29, Vlastimil Babka wrote:
<span class="quote">&gt; On 11/10/2015 01:51 PM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Mon 09-11-15 23:04:15, Vlastimil Babka wrote:</span>
<span class="quote">&gt; &gt;&gt; On 5.11.2015 17:15, mhocko@kernel.org wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; __GFP_REPEAT has a rather weak semantic but since it has been introduced</span>
<span class="quote">&gt; &gt;&gt; &gt; around 2.6.12 it has been ignored for low order allocations. Yet we have</span>
<span class="quote">&gt; &gt;&gt; &gt; the full kernel tree with its usage for apparently order-0 allocations.</span>
<span class="quote">&gt; &gt;&gt; &gt; This is really confusing because __GFP_REPEAT is explicitly documented</span>
<span class="quote">&gt; &gt;&gt; &gt; to allow allocation failures which is a weaker semantic than the current</span>
<span class="quote">&gt; &gt;&gt; &gt; order-0 has (basically nofail).</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; Let&#39;s simply reap out __GFP_REPEAT from those places. This would allow</span>
<span class="quote">&gt; &gt;&gt; &gt; to identify place which really need allocator to retry harder and</span>
<span class="quote">&gt; &gt;&gt; &gt; formulate a more specific semantic for what the flag is supposed to do</span>
<span class="quote">&gt; &gt;&gt; &gt; actually.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; So at first I thought &quot;yeah that&#39;s obvious&quot;, but then after some more thinking,</span>
<span class="quote">&gt; &gt;&gt; I&#39;m not so sure anymore.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Thanks for looking into this! The primary purpose of this patch series was</span>
<span class="quote">&gt; &gt; to start the discussion. I&#39;ve only now realized I forgot to add RFC, sorry</span>
<span class="quote">&gt; &gt; about that.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; I think we should formulate the semantic first, then do any changes. Also, let&#39;s</span>
<span class="quote">&gt; &gt;&gt; look at the flag description (which comes from pre-git):</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It&#39;s rather hard to formulate one without examining the current users...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure, but changing existing users is a different thing :)</span>

Chicken &amp; Egg I guess?
<span class="quote">
&gt; &gt;&gt;  * __GFP_REPEAT: Try hard to allocate the memory, but the allocation attempt</span>
<span class="quote">&gt; &gt;&gt;  * _might_ fail.  This depends upon the particular VM implementation.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; So we say it&#39;s implementation detail, and IIRC the same is said about which</span>
<span class="quote">&gt; &gt;&gt; orders are considered costly and which not, and the associated rules. So, can we</span>
<span class="quote">&gt; &gt;&gt; blame callers that happen to use __GFP_REPEAT essentially as a no-op in the</span>
<span class="quote">&gt; &gt;&gt; current implementation? And is it a problem that they do that?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Well, I think that many users simply copy&amp;pasted the code along with the</span>
<span class="quote">&gt; &gt; flag. I have failed to find any justification for adding this flag for</span>
<span class="quote">&gt; &gt; basically all the cases I&#39;ve checked.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; My understanding is that the overal motivation for the flag was to</span>
<span class="quote">&gt; &gt; fortify the allocation requests rather than weaken them. But if we were</span>
<span class="quote">&gt; &gt; literal then __GFP_REPEAT is in fact weaker than GFP_KERNEL for lower</span>
<span class="quote">&gt; &gt; orders. It is true that the later one is so only implicitly - and as an</span>
<span class="quote">&gt; &gt; implementation detail.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK I admit I didn&#39;t realize fully that __GFP_REPEAT is supposed to be weaker,</span>
<span class="quote">&gt; although you did write it quite explicitly in the changelog. It&#39;s just</span>
<span class="quote">&gt; completely counterintuitive given the name of the flag!</span>

Yeah, I guess this is basically because this has always been for costly
allocations.

[...]

I am not sure whether we found any conclusion here. Are there any strong
arguments against patch 1? I think that should be relatively
non-controversial. What about patch 2? I think it should be ok as well
as we are basically removing the flag which has never had any effect.

I would like to proceed with this further by going through remaining users.
Most of them depend on a variable size and I am not familiar with the
code so I will talk to maintainer to find out reasoning behind using the
flag. Once we have reasonable number of them I would like to go on and
rename the flag to __GFP_BEST_AFFORD and make it independent on the
order. It would still trigger OOM killer where applicable but wouldn&#39;t
retry endlessly.

Does this sound like a reasonable plan?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 28, 2015, 10:08 a.m.</div>
<pre class="content">
On Fri 27-11-15 10:38:07, Michal Hocko wrote:
[...]
<span class="quote">&gt; I am not sure whether we found any conclusion here. Are there any strong</span>
<span class="quote">&gt; arguments against patch 1? I think that should be relatively</span>
<span class="quote">&gt; non-controversial. What about patch 2? I think it should be ok as well</span>
<span class="quote">&gt; as we are basically removing the flag which has never had any effect.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would like to proceed with this further by going through remaining users.</span>
<span class="quote">&gt; Most of them depend on a variable size and I am not familiar with the</span>
<span class="quote">&gt; code so I will talk to maintainer to find out reasoning behind using the</span>
<span class="quote">&gt; flag. Once we have reasonable number of them I would like to go on and</span>
<span class="quote">&gt; rename the flag to __GFP_BEST_AFFORD and make it independent on the</span>

ble, __GFP_BEST_EFFORT I meant of course...
<span class="quote">
&gt; order. It would still trigger OOM killer where applicable but wouldn&#39;t</span>
<span class="quote">&gt; retry endlessly.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Does this sound like a reasonable plan?</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Nov. 30, 2015, 5:02 p.m.</div>
<pre class="content">
On 11/27/2015 10:38 AM, Michal Hocko wrote:
<span class="quote">&gt; On Wed 18-11-15 15:15:29, Vlastimil Babka wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I am not sure whether we found any conclusion here. Are there any strong</span>
<span class="quote">&gt; arguments against patch 1? I think that should be relatively</span>
<span class="quote">&gt; non-controversial.</span>

Agreed.
<span class="quote">
&gt; What about patch 2? I think it should be ok as well</span>
<span class="quote">&gt; as we are basically removing the flag which has never had any effect.</span>

Right.
<span class="quote">
&gt; I would like to proceed with this further by going through remaining users.</span>
<span class="quote">&gt; Most of them depend on a variable size and I am not familiar with the</span>
<span class="quote">&gt; code so I will talk to maintainer to find out reasoning behind using the</span>
<span class="quote">&gt; flag. Once we have reasonable number of them I would like to go on and</span>
<span class="quote">&gt; rename the flag to __GFP_BEST_AFFORD and make it independent on the</span>
<span class="quote">&gt; order. It would still trigger OOM killer where applicable but wouldn&#39;t</span>
<span class="quote">&gt; retry endlessly.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Does this sound like a reasonable plan?</span>

I think we should consider all the related flags together before 
starting renaming them. So IIUC the current state is:

~__GFP_DIRECT_RECLAIM - no reclaim/compaction, fails regardless of 
order; good for allocations that prefer their fallback to the latency of 
reclaim/compaction

__GFP_NORETRY - only one reclaim and two compaction attempts, then fails 
regardless of order; some tradeoff between allocation latency and fallback?

__GFP_REPEAT - for costly orders, tries harder to reclaim before oom, 
otherwise no difference - doesn&#39;t fail for non-costly orders, although 
comment says it could.

__GFP_NOFAIL - cannot fail

So the issue I see with simply renaming __GFP_REPEAT to 
__GFP_BEST_AFFORD and making it possible to fail for low orders, is that 
it will conflate the new failure possibility with the existing &quot;try 
harder to reclaim before oom&quot;. As I mentioned before, &quot;trying harder&quot; 
could be also extended to mean something for compaction, but that would 
further muddle the meaning of the flag. Maybe the cleanest solution 
would be to have separate flags for &quot;possible to fail&quot; (let&#39;s say 
__GFP_MAYFAIL for now) and &quot;try harder&quot; (e.g. __GFP_TRY_HARDER)? And 
introduce two new higher-level &quot;flags&quot; of a GFP_* kind, that callers 
would use instead of GFP_KERNEL, where one would mean 
GFP_KERNEL|__GFP_MAYFAIL and the other 
GFP_KERNEL|__GFP_TRY_HARDER|__GFP_MAYFAIL.

The second thing to consider, is __GFP_NORETRY useful? The latency 
savings are quite vague. Maybe we could just remove this flag to make 
space for __GFP_MAYFAIL?
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Dec. 1, 2015, 4:27 p.m.</div>
<pre class="content">
On Mon 30-11-15 18:02:33, Vlastimil Babka wrote:
[...]
<span class="quote">&gt; I think we should consider all the related flags together before starting</span>
<span class="quote">&gt; renaming them. So IIUC the current state is:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ~__GFP_DIRECT_RECLAIM - no reclaim/compaction, fails regardless of order;</span>
<span class="quote">&gt; good for allocations that prefer their fallback to the latency of</span>
<span class="quote">&gt; reclaim/compaction</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; __GFP_NORETRY - only one reclaim and two compaction attempts, then fails</span>
<span class="quote">&gt; regardless of order; some tradeoff between allocation latency and fallback?</span>

Also doesn&#39;t invoke OOM killer.
<span class="quote">
&gt; __GFP_REPEAT - for costly orders, tries harder to reclaim before oom,</span>
<span class="quote">&gt; otherwise no difference - doesn&#39;t fail for non-costly orders, although</span>
<span class="quote">&gt; comment says it could.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; __GFP_NOFAIL - cannot fail</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So the issue I see with simply renaming __GFP_REPEAT to __GFP_BEST_AFFORD</span>
<span class="quote">&gt; and making it possible to fail for low orders, is that it will conflate the</span>
<span class="quote">&gt; new failure possibility with the existing &quot;try harder to reclaim before</span>
<span class="quote">&gt; oom&quot;. As I mentioned before, &quot;trying harder&quot; could be also extended to mean</span>
<span class="quote">&gt; something for compaction, but that would further muddle the meaning of the</span>
<span class="quote">&gt; flag. Maybe the cleanest solution would be to have separate flags for</span>
<span class="quote">&gt; &quot;possible to fail&quot; (let&#39;s say __GFP_MAYFAIL for now) and &quot;try harder&quot; (e.g.</span>
<span class="quote">&gt; __GFP_TRY_HARDER)? And introduce two new higher-level &quot;flags&quot; of a GFP_*</span>
<span class="quote">&gt; kind, that callers would use instead of GFP_KERNEL, where one would mean</span>
<span class="quote">&gt; GFP_KERNEL|__GFP_MAYFAIL and the other</span>
<span class="quote">&gt; GFP_KERNEL|__GFP_TRY_HARDER|__GFP_MAYFAIL.</span>

I will think about that but this sounds quite confusing to me. All the
allocations on behalf of a user process are MAYFAIL basically (e.g. the
oom victim failure case) unless they are explicitly __GFP_NOFAIL. It
also sounds that ~__GFP_NOFAIL should imply MAYFAIL automatically.
__GFP_BEST_EFFORT on the other hand clearly states that the allocator
should try its best but it can fail. The way how it achieves that is
an implementation detail and users do not have to care. In your above
hierarchy of QoS we have:
- no reclaim ~__GFP_DIRECT_RECLAIM - optimistic allocation with a
  fallback (e.g. smaller allocation request)
- no destructive reclaim __GFP_NORETRY - allocation with a more
  expensive fallback (e.g. vmalloc)
- all reclaim types but only fail if there is no good hope for success
  __GFP_BEST_EFFORT (fail rather than invoke the OOM killer second time)
  user allocations
- no failure allowed __GFP_NOFAIL - failure mode is not acceptable

we can keep the current implicit &quot;low order imply __GFP_NOFAIL&quot; behavior
of the GFP_KERNEL and still offer users to use __GFP_BEST_EFFORT as a
way to override it.
<span class="quote">
&gt; The second thing to consider, is __GFP_NORETRY useful? The latency savings</span>
<span class="quote">&gt; are quite vague. Maybe we could just remove this flag to make space for</span>
<span class="quote">&gt; __GFP_MAYFAIL?</span>

There are users who would like to see some reclaim but rather fail then
see the OOM killer. I assume there are also users who can handle the
failure but the OOM killer is not a big deal for them. I think that
GFP_USER is an example of the later.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Dec. 21, 2015, 12:18 p.m.</div>
<pre class="content">
On 12/01/2015 05:27 PM, Michal Hocko wrote:
<span class="quote">&gt; On Mon 30-11-15 18:02:33, Vlastimil Babka wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;&gt; So the issue I see with simply renaming __GFP_REPEAT to __GFP_BEST_AFFORD</span>
<span class="quote">&gt;&gt; and making it possible to fail for low orders, is that it will conflate the</span>
<span class="quote">&gt;&gt; new failure possibility with the existing &quot;try harder to reclaim before</span>
<span class="quote">&gt;&gt; oom&quot;. As I mentioned before, &quot;trying harder&quot; could be also extended to mean</span>
<span class="quote">&gt;&gt; something for compaction, but that would further muddle the meaning of the</span>
<span class="quote">&gt;&gt; flag. Maybe the cleanest solution would be to have separate flags for</span>
<span class="quote">&gt;&gt; &quot;possible to fail&quot; (let&#39;s say __GFP_MAYFAIL for now) and &quot;try harder&quot; (e.g.</span>
<span class="quote">&gt;&gt; __GFP_TRY_HARDER)? And introduce two new higher-level &quot;flags&quot; of a GFP_*</span>
<span class="quote">&gt;&gt; kind, that callers would use instead of GFP_KERNEL, where one would mean</span>
<span class="quote">&gt;&gt; GFP_KERNEL|__GFP_MAYFAIL and the other</span>
<span class="quote">&gt;&gt; GFP_KERNEL|__GFP_TRY_HARDER|__GFP_MAYFAIL.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I will think about that but this sounds quite confusing to me. All the</span>
<span class="quote">&gt; allocations on behalf of a user process are MAYFAIL basically (e.g. the</span>
<span class="quote">&gt; oom victim failure case) unless they are explicitly __GFP_NOFAIL. It</span>
<span class="quote">&gt; also sounds that ~__GFP_NOFAIL should imply MAYFAIL automatically.</span>
<span class="quote">&gt; __GFP_BEST_EFFORT on the other hand clearly states that the allocator</span>
<span class="quote">&gt; should try its best but it can fail. The way how it achieves that is</span>
<span class="quote">&gt; an implementation detail and users do not have to care. In your above</span>
<span class="quote">&gt; hierarchy of QoS we have:</span>
<span class="quote">&gt; - no reclaim ~__GFP_DIRECT_RECLAIM - optimistic allocation with a</span>
<span class="quote">&gt;    fallback (e.g. smaller allocation request)</span>
<span class="quote">&gt; - no destructive reclaim __GFP_NORETRY - allocation with a more</span>
<span class="quote">&gt;    expensive fallback (e.g. vmalloc)</span>

Maybe it would be less confusing / more consistent if __GFP_NORETRY was 
renamed to __GFP_LOW_EFFORT ?
<span class="quote">
&gt; - all reclaim types but only fail if there is no good hope for success</span>
<span class="quote">&gt;    __GFP_BEST_EFFORT (fail rather than invoke the OOM killer second time)</span>
<span class="quote">&gt;    user allocations</span>
<span class="quote">&gt; - no failure allowed __GFP_NOFAIL - failure mode is not acceptable</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; we can keep the current implicit &quot;low order imply __GFP_NOFAIL&quot; behavior</span>
<span class="quote">&gt; of the GFP_KERNEL and still offer users to use __GFP_BEST_EFFORT as a</span>
<span class="quote">&gt; way to override it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; The second thing to consider, is __GFP_NORETRY useful? The latency savings</span>
<span class="quote">&gt;&gt; are quite vague. Maybe we could just remove this flag to make space for</span>
<span class="quote">&gt;&gt; __GFP_MAYFAIL?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; There are users who would like to see some reclaim but rather fail then</span>
<span class="quote">&gt; see the OOM killer. I assume there are also users who can handle the</span>
<span class="quote">&gt; failure but the OOM killer is not a big deal for them. I think that</span>
<span class="quote">&gt; GFP_USER is an example of the later.</span>
<span class="quote">&gt;</span>

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/alpha/include/asm/pgalloc.h b/arch/alpha/include/asm/pgalloc.h</span>
<span class="p_header">index aab14a019c20..c2ebb6f36c9d 100644</span>
<span class="p_header">--- a/arch/alpha/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/alpha/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -40,7 +40,7 @@</span> <span class="p_context"> pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
 static inline pmd_t *
 pmd_alloc_one(struct mm_struct *mm, unsigned long address)
 {
<span class="p_del">-	pmd_t *ret = (pmd_t *)__get_free_page(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO);</span>
<span class="p_add">+	pmd_t *ret = (pmd_t *)__get_free_page(GFP_KERNEL|__GFP_ZERO);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -53,7 +53,7 @@</span> <span class="p_context"> pmd_free(struct mm_struct *mm, pmd_t *pmd)</span>
 static inline pte_t *
 pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)
 {
<span class="p_del">-	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO);</span>
<span class="p_add">+	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_ZERO);</span>
 	return pte;
 }
 
<span class="p_header">diff --git a/arch/arm/include/asm/pgalloc.h b/arch/arm/include/asm/pgalloc.h</span>
<span class="p_header">index 19cfab526d13..20febb368844 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -29,7 +29,7 @@</span> <span class="p_context"></span>
 
 static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long addr)
 {
<span class="p_del">-	return (pmd_t *)get_zeroed_page(GFP_KERNEL | __GFP_REPEAT);</span>
<span class="p_add">+	return (pmd_t *)get_zeroed_page(GFP_KERNEL);</span>
 }
 
 static inline void pmd_free(struct mm_struct *mm, pmd_t *pmd)
<span class="p_header">diff --git a/arch/avr32/include/asm/pgalloc.h b/arch/avr32/include/asm/pgalloc.h</span>
<span class="p_header">index 1aba19d68c5e..db039cb368be 100644</span>
<span class="p_header">--- a/arch/avr32/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/avr32/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -43,7 +43,7 @@</span> <span class="p_context"> static inline void pgd_ctor(void *x)</span>
  */
 static inline pgd_t *pgd_alloc(struct mm_struct *mm)
 {
<span class="p_del">-	return quicklist_alloc(QUICK_PGD, GFP_KERNEL | __GFP_REPEAT, pgd_ctor);</span>
<span class="p_add">+	return quicklist_alloc(QUICK_PGD, GFP_KERNEL, pgd_ctor);</span>
 }
 
 static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)
<span class="p_chunk">@@ -54,7 +54,7 @@</span> <span class="p_context"> static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
 static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,
 					  unsigned long address)
 {
<span class="p_del">-	return quicklist_alloc(QUICK_PT, GFP_KERNEL | __GFP_REPEAT, NULL);</span>
<span class="p_add">+	return quicklist_alloc(QUICK_PT, GFP_KERNEL, NULL);</span>
 }
 
 static inline pgtable_t pte_alloc_one(struct mm_struct *mm,
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> static inline pgtable_t pte_alloc_one(struct mm_struct *mm,</span>
 	struct page *page;
 	void *pg;
 
<span class="p_del">-	pg = quicklist_alloc(QUICK_PT, GFP_KERNEL | __GFP_REPEAT, NULL);</span>
<span class="p_add">+	pg = quicklist_alloc(QUICK_PT, GFP_KERNEL, NULL);</span>
 	if (!pg)
 		return NULL;
 
<span class="p_header">diff --git a/arch/cris/include/asm/pgalloc.h b/arch/cris/include/asm/pgalloc.h</span>
<span class="p_header">index 235ece437ddd..42f1affb9c2d 100644</span>
<span class="p_header">--- a/arch/cris/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/cris/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -24,14 +24,14 @@</span> <span class="p_context"> static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
 
 static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)
 {
<span class="p_del">-  	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO);</span>
<span class="p_add">+	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_ZERO);</span>
  	return pte;
 }
 
 static inline pgtable_t pte_alloc_one(struct mm_struct *mm, unsigned long address)
 {
 	struct page *pte;
<span class="p_del">-	pte = alloc_pages(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO, 0);</span>
<span class="p_add">+	pte = alloc_pages(GFP_KERNEL|__GFP_ZERO, 0);</span>
 	if (!pte)
 		return NULL;
 	if (!pgtable_page_ctor(pte)) {
<span class="p_header">diff --git a/arch/frv/mm/pgalloc.c b/arch/frv/mm/pgalloc.c</span>
<span class="p_header">index 41907d25ed38..c9ed14f6c67d 100644</span>
<span class="p_header">--- a/arch/frv/mm/pgalloc.c</span>
<span class="p_header">+++ b/arch/frv/mm/pgalloc.c</span>
<span class="p_chunk">@@ -22,7 +22,7 @@</span> <span class="p_context"> pgd_t swapper_pg_dir[PTRS_PER_PGD] __attribute__((aligned(PAGE_SIZE)));</span>
 
 pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)
 {
<span class="p_del">-	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_REPEAT);</span>
<span class="p_add">+	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL);</span>
 	if (pte)
 		clear_page(pte);
 	return pte;
<span class="p_chunk">@@ -33,9 +33,9 @@</span> <span class="p_context"> pgtable_t pte_alloc_one(struct mm_struct *mm, unsigned long address)</span>
 	struct page *page;
 
 #ifdef CONFIG_HIGHPTE
<span class="p_del">-	page = alloc_pages(GFP_KERNEL|__GFP_HIGHMEM|__GFP_REPEAT, 0);</span>
<span class="p_add">+	page = alloc_pages(GFP_KERNEL|__GFP_HIGHMEM, 0);</span>
 #else
<span class="p_del">-	page = alloc_pages(GFP_KERNEL|__GFP_REPEAT, 0);</span>
<span class="p_add">+	page = alloc_pages(GFP_KERNEL, 0);</span>
 #endif
 	if (!page)
 		return NULL;
<span class="p_header">diff --git a/arch/hexagon/include/asm/pgalloc.h b/arch/hexagon/include/asm/pgalloc.h</span>
<span class="p_header">index 77da3b0ae3c2..eeebf862c46c 100644</span>
<span class="p_header">--- a/arch/hexagon/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/hexagon/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -64,7 +64,7 @@</span> <span class="p_context"> static inline struct page *pte_alloc_one(struct mm_struct *mm,</span>
 {
 	struct page *pte;
 
<span class="p_del">-	pte = alloc_page(GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+	pte = alloc_page(GFP_KERNEL | __GFP_ZERO);</span>
 	if (!pte)
 		return NULL;
 	if (!pgtable_page_ctor(pte)) {
<span class="p_chunk">@@ -78,7 +78,7 @@</span> <span class="p_context"> static inline struct page *pte_alloc_one(struct mm_struct *mm,</span>
 static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,
 					  unsigned long address)
 {
<span class="p_del">-	gfp_t flags =  GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO;</span>
<span class="p_add">+	gfp_t flags =  GFP_KERNEL | __GFP_ZERO;</span>
 	return (pte_t *) __get_free_page(flags);
 }
 
<span class="p_header">diff --git a/arch/m68k/include/asm/mcf_pgalloc.h b/arch/m68k/include/asm/mcf_pgalloc.h</span>
<span class="p_header">index f9924fbcfe42..fb95aed5f428 100644</span>
<span class="p_header">--- a/arch/m68k/include/asm/mcf_pgalloc.h</span>
<span class="p_header">+++ b/arch/m68k/include/asm/mcf_pgalloc.h</span>
<span class="p_chunk">@@ -14,7 +14,7 @@</span> <span class="p_context"> extern const char bad_pmd_string[];</span>
 extern inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,
 	unsigned long address)
 {
<span class="p_del">-	unsigned long page = __get_free_page(GFP_DMA|__GFP_REPEAT);</span>
<span class="p_add">+	unsigned long page = __get_free_page(GFP_DMA);</span>
 
 	if (!page)
 		return NULL;
<span class="p_chunk">@@ -51,7 +51,7 @@</span> <span class="p_context"> static inline void __pte_free_tlb(struct mmu_gather *tlb, pgtable_t page,</span>
 static inline struct page *pte_alloc_one(struct mm_struct *mm,
 	unsigned long address)
 {
<span class="p_del">-	struct page *page = alloc_pages(GFP_DMA|__GFP_REPEAT, 0);</span>
<span class="p_add">+	struct page *page = alloc_pages(GFP_DMA, 0);</span>
 	pte_t *pte;
 
 	if (!page)
<span class="p_header">diff --git a/arch/m68k/include/asm/motorola_pgalloc.h b/arch/m68k/include/asm/motorola_pgalloc.h</span>
<span class="p_header">index 24bcba496c75..c895b987202c 100644</span>
<span class="p_header">--- a/arch/m68k/include/asm/motorola_pgalloc.h</span>
<span class="p_header">+++ b/arch/m68k/include/asm/motorola_pgalloc.h</span>
<span class="p_chunk">@@ -11,7 +11,7 @@</span> <span class="p_context"> static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long ad</span>
 {
 	pte_t *pte;
 
<span class="p_del">-	pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO);</span>
<span class="p_add">+	pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_ZERO);</span>
 	if (pte) {
 		__flush_page_to_ram(pte);
 		flush_tlb_kernel_page(pte);
<span class="p_chunk">@@ -32,7 +32,7 @@</span> <span class="p_context"> static inline pgtable_t pte_alloc_one(struct mm_struct *mm, unsigned long addres</span>
 	struct page *page;
 	pte_t *pte;
 
<span class="p_del">-	page = alloc_pages(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO, 0);</span>
<span class="p_add">+	page = alloc_pages(GFP_KERNEL|__GFP_ZERO, 0);</span>
 	if(!page)
 		return NULL;
 	if (!pgtable_page_ctor(page)) {
<span class="p_header">diff --git a/arch/m68k/include/asm/sun3_pgalloc.h b/arch/m68k/include/asm/sun3_pgalloc.h</span>
<span class="p_header">index 0931388de47f..1901f61f926f 100644</span>
<span class="p_header">--- a/arch/m68k/include/asm/sun3_pgalloc.h</span>
<span class="p_header">+++ b/arch/m68k/include/asm/sun3_pgalloc.h</span>
<span class="p_chunk">@@ -37,7 +37,7 @@</span> <span class="p_context"> do {							\</span>
 static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,
 					  unsigned long address)
 {
<span class="p_del">-	unsigned long page = __get_free_page(GFP_KERNEL|__GFP_REPEAT);</span>
<span class="p_add">+	unsigned long page = __get_free_page(GFP_KERNEL);</span>
 
 	if (!page)
 		return NULL;
<span class="p_chunk">@@ -49,7 +49,7 @@</span> <span class="p_context"> static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,</span>
 static inline pgtable_t pte_alloc_one(struct mm_struct *mm,
 					unsigned long address)
 {
<span class="p_del">-        struct page *page = alloc_pages(GFP_KERNEL|__GFP_REPEAT, 0);</span>
<span class="p_add">+        struct page *page = alloc_pages(GFP_KERNEL, 0);</span>
 
 	if (page == NULL)
 		return NULL;
<span class="p_header">diff --git a/arch/metag/include/asm/pgalloc.h b/arch/metag/include/asm/pgalloc.h</span>
<span class="p_header">index 3104df0a4822..c2caa1ee4360 100644</span>
<span class="p_header">--- a/arch/metag/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/metag/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -42,8 +42,7 @@</span> <span class="p_context"> static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
 static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,
 					  unsigned long address)
 {
<span class="p_del">-	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL | __GFP_REPEAT |</span>
<span class="p_del">-					      __GFP_ZERO);</span>
<span class="p_add">+	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL | __GFP_ZERO);</span>
 	return pte;
 }
 
<span class="p_chunk">@@ -51,7 +50,7 @@</span> <span class="p_context"> static inline pgtable_t pte_alloc_one(struct mm_struct *mm,</span>
 				      unsigned long address)
 {
 	struct page *pte;
<span class="p_del">-	pte = alloc_pages(GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO, 0);</span>
<span class="p_add">+	pte = alloc_pages(GFP_KERNEL  | __GFP_ZERO, 0);</span>
 	if (!pte)
 		return NULL;
 	if (!pgtable_page_ctor(pte)) {
<span class="p_header">diff --git a/arch/microblaze/include/asm/pgalloc.h b/arch/microblaze/include/asm/pgalloc.h</span>
<span class="p_header">index 61436d69775c..7c89390c0c13 100644</span>
<span class="p_header">--- a/arch/microblaze/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/microblaze/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -116,9 +116,9 @@</span> <span class="p_context"> static inline struct page *pte_alloc_one(struct mm_struct *mm,</span>
 	struct page *ptepage;
 
 #ifdef CONFIG_HIGHPTE
<span class="p_del">-	int flags = GFP_KERNEL | __GFP_HIGHMEM | __GFP_REPEAT;</span>
<span class="p_add">+	int flags = GFP_KERNEL | __GFP_HIGHMEM;</span>
 #else
<span class="p_del">-	int flags = GFP_KERNEL | __GFP_REPEAT;</span>
<span class="p_add">+	int flags = GFP_KERNEL;</span>
 #endif
 
 	ptepage = alloc_pages(flags, 0);
<span class="p_header">diff --git a/arch/microblaze/mm/pgtable.c b/arch/microblaze/mm/pgtable.c</span>
<span class="p_header">index 4f4520e779a5..eb99fcc76088 100644</span>
<span class="p_header">--- a/arch/microblaze/mm/pgtable.c</span>
<span class="p_header">+++ b/arch/microblaze/mm/pgtable.c</span>
<span class="p_chunk">@@ -239,8 +239,7 @@</span> <span class="p_context"> __init_refok pte_t *pte_alloc_one_kernel(struct mm_struct *mm,</span>
 {
 	pte_t *pte;
 	if (mem_init_done) {
<span class="p_del">-		pte = (pte_t *)__get_free_page(GFP_KERNEL |</span>
<span class="p_del">-					__GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+		pte = (pte_t *)__get_free_page(GFP_KERNEL | __GFP_ZERO);</span>
 	} else {
 		pte = (pte_t *)early_get_page();
 		if (pte)
<span class="p_header">diff --git a/arch/mn10300/mm/pgtable.c b/arch/mn10300/mm/pgtable.c</span>
<span class="p_header">index e77a7c728081..9577cf768875 100644</span>
<span class="p_header">--- a/arch/mn10300/mm/pgtable.c</span>
<span class="p_header">+++ b/arch/mn10300/mm/pgtable.c</span>
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> void set_pmd_pfn(unsigned long vaddr, unsigned long pfn, pgprot_t flags)</span>
 
 pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)
 {
<span class="p_del">-	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_REPEAT);</span>
<span class="p_add">+	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL);</span>
 	if (pte)
 		clear_page(pte);
 	return pte;
<span class="p_chunk">@@ -74,9 +74,9 @@</span> <span class="p_context"> struct page *pte_alloc_one(struct mm_struct *mm, unsigned long address)</span>
 	struct page *pte;
 
 #ifdef CONFIG_HIGHPTE
<span class="p_del">-	pte = alloc_pages(GFP_KERNEL|__GFP_HIGHMEM|__GFP_REPEAT, 0);</span>
<span class="p_add">+	pte = alloc_pages(GFP_KERNEL|__GFP_HIGHMEM, 0);</span>
 #else
<span class="p_del">-	pte = alloc_pages(GFP_KERNEL|__GFP_REPEAT, 0);</span>
<span class="p_add">+	pte = alloc_pages(GFP_KERNEL, 0);</span>
 #endif
 	if (!pte)
 		return NULL;
<span class="p_header">diff --git a/arch/openrisc/include/asm/pgalloc.h b/arch/openrisc/include/asm/pgalloc.h</span>
<span class="p_header">index 21484e5b9e9a..87eebd185089 100644</span>
<span class="p_header">--- a/arch/openrisc/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/openrisc/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -77,7 +77,7 @@</span> <span class="p_context"> static inline struct page *pte_alloc_one(struct mm_struct *mm,</span>
 					 unsigned long address)
 {
 	struct page *pte;
<span class="p_del">-	pte = alloc_pages(GFP_KERNEL|__GFP_REPEAT, 0);</span>
<span class="p_add">+	pte = alloc_pages(GFP_KERNEL, 0);</span>
 	if (!pte)
 		return NULL;
 	clear_page(page_address(pte));
<span class="p_header">diff --git a/arch/openrisc/mm/ioremap.c b/arch/openrisc/mm/ioremap.c</span>
<span class="p_header">index 62b08ef392be..5b2a95116e8f 100644</span>
<span class="p_header">--- a/arch/openrisc/mm/ioremap.c</span>
<span class="p_header">+++ b/arch/openrisc/mm/ioremap.c</span>
<span class="p_chunk">@@ -122,7 +122,7 @@</span> <span class="p_context"> pte_t __init_refok *pte_alloc_one_kernel(struct mm_struct *mm,</span>
 	pte_t *pte;
 
 	if (likely(mem_init_done)) {
<span class="p_del">-		pte = (pte_t *) __get_free_page(GFP_KERNEL | __GFP_REPEAT);</span>
<span class="p_add">+		pte = (pte_t *) __get_free_page(GFP_KERNEL);</span>
 	} else {
 		pte = (pte_t *) alloc_bootmem_low_pages(PAGE_SIZE);
 #if 0
<span class="p_header">diff --git a/arch/parisc/include/asm/pgalloc.h b/arch/parisc/include/asm/pgalloc.h</span>
<span class="p_header">index 3edbb9fc91b4..b7e4027aac4b 100644</span>
<span class="p_header">--- a/arch/parisc/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/parisc/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -124,7 +124,7 @@</span> <span class="p_context"> pmd_populate_kernel(struct mm_struct *mm, pmd_t *pmd, pte_t *pte)</span>
 static inline pgtable_t
 pte_alloc_one(struct mm_struct *mm, unsigned long address)
 {
<span class="p_del">-	struct page *page = alloc_page(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO);</span>
<span class="p_add">+	struct page *page = alloc_page(GFP_KERNEL|__GFP_ZERO);</span>
 	if (!page)
 		return NULL;
 	if (!pgtable_page_ctor(page)) {
<span class="p_chunk">@@ -137,7 +137,7 @@</span> <span class="p_context"> pte_alloc_one(struct mm_struct *mm, unsigned long address)</span>
 static inline pte_t *
 pte_alloc_one_kernel(struct mm_struct *mm, unsigned long addr)
 {
<span class="p_del">-	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO);</span>
<span class="p_add">+	pte_t *pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_ZERO);</span>
 	return pte;
 }
 
<span class="p_header">diff --git a/arch/powerpc/include/asm/pgalloc-64.h b/arch/powerpc/include/asm/pgalloc-64.h</span>
<span class="p_header">index 4b0be20fcbfd..4e19f734447b 100644</span>
<span class="p_header">--- a/arch/powerpc/include/asm/pgalloc-64.h</span>
<span class="p_header">+++ b/arch/powerpc/include/asm/pgalloc-64.h</span>
<span class="p_chunk">@@ -79,7 +79,7 @@</span> <span class="p_context"> static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)</span>
 static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,
 					  unsigned long address)
 {
<span class="p_del">-	return (pte_t *)__get_free_page(GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+	return (pte_t *)__get_free_page(GFP_KERNEL | __GFP_ZERO);</span>
 }
 
 static inline pgtable_t pte_alloc_one(struct mm_struct *mm,
<span class="p_header">diff --git a/arch/powerpc/mm/pgtable_32.c b/arch/powerpc/mm/pgtable_32.c</span>
<span class="p_header">index 7692d1bb1bc6..494b1838485c 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/pgtable_32.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/pgtable_32.c</span>
<span class="p_chunk">@@ -109,7 +109,7 @@</span> <span class="p_context"> __init_refok pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long add</span>
 	pte_t *pte;
 
 	if (slab_is_available()) {
<span class="p_del">-		pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO);</span>
<span class="p_add">+		pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_ZERO);</span>
 	} else {
 		pte = __va(memblock_alloc(PAGE_SIZE, PAGE_SIZE));
 		if (pte)
<span class="p_chunk">@@ -122,7 +122,7 @@</span> <span class="p_context"> pgtable_t pte_alloc_one(struct mm_struct *mm, unsigned long address)</span>
 {
 	struct page *ptepage;
 
<span class="p_del">-	gfp_t flags = GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO;</span>
<span class="p_add">+	gfp_t flags = GFP_KERNEL | __GFP_ZERO;</span>
 
 	ptepage = alloc_pages(flags, 0);
 	if (!ptepage)
<span class="p_header">diff --git a/arch/powerpc/mm/pgtable_64.c b/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_header">index 422c59a24561..4e225504ed19 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/pgtable_64.c</span>
<span class="p_chunk">@@ -386,8 +386,7 @@</span> <span class="p_context"> static pte_t *get_from_cache(struct mm_struct *mm)</span>
 static pte_t *__alloc_for_cache(struct mm_struct *mm, int kernel)
 {
 	void *ret = NULL;
<span class="p_del">-	struct page *page = alloc_page(GFP_KERNEL | __GFP_NOTRACK |</span>
<span class="p_del">-				       __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+	struct page *page = alloc_page(GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
 	if (!page)
 		return NULL;
 	if (!kernel &amp;&amp; !pgtable_page_ctor(page)) {
<span class="p_header">diff --git a/arch/s390/mm/pgtable.c b/arch/s390/mm/pgtable.c</span>
<span class="p_header">index 34f3790fe459..76c98c344c60 100644</span>
<span class="p_header">--- a/arch/s390/mm/pgtable.c</span>
<span class="p_header">+++ b/arch/s390/mm/pgtable.c</span>
<span class="p_chunk">@@ -966,7 +966,7 @@</span> <span class="p_context"> unsigned long *page_table_alloc(struct mm_struct *mm)</span>
 			return table;
 	}
 	/* Allocate a fresh page */
<span class="p_del">-	page = alloc_page(GFP_KERNEL|__GFP_REPEAT);</span>
<span class="p_add">+	page = alloc_page(GFP_KERNEL);</span>
 	if (!page)
 		return NULL;
 	if (!pgtable_page_ctor(page)) {
<span class="p_header">diff --git a/arch/sh/include/asm/pgalloc.h b/arch/sh/include/asm/pgalloc.h</span>
<span class="p_header">index a33673b3687d..f3f42c84c40f 100644</span>
<span class="p_header">--- a/arch/sh/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/sh/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -34,7 +34,7 @@</span> <span class="p_context"> static inline void pmd_populate(struct mm_struct *mm, pmd_t *pmd,</span>
 static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,
 					  unsigned long address)
 {
<span class="p_del">-	return quicklist_alloc(QUICK_PT, GFP_KERNEL | __GFP_REPEAT, NULL);</span>
<span class="p_add">+	return quicklist_alloc(QUICK_PT, GFP_KERNEL, NULL);</span>
 }
 
 static inline pgtable_t pte_alloc_one(struct mm_struct *mm,
<span class="p_chunk">@@ -43,7 +43,7 @@</span> <span class="p_context"> static inline pgtable_t pte_alloc_one(struct mm_struct *mm,</span>
 	struct page *page;
 	void *pg;
 
<span class="p_del">-	pg = quicklist_alloc(QUICK_PT, GFP_KERNEL | __GFP_REPEAT, NULL);</span>
<span class="p_add">+	pg = quicklist_alloc(QUICK_PT, GFP_KERNEL, NULL);</span>
 	if (!pg)
 		return NULL;
 	page = virt_to_page(pg);
<span class="p_header">diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c</span>
<span class="p_header">index 3025bd57f7ab..1dee80aaed0f 100644</span>
<span class="p_header">--- a/arch/sparc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/init_64.c</span>
<span class="p_chunk">@@ -2712,8 +2712,7 @@</span> <span class="p_context"> void __flush_tlb_all(void)</span>
 pte_t *pte_alloc_one_kernel(struct mm_struct *mm,
 			    unsigned long address)
 {
<span class="p_del">-	struct page *page = alloc_page(GFP_KERNEL | __GFP_NOTRACK |</span>
<span class="p_del">-				       __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+	struct page *page = alloc_page(GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
 	pte_t *pte = NULL;
 
 	if (page)
<span class="p_chunk">@@ -2725,8 +2724,7 @@</span> <span class="p_context"> pte_t *pte_alloc_one_kernel(struct mm_struct *mm,</span>
 pgtable_t pte_alloc_one(struct mm_struct *mm,
 			unsigned long address)
 {
<span class="p_del">-	struct page *page = alloc_page(GFP_KERNEL | __GFP_NOTRACK |</span>
<span class="p_del">-				       __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+	struct page *page = alloc_page(GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
 	if (!page)
 		return NULL;
 	if (!pgtable_page_ctor(page)) {
<span class="p_header">diff --git a/arch/um/kernel/mem.c b/arch/um/kernel/mem.c</span>
<span class="p_header">index b2a2dff50b4e..e7437ec62710 100644</span>
<span class="p_header">--- a/arch/um/kernel/mem.c</span>
<span class="p_header">+++ b/arch/um/kernel/mem.c</span>
<span class="p_chunk">@@ -204,7 +204,7 @@</span> <span class="p_context"> pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)</span>
 {
 	pte_t *pte;
 
<span class="p_del">-	pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO);</span>
<span class="p_add">+	pte = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_ZERO);</span>
 	return pte;
 }
 
<span class="p_chunk">@@ -212,7 +212,7 @@</span> <span class="p_context"> pgtable_t pte_alloc_one(struct mm_struct *mm, unsigned long address)</span>
 {
 	struct page *pte;
 
<span class="p_del">-	pte = alloc_page(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO);</span>
<span class="p_add">+	pte = alloc_page(GFP_KERNEL|__GFP_ZERO);</span>
 	if (!pte)
 		return NULL;
 	if (!pgtable_page_ctor(pte)) {
<span class="p_header">diff --git a/arch/x86/include/asm/pgalloc.h b/arch/x86/include/asm/pgalloc.h</span>
<span class="p_header">index bf7f8b55b0f9..574c23cf761a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -81,7 +81,7 @@</span> <span class="p_context"> static inline void pmd_populate(struct mm_struct *mm, pmd_t *pmd,</span>
 static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long addr)
 {
 	struct page *page;
<span class="p_del">-	page = alloc_pages(GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO, 0);</span>
<span class="p_add">+	page = alloc_pages(GFP_KERNEL |  __GFP_ZERO, 0);</span>
 	if (!page)
 		return NULL;
 	if (!pgtable_pmd_page_ctor(page)) {
<span class="p_chunk">@@ -125,7 +125,7 @@</span> <span class="p_context"> static inline void pgd_populate(struct mm_struct *mm, pgd_t *pgd, pud_t *pud)</span>
 
 static inline pud_t *pud_alloc_one(struct mm_struct *mm, unsigned long addr)
 {
<span class="p_del">-	return (pud_t *)get_zeroed_page(GFP_KERNEL|__GFP_REPEAT);</span>
<span class="p_add">+	return (pud_t *)get_zeroed_page(GFP_KERNEL);</span>
 }
 
 static inline void pud_free(struct mm_struct *mm, pud_t *pud)
<span class="p_header">diff --git a/arch/x86/xen/p2m.c b/arch/x86/xen/p2m.c</span>
<span class="p_header">index cab9f766bb06..dd2a49a8aacc 100644</span>
<span class="p_header">--- a/arch/x86/xen/p2m.c</span>
<span class="p_header">+++ b/arch/x86/xen/p2m.c</span>
<span class="p_chunk">@@ -182,7 +182,7 @@</span> <span class="p_context"> static void * __ref alloc_p2m_page(void)</span>
 	if (unlikely(!slab_is_available()))
 		return alloc_bootmem_align(PAGE_SIZE, PAGE_SIZE);
 
<span class="p_del">-	return (void *)__get_free_page(GFP_KERNEL | __GFP_REPEAT);</span>
<span class="p_add">+	return (void *)__get_free_page(GFP_KERNEL);</span>
 }
 
 static void __ref free_p2m_page(void *p)
<span class="p_header">diff --git a/arch/xtensa/include/asm/pgalloc.h b/arch/xtensa/include/asm/pgalloc.h</span>
<span class="p_header">index d38eb9237e64..1065bc8bcae5 100644</span>
<span class="p_header">--- a/arch/xtensa/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/xtensa/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -44,7 +44,7 @@</span> <span class="p_context"> static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,</span>
 	pte_t *ptep;
 	int i;
 
<span class="p_del">-	ptep = (pte_t *)__get_free_page(GFP_KERNEL|__GFP_REPEAT);</span>
<span class="p_add">+	ptep = (pte_t *)__get_free_page(GFP_KERNEL);</span>
 	if (!ptep)
 		return NULL;
 	for (i = 0; i &lt; 1024; i++)
<span class="p_header">diff --git a/drivers/block/aoe/aoecmd.c b/drivers/block/aoe/aoecmd.c</span>
<span class="p_header">index ad80c85e0857..5ac30cbba002 100644</span>
<span class="p_header">--- a/drivers/block/aoe/aoecmd.c</span>
<span class="p_header">+++ b/drivers/block/aoe/aoecmd.c</span>
<span class="p_chunk">@@ -1750,7 +1750,7 @@</span> <span class="p_context"> aoecmd_init(void)</span>
 	int ret;
 
 	/* get_zeroed_page returns page with ref count 1 */
<span class="p_del">-	p = (void *) get_zeroed_page(GFP_KERNEL | __GFP_REPEAT);</span>
<span class="p_add">+	p = (void *) get_zeroed_page(GFP_KERNEL);</span>
 	if (!p)
 		return -ENOMEM;
 	empty_page = virt_to_page(p);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



