
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[2/3] sched: Introduce struct rq_flags - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [2/3] sched: Introduce struct rq_flags</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 28, 2016, 3:41 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160428154448.580698005@infradead.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8972211/mbox/"
   >mbox</a>
|
   <a href="/patch/8972211/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8972211/">/patch/8972211/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 41D789F1D3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 28 Apr 2016 15:46:15 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 17D4D20256
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 28 Apr 2016 15:46:14 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id C5529202F2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 28 Apr 2016 15:46:12 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752990AbcD1Pp5 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 28 Apr 2016 11:45:57 -0400
Received: from merlin.infradead.org ([205.233.59.134]:47073 &quot;EHLO
	merlin.infradead.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751513AbcD1Ppl (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 28 Apr 2016 11:45:41 -0400
Received: from j217100.upc-j.chello.nl ([24.132.217.100] helo=twins)
	by merlin.infradead.org with esmtpsa (Exim 4.85_2 #1 (Red Hat Linux))
	id 1avo89-0005fz-MA; Thu, 28 Apr 2016 15:45:29 +0000
Received: by twins (Postfix, from userid 0)
	id D4147101F7606; Thu, 28 Apr 2016 17:45:29 +0200 (CEST)
Message-Id: &lt;20160428154448.580698005@infradead.org&gt;
User-Agent: quilt/0.61-1
Date: Thu, 28 Apr 2016 17:41:01 +0200
From: Peter Zijlstra &lt;peterz@infradead.org&gt;
To: mingo@kernel.org, tglx@linutronix.de, mgalbraith@suse.de
Cc: linux-kernel@vger.kernel.org,
	&quot;Peter Zijlstra (Intel)&quot; &lt;peterz@infradead.org&gt;
Subject: [PATCH 2/3] sched: Introduce struct rq_flags
References: &lt;20160428154059.606567811@infradead.org&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Disposition: inline; filename=peterz-sched-task_rq_lock-flags.patch
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - April 28, 2016, 3:41 p.m.</div>
<pre class="content">
In order to pass around more than just the IRQ flags, add a rq_flags
structure.

No difference in code generation for the x86_64-defconfig build I
tested.
<span class="signed-off-by">
Signed-off-by: Peter Zijlstra (Intel) &lt;peterz@infradead.org&gt;</span>
---
 kernel/sched/core.c     |   98 ++++++++++++++++++++++++------------------------
 kernel/sched/deadline.c |    6 +-
 kernel/sched/sched.h    |   14 ++++--
 3 files changed, 62 insertions(+), 56 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -173,7 +173,7 @@</span> <span class="p_context"> static struct rq *this_rq_lock(void)</span>
 /*
  * __task_rq_lock - lock the rq @p resides on.
  */
<span class="p_del">-struct rq *__task_rq_lock(struct task_struct *p)</span>
<span class="p_add">+struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)</span>
 	__acquires(rq-&gt;lock)
 {
 	struct rq *rq;
<span class="p_chunk">@@ -197,14 +197,14 @@</span> <span class="p_context"> struct rq *__task_rq_lock(struct task_st</span>
 /*
  * task_rq_lock - lock p-&gt;pi_lock and lock the rq @p resides on.
  */
<span class="p_del">-struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)</span>
<span class="p_add">+struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)</span>
 	__acquires(p-&gt;pi_lock)
 	__acquires(rq-&gt;lock)
 {
 	struct rq *rq;
 
 	for (;;) {
<span class="p_del">-		raw_spin_lock_irqsave(&amp;p-&gt;pi_lock, *flags);</span>
<span class="p_add">+		raw_spin_lock_irqsave(&amp;p-&gt;pi_lock, rf-&gt;flags);</span>
 		rq = task_rq(p);
 		raw_spin_lock(&amp;rq-&gt;lock);
 		/*
<span class="p_chunk">@@ -228,7 +228,7 @@</span> <span class="p_context"> struct rq *task_rq_lock(struct task_stru</span>
 			return rq;
 		}
 		raw_spin_unlock(&amp;rq-&gt;lock);
<span class="p_del">-		raw_spin_unlock_irqrestore(&amp;p-&gt;pi_lock, *flags);</span>
<span class="p_add">+		raw_spin_unlock_irqrestore(&amp;p-&gt;pi_lock, rf-&gt;flags);</span>
 
 		while (unlikely(task_on_rq_migrating(p)))
 			cpu_relax();
<span class="p_chunk">@@ -1147,12 +1147,12 @@</span> <span class="p_context"> void do_set_cpus_allowed(struct task_str</span>
 static int __set_cpus_allowed_ptr(struct task_struct *p,
 				  const struct cpumask *new_mask, bool check)
 {
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-	struct rq *rq;</span>
 	unsigned int dest_cpu;
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct rq *rq;</span>
 	int ret = 0;
 
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 
 	/*
 	 * Must re-check here, to close a race against __kthread_bind(),
<span class="p_chunk">@@ -1181,7 +1181,7 @@</span> <span class="p_context"> static int __set_cpus_allowed_ptr(struct</span>
 	if (task_running(rq, p) || p-&gt;state == TASK_WAKING) {
 		struct migration_arg arg = { p, dest_cpu };
 		/* Need help from migration thread: drop lock and wait. */
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 		stop_one_cpu(cpu_of(rq), migration_cpu_stop, &amp;arg);
 		tlb_migrate_finish(p-&gt;mm);
 		return 0;
<span class="p_chunk">@@ -1195,7 +1195,7 @@</span> <span class="p_context"> static int __set_cpus_allowed_ptr(struct</span>
 		lockdep_pin_lock(&amp;rq-&gt;lock);
 	}
 out:
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -1379,8 +1379,8 @@</span> <span class="p_context"> int migrate_swap(struct task_struct *cur</span>
  */
 unsigned long wait_task_inactive(struct task_struct *p, long match_state)
 {
<span class="p_del">-	unsigned long flags;</span>
 	int running, queued;
<span class="p_add">+	struct rq_flags rf;</span>
 	unsigned long ncsw;
 	struct rq *rq;
 
<span class="p_chunk">@@ -1415,14 +1415,14 @@</span> <span class="p_context"> unsigned long wait_task_inactive(struct</span>
 		 * lock now, to be *sure*. If we&#39;re wrong, we&#39;ll
 		 * just go back and repeat.
 		 */
<span class="p_del">-		rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+		rq = task_rq_lock(p, &amp;rf);</span>
 		trace_sched_wait_task(p);
 		running = task_running(rq, p);
 		queued = task_on_rq_queued(p);
 		ncsw = 0;
 		if (!match_state || p-&gt;state == match_state)
 			ncsw = p-&gt;nvcsw | LONG_MIN; /* sets MSB */
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 
 		/*
 		 * If it changed from the expected state, bail out now.
<span class="p_chunk">@@ -1720,17 +1720,18 @@</span> <span class="p_context"> ttwu_do_activate(struct rq *rq, struct t</span>
  */
 static int ttwu_remote(struct task_struct *p, int wake_flags)
 {
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 	int ret = 0;
 
<span class="p_del">-	rq = __task_rq_lock(p);</span>
<span class="p_add">+	rq = __task_rq_lock(p, &amp;rf);</span>
 	if (task_on_rq_queued(p)) {
 		/* check_preempt_curr() may use rq clock */
 		update_rq_clock(rq);
 		ttwu_do_wakeup(rq, p, wake_flags);
 		ret = 1;
 	}
<span class="p_del">-	__task_rq_unlock(rq);</span>
<span class="p_add">+	__task_rq_unlock(rq, &amp;rf);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -2483,12 +2484,12 @@</span> <span class="p_context"> extern void init_dl_bw(struct dl_bw *dl_</span>
  */
 void wake_up_new_task(struct task_struct *p)
 {
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 
<span class="p_del">-	raw_spin_lock_irqsave(&amp;p-&gt;pi_lock, flags);</span>
 	/* Initialize new task&#39;s runnable average */
 	init_entity_runnable_average(&amp;p-&gt;se);
<span class="p_add">+	raw_spin_lock_irqsave(&amp;p-&gt;pi_lock, rf.flags);</span>
 #ifdef CONFIG_SMP
 	/*
 	 * Fork balancing, do it here and not earlier because:
<span class="p_chunk">@@ -2500,7 +2501,7 @@</span> <span class="p_context"> void wake_up_new_task(struct task_struct</span>
 	/* Post initialize new task&#39;s util average when its cfs_rq is set */
 	post_init_entity_util_avg(&amp;p-&gt;se);
 
<span class="p_del">-	rq = __task_rq_lock(p);</span>
<span class="p_add">+	rq = __task_rq_lock(p, &amp;rf);</span>
 	activate_task(rq, p, 0);
 	p-&gt;on_rq = TASK_ON_RQ_QUEUED;
 	trace_sched_wakeup_new(p);
<span class="p_chunk">@@ -2516,7 +2517,7 @@</span> <span class="p_context"> void wake_up_new_task(struct task_struct</span>
 		lockdep_pin_lock(&amp;rq-&gt;lock);
 	}
 #endif
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 }
 
 #ifdef CONFIG_PREEMPT_NOTIFIERS
<span class="p_chunk">@@ -2932,7 +2933,7 @@</span> <span class="p_context"> EXPORT_PER_CPU_SYMBOL(kernel_cpustat);</span>
  */
 unsigned long long task_sched_runtime(struct task_struct *p)
 {
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 	u64 ns;
 
<span class="p_chunk">@@ -2952,7 +2953,7 @@</span> <span class="p_context"> unsigned long long task_sched_runtime(st</span>
 		return p-&gt;se.sum_exec_runtime;
 #endif
 
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 	/*
 	 * Must be -&gt;curr _and_ -&gt;on_rq.  If dequeued, we would
 	 * project cycles that may never be accounted to this
<span class="p_chunk">@@ -2963,7 +2964,7 @@</span> <span class="p_context"> unsigned long long task_sched_runtime(st</span>
 		p-&gt;sched_class-&gt;update_curr(rq);
 	}
 	ns = p-&gt;se.sum_exec_runtime;
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 
 	return ns;
 }
<span class="p_chunk">@@ -3521,12 +3522,13 @@</span> <span class="p_context"> EXPORT_SYMBOL(default_wake_function);</span>
 void rt_mutex_setprio(struct task_struct *p, int prio)
 {
 	int oldprio, queued, running, queue_flag = DEQUEUE_SAVE | DEQUEUE_MOVE;
<span class="p_del">-	struct rq *rq;</span>
 	const struct sched_class *prev_class;
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct rq *rq;</span>
 
 	BUG_ON(prio &gt; MAX_PRIO);
 
<span class="p_del">-	rq = __task_rq_lock(p);</span>
<span class="p_add">+	rq = __task_rq_lock(p, &amp;rf);</span>
 
 	/*
 	 * Idle task boosting is a nono in general. There is one
<span class="p_chunk">@@ -3602,7 +3604,7 @@</span> <span class="p_context"> void rt_mutex_setprio(struct task_struct</span>
 	check_class_changed(rq, p, prev_class, oldprio);
 out_unlock:
 	preempt_disable(); /* avoid rq from going away on us */
<span class="p_del">-	__task_rq_unlock(rq);</span>
<span class="p_add">+	__task_rq_unlock(rq, &amp;rf);</span>
 
 	balance_callback(rq);
 	preempt_enable();
<span class="p_chunk">@@ -3612,7 +3614,7 @@</span> <span class="p_context"> void rt_mutex_setprio(struct task_struct</span>
 void set_user_nice(struct task_struct *p, long nice)
 {
 	int old_prio, delta, queued;
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 
 	if (task_nice(p) == nice || nice &lt; MIN_NICE || nice &gt; MAX_NICE)
<span class="p_chunk">@@ -3621,7 +3623,7 @@</span> <span class="p_context"> void set_user_nice(struct task_struct *p</span>
 	 * We have to be careful, if called from sys_setpriority(),
 	 * the task might be in the middle of scheduling on another CPU.
 	 */
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 	/*
 	 * The RT priorities are set via sched_setscheduler(), but we still
 	 * allow the &#39;normal&#39; nice value to be set - but as expected
<span class="p_chunk">@@ -3652,7 +3654,7 @@</span> <span class="p_context"> void set_user_nice(struct task_struct *p</span>
 			resched_curr(rq);
 	}
 out_unlock:
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 }
 EXPORT_SYMBOL(set_user_nice);
 
<span class="p_chunk">@@ -3949,11 +3951,11 @@</span> <span class="p_context"> static int __sched_setscheduler(struct t</span>
 		      MAX_RT_PRIO - 1 - attr-&gt;sched_priority;
 	int retval, oldprio, oldpolicy = -1, queued, running;
 	int new_effective_prio, policy = attr-&gt;sched_policy;
<span class="p_del">-	unsigned long flags;</span>
 	const struct sched_class *prev_class;
<span class="p_del">-	struct rq *rq;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	int reset_on_fork;
 	int queue_flags = DEQUEUE_SAVE | DEQUEUE_MOVE;
<span class="p_add">+	struct rq *rq;</span>
 
 	/* may grab non-irq protected spin_locks */
 	BUG_ON(in_interrupt());
<span class="p_chunk">@@ -4048,13 +4050,13 @@</span> <span class="p_context"> static int __sched_setscheduler(struct t</span>
 	 * To be able to change p-&gt;policy safely, the appropriate
 	 * runqueue lock must be held.
 	 */
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 
 	/*
 	 * Changing the policy of the stop threads its a very bad idea
 	 */
 	if (p == rq-&gt;stop) {
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 		return -EINVAL;
 	}
 
<span class="p_chunk">@@ -4071,7 +4073,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct t</span>
 			goto change;
 
 		p-&gt;sched_reset_on_fork = reset_on_fork;
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 		return 0;
 	}
 change:
<span class="p_chunk">@@ -4085,7 +4087,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct t</span>
 		if (rt_bandwidth_enabled() &amp;&amp; rt_policy(policy) &amp;&amp;
 				task_group(p)-&gt;rt_bandwidth.rt_runtime == 0 &amp;&amp;
 				!task_group_is_autogroup(task_group(p))) {
<span class="p_del">-			task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+			task_rq_unlock(rq, p, &amp;rf);</span>
 			return -EPERM;
 		}
 #endif
<span class="p_chunk">@@ -4100,7 +4102,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct t</span>
 			 */
 			if (!cpumask_subset(span, &amp;p-&gt;cpus_allowed) ||
 			    rq-&gt;rd-&gt;dl_bw.bw == 0) {
<span class="p_del">-				task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+				task_rq_unlock(rq, p, &amp;rf);</span>
 				return -EPERM;
 			}
 		}
<span class="p_chunk">@@ -4110,7 +4112,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct t</span>
 	/* recheck policy now with rq lock held */
 	if (unlikely(oldpolicy != -1 &amp;&amp; oldpolicy != p-&gt;policy)) {
 		policy = oldpolicy = -1;
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 		goto recheck;
 	}
 
<span class="p_chunk">@@ -4120,7 +4122,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct t</span>
 	 * is available.
 	 */
 	if ((dl_policy(policy) || dl_task(p)) &amp;&amp; dl_overflow(p, policy, attr)) {
<span class="p_del">-		task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+		task_rq_unlock(rq, p, &amp;rf);</span>
 		return -EBUSY;
 	}
 
<span class="p_chunk">@@ -4165,7 +4167,7 @@</span> <span class="p_context"> static int __sched_setscheduler(struct t</span>
 
 	check_class_changed(rq, p, prev_class, oldprio);
 	preempt_disable(); /* avoid rq from going away on us */
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 
 	if (pi)
 		rt_mutex_adjust_pi(p);
<span class="p_chunk">@@ -5018,10 +5020,10 @@</span> <span class="p_context"> SYSCALL_DEFINE2(sched_rr_get_interval, p</span>
 {
 	struct task_struct *p;
 	unsigned int time_slice;
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct timespec t;</span>
 	struct rq *rq;
 	int retval;
<span class="p_del">-	struct timespec t;</span>
 
 	if (pid &lt; 0)
 		return -EINVAL;
<span class="p_chunk">@@ -5036,11 +5038,11 @@</span> <span class="p_context"> SYSCALL_DEFINE2(sched_rr_get_interval, p</span>
 	if (retval)
 		goto out_unlock;
 
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 	time_slice = 0;
 	if (p-&gt;sched_class-&gt;get_rr_interval)
 		time_slice = p-&gt;sched_class-&gt;get_rr_interval(rq, p);
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 
 	rcu_read_unlock();
 	jiffies_to_timespec(time_slice, &amp;t);
<span class="p_chunk">@@ -5304,11 +5306,11 @@</span> <span class="p_context"> int migrate_task_to(struct task_struct *</span>
  */
 void sched_setnuma(struct task_struct *p, int nid)
 {
<span class="p_del">-	struct rq *rq;</span>
<span class="p_del">-	unsigned long flags;</span>
 	bool queued, running;
<span class="p_add">+	struct rq_flags rf;</span>
<span class="p_add">+	struct rq *rq;</span>
 
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 	queued = task_on_rq_queued(p);
 	running = task_current(rq, p);
 
<span class="p_chunk">@@ -5323,7 +5325,7 @@</span> <span class="p_context"> void sched_setnuma(struct task_struct *p</span>
 		p-&gt;sched_class-&gt;set_curr_task(rq);
 	if (queued)
 		enqueue_task(rq, p, ENQUEUE_RESTORE);
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 }
 #endif /* CONFIG_NUMA_BALANCING */
 
<span class="p_chunk">@@ -7754,10 +7756,10 @@</span> <span class="p_context"> void sched_move_task(struct task_struct</span>
 {
 	struct task_group *tg;
 	int queued, running;
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 
<span class="p_del">-	rq = task_rq_lock(tsk, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(tsk, &amp;rf);</span>
 
 	running = task_current(rq, tsk);
 	queued = task_on_rq_queued(tsk);
<span class="p_chunk">@@ -7789,7 +7791,7 @@</span> <span class="p_context"> void sched_move_task(struct task_struct</span>
 	if (queued)
 		enqueue_task(rq, tsk, ENQUEUE_RESTORE | ENQUEUE_MOVE);
 
<span class="p_del">-	task_rq_unlock(rq, tsk, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, tsk, &amp;rf);</span>
 }
 #endif /* CONFIG_CGROUP_SCHED */
 
<span class="p_header">--- a/kernel/sched/deadline.c</span>
<span class="p_header">+++ b/kernel/sched/deadline.c</span>
<span class="p_chunk">@@ -591,10 +591,10 @@</span> <span class="p_context"> static enum hrtimer_restart dl_task_time</span>
 						     struct sched_dl_entity,
 						     dl_timer);
 	struct task_struct *p = dl_task_of(dl_se);
<span class="p_del">-	unsigned long flags;</span>
<span class="p_add">+	struct rq_flags rf;</span>
 	struct rq *rq;
 
<span class="p_del">-	rq = task_rq_lock(p, &amp;flags);</span>
<span class="p_add">+	rq = task_rq_lock(p, &amp;rf);</span>
 
 	/*
 	 * The task might have changed its scheduling policy to something
<span class="p_chunk">@@ -677,7 +677,7 @@</span> <span class="p_context"> static enum hrtimer_restart dl_task_time</span>
 #endif
 
 unlock:
<span class="p_del">-	task_rq_unlock(rq, p, &amp;flags);</span>
<span class="p_add">+	task_rq_unlock(rq, p, &amp;rf);</span>
 
 	/*
 	 * This can free the task_struct, including this hrtimer, do not touch
<span class="p_header">--- a/kernel/sched/sched.h</span>
<span class="p_header">+++ b/kernel/sched/sched.h</span>
<span class="p_chunk">@@ -1451,13 +1451,17 @@</span> <span class="p_context"> static inline void sched_rt_avg_update(s</span>
 static inline void sched_avg_update(struct rq *rq) { }
 #endif
 
<span class="p_del">-struct rq *__task_rq_lock(struct task_struct *p)</span>
<span class="p_add">+struct rq_flags {</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct rq *__task_rq_lock(struct task_struct *p, struct rq_flags *rf)</span>
 	__acquires(rq-&gt;lock);
<span class="p_del">-struct rq *task_rq_lock(struct task_struct *p, unsigned long *flags)</span>
<span class="p_add">+struct rq *task_rq_lock(struct task_struct *p, struct rq_flags *rf)</span>
 	__acquires(p-&gt;pi_lock)
 	__acquires(rq-&gt;lock);
 
<span class="p_del">-static inline void __task_rq_unlock(struct rq *rq)</span>
<span class="p_add">+static inline void __task_rq_unlock(struct rq *rq, struct rq_flags *rf)</span>
 	__releases(rq-&gt;lock)
 {
 	lockdep_unpin_lock(&amp;rq-&gt;lock);
<span class="p_chunk">@@ -1465,13 +1469,13 @@</span> <span class="p_context"> static inline void __task_rq_unlock(stru</span>
 }
 
 static inline void
<span class="p_del">-task_rq_unlock(struct rq *rq, struct task_struct *p, unsigned long *flags)</span>
<span class="p_add">+task_rq_unlock(struct rq *rq, struct task_struct *p, struct rq_flags *rf)</span>
 	__releases(rq-&gt;lock)
 	__releases(p-&gt;pi_lock)
 {
 	lockdep_unpin_lock(&amp;rq-&gt;lock);
 	raw_spin_unlock(&amp;rq-&gt;lock);
<span class="p_del">-	raw_spin_unlock_irqrestore(&amp;p-&gt;pi_lock, *flags);</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;p-&gt;pi_lock, rf-&gt;flags);</span>
 }
 
 #ifdef CONFIG_SMP

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



