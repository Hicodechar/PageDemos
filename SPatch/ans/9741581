
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4/7] RISC-V: arch/riscv/include - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4/7] RISC-V: arch/riscv/include</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 23, 2017, 12:41 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170523004107.536-5-palmer@dabbelt.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9741581/mbox/"
   >mbox</a>
|
   <a href="/patch/9741581/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9741581/">/patch/9741581/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	43C18601C2 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 23 May 2017 00:43:47 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 294A52876F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 23 May 2017 00:43:47 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 1B71D28779; Tue, 23 May 2017 00:43:47 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, RCVD_IN_DNSWL_HI,
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6167C2877B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 23 May 2017 00:43:39 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S935551AbdEWAnb (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 22 May 2017 20:43:31 -0400
Received: from mail-pf0-f193.google.com ([209.85.192.193]:33308 &quot;EHLO
	mail-pf0-f193.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S964927AbdEWAnM (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 22 May 2017 20:43:12 -0400
Received: by mail-pf0-f193.google.com with SMTP id f27so23455016pfe.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 22 May 2017 17:43:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=dabbelt-com.20150623.gappssmtp.com; s=20150623;
	h=from:to:to:to:cc:cc:subject:date:message-id:in-reply-to:references; 
	bh=mA/yxJDYH0MRmPnzBbqNoFjGMhWJhjDgzWkn/+maai8=;
	b=GLLf5As5lnbLjbuvMqkuktYDsppihCgDqfCdKHWRGCx4pcEPowBPRXNF4oiZKWBiTn
	DsjNB9i13P+3/K0aQ7fDVhMPCoV0VY1GxR1x4nHa+PG12+HnNHY4imkuwFgPvXlG0fL9
	jU8oVMa3WRETnsqgOMctkhA2275E1qn41WRCfge4/RTJpnBBVRar/notHAKTS5NzEhry
	xwFzUyrNTnAlTOsN3ZVuaQdQOojd7ZIm7CWCCgFxZ47bhfzJ+sndZszhZzdn+Vz1pH6h
	RTvSku1p/UdVoIzm3El8yErClma93skIHIvlutwd+HhLTG9nySgAaCh5CtdbqKfPHq1V
	fVPQ==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:to:to:cc:cc:subject:date:message-id
	:in-reply-to:references;
	bh=mA/yxJDYH0MRmPnzBbqNoFjGMhWJhjDgzWkn/+maai8=;
	b=G0Zm9JivazRLbbBcb22/5M6e7RbtdnbVivL9IIr0WuxdcgCL4Ktb6kY8wqhmeVGtzA
	9kx1jUUw25VpC8Z7FwfmunRG1DSJAuwOuUEpLixh44aMvrIusDnY9M6fzUmyd0pTTI05
	s/iZ3qePFTX+DCRzNQyAE2k2rcBj7Gvx4CHho8wq8bVB6cgcGkUFDyGfxr+1MXxrSazQ
	rFpfctLRCBtWDdK3dgeNlMOUhlKyKGmO3KW4ZWhyuwm37BZn+MgTSvZfPbYu3VctAskW
	2FSTEUxRJMtvXJR0o1kk4rm4UJCJ4TTPVFVjTiaf+HwMgYuWb+ZPcjVbppstzh7V95ih
	1FtQ==
X-Gm-Message-State: AODbwcDdAqw7JrApu7Aa0UwcsJi4yplwDjid4lGEuDbsbjSKIuTpqxzi
	myfv9nmmzGgFw9uw
X-Received: by 10.84.254.70 with SMTP id a6mr32389311pln.64.1495500188823;
	Mon, 22 May 2017 17:43:08 -0700 (PDT)
Received: from localhost (c-24-5-193-41.hsd1.ca.comcast.net. [24.5.193.41])
	by smtp.gmail.com with ESMTPSA id
	b72sm22917279pfj.36.2017.05.22.17.43.07
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 22 May 2017 17:43:08 -0700 (PDT)
From: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
To: linux-kernel@vger.kernel.org
To: Arnd Bergmann &lt;arnd@arndb.de&gt;
To: olof@lixom.net
Cc: albert@sifive.com
Cc: Palmer Dabbelt &lt;palmer@dabbelt.com&gt;
Subject: [PATCH 4/7] RISC-V: arch/riscv/include
Date: Mon, 22 May 2017 17:41:04 -0700
Message-Id: &lt;20170523004107.536-5-palmer@dabbelt.com&gt;
X-Mailer: git-send-email 2.13.0
In-Reply-To: &lt;20170523004107.536-1-palmer@dabbelt.com&gt;
References: &lt;20170523004107.536-1-palmer@dabbelt.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - May 23, 2017, 12:41 a.m.</div>
<pre class="content">
---
 arch/riscv/include/asm/Kbuild             |  60 ++++
 arch/riscv/include/asm/asm-offsets.h      |   1 +
 arch/riscv/include/asm/asm.h              |  65 +++++
 arch/riscv/include/asm/atomic.h           | 349 +++++++++++++++++++++++
 arch/riscv/include/asm/atomic64.h         | 355 +++++++++++++++++++++++
 arch/riscv/include/asm/barrier.h          |  33 +++
 arch/riscv/include/asm/bitops.h           | 229 +++++++++++++++
 arch/riscv/include/asm/bug.h              |  81 ++++++
 arch/riscv/include/asm/cache.h            |  23 ++
 arch/riscv/include/asm/cacheflush.h       |  40 +++
 arch/riscv/include/asm/cmpxchg.h          | 125 ++++++++
 arch/riscv/include/asm/csr.h              | 126 +++++++++
 arch/riscv/include/asm/delay.h            |  29 ++
 arch/riscv/include/asm/device.h           |  28 ++
 arch/riscv/include/asm/dma-mapping.h      |  61 ++++
 arch/riscv/include/asm/elf.h              |  83 ++++++
 arch/riscv/include/asm/io.h               |  36 +++
 arch/riscv/include/asm/irq.h              |  32 +++
 arch/riscv/include/asm/irqflags.h         |  64 +++++
 arch/riscv/include/asm/kprobes.h          |  23 ++
 arch/riscv/include/asm/linkage.h          |  21 ++
 arch/riscv/include/asm/mmu.h              |  27 ++
 arch/riscv/include/asm/mmu_context.h      |  70 +++++
 arch/riscv/include/asm/page.h             | 138 +++++++++
 arch/riscv/include/asm/pci.h              |  51 ++++
 arch/riscv/include/asm/pgalloc.h          | 126 +++++++++
 arch/riscv/include/asm/pgtable-32.h       |  26 ++
 arch/riscv/include/asm/pgtable-64.h       |  85 ++++++
 arch/riscv/include/asm/pgtable-bits.h     |  49 ++++
 arch/riscv/include/asm/pgtable.h          | 426 ++++++++++++++++++++++++++++
 arch/riscv/include/asm/processor.h        | 103 +++++++
 arch/riscv/include/asm/ptrace.h           | 117 ++++++++
 arch/riscv/include/asm/sbi.h              | 101 +++++++
 arch/riscv/include/asm/serial.h           |  43 +++
 arch/riscv/include/asm/setup.h            |  20 ++
 arch/riscv/include/asm/smp.h              |  42 +++
 arch/riscv/include/asm/spinlock.h         | 156 ++++++++++
 arch/riscv/include/asm/spinlock_types.h   |  34 +++
 arch/riscv/include/asm/string.h           |  31 ++
 arch/riscv/include/asm/switch_to.h        |  71 +++++
 arch/riscv/include/asm/syscall.h          |  91 ++++++
 arch/riscv/include/asm/syscalls.h         |  26 ++
 arch/riscv/include/asm/thread_info.h      | 103 +++++++
 arch/riscv/include/asm/timex.h            |  55 ++++
 arch/riscv/include/asm/tlb.h              |  25 ++
 arch/riscv/include/asm/tlbflush.h         |  95 +++++++
 arch/riscv/include/asm/uaccess.h          | 455 ++++++++++++++++++++++++++++++
 arch/riscv/include/asm/unistd.h           |  17 ++
 arch/riscv/include/asm/vdso.h             |  32 +++
 arch/riscv/include/asm/word-at-a-time.h   |  56 ++++
 arch/riscv/include/uapi/asm/Kbuild        |  10 +
 arch/riscv/include/uapi/asm/auxvec.h      |  24 ++
 arch/riscv/include/uapi/asm/bitsperlong.h |  25 ++
 arch/riscv/include/uapi/asm/byteorder.h   |  23 ++
 arch/riscv/include/uapi/asm/elf.h         |  83 ++++++
 arch/riscv/include/uapi/asm/ptrace.h      |  69 +++++
 arch/riscv/include/uapi/asm/sigcontext.h  |  30 ++
 arch/riscv/include/uapi/asm/siginfo.h     |  24 ++
 arch/riscv/include/uapi/asm/unistd.h      |  23 ++
 59 files changed, 4846 insertions(+)
 create mode 100644 arch/riscv/include/asm/Kbuild
 create mode 100644 arch/riscv/include/asm/asm-offsets.h
 create mode 100644 arch/riscv/include/asm/asm.h
 create mode 100644 arch/riscv/include/asm/atomic.h
 create mode 100644 arch/riscv/include/asm/atomic64.h
 create mode 100644 arch/riscv/include/asm/barrier.h
 create mode 100644 arch/riscv/include/asm/bitops.h
 create mode 100644 arch/riscv/include/asm/bug.h
 create mode 100644 arch/riscv/include/asm/cache.h
 create mode 100644 arch/riscv/include/asm/cacheflush.h
 create mode 100644 arch/riscv/include/asm/cmpxchg.h
 create mode 100644 arch/riscv/include/asm/csr.h
 create mode 100644 arch/riscv/include/asm/delay.h
 create mode 100644 arch/riscv/include/asm/device.h
 create mode 100644 arch/riscv/include/asm/dma-mapping.h
 create mode 100644 arch/riscv/include/asm/elf.h
 create mode 100644 arch/riscv/include/asm/io.h
 create mode 100644 arch/riscv/include/asm/irq.h
 create mode 100644 arch/riscv/include/asm/irqflags.h
 create mode 100644 arch/riscv/include/asm/kprobes.h
 create mode 100644 arch/riscv/include/asm/linkage.h
 create mode 100644 arch/riscv/include/asm/mmu.h
 create mode 100644 arch/riscv/include/asm/mmu_context.h
 create mode 100644 arch/riscv/include/asm/page.h
 create mode 100644 arch/riscv/include/asm/pci.h
 create mode 100644 arch/riscv/include/asm/pgalloc.h
 create mode 100644 arch/riscv/include/asm/pgtable-32.h
 create mode 100644 arch/riscv/include/asm/pgtable-64.h
 create mode 100644 arch/riscv/include/asm/pgtable-bits.h
 create mode 100644 arch/riscv/include/asm/pgtable.h
 create mode 100644 arch/riscv/include/asm/processor.h
 create mode 100644 arch/riscv/include/asm/ptrace.h
 create mode 100644 arch/riscv/include/asm/sbi.h
 create mode 100644 arch/riscv/include/asm/serial.h
 create mode 100644 arch/riscv/include/asm/setup.h
 create mode 100644 arch/riscv/include/asm/smp.h
 create mode 100644 arch/riscv/include/asm/spinlock.h
 create mode 100644 arch/riscv/include/asm/spinlock_types.h
 create mode 100644 arch/riscv/include/asm/string.h
 create mode 100644 arch/riscv/include/asm/switch_to.h
 create mode 100644 arch/riscv/include/asm/syscall.h
 create mode 100644 arch/riscv/include/asm/syscalls.h
 create mode 100644 arch/riscv/include/asm/thread_info.h
 create mode 100644 arch/riscv/include/asm/timex.h
 create mode 100644 arch/riscv/include/asm/tlb.h
 create mode 100644 arch/riscv/include/asm/tlbflush.h
 create mode 100644 arch/riscv/include/asm/uaccess.h
 create mode 100644 arch/riscv/include/asm/unistd.h
 create mode 100644 arch/riscv/include/asm/vdso.h
 create mode 100644 arch/riscv/include/asm/word-at-a-time.h
 create mode 100644 arch/riscv/include/uapi/asm/Kbuild
 create mode 100644 arch/riscv/include/uapi/asm/auxvec.h
 create mode 100644 arch/riscv/include/uapi/asm/bitsperlong.h
 create mode 100644 arch/riscv/include/uapi/asm/byteorder.h
 create mode 100644 arch/riscv/include/uapi/asm/elf.h
 create mode 100644 arch/riscv/include/uapi/asm/ptrace.h
 create mode 100644 arch/riscv/include/uapi/asm/sigcontext.h
 create mode 100644 arch/riscv/include/uapi/asm/siginfo.h
 create mode 100644 arch/riscv/include/uapi/asm/unistd.h
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - May 23, 2017, 12:55 p.m.</div>
<pre class="content">
On Tue, May 23, 2017 at 2:41 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * atomic_read - read atomic variable</span>
<span class="quote">&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically reads the value of @v.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline int atomic_read(const atomic_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return *((volatile int *)(&amp;(v-&gt;counter)));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * atomic_set - set atomic variable</span>
<span class="quote">&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt; + * @i: required value</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically sets the value of @v to @i.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void atomic_set(atomic_t *v, int i)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       v-&gt;counter = i;</span>
<span class="quote">&gt; +}</span>

These commonly use READ_ONCE() and WRITE_ONCE,
I&#39;d recommend doing the same here to be on the safe side.
<span class="quote">
&gt; +/**</span>
<span class="quote">&gt; + * atomic64_read - read atomic64 variable</span>
<span class="quote">&gt; + * @v: pointer of type atomic64_t</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically reads the value of @v.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline s64 atomic64_read(const atomic64_t *v)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return *((volatile long *)(&amp;(v-&gt;counter)));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/**</span>
<span class="quote">&gt; + * atomic64_set - set atomic64 variable</span>
<span class="quote">&gt; + * @v: pointer to type atomic64_t</span>
<span class="quote">&gt; + * @i: required value</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Atomically sets the value of @v to @i.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +static inline void atomic64_set(atomic64_t *v, s64 i)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       v-&gt;counter = i;</span>
<span class="quote">&gt; +}</span>

same here
<span class="quote">
&gt; diff --git a/arch/riscv/include/asm/bug.h b/arch/riscv/include/asm/bug.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..10d894ac3137</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/riscv/include/asm/bug.h</span>
<span class="quote">&gt; @@ -0,0 +1,81 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +#ifndef _ASM_RISCV_BUG_H</span>
<span class="quote">&gt; +#define _ASM_RISCV_BUG_H</span>
<span class="quote">
&gt; +#ifdef CONFIG_GENERIC_BUG</span>
<span class="quote">&gt; +#define __BUG_INSN     _AC(0x00100073, UL) /* sbreak */</span>

Please have a look at the modifications I did for !CONFIG_BUG
on x86, arm and arm64. It&#39;s generally better to define BUG to a
trap even when CONFIG_BUG is disabled, otherwise you run
into undefined behavior in some code, and gcc will print annoying
warnings about that.
<span class="quote">
&gt; +#ifndef _ASM_RISCV_CACHE_H</span>
<span class="quote">&gt; +#define _ASM_RISCV_CACHE_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define L1_CACHE_SHIFT         6</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define L1_CACHE_BYTES         (1 &lt;&lt; L1_CACHE_SHIFT)</span>

Is this the only valid cache line size on riscv, or just the largest
one that is allowed?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return (dma_addr_t)paddr;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       return (phys_addr_t)dev_addr;</span>
<span class="quote">&gt; +}</span>

What do you need these for? If possible, try to remove them.
<span class="quote">
&gt; +static inline void dma_cache_sync(struct device *dev, void *vaddr, size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       /*</span>
<span class="quote">&gt; +        * RISC-V is cache-coherent, so this is mostly a no-op.</span>
<span class="quote">&gt; +        * However, we do need to ensure that dma_cache_sync()</span>
<span class="quote">&gt; +        * enforces order, hence the mb().</span>
<span class="quote">&gt; +        */</span>
<span class="quote">&gt; +       mb();</span>
<span class="quote">&gt; +}</span>

Do you even support any drivers that use
dma_alloc_noncoherent()/dma_cache_sync()?

I would guess you can just leave this out.
<span class="quote">
&gt; diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..d942555a7a08</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt; @@ -0,0 +1,36 @@</span>
<span class="quote">
&gt; +#ifndef _ASM_RISCV_IO_H</span>
<span class="quote">&gt; +#define _ASM_RISCV_IO_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm-generic/io.h&gt;</span>

I would recommend providing your own {read,write}{b,w,l,q}{,_relaxed}
helpers using inline assembly, to prevent the compiler for breaking
up accesses into byte accesses.

Also, most architectures require to some synchronization after a
non-relaxed readl() to prevent prefetching of DMA buffers, and
before a writel() to flush write buffers when a DMA gets triggered.
<span class="quote">
&gt; +#ifdef __KERNEL__</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_MMU</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void __iomem *ioremap(phys_addr_t offset, unsigned long size);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define ioremap_nocache(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt; +#define ioremap_wc(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt; +#define ioremap_wt(addr, size) ioremap((addr), (size))</span>

Is this a hard architecture limitation? Normally you really want
write-combined access on frame buffer memory and a few other
cases for performance reasons, and ioremap_wc() gets used
for by memremap() for addressing RAM in some cases, and you
normally don&#39;t want to have PTEs for the same memory using
cached and uncached page flags
<span class="quote">
&gt; diff --git a/arch/riscv/include/asm/serial.h b/arch/riscv/include/asm/serial.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..d783dbe80a4b</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/riscv/include/asm/serial.h</span>
<span class="quote">&gt; @@ -0,0 +1,43 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2014 Regents of the University of California</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt; + *   modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt; + *   as published by the Free Software Foundation, version 2.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is distributed in the hope that it will be useful, but</span>
<span class="quote">&gt; + *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="quote">&gt; + *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="quote">&gt; + *   more details.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _ASM_RISCV_SERIAL_H</span>
<span class="quote">&gt; +#define _ASM_RISCV_SERIAL_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * FIXME: interim serial support for riscv-qemu</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Currently requires that the emulator itself create a hole at addresses</span>
<span class="quote">&gt; + * 0x3f8 - 0x3ff without looking through page tables.</span>

This sounds like something we want to fix in qemu and not have in the
mainline kernel. In particular, something seems really wrong if your
inb()/outb() get remapped to physical CPU address 0+offset.
<span class="quote">
&gt; diff --git a/arch/riscv/include/asm/setup.h b/arch/riscv/include/asm/setup.h</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..e457854e9988</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/riscv/include/asm/setup.h</span>
<span class="quote">&gt; @@ -0,0 +1,20 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2012 Regents of the University of California</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt; + *   modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt; + *   as published by the Free Software Foundation, version 2.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *   This program is distributed in the hope that it will be useful, but</span>
<span class="quote">&gt; + *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt; + *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="quote">&gt; + *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="quote">&gt; + *   more details.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifndef _ASM_RISCV_SETUP_H</span>
<span class="quote">&gt; +#define _ASM_RISCV_SETUP_H</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm-generic/setup.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#endif /* _ASM_RISCV_SETUP_H */</span>

Can you remove this file and add it to asm/Kbuild as generic-y instead?
<span class="quote">
&gt; +/*</span>
<span class="quote">&gt; + * low level task data that entry.S needs immediate access to</span>
<span class="quote">&gt; + * - this struct should fit entirely inside of one cache line</span>
<span class="quote">&gt; + * - this struct resides at the bottom of the supervisor stack</span>
<span class="quote">&gt; + * - if the members of this struct changes, the assembly constants</span>
<span class="quote">&gt; + *   in asm-offsets.c must be updated accordingly</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +struct thread_info {</span>
<span class="quote">&gt; +       struct task_struct      *task;          /* main task structure */</span>
<span class="quote">&gt; +       unsigned long           flags;          /* low level flags */</span>
<span class="quote">&gt; +       __u32                   cpu;            /* current CPU */</span>
<span class="quote">&gt; +       int                     preempt_count;  /* 0 =&gt; preemptable, &lt;0 =&gt; BUG */</span>
<span class="quote">&gt; +       mm_segment_t            addr_limit;</span>
<span class="quote">&gt; +};</span>

Please see 15f4eae70d36 (&quot;x86: Move thread_info into task_struct&quot;)
and try to do the same.
<span class="quote">
&gt; +#else /* !CONFIG_MMU */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void flush_tlb_all(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       BUG();</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       BUG();</span>
<span class="quote">&gt; +}</span>

The NOMMU support is rather incomplete and CONFIG_MMU is
hard-enabled, so I&#39;d just drop any !CONFIG_MMU #ifdefs.
<span class="quote">
&gt; diff --git a/arch/riscv/include/uapi/asm/Kbuild b/arch/riscv/include/uapi/asm/Kbuild</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..276b6dae745c</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/arch/riscv/include/uapi/asm/Kbuild</span>
<span class="quote">&gt; @@ -0,0 +1,10 @@</span>
<span class="quote">&gt; +# UAPI Header export list</span>
<span class="quote">&gt; +include include/uapi/asm-generic/Kbuild.asm</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +header-y += auxvec.h</span>
<span class="quote">&gt; +header-y += bitsperlong.h</span>
<span class="quote">&gt; +header-y += byteorder.h</span>
<span class="quote">&gt; +header-y += ptrace.h</span>
<span class="quote">&gt; +header-y += sigcontext.h</span>
<span class="quote">&gt; +header-y += siginfo.h</span>
<span class="quote">&gt; +header-y += unistd.h</span>

Please see
fcc8487d477a (&quot;uapi: export all headers under uapi directories&quot;)

and adapt the file accordingly
<span class="quote">
&gt; +#include &lt;asm-generic/unistd.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define __NR_sysriscv  __NR_arch_specific_syscall</span>
<span class="quote">&gt; +#ifndef __riscv_atomic</span>
<span class="quote">&gt; +__SYSCALL(__NR_sysriscv, sys_sysriscv)</span>
<span class="quote">&gt; +#endif</span>

Please make this a straight cmpxchg syscall and remove the multiplexer.
Why does the definition depend on __riscv_atomic rather than the
Kconfig symbol?

       Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=245">Benjamin Herrenschmidt</a> - May 23, 2017, 9:23 p.m.</div>
<pre class="content">
On Tue, 2017-05-23 at 14:55 +0200, Arnd Bergmann wrote:
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +#include &lt;asm-generic/io.h&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would recommend providing your own {read,write}{b,w,l,q}{,_relaxed}</span>
<span class="quote">&gt; helpers using inline assembly, to prevent the compiler for breaking</span>
<span class="quote">&gt; up accesses into byte accesses.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Also, most architectures require to some synchronization after a</span>
<span class="quote">&gt; non-relaxed readl() to prevent prefetching of DMA buffers, and</span>
<span class="quote">&gt; before a writel() to flush write buffers when a DMA gets triggered.</span>

Right, I was about to comment on that one.

The question Palmer is about the ordering semantics of non-cached
storage.

What kind of ordering is provided architecturally ? Especially
between cachable and non-cachable loads and stores ?

Also, you have PCIe right ? What is the behaviour of MSIs ?

Does your HW provide a guarantee that in the case of a series of DMA
writes to memory by a device followed by an MSI, the CPU getting the
MSI will only get it after all the previous DMA writes have reached
coherency ? (Unlike LSIs where the driver is required to do an MMIO
read from the device, MSIs are expected to be ordered with data).

Another things with the read*() accessors. It&#39;s not uncommon for
a driver to do:

	writel(1, reset_reg);
	readl(reset_reg); /* flush posted writes */
	udelay(10);
	writel(0, reset_reg);

Now, in the above case, what can typically happen if you aren&#39;t careful
is that the readl which is intended to &quot;push&quot; the previous writel, will
not actually do its job because the return value hasn&#39;t been &quot;consumed&quot;
by the processor. Thus, the CPU will stick that on some kind of load
queue and won&#39;t actually wait for the return value before hitting the
delay loop.

Thus you might end up in a situation where the writel of 1 to the
device is itself reaching the device way after you started the delay
loop, and thus end up violating the delay requirement of the HW.

On powerpc we solve that by using a special instruction construct
inside the read* accessors that prevents the CPU from executing
subsequent instructions until the read value has been returned.

You may want to consider something similar.

Cheers,
Ben.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 1, 2017, 12:56 a.m.</div>
<pre class="content">
On Tue, 23 May 2017 05:55:15 PDT (-0700), Arnd Bergmann wrote:
<span class="quote">&gt; On Tue, May 23, 2017 at 2:41 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:</span>
<span class="quote">&gt;&gt; +/**</span>
<span class="quote">&gt;&gt; + * atomic_read - read atomic variable</span>
<span class="quote">&gt;&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Atomically reads the value of @v.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline int atomic_read(const atomic_t *v)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       return *((volatile int *)(&amp;(v-&gt;counter)));</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +/**</span>
<span class="quote">&gt;&gt; + * atomic_set - set atomic variable</span>
<span class="quote">&gt;&gt; + * @v: pointer of type atomic_t</span>
<span class="quote">&gt;&gt; + * @i: required value</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Atomically sets the value of @v to @i.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +static inline void atomic_set(atomic_t *v, int i)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       v-&gt;counter = i;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; These commonly use READ_ONCE() and WRITE_ONCE,</span>
<span class="quote">&gt; I&#39;d recommend doing the same here to be on the safe side.</span>

Makes sense.  https://github.com/riscv/riscv-linux/commit/77647f9e4dccab68c69a212a63c9efe1db2b7b1c
<span class="quote">
&gt;&gt; diff --git a/arch/riscv/include/asm/bug.h b/arch/riscv/include/asm/bug.h</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 000000000000..10d894ac3137</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/riscv/include/asm/bug.h</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,81 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +#ifndef _ASM_RISCV_BUG_H</span>
<span class="quote">&gt;&gt; +#define _ASM_RISCV_BUG_H</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_GENERIC_BUG</span>
<span class="quote">&gt;&gt; +#define __BUG_INSN     _AC(0x00100073, UL) /* sbreak */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Please have a look at the modifications I did for !CONFIG_BUG</span>
<span class="quote">&gt; on x86, arm and arm64. It&#39;s generally better to define BUG to a</span>
<span class="quote">&gt; trap even when CONFIG_BUG is disabled, otherwise you run</span>
<span class="quote">&gt; into undefined behavior in some code, and gcc will print annoying</span>
<span class="quote">&gt; warnings about that.</span>

OK, seems like a good thing.  https://github.com/riscv/riscv-linux/commit/67db001653614c6555424b3812d7edfba12a6d4c
<span class="quote">
&gt;&gt; +#ifndef _ASM_RISCV_CACHE_H</span>
<span class="quote">&gt;&gt; +#define _ASM_RISCV_CACHE_H</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define L1_CACHE_SHIFT         6</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define L1_CACHE_BYTES         (1 &lt;&lt; L1_CACHE_SHIFT)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Is this the only valid cache line size on riscv, or just the largest</span>
<span class="quote">&gt; one that is allowed?</span>

The RISC-V ISA manual doesn&#39;t actually mention caches anywhere, so there&#39;s no
restriction on L1 cache line size (we tried to keep microarchitecture out of
the ISA specification).  We provide the actual cache parameters as part of the
device tree, but it looks like this needs to be known staticly in some places
so we can&#39;t use that everywhere.

We could always make this a Kconfig parameter.
<span class="quote">
&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       return (dma_addr_t)paddr;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       return (phys_addr_t)dev_addr;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What do you need these for? If possible, try to remove them.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +static inline void dma_cache_sync(struct device *dev, void *vaddr, size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       /*</span>
<span class="quote">&gt;&gt; +        * RISC-V is cache-coherent, so this is mostly a no-op.</span>
<span class="quote">&gt;&gt; +        * However, we do need to ensure that dma_cache_sync()</span>
<span class="quote">&gt;&gt; +        * enforces order, hence the mb().</span>
<span class="quote">&gt;&gt; +        */</span>
<span class="quote">&gt;&gt; +       mb();</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Do you even support any drivers that use</span>
<span class="quote">&gt; dma_alloc_noncoherent()/dma_cache_sync()?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I would guess you can just leave this out.</span>

These must have been vestigial code, they appear safe to remove.

  https://github.com/riscv/riscv-linux/commit/d1c88783d5ff66464a25173f7a4af139f0ebf5e2
<span class="quote">
&gt;&gt; diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 000000000000..d942555a7a08</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,36 @@</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +#ifndef _ASM_RISCV_IO_H</span>
<span class="quote">&gt;&gt; +#define _ASM_RISCV_IO_H</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/io.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I would recommend providing your own {read,write}{b,w,l,q}{,_relaxed}</span>
<span class="quote">&gt; helpers using inline assembly, to prevent the compiler for breaking</span>
<span class="quote">&gt; up accesses into byte accesses.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Also, most architectures require to some synchronization after a</span>
<span class="quote">&gt; non-relaxed readl() to prevent prefetching of DMA buffers, and</span>
<span class="quote">&gt; before a writel() to flush write buffers when a DMA gets triggered.</span>

Makes sense.  These were all OK on existing implementations (as there&#39;s no
writable PMAs, so all MMIO regions are strictly ordered), but that&#39;s not
actually what the RISC-V ISA says.  I patterned this on arm64

  https://github.com/riscv/riscv-linux/commit/e200fa29a69451ef4d575076e4d2af6b7877b1fa

where I think the only odd thing is our definition of mmiowb

  +/* IO barriers.  These only fence on the IO bits because they&#39;re only required
  + * to order device access.  We&#39;re defining mmiowb because our AMO instructions
  + * (which are used to implement locks) don&#39;t specify ordering.  From Chapter 7
  + * of v2.2 of the user ISA:
  + * &quot;The bits order accesses to one of the two address domains, memory or I/O,
  + * depending on which address domain the atomic instruction is accessing. No
  + * ordering constraint is implied to accesses to the other domain, and a FENCE
  + * instruction should be used to order across both domains.&quot;
  + */
  +
  +#define __iormb()               __asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);
  +#define __iowmb()               __asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);
  +
  +#define mmiowb()                __asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);

which I think is correct.
<span class="quote">
&gt;&gt; +#ifdef __KERNEL__</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifdef CONFIG_MMU</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +extern void __iomem *ioremap(phys_addr_t offset, unsigned long size);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define ioremap_nocache(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt;&gt; +#define ioremap_wc(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt;&gt; +#define ioremap_wt(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Is this a hard architecture limitation? Normally you really want</span>
<span class="quote">&gt; write-combined access on frame buffer memory and a few other</span>
<span class="quote">&gt; cases for performance reasons, and ioremap_wc() gets used</span>
<span class="quote">&gt; for by memremap() for addressing RAM in some cases, and you</span>
<span class="quote">&gt; normally don&#39;t want to have PTEs for the same memory using</span>
<span class="quote">&gt; cached and uncached page flags</span>

This is currently an architecture limitation.  In RISC-V these properties are
known as PMAs (Physical Memory Attributes).  While the supervisor spec mentions
PMAs, it doesn&#39;t provide a mechanism to read or write them so they are
essentially unspecified.  PMAs will be properly defined as part of the platform
specification, which isn&#39;t written yet.
<span class="quote">
&gt;&gt; diff --git a/arch/riscv/include/asm/serial.h b/arch/riscv/include/asm/serial.h</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 000000000000..d783dbe80a4b</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/riscv/include/asm/serial.h</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,43 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2014 Regents of the University of California</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *   This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt;&gt; + *   modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt;&gt; + *   as published by the Free Software Foundation, version 2.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *   This program is distributed in the hope that it will be useful, but</span>
<span class="quote">&gt;&gt; + *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt;&gt; + *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="quote">&gt;&gt; + *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="quote">&gt;&gt; + *   more details.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef _ASM_RISCV_SERIAL_H</span>
<span class="quote">&gt;&gt; +#define _ASM_RISCV_SERIAL_H</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * FIXME: interim serial support for riscv-qemu</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + * Currently requires that the emulator itself create a hole at addresses</span>
<span class="quote">&gt;&gt; + * 0x3f8 - 0x3ff without looking through page tables.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This sounds like something we want to fix in qemu and not have in the</span>
<span class="quote">&gt; mainline kernel. In particular, something seems really wrong if your</span>
<span class="quote">&gt; inb()/outb() get remapped to physical CPU address 0+offset.</span>

Sorry, we had some hacks floating around for QEMU from before we actually had
any devices interfaces working (ie, before device tree and proper MMIO
support).  I&#39;ll go through and drop these before v2.
<span class="quote">
&gt;&gt; diff --git a/arch/riscv/include/asm/setup.h b/arch/riscv/include/asm/setup.h</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 000000000000..e457854e9988</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/riscv/include/asm/setup.h</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,20 @@</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * Copyright (C) 2012 Regents of the University of California</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *   This program is free software; you can redistribute it and/or</span>
<span class="quote">&gt;&gt; + *   modify it under the terms of the GNU General Public License</span>
<span class="quote">&gt;&gt; + *   as published by the Free Software Foundation, version 2.</span>
<span class="quote">&gt;&gt; + *</span>
<span class="quote">&gt;&gt; + *   This program is distributed in the hope that it will be useful, but</span>
<span class="quote">&gt;&gt; + *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="quote">&gt;&gt; + *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="quote">&gt;&gt; + *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="quote">&gt;&gt; + *   more details.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#ifndef _ASM_RISCV_SETUP_H</span>
<span class="quote">&gt;&gt; +#define _ASM_RISCV_SETUP_H</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#include &lt;asm-generic/setup.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#endif /* _ASM_RISCV_SETUP_H */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Can you remove this file and add it to asm/Kbuild as generic-y instead?</span>

Yes.  https://github.com/riscv/riscv-linux/commit/8f35ce93bd1230ac0cb4aa92e5673c61e50dd862
<span class="quote">
&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * low level task data that entry.S needs immediate access to</span>
<span class="quote">&gt;&gt; + * - this struct should fit entirely inside of one cache line</span>
<span class="quote">&gt;&gt; + * - this struct resides at the bottom of the supervisor stack</span>
<span class="quote">&gt;&gt; + * - if the members of this struct changes, the assembly constants</span>
<span class="quote">&gt;&gt; + *   in asm-offsets.c must be updated accordingly</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +struct thread_info {</span>
<span class="quote">&gt;&gt; +       struct task_struct      *task;          /* main task structure */</span>
<span class="quote">&gt;&gt; +       unsigned long           flags;          /* low level flags */</span>
<span class="quote">&gt;&gt; +       __u32                   cpu;            /* current CPU */</span>
<span class="quote">&gt;&gt; +       int                     preempt_count;  /* 0 =&gt; preemptable, &lt;0 =&gt; BUG */</span>
<span class="quote">&gt;&gt; +       mm_segment_t            addr_limit;</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Please see 15f4eae70d36 (&quot;x86: Move thread_info into task_struct&quot;)</span>
<span class="quote">&gt; and try to do the same.</span>

OK, here&#39;s my attempt

  https://github.com/riscv/riscv-linux/commit/c618553e7aa65c85564a5d0a868ec7e6cf634afd

Since there&#39;s some actual meat, I left a commit message (these are more just
notes for me for my v2, I&#39;ll be squashing everything)

&quot;
This is patterned more off the arm64 move than the x86 one, since we
still need to have at least addr_limit to emulate FS.

The patch itself changes sscratch from holding SP to holding TP, which
contains a pointer to task_struct.  thread_info must be at a 0 offset
from task_struct, but it looks like that&#39;s already enforced with a big
comment.  We now store both the user and kernel SP in task_struct, but
those are really acting more as extra scratch space than pemanent
storage.
&quot;
<span class="quote">
&gt;&gt; +#else /* !CONFIG_MMU */</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void flush_tlb_all(void)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       BUG();</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       BUG();</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The NOMMU support is rather incomplete and CONFIG_MMU is</span>
<span class="quote">&gt; hard-enabled, so I&#39;d just drop any !CONFIG_MMU #ifdefs.</span>

OK.  I&#39;ve left in the &quot;#ifdef CONFIG_MMU&quot; blocks as the #ifdef/#endif doesn&#39;t
really add any code, but I can go ahead and drop the #ifdef if you think that&#39;s
better.

  https://github.com/riscv/riscv-linux/commit/e98ca23adfb9422bebc87cbfb58f70d4a63cf067
<span class="quote">
&gt;&gt; diff --git a/arch/riscv/include/uapi/asm/Kbuild b/arch/riscv/include/uapi/asm/Kbuild</span>
<span class="quote">&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt; index 000000000000..276b6dae745c</span>
<span class="quote">&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt; +++ b/arch/riscv/include/uapi/asm/Kbuild</span>
<span class="quote">&gt;&gt; @@ -0,0 +1,10 @@</span>
<span class="quote">&gt;&gt; +# UAPI Header export list</span>
<span class="quote">&gt;&gt; +include include/uapi/asm-generic/Kbuild.asm</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +header-y += auxvec.h</span>
<span class="quote">&gt;&gt; +header-y += bitsperlong.h</span>
<span class="quote">&gt;&gt; +header-y += byteorder.h</span>
<span class="quote">&gt;&gt; +header-y += ptrace.h</span>
<span class="quote">&gt;&gt; +header-y += sigcontext.h</span>
<span class="quote">&gt;&gt; +header-y += siginfo.h</span>
<span class="quote">&gt;&gt; +header-y += unistd.h</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Please see</span>
<span class="quote">&gt; fcc8487d477a (&quot;uapi: export all headers under uapi directories&quot;)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; and adapt the file accordingly</span>

https://github.com/riscv/riscv-linux/commit/52c5e300b498742390434891db34f9dbacd082e9
<span class="quote">
&gt;&gt; +#include &lt;asm-generic/unistd.h&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define __NR_sysriscv  __NR_arch_specific_syscall</span>
<span class="quote">&gt;&gt; +#ifndef __riscv_atomic</span>
<span class="quote">&gt;&gt; +__SYSCALL(__NR_sysriscv, sys_sysriscv)</span>
<span class="quote">&gt;&gt; +#endif</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Please make this a straight cmpxchg syscall and remove the multiplexer.</span>
<span class="quote">&gt; Why does the definition depend on __riscv_atomic rather than the</span>
<span class="quote">&gt; Kconfig symbol?</span>

I think that was just an oversight: that&#39;s not the right switch.  Either you or
someone else pointed out some problems with this.  There&#39;s going to be an
interposer in the VDSO, and then we&#39;ll always enable the system call.

I can change this to two system calls: sysriscv_cmpxchg32 and
sysriscv_cmpxchg64.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - June 1, 2017, 9 a.m.</div>
<pre class="content">
On Thu, Jun 1, 2017 at 2:56 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:
<span class="quote">&gt; On Tue, 23 May 2017 05:55:15 PDT (-0700), Arnd Bergmann wrote:</span>
<span class="quote">&gt;&gt; On Tue, May 23, 2017 at 2:41 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:</span>
<span class="quote">

&gt;&gt;&gt; +#ifndef _ASM_RISCV_CACHE_H</span>
<span class="quote">&gt;&gt;&gt; +#define _ASM_RISCV_CACHE_H</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#define L1_CACHE_SHIFT         6</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#define L1_CACHE_BYTES         (1 &lt;&lt; L1_CACHE_SHIFT)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Is this the only valid cache line size on riscv, or just the largest</span>
<span class="quote">&gt;&gt; one that is allowed?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The RISC-V ISA manual doesn&#39;t actually mention caches anywhere, so there&#39;s no</span>
<span class="quote">&gt; restriction on L1 cache line size (we tried to keep microarchitecture out of</span>
<span class="quote">&gt; the ISA specification).  We provide the actual cache parameters as part of the</span>
<span class="quote">&gt; device tree, but it looks like this needs to be known staticly in some places</span>
<span class="quote">&gt; so we can&#39;t use that everywhere.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We could always make this a Kconfig parameter.</span>

The cache line size is used in a couple of places, let&#39;s go through the most
common ones to see where that abstraction might be leaky and you actually
get an architectural effect:

- On SMP machines, ____cacheline_aligned_in_smp is used to annotate
  data structures used in lockless algorithms, typically with one CPU writing
  to some members of a structure, and another CPU reading from it but
  not writing the same members. Depending on the architecture, having a
  larger actual alignment than L1_CACHE_BYTES will either lead to
  bad performance from cache line ping pong, or actual data corruption.

- On systems with DMA masters that are not fully coherent,
  ____cacheline_aligned is used to annotate data structures used
  for DMA buffers, to make sure that the cache maintenance operations
  in dma_sync_*_for_*() helpers don&#39;t corrup data outside of the
  DMA buffer. You don&#39;t seem to support noncoherent DMA masters
  or the cache maintenance operations required to use those, so this
  might not be a problem until someone adds an extension for those.

- Depending on the bus interconnect, a coherent DMA master might
  not be able to update partial cache lines, so you need the same
  annotation.

- The kmalloc() family of memory allocators aligns data to the cache
  line size, for both DMA and SMP synchronization above.

- Many architectures have cache line prefetch, flush, zero or copy
  instructions  that are used for important performance optimizations
  but that are typically defined on a cacheline granularity. I don&#39;t
  think you currently have any of them, but it seems likely that there
  will be demand for them later.

Having a larger than necessary alignment can waste substantial amounts
of memory for arrays of cache line aligned structures (typically
per-cpu arrays), but otherwise should not cause harm.
<span class="quote">
&gt;&gt;&gt; diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt;&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt;&gt; index 000000000000..d942555a7a08</span>
<span class="quote">&gt;&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt;&gt;&gt; @@ -0,0 +1,36 @@</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; +#ifndef _ASM_RISCV_IO_H</span>
<span class="quote">&gt;&gt;&gt; +#define _ASM_RISCV_IO_H</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#include &lt;asm-generic/io.h&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I would recommend providing your own {read,write}{b,w,l,q}{,_relaxed}</span>
<span class="quote">&gt;&gt; helpers using inline assembly, to prevent the compiler for breaking</span>
<span class="quote">&gt;&gt; up accesses into byte accesses.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Also, most architectures require to some synchronization after a</span>
<span class="quote">&gt;&gt; non-relaxed readl() to prevent prefetching of DMA buffers, and</span>
<span class="quote">&gt;&gt; before a writel() to flush write buffers when a DMA gets triggered.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Makes sense.  These were all OK on existing implementations (as there&#39;s no</span>
<span class="quote">&gt; writable PMAs, so all MMIO regions are strictly ordered), but that&#39;s not</span>
<span class="quote">&gt; actually what the RISC-V ISA says.  I patterned this on arm64</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   https://github.com/riscv/riscv-linux/commit/e200fa29a69451ef4d575076e4d2af6b7877b1fa</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; where I think the only odd thing is our definition of mmiowb</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   +/* IO barriers.  These only fence on the IO bits because they&#39;re only required</span>
<span class="quote">&gt;   + * to order device access.  We&#39;re defining mmiowb because our AMO instructions</span>
<span class="quote">&gt;   + * (which are used to implement locks) don&#39;t specify ordering.  From Chapter 7</span>
<span class="quote">&gt;   + * of v2.2 of the user ISA:</span>
<span class="quote">&gt;   + * &quot;The bits order accesses to one of the two address domains, memory or I/O,</span>
<span class="quote">&gt;   + * depending on which address domain the atomic instruction is accessing. No</span>
<span class="quote">&gt;   + * ordering constraint is implied to accesses to the other domain, and a FENCE</span>
<span class="quote">&gt;   + * instruction should be used to order across both domains.&quot;</span>
<span class="quote">&gt;   + */</span>
<span class="quote">&gt;   +</span>
<span class="quote">&gt;   +#define __iormb()               __asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;   +#define __iowmb()               __asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>

Looks ok, yes.
<span class="quote">
&gt;   +#define mmiowb()                __asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; which I think is correct.</span>

I can never remember what exactly this one does.
<span class="quote">
&gt;&gt;&gt; +#ifdef __KERNEL__</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#ifdef CONFIG_MMU</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +extern void __iomem *ioremap(phys_addr_t offset, unsigned long size);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#define ioremap_nocache(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt;&gt;&gt; +#define ioremap_wc(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt;&gt;&gt; +#define ioremap_wt(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Is this a hard architecture limitation? Normally you really want</span>
<span class="quote">&gt;&gt; write-combined access on frame buffer memory and a few other</span>
<span class="quote">&gt;&gt; cases for performance reasons, and ioremap_wc() gets used</span>
<span class="quote">&gt;&gt; for by memremap() for addressing RAM in some cases, and you</span>
<span class="quote">&gt;&gt; normally don&#39;t want to have PTEs for the same memory using</span>
<span class="quote">&gt;&gt; cached and uncached page flags</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This is currently an architecture limitation.  In RISC-V these properties are</span>
<span class="quote">&gt; known as PMAs (Physical Memory Attributes).  While the supervisor spec mentions</span>
<span class="quote">&gt; PMAs, it doesn&#39;t provide a mechanism to read or write them so they are</span>
<span class="quote">&gt; essentially unspecified.  PMAs will be properly defined as part of the platform</span>
<span class="quote">&gt; specification, which isn&#39;t written yet.</span>

Ok. Maybe add that as a comment above these definitions then.
<span class="quote">
&gt;&gt;&gt; +/*</span>
<span class="quote">&gt;&gt;&gt; + * low level task data that entry.S needs immediate access to</span>
<span class="quote">&gt;&gt;&gt; + * - this struct should fit entirely inside of one cache line</span>
<span class="quote">&gt;&gt;&gt; + * - this struct resides at the bottom of the supervisor stack</span>
<span class="quote">&gt;&gt;&gt; + * - if the members of this struct changes, the assembly constants</span>
<span class="quote">&gt;&gt;&gt; + *   in asm-offsets.c must be updated accordingly</span>
<span class="quote">&gt;&gt;&gt; + */</span>
<span class="quote">&gt;&gt;&gt; +struct thread_info {</span>
<span class="quote">&gt;&gt;&gt; +       struct task_struct      *task;          /* main task structure */</span>
<span class="quote">&gt;&gt;&gt; +       unsigned long           flags;          /* low level flags */</span>
<span class="quote">&gt;&gt;&gt; +       __u32                   cpu;            /* current CPU */</span>
<span class="quote">&gt;&gt;&gt; +       int                     preempt_count;  /* 0 =&gt; preemptable, &lt;0 =&gt; BUG */</span>
<span class="quote">&gt;&gt;&gt; +       mm_segment_t            addr_limit;</span>
<span class="quote">&gt;&gt;&gt; +};</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Please see 15f4eae70d36 (&quot;x86: Move thread_info into task_struct&quot;)</span>
<span class="quote">&gt;&gt; and try to do the same.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; OK, here&#39;s my attempt</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   https://github.com/riscv/riscv-linux/commit/c618553e7aa65c85564a5d0a868ec7e6cf634afd</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Since there&#39;s some actual meat, I left a commit message (these are more just</span>
<span class="quote">&gt; notes for me for my v2, I&#39;ll be squashing everything)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; This is patterned more off the arm64 move than the x86 one, since we</span>
<span class="quote">&gt; still need to have at least addr_limit to emulate FS.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The patch itself changes sscratch from holding SP to holding TP, which</span>
<span class="quote">&gt; contains a pointer to task_struct.  thread_info must be at a 0 offset</span>
<span class="quote">&gt; from task_struct, but it looks like that&#39;s already enforced with a big</span>
<span class="quote">&gt; comment.  We now store both the user and kernel SP in task_struct, but</span>
<span class="quote">&gt; those are really acting more as extra scratch space than pemanent</span>
<span class="quote">&gt; storage.</span>
<span class="quote">&gt; &quot;</span>

I haven&#39;t looked at all the details of the x86 patch, but it seems they
decided to put the arch specific members into &#39;struct thread_struct&#39;
rather than &#39;struct thread_info&#39;, so I&#39;d suggest you do the same here for
consistency, unless there is a strong reason against doing it.
<span class="quote">
&gt;&gt;&gt; +#else /* !CONFIG_MMU */</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +static inline void flush_tlb_all(void)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +       BUG();</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +       BUG();</span>
<span class="quote">&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The NOMMU support is rather incomplete and CONFIG_MMU is</span>
<span class="quote">&gt;&gt; hard-enabled, so I&#39;d just drop any !CONFIG_MMU #ifdefs.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; OK.  I&#39;ve left in the &quot;#ifdef CONFIG_MMU&quot; blocks as the #ifdef/#endif doesn&#39;t</span>
<span class="quote">&gt; really add any code, but I can go ahead and drop the #ifdef if you think that&#39;s</span>
<span class="quote">&gt; better.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   https://github.com/riscv/riscv-linux/commit/e98ca23adfb9422bebc87cbfb58f70d4a63cf067</span>

Ok.
<span class="quote">
&gt;&gt;&gt; +#include &lt;asm-generic/unistd.h&gt;</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +#define __NR_sysriscv  __NR_arch_specific_syscall</span>
<span class="quote">&gt;&gt;&gt; +#ifndef __riscv_atomic</span>
<span class="quote">&gt;&gt;&gt; +__SYSCALL(__NR_sysriscv, sys_sysriscv)</span>
<span class="quote">&gt;&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Please make this a straight cmpxchg syscall and remove the multiplexer.</span>
<span class="quote">&gt;&gt; Why does the definition depend on __riscv_atomic rather than the</span>
<span class="quote">&gt;&gt; Kconfig symbol?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think that was just an oversight: that&#39;s not the right switch.  Either you or</span>
<span class="quote">&gt; someone else pointed out some problems with this.  There&#39;s going to be an</span>
<span class="quote">&gt; interposer in the VDSO, and then we&#39;ll always enable the system call.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I can change this to two system calls: sysriscv_cmpxchg32 and</span>
<span class="quote">&gt; sysriscv_cmpxchg64.</span>

Sounds good.

        Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 3, 2017, 2 a.m.</div>
<pre class="content">
On Tue, 23 May 2017 14:23:50 PDT (-0700), benh@kernel.crashing.org wrote:
<span class="quote">&gt; On Tue, 2017-05-23 at 14:55 +0200, Arnd Bergmann wrote:</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +#include &lt;asm-generic/io.h&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I would recommend providing your own {read,write}{b,w,l,q}{,_relaxed}</span>
<span class="quote">&gt;&gt; helpers using inline assembly, to prevent the compiler for breaking</span>
<span class="quote">&gt;&gt; up accesses into byte accesses.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Also, most architectures require to some synchronization after a</span>
<span class="quote">&gt;&gt; non-relaxed readl() to prevent prefetching of DMA buffers, and</span>
<span class="quote">&gt;&gt; before a writel() to flush write buffers when a DMA gets triggered.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Right, I was about to comment on that one.</span>

Well, you&#39;re both correct: what was there just isn&#39;t correct.  Our
implementations were safe because they don&#39;t have aggressive MMIO systems, but
that won&#39;t remain true for long.

I&#39;ve gone ahead and added a proper IO implementation patterned on arm64.  It&#39;ll
be part of the v2 patch set.  Here&#39;s the bulk of the patch, if you&#39;re curious

  https://github.com/riscv/riscv-linux/commit/e200fa29a69451ef4d575076e4d2af6b7877b1fa
<span class="quote">
&gt; The question Palmer is about the ordering semantics of non-cached</span>
<span class="quote">&gt; storage.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What kind of ordering is provided architecturally ? Especially</span>
<span class="quote">&gt; between cachable and non-cachable loads and stores ?</span>

The memory model on RISC-V is pretty weak.  Without fences there are no
ordering constraints.  We provide 2 &quot;ordering spaces&quot; (an odd name I just made
up, there&#39;s one address space): the IO space and the regular memory space.  The
base RISC-V ordering primitive is a fence, which takes a predecessor set and
successor set.  Fences can look like

  fence IORW,IORW

where the left side is the predecessor set and the right side is the successor
set.  The fence enforces ordering between any operations in the two sets: all
operations in the predecessor set must be globally visible before any operation
in the successor set becomes visible anywhere.

For example, if you&#39;re emitting a DMA transaction you&#39;d have to do something
like

  build_message_in_memory()
  fence w,o
  set_control_register()

with the fence ensuring all the memory writes are visible before the control
register write.

More information can be found in the ISA manuals

  https://github.com/riscv/riscv-isa-manual/releases/download/riscv-user-2.2/riscv-spec-v2.2.pdf
  https://github.com/riscv/riscv-isa-manual/releases/download/riscv-priv-1.10/riscv-privileged-v1.10.pdf
<span class="quote">
&gt; Also, you have PCIe right ? What is the behaviour of MSIs ?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Does your HW provide a guarantee that in the case of a series of DMA</span>
<span class="quote">&gt; writes to memory by a device followed by an MSI, the CPU getting the</span>
<span class="quote">&gt; MSI will only get it after all the previous DMA writes have reached</span>
<span class="quote">&gt; coherency ? (Unlike LSIs where the driver is required to do an MMIO</span>
<span class="quote">&gt; read from the device, MSIs are expected to be ordered with data).</span>

We do have PCIe, but I&#39;m not particularly familiar with it as I haven&#39;t spent
any time hacking on our PCIe hardware or driver.  My understanding here is that
PCIe defines that MSIs must not be reordered before the DMA writes, so the
implementation is required to enforce this ordering.  Thus it&#39;s a problem for
the PCIe controller implementation (which isn&#39;t covered by RISC-V) and therefor
doesn&#39;t need any ordering enforced by the driver.

If I&#39;m correct in the assumption that the hardware is required to enforce these
ordering constraints then I think this isn&#39;t a RISC-V issue.  I&#39;ll go bug our
PCIe guys to make sure everything is kosher in that case, but it&#39;s an ordering
constraint that is possible to enforce in our coherence protocol so it&#39;s not a
fundamental problem (and just an implementation one at that).

If the hardware isn&#39;t required to enforce the ordering, then we&#39;ll need a fence
before handling the data.
<span class="quote">
&gt; Another things with the read*() accessors. It&#39;s not uncommon for</span>
<span class="quote">&gt; a driver to do:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; 	writel(1, reset_reg);</span>
<span class="quote">&gt; 	readl(reset_reg); /* flush posted writes */</span>
<span class="quote">&gt; 	udelay(10);</span>
<span class="quote">&gt; 	writel(0, reset_reg);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Now, in the above case, what can typically happen if you aren&#39;t careful</span>
<span class="quote">&gt; is that the readl which is intended to &quot;push&quot; the previous writel, will</span>
<span class="quote">&gt; not actually do its job because the return value hasn&#39;t been &quot;consumed&quot;</span>
<span class="quote">&gt; by the processor. Thus, the CPU will stick that on some kind of load</span>
<span class="quote">&gt; queue and won&#39;t actually wait for the return value before hitting the</span>
<span class="quote">&gt; delay loop.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Thus you might end up in a situation where the writel of 1 to the</span>
<span class="quote">&gt; device is itself reaching the device way after you started the delay</span>
<span class="quote">&gt; loop, and thus end up violating the delay requirement of the HW.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On powerpc we solve that by using a special instruction construct</span>
<span class="quote">&gt; inside the read* accessors that prevents the CPU from executing</span>
<span class="quote">&gt; subsequent instructions until the read value has been returned.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; You may want to consider something similar.</span>

Ooh, that&#39;s a fun one :).   I bugged Andrew (the ISA wizard), and this might
require some clarification in the ISA manual.  The code we emit will look
something like

  fence io,o
  st 1, RESET_REG
  ld RESET_REG
  fence o,io
loop:
  rdtime
  blt loop
  fence io,o
  st 0, RESET_REG

Since the fences just enforce ordering between the loads and stores, there&#39;s
nothing that prevents the processor from releasing the store before the delay
loop completes.  I think we might be safe here because you&#39;re not allowed to
make speculative writes visible, but arguably that&#39;s not a speculative write
because you can predict the timer will keep increasing.  The distinction is
somewhat academic, though: I&#39;m not sure what purpose that very specific sort of
predictor would have aside from breaking existing code.

This issue of &quot;how is time ordered&quot; has come up a handful of times, most of
which don&#39;t have the loop.  The idea of adding the result rdtime instruction to
the IO space.  I&#39;ve opened a spec bug

  https://github.com/riscv/riscv-isa-manual/issues/78

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 6, 2017, 4:56 a.m.</div>
<pre class="content">
On Thu, 01 Jun 2017 02:00:22 PDT (-0700), Arnd Bergmann wrote:
<span class="quote">&gt; On Thu, Jun 1, 2017 at 2:56 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On Tue, 23 May 2017 05:55:15 PDT (-0700), Arnd Bergmann wrote:</span>
<span class="quote">&gt;&gt;&gt; On Tue, May 23, 2017 at 2:41 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +#ifndef _ASM_RISCV_CACHE_H</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define _ASM_RISCV_CACHE_H</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define L1_CACHE_SHIFT         6</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define L1_CACHE_BYTES         (1 &lt;&lt; L1_CACHE_SHIFT)</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Is this the only valid cache line size on riscv, or just the largest</span>
<span class="quote">&gt;&gt;&gt; one that is allowed?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The RISC-V ISA manual doesn&#39;t actually mention caches anywhere, so there&#39;s no</span>
<span class="quote">&gt;&gt; restriction on L1 cache line size (we tried to keep microarchitecture out of</span>
<span class="quote">&gt;&gt; the ISA specification).  We provide the actual cache parameters as part of the</span>
<span class="quote">&gt;&gt; device tree, but it looks like this needs to be known staticly in some places</span>
<span class="quote">&gt;&gt; so we can&#39;t use that everywhere.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; We could always make this a Kconfig parameter.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The cache line size is used in a couple of places, let&#39;s go through the most</span>
<span class="quote">&gt; common ones to see where that abstraction might be leaky and you actually</span>
<span class="quote">&gt; get an architectural effect:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; - On SMP machines, ____cacheline_aligned_in_smp is used to annotate</span>
<span class="quote">&gt;   data structures used in lockless algorithms, typically with one CPU writing</span>
<span class="quote">&gt;   to some members of a structure, and another CPU reading from it but</span>
<span class="quote">&gt;   not writing the same members. Depending on the architecture, having a</span>
<span class="quote">&gt;   larger actual alignment than L1_CACHE_BYTES will either lead to</span>
<span class="quote">&gt;   bad performance from cache line ping pong, or actual data corruption.</span>

On RISC-V it&#39;s just a performance problem, so at least it&#39;s not catastrophic.
<span class="quote">
&gt; - On systems with DMA masters that are not fully coherent,</span>
<span class="quote">&gt;   ____cacheline_aligned is used to annotate data structures used</span>
<span class="quote">&gt;   for DMA buffers, to make sure that the cache maintenance operations</span>
<span class="quote">&gt;   in dma_sync_*_for_*() helpers don&#39;t corrup data outside of the</span>
<span class="quote">&gt;   DMA buffer. You don&#39;t seem to support noncoherent DMA masters</span>
<span class="quote">&gt;   or the cache maintenance operations required to use those, so this</span>
<span class="quote">&gt;   might not be a problem until someone adds an extension for those.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; - Depending on the bus interconnect, a coherent DMA master might</span>
<span class="quote">&gt;   not be able to update partial cache lines, so you need the same</span>
<span class="quote">&gt;   annotation.</span>

Well, our (SiFive&#39;s) bus is easy to master so hopefully we won&#39;t end up doing
that.  There is, of course, the rest of the world -- but that&#39;s just a bridge
we&#39;ll have to cross later (if such an implementation arises).
<span class="quote">
&gt; - The kmalloc() family of memory allocators aligns data to the cache</span>
<span class="quote">&gt;   line size, for both DMA and SMP synchronization above.</span>

Ya, but luckily just a performance problem on RISC-V.
<span class="quote">
&gt; - Many architectures have cache line prefetch, flush, zero or copy</span>
<span class="quote">&gt;   instructions  that are used for important performance optimizations</span>
<span class="quote">&gt;   but that are typically defined on a cacheline granularity. I don&#39;t</span>
<span class="quote">&gt;   think you currently have any of them, but it seems likely that there</span>
<span class="quote">&gt;   will be demand for them later.</span>

We actually have an implicit prefetch (loads to x0, the zero register), but
it still has all the load side-effects so nothing uses it.
<span class="quote">
&gt; Having a larger than necessary alignment can waste substantial amounts</span>
<span class="quote">&gt; of memory for arrays of cache line aligned structures (typically</span>
<span class="quote">&gt; per-cpu arrays), but otherwise should not cause harm.</span>

I bugged our L1 guy and he says 64-byte lines are a bit of a magic number
because of how they line up with DIMMs.  Since there&#39;s no spec to define this,
there&#39;s no correct answer.  I&#39;d be amenable to making this a Kconfig option,
but I think we&#39;ll leave it alone for now.  It does match the extant
implementations.
<span class="quote">
&gt;&gt;&gt;&gt; diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt;&gt;&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt;&gt;&gt; index 000000000000..d942555a7a08</span>
<span class="quote">&gt;&gt;&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt;&gt;&gt; +++ b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -0,0 +1,36 @@</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +#ifndef _ASM_RISCV_IO_H</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define _ASM_RISCV_IO_H</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +#include &lt;asm-generic/io.h&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I would recommend providing your own {read,write}{b,w,l,q}{,_relaxed}</span>
<span class="quote">&gt;&gt;&gt; helpers using inline assembly, to prevent the compiler for breaking</span>
<span class="quote">&gt;&gt;&gt; up accesses into byte accesses.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Also, most architectures require to some synchronization after a</span>
<span class="quote">&gt;&gt;&gt; non-relaxed readl() to prevent prefetching of DMA buffers, and</span>
<span class="quote">&gt;&gt;&gt; before a writel() to flush write buffers when a DMA gets triggered.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Makes sense.  These were all OK on existing implementations (as there&#39;s no</span>
<span class="quote">&gt;&gt; writable PMAs, so all MMIO regions are strictly ordered), but that&#39;s not</span>
<span class="quote">&gt;&gt; actually what the RISC-V ISA says.  I patterned this on arm64</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   https://github.com/riscv/riscv-linux/commit/e200fa29a69451ef4d575076e4d2af6b7877b1fa</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; where I think the only odd thing is our definition of mmiowb</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   +/* IO barriers.  These only fence on the IO bits because they&#39;re only required</span>
<span class="quote">&gt;&gt;   + * to order device access.  We&#39;re defining mmiowb because our AMO instructions</span>
<span class="quote">&gt;&gt;   + * (which are used to implement locks) don&#39;t specify ordering.  From Chapter 7</span>
<span class="quote">&gt;&gt;   + * of v2.2 of the user ISA:</span>
<span class="quote">&gt;&gt;   + * &quot;The bits order accesses to one of the two address domains, memory or I/O,</span>
<span class="quote">&gt;&gt;   + * depending on which address domain the atomic instruction is accessing. No</span>
<span class="quote">&gt;&gt;   + * ordering constraint is implied to accesses to the other domain, and a FENCE</span>
<span class="quote">&gt;&gt;   + * instruction should be used to order across both domains.&quot;</span>
<span class="quote">&gt;&gt;   + */</span>
<span class="quote">&gt;&gt;   +</span>
<span class="quote">&gt;&gt;   +#define __iormb()               __asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt;   +#define __iowmb()               __asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Looks ok, yes.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;   +#define mmiowb()                __asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; which I think is correct.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I can never remember what exactly this one does.</span>

I can&#39;t find the reference again, but what I found said that if your atomics
(or whatever&#39;s used for locking) don&#39;t stay ordered with your MMIO accesses,
then you should define mmiowb to ensure ordering.  I managed to screw this up,
as there&#39;s no &quot;w&quot; in the successor set (to actually enforce the AMO ordering).
This is somewhat confirmed by

  https://lkml.org/lkml/2006/8/31/174
  Subject: Re: When to use mmiowb()?
  AFAICT, they&#39;re both right.  Generally, mmiowb() should be used prior to
  unlock in a critical section whose last PIO operation is a writeX.

Thus, I think the actual fence should be at least

  fence o,w

Documentation/memory-barries.txt says

  &quot;
  The Linux kernel also has a special barrier for use with memory-mapped I/O
  writes:

  	mmiowb();

  This is a variation on the mandatory write barrier that causes writes to weakly
  ordered I/O regions to be partially ordered.  Its effects may go beyond the
  CPU-&gt;Hardware interface and actually affect the hardware at some level.

  See the subsection &quot;Acquires vs I/O accesses&quot; for more information.
  &quot;

  &quot;
  ACQUIRES VS I/O ACCESSES
  ------------------------

  Under certain circumstances (especially involving NUMA), I/O accesses within
  two spinlocked sections on two different CPUs may be seen as interleaved by the
  PCI bridge, because the PCI bridge does not necessarily participate in the
  cache-coherence protocol, and is therefore incapable of issuing the required
  read memory barriers.

  For example:

  	CPU 1				CPU 2
  	===============================	===============================
  	spin_lock(Q)
  	writel(0, ADDR)
  	writel(1, DATA);
  	spin_unlock(Q);
  					spin_lock(Q);
  					writel(4, ADDR);
  					writel(5, DATA);
  					spin_unlock(Q);

  may be seen by the PCI bridge as follows:

  	STORE *ADDR = 0, STORE *ADDR = 4, STORE *DATA = 1, STORE *DATA = 5

  which would probably cause the hardware to malfunction.


  What is necessary here is to intervene with an mmiowb() before dropping the
  spinlock, for example:

  	CPU 1				CPU 2
  	===============================	===============================
  	spin_lock(Q)
  	writel(0, ADDR)
  	writel(1, DATA);
  	mmiowb();
  	spin_unlock(Q);
  					spin_lock(Q);
  					writel(4, ADDR);
  					writel(5, DATA);
  					mmiowb();
  					spin_unlock(Q);

  this will ensure that the two stores issued on CPU 1 appear at the PCI bridge
  before either of the stores issued on CPU 2.


  Furthermore, following a store by a load from the same device obviates the need
  for the mmiowb(), because the load forces the store to complete before the load
  is performed:

  	CPU 1				CPU 2
  	===============================	===============================
  	spin_lock(Q)
  	writel(0, ADDR)
  	a = readl(DATA);
  	spin_unlock(Q);
  					spin_lock(Q);
  					writel(4, ADDR);
  					b = readl(DATA);
  					spin_unlock(Q);


  See Documentation/driver-api/device-io.rst for more information.
  &quot;

which matches what&#39;s above.  I think &quot;fence o,w&quot; is sufficient for a mmiowb on
RISC-V.  I&#39;ll make the change.
<span class="quote">
&gt;&gt;&gt;&gt; +#ifdef __KERNEL__</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +#ifdef CONFIG_MMU</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +extern void __iomem *ioremap(phys_addr_t offset, unsigned long size);</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define ioremap_nocache(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define ioremap_wc(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define ioremap_wt(addr, size) ioremap((addr), (size))</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Is this a hard architecture limitation? Normally you really want</span>
<span class="quote">&gt;&gt;&gt; write-combined access on frame buffer memory and a few other</span>
<span class="quote">&gt;&gt;&gt; cases for performance reasons, and ioremap_wc() gets used</span>
<span class="quote">&gt;&gt;&gt; for by memremap() for addressing RAM in some cases, and you</span>
<span class="quote">&gt;&gt;&gt; normally don&#39;t want to have PTEs for the same memory using</span>
<span class="quote">&gt;&gt;&gt; cached and uncached page flags</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This is currently an architecture limitation.  In RISC-V these properties are</span>
<span class="quote">&gt;&gt; known as PMAs (Physical Memory Attributes).  While the supervisor spec mentions</span>
<span class="quote">&gt;&gt; PMAs, it doesn&#39;t provide a mechanism to read or write them so they are</span>
<span class="quote">&gt;&gt; essentially unspecified.  PMAs will be properly defined as part of the platform</span>
<span class="quote">&gt;&gt; specification, which isn&#39;t written yet.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ok. Maybe add that as a comment above these definitions then.</span>

OK.
<span class="quote">
&gt;&gt;&gt;&gt; +/*</span>
<span class="quote">&gt;&gt;&gt;&gt; + * low level task data that entry.S needs immediate access to</span>
<span class="quote">&gt;&gt;&gt;&gt; + * - this struct should fit entirely inside of one cache line</span>
<span class="quote">&gt;&gt;&gt;&gt; + * - this struct resides at the bottom of the supervisor stack</span>
<span class="quote">&gt;&gt;&gt;&gt; + * - if the members of this struct changes, the assembly constants</span>
<span class="quote">&gt;&gt;&gt;&gt; + *   in asm-offsets.c must be updated accordingly</span>
<span class="quote">&gt;&gt;&gt;&gt; + */</span>
<span class="quote">&gt;&gt;&gt;&gt; +struct thread_info {</span>
<span class="quote">&gt;&gt;&gt;&gt; +       struct task_struct      *task;          /* main task structure */</span>
<span class="quote">&gt;&gt;&gt;&gt; +       unsigned long           flags;          /* low level flags */</span>
<span class="quote">&gt;&gt;&gt;&gt; +       __u32                   cpu;            /* current CPU */</span>
<span class="quote">&gt;&gt;&gt;&gt; +       int                     preempt_count;  /* 0 =&gt; preemptable, &lt;0 =&gt; BUG */</span>
<span class="quote">&gt;&gt;&gt;&gt; +       mm_segment_t            addr_limit;</span>
<span class="quote">&gt;&gt;&gt;&gt; +};</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Please see 15f4eae70d36 (&quot;x86: Move thread_info into task_struct&quot;)</span>
<span class="quote">&gt;&gt;&gt; and try to do the same.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; OK, here&#39;s my attempt</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   https://github.com/riscv/riscv-linux/commit/c618553e7aa65c85564a5d0a868ec7e6cf634afd</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Since there&#39;s some actual meat, I left a commit message (these are more just</span>
<span class="quote">&gt;&gt; notes for me for my v2, I&#39;ll be squashing everything)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &quot;</span>
<span class="quote">&gt;&gt; This is patterned more off the arm64 move than the x86 one, since we</span>
<span class="quote">&gt;&gt; still need to have at least addr_limit to emulate FS.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The patch itself changes sscratch from holding SP to holding TP, which</span>
<span class="quote">&gt;&gt; contains a pointer to task_struct.  thread_info must be at a 0 offset</span>
<span class="quote">&gt;&gt; from task_struct, but it looks like that&#39;s already enforced with a big</span>
<span class="quote">&gt;&gt; comment.  We now store both the user and kernel SP in task_struct, but</span>
<span class="quote">&gt;&gt; those are really acting more as extra scratch space than pemanent</span>
<span class="quote">&gt;&gt; storage.</span>
<span class="quote">&gt;&gt; &quot;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I haven&#39;t looked at all the details of the x86 patch, but it seems they</span>
<span class="quote">&gt; decided to put the arch specific members into &#39;struct thread_struct&#39;</span>
<span class="quote">&gt; rather than &#39;struct thread_info&#39;, so I&#39;d suggest you do the same here for</span>
<span class="quote">&gt; consistency, unless there is a strong reason against doing it.</span>

We actually can&#39;t put them in &quot;struct thread_struct&quot; in a sane manner.  On
context switches on RISC-V there are no register saved, instead we use the
instructions

  csrrw REG, sscratch, REG

which swaps some register (used to be sp, now tp) with the &quot;sscratch&quot; CSR (a
register only visible to the supervisor).  At this point we only have one
register to work with in order to save the user state.  Since &quot;struct
thread_info&quot; is 0-offset from &quot;struct task_struct&quot;, we&#39;re guaranteed to be able
to access it using our one addressing mode (a 12-bit signed constant offset
from a register).

Unfortunately, &quot;struct thread_struct&quot; is at a potentially large offset from the
saved TP.  We could do something silly like

  addi tp, tp, OFFSET_1
  addi tp, tp, OFFSET_2
  addi tp, tp, OFFSET_3

but for an &quot;allyesconfig&quot; we end up with offsets of about 9KiB, which would
require 4 additional instructions to find &quot;struct thread_struct&quot;.  While this
is possible, I&#39;d prefer to avoid the extra cycles.  We usually handle long
immediate with a two-instruction sequence

  li REG, IMM31-12 (loads the high bits of REG, zeroing the low bits)
  addi REG, REG, IMM11-0 (loads the low bits of REG, leaving the high bits alone)

but there&#39;s no scratch register here so we can&#39;t use that.  We don&#39;t have a
long-immediate-add instruction.

The arguments in x86 land for moving everything to &quot;struct thread_struct&quot; were
that they always know it&#39;s in &quot;struct task_struct&quot;, but since that&#39;s all we&#39;re
supporting on RISC-V I don&#39;t think it counts.  There were also discussions of
eliminating &quot;struct thread_info&quot;, but unless there&#39;s something at the start of
&quot;struct task_struct&quot; then RISC-V will have to pay a bunch of cycles on a
context switch.
<span class="quote">
&gt;&gt;&gt;&gt; +#else /* !CONFIG_MMU */</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline void flush_tlb_all(void)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +       BUG();</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt;&gt; +       BUG();</span>
<span class="quote">&gt;&gt;&gt;&gt; +}</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; The NOMMU support is rather incomplete and CONFIG_MMU is</span>
<span class="quote">&gt;&gt;&gt; hard-enabled, so I&#39;d just drop any !CONFIG_MMU #ifdefs.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; OK.  I&#39;ve left in the &quot;#ifdef CONFIG_MMU&quot; blocks as the #ifdef/#endif doesn&#39;t</span>
<span class="quote">&gt;&gt; really add any code, but I can go ahead and drop the #ifdef if you think that&#39;s</span>
<span class="quote">&gt;&gt; better.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   https://github.com/riscv/riscv-linux/commit/e98ca23adfb9422bebc87cbfb58f70d4a63cf067</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Ok.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +#include &lt;asm-generic/unistd.h&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define __NR_sysriscv  __NR_arch_specific_syscall</span>
<span class="quote">&gt;&gt;&gt;&gt; +#ifndef __riscv_atomic</span>
<span class="quote">&gt;&gt;&gt;&gt; +__SYSCALL(__NR_sysriscv, sys_sysriscv)</span>
<span class="quote">&gt;&gt;&gt;&gt; +#endif</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Please make this a straight cmpxchg syscall and remove the multiplexer.</span>
<span class="quote">&gt;&gt;&gt; Why does the definition depend on __riscv_atomic rather than the</span>
<span class="quote">&gt;&gt;&gt; Kconfig symbol?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think that was just an oversight: that&#39;s not the right switch.  Either you or</span>
<span class="quote">&gt;&gt; someone else pointed out some problems with this.  There&#39;s going to be an</span>
<span class="quote">&gt;&gt; interposer in the VDSO, and then we&#39;ll always enable the system call.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I can change this to two system calls: sysriscv_cmpxchg32 and</span>
<span class="quote">&gt;&gt; sysriscv_cmpxchg64.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Sounds good.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         Arnd</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - June 6, 2017, 8:54 a.m.</div>
<pre class="content">
On Tue, Jun 6, 2017 at 6:56 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:
<span class="quote">&gt; On Thu, 01 Jun 2017 02:00:22 PDT (-0700), Arnd Bergmann wrote:</span>
<span class="quote">&gt;&gt; On Thu, Jun 1, 2017 at 2:56 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; On Tue, 23 May 2017 05:55:15 PDT (-0700), Arnd Bergmann wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Tue, May 23, 2017 at 2:41 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:</span>
<span class="quote">&gt;&gt; - Many architectures have cache line prefetch, flush, zero or copy</span>
<span class="quote">&gt;&gt;   instructions  that are used for important performance optimizations</span>
<span class="quote">&gt;&gt;   but that are typically defined on a cacheline granularity. I don&#39;t</span>
<span class="quote">&gt;&gt;   think you currently have any of them, but it seems likely that there</span>
<span class="quote">&gt;&gt;   will be demand for them later.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; We actually have an implicit prefetch (loads to x0, the zero register), but</span>
<span class="quote">&gt; it still has all the load side-effects so nothing uses it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Having a larger than necessary alignment can waste substantial amounts</span>
<span class="quote">&gt;&gt; of memory for arrays of cache line aligned structures (typically</span>
<span class="quote">&gt;&gt; per-cpu arrays), but otherwise should not cause harm.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I bugged our L1 guy and he says 64-byte lines are a bit of a magic number</span>
<span class="quote">&gt; because of how they line up with DIMMs.  Since there&#39;s no spec to define this,</span>
<span class="quote">&gt; there&#39;s no correct answer.  I&#39;d be amenable to making this a Kconfig option,</span>
<span class="quote">&gt; but I think we&#39;ll leave it alone for now.  It does match the extant</span>
<span class="quote">&gt; implementations.</span>

Hmm, this sounds like a hole in the architecture definition: if you have an
instruction that performs a prefetch (even one that is not easily usable),
I would argue that the cache line size has become a feature of the
architecture and is no longer strictly an implementation detail of the
microarchitecture.

Regarding the memory interface, a lot of systems use two DIMMs on
each memory channel for 128-bit parallel buses, and with LP-DDRx
controllers, you might have a width as small as 16 bits.
<span class="quote">
&gt;&gt;&gt;&gt;&gt; diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; index 000000000000..d942555a7a08</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +++ b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; @@ -0,0 +1,36 @@</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +#ifndef _ASM_RISCV_IO_H</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +#define _ASM_RISCV_IO_H</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; +#include &lt;asm-generic/io.h&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; I would recommend providing your own {read,write}{b,w,l,q}{,_relaxed}</span>
<span class="quote">&gt;&gt;&gt;&gt; helpers using inline assembly, to prevent the compiler for breaking</span>
<span class="quote">&gt;&gt;&gt;&gt; up accesses into byte accesses.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Also, most architectures require to some synchronization after a</span>
<span class="quote">&gt;&gt;&gt;&gt; non-relaxed readl() to prevent prefetching of DMA buffers, and</span>
<span class="quote">&gt;&gt;&gt;&gt; before a writel() to flush write buffers when a DMA gets triggered.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Makes sense.  These were all OK on existing implementations (as there&#39;s no</span>
<span class="quote">&gt;&gt;&gt; writable PMAs, so all MMIO regions are strictly ordered), but that&#39;s not</span>
<span class="quote">&gt;&gt;&gt; actually what the RISC-V ISA says.  I patterned this on arm64</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;   https://github.com/riscv/riscv-linux/commit/e200fa29a69451ef4d575076e4d2af6b7877b1fa</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; where I think the only odd thing is our definition of mmiowb</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;   +/* IO barriers.  These only fence on the IO bits because they&#39;re only required</span>
<span class="quote">&gt;&gt;&gt;   + * to order device access.  We&#39;re defining mmiowb because our AMO instructions</span>
<span class="quote">&gt;&gt;&gt;   + * (which are used to implement locks) don&#39;t specify ordering.  From Chapter 7</span>
<span class="quote">&gt;&gt;&gt;   + * of v2.2 of the user ISA:</span>
<span class="quote">&gt;&gt;&gt;   + * &quot;The bits order accesses to one of the two address domains, memory or I/O,</span>
<span class="quote">&gt;&gt;&gt;   + * depending on which address domain the atomic instruction is accessing. No</span>
<span class="quote">&gt;&gt;&gt;   + * ordering constraint is implied to accesses to the other domain, and a FENCE</span>
<span class="quote">&gt;&gt;&gt;   + * instruction should be used to order across both domains.&quot;</span>
<span class="quote">&gt;&gt;&gt;   + */</span>
<span class="quote">&gt;&gt;&gt;   +</span>
<span class="quote">&gt;&gt;&gt;   +#define __iormb()               __asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt;&gt;   +#define __iowmb()               __asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Looks ok, yes.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;   +#define mmiowb()                __asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; which I think is correct.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I can never remember what exactly this one does.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I can&#39;t find the reference again, but what I found said that if your atomics</span>
<span class="quote">&gt; (or whatever&#39;s used for locking) don&#39;t stay ordered with your MMIO accesses,</span>
<span class="quote">&gt; then you should define mmiowb to ensure ordering.  I managed to screw this up,</span>
<span class="quote">&gt; as there&#39;s no &quot;w&quot; in the successor set (to actually enforce the AMO ordering).</span>
<span class="quote">&gt; This is somewhat confirmed by</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   https://lkml.org/lkml/2006/8/31/174</span>
<span class="quote">&gt;   Subject: Re: When to use mmiowb()?</span>
<span class="quote">&gt;   AFAICT, they&#39;re both right.  Generally, mmiowb() should be used prior to</span>
<span class="quote">&gt;   unlock in a critical section whose last PIO operation is a writeX.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Thus, I think the actual fence should be at least</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   fence o,w</span>
...
<span class="quote">&gt; which matches what&#39;s above.  I think &quot;fence o,w&quot; is sufficient for a mmiowb on</span>
<span class="quote">&gt; RISC-V.  I&#39;ll make the change.</span>

This sounds reasonable according to the documentation, but with your
longer explanation of the barriers, I think the __iormb/__iowmb definitions
above are wrong. What you actually need I think is

void writel(u32 v, volatile void __iomem *addr)
{
         asm volatile(&quot;fence w,o&quot; : : : &quot;memory&quot;);
         writel_relaxed(v, addr);
}

u32 readl(volatile void __iomem *addr)
{
         u32 ret = readl_relaxed(addr);
         asm volatile(&quot;fence i,r&quot; : : : &quot;memory&quot;);
         return ret;
}

to synchronize between DMA and I/O. The barriers you listed above
in contrast appear to be directed at synchronizing I/O with other I/O.
We normally assume that this is not required when you have
subsequent MMIO accesses on the same device (on PCI) or the
same address region (per ARM architecture and others). If you do
need to enforce ordering between MMIO, you might even need to
add those barriers in the relaxed version to be portable with drivers
written for ARM SoCs:

void writel_relaxed(u32 v, volatile void __iomem *addr)
{
        __raw_writel((__force u32)cpu_to_le32(v, addr)
         asm volatile(&quot;fence o,io&quot; : : : &quot;memory&quot;);
}

u32 readl_relaxed(volatile void __iomem *addr)
{
         asm volatile(&quot;fence i,io&quot; : : : &quot;memory&quot;);
         return le32_to_cpu((__force __le32)__raw_readl(addr));
}

You then end up with a barrier before and after each regular
readl/writel in order to synchronize both with DMA and MMIO
instrictructions, and you still need the extre mmiowb() to
synchronize against the spinlock.

My memory on mmiowb is still a bit cloudy, but I think we don&#39;t
need that on ARM, and while PowerPC originally needed it, it is
now implied by the spin_unlock(). If this is actually right, you might
want to do the same here. Very few drivers actually use mmiowb(),
but there might be more drivers that would need it if your
spin_unlock() doesn&#39;t synchronize against writel() or writel_relaxed().
Maybe it the mmiowb() should really be implied by writel() but not
writel_relaxed()? That might be sensible, but only if we do it
the same way on powerpc, which currently doesn&#39;t have
writel_relaxed() any more relaxed than writel()
<span class="quote">
&gt; The arguments in x86 land for moving everything to &quot;struct thread_struct&quot; were</span>
<span class="quote">&gt; that they always know it&#39;s in &quot;struct task_struct&quot;, but since that&#39;s all we&#39;re</span>
<span class="quote">&gt; supporting on RISC-V I don&#39;t think it counts.  There were also discussions of</span>
<span class="quote">&gt; eliminating &quot;struct thread_info&quot;, but unless there&#39;s something at the start of</span>
<span class="quote">&gt; &quot;struct task_struct&quot; then RISC-V will have to pay a bunch of cycles on a</span>
<span class="quote">&gt; context switch.</span>

Ok. Since you have THREAD_INFO_IN_TASK, it probably doesn&#39;t matter
too much then, it&#39;s just a bit inconsistent with the other architectures, but
the effect is the same, so just do it the way that is more efficient for you.

      Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=90941">Palmer Dabbelt</a> - June 6, 2017, 7:07 p.m.</div>
<pre class="content">
On Tue, 06 Jun 2017 01:54:23 PDT (-0700), Arnd Bergmann wrote:
<span class="quote">&gt; On Tue, Jun 6, 2017 at 6:56 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:</span>
<span class="quote">&gt;&gt; On Thu, 01 Jun 2017 02:00:22 PDT (-0700), Arnd Bergmann wrote:</span>
<span class="quote">&gt;&gt;&gt; On Thu, Jun 1, 2017 at 2:56 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On Tue, 23 May 2017 05:55:15 PDT (-0700), Arnd Bergmann wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; On Tue, May 23, 2017 at 2:41 AM, Palmer Dabbelt &lt;palmer@dabbelt.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; new file mode 100644</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; index 000000000000..d942555a7a08</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; --- /dev/null</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +++ b/arch/riscv/include/asm/io.h</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; @@ -0,0 +1,36 @@</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +#ifndef _ASM_RISCV_IO_H</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +#define _ASM_RISCV_IO_H</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;&gt; +#include &lt;asm-generic/io.h&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; I would recommend providing your own {read,write}{b,w,l,q}{,_relaxed}</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; helpers using inline assembly, to prevent the compiler for breaking</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; up accesses into byte accesses.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; Also, most architectures require to some synchronization after a</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; non-relaxed readl() to prevent prefetching of DMA buffers, and</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; before a writel() to flush write buffers when a DMA gets triggered.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Makes sense.  These were all OK on existing implementations (as there&#39;s no</span>
<span class="quote">&gt;&gt;&gt;&gt; writable PMAs, so all MMIO regions are strictly ordered), but that&#39;s not</span>
<span class="quote">&gt;&gt;&gt;&gt; actually what the RISC-V ISA says.  I patterned this on arm64</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;   https://github.com/riscv/riscv-linux/commit/e200fa29a69451ef4d575076e4d2af6b7877b1fa</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; where I think the only odd thing is our definition of mmiowb</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;   +/* IO barriers.  These only fence on the IO bits because they&#39;re only required</span>
<span class="quote">&gt;&gt;&gt;&gt;   + * to order device access.  We&#39;re defining mmiowb because our AMO instructions</span>
<span class="quote">&gt;&gt;&gt;&gt;   + * (which are used to implement locks) don&#39;t specify ordering.  From Chapter 7</span>
<span class="quote">&gt;&gt;&gt;&gt;   + * of v2.2 of the user ISA:</span>
<span class="quote">&gt;&gt;&gt;&gt;   + * &quot;The bits order accesses to one of the two address domains, memory or I/O,</span>
<span class="quote">&gt;&gt;&gt;&gt;   + * depending on which address domain the atomic instruction is accessing. No</span>
<span class="quote">&gt;&gt;&gt;&gt;   + * ordering constraint is implied to accesses to the other domain, and a FENCE</span>
<span class="quote">&gt;&gt;&gt;&gt;   + * instruction should be used to order across both domains.&quot;</span>
<span class="quote">&gt;&gt;&gt;&gt;   + */</span>
<span class="quote">&gt;&gt;&gt;&gt;   +</span>
<span class="quote">&gt;&gt;&gt;&gt;   +#define __iormb()               __asm__ __volatile__ (&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt;&gt;&gt;   +#define __iowmb()               __asm__ __volatile__ (&quot;fence io,o&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Looks ok, yes.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;   +#define mmiowb()                __asm__ __volatile__ (&quot;fence io,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; which I think is correct.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; I can never remember what exactly this one does.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I can&#39;t find the reference again, but what I found said that if your atomics</span>
<span class="quote">&gt;&gt; (or whatever&#39;s used for locking) don&#39;t stay ordered with your MMIO accesses,</span>
<span class="quote">&gt;&gt; then you should define mmiowb to ensure ordering.  I managed to screw this up,</span>
<span class="quote">&gt;&gt; as there&#39;s no &quot;w&quot; in the successor set (to actually enforce the AMO ordering).</span>
<span class="quote">&gt;&gt; This is somewhat confirmed by</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   https://lkml.org/lkml/2006/8/31/174</span>
<span class="quote">&gt;&gt;   Subject: Re: When to use mmiowb()?</span>
<span class="quote">&gt;&gt;   AFAICT, they&#39;re both right.  Generally, mmiowb() should be used prior to</span>
<span class="quote">&gt;&gt;   unlock in a critical section whose last PIO operation is a writeX.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Thus, I think the actual fence should be at least</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;   fence o,w</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;&gt; which matches what&#39;s above.  I think &quot;fence o,w&quot; is sufficient for a mmiowb on</span>
<span class="quote">&gt;&gt; RISC-V.  I&#39;ll make the change.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This sounds reasonable according to the documentation, but with your</span>
<span class="quote">&gt; longer explanation of the barriers, I think the __iormb/__iowmb definitions</span>
<span class="quote">&gt; above are wrong. What you actually need I think is</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; void writel(u32 v, volatile void __iomem *addr)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;          asm volatile(&quot;fence w,o&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;          writel_relaxed(v, addr);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; u32 readl(volatile void __iomem *addr)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;          u32 ret = readl_relaxed(addr);</span>
<span class="quote">&gt;          asm volatile(&quot;fence i,r&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;          return ret;</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; to synchronize between DMA and I/O. The barriers you listed above</span>
<span class="quote">&gt; in contrast appear to be directed at synchronizing I/O with other I/O.</span>
<span class="quote">&gt; We normally assume that this is not required when you have</span>
<span class="quote">&gt; subsequent MMIO accesses on the same device (on PCI) or the</span>
<span class="quote">&gt; same address region (per ARM architecture and others). If you do</span>
<span class="quote">&gt; need to enforce ordering between MMIO, you might even need to</span>
<span class="quote">&gt; add those barriers in the relaxed version to be portable with drivers</span>
<span class="quote">&gt; written for ARM SoCs:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; void writel_relaxed(u32 v, volatile void __iomem *addr)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         __raw_writel((__force u32)cpu_to_le32(v, addr)</span>
<span class="quote">&gt;          asm volatile(&quot;fence o,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; u32 readl_relaxed(volatile void __iomem *addr)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;          asm volatile(&quot;fence i,io&quot; : : : &quot;memory&quot;);</span>
<span class="quote">&gt;          return le32_to_cpu((__force __le32)__raw_readl(addr));</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; You then end up with a barrier before and after each regular</span>
<span class="quote">&gt; readl/writel in order to synchronize both with DMA and MMIO</span>
<span class="quote">&gt; instrictructions, and you still need the extre mmiowb() to</span>
<span class="quote">&gt; synchronize against the spinlock.</span>

Ah, thanks.  I guess I just had those wrong.  I&#39;ll fix the non-relaxed versions
and add relaxed versions that have the fence.
<span class="quote">
&gt; My memory on mmiowb is still a bit cloudy, but I think we don&#39;t</span>
<span class="quote">&gt; need that on ARM, and while PowerPC originally needed it, it is</span>
<span class="quote">&gt; now implied by the spin_unlock(). If this is actually right, you might</span>
<span class="quote">&gt; want to do the same here. Very few drivers actually use mmiowb(),</span>
<span class="quote">&gt; but there might be more drivers that would need it if your</span>
<span class="quote">&gt; spin_unlock() doesn&#39;t synchronize against writel() or writel_relaxed().</span>
<span class="quote">&gt; Maybe it the mmiowb() should really be implied by writel() but not</span>
<span class="quote">&gt; writel_relaxed()? That might be sensible, but only if we do it</span>
<span class="quote">&gt; the same way on powerpc, which currently doesn&#39;t have</span>
<span class="quote">&gt; writel_relaxed() any more relaxed than writel()</span>

I like the idea of putting this in spin_unlock() better, particularly if
PowerPC does it that way.  I was worried that with mmiowb being such an
esoteric thing that it would be wrong all over the place, this feels much
better.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/riscv/include/asm/Kbuild b/arch/riscv/include/asm/Kbuild</span>
new file mode 100644
<span class="p_header">index 000000000000..d861450998bb</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/Kbuild</span>
<span class="p_chunk">@@ -0,0 +1,60 @@</span> <span class="p_context"></span>
<span class="p_add">+generic-y += bugs.h</span>
<span class="p_add">+generic-y += cacheflush.h</span>
<span class="p_add">+generic-y += checksum.h</span>
<span class="p_add">+generic-y += clkdev.h</span>
<span class="p_add">+generic-y += cputime.h</span>
<span class="p_add">+generic-y += current.h</span>
<span class="p_add">+generic-y += div64.h</span>
<span class="p_add">+generic-y += dma.h</span>
<span class="p_add">+generic-y += emergency-restart.h</span>
<span class="p_add">+generic-y += errno.h</span>
<span class="p_add">+generic-y += exec.h</span>
<span class="p_add">+generic-y += fb.h</span>
<span class="p_add">+generic-y += fcntl.h</span>
<span class="p_add">+generic-y += ftrace.h</span>
<span class="p_add">+generic-y += futex.h</span>
<span class="p_add">+generic-y += hardirq.h</span>
<span class="p_add">+generic-y += hash.h</span>
<span class="p_add">+generic-y += hw_irq.h</span>
<span class="p_add">+generic-y += ioctl.h</span>
<span class="p_add">+generic-y += ioctls.h</span>
<span class="p_add">+generic-y += ipcbuf.h</span>
<span class="p_add">+generic-y += irq_regs.h</span>
<span class="p_add">+generic-y += irq_work.h</span>
<span class="p_add">+generic-y += kdebug.h</span>
<span class="p_add">+generic-y += kmap_types.h</span>
<span class="p_add">+generic-y += kvm_para.h</span>
<span class="p_add">+generic-y += local.h</span>
<span class="p_add">+generic-y += mm-arch-hooks.h</span>
<span class="p_add">+generic-y += mman.h</span>
<span class="p_add">+generic-y += module.h</span>
<span class="p_add">+generic-y += msgbuf.h</span>
<span class="p_add">+generic-y += mutex.h</span>
<span class="p_add">+generic-y += param.h</span>
<span class="p_add">+generic-y += percpu.h</span>
<span class="p_add">+generic-y += poll.h</span>
<span class="p_add">+generic-y += posix_types.h</span>
<span class="p_add">+generic-y += preempt.h</span>
<span class="p_add">+generic-y += resource.h</span>
<span class="p_add">+generic-y += scatterlist.h</span>
<span class="p_add">+generic-y += sections.h</span>
<span class="p_add">+generic-y += sembuf.h</span>
<span class="p_add">+generic-y += shmbuf.h</span>
<span class="p_add">+generic-y += shmparam.h</span>
<span class="p_add">+generic-y += signal.h</span>
<span class="p_add">+generic-y += socket.h</span>
<span class="p_add">+generic-y += sockios.h</span>
<span class="p_add">+generic-y += stat.h</span>
<span class="p_add">+generic-y += statfs.h</span>
<span class="p_add">+generic-y += swab.h</span>
<span class="p_add">+generic-y += termbits.h</span>
<span class="p_add">+generic-y += termios.h</span>
<span class="p_add">+generic-y += topology.h</span>
<span class="p_add">+generic-y += trace_clock.h</span>
<span class="p_add">+generic-y += types.h</span>
<span class="p_add">+generic-y += ucontext.h</span>
<span class="p_add">+generic-y += unaligned.h</span>
<span class="p_add">+generic-y += user.h</span>
<span class="p_add">+generic-y += vga.h</span>
<span class="p_add">+generic-y += vmlinux.lds.h</span>
<span class="p_add">+generic-y += xor.h</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/asm-offsets.h b/arch/riscv/include/asm/asm-offsets.h</span>
new file mode 100644
<span class="p_header">index 000000000000..d370ee36a182</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/asm-offsets.h</span>
<span class="p_chunk">@@ -0,0 +1 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;generated/asm-offsets.h&gt;</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/asm.h b/arch/riscv/include/asm/asm.h</span>
new file mode 100644
<span class="p_header">index 000000000000..87f27603286c</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/asm.h</span>
<span class="p_chunk">@@ -0,0 +1,65 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ * </span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ASM_H</span>
<span class="p_add">+#define _ASM_RISCV_ASM_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __ASSEMBLY__</span>
<span class="p_add">+#define __ASM_STR(x)	x</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define __ASM_STR(x)	#x</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if __riscv_xlen == 64</span>
<span class="p_add">+#define __REG_SEL(a, b)	__ASM_STR(a)</span>
<span class="p_add">+#elif __riscv_xlen == 32</span>
<span class="p_add">+#define __REG_SEL(a, b)	__ASM_STR(b)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected __riscv_xlen&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define REG_L		__REG_SEL(ld, lw)</span>
<span class="p_add">+#define REG_S		__REG_SEL(sd, sw)</span>
<span class="p_add">+#define SZREG		__REG_SEL(8, 4)</span>
<span class="p_add">+#define LGREG		__REG_SEL(3, 2)</span>
<span class="p_add">+</span>
<span class="p_add">+#if __SIZEOF_POINTER__ == 8</span>
<span class="p_add">+#define __PTR_SEL(a, b)	__ASM_STR(a)</span>
<span class="p_add">+#elif __SIZEOF_POINTER__ == 4</span>
<span class="p_add">+#define __PTR_SEL(a, b)	__ASM_STR(b)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected __SIZEOF_POINTER__&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTR		__PTR_SEL(.dword, .word)</span>
<span class="p_add">+#define SZPTR		__PTR_SEL(8, 4)</span>
<span class="p_add">+#define LGPTR		__PTR_SEL(3, 2)</span>
<span class="p_add">+</span>
<span class="p_add">+#if (__SIZEOF_INT__ == 4)</span>
<span class="p_add">+#define INT		__ASM_STR(.word)</span>
<span class="p_add">+#define SZINT		__ASM_STR(4)</span>
<span class="p_add">+#define LGINT		__ASM_STR(2)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected __SIZEOF_INT__&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if (__SIZEOF_SHORT__ == 2)</span>
<span class="p_add">+#define SHORT		__ASM_STR(.half)</span>
<span class="p_add">+#define SZSHORT		__ASM_STR(2)</span>
<span class="p_add">+#define LGSHORT		__ASM_STR(1)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected __SIZEOF_SHORT__&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ASM_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/atomic.h b/arch/riscv/include/asm/atomic.h</span>
new file mode 100644
<span class="p_header">index 000000000000..2f6f78c5ddd8</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/atomic.h</span>
<span class="p_chunk">@@ -0,0 +1,349 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ * modify it under the terms of the GNU General Public Licence</span>
<span class="p_add">+ * as published by the Free Software Foundation; either version</span>
<span class="p_add">+ * 2 of the Licence, or (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+#define _ASM_RISCV_ATOMIC_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_RV_ATOMIC</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cmpxchg.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC_INIT(i)	{ (i) }</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_read - read atomic variable</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically reads the value of @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_read(const atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return *((volatile int *)(&amp;(v-&gt;counter)));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_set - set atomic variable</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ * @i: required value</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically sets the value of @v to @i.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_set(atomic_t *v, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	v-&gt;counter = i;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_add - add integer to atomic variable</span>
<span class="p_add">+ * @i: integer value to add</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @i to @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_add(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.w zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (i));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_fetch_add atomic_fetch_add</span>
<span class="p_add">+static inline int atomic_fetch_add(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.w %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_sub - subtract integer from atomic variable</span>
<span class="p_add">+ * @i: integer value to subtract</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically subtracts @i from @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_sub(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_add(-i, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_fetch_sub atomic_fetch_sub</span>
<span class="p_add">+static inline int atomic_fetch_sub(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amosub.w %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_add_return - add integer to atomic variable</span>
<span class="p_add">+ * @i: integer value to add</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @i to @v and returns the result</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_add_return(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register int c;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.w %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (c), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (i));</span>
<span class="p_add">+	return (c + i);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_sub_return - subtract integer from atomic variable</span>
<span class="p_add">+ * @i: integer value to subtract</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically subtracts @i from @v and returns the result</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_sub_return(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic_add_return(-i, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_inc - increment atomic variable</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically increments @v by 1.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_inc(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_add(1, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_dec - decrement atomic variable</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically decrements @v by 1.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_dec(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_add(-1, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int atomic_inc_return(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic_add_return(1, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int atomic_dec_return(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic_sub_return(1, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_sub_and_test - subtract value from variable and test result</span>
<span class="p_add">+ * @i: integer value to subtract</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically subtracts @i from @v and returns</span>
<span class="p_add">+ * true if the result is zero, or false for all</span>
<span class="p_add">+ * other cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_sub_and_test(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic_sub_return(i, v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_inc_and_test - increment and test</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically increments @v by 1</span>
<span class="p_add">+ * and returns true if the result is zero, or false for all</span>
<span class="p_add">+ * other cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_inc_and_test(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic_inc_return(v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_dec_and_test - decrement and test</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically decrements @v by 1 and</span>
<span class="p_add">+ * returns true if the result is 0, or false for all other</span>
<span class="p_add">+ * cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_dec_and_test(atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic_dec_return(v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_add_negative - add and test if negative</span>
<span class="p_add">+ * @i: integer value to add</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @i to @v and returns true</span>
<span class="p_add">+ * if the result is negative, or false when</span>
<span class="p_add">+ * result is greater than or equal to zero.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic_add_negative(int i, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic_add_return(i, v) &lt; 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int atomic_xchg(atomic_t *v, int n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register int c;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (c), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (n));</span>
<span class="p_add">+	return c;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int atomic_cmpxchg(atomic_t *v, int o, int n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return cmpxchg(&amp;(v-&gt;counter), o, n);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __atomic_add_unless - add unless the number is already a given value</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ * @a: the amount to add to v...</span>
<span class="p_add">+ * @u: ...unless v is equal to u.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @a to @v, so long as @v was not already @u.</span>
<span class="p_add">+ * Returns the old value of @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int __atomic_add_unless(atomic_t *v, int a, int u)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register int prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+	&quot;0:&quot;</span>
<span class="p_add">+		&quot;lr.w %0, %2\n&quot;</span>
<span class="p_add">+		&quot;beq  %0, %4, 1f\n&quot;</span>
<span class="p_add">+		&quot;add  %1, %0, %3\n&quot;</span>
<span class="p_add">+		&quot;sc.w %1, %1, %2\n&quot;</span>
<span class="p_add">+		&quot;bnez %1, 0b\n&quot;</span>
<span class="p_add">+	&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (prev), &quot;=&amp;r&quot; (rc), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a), &quot;r&quot; (u));</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_and - Atomically clear bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be retained</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically retains the bits set in @mask from @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_and(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoand.w zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_fetch_and atomic_fetch_and</span>
<span class="p_add">+static inline int atomic_fetch_and(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoand.w %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_or - Atomically set bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be set</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically sets the bits set in @mask in @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_or(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoor.w zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_fetch_or atomic_fetch_or</span>
<span class="p_add">+static inline int atomic_fetch_or(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoor.w %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic_xor - Atomically flips bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be flipped</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically flips the bits set in @mask in @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic_xor(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoxor.w zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define atomic_fetch_xor atomic_fetch_xor</span>
<span class="p_add">+static inline int atomic_fetch_xor(unsigned int mask, atomic_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int out;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoxor.w %2, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+	return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Assume that atomic operations are already serializing */</span>
<span class="p_add">+#define smp_mb__before_atomic_dec()	barrier()</span>
<span class="p_add">+#define smp_mb__after_atomic_dec()	barrier()</span>
<span class="p_add">+#define smp_mb__before_atomic_inc()	barrier()</span>
<span class="p_add">+#define smp_mb__after_atomic_inc()	barrier()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_RV_ATOMIC */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/atomic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_RV_ATOMIC */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/atomic64.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ATOMIC_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/atomic64.h b/arch/riscv/include/asm/atomic64.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5ffcc32e2d1e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/atomic64.h</span>
<span class="p_chunk">@@ -0,0 +1,355 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2007 Red Hat, Inc. All Rights Reserved.</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ * modify it under the terms of the GNU General Public Licence</span>
<span class="p_add">+ * as published by the Free Software Foundation; either version</span>
<span class="p_add">+ * 2 of the Licence, or (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ATOMIC64_H</span>
<span class="p_add">+#define _ASM_RISCV_ATOMIC64_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_ATOMIC64</span>
<span class="p_add">+#include &lt;asm-generic/atomic64.h&gt;</span>
<span class="p_add">+#else /* !CONFIG_GENERIC_ATOMIC64 */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ATOMIC64_INIT(i)	{ (i) }</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_read - read atomic64 variable</span>
<span class="p_add">+ * @v: pointer of type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically reads the value of @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline s64 atomic64_read(const atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return *((volatile long *)(&amp;(v-&gt;counter)));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_set - set atomic64 variable</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ * @i: required value</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically sets the value of @v to @i.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_set(atomic64_t *v, s64 i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	v-&gt;counter = i;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_add - add integer to atomic64 variable</span>
<span class="p_add">+ * @i: integer value to add</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @i to @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_add(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.d zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long atomic64_fetch_add(unsigned long mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        long out;</span>
<span class="p_add">+</span>
<span class="p_add">+        __asm__ __volatile__ (</span>
<span class="p_add">+                &quot;amoadd.d %2, %1, %0&quot;</span>
<span class="p_add">+                : &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+                : &quot;r&quot; (mask));</span>
<span class="p_add">+        return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_sub - subtract the atomic64 variable</span>
<span class="p_add">+ * @i: integer value to subtract</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically subtracts @i from @v.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_sub(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic64_add(-a, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long atomic64_fetch_sub(unsigned long mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        long out;</span>
<span class="p_add">+</span>
<span class="p_add">+        __asm__ __volatile__ (</span>
<span class="p_add">+                &quot;amosub.d %2, %1, %0&quot;</span>
<span class="p_add">+                : &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+                : &quot;r&quot; (mask));</span>
<span class="p_add">+        return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_add_return - add and return</span>
<span class="p_add">+ * @i: integer value to add</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @i to @v and returns @i + @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline s64 atomic64_add_return(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register s64 c;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoadd.d %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (c), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (a));</span>
<span class="p_add">+	return (c + a);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline s64 atomic64_sub_return(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic64_add_return(-a, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_inc - increment atomic64 variable</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically increments @v by 1.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_inc(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic64_add(1L, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_dec - decrement atomic64 variable</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically decrements @v by 1.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_dec(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic64_add(-1L, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline s64 atomic64_inc_return(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic64_add_return(1L, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline s64 atomic64_dec_return(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic64_add_return(-1L, v);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_inc_and_test - increment and test</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically increments @v by 1</span>
<span class="p_add">+ * and returns true if the result is zero, or false for all</span>
<span class="p_add">+ * other cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic64_inc_and_test(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic64_inc_return(v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_dec_and_test - decrement and test</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically decrements @v by 1 and</span>
<span class="p_add">+ * returns true if the result is 0, or false for all other</span>
<span class="p_add">+ * cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic64_dec_and_test(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic64_dec_return(v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_sub_and_test - subtract value from variable and test result</span>
<span class="p_add">+ * @a: integer value to subtract</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically subtracts @a from @v and returns</span>
<span class="p_add">+ * true if the result is zero, or false for all</span>
<span class="p_add">+ * other cases.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic64_sub_and_test(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic64_sub_return(a, v) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_add_negative - add and test if negative</span>
<span class="p_add">+ * @a: integer value to add</span>
<span class="p_add">+ * @v: pointer to type atomic64_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @a to @v and returns true</span>
<span class="p_add">+ * if the result is negative, or false when</span>
<span class="p_add">+ * result is greater than or equal to zero.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic64_add_negative(s64 a, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (atomic64_add_return(a, v) &lt; 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+static inline s64 atomic64_xchg(atomic64_t *v, s64 n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register s64 c;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.d %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (c), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (n));</span>
<span class="p_add">+	return c;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline s64 atomic64_cmpxchg(atomic64_t *v, s64 o, s64 n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return cmpxchg(&amp;(v-&gt;counter), o, n);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * atomic64_dec_if_positive - decrement by 1 if old value positive</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The function returns the old value of *v minus 1, even if</span>
<span class="p_add">+ * the atomic variable, v, was not decremented.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline s64 atomic64_dec_if_positive(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register s64 prev, rc;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+	&quot;0:&quot;</span>
<span class="p_add">+		&quot;lr.d %0, %2\n&quot;</span>
<span class="p_add">+		&quot;add  %0, %0, -1\n&quot;</span>
<span class="p_add">+		&quot;bltz %0, 1f\n&quot;</span>
<span class="p_add">+		&quot;sc.w %1, %0, %2\n&quot;</span>
<span class="p_add">+		&quot;bnez %1, 0b\n&quot;</span>
<span class="p_add">+	&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (prev), &quot;=r&quot; (rc), &quot;+A&quot; (v-&gt;counter));</span>
<span class="p_add">+	return prev;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_add_unless - add unless the number is a given value</span>
<span class="p_add">+ * @v: pointer of type atomic64_t</span>
<span class="p_add">+ * @a: the amount to add to v...</span>
<span class="p_add">+ * @u: ...unless v is equal to u.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically adds @a to @v, so long as it was not @u.</span>
<span class="p_add">+ * Returns true if the addition occurred and false otherwise.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int atomic64_add_unless(atomic64_t *v, s64 a, s64 u)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register s64 tmp;</span>
<span class="p_add">+	register int rc = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+	&quot;0:&quot;</span>
<span class="p_add">+		&quot;lr.d %0, %2\n&quot;</span>
<span class="p_add">+		&quot;beq  %0, %z4, 1f\n&quot;</span>
<span class="p_add">+		&quot;add  %0, %0, %3\n&quot;</span>
<span class="p_add">+		&quot;sc.d %1, %0, %2\n&quot;</span>
<span class="p_add">+		&quot;bnez %1, 0b\n&quot;</span>
<span class="p_add">+	&quot;1:&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (tmp), &quot;=&amp;r&quot; (rc), &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;rI&quot; (a), &quot;rJ&quot; (u));</span>
<span class="p_add">+	return !rc;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int atomic64_inc_not_zero(atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic64_add_unless(v, 1, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_and - Atomically clear bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be retained</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically retains the bits set in @mask from @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_and(s64 mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoand.d zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long atomic64_fetch_and(unsigned long mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        long out;</span>
<span class="p_add">+</span>
<span class="p_add">+        __asm__ __volatile__ (</span>
<span class="p_add">+                &quot;amoand.d %2, %1, %0&quot;</span>
<span class="p_add">+                : &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+                : &quot;r&quot; (mask));</span>
<span class="p_add">+        return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_or - Atomically set bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be set</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically sets the bits set in @mask in @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_or(s64 mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoor.d zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long atomic64_fetch_or(unsigned long mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        long out;</span>
<span class="p_add">+</span>
<span class="p_add">+        __asm__ __volatile__ (</span>
<span class="p_add">+                &quot;amoor.d %2, %1, %0&quot;</span>
<span class="p_add">+                : &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+                : &quot;r&quot; (mask));</span>
<span class="p_add">+        return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * atomic64_xor - Atomically flips bits in atomic variable</span>
<span class="p_add">+ * @mask: Mask of the bits to be flipped</span>
<span class="p_add">+ * @v: pointer of type atomic_t</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Atomically flips the bits set in @mask in @v</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void atomic64_xor(s64 mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoxor.d zero, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (v-&gt;counter)</span>
<span class="p_add">+		: &quot;r&quot; (mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long atomic64_fetch_xor(unsigned long mask, atomic64_t *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+        long out;</span>
<span class="p_add">+</span>
<span class="p_add">+        __asm__ __volatile__ (</span>
<span class="p_add">+                &quot;amoxor.d %2, %1, %0&quot;</span>
<span class="p_add">+                : &quot;+A&quot; (v-&gt;counter), &quot;=r&quot; (out)</span>
<span class="p_add">+                : &quot;r&quot; (mask));</span>
<span class="p_add">+        return out;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_GENERIC_ATOMIC64 */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ATOMIC64_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/barrier.h b/arch/riscv/include/asm/barrier.h</span>
new file mode 100644
<span class="p_header">index 000000000000..e340a80135ae</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/barrier.h</span>
<span class="p_chunk">@@ -0,0 +1,33 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Based on arch/arm/include/asm/barrier.h</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2013 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+#define _ASM_RISCV_BARRIER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define nop()	__asm__ __volatile__ (&quot;nop&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+#define mb()	__asm__ __volatile__ (&quot;fence&quot; : : : &quot;memory&quot;)</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BARRIER_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/bitops.h b/arch/riscv/include/asm/bitops.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c470f02f6f06</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/bitops.h</span>
<span class="p_chunk">@@ -0,0 +1,229 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+#define _ASM_RISCV_BITOPS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_BITOPS_H</span>
<span class="p_add">+#error &quot;Only &lt;linux/bitops.h&gt; can be included directly&quot;</span>
<span class="p_add">+#endif /* _LINUX_BITOPS_H */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/irqflags.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+#include &lt;asm/bitsperlong.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_RV_ATOMIC</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef smp_mb__before_clear_bit</span>
<span class="p_add">+#define smp_mb__before_clear_bit()  smp_mb()</span>
<span class="p_add">+#define smp_mb__after_clear_bit()   smp_mb()</span>
<span class="p_add">+#endif /* smp_mb__before_clear_bit */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__ffs.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffz.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/__fls.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/fls64.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/find.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/sched.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ffs.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/hweight.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#if (BITS_PER_LONG == 64)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.d&quot;</span>
<span class="p_add">+#elif (BITS_PER_LONG == 32)</span>
<span class="p_add">+#define __AMO(op)	&quot;amo&quot; #op &quot;.w&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unexpected BITS_PER_LONG&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define __test_and_op_bit(op, mod, nr, addr)			\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __res, __mask;				\</span>
<span class="p_add">+	__mask = BIT_MASK(nr);					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) &quot; %0, %2, %1&quot;				\</span>
<span class="p_add">+		: &quot;=r&quot; (__res), &quot;+A&quot; (addr[BIT_WORD(nr)])	\</span>
<span class="p_add">+		: &quot;r&quot; (mod(__mask)));				\</span>
<span class="p_add">+	((__res &amp; __mask) != 0);				\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define __op_bit(op, mod, nr, addr)				\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		__AMO(op) &quot; zero, %1, %0&quot;			\</span>
<span class="p_add">+		: &quot;+A&quot; (addr[BIT_WORD(nr)])			\</span>
<span class="p_add">+		: &quot;r&quot; (mod(BIT_MASK(nr))))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Bitmask modifiers */</span>
<span class="p_add">+#define __NOP(x)	(x)</span>
<span class="p_add">+#define __NOT(x)	(~(x))</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit - Set a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It may be reordered on other architectures than x86.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_clear_bit - Clear a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It can be reordered on other architectures other than x86.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_change_bit - Change a bit and return its old value</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and cannot be reordered.</span>
<span class="p_add">+ * It also implies a memory barrier.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __test_and_op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * set_bit - Atomically set a bit in memory</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function is atomic and may not be reordered.  See __set_bit()</span>
<span class="p_add">+ * if you do not require the atomic guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: there are no guarantees that this function will not be reordered</span>
<span class="p_add">+ * on non x86 architectures, so if you are writing portable code,</span>
<span class="p_add">+ * make sure not to rely on its reordering guarantees.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(or, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit - Clears a bit in memory</span>
<span class="p_add">+ * @nr: Bit to clear</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * clear_bit() is atomic and may not be reordered.  However, it does</span>
<span class="p_add">+ * not contain a memory barrier, so if it is used for locking purposes,</span>
<span class="p_add">+ * you should call smp_mb__before_clear_bit() and/or smp_mb__after_clear_bit()</span>
<span class="p_add">+ * in order to ensure changes are visible on other processors.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(and, __NOT, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * change_bit - Toggle a bit in memory</span>
<span class="p_add">+ * @nr: Bit to change</span>
<span class="p_add">+ * @addr: Address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * change_bit() is atomic and may not be reordered. It may be</span>
<span class="p_add">+ * reordered on other architectures than x86.</span>
<span class="p_add">+ * Note that @nr may be almost arbitrarily large; this function is not</span>
<span class="p_add">+ * restricted to acting on a single-word quantity.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void change_bit(int nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__op_bit(xor, __NOP, nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * test_and_set_bit_lock - Set a bit and return its old value, for lock</span>
<span class="p_add">+ * @nr: Bit to set</span>
<span class="p_add">+ * @addr: Address to count from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides acquire barrier semantics.</span>
<span class="p_add">+ * It can be used to implement bit locks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int test_and_set_bit_lock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return test_and_set_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is atomic and provides release barrier semantics.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __clear_bit_unlock - Clear a bit in memory, for unlock</span>
<span class="p_add">+ * @nr: the bit to set</span>
<span class="p_add">+ * @addr: the address to start counting from</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This operation is like clear_bit_unlock, however it is not atomic.</span>
<span class="p_add">+ * It does provide release barrier semantics so it can be used to unlock</span>
<span class="p_add">+ * a bit lock, however it would only be used if no other CPU can modify</span>
<span class="p_add">+ * any bits in the memory until the lock is released (a good example is</span>
<span class="p_add">+ * if the bit lock itself protects access to the other bits in the word).</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void __clear_bit_unlock(</span>
<span class="p_add">+	unsigned long nr, volatile unsigned long *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	clear_bit(nr, addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#undef __test_and_op_bit</span>
<span class="p_add">+#undef __op_bit</span>
<span class="p_add">+#undef __NOP</span>
<span class="p_add">+#undef __NOT</span>
<span class="p_add">+#undef __AMO</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops/non-atomic.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/le.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/bitops/ext2-atomic.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_RV_ATOMIC */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitops.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_RV_ATOMIC */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BITOPS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/bug.h b/arch/riscv/include/asm/bug.h</span>
new file mode 100644
<span class="p_header">index 000000000000..10d894ac3137</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/bug.h</span>
<span class="p_chunk">@@ -0,0 +1,81 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_BUG_H</span>
<span class="p_add">+#define _ASM_RISCV_BUG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/asm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_BUG</span>
<span class="p_add">+#define __BUG_INSN	_AC(0x00100073, UL) /* sbreak */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+typedef u32 bug_insn_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_GENERIC_BUG_RELATIVE_POINTERS</span>
<span class="p_add">+#define __BUG_ENTRY_ADDR	INT &quot; 1b - 2b&quot;</span>
<span class="p_add">+#define __BUG_ENTRY_FILE	INT &quot; %0 - 2b&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define __BUG_ENTRY_ADDR	PTR &quot; 1b&quot;</span>
<span class="p_add">+#define __BUG_ENTRY_FILE	PTR &quot; %0&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_DEBUG_BUGVERBOSE</span>
<span class="p_add">+#define __BUG_ENTRY			\</span>
<span class="p_add">+	__BUG_ENTRY_ADDR &quot;\n\t&quot;		\</span>
<span class="p_add">+	__BUG_ENTRY_FILE &quot;\n\t&quot;		\</span>
<span class="p_add">+	SHORT &quot; %1&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define __BUG_ENTRY			\</span>
<span class="p_add">+	__BUG_ENTRY_ADDR</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define BUG()							\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;1:\n\t&quot;					\</span>
<span class="p_add">+			&quot;sbreak\n&quot;				\</span>
<span class="p_add">+			&quot;.pushsection __bug_table,\&quot;a\&quot;\n\t&quot;	\</span>
<span class="p_add">+		&quot;2:\n\t&quot;					\</span>
<span class="p_add">+			__BUG_ENTRY &quot;\n\t&quot;			\</span>
<span class="p_add">+			&quot;.org 2b + %2\n\t&quot;			\</span>
<span class="p_add">+			&quot;.popsection&quot;				\</span>
<span class="p_add">+		:						\</span>
<span class="p_add">+		: &quot;i&quot; (__FILE__), &quot;i&quot; (__LINE__),		\</span>
<span class="p_add">+		  &quot;i&quot; (sizeof(struct bug_entry)));		\</span>
<span class="p_add">+	unreachable();						\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define HAVE_ARCH_BUG</span>
<span class="p_add">+#endif /* !__ASSEMBLY__ */</span>
<span class="p_add">+#endif /* CONFIG_GENERIC_BUG */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bug.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+struct pt_regs;</span>
<span class="p_add">+struct task_struct;</span>
<span class="p_add">+</span>
<span class="p_add">+extern void die(struct pt_regs *regs, const char *str);</span>
<span class="p_add">+extern void do_trap(struct pt_regs *regs, int signo, int code,</span>
<span class="p_add">+	unsigned long addr, struct task_struct *tsk);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !__ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_BUG_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cache.h b/arch/riscv/include/asm/cache.h</span>
new file mode 100644
<span class="p_header">index 000000000000..02082e118178</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cache.h</span>
<span class="p_chunk">@@ -0,0 +1,23 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2017 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CACHE_H</span>
<span class="p_add">+#define _ASM_RISCV_CACHE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define L1_CACHE_SHIFT		6</span>
<span class="p_add">+</span>
<span class="p_add">+#define L1_CACHE_BYTES		(1 &lt;&lt; L1_CACHE_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CACHE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cacheflush.h b/arch/riscv/include/asm/cacheflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0546ae75d368</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cacheflush.h</span>
<span class="p_chunk">@@ -0,0 +1,40 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_CACHEFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/cacheflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#undef flush_icache_range</span>
<span class="p_add">+#undef flush_icache_user_range</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void local_flush_icache_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	asm volatile (&quot;fence.i&quot; ::: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) local_flush_icache_all()</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) local_flush_icache_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_icache_range(start, end) sbi_remote_fence_i(0)</span>
<span class="p_add">+#define flush_icache_user_range(vma, pg, addr, len) sbi_remote_fence_i(0)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CACHEFLUSH_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/cmpxchg.h b/arch/riscv/include/asm/cmpxchg.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c875e6279902</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -0,0 +1,125 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+#define _ASM_RISCV_CMPXCHG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_RV_ATOMIC</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __xchg(new, ptr, size)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);				\</span>
<span class="p_add">+	__typeof__(new) __new = (new);				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;				\</span>
<span class="p_add">+	switch (size) {						\</span>
<span class="p_add">+	case 4:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.w %0, %2, %1&quot;			\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new));				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 8:							\</span>
<span class="p_add">+		__asm__ __volatile__ (				\</span>
<span class="p_add">+			&quot;amoswap.d %0, %2, %1&quot;			\</span>
<span class="p_add">+			: &quot;=r&quot; (__ret), &quot;+A&quot; (*__ptr)		\</span>
<span class="p_add">+			: &quot;r&quot; (__new));				\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	default:						\</span>
<span class="p_add">+		BUILD_BUG();					\</span>
<span class="p_add">+	}							\</span>
<span class="p_add">+	__ret;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define xchg(ptr, x)    (__xchg((x), (ptr), sizeof(*(ptr))))</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Atomic compare and exchange.  Compare OLD with MEM, if identical,</span>
<span class="p_add">+ * store NEW in MEM.  Return the initial value in MEM.  Success is</span>
<span class="p_add">+ * indicated by comparing RETURN with OLD.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __cmpxchg(ptr, old, new, size)					\</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);					\</span>
<span class="p_add">+	__typeof__(old) __old = (old);					\</span>
<span class="p_add">+	__typeof__(new) __new = (new);					\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;					\</span>
<span class="p_add">+	register unsigned int __rc;					\</span>
<span class="p_add">+	switch (size) {							\</span>
<span class="p_add">+	case 4:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.w %0, %2\n&quot;					\</span>
<span class="p_add">+			&quot;bne  %0, %z3, 1f\n&quot;				\</span>
<span class="p_add">+			&quot;sc.w %1, %z4, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bnez %1, 0b\n&quot;					\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	case 8:								\</span>
<span class="p_add">+		__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;0:&quot;							\</span>
<span class="p_add">+			&quot;lr.d %0, %2\n&quot;					\</span>
<span class="p_add">+			&quot;bne  %0, %z3, 1f\n&quot;				\</span>
<span class="p_add">+			&quot;sc.d %1, %z4, %2\n&quot;				\</span>
<span class="p_add">+			&quot;bnez %1, 0b\n&quot;					\</span>
<span class="p_add">+		&quot;1:&quot;							\</span>
<span class="p_add">+			: &quot;=&amp;r&quot; (__ret), &quot;=&amp;r&quot; (__rc), &quot;+A&quot; (*__ptr)	\</span>
<span class="p_add">+			: &quot;rJ&quot; (__old), &quot;rJ&quot; (__new));			\</span>
<span class="p_add">+		break;							\</span>
<span class="p_add">+	default:							\</span>
<span class="p_add">+		BUILD_BUG();						\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+	__ret;								\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define __cmpxchg_mb(ptr, old, new, size)			\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	__typeof__(*(ptr)) __ret;				\</span>
<span class="p_add">+	smp_mb();						\</span>
<span class="p_add">+	__ret = __cmpxchg((ptr), (old), (new), (size));		\</span>
<span class="p_add">+	smp_mb();						\</span>
<span class="p_add">+	__ret;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg_mb((ptr), (o), (n), sizeof(*(ptr))))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg_local(ptr, o, n) \</span>
<span class="p_add">+	(__cmpxchg((ptr), (o), (n), sizeof(*(ptr))))</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64(ptr, o, n)			\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define cmpxchg64_local(ptr, o, n)		\</span>
<span class="p_add">+({						\</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(*(ptr)) != 8);	\</span>
<span class="p_add">+	cmpxchg_local((ptr), (o), (n));		\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_RV_ATOMIC */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/cmpxchg.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_RV_ATOMIC */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CMPXCHG_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/csr.h b/arch/riscv/include/asm/csr.h</span>
new file mode 100644
<span class="p_header">index 000000000000..9df94cb0041a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/csr.h</span>
<span class="p_chunk">@@ -0,0 +1,126 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_CSR_H</span>
<span class="p_add">+#define _ASM_RISCV_CSR_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Status register flags */</span>
<span class="p_add">+#define SR_IE   _AC(0x00000002, UL) /* Interrupt Enable */</span>
<span class="p_add">+#define SR_PIE  _AC(0x00000020, UL) /* Previous IE */</span>
<span class="p_add">+#define SR_PS   _AC(0x00000100, UL) /* Previously Supervisor */</span>
<span class="p_add">+#define SR_SUM  _AC(0x00040000, UL) /* Supervisor may access User Memory */</span>
<span class="p_add">+</span>
<span class="p_add">+#define SR_FS           _AC(0x00006000, UL) /* Floating-point Status */</span>
<span class="p_add">+#define SR_FS_OFF       _AC(0x00000000, UL)</span>
<span class="p_add">+#define SR_FS_INITIAL   _AC(0x00002000, UL)</span>
<span class="p_add">+#define SR_FS_CLEAN     _AC(0x00004000, UL)</span>
<span class="p_add">+#define SR_FS_DIRTY     _AC(0x00006000, UL)</span>
<span class="p_add">+</span>
<span class="p_add">+#define SR_XS           _AC(0x00018000, UL) /* Extension Status */</span>
<span class="p_add">+#define SR_XS_OFF       _AC(0x00000000, UL)</span>
<span class="p_add">+#define SR_XS_INITIAL   _AC(0x00008000, UL)</span>
<span class="p_add">+#define SR_XS_CLEAN     _AC(0x00010000, UL)</span>
<span class="p_add">+#define SR_XS_DIRTY     _AC(0x00018000, UL)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_64BIT</span>
<span class="p_add">+#define SR_SD   _AC(0x80000000, UL) /* FS/XS dirty */</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define SR_SD   _AC(0x8000000000000000, UL) /* FS/XS dirty */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* SPTBR flags */</span>
<span class="p_add">+#if __riscv_xlen == 32</span>
<span class="p_add">+#define SPTBR_PPN     _AC(0x003FFFFF, UL)</span>
<span class="p_add">+#define SPTBR_MODE_32 _AC(0x80000000, UL)</span>
<span class="p_add">+#define SPTBR_MODE    SPTBR_MODE_32</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define SPTBR_PPN     _AC(0x00000FFFFFFFFFFF, UL)</span>
<span class="p_add">+#define SPTBR_MODE_39 _AC(0x8000000000000000, UL)</span>
<span class="p_add">+#define SPTBR_MODE    SPTBR_MODE_39</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/* Interrupt Enable and Interrupt Pending flags */</span>
<span class="p_add">+#define SIE_SSIE _AC(0x00000002, UL) /* Software Interrupt Enable */</span>
<span class="p_add">+#define SIE_STIE _AC(0x00000020, UL) /* Timer Interrupt Enable */</span>
<span class="p_add">+</span>
<span class="p_add">+#define EXC_INST_MISALIGNED     0</span>
<span class="p_add">+#define EXC_INST_ACCESS         1</span>
<span class="p_add">+#define EXC_BREAKPOINT          3</span>
<span class="p_add">+#define EXC_LOAD_ACCESS         5</span>
<span class="p_add">+#define EXC_STORE_ACCESS        7</span>
<span class="p_add">+#define EXC_SYSCALL             8</span>
<span class="p_add">+#define EXC_INST_PAGE_FAULT     12</span>
<span class="p_add">+#define EXC_LOAD_PAGE_FAULT     13</span>
<span class="p_add">+#define EXC_STORE_PAGE_FAULT    15</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_swap(csr, val)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrrw %0, &quot; #csr &quot;, %1&quot;		\</span>
<span class="p_add">+			      : &quot;=r&quot; (__v) : &quot;rK&quot; (__v));	\</span>
<span class="p_add">+	__v;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_read(csr)						\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	register unsigned long __v;				\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrr %0, &quot; #csr			\</span>
<span class="p_add">+			      : &quot;=r&quot; (__v));			\</span>
<span class="p_add">+	__v;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_write(csr, val)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrw &quot; #csr &quot;, %0&quot;		\</span>
<span class="p_add">+			      : : &quot;rK&quot; (__v));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_read_set(csr, val)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrrs %0, &quot; #csr &quot;, %1&quot;		\</span>
<span class="p_add">+			      : &quot;=r&quot; (__v) : &quot;rK&quot; (__v));	\</span>
<span class="p_add">+	__v;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_set(csr, val)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrs &quot; #csr &quot;, %0&quot;		\</span>
<span class="p_add">+			      : : &quot;rK&quot; (__v));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_read_clear(csr, val)				\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrrc %0, &quot; #csr &quot;, %1&quot;		\</span>
<span class="p_add">+			      : &quot;=r&quot; (__v) : &quot;rK&quot; (__v));	\</span>
<span class="p_add">+	__v;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#define csr_clear(csr, val)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	unsigned long __v = (unsigned long)(val);		\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrc &quot; #csr &quot;, %0&quot;		\</span>
<span class="p_add">+			      : : &quot;rK&quot; (__v));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_CSR_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/delay.h b/arch/riscv/include/asm/delay.h</span>
new file mode 100644
<span class="p_header">index 000000000000..18d399d3e689</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/delay.h</span>
<span class="p_chunk">@@ -0,0 +1,29 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2016 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_DELAY_H</span>
<span class="p_add">+#define _ASM_RISCV_DELAY_H</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long timebase;</span>
<span class="p_add">+</span>
<span class="p_add">+#define udelay udelay</span>
<span class="p_add">+extern void udelay(unsigned long usecs);</span>
<span class="p_add">+</span>
<span class="p_add">+#define ndelay ndelay</span>
<span class="p_add">+extern void ndelay(unsigned long nsecs);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __delay(unsigned long cycles);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_DELAY_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/device.h b/arch/riscv/include/asm/device.h</span>
new file mode 100644
<span class="p_header">index 000000000000..89e8ce022e37</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/device.h</span>
<span class="p_chunk">@@ -0,0 +1,28 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2016 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_DEVICE_H</span>
<span class="p_add">+#define _ASM_RISCV_DEVICE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/sysfs.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct dev_archdata {</span>
<span class="p_add">+	struct dma_map_ops *dma_ops;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct pdev_archdata {</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_DEVICE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/dma-mapping.h b/arch/riscv/include/asm/dma-mapping.h</span>
new file mode 100644
<span class="p_header">index 000000000000..f4d485780db3</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/dma-mapping.h</span>
<span class="p_chunk">@@ -0,0 +1,61 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2003-2004 Hewlett-Packard Co</span>
<span class="p_add">+ *	David Mosberger-Tang &lt;davidm@hpl.hp.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2016 SiFive, Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __ASM_RISCV_DMA_MAPPING_H</span>
<span class="p_add">+#define __ASM_RISCV_DMA_MAPPING_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+/* Use ops-&gt;dma_mapping_error (if it exists) or assume success */</span>
<span class="p_add">+// #undef DMA_ERROR_CODE</span>
<span class="p_add">+</span>
<span class="p_add">+static inline const struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;dma_noop_ops;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool dma_capable(struct device *dev, dma_addr_t addr, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!dev-&gt;dma_mask)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return addr + size - 1 &lt;= *dev-&gt;dma_mask;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline dma_addr_t phys_to_dma(struct device *dev, phys_addr_t paddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (dma_addr_t)paddr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline phys_addr_t dma_to_phys(struct device *dev, dma_addr_t dev_addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (phys_addr_t)dev_addr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void dma_cache_sync(struct device *dev, void *vaddr, size_t size, enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * RISC-V is cache-coherent, so this is mostly a no-op.</span>
<span class="p_add">+	 * However, we do need to ensure that dma_cache_sync()</span>
<span class="p_add">+	 * enforces order, hence the mb().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	mb();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif	/* __KERNEL__ */</span>
<span class="p_add">+#endif	/* __ASM_RISCV_DMA_MAPPING_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/elf.h b/arch/riscv/include/asm/elf.h</span>
new file mode 100644
<span class="p_header">index 000000000000..5ded3f6f83ea</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/elf.h</span>
<span class="p_chunk">@@ -0,0 +1,83 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2003 Matjaz Breskvar &lt;phoenix@bsemi.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2010-2011 Jonas Bonn &lt;jonas@southpole.se&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_ELF_H</span>
<span class="p_add">+#define _ASM_RISCV_ELF_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;uapi/asm/elf.h&gt;</span>
<span class="p_add">+#include &lt;asm/auxvec.h&gt;</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* TODO: Move definition into include/uapi/linux/elf-em.h */</span>
<span class="p_add">+#define EM_RISCV	0xF3</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These are used to set parameters in the core dumps.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ELF_ARCH	EM_RISCV</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define ELF_CLASS	ELFCLASS64</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define ELF_CLASS	ELFCLASS32</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(__LITTLE_ENDIAN)</span>
<span class="p_add">+#define ELF_DATA	ELFDATA2LSB</span>
<span class="p_add">+#elif defined(__BIG_ENDIAN)</span>
<span class="p_add">+#define ELF_DATA	ELFDATA2MSB</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unknown endianness&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is used to ensure we don&#39;t load something for the wrong architecture.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define elf_check_arch(x) ((x)-&gt;e_machine == EM_RISCV)</span>
<span class="p_add">+</span>
<span class="p_add">+#define CORE_DUMP_USE_REGSET</span>
<span class="p_add">+#define ELF_EXEC_PAGESIZE	(PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is the location that an ET_DYN program is loaded if exec&#39;ed.  Typical</span>
<span class="p_add">+ * use of this is to invoke &quot;./ld.so someprog&quot; to test out a new version of</span>
<span class="p_add">+ * the loader.  We need to make sure that it is out of the way of the program</span>
<span class="p_add">+ * that it will &quot;exec&quot;, and that there is sufficient room for the brk.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ELF_ET_DYN_BASE		((TASK_SIZE / 3) * 2)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This yields a mask that user programs can use to figure out what</span>
<span class="p_add">+ * instruction set this CPU supports.  This could be done in user space,</span>
<span class="p_add">+ * but it&#39;s not easy, and we&#39;ve already done it here.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ELF_HWCAP	(0)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This yields a string that ld.so will use to load implementation</span>
<span class="p_add">+ * specific libraries for optimization.  This is more specific in</span>
<span class="p_add">+ * intent than poking at uname or /proc/cpuinfo.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ELF_PLATFORM	(NULL)</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_DLINFO						\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	NEW_AUX_ENT(AT_SYSINFO_EHDR,				\</span>
<span class="p_add">+		(elf_addr_t)current-&gt;mm-&gt;context.vdso);		\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_HAS_SETUP_ADDITIONAL_PAGES</span>
<span class="p_add">+struct linux_binprm;</span>
<span class="p_add">+extern int arch_setup_additional_pages(struct linux_binprm *bprm,</span>
<span class="p_add">+	int uses_interp);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_ELF_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/io.h b/arch/riscv/include/asm/io.h</span>
new file mode 100644
<span class="p_header">index 000000000000..d942555a7a08</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/io.h</span>
<span class="p_chunk">@@ -0,0 +1,36 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2013 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_IO_H</span>
<span class="p_add">+#define _ASM_RISCV_IO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/io.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __iomem *ioremap(phys_addr_t offset, unsigned long size);</span>
<span class="p_add">+</span>
<span class="p_add">+#define ioremap_nocache(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wc(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+#define ioremap_wt(addr, size) ioremap((addr), (size))</span>
<span class="p_add">+</span>
<span class="p_add">+extern void iounmap(void __iomem *addr);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_IO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/irq.h b/arch/riscv/include/asm/irq.h</span>
new file mode 100644
<span class="p_header">index 000000000000..ce024e60f585</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/irq.h</span>
<span class="p_chunk">@@ -0,0 +1,32 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_IRQ_H</span>
<span class="p_add">+#define _ASM_RISCV_IRQ_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define NR_IRQS         0</span>
<span class="p_add">+</span>
<span class="p_add">+#define INTERRUPT_CAUSE_SOFTWARE    1</span>
<span class="p_add">+#define INTERRUPT_CAUSE_TIMER       5</span>
<span class="p_add">+#define INTERRUPT_CAUSE_EXTERNAL    9</span>
<span class="p_add">+</span>
<span class="p_add">+void riscv_timer_interrupt(void);</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/irq.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* The value of csr sie before init_traps runs (core is up) */</span>
<span class="p_add">+DECLARE_PER_CPU(atomic_long_t, riscv_early_sie);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_IRQ_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/irqflags.h b/arch/riscv/include/asm/irqflags.h</span>
new file mode 100644
<span class="p_header">index 000000000000..f86d8173a2de</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/irqflags.h</span>
<span class="p_chunk">@@ -0,0 +1,64 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_IRQFLAGS_H</span>
<span class="p_add">+#define _ASM_RISCV_IRQFLAGS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/processor.h&gt;</span>
<span class="p_add">+#include &lt;asm/csr.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* read interrupt enabled status */</span>
<span class="p_add">+static inline unsigned long arch_local_save_flags(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return csr_read(sstatus);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* unconditionally enable interrupts */</span>
<span class="p_add">+static inline void arch_local_irq_enable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	csr_set(sstatus, SR_IE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* unconditionally disable interrupts */</span>
<span class="p_add">+static inline void arch_local_irq_disable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	csr_clear(sstatus, SR_IE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* get status and disable interrupts */</span>
<span class="p_add">+static inline unsigned long arch_local_irq_save(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return csr_read_clear(sstatus, SR_IE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* test flags */</span>
<span class="p_add">+static inline int arch_irqs_disabled_flags(unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !(flags &amp; SR_IE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* test hardware interrupt enable bit */</span>
<span class="p_add">+static inline int arch_irqs_disabled(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return arch_irqs_disabled_flags(arch_local_save_flags());</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* set interrupt enabled status */</span>
<span class="p_add">+static inline void arch_local_irq_restore(unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	csr_set(sstatus, flags &amp; SR_IE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_IRQFLAGS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/kprobes.h b/arch/riscv/include/asm/kprobes.h</span>
new file mode 100644
<span class="p_header">index 000000000000..61a03e7c1975</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/kprobes.h</span>
<span class="p_chunk">@@ -0,0 +1,23 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ASM_RISCV_KPROBES_H</span>
<span class="p_add">+#define ASM_RISCV_KPROBES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_KPROBES</span>
<span class="p_add">+#error &quot;RISC-V doesn&#39;t skpport CONFIG_KPROBES&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/linkage.h b/arch/riscv/include/asm/linkage.h</span>
new file mode 100644
<span class="p_header">index 000000000000..6d4d4f6b6951</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/linkage.h</span>
<span class="p_chunk">@@ -0,0 +1,21 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_LINKAGE_H</span>
<span class="p_add">+#define _ASM_RISCV_LINKAGE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ALIGN		.balign 4</span>
<span class="p_add">+#define __ALIGN_STR	&quot;.balign 4&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_LINKAGE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/mmu.h b/arch/riscv/include/asm/mmu.h</span>
new file mode 100644
<span class="p_header">index 000000000000..9eeee484fd81</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/mmu.h</span>
<span class="p_chunk">@@ -0,0 +1,27 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_MMU_H</span>
<span class="p_add">+#define _ASM_RISCV_MMU_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	void *vdso;</span>
<span class="p_add">+} mm_context_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_MMU_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/mmu_context.h b/arch/riscv/include/asm/mmu_context.h</span>
new file mode 100644
<span class="p_header">index 000000000000..44053fefa533</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -0,0 +1,70 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_MMU_CONTEXT_H</span>
<span class="p_add">+#define _ASM_RISCV_MMU_CONTEXT_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/mm_hooks.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void enter_lazy_tlb(struct mm_struct *mm,</span>
<span class="p_add">+	struct task_struct *task)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Initialize context-related info for a new mm_struct */</span>
<span class="p_add">+static inline int init_new_context(struct task_struct *task,</span>
<span class="p_add">+	struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void destroy_context(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *current_pgdir(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_to_virt(csr_read(sptbr) &amp; SPTBR_PPN);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void set_pgdir(pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	csr_write(sptbr, virt_to_pfn(pgd) | SPTBR_MODE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void switch_mm(struct mm_struct *prev,</span>
<span class="p_add">+	struct mm_struct *next, struct task_struct *task)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (likely(prev != next)) {</span>
<span class="p_add">+		set_pgdir(next-&gt;pgd);</span>
<span class="p_add">+		local_flush_tlb_all();</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void activate_mm(struct mm_struct *prev,</span>
<span class="p_add">+			       struct mm_struct *next)</span>
<span class="p_add">+{</span>
<span class="p_add">+	switch_mm(prev, next, NULL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void deactivate_mm(struct task_struct *task,</span>
<span class="p_add">+	struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_MMU_CONTEXT_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/page.h b/arch/riscv/include/asm/page.h</span>
new file mode 100644
<span class="p_header">index 000000000000..1cde0b295d25</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/page.h</span>
<span class="p_chunk">@@ -0,0 +1,138 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ * Copyright (C) 2017 SiFive</span>
<span class="p_add">+ * Copyright (C) 2017 XiaojingZhu &lt;zhuxiaoj@ict.ac.cn&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PAGE_H</span>
<span class="p_add">+#define _ASM_RISCV_PAGE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/pfn.h&gt;</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_SHIFT	(12)</span>
<span class="p_add">+#define PAGE_SIZE	(_AC(1, UL) &lt;&lt; PAGE_SHIFT)</span>
<span class="p_add">+#define PAGE_MASK	(~(PAGE_SIZE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PAGE_OFFSET -- the first address of the first page of memory.</span>
<span class="p_add">+ * When not using MMU this corresponds to the first free page in</span>
<span class="p_add">+ * physical memory (aligned on a page boundary).</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define PAGE_OFFSET		_AC(0xffffffff80000000, UL)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define PAGE_OFFSET		_AC(0xc0000000, UL)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define KERN_VIRT_SIZE (-PAGE_OFFSET)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_UP(addr)	(((addr)+((PAGE_SIZE)-1))&amp;(~((PAGE_SIZE)-1)))</span>
<span class="p_add">+#define PAGE_DOWN(addr)	((addr)&amp;(~((PAGE_SIZE)-1)))</span>
<span class="p_add">+</span>
<span class="p_add">+/* align addr on a size boundary - adjust address up/down if needed */</span>
<span class="p_add">+#define _ALIGN_UP(addr, size)	(((addr)+((size)-1))&amp;(~((size)-1)))</span>
<span class="p_add">+#define _ALIGN_DOWN(addr, size)	((addr)&amp;(~((size)-1)))</span>
<span class="p_add">+</span>
<span class="p_add">+/* align addr on a size boundary - adjust address up if needed */</span>
<span class="p_add">+#define _ALIGN(addr, size)	_ALIGN_UP(addr, size)</span>
<span class="p_add">+</span>
<span class="p_add">+#define clear_page(pgaddr)			memset((pgaddr), 0, PAGE_SIZE)</span>
<span class="p_add">+#define copy_page(to, from)			memcpy((to), (from), PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define clear_user_page(pgaddr, vaddr, page)	memset((pgaddr), 0, PAGE_SIZE)</span>
<span class="p_add">+#define copy_user_page(vto, vfrom, vaddr, topg) \</span>
<span class="p_add">+			memcpy((vto), (vfrom), PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Use struct definitions to apply C type checking</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Page Global Directory entry */</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	unsigned long pgd;</span>
<span class="p_add">+} pgd_t;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Page Table entry */</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	unsigned long pte;</span>
<span class="p_add">+} pte_t;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	unsigned long pgprot;</span>
<span class="p_add">+} pgprot_t;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct page *pgtable_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define pte_val(x)	((x).pte)</span>
<span class="p_add">+#define pgd_val(x)	((x).pgd)</span>
<span class="p_add">+#define pgprot_val(x)	((x).pgprot)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pte(x)	((pte_t) { (x) })</span>
<span class="p_add">+#define __pgd(x)	((pgd_t) { (x) })</span>
<span class="p_add">+#define __pgprot(x)	((pgprot_t) { (x) })</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BITS</span>
<span class="p_add">+#define PTE_FMT &quot;%016lx&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define PTE_FMT &quot;%08lx&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long va_pa_offset;</span>
<span class="p_add">+extern unsigned long pfn_base;</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long max_low_pfn;</span>
<span class="p_add">+extern unsigned long min_low_pfn;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pa(x)		((unsigned long)(x) - va_pa_offset)</span>
<span class="p_add">+#define __va(x)		((void *)((unsigned long) (x) + va_pa_offset))</span>
<span class="p_add">+</span>
<span class="p_add">+#define phys_to_pfn(phys)	(PFN_DOWN(phys))</span>
<span class="p_add">+#define pfn_to_phys(pfn)	(PFN_PHYS(pfn))</span>
<span class="p_add">+</span>
<span class="p_add">+#define virt_to_pfn(vaddr)	(phys_to_pfn(__pa(vaddr)))</span>
<span class="p_add">+#define pfn_to_virt(pfn)	(__va(pfn_to_phys(pfn)))</span>
<span class="p_add">+</span>
<span class="p_add">+#define virt_to_page(vaddr)	(pfn_to_page(virt_to_pfn(vaddr)))</span>
<span class="p_add">+#define page_to_virt(page)	(pfn_to_virt(page_to_pfn(page)))</span>
<span class="p_add">+</span>
<span class="p_add">+#define page_to_phys(page)	(pfn_to_phys(page_to_pfn(page)))</span>
<span class="p_add">+#define page_to_bus(page)	(page_to_phys(page))</span>
<span class="p_add">+#define phys_to_page(paddr)	(pfn_to_page(phys_to_pfn(paddr)))</span>
<span class="p_add">+</span>
<span class="p_add">+#define pfn_valid(pfn)		(((pfn) &gt;= pfn_base) &amp;&amp; (((pfn)-pfn_base) &lt; max_mapnr))</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_PFN_OFFSET		(pfn_base)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define virt_addr_valid(vaddr)	(pfn_valid(virt_to_pfn(vaddr)))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define VM_DATA_DEFAULT_FLAGS	(VM_READ | VM_WRITE | \</span>
<span class="p_add">+				 VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC)</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/memory_model.h&gt;</span>
<span class="p_add">+#include &lt;asm-generic/getorder.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* vDSO support */</span>
<span class="p_add">+/* We do define AT_SYSINFO_EHDR but don&#39;t use the gate mechanism */</span>
<span class="p_add">+#define __HAVE_ARCH_GATE_AREA</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PAGE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pci.h b/arch/riscv/include/asm/pci.h</span>
new file mode 100644
<span class="p_header">index 000000000000..ad46530f5faa</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pci.h</span>
<span class="p_chunk">@@ -0,0 +1,51 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2016 SiFive</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASM_RISCV_PCI_H</span>
<span class="p_add">+#define __ASM_RISCV_PCI_H</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/dma-mapping.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/io.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define PCIBIOS_MIN_IO		0x1000</span>
<span class="p_add">+#define PCIBIOS_MIN_MEM		0</span>
<span class="p_add">+</span>
<span class="p_add">+/* RISC-V shim does not initialize PCI bus */</span>
<span class="p_add">+#define pcibios_assign_all_busses() 1</span>
<span class="p_add">+</span>
<span class="p_add">+/* RISC-V TileLink and PCIe share the share address space */</span>
<span class="p_add">+#define PCI_DMA_BUS_IS_PHYS 1</span>
<span class="p_add">+</span>
<span class="p_add">+extern int isa_dma_bridge_buggy;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PCI</span>
<span class="p_add">+static inline int pci_get_legacy_ide_irq(struct pci_dev *dev, int channel)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* no legacy IRQ on risc-v */</span>
<span class="p_add">+	return -ENODEV;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pci_proc_domain(struct pci_bus *bus)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* always show the domain in /proc */</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif  /* CONFIG_PCI */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif  /* __KERNEL__ */</span>
<span class="p_add">+#endif  /* __ASM_PCI_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pgalloc.h b/arch/riscv/include/asm/pgalloc.h</span>
new file mode 100644
<span class="p_header">index 000000000000..fde046080e5d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -0,0 +1,126 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PGALLOC_H</span>
<span class="p_add">+#define _ASM_RISCV_PGALLOC_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_populate_kernel(struct mm_struct *mm,</span>
<span class="p_add">+	pmd_t *pmd, pte_t *pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long pfn = virt_to_pfn(pte);</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pmd(pmd, __pmd((pfn &lt;&lt; _PAGE_PFN_SHIFT) | _PAGE_TABLE));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_populate(struct mm_struct *mm,</span>
<span class="p_add">+	pmd_t *pmd, pgtable_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long pfn = virt_to_pfn(page_address(pte));</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pmd(pmd, __pmd((pfn &lt;&lt; _PAGE_PFN_SHIFT) | _PAGE_TABLE));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __PAGETABLE_PMD_FOLDED</span>
<span class="p_add">+static inline void pud_populate(struct mm_struct *mm, pud_t *pud, pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long pfn = virt_to_pfn(pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+	set_pud(pud, __pud((pfn &lt;&lt; _PAGE_PFN_SHIFT) | _PAGE_TABLE));</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* __PAGETABLE_PMD_FOLDED */</span>
<span class="p_add">+</span>
<span class="p_add">+#define pmd_pgtable(pmd)	pmd_page(pmd)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *pgd_alloc(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd = (pgd_t *)__get_free_page(GFP_KERNEL);</span>
<span class="p_add">+	if (likely(pgd != NULL)) {</span>
<span class="p_add">+		memset(pgd, 0, USER_PTRS_PER_PGD * sizeof(pgd_t));</span>
<span class="p_add">+		/* Copy kernel mappings */</span>
<span class="p_add">+		memcpy(pgd + USER_PTRS_PER_PGD,</span>
<span class="p_add">+			init_mm.pgd + USER_PTRS_PER_PGD,</span>
<span class="p_add">+			(PTRS_PER_PGD - USER_PTRS_PER_PGD) * sizeof(pgd_t));</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pgd_free(struct mm_struct *mm, pgd_t *pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_page((unsigned long)pgd);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __PAGETABLE_PMD_FOLDED</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t *pmd_alloc_one(struct mm_struct *mm, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pmd_t *)__get_free_page(</span>
<span class="p_add">+		GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_free(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_page((unsigned long)pmd);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pmd_free_tlb(tlb, pmd, addr)  pmd_free((tlb)-&gt;mm, pmd)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __PAGETABLE_PMD_FOLDED */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t *pte_alloc_one_kernel(struct mm_struct *mm,</span>
<span class="p_add">+	unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pte_t *)__get_free_page(</span>
<span class="p_add">+		GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *pte_alloc_one(struct mm_struct *mm,</span>
<span class="p_add">+	unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = alloc_page(GFP_KERNEL | __GFP_REPEAT | __GFP_ZERO);</span>
<span class="p_add">+	if (likely(pte != NULL)) {</span>
<span class="p_add">+		pgtable_page_ctor(pte);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pte_free_kernel(struct mm_struct *mm, pte_t *pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_page((unsigned long)pte);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pte_free(struct mm_struct *mm, pgtable_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgtable_page_dtor(pte);</span>
<span class="p_add">+	__free_page(pte);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pte_free_tlb(tlb, pte, buf)   \</span>
<span class="p_add">+do {                                    \</span>
<span class="p_add">+	pgtable_page_dtor(pte);         \</span>
<span class="p_add">+	tlb_remove_page((tlb), pte);    \</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void check_pgt_cache(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PGALLOC_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pgtable-32.h b/arch/riscv/include/asm/pgtable-32.h</span>
new file mode 100644
<span class="p_header">index 000000000000..ae6ad80f80a5</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pgtable-32.h</span>
<span class="p_chunk">@@ -0,0 +1,26 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PGTABLE_32_H</span>
<span class="p_add">+#define _ASM_RISCV_PGTABLE_32_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/pgtable-nopmd.h&gt;</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Size of region mapped by a page global directory */</span>
<span class="p_add">+#define PGDIR_SHIFT     22</span>
<span class="p_add">+#define PGDIR_SIZE      (_AC(1, UL) &lt;&lt; PGDIR_SHIFT)</span>
<span class="p_add">+#define PGDIR_MASK      (~(PGDIR_SIZE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PGTABLE_32_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pgtable-64.h b/arch/riscv/include/asm/pgtable-64.h</span>
new file mode 100644
<span class="p_header">index 000000000000..f4f6dd1690f1</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pgtable-64.h</span>
<span class="p_chunk">@@ -0,0 +1,85 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PGTABLE_64_H</span>
<span class="p_add">+#define _ASM_RISCV_PGTABLE_64_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define PGDIR_SHIFT     30</span>
<span class="p_add">+/* Size of region mapped by a page global directory */</span>
<span class="p_add">+#define PGDIR_SIZE      (_AC(1, UL) &lt;&lt; PGDIR_SHIFT)</span>
<span class="p_add">+#define PGDIR_MASK      (~(PGDIR_SIZE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_SHIFT       21</span>
<span class="p_add">+/* Size of region mapped by a page middle directory */</span>
<span class="p_add">+#define PMD_SIZE        (_AC(1, UL) &lt;&lt; PMD_SHIFT)</span>
<span class="p_add">+#define PMD_MASK        (~(PMD_SIZE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Page Middle Directory entry */</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	unsigned long pmd;</span>
<span class="p_add">+} pmd_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define pmd_val(x)      ((x).pmd)</span>
<span class="p_add">+#define __pmd(x)        ((pmd_t) { (x) })</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTRS_PER_PMD    (PAGE_SIZE / sizeof(pmd_t))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pud_present(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pud_val(pud) &amp; _PAGE_PRESENT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pud_none(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pud_val(pud) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pud_bad(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !pud_present(pud);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void set_pud(pud_t *pudp, pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*pudp = pud;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pud_clear(pud_t *pudp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	set_pud(pudp, __pud(0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long pud_page_vaddr(pud_t pud)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (unsigned long)pfn_to_virt(pud_val(pud) &gt;&gt; _PAGE_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pmd_index(addr) (((addr) &gt;&gt; PMD_SHIFT) &amp; (PTRS_PER_PMD - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t *pmd_offset(pud_t *pud, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pmd_t *)pud_page_vaddr(*pud) + pmd_index(addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t pfn_pmd(unsigned long pfn, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pmd((pfn &lt;&lt; _PAGE_PFN_SHIFT) | pgprot_val(prot));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pmd_ERROR(e) \</span>
<span class="p_add">+	pr_err(&quot;%s:%d: bad pmd %016lx.\n&quot;, __FILE__, __LINE__, pmd_val(e))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PGTABLE_64_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pgtable-bits.h b/arch/riscv/include/asm/pgtable-bits.h</span>
new file mode 100644
<span class="p_header">index 000000000000..79eb880a1749</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pgtable-bits.h</span>
<span class="p_chunk">@@ -0,0 +1,49 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PGTABLE_BITS_H</span>
<span class="p_add">+#define _ASM_RISCV_PGTABLE_BITS_H</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PTE format:</span>
<span class="p_add">+ * | XLEN-1  10 | 9             8 | 7 | 6 | 5 | 4 | 3 | 2 | 1 | 0</span>
<span class="p_add">+ *       PFN      reserved for SW   D   A   G   U   X   W   R   V</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_ACCESSED_OFFSET 6</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_PRESENT   (1 &lt;&lt; 0)</span>
<span class="p_add">+#define _PAGE_READ      (1 &lt;&lt; 1)    /* Readable */</span>
<span class="p_add">+#define _PAGE_WRITE     (1 &lt;&lt; 2)    /* Writable */</span>
<span class="p_add">+#define _PAGE_EXEC      (1 &lt;&lt; 3)    /* Executable */</span>
<span class="p_add">+#define _PAGE_USER      (1 &lt;&lt; 4)    /* User */</span>
<span class="p_add">+#define _PAGE_GLOBAL    (1 &lt;&lt; 5)    /* Global */</span>
<span class="p_add">+#define _PAGE_ACCESSED  (1 &lt;&lt; _PAGE_ACCESSED_OFFSET)  /* Set by hardware on any access */</span>
<span class="p_add">+#define _PAGE_DIRTY     (1 &lt;&lt; 7)    /* Set by hardware on any write */</span>
<span class="p_add">+#define _PAGE_SOFT      (1 &lt;&lt; 8)    /* Reserved for software */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_SPECIAL   _PAGE_SOFT</span>
<span class="p_add">+#define _PAGE_TABLE     _PAGE_PRESENT</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_PFN_SHIFT 10</span>
<span class="p_add">+</span>
<span class="p_add">+/* Set of bits to preserve across pte_modify() */</span>
<span class="p_add">+#define _PAGE_CHG_MASK  (~(unsigned long)(_PAGE_PRESENT | _PAGE_READ |	\</span>
<span class="p_add">+					  _PAGE_WRITE | _PAGE_EXEC |	\</span>
<span class="p_add">+					  _PAGE_USER | _PAGE_GLOBAL))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Advertise support for _PAGE_SPECIAL */</span>
<span class="p_add">+#define __HAVE_ARCH_PTE_SPECIAL</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PGTABLE_BITS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/pgtable.h b/arch/riscv/include/asm/pgtable.h</span>
new file mode 100644
<span class="p_header">index 000000000000..8d88c80b0c5e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -0,0 +1,426 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PGTABLE_H</span>
<span class="p_add">+#define _ASM_RISCV_PGTABLE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mmzone.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/pgtable-bits.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+/* Page Upper Directory not used in RISC-V */</span>
<span class="p_add">+#include &lt;asm-generic/pgtable-nopud.h&gt;</span>
<span class="p_add">+#include &lt;asm/page.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm_types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#include &lt;asm/pgtable-64.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#include &lt;asm/pgtable-32.h&gt;</span>
<span class="p_add">+#endif /* CONFIG_64BIT */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Number of entries in the page global directory */</span>
<span class="p_add">+#define PTRS_PER_PGD    (PAGE_SIZE / sizeof(pgd_t))</span>
<span class="p_add">+/* Number of entries in the page table */</span>
<span class="p_add">+#define PTRS_PER_PTE    (PAGE_SIZE / sizeof(pte_t))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Number of PGD entries that a user-mode program can use */</span>
<span class="p_add">+#define USER_PTRS_PER_PGD   (TASK_SIZE / PGDIR_SIZE)</span>
<span class="p_add">+#define FIRST_USER_ADDRESS  0</span>
<span class="p_add">+</span>
<span class="p_add">+/* Page protection bits */</span>
<span class="p_add">+#define _PAGE_BASE	(_PAGE_PRESENT | _PAGE_ACCESSED | _PAGE_USER)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_NONE		__pgprot(0)</span>
<span class="p_add">+#define PAGE_READ		__pgprot(_PAGE_BASE | _PAGE_READ)</span>
<span class="p_add">+#define PAGE_WRITE		__pgprot(_PAGE_BASE | _PAGE_READ | _PAGE_WRITE)</span>
<span class="p_add">+#define PAGE_EXEC		__pgprot(_PAGE_BASE | _PAGE_EXEC)</span>
<span class="p_add">+#define PAGE_READ_EXEC		__pgprot(_PAGE_BASE | _PAGE_READ | _PAGE_EXEC)</span>
<span class="p_add">+#define PAGE_WRITE_EXEC		__pgprot(_PAGE_BASE | _PAGE_READ |	\</span>
<span class="p_add">+					 _PAGE_EXEC | _PAGE_WRITE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_COPY		PAGE_READ</span>
<span class="p_add">+#define PAGE_COPY_EXEC		PAGE_EXEC</span>
<span class="p_add">+#define PAGE_COPY_READ_EXEC	PAGE_READ_EXEC</span>
<span class="p_add">+#define PAGE_SHARED		PAGE_WRITE</span>
<span class="p_add">+#define PAGE_SHARED_EXEC	PAGE_WRITE_EXEC</span>
<span class="p_add">+</span>
<span class="p_add">+#define _PAGE_KERNEL		_PAGE_READ \</span>
<span class="p_add">+				| _PAGE_WRITE \</span>
<span class="p_add">+				| _PAGE_PRESENT \</span>
<span class="p_add">+				| _PAGE_ACCESSED \</span>
<span class="p_add">+				| _PAGE_DIRTY</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_KERNEL		__pgprot(_PAGE_KERNEL)</span>
<span class="p_add">+#define PAGE_KERNEL_EXEC	__pgprot(_PAGE_KERNEL | _PAGE_EXEC)</span>
<span class="p_add">+</span>
<span class="p_add">+extern pgd_t swapper_pg_dir[];</span>
<span class="p_add">+</span>
<span class="p_add">+/* MAP_PRIVATE permissions: xwr (copy-on-write) */</span>
<span class="p_add">+#define __P000	PAGE_NONE</span>
<span class="p_add">+#define __P001	PAGE_READ</span>
<span class="p_add">+#define __P010	PAGE_COPY</span>
<span class="p_add">+#define __P011	PAGE_COPY</span>
<span class="p_add">+#define __P100	PAGE_EXEC</span>
<span class="p_add">+#define __P101	PAGE_READ_EXEC</span>
<span class="p_add">+#define __P110	PAGE_COPY_EXEC</span>
<span class="p_add">+#define __P111	PAGE_COPY_READ_EXEC</span>
<span class="p_add">+</span>
<span class="p_add">+/* MAP_SHARED permissions: xwr */</span>
<span class="p_add">+#define __S000	PAGE_NONE</span>
<span class="p_add">+#define __S001	PAGE_READ</span>
<span class="p_add">+#define __S010	PAGE_SHARED</span>
<span class="p_add">+#define __S011	PAGE_SHARED</span>
<span class="p_add">+#define __S100	PAGE_EXEC</span>
<span class="p_add">+#define __S101	PAGE_READ_EXEC</span>
<span class="p_add">+#define __S110	PAGE_SHARED_EXEC</span>
<span class="p_add">+#define __S111	PAGE_SHARED_EXEC</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * ZERO_PAGE is a global shared page that is always zero,</span>
<span class="p_add">+ * used for zero-mapped memory areas, etc.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern unsigned long empty_zero_page[PAGE_SIZE / sizeof(unsigned long)];</span>
<span class="p_add">+#define ZERO_PAGE(vaddr) (virt_to_page(empty_zero_page))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pmd_present(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pmd_val(pmd) &amp; _PAGE_PRESENT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pmd_none(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pmd_val(pmd) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pmd_bad(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !pmd_present(pmd);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void set_pmd(pmd_t *pmdp, pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*pmdp = pmd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_clear(pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	set_pmd(pmdp, __pmd(0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t pfn_pgd(unsigned long pfn, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pgd((pfn &lt;&lt; _PAGE_PFN_SHIFT) | pgprot_val(prot));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pgd_index(addr) (((addr) &gt;&gt; PGDIR_SHIFT) &amp; (PTRS_PER_PGD - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Locate an entry in the page global directory */</span>
<span class="p_add">+static inline pgd_t *pgd_offset(const struct mm_struct *mm, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return mm-&gt;pgd + pgd_index(addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+/* Locate an entry in the kernel page global directory */</span>
<span class="p_add">+#define pgd_offset_k(addr)      pgd_offset(&amp;init_mm, (addr))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *pmd_page(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_to_page(pmd_val(pmd) &gt;&gt; _PAGE_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long pmd_page_vaddr(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (unsigned long)pfn_to_virt(pmd_val(pmd) &gt;&gt; _PAGE_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Yields the page frame number (PFN) of a page table entry */</span>
<span class="p_add">+static inline unsigned long pte_pfn(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pte_val(pte) &gt;&gt; _PAGE_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pte_page(x)     pfn_to_page(pte_pfn(x))</span>
<span class="p_add">+</span>
<span class="p_add">+/* Constructs a page table entry */</span>
<span class="p_add">+static inline pte_t pfn_pte(unsigned long pfn, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte((pfn &lt;&lt; _PAGE_PFN_SHIFT) | pgprot_val(prot));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t mk_pte(struct page *page, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_pte(page_to_pfn(page), prot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pte_index(addr) (((addr) &gt;&gt; PAGE_SHIFT) &amp; (PTRS_PER_PTE - 1))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t *pte_offset_kernel(pmd_t *pmd, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pte_t *)pmd_page_vaddr(*pmd) + pte_index(addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pte_offset_map(dir, addr)	pte_offset_kernel((dir), (addr))</span>
<span class="p_add">+#define pte_unmap(pte)			((void)(pte))</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Certain architectures need to do special things when PTEs within</span>
<span class="p_add">+ * a page table are directly modified.  Thus, the following hook is</span>
<span class="p_add">+ * made available.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void set_pte(pte_t *ptep, pte_t pteval)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*ptep = pteval;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void set_pte_at(struct mm_struct *mm,</span>
<span class="p_add">+	unsigned long addr, pte_t *ptep, pte_t pteval)</span>
<span class="p_add">+{</span>
<span class="p_add">+	set_pte(ptep, pteval);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pte_clear(struct mm_struct *mm,</span>
<span class="p_add">+	unsigned long addr, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	set_pte_at(mm, addr, ptep, __pte(0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_present(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pte_val(pte) &amp; _PAGE_PRESENT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_none(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pte_val(pte) == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* static inline int pte_read(pte_t pte) */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_write(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_val(pte) &amp; _PAGE_WRITE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_huge(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_present(pte)</span>
<span class="p_add">+		&amp;&amp; (pte_val(pte) &amp; (_PAGE_READ | _PAGE_WRITE | _PAGE_EXEC));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* static inline int pte_exec(pte_t pte) */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_dirty(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_val(pte) &amp; _PAGE_DIRTY;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_young(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_val(pte) &amp; _PAGE_ACCESSED;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int pte_special(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_val(pte) &amp; _PAGE_SPECIAL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* static inline pte_t pte_rdprotect(pte_t pte) */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_wrprotect(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) &amp; ~(_PAGE_WRITE));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* static inline pte_t pte_mkread(pte_t pte) */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkwrite(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) | _PAGE_WRITE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* static inline pte_t pte_mkexec(pte_t pte) */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkdirty(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) | _PAGE_DIRTY);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkclean(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) &amp; ~(_PAGE_DIRTY));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkyoung(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) | _PAGE_ACCESSED);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkold(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) &amp; ~(_PAGE_ACCESSED));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pte_t pte_mkspecial(pte_t pte)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(pte_val(pte) | _PAGE_SPECIAL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Modify page protection bits */</span>
<span class="p_add">+static inline pte_t pte_modify(pte_t pte, pgprot_t newprot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte((pte_val(pte) &amp; _PAGE_CHG_MASK) | pgprot_val(newprot));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define pgd_ERROR(e) \</span>
<span class="p_add">+	pr_err(&quot;%s:%d: bad pgd &quot; PTE_FMT &quot;.\n&quot;, __FILE__, __LINE__, pgd_val(e))</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Commit new configuration to MMU hardware */</span>
<span class="p_add">+static inline void update_mmu_cache(struct vm_area_struct *vma,</span>
<span class="p_add">+	unsigned long address, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* The kernel assumes that TLBs don&#39;t cache invalid entries, but</span>
<span class="p_add">+	 * in RISC-V, SFENCE.VMA specifies an ordering constraint, not a</span>
<span class="p_add">+	 * cache flush; it is necessary even after writing invalid entries.</span>
<span class="p_add">+	 * Relying on flush_tlb_fix_spurious_fault would suffice, but</span>
<span class="p_add">+	 * the extra traps reduce performance.  So, eagerly SFENCE.VMA. */</span>
<span class="p_add">+	local_flush_tlb_page(address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTE_SAME</span>
<span class="p_add">+static inline int pte_same(pte_t pte_a, pte_t pte_b)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pte_val(pte_a) == pte_val(pte_b);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_SET_ACCESS_FLAGS</span>
<span class="p_add">+static inline int ptep_set_access_flags(struct vm_area_struct *vma,</span>
<span class="p_add">+					unsigned long address, pte_t *ptep,</span>
<span class="p_add">+					pte_t entry, int dirty)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!pte_same(*ptep, entry))</span>
<span class="p_add">+		set_pte_at(vma-&gt;vm_mm, address, ptep, entry);</span>
<span class="p_add">+	/* update_mmu_cache will unconditionally execute, handling both</span>
<span class="p_add">+	 * the case that the PTE changed and the spurious fault case. */</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_GET_AND_CLEAR</span>
<span class="p_add">+static inline pte_t ptep_get_and_clear(struct mm_struct *mm,</span>
<span class="p_add">+				       unsigned long address, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __pte(atomic_long_xchg((atomic_long_t *)ptep, 0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_TEST_AND_CLEAR_YOUNG</span>
<span class="p_add">+static inline int ptep_test_and_clear_young(struct vm_area_struct *vma,</span>
<span class="p_add">+					    unsigned long address,</span>
<span class="p_add">+					    pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!pte_young(*ptep))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	return test_and_clear_bit(_PAGE_ACCESSED_OFFSET, &amp;pte_val(*ptep));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_SET_WRPROTECT</span>
<span class="p_add">+static inline void ptep_set_wrprotect(struct mm_struct *mm,</span>
<span class="p_add">+				      unsigned long address, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	atomic_long_and(~(unsigned long)_PAGE_WRITE, (atomic_long_t *)ptep);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_CLEAR_YOUNG_FLUSH</span>
<span class="p_add">+static inline int ptep_clear_flush_young(struct vm_area_struct *vma,</span>
<span class="p_add">+					 unsigned long address, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This comment is borrowed from x86, but applies equally to RISC-V:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Clearing the accessed bit without a TLB flush</span>
<span class="p_add">+	 * doesn&#39;t cause data corruption. [ It could cause incorrect</span>
<span class="p_add">+	 * page aging and the (mistaken) reclaim of hot pages, but the</span>
<span class="p_add">+	 * chance of that should be relatively low. ]</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * So as a performance optimization don&#39;t flush the TLB when</span>
<span class="p_add">+	 * clearing the accessed bit, it will eventually be flushed by</span>
<span class="p_add">+	 * a context switch or a VM operation anyway. [ In the rare</span>
<span class="p_add">+	 * event of it not getting flushed for a long time the delay</span>
<span class="p_add">+	 * shouldn&#39;t really matter because there&#39;s no real memory</span>
<span class="p_add">+	 * pressure for swapout to react to. ]</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return ptep_test_and_clear_young(vma, address, ptep);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Encode and decode a swap entry</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Format of swap PTE:</span>
<span class="p_add">+ *	bit            0:	_PAGE_PRESENT (zero)</span>
<span class="p_add">+ *	bit            1:	reserved for future use (zero)</span>
<span class="p_add">+ *	bits      2 to 6:	swap type</span>
<span class="p_add">+ *	bits 7 to XLEN-1:	swap offset</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __SWP_TYPE_SHIFT	2</span>
<span class="p_add">+#define __SWP_TYPE_BITS		5</span>
<span class="p_add">+#define __SWP_TYPE_MASK		((1UL &lt;&lt; __SWP_TYPE_BITS) - 1)</span>
<span class="p_add">+#define __SWP_OFFSET_SHIFT	(__SWP_TYPE_BITS + __SWP_TYPE_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+#define MAX_SWAPFILES_CHECK()	\</span>
<span class="p_add">+	BUILD_BUG_ON(MAX_SWAPFILES_SHIFT &gt; __SWP_TYPE_BITS)</span>
<span class="p_add">+</span>
<span class="p_add">+#define __swp_type(x)		(((x).val &gt;&gt; __SWP_TYPE_SHIFT) &amp; __SWP_TYPE_MASK)</span>
<span class="p_add">+#define __swp_offset(x)		((x).val &gt;&gt; __SWP_OFFSET_SHIFT)</span>
<span class="p_add">+#define __swp_entry(type, offset) ((swp_entry_t) \</span>
<span class="p_add">+	{ ((type) &lt;&lt; __SWP_TYPE_SHIFT) | ((offset) &lt;&lt; __SWP_OFFSET_SHIFT) })</span>
<span class="p_add">+</span>
<span class="p_add">+#define __pte_to_swp_entry(pte)	((swp_entry_t) { pte_val(pte) })</span>
<span class="p_add">+#define __swp_entry_to_pte(x)	((pte_t) { (x).val })</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_FLATMEM</span>
<span class="p_add">+#define kern_addr_valid(addr)   (1) /* FIXME */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+extern void paging_init(void);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pgtable_cache_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* No page table caches to initialize */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+#define VMALLOC_SIZE     (KERN_VIRT_SIZE &gt;&gt; 1)</span>
<span class="p_add">+#define VMALLOC_END      (PAGE_OFFSET - 1)</span>
<span class="p_add">+#define VMALLOC_START    (PAGE_OFFSET - VMALLOC_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Task size is 0x40000000000 for RV64 or 0xb800000 for RV32.</span>
<span class="p_add">+ * Note that PGDIR_SIZE must evenly divide TASK_SIZE.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define TASK_SIZE (PGDIR_SIZE * PTRS_PER_PGD / 2)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define TASK_SIZE VMALLOC_START</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/pgtable.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !__ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PGTABLE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/processor.h b/arch/riscv/include/asm/processor.h</span>
new file mode 100644
<span class="p_header">index 000000000000..4f749e8b936b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/processor.h</span>
<span class="p_chunk">@@ -0,0 +1,103 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PROCESSOR_H</span>
<span class="p_add">+#define _ASM_RISCV_PROCESSOR_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This decides where the kernel will search for a free chunk of vm</span>
<span class="p_add">+ * space during mmap&#39;s.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define TASK_UNMAPPED_BASE	PAGE_ALIGN(TASK_SIZE &gt;&gt; 1)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+#define STACK_TOP		TASK_SIZE</span>
<span class="p_add">+#define STACK_TOP_MAX		STACK_TOP</span>
<span class="p_add">+#define STACK_ALIGN		16</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+struct task_struct;</span>
<span class="p_add">+struct pt_regs;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Default implementation of macro that returns current</span>
<span class="p_add">+ * instruction pointer (&quot;program counter&quot;).</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define current_text_addr()	({ __label__ _l; _l: &amp;&amp;_l;})</span>
<span class="p_add">+</span>
<span class="p_add">+/* CPU-specific state of a task */</span>
<span class="p_add">+struct thread_struct {</span>
<span class="p_add">+	/* Callee-saved registers */</span>
<span class="p_add">+	unsigned long ra;</span>
<span class="p_add">+	unsigned long sp;	/* Kernel mode stack */</span>
<span class="p_add">+	unsigned long s[12];	/* s[0]: frame pointer */</span>
<span class="p_add">+	struct user_fpregs_struct fstate;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define INIT_THREAD {					\</span>
<span class="p_add">+	.sp = sizeof(init_stack) + (long)&amp;init_stack,	\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Return saved (kernel) PC of a blocked thread. */</span>
<span class="p_add">+#define thread_saved_pc(t)	((t)-&gt;thread.ra)</span>
<span class="p_add">+#define thread_saved_sp(t)	((t)-&gt;thread.sp)</span>
<span class="p_add">+#define thread_saved_fp(t)	((t)-&gt;thread.s[0])</span>
<span class="p_add">+</span>
<span class="p_add">+#define task_pt_regs(tsk)						\</span>
<span class="p_add">+	((struct pt_regs *)(task_stack_page(tsk) + THREAD_SIZE		\</span>
<span class="p_add">+			    - ALIGN(sizeof(struct pt_regs), STACK_ALIGN)))</span>
<span class="p_add">+</span>
<span class="p_add">+#define KSTK_EIP(tsk)		(task_pt_regs(tsk)-&gt;sepc)</span>
<span class="p_add">+#define KSTK_ESP(tsk)		(task_pt_regs(tsk)-&gt;sp)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Do necessary setup to start up a newly executed thread. */</span>
<span class="p_add">+extern void start_thread(struct pt_regs *regs,</span>
<span class="p_add">+			unsigned long pc, unsigned long sp);</span>
<span class="p_add">+</span>
<span class="p_add">+/* Free all resources held by a thread. */</span>
<span class="p_add">+static inline void release_thread(struct task_struct *dead_task)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long get_wchan(struct task_struct *p);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void cpu_relax(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef __riscv_muldiv</span>
<span class="p_add">+	int dummy;</span>
<span class="p_add">+	/* In lieu of a halt instruction, induce a long-latency stall. */</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;div %0, %0, zero&quot; : &quot;=r&quot; (dummy));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	barrier();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void wait_for_interrupt(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;wfi&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct device_node;</span>
<span class="p_add">+extern int riscv_of_processor_hart(struct device_node *node);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PROCESSOR_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/ptrace.h b/arch/riscv/include/asm/ptrace.h</span>
new file mode 100644
<span class="p_header">index 000000000000..9fa00d56a1f9</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/ptrace.h</span>
<span class="p_chunk">@@ -0,0 +1,117 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_PTRACE_H</span>
<span class="p_add">+#define _ASM_RISCV_PTRACE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;uapi/asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/csr.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+struct pt_regs {</span>
<span class="p_add">+	unsigned long sepc;</span>
<span class="p_add">+	unsigned long ra;</span>
<span class="p_add">+	unsigned long sp;</span>
<span class="p_add">+	unsigned long gp;</span>
<span class="p_add">+	unsigned long tp;</span>
<span class="p_add">+	unsigned long t0;</span>
<span class="p_add">+	unsigned long t1;</span>
<span class="p_add">+	unsigned long t2;</span>
<span class="p_add">+	unsigned long s0;</span>
<span class="p_add">+	unsigned long s1;</span>
<span class="p_add">+	unsigned long a0;</span>
<span class="p_add">+	unsigned long a1;</span>
<span class="p_add">+	unsigned long a2;</span>
<span class="p_add">+	unsigned long a3;</span>
<span class="p_add">+	unsigned long a4;</span>
<span class="p_add">+	unsigned long a5;</span>
<span class="p_add">+	unsigned long a6;</span>
<span class="p_add">+	unsigned long a7;</span>
<span class="p_add">+	unsigned long s2;</span>
<span class="p_add">+	unsigned long s3;</span>
<span class="p_add">+	unsigned long s4;</span>
<span class="p_add">+	unsigned long s5;</span>
<span class="p_add">+	unsigned long s6;</span>
<span class="p_add">+	unsigned long s7;</span>
<span class="p_add">+	unsigned long s8;</span>
<span class="p_add">+	unsigned long s9;</span>
<span class="p_add">+	unsigned long s10;</span>
<span class="p_add">+	unsigned long s11;</span>
<span class="p_add">+	unsigned long t3;</span>
<span class="p_add">+	unsigned long t4;</span>
<span class="p_add">+	unsigned long t5;</span>
<span class="p_add">+	unsigned long t6;</span>
<span class="p_add">+	/* Supervisor CSRs */</span>
<span class="p_add">+	unsigned long sstatus;</span>
<span class="p_add">+	unsigned long sbadaddr;</span>
<span class="p_add">+	unsigned long scause;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define REG_FMT &quot;%016lx&quot;</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define REG_FMT &quot;%08lx&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define user_mode(regs) (((regs)-&gt;sstatus &amp; SR_PS) == 0)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Helpers for working with the instruction pointer */</span>
<span class="p_add">+#define GET_IP(regs) ((regs)-&gt;sepc)</span>
<span class="p_add">+#define SET_IP(regs, val) (GET_IP(regs) = (val))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long instruction_pointer(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return GET_IP(regs);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void instruction_pointer_set(struct pt_regs *regs,</span>
<span class="p_add">+                                           unsigned long val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SET_IP(regs, val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define profile_pc(regs) instruction_pointer(regs)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Helpers for working with the user stack pointer */</span>
<span class="p_add">+#define GET_USP(regs) ((regs)-&gt;sp)</span>
<span class="p_add">+#define SET_USP(regs, val) (GET_USP(regs) = (val))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long user_stack_pointer(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return GET_USP(regs);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void user_stack_pointer_set(struct pt_regs *regs,</span>
<span class="p_add">+                                          unsigned long val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SET_USP(regs, val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Helpers for working with the frame pointer */</span>
<span class="p_add">+#define GET_FP(regs) ((regs)-&gt;s0)</span>
<span class="p_add">+#define SET_FP(regs, val) (GET_FP(regs) = (val))</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long frame_pointer(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return GET_FP(regs);</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void frame_pointer_set(struct pt_regs *regs,</span>
<span class="p_add">+                                     unsigned long val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SET_FP(regs, val);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_PTRACE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/sbi.h b/arch/riscv/include/asm/sbi.h</span>
new file mode 100644
<span class="p_header">index 000000000000..a86ecf9cc1dd</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/sbi.h</span>
<span class="p_chunk">@@ -0,0 +1,101 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SBI_H</span>
<span class="p_add">+#define _ASM_RISCV_SBI_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define SBI_SET_TIMER 0</span>
<span class="p_add">+#define SBI_CONSOLE_PUTCHAR 1</span>
<span class="p_add">+#define SBI_CONSOLE_GETCHAR 2</span>
<span class="p_add">+#define SBI_CLEAR_IPI 3</span>
<span class="p_add">+#define SBI_SEND_IPI 4</span>
<span class="p_add">+#define SBI_REMOTE_FENCE_I 5</span>
<span class="p_add">+#define SBI_REMOTE_SFENCE_VMA 6</span>
<span class="p_add">+#define SBI_REMOTE_SFENCE_VMA_ASID 7</span>
<span class="p_add">+#define SBI_SHUTDOWN 8</span>
<span class="p_add">+</span>
<span class="p_add">+#define SBI_CALL(which, arg0, arg1, arg2) ({			\</span>
<span class="p_add">+	register uintptr_t a0 asm (&quot;a0&quot;) = (uintptr_t)(arg0);	\</span>
<span class="p_add">+	register uintptr_t a1 asm (&quot;a1&quot;) = (uintptr_t)(arg1);	\</span>
<span class="p_add">+	register uintptr_t a2 asm (&quot;a2&quot;) = (uintptr_t)(arg2);	\</span>
<span class="p_add">+	register uintptr_t a7 asm (&quot;a7&quot;) = (uintptr_t)(which);	\</span>
<span class="p_add">+	asm volatile (&quot;ecall&quot;					\</span>
<span class="p_add">+		      : &quot;+r&quot; (a0)				\</span>
<span class="p_add">+		      : &quot;r&quot; (a1), &quot;r&quot; (a2), &quot;r&quot; (a7)		\</span>
<span class="p_add">+		      : &quot;memory&quot;);				\</span>
<span class="p_add">+	a0;							\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/* Lazy implementations until SBI is finalized */</span>
<span class="p_add">+#define SBI_CALL_0(which) SBI_CALL(which, 0, 0, 0)</span>
<span class="p_add">+#define SBI_CALL_1(which, arg0) SBI_CALL(which, arg0, 0, 0)</span>
<span class="p_add">+#define SBI_CALL_2(which, arg0, arg1) SBI_CALL(which, arg0, arg1, 0)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_console_putchar(int ch)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_1(SBI_CONSOLE_PUTCHAR, ch);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int sbi_console_getchar(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return SBI_CALL_0(SBI_CONSOLE_GETCHAR);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_set_timer(uint64_t stime_value)</span>
<span class="p_add">+{</span>
<span class="p_add">+#if __riscv_xlen == 32</span>
<span class="p_add">+	SBI_CALL_2(SBI_SET_TIMER, stime_value, stime_value &gt;&gt; 32);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	SBI_CALL_1(SBI_SET_TIMER, stime_value);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_shutdown(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_0(SBI_SHUTDOWN);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_clear_ipi(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_0(SBI_CLEAR_IPI);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_send_ipi(const unsigned long *hart_mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_1(SBI_SEND_IPI, hart_mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_remote_fence_i(const unsigned long *hart_mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_1(SBI_REMOTE_FENCE_I, hart_mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_remote_sfence_vma(const unsigned long *hart_mask,</span>
<span class="p_add">+					 unsigned long start,</span>
<span class="p_add">+					 unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_1(SBI_REMOTE_SFENCE_VMA, hart_mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void sbi_remote_sfence_vma_asid(const unsigned long *hart_mask,</span>
<span class="p_add">+					      unsigned long start,</span>
<span class="p_add">+					      unsigned long size,</span>
<span class="p_add">+					      unsigned long asid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	SBI_CALL_1(SBI_REMOTE_SFENCE_VMA_ASID, hart_mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/serial.h b/arch/riscv/include/asm/serial.h</span>
new file mode 100644
<span class="p_header">index 000000000000..d783dbe80a4b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/serial.h</span>
<span class="p_chunk">@@ -0,0 +1,43 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SERIAL_H</span>
<span class="p_add">+#define _ASM_RISCV_SERIAL_H</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * FIXME: interim serial support for riscv-qemu</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Currently requires that the emulator itself create a hole at addresses</span>
<span class="p_add">+ * 0x3f8 - 0x3ff without looking through page tables.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This assumes you have a 1.8432 MHz clock for your UART.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define BASE_BAUD (1843200 / 16)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Standard COM flags */</span>
<span class="p_add">+#ifdef CONFIG_SERIAL_DETECT_IRQ</span>
<span class="p_add">+#define STD_COM_FLAGS (ASYNC_BOOT_AUTOCONF | ASYNC_SKIP_TEST | ASYNC_AUTO_IRQ)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define STD_COM_FLAGS (ASYNC_BOOT_AUTOCONF | ASYNC_SKIP_TEST)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define SERIAL_PORT_DFNS			\</span>
<span class="p_add">+	{	/* ttyS0 */			\</span>
<span class="p_add">+		.baud_base = BASE_BAUD,		\</span>
<span class="p_add">+		.port      = 0x3F8,		\</span>
<span class="p_add">+		.irq       = 4,			\</span>
<span class="p_add">+		.flags     = STD_COM_FLAGS,	\</span>
<span class="p_add">+	},</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SERIAL_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/setup.h b/arch/riscv/include/asm/setup.h</span>
new file mode 100644
<span class="p_header">index 000000000000..e457854e9988</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/setup.h</span>
<span class="p_chunk">@@ -0,0 +1,20 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SETUP_H</span>
<span class="p_add">+#define _ASM_RISCV_SETUP_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/setup.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SETUP_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/smp.h b/arch/riscv/include/asm/smp.h</span>
new file mode 100644
<span class="p_header">index 000000000000..1fd60220ef29</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/smp.h</span>
<span class="p_chunk">@@ -0,0 +1,42 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SMP_H</span>
<span class="p_add">+#define _ASM_RISCV_SMP_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/cpumask.h&gt;</span>
<span class="p_add">+#include &lt;linux/irqreturn.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+/* SMP initialization hook for setup_arch */</span>
<span class="p_add">+void __init init_clockevent(void);</span>
<span class="p_add">+</span>
<span class="p_add">+/* SMP initialization hook for setup_arch */</span>
<span class="p_add">+void __init setup_smp(void);</span>
<span class="p_add">+</span>
<span class="p_add">+/* Hook for the generic smp_call_function_many() routine. */</span>
<span class="p_add">+void arch_send_call_function_ipi_mask(struct cpumask *mask);</span>
<span class="p_add">+</span>
<span class="p_add">+/* Hook for the generic smp_call_function_single() routine. */</span>
<span class="p_add">+void arch_send_call_function_single_ipi(int cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+#define raw_smp_processor_id() (current_thread_info()-&gt;cpu)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Interprocessor interrupt handler */</span>
<span class="p_add">+irqreturn_t handle_ipi(void);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SMP_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock.h b/arch/riscv/include/asm/spinlock.h</span>
new file mode 100644
<span class="p_header">index 000000000000..baf0f441f0e3</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock.h</span>
<span class="p_chunk">@@ -0,0 +1,156 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;asm/current.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Simple spin lock operations.  These provide no fairness guarantees.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_spin_lock_flags(lock, flags) arch_spin_lock(lock)</span>
<span class="p_add">+#define arch_spin_is_locked(x)	((x)-&gt;lock != 0)</span>
<span class="p_add">+#define arch_spin_unlock_wait(x) \</span>
<span class="p_add">+		do { cpu_relax(); } while ((x)-&gt;lock)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp = 1, busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.aq %0, %2, %1&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (busy), &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (tmp)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (1) {</span>
<span class="p_add">+		if (arch_spin_is_locked(lock))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (arch_spin_trylock(lock))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/***********************************************************/</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock &gt;= 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_can_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return lock-&gt;lock == 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_lock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_read_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bltz	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	addi	%1, %1, 1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+	</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int arch_write_trylock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int busy;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;1:	lr.w	%1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1f\n&quot;</span>
<span class="p_add">+		&quot;	li	%1, -1\n&quot;</span>
<span class="p_add">+		&quot;	sc.w.aq	%1, %1, %0\n&quot;</span>
<span class="p_add">+		&quot;	bnez	%1, 1b\n&quot;</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock), &quot;=&amp;r&quot; (busy)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return !busy;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_read_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__(</span>
<span class="p_add">+		&quot;amoadd.w.rl x0, %1, %0&quot;</span>
<span class="p_add">+		: &quot;+A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		: &quot;r&quot; (-1)</span>
<span class="p_add">+		: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_write_unlock(arch_rwlock_t *lock)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;amoswap.w.rl x0, x0, %0&quot;</span>
<span class="p_add">+		: &quot;=A&quot; (lock-&gt;lock)</span>
<span class="p_add">+		:: &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define arch_read_lock_flags(lock, flags) arch_read_lock(lock)</span>
<span class="p_add">+#define arch_write_lock_flags(lock, flags) arch_write_lock(lock)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SPINLOCK_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/spinlock_types.h b/arch/riscv/include/asm/spinlock_types.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0a4ff7086e8a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/spinlock_types.h</span>
<span class="p_chunk">@@ -0,0 +1,34 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+#define _ASM_RISCV_SPINLOCK_TYPES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __LINUX_SPINLOCK_TYPES_H</span>
<span class="p_add">+# error &quot;please don&#39;t include this file directly&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_spinlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_SPIN_LOCK_UNLOCKED	{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	volatile unsigned int lock;</span>
<span class="p_add">+} arch_rwlock_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_RW_LOCK_UNLOCKED		{ 0 }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/string.h b/arch/riscv/include/asm/string.h</span>
new file mode 100644
<span class="p_header">index 000000000000..b394c02a537a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/string.h</span>
<span class="p_chunk">@@ -0,0 +1,31 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2013 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_STRING_H</span>
<span class="p_add">+#define _ASM_RISCV_STRING_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_MEMSET</span>
<span class="p_add">+extern asmlinkage void *memset(void *, int, size_t);</span>
<span class="p_add">+</span>
<span class="p_add">+#define __HAVE_ARCH_MEMCPY</span>
<span class="p_add">+extern asmlinkage void *memcpy(void *, const void *, size_t);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_STRING_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/switch_to.h b/arch/riscv/include/asm/switch_to.h</span>
new file mode 100644
<span class="p_header">index 000000000000..d49c225baff9</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/switch_to.h</span>
<span class="p_chunk">@@ -0,0 +1,71 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SWITCH_TO_H</span>
<span class="p_add">+#define _ASM_RISCV_SWITCH_TO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/processor.h&gt;</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/csr.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __fstate_save(struct task_struct *);</span>
<span class="p_add">+extern void __fstate_restore(struct task_struct *);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __fstate_clean(struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	regs-&gt;sstatus |= (regs-&gt;sstatus &amp; ~(SR_FS)) | SR_FS_CLEAN;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void fstate_save(struct task_struct *task,</span>
<span class="p_add">+                               struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if ((regs-&gt;sstatus &amp; SR_FS) == SR_FS_DIRTY) {</span>
<span class="p_add">+		__fstate_save(task);</span>
<span class="p_add">+		__fstate_clean(regs);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void fstate_restore(struct task_struct *task,</span>
<span class="p_add">+                                  struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if ((regs-&gt;sstatus &amp; SR_FS) != SR_FS_OFF) {</span>
<span class="p_add">+		__fstate_restore(task);</span>
<span class="p_add">+		__fstate_clean(regs);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void __switch_to_aux(struct task_struct *prev,</span>
<span class="p_add">+                                   struct task_struct *next)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct pt_regs *regs;</span>
<span class="p_add">+</span>
<span class="p_add">+	regs = task_pt_regs(prev);</span>
<span class="p_add">+	if (unlikely(regs-&gt;sstatus &amp; SR_SD)) {</span>
<span class="p_add">+		fstate_save(prev, regs);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	fstate_restore(next, task_pt_regs(next));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct task_struct *__switch_to(struct task_struct *,</span>
<span class="p_add">+                                       struct task_struct *);</span>
<span class="p_add">+</span>
<span class="p_add">+#define switch_to(prev, next, last)			\</span>
<span class="p_add">+do {							\</span>
<span class="p_add">+	struct task_struct *__prev = (prev);		\</span>
<span class="p_add">+	struct task_struct *__next = (next);		\</span>
<span class="p_add">+	__switch_to_aux(__prev, __next);		\</span>
<span class="p_add">+	((last) = __switch_to(__prev, __next));		\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SWITCH_TO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/syscall.h b/arch/riscv/include/asm/syscall.h</span>
new file mode 100644
<span class="p_header">index 000000000000..070609831f85</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/syscall.h</span>
<span class="p_chunk">@@ -0,0 +1,91 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2008-2009 Red Hat, Inc.  All rights reserved.</span>
<span class="p_add">+ * Copyright 2010 Tilera Corporation. All Rights Reserved.</span>
<span class="p_add">+ * Copyright 2015 Regents of the University of California, Berkeley</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * See asm-generic/syscall.h for descriptions of what we must do here.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SYSCALL_H</span>
<span class="p_add">+#define _ASM_RISCV_SYSCALL_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/err.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* The array of function pointers for syscalls. */</span>
<span class="p_add">+extern void *sys_call_table[];</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Only the low 32 bits of orig_r0 are meaningful, so we return int.</span>
<span class="p_add">+ * This importantly ignores the high bits on 64-bit, so comparisons</span>
<span class="p_add">+ * sign-extend the low 32 bits.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int syscall_get_nr(struct task_struct *task,</span>
<span class="p_add">+				 struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return regs-&gt;a7;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void syscall_set_nr(struct task_struct *task,</span>
<span class="p_add">+				  struct pt_regs *regs,</span>
<span class="p_add">+				  int sysno)</span>
<span class="p_add">+{</span>
<span class="p_add">+	regs-&gt;a7 = sysno;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void syscall_rollback(struct task_struct *task,</span>
<span class="p_add">+				    struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* FIXME: We can&#39;t do this... */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long syscall_get_error(struct task_struct *task,</span>
<span class="p_add">+				     struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long error = regs-&gt;a0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return IS_ERR_VALUE(error) ? error : 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline long syscall_get_return_value(struct task_struct *task,</span>
<span class="p_add">+					    struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return regs-&gt;a0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void syscall_set_return_value(struct task_struct *task,</span>
<span class="p_add">+					    struct pt_regs *regs,</span>
<span class="p_add">+					    int error, long val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	regs-&gt;a0 = (long) error ?: val;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void syscall_get_arguments(struct task_struct *task,</span>
<span class="p_add">+					 struct pt_regs *regs,</span>
<span class="p_add">+					 unsigned int i, unsigned int n,</span>
<span class="p_add">+					 unsigned long *args)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG_ON(i + n &gt; 6);</span>
<span class="p_add">+	memcpy(args, &amp;regs-&gt;a0 + i * sizeof(regs-&gt;a0), n * sizeof(args[0]));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void syscall_set_arguments(struct task_struct *task,</span>
<span class="p_add">+					 struct pt_regs *regs,</span>
<span class="p_add">+					 unsigned int i, unsigned int n,</span>
<span class="p_add">+					 const unsigned long *args)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG_ON(i + n &gt; 6);</span>
<span class="p_add">+	memcpy(&amp;regs-&gt;a0 + i * sizeof(regs-&gt;a0), args, n * sizeof(regs-&gt;a0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif	/* _ASM_TILE_SYSCALL_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/syscalls.h b/arch/riscv/include/asm/syscalls.h</span>
new file mode 100644
<span class="p_header">index 000000000000..1dd23596b0d3</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/syscalls.h</span>
<span class="p_chunk">@@ -0,0 +1,26 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2014 Darius Rad &lt;darius@bluespec.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_SYSCALLS_H</span>
<span class="p_add">+#define _ASM_RISCV_SYSCALLS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/syscalls.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* kernel/sys_riscv.c */</span>
<span class="p_add">+asmlinkage long sys_sysriscv(unsigned long, unsigned long,</span>
<span class="p_add">+	unsigned long, unsigned long);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_SYSCALLS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/thread_info.h b/arch/riscv/include/asm/thread_info.h</span>
new file mode 100644
<span class="p_header">index 000000000000..7446f960b71a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/thread_info.h</span>
<span class="p_chunk">@@ -0,0 +1,103 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_THREAD_INFO_H</span>
<span class="p_add">+#define _ASM_RISCV_THREAD_INFO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __KERNEL__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/page.h&gt;</span>
<span class="p_add">+#include &lt;linux/const.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* thread information allocation */</span>
<span class="p_add">+#define THREAD_SIZE_ORDER	(1)</span>
<span class="p_add">+#define THREAD_SIZE		(PAGE_SIZE &lt;&lt; THREAD_SIZE_ORDER)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/processor.h&gt;</span>
<span class="p_add">+#include &lt;asm/csr.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+typedef unsigned long mm_segment_t;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * low level task data that entry.S needs immediate access to</span>
<span class="p_add">+ * - this struct should fit entirely inside of one cache line</span>
<span class="p_add">+ * - this struct resides at the bottom of the supervisor stack</span>
<span class="p_add">+ * - if the members of this struct changes, the assembly constants</span>
<span class="p_add">+ *   in asm-offsets.c must be updated accordingly</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct thread_info {</span>
<span class="p_add">+	struct task_struct	*task;		/* main task structure */</span>
<span class="p_add">+	unsigned long		flags;		/* low level flags */</span>
<span class="p_add">+	__u32			cpu;		/* current CPU */</span>
<span class="p_add">+	int                     preempt_count;  /* 0 =&gt; preemptable, &lt;0 =&gt; BUG */</span>
<span class="p_add">+	mm_segment_t		addr_limit;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * macros/functions for gaining access to the thread information structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * preempt_count needs to be 1 initially, until the scheduler is functional.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define INIT_THREAD_INFO(tsk)			\</span>
<span class="p_add">+{						\</span>
<span class="p_add">+	.task		= &amp;tsk,			\</span>
<span class="p_add">+	.flags		= 0,			\</span>
<span class="p_add">+	.cpu		= 0,			\</span>
<span class="p_add">+	.preempt_count	= INIT_PREEMPT_COUNT,	\</span>
<span class="p_add">+	.addr_limit	= KERNEL_DS,		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define init_thread_info	(init_thread_union.thread_info)</span>
<span class="p_add">+#define init_stack		(init_thread_union.stack)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Pointer to the thread_info struct of the current process</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline struct thread_info *current_thread_info(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	register struct thread_info *tp __asm__ (&quot;tp&quot;);</span>
<span class="p_add">+	return tp;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !__ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * thread information flags</span>
<span class="p_add">+ * - these are process state flags that various assembly files may need to</span>
<span class="p_add">+ *   access</span>
<span class="p_add">+ * - pending work-to-be-done flags are in lowest half-word</span>
<span class="p_add">+ * - other flags in upper half-word(s)</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define TIF_SYSCALL_TRACE	0	/* syscall trace active */</span>
<span class="p_add">+#define TIF_NOTIFY_RESUME	1	/* callback before returning to user */</span>
<span class="p_add">+#define TIF_SIGPENDING		2	/* signal pending */</span>
<span class="p_add">+#define TIF_NEED_RESCHED	3	/* rescheduling necessary */</span>
<span class="p_add">+#define TIF_RESTORE_SIGMASK	4	/* restore signal mask in do_signal() */</span>
<span class="p_add">+#define TIF_MEMDIE		5	/* is terminating due to OOM killer */</span>
<span class="p_add">+#define TIF_SYSCALL_TRACEPOINT  6       /* syscall tracepoint instrumentation */</span>
<span class="p_add">+</span>
<span class="p_add">+#define _TIF_SYSCALL_TRACE	(1 &lt;&lt; TIF_SYSCALL_TRACE)</span>
<span class="p_add">+#define _TIF_NOTIFY_RESUME	(1 &lt;&lt; TIF_NOTIFY_RESUME)</span>
<span class="p_add">+#define _TIF_SIGPENDING		(1 &lt;&lt; TIF_SIGPENDING)</span>
<span class="p_add">+#define _TIF_NEED_RESCHED	(1 &lt;&lt; TIF_NEED_RESCHED)</span>
<span class="p_add">+</span>
<span class="p_add">+#define _TIF_WORK_MASK \</span>
<span class="p_add">+	(_TIF_NOTIFY_RESUME | _TIF_SIGPENDING | _TIF_NEED_RESCHED)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __KERNEL__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_THREAD_INFO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/timex.h b/arch/riscv/include/asm/timex.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0a236ab7d178</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/timex.h</span>
<span class="p_chunk">@@ -0,0 +1,55 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TIMEX_H</span>
<span class="p_add">+#define _ASM_RISCV_TIMEX_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/param.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define CLOCK_TICK_RATE (HZ * 100UL)</span>
<span class="p_add">+</span>
<span class="p_add">+typedef unsigned long cycles_t;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline cycles_t get_cycles(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#if __riscv_xlen &gt;= 64</span>
<span class="p_add">+	cycles_t n;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;rdtime %0&quot;</span>
<span class="p_add">+		: &quot;=r&quot; (n));</span>
<span class="p_add">+	return n;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	u32 lo, hi, tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	__asm__ __volatile__ (</span>
<span class="p_add">+		&quot;1:\n&quot;</span>
<span class="p_add">+		&quot;rdtimeh %0\n&quot;</span>
<span class="p_add">+		&quot;rdtime %1\n&quot;</span>
<span class="p_add">+		&quot;rdtimeh %2\n&quot;</span>
<span class="p_add">+		&quot;bne %0, %2, 1b&quot;</span>
<span class="p_add">+		: &quot;=&amp;r&quot; (hi), &quot;=&amp;r&quot; (lo), &quot;=&amp;r&quot; (tmp));</span>
<span class="p_add">+	return ((u64)hi &lt;&lt; 32) | lo;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define ARCH_HAS_READ_CURRENT_TIMER</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int read_current_timer(unsigned long *timer_val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*timer_val = get_cycles();</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TIMEX_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlb.h b/arch/riscv/include/asm/tlb.h</span>
new file mode 100644
<span class="p_header">index 000000000000..6195b0ea718b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlb.h</span>
<span class="p_chunk">@@ -0,0 +1,25 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLB_H</span>
<span class="p_add">+#define _ASM_RISCV_TLB_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/tlb.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void tlb_flush(struct mmu_gather *tlb)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_mm(tlb-&gt;mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLB_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/tlbflush.h b/arch/riscv/include/asm/tlbflush.h</span>
new file mode 100644
<span class="p_header">index 000000000000..292879e9cc04</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -0,0 +1,95 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2009 Chen Liqin &lt;liqin.chen@sunplusct.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+#define _ASM_RISCV_TLBFLUSH_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush entire local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma&quot; : : : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush one page from local TLB */</span>
<span class="p_add">+static inline void local_flush_tlb_page(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;sfence.vma %0&quot; : : &quot;r&quot; (addr) : &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_SMP</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() local_flush_tlb_all()</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) local_flush_tlb_page(addr)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) local_flush_tlb_all()</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/sbi.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define flush_tlb_all() sbi_remote_sfence_vma(0, 0, -1)</span>
<span class="p_add">+#define flush_tlb_page(vma, addr) flush_tlb_range(vma, addr, 0)</span>
<span class="p_add">+#define flush_tlb_range(vma, start, end) \</span>
<span class="p_add">+	sbi_remote_sfence_vma(0, start, (end) - (start))</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_SMP */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush the TLB entries of the specified mm context */</span>
<span class="p_add">+static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Flush a range of kernel pages */</span>
<span class="p_add">+static inline void flush_tlb_kernel_range(unsigned long start,</span>
<span class="p_add">+	unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	flush_tlb_all();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void flush_tlb_all(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void flush_tlb_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void flush_tlb_page(struct vm_area_struct *vma,</span>
<span class="p_add">+	unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void flush_tlb_range(struct vm_area_struct *vma,</span>
<span class="p_add">+	unsigned long start, unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void flush_tlb_kernel_range(unsigned long start,</span>
<span class="p_add">+	unsigned long end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_TLBFLUSH_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/uaccess.h b/arch/riscv/include/asm/uaccess.h</span>
new file mode 100644
<span class="p_header">index 000000000000..3add03baca0f</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -0,0 +1,455 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This file was copied from include/asm-generic/uaccess.h</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_UACCESS_H</span>
<span class="p_add">+#define _ASM_RISCV_UACCESS_H</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * User space memory access functions</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &lt;linux/compiler.h&gt;</span>
<span class="p_add">+#include &lt;linux/thread_info.h&gt;</span>
<span class="p_add">+#include &lt;asm/byteorder.h&gt;</span>
<span class="p_add">+#include &lt;asm/asm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_RV_PUM</span>
<span class="p_add">+#define __enable_user_access()						\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrs sstatus, %0&quot; : : &quot;r&quot; (SR_SUM))</span>
<span class="p_add">+#define __disable_user_access()						\</span>
<span class="p_add">+	__asm__ __volatile__ (&quot;csrc sstatus, %0&quot; : : &quot;r&quot; (SR_SUM))</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define __enable_user_access()</span>
<span class="p_add">+#define __disable_user_access()</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The fs value determines whether argument validity checking should be</span>
<span class="p_add">+ * performed or not.  If get_fs() == USER_DS, checking is performed, with</span>
<span class="p_add">+ * get_fs() == KERNEL_DS, checking is bypassed.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For historical reasons, these macros are grossly misnamed.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define KERNEL_DS	(~0UL)</span>
<span class="p_add">+#define USER_DS		(TASK_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define get_ds()	(KERNEL_DS)</span>
<span class="p_add">+#define get_fs()	(current_thread_info()-&gt;addr_limit)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void set_fs(mm_segment_t fs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	current_thread_info()-&gt;addr_limit = fs;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define segment_eq(a, b) ((a) == (b))</span>
<span class="p_add">+</span>
<span class="p_add">+#define user_addr_max()	(get_fs())</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#define VERIFY_READ	0</span>
<span class="p_add">+#define VERIFY_WRITE	1</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * access_ok: - Checks if a user space pointer is valid</span>
<span class="p_add">+ * @type: Type of access: %VERIFY_READ or %VERIFY_WRITE.  Note that</span>
<span class="p_add">+ *        %VERIFY_WRITE is a superset of %VERIFY_READ - if it is safe</span>
<span class="p_add">+ *        to write to a block, it is always safe to read from it.</span>
<span class="p_add">+ * @addr: User space pointer to start of block to check</span>
<span class="p_add">+ * @size: Size of block to check</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Checks if a pointer to a block of memory in user space is valid.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns true (nonzero) if the memory block may be valid, false (zero)</span>
<span class="p_add">+ * if it is definitely invalid.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that, depending on architecture, this function probably just</span>
<span class="p_add">+ * checks that the pointer is in the user space range - after calling</span>
<span class="p_add">+ * this function, memory access functions may still return -EFAULT.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define access_ok(type, addr, size) ({					\</span>
<span class="p_add">+	__chk_user_ptr(addr);						\</span>
<span class="p_add">+	likely(__access_ok((unsigned long __force)(addr), (size)));	\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/* Ensure that the range [addr, addr+size) is within the process&#39;s</span>
<span class="p_add">+ * address space</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int __access_ok(unsigned long addr, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const mm_segment_t fs = get_fs();</span>
<span class="p_add">+	return (size &lt;= fs) &amp;&amp; (addr &lt;= (fs - size));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The exception table consists of pairs of addresses: the first is the</span>
<span class="p_add">+ * address of an instruction that is allowed to fault, and the second is</span>
<span class="p_add">+ * the address at which the program should continue.  No registers are</span>
<span class="p_add">+ * modified, so it is entirely up to the continuation code to figure out</span>
<span class="p_add">+ * what to do.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * All the routines below use bits of fixup code that are out of line</span>
<span class="p_add">+ * with the main instruction path.  This means when everything is well,</span>
<span class="p_add">+ * we don&#39;t even have to jump over them.  Further, they do not intrude</span>
<span class="p_add">+ * on our cache or tlb entries.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+struct exception_table_entry {</span>
<span class="p_add">+	unsigned long insn, fixup;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+extern int fixup_exception(struct pt_regs *);</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(__LITTLE_ENDIAN)</span>
<span class="p_add">+#define __MSW	1</span>
<span class="p_add">+#define __LSW	0</span>
<span class="p_add">+#elif defined(__BIG_ENDIAN)</span>
<span class="p_add">+#define __MSW	0</span>
<span class="p_add">+#define	__LSW	1</span>
<span class="p_add">+#else</span>
<span class="p_add">+#error &quot;Unknown endianness&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The &quot;__xxx&quot; versions of the user access functions do not verify the address</span>
<span class="p_add">+ * space - it must have been done previously with a separate &quot;access_ok()&quot;</span>
<span class="p_add">+ * call.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+#define __get_user_asm(insn, x, ptr, err)			\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	uintptr_t __tmp;					\</span>
<span class="p_add">+	__enable_user_access();					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;1:\n&quot;						\</span>
<span class="p_add">+		&quot;	&quot; insn &quot; %1, %3\n&quot;			\</span>
<span class="p_add">+		&quot;2:\n&quot;						\</span>
<span class="p_add">+		&quot;	.section .fixup,\&quot;ax\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign 4\n&quot;				\</span>
<span class="p_add">+		&quot;3:\n&quot;						\</span>
<span class="p_add">+		&quot;	li %0, %4\n&quot;				\</span>
<span class="p_add">+		&quot;	li %1, 0\n&quot;				\</span>
<span class="p_add">+		&quot;	jump 2b, %2\n&quot;				\</span>
<span class="p_add">+		&quot;	.previous\n&quot;				\</span>
<span class="p_add">+		&quot;	.section __ex_table,\&quot;a\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign &quot; SZPTR &quot;\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; PTR &quot; 1b, 3b\n&quot;			\</span>
<span class="p_add">+		&quot;	.previous&quot;				\</span>
<span class="p_add">+		: &quot;+r&quot; (err), &quot;=&amp;r&quot; (x), &quot;=r&quot; (__tmp)		\</span>
<span class="p_add">+		: &quot;m&quot; (*(ptr)), &quot;i&quot; (-EFAULT));			\</span>
<span class="p_add">+	__disable_user_access();				\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#else /* !CONFIG_MMU */</span>
<span class="p_add">+#define __get_user_asm(insn, x, ptr, err)			\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		insn &quot; %0, %1&quot;					\</span>
<span class="p_add">+		: &quot;=r&quot; (x)					\</span>
<span class="p_add">+		: &quot;m&quot; (*(ptr)))</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define __get_user_8(x, ptr, err) \</span>
<span class="p_add">+	__get_user_asm(&quot;ld&quot;, x, ptr, err)</span>
<span class="p_add">+#else /* !CONFIG_64BIT */</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+#define __get_user_8(x, ptr, err)				\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	u32 __user *__ptr = (u32 __user *)(ptr);		\</span>
<span class="p_add">+	u32 __lo, __hi;						\</span>
<span class="p_add">+	uintptr_t __tmp;					\</span>
<span class="p_add">+	__enable_user_access();					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;1:\n&quot;						\</span>
<span class="p_add">+		&quot;	lw %1, %4\n&quot;				\</span>
<span class="p_add">+		&quot;2:\n&quot;						\</span>
<span class="p_add">+		&quot;	lw %2, %5\n&quot;				\</span>
<span class="p_add">+		&quot;3:\n&quot;						\</span>
<span class="p_add">+		&quot;	.section .fixup,\&quot;ax\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign 4\n&quot;				\</span>
<span class="p_add">+		&quot;4:\n&quot;						\</span>
<span class="p_add">+		&quot;	li %0, %6\n&quot;				\</span>
<span class="p_add">+		&quot;	li %1, 0\n&quot;				\</span>
<span class="p_add">+		&quot;	li %2, 0\n&quot;				\</span>
<span class="p_add">+		&quot;	jump 3b, %3\n&quot;				\</span>
<span class="p_add">+		&quot;	.previous\n&quot;				\</span>
<span class="p_add">+		&quot;	.section __ex_table,\&quot;a\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign &quot; SZPTR &quot;\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; PTR &quot; 1b, 4b\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; PTR &quot; 2b, 4b\n&quot;			\</span>
<span class="p_add">+		&quot;	.previous&quot;				\</span>
<span class="p_add">+		: &quot;+r&quot; (err), &quot;=&amp;r&quot; (__lo), &quot;=r&quot; (__hi),	\</span>
<span class="p_add">+			&quot;=r&quot; (__tmp)				\</span>
<span class="p_add">+		: &quot;m&quot; (__ptr[__LSW]), &quot;m&quot; (__ptr[__MSW]),	\</span>
<span class="p_add">+			&quot;i&quot; (-EFAULT));				\</span>
<span class="p_add">+	__disable_user_access();				\</span>
<span class="p_add">+	(x) = (__typeof__(x))((__typeof__((x)-(x)))(		\</span>
<span class="p_add">+		(((u64)__hi &lt;&lt; 32) | __lo)));			\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#else /* !CONFIG_MMU */</span>
<span class="p_add">+#define __get_user_8(x, ptr, err) \</span>
<span class="p_add">+	(x) = (__typeof__(x))(*((u64 __user *)(ptr)))</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+#endif /* CONFIG_64BIT */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __get_user: - Get a simple variable from user space, with less checking.</span>
<span class="p_add">+ * @x:   Variable to store result.</span>
<span class="p_add">+ * @ptr: Source address, in user space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This macro copies a single simple variable from user space to kernel</span>
<span class="p_add">+ * space.  It supports simple types like char and int, but not larger</span>
<span class="p_add">+ * data types like structures or arrays.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ptr must have pointer-to-simple-variable type, and the result of</span>
<span class="p_add">+ * dereferencing @ptr must be assignable to @x without a cast.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Caller must check the pointer with access_ok() before calling this</span>
<span class="p_add">+ * function.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns zero on success, or -EFAULT on error.</span>
<span class="p_add">+ * On error, the variable @x is set to zero.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __get_user(x, ptr)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	register int __gu_err = 0;				\</span>
<span class="p_add">+	const __typeof__(*(ptr)) __user *__gu_ptr = (ptr);	\</span>
<span class="p_add">+	__chk_user_ptr(__gu_ptr);				\</span>
<span class="p_add">+	switch (sizeof(*__gu_ptr)) {				\</span>
<span class="p_add">+	case 1:							\</span>
<span class="p_add">+		__get_user_asm(&quot;lb&quot;, (x), __gu_ptr, __gu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 2:							\</span>
<span class="p_add">+		__get_user_asm(&quot;lh&quot;, (x), __gu_ptr, __gu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 4:							\</span>
<span class="p_add">+		__get_user_asm(&quot;lw&quot;, (x), __gu_ptr, __gu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 8:							\</span>
<span class="p_add">+		__get_user_8((x), __gu_ptr, __gu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	default:						\</span>
<span class="p_add">+		BUILD_BUG();					\</span>
<span class="p_add">+	}							\</span>
<span class="p_add">+	__gu_err;						\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * get_user: - Get a simple variable from user space.</span>
<span class="p_add">+ * @x:   Variable to store result.</span>
<span class="p_add">+ * @ptr: Source address, in user space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This macro copies a single simple variable from user space to kernel</span>
<span class="p_add">+ * space.  It supports simple types like char and int, but not larger</span>
<span class="p_add">+ * data types like structures or arrays.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ptr must have pointer-to-simple-variable type, and the result of</span>
<span class="p_add">+ * dereferencing @ptr must be assignable to @x without a cast.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns zero on success, or -EFAULT on error.</span>
<span class="p_add">+ * On error, the variable @x is set to zero.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define get_user(x, ptr)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	const __typeof__(*(ptr)) __user *__p = (ptr);		\</span>
<span class="p_add">+	might_fault();						\</span>
<span class="p_add">+	access_ok(VERIFY_READ, __p, sizeof(*__p)) ?		\</span>
<span class="p_add">+		__get_user((x), __p) :				\</span>
<span class="p_add">+		((x) = 0, -EFAULT);				\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+#define __put_user_asm(insn, x, ptr, err)			\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	uintptr_t __tmp;					\</span>
<span class="p_add">+	__typeof__(*(ptr)) __x = x;				\</span>
<span class="p_add">+	__enable_user_access();					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;1:\n&quot;						\</span>
<span class="p_add">+		&quot;	&quot; insn &quot; %z3, %2\n&quot;			\</span>
<span class="p_add">+		&quot;2:\n&quot;						\</span>
<span class="p_add">+		&quot;	.section .fixup,\&quot;ax\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign 4\n&quot;				\</span>
<span class="p_add">+		&quot;3:\n&quot;						\</span>
<span class="p_add">+		&quot;	li %0, %4\n&quot;				\</span>
<span class="p_add">+		&quot;	jump 2b, %1\n&quot;				\</span>
<span class="p_add">+		&quot;	.previous\n&quot;				\</span>
<span class="p_add">+		&quot;	.section __ex_table,\&quot;a\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign &quot; SZPTR &quot;\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; PTR &quot; 1b, 3b\n&quot;			\</span>
<span class="p_add">+		&quot;	.previous&quot;				\</span>
<span class="p_add">+		: &quot;+r&quot; (err), &quot;=r&quot; (__tmp), &quot;=m&quot; (*(ptr))	\</span>
<span class="p_add">+		: &quot;rJ&quot; (__x), &quot;i&quot; (-EFAULT));			\</span>
<span class="p_add">+	__disable_user_access();				\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#else /* !CONFIG_MMU */</span>
<span class="p_add">+#define __put_user_asm(insn, x, ptr, err)			\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		insn &quot; %z1, %0&quot;					\</span>
<span class="p_add">+		: &quot;=m&quot; (*(ptr))					\</span>
<span class="p_add">+		: &quot;rJ&quot; ((__typeof__(*(ptr))) x))</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+#define __put_user_8(x, ptr, err) \</span>
<span class="p_add">+	__put_user_asm(&quot;sd&quot;, x, ptr, err)</span>
<span class="p_add">+#else /* !CONFIG_64BIT */</span>
<span class="p_add">+#ifdef CONFIG_MMU</span>
<span class="p_add">+#define __put_user_8(x, ptr, err)				\</span>
<span class="p_add">+do {								\</span>
<span class="p_add">+	u32 __user *__ptr = (u32 __user *)(ptr);		\</span>
<span class="p_add">+	u64 __x = (__typeof__((x)-(x)))(x);			\</span>
<span class="p_add">+	uintptr_t __tmp;					\</span>
<span class="p_add">+	__enable_user_access();					\</span>
<span class="p_add">+	__asm__ __volatile__ (					\</span>
<span class="p_add">+		&quot;1:\n&quot;						\</span>
<span class="p_add">+		&quot;	sw %z4, %2\n&quot;				\</span>
<span class="p_add">+		&quot;2:\n&quot;						\</span>
<span class="p_add">+		&quot;	sw %z5, %3\n&quot;				\</span>
<span class="p_add">+		&quot;3:\n&quot;						\</span>
<span class="p_add">+		&quot;	.section .fixup,\&quot;ax\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign 4\n&quot;				\</span>
<span class="p_add">+		&quot;4:\n&quot;						\</span>
<span class="p_add">+		&quot;	li %0, %6\n&quot;				\</span>
<span class="p_add">+		&quot;	jump 2b, %1\n&quot;				\</span>
<span class="p_add">+		&quot;	.previous\n&quot;				\</span>
<span class="p_add">+		&quot;	.section __ex_table,\&quot;a\&quot;\n&quot;		\</span>
<span class="p_add">+		&quot;	.balign &quot; SZPTR &quot;\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; PTR &quot; 1b, 4b\n&quot;			\</span>
<span class="p_add">+		&quot;	&quot; PTR &quot; 2b, 4b\n&quot;			\</span>
<span class="p_add">+		&quot;	.previous&quot;				\</span>
<span class="p_add">+		: &quot;+r&quot; (err), &quot;=r&quot; (__tmp),			\</span>
<span class="p_add">+			&quot;=m&quot; (__ptr[__LSW]),			\</span>
<span class="p_add">+			&quot;=m&quot; (__ptr[__MSW])			\</span>
<span class="p_add">+		: &quot;rJ&quot; (__x), &quot;rJ&quot; (__x &gt;&gt; 32), &quot;i&quot; (-EFAULT));	\</span>
<span class="p_add">+	__disable_user_access();				\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#else /* !CONFIG_MMU */</span>
<span class="p_add">+#define __put_user_8(x, ptr, err)				\</span>
<span class="p_add">+	*((u64 __user *)(ptr)) = (u64)(x)</span>
<span class="p_add">+#endif /* CONFIG_MMU */</span>
<span class="p_add">+#endif /* CONFIG_64BIT */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * __put_user: - Write a simple value into user space, with less checking.</span>
<span class="p_add">+ * @x:   Value to copy to user space.</span>
<span class="p_add">+ * @ptr: Destination address, in user space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This macro copies a single simple value from kernel space to user</span>
<span class="p_add">+ * space.  It supports simple types like char and int, but not larger</span>
<span class="p_add">+ * data types like structures or arrays.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ptr must have pointer-to-simple-variable type, and @x must be assignable</span>
<span class="p_add">+ * to the result of dereferencing @ptr.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Caller must check the pointer with access_ok() before calling this</span>
<span class="p_add">+ * function.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns zero on success, or -EFAULT on error.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __put_user(x, ptr)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	register int __pu_err = 0;				\</span>
<span class="p_add">+	__typeof__(*(ptr)) __user *__gu_ptr = (ptr);		\</span>
<span class="p_add">+	__chk_user_ptr(__gu_ptr);				\</span>
<span class="p_add">+	switch (sizeof(*__gu_ptr)) {				\</span>
<span class="p_add">+	case 1:							\</span>
<span class="p_add">+		__put_user_asm(&quot;sb&quot;, (x), __gu_ptr, __pu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 2:							\</span>
<span class="p_add">+		__put_user_asm(&quot;sh&quot;, (x), __gu_ptr, __pu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 4:							\</span>
<span class="p_add">+		__put_user_asm(&quot;sw&quot;, (x), __gu_ptr, __pu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	case 8:							\</span>
<span class="p_add">+		__put_user_8((x), __gu_ptr, __pu_err);	\</span>
<span class="p_add">+		break;						\</span>
<span class="p_add">+	default:						\</span>
<span class="p_add">+		BUILD_BUG();					\</span>
<span class="p_add">+	}							\</span>
<span class="p_add">+	__pu_err;						\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * put_user: - Write a simple value into user space.</span>
<span class="p_add">+ * @x:   Value to copy to user space.</span>
<span class="p_add">+ * @ptr: Destination address, in user space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Context: User context only.  This function may sleep.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This macro copies a single simple value from kernel space to user</span>
<span class="p_add">+ * space.  It supports simple types like char and int, but not larger</span>
<span class="p_add">+ * data types like structures or arrays.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ptr must have pointer-to-simple-variable type, and @x must be assignable</span>
<span class="p_add">+ * to the result of dereferencing @ptr.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns zero on success, or -EFAULT on error.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define put_user(x, ptr)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	__typeof__(*(ptr)) __user *__p = (ptr);			\</span>
<span class="p_add">+	might_fault();						\</span>
<span class="p_add">+	access_ok(VERIFY_WRITE, __p, sizeof(*__p)) ?		\</span>
<span class="p_add">+		__put_user((x), __p) :				\</span>
<span class="p_add">+		-EFAULT;					\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long __must_check __copy_user(void __user *to,</span>
<span class="p_add">+	const void __user *from, unsigned long n);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long</span>
<span class="p_add">+raw_copy_from_user(void *to, const void __user *from, unsigned long n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __copy_user(to, from, n);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long</span>
<span class="p_add">+raw_copy_to_user(void __user *to, const void *from, unsigned long n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __copy_user(to, from, n);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern long strncpy_from_user(char *dest, const char __user *src, long count);</span>
<span class="p_add">+</span>
<span class="p_add">+extern long __must_check strlen_user(const char __user *str);</span>
<span class="p_add">+extern long __must_check strnlen_user(const char __user *str, long n);</span>
<span class="p_add">+</span>
<span class="p_add">+extern unsigned long __must_check __clear_user(void __user *addr, unsigned long n);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long __must_check clear_user(void __user *to, unsigned long n)</span>
<span class="p_add">+{</span>
<span class="p_add">+	might_fault();</span>
<span class="p_add">+	return access_ok(VERIFY_WRITE, to, n) ?</span>
<span class="p_add">+		__clear_user(to, n) : n;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_UACCESS_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/unistd.h b/arch/riscv/include/asm/unistd.h</span>
new file mode 100644
<span class="p_header">index 000000000000..3f5820f50912</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/unistd.h</span>
<span class="p_chunk">@@ -0,0 +1,17 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_HAVE_MMU</span>
<span class="p_add">+#define __ARCH_WANT_SYS_CLONE</span>
<span class="p_add">+#include &lt;uapi/asm/unistd.h&gt;</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/vdso.h b/arch/riscv/include/asm/vdso.h</span>
new file mode 100644
<span class="p_header">index 000000000000..95768a3810a7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/vdso.h</span>
<span class="p_chunk">@@ -0,0 +1,32 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Limited</span>
<span class="p_add">+ * Copyright (C) 2014 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_VDSO_H</span>
<span class="p_add">+#define _ASM_RISCV_VDSO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct vdso_data {</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define VDSO_SYMBOL(base, name)					\</span>
<span class="p_add">+({								\</span>
<span class="p_add">+	extern const char __vdso_##name[];			\</span>
<span class="p_add">+	(void __user *)((unsigned long)(base) + __vdso_##name);	\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_VDSO_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/asm/word-at-a-time.h b/arch/riscv/include/asm/word-at-a-time.h</span>
new file mode 100644
<span class="p_header">index 000000000000..57fcb40f2616</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/asm/word-at-a-time.h</span>
<span class="p_chunk">@@ -0,0 +1,56 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ * Derived from arch/x86/include/asm/word-at-a-time.h</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_RISCV_WORD_AT_A_TIME_H</span>
<span class="p_add">+#define _ASM_RISCV_WORD_AT_A_TIME_H</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct word_at_a_time {</span>
<span class="p_add">+	const unsigned long one_bits, high_bits;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define WORD_AT_A_TIME_CONSTANTS { REPEAT_BYTE(0x01), REPEAT_BYTE(0x80) }</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long has_zero(unsigned long val,</span>
<span class="p_add">+	unsigned long *bits, const struct word_at_a_time *c)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long mask = ((val - c-&gt;one_bits) &amp; ~val) &amp; c-&gt;high_bits;</span>
<span class="p_add">+	*bits = mask;</span>
<span class="p_add">+	return mask;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long prep_zero_mask(unsigned long val,</span>
<span class="p_add">+	unsigned long bits, const struct word_at_a_time *c)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return bits;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long create_zero_mask(unsigned long bits)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bits = (bits - 1) &amp; ~bits;</span>
<span class="p_add">+	return bits &gt;&gt; 7;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long find_zero(unsigned long mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return fls64(mask) &gt;&gt; 3;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* The mask we created is directly usable as a bytemask */</span>
<span class="p_add">+#define zero_bytemask(mask) (mask)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_RISCV_WORD_AT_A_TIME_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/Kbuild b/arch/riscv/include/uapi/asm/Kbuild</span>
new file mode 100644
<span class="p_header">index 000000000000..276b6dae745c</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/Kbuild</span>
<span class="p_chunk">@@ -0,0 +1,10 @@</span> <span class="p_context"></span>
<span class="p_add">+# UAPI Header export list</span>
<span class="p_add">+include include/uapi/asm-generic/Kbuild.asm</span>
<span class="p_add">+</span>
<span class="p_add">+header-y += auxvec.h</span>
<span class="p_add">+header-y += bitsperlong.h</span>
<span class="p_add">+header-y += byteorder.h</span>
<span class="p_add">+header-y += ptrace.h</span>
<span class="p_add">+header-y += sigcontext.h</span>
<span class="p_add">+header-y += siginfo.h</span>
<span class="p_add">+header-y += unistd.h</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/auxvec.h b/arch/riscv/include/uapi/asm/auxvec.h</span>
new file mode 100644
<span class="p_header">index 000000000000..1376515547cd</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/auxvec.h</span>
<span class="p_chunk">@@ -0,0 +1,24 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_RISCV_AUXVEC_H</span>
<span class="p_add">+#define _UAPI_ASM_RISCV_AUXVEC_H</span>
<span class="p_add">+</span>
<span class="p_add">+/* vDSO location */</span>
<span class="p_add">+#define AT_SYSINFO_EHDR 33</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_RISCV_AUXVEC_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/bitsperlong.h b/arch/riscv/include/uapi/asm/bitsperlong.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0b3cb52fd29d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/bitsperlong.h</span>
<span class="p_chunk">@@ -0,0 +1,25 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_RISCV_BITSPERLONG_H</span>
<span class="p_add">+#define _UAPI_ASM_RISCV_BITSPERLONG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define __BITS_PER_LONG (__SIZEOF_POINTER__ * 8)</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/bitsperlong.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_RISCV_BITSPERLONG_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/byteorder.h b/arch/riscv/include/uapi/asm/byteorder.h</span>
new file mode 100644
<span class="p_header">index 000000000000..4ca38af2cd32</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/byteorder.h</span>
<span class="p_chunk">@@ -0,0 +1,23 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2015 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_RISCV_BYTEORDER_H</span>
<span class="p_add">+#define _UAPI_ASM_RISCV_BYTEORDER_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/byteorder/little_endian.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_RISCV_BYTEORDER_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/elf.h b/arch/riscv/include/uapi/asm/elf.h</span>
new file mode 100644
<span class="p_header">index 000000000000..e438edd97589</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/elf.h</span>
<span class="p_chunk">@@ -0,0 +1,83 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2003 Matjaz Breskvar &lt;phoenix@bsemi.com&gt;</span>
<span class="p_add">+ * Copyright (C) 2010-2011 Jonas Bonn &lt;jonas@southpole.se&gt;</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_ELF_H</span>
<span class="p_add">+#define _UAPI_ASM_ELF_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* ELF register definitions */</span>
<span class="p_add">+typedef unsigned long elf_greg_t;</span>
<span class="p_add">+typedef struct user_regs_struct elf_gregset_t;</span>
<span class="p_add">+#define ELF_NGREG (sizeof(elf_gregset_t) / sizeof(elf_greg_t))</span>
<span class="p_add">+</span>
<span class="p_add">+typedef struct user_fpregs_struct elf_fpregset_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define ELF_RISCV_R_SYM(r_info) ((r_info) &gt;&gt; 32)</span>
<span class="p_add">+#define ELF_RISCV_R_TYPE(r_info) ((r_info) &amp; 0xffffffff)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * RISC-V relocation types</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Relocation types used by the dynamic linker */</span>
<span class="p_add">+#define R_RISCV_NONE		0</span>
<span class="p_add">+#define R_RISCV_32		1</span>
<span class="p_add">+#define R_RISCV_64		2</span>
<span class="p_add">+#define R_RISCV_RELATIVE	3</span>
<span class="p_add">+#define R_RISCV_COPY		4</span>
<span class="p_add">+#define R_RISCV_JUMP_SLOT	5</span>
<span class="p_add">+#define R_RISCV_TLS_DTPMOD32	6</span>
<span class="p_add">+#define R_RISCV_TLS_DTPMOD64	7</span>
<span class="p_add">+#define R_RISCV_TLS_DTPREL32	8</span>
<span class="p_add">+#define R_RISCV_TLS_DTPREL64	9</span>
<span class="p_add">+#define R_RISCV_TLS_TPREL32	10</span>
<span class="p_add">+#define R_RISCV_TLS_TPREL64	11</span>
<span class="p_add">+</span>
<span class="p_add">+/* Relocation types not used by the dynamic linker */</span>
<span class="p_add">+#define R_RISCV_BRANCH		16</span>
<span class="p_add">+#define R_RISCV_JAL		17</span>
<span class="p_add">+#define R_RISCV_CALL		18</span>
<span class="p_add">+#define R_RISCV_CALL_PLT	19</span>
<span class="p_add">+#define R_RISCV_GOT_HI20	20</span>
<span class="p_add">+#define R_RISCV_TLS_GOT_HI20	21</span>
<span class="p_add">+#define R_RISCV_TLS_GD_HI20	22</span>
<span class="p_add">+#define R_RISCV_PCREL_HI20	23</span>
<span class="p_add">+#define R_RISCV_PCREL_LO12_I	24</span>
<span class="p_add">+#define R_RISCV_PCREL_LO12_S	25</span>
<span class="p_add">+#define R_RISCV_HI20		26</span>
<span class="p_add">+#define R_RISCV_LO12_I		27</span>
<span class="p_add">+#define R_RISCV_LO12_S		28</span>
<span class="p_add">+#define R_RISCV_TPREL_HI20	29</span>
<span class="p_add">+#define R_RISCV_TPREL_LO12_I	30</span>
<span class="p_add">+#define R_RISCV_TPREL_LO12_S	31</span>
<span class="p_add">+#define R_RISCV_TPREL_ADD	32</span>
<span class="p_add">+#define R_RISCV_ADD8		33</span>
<span class="p_add">+#define R_RISCV_ADD16		34</span>
<span class="p_add">+#define R_RISCV_ADD32		35</span>
<span class="p_add">+#define R_RISCV_ADD64		36</span>
<span class="p_add">+#define R_RISCV_SUB8		37</span>
<span class="p_add">+#define R_RISCV_SUB16		38</span>
<span class="p_add">+#define R_RISCV_SUB32		39</span>
<span class="p_add">+#define R_RISCV_SUB64		40</span>
<span class="p_add">+#define R_RISCV_GNU_VTINHERIT	41</span>
<span class="p_add">+#define R_RISCV_GNU_VTENTRY	42</span>
<span class="p_add">+#define R_RISCV_ALIGN		43</span>
<span class="p_add">+#define R_RISCV_RVC_BRANCH	44</span>
<span class="p_add">+#define R_RISCV_RVC_JUMP	45</span>
<span class="p_add">+#define R_RISCV_LUI			46</span>
<span class="p_add">+#define R_RISCV_GPREL_I		47</span>
<span class="p_add">+#define R_RISCV_GPREL_S		48</span>
<span class="p_add">+#define R_RISCV_TPREL_I		49</span>
<span class="p_add">+#define R_RISCV_TPREL_S		50</span>
<span class="p_add">+#define R_RISCV_RELAX		51</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_ELF_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/ptrace.h b/arch/riscv/include/uapi/asm/ptrace.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c5b93028697c</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/ptrace.h</span>
<span class="p_chunk">@@ -0,0 +1,69 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_RISCV_PTRACE_H</span>
<span class="p_add">+#define _UAPI_ASM_RISCV_PTRACE_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* User-mode register state for core dumps, ptrace, sigcontext</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This decouples struct pt_regs from the userspace ABI.</span>
<span class="p_add">+ * struct user_regs_struct must form a prefix of struct pt_regs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct user_regs_struct {</span>
<span class="p_add">+	unsigned long pc;</span>
<span class="p_add">+	unsigned long ra;</span>
<span class="p_add">+	unsigned long sp;</span>
<span class="p_add">+	unsigned long gp;</span>
<span class="p_add">+	unsigned long tp;</span>
<span class="p_add">+	unsigned long t0;</span>
<span class="p_add">+	unsigned long t1;</span>
<span class="p_add">+	unsigned long t2;</span>
<span class="p_add">+	unsigned long s0;</span>
<span class="p_add">+	unsigned long s1;</span>
<span class="p_add">+	unsigned long a0;</span>
<span class="p_add">+	unsigned long a1;</span>
<span class="p_add">+	unsigned long a2;</span>
<span class="p_add">+	unsigned long a3;</span>
<span class="p_add">+	unsigned long a4;</span>
<span class="p_add">+	unsigned long a5;</span>
<span class="p_add">+	unsigned long a6;</span>
<span class="p_add">+	unsigned long a7;</span>
<span class="p_add">+	unsigned long s2;</span>
<span class="p_add">+	unsigned long s3;</span>
<span class="p_add">+	unsigned long s4;</span>
<span class="p_add">+	unsigned long s5;</span>
<span class="p_add">+	unsigned long s6;</span>
<span class="p_add">+	unsigned long s7;</span>
<span class="p_add">+	unsigned long s8;</span>
<span class="p_add">+	unsigned long s9;</span>
<span class="p_add">+	unsigned long s10;</span>
<span class="p_add">+	unsigned long s11;</span>
<span class="p_add">+	unsigned long t3;</span>
<span class="p_add">+	unsigned long t4;</span>
<span class="p_add">+	unsigned long t5;</span>
<span class="p_add">+	unsigned long t6;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct user_fpregs_struct {</span>
<span class="p_add">+	__u64 f[32];</span>
<span class="p_add">+	__u32 fcsr;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_RISCV_PTRACE_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/sigcontext.h b/arch/riscv/include/uapi/asm/sigcontext.h</span>
new file mode 100644
<span class="p_header">index 000000000000..04967aade3a6</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/sigcontext.h</span>
<span class="p_chunk">@@ -0,0 +1,30 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _UAPI_ASM_RISCV_SIGCONTEXT_H</span>
<span class="p_add">+#define _UAPI_ASM_RISCV_SIGCONTEXT_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Signal context structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This contains the context saved before a signal handler is invoked;</span>
<span class="p_add">+ * it is restored by sys_sigreturn / sys_rt_sigreturn.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct sigcontext {</span>
<span class="p_add">+	struct user_regs_struct sc_regs;</span>
<span class="p_add">+	struct user_fpregs_struct sc_fpregs;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _UAPI_ASM_RISCV_SIGCONTEXT_H */</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/siginfo.h b/arch/riscv/include/uapi/asm/siginfo.h</span>
new file mode 100644
<span class="p_header">index 000000000000..f96849aac662</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/siginfo.h</span>
<span class="p_chunk">@@ -0,0 +1,24 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 ARM Ltd.</span>
<span class="p_add">+ * Copyright (C) 2016 SiFive, Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef __ASM_SIGINFO_H</span>
<span class="p_add">+#define __ASM_SIGINFO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define __ARCH_SI_PREAMBLE_SIZE	(__SIZEOF_POINTER__ == 4 ? 12 : 16)</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/siginfo.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/riscv/include/uapi/asm/unistd.h b/arch/riscv/include/uapi/asm/unistd.h</span>
new file mode 100644
<span class="p_header">index 000000000000..124810f71633</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/riscv/include/uapi/asm/unistd.h</span>
<span class="p_chunk">@@ -0,0 +1,23 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2012 Regents of the University of California</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is free software; you can redistribute it and/or</span>
<span class="p_add">+ *   modify it under the terms of the GNU General Public License</span>
<span class="p_add">+ *   as published by the Free Software Foundation, version 2.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ *   WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ *   MERCHANTABILITY OR FITNESS FOR A PARTICULAR PURPOSE, GOOD TITLE or</span>
<span class="p_add">+ *   NON INFRINGEMENT.  See the GNU General Public License for</span>
<span class="p_add">+ *   more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm-generic/unistd.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define __NR_sysriscv  __NR_arch_specific_syscall</span>
<span class="p_add">+#ifndef __riscv_atomic</span>
<span class="p_add">+__SYSCALL(__NR_sysriscv, sys_sysriscv)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define RISCV_ATOMIC_CMPXCHG    1</span>
<span class="p_add">+#define RISCV_ATOMIC_CMPXCHG64  2</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



