
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 4.9.21 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 4.9.21</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 8, 2017, 8:07 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170408080746.GB14290@kroah.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9670989/mbox/"
   >mbox</a>
|
   <a href="/patch/9670989/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9670989/">/patch/9670989/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	3DCD66028A for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat,  8 Apr 2017 08:09:07 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2261928462
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat,  8 Apr 2017 08:09:07 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 123EA285D2; Sat,  8 Apr 2017 08:09:07 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1101628462
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sat,  8 Apr 2017 08:09:00 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751154AbdDHIIz (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sat, 8 Apr 2017 04:08:55 -0400
Received: from mail.linuxfoundation.org ([140.211.169.12]:55146 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752176AbdDHIIF (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sat, 8 Apr 2017 04:08:05 -0400
Received: from localhost (D57E6652.static.ziggozakelijk.nl [213.126.102.82])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id EE40F5AC;
	Sat,  8 Apr 2017 08:07:56 +0000 (UTC)
Date: Sat, 8 Apr 2017 10:07:46 +0200
From: Greg KH &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, stable@vger.kernel.org
Cc: lwn@lwn.net, Jiri Slaby &lt;jslaby@suse.cz&gt;
Subject: Re: Linux 4.9.21
Message-ID: &lt;20170408080746.GB14290@kroah.com&gt;
References: &lt;20170408080741.GA14290@kroah.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20170408080741.GA14290@kroah.com&gt;
User-Agent: Mutt/1.8.0 (2017-02-23)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - April 8, 2017, 8:07 a.m.</div>
<pre class="content">

</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index 44960184701a..1523557bd61f 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,6 +1,6 @@</span> <span class="p_context"></span>
 VERSION = 4
 PATCHLEVEL = 9
<span class="p_del">-SUBLEVEL = 20</span>
<span class="p_add">+SUBLEVEL = 21</span>
 EXTRAVERSION =
 NAME = Roaring Lionus
 
<span class="p_header">diff --git a/arch/arm/boot/dts/bcm5301x.dtsi b/arch/arm/boot/dts/bcm5301x.dtsi</span>
<span class="p_header">index ae4b3880616d..4616452ce74d 100644</span>
<span class="p_header">--- a/arch/arm/boot/dts/bcm5301x.dtsi</span>
<span class="p_header">+++ b/arch/arm/boot/dts/bcm5301x.dtsi</span>
<span class="p_chunk">@@ -66,14 +66,14 @@</span> <span class="p_context"></span>
 		timer@20200 {
 			compatible = &quot;arm,cortex-a9-global-timer&quot;;
 			reg = &lt;0x20200 0x100&gt;;
<span class="p_del">-			interrupts = &lt;GIC_PPI 11 IRQ_TYPE_LEVEL_HIGH&gt;;</span>
<span class="p_add">+			interrupts = &lt;GIC_PPI 11 IRQ_TYPE_EDGE_RISING&gt;;</span>
 			clocks = &lt;&amp;periph_clk&gt;;
 		};
 
 		local-timer@20600 {
 			compatible = &quot;arm,cortex-a9-twd-timer&quot;;
 			reg = &lt;0x20600 0x100&gt;;
<span class="p_del">-			interrupts = &lt;GIC_PPI 13 IRQ_TYPE_LEVEL_HIGH&gt;;</span>
<span class="p_add">+			interrupts = &lt;GIC_PPI 13 IRQ_TYPE_EDGE_RISING&gt;;</span>
 			clocks = &lt;&amp;periph_clk&gt;;
 		};
 
<span class="p_header">diff --git a/arch/arm/mach-bcm/bcm_5301x.c b/arch/arm/mach-bcm/bcm_5301x.c</span>
<span class="p_header">index c8830a2b0d60..fe067f6cebb6 100644</span>
<span class="p_header">--- a/arch/arm/mach-bcm/bcm_5301x.c</span>
<span class="p_header">+++ b/arch/arm/mach-bcm/bcm_5301x.c</span>
<span class="p_chunk">@@ -9,14 +9,42 @@</span> <span class="p_context"></span>
 #include &lt;asm/hardware/cache-l2x0.h&gt;
 
 #include &lt;asm/mach/arch.h&gt;
<span class="p_add">+#include &lt;asm/siginfo.h&gt;</span>
<span class="p_add">+#include &lt;asm/signal.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define FSR_EXTERNAL		(1 &lt;&lt; 12)</span>
<span class="p_add">+#define FSR_READ		(0 &lt;&lt; 10)</span>
<span class="p_add">+#define FSR_IMPRECISE		0x0406</span>
 
 static const char *const bcm5301x_dt_compat[] __initconst = {
 	&quot;brcm,bcm4708&quot;,
 	NULL,
 };
 
<span class="p_add">+static int bcm5301x_abort_handler(unsigned long addr, unsigned int fsr,</span>
<span class="p_add">+				  struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We want to ignore aborts forwarded from the PCIe bus that are</span>
<span class="p_add">+	 * expected and shouldn&#39;t really be passed by the PCIe controller.</span>
<span class="p_add">+	 * The biggest disadvantage is the same FSR code may be reported when</span>
<span class="p_add">+	 * reading non-existing APB register and we shouldn&#39;t ignore that.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (fsr == (FSR_EXTERNAL | FSR_READ | FSR_IMPRECISE))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init bcm5301x_init_early(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	hook_fault_code(16 + 6, bcm5301x_abort_handler, SIGBUS, BUS_OBJERR,</span>
<span class="p_add">+			&quot;imprecise external abort&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 DT_MACHINE_START(BCM5301X, &quot;BCM5301X&quot;)
 	.l2c_aux_val	= 0,
 	.l2c_aux_mask	= ~0,
 	.dt_compat	= bcm5301x_dt_compat,
<span class="p_add">+	.init_early	= bcm5301x_init_early,</span>
 MACHINE_END
<span class="p_header">diff --git a/arch/mips/lantiq/irq.c b/arch/mips/lantiq/irq.c</span>
<span class="p_header">index 8ac0e5994ed2..0ddf3698b85d 100644</span>
<span class="p_header">--- a/arch/mips/lantiq/irq.c</span>
<span class="p_header">+++ b/arch/mips/lantiq/irq.c</span>
<span class="p_chunk">@@ -269,6 +269,11 @@</span> <span class="p_context"> static void ltq_hw5_irqdispatch(void)</span>
 DEFINE_HWx_IRQDISPATCH(5)
 #endif
 
<span class="p_add">+static void ltq_hw_irq_handler(struct irq_desc *desc)</span>
<span class="p_add">+{</span>
<span class="p_add">+	ltq_hw_irqdispatch(irq_desc_get_irq(desc) - 2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MIPS_MT_SMP
 void __init arch_init_ipiirq(int irq, struct irqaction *action)
 {
<span class="p_chunk">@@ -313,23 +318,19 @@</span> <span class="p_context"> static struct irqaction irq_call = {</span>
 asmlinkage void plat_irq_dispatch(void)
 {
 	unsigned int pending = read_c0_status() &amp; read_c0_cause() &amp; ST0_IM;
<span class="p_del">-	unsigned int i;</span>
<span class="p_del">-</span>
<span class="p_del">-	if ((MIPS_CPU_TIMER_IRQ == 7) &amp;&amp; (pending &amp; CAUSEF_IP7)) {</span>
<span class="p_del">-		do_IRQ(MIPS_CPU_TIMER_IRQ);</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		for (i = 0; i &lt; MAX_IM; i++) {</span>
<span class="p_del">-			if (pending &amp; (CAUSEF_IP2 &lt;&lt; i)) {</span>
<span class="p_del">-				ltq_hw_irqdispatch(i);</span>
<span class="p_del">-				goto out;</span>
<span class="p_del">-			}</span>
<span class="p_del">-		}</span>
<span class="p_add">+	int irq;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pending) {</span>
<span class="p_add">+		spurious_interrupt();</span>
<span class="p_add">+		return;</span>
 	}
<span class="p_del">-	pr_alert(&quot;Spurious IRQ: CAUSE=0x%08x\n&quot;, read_c0_status());</span>
 
<span class="p_del">-out:</span>
<span class="p_del">-	return;</span>
<span class="p_add">+	pending &gt;&gt;= CAUSEB_IP;</span>
<span class="p_add">+	while (pending) {</span>
<span class="p_add">+		irq = fls(pending) - 1;</span>
<span class="p_add">+		do_IRQ(MIPS_CPU_IRQ_BASE + irq);</span>
<span class="p_add">+		pending &amp;= ~BIT(irq);</span>
<span class="p_add">+	}</span>
 }
 
 static int icu_map(struct irq_domain *d, unsigned int irq, irq_hw_number_t hw)
<span class="p_chunk">@@ -354,11 +355,6 @@</span> <span class="p_context"> static const struct irq_domain_ops irq_domain_ops = {</span>
 	.map = icu_map,
 };
 
<span class="p_del">-static struct irqaction cascade = {</span>
<span class="p_del">-	.handler = no_action,</span>
<span class="p_del">-	.name = &quot;cascade&quot;,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 int __init icu_of_init(struct device_node *node, struct device_node *parent)
 {
 	struct device_node *eiu_node;
<span class="p_chunk">@@ -390,7 +386,7 @@</span> <span class="p_context"> int __init icu_of_init(struct device_node *node, struct device_node *parent)</span>
 	mips_cpu_irq_init();
 
 	for (i = 0; i &lt; MAX_IM; i++)
<span class="p_del">-		setup_irq(i + 2, &amp;cascade);</span>
<span class="p_add">+		irq_set_chained_handler(i + 2, ltq_hw_irq_handler);</span>
 
 	if (cpu_has_vint) {
 		pr_info(&quot;Setting up vectored interrupts\n&quot;);
<span class="p_header">diff --git a/arch/parisc/include/asm/uaccess.h b/arch/parisc/include/asm/uaccess.h</span>
<span class="p_header">index 9a2aee1b90fc..7fcf5128996a 100644</span>
<span class="p_header">--- a/arch/parisc/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/parisc/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -68,6 +68,15 @@</span> <span class="p_context"> struct exception_table_entry {</span>
 	&quot;.previous\n&quot;
 
 /*
<span class="p_add">+ * ASM_EXCEPTIONTABLE_ENTRY_EFAULT() creates a special exception table entry</span>
<span class="p_add">+ * (with lowest bit set) for which the fault handler in fixup_exception() will</span>
<span class="p_add">+ * load -EFAULT into %r8 for a read or write fault, and zeroes the target</span>
<span class="p_add">+ * register in case of a read fault in get_user().</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define ASM_EXCEPTIONTABLE_ENTRY_EFAULT( fault_addr, except_addr )\</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY( fault_addr, except_addr + 1)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * The page fault handler stores, in a per-cpu area, the following information
  * if a fixup routine is available.
  */
<span class="p_chunk">@@ -94,7 +103,7 @@</span> <span class="p_context"> struct exception_data {</span>
 #define __get_user(x, ptr)                               \
 ({                                                       \
 	register long __gu_err __asm__ (&quot;r8&quot;) = 0;       \
<span class="p_del">-	register long __gu_val __asm__ (&quot;r9&quot;) = 0;       \</span>
<span class="p_add">+	register long __gu_val;				 \</span>
 							 \
 	load_sr2();					 \
 	switch (sizeof(*(ptr))) {			 \
<span class="p_chunk">@@ -110,22 +119,23 @@</span> <span class="p_context"> struct exception_data {</span>
 })
 
 #define __get_user_asm(ldx, ptr)                        \
<span class="p_del">-	__asm__(&quot;\n1:\t&quot; ldx &quot;\t0(%%sr2,%2),%0\n\t&quot;	\</span>
<span class="p_del">-		ASM_EXCEPTIONTABLE_ENTRY(1b, fixup_get_user_skip_1)\</span>
<span class="p_add">+	__asm__(&quot;1: &quot; ldx &quot; 0(%%sr2,%2),%0\n&quot;		\</span>
<span class="p_add">+		&quot;9:\n&quot;					\</span>
<span class="p_add">+		ASM_EXCEPTIONTABLE_ENTRY_EFAULT(1b, 9b)	\</span>
 		: &quot;=r&quot;(__gu_val), &quot;=r&quot;(__gu_err)        \
<span class="p_del">-		: &quot;r&quot;(ptr), &quot;1&quot;(__gu_err)		\</span>
<span class="p_del">-		: &quot;r1&quot;);</span>
<span class="p_add">+		: &quot;r&quot;(ptr), &quot;1&quot;(__gu_err));</span>
 
 #if !defined(CONFIG_64BIT)
 
 #define __get_user_asm64(ptr) 				\
<span class="p_del">-	__asm__(&quot;\n1:\tldw 0(%%sr2,%2),%0&quot;		\</span>
<span class="p_del">-		&quot;\n2:\tldw 4(%%sr2,%2),%R0\n\t&quot;		\</span>
<span class="p_del">-		ASM_EXCEPTIONTABLE_ENTRY(1b, fixup_get_user_skip_2)\</span>
<span class="p_del">-		ASM_EXCEPTIONTABLE_ENTRY(2b, fixup_get_user_skip_1)\</span>
<span class="p_add">+	__asm__(&quot;   copy %%r0,%R0\n&quot;			\</span>
<span class="p_add">+		&quot;1: ldw 0(%%sr2,%2),%0\n&quot;		\</span>
<span class="p_add">+		&quot;2: ldw 4(%%sr2,%2),%R0\n&quot;		\</span>
<span class="p_add">+		&quot;9:\n&quot;					\</span>
<span class="p_add">+		ASM_EXCEPTIONTABLE_ENTRY_EFAULT(1b, 9b)	\</span>
<span class="p_add">+		ASM_EXCEPTIONTABLE_ENTRY_EFAULT(2b, 9b)	\</span>
 		: &quot;=r&quot;(__gu_val), &quot;=r&quot;(__gu_err)	\
<span class="p_del">-		: &quot;r&quot;(ptr), &quot;1&quot;(__gu_err)		\</span>
<span class="p_del">-		: &quot;r1&quot;);</span>
<span class="p_add">+		: &quot;r&quot;(ptr), &quot;1&quot;(__gu_err));</span>
 
 #endif /* !defined(CONFIG_64BIT) */
 
<span class="p_chunk">@@ -151,32 +161,31 @@</span> <span class="p_context"> struct exception_data {</span>
  * The &quot;__put_user/kernel_asm()&quot; macros tell gcc they read from memory
  * instead of writing. This is because they do not write to any memory
  * gcc knows about, so there are no aliasing issues. These macros must
<span class="p_del">- * also be aware that &quot;fixup_put_user_skip_[12]&quot; are executed in the</span>
<span class="p_del">- * context of the fault, and any registers used there must be listed</span>
<span class="p_del">- * as clobbers. In this case only &quot;r1&quot; is used by the current routines.</span>
<span class="p_del">- * r8/r9 are already listed as err/val.</span>
<span class="p_add">+ * also be aware that fixups are executed in the context of the fault,</span>
<span class="p_add">+ * and any registers used there must be listed as clobbers.</span>
<span class="p_add">+ * r8 is already listed as err.</span>
  */
 
 #define __put_user_asm(stx, x, ptr)                         \
 	__asm__ __volatile__ (                              \
<span class="p_del">-		&quot;\n1:\t&quot; stx &quot;\t%2,0(%%sr2,%1)\n\t&quot;	    \</span>
<span class="p_del">-		ASM_EXCEPTIONTABLE_ENTRY(1b, fixup_put_user_skip_1)\</span>
<span class="p_add">+		&quot;1: &quot; stx &quot; %2,0(%%sr2,%1)\n&quot;		    \</span>
<span class="p_add">+		&quot;9:\n&quot;					    \</span>
<span class="p_add">+		ASM_EXCEPTIONTABLE_ENTRY_EFAULT(1b, 9b)	    \</span>
 		: &quot;=r&quot;(__pu_err)                            \
<span class="p_del">-		: &quot;r&quot;(ptr), &quot;r&quot;(x), &quot;0&quot;(__pu_err)	    \</span>
<span class="p_del">-		: &quot;r1&quot;)</span>
<span class="p_add">+		: &quot;r&quot;(ptr), &quot;r&quot;(x), &quot;0&quot;(__pu_err))</span>
 
 
 #if !defined(CONFIG_64BIT)
 
 #define __put_user_asm64(__val, ptr) do {	    	    \
 	__asm__ __volatile__ (				    \
<span class="p_del">-		&quot;\n1:\tstw %2,0(%%sr2,%1)&quot;		    \</span>
<span class="p_del">-		&quot;\n2:\tstw %R2,4(%%sr2,%1)\n\t&quot;		    \</span>
<span class="p_del">-		ASM_EXCEPTIONTABLE_ENTRY(1b, fixup_put_user_skip_2)\</span>
<span class="p_del">-		ASM_EXCEPTIONTABLE_ENTRY(2b, fixup_put_user_skip_1)\</span>
<span class="p_add">+		&quot;1: stw %2,0(%%sr2,%1)\n&quot;		    \</span>
<span class="p_add">+		&quot;2: stw %R2,4(%%sr2,%1)\n&quot;		    \</span>
<span class="p_add">+		&quot;9:\n&quot;					    \</span>
<span class="p_add">+		ASM_EXCEPTIONTABLE_ENTRY_EFAULT(1b, 9b)	    \</span>
<span class="p_add">+		ASM_EXCEPTIONTABLE_ENTRY_EFAULT(2b, 9b)	    \</span>
 		: &quot;=r&quot;(__pu_err)                            \
<span class="p_del">-		: &quot;r&quot;(ptr), &quot;r&quot;(__val), &quot;0&quot;(__pu_err) \</span>
<span class="p_del">-		: &quot;r1&quot;);				    \</span>
<span class="p_add">+		: &quot;r&quot;(ptr), &quot;r&quot;(__val), &quot;0&quot;(__pu_err));	    \</span>
 } while (0)
 
 #endif /* !defined(CONFIG_64BIT) */
<span class="p_header">diff --git a/arch/parisc/kernel/parisc_ksyms.c b/arch/parisc/kernel/parisc_ksyms.c</span>
<span class="p_header">index 3cad8aadc69e..4e6f0d93154f 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/parisc_ksyms.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/parisc_ksyms.c</span>
<span class="p_chunk">@@ -47,16 +47,6 @@</span> <span class="p_context"> EXPORT_SYMBOL(__cmpxchg_u64);</span>
 EXPORT_SYMBOL(lclear_user);
 EXPORT_SYMBOL(lstrnlen_user);
 
<span class="p_del">-/* Global fixups - defined as int to avoid creation of function pointers */</span>
<span class="p_del">-extern int fixup_get_user_skip_1;</span>
<span class="p_del">-extern int fixup_get_user_skip_2;</span>
<span class="p_del">-extern int fixup_put_user_skip_1;</span>
<span class="p_del">-extern int fixup_put_user_skip_2;</span>
<span class="p_del">-EXPORT_SYMBOL(fixup_get_user_skip_1);</span>
<span class="p_del">-EXPORT_SYMBOL(fixup_get_user_skip_2);</span>
<span class="p_del">-EXPORT_SYMBOL(fixup_put_user_skip_1);</span>
<span class="p_del">-EXPORT_SYMBOL(fixup_put_user_skip_2);</span>
<span class="p_del">-</span>
 #ifndef CONFIG_64BIT
 /* Needed so insmod can set dp value */
 extern int $global$;
<span class="p_header">diff --git a/arch/parisc/kernel/process.c b/arch/parisc/kernel/process.c</span>
<span class="p_header">index e81afc378850..e7ffde2758fc 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/process.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/process.c</span>
<span class="p_chunk">@@ -140,6 +140,8 @@</span> <span class="p_context"> void machine_power_off(void)</span>
 	printk(KERN_EMERG &quot;System shut down completed.\n&quot;
 	       &quot;Please power this system off now.&quot;);
 
<span class="p_add">+	/* prevent soft lockup/stalled CPU messages for endless loop. */</span>
<span class="p_add">+	rcu_sysrq_start();</span>
 	for (;;);
 }
 
<span class="p_header">diff --git a/arch/parisc/lib/Makefile b/arch/parisc/lib/Makefile</span>
<span class="p_header">index 8fa92b8d839a..f2dac4d73b1b 100644</span>
<span class="p_header">--- a/arch/parisc/lib/Makefile</span>
<span class="p_header">+++ b/arch/parisc/lib/Makefile</span>
<span class="p_chunk">@@ -2,7 +2,7 @@</span> <span class="p_context"></span>
 # Makefile for parisc-specific library files
 #
 
<span class="p_del">-lib-y	:= lusercopy.o bitops.o checksum.o io.o memset.o fixup.o memcpy.o \</span>
<span class="p_add">+lib-y	:= lusercopy.o bitops.o checksum.o io.o memset.o memcpy.o \</span>
 	   ucmpdi2.o delay.o
 
 obj-y	:= iomap.o
<span class="p_header">diff --git a/arch/parisc/lib/fixup.S b/arch/parisc/lib/fixup.S</span>
deleted file mode 100644
<span class="p_header">index a5b72f22c7a6..000000000000</span>
<span class="p_header">--- a/arch/parisc/lib/fixup.S</span>
<span class="p_header">+++ /dev/null</span>
<span class="p_chunk">@@ -1,98 +0,0 @@</span> <span class="p_context"></span>
<span class="p_del">-/*</span>
<span class="p_del">- * Linux/PA-RISC Project (http://www.parisc-linux.org/)</span>
<span class="p_del">- *</span>
<span class="p_del">- *  Copyright (C) 2004  Randolph Chung &lt;tausq@debian.org&gt;</span>
<span class="p_del">- *</span>
<span class="p_del">- *    This program is free software; you can redistribute it and/or modify</span>
<span class="p_del">- *    it under the terms of the GNU General Public License as published by</span>
<span class="p_del">- *    the Free Software Foundation; either version 2, or (at your option)</span>
<span class="p_del">- *    any later version.</span>
<span class="p_del">- *</span>
<span class="p_del">- *    This program is distributed in the hope that it will be useful,</span>
<span class="p_del">- *    but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_del">- *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_del">- *    GNU General Public License for more details.</span>
<span class="p_del">- *</span>
<span class="p_del">- *    You should have received a copy of the GNU General Public License</span>
<span class="p_del">- *    along with this program; if not, write to the Free Software</span>
<span class="p_del">- *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.</span>
<span class="p_del">- * </span>
<span class="p_del">- * Fixup routines for kernel exception handling.</span>
<span class="p_del">- */</span>
<span class="p_del">-#include &lt;asm/asm-offsets.h&gt;</span>
<span class="p_del">-#include &lt;asm/assembly.h&gt;</span>
<span class="p_del">-#include &lt;asm/errno.h&gt;</span>
<span class="p_del">-#include &lt;linux/linkage.h&gt;</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_del">-	.macro  get_fault_ip t1 t2</span>
<span class="p_del">-	loadgp</span>
<span class="p_del">-	addil LT%__per_cpu_offset,%r27</span>
<span class="p_del">-	LDREG RT%__per_cpu_offset(%r1),\t1</span>
<span class="p_del">-	/* t2 = smp_processor_id() */</span>
<span class="p_del">-	mfctl 30,\t2</span>
<span class="p_del">-	ldw TI_CPU(\t2),\t2</span>
<span class="p_del">-#ifdef CONFIG_64BIT</span>
<span class="p_del">-	extrd,u \t2,63,32,\t2</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	/* t2 = &amp;__per_cpu_offset[smp_processor_id()]; */</span>
<span class="p_del">-	LDREGX \t2(\t1),\t2 </span>
<span class="p_del">-	addil LT%exception_data,%r27</span>
<span class="p_del">-	LDREG RT%exception_data(%r1),\t1</span>
<span class="p_del">-	/* t1 = this_cpu_ptr(&amp;exception_data) */</span>
<span class="p_del">-	add,l \t1,\t2,\t1</span>
<span class="p_del">-	/* %r27 = t1-&gt;fault_gp - restore gp */</span>
<span class="p_del">-	LDREG EXCDATA_GP(\t1), %r27</span>
<span class="p_del">-	/* t1 = t1-&gt;fault_ip */</span>
<span class="p_del">-	LDREG EXCDATA_IP(\t1), \t1</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-#else</span>
<span class="p_del">-	.macro  get_fault_ip t1 t2</span>
<span class="p_del">-	loadgp</span>
<span class="p_del">-	/* t1 = this_cpu_ptr(&amp;exception_data) */</span>
<span class="p_del">-	addil LT%exception_data,%r27</span>
<span class="p_del">-	LDREG RT%exception_data(%r1),\t2</span>
<span class="p_del">-	/* %r27 = t2-&gt;fault_gp - restore gp */</span>
<span class="p_del">-	LDREG EXCDATA_GP(\t2), %r27</span>
<span class="p_del">-	/* t1 = t2-&gt;fault_ip */</span>
<span class="p_del">-	LDREG EXCDATA_IP(\t2), \t1</span>
<span class="p_del">-	.endm</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-	.level LEVEL</span>
<span class="p_del">-</span>
<span class="p_del">-	.text</span>
<span class="p_del">-	.section .fixup, &quot;ax&quot;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* get_user() fixups, store -EFAULT in r8, and 0 in r9 */</span>
<span class="p_del">-ENTRY_CFI(fixup_get_user_skip_1)</span>
<span class="p_del">-	get_fault_ip %r1,%r8</span>
<span class="p_del">-	ldo 4(%r1), %r1</span>
<span class="p_del">-	ldi -EFAULT, %r8</span>
<span class="p_del">-	bv %r0(%r1)</span>
<span class="p_del">-	copy %r0, %r9</span>
<span class="p_del">-ENDPROC_CFI(fixup_get_user_skip_1)</span>
<span class="p_del">-</span>
<span class="p_del">-ENTRY_CFI(fixup_get_user_skip_2)</span>
<span class="p_del">-	get_fault_ip %r1,%r8</span>
<span class="p_del">-	ldo 8(%r1), %r1</span>
<span class="p_del">-	ldi -EFAULT, %r8</span>
<span class="p_del">-	bv %r0(%r1)</span>
<span class="p_del">-	copy %r0, %r9</span>
<span class="p_del">-ENDPROC_CFI(fixup_get_user_skip_2)</span>
<span class="p_del">-</span>
<span class="p_del">-	/* put_user() fixups, store -EFAULT in r8 */</span>
<span class="p_del">-ENTRY_CFI(fixup_put_user_skip_1)</span>
<span class="p_del">-	get_fault_ip %r1,%r8</span>
<span class="p_del">-	ldo 4(%r1), %r1</span>
<span class="p_del">-	bv %r0(%r1)</span>
<span class="p_del">-	ldi -EFAULT, %r8</span>
<span class="p_del">-ENDPROC_CFI(fixup_put_user_skip_1)</span>
<span class="p_del">-</span>
<span class="p_del">-ENTRY_CFI(fixup_put_user_skip_2)</span>
<span class="p_del">-	get_fault_ip %r1,%r8</span>
<span class="p_del">-	ldo 8(%r1), %r1</span>
<span class="p_del">-	bv %r0(%r1)</span>
<span class="p_del">-	ldi -EFAULT, %r8</span>
<span class="p_del">-ENDPROC_CFI(fixup_put_user_skip_2)</span>
<span class="p_del">-</span>
<span class="p_header">diff --git a/arch/parisc/lib/lusercopy.S b/arch/parisc/lib/lusercopy.S</span>
<span class="p_header">index 56845de6b5df..f01188c044ee 100644</span>
<span class="p_header">--- a/arch/parisc/lib/lusercopy.S</span>
<span class="p_header">+++ b/arch/parisc/lib/lusercopy.S</span>
<span class="p_chunk">@@ -5,6 +5,8 @@</span> <span class="p_context"></span>
  *    Copyright (C) 2000 Richard Hirst &lt;rhirst with parisc-linux.org&gt;
  *    Copyright (C) 2001 Matthieu Delahaye &lt;delahaym at esiee.fr&gt;
  *    Copyright (C) 2003 Randolph Chung &lt;tausq with parisc-linux.org&gt;
<span class="p_add">+ *    Copyright (C) 2017 Helge Deller &lt;deller@gmx.de&gt;</span>
<span class="p_add">+ *    Copyright (C) 2017 John David Anglin &lt;dave.anglin@bell.net&gt;</span>
  *
  *
  *    This program is free software; you can redistribute it and/or modify
<span class="p_chunk">@@ -132,4 +134,320 @@</span> <span class="p_context"> ENDPROC_CFI(lstrnlen_user)</span>
 
 	.procend
 
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * unsigned long pa_memcpy(void *dstp, const void *srcp, unsigned long len)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Inputs:</span>
<span class="p_add">+ * - sr1 already contains space of source region</span>
<span class="p_add">+ * - sr2 already contains space of destination region</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns:</span>
<span class="p_add">+ * - number of bytes that could not be copied.</span>
<span class="p_add">+ *   On success, this will be zero.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This code is based on a C-implementation of a copy routine written by</span>
<span class="p_add">+ * Randolph Chung, which in turn was derived from the glibc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Several strategies are tried to try to get the best performance for various</span>
<span class="p_add">+ * conditions. In the optimal case, we copy by loops that copy 32- or 16-bytes</span>
<span class="p_add">+ * at a time using general registers.  Unaligned copies are handled either by</span>
<span class="p_add">+ * aligning the destination and then using shift-and-write method, or in a few</span>
<span class="p_add">+ * cases by falling back to a byte-at-a-time copy.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Testing with various alignments and buffer sizes shows that this code is</span>
<span class="p_add">+ * often &gt;10x faster than a simple byte-at-a-time copy, even for strangely</span>
<span class="p_add">+ * aligned operands. It is interesting to note that the glibc version of memcpy</span>
<span class="p_add">+ * (written in C) is actually quite fast already. This routine is able to beat</span>
<span class="p_add">+ * it by 30-40% for aligned copies because of the loop unrolling, but in some</span>
<span class="p_add">+ * cases the glibc version is still slightly faster. This lends more</span>
<span class="p_add">+ * credibility that gcc can generate very good code as long as we are careful.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Possible optimizations:</span>
<span class="p_add">+ * - add cache prefetching</span>
<span class="p_add">+ * - try not to use the post-increment address modifiers; they may create</span>
<span class="p_add">+ *   additional interlocks. Assumption is that those were only efficient on old</span>
<span class="p_add">+ *   machines (pre PA8000 processors)</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+	dst = arg0</span>
<span class="p_add">+	src = arg1</span>
<span class="p_add">+	len = arg2</span>
<span class="p_add">+	end = arg3</span>
<span class="p_add">+	t1  = r19</span>
<span class="p_add">+	t2  = r20</span>
<span class="p_add">+	t3  = r21</span>
<span class="p_add">+	t4  = r22</span>
<span class="p_add">+	srcspc = sr1</span>
<span class="p_add">+	dstspc = sr2</span>
<span class="p_add">+</span>
<span class="p_add">+	t0 = r1</span>
<span class="p_add">+	a1 = t1</span>
<span class="p_add">+	a2 = t2</span>
<span class="p_add">+	a3 = t3</span>
<span class="p_add">+	a0 = t4</span>
<span class="p_add">+</span>
<span class="p_add">+	save_src = ret0</span>
<span class="p_add">+	save_dst = ret1</span>
<span class="p_add">+	save_len = r31</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY_CFI(pa_memcpy)</span>
<span class="p_add">+	.proc</span>
<span class="p_add">+	.callinfo NO_CALLS</span>
<span class="p_add">+	.entry</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Last destination address */</span>
<span class="p_add">+	add	dst,len,end</span>
<span class="p_add">+</span>
<span class="p_add">+	/* short copy with less than 16 bytes? */</span>
<span class="p_add">+	cmpib,&gt;&gt;=,n 15,len,.Lbyte_loop</span>
<span class="p_add">+</span>
<span class="p_add">+	/* same alignment? */</span>
<span class="p_add">+	xor	src,dst,t0</span>
<span class="p_add">+	extru	t0,31,2,t1</span>
<span class="p_add">+	cmpib,&lt;&gt;,n  0,t1,.Lunaligned_copy</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+	/* only do 64-bit copies if we can get aligned. */</span>
<span class="p_add">+	extru	t0,31,3,t1</span>
<span class="p_add">+	cmpib,&lt;&gt;,n  0,t1,.Lalign_loop32</span>
<span class="p_add">+</span>
<span class="p_add">+	/* loop until we are 64-bit aligned */</span>
<span class="p_add">+.Lalign_loop64:</span>
<span class="p_add">+	extru	dst,31,3,t1</span>
<span class="p_add">+	cmpib,=,n	0,t1,.Lcopy_loop_16</span>
<span class="p_add">+20:	ldb,ma	1(srcspc,src),t1</span>
<span class="p_add">+21:	stb,ma	t1,1(dstspc,dst)</span>
<span class="p_add">+	b	.Lalign_loop64</span>
<span class="p_add">+	ldo	-1(len),len</span>
<span class="p_add">+</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(20b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(21b,.Lcopy_done)</span>
<span class="p_add">+</span>
<span class="p_add">+	ldi	31,t0</span>
<span class="p_add">+.Lcopy_loop_16:</span>
<span class="p_add">+	cmpb,COND(&gt;&gt;=),n t0,len,.Lword_loop</span>
<span class="p_add">+</span>
<span class="p_add">+10:	ldd	0(srcspc,src),t1</span>
<span class="p_add">+11:	ldd	8(srcspc,src),t2</span>
<span class="p_add">+	ldo	16(src),src</span>
<span class="p_add">+12:	std,ma	t1,8(dstspc,dst)</span>
<span class="p_add">+13:	std,ma	t2,8(dstspc,dst)</span>
<span class="p_add">+14:	ldd	0(srcspc,src),t1</span>
<span class="p_add">+15:	ldd	8(srcspc,src),t2</span>
<span class="p_add">+	ldo	16(src),src</span>
<span class="p_add">+16:	std,ma	t1,8(dstspc,dst)</span>
<span class="p_add">+17:	std,ma	t2,8(dstspc,dst)</span>
<span class="p_add">+</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(10b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(11b,.Lcopy16_fault)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(12b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(13b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(14b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(15b,.Lcopy16_fault)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(16b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(17b,.Lcopy_done)</span>
<span class="p_add">+</span>
<span class="p_add">+	b	.Lcopy_loop_16</span>
<span class="p_add">+	ldo	-32(len),len</span>
<span class="p_add">+</span>
<span class="p_add">+.Lword_loop:</span>
<span class="p_add">+	cmpib,COND(&gt;&gt;=),n 3,len,.Lbyte_loop</span>
<span class="p_add">+20:	ldw,ma	4(srcspc,src),t1</span>
<span class="p_add">+21:	stw,ma	t1,4(dstspc,dst)</span>
<span class="p_add">+	b	.Lword_loop</span>
<span class="p_add">+	ldo	-4(len),len</span>
<span class="p_add">+</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(20b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(21b,.Lcopy_done)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_64BIT */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* loop until we are 32-bit aligned */</span>
<span class="p_add">+.Lalign_loop32:</span>
<span class="p_add">+	extru	dst,31,2,t1</span>
<span class="p_add">+	cmpib,=,n	0,t1,.Lcopy_loop_4</span>
<span class="p_add">+20:	ldb,ma	1(srcspc,src),t1</span>
<span class="p_add">+21:	stb,ma	t1,1(dstspc,dst)</span>
<span class="p_add">+	b	.Lalign_loop32</span>
<span class="p_add">+	ldo	-1(len),len</span>
<span class="p_add">+</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(20b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(21b,.Lcopy_done)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+.Lcopy_loop_4:</span>
<span class="p_add">+	cmpib,COND(&gt;&gt;=),n 15,len,.Lbyte_loop</span>
<span class="p_add">+</span>
<span class="p_add">+10:	ldw	0(srcspc,src),t1</span>
<span class="p_add">+11:	ldw	4(srcspc,src),t2</span>
<span class="p_add">+12:	stw,ma	t1,4(dstspc,dst)</span>
<span class="p_add">+13:	stw,ma	t2,4(dstspc,dst)</span>
<span class="p_add">+14:	ldw	8(srcspc,src),t1</span>
<span class="p_add">+15:	ldw	12(srcspc,src),t2</span>
<span class="p_add">+	ldo	16(src),src</span>
<span class="p_add">+16:	stw,ma	t1,4(dstspc,dst)</span>
<span class="p_add">+17:	stw,ma	t2,4(dstspc,dst)</span>
<span class="p_add">+</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(10b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(11b,.Lcopy8_fault)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(12b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(13b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(14b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(15b,.Lcopy8_fault)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(16b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(17b,.Lcopy_done)</span>
<span class="p_add">+</span>
<span class="p_add">+	b	.Lcopy_loop_4</span>
<span class="p_add">+	ldo	-16(len),len</span>
<span class="p_add">+</span>
<span class="p_add">+.Lbyte_loop:</span>
<span class="p_add">+	cmpclr,COND(&lt;&gt;) len,%r0,%r0</span>
<span class="p_add">+	b,n	.Lcopy_done</span>
<span class="p_add">+20:	ldb	0(srcspc,src),t1</span>
<span class="p_add">+	ldo	1(src),src</span>
<span class="p_add">+21:	stb,ma	t1,1(dstspc,dst)</span>
<span class="p_add">+	b	.Lbyte_loop</span>
<span class="p_add">+	ldo	-1(len),len</span>
<span class="p_add">+</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(20b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(21b,.Lcopy_done)</span>
<span class="p_add">+</span>
<span class="p_add">+.Lcopy_done:</span>
<span class="p_add">+	bv	%r0(%r2)</span>
<span class="p_add">+	sub	end,dst,ret0</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+	/* src and dst are not aligned the same way. */</span>
<span class="p_add">+	/* need to go the hard way */</span>
<span class="p_add">+.Lunaligned_copy:</span>
<span class="p_add">+	/* align until dst is 32bit-word-aligned */</span>
<span class="p_add">+	extru	dst,31,2,t1</span>
<span class="p_add">+	cmpib,COND(=),n	0,t1,.Lcopy_dstaligned</span>
<span class="p_add">+20:	ldb	0(srcspc,src),t1</span>
<span class="p_add">+	ldo	1(src),src</span>
<span class="p_add">+21:	stb,ma	t1,1(dstspc,dst)</span>
<span class="p_add">+	b	.Lunaligned_copy</span>
<span class="p_add">+	ldo	-1(len),len</span>
<span class="p_add">+</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(20b,.Lcopy_done)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(21b,.Lcopy_done)</span>
<span class="p_add">+</span>
<span class="p_add">+.Lcopy_dstaligned:</span>
<span class="p_add">+</span>
<span class="p_add">+	/* store src, dst and len in safe place */</span>
<span class="p_add">+	copy	src,save_src</span>
<span class="p_add">+	copy	dst,save_dst</span>
<span class="p_add">+	copy	len,save_len</span>
<span class="p_add">+</span>
<span class="p_add">+	/* len now needs give number of words to copy */</span>
<span class="p_add">+	SHRREG	len,2,len</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Copy from a not-aligned src to an aligned dst using shifts.</span>
<span class="p_add">+	 * Handles 4 words per loop.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	depw,z src,28,2,t0</span>
<span class="p_add">+	subi 32,t0,t0</span>
<span class="p_add">+	mtsar t0</span>
<span class="p_add">+	extru len,31,2,t0</span>
<span class="p_add">+	cmpib,= 2,t0,.Lcase2</span>
<span class="p_add">+	/* Make src aligned by rounding it down.  */</span>
<span class="p_add">+	depi 0,31,2,src</span>
<span class="p_add">+</span>
<span class="p_add">+	cmpiclr,&lt;&gt; 3,t0,%r0</span>
<span class="p_add">+	b,n .Lcase3</span>
<span class="p_add">+	cmpiclr,&lt;&gt; 1,t0,%r0</span>
<span class="p_add">+	b,n .Lcase1</span>
<span class="p_add">+.Lcase0:</span>
<span class="p_add">+	cmpb,= %r0,len,.Lcda_finish</span>
<span class="p_add">+	nop</span>
<span class="p_add">+</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a3</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a0</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+	b,n .Ldo3</span>
<span class="p_add">+.Lcase1:</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a2</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a3</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+	ldo -1(len),len</span>
<span class="p_add">+	cmpb,=,n %r0,len,.Ldo0</span>
<span class="p_add">+.Ldo4:</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a0</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+	shrpw a2, a3, %sar, t0</span>
<span class="p_add">+1:	stw,ma t0, 4(dstspc,dst)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcopy_done)</span>
<span class="p_add">+.Ldo3:</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a1</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+	shrpw a3, a0, %sar, t0</span>
<span class="p_add">+1:	stw,ma t0, 4(dstspc,dst)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcopy_done)</span>
<span class="p_add">+.Ldo2:</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a2</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+	shrpw a0, a1, %sar, t0</span>
<span class="p_add">+1:	stw,ma t0, 4(dstspc,dst)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcopy_done)</span>
<span class="p_add">+.Ldo1:</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a3</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+	shrpw a1, a2, %sar, t0</span>
<span class="p_add">+1:	stw,ma t0, 4(dstspc,dst)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcopy_done)</span>
<span class="p_add">+	ldo -4(len),len</span>
<span class="p_add">+	cmpb,&lt;&gt; %r0,len,.Ldo4</span>
<span class="p_add">+	nop</span>
<span class="p_add">+.Ldo0:</span>
<span class="p_add">+	shrpw a2, a3, %sar, t0</span>
<span class="p_add">+1:	stw,ma t0, 4(dstspc,dst)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcopy_done)</span>
<span class="p_add">+</span>
<span class="p_add">+.Lcda_rdfault:</span>
<span class="p_add">+.Lcda_finish:</span>
<span class="p_add">+	/* calculate new src, dst and len and jump to byte-copy loop */</span>
<span class="p_add">+	sub	dst,save_dst,t0</span>
<span class="p_add">+	add	save_src,t0,src</span>
<span class="p_add">+	b	.Lbyte_loop</span>
<span class="p_add">+	sub	save_len,t0,len</span>
<span class="p_add">+</span>
<span class="p_add">+.Lcase3:</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a0</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a1</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+	b .Ldo2</span>
<span class="p_add">+	ldo 1(len),len</span>
<span class="p_add">+.Lcase2:</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a1</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+1:	ldw,ma 4(srcspc,src), a2</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(1b,.Lcda_rdfault)</span>
<span class="p_add">+	b .Ldo1</span>
<span class="p_add">+	ldo 2(len),len</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+	/* fault exception fixup handlers: */</span>
<span class="p_add">+#ifdef CONFIG_64BIT</span>
<span class="p_add">+.Lcopy16_fault:</span>
<span class="p_add">+10:	b	.Lcopy_done</span>
<span class="p_add">+	std,ma	t1,8(dstspc,dst)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(10b,.Lcopy_done)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+.Lcopy8_fault:</span>
<span class="p_add">+10:	b	.Lcopy_done</span>
<span class="p_add">+	stw,ma	t1,4(dstspc,dst)</span>
<span class="p_add">+	ASM_EXCEPTIONTABLE_ENTRY(10b,.Lcopy_done)</span>
<span class="p_add">+</span>
<span class="p_add">+	.exit</span>
<span class="p_add">+ENDPROC_CFI(pa_memcpy)</span>
<span class="p_add">+	.procend</span>
<span class="p_add">+</span>
 	.end
<span class="p_header">diff --git a/arch/parisc/lib/memcpy.c b/arch/parisc/lib/memcpy.c</span>
<span class="p_header">index f82ff10ed974..b3d47ec1d80a 100644</span>
<span class="p_header">--- a/arch/parisc/lib/memcpy.c</span>
<span class="p_header">+++ b/arch/parisc/lib/memcpy.c</span>
<span class="p_chunk">@@ -2,7 +2,7 @@</span> <span class="p_context"></span>
  *    Optimized memory copy routines.
  *
  *    Copyright (C) 2004 Randolph Chung &lt;tausq@debian.org&gt;
<span class="p_del">- *    Copyright (C) 2013 Helge Deller &lt;deller@gmx.de&gt;</span>
<span class="p_add">+ *    Copyright (C) 2013-2017 Helge Deller &lt;deller@gmx.de&gt;</span>
  *
  *    This program is free software; you can redistribute it and/or modify
  *    it under the terms of the GNU General Public License as published by
<span class="p_chunk">@@ -21,474 +21,21 @@</span> <span class="p_context"></span>
  *    Portions derived from the GNU C Library
  *    Copyright (C) 1991, 1997, 2003 Free Software Foundation, Inc.
  *
<span class="p_del">- * Several strategies are tried to try to get the best performance for various</span>
<span class="p_del">- * conditions. In the optimal case, we copy 64-bytes in an unrolled loop using </span>
<span class="p_del">- * fp regs. This is followed by loops that copy 32- or 16-bytes at a time using</span>
<span class="p_del">- * general registers.  Unaligned copies are handled either by aligning the </span>
<span class="p_del">- * destination and then using shift-and-write method, or in a few cases by </span>
<span class="p_del">- * falling back to a byte-at-a-time copy.</span>
<span class="p_del">- *</span>
<span class="p_del">- * I chose to implement this in C because it is easier to maintain and debug,</span>
<span class="p_del">- * and in my experiments it appears that the C code generated by gcc (3.3/3.4</span>
<span class="p_del">- * at the time of writing) is fairly optimal. Unfortunately some of the </span>
<span class="p_del">- * semantics of the copy routine (exception handling) is difficult to express</span>
<span class="p_del">- * in C, so we have to play some tricks to get it to work.</span>
<span class="p_del">- *</span>
<span class="p_del">- * All the loads and stores are done via explicit asm() code in order to use</span>
<span class="p_del">- * the right space registers. </span>
<span class="p_del">- * </span>
<span class="p_del">- * Testing with various alignments and buffer sizes shows that this code is </span>
<span class="p_del">- * often &gt;10x faster than a simple byte-at-a-time copy, even for strangely</span>
<span class="p_del">- * aligned operands. It is interesting to note that the glibc version</span>
<span class="p_del">- * of memcpy (written in C) is actually quite fast already. This routine is </span>
<span class="p_del">- * able to beat it by 30-40% for aligned copies because of the loop unrolling, </span>
<span class="p_del">- * but in some cases the glibc version is still slightly faster. This lends </span>
<span class="p_del">- * more credibility that gcc can generate very good code as long as we are </span>
<span class="p_del">- * careful.</span>
<span class="p_del">- *</span>
<span class="p_del">- * TODO:</span>
<span class="p_del">- * - cache prefetching needs more experimentation to get optimal settings</span>
<span class="p_del">- * - try not to use the post-increment address modifiers; they create additional</span>
<span class="p_del">- *   interlocks</span>
<span class="p_del">- * - replace byte-copy loops with stybs sequences</span>
  */
 
<span class="p_del">-#ifdef __KERNEL__</span>
 #include &lt;linux/module.h&gt;
 #include &lt;linux/compiler.h&gt;
 #include &lt;linux/uaccess.h&gt;
<span class="p_del">-#define s_space &quot;%%sr1&quot;</span>
<span class="p_del">-#define d_space &quot;%%sr2&quot;</span>
<span class="p_del">-#else</span>
<span class="p_del">-#include &quot;memcpy.h&quot;</span>
<span class="p_del">-#define s_space &quot;%%sr0&quot;</span>
<span class="p_del">-#define d_space &quot;%%sr0&quot;</span>
<span class="p_del">-#define pa_memcpy new2_copy</span>
<span class="p_del">-#endif</span>
 
 DECLARE_PER_CPU(struct exception_data, exception_data);
 
<span class="p_del">-#define preserve_branch(label)	do {					\</span>
<span class="p_del">-	volatile int dummy = 0;						\</span>
<span class="p_del">-	/* The following branch is never taken, it&#39;s just here to  */	\</span>
<span class="p_del">-	/* prevent gcc from optimizing away our exception code. */ 	\</span>
<span class="p_del">-	if (unlikely(dummy != dummy))					\</span>
<span class="p_del">-		goto label;						\</span>
<span class="p_del">-} while (0)</span>
<span class="p_del">-</span>
 #define get_user_space() (segment_eq(get_fs(), KERNEL_DS) ? 0 : mfsp(3))
 #define get_kernel_space() (0)
 
<span class="p_del">-#define MERGE(w0, sh_1, w1, sh_2)  ({					\</span>
<span class="p_del">-	unsigned int _r;						\</span>
<span class="p_del">-	asm volatile (							\</span>
<span class="p_del">-	&quot;mtsar %3\n&quot;							\</span>
<span class="p_del">-	&quot;shrpw %1, %2, %%sar, %0\n&quot;					\</span>
<span class="p_del">-	: &quot;=r&quot;(_r)							\</span>
<span class="p_del">-	: &quot;r&quot;(w0), &quot;r&quot;(w1), &quot;r&quot;(sh_2)					\</span>
<span class="p_del">-	);								\</span>
<span class="p_del">-	_r;								\</span>
<span class="p_del">-})</span>
<span class="p_del">-#define THRESHOLD	16</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef DEBUG_MEMCPY</span>
<span class="p_del">-#define DPRINTF(fmt, args...) do { printk(KERN_DEBUG &quot;%s:%d:%s &quot;, __FILE__, __LINE__, __func__ ); printk(KERN_DEBUG fmt, ##args ); } while (0)</span>
<span class="p_del">-#else</span>
<span class="p_del">-#define DPRINTF(fmt, args...)</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#define def_load_ai_insn(_insn,_sz,_tt,_s,_a,_t,_e)	\</span>
<span class="p_del">-	__asm__ __volatile__ (				\</span>
<span class="p_del">-	&quot;1:\t&quot; #_insn &quot;,ma &quot; #_sz &quot;(&quot; _s &quot;,%1), %0\n\t&quot;	\</span>
<span class="p_del">-	ASM_EXCEPTIONTABLE_ENTRY(1b,_e)			\</span>
<span class="p_del">-	: _tt(_t), &quot;+r&quot;(_a)				\</span>
<span class="p_del">-	: 						\</span>
<span class="p_del">-	: &quot;r8&quot;)</span>
<span class="p_del">-</span>
<span class="p_del">-#define def_store_ai_insn(_insn,_sz,_tt,_s,_a,_t,_e) 	\</span>
<span class="p_del">-	__asm__ __volatile__ (				\</span>
<span class="p_del">-	&quot;1:\t&quot; #_insn &quot;,ma %1, &quot; #_sz &quot;(&quot; _s &quot;,%0)\n\t&quot;	\</span>
<span class="p_del">-	ASM_EXCEPTIONTABLE_ENTRY(1b,_e)			\</span>
<span class="p_del">-	: &quot;+r&quot;(_a) 					\</span>
<span class="p_del">-	: _tt(_t)					\</span>
<span class="p_del">-	: &quot;r8&quot;)</span>
<span class="p_del">-</span>
<span class="p_del">-#define ldbma(_s, _a, _t, _e) def_load_ai_insn(ldbs,1,&quot;=r&quot;,_s,_a,_t,_e)</span>
<span class="p_del">-#define stbma(_s, _t, _a, _e) def_store_ai_insn(stbs,1,&quot;r&quot;,_s,_a,_t,_e)</span>
<span class="p_del">-#define ldwma(_s, _a, _t, _e) def_load_ai_insn(ldw,4,&quot;=r&quot;,_s,_a,_t,_e)</span>
<span class="p_del">-#define stwma(_s, _t, _a, _e) def_store_ai_insn(stw,4,&quot;r&quot;,_s,_a,_t,_e)</span>
<span class="p_del">-#define flddma(_s, _a, _t, _e) def_load_ai_insn(fldd,8,&quot;=f&quot;,_s,_a,_t,_e)</span>
<span class="p_del">-#define fstdma(_s, _t, _a, _e) def_store_ai_insn(fstd,8,&quot;f&quot;,_s,_a,_t,_e)</span>
<span class="p_del">-</span>
<span class="p_del">-#define def_load_insn(_insn,_tt,_s,_o,_a,_t,_e) 	\</span>
<span class="p_del">-	__asm__ __volatile__ (				\</span>
<span class="p_del">-	&quot;1:\t&quot; #_insn &quot; &quot; #_o &quot;(&quot; _s &quot;,%1), %0\n\t&quot;	\</span>
<span class="p_del">-	ASM_EXCEPTIONTABLE_ENTRY(1b,_e)			\</span>
<span class="p_del">-	: _tt(_t) 					\</span>
<span class="p_del">-	: &quot;r&quot;(_a)					\</span>
<span class="p_del">-	: &quot;r8&quot;)</span>
<span class="p_del">-</span>
<span class="p_del">-#define def_store_insn(_insn,_tt,_s,_t,_o,_a,_e) 	\</span>
<span class="p_del">-	__asm__ __volatile__ (				\</span>
<span class="p_del">-	&quot;1:\t&quot; #_insn &quot; %0, &quot; #_o &quot;(&quot; _s &quot;,%1)\n\t&quot; 	\</span>
<span class="p_del">-	ASM_EXCEPTIONTABLE_ENTRY(1b,_e)			\</span>
<span class="p_del">-	: 						\</span>
<span class="p_del">-	: _tt(_t), &quot;r&quot;(_a)				\</span>
<span class="p_del">-	: &quot;r8&quot;)</span>
<span class="p_del">-</span>
<span class="p_del">-#define ldw(_s,_o,_a,_t,_e)	def_load_insn(ldw,&quot;=r&quot;,_s,_o,_a,_t,_e)</span>
<span class="p_del">-#define stw(_s,_t,_o,_a,_e) 	def_store_insn(stw,&quot;r&quot;,_s,_t,_o,_a,_e)</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef  CONFIG_PREFETCH</span>
<span class="p_del">-static inline void prefetch_src(const void *addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__asm__(&quot;ldw 0(&quot; s_space &quot;,%0), %%r0&quot; : : &quot;r&quot; (addr));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void prefetch_dst(const void *addr)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__asm__(&quot;ldd 0(&quot; d_space &quot;,%0), %%r0&quot; : : &quot;r&quot; (addr));</span>
<span class="p_del">-}</span>
<span class="p_del">-#else</span>
<span class="p_del">-#define prefetch_src(addr) do { } while(0)</span>
<span class="p_del">-#define prefetch_dst(addr) do { } while(0)</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#define PA_MEMCPY_OK		0</span>
<span class="p_del">-#define PA_MEMCPY_LOAD_ERROR	1</span>
<span class="p_del">-#define PA_MEMCPY_STORE_ERROR	2</span>
<span class="p_del">-</span>
<span class="p_del">-/* Copy from a not-aligned src to an aligned dst, using shifts. Handles 4 words</span>
<span class="p_del">- * per loop.  This code is derived from glibc. </span>
<span class="p_del">- */</span>
<span class="p_del">-static noinline unsigned long copy_dstaligned(unsigned long dst,</span>
<span class="p_del">-					unsigned long src, unsigned long len)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/* gcc complains that a2 and a3 may be uninitialized, but actually</span>
<span class="p_del">-	 * they cannot be.  Initialize a2/a3 to shut gcc up.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	register unsigned int a0, a1, a2 = 0, a3 = 0;</span>
<span class="p_del">-	int sh_1, sh_2;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* prefetch_src((const void *)src); */</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Calculate how to shift a word read at the memory operation</span>
<span class="p_del">-	   aligned srcp to make it aligned for copy.  */</span>
<span class="p_del">-	sh_1 = 8 * (src % sizeof(unsigned int));</span>
<span class="p_del">-	sh_2 = 8 * sizeof(unsigned int) - sh_1;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Make src aligned by rounding it down.  */</span>
<span class="p_del">-	src &amp;= -sizeof(unsigned int);</span>
<span class="p_del">-</span>
<span class="p_del">-	switch (len % 4)</span>
<span class="p_del">-	{</span>
<span class="p_del">-		case 2:</span>
<span class="p_del">-			/* a1 = ((unsigned int *) src)[0];</span>
<span class="p_del">-			   a2 = ((unsigned int *) src)[1]; */</span>
<span class="p_del">-			ldw(s_space, 0, src, a1, cda_ldw_exc);</span>
<span class="p_del">-			ldw(s_space, 4, src, a2, cda_ldw_exc);</span>
<span class="p_del">-			src -= 1 * sizeof(unsigned int);</span>
<span class="p_del">-			dst -= 3 * sizeof(unsigned int);</span>
<span class="p_del">-			len += 2;</span>
<span class="p_del">-			goto do1;</span>
<span class="p_del">-		case 3:</span>
<span class="p_del">-			/* a0 = ((unsigned int *) src)[0];</span>
<span class="p_del">-			   a1 = ((unsigned int *) src)[1]; */</span>
<span class="p_del">-			ldw(s_space, 0, src, a0, cda_ldw_exc);</span>
<span class="p_del">-			ldw(s_space, 4, src, a1, cda_ldw_exc);</span>
<span class="p_del">-			src -= 0 * sizeof(unsigned int);</span>
<span class="p_del">-			dst -= 2 * sizeof(unsigned int);</span>
<span class="p_del">-			len += 1;</span>
<span class="p_del">-			goto do2;</span>
<span class="p_del">-		case 0:</span>
<span class="p_del">-			if (len == 0)</span>
<span class="p_del">-				return PA_MEMCPY_OK;</span>
<span class="p_del">-			/* a3 = ((unsigned int *) src)[0];</span>
<span class="p_del">-			   a0 = ((unsigned int *) src)[1]; */</span>
<span class="p_del">-			ldw(s_space, 0, src, a3, cda_ldw_exc);</span>
<span class="p_del">-			ldw(s_space, 4, src, a0, cda_ldw_exc);</span>
<span class="p_del">-			src -=-1 * sizeof(unsigned int);</span>
<span class="p_del">-			dst -= 1 * sizeof(unsigned int);</span>
<span class="p_del">-			len += 0;</span>
<span class="p_del">-			goto do3;</span>
<span class="p_del">-		case 1:</span>
<span class="p_del">-			/* a2 = ((unsigned int *) src)[0];</span>
<span class="p_del">-			   a3 = ((unsigned int *) src)[1]; */</span>
<span class="p_del">-			ldw(s_space, 0, src, a2, cda_ldw_exc);</span>
<span class="p_del">-			ldw(s_space, 4, src, a3, cda_ldw_exc);</span>
<span class="p_del">-			src -=-2 * sizeof(unsigned int);</span>
<span class="p_del">-			dst -= 0 * sizeof(unsigned int);</span>
<span class="p_del">-			len -= 1;</span>
<span class="p_del">-			if (len == 0)</span>
<span class="p_del">-				goto do0;</span>
<span class="p_del">-			goto do4;			/* No-op.  */</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	do</span>
<span class="p_del">-	{</span>
<span class="p_del">-		/* prefetch_src((const void *)(src + 4 * sizeof(unsigned int))); */</span>
<span class="p_del">-do4:</span>
<span class="p_del">-		/* a0 = ((unsigned int *) src)[0]; */</span>
<span class="p_del">-		ldw(s_space, 0, src, a0, cda_ldw_exc);</span>
<span class="p_del">-		/* ((unsigned int *) dst)[0] = MERGE (a2, sh_1, a3, sh_2); */</span>
<span class="p_del">-		stw(d_space, MERGE (a2, sh_1, a3, sh_2), 0, dst, cda_stw_exc);</span>
<span class="p_del">-do3:</span>
<span class="p_del">-		/* a1 = ((unsigned int *) src)[1]; */</span>
<span class="p_del">-		ldw(s_space, 4, src, a1, cda_ldw_exc);</span>
<span class="p_del">-		/* ((unsigned int *) dst)[1] = MERGE (a3, sh_1, a0, sh_2); */</span>
<span class="p_del">-		stw(d_space, MERGE (a3, sh_1, a0, sh_2), 4, dst, cda_stw_exc);</span>
<span class="p_del">-do2:</span>
<span class="p_del">-		/* a2 = ((unsigned int *) src)[2]; */</span>
<span class="p_del">-		ldw(s_space, 8, src, a2, cda_ldw_exc);</span>
<span class="p_del">-		/* ((unsigned int *) dst)[2] = MERGE (a0, sh_1, a1, sh_2); */</span>
<span class="p_del">-		stw(d_space, MERGE (a0, sh_1, a1, sh_2), 8, dst, cda_stw_exc);</span>
<span class="p_del">-do1:</span>
<span class="p_del">-		/* a3 = ((unsigned int *) src)[3]; */</span>
<span class="p_del">-		ldw(s_space, 12, src, a3, cda_ldw_exc);</span>
<span class="p_del">-		/* ((unsigned int *) dst)[3] = MERGE (a1, sh_1, a2, sh_2); */</span>
<span class="p_del">-		stw(d_space, MERGE (a1, sh_1, a2, sh_2), 12, dst, cda_stw_exc);</span>
<span class="p_del">-</span>
<span class="p_del">-		src += 4 * sizeof(unsigned int);</span>
<span class="p_del">-		dst += 4 * sizeof(unsigned int);</span>
<span class="p_del">-		len -= 4;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	while (len != 0);</span>
<span class="p_del">-</span>
<span class="p_del">-do0:</span>
<span class="p_del">-	/* ((unsigned int *) dst)[0] = MERGE (a2, sh_1, a3, sh_2); */</span>
<span class="p_del">-	stw(d_space, MERGE (a2, sh_1, a3, sh_2), 0, dst, cda_stw_exc);</span>
<span class="p_del">-</span>
<span class="p_del">-	preserve_branch(handle_load_error);</span>
<span class="p_del">-	preserve_branch(handle_store_error);</span>
<span class="p_del">-</span>
<span class="p_del">-	return PA_MEMCPY_OK;</span>
<span class="p_del">-</span>
<span class="p_del">-handle_load_error:</span>
<span class="p_del">-	__asm__ __volatile__ (&quot;cda_ldw_exc:\n&quot;);</span>
<span class="p_del">-	return PA_MEMCPY_LOAD_ERROR;</span>
<span class="p_del">-</span>
<span class="p_del">-handle_store_error:</span>
<span class="p_del">-	__asm__ __volatile__ (&quot;cda_stw_exc:\n&quot;);</span>
<span class="p_del">-	return PA_MEMCPY_STORE_ERROR;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-/* Returns PA_MEMCPY_OK, PA_MEMCPY_LOAD_ERROR or PA_MEMCPY_STORE_ERROR.</span>
<span class="p_del">- * In case of an access fault the faulty address can be read from the per_cpu</span>
<span class="p_del">- * exception data struct. */</span>
<span class="p_del">-static noinline unsigned long pa_memcpy_internal(void *dstp, const void *srcp,</span>
<span class="p_del">-					unsigned long len)</span>
<span class="p_del">-{</span>
<span class="p_del">-	register unsigned long src, dst, t1, t2, t3;</span>
<span class="p_del">-	register unsigned char *pcs, *pcd;</span>
<span class="p_del">-	register unsigned int *pws, *pwd;</span>
<span class="p_del">-	register double *pds, *pdd;</span>
<span class="p_del">-	unsigned long ret;</span>
<span class="p_del">-</span>
<span class="p_del">-	src = (unsigned long)srcp;</span>
<span class="p_del">-	dst = (unsigned long)dstp;</span>
<span class="p_del">-	pcs = (unsigned char *)srcp;</span>
<span class="p_del">-	pcd = (unsigned char *)dstp;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* prefetch_src((const void *)srcp); */</span>
<span class="p_del">-</span>
<span class="p_del">-	if (len &lt; THRESHOLD)</span>
<span class="p_del">-		goto byte_copy;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Check alignment */</span>
<span class="p_del">-	t1 = (src ^ dst);</span>
<span class="p_del">-	if (unlikely(t1 &amp; (sizeof(double)-1)))</span>
<span class="p_del">-		goto unaligned_copy;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* src and dst have same alignment. */</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Copy bytes till we are double-aligned. */</span>
<span class="p_del">-	t2 = src &amp; (sizeof(double) - 1);</span>
<span class="p_del">-	if (unlikely(t2 != 0)) {</span>
<span class="p_del">-		t2 = sizeof(double) - t2;</span>
<span class="p_del">-		while (t2 &amp;&amp; len) {</span>
<span class="p_del">-			/* *pcd++ = *pcs++; */</span>
<span class="p_del">-			ldbma(s_space, pcs, t3, pmc_load_exc);</span>
<span class="p_del">-			len--;</span>
<span class="p_del">-			stbma(d_space, t3, pcd, pmc_store_exc);</span>
<span class="p_del">-			t2--;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	pds = (double *)pcs;</span>
<span class="p_del">-	pdd = (double *)pcd;</span>
<span class="p_del">-</span>
<span class="p_del">-#if 0</span>
<span class="p_del">-	/* Copy 8 doubles at a time */</span>
<span class="p_del">-	while (len &gt;= 8*sizeof(double)) {</span>
<span class="p_del">-		register double r1, r2, r3, r4, r5, r6, r7, r8;</span>
<span class="p_del">-		/* prefetch_src((char *)pds + L1_CACHE_BYTES); */</span>
<span class="p_del">-		flddma(s_space, pds, r1, pmc_load_exc);</span>
<span class="p_del">-		flddma(s_space, pds, r2, pmc_load_exc);</span>
<span class="p_del">-		flddma(s_space, pds, r3, pmc_load_exc);</span>
<span class="p_del">-		flddma(s_space, pds, r4, pmc_load_exc);</span>
<span class="p_del">-		fstdma(d_space, r1, pdd, pmc_store_exc);</span>
<span class="p_del">-		fstdma(d_space, r2, pdd, pmc_store_exc);</span>
<span class="p_del">-		fstdma(d_space, r3, pdd, pmc_store_exc);</span>
<span class="p_del">-		fstdma(d_space, r4, pdd, pmc_store_exc);</span>
<span class="p_del">-</span>
<span class="p_del">-#if 0</span>
<span class="p_del">-		if (L1_CACHE_BYTES &lt;= 32)</span>
<span class="p_del">-			prefetch_src((char *)pds + L1_CACHE_BYTES);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-		flddma(s_space, pds, r5, pmc_load_exc);</span>
<span class="p_del">-		flddma(s_space, pds, r6, pmc_load_exc);</span>
<span class="p_del">-		flddma(s_space, pds, r7, pmc_load_exc);</span>
<span class="p_del">-		flddma(s_space, pds, r8, pmc_load_exc);</span>
<span class="p_del">-		fstdma(d_space, r5, pdd, pmc_store_exc);</span>
<span class="p_del">-		fstdma(d_space, r6, pdd, pmc_store_exc);</span>
<span class="p_del">-		fstdma(d_space, r7, pdd, pmc_store_exc);</span>
<span class="p_del">-		fstdma(d_space, r8, pdd, pmc_store_exc);</span>
<span class="p_del">-		len -= 8*sizeof(double);</span>
<span class="p_del">-	}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-	pws = (unsigned int *)pds;</span>
<span class="p_del">-	pwd = (unsigned int *)pdd;</span>
<span class="p_del">-</span>
<span class="p_del">-word_copy:</span>
<span class="p_del">-	while (len &gt;= 8*sizeof(unsigned int)) {</span>
<span class="p_del">-		register unsigned int r1,r2,r3,r4,r5,r6,r7,r8;</span>
<span class="p_del">-		/* prefetch_src((char *)pws + L1_CACHE_BYTES); */</span>
<span class="p_del">-		ldwma(s_space, pws, r1, pmc_load_exc);</span>
<span class="p_del">-		ldwma(s_space, pws, r2, pmc_load_exc);</span>
<span class="p_del">-		ldwma(s_space, pws, r3, pmc_load_exc);</span>
<span class="p_del">-		ldwma(s_space, pws, r4, pmc_load_exc);</span>
<span class="p_del">-		stwma(d_space, r1, pwd, pmc_store_exc);</span>
<span class="p_del">-		stwma(d_space, r2, pwd, pmc_store_exc);</span>
<span class="p_del">-		stwma(d_space, r3, pwd, pmc_store_exc);</span>
<span class="p_del">-		stwma(d_space, r4, pwd, pmc_store_exc);</span>
<span class="p_del">-</span>
<span class="p_del">-		ldwma(s_space, pws, r5, pmc_load_exc);</span>
<span class="p_del">-		ldwma(s_space, pws, r6, pmc_load_exc);</span>
<span class="p_del">-		ldwma(s_space, pws, r7, pmc_load_exc);</span>
<span class="p_del">-		ldwma(s_space, pws, r8, pmc_load_exc);</span>
<span class="p_del">-		stwma(d_space, r5, pwd, pmc_store_exc);</span>
<span class="p_del">-		stwma(d_space, r6, pwd, pmc_store_exc);</span>
<span class="p_del">-		stwma(d_space, r7, pwd, pmc_store_exc);</span>
<span class="p_del">-		stwma(d_space, r8, pwd, pmc_store_exc);</span>
<span class="p_del">-		len -= 8*sizeof(unsigned int);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	while (len &gt;= 4*sizeof(unsigned int)) {</span>
<span class="p_del">-		register unsigned int r1,r2,r3,r4;</span>
<span class="p_del">-		ldwma(s_space, pws, r1, pmc_load_exc);</span>
<span class="p_del">-		ldwma(s_space, pws, r2, pmc_load_exc);</span>
<span class="p_del">-		ldwma(s_space, pws, r3, pmc_load_exc);</span>
<span class="p_del">-		ldwma(s_space, pws, r4, pmc_load_exc);</span>
<span class="p_del">-		stwma(d_space, r1, pwd, pmc_store_exc);</span>
<span class="p_del">-		stwma(d_space, r2, pwd, pmc_store_exc);</span>
<span class="p_del">-		stwma(d_space, r3, pwd, pmc_store_exc);</span>
<span class="p_del">-		stwma(d_space, r4, pwd, pmc_store_exc);</span>
<span class="p_del">-		len -= 4*sizeof(unsigned int);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	pcs = (unsigned char *)pws;</span>
<span class="p_del">-	pcd = (unsigned char *)pwd;</span>
<span class="p_del">-</span>
<span class="p_del">-byte_copy:</span>
<span class="p_del">-	while (len) {</span>
<span class="p_del">-		/* *pcd++ = *pcs++; */</span>
<span class="p_del">-		ldbma(s_space, pcs, t3, pmc_load_exc);</span>
<span class="p_del">-		stbma(d_space, t3, pcd, pmc_store_exc);</span>
<span class="p_del">-		len--;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return PA_MEMCPY_OK;</span>
<span class="p_del">-</span>
<span class="p_del">-unaligned_copy:</span>
<span class="p_del">-	/* possibly we are aligned on a word, but not on a double... */</span>
<span class="p_del">-	if (likely((t1 &amp; (sizeof(unsigned int)-1)) == 0)) {</span>
<span class="p_del">-		t2 = src &amp; (sizeof(unsigned int) - 1);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (unlikely(t2 != 0)) {</span>
<span class="p_del">-			t2 = sizeof(unsigned int) - t2;</span>
<span class="p_del">-			while (t2) {</span>
<span class="p_del">-				/* *pcd++ = *pcs++; */</span>
<span class="p_del">-				ldbma(s_space, pcs, t3, pmc_load_exc);</span>
<span class="p_del">-				stbma(d_space, t3, pcd, pmc_store_exc);</span>
<span class="p_del">-				len--;</span>
<span class="p_del">-				t2--;</span>
<span class="p_del">-			}</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		pws = (unsigned int *)pcs;</span>
<span class="p_del">-		pwd = (unsigned int *)pcd;</span>
<span class="p_del">-		goto word_copy;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Align the destination.  */</span>
<span class="p_del">-	if (unlikely((dst &amp; (sizeof(unsigned int) - 1)) != 0)) {</span>
<span class="p_del">-		t2 = sizeof(unsigned int) - (dst &amp; (sizeof(unsigned int) - 1));</span>
<span class="p_del">-		while (t2) {</span>
<span class="p_del">-			/* *pcd++ = *pcs++; */</span>
<span class="p_del">-			ldbma(s_space, pcs, t3, pmc_load_exc);</span>
<span class="p_del">-			stbma(d_space, t3, pcd, pmc_store_exc);</span>
<span class="p_del">-			len--;</span>
<span class="p_del">-			t2--;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		dst = (unsigned long)pcd;</span>
<span class="p_del">-		src = (unsigned long)pcs;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = copy_dstaligned(dst, src, len / sizeof(unsigned int));</span>
<span class="p_del">-	if (ret)</span>
<span class="p_del">-		return ret;</span>
<span class="p_del">-</span>
<span class="p_del">-	pcs += (len &amp; -sizeof(unsigned int));</span>
<span class="p_del">-	pcd += (len &amp; -sizeof(unsigned int));</span>
<span class="p_del">-	len %= sizeof(unsigned int);</span>
<span class="p_del">-</span>
<span class="p_del">-	preserve_branch(handle_load_error);</span>
<span class="p_del">-	preserve_branch(handle_store_error);</span>
<span class="p_del">-</span>
<span class="p_del">-	goto byte_copy;</span>
<span class="p_del">-</span>
<span class="p_del">-handle_load_error:</span>
<span class="p_del">-	__asm__ __volatile__ (&quot;pmc_load_exc:\n&quot;);</span>
<span class="p_del">-	return PA_MEMCPY_LOAD_ERROR;</span>
<span class="p_del">-</span>
<span class="p_del">-handle_store_error:</span>
<span class="p_del">-	__asm__ __volatile__ (&quot;pmc_store_exc:\n&quot;);</span>
<span class="p_del">-	return PA_MEMCPY_STORE_ERROR;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
 /* Returns 0 for success, otherwise, returns number of bytes not transferred. */
<span class="p_del">-static unsigned long pa_memcpy(void *dstp, const void *srcp, unsigned long len)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long ret, fault_addr, reference;</span>
<span class="p_del">-	struct exception_data *d;</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = pa_memcpy_internal(dstp, srcp, len);</span>
<span class="p_del">-	if (likely(ret == PA_MEMCPY_OK))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* if a load or store fault occured we can get the faulty addr */</span>
<span class="p_del">-	d = this_cpu_ptr(&amp;exception_data);</span>
<span class="p_del">-	fault_addr = d-&gt;fault_addr;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* error in load or store? */</span>
<span class="p_del">-	if (ret == PA_MEMCPY_LOAD_ERROR)</span>
<span class="p_del">-		reference = (unsigned long) srcp;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		reference = (unsigned long) dstp;</span>
<span class="p_add">+extern unsigned long pa_memcpy(void *dst, const void *src,</span>
<span class="p_add">+				unsigned long len);</span>
 
<span class="p_del">-	DPRINTF(&quot;pa_memcpy: fault type = %lu, len=%lu fault_addr=%lu ref=%lu\n&quot;,</span>
<span class="p_del">-		ret, len, fault_addr, reference);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (fault_addr &gt;= reference)</span>
<span class="p_del">-		return len - (fault_addr - reference);</span>
<span class="p_del">-	else</span>
<span class="p_del">-		return len;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef __KERNEL__</span>
 unsigned long __copy_to_user(void __user *dst, const void *src,
 			     unsigned long len)
 {
<span class="p_chunk">@@ -537,5 +84,3 @@</span> <span class="p_context"> long probe_kernel_read(void *dst, const void *src, size_t size)</span>
 
 	return __probe_kernel_read(dst, src, size);
 }
<span class="p_del">-</span>
<span class="p_del">-#endif</span>
<span class="p_header">diff --git a/arch/parisc/mm/fault.c b/arch/parisc/mm/fault.c</span>
<span class="p_header">index 1a0b4f63f0e9..040c48fc5391 100644</span>
<span class="p_header">--- a/arch/parisc/mm/fault.c</span>
<span class="p_header">+++ b/arch/parisc/mm/fault.c</span>
<span class="p_chunk">@@ -149,6 +149,23 @@</span> <span class="p_context"> int fixup_exception(struct pt_regs *regs)</span>
 		d-&gt;fault_space = regs-&gt;isr;
 		d-&gt;fault_addr = regs-&gt;ior;
 
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Fix up get_user() and put_user().</span>
<span class="p_add">+		 * ASM_EXCEPTIONTABLE_ENTRY_EFAULT() sets the least-significant</span>
<span class="p_add">+		 * bit in the relative address of the fixup routine to indicate</span>
<span class="p_add">+		 * that %r8 should be loaded with -EFAULT to report a userspace</span>
<span class="p_add">+		 * access error.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (fix-&gt;fixup &amp; 1) {</span>
<span class="p_add">+			regs-&gt;gr[8] = -EFAULT;</span>
<span class="p_add">+</span>
<span class="p_add">+			/* zero target register for get_user() */</span>
<span class="p_add">+			if (parisc_acctyp(0, regs-&gt;iir) == VM_READ) {</span>
<span class="p_add">+				int treg = regs-&gt;iir &amp; 0x1f;</span>
<span class="p_add">+				regs-&gt;gr[treg] = 0;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		regs-&gt;iaoq[0] = (unsigned long)&amp;fix-&gt;fixup + fix-&gt;fixup;
 		regs-&gt;iaoq[0] &amp;= ~3;
 		/*
<span class="p_header">diff --git a/arch/x86/lib/memcpy_64.S b/arch/x86/lib/memcpy_64.S</span>
<span class="p_header">index 779782f58324..9a53a06e5a3e 100644</span>
<span class="p_header">--- a/arch/x86/lib/memcpy_64.S</span>
<span class="p_header">+++ b/arch/x86/lib/memcpy_64.S</span>
<span class="p_chunk">@@ -290,7 +290,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(memcpy_mcsafe_unrolled)</span>
 	_ASM_EXTABLE_FAULT(.L_copy_leading_bytes, .L_memcpy_mcsafe_fail)
 	_ASM_EXTABLE_FAULT(.L_cache_w0, .L_memcpy_mcsafe_fail)
 	_ASM_EXTABLE_FAULT(.L_cache_w1, .L_memcpy_mcsafe_fail)
<span class="p_del">-	_ASM_EXTABLE_FAULT(.L_cache_w3, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_cache_w2, .L_memcpy_mcsafe_fail)</span>
 	_ASM_EXTABLE_FAULT(.L_cache_w3, .L_memcpy_mcsafe_fail)
 	_ASM_EXTABLE_FAULT(.L_cache_w4, .L_memcpy_mcsafe_fail)
 	_ASM_EXTABLE_FAULT(.L_cache_w5, .L_memcpy_mcsafe_fail)
<span class="p_header">diff --git a/arch/x86/mm/kaslr.c b/arch/x86/mm/kaslr.c</span>
<span class="p_header">index 887e57182716..aed206475aa7 100644</span>
<span class="p_header">--- a/arch/x86/mm/kaslr.c</span>
<span class="p_header">+++ b/arch/x86/mm/kaslr.c</span>
<span class="p_chunk">@@ -48,7 +48,7 @@</span> <span class="p_context"> static const unsigned long vaddr_start = __PAGE_OFFSET_BASE;</span>
 #if defined(CONFIG_X86_ESPFIX64)
 static const unsigned long vaddr_end = ESPFIX_BASE_ADDR;
 #elif defined(CONFIG_EFI)
<span class="p_del">-static const unsigned long vaddr_end = EFI_VA_START;</span>
<span class="p_add">+static const unsigned long vaddr_end = EFI_VA_END;</span>
 #else
 static const unsigned long vaddr_end = __START_KERNEL_map;
 #endif
<span class="p_chunk">@@ -105,7 +105,7 @@</span> <span class="p_context"> void __init kernel_randomize_memory(void)</span>
 	 */
 	BUILD_BUG_ON(vaddr_start &gt;= vaddr_end);
 	BUILD_BUG_ON(IS_ENABLED(CONFIG_X86_ESPFIX64) &amp;&amp;
<span class="p_del">-		     vaddr_end &gt;= EFI_VA_START);</span>
<span class="p_add">+		     vaddr_end &gt;= EFI_VA_END);</span>
 	BUILD_BUG_ON((IS_ENABLED(CONFIG_X86_ESPFIX64) ||
 		      IS_ENABLED(CONFIG_EFI)) &amp;&amp;
 		     vaddr_end &gt;= __START_KERNEL_map);
<span class="p_header">diff --git a/arch/x86/xen/setup.c b/arch/x86/xen/setup.c</span>
<span class="p_header">index f8960fca0827..9f21b0c5945d 100644</span>
<span class="p_header">--- a/arch/x86/xen/setup.c</span>
<span class="p_header">+++ b/arch/x86/xen/setup.c</span>
<span class="p_chunk">@@ -713,10 +713,9 @@</span> <span class="p_context"> static void __init xen_reserve_xen_mfnlist(void)</span>
 		size = PFN_PHYS(xen_start_info-&gt;nr_p2m_frames);
 	}
 
<span class="p_del">-	if (!xen_is_e820_reserved(start, size)) {</span>
<span class="p_del">-		memblock_reserve(start, size);</span>
<span class="p_add">+	memblock_reserve(start, size);</span>
<span class="p_add">+	if (!xen_is_e820_reserved(start, size))</span>
 		return;
<span class="p_del">-	}</span>
 
 #ifdef CONFIG_X86_32
 	/*
<span class="p_chunk">@@ -727,6 +726,7 @@</span> <span class="p_context"> static void __init xen_reserve_xen_mfnlist(void)</span>
 	BUG();
 #else
 	xen_relocate_p2m();
<span class="p_add">+	memblock_free(start, size);</span>
 #endif
 }
 
<span class="p_header">diff --git a/block/bio.c b/block/bio.c</span>
<span class="p_header">index db85c5753a76..655c9016052a 100644</span>
<span class="p_header">--- a/block/bio.c</span>
<span class="p_header">+++ b/block/bio.c</span>
<span class="p_chunk">@@ -372,10 +372,14 @@</span> <span class="p_context"> static void punt_bios_to_rescuer(struct bio_set *bs)</span>
 	bio_list_init(&amp;punt);
 	bio_list_init(&amp;nopunt);
 
<span class="p_del">-	while ((bio = bio_list_pop(current-&gt;bio_list)))</span>
<span class="p_add">+	while ((bio = bio_list_pop(&amp;current-&gt;bio_list[0])))</span>
 		bio_list_add(bio-&gt;bi_pool == bs ? &amp;punt : &amp;nopunt, bio);
<span class="p_add">+	current-&gt;bio_list[0] = nopunt;</span>
 
<span class="p_del">-	*current-&gt;bio_list = nopunt;</span>
<span class="p_add">+	bio_list_init(&amp;nopunt);</span>
<span class="p_add">+	while ((bio = bio_list_pop(&amp;current-&gt;bio_list[1])))</span>
<span class="p_add">+		bio_list_add(bio-&gt;bi_pool == bs ? &amp;punt : &amp;nopunt, bio);</span>
<span class="p_add">+	current-&gt;bio_list[1] = nopunt;</span>
 
 	spin_lock(&amp;bs-&gt;rescue_lock);
 	bio_list_merge(&amp;bs-&gt;rescue_list, &amp;punt);
<span class="p_chunk">@@ -462,7 +466,9 @@</span> <span class="p_context"> struct bio *bio_alloc_bioset(gfp_t gfp_mask, int nr_iovecs, struct bio_set *bs)</span>
 		 * we retry with the original gfp_flags.
 		 */
 
<span class="p_del">-		if (current-&gt;bio_list &amp;&amp; !bio_list_empty(current-&gt;bio_list))</span>
<span class="p_add">+		if (current-&gt;bio_list &amp;&amp;</span>
<span class="p_add">+		    (!bio_list_empty(&amp;current-&gt;bio_list[0]) ||</span>
<span class="p_add">+		     !bio_list_empty(&amp;current-&gt;bio_list[1])))</span>
 			gfp_mask &amp;= ~__GFP_DIRECT_RECLAIM;
 
 		p = mempool_alloc(bs-&gt;bio_pool, gfp_mask);
<span class="p_header">diff --git a/block/blk-core.c b/block/blk-core.c</span>
<span class="p_header">index 14d7c0740dc0..d1f2801ce836 100644</span>
<span class="p_header">--- a/block/blk-core.c</span>
<span class="p_header">+++ b/block/blk-core.c</span>
<span class="p_chunk">@@ -1994,7 +1994,14 @@</span> <span class="p_context"> generic_make_request_checks(struct bio *bio)</span>
  */
 blk_qc_t generic_make_request(struct bio *bio)
 {
<span class="p_del">-	struct bio_list bio_list_on_stack;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * bio_list_on_stack[0] contains bios submitted by the current</span>
<span class="p_add">+	 * make_request_fn.</span>
<span class="p_add">+	 * bio_list_on_stack[1] contains bios that were submitted before</span>
<span class="p_add">+	 * the current make_request_fn, but that haven&#39;t been processed</span>
<span class="p_add">+	 * yet.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct bio_list bio_list_on_stack[2];</span>
 	blk_qc_t ret = BLK_QC_T_NONE;
 
 	if (!generic_make_request_checks(bio))
<span class="p_chunk">@@ -2011,7 +2018,7 @@</span> <span class="p_context"> blk_qc_t generic_make_request(struct bio *bio)</span>
 	 * should be added at the tail
 	 */
 	if (current-&gt;bio_list) {
<span class="p_del">-		bio_list_add(current-&gt;bio_list, bio);</span>
<span class="p_add">+		bio_list_add(&amp;current-&gt;bio_list[0], bio);</span>
 		goto out;
 	}
 
<span class="p_chunk">@@ -2030,23 +2037,39 @@</span> <span class="p_context"> blk_qc_t generic_make_request(struct bio *bio)</span>
 	 * bio_list, and call into -&gt;make_request() again.
 	 */
 	BUG_ON(bio-&gt;bi_next);
<span class="p_del">-	bio_list_init(&amp;bio_list_on_stack);</span>
<span class="p_del">-	current-&gt;bio_list = &amp;bio_list_on_stack;</span>
<span class="p_add">+	bio_list_init(&amp;bio_list_on_stack[0]);</span>
<span class="p_add">+	current-&gt;bio_list = bio_list_on_stack;</span>
 	do {
 		struct request_queue *q = bdev_get_queue(bio-&gt;bi_bdev);
 
 		if (likely(blk_queue_enter(q, false) == 0)) {
<span class="p_add">+			struct bio_list lower, same;</span>
<span class="p_add">+</span>
<span class="p_add">+			/* Create a fresh bio_list for all subordinate requests */</span>
<span class="p_add">+			bio_list_on_stack[1] = bio_list_on_stack[0];</span>
<span class="p_add">+			bio_list_init(&amp;bio_list_on_stack[0]);</span>
 			ret = q-&gt;make_request_fn(q, bio);
 
 			blk_queue_exit(q);
 
<span class="p_del">-			bio = bio_list_pop(current-&gt;bio_list);</span>
<span class="p_add">+			/* sort new bios into those for a lower level</span>
<span class="p_add">+			 * and those for the same level</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			bio_list_init(&amp;lower);</span>
<span class="p_add">+			bio_list_init(&amp;same);</span>
<span class="p_add">+			while ((bio = bio_list_pop(&amp;bio_list_on_stack[0])) != NULL)</span>
<span class="p_add">+				if (q == bdev_get_queue(bio-&gt;bi_bdev))</span>
<span class="p_add">+					bio_list_add(&amp;same, bio);</span>
<span class="p_add">+				else</span>
<span class="p_add">+					bio_list_add(&amp;lower, bio);</span>
<span class="p_add">+			/* now assemble so we handle the lowest level first */</span>
<span class="p_add">+			bio_list_merge(&amp;bio_list_on_stack[0], &amp;lower);</span>
<span class="p_add">+			bio_list_merge(&amp;bio_list_on_stack[0], &amp;same);</span>
<span class="p_add">+			bio_list_merge(&amp;bio_list_on_stack[0], &amp;bio_list_on_stack[1]);</span>
 		} else {
<span class="p_del">-			struct bio *bio_next = bio_list_pop(current-&gt;bio_list);</span>
<span class="p_del">-</span>
 			bio_io_error(bio);
<span class="p_del">-			bio = bio_next;</span>
 		}
<span class="p_add">+		bio = bio_list_pop(&amp;bio_list_on_stack[0]);</span>
 	} while (bio);
 	current-&gt;bio_list = NULL; /* deactivate */
 
<span class="p_header">diff --git a/drivers/acpi/Makefile b/drivers/acpi/Makefile</span>
<span class="p_header">index 9ed087853dee..4c5678cfa9c4 100644</span>
<span class="p_header">--- a/drivers/acpi/Makefile</span>
<span class="p_header">+++ b/drivers/acpi/Makefile</span>
<span class="p_chunk">@@ -2,7 +2,6 @@</span> <span class="p_context"></span>
 # Makefile for the Linux ACPI interpreter
 #
 
<span class="p_del">-ccflags-y			:= -Os</span>
 ccflags-$(CONFIG_ACPI_DEBUG)	+= -DACPI_DEBUG_OUTPUT
 
 #
<span class="p_header">diff --git a/drivers/acpi/acpi_platform.c b/drivers/acpi/acpi_platform.c</span>
<span class="p_header">index b4c1a6a51da4..03250e1f1103 100644</span>
<span class="p_header">--- a/drivers/acpi/acpi_platform.c</span>
<span class="p_header">+++ b/drivers/acpi/acpi_platform.c</span>
<span class="p_chunk">@@ -25,9 +25,11 @@</span> <span class="p_context"></span>
 ACPI_MODULE_NAME(&quot;platform&quot;);
 
 static const struct acpi_device_id forbidden_id_list[] = {
<span class="p_del">-	{&quot;PNP0000&quot;, 0},	/* PIC */</span>
<span class="p_del">-	{&quot;PNP0100&quot;, 0},	/* Timer */</span>
<span class="p_del">-	{&quot;PNP0200&quot;, 0},	/* AT DMA Controller */</span>
<span class="p_add">+	{&quot;PNP0000&quot;,  0},	/* PIC */</span>
<span class="p_add">+	{&quot;PNP0100&quot;,  0},	/* Timer */</span>
<span class="p_add">+	{&quot;PNP0200&quot;,  0},	/* AT DMA Controller */</span>
<span class="p_add">+	{&quot;ACPI0009&quot;, 0},	/* IOxAPIC */</span>
<span class="p_add">+	{&quot;ACPI000A&quot;, 0},	/* IOAPIC */</span>
 	{&quot;&quot;, 0},
 };
 
<span class="p_header">diff --git a/drivers/gpu/drm/etnaviv/etnaviv_gpu.c b/drivers/gpu/drm/etnaviv/etnaviv_gpu.c</span>
<span class="p_header">index b1254f885fed..b87d27859141 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/etnaviv/etnaviv_gpu.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/etnaviv/etnaviv_gpu.c</span>
<span class="p_chunk">@@ -1299,6 +1299,8 @@</span> <span class="p_context"> int etnaviv_gpu_submit(struct etnaviv_gpu *gpu,</span>
 		goto out_pm_put;
 	}
 
<span class="p_add">+	mutex_lock(&amp;gpu-&gt;lock);</span>
<span class="p_add">+</span>
 	fence = etnaviv_gpu_fence_alloc(gpu);
 	if (!fence) {
 		event_free(gpu, event);
<span class="p_chunk">@@ -1306,8 +1308,6 @@</span> <span class="p_context"> int etnaviv_gpu_submit(struct etnaviv_gpu *gpu,</span>
 		goto out_pm_put;
 	}
 
<span class="p_del">-	mutex_lock(&amp;gpu-&gt;lock);</span>
<span class="p_del">-</span>
 	gpu-&gt;event[event].fence = fence;
 	submit-&gt;fence = fence-&gt;seqno;
 	gpu-&gt;active_fence = submit-&gt;fence;
<span class="p_header">diff --git a/drivers/gpu/drm/radeon/radeon_ttm.c b/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="p_header">index 3de5e6e21662..4ce04e06d9ac 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/radeon/radeon_ttm.c</span>
<span class="p_chunk">@@ -213,8 +213,8 @@</span> <span class="p_context"> static void radeon_evict_flags(struct ttm_buffer_object *bo,</span>
 			rbo-&gt;placement.num_busy_placement = 0;
 			for (i = 0; i &lt; rbo-&gt;placement.num_placement; i++) {
 				if (rbo-&gt;placements[i].flags &amp; TTM_PL_FLAG_VRAM) {
<span class="p_del">-					if (rbo-&gt;placements[0].fpfn &lt; fpfn)</span>
<span class="p_del">-						rbo-&gt;placements[0].fpfn = fpfn;</span>
<span class="p_add">+					if (rbo-&gt;placements[i].fpfn &lt; fpfn)</span>
<span class="p_add">+						rbo-&gt;placements[i].fpfn = fpfn;</span>
 				} else {
 					rbo-&gt;placement.busy_placement =
 						&amp;rbo-&gt;placements[i];
<span class="p_header">diff --git a/drivers/gpu/drm/vc4/vc4_crtc.c b/drivers/gpu/drm/vc4/vc4_crtc.c</span>
<span class="p_header">index 7aadce1f7e7a..c7e6c9839c9a 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/vc4/vc4_crtc.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/vc4/vc4_crtc.c</span>
<span class="p_chunk">@@ -842,6 +842,17 @@</span> <span class="p_context"> static void vc4_crtc_destroy_state(struct drm_crtc *crtc,</span>
 	drm_atomic_helper_crtc_destroy_state(crtc, state);
 }
 
<span class="p_add">+static void</span>
<span class="p_add">+vc4_crtc_reset(struct drm_crtc *crtc)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (crtc-&gt;state)</span>
<span class="p_add">+		__drm_atomic_helper_crtc_destroy_state(crtc-&gt;state);</span>
<span class="p_add">+</span>
<span class="p_add">+	crtc-&gt;state = kzalloc(sizeof(struct vc4_crtc_state), GFP_KERNEL);</span>
<span class="p_add">+	if (crtc-&gt;state)</span>
<span class="p_add">+		crtc-&gt;state-&gt;crtc = crtc;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static const struct drm_crtc_funcs vc4_crtc_funcs = {
 	.set_config = drm_atomic_helper_set_config,
 	.destroy = vc4_crtc_destroy,
<span class="p_chunk">@@ -849,7 +860,7 @@</span> <span class="p_context"> static const struct drm_crtc_funcs vc4_crtc_funcs = {</span>
 	.set_property = NULL,
 	.cursor_set = NULL, /* handled by drm_mode_cursor_universal */
 	.cursor_move = NULL, /* handled by drm_mode_cursor_universal */
<span class="p_del">-	.reset = drm_atomic_helper_crtc_reset,</span>
<span class="p_add">+	.reset = vc4_crtc_reset,</span>
 	.atomic_duplicate_state = vc4_crtc_duplicate_state,
 	.atomic_destroy_state = vc4_crtc_destroy_state,
 	.gamma_set = vc4_crtc_gamma_set,
<span class="p_header">diff --git a/drivers/hid/wacom_sys.c b/drivers/hid/wacom_sys.c</span>
<span class="p_header">index 5e7a5648e708..0c535d0f3b95 100644</span>
<span class="p_header">--- a/drivers/hid/wacom_sys.c</span>
<span class="p_header">+++ b/drivers/hid/wacom_sys.c</span>
<span class="p_chunk">@@ -2017,6 +2017,14 @@</span> <span class="p_context"> static int wacom_parse_and_register(struct wacom *wacom, bool wireless)</span>
 
 	wacom_update_name(wacom, wireless ? &quot; (WL)&quot; : &quot;&quot;);
 
<span class="p_add">+	/* pen only Bamboo neither support touch nor pad */</span>
<span class="p_add">+	if ((features-&gt;type == BAMBOO_PEN) &amp;&amp;</span>
<span class="p_add">+	    ((features-&gt;device_type &amp; WACOM_DEVICETYPE_TOUCH) ||</span>
<span class="p_add">+	    (features-&gt;device_type &amp; WACOM_DEVICETYPE_PAD))) {</span>
<span class="p_add">+		error = -ENODEV;</span>
<span class="p_add">+		goto fail;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	error = wacom_add_shared_data(hdev);
 	if (error)
 		goto fail;
<span class="p_chunk">@@ -2064,14 +2072,6 @@</span> <span class="p_context"> static int wacom_parse_and_register(struct wacom *wacom, bool wireless)</span>
 		goto fail_quirks;
 	}
 
<span class="p_del">-	/* pen only Bamboo neither support touch nor pad */</span>
<span class="p_del">-	if ((features-&gt;type == BAMBOO_PEN) &amp;&amp;</span>
<span class="p_del">-	    ((features-&gt;device_type &amp; WACOM_DEVICETYPE_TOUCH) ||</span>
<span class="p_del">-	    (features-&gt;device_type &amp; WACOM_DEVICETYPE_PAD))) {</span>
<span class="p_del">-		error = -ENODEV;</span>
<span class="p_del">-		goto fail_quirks;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	if (features-&gt;device_type &amp; WACOM_DEVICETYPE_WL_MONITOR)
 		error = hid_hw_open(hdev);
 
<span class="p_header">diff --git a/drivers/md/dm.c b/drivers/md/dm.c</span>
<span class="p_header">index 628ba001bb3c..e66f4040d84b 100644</span>
<span class="p_header">--- a/drivers/md/dm.c</span>
<span class="p_header">+++ b/drivers/md/dm.c</span>
<span class="p_chunk">@@ -986,26 +986,29 @@</span> <span class="p_context"> static void flush_current_bio_list(struct blk_plug_cb *cb, bool from_schedule)</span>
 	struct dm_offload *o = container_of(cb, struct dm_offload, cb);
 	struct bio_list list;
 	struct bio *bio;
<span class="p_add">+	int i;</span>
 
 	INIT_LIST_HEAD(&amp;o-&gt;cb.list);
 
 	if (unlikely(!current-&gt;bio_list))
 		return;
 
<span class="p_del">-	list = *current-&gt;bio_list;</span>
<span class="p_del">-	bio_list_init(current-&gt;bio_list);</span>
<span class="p_del">-</span>
<span class="p_del">-	while ((bio = bio_list_pop(&amp;list))) {</span>
<span class="p_del">-		struct bio_set *bs = bio-&gt;bi_pool;</span>
<span class="p_del">-		if (unlikely(!bs) || bs == fs_bio_set) {</span>
<span class="p_del">-			bio_list_add(current-&gt;bio_list, bio);</span>
<span class="p_del">-			continue;</span>
<span class="p_add">+	for (i = 0; i &lt; 2; i++) {</span>
<span class="p_add">+		list = current-&gt;bio_list[i];</span>
<span class="p_add">+		bio_list_init(&amp;current-&gt;bio_list[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		while ((bio = bio_list_pop(&amp;list))) {</span>
<span class="p_add">+			struct bio_set *bs = bio-&gt;bi_pool;</span>
<span class="p_add">+			if (unlikely(!bs) || bs == fs_bio_set) {</span>
<span class="p_add">+				bio_list_add(&amp;current-&gt;bio_list[i], bio);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			spin_lock(&amp;bs-&gt;rescue_lock);</span>
<span class="p_add">+			bio_list_add(&amp;bs-&gt;rescue_list, bio);</span>
<span class="p_add">+			queue_work(bs-&gt;rescue_workqueue, &amp;bs-&gt;rescue_work);</span>
<span class="p_add">+			spin_unlock(&amp;bs-&gt;rescue_lock);</span>
 		}
<span class="p_del">-</span>
<span class="p_del">-		spin_lock(&amp;bs-&gt;rescue_lock);</span>
<span class="p_del">-		bio_list_add(&amp;bs-&gt;rescue_list, bio);</span>
<span class="p_del">-		queue_work(bs-&gt;rescue_workqueue, &amp;bs-&gt;rescue_work);</span>
<span class="p_del">-		spin_unlock(&amp;bs-&gt;rescue_lock);</span>
 	}
 }
 
<span class="p_header">diff --git a/drivers/md/raid10.c b/drivers/md/raid10.c</span>
<span class="p_header">index 55b5e0e77b17..4c4aab02e311 100644</span>
<span class="p_header">--- a/drivers/md/raid10.c</span>
<span class="p_header">+++ b/drivers/md/raid10.c</span>
<span class="p_chunk">@@ -941,7 +941,8 @@</span> <span class="p_context"> static void wait_barrier(struct r10conf *conf)</span>
 				    !conf-&gt;barrier ||
 				    (atomic_read(&amp;conf-&gt;nr_pending) &amp;&amp;
 				     current-&gt;bio_list &amp;&amp;
<span class="p_del">-				     !bio_list_empty(current-&gt;bio_list)),</span>
<span class="p_add">+				     (!bio_list_empty(&amp;current-&gt;bio_list[0]) ||</span>
<span class="p_add">+				      !bio_list_empty(&amp;current-&gt;bio_list[1]))),</span>
 				    conf-&gt;resync_lock);
 		conf-&gt;nr_waiting--;
 		if (!conf-&gt;nr_waiting)
<span class="p_header">diff --git a/drivers/mmc/host/sdhci-of-at91.c b/drivers/mmc/host/sdhci-of-at91.c</span>
<span class="p_header">index 387ae1cbf698..a8b430ff117b 100644</span>
<span class="p_header">--- a/drivers/mmc/host/sdhci-of-at91.c</span>
<span class="p_header">+++ b/drivers/mmc/host/sdhci-of-at91.c</span>
<span class="p_chunk">@@ -29,6 +29,8 @@</span> <span class="p_context"></span>
 
 #include &quot;sdhci-pltfm.h&quot;
 
<span class="p_add">+#define SDMMC_MC1R	0x204</span>
<span class="p_add">+#define		SDMMC_MC1R_DDR		BIT(3)</span>
 #define SDMMC_CACR	0x230
 #define		SDMMC_CACR_CAPWREN	BIT(0)
 #define		SDMMC_CACR_KEY		(0x46 &lt;&lt; 8)
<span class="p_chunk">@@ -103,11 +105,18 @@</span> <span class="p_context"> static void sdhci_at91_set_power(struct sdhci_host *host, unsigned char mode,</span>
 	sdhci_set_power_noreg(host, mode, vdd);
 }
 
<span class="p_add">+void sdhci_at91_set_uhs_signaling(struct sdhci_host *host, unsigned int timing)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (timing == MMC_TIMING_MMC_DDR52)</span>
<span class="p_add">+		sdhci_writeb(host, SDMMC_MC1R_DDR, SDMMC_MC1R);</span>
<span class="p_add">+	sdhci_set_uhs_signaling(host, timing);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static const struct sdhci_ops sdhci_at91_sama5d2_ops = {
 	.set_clock		= sdhci_at91_set_clock,
 	.set_bus_width		= sdhci_set_bus_width,
 	.reset			= sdhci_reset,
<span class="p_del">-	.set_uhs_signaling	= sdhci_set_uhs_signaling,</span>
<span class="p_add">+	.set_uhs_signaling	= sdhci_at91_set_uhs_signaling,</span>
 	.set_power		= sdhci_at91_set_power,
 };
 
<span class="p_header">diff --git a/drivers/mmc/host/sdhci.c b/drivers/mmc/host/sdhci.c</span>
<span class="p_header">index a983ba0349fb..7d275e72903a 100644</span>
<span class="p_header">--- a/drivers/mmc/host/sdhci.c</span>
<span class="p_header">+++ b/drivers/mmc/host/sdhci.c</span>
<span class="p_chunk">@@ -1823,6 +1823,9 @@</span> <span class="p_context"> static void sdhci_enable_sdio_irq(struct mmc_host *mmc, int enable)</span>
 	struct sdhci_host *host = mmc_priv(mmc);
 	unsigned long flags;
 
<span class="p_add">+	if (enable)</span>
<span class="p_add">+		pm_runtime_get_noresume(host-&gt;mmc-&gt;parent);</span>
<span class="p_add">+</span>
 	spin_lock_irqsave(&amp;host-&gt;lock, flags);
 	if (enable)
 		host-&gt;flags |= SDHCI_SDIO_IRQ_ENABLED;
<span class="p_chunk">@@ -1831,6 +1834,9 @@</span> <span class="p_context"> static void sdhci_enable_sdio_irq(struct mmc_host *mmc, int enable)</span>
 
 	sdhci_enable_sdio_irq_nolock(host, enable);
 	spin_unlock_irqrestore(&amp;host-&gt;lock, flags);
<span class="p_add">+</span>
<span class="p_add">+	if (!enable)</span>
<span class="p_add">+		pm_runtime_put_noidle(host-&gt;mmc-&gt;parent);</span>
 }
 
 static int sdhci_start_signal_voltage_switch(struct mmc_host *mmc,
<span class="p_header">diff --git a/drivers/nvme/host/core.c b/drivers/nvme/host/core.c</span>
<span class="p_header">index da10b484bd25..bde769b11e3b 100644</span>
<span class="p_header">--- a/drivers/nvme/host/core.c</span>
<span class="p_header">+++ b/drivers/nvme/host/core.c</span>
<span class="p_chunk">@@ -2057,9 +2057,9 @@</span> <span class="p_context"> void nvme_kill_queues(struct nvme_ctrl *ctrl)</span>
 		 * Revalidating a dead namespace sets capacity to 0. This will
 		 * end buffered writers dirtying pages that can&#39;t be synced.
 		 */
<span class="p_del">-		if (ns-&gt;disk &amp;&amp; !test_and_set_bit(NVME_NS_DEAD, &amp;ns-&gt;flags))</span>
<span class="p_del">-			revalidate_disk(ns-&gt;disk);</span>
<span class="p_del">-</span>
<span class="p_add">+		if (!ns-&gt;disk || test_and_set_bit(NVME_NS_DEAD, &amp;ns-&gt;flags))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		revalidate_disk(ns-&gt;disk);</span>
 		blk_set_queue_dying(ns-&gt;queue);
 		blk_mq_abort_requeue_list(ns-&gt;queue);
 		blk_mq_start_stopped_hw_queues(ns-&gt;queue, true);
<span class="p_header">diff --git a/drivers/nvme/host/pci.c b/drivers/nvme/host/pci.c</span>
<span class="p_header">index 5e52034ab010..8a9c186898c7 100644</span>
<span class="p_header">--- a/drivers/nvme/host/pci.c</span>
<span class="p_header">+++ b/drivers/nvme/host/pci.c</span>
<span class="p_chunk">@@ -1983,8 +1983,10 @@</span> <span class="p_context"> static void nvme_remove(struct pci_dev *pdev)</span>
 
 	pci_set_drvdata(pdev, NULL);
 
<span class="p_del">-	if (!pci_device_is_present(pdev))</span>
<span class="p_add">+	if (!pci_device_is_present(pdev)) {</span>
 		nvme_change_ctrl_state(&amp;dev-&gt;ctrl, NVME_CTRL_DEAD);
<span class="p_add">+		nvme_dev_disable(dev, false);</span>
<span class="p_add">+	}</span>
 
 	flush_work(&amp;dev-&gt;reset_work);
 	nvme_uninit_ctrl(&amp;dev-&gt;ctrl);
<span class="p_header">diff --git a/drivers/pci/host/pcie-iproc-bcma.c b/drivers/pci/host/pcie-iproc-bcma.c</span>
<span class="p_header">index 8ce089043a27..46ca8ed031fe 100644</span>
<span class="p_header">--- a/drivers/pci/host/pcie-iproc-bcma.c</span>
<span class="p_header">+++ b/drivers/pci/host/pcie-iproc-bcma.c</span>
<span class="p_chunk">@@ -44,8 +44,7 @@</span> <span class="p_context"> static int iproc_pcie_bcma_probe(struct bcma_device *bdev)</span>
 {
 	struct device *dev = &amp;bdev-&gt;dev;
 	struct iproc_pcie *pcie;
<span class="p_del">-	LIST_HEAD(res);</span>
<span class="p_del">-	struct resource res_mem;</span>
<span class="p_add">+	LIST_HEAD(resources);</span>
 	int ret;
 
 	pcie = devm_kzalloc(dev, sizeof(*pcie), GFP_KERNEL);
<span class="p_chunk">@@ -62,22 +61,23 @@</span> <span class="p_context"> static int iproc_pcie_bcma_probe(struct bcma_device *bdev)</span>
 
 	pcie-&gt;base_addr = bdev-&gt;addr;
 
<span class="p_del">-	res_mem.start = bdev-&gt;addr_s[0];</span>
<span class="p_del">-	res_mem.end = bdev-&gt;addr_s[0] + SZ_128M - 1;</span>
<span class="p_del">-	res_mem.name = &quot;PCIe MEM space&quot;;</span>
<span class="p_del">-	res_mem.flags = IORESOURCE_MEM;</span>
<span class="p_del">-	pci_add_resource(&amp;res, &amp;res_mem);</span>
<span class="p_add">+	pcie-&gt;mem.start = bdev-&gt;addr_s[0];</span>
<span class="p_add">+	pcie-&gt;mem.end = bdev-&gt;addr_s[0] + SZ_128M - 1;</span>
<span class="p_add">+	pcie-&gt;mem.name = &quot;PCIe MEM space&quot;;</span>
<span class="p_add">+	pcie-&gt;mem.flags = IORESOURCE_MEM;</span>
<span class="p_add">+	pci_add_resource(&amp;resources, &amp;pcie-&gt;mem);</span>
 
 	pcie-&gt;map_irq = iproc_pcie_bcma_map_irq;
 
<span class="p_del">-	ret = iproc_pcie_setup(pcie, &amp;res);</span>
<span class="p_del">-	if (ret)</span>
<span class="p_add">+	ret = iproc_pcie_setup(pcie, &amp;resources);</span>
<span class="p_add">+	if (ret) {</span>
 		dev_err(dev, &quot;PCIe controller setup failed\n&quot;);
<span class="p_del">-</span>
<span class="p_del">-	pci_free_resource_list(&amp;res);</span>
<span class="p_add">+		pci_free_resource_list(&amp;resources);</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
 
 	bcma_set_drvdata(bdev, pcie);
<span class="p_del">-	return ret;</span>
<span class="p_add">+	return 0;</span>
 }
 
 static void iproc_pcie_bcma_remove(struct bcma_device *bdev)
<span class="p_header">diff --git a/drivers/pci/host/pcie-iproc-platform.c b/drivers/pci/host/pcie-iproc-platform.c</span>
<span class="p_header">index a3de087976b3..7dcaddcd2f16 100644</span>
<span class="p_header">--- a/drivers/pci/host/pcie-iproc-platform.c</span>
<span class="p_header">+++ b/drivers/pci/host/pcie-iproc-platform.c</span>
<span class="p_chunk">@@ -46,7 +46,7 @@</span> <span class="p_context"> static int iproc_pcie_pltfm_probe(struct platform_device *pdev)</span>
 	struct device_node *np = dev-&gt;of_node;
 	struct resource reg;
 	resource_size_t iobase = 0;
<span class="p_del">-	LIST_HEAD(res);</span>
<span class="p_add">+	LIST_HEAD(resources);</span>
 	int ret;
 
 	of_id = of_match_device(iproc_pcie_of_match_table, dev);
<span class="p_chunk">@@ -108,23 +108,24 @@</span> <span class="p_context"> static int iproc_pcie_pltfm_probe(struct platform_device *pdev)</span>
 		pcie-&gt;phy = NULL;
 	}
 
<span class="p_del">-	ret = of_pci_get_host_bridge_resources(np, 0, 0xff, &amp;res, &amp;iobase);</span>
<span class="p_add">+	ret = of_pci_get_host_bridge_resources(np, 0, 0xff, &amp;resources,</span>
<span class="p_add">+					       &amp;iobase);</span>
 	if (ret) {
<span class="p_del">-		dev_err(dev,</span>
<span class="p_del">-			&quot;unable to get PCI host bridge resources\n&quot;);</span>
<span class="p_add">+		dev_err(dev, &quot;unable to get PCI host bridge resources\n&quot;);</span>
 		return ret;
 	}
 
 	pcie-&gt;map_irq = of_irq_parse_and_map_pci;
 
<span class="p_del">-	ret = iproc_pcie_setup(pcie, &amp;res);</span>
<span class="p_del">-	if (ret)</span>
<span class="p_add">+	ret = iproc_pcie_setup(pcie, &amp;resources);</span>
<span class="p_add">+	if (ret) {</span>
 		dev_err(dev, &quot;PCIe controller setup failed\n&quot;);
<span class="p_del">-</span>
<span class="p_del">-	pci_free_resource_list(&amp;res);</span>
<span class="p_add">+		pci_free_resource_list(&amp;resources);</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
 
 	platform_set_drvdata(pdev, pcie);
<span class="p_del">-	return ret;</span>
<span class="p_add">+	return 0;</span>
 }
 
 static int iproc_pcie_pltfm_remove(struct platform_device *pdev)
<span class="p_header">diff --git a/drivers/pci/host/pcie-iproc.h b/drivers/pci/host/pcie-iproc.h</span>
<span class="p_header">index e84d93c53c7b..fa4226742bcd 100644</span>
<span class="p_header">--- a/drivers/pci/host/pcie-iproc.h</span>
<span class="p_header">+++ b/drivers/pci/host/pcie-iproc.h</span>
<span class="p_chunk">@@ -68,6 +68,7 @@</span> <span class="p_context"> struct iproc_pcie {</span>
 #ifdef CONFIG_ARM
 	struct pci_sys_data sysdata;
 #endif
<span class="p_add">+	struct resource mem;</span>
 	struct pci_bus *root_bus;
 	struct phy *phy;
 	int (*map_irq)(const struct pci_dev *, u8, u8);
<span class="p_header">diff --git a/drivers/scsi/device_handler/scsi_dh_alua.c b/drivers/scsi/device_handler/scsi_dh_alua.c</span>
<span class="p_header">index 7bb20684e9fa..d3145799b92f 100644</span>
<span class="p_header">--- a/drivers/scsi/device_handler/scsi_dh_alua.c</span>
<span class="p_header">+++ b/drivers/scsi/device_handler/scsi_dh_alua.c</span>
<span class="p_chunk">@@ -113,7 +113,7 @@</span> <span class="p_context"> struct alua_queue_data {</span>
 #define ALUA_POLICY_SWITCH_ALL		1
 
 static void alua_rtpg_work(struct work_struct *work);
<span class="p_del">-static void alua_rtpg_queue(struct alua_port_group *pg,</span>
<span class="p_add">+static bool alua_rtpg_queue(struct alua_port_group *pg,</span>
 			    struct scsi_device *sdev,
 			    struct alua_queue_data *qdata, bool force);
 static void alua_check(struct scsi_device *sdev, bool force);
<span class="p_chunk">@@ -862,7 +862,13 @@</span> <span class="p_context"> static void alua_rtpg_work(struct work_struct *work)</span>
 	kref_put(&amp;pg-&gt;kref, release_port_group);
 }
 
<span class="p_del">-static void alua_rtpg_queue(struct alua_port_group *pg,</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * alua_rtpg_queue() - cause RTPG to be submitted asynchronously</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns true if and only if alua_rtpg_work() will be called asynchronously.</span>
<span class="p_add">+ * That function is responsible for calling @qdata-&gt;fn().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static bool alua_rtpg_queue(struct alua_port_group *pg,</span>
 			    struct scsi_device *sdev,
 			    struct alua_queue_data *qdata, bool force)
 {
<span class="p_chunk">@@ -870,8 +876,8 @@</span> <span class="p_context"> static void alua_rtpg_queue(struct alua_port_group *pg,</span>
 	unsigned long flags;
 	struct workqueue_struct *alua_wq = kaluad_wq;
 
<span class="p_del">-	if (!pg)</span>
<span class="p_del">-		return;</span>
<span class="p_add">+	if (!pg || scsi_device_get(sdev))</span>
<span class="p_add">+		return false;</span>
 
 	spin_lock_irqsave(&amp;pg-&gt;lock, flags);
 	if (qdata) {
<span class="p_chunk">@@ -884,14 +890,12 @@</span> <span class="p_context"> static void alua_rtpg_queue(struct alua_port_group *pg,</span>
 		pg-&gt;flags |= ALUA_PG_RUN_RTPG;
 		kref_get(&amp;pg-&gt;kref);
 		pg-&gt;rtpg_sdev = sdev;
<span class="p_del">-		scsi_device_get(sdev);</span>
 		start_queue = 1;
 	} else if (!(pg-&gt;flags &amp; ALUA_PG_RUN_RTPG) &amp;&amp; force) {
 		pg-&gt;flags |= ALUA_PG_RUN_RTPG;
 		/* Do not queue if the worker is already running */
 		if (!(pg-&gt;flags &amp; ALUA_PG_RUNNING)) {
 			kref_get(&amp;pg-&gt;kref);
<span class="p_del">-			sdev = NULL;</span>
 			start_queue = 1;
 		}
 	}
<span class="p_chunk">@@ -900,13 +904,17 @@</span> <span class="p_context"> static void alua_rtpg_queue(struct alua_port_group *pg,</span>
 		alua_wq = kaluad_sync_wq;
 	spin_unlock_irqrestore(&amp;pg-&gt;lock, flags);
 
<span class="p_del">-	if (start_queue &amp;&amp;</span>
<span class="p_del">-	    !queue_delayed_work(alua_wq, &amp;pg-&gt;rtpg_work,</span>
<span class="p_del">-				msecs_to_jiffies(ALUA_RTPG_DELAY_MSECS))) {</span>
<span class="p_del">-		if (sdev)</span>
<span class="p_del">-			scsi_device_put(sdev);</span>
<span class="p_del">-		kref_put(&amp;pg-&gt;kref, release_port_group);</span>
<span class="p_add">+	if (start_queue) {</span>
<span class="p_add">+		if (queue_delayed_work(alua_wq, &amp;pg-&gt;rtpg_work,</span>
<span class="p_add">+				msecs_to_jiffies(ALUA_RTPG_DELAY_MSECS)))</span>
<span class="p_add">+			sdev = NULL;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			kref_put(&amp;pg-&gt;kref, release_port_group);</span>
 	}
<span class="p_add">+	if (sdev)</span>
<span class="p_add">+		scsi_device_put(sdev);</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
 }
 
 /*
<span class="p_chunk">@@ -1007,11 +1015,13 @@</span> <span class="p_context"> static int alua_activate(struct scsi_device *sdev,</span>
 		mutex_unlock(&amp;h-&gt;init_mutex);
 		goto out;
 	}
<span class="p_del">-	fn = NULL;</span>
 	rcu_read_unlock();
 	mutex_unlock(&amp;h-&gt;init_mutex);
 
<span class="p_del">-	alua_rtpg_queue(pg, sdev, qdata, true);</span>
<span class="p_add">+	if (alua_rtpg_queue(pg, sdev, qdata, true))</span>
<span class="p_add">+		fn = NULL;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		err = SCSI_DH_DEV_OFFLINED;</span>
 	kref_put(&amp;pg-&gt;kref, release_port_group);
 out:
 	if (fn)
<span class="p_header">diff --git a/drivers/scsi/libsas/sas_ata.c b/drivers/scsi/libsas/sas_ata.c</span>
<span class="p_header">index 763f012fdeca..87f5e694dbed 100644</span>
<span class="p_header">--- a/drivers/scsi/libsas/sas_ata.c</span>
<span class="p_header">+++ b/drivers/scsi/libsas/sas_ata.c</span>
<span class="p_chunk">@@ -221,7 +221,7 @@</span> <span class="p_context"> static unsigned int sas_ata_qc_issue(struct ata_queued_cmd *qc)</span>
 		task-&gt;num_scatter = qc-&gt;n_elem;
 	} else {
 		for_each_sg(qc-&gt;sg, sg, qc-&gt;n_elem, si)
<span class="p_del">-			xfer += sg-&gt;length;</span>
<span class="p_add">+			xfer += sg_dma_len(sg);</span>
 
 		task-&gt;total_xfer_len = xfer;
 		task-&gt;num_scatter = si;
<span class="p_header">diff --git a/drivers/scsi/qla2xxx/qla_attr.c b/drivers/scsi/qla2xxx/qla_attr.c</span>
<span class="p_header">index fe7469c901f7..ad33238cef17 100644</span>
<span class="p_header">--- a/drivers/scsi/qla2xxx/qla_attr.c</span>
<span class="p_header">+++ b/drivers/scsi/qla2xxx/qla_attr.c</span>
<span class="p_chunk">@@ -2153,8 +2153,6 @@</span> <span class="p_context"> qla24xx_vport_delete(struct fc_vport *fc_vport)</span>
 		    &quot;Timer for the VP[%d] has stopped\n&quot;, vha-&gt;vp_idx);
 	}
 
<span class="p_del">-	BUG_ON(atomic_read(&amp;vha-&gt;vref_count));</span>
<span class="p_del">-</span>
 	qla2x00_free_fcports(vha);
 
 	mutex_lock(&amp;ha-&gt;vport_lock);
<span class="p_header">diff --git a/drivers/scsi/qla2xxx/qla_def.h b/drivers/scsi/qla2xxx/qla_def.h</span>
<span class="p_header">index 73b12e41d992..8e63a7b90277 100644</span>
<span class="p_header">--- a/drivers/scsi/qla2xxx/qla_def.h</span>
<span class="p_header">+++ b/drivers/scsi/qla2xxx/qla_def.h</span>
<span class="p_chunk">@@ -3742,6 +3742,7 @@</span> <span class="p_context"> typedef struct scsi_qla_host {</span>
 	struct qla8044_reset_template reset_tmplt;
 	struct qla_tgt_counters tgt_counters;
 	uint16_t	bbcr;
<span class="p_add">+	wait_queue_head_t vref_waitq;</span>
 } scsi_qla_host_t;
 
 struct qla27xx_image_status {
<span class="p_chunk">@@ -3780,6 +3781,7 @@</span> <span class="p_context"> struct qla_tgt_vp_map {</span>
 	mb();						     \
 	if (__vha-&gt;flags.delete_progress) {		     \
 		atomic_dec(&amp;__vha-&gt;vref_count);		     \
<span class="p_add">+		wake_up(&amp;__vha-&gt;vref_waitq);		\</span>
 		__bail = 1;				     \
 	} else {					     \
 		__bail = 0;				     \
<span class="p_chunk">@@ -3788,6 +3790,7 @@</span> <span class="p_context"> struct qla_tgt_vp_map {</span>
 
 #define QLA_VHA_MARK_NOT_BUSY(__vha) do {		     \
 	atomic_dec(&amp;__vha-&gt;vref_count);			     \
<span class="p_add">+	wake_up(&amp;__vha-&gt;vref_waitq);			\</span>
 } while (0)
 
 /*
<span class="p_header">diff --git a/drivers/scsi/qla2xxx/qla_init.c b/drivers/scsi/qla2xxx/qla_init.c</span>
<span class="p_header">index 5b09296b46a3..8f12f6baa6b8 100644</span>
<span class="p_header">--- a/drivers/scsi/qla2xxx/qla_init.c</span>
<span class="p_header">+++ b/drivers/scsi/qla2xxx/qla_init.c</span>
<span class="p_chunk">@@ -4356,6 +4356,7 @@</span> <span class="p_context"> qla2x00_update_fcports(scsi_qla_host_t *base_vha)</span>
 			}
 		}
 		atomic_dec(&amp;vha-&gt;vref_count);
<span class="p_add">+		wake_up(&amp;vha-&gt;vref_waitq);</span>
 	}
 	spin_unlock_irqrestore(&amp;ha-&gt;vport_slock, flags);
 }
<span class="p_header">diff --git a/drivers/scsi/qla2xxx/qla_mid.c b/drivers/scsi/qla2xxx/qla_mid.c</span>
<span class="p_header">index cf7ba52bae66..3dfb54abc874 100644</span>
<span class="p_header">--- a/drivers/scsi/qla2xxx/qla_mid.c</span>
<span class="p_header">+++ b/drivers/scsi/qla2xxx/qla_mid.c</span>
<span class="p_chunk">@@ -74,13 +74,14 @@</span> <span class="p_context"> qla24xx_deallocate_vp_id(scsi_qla_host_t *vha)</span>
 	 * ensures no active vp_list traversal while the vport is removed
 	 * from the queue)
 	 */
<span class="p_del">-	spin_lock_irqsave(&amp;ha-&gt;vport_slock, flags);</span>
<span class="p_del">-	while (atomic_read(&amp;vha-&gt;vref_count)) {</span>
<span class="p_del">-		spin_unlock_irqrestore(&amp;ha-&gt;vport_slock, flags);</span>
<span class="p_del">-</span>
<span class="p_del">-		msleep(500);</span>
<span class="p_add">+	wait_event_timeout(vha-&gt;vref_waitq, atomic_read(&amp;vha-&gt;vref_count),</span>
<span class="p_add">+	    10*HZ);</span>
 
<span class="p_del">-		spin_lock_irqsave(&amp;ha-&gt;vport_slock, flags);</span>
<span class="p_add">+	spin_lock_irqsave(&amp;ha-&gt;vport_slock, flags);</span>
<span class="p_add">+	if (atomic_read(&amp;vha-&gt;vref_count)) {</span>
<span class="p_add">+		ql_dbg(ql_dbg_vport, vha, 0xfffa,</span>
<span class="p_add">+		    &quot;vha-&gt;vref_count=%u timeout\n&quot;, vha-&gt;vref_count.counter);</span>
<span class="p_add">+		vha-&gt;vref_count = (atomic_t)ATOMIC_INIT(0);</span>
 	}
 	list_del(&amp;vha-&gt;list);
 	qlt_update_vp_map(vha, RESET_VP_IDX);
<span class="p_chunk">@@ -269,6 +270,7 @@</span> <span class="p_context"> qla2x00_alert_all_vps(struct rsp_que *rsp, uint16_t *mb)</span>
 
 			spin_lock_irqsave(&amp;ha-&gt;vport_slock, flags);
 			atomic_dec(&amp;vha-&gt;vref_count);
<span class="p_add">+			wake_up(&amp;vha-&gt;vref_waitq);</span>
 		}
 		i++;
 	}
<span class="p_header">diff --git a/drivers/scsi/qla2xxx/qla_os.c b/drivers/scsi/qla2xxx/qla_os.c</span>
<span class="p_header">index bea819e5336d..4f361d8d84be 100644</span>
<span class="p_header">--- a/drivers/scsi/qla2xxx/qla_os.c</span>
<span class="p_header">+++ b/drivers/scsi/qla2xxx/qla_os.c</span>
<span class="p_chunk">@@ -4045,6 +4045,7 @@</span> <span class="p_context"> struct scsi_qla_host *qla2x00_create_host(struct scsi_host_template *sht,</span>
 
 	spin_lock_init(&amp;vha-&gt;work_lock);
 	spin_lock_init(&amp;vha-&gt;cmd_list_lock);
<span class="p_add">+	init_waitqueue_head(&amp;vha-&gt;vref_waitq);</span>
 
 	sprintf(vha-&gt;host_str, &quot;%s_%ld&quot;, QLA2XXX_DRIVER_NAME, vha-&gt;host_no);
 	ql_dbg(ql_dbg_init, vha, 0x0041,
<span class="p_header">diff --git a/drivers/scsi/sg.c b/drivers/scsi/sg.c</span>
<span class="p_header">index 121de0aaa6ad..f753df25ba34 100644</span>
<span class="p_header">--- a/drivers/scsi/sg.c</span>
<span class="p_header">+++ b/drivers/scsi/sg.c</span>
<span class="p_chunk">@@ -998,6 +998,8 @@</span> <span class="p_context"> sg_ioctl(struct file *filp, unsigned int cmd_in, unsigned long arg)</span>
 		result = get_user(val, ip);
 		if (result)
 			return result;
<span class="p_add">+		if (val &gt; SG_MAX_CDB_SIZE)</span>
<span class="p_add">+			return -ENOMEM;</span>
 		sfp-&gt;next_cmd_len = (val &gt; 0) ? val : 0;
 		return 0;
 	case SG_GET_VERSION_NUM:
<span class="p_header">diff --git a/drivers/tty/serial/atmel_serial.c b/drivers/tty/serial/atmel_serial.c</span>
<span class="p_header">index fabbe76203bb..4d079cdaa7a3 100644</span>
<span class="p_header">--- a/drivers/tty/serial/atmel_serial.c</span>
<span class="p_header">+++ b/drivers/tty/serial/atmel_serial.c</span>
<span class="p_chunk">@@ -1938,6 +1938,11 @@</span> <span class="p_context"> static void atmel_flush_buffer(struct uart_port *port)</span>
 		atmel_uart_writel(port, ATMEL_PDC_TCR, 0);
 		atmel_port-&gt;pdc_tx.ofs = 0;
 	}
<span class="p_add">+	/*</span>
<span class="p_add">+	 * in uart_flush_buffer(), the xmit circular buffer has just</span>
<span class="p_add">+	 * been cleared, so we have to reset tx_len accordingly.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	atmel_port-&gt;tx_len = 0;</span>
 }
 
 /*
<span class="p_chunk">@@ -2471,6 +2476,9 @@</span> <span class="p_context"> static void atmel_console_write(struct console *co, const char *s, u_int count)</span>
 	pdc_tx = atmel_uart_readl(port, ATMEL_PDC_PTSR) &amp; ATMEL_PDC_TXTEN;
 	atmel_uart_writel(port, ATMEL_PDC_PTCR, ATMEL_PDC_TXTDIS);
 
<span class="p_add">+	/* Make sure that tx path is actually able to send characters */</span>
<span class="p_add">+	atmel_uart_writel(port, ATMEL_US_CR, ATMEL_US_TXEN);</span>
<span class="p_add">+</span>
 	uart_console_write(port, s, count, atmel_console_putchar);
 
 	/*
<span class="p_header">diff --git a/drivers/tty/serial/mxs-auart.c b/drivers/tty/serial/mxs-auart.c</span>
<span class="p_header">index 770454e0dfa3..07390f8c3681 100644</span>
<span class="p_header">--- a/drivers/tty/serial/mxs-auart.c</span>
<span class="p_header">+++ b/drivers/tty/serial/mxs-auart.c</span>
<span class="p_chunk">@@ -1085,7 +1085,7 @@</span> <span class="p_context"> static void mxs_auart_settermios(struct uart_port *u,</span>
 					AUART_LINECTRL_BAUD_DIV_MAX);
 		baud_max = u-&gt;uartclk * 32 / AUART_LINECTRL_BAUD_DIV_MIN;
 		baud = uart_get_baud_rate(u, termios, old, baud_min, baud_max);
<span class="p_del">-		div = u-&gt;uartclk * 32 / baud;</span>
<span class="p_add">+		div = DIV_ROUND_CLOSEST(u-&gt;uartclk * 32, baud);</span>
 	}
 
 	ctrl |= AUART_LINECTRL_BAUD_DIVFRAC(div &amp; 0x3F);
<span class="p_header">diff --git a/drivers/usb/core/hcd.c b/drivers/usb/core/hcd.c</span>
<span class="p_header">index 479e223f9cff..f029aad67183 100644</span>
<span class="p_header">--- a/drivers/usb/core/hcd.c</span>
<span class="p_header">+++ b/drivers/usb/core/hcd.c</span>
<span class="p_chunk">@@ -520,8 +520,10 @@</span> <span class="p_context"> static int rh_call_control (struct usb_hcd *hcd, struct urb *urb)</span>
 	 */
 	tbuf_size =  max_t(u16, sizeof(struct usb_hub_descriptor), wLength);
 	tbuf = kzalloc(tbuf_size, GFP_KERNEL);
<span class="p_del">-	if (!tbuf)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_add">+	if (!tbuf) {</span>
<span class="p_add">+		status = -ENOMEM;</span>
<span class="p_add">+		goto err_alloc;</span>
<span class="p_add">+	}</span>
 
 	bufp = tbuf;
 
<span class="p_chunk">@@ -734,6 +736,7 @@</span> <span class="p_context"> static int rh_call_control (struct usb_hcd *hcd, struct urb *urb)</span>
 	}
 
 	kfree(tbuf);
<span class="p_add">+ err_alloc:</span>
 
 	/* any errors get returned through the urb completion */
 	spin_lock_irq(&amp;hcd_root_hub_lock);
<span class="p_header">diff --git a/fs/nfs/nfs4proc.c b/fs/nfs/nfs4proc.c</span>
<span class="p_header">index 1536aeb0abab..4e894d301c88 100644</span>
<span class="p_header">--- a/fs/nfs/nfs4proc.c</span>
<span class="p_header">+++ b/fs/nfs/nfs4proc.c</span>
<span class="p_chunk">@@ -2532,17 +2532,14 @@</span> <span class="p_context"> static void nfs41_check_delegation_stateid(struct nfs4_state *state)</span>
 	}
 
 	nfs4_stateid_copy(&amp;stateid, &amp;delegation-&gt;stateid);
<span class="p_del">-	if (test_bit(NFS_DELEGATION_REVOKED, &amp;delegation-&gt;flags)) {</span>
<span class="p_add">+	if (test_bit(NFS_DELEGATION_REVOKED, &amp;delegation-&gt;flags) ||</span>
<span class="p_add">+		!test_and_clear_bit(NFS_DELEGATION_TEST_EXPIRED,</span>
<span class="p_add">+			&amp;delegation-&gt;flags)) {</span>
 		rcu_read_unlock();
 		nfs_finish_clear_delegation_stateid(state, &amp;stateid);
 		return;
 	}
 
<span class="p_del">-	if (!test_and_clear_bit(NFS_DELEGATION_TEST_EXPIRED, &amp;delegation-&gt;flags)) {</span>
<span class="p_del">-		rcu_read_unlock();</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	cred = get_rpccred(delegation-&gt;cred);
 	rcu_read_unlock();
 	status = nfs41_test_and_free_expired_stateid(server, &amp;stateid, cred);
<span class="p_header">diff --git a/fs/nfsd/nfsproc.c b/fs/nfsd/nfsproc.c</span>
<span class="p_header">index 010aff5c5a79..536009e50387 100644</span>
<span class="p_header">--- a/fs/nfsd/nfsproc.c</span>
<span class="p_header">+++ b/fs/nfsd/nfsproc.c</span>
<span class="p_chunk">@@ -790,6 +790,7 @@</span> <span class="p_context"> nfserrno (int errno)</span>
 		{ nfserr_serverfault, -ESERVERFAULT },
 		{ nfserr_serverfault, -ENFILE },
 		{ nfserr_io, -EUCLEAN },
<span class="p_add">+		{ nfserr_perm, -ENOKEY },</span>
 	};
 	int	i;
 
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_ag_resv.c b/fs/xfs/libxfs/xfs_ag_resv.c</span>
<span class="p_header">index d346d42c54d1..33db69be4832 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_ag_resv.c</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_ag_resv.c</span>
<span class="p_chunk">@@ -39,6 +39,7 @@</span> <span class="p_context"></span>
 #include &quot;xfs_rmap_btree.h&quot;
 #include &quot;xfs_btree.h&quot;
 #include &quot;xfs_refcount_btree.h&quot;
<span class="p_add">+#include &quot;xfs_ialloc_btree.h&quot;</span>
 
 /*
  * Per-AG Block Reservations
<span class="p_chunk">@@ -200,22 +201,30 @@</span> <span class="p_context"> __xfs_ag_resv_init(</span>
 	struct xfs_mount		*mp = pag-&gt;pag_mount;
 	struct xfs_ag_resv		*resv;
 	int				error;
<span class="p_add">+	xfs_extlen_t			reserved;</span>
 
<span class="p_del">-	resv = xfs_perag_resv(pag, type);</span>
 	if (used &gt; ask)
 		ask = used;
<span class="p_del">-	resv-&gt;ar_asked = ask;</span>
<span class="p_del">-	resv-&gt;ar_reserved = resv-&gt;ar_orig_reserved = ask - used;</span>
<span class="p_del">-	mp-&gt;m_ag_max_usable -= ask;</span>
<span class="p_add">+	reserved = ask - used;</span>
 
<span class="p_del">-	trace_xfs_ag_resv_init(pag, type, ask);</span>
<span class="p_del">-</span>
<span class="p_del">-	error = xfs_mod_fdblocks(mp, -(int64_t)resv-&gt;ar_reserved, true);</span>
<span class="p_del">-	if (error)</span>
<span class="p_add">+	error = xfs_mod_fdblocks(mp, -(int64_t)reserved, true);</span>
<span class="p_add">+	if (error) {</span>
 		trace_xfs_ag_resv_init_error(pag-&gt;pag_mount, pag-&gt;pag_agno,
 				error, _RET_IP_);
<span class="p_add">+		xfs_warn(mp,</span>
<span class="p_add">+&quot;Per-AG reservation for AG %u failed.  Filesystem may run out of space.&quot;,</span>
<span class="p_add">+				pag-&gt;pag_agno);</span>
<span class="p_add">+		return error;</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	return error;</span>
<span class="p_add">+	mp-&gt;m_ag_max_usable -= ask;</span>
<span class="p_add">+</span>
<span class="p_add">+	resv = xfs_perag_resv(pag, type);</span>
<span class="p_add">+	resv-&gt;ar_asked = ask;</span>
<span class="p_add">+	resv-&gt;ar_reserved = resv-&gt;ar_orig_reserved = reserved;</span>
<span class="p_add">+</span>
<span class="p_add">+	trace_xfs_ag_resv_init(pag, type, ask);</span>
<span class="p_add">+	return 0;</span>
 }
 
 /* Create a per-AG block reservation. */
<span class="p_chunk">@@ -223,6 +232,8 @@</span> <span class="p_context"> int</span>
 xfs_ag_resv_init(
 	struct xfs_perag		*pag)
 {
<span class="p_add">+	struct xfs_mount		*mp = pag-&gt;pag_mount;</span>
<span class="p_add">+	xfs_agnumber_t			agno = pag-&gt;pag_agno;</span>
 	xfs_extlen_t			ask;
 	xfs_extlen_t			used;
 	int				error = 0;
<span class="p_chunk">@@ -231,23 +242,45 @@</span> <span class="p_context"> xfs_ag_resv_init(</span>
 	if (pag-&gt;pag_meta_resv.ar_asked == 0) {
 		ask = used = 0;
 
<span class="p_del">-		error = xfs_refcountbt_calc_reserves(pag-&gt;pag_mount,</span>
<span class="p_del">-				pag-&gt;pag_agno, &amp;ask, &amp;used);</span>
<span class="p_add">+		error = xfs_refcountbt_calc_reserves(mp, agno, &amp;ask, &amp;used);</span>
 		if (error)
 			goto out;
 
<span class="p_del">-		error = __xfs_ag_resv_init(pag, XFS_AG_RESV_METADATA,</span>
<span class="p_del">-				ask, used);</span>
<span class="p_add">+		error = xfs_finobt_calc_reserves(mp, agno, &amp;ask, &amp;used);</span>
 		if (error)
 			goto out;
<span class="p_add">+</span>
<span class="p_add">+		error = __xfs_ag_resv_init(pag, XFS_AG_RESV_METADATA,</span>
<span class="p_add">+				ask, used);</span>
<span class="p_add">+		if (error) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Because we didn&#39;t have per-AG reservations when the</span>
<span class="p_add">+			 * finobt feature was added we might not be able to</span>
<span class="p_add">+			 * reserve all needed blocks.  Warn and fall back to the</span>
<span class="p_add">+			 * old and potentially buggy code in that case, but</span>
<span class="p_add">+			 * ensure we do have the reservation for the refcountbt.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			ask = used = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+			mp-&gt;m_inotbt_nores = true;</span>
<span class="p_add">+</span>
<span class="p_add">+			error = xfs_refcountbt_calc_reserves(mp, agno, &amp;ask,</span>
<span class="p_add">+					&amp;used);</span>
<span class="p_add">+			if (error)</span>
<span class="p_add">+				goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+			error = __xfs_ag_resv_init(pag, XFS_AG_RESV_METADATA,</span>
<span class="p_add">+					ask, used);</span>
<span class="p_add">+			if (error)</span>
<span class="p_add">+				goto out;</span>
<span class="p_add">+		}</span>
 	}
 
 	/* Create the AGFL metadata reservation */
 	if (pag-&gt;pag_agfl_resv.ar_asked == 0) {
 		ask = used = 0;
 
<span class="p_del">-		error = xfs_rmapbt_calc_reserves(pag-&gt;pag_mount, pag-&gt;pag_agno,</span>
<span class="p_del">-				&amp;ask, &amp;used);</span>
<span class="p_add">+		error = xfs_rmapbt_calc_reserves(mp, agno, &amp;ask, &amp;used);</span>
 		if (error)
 			goto out;
 
<span class="p_chunk">@@ -256,9 +289,16 @@</span> <span class="p_context"> xfs_ag_resv_init(</span>
 			goto out;
 	}
 
<span class="p_add">+#ifdef DEBUG</span>
<span class="p_add">+	/* need to read in the AGF for the ASSERT below to work */</span>
<span class="p_add">+	error = xfs_alloc_pagf_init(pag-&gt;pag_mount, NULL, pag-&gt;pag_agno, 0);</span>
<span class="p_add">+	if (error)</span>
<span class="p_add">+		return error;</span>
<span class="p_add">+</span>
 	ASSERT(xfs_perag_resv(pag, XFS_AG_RESV_METADATA)-&gt;ar_reserved +
 	       xfs_perag_resv(pag, XFS_AG_RESV_AGFL)-&gt;ar_reserved &lt;=
 	       pag-&gt;pagf_freeblks + pag-&gt;pagf_flcount);
<span class="p_add">+#endif</span>
 out:
 	return error;
 }
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_bmap.c b/fs/xfs/libxfs/xfs_bmap.c</span>
<span class="p_header">index f52fd63fce19..5a508b011e27 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_bmap.c</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_bmap.c</span>
<span class="p_chunk">@@ -769,8 +769,8 @@</span> <span class="p_context"> xfs_bmap_extents_to_btree(</span>
 		args.type = XFS_ALLOCTYPE_START_BNO;
 		args.fsbno = XFS_INO_TO_FSB(mp, ip-&gt;i_ino);
 	} else if (dfops-&gt;dop_low) {
<span class="p_del">-try_another_ag:</span>
 		args.type = XFS_ALLOCTYPE_START_BNO;
<span class="p_add">+try_another_ag:</span>
 		args.fsbno = *firstblock;
 	} else {
 		args.type = XFS_ALLOCTYPE_NEAR_BNO;
<span class="p_chunk">@@ -796,17 +796,19 @@</span> <span class="p_context"> xfs_bmap_extents_to_btree(</span>
 	if (xfs_sb_version_hasreflink(&amp;cur-&gt;bc_mp-&gt;m_sb) &amp;&amp;
 	    args.fsbno == NULLFSBLOCK &amp;&amp;
 	    args.type == XFS_ALLOCTYPE_NEAR_BNO) {
<span class="p_del">-		dfops-&gt;dop_low = true;</span>
<span class="p_add">+		args.type = XFS_ALLOCTYPE_FIRST_AG;</span>
 		goto try_another_ag;
 	}
<span class="p_add">+	if (WARN_ON_ONCE(args.fsbno == NULLFSBLOCK)) {</span>
<span class="p_add">+		xfs_iroot_realloc(ip, -1, whichfork);</span>
<span class="p_add">+		xfs_btree_del_cursor(cur, XFS_BTREE_ERROR);</span>
<span class="p_add">+		return -ENOSPC;</span>
<span class="p_add">+	}</span>
 	/*
 	 * Allocation can&#39;t fail, the space was reserved.
 	 */
<span class="p_del">-	ASSERT(args.fsbno != NULLFSBLOCK);</span>
 	ASSERT(*firstblock == NULLFSBLOCK ||
<span class="p_del">-	       args.agno == XFS_FSB_TO_AGNO(mp, *firstblock) ||</span>
<span class="p_del">-	       (dfops-&gt;dop_low &amp;&amp;</span>
<span class="p_del">-		args.agno &gt; XFS_FSB_TO_AGNO(mp, *firstblock)));</span>
<span class="p_add">+	       args.agno &gt;= XFS_FSB_TO_AGNO(mp, *firstblock));</span>
 	*firstblock = cur-&gt;bc_private.b.firstblock = args.fsbno;
 	cur-&gt;bc_private.b.allocated++;
 	ip-&gt;i_d.di_nblocks++;
<span class="p_chunk">@@ -1278,7 +1280,6 @@</span> <span class="p_context"> xfs_bmap_read_extents(</span>
 	/* REFERENCED */
 	xfs_extnum_t		room;	/* number of entries there&#39;s room for */
 
<span class="p_del">-	bno = NULLFSBLOCK;</span>
 	mp = ip-&gt;i_mount;
 	ifp = XFS_IFORK_PTR(ip, whichfork);
 	exntf = (whichfork != XFS_DATA_FORK) ? XFS_EXTFMT_NOSTATE :
<span class="p_chunk">@@ -1291,9 +1292,7 @@</span> <span class="p_context"> xfs_bmap_read_extents(</span>
 	ASSERT(level &gt; 0);
 	pp = XFS_BMAP_BROOT_PTR_ADDR(mp, block, 1, ifp-&gt;if_broot_bytes);
 	bno = be64_to_cpu(*pp);
<span class="p_del">-	ASSERT(bno != NULLFSBLOCK);</span>
<span class="p_del">-	ASSERT(XFS_FSB_TO_AGNO(mp, bno) &lt; mp-&gt;m_sb.sb_agcount);</span>
<span class="p_del">-	ASSERT(XFS_FSB_TO_AGBNO(mp, bno) &lt; mp-&gt;m_sb.sb_agblocks);</span>
<span class="p_add">+</span>
 	/*
 	 * Go down the tree until leaf level is reached, following the first
 	 * pointer (leftmost) at each level.
<span class="p_chunk">@@ -1955,6 +1954,7 @@</span> <span class="p_context"> xfs_bmap_add_extent_delay_real(</span>
 		 */
 		trace_xfs_bmap_pre_update(bma-&gt;ip, bma-&gt;idx, state, _THIS_IP_);
 		xfs_bmbt_set_startblock(ep, new-&gt;br_startblock);
<span class="p_add">+		xfs_bmbt_set_state(ep, new-&gt;br_state);</span>
 		trace_xfs_bmap_post_update(bma-&gt;ip, bma-&gt;idx, state, _THIS_IP_);
 
 		(*nextents)++;
<span class="p_chunk">@@ -2293,6 +2293,7 @@</span> <span class="p_context"> STATIC int				/* error */</span>
 xfs_bmap_add_extent_unwritten_real(
 	struct xfs_trans	*tp,
 	xfs_inode_t		*ip,	/* incore inode pointer */
<span class="p_add">+	int			whichfork,</span>
 	xfs_extnum_t		*idx,	/* extent number to update/insert */
 	xfs_btree_cur_t		**curp,	/* if *curp is null, not a btree */
 	xfs_bmbt_irec_t		*new,	/* new data to add to file extents */
<span class="p_chunk">@@ -2312,12 +2313,14 @@</span> <span class="p_context"> xfs_bmap_add_extent_unwritten_real(</span>
 					/* left is 0, right is 1, prev is 2 */
 	int			rval=0;	/* return value (logging flags) */
 	int			state = 0;/* state bits, accessed thru macros */
<span class="p_del">-	struct xfs_mount	*mp = tp-&gt;t_mountp;</span>
<span class="p_add">+	struct xfs_mount	*mp = ip-&gt;i_mount;</span>
 
 	*logflagsp = 0;
 
 	cur = *curp;
<span class="p_del">-	ifp = XFS_IFORK_PTR(ip, XFS_DATA_FORK);</span>
<span class="p_add">+	ifp = XFS_IFORK_PTR(ip, whichfork);</span>
<span class="p_add">+	if (whichfork == XFS_COW_FORK)</span>
<span class="p_add">+		state |= BMAP_COWFORK;</span>
 
 	ASSERT(*idx &gt;= 0);
 	ASSERT(*idx &lt;= xfs_iext_count(ifp));
<span class="p_chunk">@@ -2376,7 +2379,7 @@</span> <span class="p_context"> xfs_bmap_add_extent_unwritten_real(</span>
 	 * Don&#39;t set contiguous if the combined extent would be too large.
 	 * Also check for all-three-contiguous being too large.
 	 */
<span class="p_del">-	if (*idx &lt; xfs_iext_count(&amp;ip-&gt;i_df) - 1) {</span>
<span class="p_add">+	if (*idx &lt; xfs_iext_count(ifp) - 1) {</span>
 		state |= BMAP_RIGHT_VALID;
 		xfs_bmbt_get_all(xfs_iext_get_ext(ifp, *idx + 1), &amp;RIGHT);
 		if (isnullstartblock(RIGHT.br_startblock))
<span class="p_chunk">@@ -2416,7 +2419,8 @@</span> <span class="p_context"> xfs_bmap_add_extent_unwritten_real(</span>
 		trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
 
 		xfs_iext_remove(ip, *idx + 1, 2, state);
<span class="p_del">-		ip-&gt;i_d.di_nextents -= 2;</span>
<span class="p_add">+		XFS_IFORK_NEXT_SET(ip, whichfork,</span>
<span class="p_add">+				XFS_IFORK_NEXTENTS(ip, whichfork) - 2);</span>
 		if (cur == NULL)
 			rval = XFS_ILOG_CORE | XFS_ILOG_DEXT;
 		else {
<span class="p_chunk">@@ -2459,7 +2463,8 @@</span> <span class="p_context"> xfs_bmap_add_extent_unwritten_real(</span>
 		trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
 
 		xfs_iext_remove(ip, *idx + 1, 1, state);
<span class="p_del">-		ip-&gt;i_d.di_nextents--;</span>
<span class="p_add">+		XFS_IFORK_NEXT_SET(ip, whichfork,</span>
<span class="p_add">+				XFS_IFORK_NEXTENTS(ip, whichfork) - 1);</span>
 		if (cur == NULL)
 			rval = XFS_ILOG_CORE | XFS_ILOG_DEXT;
 		else {
<span class="p_chunk">@@ -2494,7 +2499,8 @@</span> <span class="p_context"> xfs_bmap_add_extent_unwritten_real(</span>
 		xfs_bmbt_set_state(ep, newext);
 		trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
 		xfs_iext_remove(ip, *idx + 1, 1, state);
<span class="p_del">-		ip-&gt;i_d.di_nextents--;</span>
<span class="p_add">+		XFS_IFORK_NEXT_SET(ip, whichfork,</span>
<span class="p_add">+				XFS_IFORK_NEXTENTS(ip, whichfork) - 1);</span>
 		if (cur == NULL)
 			rval = XFS_ILOG_CORE | XFS_ILOG_DEXT;
 		else {
<span class="p_chunk">@@ -2606,7 +2612,8 @@</span> <span class="p_context"> xfs_bmap_add_extent_unwritten_real(</span>
 		trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
 
 		xfs_iext_insert(ip, *idx, 1, new, state);
<span class="p_del">-		ip-&gt;i_d.di_nextents++;</span>
<span class="p_add">+		XFS_IFORK_NEXT_SET(ip, whichfork,</span>
<span class="p_add">+				XFS_IFORK_NEXTENTS(ip, whichfork) + 1);</span>
 		if (cur == NULL)
 			rval = XFS_ILOG_CORE | XFS_ILOG_DEXT;
 		else {
<span class="p_chunk">@@ -2684,7 +2691,8 @@</span> <span class="p_context"> xfs_bmap_add_extent_unwritten_real(</span>
 		++*idx;
 		xfs_iext_insert(ip, *idx, 1, new, state);
 
<span class="p_del">-		ip-&gt;i_d.di_nextents++;</span>
<span class="p_add">+		XFS_IFORK_NEXT_SET(ip, whichfork,</span>
<span class="p_add">+				XFS_IFORK_NEXTENTS(ip, whichfork) + 1);</span>
 		if (cur == NULL)
 			rval = XFS_ILOG_CORE | XFS_ILOG_DEXT;
 		else {
<span class="p_chunk">@@ -2732,7 +2740,8 @@</span> <span class="p_context"> xfs_bmap_add_extent_unwritten_real(</span>
 		++*idx;
 		xfs_iext_insert(ip, *idx, 2, &amp;r[0], state);
 
<span class="p_del">-		ip-&gt;i_d.di_nextents += 2;</span>
<span class="p_add">+		XFS_IFORK_NEXT_SET(ip, whichfork,</span>
<span class="p_add">+				XFS_IFORK_NEXTENTS(ip, whichfork) + 2);</span>
 		if (cur == NULL)
 			rval = XFS_ILOG_CORE | XFS_ILOG_DEXT;
 		else {
<span class="p_chunk">@@ -2786,17 +2795,17 @@</span> <span class="p_context"> xfs_bmap_add_extent_unwritten_real(</span>
 	}
 
 	/* update reverse mappings */
<span class="p_del">-	error = xfs_rmap_convert_extent(mp, dfops, ip, XFS_DATA_FORK, new);</span>
<span class="p_add">+	error = xfs_rmap_convert_extent(mp, dfops, ip, whichfork, new);</span>
 	if (error)
 		goto done;
 
 	/* convert to a btree if necessary */
<span class="p_del">-	if (xfs_bmap_needs_btree(ip, XFS_DATA_FORK)) {</span>
<span class="p_add">+	if (xfs_bmap_needs_btree(ip, whichfork)) {</span>
 		int	tmp_logflags;	/* partial log flag return val */
 
 		ASSERT(cur == NULL);
 		error = xfs_bmap_extents_to_btree(tp, ip, first, dfops, &amp;cur,
<span class="p_del">-				0, &amp;tmp_logflags, XFS_DATA_FORK);</span>
<span class="p_add">+				0, &amp;tmp_logflags, whichfork);</span>
 		*logflagsp |= tmp_logflags;
 		if (error)
 			goto done;
<span class="p_chunk">@@ -2808,7 +2817,7 @@</span> <span class="p_context"> xfs_bmap_add_extent_unwritten_real(</span>
 		*curp = cur;
 	}
 
<span class="p_del">-	xfs_bmap_check_leaf_extents(*curp, ip, XFS_DATA_FORK);</span>
<span class="p_add">+	xfs_bmap_check_leaf_extents(*curp, ip, whichfork);</span>
 done:
 	*logflagsp |= rval;
 	return error;
<span class="p_chunk">@@ -2900,7 +2909,8 @@</span> <span class="p_context"> xfs_bmap_add_extent_hole_delay(</span>
 		oldlen = startblockval(left.br_startblock) +
 			startblockval(new-&gt;br_startblock) +
 			startblockval(right.br_startblock);
<span class="p_del">-		newlen = xfs_bmap_worst_indlen(ip, temp);</span>
<span class="p_add">+		newlen = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(ip, temp),</span>
<span class="p_add">+					 oldlen);</span>
 		xfs_bmbt_set_startblock(xfs_iext_get_ext(ifp, *idx),
 			nullstartblock((int)newlen));
 		trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
<span class="p_chunk">@@ -2921,7 +2931,8 @@</span> <span class="p_context"> xfs_bmap_add_extent_hole_delay(</span>
 		xfs_bmbt_set_blockcount(xfs_iext_get_ext(ifp, *idx), temp);
 		oldlen = startblockval(left.br_startblock) +
 			startblockval(new-&gt;br_startblock);
<span class="p_del">-		newlen = xfs_bmap_worst_indlen(ip, temp);</span>
<span class="p_add">+		newlen = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(ip, temp),</span>
<span class="p_add">+					 oldlen);</span>
 		xfs_bmbt_set_startblock(xfs_iext_get_ext(ifp, *idx),
 			nullstartblock((int)newlen));
 		trace_xfs_bmap_post_update(ip, *idx, state, _THIS_IP_);
<span class="p_chunk">@@ -2937,7 +2948,8 @@</span> <span class="p_context"> xfs_bmap_add_extent_hole_delay(</span>
 		temp = new-&gt;br_blockcount + right.br_blockcount;
 		oldlen = startblockval(new-&gt;br_startblock) +
 			startblockval(right.br_startblock);
<span class="p_del">-		newlen = xfs_bmap_worst_indlen(ip, temp);</span>
<span class="p_add">+		newlen = XFS_FILBLKS_MIN(xfs_bmap_worst_indlen(ip, temp),</span>
<span class="p_add">+					 oldlen);</span>
 		xfs_bmbt_set_allf(xfs_iext_get_ext(ifp, *idx),
 			new-&gt;br_startoff,
 			nullstartblock((int)newlen), temp, right.br_state);
<span class="p_chunk">@@ -3913,17 +3925,13 @@</span> <span class="p_context"> xfs_bmap_btalloc(</span>
 		 * the first block that was allocated.
 		 */
 		ASSERT(*ap-&gt;firstblock == NULLFSBLOCK ||
<span class="p_del">-		       XFS_FSB_TO_AGNO(mp, *ap-&gt;firstblock) ==</span>
<span class="p_del">-		       XFS_FSB_TO_AGNO(mp, args.fsbno) ||</span>
<span class="p_del">-		       (ap-&gt;dfops-&gt;dop_low &amp;&amp;</span>
<span class="p_del">-			XFS_FSB_TO_AGNO(mp, *ap-&gt;firstblock) &lt;</span>
<span class="p_del">-			XFS_FSB_TO_AGNO(mp, args.fsbno)));</span>
<span class="p_add">+		       XFS_FSB_TO_AGNO(mp, *ap-&gt;firstblock) &lt;=</span>
<span class="p_add">+		       XFS_FSB_TO_AGNO(mp, args.fsbno));</span>
 
 		ap-&gt;blkno = args.fsbno;
 		if (*ap-&gt;firstblock == NULLFSBLOCK)
 			*ap-&gt;firstblock = args.fsbno;
<span class="p_del">-		ASSERT(nullfb || fb_agno == args.agno ||</span>
<span class="p_del">-		       (ap-&gt;dfops-&gt;dop_low &amp;&amp; fb_agno &lt; args.agno));</span>
<span class="p_add">+		ASSERT(nullfb || fb_agno &lt;= args.agno);</span>
 		ap-&gt;length = args.len;
 		if (!(ap-&gt;flags &amp; XFS_BMAPI_COWFORK))
 			ap-&gt;ip-&gt;i_d.di_nblocks += args.len;
<span class="p_chunk">@@ -4249,6 +4257,19 @@</span> <span class="p_context"> xfs_bmapi_read(</span>
 	return 0;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Add a delayed allocation extent to an inode. Blocks are reserved from the</span>
<span class="p_add">+ * global pool and the extent inserted into the inode in-core extent tree.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On entry, got refers to the first extent beyond the offset of the extent to</span>
<span class="p_add">+ * allocate or eof is specified if no such extent exists. On return, got refers</span>
<span class="p_add">+ * to the extent record that was inserted to the inode fork.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that the allocated extent may have been merged with contiguous extents</span>
<span class="p_add">+ * during insertion into the inode fork. Thus, got does not reflect the current</span>
<span class="p_add">+ * state of the inode fork on return. If necessary, the caller can use lastx to</span>
<span class="p_add">+ * look up the updated record in the inode fork.</span>
<span class="p_add">+ */</span>
 int
 xfs_bmapi_reserve_delalloc(
 	struct xfs_inode	*ip,
<span class="p_chunk">@@ -4335,13 +4356,8 @@</span> <span class="p_context"> xfs_bmapi_reserve_delalloc(</span>
 	got-&gt;br_startblock = nullstartblock(indlen);
 	got-&gt;br_blockcount = alen;
 	got-&gt;br_state = XFS_EXT_NORM;
<span class="p_del">-	xfs_bmap_add_extent_hole_delay(ip, whichfork, lastx, got);</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Update our extent pointer, given that xfs_bmap_add_extent_hole_delay</span>
<span class="p_del">-	 * might have merged it into one of the neighbouring ones.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	xfs_bmbt_get_all(xfs_iext_get_ext(ifp, *lastx), got);</span>
<span class="p_add">+	xfs_bmap_add_extent_hole_delay(ip, whichfork, lastx, got);</span>
 
 	/*
 	 * Tag the inode if blocks were preallocated. Note that COW fork
<span class="p_chunk">@@ -4353,10 +4369,6 @@</span> <span class="p_context"> xfs_bmapi_reserve_delalloc(</span>
 	if (whichfork == XFS_COW_FORK &amp;&amp; (prealloc || aoff &lt; off || alen &gt; len))
 		xfs_inode_set_cowblocks_tag(ip);
 
<span class="p_del">-	ASSERT(got-&gt;br_startoff &lt;= aoff);</span>
<span class="p_del">-	ASSERT(got-&gt;br_startoff + got-&gt;br_blockcount &gt;= aoff + alen);</span>
<span class="p_del">-	ASSERT(isnullstartblock(got-&gt;br_startblock));</span>
<span class="p_del">-	ASSERT(got-&gt;br_state == XFS_EXT_NORM);</span>
 	return 0;
 
 out_unreserve_blocks:
<span class="p_chunk">@@ -4461,10 +4473,16 @@</span> <span class="p_context"> xfs_bmapi_allocate(</span>
 	bma-&gt;got.br_state = XFS_EXT_NORM;
 
 	/*
<span class="p_del">-	 * A wasdelay extent has been initialized, so shouldn&#39;t be flagged</span>
<span class="p_del">-	 * as unwritten.</span>
<span class="p_add">+	 * In the data fork, a wasdelay extent has been initialized, so</span>
<span class="p_add">+	 * shouldn&#39;t be flagged as unwritten.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * For the cow fork, however, we convert delalloc reservations</span>
<span class="p_add">+	 * (extents allocated for speculative preallocation) to</span>
<span class="p_add">+	 * allocated unwritten extents, and only convert the unwritten</span>
<span class="p_add">+	 * extents to real extents when we&#39;re about to write the data.</span>
 	 */
<span class="p_del">-	if (!bma-&gt;wasdel &amp;&amp; (bma-&gt;flags &amp; XFS_BMAPI_PREALLOC) &amp;&amp;</span>
<span class="p_add">+	if ((!bma-&gt;wasdel || (bma-&gt;flags &amp; XFS_BMAPI_COWFORK)) &amp;&amp;</span>
<span class="p_add">+	    (bma-&gt;flags &amp; XFS_BMAPI_PREALLOC) &amp;&amp;</span>
 	    xfs_sb_version_hasextflgbit(&amp;mp-&gt;m_sb))
 		bma-&gt;got.br_state = XFS_EXT_UNWRITTEN;
 
<span class="p_chunk">@@ -4515,8 +4533,6 @@</span> <span class="p_context"> xfs_bmapi_convert_unwritten(</span>
 			(XFS_BMAPI_PREALLOC | XFS_BMAPI_CONVERT))
 		return 0;
 
<span class="p_del">-	ASSERT(whichfork != XFS_COW_FORK);</span>
<span class="p_del">-</span>
 	/*
 	 * Modify (by adding) the state flag, if writing.
 	 */
<span class="p_chunk">@@ -4541,8 +4557,8 @@</span> <span class="p_context"> xfs_bmapi_convert_unwritten(</span>
 			return error;
 	}
 
<span class="p_del">-	error = xfs_bmap_add_extent_unwritten_real(bma-&gt;tp, bma-&gt;ip, &amp;bma-&gt;idx,</span>
<span class="p_del">-			&amp;bma-&gt;cur, mval, bma-&gt;firstblock, bma-&gt;dfops,</span>
<span class="p_add">+	error = xfs_bmap_add_extent_unwritten_real(bma-&gt;tp, bma-&gt;ip, whichfork,</span>
<span class="p_add">+			&amp;bma-&gt;idx, &amp;bma-&gt;cur, mval, bma-&gt;firstblock, bma-&gt;dfops,</span>
 			&amp;tmp_logflags);
 	/*
 	 * Log the inode core unconditionally in the unwritten extent conversion
<span class="p_chunk">@@ -4551,8 +4567,12 @@</span> <span class="p_context"> xfs_bmapi_convert_unwritten(</span>
 	 * in the transaction for the sake of fsync(), even if nothing has
 	 * changed, because fsync() will not force the log for this transaction
 	 * unless it sees the inode pinned.
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note: If we&#39;re only converting cow fork extents, there aren&#39;t</span>
<span class="p_add">+	 * any on-disk updates to make, so we don&#39;t need to log anything.</span>
 	 */
<span class="p_del">-	bma-&gt;logflags |= tmp_logflags | XFS_ILOG_CORE;</span>
<span class="p_add">+	if (whichfork != XFS_COW_FORK)</span>
<span class="p_add">+		bma-&gt;logflags |= tmp_logflags | XFS_ILOG_CORE;</span>
 	if (error)
 		return error;
 
<span class="p_chunk">@@ -4626,15 +4646,15 @@</span> <span class="p_context"> xfs_bmapi_write(</span>
 	ASSERT(*nmap &gt;= 1);
 	ASSERT(*nmap &lt;= XFS_BMAP_MAX_NMAP);
 	ASSERT(!(flags &amp; XFS_BMAPI_IGSTATE));
<span class="p_del">-	ASSERT(tp != NULL);</span>
<span class="p_add">+	ASSERT(tp != NULL ||</span>
<span class="p_add">+	       (flags &amp; (XFS_BMAPI_CONVERT | XFS_BMAPI_COWFORK)) ==</span>
<span class="p_add">+			(XFS_BMAPI_CONVERT | XFS_BMAPI_COWFORK));</span>
 	ASSERT(len &gt; 0);
 	ASSERT(XFS_IFORK_FORMAT(ip, whichfork) != XFS_DINODE_FMT_LOCAL);
 	ASSERT(xfs_isilocked(ip, XFS_ILOCK_EXCL));
 	ASSERT(!(flags &amp; XFS_BMAPI_REMAP) || whichfork == XFS_DATA_FORK);
 	ASSERT(!(flags &amp; XFS_BMAPI_PREALLOC) || !(flags &amp; XFS_BMAPI_REMAP));
 	ASSERT(!(flags &amp; XFS_BMAPI_CONVERT) || !(flags &amp; XFS_BMAPI_REMAP));
<span class="p_del">-	ASSERT(!(flags &amp; XFS_BMAPI_PREALLOC) || whichfork != XFS_COW_FORK);</span>
<span class="p_del">-	ASSERT(!(flags &amp; XFS_BMAPI_CONVERT) || whichfork != XFS_COW_FORK);</span>
 
 	/* zeroing is for currently only for data extents, not metadata */
 	ASSERT((flags &amp; (XFS_BMAPI_METADATA | XFS_BMAPI_ZERO)) !=
<span class="p_chunk">@@ -4840,13 +4860,9 @@</span> <span class="p_context"> xfs_bmapi_write(</span>
 	if (bma.cur) {
 		if (!error) {
 			ASSERT(*firstblock == NULLFSBLOCK ||
<span class="p_del">-			       XFS_FSB_TO_AGNO(mp, *firstblock) ==</span>
<span class="p_add">+			       XFS_FSB_TO_AGNO(mp, *firstblock) &lt;=</span>
 			       XFS_FSB_TO_AGNO(mp,
<span class="p_del">-				       bma.cur-&gt;bc_private.b.firstblock) ||</span>
<span class="p_del">-			       (dfops-&gt;dop_low &amp;&amp;</span>
<span class="p_del">-				XFS_FSB_TO_AGNO(mp, *firstblock) &lt;</span>
<span class="p_del">-				XFS_FSB_TO_AGNO(mp,</span>
<span class="p_del">-					bma.cur-&gt;bc_private.b.firstblock)));</span>
<span class="p_add">+				       bma.cur-&gt;bc_private.b.firstblock));</span>
 			*firstblock = bma.cur-&gt;bc_private.b.firstblock;
 		}
 		xfs_btree_del_cursor(bma.cur,
<span class="p_chunk">@@ -4881,34 +4897,59 @@</span> <span class="p_context"> xfs_bmap_split_indlen(</span>
 	xfs_filblks_t			len2 = *indlen2;
 	xfs_filblks_t			nres = len1 + len2; /* new total res. */
 	xfs_filblks_t			stolen = 0;
<span class="p_add">+	xfs_filblks_t			resfactor;</span>
 
 	/*
 	 * Steal as many blocks as we can to try and satisfy the worst case
 	 * indlen for both new extents.
 	 */
<span class="p_del">-	while (nres &gt; ores &amp;&amp; avail) {</span>
<span class="p_del">-		nres--;</span>
<span class="p_del">-		avail--;</span>
<span class="p_del">-		stolen++;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (ores &lt; nres &amp;&amp; avail)</span>
<span class="p_add">+		stolen = XFS_FILBLKS_MIN(nres - ores, avail);</span>
<span class="p_add">+	ores += stolen;</span>
<span class="p_add">+</span>
<span class="p_add">+	 /* nothing else to do if we&#39;ve satisfied the new reservation */</span>
<span class="p_add">+	if (ores &gt;= nres)</span>
<span class="p_add">+		return stolen;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We can&#39;t meet the total required reservation for the two extents.</span>
<span class="p_add">+	 * Calculate the percent of the overall shortage between both extents</span>
<span class="p_add">+	 * and apply this percentage to each of the requested indlen values.</span>
<span class="p_add">+	 * This distributes the shortage fairly and reduces the chances that one</span>
<span class="p_add">+	 * of the two extents is left with nothing when extents are repeatedly</span>
<span class="p_add">+	 * split.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	resfactor = (ores * 100);</span>
<span class="p_add">+	do_div(resfactor, nres);</span>
<span class="p_add">+	len1 *= resfactor;</span>
<span class="p_add">+	do_div(len1, 100);</span>
<span class="p_add">+	len2 *= resfactor;</span>
<span class="p_add">+	do_div(len2, 100);</span>
<span class="p_add">+	ASSERT(len1 + len2 &lt;= ores);</span>
<span class="p_add">+	ASSERT(len1 &lt; *indlen1 &amp;&amp; len2 &lt; *indlen2);</span>
 
 	/*
<span class="p_del">-	 * The only blocks available are those reserved for the original</span>
<span class="p_del">-	 * extent and what we can steal from the extent being removed.</span>
<span class="p_del">-	 * If this still isn&#39;t enough to satisfy the combined</span>
<span class="p_del">-	 * requirements for the two new extents, skim blocks off of each</span>
<span class="p_del">-	 * of the new reservations until they match what is available.</span>
<span class="p_add">+	 * Hand out the remainder to each extent. If one of the two reservations</span>
<span class="p_add">+	 * is zero, we want to make sure that one gets a block first. The loop</span>
<span class="p_add">+	 * below starts with len1, so hand len2 a block right off the bat if it</span>
<span class="p_add">+	 * is zero.</span>
 	 */
<span class="p_del">-	while (nres &gt; ores) {</span>
<span class="p_del">-		if (len1) {</span>
<span class="p_del">-			len1--;</span>
<span class="p_del">-			nres--;</span>
<span class="p_add">+	ores -= (len1 + len2);</span>
<span class="p_add">+	ASSERT((*indlen1 - len1) + (*indlen2 - len2) &gt;= ores);</span>
<span class="p_add">+	if (ores &amp;&amp; !len2 &amp;&amp; *indlen2) {</span>
<span class="p_add">+		len2++;</span>
<span class="p_add">+		ores--;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	while (ores) {</span>
<span class="p_add">+		if (len1 &lt; *indlen1) {</span>
<span class="p_add">+			len1++;</span>
<span class="p_add">+			ores--;</span>
 		}
<span class="p_del">-		if (nres == ores)</span>
<span class="p_add">+		if (!ores)</span>
 			break;
<span class="p_del">-		if (len2) {</span>
<span class="p_del">-			len2--;</span>
<span class="p_del">-			nres--;</span>
<span class="p_add">+		if (len2 &lt; *indlen2) {</span>
<span class="p_add">+			len2++;</span>
<span class="p_add">+			ores--;</span>
 		}
 	}
 
<span class="p_chunk">@@ -5656,8 +5697,8 @@</span> <span class="p_context"> __xfs_bunmapi(</span>
 			}
 			del.br_state = XFS_EXT_UNWRITTEN;
 			error = xfs_bmap_add_extent_unwritten_real(tp, ip,
<span class="p_del">-					&amp;lastx, &amp;cur, &amp;del, firstblock, dfops,</span>
<span class="p_del">-					&amp;logflags);</span>
<span class="p_add">+					whichfork, &amp;lastx, &amp;cur, &amp;del,</span>
<span class="p_add">+					firstblock, dfops, &amp;logflags);</span>
 			if (error)
 				goto error0;
 			goto nodelete;
<span class="p_chunk">@@ -5714,8 +5755,9 @@</span> <span class="p_context"> __xfs_bunmapi(</span>
 				prev.br_state = XFS_EXT_UNWRITTEN;
 				lastx--;
 				error = xfs_bmap_add_extent_unwritten_real(tp,
<span class="p_del">-						ip, &amp;lastx, &amp;cur, &amp;prev,</span>
<span class="p_del">-						firstblock, dfops, &amp;logflags);</span>
<span class="p_add">+						ip, whichfork, &amp;lastx, &amp;cur,</span>
<span class="p_add">+						&amp;prev, firstblock, dfops,</span>
<span class="p_add">+						&amp;logflags);</span>
 				if (error)
 					goto error0;
 				goto nodelete;
<span class="p_chunk">@@ -5723,8 +5765,9 @@</span> <span class="p_context"> __xfs_bunmapi(</span>
 				ASSERT(del.br_state == XFS_EXT_NORM);
 				del.br_state = XFS_EXT_UNWRITTEN;
 				error = xfs_bmap_add_extent_unwritten_real(tp,
<span class="p_del">-						ip, &amp;lastx, &amp;cur, &amp;del,</span>
<span class="p_del">-						firstblock, dfops, &amp;logflags);</span>
<span class="p_add">+						ip, whichfork, &amp;lastx, &amp;cur,</span>
<span class="p_add">+						&amp;del, firstblock, dfops,</span>
<span class="p_add">+						&amp;logflags);</span>
 				if (error)
 					goto error0;
 				goto nodelete;
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_bmap_btree.c b/fs/xfs/libxfs/xfs_bmap_btree.c</span>
<span class="p_header">index f76c1693ff01..5c3918678bb6 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_bmap_btree.c</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_bmap_btree.c</span>
<span class="p_chunk">@@ -453,8 +453,8 @@</span> <span class="p_context"> xfs_bmbt_alloc_block(</span>
 
 	if (args.fsbno == NULLFSBLOCK) {
 		args.fsbno = be64_to_cpu(start-&gt;l);
<span class="p_del">-try_another_ag:</span>
 		args.type = XFS_ALLOCTYPE_START_BNO;
<span class="p_add">+try_another_ag:</span>
 		/*
 		 * Make sure there is sufficient room left in the AG to
 		 * complete a full tree split for an extent insert.  If
<span class="p_chunk">@@ -494,8 +494,8 @@</span> <span class="p_context"> xfs_bmbt_alloc_block(</span>
 	if (xfs_sb_version_hasreflink(&amp;cur-&gt;bc_mp-&gt;m_sb) &amp;&amp;
 	    args.fsbno == NULLFSBLOCK &amp;&amp;
 	    args.type == XFS_ALLOCTYPE_NEAR_BNO) {
<span class="p_del">-		cur-&gt;bc_private.b.dfops-&gt;dop_low = true;</span>
 		args.fsbno = cur-&gt;bc_private.b.firstblock;
<span class="p_add">+		args.type = XFS_ALLOCTYPE_FIRST_AG;</span>
 		goto try_another_ag;
 	}
 
<span class="p_chunk">@@ -512,7 +512,7 @@</span> <span class="p_context"> xfs_bmbt_alloc_block(</span>
 			goto error0;
 		cur-&gt;bc_private.b.dfops-&gt;dop_low = true;
 	}
<span class="p_del">-	if (args.fsbno == NULLFSBLOCK) {</span>
<span class="p_add">+	if (WARN_ON_ONCE(args.fsbno == NULLFSBLOCK)) {</span>
 		XFS_BTREE_TRACE_CURSOR(cur, XBT_EXIT);
 		*stat = 0;
 		return 0;
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_btree.c b/fs/xfs/libxfs/xfs_btree.c</span>
<span class="p_header">index 21e6a6ab6b9a..2849d3fa3d0b 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_btree.c</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_btree.c</span>
<span class="p_chunk">@@ -810,7 +810,8 @@</span> <span class="p_context"> xfs_btree_read_bufl(</span>
 	xfs_daddr_t		d;		/* real disk block address */
 	int			error;
 
<span class="p_del">-	ASSERT(fsbno != NULLFSBLOCK);</span>
<span class="p_add">+	if (!XFS_FSB_SANITY_CHECK(mp, fsbno))</span>
<span class="p_add">+		return -EFSCORRUPTED;</span>
 	d = XFS_FSB_TO_DADDR(mp, fsbno);
 	error = xfs_trans_read_buf(mp, tp, mp-&gt;m_ddev_targp, d,
 				   mp-&gt;m_bsize, lock, &amp;bp, ops);
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_btree.h b/fs/xfs/libxfs/xfs_btree.h</span>
<span class="p_header">index c2b01d1c79ee..3b0fc1afada5 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_btree.h</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_btree.h</span>
<span class="p_chunk">@@ -491,7 +491,7 @@</span> <span class="p_context"> static inline int xfs_btree_get_level(struct xfs_btree_block *block)</span>
 #define	XFS_FILBLKS_MAX(a,b)	max_t(xfs_filblks_t, (a), (b))
 
 #define	XFS_FSB_SANITY_CHECK(mp,fsb)	\
<span class="p_del">-	(XFS_FSB_TO_AGNO(mp, fsb) &lt; mp-&gt;m_sb.sb_agcount &amp;&amp; \</span>
<span class="p_add">+	(fsb &amp;&amp; XFS_FSB_TO_AGNO(mp, fsb) &lt; mp-&gt;m_sb.sb_agcount &amp;&amp; \</span>
 		XFS_FSB_TO_AGBNO(mp, fsb) &lt; mp-&gt;m_sb.sb_agblocks)
 
 /*
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_da_btree.c b/fs/xfs/libxfs/xfs_da_btree.c</span>
<span class="p_header">index f2dc1a950c85..1bdf2888295b 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_da_btree.c</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_da_btree.c</span>
<span class="p_chunk">@@ -2633,7 +2633,7 @@</span> <span class="p_context"> xfs_da_read_buf(</span>
 /*
  * Readahead the dir/attr block.
  */
<span class="p_del">-xfs_daddr_t</span>
<span class="p_add">+int</span>
 xfs_da_reada_buf(
 	struct xfs_inode	*dp,
 	xfs_dablk_t		bno,
<span class="p_chunk">@@ -2664,7 +2664,5 @@</span> <span class="p_context"> xfs_da_reada_buf(</span>
 	if (mapp != &amp;map)
 		kmem_free(mapp);
 
<span class="p_del">-	if (error)</span>
<span class="p_del">-		return -1;</span>
<span class="p_del">-	return mappedbno;</span>
<span class="p_add">+	return error;</span>
 }
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_da_btree.h b/fs/xfs/libxfs/xfs_da_btree.h</span>
<span class="p_header">index 98c75cbe6ac2..4e29cb6a3627 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_da_btree.h</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_da_btree.h</span>
<span class="p_chunk">@@ -201,7 +201,7 @@</span> <span class="p_context"> int	xfs_da_read_buf(struct xfs_trans *trans, struct xfs_inode *dp,</span>
 			       xfs_dablk_t bno, xfs_daddr_t mappedbno,
 			       struct xfs_buf **bpp, int whichfork,
 			       const struct xfs_buf_ops *ops);
<span class="p_del">-xfs_daddr_t	xfs_da_reada_buf(struct xfs_inode *dp, xfs_dablk_t bno,</span>
<span class="p_add">+int	xfs_da_reada_buf(struct xfs_inode *dp, xfs_dablk_t bno,</span>
 				xfs_daddr_t mapped_bno, int whichfork,
 				const struct xfs_buf_ops *ops);
 int	xfs_da_shrink_inode(xfs_da_args_t *args, xfs_dablk_t dead_blkno,
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_dir2_node.c b/fs/xfs/libxfs/xfs_dir2_node.c</span>
<span class="p_header">index 75a557432d0f..bbd1238852b3 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_dir2_node.c</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_dir2_node.c</span>
<span class="p_chunk">@@ -155,6 +155,42 @@</span> <span class="p_context"> const struct xfs_buf_ops xfs_dir3_free_buf_ops = {</span>
 	.verify_write = xfs_dir3_free_write_verify,
 };
 
<span class="p_add">+/* Everything ok in the free block header? */</span>
<span class="p_add">+static bool</span>
<span class="p_add">+xfs_dir3_free_header_check(</span>
<span class="p_add">+	struct xfs_inode	*dp,</span>
<span class="p_add">+	xfs_dablk_t		fbno,</span>
<span class="p_add">+	struct xfs_buf		*bp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct xfs_mount	*mp = dp-&gt;i_mount;</span>
<span class="p_add">+	unsigned int		firstdb;</span>
<span class="p_add">+	int			maxbests;</span>
<span class="p_add">+</span>
<span class="p_add">+	maxbests = dp-&gt;d_ops-&gt;free_max_bests(mp-&gt;m_dir_geo);</span>
<span class="p_add">+	firstdb = (xfs_dir2_da_to_db(mp-&gt;m_dir_geo, fbno) -</span>
<span class="p_add">+		   xfs_dir2_byte_to_db(mp-&gt;m_dir_geo, XFS_DIR2_FREE_OFFSET)) *</span>
<span class="p_add">+			maxbests;</span>
<span class="p_add">+	if (xfs_sb_version_hascrc(&amp;mp-&gt;m_sb)) {</span>
<span class="p_add">+		struct xfs_dir3_free_hdr *hdr3 = bp-&gt;b_addr;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (be32_to_cpu(hdr3-&gt;firstdb) != firstdb)</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		if (be32_to_cpu(hdr3-&gt;nvalid) &gt; maxbests)</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		if (be32_to_cpu(hdr3-&gt;nvalid) &lt; be32_to_cpu(hdr3-&gt;nused))</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		struct xfs_dir2_free_hdr *hdr = bp-&gt;b_addr;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (be32_to_cpu(hdr-&gt;firstdb) != firstdb)</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		if (be32_to_cpu(hdr-&gt;nvalid) &gt; maxbests)</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		if (be32_to_cpu(hdr-&gt;nvalid) &lt; be32_to_cpu(hdr-&gt;nused))</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
 
 static int
 __xfs_dir3_free_read(
<span class="p_chunk">@@ -168,11 +204,22 @@</span> <span class="p_context"> __xfs_dir3_free_read(</span>
 
 	err = xfs_da_read_buf(tp, dp, fbno, mappedbno, bpp,
 				XFS_DATA_FORK, &amp;xfs_dir3_free_buf_ops);
<span class="p_add">+	if (err || !*bpp)</span>
<span class="p_add">+		return err;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check things that we can&#39;t do in the verifier. */</span>
<span class="p_add">+	if (!xfs_dir3_free_header_check(dp, fbno, *bpp)) {</span>
<span class="p_add">+		xfs_buf_ioerror(*bpp, -EFSCORRUPTED);</span>
<span class="p_add">+		xfs_verifier_error(*bpp);</span>
<span class="p_add">+		xfs_trans_brelse(tp, *bpp);</span>
<span class="p_add">+		return -EFSCORRUPTED;</span>
<span class="p_add">+	}</span>
 
 	/* try read returns without an error or *bpp if it lands in a hole */
<span class="p_del">-	if (!err &amp;&amp; tp &amp;&amp; *bpp)</span>
<span class="p_add">+	if (tp)</span>
 		xfs_trans_buf_set_type(tp, *bpp, XFS_BLFT_DIR_FREE_BUF);
<span class="p_del">-	return err;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
 }
 
 int
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_ialloc.c b/fs/xfs/libxfs/xfs_ialloc.c</span>
<span class="p_header">index d45c03779dae..a2818f6e8598 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_ialloc.c</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_ialloc.c</span>
<span class="p_chunk">@@ -51,8 +51,7 @@</span> <span class="p_context"> xfs_ialloc_cluster_alignment(</span>
 	struct xfs_mount	*mp)
 {
 	if (xfs_sb_version_hasalign(&amp;mp-&gt;m_sb) &amp;&amp;
<span class="p_del">-	    mp-&gt;m_sb.sb_inoalignmt &gt;=</span>
<span class="p_del">-			XFS_B_TO_FSBT(mp, mp-&gt;m_inode_cluster_size))</span>
<span class="p_add">+	    mp-&gt;m_sb.sb_inoalignmt &gt;= xfs_icluster_size_fsb(mp))</span>
 		return mp-&gt;m_sb.sb_inoalignmt;
 	return 1;
 }
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_ialloc_btree.c b/fs/xfs/libxfs/xfs_ialloc_btree.c</span>
<span class="p_header">index 6c6b95947e71..b9c351ff0422 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_ialloc_btree.c</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_ialloc_btree.c</span>
<span class="p_chunk">@@ -82,11 +82,12 @@</span> <span class="p_context"> xfs_finobt_set_root(</span>
 }
 
 STATIC int
<span class="p_del">-xfs_inobt_alloc_block(</span>
<span class="p_add">+__xfs_inobt_alloc_block(</span>
 	struct xfs_btree_cur	*cur,
 	union xfs_btree_ptr	*start,
 	union xfs_btree_ptr	*new,
<span class="p_del">-	int			*stat)</span>
<span class="p_add">+	int			*stat,</span>
<span class="p_add">+	enum xfs_ag_resv_type	resv)</span>
 {
 	xfs_alloc_arg_t		args;		/* block allocation args */
 	int			error;		/* error return value */
<span class="p_chunk">@@ -103,6 +104,7 @@</span> <span class="p_context"> xfs_inobt_alloc_block(</span>
 	args.maxlen = 1;
 	args.prod = 1;
 	args.type = XFS_ALLOCTYPE_NEAR_BNO;
<span class="p_add">+	args.resv = resv;</span>
 
 	error = xfs_alloc_vextent(&amp;args);
 	if (error) {
<span class="p_chunk">@@ -123,6 +125,27 @@</span> <span class="p_context"> xfs_inobt_alloc_block(</span>
 }
 
 STATIC int
<span class="p_add">+xfs_inobt_alloc_block(</span>
<span class="p_add">+	struct xfs_btree_cur	*cur,</span>
<span class="p_add">+	union xfs_btree_ptr	*start,</span>
<span class="p_add">+	union xfs_btree_ptr	*new,</span>
<span class="p_add">+	int			*stat)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __xfs_inobt_alloc_block(cur, start, new, stat, XFS_AG_RESV_NONE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+STATIC int</span>
<span class="p_add">+xfs_finobt_alloc_block(</span>
<span class="p_add">+	struct xfs_btree_cur	*cur,</span>
<span class="p_add">+	union xfs_btree_ptr	*start,</span>
<span class="p_add">+	union xfs_btree_ptr	*new,</span>
<span class="p_add">+	int			*stat)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __xfs_inobt_alloc_block(cur, start, new, stat,</span>
<span class="p_add">+			XFS_AG_RESV_METADATA);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+STATIC int</span>
 xfs_inobt_free_block(
 	struct xfs_btree_cur	*cur,
 	struct xfs_buf		*bp)
<span class="p_chunk">@@ -328,7 +351,7 @@</span> <span class="p_context"> static const struct xfs_btree_ops xfs_finobt_ops = {</span>
 
 	.dup_cursor		= xfs_inobt_dup_cursor,
 	.set_root		= xfs_finobt_set_root,
<span class="p_del">-	.alloc_block		= xfs_inobt_alloc_block,</span>
<span class="p_add">+	.alloc_block		= xfs_finobt_alloc_block,</span>
 	.free_block		= xfs_inobt_free_block,
 	.get_minrecs		= xfs_inobt_get_minrecs,
 	.get_maxrecs		= xfs_inobt_get_maxrecs,
<span class="p_chunk">@@ -478,3 +501,64 @@</span> <span class="p_context"> xfs_inobt_rec_check_count(</span>
 	return 0;
 }
 #endif	/* DEBUG */
<span class="p_add">+</span>
<span class="p_add">+static xfs_extlen_t</span>
<span class="p_add">+xfs_inobt_max_size(</span>
<span class="p_add">+	struct xfs_mount	*mp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* Bail out if we&#39;re uninitialized, which can happen in mkfs. */</span>
<span class="p_add">+	if (mp-&gt;m_inobt_mxr[0] == 0)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return xfs_btree_calc_size(mp, mp-&gt;m_inobt_mnr,</span>
<span class="p_add">+		(uint64_t)mp-&gt;m_sb.sb_agblocks * mp-&gt;m_sb.sb_inopblock /</span>
<span class="p_add">+				XFS_INODES_PER_CHUNK);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int</span>
<span class="p_add">+xfs_inobt_count_blocks(</span>
<span class="p_add">+	struct xfs_mount	*mp,</span>
<span class="p_add">+	xfs_agnumber_t		agno,</span>
<span class="p_add">+	xfs_btnum_t		btnum,</span>
<span class="p_add">+	xfs_extlen_t		*tree_blocks)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct xfs_buf		*agbp;</span>
<span class="p_add">+	struct xfs_btree_cur	*cur;</span>
<span class="p_add">+	int			error;</span>
<span class="p_add">+</span>
<span class="p_add">+	error = xfs_ialloc_read_agi(mp, NULL, agno, &amp;agbp);</span>
<span class="p_add">+	if (error)</span>
<span class="p_add">+		return error;</span>
<span class="p_add">+</span>
<span class="p_add">+	cur = xfs_inobt_init_cursor(mp, NULL, agbp, agno, btnum);</span>
<span class="p_add">+	error = xfs_btree_count_blocks(cur, tree_blocks);</span>
<span class="p_add">+	xfs_btree_del_cursor(cur, error ? XFS_BTREE_ERROR : XFS_BTREE_NOERROR);</span>
<span class="p_add">+	xfs_buf_relse(agbp);</span>
<span class="p_add">+</span>
<span class="p_add">+	return error;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Figure out how many blocks to reserve and how many are used by this btree.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int</span>
<span class="p_add">+xfs_finobt_calc_reserves(</span>
<span class="p_add">+	struct xfs_mount	*mp,</span>
<span class="p_add">+	xfs_agnumber_t		agno,</span>
<span class="p_add">+	xfs_extlen_t		*ask,</span>
<span class="p_add">+	xfs_extlen_t		*used)</span>
<span class="p_add">+{</span>
<span class="p_add">+	xfs_extlen_t		tree_len = 0;</span>
<span class="p_add">+	int			error;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!xfs_sb_version_hasfinobt(&amp;mp-&gt;m_sb))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	error = xfs_inobt_count_blocks(mp, agno, XFS_BTNUM_FINO, &amp;tree_len);</span>
<span class="p_add">+	if (error)</span>
<span class="p_add">+		return error;</span>
<span class="p_add">+</span>
<span class="p_add">+	*ask += xfs_inobt_max_size(mp);</span>
<span class="p_add">+	*used += tree_len;</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_ialloc_btree.h b/fs/xfs/libxfs/xfs_ialloc_btree.h</span>
<span class="p_header">index bd88453217ce..aa81e2e63f3f 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_ialloc_btree.h</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_ialloc_btree.h</span>
<span class="p_chunk">@@ -72,4 +72,7 @@</span> <span class="p_context"> int xfs_inobt_rec_check_count(struct xfs_mount *,</span>
 #define xfs_inobt_rec_check_count(mp, rec)	0
 #endif	/* DEBUG */
 
<span class="p_add">+int xfs_finobt_calc_reserves(struct xfs_mount *mp, xfs_agnumber_t agno,</span>
<span class="p_add">+		xfs_extlen_t *ask, xfs_extlen_t *used);</span>
<span class="p_add">+</span>
 #endif	/* __XFS_IALLOC_BTREE_H__ */
<span class="p_header">diff --git a/fs/xfs/libxfs/xfs_inode_fork.c b/fs/xfs/libxfs/xfs_inode_fork.c</span>
<span class="p_header">index 222e103356c6..25c1e078aef6 100644</span>
<span class="p_header">--- a/fs/xfs/libxfs/xfs_inode_fork.c</span>
<span class="p_header">+++ b/fs/xfs/libxfs/xfs_inode_fork.c</span>
<span class="p_chunk">@@ -26,6 +26,7 @@</span> <span class="p_context"></span>
 #include &quot;xfs_inode.h&quot;
 #include &quot;xfs_trans.h&quot;
 #include &quot;xfs_inode_item.h&quot;
<span class="p_add">+#include &quot;xfs_btree.h&quot;</span>
 #include &quot;xfs_bmap_btree.h&quot;
 #include &quot;xfs_bmap.h&quot;
 #include &quot;xfs_error.h&quot;
<span class="p_chunk">@@ -429,11 +430,13 @@</span> <span class="p_context"> xfs_iformat_btree(</span>
 	/* REFERENCED */
 	int			nrecs;
 	int			size;
<span class="p_add">+	int			level;</span>
 
 	ifp = XFS_IFORK_PTR(ip, whichfork);
 	dfp = (xfs_bmdr_block_t *)XFS_DFORK_PTR(dip, whichfork);
 	size = XFS_BMAP_BROOT_SPACE(mp, dfp);
 	nrecs = be16_to_cpu(dfp-&gt;bb_numrecs);
<span class="p_add">+	level = be16_to_cpu(dfp-&gt;bb_level);</span>
 
 	/*
 	 * blow out if -- fork has less extents than can fit in
<span class="p_chunk">@@ -446,7 +449,8 @@</span> <span class="p_context"> xfs_iformat_btree(</span>
 					XFS_IFORK_MAXEXT(ip, whichfork) ||
 		     XFS_BMDR_SPACE_CALC(nrecs) &gt;
 					XFS_DFORK_SIZE(dip, mp, whichfork) ||
<span class="p_del">-		     XFS_IFORK_NEXTENTS(ip, whichfork) &gt; ip-&gt;i_d.di_nblocks)) {</span>
<span class="p_add">+		     XFS_IFORK_NEXTENTS(ip, whichfork) &gt; ip-&gt;i_d.di_nblocks) ||</span>
<span class="p_add">+		     level == 0 || level &gt; XFS_BTREE_MAXLEVELS) {</span>
 		xfs_warn(mp, &quot;corrupt inode %Lu (btree).&quot;,
 					(unsigned long long) ip-&gt;i_ino);
 		XFS_CORRUPTION_ERROR(&quot;xfs_iformat_btree&quot;, XFS_ERRLEVEL_LOW,
<span class="p_chunk">@@ -497,15 +501,14 @@</span> <span class="p_context"> xfs_iread_extents(</span>
 	 * We know that the size is valid (it&#39;s checked in iformat_btree)
 	 */
 	ifp-&gt;if_bytes = ifp-&gt;if_real_bytes = 0;
<span class="p_del">-	ifp-&gt;if_flags |= XFS_IFEXTENTS;</span>
 	xfs_iext_add(ifp, 0, nextents);
 	error = xfs_bmap_read_extents(tp, ip, whichfork);
 	if (error) {
 		xfs_iext_destroy(ifp);
<span class="p_del">-		ifp-&gt;if_flags &amp;= ~XFS_IFEXTENTS;</span>
 		return error;
 	}
 	xfs_validate_extents(ifp, nextents, XFS_EXTFMT_INODE(ip));
<span class="p_add">+	ifp-&gt;if_flags |= XFS_IFEXTENTS;</span>
 	return 0;
 }
 /*
<span class="p_header">diff --git a/fs/xfs/xfs_aops.c b/fs/xfs/xfs_aops.c</span>
<span class="p_header">index 06763f5cc701..0457abe4118a 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_aops.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_aops.c</span>
<span class="p_chunk">@@ -279,54 +279,49 @@</span> <span class="p_context"> xfs_end_io(</span>
 	struct xfs_ioend	*ioend =
 		container_of(work, struct xfs_ioend, io_work);
 	struct xfs_inode	*ip = XFS_I(ioend-&gt;io_inode);
<span class="p_add">+	xfs_off_t		offset = ioend-&gt;io_offset;</span>
<span class="p_add">+	size_t			size = ioend-&gt;io_size;</span>
 	int			error = ioend-&gt;io_bio-&gt;bi_error;
 
 	/*
<span class="p_del">-	 * Set an error if the mount has shut down and proceed with end I/O</span>
<span class="p_del">-	 * processing so it can perform whatever cleanups are necessary.</span>
<span class="p_add">+	 * Just clean up the in-memory strutures if the fs has been shut down.</span>
 	 */
<span class="p_del">-	if (XFS_FORCED_SHUTDOWN(ip-&gt;i_mount))</span>
<span class="p_add">+	if (XFS_FORCED_SHUTDOWN(ip-&gt;i_mount)) {</span>
 		error = -EIO;
<span class="p_add">+		goto done;</span>
<span class="p_add">+	}</span>
 
 	/*
<span class="p_del">-	 * For a CoW extent, we need to move the mapping from the CoW fork</span>
<span class="p_del">-	 * to the data fork.  If instead an error happened, just dump the</span>
<span class="p_del">-	 * new blocks.</span>
<span class="p_add">+	 * Clean up any COW blocks on an I/O error.</span>
 	 */
<span class="p_del">-	if (ioend-&gt;io_type == XFS_IO_COW) {</span>
<span class="p_del">-		if (error)</span>
<span class="p_del">-			goto done;</span>
<span class="p_del">-		if (ioend-&gt;io_bio-&gt;bi_error) {</span>
<span class="p_del">-			error = xfs_reflink_cancel_cow_range(ip,</span>
<span class="p_del">-					ioend-&gt;io_offset, ioend-&gt;io_size);</span>
<span class="p_del">-			goto done;</span>
<span class="p_add">+	if (unlikely(error)) {</span>
<span class="p_add">+		switch (ioend-&gt;io_type) {</span>
<span class="p_add">+		case XFS_IO_COW:</span>
<span class="p_add">+			xfs_reflink_cancel_cow_range(ip, offset, size, true);</span>
<span class="p_add">+			break;</span>
 		}
<span class="p_del">-		error = xfs_reflink_end_cow(ip, ioend-&gt;io_offset,</span>
<span class="p_del">-				ioend-&gt;io_size);</span>
<span class="p_del">-		if (error)</span>
<span class="p_del">-			goto done;</span>
<span class="p_add">+</span>
<span class="p_add">+		goto done;</span>
 	}
 
 	/*
<span class="p_del">-	 * For unwritten extents we need to issue transactions to convert a</span>
<span class="p_del">-	 * range to normal written extens after the data I/O has finished.</span>
<span class="p_del">-	 * Detecting and handling completion IO errors is done individually</span>
<span class="p_del">-	 * for each case as different cleanup operations need to be performed</span>
<span class="p_del">-	 * on error.</span>
<span class="p_add">+	 * Success:  commit the COW or unwritten blocks if needed.</span>
 	 */
<span class="p_del">-	if (ioend-&gt;io_type == XFS_IO_UNWRITTEN) {</span>
<span class="p_del">-		if (error)</span>
<span class="p_del">-			goto done;</span>
<span class="p_del">-		error = xfs_iomap_write_unwritten(ip, ioend-&gt;io_offset,</span>
<span class="p_del">-						  ioend-&gt;io_size);</span>
<span class="p_del">-	} else if (ioend-&gt;io_append_trans) {</span>
<span class="p_del">-		error = xfs_setfilesize_ioend(ioend, error);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		ASSERT(!xfs_ioend_is_append(ioend) ||</span>
<span class="p_del">-		       ioend-&gt;io_type == XFS_IO_COW);</span>
<span class="p_add">+	switch (ioend-&gt;io_type) {</span>
<span class="p_add">+	case XFS_IO_COW:</span>
<span class="p_add">+		error = xfs_reflink_end_cow(ip, offset, size);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case XFS_IO_UNWRITTEN:</span>
<span class="p_add">+		error = xfs_iomap_write_unwritten(ip, offset, size);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		ASSERT(!xfs_ioend_is_append(ioend) || ioend-&gt;io_append_trans);</span>
<span class="p_add">+		break;</span>
 	}
 
 done:
<span class="p_add">+	if (ioend-&gt;io_append_trans)</span>
<span class="p_add">+		error = xfs_setfilesize_ioend(ioend, error);</span>
 	xfs_destroy_ioend(ioend, error);
 }
 
<span class="p_chunk">@@ -486,6 +481,12 @@</span> <span class="p_context"> xfs_submit_ioend(</span>
 	struct xfs_ioend	*ioend,
 	int			status)
 {
<span class="p_add">+	/* Convert CoW extents to regular */</span>
<span class="p_add">+	if (!status &amp;&amp; ioend-&gt;io_type == XFS_IO_COW) {</span>
<span class="p_add">+		status = xfs_reflink_convert_cow(XFS_I(ioend-&gt;io_inode),</span>
<span class="p_add">+				ioend-&gt;io_offset, ioend-&gt;io_size);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* Reserve log space if we might write beyond the on-disk inode size. */
 	if (!status &amp;&amp;
 	    ioend-&gt;io_type != XFS_IO_UNWRITTEN &amp;&amp;
<span class="p_chunk">@@ -1257,44 +1258,6 @@</span> <span class="p_context"> xfs_map_trim_size(</span>
 	bh_result-&gt;b_size = mapping_size;
 }
 
<span class="p_del">-/* Bounce unaligned directio writes to the page cache. */</span>
<span class="p_del">-static int</span>
<span class="p_del">-xfs_bounce_unaligned_dio_write(</span>
<span class="p_del">-	struct xfs_inode	*ip,</span>
<span class="p_del">-	xfs_fileoff_t		offset_fsb,</span>
<span class="p_del">-	struct xfs_bmbt_irec	*imap)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct xfs_bmbt_irec	irec;</span>
<span class="p_del">-	xfs_fileoff_t		delta;</span>
<span class="p_del">-	bool			shared;</span>
<span class="p_del">-	bool			x;</span>
<span class="p_del">-	int			error;</span>
<span class="p_del">-</span>
<span class="p_del">-	irec = *imap;</span>
<span class="p_del">-	if (offset_fsb &gt; irec.br_startoff) {</span>
<span class="p_del">-		delta = offset_fsb - irec.br_startoff;</span>
<span class="p_del">-		irec.br_blockcount -= delta;</span>
<span class="p_del">-		irec.br_startblock += delta;</span>
<span class="p_del">-		irec.br_startoff = offset_fsb;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	error = xfs_reflink_trim_around_shared(ip, &amp;irec, &amp;shared, &amp;x);</span>
<span class="p_del">-	if (error)</span>
<span class="p_del">-		return error;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We&#39;re here because we&#39;re trying to do a directio write to a</span>
<span class="p_del">-	 * region that isn&#39;t aligned to a filesystem block.  If any part</span>
<span class="p_del">-	 * of the extent is shared, fall back to buffered mode to handle</span>
<span class="p_del">-	 * the RMW.  This is done by returning -EREMCHG (&quot;remote addr</span>
<span class="p_del">-	 * changed&quot;), which is caught further up the call stack.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (shared) {</span>
<span class="p_del">-		trace_xfs_reflink_bounce_dio_write(ip, imap);</span>
<span class="p_del">-		return -EREMCHG;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 STATIC int
 __xfs_get_blocks(
 	struct inode		*inode,
<span class="p_chunk">@@ -1432,13 +1395,6 @@</span> <span class="p_context"> __xfs_get_blocks(</span>
 	if (imap.br_startblock != HOLESTARTBLOCK &amp;&amp;
 	    imap.br_startblock != DELAYSTARTBLOCK &amp;&amp;
 	    (create || !ISUNWRITTEN(&amp;imap))) {
<span class="p_del">-		if (create &amp;&amp; direct &amp;&amp; !is_cow) {</span>
<span class="p_del">-			error = xfs_bounce_unaligned_dio_write(ip, offset_fsb,</span>
<span class="p_del">-					&amp;imap);</span>
<span class="p_del">-			if (error)</span>
<span class="p_del">-				return error;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
 		xfs_map_buffer(inode, bh_result, &amp;imap, offset);
 		if (ISUNWRITTEN(&amp;imap))
 			set_buffer_unwritten(bh_result);
<span class="p_header">diff --git a/fs/xfs/xfs_bmap_util.c b/fs/xfs/xfs_bmap_util.c</span>
<span class="p_header">index efb8ccd6bbf2..5c395e485170 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_bmap_util.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_bmap_util.c</span>
<span class="p_chunk">@@ -917,17 +917,18 @@</span> <span class="p_context"> xfs_can_free_eofblocks(struct xfs_inode *ip, bool force)</span>
  */
 int
 xfs_free_eofblocks(
<span class="p_del">-	xfs_mount_t	*mp,</span>
<span class="p_del">-	xfs_inode_t	*ip,</span>
<span class="p_del">-	bool		need_iolock)</span>
<span class="p_add">+	struct xfs_inode	*ip)</span>
 {
<span class="p_del">-	xfs_trans_t	*tp;</span>
<span class="p_del">-	int		error;</span>
<span class="p_del">-	xfs_fileoff_t	end_fsb;</span>
<span class="p_del">-	xfs_fileoff_t	last_fsb;</span>
<span class="p_del">-	xfs_filblks_t	map_len;</span>
<span class="p_del">-	int		nimaps;</span>
<span class="p_del">-	xfs_bmbt_irec_t	imap;</span>
<span class="p_add">+	struct xfs_trans	*tp;</span>
<span class="p_add">+	int			error;</span>
<span class="p_add">+	xfs_fileoff_t		end_fsb;</span>
<span class="p_add">+	xfs_fileoff_t		last_fsb;</span>
<span class="p_add">+	xfs_filblks_t		map_len;</span>
<span class="p_add">+	int			nimaps;</span>
<span class="p_add">+	struct xfs_bmbt_irec	imap;</span>
<span class="p_add">+	struct xfs_mount	*mp = ip-&gt;i_mount;</span>
<span class="p_add">+</span>
<span class="p_add">+	ASSERT(xfs_isilocked(ip, XFS_IOLOCK_EXCL));</span>
 
 	/*
 	 * Figure out if there are any blocks beyond the end
<span class="p_chunk">@@ -944,6 +945,10 @@</span> <span class="p_context"> xfs_free_eofblocks(</span>
 	error = xfs_bmapi_read(ip, end_fsb, map_len, &amp;imap, &amp;nimaps, 0);
 	xfs_iunlock(ip, XFS_ILOCK_SHARED);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If there are blocks after the end of file, truncate the file to its</span>
<span class="p_add">+	 * current size to free them up.</span>
<span class="p_add">+	 */</span>
 	if (!error &amp;&amp; (nimaps != 0) &amp;&amp;
 	    (imap.br_startblock != HOLESTARTBLOCK ||
 	     ip-&gt;i_delayed_blks)) {
<span class="p_chunk">@@ -954,22 +959,13 @@</span> <span class="p_context"> xfs_free_eofblocks(</span>
 		if (error)
 			return error;
 
<span class="p_del">-		/*</span>
<span class="p_del">-		 * There are blocks after the end of file.</span>
<span class="p_del">-		 * Free them up now by truncating the file to</span>
<span class="p_del">-		 * its current size.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (need_iolock) {</span>
<span class="p_del">-			if (!xfs_ilock_nowait(ip, XFS_IOLOCK_EXCL))</span>
<span class="p_del">-				return -EAGAIN;</span>
<span class="p_del">-		}</span>
<span class="p_add">+		/* wait on dio to ensure i_size has settled */</span>
<span class="p_add">+		inode_dio_wait(VFS_I(ip));</span>
 
 		error = xfs_trans_alloc(mp, &amp;M_RES(mp)-&gt;tr_itruncate, 0, 0, 0,
 				&amp;tp);
 		if (error) {
 			ASSERT(XFS_FORCED_SHUTDOWN(mp));
<span class="p_del">-			if (need_iolock)</span>
<span class="p_del">-				xfs_iunlock(ip, XFS_IOLOCK_EXCL);</span>
 			return error;
 		}
 
<span class="p_chunk">@@ -997,8 +993,6 @@</span> <span class="p_context"> xfs_free_eofblocks(</span>
 		}
 
 		xfs_iunlock(ip, XFS_ILOCK_EXCL);
<span class="p_del">-		if (need_iolock)</span>
<span class="p_del">-			xfs_iunlock(ip, XFS_IOLOCK_EXCL);</span>
 	}
 	return error;
 }
<span class="p_chunk">@@ -1393,10 +1387,16 @@</span> <span class="p_context"> xfs_shift_file_space(</span>
 	xfs_fileoff_t		stop_fsb;
 	xfs_fileoff_t		next_fsb;
 	xfs_fileoff_t		shift_fsb;
<span class="p_add">+	uint			resblks;</span>
 
 	ASSERT(direction == SHIFT_LEFT || direction == SHIFT_RIGHT);
 
 	if (direction == SHIFT_LEFT) {
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Reserve blocks to cover potential extent merges after left</span>
<span class="p_add">+		 * shift operations.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		resblks = XFS_DIOSTRAT_SPACE_RES(mp, 0);</span>
 		next_fsb = XFS_B_TO_FSB(mp, offset + len);
 		stop_fsb = XFS_B_TO_FSB(mp, VFS_I(ip)-&gt;i_size);
 	} else {
<span class="p_chunk">@@ -1404,6 +1404,7 @@</span> <span class="p_context"> xfs_shift_file_space(</span>
 		 * If right shift, delegate the work of initialization of
 		 * next_fsb to xfs_bmap_shift_extent as it has ilock held.
 		 */
<span class="p_add">+		resblks = 0;</span>
 		next_fsb = NULLFSBLOCK;
 		stop_fsb = XFS_B_TO_FSB(mp, offset);
 	}
<span class="p_chunk">@@ -1415,7 +1416,7 @@</span> <span class="p_context"> xfs_shift_file_space(</span>
 	 * into the accessible region of the file.
 	 */
 	if (xfs_can_free_eofblocks(ip, true)) {
<span class="p_del">-		error = xfs_free_eofblocks(mp, ip, false);</span>
<span class="p_add">+		error = xfs_free_eofblocks(ip);</span>
 		if (error)
 			return error;
 	}
<span class="p_chunk">@@ -1445,21 +1446,14 @@</span> <span class="p_context"> xfs_shift_file_space(</span>
 	}
 
 	while (!error &amp;&amp; !done) {
<span class="p_del">-		/*</span>
<span class="p_del">-		 * We would need to reserve permanent block for transaction.</span>
<span class="p_del">-		 * This will come into picture when after shifting extent into</span>
<span class="p_del">-		 * hole we found that adjacent extents can be merged which</span>
<span class="p_del">-		 * may lead to freeing of a block during record update.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		error = xfs_trans_alloc(mp, &amp;M_RES(mp)-&gt;tr_write,</span>
<span class="p_del">-				XFS_DIOSTRAT_SPACE_RES(mp, 0), 0, 0, &amp;tp);</span>
<span class="p_add">+		error = xfs_trans_alloc(mp, &amp;M_RES(mp)-&gt;tr_write, resblks, 0, 0,</span>
<span class="p_add">+					&amp;tp);</span>
 		if (error)
 			break;
 
 		xfs_ilock(ip, XFS_ILOCK_EXCL);
 		error = xfs_trans_reserve_quota(tp, mp, ip-&gt;i_udquot,
<span class="p_del">-				ip-&gt;i_gdquot, ip-&gt;i_pdquot,</span>
<span class="p_del">-				XFS_DIOSTRAT_SPACE_RES(mp, 0), 0,</span>
<span class="p_add">+				ip-&gt;i_gdquot, ip-&gt;i_pdquot, resblks, 0,</span>
 				XFS_QMOPT_RES_REGBLKS);
 		if (error)
 			goto out_trans_cancel;
<span class="p_header">diff --git a/fs/xfs/xfs_bmap_util.h b/fs/xfs/xfs_bmap_util.h</span>
<span class="p_header">index 68a621a8e0c0..f1005393785c 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_bmap_util.h</span>
<span class="p_header">+++ b/fs/xfs/xfs_bmap_util.h</span>
<span class="p_chunk">@@ -63,8 +63,7 @@</span> <span class="p_context"> int	xfs_insert_file_space(struct xfs_inode *, xfs_off_t offset,</span>
 
 /* EOF block manipulation functions */
 bool	xfs_can_free_eofblocks(struct xfs_inode *ip, bool force);
<span class="p_del">-int	xfs_free_eofblocks(struct xfs_mount *mp, struct xfs_inode *ip,</span>
<span class="p_del">-			   bool need_iolock);</span>
<span class="p_add">+int	xfs_free_eofblocks(struct xfs_inode *ip);</span>
 
 int	xfs_swap_extents(struct xfs_inode *ip, struct xfs_inode *tip,
 			 struct xfs_swapext *sx);
<span class="p_header">diff --git a/fs/xfs/xfs_buf_item.c b/fs/xfs/xfs_buf_item.c</span>
<span class="p_header">index 2975cb2319f4..0306168af332 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_buf_item.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_buf_item.c</span>
<span class="p_chunk">@@ -1162,6 +1162,7 @@</span> <span class="p_context"> xfs_buf_iodone_callbacks(</span>
 	 */
 	bp-&gt;b_last_error = 0;
 	bp-&gt;b_retries = 0;
<span class="p_add">+	bp-&gt;b_first_retry_time = 0;</span>
 
 	xfs_buf_do_callbacks(bp);
 	bp-&gt;b_fspriv = NULL;
<span class="p_header">diff --git a/fs/xfs/xfs_extent_busy.c b/fs/xfs/xfs_extent_busy.c</span>
<span class="p_header">index 162dc186cf04..29c2f997aedf 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_extent_busy.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_extent_busy.c</span>
<span class="p_chunk">@@ -45,18 +45,7 @@</span> <span class="p_context"> xfs_extent_busy_insert(</span>
 	struct rb_node		**rbp;
 	struct rb_node		*parent = NULL;
 
<span class="p_del">-	new = kmem_zalloc(sizeof(struct xfs_extent_busy), KM_MAYFAIL);</span>
<span class="p_del">-	if (!new) {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * No Memory!  Since it is now not possible to track the free</span>
<span class="p_del">-		 * block, make this a synchronous transaction to insure that</span>
<span class="p_del">-		 * the block is not reused before this transaction commits.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		trace_xfs_extent_busy_enomem(tp-&gt;t_mountp, agno, bno, len);</span>
<span class="p_del">-		xfs_trans_set_sync(tp);</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_add">+	new = kmem_zalloc(sizeof(struct xfs_extent_busy), KM_SLEEP);</span>
 	new-&gt;agno = agno;
 	new-&gt;bno = bno;
 	new-&gt;length = len;
<span class="p_header">diff --git a/fs/xfs/xfs_file.c b/fs/xfs/xfs_file.c</span>
<span class="p_header">index 9a5d64b5f35a..1209ad29e902 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_file.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_file.c</span>
<span class="p_chunk">@@ -554,6 +554,15 @@</span> <span class="p_context"> xfs_file_dio_aio_write(</span>
 	if ((iocb-&gt;ki_pos &amp; mp-&gt;m_blockmask) ||
 	    ((iocb-&gt;ki_pos + count) &amp; mp-&gt;m_blockmask)) {
 		unaligned_io = 1;
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We can&#39;t properly handle unaligned direct I/O to reflink</span>
<span class="p_add">+		 * files yet, as we can&#39;t unshare a partial block.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (xfs_is_reflink_inode(ip)) {</span>
<span class="p_add">+			trace_xfs_reflink_bounce_dio_write(ip, iocb-&gt;ki_pos, count);</span>
<span class="p_add">+			return -EREMCHG;</span>
<span class="p_add">+		}</span>
 		iolock = XFS_IOLOCK_EXCL;
 	} else {
 		iolock = XFS_IOLOCK_SHARED;
<span class="p_chunk">@@ -675,8 +684,10 @@</span> <span class="p_context"> xfs_file_buffered_aio_write(</span>
 	struct xfs_inode	*ip = XFS_I(inode);
 	ssize_t			ret;
 	int			enospc = 0;
<span class="p_del">-	int			iolock = XFS_IOLOCK_EXCL;</span>
<span class="p_add">+	int			iolock;</span>
 
<span class="p_add">+write_retry:</span>
<span class="p_add">+	iolock = XFS_IOLOCK_EXCL;</span>
 	xfs_rw_ilock(ip, iolock);
 
 	ret = xfs_file_aio_write_checks(iocb, from, &amp;iolock);
<span class="p_chunk">@@ -686,7 +697,6 @@</span> <span class="p_context"> xfs_file_buffered_aio_write(</span>
 	/* We can write back this queue in page reclaim */
 	current-&gt;backing_dev_info = inode_to_bdi(inode);
 
<span class="p_del">-write_retry:</span>
 	trace_xfs_file_buffered_write(ip, iov_iter_count(from), iocb-&gt;ki_pos);
 	ret = iomap_file_buffered_write(iocb, from, &amp;xfs_iomap_ops);
 	if (likely(ret &gt;= 0))
<span class="p_chunk">@@ -702,18 +712,21 @@</span> <span class="p_context"> xfs_file_buffered_aio_write(</span>
 	 * running at the same time.
 	 */
 	if (ret == -EDQUOT &amp;&amp; !enospc) {
<span class="p_add">+		xfs_rw_iunlock(ip, iolock);</span>
 		enospc = xfs_inode_free_quota_eofblocks(ip);
 		if (enospc)
 			goto write_retry;
 		enospc = xfs_inode_free_quota_cowblocks(ip);
 		if (enospc)
 			goto write_retry;
<span class="p_add">+		iolock = 0;</span>
 	} else if (ret == -ENOSPC &amp;&amp; !enospc) {
 		struct xfs_eofblocks eofb = {0};
 
 		enospc = 1;
 		xfs_flush_inodes(ip-&gt;i_mount);
<span class="p_del">-		eofb.eof_scan_owner = ip-&gt;i_ino; /* for locking */</span>
<span class="p_add">+</span>
<span class="p_add">+		xfs_rw_iunlock(ip, iolock);</span>
 		eofb.eof_flags = XFS_EOF_FLAGS_SYNC;
 		xfs_icache_free_eofblocks(ip-&gt;i_mount, &amp;eofb);
 		goto write_retry;
<span class="p_chunk">@@ -721,7 +734,8 @@</span> <span class="p_context"> xfs_file_buffered_aio_write(</span>
 
 	current-&gt;backing_dev_info = NULL;
 out:
<span class="p_del">-	xfs_rw_iunlock(ip, iolock);</span>
<span class="p_add">+	if (iolock)</span>
<span class="p_add">+		xfs_rw_iunlock(ip, iolock);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -987,9 +1001,9 @@</span> <span class="p_context"> xfs_dir_open(</span>
 	 */
 	mode = xfs_ilock_data_map_shared(ip);
 	if (ip-&gt;i_d.di_nextents &gt; 0)
<span class="p_del">-		xfs_dir3_data_readahead(ip, 0, -1);</span>
<span class="p_add">+		error = xfs_dir3_data_readahead(ip, 0, -1);</span>
 	xfs_iunlock(ip, mode);
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return error;</span>
 }
 
 STATIC int
<span class="p_header">diff --git a/fs/xfs/xfs_icache.c b/fs/xfs/xfs_icache.c</span>
<span class="p_header">index 29cc9886a3cb..3fb1f3fb8efe 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_icache.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_icache.c</span>
<span class="p_chunk">@@ -1324,13 +1324,10 @@</span> <span class="p_context"> xfs_inode_free_eofblocks(</span>
 	int			flags,
 	void			*args)
 {
<span class="p_del">-	int ret;</span>
<span class="p_add">+	int ret = 0;</span>
 	struct xfs_eofblocks *eofb = args;
<span class="p_del">-	bool need_iolock = true;</span>
 	int match;
 
<span class="p_del">-	ASSERT(!eofb || (eofb &amp;&amp; eofb-&gt;eof_scan_owner != 0));</span>
<span class="p_del">-</span>
 	if (!xfs_can_free_eofblocks(ip, false)) {
 		/* inode could be preallocated or append-only */
 		trace_xfs_inode_free_eofblocks_invalid(ip);
<span class="p_chunk">@@ -1358,21 +1355,19 @@</span> <span class="p_context"> xfs_inode_free_eofblocks(</span>
 		if (eofb-&gt;eof_flags &amp; XFS_EOF_FLAGS_MINFILESIZE &amp;&amp;
 		    XFS_ISIZE(ip) &lt; eofb-&gt;eof_min_file_size)
 			return 0;
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * A scan owner implies we already hold the iolock. Skip it in</span>
<span class="p_del">-		 * xfs_free_eofblocks() to avoid deadlock. This also eliminates</span>
<span class="p_del">-		 * the possibility of EAGAIN being returned.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (eofb-&gt;eof_scan_owner == ip-&gt;i_ino)</span>
<span class="p_del">-			need_iolock = false;</span>
 	}
 
<span class="p_del">-	ret = xfs_free_eofblocks(ip-&gt;i_mount, ip, need_iolock);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* don&#39;t revisit the inode if we&#39;re not waiting */</span>
<span class="p_del">-	if (ret == -EAGAIN &amp;&amp; !(flags &amp; SYNC_WAIT))</span>
<span class="p_del">-		ret = 0;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If the caller is waiting, return -EAGAIN to keep the background</span>
<span class="p_add">+	 * scanner moving and revisit the inode in a subsequent pass.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!xfs_ilock_nowait(ip, XFS_IOLOCK_EXCL)) {</span>
<span class="p_add">+		if (flags &amp; SYNC_WAIT)</span>
<span class="p_add">+			ret = -EAGAIN;</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	ret = xfs_free_eofblocks(ip);</span>
<span class="p_add">+	xfs_iunlock(ip, XFS_IOLOCK_EXCL);</span>
 
 	return ret;
 }
<span class="p_chunk">@@ -1419,15 +1414,10 @@</span> <span class="p_context"> __xfs_inode_free_quota_eofblocks(</span>
 	struct xfs_eofblocks eofb = {0};
 	struct xfs_dquot *dq;
 
<span class="p_del">-	ASSERT(xfs_isilocked(ip, XFS_IOLOCK_EXCL));</span>
<span class="p_del">-</span>
 	/*
<span class="p_del">-	 * Set the scan owner to avoid a potential livelock. Otherwise, the scan</span>
<span class="p_del">-	 * can repeatedly trylock on the inode we&#39;re currently processing. We</span>
<span class="p_del">-	 * run a sync scan to increase effectiveness and use the union filter to</span>
<span class="p_add">+	 * Run a sync scan to increase effectiveness and use the union filter to</span>
 	 * cover all applicable quotas in a single scan.
 	 */
<span class="p_del">-	eofb.eof_scan_owner = ip-&gt;i_ino;</span>
 	eofb.eof_flags = XFS_EOF_FLAGS_UNION|XFS_EOF_FLAGS_SYNC;
 
 	if (XFS_IS_UQUOTA_ENFORCED(ip-&gt;i_mount)) {
<span class="p_chunk">@@ -1579,12 +1569,9 @@</span> <span class="p_context"> xfs_inode_free_cowblocks(</span>
 {
 	int ret;
 	struct xfs_eofblocks *eofb = args;
<span class="p_del">-	bool need_iolock = true;</span>
 	int match;
 	struct xfs_ifork	*ifp = XFS_IFORK_PTR(ip, XFS_COW_FORK);
 
<span class="p_del">-	ASSERT(!eofb || (eofb &amp;&amp; eofb-&gt;eof_scan_owner != 0));</span>
<span class="p_del">-</span>
 	/*
 	 * Just clear the tag if we have an empty cow fork or none at all. It&#39;s
 	 * possible the inode was fully unshared since it was originally tagged.
<span class="p_chunk">@@ -1617,28 +1604,16 @@</span> <span class="p_context"> xfs_inode_free_cowblocks(</span>
 		if (eofb-&gt;eof_flags &amp; XFS_EOF_FLAGS_MINFILESIZE &amp;&amp;
 		    XFS_ISIZE(ip) &lt; eofb-&gt;eof_min_file_size)
 			return 0;
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * A scan owner implies we already hold the iolock. Skip it in</span>
<span class="p_del">-		 * xfs_free_eofblocks() to avoid deadlock. This also eliminates</span>
<span class="p_del">-		 * the possibility of EAGAIN being returned.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (eofb-&gt;eof_scan_owner == ip-&gt;i_ino)</span>
<span class="p_del">-			need_iolock = false;</span>
 	}
 
 	/* Free the CoW blocks */
<span class="p_del">-	if (need_iolock) {</span>
<span class="p_del">-		xfs_ilock(ip, XFS_IOLOCK_EXCL);</span>
<span class="p_del">-		xfs_ilock(ip, XFS_MMAPLOCK_EXCL);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	xfs_ilock(ip, XFS_IOLOCK_EXCL);</span>
<span class="p_add">+	xfs_ilock(ip, XFS_MMAPLOCK_EXCL);</span>
 
<span class="p_del">-	ret = xfs_reflink_cancel_cow_range(ip, 0, NULLFILEOFF);</span>
<span class="p_add">+	ret = xfs_reflink_cancel_cow_range(ip, 0, NULLFILEOFF, false);</span>
 
<span class="p_del">-	if (need_iolock) {</span>
<span class="p_del">-		xfs_iunlock(ip, XFS_MMAPLOCK_EXCL);</span>
<span class="p_del">-		xfs_iunlock(ip, XFS_IOLOCK_EXCL);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	xfs_iunlock(ip, XFS_MMAPLOCK_EXCL);</span>
<span class="p_add">+	xfs_iunlock(ip, XFS_IOLOCK_EXCL);</span>
 
 	return ret;
 }
<span class="p_header">diff --git a/fs/xfs/xfs_icache.h b/fs/xfs/xfs_icache.h</span>
<span class="p_header">index a1e02f4708ab..8a7c849b4dea 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_icache.h</span>
<span class="p_header">+++ b/fs/xfs/xfs_icache.h</span>
<span class="p_chunk">@@ -27,7 +27,6 @@</span> <span class="p_context"> struct xfs_eofblocks {</span>
 	kgid_t		eof_gid;
 	prid_t		eof_prid;
 	__u64		eof_min_file_size;
<span class="p_del">-	xfs_ino_t	eof_scan_owner;</span>
 };
 
 #define SYNC_WAIT		0x0001	/* wait for i/o to complete */
<span class="p_chunk">@@ -102,7 +101,6 @@</span> <span class="p_context"> xfs_fs_eofblocks_from_user(</span>
 	dst-&gt;eof_flags = src-&gt;eof_flags;
 	dst-&gt;eof_prid = src-&gt;eof_prid;
 	dst-&gt;eof_min_file_size = src-&gt;eof_min_file_size;
<span class="p_del">-	dst-&gt;eof_scan_owner = NULLFSINO;</span>
 
 	dst-&gt;eof_uid = INVALID_UID;
 	if (src-&gt;eof_flags &amp; XFS_EOF_FLAGS_UID) {
<span class="p_header">diff --git a/fs/xfs/xfs_inode.c b/fs/xfs/xfs_inode.c</span>
<span class="p_header">index 512ff13ed66a..e50636c9a89c 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_inode.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_inode.c</span>
<span class="p_chunk">@@ -1624,7 +1624,7 @@</span> <span class="p_context"> xfs_itruncate_extents(</span>
 
 	/* Remove all pending CoW reservations. */
 	error = xfs_reflink_cancel_cow_blocks(ip, &amp;tp, first_unmap_block,
<span class="p_del">-			last_block);</span>
<span class="p_add">+			last_block, true);</span>
 	if (error)
 		goto out;
 
<span class="p_chunk">@@ -1701,32 +1701,34 @@</span> <span class="p_context"> xfs_release(</span>
 	if (xfs_can_free_eofblocks(ip, false)) {
 
 		/*
<span class="p_add">+		 * Check if the inode is being opened, written and closed</span>
<span class="p_add">+		 * frequently and we have delayed allocation blocks outstanding</span>
<span class="p_add">+		 * (e.g. streaming writes from the NFS server), truncating the</span>
<span class="p_add">+		 * blocks past EOF will cause fragmentation to occur.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * In this case don&#39;t do the truncation, but we have to be</span>
<span class="p_add">+		 * careful how we detect this case. Blocks beyond EOF show up as</span>
<span class="p_add">+		 * i_delayed_blks even when the inode is clean, so we need to</span>
<span class="p_add">+		 * truncate them away first before checking for a dirty release.</span>
<span class="p_add">+		 * Hence on the first dirty close we will still remove the</span>
<span class="p_add">+		 * speculative allocation, but after that we will leave it in</span>
<span class="p_add">+		 * place.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (xfs_iflags_test(ip, XFS_IDIRTY_RELEASE))</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		/*</span>
 		 * If we can&#39;t get the iolock just skip truncating the blocks
 		 * past EOF because we could deadlock with the mmap_sem
<span class="p_del">-		 * otherwise.  We&#39;ll get another chance to drop them once the</span>
<span class="p_add">+		 * otherwise. We&#39;ll get another chance to drop them once the</span>
 		 * last reference to the inode is dropped, so we&#39;ll never leak
 		 * blocks permanently.
<span class="p_del">-		 *</span>
<span class="p_del">-		 * Further, check if the inode is being opened, written and</span>
<span class="p_del">-		 * closed frequently and we have delayed allocation blocks</span>
<span class="p_del">-		 * outstanding (e.g. streaming writes from the NFS server),</span>
<span class="p_del">-		 * truncating the blocks past EOF will cause fragmentation to</span>
<span class="p_del">-		 * occur.</span>
<span class="p_del">-		 *</span>
<span class="p_del">-		 * In this case don&#39;t do the truncation, either, but we have to</span>
<span class="p_del">-		 * be careful how we detect this case. Blocks beyond EOF show</span>
<span class="p_del">-		 * up as i_delayed_blks even when the inode is clean, so we</span>
<span class="p_del">-		 * need to truncate them away first before checking for a dirty</span>
<span class="p_del">-		 * release. Hence on the first dirty close we will still remove</span>
<span class="p_del">-		 * the speculative allocation, but after that we will leave it</span>
<span class="p_del">-		 * in place.</span>
 		 */
<span class="p_del">-		if (xfs_iflags_test(ip, XFS_IDIRTY_RELEASE))</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-		error = xfs_free_eofblocks(mp, ip, true);</span>
<span class="p_del">-		if (error &amp;&amp; error != -EAGAIN)</span>
<span class="p_del">-			return error;</span>
<span class="p_add">+		if (xfs_ilock_nowait(ip, XFS_IOLOCK_EXCL)) {</span>
<span class="p_add">+			error = xfs_free_eofblocks(ip);</span>
<span class="p_add">+			xfs_iunlock(ip, XFS_IOLOCK_EXCL);</span>
<span class="p_add">+			if (error)</span>
<span class="p_add">+				return error;</span>
<span class="p_add">+		}</span>
 
 		/* delalloc blocks after truncation means it really is dirty */
 		if (ip-&gt;i_delayed_blks)
<span class="p_chunk">@@ -1801,22 +1803,23 @@</span> <span class="p_context"> xfs_inactive_ifree(</span>
 	int			error;
 
 	/*
<span class="p_del">-	 * The ifree transaction might need to allocate blocks for record</span>
<span class="p_del">-	 * insertion to the finobt. We don&#39;t want to fail here at ENOSPC, so</span>
<span class="p_del">-	 * allow ifree to dip into the reserved block pool if necessary.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * Freeing large sets of inodes generally means freeing inode chunks,</span>
<span class="p_del">-	 * directory and file data blocks, so this should be relatively safe.</span>
<span class="p_del">-	 * Only under severe circumstances should it be possible to free enough</span>
<span class="p_del">-	 * inodes to exhaust the reserve block pool via finobt expansion while</span>
<span class="p_del">-	 * at the same time not creating free space in the filesystem.</span>
<span class="p_add">+	 * We try to use a per-AG reservation for any block needed by the finobt</span>
<span class="p_add">+	 * tree, but as the finobt feature predates the per-AG reservation</span>
<span class="p_add">+	 * support a degraded file system might not have enough space for the</span>
<span class="p_add">+	 * reservation at mount time.  In that case try to dip into the reserved</span>
<span class="p_add">+	 * pool and pray.</span>
 	 *
 	 * Send a warning if the reservation does happen to fail, as the inode
 	 * now remains allocated and sits on the unlinked list until the fs is
 	 * repaired.
 	 */
<span class="p_del">-	error = xfs_trans_alloc(mp, &amp;M_RES(mp)-&gt;tr_ifree,</span>
<span class="p_del">-			XFS_IFREE_SPACE_RES(mp), 0, XFS_TRANS_RESERVE, &amp;tp);</span>
<span class="p_add">+	if (unlikely(mp-&gt;m_inotbt_nores)) {</span>
<span class="p_add">+		error = xfs_trans_alloc(mp, &amp;M_RES(mp)-&gt;tr_ifree,</span>
<span class="p_add">+				XFS_IFREE_SPACE_RES(mp), 0, XFS_TRANS_RESERVE,</span>
<span class="p_add">+				&amp;tp);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		error = xfs_trans_alloc(mp, &amp;M_RES(mp)-&gt;tr_ifree, 0, 0, 0, &amp;tp);</span>
<span class="p_add">+	}</span>
 	if (error) {
 		if (error == -ENOSPC) {
 			xfs_warn_ratelimited(mp,
<span class="p_chunk">@@ -1912,8 +1915,11 @@</span> <span class="p_context"> xfs_inactive(</span>
 		 * cache. Post-eof blocks must be freed, lest we end up with
 		 * broken free space accounting.
 		 */
<span class="p_del">-		if (xfs_can_free_eofblocks(ip, true))</span>
<span class="p_del">-			xfs_free_eofblocks(mp, ip, false);</span>
<span class="p_add">+		if (xfs_can_free_eofblocks(ip, true)) {</span>
<span class="p_add">+			xfs_ilock(ip, XFS_IOLOCK_EXCL);</span>
<span class="p_add">+			xfs_free_eofblocks(ip);</span>
<span class="p_add">+			xfs_iunlock(ip, XFS_IOLOCK_EXCL);</span>
<span class="p_add">+		}</span>
 
 		return;
 	}
<span class="p_header">diff --git a/fs/xfs/xfs_iomap.c b/fs/xfs/xfs_iomap.c</span>
<span class="p_header">index e8889614cec3..360562484e7b 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_iomap.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_iomap.c</span>
<span class="p_chunk">@@ -637,6 +637,11 @@</span> <span class="p_context"> xfs_file_iomap_begin_delay(</span>
 		goto out_unlock;
 	}
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Flag newly allocated delalloc blocks with IOMAP_F_NEW so we punch</span>
<span class="p_add">+	 * them out if the write happens to fail.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	iomap-&gt;flags = IOMAP_F_NEW;</span>
 	trace_xfs_iomap_alloc(ip, offset, count, 0, &amp;got);
 done:
 	if (isnullstartblock(got.br_startblock))
<span class="p_chunk">@@ -685,7 +690,7 @@</span> <span class="p_context"> xfs_iomap_write_allocate(</span>
 	int		nres;
 
 	if (whichfork == XFS_COW_FORK)
<span class="p_del">-		flags |= XFS_BMAPI_COWFORK;</span>
<span class="p_add">+		flags |= XFS_BMAPI_COWFORK | XFS_BMAPI_PREALLOC;</span>
 
 	/*
 	 * Make sure that the dquots are there.
<span class="p_chunk">@@ -1061,7 +1066,8 @@</span> <span class="p_context"> xfs_file_iomap_end_delalloc(</span>
 	struct xfs_inode	*ip,
 	loff_t			offset,
 	loff_t			length,
<span class="p_del">-	ssize_t			written)</span>
<span class="p_add">+	ssize_t			written,</span>
<span class="p_add">+	struct iomap		*iomap)</span>
 {
 	struct xfs_mount	*mp = ip-&gt;i_mount;
 	xfs_fileoff_t		start_fsb;
<span class="p_chunk">@@ -1080,14 +1086,14 @@</span> <span class="p_context"> xfs_file_iomap_end_delalloc(</span>
 	end_fsb = XFS_B_TO_FSB(mp, offset + length);
 
 	/*
<span class="p_del">-	 * Trim back delalloc blocks if we didn&#39;t manage to write the whole</span>
<span class="p_del">-	 * range reserved.</span>
<span class="p_add">+	 * Trim delalloc blocks if they were allocated by this write and we</span>
<span class="p_add">+	 * didn&#39;t manage to write the whole range.</span>
 	 *
 	 * We don&#39;t need to care about racing delalloc as we hold i_mutex
 	 * across the reserve/allocate/unreserve calls. If there are delalloc
 	 * blocks in the range, they are ours.
 	 */
<span class="p_del">-	if (start_fsb &lt; end_fsb) {</span>
<span class="p_add">+	if ((iomap-&gt;flags &amp; IOMAP_F_NEW) &amp;&amp; start_fsb &lt; end_fsb) {</span>
 		truncate_pagecache_range(VFS_I(ip), XFS_FSB_TO_B(mp, start_fsb),
 					 XFS_FSB_TO_B(mp, end_fsb) - 1);
 
<span class="p_chunk">@@ -1117,7 +1123,7 @@</span> <span class="p_context"> xfs_file_iomap_end(</span>
 {
 	if ((flags &amp; IOMAP_WRITE) &amp;&amp; iomap-&gt;type == IOMAP_DELALLOC)
 		return xfs_file_iomap_end_delalloc(XFS_I(inode), offset,
<span class="p_del">-				length, written);</span>
<span class="p_add">+				length, written, iomap);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/fs/xfs/xfs_mount.c b/fs/xfs/xfs_mount.c</span>
<span class="p_header">index b341f10cf481..13796f212f98 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_mount.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_mount.c</span>
<span class="p_chunk">@@ -502,8 +502,7 @@</span> <span class="p_context"> STATIC void</span>
 xfs_set_inoalignment(xfs_mount_t *mp)
 {
 	if (xfs_sb_version_hasalign(&amp;mp-&gt;m_sb) &amp;&amp;
<span class="p_del">-	    mp-&gt;m_sb.sb_inoalignmt &gt;=</span>
<span class="p_del">-	    XFS_B_TO_FSBT(mp, mp-&gt;m_inode_cluster_size))</span>
<span class="p_add">+		mp-&gt;m_sb.sb_inoalignmt &gt;= xfs_icluster_size_fsb(mp))</span>
 		mp-&gt;m_inoalign_mask = mp-&gt;m_sb.sb_inoalignmt - 1;
 	else
 		mp-&gt;m_inoalign_mask = 0;
<span class="p_header">diff --git a/fs/xfs/xfs_mount.h b/fs/xfs/xfs_mount.h</span>
<span class="p_header">index 819b80b15bfb..1bf878b0492c 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_mount.h</span>
<span class="p_header">+++ b/fs/xfs/xfs_mount.h</span>
<span class="p_chunk">@@ -140,6 +140,7 @@</span> <span class="p_context"> typedef struct xfs_mount {</span>
 	int			m_fixedfsid[2];	/* unchanged for life of FS */
 	uint			m_dmevmask;	/* DMI events for this FS */
 	__uint64_t		m_flags;	/* global mount flags */
<span class="p_add">+	bool			m_inotbt_nores; /* no per-AG finobt resv. */</span>
 	int			m_ialloc_inos;	/* inodes in inode allocation */
 	int			m_ialloc_blks;	/* blocks in inode allocation */
 	int			m_ialloc_min_blks;/* min blocks in sparse inode
<span class="p_header">diff --git a/fs/xfs/xfs_reflink.c b/fs/xfs/xfs_reflink.c</span>
<span class="p_header">index 4d3f74e3c5e1..2252f163c38f 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_reflink.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_reflink.c</span>
<span class="p_chunk">@@ -82,11 +82,22 @@</span> <span class="p_context"></span>
  * mappings are a reservation against the free space in the filesystem;
  * adjacent mappings can also be combined into fewer larger mappings.
  *
<span class="p_add">+ * As an optimization, the CoW extent size hint (cowextsz) creates</span>
<span class="p_add">+ * outsized aligned delalloc reservations in the hope of landing out of</span>
<span class="p_add">+ * order nearby CoW writes in a single extent on disk, thereby reducing</span>
<span class="p_add">+ * fragmentation and improving future performance.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * D: --RRRRRRSSSRRRRRRRR--- (data fork)</span>
<span class="p_add">+ * C: ------DDDDDDD--------- (CoW fork)</span>
<span class="p_add">+ *</span>
  * When dirty pages are being written out (typically in writepage), the
<span class="p_del">- * delalloc reservations are converted into real mappings by allocating</span>
<span class="p_del">- * blocks and replacing the delalloc mapping with real ones.  A delalloc</span>
<span class="p_del">- * mapping can be replaced by several real ones if the free space is</span>
<span class="p_del">- * fragmented.</span>
<span class="p_add">+ * delalloc reservations are converted into unwritten mappings by</span>
<span class="p_add">+ * allocating blocks and replacing the delalloc mapping with real ones.</span>
<span class="p_add">+ * A delalloc mapping can be replaced by several unwritten ones if the</span>
<span class="p_add">+ * free space is fragmented.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * D: --RRRRRRSSSRRRRRRRR---</span>
<span class="p_add">+ * C: ------UUUUUUU---------</span>
  *
  * We want to adapt the delalloc mechanism for copy-on-write, since the
  * write paths are similar.  The first two steps (creating the reservation
<span class="p_chunk">@@ -101,13 +112,29 @@</span> <span class="p_context"></span>
  * Block-aligned directio writes will use the same mechanism as buffered
  * writes.
  *
<span class="p_add">+ * Just prior to submitting the actual disk write requests, we convert</span>
<span class="p_add">+ * the extents representing the range of the file actually being written</span>
<span class="p_add">+ * (as opposed to extra pieces created for the cowextsize hint) to real</span>
<span class="p_add">+ * extents.  This will become important in the next step:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * D: --RRRRRRSSSRRRRRRRR---</span>
<span class="p_add">+ * C: ------UUrrUUU---------</span>
<span class="p_add">+ *</span>
  * CoW remapping must be done after the data block write completes,
  * because we don&#39;t want to destroy the old data fork map until we&#39;re sure
  * the new block has been written.  Since the new mappings are kept in a
  * separate fork, we can simply iterate these mappings to find the ones
  * that cover the file blocks that we just CoW&#39;d.  For each extent, simply
  * unmap the corresponding range in the data fork, map the new range into
<span class="p_del">- * the data fork, and remove the extent from the CoW fork.</span>
<span class="p_add">+ * the data fork, and remove the extent from the CoW fork.  Because of</span>
<span class="p_add">+ * the presence of the cowextsize hint, however, we must be careful</span>
<span class="p_add">+ * only to remap the blocks that we&#39;ve actually written out --  we must</span>
<span class="p_add">+ * never remap delalloc reservations nor CoW staging blocks that have</span>
<span class="p_add">+ * yet to be written.  This corresponds exactly to the real extents in</span>
<span class="p_add">+ * the CoW fork:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * D: --RRRRRRrrSRRRRRRRR---</span>
<span class="p_add">+ * C: ------UU--UUU---------</span>
  *
  * Since the remapping operation can be applied to an arbitrary file
  * range, we record the need for the remap step as a flag in the ioend
<span class="p_chunk">@@ -296,6 +323,65 @@</span> <span class="p_context"> xfs_reflink_reserve_cow(</span>
 	return 0;
 }
 
<span class="p_add">+/* Convert part of an unwritten CoW extent to a real one. */</span>
<span class="p_add">+STATIC int</span>
<span class="p_add">+xfs_reflink_convert_cow_extent(</span>
<span class="p_add">+	struct xfs_inode		*ip,</span>
<span class="p_add">+	struct xfs_bmbt_irec		*imap,</span>
<span class="p_add">+	xfs_fileoff_t			offset_fsb,</span>
<span class="p_add">+	xfs_filblks_t			count_fsb,</span>
<span class="p_add">+	struct xfs_defer_ops		*dfops)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct xfs_bmbt_irec		irec = *imap;</span>
<span class="p_add">+	xfs_fsblock_t			first_block;</span>
<span class="p_add">+	int				nimaps = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (imap-&gt;br_state == XFS_EXT_NORM)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	xfs_trim_extent(&amp;irec, offset_fsb, count_fsb);</span>
<span class="p_add">+	trace_xfs_reflink_convert_cow(ip, &amp;irec);</span>
<span class="p_add">+	if (irec.br_blockcount == 0)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	return xfs_bmapi_write(NULL, ip, irec.br_startoff, irec.br_blockcount,</span>
<span class="p_add">+			XFS_BMAPI_COWFORK | XFS_BMAPI_CONVERT, &amp;first_block,</span>
<span class="p_add">+			0, &amp;irec, &amp;nimaps, dfops);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Convert all of the unwritten CoW extents in a file&#39;s range to real ones. */</span>
<span class="p_add">+int</span>
<span class="p_add">+xfs_reflink_convert_cow(</span>
<span class="p_add">+	struct xfs_inode	*ip,</span>
<span class="p_add">+	xfs_off_t		offset,</span>
<span class="p_add">+	xfs_off_t		count)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct xfs_bmbt_irec	got;</span>
<span class="p_add">+	struct xfs_defer_ops	dfops;</span>
<span class="p_add">+	struct xfs_mount	*mp = ip-&gt;i_mount;</span>
<span class="p_add">+	struct xfs_ifork	*ifp = XFS_IFORK_PTR(ip, XFS_COW_FORK);</span>
<span class="p_add">+	xfs_fileoff_t		offset_fsb = XFS_B_TO_FSBT(mp, offset);</span>
<span class="p_add">+	xfs_fileoff_t		end_fsb = XFS_B_TO_FSB(mp, offset + count);</span>
<span class="p_add">+	xfs_extnum_t		idx;</span>
<span class="p_add">+	bool			found;</span>
<span class="p_add">+	int			error = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	xfs_ilock(ip, XFS_ILOCK_EXCL);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Convert all the extents to real from unwritten. */</span>
<span class="p_add">+	for (found = xfs_iext_lookup_extent(ip, ifp, offset_fsb, &amp;idx, &amp;got);</span>
<span class="p_add">+	     found &amp;&amp; got.br_startoff &lt; end_fsb;</span>
<span class="p_add">+	     found = xfs_iext_get_extent(ifp, ++idx, &amp;got)) {</span>
<span class="p_add">+		error = xfs_reflink_convert_cow_extent(ip, &amp;got, offset_fsb,</span>
<span class="p_add">+				end_fsb - offset_fsb, &amp;dfops);</span>
<span class="p_add">+		if (error)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Finish up. */</span>
<span class="p_add">+	xfs_iunlock(ip, XFS_ILOCK_EXCL);</span>
<span class="p_add">+	return error;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Allocate all CoW reservations covering a range of blocks in a file. */
 static int
 __xfs_reflink_allocate_cow(
<span class="p_chunk">@@ -328,6 +414,7 @@</span> <span class="p_context"> __xfs_reflink_allocate_cow(</span>
 		goto out_unlock;
 	ASSERT(nimaps == 1);
 
<span class="p_add">+	/* Make sure there&#39;s a CoW reservation for it. */</span>
 	error = xfs_reflink_reserve_cow(ip, &amp;imap, &amp;shared);
 	if (error)
 		goto out_trans_cancel;
<span class="p_chunk">@@ -337,14 +424,16 @@</span> <span class="p_context"> __xfs_reflink_allocate_cow(</span>
 		goto out_trans_cancel;
 	}
 
<span class="p_add">+	/* Allocate the entire reservation as unwritten blocks. */</span>
 	xfs_trans_ijoin(tp, ip, 0);
 	error = xfs_bmapi_write(tp, ip, imap.br_startoff, imap.br_blockcount,
<span class="p_del">-			XFS_BMAPI_COWFORK, &amp;first_block,</span>
<span class="p_add">+			XFS_BMAPI_COWFORK | XFS_BMAPI_PREALLOC, &amp;first_block,</span>
 			XFS_EXTENTADD_SPACE_RES(mp, XFS_DATA_FORK),
 			&amp;imap, &amp;nimaps, &amp;dfops);
 	if (error)
 		goto out_trans_cancel;
 
<span class="p_add">+	/* Finish up. */</span>
 	error = xfs_defer_finish(&amp;tp, &amp;dfops, NULL);
 	if (error)
 		goto out_trans_cancel;
<span class="p_chunk">@@ -389,11 +478,12 @@</span> <span class="p_context"> xfs_reflink_allocate_cow_range(</span>
 		if (error) {
 			trace_xfs_reflink_allocate_cow_range_error(ip, error,
 					_RET_IP_);
<span class="p_del">-			break;</span>
<span class="p_add">+			return error;</span>
 		}
 	}
 
<span class="p_del">-	return error;</span>
<span class="p_add">+	/* Convert the CoW extents to regular. */</span>
<span class="p_add">+	return xfs_reflink_convert_cow(ip, offset, count);</span>
 }
 
 /*
<span class="p_chunk">@@ -481,14 +571,18 @@</span> <span class="p_context"> xfs_reflink_trim_irec_to_next_cow(</span>
 }
 
 /*
<span class="p_del">- * Cancel all pending CoW reservations for some block range of an inode.</span>
<span class="p_add">+ * Cancel CoW reservations for some block range of an inode.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If cancel_real is true this function cancels all COW fork extents for the</span>
<span class="p_add">+ * inode; if cancel_real is false, real extents are not cleared.</span>
  */
 int
 xfs_reflink_cancel_cow_blocks(
 	struct xfs_inode		*ip,
 	struct xfs_trans		**tpp,
 	xfs_fileoff_t			offset_fsb,
<span class="p_del">-	xfs_fileoff_t			end_fsb)</span>
<span class="p_add">+	xfs_fileoff_t			end_fsb,</span>
<span class="p_add">+	bool				cancel_real)</span>
 {
 	struct xfs_ifork		*ifp = XFS_IFORK_PTR(ip, XFS_COW_FORK);
 	struct xfs_bmbt_irec		got, prev, del;
<span class="p_chunk">@@ -515,7 +609,7 @@</span> <span class="p_context"> xfs_reflink_cancel_cow_blocks(</span>
 					&amp;idx, &amp;got, &amp;del);
 			if (error)
 				break;
<span class="p_del">-		} else {</span>
<span class="p_add">+		} else if (del.br_state == XFS_EXT_UNWRITTEN || cancel_real) {</span>
 			xfs_trans_ijoin(*tpp, ip, 0);
 			xfs_defer_init(&amp;dfops, &amp;firstfsb);
 
<span class="p_chunk">@@ -558,13 +652,17 @@</span> <span class="p_context"> xfs_reflink_cancel_cow_blocks(</span>
 }
 
 /*
<span class="p_del">- * Cancel all pending CoW reservations for some byte range of an inode.</span>
<span class="p_add">+ * Cancel CoW reservations for some byte range of an inode.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If cancel_real is true this function cancels all COW fork extents for the</span>
<span class="p_add">+ * inode; if cancel_real is false, real extents are not cleared.</span>
  */
 int
 xfs_reflink_cancel_cow_range(
 	struct xfs_inode	*ip,
 	xfs_off_t		offset,
<span class="p_del">-	xfs_off_t		count)</span>
<span class="p_add">+	xfs_off_t		count,</span>
<span class="p_add">+	bool			cancel_real)</span>
 {
 	struct xfs_trans	*tp;
 	xfs_fileoff_t		offset_fsb;
<span class="p_chunk">@@ -590,7 +688,8 @@</span> <span class="p_context"> xfs_reflink_cancel_cow_range(</span>
 	xfs_trans_ijoin(tp, ip, 0);
 
 	/* Scrape out the old CoW reservations */
<span class="p_del">-	error = xfs_reflink_cancel_cow_blocks(ip, &amp;tp, offset_fsb, end_fsb);</span>
<span class="p_add">+	error = xfs_reflink_cancel_cow_blocks(ip, &amp;tp, offset_fsb, end_fsb,</span>
<span class="p_add">+			cancel_real);</span>
 	if (error)
 		goto out_cancel;
 
<span class="p_chunk">@@ -669,6 +768,16 @@</span> <span class="p_context"> xfs_reflink_end_cow(</span>
 
 		ASSERT(!isnullstartblock(got.br_startblock));
 
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Don&#39;t remap unwritten extents; these are</span>
<span class="p_add">+		 * speculatively preallocated CoW extents that have been</span>
<span class="p_add">+		 * allocated but have not yet been involved in a write.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (got.br_state == XFS_EXT_UNWRITTEN) {</span>
<span class="p_add">+			idx--;</span>
<span class="p_add">+			goto next_extent;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		/* Unmap the old blocks in the data fork. */
 		xfs_defer_init(&amp;dfops, &amp;firstfsb);
 		rlen = del.br_blockcount;
<span class="p_chunk">@@ -885,13 +994,14 @@</span> <span class="p_context"> STATIC int</span>
 xfs_reflink_update_dest(
 	struct xfs_inode	*dest,
 	xfs_off_t		newlen,
<span class="p_del">-	xfs_extlen_t		cowextsize)</span>
<span class="p_add">+	xfs_extlen_t		cowextsize,</span>
<span class="p_add">+	bool			is_dedupe)</span>
 {
 	struct xfs_mount	*mp = dest-&gt;i_mount;
 	struct xfs_trans	*tp;
 	int			error;
 
<span class="p_del">-	if (newlen &lt;= i_size_read(VFS_I(dest)) &amp;&amp; cowextsize == 0)</span>
<span class="p_add">+	if (is_dedupe &amp;&amp; newlen &lt;= i_size_read(VFS_I(dest)) &amp;&amp; cowextsize == 0)</span>
 		return 0;
 
 	error = xfs_trans_alloc(mp, &amp;M_RES(mp)-&gt;tr_ichange, 0, 0, 0, &amp;tp);
<span class="p_chunk">@@ -912,6 +1022,10 @@</span> <span class="p_context"> xfs_reflink_update_dest(</span>
 		dest-&gt;i_d.di_flags2 |= XFS_DIFLAG2_COWEXTSIZE;
 	}
 
<span class="p_add">+	if (!is_dedupe) {</span>
<span class="p_add">+		xfs_trans_ichgtime(tp, dest,</span>
<span class="p_add">+				   XFS_ICHGTIME_MOD | XFS_ICHGTIME_CHG);</span>
<span class="p_add">+	}</span>
 	xfs_trans_log_inode(tp, dest, XFS_ILOG_CORE);
 
 	error = xfs_trans_commit(tp);
<span class="p_chunk">@@ -1428,7 +1542,8 @@</span> <span class="p_context"> xfs_reflink_remap_range(</span>
 	    !(dest-&gt;i_d.di_flags2 &amp; XFS_DIFLAG2_COWEXTSIZE))
 		cowextsize = src-&gt;i_d.di_cowextsize;
 
<span class="p_del">-	ret = xfs_reflink_update_dest(dest, pos_out + len, cowextsize);</span>
<span class="p_add">+	ret = xfs_reflink_update_dest(dest, pos_out + len, cowextsize,</span>
<span class="p_add">+			is_dedupe);</span>
 
 out_unlock:
 	xfs_iunlock(src, XFS_MMAPLOCK_EXCL);
<span class="p_chunk">@@ -1580,7 +1695,7 @@</span> <span class="p_context"> xfs_reflink_clear_inode_flag(</span>
 	 * We didn&#39;t find any shared blocks so turn off the reflink flag.
 	 * First, get rid of any leftover CoW mappings.
 	 */
<span class="p_del">-	error = xfs_reflink_cancel_cow_blocks(ip, tpp, 0, NULLFILEOFF);</span>
<span class="p_add">+	error = xfs_reflink_cancel_cow_blocks(ip, tpp, 0, NULLFILEOFF, true);</span>
 	if (error)
 		return error;
 
<span class="p_header">diff --git a/fs/xfs/xfs_reflink.h b/fs/xfs/xfs_reflink.h</span>
<span class="p_header">index 97ea9b487884..a57966fc7ddd 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_reflink.h</span>
<span class="p_header">+++ b/fs/xfs/xfs_reflink.h</span>
<span class="p_chunk">@@ -30,6 +30,8 @@</span> <span class="p_context"> extern int xfs_reflink_reserve_cow(struct xfs_inode *ip,</span>
 		struct xfs_bmbt_irec *imap, bool *shared);
 extern int xfs_reflink_allocate_cow_range(struct xfs_inode *ip,
 		xfs_off_t offset, xfs_off_t count);
<span class="p_add">+extern int xfs_reflink_convert_cow(struct xfs_inode *ip, xfs_off_t offset,</span>
<span class="p_add">+		xfs_off_t count);</span>
 extern bool xfs_reflink_find_cow_mapping(struct xfs_inode *ip, xfs_off_t offset,
 		struct xfs_bmbt_irec *imap, bool *need_alloc);
 extern int xfs_reflink_trim_irec_to_next_cow(struct xfs_inode *ip,
<span class="p_chunk">@@ -37,9 +39,9 @@</span> <span class="p_context"> extern int xfs_reflink_trim_irec_to_next_cow(struct xfs_inode *ip,</span>
 
 extern int xfs_reflink_cancel_cow_blocks(struct xfs_inode *ip,
 		struct xfs_trans **tpp, xfs_fileoff_t offset_fsb,
<span class="p_del">-		xfs_fileoff_t end_fsb);</span>
<span class="p_add">+		xfs_fileoff_t end_fsb, bool cancel_real);</span>
 extern int xfs_reflink_cancel_cow_range(struct xfs_inode *ip, xfs_off_t offset,
<span class="p_del">-		xfs_off_t count);</span>
<span class="p_add">+		xfs_off_t count, bool cancel_real);</span>
 extern int xfs_reflink_end_cow(struct xfs_inode *ip, xfs_off_t offset,
 		xfs_off_t count);
 extern int xfs_reflink_recover_cow(struct xfs_mount *mp);
<span class="p_header">diff --git a/fs/xfs/xfs_super.c b/fs/xfs/xfs_super.c</span>
<span class="p_header">index ade4691e3f74..dbbd3f1fd2b7 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_super.c</span>
<span class="p_header">+++ b/fs/xfs/xfs_super.c</span>
<span class="p_chunk">@@ -948,7 +948,7 @@</span> <span class="p_context"> xfs_fs_destroy_inode(</span>
 	XFS_STATS_INC(ip-&gt;i_mount, vn_remove);
 
 	if (xfs_is_reflink_inode(ip)) {
<span class="p_del">-		error = xfs_reflink_cancel_cow_range(ip, 0, NULLFILEOFF);</span>
<span class="p_add">+		error = xfs_reflink_cancel_cow_range(ip, 0, NULLFILEOFF, true);</span>
 		if (error &amp;&amp; !XFS_FORCED_SHUTDOWN(ip-&gt;i_mount))
 			xfs_warn(ip-&gt;i_mount,
 &quot;Error %d while evicting CoW blocks for inode %llu.&quot;,
<span class="p_header">diff --git a/fs/xfs/xfs_trace.h b/fs/xfs/xfs_trace.h</span>
<span class="p_header">index 0907752be62d..828f383df121 100644</span>
<span class="p_header">--- a/fs/xfs/xfs_trace.h</span>
<span class="p_header">+++ b/fs/xfs/xfs_trace.h</span>
<span class="p_chunk">@@ -3183,6 +3183,7 @@</span> <span class="p_context"> DECLARE_EVENT_CLASS(xfs_inode_irec_class,</span>
 		__field(xfs_fileoff_t, lblk)
 		__field(xfs_extlen_t, len)
 		__field(xfs_fsblock_t, pblk)
<span class="p_add">+		__field(int, state)</span>
 	),
 	TP_fast_assign(
 		__entry-&gt;dev = VFS_I(ip)-&gt;i_sb-&gt;s_dev;
<span class="p_chunk">@@ -3190,13 +3191,15 @@</span> <span class="p_context"> DECLARE_EVENT_CLASS(xfs_inode_irec_class,</span>
 		__entry-&gt;lblk = irec-&gt;br_startoff;
 		__entry-&gt;len = irec-&gt;br_blockcount;
 		__entry-&gt;pblk = irec-&gt;br_startblock;
<span class="p_add">+		__entry-&gt;state = irec-&gt;br_state;</span>
 	),
<span class="p_del">-	TP_printk(&quot;dev %d:%d ino 0x%llx lblk 0x%llx len 0x%x pblk %llu&quot;,</span>
<span class="p_add">+	TP_printk(&quot;dev %d:%d ino 0x%llx lblk 0x%llx len 0x%x pblk %llu st %d&quot;,</span>
 		  MAJOR(__entry-&gt;dev), MINOR(__entry-&gt;dev),
 		  __entry-&gt;ino,
 		  __entry-&gt;lblk,
 		  __entry-&gt;len,
<span class="p_del">-		  __entry-&gt;pblk)</span>
<span class="p_add">+		  __entry-&gt;pblk,</span>
<span class="p_add">+		  __entry-&gt;state)</span>
 );
 #define DEFINE_INODE_IREC_EVENT(name) \
 DEFINE_EVENT(xfs_inode_irec_class, name, \
<span class="p_chunk">@@ -3345,11 +3348,12 @@</span> <span class="p_context"> DEFINE_INODE_IREC_EVENT(xfs_reflink_trim_around_shared);</span>
 DEFINE_INODE_IREC_EVENT(xfs_reflink_cow_alloc);
 DEFINE_INODE_IREC_EVENT(xfs_reflink_cow_found);
 DEFINE_INODE_IREC_EVENT(xfs_reflink_cow_enospc);
<span class="p_add">+DEFINE_INODE_IREC_EVENT(xfs_reflink_convert_cow);</span>
 
 DEFINE_RW_EVENT(xfs_reflink_reserve_cow);
 DEFINE_RW_EVENT(xfs_reflink_allocate_cow_range);
 
<span class="p_del">-DEFINE_INODE_IREC_EVENT(xfs_reflink_bounce_dio_write);</span>
<span class="p_add">+DEFINE_SIMPLE_IO_EVENT(xfs_reflink_bounce_dio_write);</span>
 DEFINE_IOMAP_EVENT(xfs_reflink_find_cow_mapping);
 DEFINE_INODE_IREC_EVENT(xfs_reflink_trim_irec);
 
<span class="p_header">diff --git a/include/linux/kvm_host.h b/include/linux/kvm_host.h</span>
<span class="p_header">index 01c0b9cc3915..8c58db2c09c6 100644</span>
<span class="p_header">--- a/include/linux/kvm_host.h</span>
<span class="p_header">+++ b/include/linux/kvm_host.h</span>
<span class="p_chunk">@@ -162,8 +162,8 @@</span> <span class="p_context"> int kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,</span>
 		    int len, void *val);
 int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,
 			    int len, struct kvm_io_device *dev);
<span class="p_del">-int kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,</span>
<span class="p_del">-			      struct kvm_io_device *dev);</span>
<span class="p_add">+void kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,</span>
<span class="p_add">+			       struct kvm_io_device *dev);</span>
 struct kvm_io_device *kvm_io_bus_get_dev(struct kvm *kvm, enum kvm_bus bus_idx,
 					 gpa_t addr);
 
<span class="p_header">diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="p_header">index 254698856b8f..8b35bdbdc214 100644</span>
<span class="p_header">--- a/include/linux/memcontrol.h</span>
<span class="p_header">+++ b/include/linux/memcontrol.h</span>
<span class="p_chunk">@@ -739,6 +739,12 @@</span> <span class="p_context"> static inline bool mem_cgroup_oom_synchronize(bool wait)</span>
 	return false;
 }
 
<span class="p_add">+static inline void mem_cgroup_update_page_stat(struct page *page,</span>
<span class="p_add">+					       enum mem_cgroup_stat_index idx,</span>
<span class="p_add">+					       int nr)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void mem_cgroup_inc_page_stat(struct page *page,
 					    enum mem_cgroup_stat_index idx)
 {
<span class="p_header">diff --git a/kernel/padata.c b/kernel/padata.c</span>
<span class="p_header">index 7848f0566403..b4a3c0ae649b 100644</span>
<span class="p_header">--- a/kernel/padata.c</span>
<span class="p_header">+++ b/kernel/padata.c</span>
<span class="p_chunk">@@ -190,19 +190,20 @@</span> <span class="p_context"> static struct padata_priv *padata_get_next(struct parallel_data *pd)</span>
 
 	reorder = &amp;next_queue-&gt;reorder;
 
<span class="p_add">+	spin_lock(&amp;reorder-&gt;lock);</span>
 	if (!list_empty(&amp;reorder-&gt;list)) {
 		padata = list_entry(reorder-&gt;list.next,
 				    struct padata_priv, list);
 
<span class="p_del">-		spin_lock(&amp;reorder-&gt;lock);</span>
 		list_del_init(&amp;padata-&gt;list);
 		atomic_dec(&amp;pd-&gt;reorder_objects);
<span class="p_del">-		spin_unlock(&amp;reorder-&gt;lock);</span>
 
 		pd-&gt;processed++;
 
<span class="p_add">+		spin_unlock(&amp;reorder-&gt;lock);</span>
 		goto out;
 	}
<span class="p_add">+	spin_unlock(&amp;reorder-&gt;lock);</span>
 
 	if (__this_cpu_read(pd-&gt;pqueue-&gt;cpu_index) == next_queue-&gt;cpu_index) {
 		padata = ERR_PTR(-ENODATA);
<span class="p_header">diff --git a/lib/syscall.c b/lib/syscall.c</span>
<span class="p_header">index 63239e097b13..a72cd0996230 100644</span>
<span class="p_header">--- a/lib/syscall.c</span>
<span class="p_header">+++ b/lib/syscall.c</span>
<span class="p_chunk">@@ -11,6 +11,7 @@</span> <span class="p_context"> static int collect_syscall(struct task_struct *target, long *callno,</span>
 
 	if (!try_get_task_stack(target)) {
 		/* Task has no stack, so the task isn&#39;t in a syscall. */
<span class="p_add">+		*sp = *pc = 0;</span>
 		*callno = -1;
 		return 0;
 	}
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index b6adedbafaf5..65c36acf8a6b 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -4471,6 +4471,7 @@</span> <span class="p_context"> follow_huge_pmd(struct mm_struct *mm, unsigned long address,</span>
 {
 	struct page *page = NULL;
 	spinlock_t *ptl;
<span class="p_add">+	pte_t pte;</span>
 retry:
 	ptl = pmd_lockptr(mm, pmd);
 	spin_lock(ptl);
<span class="p_chunk">@@ -4480,12 +4481,13 @@</span> <span class="p_context"> follow_huge_pmd(struct mm_struct *mm, unsigned long address,</span>
 	 */
 	if (!pmd_huge(*pmd))
 		goto out;
<span class="p_del">-	if (pmd_present(*pmd)) {</span>
<span class="p_add">+	pte = huge_ptep_get((pte_t *)pmd);</span>
<span class="p_add">+	if (pte_present(pte)) {</span>
 		page = pmd_page(*pmd) + ((address &amp; ~PMD_MASK) &gt;&gt; PAGE_SHIFT);
 		if (flags &amp; FOLL_GET)
 			get_page(page);
 	} else {
<span class="p_del">-		if (is_hugetlb_entry_migration(huge_ptep_get((pte_t *)pmd))) {</span>
<span class="p_add">+		if (is_hugetlb_entry_migration(pte)) {</span>
 			spin_unlock(ptl);
 			__migration_entry_wait(mm, (pte_t *)pmd, ptl);
 			goto retry;
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 1ef36404e7b2..cd37c1c7e21b 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1295,7 +1295,7 @@</span> <span class="p_context"> void page_add_file_rmap(struct page *page, bool compound)</span>
 			goto out;
 	}
 	__mod_node_page_state(page_pgdat(page), NR_FILE_MAPPED, nr);
<span class="p_del">-	mem_cgroup_inc_page_stat(page, MEM_CGROUP_STAT_FILE_MAPPED);</span>
<span class="p_add">+	mem_cgroup_update_page_stat(page, MEM_CGROUP_STAT_FILE_MAPPED, nr);</span>
 out:
 	unlock_page_memcg(page);
 }
<span class="p_chunk">@@ -1335,7 +1335,7 @@</span> <span class="p_context"> static void page_remove_file_rmap(struct page *page, bool compound)</span>
 	 * pte lock(a spinlock) is held, which implies preemption disabled.
 	 */
 	__mod_node_page_state(page_pgdat(page), NR_FILE_MAPPED, -nr);
<span class="p_del">-	mem_cgroup_dec_page_stat(page, MEM_CGROUP_STAT_FILE_MAPPED);</span>
<span class="p_add">+	mem_cgroup_update_page_stat(page, MEM_CGROUP_STAT_FILE_MAPPED, -nr);</span>
 
 	if (unlikely(PageMlocked(page)))
 		clear_page_mlock(page);
<span class="p_header">diff --git a/mm/workingset.c b/mm/workingset.c</span>
<span class="p_header">index 33f6f4db32fd..4c4f05655e6e 100644</span>
<span class="p_header">--- a/mm/workingset.c</span>
<span class="p_header">+++ b/mm/workingset.c</span>
<span class="p_chunk">@@ -492,7 +492,7 @@</span> <span class="p_context"> static int __init workingset_init(void)</span>
 	pr_info(&quot;workingset: timestamp_bits=%d max_order=%d bucket_order=%u\n&quot;,
 	       timestamp_bits, max_order, bucket_order);
 
<span class="p_del">-	ret = list_lru_init_key(&amp;workingset_shadow_nodes, &amp;shadow_nodes_key);</span>
<span class="p_add">+	ret = __list_lru_init(&amp;workingset_shadow_nodes, true, &amp;shadow_nodes_key);</span>
 	if (ret)
 		goto err;
 	ret = register_shrinker(&amp;workingset_shadow_shrinker);
<span class="p_header">diff --git a/net/ceph/messenger.c b/net/ceph/messenger.c</span>
<span class="p_header">index 2efb335deada..25a30be862e9 100644</span>
<span class="p_header">--- a/net/ceph/messenger.c</span>
<span class="p_header">+++ b/net/ceph/messenger.c</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/kthread.h&gt;
 #include &lt;linux/net.h&gt;
 #include &lt;linux/nsproxy.h&gt;
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/socket.h&gt;
 #include &lt;linux/string.h&gt;
<span class="p_chunk">@@ -469,11 +470,16 @@</span> <span class="p_context"> static int ceph_tcp_connect(struct ceph_connection *con)</span>
 {
 	struct sockaddr_storage *paddr = &amp;con-&gt;peer_addr.in_addr;
 	struct socket *sock;
<span class="p_add">+	unsigned int noio_flag;</span>
 	int ret;
 
 	BUG_ON(con-&gt;sock);
<span class="p_add">+</span>
<span class="p_add">+	/* sock_create_kern() allocates with GFP_KERNEL */</span>
<span class="p_add">+	noio_flag = memalloc_noio_save();</span>
 	ret = sock_create_kern(read_pnet(&amp;con-&gt;msgr-&gt;net), paddr-&gt;ss_family,
 			       SOCK_STREAM, IPPROTO_TCP, &amp;sock);
<span class="p_add">+	memalloc_noio_restore(noio_flag);</span>
 	if (ret)
 		return ret;
 	sock-&gt;sk-&gt;sk_allocation = GFP_NOFS;
<span class="p_header">diff --git a/sound/core/seq/seq_fifo.c b/sound/core/seq/seq_fifo.c</span>
<span class="p_header">index 3f4efcb85df5..3490d21ab9e7 100644</span>
<span class="p_header">--- a/sound/core/seq/seq_fifo.c</span>
<span class="p_header">+++ b/sound/core/seq/seq_fifo.c</span>
<span class="p_chunk">@@ -265,6 +265,10 @@</span> <span class="p_context"> int snd_seq_fifo_resize(struct snd_seq_fifo *f, int poolsize)</span>
 	/* NOTE: overflow flag is not cleared */
 	spin_unlock_irqrestore(&amp;f-&gt;lock, flags);
 
<span class="p_add">+	/* close the old pool and wait until all users are gone */</span>
<span class="p_add">+	snd_seq_pool_mark_closing(oldpool);</span>
<span class="p_add">+	snd_use_lock_sync(&amp;f-&gt;use_lock);</span>
<span class="p_add">+</span>
 	/* release cells in old pool */
 	for (cell = oldhead; cell; cell = next) {
 		next = cell-&gt;next;
<span class="p_header">diff --git a/sound/pci/hda/patch_realtek.c b/sound/pci/hda/patch_realtek.c</span>
<span class="p_header">index 112caa2d3c14..bb1aad39d987 100644</span>
<span class="p_header">--- a/sound/pci/hda/patch_realtek.c</span>
<span class="p_header">+++ b/sound/pci/hda/patch_realtek.c</span>
<span class="p_chunk">@@ -4846,6 +4846,7 @@</span> <span class="p_context"> enum {</span>
 	ALC292_FIXUP_DISABLE_AAMIX,
 	ALC293_FIXUP_DISABLE_AAMIX_MULTIJACK,
 	ALC298_FIXUP_DELL1_MIC_NO_PRESENCE,
<span class="p_add">+	ALC298_FIXUP_DELL_AIO_MIC_NO_PRESENCE,</span>
 	ALC275_FIXUP_DELL_XPS,
 	ALC256_FIXUP_DELL_XPS_13_HEADPHONE_NOISE,
 	ALC293_FIXUP_LENOVO_SPK_NOISE,
<span class="p_chunk">@@ -5446,6 +5447,15 @@</span> <span class="p_context"> static const struct hda_fixup alc269_fixups[] = {</span>
 		.chained = true,
 		.chain_id = ALC269_FIXUP_HEADSET_MODE
 	},
<span class="p_add">+	[ALC298_FIXUP_DELL_AIO_MIC_NO_PRESENCE] = {</span>
<span class="p_add">+		.type = HDA_FIXUP_PINS,</span>
<span class="p_add">+		.v.pins = (const struct hda_pintbl[]) {</span>
<span class="p_add">+			{ 0x18, 0x01a1913c }, /* use as headset mic, without its own jack detect */</span>
<span class="p_add">+			{ }</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.chained = true,</span>
<span class="p_add">+		.chain_id = ALC269_FIXUP_HEADSET_MODE</span>
<span class="p_add">+	},</span>
 	[ALC275_FIXUP_DELL_XPS] = {
 		.type = HDA_FIXUP_VERBS,
 		.v.verbs = (const struct hda_verb[]) {
<span class="p_chunk">@@ -5518,7 +5528,7 @@</span> <span class="p_context"> static const struct hda_fixup alc269_fixups[] = {</span>
 		.type = HDA_FIXUP_FUNC,
 		.v.func = alc298_fixup_speaker_volume,
 		.chained = true,
<span class="p_del">-		.chain_id = ALC298_FIXUP_DELL1_MIC_NO_PRESENCE,</span>
<span class="p_add">+		.chain_id = ALC298_FIXUP_DELL_AIO_MIC_NO_PRESENCE,</span>
 	},
 	[ALC256_FIXUP_DELL_INSPIRON_7559_SUBWOOFER] = {
 		.type = HDA_FIXUP_PINS,
<span class="p_header">diff --git a/sound/soc/atmel/atmel-classd.c b/sound/soc/atmel/atmel-classd.c</span>
<span class="p_header">index 89ac5f5a93eb..7ae46c2647d4 100644</span>
<span class="p_header">--- a/sound/soc/atmel/atmel-classd.c</span>
<span class="p_header">+++ b/sound/soc/atmel/atmel-classd.c</span>
<span class="p_chunk">@@ -349,7 +349,7 @@</span> <span class="p_context"> static int atmel_classd_codec_dai_digital_mute(struct snd_soc_dai *codec_dai,</span>
 }
 
 #define CLASSD_ACLK_RATE_11M2896_MPY_8 (112896 * 100 * 8)
<span class="p_del">-#define CLASSD_ACLK_RATE_12M288_MPY_8  (12228 * 1000 * 8)</span>
<span class="p_add">+#define CLASSD_ACLK_RATE_12M288_MPY_8  (12288 * 1000 * 8)</span>
 
 static struct {
 	int rate;
<span class="p_header">diff --git a/sound/soc/intel/skylake/skl-topology.c b/sound/soc/intel/skylake/skl-topology.c</span>
<span class="p_header">index b5b1934d8550..bef8a4546c12 100644</span>
<span class="p_header">--- a/sound/soc/intel/skylake/skl-topology.c</span>
<span class="p_header">+++ b/sound/soc/intel/skylake/skl-topology.c</span>
<span class="p_chunk">@@ -448,7 +448,7 @@</span> <span class="p_context"> static int skl_tplg_set_module_init_data(struct snd_soc_dapm_widget *w)</span>
 			if (bc-&gt;set_params != SKL_PARAM_INIT)
 				continue;
 
<span class="p_del">-			mconfig-&gt;formats_config.caps = (u32 *)&amp;bc-&gt;params;</span>
<span class="p_add">+			mconfig-&gt;formats_config.caps = (u32 *)bc-&gt;params;</span>
 			mconfig-&gt;formats_config.caps_size = bc-&gt;size;
 
 			break;
<span class="p_header">diff --git a/virt/kvm/eventfd.c b/virt/kvm/eventfd.c</span>
<span class="p_header">index a29786dd9522..4d28a9ddbee0 100644</span>
<span class="p_header">--- a/virt/kvm/eventfd.c</span>
<span class="p_header">+++ b/virt/kvm/eventfd.c</span>
<span class="p_chunk">@@ -870,7 +870,8 @@</span> <span class="p_context"> kvm_deassign_ioeventfd_idx(struct kvm *kvm, enum kvm_bus bus_idx,</span>
 			continue;
 
 		kvm_io_bus_unregister_dev(kvm, bus_idx, &amp;p-&gt;dev);
<span class="p_del">-		kvm-&gt;buses[bus_idx]-&gt;ioeventfd_count--;</span>
<span class="p_add">+		if (kvm-&gt;buses[bus_idx])</span>
<span class="p_add">+			kvm-&gt;buses[bus_idx]-&gt;ioeventfd_count--;</span>
 		ioeventfd_release(p);
 		ret = 0;
 		break;
<span class="p_header">diff --git a/virt/kvm/kvm_main.c b/virt/kvm/kvm_main.c</span>
<span class="p_header">index 7f9ee2929cfe..f4c6d4f6d2e8 100644</span>
<span class="p_header">--- a/virt/kvm/kvm_main.c</span>
<span class="p_header">+++ b/virt/kvm/kvm_main.c</span>
<span class="p_chunk">@@ -720,8 +720,11 @@</span> <span class="p_context"> static void kvm_destroy_vm(struct kvm *kvm)</span>
 	list_del(&amp;kvm-&gt;vm_list);
 	spin_unlock(&amp;kvm_lock);
 	kvm_free_irq_routing(kvm);
<span class="p_del">-	for (i = 0; i &lt; KVM_NR_BUSES; i++)</span>
<span class="p_del">-		kvm_io_bus_destroy(kvm-&gt;buses[i]);</span>
<span class="p_add">+	for (i = 0; i &lt; KVM_NR_BUSES; i++) {</span>
<span class="p_add">+		if (kvm-&gt;buses[i])</span>
<span class="p_add">+			kvm_io_bus_destroy(kvm-&gt;buses[i]);</span>
<span class="p_add">+		kvm-&gt;buses[i] = NULL;</span>
<span class="p_add">+	}</span>
 	kvm_coalesced_mmio_free(kvm);
 #if defined(CONFIG_MMU_NOTIFIER) &amp;&amp; defined(KVM_ARCH_WANT_MMU_NOTIFIER)
 	mmu_notifier_unregister(&amp;kvm-&gt;mmu_notifier, kvm-&gt;mm);
<span class="p_chunk">@@ -3463,6 +3466,8 @@</span> <span class="p_context"> int kvm_io_bus_write(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,</span>
 	};
 
 	bus = srcu_dereference(vcpu-&gt;kvm-&gt;buses[bus_idx], &amp;vcpu-&gt;kvm-&gt;srcu);
<span class="p_add">+	if (!bus)</span>
<span class="p_add">+		return -ENOMEM;</span>
 	r = __kvm_io_bus_write(vcpu, bus, &amp;range, val);
 	return r &lt; 0 ? r : 0;
 }
<span class="p_chunk">@@ -3480,6 +3485,8 @@</span> <span class="p_context"> int kvm_io_bus_write_cookie(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx,</span>
 	};
 
 	bus = srcu_dereference(vcpu-&gt;kvm-&gt;buses[bus_idx], &amp;vcpu-&gt;kvm-&gt;srcu);
<span class="p_add">+	if (!bus)</span>
<span class="p_add">+		return -ENOMEM;</span>
 
 	/* First try the device referenced by cookie. */
 	if ((cookie &gt;= 0) &amp;&amp; (cookie &lt; bus-&gt;dev_count) &amp;&amp;
<span class="p_chunk">@@ -3530,6 +3537,8 @@</span> <span class="p_context"> int kvm_io_bus_read(struct kvm_vcpu *vcpu, enum kvm_bus bus_idx, gpa_t addr,</span>
 	};
 
 	bus = srcu_dereference(vcpu-&gt;kvm-&gt;buses[bus_idx], &amp;vcpu-&gt;kvm-&gt;srcu);
<span class="p_add">+	if (!bus)</span>
<span class="p_add">+		return -ENOMEM;</span>
 	r = __kvm_io_bus_read(vcpu, bus, &amp;range, val);
 	return r &lt; 0 ? r : 0;
 }
<span class="p_chunk">@@ -3542,6 +3551,9 @@</span> <span class="p_context"> int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,</span>
 	struct kvm_io_bus *new_bus, *bus;
 
 	bus = kvm-&gt;buses[bus_idx];
<span class="p_add">+	if (!bus)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
 	/* exclude ioeventfd which is limited by maximum fd */
 	if (bus-&gt;dev_count - bus-&gt;ioeventfd_count &gt; NR_IOBUS_DEVS - 1)
 		return -ENOSPC;
<span class="p_chunk">@@ -3561,37 +3573,41 @@</span> <span class="p_context"> int kvm_io_bus_register_dev(struct kvm *kvm, enum kvm_bus bus_idx, gpa_t addr,</span>
 }
 
 /* Caller must hold slots_lock. */
<span class="p_del">-int kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,</span>
<span class="p_del">-			      struct kvm_io_device *dev)</span>
<span class="p_add">+void kvm_io_bus_unregister_dev(struct kvm *kvm, enum kvm_bus bus_idx,</span>
<span class="p_add">+			       struct kvm_io_device *dev)</span>
 {
<span class="p_del">-	int i, r;</span>
<span class="p_add">+	int i;</span>
 	struct kvm_io_bus *new_bus, *bus;
 
 	bus = kvm-&gt;buses[bus_idx];
<span class="p_del">-	r = -ENOENT;</span>
<span class="p_add">+	if (!bus)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
 	for (i = 0; i &lt; bus-&gt;dev_count; i++)
 		if (bus-&gt;range[i].dev == dev) {
<span class="p_del">-			r = 0;</span>
 			break;
 		}
 
<span class="p_del">-	if (r)</span>
<span class="p_del">-		return r;</span>
<span class="p_add">+	if (i == bus-&gt;dev_count)</span>
<span class="p_add">+		return;</span>
 
 	new_bus = kmalloc(sizeof(*bus) + ((bus-&gt;dev_count - 1) *
 			  sizeof(struct kvm_io_range)), GFP_KERNEL);
<span class="p_del">-	if (!new_bus)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_add">+	if (!new_bus)  {</span>
<span class="p_add">+		pr_err(&quot;kvm: failed to shrink bus, removing it completely\n&quot;);</span>
<span class="p_add">+		goto broken;</span>
<span class="p_add">+	}</span>
 
 	memcpy(new_bus, bus, sizeof(*bus) + i * sizeof(struct kvm_io_range));
 	new_bus-&gt;dev_count--;
 	memcpy(new_bus-&gt;range + i, bus-&gt;range + i + 1,
 	       (new_bus-&gt;dev_count - i) * sizeof(struct kvm_io_range));
 
<span class="p_add">+broken:</span>
 	rcu_assign_pointer(kvm-&gt;buses[bus_idx], new_bus);
 	synchronize_srcu_expedited(&amp;kvm-&gt;srcu);
 	kfree(bus);
<span class="p_del">-	return r;</span>
<span class="p_add">+	return;</span>
 }
 
 struct kvm_io_device *kvm_io_bus_get_dev(struct kvm *kvm, enum kvm_bus bus_idx,
<span class="p_chunk">@@ -3604,6 +3620,8 @@</span> <span class="p_context"> struct kvm_io_device *kvm_io_bus_get_dev(struct kvm *kvm, enum kvm_bus bus_idx,</span>
 	srcu_idx = srcu_read_lock(&amp;kvm-&gt;srcu);
 
 	bus = srcu_dereference(kvm-&gt;buses[bus_idx], &amp;kvm-&gt;srcu);
<span class="p_add">+	if (!bus)</span>
<span class="p_add">+		goto out_unlock;</span>
 
 	dev_idx = kvm_io_bus_get_first_dev(bus, addr, 1);
 	if (dev_idx &lt; 0)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



