
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,pull] x86/pti: Fixes and updates for 4.15 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,pull] x86/pti: Fixes and updates for 4.15</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 5, 2018, 6:53 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;alpine.DEB.2.20.1801051943080.2376@nanos&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10146935/mbox/"
   >mbox</a>
|
   <a href="/patch/10146935/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10146935/">/patch/10146935/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	804056082F for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  5 Jan 2018 18:54:07 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6C6D728936
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  5 Jan 2018 18:54:07 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 615872893B; Fri,  5 Jan 2018 18:54:07 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7103C28936
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  5 Jan 2018 18:54:06 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752644AbeAESyD (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 5 Jan 2018 13:54:03 -0500
Received: from Galois.linutronix.de ([146.0.238.70]:45129 &quot;EHLO
	Galois.linutronix.de&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752332AbeAESyB (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 5 Jan 2018 13:54:01 -0500
Received: from p4fea5f09.dip0.t-ipconnect.de ([79.234.95.9] helo=nanos)
	by Galois.linutronix.de with esmtpsa
	(TLS1.2:DHE_RSA_AES_256_CBC_SHA256:256) (Exim 4.80)
	(envelope-from &lt;tglx@linutronix.de&gt;)
	id 1eXX5q-0002WN-0j; Fri, 05 Jan 2018 19:51:50 +0100
Date: Fri, 5 Jan 2018 19:53:59 +0100 (CET)
From: Thomas Gleixner &lt;tglx@linutronix.de&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
cc: LKML &lt;linux-kernel@vger.kernel.org&gt;, Ingo Molnar &lt;mingo@kernel.org&gt;,
	Greg KH &lt;gregkh@linuxfoundation.org&gt;
Subject: [GIT pull] x86/pti: Fixes and updates for 4.15
Message-ID: &lt;alpine.DEB.2.20.1801051943080.2376@nanos&gt;
User-Agent: Alpine 2.20 (DEB 67 2015-01-07)
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
X-Linutronix-Spam-Score: -1.0
X-Linutronix-Spam-Level: -
X-Linutronix-Spam-Status: No , -1.0 points, 5.0 required, ALL_TRUSTED=-1,
	SHORTCIRCUIT=-0.0001
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Jan. 5, 2018, 6:53 p.m.</div>
<pre class="content">
Linus,

please pull the latest x86-pti-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-pti-for-linus

Another small stash of fixes for fallout from the PTI work:

 - Fix the modules vs. KASAN breakage which was caused by making
   MODULES_END depend of the fixmap size. That was done when the cpu entry
   area moved into the fixmap, but now that we have a separate map space
   for that this is causing more issues than it solves.

 - Use the proper cache flush methods for the debugstore buffers as they
   are mapped/unmapped during runtime and not statically mapped at boot
   time like the rest of the cpu entry area.

 - Make the map layout of the cpu_entry_area consistent for 4 and 5 level
   paging and fix the KASLR vaddr_end wreckage.

 - Use PER_CPU_EXPORT for per cpu variable and while at it unbreak nvidia
   gfx drivers by dropping the GPL export. The subject line of the commit
   tells it the other way around, but I noticed that too late.

 - Fix the ASM alternative macros so they can be used in the middle of an
   inline asm block.

 - Rename the BUG_CPU_INSECURE flag to BUG_CPU_MELTDOWN so the attack
   vector is properly identified. The Spectre mitigations will come with
   their own bug bits later.

Thanks,

	tglx

------------------&gt;
Andrey Ryabinin (1):
      x86/mm: Set MODULES_END to 0xffffffffff000000

David Woodhouse (1):
      x86/alternatives: Add missing &#39;\n&#39; at end of ALTERNATIVE inline asm

Peter Zijlstra (1):
      x86/events/intel/ds: Use the proper cache flush method for mapping ds buffers

Thomas Gleixner (4):
      x86/mm: Map cpu_entry_area at the same place on 4/5 level
      x86/kaslr: Fix the vaddr_end mess
      x86/tlb: Drop the _GPL from the cpu_tlbstate export
      x86/pti: Rename BUG_CPU_INSECURE to BUG_CPU_MELTDOWN


 Documentation/x86/x86_64/mm.txt         | 18 +++++++++++-------
 arch/x86/events/intel/ds.c              | 16 ++++++++++++++++
 arch/x86/include/asm/alternative.h      |  4 ++--
 arch/x86/include/asm/cpufeatures.h      |  2 +-
 arch/x86/include/asm/pgtable_64_types.h | 14 ++++++++++----
 arch/x86/kernel/cpu/common.c            |  2 +-
 arch/x86/mm/dump_pagetables.c           |  2 +-
 arch/x86/mm/init.c                      |  2 +-
 arch/x86/mm/kaslr.c                     | 32 +++++++++-----------------------
 arch/x86/mm/pti.c                       |  6 +++---
 10 files changed, 55 insertions(+), 43 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/x86/x86_64/mm.txt b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">index ad41b3813f0a..ea91cb61a602 100644</span>
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -12,8 +12,9 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_del">-fffffe0000000000 - fffffe7fffffffff (=39 bits) LDT remap for PTI</span>
<span class="p_del">-fffffe8000000000 - fffffeffffffffff (=39 bits) cpu_entry_area mapping</span>
<span class="p_add">+				    vaddr_end for KASLR</span>
<span class="p_add">+fffffe0000000000 - fffffe7fffffffff (=39 bits) cpu_entry_area mapping</span>
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) LDT remap for PTI</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
<span class="p_chunk">@@ -37,13 +38,15 @@</span> <span class="p_context"> ffd4000000000000 - ffd5ffffffffffff (=49 bits) virtual memory map (512TB)</span>
 ... unused hole ...
 ffdf000000000000 - fffffc0000000000 (=53 bits) kasan shadow memory (8PB)
 ... unused hole ...
<span class="p_del">-fffffe8000000000 - fffffeffffffffff (=39 bits) cpu_entry_area mapping</span>
<span class="p_add">+				    vaddr_end for KASLR</span>
<span class="p_add">+fffffe0000000000 - fffffe7fffffffff (=39 bits) cpu_entry_area mapping</span>
<span class="p_add">+... unused hole ...</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
 ... unused hole ...
 ffffffff80000000 - ffffffff9fffffff (=512 MB)  kernel text mapping, from phys 0
<span class="p_del">-ffffffffa0000000 - [fixmap start]   (~1526 MB) module mapping space</span>
<span class="p_add">+ffffffffa0000000 - fffffffffeffffff (1520 MB) module mapping space</span>
 [fixmap start]   - ffffffffff5fffff kernel-internal fixmap range
 ffffffffff600000 - ffffffffff600fff (=4 kB) legacy vsyscall ABI
 ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole
<span class="p_chunk">@@ -67,9 +70,10 @@</span> <span class="p_context"> memory window (this size is arbitrary, it can be raised later if needed).</span>
 The mappings are not part of any other kernel PGD and are only available
 during EFI runtime calls.
 
<span class="p_del">-The module mapping space size changes based on the CONFIG requirements for the</span>
<span class="p_del">-following fixmap section.</span>
<span class="p_del">-</span>
 Note that if CONFIG_RANDOMIZE_MEMORY is enabled, the direct mapping of all
 physical memory, vmalloc/ioremap space and virtual memory map are randomized.
 Their order is preserved but their base will be offset early at boot time.
<span class="p_add">+</span>
<span class="p_add">+Be very careful vs. KASLR when changing anything here. The KASLR address</span>
<span class="p_add">+range must not overlap with anything except the KASAN shadow area, which is</span>
<span class="p_add">+correct as KASAN disables KASLR.</span>
<span class="p_header">diff --git a/arch/x86/events/intel/ds.c b/arch/x86/events/intel/ds.c</span>
<span class="p_header">index 8f0aace08b87..8156e47da7ba 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/ds.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/ds.c</span>
<span class="p_chunk">@@ -5,6 +5,7 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/cpu_entry_area.h&gt;
 #include &lt;asm/perf_event.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 #include &lt;asm/insn.h&gt;
 
 #include &quot;../perf_event.h&quot;
<span class="p_chunk">@@ -283,20 +284,35 @@</span> <span class="p_context"> static DEFINE_PER_CPU(void *, insn_buffer);</span>
 
 static void ds_update_cea(void *cea, void *addr, size_t size, pgprot_t prot)
 {
<span class="p_add">+	unsigned long start = (unsigned long)cea;</span>
 	phys_addr_t pa;
 	size_t msz = 0;
 
 	pa = virt_to_phys(addr);
<span class="p_add">+</span>
<span class="p_add">+	preempt_disable();</span>
 	for (; msz &lt; size; msz += PAGE_SIZE, pa += PAGE_SIZE, cea += PAGE_SIZE)
 		cea_set_pte(cea, pa, prot);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This is a cross-CPU update of the cpu_entry_area, we must shoot down</span>
<span class="p_add">+	 * all TLB entries for it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	flush_tlb_kernel_range(start, start + size);</span>
<span class="p_add">+	preempt_enable();</span>
 }
 
 static void ds_clear_cea(void *cea, size_t size)
 {
<span class="p_add">+	unsigned long start = (unsigned long)cea;</span>
 	size_t msz = 0;
 
<span class="p_add">+	preempt_disable();</span>
 	for (; msz &lt; size; msz += PAGE_SIZE, cea += PAGE_SIZE)
 		cea_set_pte(cea, 0, PAGE_NONE);
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(start, start + size);</span>
<span class="p_add">+	preempt_enable();</span>
 }
 
 static void *dsalloc_pages(size_t size, gfp_t flags, int cpu)
<span class="p_header">diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h</span>
<span class="p_header">index dbfd0854651f..cf5961ca8677 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/alternative.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/alternative.h</span>
<span class="p_chunk">@@ -140,7 +140,7 @@</span> <span class="p_context"> static inline int alternatives_text_reserved(void *start, void *end)</span>
 	&quot;.popsection\n&quot;							\
 	&quot;.pushsection .altinstr_replacement, \&quot;ax\&quot;\n&quot;			\
 	ALTINSTR_REPLACEMENT(newinstr, feature, 1)			\
<span class="p_del">-	&quot;.popsection&quot;</span>
<span class="p_add">+	&quot;.popsection\n&quot;</span>
 
 #define ALTERNATIVE_2(oldinstr, newinstr1, feature1, newinstr2, feature2)\
 	OLDINSTR_2(oldinstr, 1, 2)					\
<span class="p_chunk">@@ -151,7 +151,7 @@</span> <span class="p_context"> static inline int alternatives_text_reserved(void *start, void *end)</span>
 	&quot;.pushsection .altinstr_replacement, \&quot;ax\&quot;\n&quot;			\
 	ALTINSTR_REPLACEMENT(newinstr1, feature1, 1)			\
 	ALTINSTR_REPLACEMENT(newinstr2, feature2, 2)			\
<span class="p_del">-	&quot;.popsection&quot;</span>
<span class="p_add">+	&quot;.popsection\n&quot;</span>
 
 /*
  * Alternative instructions for different CPU types or capabilities.
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 07cdd1715705..21ac898df2d8 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -341,6 +341,6 @@</span> <span class="p_context"></span>
 #define X86_BUG_SWAPGS_FENCE		X86_BUG(11) /* SWAPGS without input dep on GS */
 #define X86_BUG_MONITOR			X86_BUG(12) /* IPI required to wake up remote CPU */
 #define X86_BUG_AMD_E400		X86_BUG(13) /* CPU is among the affected by Erratum 400 */
<span class="p_del">-#define X86_BUG_CPU_INSECURE		X86_BUG(14) /* CPU is insecure and needs kernel page table isolation */</span>
<span class="p_add">+#define X86_BUG_CPU_MELTDOWN		X86_BUG(14) /* CPU is affected by meltdown attack and needs kernel page table isolation */</span>
 
 #endif /* _ASM_X86_CPUFEATURES_H */
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64_types.h b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">index b97a539bcdee..6b8f73dcbc2c 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -75,7 +75,13 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define PGDIR_SIZE	(_AC(1, UL) &lt;&lt; PGDIR_SHIFT)
 #define PGDIR_MASK	(~(PGDIR_SIZE - 1))
 
<span class="p_del">-/* See Documentation/x86/x86_64/mm.txt for a description of the memory map. */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * See Documentation/x86/x86_64/mm.txt for a description of the memory map.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Be very careful vs. KASLR when changing anything here. The KASLR address</span>
<span class="p_add">+ * range must not overlap with anything except the KASAN shadow area, which</span>
<span class="p_add">+ * is correct as KASAN disables KASLR.</span>
<span class="p_add">+ */</span>
 #define MAXMEM			_AC(__AC(1, UL) &lt;&lt; MAX_PHYSMEM_BITS, UL)
 
 #ifdef CONFIG_X86_5LEVEL
<span class="p_chunk">@@ -88,7 +94,7 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 # define VMALLOC_SIZE_TB	_AC(32, UL)
 # define __VMALLOC_BASE		_AC(0xffffc90000000000, UL)
 # define __VMEMMAP_BASE		_AC(0xffffea0000000000, UL)
<span class="p_del">-# define LDT_PGD_ENTRY		_AC(-4, UL)</span>
<span class="p_add">+# define LDT_PGD_ENTRY		_AC(-3, UL)</span>
 # define LDT_BASE_ADDR		(LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)
 #endif
 
<span class="p_chunk">@@ -104,13 +110,13 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 
 #define MODULES_VADDR		(__START_KERNEL_map + KERNEL_IMAGE_SIZE)
 /* The module sections ends with the start of the fixmap */
<span class="p_del">-#define MODULES_END		__fix_to_virt(__end_of_fixed_addresses + 1)</span>
<span class="p_add">+#define MODULES_END		_AC(0xffffffffff000000, UL)</span>
 #define MODULES_LEN		(MODULES_END - MODULES_VADDR)
 
 #define ESPFIX_PGD_ENTRY	_AC(-2, UL)
 #define ESPFIX_BASE_ADDR	(ESPFIX_PGD_ENTRY &lt;&lt; P4D_SHIFT)
 
<span class="p_del">-#define CPU_ENTRY_AREA_PGD	_AC(-3, UL)</span>
<span class="p_add">+#define CPU_ENTRY_AREA_PGD	_AC(-4, UL)</span>
 #define CPU_ENTRY_AREA_BASE	(CPU_ENTRY_AREA_PGD &lt;&lt; P4D_SHIFT)
 
 #define EFI_VA_START		( -4 * (_AC(1, UL) &lt;&lt; 30))
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index b1be494ab4e8..2d3bd2215e5b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -900,7 +900,7 @@</span> <span class="p_context"> static void __init early_identify_cpu(struct cpuinfo_x86 *c)</span>
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
 
 	if (c-&gt;x86_vendor != X86_VENDOR_AMD)
<span class="p_del">-		setup_force_cpu_bug(X86_BUG_CPU_INSECURE);</span>
<span class="p_add">+		setup_force_cpu_bug(X86_BUG_CPU_MELTDOWN);</span>
 
 	fpu__init_system(c);
 
<span class="p_header">diff --git a/arch/x86/mm/dump_pagetables.c b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">index f56902c1f04b..2a4849e92831 100644</span>
<span class="p_header">--- a/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_chunk">@@ -61,10 +61,10 @@</span> <span class="p_context"> enum address_markers_idx {</span>
 	KASAN_SHADOW_START_NR,
 	KASAN_SHADOW_END_NR,
 #endif
<span class="p_add">+	CPU_ENTRY_AREA_NR,</span>
 #if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; !defined(CONFIG_X86_5LEVEL)
 	LDT_NR,
 #endif
<span class="p_del">-	CPU_ENTRY_AREA_NR,</span>
 #ifdef CONFIG_X86_ESPFIX64
 	ESPFIX_START_NR,
 #endif
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index 80259ad8c386..6b462a472a7b 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -870,7 +870,7 @@</span> <span class="p_context"> __visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {</span>
 	.next_asid = 1,
 	.cr4 = ~0UL,	/* fail hard if we screw up cr4 shadow initialization */
 };
<span class="p_del">-EXPORT_SYMBOL_GPL(cpu_tlbstate);</span>
<span class="p_add">+EXPORT_PER_CPU_SYMBOL(cpu_tlbstate);</span>
 
 void update_cache_mode_entry(unsigned entry, enum page_cache_mode cache)
 {
<span class="p_header">diff --git a/arch/x86/mm/kaslr.c b/arch/x86/mm/kaslr.c</span>
<span class="p_header">index 879ef930e2c2..aedebd2ebf1e 100644</span>
<span class="p_header">--- a/arch/x86/mm/kaslr.c</span>
<span class="p_header">+++ b/arch/x86/mm/kaslr.c</span>
<span class="p_chunk">@@ -34,25 +34,14 @@</span> <span class="p_context"></span>
 #define TB_SHIFT 40
 
 /*
<span class="p_del">- * Virtual address start and end range for randomization. The end changes base</span>
<span class="p_del">- * on configuration to have the highest amount of space for randomization.</span>
<span class="p_del">- * It increases the possible random position for each randomized region.</span>
<span class="p_add">+ * Virtual address start and end range for randomization.</span>
  *
<span class="p_del">- * You need to add an if/def entry if you introduce a new memory region</span>
<span class="p_del">- * compatible with KASLR. Your entry must be in logical order with memory</span>
<span class="p_del">- * layout. For example, ESPFIX is before EFI because its virtual address is</span>
<span class="p_del">- * before. You also need to add a BUILD_BUG_ON() in kernel_randomize_memory() to</span>
<span class="p_del">- * ensure that this order is correct and won&#39;t be changed.</span>
<span class="p_add">+ * The end address could depend on more configuration options to make the</span>
<span class="p_add">+ * highest amount of space for randomization available, but that&#39;s too hard</span>
<span class="p_add">+ * to keep straight and caused issues already.</span>
  */
 static const unsigned long vaddr_start = __PAGE_OFFSET_BASE;
<span class="p_del">-</span>
<span class="p_del">-#if defined(CONFIG_X86_ESPFIX64)</span>
<span class="p_del">-static const unsigned long vaddr_end = ESPFIX_BASE_ADDR;</span>
<span class="p_del">-#elif defined(CONFIG_EFI)</span>
<span class="p_del">-static const unsigned long vaddr_end = EFI_VA_END;</span>
<span class="p_del">-#else</span>
<span class="p_del">-static const unsigned long vaddr_end = __START_KERNEL_map;</span>
<span class="p_del">-#endif</span>
<span class="p_add">+static const unsigned long vaddr_end = CPU_ENTRY_AREA_BASE;</span>
 
 /* Default values */
 unsigned long page_offset_base = __PAGE_OFFSET_BASE;
<span class="p_chunk">@@ -101,15 +90,12 @@</span> <span class="p_context"> void __init kernel_randomize_memory(void)</span>
 	unsigned long remain_entropy;
 
 	/*
<span class="p_del">-	 * All these BUILD_BUG_ON checks ensures the memory layout is</span>
<span class="p_del">-	 * consistent with the vaddr_start/vaddr_end variables.</span>
<span class="p_add">+	 * These BUILD_BUG_ON checks ensure the memory layout is consistent</span>
<span class="p_add">+	 * with the vaddr_start/vaddr_end variables. These checks are very</span>
<span class="p_add">+	 * limited....</span>
 	 */
 	BUILD_BUG_ON(vaddr_start &gt;= vaddr_end);
<span class="p_del">-	BUILD_BUG_ON(IS_ENABLED(CONFIG_X86_ESPFIX64) &amp;&amp;</span>
<span class="p_del">-		     vaddr_end &gt;= EFI_VA_END);</span>
<span class="p_del">-	BUILD_BUG_ON((IS_ENABLED(CONFIG_X86_ESPFIX64) ||</span>
<span class="p_del">-		      IS_ENABLED(CONFIG_EFI)) &amp;&amp;</span>
<span class="p_del">-		     vaddr_end &gt;= __START_KERNEL_map);</span>
<span class="p_add">+	BUILD_BUG_ON(vaddr_end != CPU_ENTRY_AREA_BASE);</span>
 	BUILD_BUG_ON(vaddr_end &gt; __START_KERNEL_map);
 
 	if (!kaslr_memory_enabled())
<span class="p_header">diff --git a/arch/x86/mm/pti.c b/arch/x86/mm/pti.c</span>
<span class="p_header">index 2da28ba97508..43d4a4a29037 100644</span>
<span class="p_header">--- a/arch/x86/mm/pti.c</span>
<span class="p_header">+++ b/arch/x86/mm/pti.c</span>
<span class="p_chunk">@@ -56,13 +56,13 @@</span> <span class="p_context"></span>
 
 static void __init pti_print_if_insecure(const char *reason)
 {
<span class="p_del">-	if (boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+	if (boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))</span>
 		pr_info(&quot;%s\n&quot;, reason);
 }
 
 static void __init pti_print_if_secure(const char *reason)
 {
<span class="p_del">-	if (!boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))</span>
 		pr_info(&quot;%s\n&quot;, reason);
 }
 
<span class="p_chunk">@@ -96,7 +96,7 @@</span> <span class="p_context"> void __init pti_check_boottime_disable(void)</span>
 	}
 
 autosel:
<span class="p_del">-	if (!boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_CPU_MELTDOWN))</span>
 		return;
 enable:
 	setup_force_cpu_cap(X86_FEATURE_PTI);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



