
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[6/6] x86: Fix stray A/D bit setting into non-present PTEs - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [6/6] x86: Fix stray A/D bit setting into non-present PTEs</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2302">Dave Hansen</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 1, 2016, 12:12 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160701001218.3D316260@viggo.jf.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9209119/mbox/"
   >mbox</a>
|
   <a href="/patch/9209119/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9209119/">/patch/9209119/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	B3D636075F for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  1 Jul 2016 00:14:14 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A42762868A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  1 Jul 2016 00:14:14 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 98D5E28691; Fri,  1 Jul 2016 00:14:14 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3F4212868A
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  1 Jul 2016 00:14:13 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752706AbcGAAOF (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 30 Jun 2016 20:14:05 -0400
Received: from mga04.intel.com ([192.55.52.120]:30254 &quot;EHLO mga04.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751673AbcGAANM (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 30 Jun 2016 20:13:12 -0400
Received: from fmsmga003.fm.intel.com ([10.253.24.29])
	by fmsmga104.fm.intel.com with ESMTP; 30 Jun 2016 17:12:18 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.26,554,1459839600&quot;; d=&quot;scan&#39;208&quot;;a=&quot;727652146&quot;
Received: from viggo.jf.intel.com (HELO localhost.localdomain)
	([10.54.39.121])
	by FMSMGA003.fm.intel.com with ESMTP; 30 Jun 2016 17:12:18 -0700
Subject: [PATCH 6/6] x86: Fix stray A/D bit setting into non-present PTEs
To: linux-kernel@vger.kernel.org
Cc: x86@kernel.org, linux-mm@kvack.org, torvalds@linux-foundation.org,
	akpm@linux-foundation.org, bp@alien8.de, ak@linux.intel.com,
	mhocko@suse.com, Dave Hansen &lt;dave@sr71.net&gt;, dave.hansen@linux.intel.com
From: Dave Hansen &lt;dave@sr71.net&gt;
Date: Thu, 30 Jun 2016 17:12:18 -0700
References: &lt;20160701001209.7DA24D1C@viggo.jf.intel.com&gt;
In-Reply-To: &lt;20160701001209.7DA24D1C@viggo.jf.intel.com&gt;
Message-Id: &lt;20160701001218.3D316260@viggo.jf.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2302">Dave Hansen</a> - July 1, 2016, 12:12 a.m.</div>
<pre class="content">
<span class="from">From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>

The Intel(R) Xeon Phi(TM) Processor x200 Family (codename: Knights
Landing) has an erratum where a processor thread setting the Accessed
or Dirty bits may not do so atomically against its checks for the
Present bit.  This may cause a thread (which is about to page fault)
to set A and/or D, even though the Present bit had already been
atomically cleared.

If the PTE is used for storing a swap index or a NUMA migration index,
the A bit could be misinterpreted as part of the swap type.  The stray
bits being set cause a software-cleared PTE to be interpreted as a
swap entry.  In some cases (like when the swap index ends up being
for a non-existent swapfile), the kernel detects the stray value
and WARN()s about it, but there is no guarantee that the kernel can
always detect it.

There are basically three things we have to do to work around this
erratum:

1. Extra TLB flushes when we clear PTEs that might be affected by
   this erratum.
2. Extra pass back over the suspect PTEs after the TLBs have been
   flushed to clear stray bits.
3. Make sure to hold ptl in pte_unmap_same() (underneath
   do_swap_page()) to keep the swap code from observing the bad
   entries between #1 and #2.

Notes:
 * The little pte++ in zap_pte_range() is to ensure that &#39;pte&#39;
   points _past_ the last PTE that was cleared so that the
   whole range can be cleaned up.
 * We could do more of the new arch_*() helpers inside the
   existing TLB flush callers if we passed the old &#39;ptent&#39;
   in as an argument.  That would require a more invasive
   rework, though.
 * change_pte_range() does not need to be modified.  It fully
   writes back over the PTE after clearing it.  It also does
   this inside the ptl, so the cleared PTE potentially
   containing stray bits is never visible to anyone.
 * move_ptes() does remote TLB flushes after each PTE clear.
   This is slow, but mremap() is not as important as munmap()
   Leave it simple for now.
 * could apply A/D optimization to huge pages too
 * As far as I can tell, sites that just change PTE permissions
   are OK.  They generally do some variant of:
	ptent = ptep_get_and_clear(...);
	ptent = pte_mksomething(ptent);
	set_pte_at(mm, addr, pte, ptent);
	tlb_flush...();
   This is OK because the cleared PTE (which might contain
   the stray bits) is written over by set_pte_at() and this
   all happens under the pagetable lock.
   Examples of this:
    * madvise_free_pte_range()
    * hugetlb_change_protection()
    * clear_soft_dirty()
    * move_ptes() - new PTE will not fault so will not hit
      erratum
    * change_pte_range()
<span class="signed-off-by">
Signed-off-by: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>
---

 b/arch/arm/include/asm/tlb.h         |    1 
 b/arch/ia64/include/asm/tlb.h        |    1 
 b/arch/s390/include/asm/tlb.h        |    1 
 b/arch/sh/include/asm/tlb.h          |    1 
 b/arch/um/include/asm/tlb.h          |    2 
 b/arch/x86/include/asm/cpufeatures.h |    1 
 b/arch/x86/include/asm/pgtable.h     |   32 +++++++++
 b/arch/x86/include/asm/tlbflush.h    |   37 +++++++++++
 b/arch/x86/kernel/cpu/intel.c        |  113 +++++++++++++++++++++++++++++++++++
 b/include/asm-generic/tlb.h          |    4 +
 b/include/linux/mm.h                 |   17 +++++
 b/mm/memory.c                        |   12 ++-
 b/mm/mremap.c                        |    9 ++
 b/mm/rmap.c                          |    4 +
 b/mm/vmalloc.c                       |    1 
 15 files changed, 231 insertions(+), 5 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=55071">Nadav Amit</a> - July 1, 2016, 1:50 a.m.</div>
<pre class="content">
Dave Hansen &lt;dave@sr71.net&gt; wrote:
<span class="quote">
&gt; +pte_t ptep_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt; +		       pte_t *ptep)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	pte_t pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pte = ptep_get_and_clear(mm, address, ptep);</span>
<span class="quote">&gt; +	if (pte_accessible(mm, pte)) {</span>
<span class="quote">&gt; +		flush_tlb_page(vma, address);</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * Ensure that the compiler orders our set_pte()</span>
<span class="quote">&gt; +		 * after the flush_tlb_page() no matter what.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		barrier();</span>

I don’t think such a barrier (after remote TLB flush) is needed.
Eventually, if a remote flush takes place, you get csd_lock_wait() to be
called, and then smp_rmb() is called (which is essentially a barrier()
call on x86).

Regards,
Nadav
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2302">Dave Hansen</a> - July 1, 2016, 1:54 a.m.</div>
<pre class="content">
On 06/30/2016 06:50 PM, Nadav Amit wrote:
<span class="quote">&gt; Dave Hansen &lt;dave@sr71.net&gt; wrote:</span>
<span class="quote">&gt;&gt; +pte_t ptep_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;&gt; +		       pte_t *ptep)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt;&gt; +	pte_t pte;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	pte = ptep_get_and_clear(mm, address, ptep);</span>
<span class="quote">&gt;&gt; +	if (pte_accessible(mm, pte)) {</span>
<span class="quote">&gt;&gt; +		flush_tlb_page(vma, address);</span>
<span class="quote">&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * Ensure that the compiler orders our set_pte()</span>
<span class="quote">&gt;&gt; +		 * after the flush_tlb_page() no matter what.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt;&gt; +		barrier();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don’t think such a barrier (after remote TLB flush) is needed.</span>
<span class="quote">&gt; Eventually, if a remote flush takes place, you get csd_lock_wait() to be</span>
<span class="quote">&gt; called, and then smp_rmb() is called (which is essentially a barrier()</span>
<span class="quote">&gt; call on x86).</span>

Andi really wanted to make sure this got in here.  He said there was a
bug that bit him really badly once where a function got reordered.
Granted, a call _should_ be sufficient to keep the compiler from
reordering things, but this makes double sure.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - July 1, 2016, 2:55 a.m.</div>
<pre class="content">
On Thu, Jun 30, 2016 at 5:12 PM, Dave Hansen &lt;dave@sr71.net&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The Intel(R) Xeon Phi(TM) Processor x200 Family (codename: Knights</span>
<span class="quote">&gt; Landing) has an erratum where a processor thread setting the Accessed</span>
<span class="quote">&gt; or Dirty bits may not do so atomically against its checks for the</span>
<span class="quote">&gt; Present bit.  This may cause a thread (which is about to page fault)</span>
<span class="quote">&gt; to set A and/or D, even though the Present bit had already been</span>
<span class="quote">&gt; atomically cleared.</span>

So I don&#39;t think your approach is wrong, but I suspect this is
overkill, and what we should instead just do is to not use the A/D
bits at all in the swap representation.

The swap-entry representation was a bit tight on 32-bit page table
entries, but in 64-bit ones, I think we have tons of bits, don&#39;t we?
So we could decide just to not use those two bits on x86.

It&#39;s not like anybody will ever care about 32-bit page tables on
Knights Landing anyway.

So rather than add this kind of complexity and worry, how about just
simplifying the problem?

Or was there some discussion or implication I missed?

             Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=188">Brian Gerst</a> - July 1, 2016, 3:06 a.m.</div>
<pre class="content">
On Thu, Jun 30, 2016 at 10:55 PM, Linus Torvalds
&lt;torvalds@linux-foundation.org&gt; wrote:
<span class="quote">&gt; On Thu, Jun 30, 2016 at 5:12 PM, Dave Hansen &lt;dave@sr71.net&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The Intel(R) Xeon Phi(TM) Processor x200 Family (codename: Knights</span>
<span class="quote">&gt;&gt; Landing) has an erratum where a processor thread setting the Accessed</span>
<span class="quote">&gt;&gt; or Dirty bits may not do so atomically against its checks for the</span>
<span class="quote">&gt;&gt; Present bit.  This may cause a thread (which is about to page fault)</span>
<span class="quote">&gt;&gt; to set A and/or D, even though the Present bit had already been</span>
<span class="quote">&gt;&gt; atomically cleared.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So I don&#39;t think your approach is wrong, but I suspect this is</span>
<span class="quote">&gt; overkill, and what we should instead just do is to not use the A/D</span>
<span class="quote">&gt; bits at all in the swap representation.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The swap-entry representation was a bit tight on 32-bit page table</span>
<span class="quote">&gt; entries, but in 64-bit ones, I think we have tons of bits, don&#39;t we?</span>
<span class="quote">&gt; So we could decide just to not use those two bits on x86.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It&#39;s not like anybody will ever care about 32-bit page tables on</span>
<span class="quote">&gt; Knights Landing anyway.</span>

Could this affect a 32-bit guest VM?

--
Brian Gerst
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2302">Dave Hansen</a> - July 1, 2016, 4:39 a.m.</div>
<pre class="content">
On 06/30/2016 07:55 PM, Linus Torvalds wrote:
<span class="quote">&gt; On Thu, Jun 30, 2016 at 5:12 PM, Dave Hansen &lt;dave@sr71.net&gt; wrote:</span>
<span class="quote">&gt;&gt; From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt; The Intel(R) Xeon Phi(TM) Processor x200 Family (codename: Knights</span>
<span class="quote">&gt;&gt; Landing) has an erratum where a processor thread setting the Accessed</span>
<span class="quote">&gt;&gt; or Dirty bits may not do so atomically against its checks for the</span>
<span class="quote">&gt;&gt; Present bit.  This may cause a thread (which is about to page fault)</span>
<span class="quote">&gt;&gt; to set A and/or D, even though the Present bit had already been</span>
<span class="quote">&gt;&gt; atomically cleared.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I don&#39;t think your approach is wrong, but I suspect this is</span>
<span class="quote">&gt; overkill, and what we should instead just do is to not use the A/D</span>
<span class="quote">&gt; bits at all in the swap representation.</span>

We actually don&#39;t even use Dirty today.  It&#39;s (implicitly) used to
determine pte_none(), but it ends up being masked out for the
swp_offset/type() calculations entirely, much to my surprise.

I think what you suggest will work if we don&#39;t consider A/D in
pte_none().  I think there are a bunch of code path where assume that
!pte_present() &amp;&amp; !pte_none() means swap.
<span class="quote">
&gt; The swap-entry representation was a bit tight on 32-bit page table</span>
<span class="quote">&gt; entries, but in 64-bit ones, I think we have tons of bits, don&#39;t we?</span>
<span class="quote">&gt; So we could decide just to not use those two bits on x86.</span>

Yeah, we&#39;ve definitely got space.  I&#39;ll go poke around and make sure
that this works everywhere.  I agree that throwing 32-bit non-PAE under
the bus is definitely worth it here.  Nobody will care about that in a
million years.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - July 1, 2016, 5:43 a.m.</div>
<pre class="content">
On Thu, Jun 30, 2016 at 9:39 PM, Dave Hansen &lt;dave@sr71.net&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; I think what you suggest will work if we don&#39;t consider A/D in</span>
<span class="quote">&gt; pte_none().  I think there are a bunch of code path where assume that</span>
<span class="quote">&gt; !pte_present() &amp;&amp; !pte_none() means swap.</span>

Yeah, we would need to change pte_none() to mask off D/A, but I think
that might be the only real change needed (other than making sure that
we don&#39;t use the bits in the swap entries, I didn&#39;t look at that part
at all)

           Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=156">Eric W. Biederman</a> - July 1, 2016, 2:25 p.m.</div>
<pre class="content">
Linus Torvalds &lt;torvalds@linux-foundation.org&gt; writes:
<span class="quote">
&gt; On Thu, Jun 30, 2016 at 9:39 PM, Dave Hansen &lt;dave@sr71.net&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think what you suggest will work if we don&#39;t consider A/D in</span>
<span class="quote">&gt;&gt; pte_none().  I think there are a bunch of code path where assume that</span>
<span class="quote">&gt;&gt; !pte_present() &amp;&amp; !pte_none() means swap.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Yeah, we would need to change pte_none() to mask off D/A, but I think</span>
<span class="quote">&gt; that might be the only real change needed (other than making sure that</span>
<span class="quote">&gt; we don&#39;t use the bits in the swap entries, I didn&#39;t look at that part</span>
<span class="quote">&gt; at all)</span>

It looks like __pte_to_swp_entry also needs to be changed to mask out
those bits when the swap code reads pte entries.  For all of the same
reasons as pte_none.

Eric
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2302">Dave Hansen</a> - July 1, 2016, 3:51 p.m.</div>
<pre class="content">
On 07/01/2016 07:25 AM, Eric W. Biederman wrote:
<span class="quote">&gt; Linus Torvalds &lt;torvalds@linux-foundation.org&gt; writes:</span>
<span class="quote">&gt;&gt; &gt; On Thu, Jun 30, 2016 at 9:39 PM, Dave Hansen &lt;dave@sr71.net&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; I think what you suggest will work if we don&#39;t consider A/D in</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; pte_none().  I think there are a bunch of code path where assume that</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; !pte_present() &amp;&amp; !pte_none() means swap.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Yeah, we would need to change pte_none() to mask off D/A, but I think</span>
<span class="quote">&gt;&gt; &gt; that might be the only real change needed (other than making sure that</span>
<span class="quote">&gt;&gt; &gt; we don&#39;t use the bits in the swap entries, I didn&#39;t look at that part</span>
<span class="quote">&gt;&gt; &gt; at all)</span>
<span class="quote">&gt; It looks like __pte_to_swp_entry also needs to be changed to mask out</span>
<span class="quote">&gt; those bits when the swap code reads pte entries.  For all of the same</span>
<span class="quote">&gt; reasons as pte_none.</span>

I guess that would be nice, but isn&#39;t it redundant?

static inline swp_entry_t pte_to_swp_entry(pte_t pte)
{
	...
        arch_entry = __pte_to_swp_entry(pte);
	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));
}

As long as __swp_type() and __swp_offset() don&#39;t let A/D through, then
we should be OK.  This site is the only call to __pte_to_swp_entry()
that I can find in the entire codebase.

Or am I missing something?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - July 1, 2016, 4:07 p.m.</div>
<pre class="content">
On Thu, Jun 30, 2016 at 9:39 PM, Dave Hansen &lt;dave@sr71.net&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; I think what you suggest will work if we don&#39;t consider A/D in</span>
<span class="quote">&gt; pte_none().  I think there are a bunch of code path where assume that</span>
<span class="quote">&gt; !pte_present() &amp;&amp; !pte_none() means swap.</span>

Hmm.

Thinking about it some more, I still think it&#39;s a good idea to avoid
A/D bits in the swap entries and in pte_none() and friends, and it
might simplify some of this all.

But I also started worrying about us just losing sight of the dirty
bit in particular. It&#39;s not enough that we ignore the dirty bit - we&#39;d
still want to make sure that the underlying backing page gets marked
dirty, even if the CPU is buggy and ends doing it &quot;delayed&quot; after
we&#39;ve already unmapped the page.

So I get this feeling that we may need a fair chunk of your patch-series anyway.

                Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2302">Dave Hansen</a> - July 1, 2016, 4:14 p.m.</div>
<pre class="content">
On 07/01/2016 09:07 AM, Linus Torvalds wrote:
<span class="quote">&gt; But I also started worrying about us just losing sight of the dirty</span>
<span class="quote">&gt; bit in particular. It&#39;s not enough that we ignore the dirty bit - we&#39;d</span>
<span class="quote">&gt; still want to make sure that the underlying backing page gets marked</span>
<span class="quote">&gt; dirty, even if the CPU is buggy and ends doing it &quot;delayed&quot; after</span>
<span class="quote">&gt; we&#39;ve already unmapped the page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I get this feeling that we may need a fair chunk of your</span>
<span class="quote">&gt; patch-series anyway.</span>

As I understand it, the erratum only affects a thread which is about to
page fault.  The write associated with the dirty bit being set never
actually gets executed.  So, the bit really *is* stray and isn&#39;t
something we need to preserve.

Otherwise, we&#39;d be really screwed because we couldn&#39;t ever simply clear it.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - July 1, 2016, 4:25 p.m.</div>
<pre class="content">
On Fri, Jul 1, 2016 at 9:14 AM, Dave Hansen &lt;dave@sr71.net&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; As I understand it, the erratum only affects a thread which is about to</span>
<span class="quote">&gt; page fault.  The write associated with the dirty bit being set never</span>
<span class="quote">&gt; actually gets executed.  So, the bit really *is* stray and isn&#39;t</span>
<span class="quote">&gt; something we need to preserve.</span>

Ok, good.
<span class="quote">
&gt; Otherwise, we&#39;d be really screwed because we couldn&#39;t ever simply clear it.</span>

Oh, we could do the whole &quot;clear the pte, then flush the tlb, then go
back and clear the stale dirty bits and move them into the backing
page&quot;.

I was afraid we might have to do something like that.

            Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=156">Eric W. Biederman</a> - July 1, 2016, 6:12 p.m.</div>
<pre class="content">
Dave Hansen &lt;dave@sr71.net&gt; writes:
<span class="quote">
&gt; On 07/01/2016 07:25 AM, Eric W. Biederman wrote:</span>
<span class="quote">&gt;&gt; Linus Torvalds &lt;torvalds@linux-foundation.org&gt; writes:</span>
<span class="quote">&gt;&gt;&gt; &gt; On Thu, Jun 30, 2016 at 9:39 PM, Dave Hansen &lt;dave@sr71.net&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt; I think what you suggest will work if we don&#39;t consider A/D in</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt; pte_none().  I think there are a bunch of code path where assume that</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt; !pte_present() &amp;&amp; !pte_none() means swap.</span>
<span class="quote">&gt;&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt;&gt; &gt; Yeah, we would need to change pte_none() to mask off D/A, but I think</span>
<span class="quote">&gt;&gt;&gt; &gt; that might be the only real change needed (other than making sure that</span>
<span class="quote">&gt;&gt;&gt; &gt; we don&#39;t use the bits in the swap entries, I didn&#39;t look at that part</span>
<span class="quote">&gt;&gt;&gt; &gt; at all)</span>
<span class="quote">&gt;&gt; It looks like __pte_to_swp_entry also needs to be changed to mask out</span>
<span class="quote">&gt;&gt; those bits when the swap code reads pte entries.  For all of the same</span>
<span class="quote">&gt;&gt; reasons as pte_none.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I guess that would be nice, but isn&#39;t it redundant?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; static inline swp_entry_t pte_to_swp_entry(pte_t pte)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt; 	...</span>
<span class="quote">&gt;         arch_entry = __pte_to_swp_entry(pte);</span>
<span class="quote">&gt; 	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; As long as __swp_type() and __swp_offset() don&#39;t let A/D through, then</span>
<span class="quote">&gt; we should be OK.  This site is the only call to __pte_to_swp_entry()</span>
<span class="quote">&gt; that I can find in the entire codebase.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Or am I missing something?</span>

Given that __pte_to_swp_entry on x86_64 is just __pte_val or pte.pte it
does no filtering.  Similarly __swp_type(arch_entry) is a &gt;&gt; and
swp_entry(type, ...) is a &lt;&lt; of what appears to be same amount
for the swap type.

So any corruption in the upper bits of the pte will be preserved as a
swap type.

In fact I strongly suspect that the compiler can optimize out all of the
work done by &quot;swp_entry(__swp_type(arch_entry), _swp_offset(arch_entry))&quot;.

Eric
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2302">Dave Hansen</a> - July 3, 2016, 5:10 p.m.</div>
<pre class="content">
On 06/30/2016 08:06 PM, Brian Gerst wrote:
<span class="quote">&gt;&gt; &gt; It&#39;s not like anybody will ever care about 32-bit page tables on</span>
<span class="quote">&gt;&gt; &gt; Knights Landing anyway.</span>
<span class="quote">&gt; Could this affect a 32-bit guest VM?</span>

This isn&#39;t about 32-bit *mode*.  It&#39;s about using the the 32-bit 2-level
_paging_ mode that supports only 4GB virtual and 4GB physical addresses.
 That mode also doesn&#39;t support the No-eXecute (NX) bit, which basically
everyone needs today for its security benefits.

Even the little Quark CPU supports PAE (64-bit page tables).
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff -puN arch/arm/include/asm/tlb.h~knl-leak-60-actual-fix arch/arm/include/asm/tlb.h</span>
<span class="p_header">--- a/arch/arm/include/asm/tlb.h~knl-leak-60-actual-fix	2016-06-30 17:10:43.455287496 -0700</span>
<span class="p_header">+++ b/arch/arm/include/asm/tlb.h	2016-06-30 17:10:43.483288766 -0700</span>
<span class="p_chunk">@@ -264,6 +264,7 @@</span> <span class="p_context"> tlb_remove_pmd_tlb_entry(struct mmu_gath</span>
 #define pud_free_tlb(tlb, pudp, addr)	pud_free((tlb)-&gt;mm, pudp)
 
 #define tlb_migrate_finish(mm)		do { } while (0)
<span class="p_add">+#define __tlb_cleanup_pte_range(tlb, start_ptep, end_ptep) do {} while (0)</span>
 
 #endif /* CONFIG_MMU */
 #endif
<span class="p_header">diff -puN arch/ia64/include/asm/tlb.h~knl-leak-60-actual-fix arch/ia64/include/asm/tlb.h</span>
<span class="p_header">--- a/arch/ia64/include/asm/tlb.h~knl-leak-60-actual-fix	2016-06-30 17:10:43.457287587 -0700</span>
<span class="p_header">+++ b/arch/ia64/include/asm/tlb.h	2016-06-30 17:10:43.484288812 -0700</span>
<span class="p_chunk">@@ -254,6 +254,7 @@</span> <span class="p_context"> __tlb_remove_tlb_entry (struct mmu_gathe</span>
 }
 
 #define tlb_migrate_finish(mm)	platform_tlb_migrate_finish(mm)
<span class="p_add">+#define __tlb_cleanup_pte_range(tlb, start_ptep, end_ptep) do {} while (0)</span>
 
 #define tlb_start_vma(tlb, vma)			do { } while (0)
 #define tlb_end_vma(tlb, vma)			do { } while (0)
<span class="p_header">diff -puN arch/s390/include/asm/tlb.h~knl-leak-60-actual-fix arch/s390/include/asm/tlb.h</span>
<span class="p_header">--- a/arch/s390/include/asm/tlb.h~knl-leak-60-actual-fix	2016-06-30 17:10:43.460287723 -0700</span>
<span class="p_header">+++ b/arch/s390/include/asm/tlb.h	2016-06-30 17:10:43.484288812 -0700</span>
<span class="p_chunk">@@ -146,5 +146,6 @@</span> <span class="p_context"> static inline void pud_free_tlb(struct m</span>
 #define tlb_remove_tlb_entry(tlb, ptep, addr)	do { } while (0)
 #define tlb_remove_pmd_tlb_entry(tlb, pmdp, addr)	do { } while (0)
 #define tlb_migrate_finish(mm)			do { } while (0)
<span class="p_add">+#define __tlb_cleanup_pte_range(tlb, start_ptep, end_ptep) do {} while (0)</span>
 
 #endif /* _S390_TLB_H */
<span class="p_header">diff -puN arch/sh/include/asm/tlb.h~knl-leak-60-actual-fix arch/sh/include/asm/tlb.h</span>
<span class="p_header">--- a/arch/sh/include/asm/tlb.h~knl-leak-60-actual-fix	2016-06-30 17:10:43.461287768 -0700</span>
<span class="p_header">+++ b/arch/sh/include/asm/tlb.h	2016-06-30 17:10:43.484288812 -0700</span>
<span class="p_chunk">@@ -116,6 +116,7 @@</span> <span class="p_context"> static inline void tlb_remove_page(struc</span>
 #define pud_free_tlb(tlb, pudp, addr)	pud_free((tlb)-&gt;mm, pudp)
 
 #define tlb_migrate_finish(mm)		do { } while (0)
<span class="p_add">+#define __tlb_cleanup_pte_range(tlb, start_ptep, end_ptep) do {} while (0)</span>
 
 #if defined(CONFIG_CPU_SH4) || defined(CONFIG_SUPERH64)
 extern void tlb_wire_entry(struct vm_area_struct *, unsigned long, pte_t);
<span class="p_header">diff -puN arch/um/include/asm/tlb.h~knl-leak-60-actual-fix arch/um/include/asm/tlb.h</span>
<span class="p_header">--- a/arch/um/include/asm/tlb.h~knl-leak-60-actual-fix	2016-06-30 17:10:43.463287859 -0700</span>
<span class="p_header">+++ b/arch/um/include/asm/tlb.h	2016-06-30 17:10:43.485288857 -0700</span>
<span class="p_chunk">@@ -133,4 +133,6 @@</span> <span class="p_context"> static inline void tlb_remove_page(struc</span>
 
 #define tlb_migrate_finish(mm) do {} while (0)
 
<span class="p_add">+#define __tlb_cleanup_pte_range(tlb, start_ptep, end_ptep) do {} while (0)</span>
<span class="p_add">+</span>
 #endif
<span class="p_header">diff -puN arch/x86/include/asm/cpufeatures.h~knl-leak-60-actual-fix arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h~knl-leak-60-actual-fix	2016-06-30 17:10:43.464287904 -0700</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h	2016-06-30 17:10:43.485288857 -0700</span>
<span class="p_chunk">@@ -310,5 +310,6 @@</span> <span class="p_context"></span>
 #endif
 #define X86_BUG_NULL_SEG	X86_BUG(10) /* Nulling a selector preserves the base */
 #define X86_BUG_SWAPGS_FENCE	X86_BUG(11) /* SWAPGS without input dep on GS */
<span class="p_add">+#define X86_BUG_PTE_LEAK	X86_BUG(12) /* PTE may leak A/D bits after clear */</span>
 
 #endif /* _ASM_X86_CPUFEATURES_H */
<span class="p_header">diff -puN arch/x86/include/asm/pgtable.h~knl-leak-60-actual-fix arch/x86/include/asm/pgtable.h</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h~knl-leak-60-actual-fix	2016-06-30 17:10:43.466287995 -0700</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h	2016-06-30 17:10:43.486288902 -0700</span>
<span class="p_chunk">@@ -794,6 +794,12 @@</span> <span class="p_context"> extern int ptep_test_and_clear_young(str</span>
 extern int ptep_clear_flush_young(struct vm_area_struct *vma,
 				  unsigned long address, pte_t *ptep);
 
<span class="p_add">+#ifdef CONFIG_CPU_SUP_INTEL</span>
<span class="p_add">+#define __HAVE_ARCH_PTEP_CLEAR_FLUSH</span>
<span class="p_add">+extern pte_t ptep_clear_flush(struct vm_area_struct *vma,</span>
<span class="p_add">+			      unsigned long address, pte_t *ptep);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #define __HAVE_ARCH_PTEP_GET_AND_CLEAR
 static inline pte_t ptep_get_and_clear(struct mm_struct *mm, unsigned long addr,
 				       pte_t *ptep)
<span class="p_chunk">@@ -956,6 +962,32 @@</span> <span class="p_context"> static inline u16 pte_flags_pkey(unsigne</span>
 #endif
 }
 
<span class="p_add">+#ifdef CONFIG_CPU_SUP_INTEL</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These are all specific to working around an Intel-specific</span>
<span class="p_add">+ * bug and the out-of-line code is all defined in intel.c.</span>
<span class="p_add">+ */</span>
<span class="p_add">+extern void fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="p_add">+			 pte_t *ptep);</span>
<span class="p_add">+#define ARCH_HAS_FIX_PTE_LEAK 1</span>
<span class="p_add">+static inline void arch_fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="p_add">+				     pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (static_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="p_add">+		fix_pte_leak(mm, addr, ptep);</span>
<span class="p_add">+}</span>
<span class="p_add">+#define ARCH_HAS_NEEDS_SWAP_PTL 1</span>
<span class="p_add">+static inline bool arch_needs_swap_ptl(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return static_cpu_has_bug(X86_BUG_PTE_LEAK);</span>
<span class="p_add">+}</span>
<span class="p_add">+#define ARCH_DISABLE_DEFERRED_FLUSH 1</span>
<span class="p_add">+static inline bool arch_disable_deferred_flush(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return static_cpu_has_bug(X86_BUG_PTE_LEAK);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_CPU_SUP_INTEL */</span>
<span class="p_add">+</span>
 #include &lt;asm-generic/pgtable.h&gt;
 #endif	/* __ASSEMBLY__ */
 
<span class="p_header">diff -puN arch/x86/include/asm/tlbflush.h~knl-leak-60-actual-fix arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h~knl-leak-60-actual-fix	2016-06-30 17:10:43.468288086 -0700</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h	2016-06-30 17:10:43.486288902 -0700</span>
<span class="p_chunk">@@ -324,4 +324,41 @@</span> <span class="p_context"> static inline void reset_lazy_tlbstate(v</span>
 	native_flush_tlb_others(mask, mm, start, end)
 #endif
 
<span class="p_add">+static inline bool intel_detect_leaked_pte(pte_t ptent)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Hardware sets the dirty bit only on writable ptes and</span>
<span class="p_add">+	 * only on ptes where dirty is unset.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (pte_write(ptent) &amp;&amp; !pte_dirty(ptent))</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The leak occurs when the hardware sees a unset A/D bit</span>
<span class="p_add">+	 * and tries to set it.  If the PTE has both A/D bits</span>
<span class="p_add">+	 * set, then the hardware will not be going to try to set</span>
<span class="p_add">+	 * it and we have no chance for a leak.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!pte_young(ptent))</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_CPU_SUP_INTEL</span>
<span class="p_add">+void intel_cleanup_pte_range(struct mmu_gather *tlb, pte_t *start_ptep,</span>
<span class="p_add">+		pte_t *end_ptep);</span>
<span class="p_add">+#define __tlb_cleanup_pte_range(tlb, start_ptep, end_ptep) do {		\</span>
<span class="p_add">+	if (static_cpu_has_bug(X86_BUG_PTE_LEAK))			\</span>
<span class="p_add">+		intel_cleanup_pte_range(tlb, start_ptep, end_ptep);	\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#define ARCH_FLUSH_CLEARED_PTE 1</span>
<span class="p_add">+#define arch_flush_cleared_pte(tlb, ptent) do {				\</span>
<span class="p_add">+	if (static_cpu_has_bug(X86_BUG_PTE_LEAK) &amp;&amp;			\</span>
<span class="p_add">+	    intel_detect_leaked_pte(ptent)) {				\</span>
<span class="p_add">+		tlb-&gt;saw_unset_a_or_d = 1;				\</span>
<span class="p_add">+		tlb-&gt;force_batch_flush = 1;				\</span>
<span class="p_add">+	}								\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+#endif /* CONFIG_CPU_SUP_INTEL */</span>
<span class="p_add">+</span>
 #endif /* _ASM_X86_TLBFLUSH_H */
<span class="p_header">diff -puN arch/x86/kernel/cpu/intel.c~knl-leak-60-actual-fix arch/x86/kernel/cpu/intel.c</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/intel.c~knl-leak-60-actual-fix	2016-06-30 17:10:43.469288131 -0700</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/intel.c	2016-06-30 17:10:43.487288948 -0700</span>
<span class="p_chunk">@@ -9,10 +9,14 @@</span> <span class="p_context"></span>
 #include &lt;linux/uaccess.h&gt;
 
 #include &lt;asm/cpufeature.h&gt;
<span class="p_add">+#include &lt;asm/intel-family.h&gt;</span>
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/msr.h&gt;
 #include &lt;asm/bugs.h&gt;
 #include &lt;asm/cpu.h&gt;
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;trace/events/tlb.h&gt;</span>
 
 #ifdef CONFIG_X86_64
 #include &lt;linux/topology.h&gt;
<span class="p_chunk">@@ -181,6 +185,11 @@</span> <span class="p_context"> static void early_init_intel(struct cpui</span>
 		}
 	}
 
<span class="p_add">+	if (c-&gt;x86_model == INTEL_FAM6_XEON_PHI_KNL) {</span>
<span class="p_add">+		pr_info_once(&quot;x86/intel: Enabling PTE leaking workaround\n&quot;);</span>
<span class="p_add">+		set_cpu_bug(c, X86_BUG_PTE_LEAK);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/*
 	 * Intel Quark Core DevMan_001.pdf section 6.4.11
 	 * &quot;The operating system also is required to invalidate (i.e., flush)
<span class="p_chunk">@@ -820,3 +829,107 @@</span> <span class="p_context"> static const struct cpu_dev intel_cpu_de</span>
 
 cpu_dev_register(intel_cpu_dev);
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Workaround for KNL issue:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * A thread that is going to page fault due to P=0, may still</span>
<span class="p_add">+ * non atomically set A or D bits, which could corrupt swap entries.</span>
<span class="p_add">+ * Always flush the other CPUs and clear the PTE again to avoid</span>
<span class="p_add">+ * this leakage. We are excluded using the pagetable lock.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This only needs to be called on processors that might &quot;leak&quot;</span>
<span class="p_add">+ * A or D bits and have X86_BUG_PTE_LEAK set.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void fix_pte_leak(struct mm_struct *mm, unsigned long addr, pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (cpumask_any_but(mm_cpumask(mm), smp_processor_id()) &lt; nr_cpu_ids) {</span>
<span class="p_add">+		flush_tlb_others(mm_cpumask(mm), mm, addr,</span>
<span class="p_add">+				 addr + PAGE_SIZE);</span>
<span class="p_add">+		set_pte(ptep, __pte(0));</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * We batched a bunch of PTE clears up.  After the TLB has been</span>
<span class="p_add">+ * flushed for the whole batch, we might have had some leaked</span>
<span class="p_add">+ * A or D bits and need to clear them here.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This should be called with the page table lock still held.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This only needs to be called on processors that might &quot;leak&quot;</span>
<span class="p_add">+ * A or D bits and have X86_BUG_PTE_LEAK set.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void intel_cleanup_pte_range(struct mmu_gather *tlb, pte_t *start_ptep,</span>
<span class="p_add">+		pte_t *end_ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * fullmm means nobody will care that we have leaked bits</span>
<span class="p_add">+	 * laying around.  We also skip TLB flushes when doing</span>
<span class="p_add">+	 * fullmm teardown, so the additional pte clearing would</span>
<span class="p_add">+	 * not help the issue.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (tlb-&gt;fullmm)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If none of the PTEs hit inside intel_detect_leaked_pte(),</span>
<span class="p_add">+	 * then we have nothing that might have been leaked and</span>
<span class="p_add">+	 * nothing to clear.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!tlb-&gt;saw_unset_a_or_d)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Contexts calling us with NULL ptep&#39;s do not have any</span>
<span class="p_add">+	 * PTEs for us to go clear because they did not do any</span>
<span class="p_add">+	 * actual TLB invalidation.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!start_ptep || !end_ptep)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Mark that the workaround is no longer needed for</span>
<span class="p_add">+	 * this batch.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	tlb-&gt;saw_unset_a_or_d = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Ensure that the compiler orders our set_pte()</span>
<span class="p_add">+	 * after the preceding TLB flush no matter what.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	barrier();</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Re-clear out all the PTEs into which the hardware</span>
<span class="p_add">+	 * may have leaked Accessed or Dirty bits.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (pte = start_ptep; pte &lt; end_ptep; pte++)</span>
<span class="p_add">+		set_pte(pte, __pte(0));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Kinda weird to define this in here, but we only use it for</span>
<span class="p_add">+ * an Intel-specific issue.  This will get used on all</span>
<span class="p_add">+ * processors (even non-Intel) if CONFIG_CPU_SUP_INTEL=y.</span>
<span class="p_add">+ */</span>
<span class="p_add">+pte_t ptep_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_add">+		       pte_t *ptep)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	pte_t pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = ptep_get_and_clear(mm, address, ptep);</span>
<span class="p_add">+	if (pte_accessible(mm, pte)) {</span>
<span class="p_add">+		flush_tlb_page(vma, address);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Ensure that the compiler orders our set_pte()</span>
<span class="p_add">+		 * after the flush_tlb_page() no matter what.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		barrier();</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_PTE_LEAK))</span>
<span class="p_add">+			set_pte(ptep, __pte(0));</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_header">diff -puN include/asm-generic/tlb.h~knl-leak-60-actual-fix include/asm-generic/tlb.h</span>
<span class="p_header">--- a/include/asm-generic/tlb.h~knl-leak-60-actual-fix	2016-06-30 17:10:43.471288222 -0700</span>
<span class="p_header">+++ b/include/asm-generic/tlb.h	2016-06-30 17:10:43.489289039 -0700</span>
<span class="p_chunk">@@ -226,4 +226,8 @@</span> <span class="p_context"> static inline void __tlb_reset_range(str</span>
 
 #define tlb_migrate_finish(mm) do {} while (0)
 
<span class="p_add">+#ifndef __tlb_cleanup_pte_range</span>
<span class="p_add">+#define __tlb_cleanup_pte_range(tlb, start_ptep, end_ptep) do {} while (0)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif /* _ASM_GENERIC__TLB_H */
<span class="p_header">diff -puN include/linux/mm.h~knl-leak-60-actual-fix include/linux/mm.h</span>
<span class="p_header">--- a/include/linux/mm.h~knl-leak-60-actual-fix	2016-06-30 17:10:43.472288267 -0700</span>
<span class="p_header">+++ b/include/linux/mm.h	2016-06-30 17:10:43.492289175 -0700</span>
<span class="p_chunk">@@ -2404,6 +2404,23 @@</span> <span class="p_context"> static inline bool debug_guardpage_enabl</span>
 static inline bool page_is_guard(struct page *page) { return false; }
 #endif /* CONFIG_DEBUG_PAGEALLOC */
 
<span class="p_add">+#ifndef ARCH_HAS_NEEDS_SWAP_PTL</span>
<span class="p_add">+static inline bool arch_needs_swap_ptl(void) { return false; }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ARCH_HAS_FIX_PTE_LEAK</span>
<span class="p_add">+static inline void arch_fix_pte_leak(struct mm_struct *mm, unsigned long addr,</span>
<span class="p_add">+				     pte_t *ptep) {}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ARCH_DISABLE_DEFERRED_FLUSH</span>
<span class="p_add">+static inline bool arch_disable_deferred_flush(void) { return false; }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ARCH_FLUSH_CLEARED_PTE</span>
<span class="p_add">+static inline void arch_flush_cleared_pte(struct mmu_gather *tlb, pte_t ptent) {}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #if MAX_NUMNODES &gt; 1
 void __init setup_nr_node_ids(void);
 #else
<span class="p_header">diff -puN mm/memory.c~knl-leak-60-actual-fix mm/memory.c</span>
<span class="p_header">--- a/mm/memory.c~knl-leak-60-actual-fix	2016-06-30 17:10:43.474288358 -0700</span>
<span class="p_header">+++ b/mm/memory.c	2016-06-30 17:10:43.496289356 -0700</span>
<span class="p_chunk">@@ -1141,6 +1141,7 @@</span> <span class="p_context"> again:</span>
 			ptent = ptep_get_and_clear_full(mm, addr, pte,
 							tlb-&gt;fullmm);
 			tlb_remove_tlb_entry(tlb, pte, addr);
<span class="p_add">+			arch_flush_cleared_pte(tlb, ptent);</span>
 			if (unlikely(!page))
 				continue;
 
<span class="p_chunk">@@ -1166,6 +1167,7 @@</span> <span class="p_context"> again:</span>
 			if (unlikely(!__tlb_remove_page(tlb, page))) {
 				tlb-&gt;force_batch_flush = 1;
 				addr += PAGE_SIZE;
<span class="p_add">+				pte++;</span>
 				break;
 			}
 			continue;
<span class="p_chunk">@@ -1192,8 +1194,11 @@</span> <span class="p_context"> again:</span>
 	arch_leave_lazy_mmu_mode();
 
 	/* Do the actual TLB flush before dropping ptl */
<span class="p_del">-	if (tlb-&gt;force_batch_flush)</span>
<span class="p_del">-		tlb_flush_mmu_tlbonly(tlb);</span>
<span class="p_add">+	if (tlb-&gt;force_batch_flush) {</span>
<span class="p_add">+		bool did_flush = tlb_flush_mmu_tlbonly(tlb);</span>
<span class="p_add">+		if (did_flush)</span>
<span class="p_add">+			__tlb_cleanup_pte_range(tlb, start_pte, pte);</span>
<span class="p_add">+	}</span>
 	pte_unmap_unlock(start_pte, ptl);
 
 	/*
<span class="p_chunk">@@ -1965,7 +1970,8 @@</span> <span class="p_context"> static inline int pte_unmap_same(struct</span>
 {
 	int same = 1;
 #if defined(CONFIG_SMP) || defined(CONFIG_PREEMPT)
<span class="p_del">-	if (sizeof(pte_t) &gt; sizeof(unsigned long)) {</span>
<span class="p_add">+	if (unlikely(arch_needs_swap_ptl() ||</span>
<span class="p_add">+	    sizeof(pte_t) &gt; sizeof(unsigned long))) {</span>
 		spinlock_t *ptl = pte_lockptr(mm, pmd);
 		spin_lock(ptl);
 		same = pte_same(*page_table, orig_pte);
<span class="p_header">diff -puN mm/mremap.c~knl-leak-60-actual-fix mm/mremap.c</span>
<span class="p_header">--- a/mm/mremap.c~knl-leak-60-actual-fix	2016-06-30 17:10:43.476288449 -0700</span>
<span class="p_header">+++ b/mm/mremap.c	2016-06-30 17:10:43.497289401 -0700</span>
<span class="p_chunk">@@ -24,6 +24,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm-arch-hooks.h&gt;
 
 #include &lt;asm/cacheflush.h&gt;
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
 #include &lt;asm/tlbflush.h&gt;
 
 #include &quot;internal.h&quot;
<span class="p_chunk">@@ -144,10 +145,14 @@</span> <span class="p_context"> static void move_ptes(struct vm_area_str</span>
 
 	for (; old_addr &lt; old_end; old_pte++, old_addr += PAGE_SIZE,
 				   new_pte++, new_addr += PAGE_SIZE) {
<span class="p_add">+		pte_t old_ptent;</span>
<span class="p_add">+</span>
 		if (pte_none(*old_pte))
 			continue;
<span class="p_del">-		pte = ptep_get_and_clear(mm, old_addr, old_pte);</span>
<span class="p_del">-		pte = move_pte(pte, new_vma-&gt;vm_page_prot, old_addr, new_addr);</span>
<span class="p_add">+		old_ptent = ptep_get_and_clear(mm, old_addr, old_pte);</span>
<span class="p_add">+		pte = move_pte(old_ptent, new_vma-&gt;vm_page_prot,</span>
<span class="p_add">+				old_addr, new_addr);</span>
<span class="p_add">+		arch_fix_pte_leak(mm, old_addr, old_pte);</span>
 		pte = move_soft_dirty_pte(pte);
 		set_pte_at(mm, new_addr, new_pte, pte);
 	}
<span class="p_header">diff -puN mm/rmap.c~knl-leak-60-actual-fix mm/rmap.c</span>
<span class="p_header">--- a/mm/rmap.c~knl-leak-60-actual-fix	2016-06-30 17:10:43.478288539 -0700</span>
<span class="p_header">+++ b/mm/rmap.c	2016-06-30 17:10:43.498289447 -0700</span>
<span class="p_chunk">@@ -633,6 +633,10 @@</span> <span class="p_context"> static bool should_defer_flush(struct mm</span>
 {
 	bool should_defer = false;
 
<span class="p_add">+	/* x86 may need an immediate flush after a pte clear */</span>
<span class="p_add">+	if (arch_disable_deferred_flush())</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
 	if (!(flags &amp; TTU_BATCH_FLUSH))
 		return false;
 
<span class="p_header">diff -puN mm/vmalloc.c~knl-leak-60-actual-fix mm/vmalloc.c</span>
<span class="p_header">--- a/mm/vmalloc.c~knl-leak-60-actual-fix	2016-06-30 17:10:43.479288585 -0700</span>
<span class="p_header">+++ b/mm/vmalloc.c	2016-06-30 17:10:43.500289538 -0700</span>
<span class="p_chunk">@@ -66,6 +66,7 @@</span> <span class="p_context"> static void vunmap_pte_range(pmd_t *pmd,</span>
 	pte = pte_offset_kernel(pmd, addr);
 	do {
 		pte_t ptent = ptep_get_and_clear(&amp;init_mm, addr, pte);
<span class="p_add">+		arch_fix_pte_leak(&amp;init_mm, addr, pte);</span>
 		WARN_ON(!pte_none(ptent) &amp;&amp; !pte_present(ptent));
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



