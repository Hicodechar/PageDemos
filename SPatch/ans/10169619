
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86 fixes - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86 fixes</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 17, 2018, 3:41 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180117154134.bgocrlokyobeyfyu@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10169619/mbox/"
   >mbox</a>
|
   <a href="/patch/10169619/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10169619/">/patch/10169619/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	DBC59603ED for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 17 Jan 2018 15:41:45 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C2EEA283AD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 17 Jan 2018 15:41:45 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id B731E28688; Wed, 17 Jan 2018 15:41:45 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-3.3 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	FSL_HELO_FAKE, RCVD_IN_DNSWL_HI,
	T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AB9B1283AD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 17 Jan 2018 15:41:43 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753546AbeAQPll (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 17 Jan 2018 10:41:41 -0500
Received: from mail-wm0-f47.google.com ([74.125.82.47]:40041 &quot;EHLO
	mail-wm0-f47.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752068AbeAQPlj (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 17 Jan 2018 10:41:39 -0500
Received: by mail-wm0-f47.google.com with SMTP id v123so16665221wmd.5
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 17 Jan 2018 07:41:38 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:content-transfer-encoding:user-agent;
	bh=No89+LEEihnCedMEjcz/eV/oV3V5IHJ2x0bDzShjrFA=;
	b=bEnAqfoGIzydPbEPA++DGpu+Qo6w2AHHhUEAK9tWg8faxvFMDJhAA4C/zuX51eRsdT
	daMVrAN50HFdegFJFnw+Y6ne9ZHxeFnfguU910jHEblGYd7KDGRIl3wpvjlEsdx54Bng
	M+dZkrCzWSb60irxuwkjhBIPaVbLZxW1JLBAbKH0q1m+xseVAyu2cH+l1fQOA5rQ8x7u
	6G75ghWv5ApopFofyhf8OzCPQJ3xpbcdmI+GrnaSTq5i33uH3LsKJTMT/abxs5Xf6JjF
	I+5t93tRzwb7J+Q/MzD2hIuzAPO7t9YQT/xO3EBiJm9SgxxYpVslbX8CZLBPj15hUSKp
	fJzQ==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:content-transfer-encoding
	:user-agent;
	bh=No89+LEEihnCedMEjcz/eV/oV3V5IHJ2x0bDzShjrFA=;
	b=IeC62d6TudkOt/eu8HzZGinK/Yl32KT0pmM9h/An7p/jXdQYnp4bMrh2ehMoAc1Zk1
	cnuB/zZxwEvvwoWCF39aiVn/eXBG37zb1Etyo/w3uTssU3VXkryhkcXsi6AnFm4lrN/k
	NyxykGyyzX4EUMPzXS8PA70ofNc5RM8QRqlqqtSWPZA727IT2AWwqaXGRJ8nZN36SY/K
	6eCHwDffL9tVTtWARv6Vz+WXFWHMyJHHs3oUletNW//IV4E1HwFLqGta9LuWeAseWpoI
	Y4HtvU6WOvN0GfQi0rbGHq7JGyf/PhtJCHS1NxCuWxrsuMKvsCiYmmWPEpQaSRDj+lHe
	rW3A==
X-Gm-Message-State: AKwxytdpI2GbsA3DrbcIgqOCkSUdxy8rGkkq5ICpFknubu/qjC2skTsC
	cee0fZ3tiLlzqQn1X8DnzMs=
X-Google-Smtp-Source: ACJfBouVvr4hJg5eK9ZGIMua78Y1XCApTKAnlkGn16s3YQN485BVwK5WJqjEcU5eyEwKxXGddicKrQ==
X-Received: by 10.28.145.197 with SMTP id t188mr2484824wmd.132.1516203697153;
	Wed, 17 Jan 2018 07:41:37 -0800 (PST)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	e6sm1021384wra.41.2018.01.17.07.41.36
	(version=TLS1_2 cipher=ECDHE-RSA-CHACHA20-POLY1305 bits=256/256);
	Wed, 17 Jan 2018 07:41:36 -0800 (PST)
Date: Wed, 17 Jan 2018 16:41:34 +0100
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] x86 fixes
Message-ID: &lt;20180117154134.bgocrlokyobeyfyu@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=iso-8859-1
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
User-Agent: NeoMutt/20170609 (1.8.3)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Jan. 17, 2018, 3:41 p.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-urgent-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-urgent-for-linus

   # HEAD: 45d55e7bac4028af93f5fa324e69958a0b868e96 x86/apic/vector: Fix off by one in error path

Misc fixes:

 - A rather involved set of memory hardware encryption fixes to support the early 
   loading of microcode files via the initrd. These are larger than what we 
   normally take at such a late -rc stage, but there are two mitigating factors: 
   1) much of the changes are limited to the SME code itself 2) being able to 
   early load microcode has increased importance in the post-Meltdown/Spectre era.

 - An IRQ vector allocator fix

 - An Intel RDT driver use-after-free fix

 - An APIC driver bug fix/revert to make certain older systems boot again

 - A pkeys ABI fix

 - TSC calibration fixes

 - A kdump fix

  out-of-topic modifications in x86-urgent-for-linus:
  -----------------------------------------------------
  include/linux/crash_core.h         # 9f15b9120f56: kdump: Write the correct add
  kernel/crash_core.c                # 9f15b9120f56: kdump: Write the correct add

 Thanks,

	Ingo

------------------&gt;
Andi Kleen (1):
      x86/idt: Mark IDT tables __initconst

Eric W. Biederman (1):
      x86/mm/pkeys: Fix fill_sig_info_pkey

Kirill A. Shutemov (1):
      kdump: Write the correct address of mem_section into vmcoreinfo

Len Brown (3):
      x86/tsc: Future-proof native_calibrate_tsc()
      x86/tsc: Fix erroneous TSC rate on Skylake Xeon
      x86/tsc: Print tsc_khz, when it differs from cpu_khz

Thomas Gleixner (2):
      x86/intel_rdt/cqm: Prevent use after free
      x86/apic/vector: Fix off by one in error path

Tom Lendacky (5):
      x86/mm: Clean up register saving in the __enc_copy() assembly code
      x86/mm: Use a struct to reduce parameters for SME PGD mapping
      x86/mm: Centralize PMD flags in sme_encrypt_kernel()
      x86/mm: Prepare sme_encrypt_kernel() for PAGE aligned encryption
      x86/mm: Encrypt the initrd earlier for BSP microcode update

Ville Syrjälä (1):
      Revert &quot;x86/apic: Remove init_bsp_APIC()&quot;


 arch/x86/include/asm/apic.h        |   1 +
 arch/x86/include/asm/mem_encrypt.h |   4 +-
 arch/x86/kernel/apic/apic.c        |  49 +++++
 arch/x86/kernel/apic/vector.c      |   7 +-
 arch/x86/kernel/cpu/intel_rdt.c    |   8 +-
 arch/x86/kernel/head64.c           |   4 +-
 arch/x86/kernel/idt.c              |  12 +-
 arch/x86/kernel/irqinit.c          |   3 +
 arch/x86/kernel/setup.c            |  10 --
 arch/x86/kernel/tsc.c              |   9 +-
 arch/x86/mm/fault.c                |   7 +-
 arch/x86/mm/mem_encrypt.c          | 356 +++++++++++++++++++++++++++----------
 arch/x86/mm/mem_encrypt_boot.S     |  80 +++++----
 include/linux/crash_core.h         |   2 +
 kernel/crash_core.c                |   2 +-
 15 files changed, 391 insertions(+), 163 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77">Linus Torvalds</a> - Jan. 17, 2018, 8:35 p.m.</div>
<pre class="content">
On Wed, Jan 17, 2018 at 7:41 AM, Ingo Molnar &lt;mingo@kernel.org&gt; wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt;  - A kdump fix</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;   out-of-topic modifications in x86-urgent-for-linus:</span>
<span class="quote">&gt;   -----------------------------------------------------</span>
<span class="quote">&gt;   include/linux/crash_core.h         # 9f15b9120f56: kdump: Write the correct add</span>
<span class="quote">&gt;   kernel/crash_core.c                # 9f15b9120f56: kdump: Write the correct add</span>

This came through Andrew too. It all merged fine since there were no
other modifications, but it&#39;s a bit odd how this was in the x86 tree,
and even if that part makes sense it&#39;s a sign of lack of communication
at some point.

Oh well. Not a big deal. I just thought I&#39;d mention it.

               Linus
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Jan. 18, 2018, 12:24 a.m.</div>
<pre class="content">
* Linus Torvalds &lt;torvalds@linux-foundation.org&gt; wrote:
<span class="quote">
&gt; On Wed, Jan 17, 2018 at 7:41 AM, Ingo Molnar &lt;mingo@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  - A kdump fix</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;   out-of-topic modifications in x86-urgent-for-linus:</span>
<span class="quote">&gt; &gt;   -----------------------------------------------------</span>
<span class="quote">&gt; &gt;   include/linux/crash_core.h         # 9f15b9120f56: kdump: Write the correct add</span>
<span class="quote">&gt; &gt;   kernel/crash_core.c                # 9f15b9120f56: kdump: Write the correct add</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This came through Andrew too. It all merged fine since there were no</span>
<span class="quote">&gt; other modifications, but it&#39;s a bit odd how this was in the x86 tree,</span>
<span class="quote">&gt; and even if that part makes sense it&#39;s a sign of lack of communication</span>
<span class="quote">&gt; at some point.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Oh well. Not a big deal. I just thought I&#39;d mention it.</span>

Hm, I don&#39;t think Andrew Cc:-ed me when he sent it to you, so I didn&#39;t notice the 
duplication. The bug was introduced via the 5-level paging changes in the x86 
tree, so I assumed the fix would go via the x86 tree as well.

Andrew was Cc:-ed to the -tip commit.

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - Jan. 18, 2018, 12:29 a.m.</div>
<pre class="content">
On Thu, 18 Jan 2018 01:24:08 +0100 Ingo Molnar &lt;mingo@kernel.org&gt; wrote:
<span class="quote">
&gt; </span>
<span class="quote">&gt; * Linus Torvalds &lt;torvalds@linux-foundation.org&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; On Wed, Jan 17, 2018 at 7:41 AM, Ingo Molnar &lt;mingo@kernel.org&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;  - A kdump fix</span>
<span class="quote">&gt; &gt; &gt;</span>
<span class="quote">&gt; &gt; &gt;   out-of-topic modifications in x86-urgent-for-linus:</span>
<span class="quote">&gt; &gt; &gt;   -----------------------------------------------------</span>
<span class="quote">&gt; &gt; &gt;   include/linux/crash_core.h         # 9f15b9120f56: kdump: Write the correct add</span>
<span class="quote">&gt; &gt; &gt;   kernel/crash_core.c                # 9f15b9120f56: kdump: Write the correct add</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This came through Andrew too. It all merged fine since there were no</span>
<span class="quote">&gt; &gt; other modifications, but it&#39;s a bit odd how this was in the x86 tree,</span>
<span class="quote">&gt; &gt; and even if that part makes sense it&#39;s a sign of lack of communication</span>
<span class="quote">&gt; &gt; at some point.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Oh well. Not a big deal. I just thought I&#39;d mention it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hm, I don&#39;t think Andrew Cc:-ed me when he sent it to you, so I didn&#39;t notice the </span>
<span class="quote">&gt; duplication. The bug was introduced via the 5-level paging changes in the x86 </span>
<span class="quote">&gt; tree, so I assumed the fix would go via the x86 tree as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Andrew was Cc:-ed to the -tip commit.</span>

I cc&#39;ed the whole world on that one:
<span class="acked-by">
Acked-by: Baoquan He &lt;bhe@redhat.com&gt;</span>
<span class="acked-by">Acked-by: Dave Young &lt;dyoung@redhat.com&gt;</span>
Cc: Ingo Molnar &lt;mingo@redhat.com&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Greg Kroah-Hartman &lt;gregkh@linuxfoundation.org&gt;
Cc: Dave Young &lt;dyoung@redhat.com&gt;
Cc: Baoquan He &lt;bhe@redhat.com&gt;
Cc: Vivek Goyal &lt;vgoyal@redhat.com&gt;
Cc: &lt;stable@vger.kernel.org&gt;

but yeah, I make a lot of noise ;)

Ordinarily I&#39;ll auto-drop such things if they turn up in linux-next but
it seems this one got fast-tracked in -tip so that mechanism didn&#39;t
work (this is rare).
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/apic.h b/arch/x86/include/asm/apic.h</span>
<span class="p_header">index a9e57f08bfa6..98722773391d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/apic.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/apic.h</span>
<span class="p_chunk">@@ -136,6 +136,7 @@</span> <span class="p_context"> extern void disconnect_bsp_APIC(int virt_wire_setup);</span>
 extern void disable_local_APIC(void);
 extern void lapic_shutdown(void);
 extern void sync_Arb_IDs(void);
<span class="p_add">+extern void init_bsp_APIC(void);</span>
 extern void apic_intr_mode_init(void);
 extern void setup_local_APIC(void);
 extern void init_apic_mappings(void);
<span class="p_header">diff --git a/arch/x86/include/asm/mem_encrypt.h b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">index c9459a4c3c68..22c5f3e6f820 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mem_encrypt.h</span>
<span class="p_chunk">@@ -39,7 +39,7 @@</span> <span class="p_context"> void __init sme_unmap_bootdata(char *real_mode_data);</span>
 
 void __init sme_early_init(void);
 
<span class="p_del">-void __init sme_encrypt_kernel(void);</span>
<span class="p_add">+void __init sme_encrypt_kernel(struct boot_params *bp);</span>
 void __init sme_enable(struct boot_params *bp);
 
 int __init early_set_memory_decrypted(unsigned long vaddr, unsigned long size);
<span class="p_chunk">@@ -67,7 +67,7 @@</span> <span class="p_context"> static inline void __init sme_unmap_bootdata(char *real_mode_data) { }</span>
 
 static inline void __init sme_early_init(void) { }
 
<span class="p_del">-static inline void __init sme_encrypt_kernel(void) { }</span>
<span class="p_add">+static inline void __init sme_encrypt_kernel(struct boot_params *bp) { }</span>
 static inline void __init sme_enable(struct boot_params *bp) { }
 
 static inline bool sme_active(void) { return false; }
<span class="p_header">diff --git a/arch/x86/kernel/apic/apic.c b/arch/x86/kernel/apic/apic.c</span>
<span class="p_header">index 880441f24146..25ddf02598d2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/apic.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/apic.c</span>
<span class="p_chunk">@@ -1286,6 +1286,55 @@</span> <span class="p_context"> static int __init apic_intr_mode_select(void)</span>
 	return APIC_SYMMETRIC_IO;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * An initial setup of the virtual wire mode.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init init_bsp_APIC(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int value;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Don&#39;t do the setup now if we have a SMP BIOS as the</span>
<span class="p_add">+	 * through-I/O-APIC virtual wire mode might be active.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (smp_found_config || !boot_cpu_has(X86_FEATURE_APIC))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Do not trust the local APIC being empty at bootup.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	clear_local_APIC();</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Enable APIC.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	value = apic_read(APIC_SPIV);</span>
<span class="p_add">+	value &amp;= ~APIC_VECTOR_MASK;</span>
<span class="p_add">+	value |= APIC_SPIV_APIC_ENABLED;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+	/* This bit is reserved on P4/Xeon and should be cleared */</span>
<span class="p_add">+	if ((boot_cpu_data.x86_vendor == X86_VENDOR_INTEL) &amp;&amp;</span>
<span class="p_add">+	    (boot_cpu_data.x86 == 15))</span>
<span class="p_add">+		value &amp;= ~APIC_SPIV_FOCUS_DISABLED;</span>
<span class="p_add">+	else</span>
<span class="p_add">+#endif</span>
<span class="p_add">+		value |= APIC_SPIV_FOCUS_DISABLED;</span>
<span class="p_add">+	value |= SPURIOUS_APIC_VECTOR;</span>
<span class="p_add">+	apic_write(APIC_SPIV, value);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Set up the virtual wire mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	apic_write(APIC_LVT0, APIC_DM_EXTINT);</span>
<span class="p_add">+	value = APIC_DM_NMI;</span>
<span class="p_add">+	if (!lapic_is_integrated())		/* 82489DX */</span>
<span class="p_add">+		value |= APIC_LVT_LEVEL_TRIGGER;</span>
<span class="p_add">+	if (apic_extnmi == APIC_EXTNMI_NONE)</span>
<span class="p_add">+		value |= APIC_LVT_MASKED;</span>
<span class="p_add">+	apic_write(APIC_LVT1, value);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Init the interrupt delivery mode for the BSP */
 void __init apic_intr_mode_init(void)
 {
<span class="p_header">diff --git a/arch/x86/kernel/apic/vector.c b/arch/x86/kernel/apic/vector.c</span>
<span class="p_header">index f8b03bb8e725..3cc471beb50b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/vector.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/vector.c</span>
<span class="p_chunk">@@ -542,14 +542,17 @@</span> <span class="p_context"> static int x86_vector_alloc_irqs(struct irq_domain *domain, unsigned int virq,</span>
 
 		err = assign_irq_vector_policy(irqd, info);
 		trace_vector_setup(virq + i, false, err);
<span class="p_del">-		if (err)</span>
<span class="p_add">+		if (err) {</span>
<span class="p_add">+			irqd-&gt;chip_data = NULL;</span>
<span class="p_add">+			free_apic_chip_data(apicd);</span>
 			goto error;
<span class="p_add">+		}</span>
 	}
 
 	return 0;
 
 error:
<span class="p_del">-	x86_vector_free_irqs(domain, virq, i + 1);</span>
<span class="p_add">+	x86_vector_free_irqs(domain, virq, i);</span>
 	return err;
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/intel_rdt.c b/arch/x86/kernel/cpu/intel_rdt.c</span>
<span class="p_header">index 88dcf8479013..99442370de40 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/intel_rdt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/intel_rdt.c</span>
<span class="p_chunk">@@ -525,10 +525,6 @@</span> <span class="p_context"> static void domain_remove_cpu(int cpu, struct rdt_resource *r)</span>
 		 */
 		if (static_branch_unlikely(&amp;rdt_mon_enable_key))
 			rmdir_mondata_subdir_allrdtgrp(r, d-&gt;id);
<span class="p_del">-		kfree(d-&gt;ctrl_val);</span>
<span class="p_del">-		kfree(d-&gt;rmid_busy_llc);</span>
<span class="p_del">-		kfree(d-&gt;mbm_total);</span>
<span class="p_del">-		kfree(d-&gt;mbm_local);</span>
 		list_del(&amp;d-&gt;list);
 		if (is_mbm_enabled())
 			cancel_delayed_work(&amp;d-&gt;mbm_over);
<span class="p_chunk">@@ -545,6 +541,10 @@</span> <span class="p_context"> static void domain_remove_cpu(int cpu, struct rdt_resource *r)</span>
 			cancel_delayed_work(&amp;d-&gt;cqm_limbo);
 		}
 
<span class="p_add">+		kfree(d-&gt;ctrl_val);</span>
<span class="p_add">+		kfree(d-&gt;rmid_busy_llc);</span>
<span class="p_add">+		kfree(d-&gt;mbm_total);</span>
<span class="p_add">+		kfree(d-&gt;mbm_local);</span>
 		kfree(d);
 		return;
 	}
<span class="p_header">diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c</span>
<span class="p_header">index 6a5d757b9cfd..7ba5d819ebe3 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/head64.c</span>
<span class="p_chunk">@@ -157,8 +157,8 @@</span> <span class="p_context"> unsigned long __head __startup_64(unsigned long physaddr,</span>
 	p = fixup_pointer(&amp;phys_base, physaddr);
 	*p += load_delta - sme_get_me_mask();
 
<span class="p_del">-	/* Encrypt the kernel (if SME is active) */</span>
<span class="p_del">-	sme_encrypt_kernel();</span>
<span class="p_add">+	/* Encrypt the kernel and related (if SME is active) */</span>
<span class="p_add">+	sme_encrypt_kernel(bp);</span>
 
 	/*
 	 * Return the SME encryption mask (if SME is active) to be used as a
<span class="p_header">diff --git a/arch/x86/kernel/idt.c b/arch/x86/kernel/idt.c</span>
<span class="p_header">index d985cef3984f..56d99be3706a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/idt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/idt.c</span>
<span class="p_chunk">@@ -56,7 +56,7 @@</span> <span class="p_context"> struct idt_data {</span>
  * Early traps running on the DEFAULT_STACK because the other interrupt
  * stacks work only after cpu_init().
  */
<span class="p_del">-static const __initdata struct idt_data early_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data early_idts[] = {</span>
 	INTG(X86_TRAP_DB,		debug),
 	SYSG(X86_TRAP_BP,		int3),
 #ifdef CONFIG_X86_32
<span class="p_chunk">@@ -70,7 +70,7 @@</span> <span class="p_context"> static const __initdata struct idt_data early_idts[] = {</span>
  * the traps which use them are reinitialized with IST after cpu_init() has
  * set up TSS.
  */
<span class="p_del">-static const __initdata struct idt_data def_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data def_idts[] = {</span>
 	INTG(X86_TRAP_DE,		divide_error),
 	INTG(X86_TRAP_NMI,		nmi),
 	INTG(X86_TRAP_BR,		bounds),
<span class="p_chunk">@@ -108,7 +108,7 @@</span> <span class="p_context"> static const __initdata struct idt_data def_idts[] = {</span>
 /*
  * The APIC and SMP idt entries
  */
<span class="p_del">-static const __initdata struct idt_data apic_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data apic_idts[] = {</span>
 #ifdef CONFIG_SMP
 	INTG(RESCHEDULE_VECTOR,		reschedule_interrupt),
 	INTG(CALL_FUNCTION_VECTOR,	call_function_interrupt),
<span class="p_chunk">@@ -150,7 +150,7 @@</span> <span class="p_context"> static const __initdata struct idt_data apic_idts[] = {</span>
  * Early traps running on the DEFAULT_STACK because the other interrupt
  * stacks work only after cpu_init().
  */
<span class="p_del">-static const __initdata struct idt_data early_pf_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data early_pf_idts[] = {</span>
 	INTG(X86_TRAP_PF,		page_fault),
 };
 
<span class="p_chunk">@@ -158,7 +158,7 @@</span> <span class="p_context"> static const __initdata struct idt_data early_pf_idts[] = {</span>
  * Override for the debug_idt. Same as the default, but with interrupt
  * stack set to DEFAULT_STACK (0). Required for NMI trap handling.
  */
<span class="p_del">-static const __initdata struct idt_data dbg_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data dbg_idts[] = {</span>
 	INTG(X86_TRAP_DB,	debug),
 	INTG(X86_TRAP_BP,	int3),
 };
<span class="p_chunk">@@ -180,7 +180,7 @@</span> <span class="p_context"> gate_desc debug_idt_table[IDT_ENTRIES] __page_aligned_bss;</span>
  * The exceptions which use Interrupt stacks. They are setup after
  * cpu_init() when the TSS has been initialized.
  */
<span class="p_del">-static const __initdata struct idt_data ist_idts[] = {</span>
<span class="p_add">+static const __initconst struct idt_data ist_idts[] = {</span>
 	ISTG(X86_TRAP_DB,	debug,		DEBUG_STACK),
 	ISTG(X86_TRAP_NMI,	nmi,		NMI_STACK),
 	SISTG(X86_TRAP_BP,	int3,		DEBUG_STACK),
<span class="p_header">diff --git a/arch/x86/kernel/irqinit.c b/arch/x86/kernel/irqinit.c</span>
<span class="p_header">index 8da3e909e967..a539410c4ea9 100644</span>
<span class="p_header">--- a/arch/x86/kernel/irqinit.c</span>
<span class="p_header">+++ b/arch/x86/kernel/irqinit.c</span>
<span class="p_chunk">@@ -61,6 +61,9 @@</span> <span class="p_context"> void __init init_ISA_irqs(void)</span>
 	struct irq_chip *chip = legacy_pic-&gt;chip;
 	int i;
 
<span class="p_add">+#if defined(CONFIG_X86_64) || defined(CONFIG_X86_LOCAL_APIC)</span>
<span class="p_add">+	init_bsp_APIC();</span>
<span class="p_add">+#endif</span>
 	legacy_pic-&gt;init(0);
 
 	for (i = 0; i &lt; nr_legacy_irqs(); i++)
<span class="p_header">diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c</span>
<span class="p_header">index 145810b0edf6..68d7ab81c62f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/setup.c</span>
<span class="p_header">+++ b/arch/x86/kernel/setup.c</span>
<span class="p_chunk">@@ -364,16 +364,6 @@</span> <span class="p_context"> static void __init reserve_initrd(void)</span>
 	    !ramdisk_image || !ramdisk_size)
 		return;		/* No initrd provided by bootloader */
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If SME is active, this memory will be marked encrypted by the</span>
<span class="p_del">-	 * kernel when it is accessed (including relocation). However, the</span>
<span class="p_del">-	 * ramdisk image was loaded decrypted by the bootloader, so make</span>
<span class="p_del">-	 * sure that it is encrypted before accessing it. For SEV the</span>
<span class="p_del">-	 * ramdisk will already be encrypted, so only do this for SME.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (sme_active())</span>
<span class="p_del">-		sme_early_encrypt(ramdisk_image, ramdisk_end - ramdisk_image);</span>
<span class="p_del">-</span>
 	initrd_start = 0;
 
 	mapped_size = memblock_mem_size(max_pfn_mapped);
<span class="p_header">diff --git a/arch/x86/kernel/tsc.c b/arch/x86/kernel/tsc.c</span>
<span class="p_header">index 8ea117f8142e..e169e85db434 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tsc.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tsc.c</span>
<span class="p_chunk">@@ -602,7 +602,6 @@</span> <span class="p_context"> unsigned long native_calibrate_tsc(void)</span>
 		case INTEL_FAM6_KABYLAKE_DESKTOP:
 			crystal_khz = 24000;	/* 24.0 MHz */
 			break;
<span class="p_del">-		case INTEL_FAM6_SKYLAKE_X:</span>
 		case INTEL_FAM6_ATOM_DENVERTON:
 			crystal_khz = 25000;	/* 25.0 MHz */
 			break;
<span class="p_chunk">@@ -612,6 +611,8 @@</span> <span class="p_context"> unsigned long native_calibrate_tsc(void)</span>
 		}
 	}
 
<span class="p_add">+	if (crystal_khz == 0)</span>
<span class="p_add">+		return 0;</span>
 	/*
 	 * TSC frequency determined by CPUID is a &quot;hardware reported&quot;
 	 * frequency and is the most accurate one so far we have. This
<span class="p_chunk">@@ -1315,6 +1316,12 @@</span> <span class="p_context"> void __init tsc_init(void)</span>
 		(unsigned long)cpu_khz / 1000,
 		(unsigned long)cpu_khz % 1000);
 
<span class="p_add">+	if (cpu_khz != tsc_khz) {</span>
<span class="p_add">+		pr_info(&quot;Detected %lu.%03lu MHz TSC&quot;,</span>
<span class="p_add">+			(unsigned long)tsc_khz / 1000,</span>
<span class="p_add">+			(unsigned long)tsc_khz % 1000);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* Sanitize TSC ADJUST before cyc2ns gets initialized */
 	tsc_store_and_check_tsc_adjust(true);
 
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index 06fe3d51d385..b3e40773dce0 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -172,14 +172,15 @@</span> <span class="p_context"> is_prefetch(struct pt_regs *regs, unsigned long error_code, unsigned long addr)</span>
  * 6. T1   : reaches here, sees vma_pkey(vma)=5, when we really
  *	     faulted on a pte with its pkey=4.
  */
<span class="p_del">-static void fill_sig_info_pkey(int si_code, siginfo_t *info, u32 *pkey)</span>
<span class="p_add">+static void fill_sig_info_pkey(int si_signo, int si_code, siginfo_t *info,</span>
<span class="p_add">+		u32 *pkey)</span>
 {
 	/* This is effectively an #ifdef */
 	if (!boot_cpu_has(X86_FEATURE_OSPKE))
 		return;
 
 	/* Fault not from Protection Keys: nothing to do */
<span class="p_del">-	if (si_code != SEGV_PKUERR)</span>
<span class="p_add">+	if ((si_code != SEGV_PKUERR) || (si_signo != SIGSEGV))</span>
 		return;
 	/*
 	 * force_sig_info_fault() is called from a number of
<span class="p_chunk">@@ -218,7 +219,7 @@</span> <span class="p_context"> force_sig_info_fault(int si_signo, int si_code, unsigned long address,</span>
 		lsb = PAGE_SHIFT;
 	info.si_addr_lsb = lsb;
 
<span class="p_del">-	fill_sig_info_pkey(si_code, &amp;info, pkey);</span>
<span class="p_add">+	fill_sig_info_pkey(si_signo, si_code, &amp;info, pkey);</span>
 
 	force_sig_info(si_signo, &amp;info, tsk);
 }
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">index 391b13402e40..3ef362f598e3 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_chunk">@@ -464,37 +464,62 @@</span> <span class="p_context"> void swiotlb_set_mem_attributes(void *vaddr, unsigned long size)</span>
 	set_memory_decrypted((unsigned long)vaddr, size &gt;&gt; PAGE_SHIFT);
 }
 
<span class="p_del">-static void __init sme_clear_pgd(pgd_t *pgd_base, unsigned long start,</span>
<span class="p_del">-				 unsigned long end)</span>
<span class="p_add">+struct sme_populate_pgd_data {</span>
<span class="p_add">+	void	*pgtable_area;</span>
<span class="p_add">+	pgd_t	*pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmdval_t pmd_flags;</span>
<span class="p_add">+	pteval_t pte_flags;</span>
<span class="p_add">+	unsigned long paddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	unsigned long vaddr;</span>
<span class="p_add">+	unsigned long vaddr_end;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_clear_pgd(struct sme_populate_pgd_data *ppd)</span>
 {
 	unsigned long pgd_start, pgd_end, pgd_size;
 	pgd_t *pgd_p;
 
<span class="p_del">-	pgd_start = start &amp; PGDIR_MASK;</span>
<span class="p_del">-	pgd_end = end &amp; PGDIR_MASK;</span>
<span class="p_add">+	pgd_start = ppd-&gt;vaddr &amp; PGDIR_MASK;</span>
<span class="p_add">+	pgd_end = ppd-&gt;vaddr_end &amp; PGDIR_MASK;</span>
 
<span class="p_del">-	pgd_size = (((pgd_end - pgd_start) / PGDIR_SIZE) + 1);</span>
<span class="p_del">-	pgd_size *= sizeof(pgd_t);</span>
<span class="p_add">+	pgd_size = (((pgd_end - pgd_start) / PGDIR_SIZE) + 1) * sizeof(pgd_t);</span>
 
<span class="p_del">-	pgd_p = pgd_base + pgd_index(start);</span>
<span class="p_add">+	pgd_p = ppd-&gt;pgd + pgd_index(ppd-&gt;vaddr);</span>
 
 	memset(pgd_p, 0, pgd_size);
 }
 
<span class="p_del">-#define PGD_FLAGS	_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define P4D_FLAGS	_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define PUD_FLAGS	_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define PMD_FLAGS	(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_add">+#define PGD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define P4D_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define PUD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define PMD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_FLAGS_LARGE		(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_FLAGS_DEC		PMD_FLAGS_LARGE</span>
<span class="p_add">+#define PMD_FLAGS_DEC_WP	((PMD_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_add">+				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_FLAGS_ENC		(PMD_FLAGS_LARGE | _PAGE_ENC)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTE_FLAGS		(__PAGE_KERNEL_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTE_FLAGS_DEC		PTE_FLAGS</span>
<span class="p_add">+#define PTE_FLAGS_DEC_WP	((PTE_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_add">+				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTE_FLAGS_ENC		(PTE_FLAGS | _PAGE_ENC)</span>
 
<span class="p_del">-static void __init *sme_populate_pgd(pgd_t *pgd_base, void *pgtable_area,</span>
<span class="p_del">-				     unsigned long vaddr, pmdval_t pmd_val)</span>
<span class="p_add">+static pmd_t __init *sme_prepare_pgd(struct sme_populate_pgd_data *ppd)</span>
 {
 	pgd_t *pgd_p;
 	p4d_t *p4d_p;
 	pud_t *pud_p;
 	pmd_t *pmd_p;
 
<span class="p_del">-	pgd_p = pgd_base + pgd_index(vaddr);</span>
<span class="p_add">+	pgd_p = ppd-&gt;pgd + pgd_index(ppd-&gt;vaddr);</span>
 	if (native_pgd_val(*pgd_p)) {
 		if (IS_ENABLED(CONFIG_X86_5LEVEL))
 			p4d_p = (p4d_t *)(native_pgd_val(*pgd_p) &amp; ~PTE_FLAGS_MASK);
<span class="p_chunk">@@ -504,15 +529,15 @@</span> <span class="p_context"> static void __init *sme_populate_pgd(pgd_t *pgd_base, void *pgtable_area,</span>
 		pgd_t pgd;
 
 		if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
<span class="p_del">-			p4d_p = pgtable_area;</span>
<span class="p_add">+			p4d_p = ppd-&gt;pgtable_area;</span>
 			memset(p4d_p, 0, sizeof(*p4d_p) * PTRS_PER_P4D);
<span class="p_del">-			pgtable_area += sizeof(*p4d_p) * PTRS_PER_P4D;</span>
<span class="p_add">+			ppd-&gt;pgtable_area += sizeof(*p4d_p) * PTRS_PER_P4D;</span>
 
 			pgd = native_make_pgd((pgdval_t)p4d_p + PGD_FLAGS);
 		} else {
<span class="p_del">-			pud_p = pgtable_area;</span>
<span class="p_add">+			pud_p = ppd-&gt;pgtable_area;</span>
 			memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);
<span class="p_del">-			pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
<span class="p_add">+			ppd-&gt;pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
 
 			pgd = native_make_pgd((pgdval_t)pud_p + PGD_FLAGS);
 		}
<span class="p_chunk">@@ -520,58 +545,160 @@</span> <span class="p_context"> static void __init *sme_populate_pgd(pgd_t *pgd_base, void *pgtable_area,</span>
 	}
 
 	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
<span class="p_del">-		p4d_p += p4d_index(vaddr);</span>
<span class="p_add">+		p4d_p += p4d_index(ppd-&gt;vaddr);</span>
 		if (native_p4d_val(*p4d_p)) {
 			pud_p = (pud_t *)(native_p4d_val(*p4d_p) &amp; ~PTE_FLAGS_MASK);
 		} else {
 			p4d_t p4d;
 
<span class="p_del">-			pud_p = pgtable_area;</span>
<span class="p_add">+			pud_p = ppd-&gt;pgtable_area;</span>
 			memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);
<span class="p_del">-			pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
<span class="p_add">+			ppd-&gt;pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
 
 			p4d = native_make_p4d((pudval_t)pud_p + P4D_FLAGS);
 			native_set_p4d(p4d_p, p4d);
 		}
 	}
 
<span class="p_del">-	pud_p += pud_index(vaddr);</span>
<span class="p_add">+	pud_p += pud_index(ppd-&gt;vaddr);</span>
 	if (native_pud_val(*pud_p)) {
 		if (native_pud_val(*pud_p) &amp; _PAGE_PSE)
<span class="p_del">-			goto out;</span>
<span class="p_add">+			return NULL;</span>
 
 		pmd_p = (pmd_t *)(native_pud_val(*pud_p) &amp; ~PTE_FLAGS_MASK);
 	} else {
 		pud_t pud;
 
<span class="p_del">-		pmd_p = pgtable_area;</span>
<span class="p_add">+		pmd_p = ppd-&gt;pgtable_area;</span>
 		memset(pmd_p, 0, sizeof(*pmd_p) * PTRS_PER_PMD);
<span class="p_del">-		pgtable_area += sizeof(*pmd_p) * PTRS_PER_PMD;</span>
<span class="p_add">+		ppd-&gt;pgtable_area += sizeof(*pmd_p) * PTRS_PER_PMD;</span>
 
 		pud = native_make_pud((pmdval_t)pmd_p + PUD_FLAGS);
 		native_set_pud(pud_p, pud);
 	}
 
<span class="p_del">-	pmd_p += pmd_index(vaddr);</span>
<span class="p_add">+	return pmd_p;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_populate_pgd_large(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd_p;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p = sme_prepare_pgd(ppd);</span>
<span class="p_add">+	if (!pmd_p)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p += pmd_index(ppd-&gt;vaddr);</span>
 	if (!native_pmd_val(*pmd_p) || !(native_pmd_val(*pmd_p) &amp; _PAGE_PSE))
<span class="p_del">-		native_set_pmd(pmd_p, native_make_pmd(pmd_val));</span>
<span class="p_add">+		native_set_pmd(pmd_p, native_make_pmd(ppd-&gt;paddr | ppd-&gt;pmd_flags));</span>
<span class="p_add">+}</span>
 
<span class="p_del">-out:</span>
<span class="p_del">-	return pgtable_area;</span>
<span class="p_add">+static void __init sme_populate_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd_p;</span>
<span class="p_add">+	pte_t *pte_p;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p = sme_prepare_pgd(ppd);</span>
<span class="p_add">+	if (!pmd_p)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p += pmd_index(ppd-&gt;vaddr);</span>
<span class="p_add">+	if (native_pmd_val(*pmd_p)) {</span>
<span class="p_add">+		if (native_pmd_val(*pmd_p) &amp; _PAGE_PSE)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte_p = (pte_t *)(native_pmd_val(*pmd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pmd_t pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte_p = ppd-&gt;pgtable_area;</span>
<span class="p_add">+		memset(pte_p, 0, sizeof(*pte_p) * PTRS_PER_PTE);</span>
<span class="p_add">+		ppd-&gt;pgtable_area += sizeof(*pte_p) * PTRS_PER_PTE;</span>
<span class="p_add">+</span>
<span class="p_add">+		pmd = native_make_pmd((pteval_t)pte_p + PMD_FLAGS);</span>
<span class="p_add">+		native_set_pmd(pmd_p, pmd);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pte_p += pte_index(ppd-&gt;vaddr);</span>
<span class="p_add">+	if (!native_pte_val(*pte_p))</span>
<span class="p_add">+		native_set_pte(pte_p, native_make_pte(ppd-&gt;paddr | ppd-&gt;pte_flags));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init __sme_map_range_pmd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (ppd-&gt;vaddr &lt; ppd-&gt;vaddr_end) {</span>
<span class="p_add">+		sme_populate_pgd_large(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+		ppd-&gt;vaddr += PMD_PAGE_SIZE;</span>
<span class="p_add">+		ppd-&gt;paddr += PMD_PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init __sme_map_range_pte(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (ppd-&gt;vaddr &lt; ppd-&gt;vaddr_end) {</span>
<span class="p_add">+		sme_populate_pgd(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+		ppd-&gt;vaddr += PAGE_SIZE;</span>
<span class="p_add">+		ppd-&gt;paddr += PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init __sme_map_range(struct sme_populate_pgd_data *ppd,</span>
<span class="p_add">+				   pmdval_t pmd_flags, pteval_t pte_flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vaddr_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	ppd-&gt;pmd_flags = pmd_flags;</span>
<span class="p_add">+	ppd-&gt;pte_flags = pte_flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Save original end value since we modify the struct value */</span>
<span class="p_add">+	vaddr_end = ppd-&gt;vaddr_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If start is not 2MB aligned, create PTE entries */</span>
<span class="p_add">+	ppd-&gt;vaddr_end = ALIGN(ppd-&gt;vaddr, PMD_PAGE_SIZE);</span>
<span class="p_add">+	__sme_map_range_pte(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Create PMD entries */</span>
<span class="p_add">+	ppd-&gt;vaddr_end = vaddr_end &amp; PMD_PAGE_MASK;</span>
<span class="p_add">+	__sme_map_range_pmd(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If end is not 2MB aligned, create PTE entries */</span>
<span class="p_add">+	ppd-&gt;vaddr_end = vaddr_end;</span>
<span class="p_add">+	__sme_map_range_pte(ppd);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_map_range_encrypted(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__sme_map_range(ppd, PMD_FLAGS_ENC, PTE_FLAGS_ENC);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_map_range_decrypted(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__sme_map_range(ppd, PMD_FLAGS_DEC, PTE_FLAGS_DEC);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_map_range_decrypted_wp(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__sme_map_range(ppd, PMD_FLAGS_DEC_WP, PTE_FLAGS_DEC_WP);</span>
 }
 
 static unsigned long __init sme_pgtable_calc(unsigned long len)
 {
<span class="p_del">-	unsigned long p4d_size, pud_size, pmd_size;</span>
<span class="p_add">+	unsigned long p4d_size, pud_size, pmd_size, pte_size;</span>
 	unsigned long total;
 
 	/*
 	 * Perform a relatively simplistic calculation of the pagetable
<span class="p_del">-	 * entries that are needed. That mappings will be covered by 2MB</span>
<span class="p_del">-	 * PMD entries so we can conservatively calculate the required</span>
<span class="p_add">+	 * entries that are needed. Those mappings will be covered mostly</span>
<span class="p_add">+	 * by 2MB PMD entries so we can conservatively calculate the required</span>
 	 * number of P4D, PUD and PMD structures needed to perform the
<span class="p_del">-	 * mappings. Incrementing the count for each covers the case where</span>
<span class="p_del">-	 * the addresses cross entries.</span>
<span class="p_add">+	 * mappings.  For mappings that are not 2MB aligned, PTE mappings</span>
<span class="p_add">+	 * would be needed for the start and end portion of the address range</span>
<span class="p_add">+	 * that fall outside of the 2MB alignment.  This results in, at most,</span>
<span class="p_add">+	 * two extra pages to hold PTE entries for each range that is mapped.</span>
<span class="p_add">+	 * Incrementing the count for each covers the case where the addresses</span>
<span class="p_add">+	 * cross entries.</span>
 	 */
 	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {
 		p4d_size = (ALIGN(len, PGDIR_SIZE) / PGDIR_SIZE) + 1;
<span class="p_chunk">@@ -585,8 +712,9 @@</span> <span class="p_context"> static unsigned long __init sme_pgtable_calc(unsigned long len)</span>
 	}
 	pmd_size = (ALIGN(len, PUD_SIZE) / PUD_SIZE) + 1;
 	pmd_size *= sizeof(pmd_t) * PTRS_PER_PMD;
<span class="p_add">+	pte_size = 2 * sizeof(pte_t) * PTRS_PER_PTE;</span>
 
<span class="p_del">-	total = p4d_size + pud_size + pmd_size;</span>
<span class="p_add">+	total = p4d_size + pud_size + pmd_size + pte_size;</span>
 
 	/*
 	 * Now calculate the added pagetable structures needed to populate
<span class="p_chunk">@@ -610,29 +738,29 @@</span> <span class="p_context"> static unsigned long __init sme_pgtable_calc(unsigned long len)</span>
 	return total;
 }
 
<span class="p_del">-void __init sme_encrypt_kernel(void)</span>
<span class="p_add">+void __init sme_encrypt_kernel(struct boot_params *bp)</span>
 {
 	unsigned long workarea_start, workarea_end, workarea_len;
 	unsigned long execute_start, execute_end, execute_len;
 	unsigned long kernel_start, kernel_end, kernel_len;
<span class="p_add">+	unsigned long initrd_start, initrd_end, initrd_len;</span>
<span class="p_add">+	struct sme_populate_pgd_data ppd;</span>
 	unsigned long pgtable_area_len;
<span class="p_del">-	unsigned long paddr, pmd_flags;</span>
 	unsigned long decrypted_base;
<span class="p_del">-	void *pgtable_area;</span>
<span class="p_del">-	pgd_t *pgd;</span>
 
 	if (!sme_active())
 		return;
 
 	/*
<span class="p_del">-	 * Prepare for encrypting the kernel by building new pagetables with</span>
<span class="p_del">-	 * the necessary attributes needed to encrypt the kernel in place.</span>
<span class="p_add">+	 * Prepare for encrypting the kernel and initrd by building new</span>
<span class="p_add">+	 * pagetables with the necessary attributes needed to encrypt the</span>
<span class="p_add">+	 * kernel in place.</span>
 	 *
 	 *   One range of virtual addresses will map the memory occupied
<span class="p_del">-	 *   by the kernel as encrypted.</span>
<span class="p_add">+	 *   by the kernel and initrd as encrypted.</span>
 	 *
 	 *   Another range of virtual addresses will map the memory occupied
<span class="p_del">-	 *   by the kernel as decrypted and write-protected.</span>
<span class="p_add">+	 *   by the kernel and initrd as decrypted and write-protected.</span>
 	 *
 	 *     The use of write-protect attribute will prevent any of the
 	 *     memory from being cached.
<span class="p_chunk">@@ -643,6 +771,20 @@</span> <span class="p_context"> void __init sme_encrypt_kernel(void)</span>
 	kernel_end = ALIGN(__pa_symbol(_end), PMD_PAGE_SIZE);
 	kernel_len = kernel_end - kernel_start;
 
<span class="p_add">+	initrd_start = 0;</span>
<span class="p_add">+	initrd_end = 0;</span>
<span class="p_add">+	initrd_len = 0;</span>
<span class="p_add">+#ifdef CONFIG_BLK_DEV_INITRD</span>
<span class="p_add">+	initrd_len = (unsigned long)bp-&gt;hdr.ramdisk_size |</span>
<span class="p_add">+		     ((unsigned long)bp-&gt;ext_ramdisk_size &lt;&lt; 32);</span>
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		initrd_start = (unsigned long)bp-&gt;hdr.ramdisk_image |</span>
<span class="p_add">+			       ((unsigned long)bp-&gt;ext_ramdisk_image &lt;&lt; 32);</span>
<span class="p_add">+		initrd_end = PAGE_ALIGN(initrd_start + initrd_len);</span>
<span class="p_add">+		initrd_len = initrd_end - initrd_start;</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	/* Set the encryption workarea to be immediately after the kernel */
 	workarea_start = kernel_end;
 
<span class="p_chunk">@@ -665,16 +807,21 @@</span> <span class="p_context"> void __init sme_encrypt_kernel(void)</span>
 	 */
 	pgtable_area_len = sizeof(pgd_t) * PTRS_PER_PGD;
 	pgtable_area_len += sme_pgtable_calc(execute_end - kernel_start) * 2;
<span class="p_add">+	if (initrd_len)</span>
<span class="p_add">+		pgtable_area_len += sme_pgtable_calc(initrd_len) * 2;</span>
 
 	/* PUDs and PMDs needed in the current pagetables for the workarea */
 	pgtable_area_len += sme_pgtable_calc(execute_len + pgtable_area_len);
 
 	/*
 	 * The total workarea includes the executable encryption area and
<span class="p_del">-	 * the pagetable area.</span>
<span class="p_add">+	 * the pagetable area. The start of the workarea is already 2MB</span>
<span class="p_add">+	 * aligned, align the end of the workarea on a 2MB boundary so that</span>
<span class="p_add">+	 * we don&#39;t try to create/allocate PTE entries from the workarea</span>
<span class="p_add">+	 * before it is mapped.</span>
 	 */
 	workarea_len = execute_len + pgtable_area_len;
<span class="p_del">-	workarea_end = workarea_start + workarea_len;</span>
<span class="p_add">+	workarea_end = ALIGN(workarea_start + workarea_len, PMD_PAGE_SIZE);</span>
 
 	/*
 	 * Set the address to the start of where newly created pagetable
<span class="p_chunk">@@ -683,45 +830,30 @@</span> <span class="p_context"> void __init sme_encrypt_kernel(void)</span>
 	 * pagetables and when the new encrypted and decrypted kernel
 	 * mappings are populated.
 	 */
<span class="p_del">-	pgtable_area = (void *)execute_end;</span>
<span class="p_add">+	ppd.pgtable_area = (void *)execute_end;</span>
 
 	/*
 	 * Make sure the current pagetable structure has entries for
 	 * addressing the workarea.
 	 */
<span class="p_del">-	pgd = (pgd_t *)native_read_cr3_pa();</span>
<span class="p_del">-	paddr = workarea_start;</span>
<span class="p_del">-	while (paddr &lt; workarea_end) {</span>
<span class="p_del">-		pgtable_area = sme_populate_pgd(pgd, pgtable_area,</span>
<span class="p_del">-						paddr,</span>
<span class="p_del">-						paddr + PMD_FLAGS);</span>
<span class="p_del">-</span>
<span class="p_del">-		paddr += PMD_PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	ppd.pgd = (pgd_t *)native_read_cr3_pa();</span>
<span class="p_add">+	ppd.paddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end;</span>
<span class="p_add">+	sme_map_range_decrypted(&amp;ppd);</span>
 
 	/* Flush the TLB - no globals so cr3 is enough */
 	native_write_cr3(__native_read_cr3());
 
 	/*
 	 * A new pagetable structure is being built to allow for the kernel
<span class="p_del">-	 * to be encrypted. It starts with an empty PGD that will then be</span>
<span class="p_del">-	 * populated with new PUDs and PMDs as the encrypted and decrypted</span>
<span class="p_del">-	 * kernel mappings are created.</span>
<span class="p_add">+	 * and initrd to be encrypted. It starts with an empty PGD that will</span>
<span class="p_add">+	 * then be populated with new PUDs and PMDs as the encrypted and</span>
<span class="p_add">+	 * decrypted kernel mappings are created.</span>
 	 */
<span class="p_del">-	pgd = pgtable_area;</span>
<span class="p_del">-	memset(pgd, 0, sizeof(*pgd) * PTRS_PER_PGD);</span>
<span class="p_del">-	pgtable_area += sizeof(*pgd) * PTRS_PER_PGD;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Add encrypted kernel (identity) mappings */</span>
<span class="p_del">-	pmd_flags = PMD_FLAGS | _PAGE_ENC;</span>
<span class="p_del">-	paddr = kernel_start;</span>
<span class="p_del">-	while (paddr &lt; kernel_end) {</span>
<span class="p_del">-		pgtable_area = sme_populate_pgd(pgd, pgtable_area,</span>
<span class="p_del">-						paddr,</span>
<span class="p_del">-						paddr + pmd_flags);</span>
<span class="p_del">-</span>
<span class="p_del">-		paddr += PMD_PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	ppd.pgd = ppd.pgtable_area;</span>
<span class="p_add">+	memset(ppd.pgd, 0, sizeof(pgd_t) * PTRS_PER_PGD);</span>
<span class="p_add">+	ppd.pgtable_area += sizeof(pgd_t) * PTRS_PER_PGD;</span>
 
 	/*
 	 * A different PGD index/entry must be used to get different
<span class="p_chunk">@@ -730,47 +862,79 @@</span> <span class="p_context"> void __init sme_encrypt_kernel(void)</span>
 	 * the base of the mapping.
 	 */
 	decrypted_base = (pgd_index(workarea_end) + 1) &amp; (PTRS_PER_PGD - 1);
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		unsigned long check_base;</span>
<span class="p_add">+</span>
<span class="p_add">+		check_base = (pgd_index(initrd_end) + 1) &amp; (PTRS_PER_PGD - 1);</span>
<span class="p_add">+		decrypted_base = max(decrypted_base, check_base);</span>
<span class="p_add">+	}</span>
 	decrypted_base &lt;&lt;= PGDIR_SHIFT;
 
<span class="p_add">+	/* Add encrypted kernel (identity) mappings */</span>
<span class="p_add">+	ppd.paddr = kernel_start;</span>
<span class="p_add">+	ppd.vaddr = kernel_start;</span>
<span class="p_add">+	ppd.vaddr_end = kernel_end;</span>
<span class="p_add">+	sme_map_range_encrypted(&amp;ppd);</span>
<span class="p_add">+</span>
 	/* Add decrypted, write-protected kernel (non-identity) mappings */
<span class="p_del">-	pmd_flags = (PMD_FLAGS &amp; ~_PAGE_CACHE_MASK) | (_PAGE_PAT | _PAGE_PWT);</span>
<span class="p_del">-	paddr = kernel_start;</span>
<span class="p_del">-	while (paddr &lt; kernel_end) {</span>
<span class="p_del">-		pgtable_area = sme_populate_pgd(pgd, pgtable_area,</span>
<span class="p_del">-						paddr + decrypted_base,</span>
<span class="p_del">-						paddr + pmd_flags);</span>
<span class="p_del">-</span>
<span class="p_del">-		paddr += PMD_PAGE_SIZE;</span>
<span class="p_add">+	ppd.paddr = kernel_start;</span>
<span class="p_add">+	ppd.vaddr = kernel_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = kernel_end + decrypted_base;</span>
<span class="p_add">+	sme_map_range_decrypted_wp(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		/* Add encrypted initrd (identity) mappings */</span>
<span class="p_add">+		ppd.paddr = initrd_start;</span>
<span class="p_add">+		ppd.vaddr = initrd_start;</span>
<span class="p_add">+		ppd.vaddr_end = initrd_end;</span>
<span class="p_add">+		sme_map_range_encrypted(&amp;ppd);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Add decrypted, write-protected initrd (non-identity) mappings</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ppd.paddr = initrd_start;</span>
<span class="p_add">+		ppd.vaddr = initrd_start + decrypted_base;</span>
<span class="p_add">+		ppd.vaddr_end = initrd_end + decrypted_base;</span>
<span class="p_add">+		sme_map_range_decrypted_wp(&amp;ppd);</span>
 	}
 
 	/* Add decrypted workarea mappings to both kernel mappings */
<span class="p_del">-	paddr = workarea_start;</span>
<span class="p_del">-	while (paddr &lt; workarea_end) {</span>
<span class="p_del">-		pgtable_area = sme_populate_pgd(pgd, pgtable_area,</span>
<span class="p_del">-						paddr,</span>
<span class="p_del">-						paddr + PMD_FLAGS);</span>
<span class="p_add">+	ppd.paddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end;</span>
<span class="p_add">+	sme_map_range_decrypted(&amp;ppd);</span>
 
<span class="p_del">-		pgtable_area = sme_populate_pgd(pgd, pgtable_area,</span>
<span class="p_del">-						paddr + decrypted_base,</span>
<span class="p_del">-						paddr + PMD_FLAGS);</span>
<span class="p_del">-</span>
<span class="p_del">-		paddr += PMD_PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	ppd.paddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr = workarea_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end + decrypted_base;</span>
<span class="p_add">+	sme_map_range_decrypted(&amp;ppd);</span>
 
 	/* Perform the encryption */
 	sme_encrypt_execute(kernel_start, kernel_start + decrypted_base,
<span class="p_del">-			    kernel_len, workarea_start, (unsigned long)pgd);</span>
<span class="p_add">+			    kernel_len, workarea_start, (unsigned long)ppd.pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (initrd_len)</span>
<span class="p_add">+		sme_encrypt_execute(initrd_start, initrd_start + decrypted_base,</span>
<span class="p_add">+				    initrd_len, workarea_start,</span>
<span class="p_add">+				    (unsigned long)ppd.pgd);</span>
 
 	/*
 	 * At this point we are running encrypted.  Remove the mappings for
 	 * the decrypted areas - all that is needed for this is to remove
 	 * the PGD entry/entries.
 	 */
<span class="p_del">-	sme_clear_pgd(pgd, kernel_start + decrypted_base,</span>
<span class="p_del">-		      kernel_end + decrypted_base);</span>
<span class="p_add">+	ppd.vaddr = kernel_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = kernel_end + decrypted_base;</span>
<span class="p_add">+	sme_clear_pgd(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		ppd.vaddr = initrd_start + decrypted_base;</span>
<span class="p_add">+		ppd.vaddr_end = initrd_end + decrypted_base;</span>
<span class="p_add">+		sme_clear_pgd(&amp;ppd);</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	sme_clear_pgd(pgd, workarea_start + decrypted_base,</span>
<span class="p_del">-		      workarea_end + decrypted_base);</span>
<span class="p_add">+	ppd.vaddr = workarea_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end + decrypted_base;</span>
<span class="p_add">+	sme_clear_pgd(&amp;ppd);</span>
 
 	/* Flush the TLB - no globals so cr3 is enough */
 	native_write_cr3(__native_read_cr3());
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt_boot.S b/arch/x86/mm/mem_encrypt_boot.S</span>
<span class="p_header">index 730e6d541df1..01f682cf77a8 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt_boot.S</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt_boot.S</span>
<span class="p_chunk">@@ -22,9 +22,9 @@</span> <span class="p_context"> ENTRY(sme_encrypt_execute)</span>
 
 	/*
 	 * Entry parameters:
<span class="p_del">-	 *   RDI - virtual address for the encrypted kernel mapping</span>
<span class="p_del">-	 *   RSI - virtual address for the decrypted kernel mapping</span>
<span class="p_del">-	 *   RDX - length of kernel</span>
<span class="p_add">+	 *   RDI - virtual address for the encrypted mapping</span>
<span class="p_add">+	 *   RSI - virtual address for the decrypted mapping</span>
<span class="p_add">+	 *   RDX - length to encrypt</span>
 	 *   RCX - virtual address of the encryption workarea, including:
 	 *     - stack page (PAGE_SIZE)
 	 *     - encryption routine page (PAGE_SIZE)
<span class="p_chunk">@@ -41,9 +41,9 @@</span> <span class="p_context"> ENTRY(sme_encrypt_execute)</span>
 	addq	$PAGE_SIZE, %rax	/* Workarea encryption routine */
 
 	push	%r12
<span class="p_del">-	movq	%rdi, %r10		/* Encrypted kernel */</span>
<span class="p_del">-	movq	%rsi, %r11		/* Decrypted kernel */</span>
<span class="p_del">-	movq	%rdx, %r12		/* Kernel length */</span>
<span class="p_add">+	movq	%rdi, %r10		/* Encrypted area */</span>
<span class="p_add">+	movq	%rsi, %r11		/* Decrypted area */</span>
<span class="p_add">+	movq	%rdx, %r12		/* Area length */</span>
 
 	/* Copy encryption routine into the workarea */
 	movq	%rax, %rdi				/* Workarea encryption routine */
<span class="p_chunk">@@ -52,10 +52,10 @@</span> <span class="p_context"> ENTRY(sme_encrypt_execute)</span>
 	rep	movsb
 
 	/* Setup registers for call */
<span class="p_del">-	movq	%r10, %rdi		/* Encrypted kernel */</span>
<span class="p_del">-	movq	%r11, %rsi		/* Decrypted kernel */</span>
<span class="p_add">+	movq	%r10, %rdi		/* Encrypted area */</span>
<span class="p_add">+	movq	%r11, %rsi		/* Decrypted area */</span>
 	movq	%r8, %rdx		/* Pagetables used for encryption */
<span class="p_del">-	movq	%r12, %rcx		/* Kernel length */</span>
<span class="p_add">+	movq	%r12, %rcx		/* Area length */</span>
 	movq	%rax, %r8		/* Workarea encryption routine */
 	addq	$PAGE_SIZE, %r8		/* Workarea intermediate copy buffer */
 
<span class="p_chunk">@@ -71,7 +71,7 @@</span> <span class="p_context"> ENDPROC(sme_encrypt_execute)</span>
 
 ENTRY(__enc_copy)
 /*
<span class="p_del">- * Routine used to encrypt kernel.</span>
<span class="p_add">+ * Routine used to encrypt memory in place.</span>
  *   This routine must be run outside of the kernel proper since
  *   the kernel will be encrypted during the process. So this
  *   routine is defined here and then copied to an area outside
<span class="p_chunk">@@ -79,19 +79,19 @@</span> <span class="p_context"> ENTRY(__enc_copy)</span>
  *   during execution.
  *
  *   On entry the registers must be:
<span class="p_del">- *     RDI - virtual address for the encrypted kernel mapping</span>
<span class="p_del">- *     RSI - virtual address for the decrypted kernel mapping</span>
<span class="p_add">+ *     RDI - virtual address for the encrypted mapping</span>
<span class="p_add">+ *     RSI - virtual address for the decrypted mapping</span>
  *     RDX - address of the pagetables to use for encryption
<span class="p_del">- *     RCX - length of kernel</span>
<span class="p_add">+ *     RCX - length of area</span>
  *      R8 - intermediate copy buffer
  *
  *     RAX - points to this routine
  *
<span class="p_del">- * The kernel will be encrypted by copying from the non-encrypted</span>
<span class="p_del">- * kernel space to an intermediate buffer and then copying from the</span>
<span class="p_del">- * intermediate buffer back to the encrypted kernel space. The physical</span>
<span class="p_del">- * addresses of the two kernel space mappings are the same which</span>
<span class="p_del">- * results in the kernel being encrypted &quot;in place&quot;.</span>
<span class="p_add">+ * The area will be encrypted by copying from the non-encrypted</span>
<span class="p_add">+ * memory space to an intermediate buffer and then copying from the</span>
<span class="p_add">+ * intermediate buffer back to the encrypted memory space. The physical</span>
<span class="p_add">+ * addresses of the two mappings are the same which results in the area</span>
<span class="p_add">+ * being encrypted &quot;in place&quot;.</span>
  */
 	/* Enable the new page tables */
 	mov	%rdx, %cr3
<span class="p_chunk">@@ -103,47 +103,55 @@</span> <span class="p_context"> ENTRY(__enc_copy)</span>
 	orq	$X86_CR4_PGE, %rdx
 	mov	%rdx, %cr4
 
<span class="p_add">+	push	%r15</span>
<span class="p_add">+	push	%r12</span>
<span class="p_add">+</span>
<span class="p_add">+	movq	%rcx, %r9		/* Save area length */</span>
<span class="p_add">+	movq	%rdi, %r10		/* Save encrypted area address */</span>
<span class="p_add">+	movq	%rsi, %r11		/* Save decrypted area address */</span>
<span class="p_add">+</span>
 	/* Set the PAT register PA5 entry to write-protect */
<span class="p_del">-	push	%rcx</span>
 	movl	$MSR_IA32_CR_PAT, %ecx
 	rdmsr
<span class="p_del">-	push	%rdx			/* Save original PAT value */</span>
<span class="p_add">+	mov	%rdx, %r15		/* Save original PAT value */</span>
 	andl	$0xffff00ff, %edx	/* Clear PA5 */
 	orl	$0x00000500, %edx	/* Set PA5 to WP */
 	wrmsr
<span class="p_del">-	pop	%rdx			/* RDX contains original PAT value */</span>
<span class="p_del">-	pop	%rcx</span>
<span class="p_del">-</span>
<span class="p_del">-	movq	%rcx, %r9		/* Save kernel length */</span>
<span class="p_del">-	movq	%rdi, %r10		/* Save encrypted kernel address */</span>
<span class="p_del">-	movq	%rsi, %r11		/* Save decrypted kernel address */</span>
 
 	wbinvd				/* Invalidate any cache entries */
 
<span class="p_del">-	/* Copy/encrypt 2MB at a time */</span>
<span class="p_add">+	/* Copy/encrypt up to 2MB at a time */</span>
<span class="p_add">+	movq	$PMD_PAGE_SIZE, %r12</span>
 1:
<span class="p_del">-	movq	%r11, %rsi		/* Source - decrypted kernel */</span>
<span class="p_add">+	cmpq	%r12, %r9</span>
<span class="p_add">+	jnb	2f</span>
<span class="p_add">+	movq	%r9, %r12</span>
<span class="p_add">+</span>
<span class="p_add">+2:</span>
<span class="p_add">+	movq	%r11, %rsi		/* Source - decrypted area */</span>
 	movq	%r8, %rdi		/* Dest   - intermediate copy buffer */
<span class="p_del">-	movq	$PMD_PAGE_SIZE, %rcx	/* 2MB length */</span>
<span class="p_add">+	movq	%r12, %rcx</span>
 	rep	movsb
 
 	movq	%r8, %rsi		/* Source - intermediate copy buffer */
<span class="p_del">-	movq	%r10, %rdi		/* Dest   - encrypted kernel */</span>
<span class="p_del">-	movq	$PMD_PAGE_SIZE, %rcx	/* 2MB length */</span>
<span class="p_add">+	movq	%r10, %rdi		/* Dest   - encrypted area */</span>
<span class="p_add">+	movq	%r12, %rcx</span>
 	rep	movsb
 
<span class="p_del">-	addq	$PMD_PAGE_SIZE, %r11</span>
<span class="p_del">-	addq	$PMD_PAGE_SIZE, %r10</span>
<span class="p_del">-	subq	$PMD_PAGE_SIZE, %r9	/* Kernel length decrement */</span>
<span class="p_add">+	addq	%r12, %r11</span>
<span class="p_add">+	addq	%r12, %r10</span>
<span class="p_add">+	subq	%r12, %r9		/* Kernel length decrement */</span>
 	jnz	1b			/* Kernel length not zero? */
 
 	/* Restore PAT register */
<span class="p_del">-	push	%rdx			/* Save original PAT value */</span>
 	movl	$MSR_IA32_CR_PAT, %ecx
 	rdmsr
<span class="p_del">-	pop	%rdx			/* Restore original PAT value */</span>
<span class="p_add">+	mov	%r15, %rdx		/* Restore original PAT value */</span>
 	wrmsr
 
<span class="p_add">+	pop	%r12</span>
<span class="p_add">+	pop	%r15</span>
<span class="p_add">+</span>
 	ret
 .L__enc_copy_end:
 ENDPROC(__enc_copy)
<span class="p_header">diff --git a/include/linux/crash_core.h b/include/linux/crash_core.h</span>
<span class="p_header">index 06097ef30449..b511f6d24b42 100644</span>
<span class="p_header">--- a/include/linux/crash_core.h</span>
<span class="p_header">+++ b/include/linux/crash_core.h</span>
<span class="p_chunk">@@ -42,6 +42,8 @@</span> <span class="p_context"> phys_addr_t paddr_vmcoreinfo_note(void);</span>
 	vmcoreinfo_append_str(&quot;PAGESIZE=%ld\n&quot;, value)
 #define VMCOREINFO_SYMBOL(name) \
 	vmcoreinfo_append_str(&quot;SYMBOL(%s)=%lx\n&quot;, #name, (unsigned long)&amp;name)
<span class="p_add">+#define VMCOREINFO_SYMBOL_ARRAY(name) \</span>
<span class="p_add">+	vmcoreinfo_append_str(&quot;SYMBOL(%s)=%lx\n&quot;, #name, (unsigned long)name)</span>
 #define VMCOREINFO_SIZE(name) \
 	vmcoreinfo_append_str(&quot;SIZE(%s)=%lu\n&quot;, #name, \
 			      (unsigned long)sizeof(name))
<span class="p_header">diff --git a/kernel/crash_core.c b/kernel/crash_core.c</span>
<span class="p_header">index b3663896278e..4f63597c824d 100644</span>
<span class="p_header">--- a/kernel/crash_core.c</span>
<span class="p_header">+++ b/kernel/crash_core.c</span>
<span class="p_chunk">@@ -410,7 +410,7 @@</span> <span class="p_context"> static int __init crash_save_vmcoreinfo_init(void)</span>
 	VMCOREINFO_SYMBOL(contig_page_data);
 #endif
 #ifdef CONFIG_SPARSEMEM
<span class="p_del">-	VMCOREINFO_SYMBOL(mem_section);</span>
<span class="p_add">+	VMCOREINFO_SYMBOL_ARRAY(mem_section);</span>
 	VMCOREINFO_LENGTH(mem_section, NR_SECTION_ROOTS);
 	VMCOREINFO_STRUCT_SIZE(mem_section);
 	VMCOREINFO_OFFSET(mem_section, section_mem_map);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



