
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,2/3] mm: Change the interface for __tlb_remove_page - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,2/3] mm: Change the interface for __tlb_remove_page</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 3, 2016, 1:09 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1464959359-7543-2-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9153047/mbox/"
   >mbox</a>
|
   <a href="/patch/9153047/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9153047/">/patch/9153047/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	A799A60751 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  3 Jun 2016 13:09:47 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 92E7326E39
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  3 Jun 2016 13:09:47 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 87AE128309; Fri,  3 Jun 2016 13:09:47 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D05A926E39
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  3 Jun 2016 13:09:46 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752822AbcFCNJm (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 3 Jun 2016 09:09:42 -0400
Received: from e37.co.us.ibm.com ([32.97.110.158]:60574 &quot;EHLO
	e37.co.us.ibm.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752197AbcFCNJl (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 3 Jun 2016 09:09:41 -0400
Received: from localhost
	by e37.co.us.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from
	&lt;aneesh.kumar@linux.vnet.ibm.com&gt;; Fri, 3 Jun 2016 07:09:39 -0600
Received: from d03dlp03.boulder.ibm.com (9.17.202.179)
	by e37.co.us.ibm.com (192.168.1.137) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Fri, 3 Jun 2016 07:09:37 -0600
X-IBM-Helo: d03dlp03.boulder.ibm.com
X-IBM-MailFrom: aneesh.kumar@linux.vnet.ibm.com
X-IBM-RcptTo: mpe@ellerman.id.au; benh@kernel.crashing.org;
	linux-mm@kvack.org; akpm@linux-foundation.org;
	linuxppc-dev@lists.ozlabs.org; paulus@samba.org;
	linux-kernel@vger.kernel.org
Received: from b03cxnp08025.gho.boulder.ibm.com
	(b03cxnp08025.gho.boulder.ibm.com [9.17.130.17])
	by d03dlp03.boulder.ibm.com (Postfix) with ESMTP id 3A92219D8073;
	Fri,  3 Jun 2016 07:09:16 -0600 (MDT)
Received: from b03ledav001.gho.boulder.ibm.com
	(b03ledav001.gho.boulder.ibm.com [9.17.130.232])
	by b03cxnp08025.gho.boulder.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id u53D9ZXl44499176; Fri, 3 Jun 2016 06:09:35 -0700
Received: from b03ledav001.gho.boulder.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id 6B3736E038;
	Fri,  3 Jun 2016 07:09:35 -0600 (MDT)
Received: from skywalker.in.ibm.com (unknown [9.124.212.153])
	by b03ledav001.gho.boulder.ibm.com (Postfix) with ESMTP id 32BBB6E035;
	Fri,  3 Jun 2016 07:09:31 -0600 (MDT)
From: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
To: akpm@linux-foundation.org, benh@kernel.crashing.org,
	paulus@samba.org, mpe@ellerman.id.au
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	linuxppc-dev@lists.ozlabs.org,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
Subject: [PATCH v2 2/3] mm: Change the interface for __tlb_remove_page
Date: Fri,  3 Jun 2016 18:39:18 +0530
Message-Id: &lt;1464959359-7543-2-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1464959359-7543-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
References: &lt;1464959359-7543-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-TM-AS-GCONF: 00
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 16060313-0025-0000-0000-0000417105B6
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - June 3, 2016, 1:09 p.m.</div>
<pre class="content">
This update the generic and arch specific implementation to return true
if we need to do a tlb flush. That means if a __tlb_remove_page indicate
a flush is needed, the page we try to remove need to be tracked and
added again after the flush. We need to track it because we have already
update the pte to none and we can&#39;t just loop back.

This changes is done to enable us to do a tlb_flush when we try to flush
a range that consists of different page sizes. For architectures like
ppc64, we can do a range based tlb flush and we need to track page size
for that. When we try to remove a huge page, we will force a tlb flush
and starts a new mmu gather.
<span class="signed-off-by">
Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
---
changes from V1:
* Fix build error

 arch/arm/include/asm/tlb.h  | 11 +++++++----
 arch/ia64/include/asm/tlb.h | 13 ++++++++-----
 arch/s390/include/asm/tlb.h |  4 ++--
 arch/sh/include/asm/tlb.h   |  2 +-
 arch/um/include/asm/tlb.h   |  2 +-
 include/asm-generic/tlb.h   | 36 +++++++++++++++++++++++++-----------
 mm/memory.c                 | 20 ++++++++++++++------
 7 files changed, 58 insertions(+), 30 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - June 3, 2016, 4:01 p.m.</div>
<pre class="content">
Hi,

[auto build test ERROR on arm/for-next]
[also build test ERROR on v4.7-rc1 next-20160603]
[if your patch is applied to the wrong git tree, please drop us a note to help improve the system]

url:    https://github.com/0day-ci/linux/commits/Aneesh-Kumar-K-V/mm-hugetlb-Simplify-hugetlb-unmap/20160603-211611
base:   http://repo.or.cz/linux-2.6/linux-2.6-arm.git for-next
config: sh-titan_defconfig (attached as .config)
compiler: sh4-linux-gnu-gcc (Debian 5.3.1-8) 5.3.1 20160205
reproduce:
        wget https://git.kernel.org/cgit/linux/kernel/git/wfg/lkp-tests.git/plain/sbin/make.cross -O ~/bin/make.cross
        chmod +x ~/bin/make.cross
        # save the attached .config to linux build tree
        make.cross ARCH=sh 

Note: the linux-review/Aneesh-Kumar-K-V/mm-hugetlb-Simplify-hugetlb-unmap/20160603-211611 HEAD d6ef168b95ef6261d48eff011e532d6a8798f4cd builds fine.
      It only hurts bisectibility.

All errors (new ones prefixed by &gt;&gt;):

   mm/memory.c: In function &#39;zap_pte_range&#39;:
<span class="quote">&gt;&gt; mm/memory.c:1210:4: error: implicit declaration of function &#39;__tlb_adjust_range&#39; [-Werror=implicit-function-declaration]</span>
       __tlb_adjust_range(tlb, tlb-&gt;addr);
       ^
<span class="quote">&gt;&gt; mm/memory.c:1210:31: error: &#39;struct mmu_gather&#39; has no member named &#39;addr&#39;</span>
       __tlb_adjust_range(tlb, tlb-&gt;addr);
                                  ^
   cc1: some warnings being treated as errors

vim +/__tlb_adjust_range +1210 mm/memory.c

  1204		 */
  1205		if (force_flush) {
  1206			force_flush = 0;
  1207			tlb_flush_mmu_free(tlb);
  1208			if (pending_page) {
  1209				/* remove the page with new size */
<span class="quote">&gt; 1210				__tlb_adjust_range(tlb, tlb-&gt;addr);</span>
  1211				__tlb_remove_page(tlb, pending_page);
  1212				pending_page = NULL;
  1213			}

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/include/asm/tlb.h b/arch/arm/include/asm/tlb.h</span>
<span class="p_header">index 3cadb726ec88..45dea952b0e6 100644</span>
<span class="p_header">--- a/arch/arm/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/arm/include/asm/tlb.h</span>
<span class="p_chunk">@@ -209,17 +209,20 @@</span> <span class="p_context"> tlb_end_vma(struct mmu_gather *tlb, struct vm_area_struct *vma)</span>
 		tlb_flush(tlb);
 }
 
<span class="p_del">-static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
<span class="p_add">+static inline bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 {
<span class="p_add">+	if (tlb-&gt;nr == tlb-&gt;max)</span>
<span class="p_add">+		return true;</span>
 	tlb-&gt;pages[tlb-&gt;nr++] = page;
<span class="p_del">-	VM_BUG_ON(tlb-&gt;nr &gt; tlb-&gt;max);</span>
<span class="p_del">-	return tlb-&gt;max - tlb-&gt;nr;</span>
<span class="p_add">+	return false;</span>
 }
 
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 {
<span class="p_del">-	if (!__tlb_remove_page(tlb, page))</span>
<span class="p_add">+	if (__tlb_remove_page(tlb, page)) {</span>
 		tlb_flush_mmu(tlb);
<span class="p_add">+		__tlb_remove_page(tlb, page);</span>
<span class="p_add">+	}</span>
 }
 
 static inline void __pte_free_tlb(struct mmu_gather *tlb, pgtable_t pte,
<span class="p_header">diff --git a/arch/ia64/include/asm/tlb.h b/arch/ia64/include/asm/tlb.h</span>
<span class="p_header">index 39d64e0df1de..85005ab513e9 100644</span>
<span class="p_header">--- a/arch/ia64/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/ia64/include/asm/tlb.h</span>
<span class="p_chunk">@@ -205,17 +205,18 @@</span> <span class="p_context"> tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long end)</span>
  * must be delayed until after the TLB has been flushed (see comments at the beginning of
  * this file).
  */
<span class="p_del">-static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
<span class="p_add">+static inline bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 {
<span class="p_add">+	if (tlb-&gt;nr == tlb-&gt;max)</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+</span>
 	tlb-&gt;need_flush = 1;
 
 	if (!tlb-&gt;nr &amp;&amp; tlb-&gt;pages == tlb-&gt;local)
 		__tlb_alloc_page(tlb);
 
 	tlb-&gt;pages[tlb-&gt;nr++] = page;
<span class="p_del">-	VM_BUG_ON(tlb-&gt;nr &gt; tlb-&gt;max);</span>
<span class="p_del">-</span>
<span class="p_del">-	return tlb-&gt;max - tlb-&gt;nr;</span>
<span class="p_add">+	return false;</span>
 }
 
 static inline void tlb_flush_mmu_tlbonly(struct mmu_gather *tlb)
<span class="p_chunk">@@ -235,8 +236,10 @@</span> <span class="p_context"> static inline void tlb_flush_mmu(struct mmu_gather *tlb)</span>
 
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 {
<span class="p_del">-	if (!__tlb_remove_page(tlb, page))</span>
<span class="p_add">+	if (__tlb_remove_page(tlb, page)) {</span>
 		tlb_flush_mmu(tlb);
<span class="p_add">+		__tlb_remove_page(tlb, page);</span>
<span class="p_add">+	}</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/s390/include/asm/tlb.h b/arch/s390/include/asm/tlb.h</span>
<span class="p_header">index 7a92e69c50bc..6b98cb3601d5 100644</span>
<span class="p_header">--- a/arch/s390/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/s390/include/asm/tlb.h</span>
<span class="p_chunk">@@ -87,10 +87,10 @@</span> <span class="p_context"> static inline void tlb_finish_mmu(struct mmu_gather *tlb,</span>
  * tlb_ptep_clear_flush. In both flush modes the tlb for a page cache page
  * has already been freed, so just do free_page_and_swap_cache.
  */
<span class="p_del">-static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
<span class="p_add">+static inline bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 {
 	free_page_and_swap_cache(page);
<span class="p_del">-	return 1; /* avoid calling tlb_flush_mmu */</span>
<span class="p_add">+	return false; /* avoid calling tlb_flush_mmu */</span>
 }
 
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
<span class="p_header">diff --git a/arch/sh/include/asm/tlb.h b/arch/sh/include/asm/tlb.h</span>
<span class="p_header">index 62f80d2a9df9..3dec5e0734f5 100644</span>
<span class="p_header">--- a/arch/sh/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/sh/include/asm/tlb.h</span>
<span class="p_chunk">@@ -101,7 +101,7 @@</span> <span class="p_context"> static inline void tlb_flush_mmu(struct mmu_gather *tlb)</span>
 static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)
 {
 	free_page_and_swap_cache(page);
<span class="p_del">-	return 1; /* avoid calling tlb_flush_mmu */</span>
<span class="p_add">+	return false; /* avoid calling tlb_flush_mmu */</span>
 }
 
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
<span class="p_header">diff --git a/arch/um/include/asm/tlb.h b/arch/um/include/asm/tlb.h</span>
<span class="p_header">index 16eb63fac57d..c6638f8e5e90 100644</span>
<span class="p_header">--- a/arch/um/include/asm/tlb.h</span>
<span class="p_header">+++ b/arch/um/include/asm/tlb.h</span>
<span class="p_chunk">@@ -102,7 +102,7 @@</span> <span class="p_context"> static inline int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 {
 	tlb-&gt;need_flush = 1;
 	free_page_and_swap_cache(page);
<span class="p_del">-	return 1; /* avoid calling tlb_flush_mmu */</span>
<span class="p_add">+	return false; /* avoid calling tlb_flush_mmu */</span>
 }
 
 static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)
<span class="p_header">diff --git a/include/asm-generic/tlb.h b/include/asm-generic/tlb.h</span>
<span class="p_header">index 9dbb739cafa0..9b9964b56415 100644</span>
<span class="p_header">--- a/include/asm-generic/tlb.h</span>
<span class="p_header">+++ b/include/asm-generic/tlb.h</span>
<span class="p_chunk">@@ -107,6 +107,11 @@</span> <span class="p_context"> struct mmu_gather {</span>
 	struct mmu_gather_batch	local;
 	struct page		*__pages[MMU_GATHER_BUNDLE];
 	unsigned int		batch_count;
<span class="p_add">+	/*</span>
<span class="p_add">+	 * __tlb_adjust_range  will track the new addr here,</span>
<span class="p_add">+	 * that that we can adjust the range after the flush</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	unsigned long addr;</span>
 };
 
 #define HAVE_GENERIC_MMU_GATHER
<span class="p_chunk">@@ -115,23 +120,19 @@</span> <span class="p_context"> void tlb_gather_mmu(struct mmu_gather *tlb, struct mm_struct *mm, unsigned long</span>
 void tlb_flush_mmu(struct mmu_gather *tlb);
 void tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start,
 							unsigned long end);
<span class="p_del">-int __tlb_remove_page(struct mmu_gather *tlb, struct page *page);</span>
<span class="p_del">-</span>
<span class="p_del">-/* tlb_remove_page</span>
<span class="p_del">- *	Similar to __tlb_remove_page but will call tlb_flush_mmu() itself when</span>
<span class="p_del">- *	required.</span>
<span class="p_del">- */</span>
<span class="p_del">-static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!__tlb_remove_page(tlb, page))</span>
<span class="p_del">-		tlb_flush_mmu(tlb);</span>
<span class="p_del">-}</span>
<span class="p_add">+bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page);</span>
 
 static inline void __tlb_adjust_range(struct mmu_gather *tlb,
 				      unsigned long address)
 {
 	tlb-&gt;start = min(tlb-&gt;start, address);
 	tlb-&gt;end = max(tlb-&gt;end, address + PAGE_SIZE);
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Track the last address with which we adjusted the range. This</span>
<span class="p_add">+	 * will be used later to adjust again after a mmu_flush due to</span>
<span class="p_add">+	 * failed __tlb_remove_page</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	tlb-&gt;addr = address;</span>
 }
 
 static inline void __tlb_reset_range(struct mmu_gather *tlb)
<span class="p_chunk">@@ -144,6 +145,19 @@</span> <span class="p_context"> static inline void __tlb_reset_range(struct mmu_gather *tlb)</span>
 	}
 }
 
<span class="p_add">+/* tlb_remove_page</span>
<span class="p_add">+ *	Similar to __tlb_remove_page but will call tlb_flush_mmu() itself when</span>
<span class="p_add">+ *	required.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (__tlb_remove_page(tlb, page)) {</span>
<span class="p_add">+		tlb_flush_mmu(tlb);</span>
<span class="p_add">+		__tlb_adjust_range(tlb, tlb-&gt;addr);</span>
<span class="p_add">+		__tlb_remove_page(tlb, page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * In the case of tlb vma handling, we can optimise these away in the
  * case where we&#39;re doing a full MM flush.  When we&#39;re doing a munmap,
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 15322b73636b..a01db5bc756b 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -292,23 +292,24 @@</span> <span class="p_context"> void tlb_finish_mmu(struct mmu_gather *tlb, unsigned long start, unsigned long e</span>
  *	handling the additional races in SMP caused by other CPUs caching valid
  *	mappings in their TLBs. Returns the number of free page slots left.
  *	When out of page slots we must call tlb_flush_mmu().
<span class="p_add">+ *returns true if the caller should flush.</span>
  */
<span class="p_del">-int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
<span class="p_add">+bool __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 {
 	struct mmu_gather_batch *batch;
 
 	VM_BUG_ON(!tlb-&gt;end);
 
 	batch = tlb-&gt;active;
<span class="p_del">-	batch-&gt;pages[batch-&gt;nr++] = page;</span>
 	if (batch-&gt;nr == batch-&gt;max) {
 		if (!tlb_next_batch(tlb))
<span class="p_del">-			return 0;</span>
<span class="p_add">+			return true;</span>
 		batch = tlb-&gt;active;
 	}
 	VM_BUG_ON_PAGE(batch-&gt;nr &gt; batch-&gt;max, page);
 
<span class="p_del">-	return batch-&gt;max - batch-&gt;nr;</span>
<span class="p_add">+	batch-&gt;pages[batch-&gt;nr++] = page;</span>
<span class="p_add">+	return false;</span>
 }
 
 #endif /* HAVE_GENERIC_MMU_GATHER */
<span class="p_chunk">@@ -1109,6 +1110,7 @@</span> <span class="p_context"> static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
 	pte_t *start_pte;
 	pte_t *pte;
 	swp_entry_t entry;
<span class="p_add">+	struct page *pending_page = NULL;</span>
 
 again:
 	init_rss_vec(rss);
<span class="p_chunk">@@ -1160,8 +1162,9 @@</span> <span class="p_context"> again:</span>
 			page_remove_rmap(page, false);
 			if (unlikely(page_mapcount(page) &lt; 0))
 				print_bad_pte(vma, addr, ptent, page);
<span class="p_del">-			if (unlikely(!__tlb_remove_page(tlb, page))) {</span>
<span class="p_add">+			if (unlikely(__tlb_remove_page(tlb, page))) {</span>
 				force_flush = 1;
<span class="p_add">+				pending_page = page;</span>
 				addr += PAGE_SIZE;
 				break;
 			}
<span class="p_chunk">@@ -1202,7 +1205,12 @@</span> <span class="p_context"> again:</span>
 	if (force_flush) {
 		force_flush = 0;
 		tlb_flush_mmu_free(tlb);
<span class="p_del">-</span>
<span class="p_add">+		if (pending_page) {</span>
<span class="p_add">+			/* remove the page with new size */</span>
<span class="p_add">+			__tlb_adjust_range(tlb, tlb-&gt;addr);</span>
<span class="p_add">+			__tlb_remove_page(tlb, pending_page);</span>
<span class="p_add">+			pending_page = NULL;</span>
<span class="p_add">+		}</span>
 		if (addr != end)
 			goto again;
 	}

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



