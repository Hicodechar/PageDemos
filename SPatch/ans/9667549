
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[8/8] x86/mm: Allow to have userspace mappings above 47-bits - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [8/8] x86/mm: Allow to have userspace mappings above 47-bits</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 6, 2017, 2:01 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170406140106.78087-9-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9667549/mbox/"
   >mbox</a>
|
   <a href="/patch/9667549/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9667549/">/patch/9667549/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	361C06021C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  6 Apr 2017 14:02:17 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5735A2521E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  6 Apr 2017 14:02:17 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 4977726E78; Thu,  6 Apr 2017 14:02:17 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4E20E2521E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  6 Apr 2017 14:02:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S935026AbdDFOCH (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 6 Apr 2017 10:02:07 -0400
Received: from mga14.intel.com ([192.55.52.115]:4663 &quot;EHLO mga14.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S934996AbdDFOBt (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 6 Apr 2017 10:01:49 -0400
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
	d=intel.com; i=@intel.com; q=dns/txt; s=intel;
	t=1491487309; x=1523023309;
	h=from:to:cc:subject:date:message-id:in-reply-to: references;
	bh=ZYYG63ZB3qd2KPyo7KUSaCh5ic1/lC6DEjIldh04OYs=;
	b=uIwpci22dumd7lO5MdQg2PwmMrRAUp5zr0ii64XVRjJKiClSU2U1ntB/
	tDWy038ynhXIoU4tdQaQzSKMOAAPPQ==;
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
	by fmsmga103.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	06 Apr 2017 07:01:48 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.37,160,1488873600&quot;; d=&quot;scan&#39;208&quot;;a=&quot;1131901665&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by fmsmga001.fm.intel.com with ESMTP; 06 Apr 2017 07:01:45 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 14842108; Thu,  6 Apr 2017 17:01:10 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, x86@kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;
Cc: Andi Kleen &lt;ak@linux.intel.com&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andy Lutomirski &lt;luto@amacapital.net&gt;,
	linux-arch@vger.kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;
Subject: [PATCH 8/8] x86/mm: Allow to have userspace mappings above 47-bits
Date: Thu,  6 Apr 2017 17:01:06 +0300
Message-Id: &lt;20170406140106.78087-9-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170406140106.78087-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20170406140106.78087-1-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - April 6, 2017, 2:01 p.m.</div>
<pre class="content">
On x86, 5-level paging enables 56-bit userspace virtual address space.
Not all user space is ready to handle wide addresses. It&#39;s known that
at least some JIT compilers use higher bits in pointers to encode their
information. It collides with valid pointers with 5-level paging and
leads to crashes.

To mitigate this, we are not going to allocate virtual address space
above 47-bit by default.

But userspace can ask for allocation from full address space by
specifying hint address (with or without MAP_FIXED) above 47-bits.

If hint address set above 47-bit, but MAP_FIXED is not specified, we try
to look for unmapped area by specified address. If it&#39;s already
occupied, we look for unmapped area in *full* address space, rather than
from 47-bit window.

This approach helps to easily make application&#39;s memory allocator aware
about large address space without manually tracking allocated virtual
address space.

One important case we need to handle here is interaction with MPX.
MPX (without MAWA( extension cannot handle addresses above 47-bit, so we
need to make sure that MPX cannot be enabled we already have VMA above
the boundary and forbid creating such VMAs once MPX is enabled.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
Cc: Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;
---
 arch/x86/include/asm/elf.h       |  2 +-
 arch/x86/include/asm/mpx.h       |  9 +++++++++
 arch/x86/include/asm/processor.h |  9 ++++++---
 arch/x86/kernel/sys_x86_64.c     | 28 +++++++++++++++++++++++++++-
 arch/x86/mm/hugetlbpage.c        | 27 ++++++++++++++++++++++++---
 arch/x86/mm/mmap.c               |  2 +-
 arch/x86/mm/mpx.c                | 33 ++++++++++++++++++++++++++++++++-
 7 files changed, 100 insertions(+), 10 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153181">Dmitry Safonov</a> - April 6, 2017, 6:43 p.m.</div>
<pre class="content">
Hi Kirill,

On 04/06/2017 05:01 PM, Kirill A. Shutemov wrote:
<span class="quote">&gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt; leads to crashes.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt; above 47-bit by default.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; But userspace can ask for allocation from full address space by</span>
<span class="quote">&gt; specifying hint address (with or without MAP_FIXED) above 47-bits.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; If hint address set above 47-bit, but MAP_FIXED is not specified, we try</span>
<span class="quote">&gt; to look for unmapped area by specified address. If it&#39;s already</span>
<span class="quote">&gt; occupied, we look for unmapped area in *full* address space, rather than</span>
<span class="quote">&gt; from 47-bit window.</span>

Do you wish after the first over-47-bit mapping the following mmap()
calls return also over-47-bits if there is free space?
It so, you could simplify all this code by changing only mm-&gt;mmap_base
on the first over-47-bit mmap() call.
This will do simple trick.
<span class="quote">
&gt;</span>
<span class="quote">&gt; This approach helps to easily make application&#39;s memory allocator aware</span>
<span class="quote">&gt; about large address space without manually tracking allocated virtual</span>
<span class="quote">&gt; address space.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; One important case we need to handle here is interaction with MPX.</span>
<span class="quote">&gt; MPX (without MAWA( extension cannot handle addresses above 47-bit, so we</span>
<span class="quote">&gt; need to make sure that MPX cannot be enabled we already have VMA above</span>
<span class="quote">&gt; the boundary and forbid creating such VMAs once MPX is enabled.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; Cc: Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/elf.h       |  2 +-</span>
<span class="quote">&gt;  arch/x86/include/asm/mpx.h       |  9 +++++++++</span>
<span class="quote">&gt;  arch/x86/include/asm/processor.h |  9 ++++++---</span>
<span class="quote">&gt;  arch/x86/kernel/sys_x86_64.c     | 28 +++++++++++++++++++++++++++-</span>
<span class="quote">&gt;  arch/x86/mm/hugetlbpage.c        | 27 ++++++++++++++++++++++++---</span>
<span class="quote">&gt;  arch/x86/mm/mmap.c               |  2 +-</span>
<span class="quote">&gt;  arch/x86/mm/mpx.c                | 33 ++++++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;  7 files changed, 100 insertions(+), 10 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h</span>
<span class="quote">&gt; index d4d3ed456cb7..67260dbe1688 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/elf.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/elf.h</span>
<span class="quote">&gt; @@ -250,7 +250,7 @@ extern int force_personality32;</span>
<span class="quote">&gt;     the loader.  We need to make sure that it is out of the way of the program</span>
<span class="quote">&gt;     that it will &quot;exec&quot;, and that there is sufficient room for the brk.  */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -#define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)</span>
<span class="quote">&gt; +#define ELF_ET_DYN_BASE		(DEFAULT_MAP_WINDOW / 3 * 2)</span>

This will kill 32-bit userspace:
As DEFAULT_MAP_WINDOW is defined as what previously was TASK_SIZE_MAX,
not TASK_SIZE, for ia32/x32 ELF_ET_DYN_BASE will be over 4Gb.

Here is the test:
[root@localhost test]# cat hello-world.c
#include &lt;stdio.h&gt;

int main(int argc, char **argv)
{
	printf(&quot;Maybe this world is another planet&#39;s hell.\n&quot;);
         return 0;
}
[root@localhost test]# gcc -m32 hello-world.c -o hello-world
[root@localhost test]# ./hello-world
[   35.306726] hello-world[1948]: segfault at ffa5288c ip 
00000000f77b5a82 sp 00000000ffa52890 error 6 in ld-2.23.so[f77b5000+23000]
Segmentation fault (core dumped)

So, dynamic base should differ between 32/64-bits as it was with TASK_SIZE.
<span class="quote">

&gt;</span>
<span class="quote">&gt;  /* This yields a mask that user programs can use to figure out what</span>
<span class="quote">&gt;     instruction set this CPU supports.  This could be done in user space,</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/mpx.h b/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt; index a0d662be4c5b..7d7404756bb4 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt; @@ -73,6 +73,9 @@ static inline void mpx_mm_init(struct mm_struct *mm)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		      unsigned long start, unsigned long end);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="quote">&gt; +		unsigned long flags);</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline siginfo_t *mpx_generate_siginfo(struct pt_regs *regs)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -94,6 +97,12 @@ static inline void mpx_notify_unmap(struct mm_struct *mm,</span>
<span class="quote">&gt;  				    unsigned long start, unsigned long end)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline unsigned long mpx_unmapped_area_check(unsigned long addr,</span>
<span class="quote">&gt; +		unsigned long len, unsigned long flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return addr;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  #endif /* CONFIG_X86_INTEL_MPX */</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #endif /* _ASM_X86_MPX_H */</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="quote">&gt; index 3cada998a402..9f437aea7f57 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/processor.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/processor.h</span>
<span class="quote">&gt; @@ -795,6 +795,7 @@ static inline void spin_lock_prefetch(const void *x)</span>
<span class="quote">&gt;  #define IA32_PAGE_OFFSET	PAGE_OFFSET</span>
<span class="quote">&gt;  #define TASK_SIZE		PAGE_OFFSET</span>
<span class="quote">&gt;  #define TASK_SIZE_MAX		TASK_SIZE</span>
<span class="quote">&gt; +#define DEFAULT_MAP_WINDOW	TASK_SIZE</span>
<span class="quote">&gt;  #define STACK_TOP		TASK_SIZE</span>
<span class="quote">&gt;  #define STACK_TOP_MAX		STACK_TOP</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -834,7 +835,9 @@ static inline void spin_lock_prefetch(const void *x)</span>
<span class="quote">&gt;   * particular problem by preventing anything from being mapped</span>
<span class="quote">&gt;   * at the maximum canonical address.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -#define TASK_SIZE_MAX	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="quote">&gt; +#define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define DEFAULT_MAP_WINDOW	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /* This decides where the kernel will search for a free chunk of vm</span>
<span class="quote">&gt;   * space during mmap&#39;s.</span>
<span class="quote">&gt; @@ -847,7 +850,7 @@ static inline void spin_lock_prefetch(const void *x)</span>
<span class="quote">&gt;  #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \</span>
<span class="quote">&gt;  					IA32_PAGE_OFFSET : TASK_SIZE_MAX)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -#define STACK_TOP		TASK_SIZE</span>
<span class="quote">&gt; +#define STACK_TOP		DEFAULT_MAP_WINDOW</span>
<span class="quote">&gt;  #define STACK_TOP_MAX		TASK_SIZE_MAX</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #define INIT_THREAD  {						\</span>
<span class="quote">&gt; @@ -870,7 +873,7 @@ extern void start_thread(struct pt_regs *regs, unsigned long new_ip,</span>
<span class="quote">&gt;   * space during mmap&#39;s.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #define __TASK_UNMAPPED_BASE(task_size)	(PAGE_ALIGN(task_size / 3))</span>
<span class="quote">&gt; -#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE)</span>
<span class="quote">&gt; +#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(DEFAULT_MAP_WINDOW)</span>

ditto
<span class="quote">
&gt;</span>
<span class="quote">&gt;  #define KSTK_EIP(task)		(task_pt_regs(task)-&gt;ip)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="quote">&gt; index 207b8f2582c7..593a31e93812 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="quote">&gt; @@ -21,6 +21,7 @@</span>
<span class="quote">&gt;  #include &lt;asm/compat.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/ia32.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/syscalls.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mpx.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Align a virtual address to avoid aliasing in the I$ on AMD F15h.</span>
<span class="quote">&gt; @@ -132,6 +133,10 @@ arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
<span class="quote">&gt;  	struct vm_unmapped_area_info info;</span>
<span class="quote">&gt;  	unsigned long begin, end;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt; +	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (flags &amp; MAP_FIXED)</span>
<span class="quote">&gt;  		return addr;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -151,7 +156,16 @@ arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
<span class="quote">&gt;  	info.flags = 0;</span>
<span class="quote">&gt;  	info.length = len;</span>
<span class="quote">&gt;  	info.low_limit = begin;</span>
<span class="quote">&gt; -	info.high_limit = end;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; +	 * in the full address space.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; +		info.high_limit = min(end, TASK_SIZE);</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		info.high_limit = min(end, DEFAULT_MAP_WINDOW);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	info.align_mask = 0;</span>
<span class="quote">&gt;  	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	if (filp) {</span>
<span class="quote">&gt; @@ -171,6 +185,10 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt;  	unsigned long addr = addr0;</span>
<span class="quote">&gt;  	struct vm_unmapped_area_info info;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt; +	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/* requested length too big for entire address space */</span>
<span class="quote">&gt;  	if (len &gt; TASK_SIZE)</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt; @@ -195,6 +213,14 @@ arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
<span class="quote">&gt;  	info.length = len;</span>
<span class="quote">&gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt;  	info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; +	 * in the full address space.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt; +		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>

Hmm, TASK_SIZE depends now on TIF_ADDR32, which is set during exec().
That means for ia32/x32 ELF which has TASK_SIZE &lt; 4Gb as TIF_ADDR32
is set, which can do 64-bit syscalls - the subtraction will be
a negative..
<span class="quote">

&gt; +</span>
<span class="quote">&gt;  	info.align_mask = 0;</span>
<span class="quote">&gt;  	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;  	if (filp) {</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="quote">&gt; index 302f43fd9c28..9a0b89252c52 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/hugetlbpage.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="quote">&gt; @@ -18,6 +18,7 @@</span>
<span class="quote">&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt;  #include &lt;asm/elf.h&gt;</span>
<span class="quote">&gt; +#include &lt;asm/mpx.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  #if 0	/* This is just for testing */</span>
<span class="quote">&gt;  struct page *</span>
<span class="quote">&gt; @@ -87,23 +88,38 @@ static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
<span class="quote">&gt;  	info.low_limit = get_mmap_base(1);</span>
<span class="quote">&gt;  	info.high_limit = in_compat_syscall() ?</span>
<span class="quote">&gt;  		tasksize_32bit() : tasksize_64bit();</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; +	 * in the full address space.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; +		info.high_limit = TASK_SIZE;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);</span>
<span class="quote">&gt;  	info.align_offset = 0;</span>
<span class="quote">&gt;  	return vm_unmapped_area(&amp;info);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
<span class="quote">&gt; -		unsigned long addr0, unsigned long len,</span>
<span class="quote">&gt; +		unsigned long addr, unsigned long len,</span>
<span class="quote">&gt;  		unsigned long pgoff, unsigned long flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct hstate *h = hstate_file(file);</span>
<span class="quote">&gt;  	struct vm_unmapped_area_info info;</span>
<span class="quote">&gt; -	unsigned long addr;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  	info.flags = VM_UNMAPPED_AREA_TOPDOWN;</span>
<span class="quote">&gt;  	info.length = len;</span>
<span class="quote">&gt;  	info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt;  	info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="quote">&gt; +	 * in the full address space.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt; +		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>

ditto
<span class="quote">
&gt; +</span>
<span class="quote">&gt;  	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);</span>
<span class="quote">&gt;  	info.align_offset = 0;</span>
<span class="quote">&gt;  	addr = vm_unmapped_area(&amp;info);</span>
<span class="quote">&gt; @@ -118,7 +134,7 @@ static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
<span class="quote">&gt;  		VM_BUG_ON(addr != -ENOMEM);</span>
<span class="quote">&gt;  		info.flags = 0;</span>
<span class="quote">&gt;  		info.low_limit = TASK_UNMAPPED_BASE;</span>
<span class="quote">&gt; -		info.high_limit = TASK_SIZE;</span>
<span class="quote">&gt; +		info.high_limit = DEFAULT_MAP_WINDOW;</span>

ditto about 32-bits
<span class="quote">
&gt;  		addr = vm_unmapped_area(&amp;info);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; @@ -135,6 +151,11 @@ hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  	if (len &amp; ~huge_page_mask(h))</span>
<span class="quote">&gt;  		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt; +	if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	if (len &gt; TASK_SIZE)</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c</span>
<span class="quote">&gt; index 19ad095b41df..d63232a31945 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/mmap.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/mmap.c</span>
<span class="quote">&gt; @@ -44,7 +44,7 @@ unsigned long tasksize_32bit(void)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  unsigned long tasksize_64bit(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return TASK_SIZE_MAX;</span>
<span class="quote">&gt; +	return DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  static unsigned long stack_maxrandom_size(unsigned long task_size)</span>
<span class="quote">&gt; diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="quote">&gt; index cd44ae727df7..a26a1b373fd0 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/mpx.c</span>
<span class="quote">&gt; +++ b/arch/x86/mm/mpx.c</span>
<span class="quote">&gt; @@ -355,10 +355,19 @@ int mpx_enable_management(void)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	bd_base = mpx_get_bounds_dir();</span>
<span class="quote">&gt;  	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* MPX doesn&#39;t support addresses above 47-bits yet. */</span>
<span class="quote">&gt; +	if (find_vma(mm, DEFAULT_MAP_WINDOW)) {</span>
<span class="quote">&gt; +		pr_warn_once(&quot;%s (%d): MPX cannot handle addresses &quot;</span>
<span class="quote">&gt; +				&quot;above 47-bits. Disabling.&quot;,</span>
<span class="quote">&gt; +				current-&gt;comm, current-&gt;pid);</span>
<span class="quote">&gt; +		ret = -ENXIO;</span>
<span class="quote">&gt; +		goto out;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	mm-&gt;context.bd_addr = bd_base;</span>
<span class="quote">&gt;  	if (mm-&gt;context.bd_addr == MPX_INVALID_BOUNDS_DIR)</span>
<span class="quote">&gt;  		ret = -ENXIO;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; +out:</span>
<span class="quote">&gt;  	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -1038,3 +1047,25 @@ void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	if (ret)</span>
<span class="quote">&gt;  		force_sig(SIGSEGV, current);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* MPX cannot handle addresses above 47-bits yet. */</span>
<span class="quote">&gt; +unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="quote">&gt; +		unsigned long flags)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (!kernel_managing_mpx_tables(current-&gt;mm))</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +	if (addr + len &lt;= DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; +		return addr;</span>
<span class="quote">&gt; +	if (flags &amp; MAP_FIXED)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Requested len is larger than whole area we&#39;re allowed to map in.</span>
<span class="quote">&gt; +	 * Resetting hinting address wouldn&#39;t do much good -- fail early.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (len &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* Look for unmap area within DEFAULT_MAP_WINDOW */</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153181">Dmitry Safonov</a> - April 6, 2017, 7:15 p.m.</div>
<pre class="content">
On 04/06/2017 09:43 PM, Dmitry Safonov wrote:
<span class="quote">&gt; Hi Kirill,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On 04/06/2017 05:01 PM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt;&gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt;&gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt;&gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt;&gt; leads to crashes.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt;&gt; above 47-bit by default.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; But userspace can ask for allocation from full address space by</span>
<span class="quote">&gt;&gt; specifying hint address (with or without MAP_FIXED) above 47-bits.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; If hint address set above 47-bit, but MAP_FIXED is not specified, we try</span>
<span class="quote">&gt;&gt; to look for unmapped area by specified address. If it&#39;s already</span>
<span class="quote">&gt;&gt; occupied, we look for unmapped area in *full* address space, rather than</span>
<span class="quote">&gt;&gt; from 47-bit window.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Do you wish after the first over-47-bit mapping the following mmap()</span>
<span class="quote">&gt; calls return also over-47-bits if there is free space?</span>
<span class="quote">&gt; It so, you could simplify all this code by changing only mm-&gt;mmap_base</span>
<span class="quote">&gt; on the first over-47-bit mmap() call.</span>
<span class="quote">&gt; This will do simple trick.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This approach helps to easily make application&#39;s memory allocator aware</span>
<span class="quote">&gt;&gt; about large address space without manually tracking allocated virtual</span>
<span class="quote">&gt;&gt; address space.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; One important case we need to handle here is interaction with MPX.</span>
<span class="quote">&gt;&gt; MPX (without MAWA( extension cannot handle addresses above 47-bit, so we</span>
<span class="quote">&gt;&gt; need to make sure that MPX cannot be enabled we already have VMA above</span>
<span class="quote">&gt;&gt; the boundary and forbid creating such VMAs once MPX is enabled.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt; Cc: Dmitry Safonov &lt;dsafonov@virtuozzo.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  arch/x86/include/asm/elf.h       |  2 +-</span>
<span class="quote">&gt;&gt;  arch/x86/include/asm/mpx.h       |  9 +++++++++</span>
<span class="quote">&gt;&gt;  arch/x86/include/asm/processor.h |  9 ++++++---</span>
<span class="quote">&gt;&gt;  arch/x86/kernel/sys_x86_64.c     | 28 +++++++++++++++++++++++++++-</span>
<span class="quote">&gt;&gt;  arch/x86/mm/hugetlbpage.c        | 27 ++++++++++++++++++++++++---</span>
<span class="quote">&gt;&gt;  arch/x86/mm/mmap.c               |  2 +-</span>
<span class="quote">&gt;&gt;  arch/x86/mm/mpx.c                | 33 ++++++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;&gt;  7 files changed, 100 insertions(+), 10 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h</span>
<span class="quote">&gt;&gt; index d4d3ed456cb7..67260dbe1688 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/include/asm/elf.h</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/include/asm/elf.h</span>
<span class="quote">&gt;&gt; @@ -250,7 +250,7 @@ extern int force_personality32;</span>
<span class="quote">&gt;&gt;     the loader.  We need to make sure that it is out of the way of the</span>
<span class="quote">&gt;&gt; program</span>
<span class="quote">&gt;&gt;     that it will &quot;exec&quot;, and that there is sufficient room for the</span>
<span class="quote">&gt;&gt; brk.  */</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -#define ELF_ET_DYN_BASE        (TASK_SIZE / 3 * 2)</span>
<span class="quote">&gt;&gt; +#define ELF_ET_DYN_BASE        (DEFAULT_MAP_WINDOW / 3 * 2)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This will kill 32-bit userspace:</span>
<span class="quote">&gt; As DEFAULT_MAP_WINDOW is defined as what previously was TASK_SIZE_MAX,</span>
<span class="quote">&gt; not TASK_SIZE, for ia32/x32 ELF_ET_DYN_BASE will be over 4Gb.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Here is the test:</span>
<span class="quote">&gt; [root@localhost test]# cat hello-world.c</span>
<span class="quote">&gt; #include &lt;stdio.h&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; int main(int argc, char **argv)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;     printf(&quot;Maybe this world is another planet&#39;s hell.\n&quot;);</span>
<span class="quote">&gt;         return 0;</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt; [root@localhost test]# gcc -m32 hello-world.c -o hello-world</span>
<span class="quote">&gt; [root@localhost test]# ./hello-world</span>
<span class="quote">&gt; [   35.306726] hello-world[1948]: segfault at ffa5288c ip</span>
<span class="quote">&gt; 00000000f77b5a82 sp 00000000ffa52890 error 6 in ld-2.23.so[f77b5000+23000]</span>
<span class="quote">&gt; Segmentation fault (core dumped)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So, dynamic base should differ between 32/64-bits as it was with TASK_SIZE.</span>

I just tried to define it like this:
-#define DEFAULT_MAP_WINDOW     ((1UL &lt;&lt; 47) - PAGE_SIZE)
+#define DEFAULT_MAP_WINDOW     (test_thread_flag(TIF_ADDR32) ?         \
+                               IA32_PAGE_OFFSET : ((1UL &lt;&lt; 47) - 
PAGE_SIZE))

And it looks working better.
<span class="quote">
&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  /* This yields a mask that user programs can use to figure out what</span>
<span class="quote">&gt;&gt;     instruction set this CPU supports.  This could be done in user space,</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/include/asm/mpx.h b/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt;&gt; index a0d662be4c5b..7d7404756bb4 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/include/asm/mpx.h</span>
<span class="quote">&gt;&gt; @@ -73,6 +73,9 @@ static inline void mpx_mm_init(struct mm_struct *mm)</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;  void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;                unsigned long start, unsigned long end);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned</span>
<span class="quote">&gt;&gt; long len,</span>
<span class="quote">&gt;&gt; +        unsigned long flags);</span>
<span class="quote">&gt;&gt;  #else</span>
<span class="quote">&gt;&gt;  static inline siginfo_t *mpx_generate_siginfo(struct pt_regs *regs)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; @@ -94,6 +97,12 @@ static inline void mpx_notify_unmap(struct</span>
<span class="quote">&gt;&gt; mm_struct *mm,</span>
<span class="quote">&gt;&gt;                      unsigned long start, unsigned long end)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static inline unsigned long mpx_unmapped_area_check(unsigned long addr,</span>
<span class="quote">&gt;&gt; +        unsigned long len, unsigned long flags)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    return addr;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt;  #endif /* CONFIG_X86_INTEL_MPX */</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #endif /* _ASM_X86_MPX_H */</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/include/asm/processor.h</span>
<span class="quote">&gt;&gt; b/arch/x86/include/asm/processor.h</span>
<span class="quote">&gt;&gt; index 3cada998a402..9f437aea7f57 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/include/asm/processor.h</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/include/asm/processor.h</span>
<span class="quote">&gt;&gt; @@ -795,6 +795,7 @@ static inline void spin_lock_prefetch(const void *x)</span>
<span class="quote">&gt;&gt;  #define IA32_PAGE_OFFSET    PAGE_OFFSET</span>
<span class="quote">&gt;&gt;  #define TASK_SIZE        PAGE_OFFSET</span>
<span class="quote">&gt;&gt;  #define TASK_SIZE_MAX        TASK_SIZE</span>
<span class="quote">&gt;&gt; +#define DEFAULT_MAP_WINDOW    TASK_SIZE</span>
<span class="quote">&gt;&gt;  #define STACK_TOP        TASK_SIZE</span>
<span class="quote">&gt;&gt;  #define STACK_TOP_MAX        STACK_TOP</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -834,7 +835,9 @@ static inline void spin_lock_prefetch(const void *x)</span>
<span class="quote">&gt;&gt;   * particular problem by preventing anything from being mapped</span>
<span class="quote">&gt;&gt;   * at the maximum canonical address.</span>
<span class="quote">&gt;&gt;   */</span>
<span class="quote">&gt;&gt; -#define TASK_SIZE_MAX    ((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="quote">&gt;&gt; +#define TASK_SIZE_MAX    ((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +#define DEFAULT_MAP_WINDOW    ((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  /* This decides where the kernel will search for a free chunk of vm</span>
<span class="quote">&gt;&gt;   * space during mmap&#39;s.</span>
<span class="quote">&gt;&gt; @@ -847,7 +850,7 @@ static inline void spin_lock_prefetch(const void *x)</span>
<span class="quote">&gt;&gt;  #define TASK_SIZE_OF(child)    ((test_tsk_thread_flag(child,</span>
<span class="quote">&gt;&gt; TIF_ADDR32)) ? \</span>
<span class="quote">&gt;&gt;                      IA32_PAGE_OFFSET : TASK_SIZE_MAX)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -#define STACK_TOP        TASK_SIZE</span>
<span class="quote">&gt;&gt; +#define STACK_TOP        DEFAULT_MAP_WINDOW</span>
<span class="quote">&gt;&gt;  #define STACK_TOP_MAX        TASK_SIZE_MAX</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #define INIT_THREAD  {                        \</span>
<span class="quote">&gt;&gt; @@ -870,7 +873,7 @@ extern void start_thread(struct pt_regs *regs,</span>
<span class="quote">&gt;&gt; unsigned long new_ip,</span>
<span class="quote">&gt;&gt;   * space during mmap&#39;s.</span>
<span class="quote">&gt;&gt;   */</span>
<span class="quote">&gt;&gt;  #define __TASK_UNMAPPED_BASE(task_size)    (PAGE_ALIGN(task_size / 3))</span>
<span class="quote">&gt;&gt; -#define TASK_UNMAPPED_BASE        __TASK_UNMAPPED_BASE(TASK_SIZE)</span>
<span class="quote">&gt;&gt; +#define TASK_UNMAPPED_BASE</span>
<span class="quote">&gt;&gt; __TASK_UNMAPPED_BASE(DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ditto</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #define KSTK_EIP(task)        (task_pt_regs(task)-&gt;ip)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="quote">&gt;&gt; index 207b8f2582c7..593a31e93812 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="quote">&gt;&gt; @@ -21,6 +21,7 @@</span>
<span class="quote">&gt;&gt;  #include &lt;asm/compat.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;asm/ia32.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;asm/syscalls.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/mpx.h&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  /*</span>
<span class="quote">&gt;&gt;   * Align a virtual address to avoid aliasing in the I$ on AMD F15h.</span>
<span class="quote">&gt;&gt; @@ -132,6 +133,10 @@ arch_get_unmapped_area(struct file *filp,</span>
<span class="quote">&gt;&gt; unsigned long addr,</span>
<span class="quote">&gt;&gt;      struct vm_unmapped_area_info info;</span>
<span class="quote">&gt;&gt;      unsigned long begin, end;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +    addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt;&gt; +    if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt;&gt; +        return addr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;      if (flags &amp; MAP_FIXED)</span>
<span class="quote">&gt;&gt;          return addr;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -151,7 +156,16 @@ arch_get_unmapped_area(struct file *filp,</span>
<span class="quote">&gt;&gt; unsigned long addr,</span>
<span class="quote">&gt;&gt;      info.flags = 0;</span>
<span class="quote">&gt;&gt;      info.length = len;</span>
<span class="quote">&gt;&gt;      info.low_limit = begin;</span>
<span class="quote">&gt;&gt; -    info.high_limit = end;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /*</span>
<span class="quote">&gt;&gt; +     * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped</span>
<span class="quote">&gt;&gt; area</span>
<span class="quote">&gt;&gt; +     * in the full address space.</span>
<span class="quote">&gt;&gt; +     */</span>
<span class="quote">&gt;&gt; +    if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt;&gt; +        info.high_limit = min(end, TASK_SIZE);</span>
<span class="quote">&gt;&gt; +    else</span>
<span class="quote">&gt;&gt; +        info.high_limit = min(end, DEFAULT_MAP_WINDOW);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;      info.align_mask = 0;</span>
<span class="quote">&gt;&gt;      info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;&gt;      if (filp) {</span>
<span class="quote">&gt;&gt; @@ -171,6 +185,10 @@ arch_get_unmapped_area_topdown(struct file *filp,</span>
<span class="quote">&gt;&gt; const unsigned long addr0,</span>
<span class="quote">&gt;&gt;      unsigned long addr = addr0;</span>
<span class="quote">&gt;&gt;      struct vm_unmapped_area_info info;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +    addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt;&gt; +    if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt;&gt; +        return addr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;      /* requested length too big for entire address space */</span>
<span class="quote">&gt;&gt;      if (len &gt; TASK_SIZE)</span>
<span class="quote">&gt;&gt;          return -ENOMEM;</span>
<span class="quote">&gt;&gt; @@ -195,6 +213,14 @@ arch_get_unmapped_area_topdown(struct file *filp,</span>
<span class="quote">&gt;&gt; const unsigned long addr0,</span>
<span class="quote">&gt;&gt;      info.length = len;</span>
<span class="quote">&gt;&gt;      info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt;&gt;      info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /*</span>
<span class="quote">&gt;&gt; +     * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped</span>
<span class="quote">&gt;&gt; area</span>
<span class="quote">&gt;&gt; +     * in the full address space.</span>
<span class="quote">&gt;&gt; +     */</span>
<span class="quote">&gt;&gt; +    if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt;&gt; +        info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hmm, TASK_SIZE depends now on TIF_ADDR32, which is set during exec().</span>
<span class="quote">&gt; That means for ia32/x32 ELF which has TASK_SIZE &lt; 4Gb as TIF_ADDR32</span>
<span class="quote">&gt; is set, which can do 64-bit syscalls - the subtraction will be</span>
<span class="quote">&gt; a negative..</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;      info.align_mask = 0;</span>
<span class="quote">&gt;&gt;      info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;</span>
<span class="quote">&gt;&gt;      if (filp) {</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="quote">&gt;&gt; index 302f43fd9c28..9a0b89252c52 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/mm/hugetlbpage.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="quote">&gt;&gt; @@ -18,6 +18,7 @@</span>
<span class="quote">&gt;&gt;  #include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;asm/pgalloc.h&gt;</span>
<span class="quote">&gt;&gt;  #include &lt;asm/elf.h&gt;</span>
<span class="quote">&gt;&gt; +#include &lt;asm/mpx.h&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  #if 0    /* This is just for testing */</span>
<span class="quote">&gt;&gt;  struct page *</span>
<span class="quote">&gt;&gt; @@ -87,23 +88,38 @@ static unsigned long</span>
<span class="quote">&gt;&gt; hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
<span class="quote">&gt;&gt;      info.low_limit = get_mmap_base(1);</span>
<span class="quote">&gt;&gt;      info.high_limit = in_compat_syscall() ?</span>
<span class="quote">&gt;&gt;          tasksize_32bit() : tasksize_64bit();</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /*</span>
<span class="quote">&gt;&gt; +     * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped</span>
<span class="quote">&gt;&gt; area</span>
<span class="quote">&gt;&gt; +     * in the full address space.</span>
<span class="quote">&gt;&gt; +     */</span>
<span class="quote">&gt;&gt; +    if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt;&gt; +        info.high_limit = TASK_SIZE;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;      info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);</span>
<span class="quote">&gt;&gt;      info.align_offset = 0;</span>
<span class="quote">&gt;&gt;      return vm_unmapped_area(&amp;info);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  static unsigned long hugetlb_get_unmapped_area_topdown(struct file</span>
<span class="quote">&gt;&gt; *file,</span>
<span class="quote">&gt;&gt; -        unsigned long addr0, unsigned long len,</span>
<span class="quote">&gt;&gt; +        unsigned long addr, unsigned long len,</span>
<span class="quote">&gt;&gt;          unsigned long pgoff, unsigned long flags)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;      struct hstate *h = hstate_file(file);</span>
<span class="quote">&gt;&gt;      struct vm_unmapped_area_info info;</span>
<span class="quote">&gt;&gt; -    unsigned long addr;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;      info.flags = VM_UNMAPPED_AREA_TOPDOWN;</span>
<span class="quote">&gt;&gt;      info.length = len;</span>
<span class="quote">&gt;&gt;      info.low_limit = PAGE_SIZE;</span>
<span class="quote">&gt;&gt;      info.high_limit = get_mmap_base(0);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /*</span>
<span class="quote">&gt;&gt; +     * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped</span>
<span class="quote">&gt;&gt; area</span>
<span class="quote">&gt;&gt; +     * in the full address space.</span>
<span class="quote">&gt;&gt; +     */</span>
<span class="quote">&gt;&gt; +    if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt;&gt; +        info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ditto</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;      info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);</span>
<span class="quote">&gt;&gt;      info.align_offset = 0;</span>
<span class="quote">&gt;&gt;      addr = vm_unmapped_area(&amp;info);</span>
<span class="quote">&gt;&gt; @@ -118,7 +134,7 @@ static unsigned long</span>
<span class="quote">&gt;&gt; hugetlb_get_unmapped_area_topdown(struct file *file,</span>
<span class="quote">&gt;&gt;          VM_BUG_ON(addr != -ENOMEM);</span>
<span class="quote">&gt;&gt;          info.flags = 0;</span>
<span class="quote">&gt;&gt;          info.low_limit = TASK_UNMAPPED_BASE;</span>
<span class="quote">&gt;&gt; -        info.high_limit = TASK_SIZE;</span>
<span class="quote">&gt;&gt; +        info.high_limit = DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; ditto about 32-bits</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;          addr = vm_unmapped_area(&amp;info);</span>
<span class="quote">&gt;&gt;      }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; @@ -135,6 +151,11 @@ hugetlb_get_unmapped_area(struct file *file,</span>
<span class="quote">&gt;&gt; unsigned long addr,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;      if (len &amp; ~huge_page_mask(h))</span>
<span class="quote">&gt;&gt;          return -EINVAL;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="quote">&gt;&gt; +    if (IS_ERR_VALUE(addr))</span>
<span class="quote">&gt;&gt; +        return addr;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;      if (len &gt; TASK_SIZE)</span>
<span class="quote">&gt;&gt;          return -ENOMEM;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c</span>
<span class="quote">&gt;&gt; index 19ad095b41df..d63232a31945 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/mm/mmap.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/mm/mmap.c</span>
<span class="quote">&gt;&gt; @@ -44,7 +44,7 @@ unsigned long tasksize_32bit(void)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  unsigned long tasksize_64bit(void)</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt; -    return TASK_SIZE_MAX;</span>
<span class="quote">&gt;&gt; +    return DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;  static unsigned long stack_maxrandom_size(unsigned long task_size)</span>
<span class="quote">&gt;&gt; diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="quote">&gt;&gt; index cd44ae727df7..a26a1b373fd0 100644</span>
<span class="quote">&gt;&gt; --- a/arch/x86/mm/mpx.c</span>
<span class="quote">&gt;&gt; +++ b/arch/x86/mm/mpx.c</span>
<span class="quote">&gt;&gt; @@ -355,10 +355,19 @@ int mpx_enable_management(void)</span>
<span class="quote">&gt;&gt;       */</span>
<span class="quote">&gt;&gt;      bd_base = mpx_get_bounds_dir();</span>
<span class="quote">&gt;&gt;      down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /* MPX doesn&#39;t support addresses above 47-bits yet. */</span>
<span class="quote">&gt;&gt; +    if (find_vma(mm, DEFAULT_MAP_WINDOW)) {</span>
<span class="quote">&gt;&gt; +        pr_warn_once(&quot;%s (%d): MPX cannot handle addresses &quot;</span>
<span class="quote">&gt;&gt; +                &quot;above 47-bits. Disabling.&quot;,</span>
<span class="quote">&gt;&gt; +                current-&gt;comm, current-&gt;pid);</span>
<span class="quote">&gt;&gt; +        ret = -ENXIO;</span>
<span class="quote">&gt;&gt; +        goto out;</span>
<span class="quote">&gt;&gt; +    }</span>
<span class="quote">&gt;&gt;      mm-&gt;context.bd_addr = bd_base;</span>
<span class="quote">&gt;&gt;      if (mm-&gt;context.bd_addr == MPX_INVALID_BOUNDS_DIR)</span>
<span class="quote">&gt;&gt;          ret = -ENXIO;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; +out:</span>
<span class="quote">&gt;&gt;      up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt;&gt;      return ret;</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; @@ -1038,3 +1047,25 @@ void mpx_notify_unmap(struct mm_struct *mm,</span>
<span class="quote">&gt;&gt; struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;      if (ret)</span>
<span class="quote">&gt;&gt;          force_sig(SIGSEGV, current);</span>
<span class="quote">&gt;&gt;  }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +/* MPX cannot handle addresses above 47-bits yet. */</span>
<span class="quote">&gt;&gt; +unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned</span>
<span class="quote">&gt;&gt; long len,</span>
<span class="quote">&gt;&gt; +        unsigned long flags)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    if (!kernel_managing_mpx_tables(current-&gt;mm))</span>
<span class="quote">&gt;&gt; +        return addr;</span>
<span class="quote">&gt;&gt; +    if (addr + len &lt;= DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt;&gt; +        return addr;</span>
<span class="quote">&gt;&gt; +    if (flags &amp; MAP_FIXED)</span>
<span class="quote">&gt;&gt; +        return -ENOMEM;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /*</span>
<span class="quote">&gt;&gt; +     * Requested len is larger than whole area we&#39;re allowed to map in.</span>
<span class="quote">&gt;&gt; +     * Resetting hinting address wouldn&#39;t do much good -- fail early.</span>
<span class="quote">&gt;&gt; +     */</span>
<span class="quote">&gt;&gt; +    if (len &gt; DEFAULT_MAP_WINDOW)</span>
<span class="quote">&gt;&gt; +        return -ENOMEM;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /* Look for unmap area within DEFAULT_MAP_WINDOW */</span>
<span class="quote">&gt;&gt; +    return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - April 6, 2017, 11:21 p.m.</div>
<pre class="content">
On Thu, Apr 06, 2017 at 10:15:47PM +0300, Dmitry Safonov wrote:
<span class="quote">&gt; On 04/06/2017 09:43 PM, Dmitry Safonov wrote:</span>
<span class="quote">&gt; &gt; Hi Kirill,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On 04/06/2017 05:01 PM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; &gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt; &gt; &gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt; &gt; &gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt; &gt; &gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt; &gt; &gt; leads to crashes.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt; &gt; &gt; above 47-bit by default.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; But userspace can ask for allocation from full address space by</span>
<span class="quote">&gt; &gt; &gt; specifying hint address (with or without MAP_FIXED) above 47-bits.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; If hint address set above 47-bit, but MAP_FIXED is not specified, we try</span>
<span class="quote">&gt; &gt; &gt; to look for unmapped area by specified address. If it&#39;s already</span>
<span class="quote">&gt; &gt; &gt; occupied, we look for unmapped area in *full* address space, rather than</span>
<span class="quote">&gt; &gt; &gt; from 47-bit window.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Do you wish after the first over-47-bit mapping the following mmap()</span>
<span class="quote">&gt; &gt; calls return also over-47-bits if there is free space?</span>
<span class="quote">&gt; &gt; It so, you could simplify all this code by changing only mm-&gt;mmap_base</span>
<span class="quote">&gt; &gt; on the first over-47-bit mmap() call.</span>
<span class="quote">&gt; &gt; This will do simple trick.</span>

No.

I want every allocation to explicitely opt-in large address space. It&#39;s
additional fail-safe: if a library can&#39;t handle large addresses it has
better chance to survive if its own allocation will stay within 47-bits.
<span class="quote">
&gt; I just tried to define it like this:</span>
<span class="quote">&gt; -#define DEFAULT_MAP_WINDOW     ((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="quote">&gt; +#define DEFAULT_MAP_WINDOW     (test_thread_flag(TIF_ADDR32) ?         \</span>
<span class="quote">&gt; +                               IA32_PAGE_OFFSET : ((1UL &lt;&lt; 47) -</span>
<span class="quote">&gt; PAGE_SIZE))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And it looks working better.</span>

Okay, thanks. I&#39;ll send v2.
<span class="quote">
&gt; &gt; &gt; +    if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt; &gt; &gt; +        info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hmm, TASK_SIZE depends now on TIF_ADDR32, which is set during exec().</span>
<span class="quote">&gt; &gt; That means for ia32/x32 ELF which has TASK_SIZE &lt; 4Gb as TIF_ADDR32</span>
<span class="quote">&gt; &gt; is set, which can do 64-bit syscalls - the subtraction will be</span>
<span class="quote">&gt; &gt; a negative..</span>

With your proposed change to DEFAULT_MAP_WINDOW difinition it should be
okay, right?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=153181">Dmitry Safonov</a> - April 7, 2017, 10:06 a.m.</div>
<pre class="content">
On 04/07/2017 02:21 AM, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Thu, Apr 06, 2017 at 10:15:47PM +0300, Dmitry Safonov wrote:</span>
<span class="quote">&gt;&gt; On 04/06/2017 09:43 PM, Dmitry Safonov wrote:</span>
<span class="quote">&gt;&gt;&gt; Hi Kirill,</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; On 04/06/2017 05:01 PM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt;&gt;&gt;&gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt;&gt;&gt;&gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt;&gt;&gt;&gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt;&gt;&gt;&gt; leads to crashes.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt;&gt;&gt;&gt; above 47-bit by default.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; But userspace can ask for allocation from full address space by</span>
<span class="quote">&gt;&gt;&gt;&gt; specifying hint address (with or without MAP_FIXED) above 47-bits.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; If hint address set above 47-bit, but MAP_FIXED is not specified, we try</span>
<span class="quote">&gt;&gt;&gt;&gt; to look for unmapped area by specified address. If it&#39;s already</span>
<span class="quote">&gt;&gt;&gt;&gt; occupied, we look for unmapped area in *full* address space, rather than</span>
<span class="quote">&gt;&gt;&gt;&gt; from 47-bit window.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Do you wish after the first over-47-bit mapping the following mmap()</span>
<span class="quote">&gt;&gt;&gt; calls return also over-47-bits if there is free space?</span>
<span class="quote">&gt;&gt;&gt; It so, you could simplify all this code by changing only mm-&gt;mmap_base</span>
<span class="quote">&gt;&gt;&gt; on the first over-47-bit mmap() call.</span>
<span class="quote">&gt;&gt;&gt; This will do simple trick.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; No.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I want every allocation to explicitely opt-in large address space. It&#39;s</span>
<span class="quote">&gt; additional fail-safe: if a library can&#39;t handle large addresses it has</span>
<span class="quote">&gt; better chance to survive if its own allocation will stay within 47-bits.</span>

Ok
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; I just tried to define it like this:</span>
<span class="quote">&gt;&gt; -#define DEFAULT_MAP_WINDOW     ((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="quote">&gt;&gt; +#define DEFAULT_MAP_WINDOW     (test_thread_flag(TIF_ADDR32) ?         \</span>
<span class="quote">&gt;&gt; +                               IA32_PAGE_OFFSET : ((1UL &lt;&lt; 47) -</span>
<span class="quote">&gt;&gt; PAGE_SIZE))</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; And it looks working better.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Okay, thanks. I&#39;ll send v2.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +    if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="quote">&gt;&gt;&gt;&gt; +        info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Hmm, TASK_SIZE depends now on TIF_ADDR32, which is set during exec().</span>
<span class="quote">&gt;&gt;&gt; That means for ia32/x32 ELF which has TASK_SIZE &lt; 4Gb as TIF_ADDR32</span>
<span class="quote">&gt;&gt;&gt; is set, which can do 64-bit syscalls - the subtraction will be</span>
<span class="quote">&gt;&gt;&gt; a negative..</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; With your proposed change to DEFAULT_MAP_WINDOW difinition it should be</span>
<span class="quote">&gt; okay, right?</span>

I&#39;ll comment to v2 to keep all in one place.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36302">Anshuman Khandual</a> - April 7, 2017, 1:35 p.m.</div>
<pre class="content">
On 04/06/2017 07:31 PM, Kirill A. Shutemov wrote:
<span class="quote">&gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt; leads to crashes.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt; above 47-bit by default.</span>

I am wondering if the commitment of virtual space range to the
user space is kind of an API which needs to be maintained there
after. If that is the case then we need to have some plans when
increasing it from the current level.

Will those JIT compilers keep using the higher bit positions of
the pointer for ever ? Then it will limit the ability of the
kernel to expand the virtual address range later as well. I am
not saying we should not increase till the extent it does not
affect any *known* user but then we should not increase twice
for now, create the hint mechanism to be passed from the user
to avail beyond that (which will settle in as a expectation
from the kernel later on). Do the same thing again while
expanding the address range next time around. I think we need
to have a plan for this and particularly around &#39;hint&#39; mechanism
and whether it should be decided per mmap() request or at the
task level.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - April 7, 2017, 3:59 p.m.</div>
<pre class="content">
On Fri, Apr 07, 2017 at 07:05:26PM +0530, Anshuman Khandual wrote:
<span class="quote">&gt; On 04/06/2017 07:31 PM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt; &gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt; &gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt; &gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt; &gt; leads to crashes.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt; &gt; above 47-bit by default.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am wondering if the commitment of virtual space range to the</span>
<span class="quote">&gt; user space is kind of an API which needs to be maintained there</span>
<span class="quote">&gt; after. If that is the case then we need to have some plans when</span>
<span class="quote">&gt; increasing it from the current level.</span>

I don&#39;t think we should ever enable full address space for all
applications. There&#39;s no point.

/bin/true doesn&#39;t need more than 64TB of virtual memory.
And I hope never will.

By increasing virtual address space for everybody we will pay (assuming
current page table format) at least one extra page per process for moving
stack at very end of address space.

Yes, you can gain something in security by having more bits for ASLR, but
I don&#39;t think it worth the cost.
<span class="quote">
&gt; Will those JIT compilers keep using the higher bit positions of</span>
<span class="quote">&gt; the pointer for ever ? Then it will limit the ability of the</span>
<span class="quote">&gt; kernel to expand the virtual address range later as well. I am</span>
<span class="quote">&gt; not saying we should not increase till the extent it does not</span>
<span class="quote">&gt; affect any *known* user but then we should not increase twice</span>
<span class="quote">&gt; for now, create the hint mechanism to be passed from the user</span>
<span class="quote">&gt; to avail beyond that (which will settle in as a expectation</span>
<span class="quote">&gt; from the kernel later on). Do the same thing again while</span>
<span class="quote">&gt; expanding the address range next time around. I think we need</span>
<span class="quote">&gt; to have a plan for this and particularly around &#39;hint&#39; mechanism</span>
<span class="quote">&gt; and whether it should be decided per mmap() request or at the</span>
<span class="quote">&gt; task level.</span>

I think the reasonable way for an application to claim it&#39;s 63-bit clean
is to make allocations with (void *)-1 as hint address.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=42">H. Peter Anvin</a> - April 7, 2017, 4:09 p.m.</div>
<pre class="content">
On April 7, 2017 8:59:45 AM PDT, &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; wrote:
<span class="quote">&gt;On Fri, Apr 07, 2017 at 07:05:26PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt; On 04/06/2017 07:31 PM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt; &gt; On x86, 5-level paging enables 56-bit userspace virtual address</span>
<span class="quote">&gt;space.</span>
<span class="quote">&gt;&gt; &gt; Not all user space is ready to handle wide addresses. It&#39;s known</span>
<span class="quote">&gt;that</span>
<span class="quote">&gt;&gt; &gt; at least some JIT compilers use higher bits in pointers to encode</span>
<span class="quote">&gt;their</span>
<span class="quote">&gt;&gt; &gt; information. It collides with valid pointers with 5-level paging</span>
<span class="quote">&gt;and</span>
<span class="quote">&gt;&gt; &gt; leads to crashes.</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; To mitigate this, we are not going to allocate virtual address</span>
<span class="quote">&gt;space</span>
<span class="quote">&gt;&gt; &gt; above 47-bit by default.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I am wondering if the commitment of virtual space range to the</span>
<span class="quote">&gt;&gt; user space is kind of an API which needs to be maintained there</span>
<span class="quote">&gt;&gt; after. If that is the case then we need to have some plans when</span>
<span class="quote">&gt;&gt; increasing it from the current level.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;I don&#39;t think we should ever enable full address space for all</span>
<span class="quote">&gt;applications. There&#39;s no point.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;/bin/true doesn&#39;t need more than 64TB of virtual memory.</span>
<span class="quote">&gt;And I hope never will.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;By increasing virtual address space for everybody we will pay (assuming</span>
<span class="quote">&gt;current page table format) at least one extra page per process for</span>
<span class="quote">&gt;moving</span>
<span class="quote">&gt;stack at very end of address space.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;Yes, you can gain something in security by having more bits for ASLR,</span>
<span class="quote">&gt;but</span>
<span class="quote">&gt;I don&#39;t think it worth the cost.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Will those JIT compilers keep using the higher bit positions of</span>
<span class="quote">&gt;&gt; the pointer for ever ? Then it will limit the ability of the</span>
<span class="quote">&gt;&gt; kernel to expand the virtual address range later as well. I am</span>
<span class="quote">&gt;&gt; not saying we should not increase till the extent it does not</span>
<span class="quote">&gt;&gt; affect any *known* user but then we should not increase twice</span>
<span class="quote">&gt;&gt; for now, create the hint mechanism to be passed from the user</span>
<span class="quote">&gt;&gt; to avail beyond that (which will settle in as a expectation</span>
<span class="quote">&gt;&gt; from the kernel later on). Do the same thing again while</span>
<span class="quote">&gt;&gt; expanding the address range next time around. I think we need</span>
<span class="quote">&gt;&gt; to have a plan for this and particularly around &#39;hint&#39; mechanism</span>
<span class="quote">&gt;&gt; and whether it should be decided per mmap() request or at the</span>
<span class="quote">&gt;&gt; task level.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;I think the reasonable way for an application to claim it&#39;s 63-bit</span>
<span class="quote">&gt;clean</span>
<span class="quote">&gt;is to make allocations with (void *)-1 as hint address.</span>

You realize that people have said that about just about every memory threshold from 64K onward?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - April 7, 2017, 4:20 p.m.</div>
<pre class="content">
On Fri, Apr 07, 2017 at 09:09:27AM -0700, hpa@zytor.com wrote:
<span class="quote">&gt; &gt;I think the reasonable way for an application to claim it&#39;s 63-bit</span>
<span class="quote">&gt; &gt;clean</span>
<span class="quote">&gt; &gt;is to make allocations with (void *)-1 as hint address.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You realize that people have said that about just about every memory</span>

Any better solution?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=81661">Michael Ellerman</a> - April 12, 2017, 10:41 a.m.</div>
<pre class="content">
Hi Kirill,

I&#39;m interested in this because we&#39;re doing pretty much the same thing on
powerpc at the moment, and I want to make sure x86 &amp; powerpc end up with
compatible behaviour.

&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; writes:
<span class="quote">&gt; On Fri, Apr 07, 2017 at 07:05:26PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt;&gt; On 04/06/2017 07:31 PM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt; &gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt;&gt; &gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt;&gt; &gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt;&gt; &gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt;&gt; &gt; leads to crashes.</span>
<span class="quote">&gt;&gt; &gt; </span>
<span class="quote">&gt;&gt; &gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt;&gt; &gt; above 47-bit by default.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I am wondering if the commitment of virtual space range to the</span>
<span class="quote">&gt;&gt; user space is kind of an API which needs to be maintained there</span>
<span class="quote">&gt;&gt; after. If that is the case then we need to have some plans when</span>
<span class="quote">&gt;&gt; increasing it from the current level.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I don&#39;t think we should ever enable full address space for all</span>
<span class="quote">&gt; applications. There&#39;s no point.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; /bin/true doesn&#39;t need more than 64TB of virtual memory.</span>
<span class="quote">&gt; And I hope never will.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; By increasing virtual address space for everybody we will pay (assuming</span>
<span class="quote">&gt; current page table format) at least one extra page per process for moving</span>
<span class="quote">&gt; stack at very end of address space.</span>

That assumes the current layout though, it could be different.
<span class="quote">
&gt; Yes, you can gain something in security by having more bits for ASLR, but</span>
<span class="quote">&gt; I don&#39;t think it worth the cost.</span>

It may not be worth the cost now, for you, but that trade off will be
different for other people and at other times.

So I think it&#39;s quite likely some folks will be interested in the full
address range for ASLR.
<span class="quote">
&gt;&gt; expanding the address range next time around. I think we need</span>
<span class="quote">&gt;&gt; to have a plan for this and particularly around &#39;hint&#39; mechanism</span>
<span class="quote">&gt;&gt; and whether it should be decided per mmap() request or at the</span>
<span class="quote">&gt;&gt; task level.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I think the reasonable way for an application to claim it&#39;s 63-bit clean</span>
<span class="quote">&gt; is to make allocations with (void *)-1 as hint address.</span>

I do like the simplicity of that.

But I wouldn&#39;t be surprised if some (crappy) code out there already
passes an address of -1. Probably it won&#39;t break if it starts getting
high addresses, but who knows.

An alternative would be to only interpret the hint as requesting a large
address if it&#39;s &gt;= 64TB &amp;&amp; &lt; TASK_SIZE_MAX.

If we&#39;re really worried about breaking userspace then a new MMAP flag
seems like the safest option?

I don&#39;t feel particularly strongly about any option, but like I said my
main concern is that x86 &amp; powerpc end up with the same behaviour.

And whatever we end up with someone will need to do an update to the man
page for mmap.

cheers
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - April 12, 2017, 11:11 a.m.</div>
<pre class="content">
On Wed, Apr 12, 2017 at 08:41:29PM +1000, Michael Ellerman wrote:
<span class="quote">&gt; Hi Kirill,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m interested in this because we&#39;re doing pretty much the same thing on</span>
<span class="quote">&gt; powerpc at the moment, and I want to make sure x86 &amp; powerpc end up with</span>
<span class="quote">&gt; compatible behaviour.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt; writes:</span>
<span class="quote">&gt; &gt; On Fri, Apr 07, 2017 at 07:05:26PM +0530, Anshuman Khandual wrote:</span>
<span class="quote">&gt; &gt;&gt; On 04/06/2017 07:31 PM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt; On x86, 5-level paging enables 56-bit userspace virtual address space.</span>
<span class="quote">&gt; &gt;&gt; &gt; Not all user space is ready to handle wide addresses. It&#39;s known that</span>
<span class="quote">&gt; &gt;&gt; &gt; at least some JIT compilers use higher bits in pointers to encode their</span>
<span class="quote">&gt; &gt;&gt; &gt; information. It collides with valid pointers with 5-level paging and</span>
<span class="quote">&gt; &gt;&gt; &gt; leads to crashes.</span>
<span class="quote">&gt; &gt;&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; &gt; To mitigate this, we are not going to allocate virtual address space</span>
<span class="quote">&gt; &gt;&gt; &gt; above 47-bit by default.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; I am wondering if the commitment of virtual space range to the</span>
<span class="quote">&gt; &gt;&gt; user space is kind of an API which needs to be maintained there</span>
<span class="quote">&gt; &gt;&gt; after. If that is the case then we need to have some plans when</span>
<span class="quote">&gt; &gt;&gt; increasing it from the current level.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I don&#39;t think we should ever enable full address space for all</span>
<span class="quote">&gt; &gt; applications. There&#39;s no point.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; /bin/true doesn&#39;t need more than 64TB of virtual memory.</span>
<span class="quote">&gt; &gt; And I hope never will.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; By increasing virtual address space for everybody we will pay (assuming</span>
<span class="quote">&gt; &gt; current page table format) at least one extra page per process for moving</span>
<span class="quote">&gt; &gt; stack at very end of address space.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That assumes the current layout though, it could be different.</span>

True.
<span class="quote">
&gt; &gt; Yes, you can gain something in security by having more bits for ASLR, but</span>
<span class="quote">&gt; &gt; I don&#39;t think it worth the cost.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It may not be worth the cost now, for you, but that trade off will be</span>
<span class="quote">&gt; different for other people and at other times.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I think it&#39;s quite likely some folks will be interested in the full</span>
<span class="quote">&gt; address range for ASLR.</span>

We always can extend interface if/when userspace demand materialize.

Let&#39;s not invent interfaces unless we&#39;re sure there&#39;s demand.
<span class="quote">
&gt; &gt;&gt; expanding the address range next time around. I think we need</span>
<span class="quote">&gt; &gt;&gt; to have a plan for this and particularly around &#39;hint&#39; mechanism</span>
<span class="quote">&gt; &gt;&gt; and whether it should be decided per mmap() request or at the</span>
<span class="quote">&gt; &gt;&gt; task level.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I think the reasonable way for an application to claim it&#39;s 63-bit clean</span>
<span class="quote">&gt; &gt; is to make allocations with (void *)-1 as hint address.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I do like the simplicity of that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But I wouldn&#39;t be surprised if some (crappy) code out there already</span>
<span class="quote">&gt; passes an address of -1. Probably it won&#39;t break if it starts getting</span>
<span class="quote">&gt; high addresses, but who knows.</span>

To make an application break we need two thing:

 - it sets hint address to -1 by mistake;
 - it uses upper bit to encode its info;

I would be surprise if such combination exists in real world.

But let me know if you have any particular code in mind.
<span class="quote">
&gt; An alternative would be to only interpret the hint as requesting a large</span>
<span class="quote">&gt; address if it&#39;s &gt;= 64TB &amp;&amp; &lt; TASK_SIZE_MAX.</span>

Nope. That doesn&#39;t work if you take into accounting further extension of the
address space.

Consider extension x86 to 6-level page tables. User-space has 63-bit
address space. TASK_SIZE_MAX is bumped to (1UL &lt;&lt; 63) - PAGE_SIZE.

An application wants access to full address space. It gets recompiled
using new TASK_SIZE_MAX as hint address. And everything works fine.

But only on machine with 6-level paging enabled.

If we run the same application binary on machine with older kernel and
5-level paging, the application will get access to only 47-bit address
space, not 56-bit, as hint address is more than TASK_SIZE_MAX in this
configuration.
<span class="quote">
&gt; If we&#39;re really worried about breaking userspace then a new MMAP flag</span>
<span class="quote">&gt; seems like the safest option?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t feel particularly strongly about any option, but like I said my</span>
<span class="quote">&gt; main concern is that x86 &amp; powerpc end up with the same behaviour.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And whatever we end up with someone will need to do an update to the man</span>
<span class="quote">&gt; page for mmap.</span>

Sure.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/elf.h b/arch/x86/include/asm/elf.h</span>
<span class="p_header">index d4d3ed456cb7..67260dbe1688 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/elf.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/elf.h</span>
<span class="p_chunk">@@ -250,7 +250,7 @@</span> <span class="p_context"> extern int force_personality32;</span>
    the loader.  We need to make sure that it is out of the way of the program
    that it will &quot;exec&quot;, and that there is sufficient room for the brk.  */
 
<span class="p_del">-#define ELF_ET_DYN_BASE		(TASK_SIZE / 3 * 2)</span>
<span class="p_add">+#define ELF_ET_DYN_BASE		(DEFAULT_MAP_WINDOW / 3 * 2)</span>
 
 /* This yields a mask that user programs can use to figure out what
    instruction set this CPU supports.  This could be done in user space,
<span class="p_header">diff --git a/arch/x86/include/asm/mpx.h b/arch/x86/include/asm/mpx.h</span>
<span class="p_header">index a0d662be4c5b..7d7404756bb4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mpx.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mpx.h</span>
<span class="p_chunk">@@ -73,6 +73,9 @@</span> <span class="p_context"> static inline void mpx_mm_init(struct mm_struct *mm)</span>
 }
 void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,
 		      unsigned long start, unsigned long end);
<span class="p_add">+</span>
<span class="p_add">+unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		unsigned long flags);</span>
 #else
 static inline siginfo_t *mpx_generate_siginfo(struct pt_regs *regs)
 {
<span class="p_chunk">@@ -94,6 +97,12 @@</span> <span class="p_context"> static inline void mpx_notify_unmap(struct mm_struct *mm,</span>
 				    unsigned long start, unsigned long end)
 {
 }
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long mpx_unmapped_area_check(unsigned long addr,</span>
<span class="p_add">+		unsigned long len, unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return addr;</span>
<span class="p_add">+}</span>
 #endif /* CONFIG_X86_INTEL_MPX */
 
 #endif /* _ASM_X86_MPX_H */
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 3cada998a402..9f437aea7f57 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -795,6 +795,7 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define IA32_PAGE_OFFSET	PAGE_OFFSET
 #define TASK_SIZE		PAGE_OFFSET
 #define TASK_SIZE_MAX		TASK_SIZE
<span class="p_add">+#define DEFAULT_MAP_WINDOW	TASK_SIZE</span>
 #define STACK_TOP		TASK_SIZE
 #define STACK_TOP_MAX		STACK_TOP
 
<span class="p_chunk">@@ -834,7 +835,9 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
  * particular problem by preventing anything from being mapped
  * at the maximum canonical address.
  */
<span class="p_del">-#define TASK_SIZE_MAX	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
<span class="p_add">+#define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+#define DEFAULT_MAP_WINDOW	((1UL &lt;&lt; 47) - PAGE_SIZE)</span>
 
 /* This decides where the kernel will search for a free chunk of vm
  * space during mmap&#39;s.
<span class="p_chunk">@@ -847,7 +850,7 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 #define TASK_SIZE_OF(child)	((test_tsk_thread_flag(child, TIF_ADDR32)) ? \
 					IA32_PAGE_OFFSET : TASK_SIZE_MAX)
 
<span class="p_del">-#define STACK_TOP		TASK_SIZE</span>
<span class="p_add">+#define STACK_TOP		DEFAULT_MAP_WINDOW</span>
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
 #define INIT_THREAD  {						\
<span class="p_chunk">@@ -870,7 +873,7 @@</span> <span class="p_context"> extern void start_thread(struct pt_regs *regs, unsigned long new_ip,</span>
  * space during mmap&#39;s.
  */
 #define __TASK_UNMAPPED_BASE(task_size)	(PAGE_ALIGN(task_size / 3))
<span class="p_del">-#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(TASK_SIZE)</span>
<span class="p_add">+#define TASK_UNMAPPED_BASE		__TASK_UNMAPPED_BASE(DEFAULT_MAP_WINDOW)</span>
 
 #define KSTK_EIP(task)		(task_pt_regs(task)-&gt;ip)
 
<span class="p_header">diff --git a/arch/x86/kernel/sys_x86_64.c b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">index 207b8f2582c7..593a31e93812 100644</span>
<span class="p_header">--- a/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/sys_x86_64.c</span>
<span class="p_chunk">@@ -21,6 +21,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/compat.h&gt;
 #include &lt;asm/ia32.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_add">+#include &lt;asm/mpx.h&gt;</span>
 
 /*
  * Align a virtual address to avoid aliasing in the I$ on AMD F15h.
<span class="p_chunk">@@ -132,6 +133,10 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	struct vm_unmapped_area_info info;
 	unsigned long begin, end;
 
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	if (flags &amp; MAP_FIXED)
 		return addr;
 
<span class="p_chunk">@@ -151,7 +156,16 @@</span> <span class="p_context"> arch_get_unmapped_area(struct file *filp, unsigned long addr,</span>
 	info.flags = 0;
 	info.length = len;
 	info.low_limit = begin;
<span class="p_del">-	info.high_limit = end;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		info.high_limit = min(end, TASK_SIZE);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		info.high_limit = min(end, DEFAULT_MAP_WINDOW);</span>
<span class="p_add">+</span>
 	info.align_mask = 0;
 	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;
 	if (filp) {
<span class="p_chunk">@@ -171,6 +185,10 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	unsigned long addr = addr0;
 	struct vm_unmapped_area_info info;
 
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	/* requested length too big for entire address space */
 	if (len &gt; TASK_SIZE)
 		return -ENOMEM;
<span class="p_chunk">@@ -195,6 +213,14 @@</span> <span class="p_context"> arch_get_unmapped_area_topdown(struct file *filp, const unsigned long addr0,</span>
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = get_mmap_base(0);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="p_add">+		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = 0;
 	info.align_offset = pgoff &lt;&lt; PAGE_SHIFT;
 	if (filp) {
<span class="p_header">diff --git a/arch/x86/mm/hugetlbpage.c b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">index 302f43fd9c28..9a0b89252c52 100644</span>
<span class="p_header">--- a/arch/x86/mm/hugetlbpage.c</span>
<span class="p_header">+++ b/arch/x86/mm/hugetlbpage.c</span>
<span class="p_chunk">@@ -18,6 +18,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/pgalloc.h&gt;
 #include &lt;asm/elf.h&gt;
<span class="p_add">+#include &lt;asm/mpx.h&gt;</span>
 
 #if 0	/* This is just for testing */
 struct page *
<span class="p_chunk">@@ -87,23 +88,38 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_bottomup(struct file *file,</span>
 	info.low_limit = get_mmap_base(1);
 	info.high_limit = in_compat_syscall() ?
 		tasksize_32bit() : tasksize_64bit();
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		info.high_limit = TASK_SIZE;</span>
<span class="p_add">+</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	return vm_unmapped_area(&amp;info);
 }
 
 static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,
<span class="p_del">-		unsigned long addr0, unsigned long len,</span>
<span class="p_add">+		unsigned long addr, unsigned long len,</span>
 		unsigned long pgoff, unsigned long flags)
 {
 	struct hstate *h = hstate_file(file);
 	struct vm_unmapped_area_info info;
<span class="p_del">-	unsigned long addr;</span>
 
 	info.flags = VM_UNMAPPED_AREA_TOPDOWN;
 	info.length = len;
 	info.low_limit = PAGE_SIZE;
 	info.high_limit = get_mmap_base(0);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If hint address is above DEFAULT_MAP_WINDOW, look for unmapped area</span>
<span class="p_add">+	 * in the full address space.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (addr &gt; DEFAULT_MAP_WINDOW &amp;&amp; !in_compat_syscall())</span>
<span class="p_add">+		info.high_limit += TASK_SIZE - DEFAULT_MAP_WINDOW;</span>
<span class="p_add">+</span>
 	info.align_mask = PAGE_MASK &amp; ~huge_page_mask(h);
 	info.align_offset = 0;
 	addr = vm_unmapped_area(&amp;info);
<span class="p_chunk">@@ -118,7 +134,7 @@</span> <span class="p_context"> static unsigned long hugetlb_get_unmapped_area_topdown(struct file *file,</span>
 		VM_BUG_ON(addr != -ENOMEM);
 		info.flags = 0;
 		info.low_limit = TASK_UNMAPPED_BASE;
<span class="p_del">-		info.high_limit = TASK_SIZE;</span>
<span class="p_add">+		info.high_limit = DEFAULT_MAP_WINDOW;</span>
 		addr = vm_unmapped_area(&amp;info);
 	}
 
<span class="p_chunk">@@ -135,6 +151,11 @@</span> <span class="p_context"> hugetlb_get_unmapped_area(struct file *file, unsigned long addr,</span>
 
 	if (len &amp; ~huge_page_mask(h))
 		return -EINVAL;
<span class="p_add">+</span>
<span class="p_add">+	addr = mpx_unmapped_area_check(addr, len, flags);</span>
<span class="p_add">+	if (IS_ERR_VALUE(addr))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
 	if (len &gt; TASK_SIZE)
 		return -ENOMEM;
 
<span class="p_header">diff --git a/arch/x86/mm/mmap.c b/arch/x86/mm/mmap.c</span>
<span class="p_header">index 19ad095b41df..d63232a31945 100644</span>
<span class="p_header">--- a/arch/x86/mm/mmap.c</span>
<span class="p_header">+++ b/arch/x86/mm/mmap.c</span>
<span class="p_chunk">@@ -44,7 +44,7 @@</span> <span class="p_context"> unsigned long tasksize_32bit(void)</span>
 
 unsigned long tasksize_64bit(void)
 {
<span class="p_del">-	return TASK_SIZE_MAX;</span>
<span class="p_add">+	return DEFAULT_MAP_WINDOW;</span>
 }
 
 static unsigned long stack_maxrandom_size(unsigned long task_size)
<span class="p_header">diff --git a/arch/x86/mm/mpx.c b/arch/x86/mm/mpx.c</span>
<span class="p_header">index cd44ae727df7..a26a1b373fd0 100644</span>
<span class="p_header">--- a/arch/x86/mm/mpx.c</span>
<span class="p_header">+++ b/arch/x86/mm/mpx.c</span>
<span class="p_chunk">@@ -355,10 +355,19 @@</span> <span class="p_context"> int mpx_enable_management(void)</span>
 	 */
 	bd_base = mpx_get_bounds_dir();
 	down_write(&amp;mm-&gt;mmap_sem);
<span class="p_add">+</span>
<span class="p_add">+	/* MPX doesn&#39;t support addresses above 47-bits yet. */</span>
<span class="p_add">+	if (find_vma(mm, DEFAULT_MAP_WINDOW)) {</span>
<span class="p_add">+		pr_warn_once(&quot;%s (%d): MPX cannot handle addresses &quot;</span>
<span class="p_add">+				&quot;above 47-bits. Disabling.&quot;,</span>
<span class="p_add">+				current-&gt;comm, current-&gt;pid);</span>
<span class="p_add">+		ret = -ENXIO;</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
 	mm-&gt;context.bd_addr = bd_base;
 	if (mm-&gt;context.bd_addr == MPX_INVALID_BOUNDS_DIR)
 		ret = -ENXIO;
<span class="p_del">-</span>
<span class="p_add">+out:</span>
 	up_write(&amp;mm-&gt;mmap_sem);
 	return ret;
 }
<span class="p_chunk">@@ -1038,3 +1047,25 @@</span> <span class="p_context"> void mpx_notify_unmap(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	if (ret)
 		force_sig(SIGSEGV, current);
 }
<span class="p_add">+</span>
<span class="p_add">+/* MPX cannot handle addresses above 47-bits yet. */</span>
<span class="p_add">+unsigned long mpx_unmapped_area_check(unsigned long addr, unsigned long len,</span>
<span class="p_add">+		unsigned long flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!kernel_managing_mpx_tables(current-&gt;mm))</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+	if (addr + len &lt;= DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+	if (flags &amp; MAP_FIXED)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Requested len is larger than whole area we&#39;re allowed to map in.</span>
<span class="p_add">+	 * Resetting hinting address wouldn&#39;t do much good -- fail early.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (len &gt; DEFAULT_MAP_WINDOW)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Look for unmap area within DEFAULT_MAP_WINDOW */</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



