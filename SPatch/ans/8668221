
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,V2,2/2] vhost: device IOTLB API - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,V2,2/2] vhost: device IOTLB API</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2154">Jason Wang</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 25, 2016, 2:34 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1458873274-13961-3-git-send-email-jasowang@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8668221/mbox/"
   >mbox</a>
|
   <a href="/patch/8668221/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8668221/">/patch/8668221/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 22E82C0553
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 25 Mar 2016 02:35:19 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id B9601203AE
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 25 Mar 2016 02:35:17 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 298AD203A1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 25 Mar 2016 02:35:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753074AbcCYCeu (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 24 Mar 2016 22:34:50 -0400
Received: from mx1.redhat.com ([209.132.183.28]:59506 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752041AbcCYCes (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 24 Mar 2016 22:34:48 -0400
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by mx1.redhat.com (Postfix) with ESMTPS id 3955F2568;
	Fri, 25 Mar 2016 02:34:47 +0000 (UTC)
Received: from jason-ThinkPad-T430s.redhat.com (vpn1-7-152.pek2.redhat.com
	[10.72.7.152])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id u2P2YZxb008502; Thu, 24 Mar 2016 22:34:43 -0400
From: Jason Wang &lt;jasowang@redhat.com&gt;
To: mst@redhat.com, kvm@vger.kernel.org,
	virtualization@lists.linux-foundation.org, netdev@vger.kernel.org,
	linux-kernel@vger.kernel.org
Cc: peterx@redhat.com, pbonzini@redhat.com, qemu-devel@nongnu.org,
	Jason Wang &lt;jasowang@redhat.com&gt;
Subject: [RFC PATCH V2 2/2] vhost: device IOTLB API
Date: Fri, 25 Mar 2016 10:34:34 +0800
Message-Id: &lt;1458873274-13961-3-git-send-email-jasowang@redhat.com&gt;
In-Reply-To: &lt;1458873274-13961-1-git-send-email-jasowang@redhat.com&gt;
References: &lt;1458873274-13961-1-git-send-email-jasowang@redhat.com&gt;
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2154">Jason Wang</a> - March 25, 2016, 2:34 a.m.</div>
<pre class="content">
This patch tries to implement an device IOTLB for vhost. This could be
used with for co-operation with userspace(qemu) implementation of DMA
remapping.

The idea is simple. When vhost meets an IOTLB miss, it will request
the assistance of userspace to do the translation, this is done
through:

- Fill the translation request in a preset userspace address (This
  address is set through ioctl VHOST_SET_IOTLB_REQUEST_ENTRY).
- Notify userspace through eventfd (This eventfd was set through ioctl
  VHOST_SET_IOTLB_FD).
- device IOTLB were started and stopped through VHOST_RUN_IOTLB ioctl

When userspace finishes the translation, it will update the vhost
IOTLB through VHOST_UPDATE_IOTLB ioctl. Userspace is also in charge of
snooping the IOTLB invalidation of IOMMU IOTLB and use
VHOST_UPDATE_IOTLB to invalidate the possible entry in vhost.
<span class="signed-off-by">
Signed-off-by: Jason Wang &lt;jasowang@redhat.com&gt;</span>
---
 drivers/vhost/net.c        |   6 +-
 drivers/vhost/vhost.c      | 301 +++++++++++++++++++++++++++++++++++++++------
 drivers/vhost/vhost.h      |  17 ++-
 fs/eventfd.c               |   3 +-
 include/uapi/linux/vhost.h |  35 ++++++
 5 files changed, 320 insertions(+), 42 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1984">Michael S. Tsirkin</a> - April 27, 2016, 11:45 a.m.</div>
<pre class="content">
On Fri, Mar 25, 2016 at 10:34:34AM +0800, Jason Wang wrote:
<span class="quote">&gt; This patch tries to implement an device IOTLB for vhost. This could be</span>
<span class="quote">&gt; used with for co-operation with userspace(qemu) implementation of DMA</span>
<span class="quote">&gt; remapping.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The idea is simple. When vhost meets an IOTLB miss, it will request</span>
<span class="quote">&gt; the assistance of userspace to do the translation, this is done</span>
<span class="quote">&gt; through:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - Fill the translation request in a preset userspace address (This</span>
<span class="quote">&gt;   address is set through ioctl VHOST_SET_IOTLB_REQUEST_ENTRY).</span>
<span class="quote">&gt; - Notify userspace through eventfd (This eventfd was set through ioctl</span>
<span class="quote">&gt;   VHOST_SET_IOTLB_FD).</span>

Why use an eventfd for this? We use them for interrupts because
that happens to be what kvm wants, but here - why don&#39;t we
just add a generic support for reading out events
on the vhost fd itself?
<span class="quote">
&gt; - device IOTLB were started and stopped through VHOST_RUN_IOTLB ioctl</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; When userspace finishes the translation, it will update the vhost</span>
<span class="quote">&gt; IOTLB through VHOST_UPDATE_IOTLB ioctl. Userspace is also in charge of</span>
<span class="quote">&gt; snooping the IOTLB invalidation of IOMMU IOTLB and use</span>
<span class="quote">&gt; VHOST_UPDATE_IOTLB to invalidate the possible entry in vhost.</span>

There&#39;s one problem here, and that is that VQs still do not undergo
translation.  In theory VQ could be mapped in such a way
that it&#39;s not contigious in userspace memory.
<span class="quote">

&gt; Signed-off-by: Jason Wang &lt;jasowang@redhat.com&gt;</span>

What limits amount of entries that kernel keeps around?
Do we want at least a mod parameter for this?
<span class="quote">

&gt; ---</span>
<span class="quote">&gt;  drivers/vhost/net.c        |   6 +-</span>
<span class="quote">&gt;  drivers/vhost/vhost.c      | 301 +++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;  drivers/vhost/vhost.h      |  17 ++-</span>
<span class="quote">&gt;  fs/eventfd.c               |   3 +-</span>
<span class="quote">&gt;  include/uapi/linux/vhost.h |  35 ++++++</span>
<span class="quote">&gt;  5 files changed, 320 insertions(+), 42 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c</span>
<span class="quote">&gt; index 481db96..7cbdeed 100644</span>
<span class="quote">&gt; --- a/drivers/vhost/net.c</span>
<span class="quote">&gt; +++ b/drivers/vhost/net.c</span>
<span class="quote">&gt; @@ -334,7 +334,7 @@ static void handle_tx(struct vhost_net *net)</span>
<span class="quote">&gt;  		head = vhost_get_vq_desc(vq, vq-&gt;iov,</span>
<span class="quote">&gt;  					 ARRAY_SIZE(vq-&gt;iov),</span>
<span class="quote">&gt;  					 &amp;out, &amp;in,</span>
<span class="quote">&gt; -					 NULL, NULL);</span>
<span class="quote">&gt; +					 NULL, NULL, VHOST_ACCESS_RO);</span>
<span class="quote">&gt;  		/* On error, stop handling until the next kick. */</span>
<span class="quote">&gt;  		if (unlikely(head &lt; 0))</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; @@ -470,7 +470,7 @@ static int get_rx_bufs(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		r = vhost_get_vq_desc(vq, vq-&gt;iov + seg,</span>
<span class="quote">&gt;  				      ARRAY_SIZE(vq-&gt;iov) - seg, &amp;out,</span>
<span class="quote">&gt; -				      &amp;in, log, log_num);</span>
<span class="quote">&gt; +				      &amp;in, log, log_num, VHOST_ACCESS_WO);</span>
<span class="quote">&gt;  		if (unlikely(r &lt; 0))</span>
<span class="quote">&gt;  			goto err;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1083,7 +1083,7 @@ static long vhost_net_ioctl(struct file *f, unsigned int ioctl,</span>
<span class="quote">&gt;  		r = vhost_dev_ioctl(&amp;n-&gt;dev, ioctl, argp);</span>
<span class="quote">&gt;  		if (r == -ENOIOCTLCMD)</span>
<span class="quote">&gt;  			r = vhost_vring_ioctl(&amp;n-&gt;dev, ioctl, argp);</span>
<span class="quote">&gt; -		else</span>
<span class="quote">&gt; +		else if (ioctl != VHOST_UPDATE_IOTLB)</span>
<span class="quote">&gt;  			vhost_net_flush(n);</span>
<span class="quote">&gt;  		mutex_unlock(&amp;n-&gt;dev.mutex);</span>
<span class="quote">&gt;  		return r;</span>
<span class="quote">&gt; diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="quote">&gt; index 32c35a9..1dd43e8 100644</span>
<span class="quote">&gt; --- a/drivers/vhost/vhost.c</span>
<span class="quote">&gt; +++ b/drivers/vhost/vhost.c</span>
<span class="quote">&gt; @@ -280,6 +280,10 @@ static void vhost_vq_reset(struct vhost_dev *dev,</span>
<span class="quote">&gt;  	vq-&gt;call_ctx = NULL;</span>
<span class="quote">&gt;  	vq-&gt;call = NULL;</span>
<span class="quote">&gt;  	vq-&gt;log_ctx = NULL;</span>
<span class="quote">&gt; +	vq-&gt;iotlb_call = NULL;</span>
<span class="quote">&gt; +	vq-&gt;iotlb_call_ctx = NULL;</span>
<span class="quote">&gt; +	vq-&gt;iotlb_request = NULL;</span>
<span class="quote">&gt; +	vq-&gt;pending_request.flags.type = VHOST_IOTLB_INVALIDATE;</span>
<span class="quote">&gt;  	vq-&gt;umem = NULL;</span>
<span class="quote">&gt;  	vq-&gt;is_le = virtio_legacy_is_little_endian();</span>
<span class="quote">&gt;  	vhost_vq_reset_user_be(vq);</span>
<span class="quote">&gt; @@ -387,8 +391,10 @@ void vhost_dev_init(struct vhost_dev *dev,</span>
<span class="quote">&gt;  	dev-&gt;log_ctx = NULL;</span>
<span class="quote">&gt;  	dev-&gt;log_file = NULL;</span>
<span class="quote">&gt;  	dev-&gt;umem = NULL;</span>
<span class="quote">&gt; +	dev-&gt;iotlb = NULL;</span>
<span class="quote">&gt;  	dev-&gt;mm = NULL;</span>
<span class="quote">&gt;  	spin_lock_init(&amp;dev-&gt;work_lock);</span>
<span class="quote">&gt; +	spin_lock_init(&amp;dev-&gt;iotlb_lock);</span>
<span class="quote">&gt;  	INIT_LIST_HEAD(&amp;dev-&gt;work_list);</span>
<span class="quote">&gt;  	dev-&gt;worker = NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -537,6 +543,15 @@ void vhost_dev_stop(struct vhost_dev *dev)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(vhost_dev_stop);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static void vhost_umem_free(struct vhost_umem *umem,</span>
<span class="quote">&gt; +			    struct vhost_umem_node *node)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	vhost_umem_interval_tree_remove(node, &amp;umem-&gt;umem_tree);</span>
<span class="quote">&gt; +	list_del(&amp;node-&gt;link);</span>
<span class="quote">&gt; +	kfree(node);</span>
<span class="quote">&gt; +	umem-&gt;numem--;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static void vhost_umem_clean(struct vhost_umem *umem)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vhost_umem_node *node, *tmp;</span>
<span class="quote">&gt; @@ -544,11 +559,9 @@ static void vhost_umem_clean(struct vhost_umem *umem)</span>
<span class="quote">&gt;  	if (!umem)</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	list_for_each_entry_safe(node, tmp, &amp;umem-&gt;umem_list, link) {</span>
<span class="quote">&gt; -		vhost_umem_interval_tree_remove(node, &amp;umem-&gt;umem_tree);</span>
<span class="quote">&gt; -		list_del(&amp;node-&gt;link);</span>
<span class="quote">&gt; -		kvfree(node);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +	list_for_each_entry_safe(node, tmp, &amp;umem-&gt;umem_list, link)</span>
<span class="quote">&gt; +		vhost_umem_free(umem, node);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	kvfree(umem);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -580,6 +593,8 @@ void vhost_dev_cleanup(struct vhost_dev *dev, bool locked)</span>
<span class="quote">&gt;  	/* No one will access memory at this point */</span>
<span class="quote">&gt;  	vhost_umem_clean(dev-&gt;umem);</span>
<span class="quote">&gt;  	dev-&gt;umem = NULL;</span>
<span class="quote">&gt; +	vhost_umem_clean(dev-&gt;iotlb);</span>
<span class="quote">&gt; +	dev-&gt;iotlb = NULL;</span>
<span class="quote">&gt;  	WARN_ON(!list_empty(&amp;dev-&gt;work_list));</span>
<span class="quote">&gt;  	if (dev-&gt;worker) {</span>
<span class="quote">&gt;  		kthread_stop(dev-&gt;worker);</span>
<span class="quote">&gt; @@ -699,11 +714,61 @@ int vhost_vq_access_ok(struct vhost_virtqueue *vq)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(vhost_vq_access_ok);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static int vhost_new_umem_range(struct vhost_umem *umem,</span>
<span class="quote">&gt; +				u64 start, u64 size, u64 end,</span>
<span class="quote">&gt; +				u64 userspace_addr, int perm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vhost_umem_node *tmp, *node = kmalloc(sizeof(*node), GFP_ATOMIC);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!node)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (umem-&gt;numem == VHOST_IOTLB_SIZE) {</span>
<span class="quote">&gt; +		tmp = list_last_entry(&amp;umem-&gt;umem_list, typeof(*tmp), link);</span>
<span class="quote">&gt; +		vhost_umem_free(umem, tmp);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	node-&gt;start = start;</span>
<span class="quote">&gt; +	node-&gt;size = size;</span>
<span class="quote">&gt; +	node-&gt;last = end;</span>
<span class="quote">&gt; +	node-&gt;userspace_addr = userspace_addr;</span>
<span class="quote">&gt; +	node-&gt;perm = perm;</span>
<span class="quote">&gt; +	INIT_LIST_HEAD(&amp;node-&gt;link);</span>
<span class="quote">&gt; +	list_add_tail(&amp;node-&gt;link, &amp;umem-&gt;umem_list);</span>
<span class="quote">&gt; +	vhost_umem_interval_tree_insert(node, &amp;umem-&gt;umem_tree);</span>
<span class="quote">&gt; +	umem-&gt;numem++;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void vhost_del_umem_range(struct vhost_umem *umem,</span>
<span class="quote">&gt; +				 u64 start, u64 end)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vhost_umem_node *node;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	while ((node = vhost_umem_interval_tree_iter_first(&amp;umem-&gt;umem_tree,</span>
<span class="quote">&gt; +							   start, end)))</span>
<span class="quote">&gt; +		vhost_umem_free(umem, node);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static struct vhost_umem *vhost_umem_alloc(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vhost_umem *umem = vhost_kvzalloc(sizeof(*umem));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!umem)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	umem-&gt;umem_tree = RB_ROOT;</span>
<span class="quote">&gt; +	umem-&gt;numem = 0;</span>
<span class="quote">&gt; +	INIT_LIST_HEAD(&amp;umem-&gt;umem_list);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return umem;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vhost_memory mem, *newmem;</span>
<span class="quote">&gt;  	struct vhost_memory_region *region;</span>
<span class="quote">&gt; -	struct vhost_umem_node *node;</span>
<span class="quote">&gt;  	struct vhost_umem *newumem, *oldumem;</span>
<span class="quote">&gt;  	unsigned long size = offsetof(struct vhost_memory, regions);</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt; @@ -725,28 +790,23 @@ static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)</span>
<span class="quote">&gt;  		return -EFAULT;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	newumem = vhost_kvzalloc(sizeof(*newumem));</span>
<span class="quote">&gt; +	newumem = vhost_umem_alloc();</span>
<span class="quote">&gt;  	if (!newumem) {</span>
<span class="quote">&gt;  		kvfree(newmem);</span>
<span class="quote">&gt;  		return -ENOMEM;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	newumem-&gt;umem_tree = RB_ROOT;</span>
<span class="quote">&gt; -	INIT_LIST_HEAD(&amp;newumem-&gt;umem_list);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  	for (region = newmem-&gt;regions;</span>
<span class="quote">&gt;  	     region &lt; newmem-&gt;regions + mem.nregions;</span>
<span class="quote">&gt;  	     region++) {</span>
<span class="quote">&gt; -		node = vhost_kvzalloc(sizeof(*node));</span>
<span class="quote">&gt; -		if (!node)</span>
<span class="quote">&gt; +		if (vhost_new_umem_range(newumem,</span>
<span class="quote">&gt; +					 region-&gt;guest_phys_addr,</span>
<span class="quote">&gt; +					 region-&gt;memory_size,</span>
<span class="quote">&gt; +					 region-&gt;guest_phys_addr +</span>
<span class="quote">&gt; +					 region-&gt;memory_size - 1,</span>
<span class="quote">&gt; +					 region-&gt;userspace_addr,</span>
<span class="quote">&gt; +				         VHOST_ACCESS_RW))</span>
<span class="quote">&gt;  			goto err;</span>
<span class="quote">&gt; -		node-&gt;start = region-&gt;guest_phys_addr;</span>
<span class="quote">&gt; -		node-&gt;size = region-&gt;memory_size;</span>
<span class="quote">&gt; -		node-&gt;last = node-&gt;start + node-&gt;size - 1;</span>
<span class="quote">&gt; -		node-&gt;userspace_addr = region-&gt;userspace_addr;</span>
<span class="quote">&gt; -		INIT_LIST_HEAD(&amp;node-&gt;link);</span>
<span class="quote">&gt; -		list_add_tail(&amp;node-&gt;link, &amp;newumem-&gt;umem_list);</span>
<span class="quote">&gt; -		vhost_umem_interval_tree_insert(node, &amp;newumem-&gt;umem_tree);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (!memory_access_ok(d, newumem, 0))</span>
<span class="quote">&gt; @@ -782,6 +842,7 @@ long vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)</span>
<span class="quote">&gt;  	struct vhost_vring_state s;</span>
<span class="quote">&gt;  	struct vhost_vring_file f;</span>
<span class="quote">&gt;  	struct vhost_vring_addr a;</span>
<span class="quote">&gt; +	struct vhost_vring_iotlb_entry e;</span>
<span class="quote">&gt;  	u32 idx;</span>
<span class="quote">&gt;  	long r;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -910,6 +971,35 @@ long vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)</span>
<span class="quote">&gt;  		} else</span>
<span class="quote">&gt;  			filep = eventfp;</span>
<span class="quote">&gt;  		break;</span>
<span class="quote">&gt; +	case VHOST_SET_VRING_IOTLB_REQUEST:</span>
<span class="quote">&gt; +		r = -EFAULT;</span>
<span class="quote">&gt; +		if (copy_from_user(&amp;e, argp, sizeof e))</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		if (!access_ok(VERIFY_WRITE, e.userspace_addr,</span>
<span class="quote">&gt; +				sizeof(*vq-&gt;iotlb_request)))</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		r = 0;</span>
<span class="quote">&gt; +		vq-&gt;iotlb_request = (struct vhost_iotlb_entry __user *)e.userspace_addr;</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case VHOST_SET_VRING_IOTLB_CALL:</span>
<span class="quote">&gt; +		if (copy_from_user(&amp;f, argp, sizeof f)) {</span>
<span class="quote">&gt; +			r = -EFAULT;</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		eventfp = f.fd == -1 ? NULL : eventfd_fget(f.fd);</span>
<span class="quote">&gt; +		if (IS_ERR(eventfp)) {</span>
<span class="quote">&gt; +			r = PTR_ERR(eventfp);</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		if (eventfp != vq-&gt;iotlb_call) {</span>
<span class="quote">&gt; +			filep = vq-&gt;iotlb_call;</span>
<span class="quote">&gt; +			ctx = vq-&gt;iotlb_call_ctx;</span>
<span class="quote">&gt; +			vq-&gt;iotlb_call = eventfp;</span>
<span class="quote">&gt; +			vq-&gt;iotlb_call_ctx = eventfp ?</span>
<span class="quote">&gt; +				eventfd_ctx_fileget(eventfp) : NULL;</span>
<span class="quote">&gt; +		} else</span>
<span class="quote">&gt; +			filep = eventfp;</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt;  	case VHOST_SET_VRING_CALL:</span>
<span class="quote">&gt;  		if (copy_from_user(&amp;f, argp, sizeof f)) {</span>
<span class="quote">&gt;  			r = -EFAULT;</span>
<span class="quote">&gt; @@ -977,11 +1067,55 @@ long vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(vhost_vring_ioctl);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vhost_umem *niotlb, *oiotlb;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (enabled) {</span>
<span class="quote">&gt; +		niotlb = vhost_umem_alloc();</span>
<span class="quote">&gt; +		if (!niotlb)</span>
<span class="quote">&gt; +			return -ENOMEM;</span>
<span class="quote">&gt; +	} else</span>
<span class="quote">&gt; +		niotlb = NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock(&amp;d-&gt;iotlb_lock);</span>
<span class="quote">&gt; +	oiotlb = d-&gt;iotlb;</span>
<span class="quote">&gt; +	d-&gt;iotlb = niotlb;</span>
<span class="quote">&gt; +	spin_unlock(&amp;d-&gt;iotlb_lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	vhost_umem_clean(oiotlb);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void vhost_complete_iotlb_update(struct vhost_dev *d,</span>
<span class="quote">&gt; +					struct vhost_iotlb_entry *entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vhost_iotlb_entry *req;</span>
<span class="quote">&gt; +	struct vhost_virtqueue *vq;</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (i = 0; i &lt; d-&gt;nvqs; i++) {</span>
<span class="quote">&gt; +		vq = d-&gt;vqs[i];</span>
<span class="quote">&gt; +		mutex_lock(&amp;vq-&gt;mutex);</span>
<span class="quote">&gt; +		req = &amp;vq-&gt;pending_request;</span>
<span class="quote">&gt; +		if (entry-&gt;iova &lt;= req-&gt;iova &amp;&amp;</span>
<span class="quote">&gt; +		    entry-&gt;iova + entry-&gt;size - 1 &gt; req-&gt;iova &amp;&amp;</span>
<span class="quote">&gt; +		    req-&gt;flags.type == VHOST_IOTLB_MISS) {</span>
<span class="quote">&gt; +			*req = *entry;</span>
<span class="quote">&gt; +			vhost_poll_queue(&amp;vq-&gt;poll);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		mutex_unlock(&amp;vq-&gt;mutex);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  /* Caller must have device mutex */</span>
<span class="quote">&gt;  long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct file *eventfp, *filep = NULL;</span>
<span class="quote">&gt;  	struct eventfd_ctx *ctx = NULL;</span>
<span class="quote">&gt; +	struct vhost_iotlb_entry entry;</span>
<span class="quote">&gt;  	u64 p;</span>
<span class="quote">&gt;  	long r;</span>
<span class="quote">&gt;  	int i, fd;</span>
<span class="quote">&gt; @@ -1050,6 +1184,52 @@ long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)</span>
<span class="quote">&gt;  		if (filep)</span>
<span class="quote">&gt;  			fput(filep);</span>
<span class="quote">&gt;  		break;</span>
<span class="quote">&gt; +	case VHOST_RUN_IOTLB:</span>
<span class="quote">&gt; +		/* FIXME: enable and disabled */</span>
<span class="quote">&gt; +		vhost_init_device_iotlb(d, true);</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case VHOST_UPDATE_IOTLB:</span>
<span class="quote">&gt; +		r = copy_from_user(&amp;entry, argp, sizeof(entry));</span>
<span class="quote">&gt; +		if (r &lt; 0) {</span>
<span class="quote">&gt; +			r = -EFAULT;</span>
<span class="quote">&gt; +			goto done;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		spin_lock(&amp;d-&gt;iotlb_lock);</span>
<span class="quote">&gt; +		if (!d-&gt;iotlb) {</span>
<span class="quote">&gt; +			spin_unlock(&amp;d-&gt;iotlb_lock);</span>
<span class="quote">&gt; +			r = -EFAULT;</span>
<span class="quote">&gt; +			goto done;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		switch (entry.flags.type) {</span>
<span class="quote">&gt; +		case VHOST_IOTLB_UPDATE:</span>
<span class="quote">&gt; +			if (entry.flags.valid != VHOST_IOTLB_VALID) {</span>
<span class="quote">&gt; +				break;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			if (vhost_new_umem_range(d-&gt;iotlb,</span>
<span class="quote">&gt; +						 entry.iova,</span>
<span class="quote">&gt; +						 entry.size,</span>
<span class="quote">&gt; +						 entry.iova + entry.size - 1,</span>
<span class="quote">&gt; +                                                 entry.userspace_addr,</span>
<span class="quote">&gt; +                                                 entry.flags.perm)) {</span>
<span class="quote">&gt; +				r = -ENOMEM;</span>
<span class="quote">&gt; +				break;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		case VHOST_IOTLB_INVALIDATE:</span>
<span class="quote">&gt; +			vhost_del_umem_range(d-&gt;iotlb,</span>
<span class="quote">&gt; +					     entry.iova,</span>
<span class="quote">&gt; +					     entry.iova + entry.size - 1);</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		default:</span>
<span class="quote">&gt; +			r = -EINVAL;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +		spin_unlock(&amp;d-&gt;iotlb_lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!r &amp;&amp; entry.flags.type != VHOST_IOTLB_INVALIDATE)</span>
<span class="quote">&gt; +			vhost_complete_iotlb_update(d, &amp;entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt;  	default:</span>
<span class="quote">&gt;  		r = -ENOIOCTLCMD;</span>
<span class="quote">&gt;  		break;</span>
<span class="quote">&gt; @@ -1197,27 +1377,69 @@ int vhost_init_used(struct vhost_virtqueue *vq)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL_GPL(vhost_init_used);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vhost_iotlb_entry *pending = &amp;vq-&gt;pending_request;</span>
<span class="quote">&gt; +	int ret;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!vq-&gt;iotlb_call_ctx)</span>
<span class="quote">&gt; +		return -EOPNOTSUPP;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!vq-&gt;iotlb_request)</span>
<span class="quote">&gt; +		return -EOPNOTSUPP;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (pending-&gt;flags.type == VHOST_IOTLB_MISS) {</span>
<span class="quote">&gt; +		return -EEXIST;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pending-&gt;iova = iova;</span>
<span class="quote">&gt; +	pending-&gt;flags.type = VHOST_IOTLB_MISS;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ret = __copy_to_user(vq-&gt;iotlb_request, pending,</span>
<span class="quote">&gt; +			     sizeof(struct vhost_iotlb_entry));</span>
<span class="quote">&gt; +	if (ret) {</span>
<span class="quote">&gt; +		goto err;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (vq-&gt;iotlb_call_ctx)</span>
<span class="quote">&gt; +		eventfd_signal(vq-&gt;iotlb_call_ctx, 1);</span>
<span class="quote">&gt; +err:</span>
<span class="quote">&gt; +	return ret;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,</span>
<span class="quote">&gt; -			  struct iovec iov[], int iov_size)</span>
<span class="quote">&gt; +			  struct iovec iov[], int iov_size, int access)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	const struct vhost_umem_node *node;</span>
<span class="quote">&gt; -	struct vhost_umem *umem = vq-&gt;umem;</span>
<span class="quote">&gt; +	struct vhost_dev *dev = vq-&gt;dev;</span>
<span class="quote">&gt; +	struct vhost_umem *umem = dev-&gt;iotlb ? dev-&gt;iotlb : dev-&gt;umem;</span>
<span class="quote">&gt;  	struct iovec *_iov;</span>
<span class="quote">&gt;  	u64 s = 0;</span>
<span class="quote">&gt;  	int ret = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	spin_lock(&amp;dev-&gt;iotlb_lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	while ((u64)len &gt; s) {</span>
<span class="quote">&gt;  		u64 size;</span>
<span class="quote">&gt;  		if (unlikely(ret &gt;= iov_size)) {</span>
<span class="quote">&gt;  			ret = -ENOBUFS;</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		node = vhost_umem_interval_tree_iter_first(&amp;umem-&gt;umem_tree,</span>
<span class="quote">&gt;  							addr, addr + len - 1);</span>
<span class="quote">&gt;  		if (node == NULL || node-&gt;start &gt; addr) {</span>
<span class="quote">&gt; -			ret = -EFAULT;</span>
<span class="quote">&gt; +			if (umem != dev-&gt;iotlb) {</span>
<span class="quote">&gt; +				ret = -EFAULT;</span>
<span class="quote">&gt; +				break;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			ret = -EAGAIN;</span>
<span class="quote">&gt; +			break;</span>
<span class="quote">&gt; +		} else if (!(node-&gt;perm &amp; access)) {</span>
<span class="quote">&gt; +			ret = -EPERM;</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		_iov = iov + ret;</span>
<span class="quote">&gt;  		size = node-&gt;size - addr + node-&gt;start;</span>
<span class="quote">&gt;  		_iov-&gt;iov_len = min((u64)len - s, size);</span>
<span class="quote">&gt; @@ -1228,6 +1450,10 @@ static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,</span>
<span class="quote">&gt;  		++ret;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	spin_unlock(&amp;dev-&gt;iotlb_lock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (ret == -EAGAIN)</span>
<span class="quote">&gt; +		vhost_iotlb_miss(vq, addr);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1256,7 +1482,7 @@ static int get_indirect(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  			struct iovec iov[], unsigned int iov_size,</span>
<span class="quote">&gt;  			unsigned int *out_num, unsigned int *in_num,</span>
<span class="quote">&gt;  			struct vhost_log *log, unsigned int *log_num,</span>
<span class="quote">&gt; -			struct vring_desc *indirect)</span>
<span class="quote">&gt; +			struct vring_desc *indirect, int access)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vring_desc desc;</span>
<span class="quote">&gt;  	unsigned int i = 0, count, found = 0;</span>
<span class="quote">&gt; @@ -1274,9 +1500,10 @@ static int get_indirect(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	ret = translate_desc(vq, vhost64_to_cpu(vq, indirect-&gt;addr), len, vq-&gt;indirect,</span>
<span class="quote">&gt; -			     UIO_MAXIOV);</span>
<span class="quote">&gt; +			     UIO_MAXIOV, access);</span>
<span class="quote">&gt;  	if (unlikely(ret &lt; 0)) {</span>
<span class="quote">&gt; -		vq_err(vq, &quot;Translation failure %d in indirect.\n&quot;, ret);</span>
<span class="quote">&gt; +		if (ret != -EAGAIN)</span>
<span class="quote">&gt; +			vq_err(vq, &quot;Translation failure %d in indirect.\n&quot;, ret);</span>
<span class="quote">&gt;  		return ret;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	iov_iter_init(&amp;from, READ, vq-&gt;indirect, ret, len);</span>
<span class="quote">&gt; @@ -1316,10 +1543,11 @@ static int get_indirect(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),</span>
<span class="quote">&gt;  				     vhost32_to_cpu(vq, desc.len), iov + iov_count,</span>
<span class="quote">&gt; -				     iov_size - iov_count);</span>
<span class="quote">&gt; +				     iov_size - iov_count, access);</span>
<span class="quote">&gt;  		if (unlikely(ret &lt; 0)) {</span>
<span class="quote">&gt; -			vq_err(vq, &quot;Translation failure %d indirect idx %d\n&quot;,</span>
<span class="quote">&gt; -			       ret, i);</span>
<span class="quote">&gt; +			if (ret != -EAGAIN)</span>
<span class="quote">&gt; +				vq_err(vq, &quot;Translation failure %d indirect idx %d\n&quot;,</span>
<span class="quote">&gt; +					ret, i);</span>
<span class="quote">&gt;  			return ret;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		/* If this is an input descriptor, increment that count. */</span>
<span class="quote">&gt; @@ -1355,7 +1583,8 @@ static int get_indirect(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  		      struct iovec iov[], unsigned int iov_size,</span>
<span class="quote">&gt;  		      unsigned int *out_num, unsigned int *in_num,</span>
<span class="quote">&gt; -		      struct vhost_log *log, unsigned int *log_num)</span>
<span class="quote">&gt; +		      struct vhost_log *log, unsigned int *log_num,</span>
<span class="quote">&gt; +		      int access)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct vring_desc desc;</span>
<span class="quote">&gt;  	unsigned int i, head, found = 0;</span>
<span class="quote">&gt; @@ -1433,10 +1662,11 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  		if (desc.flags &amp; cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT)) {</span>
<span class="quote">&gt;  			ret = get_indirect(vq, iov, iov_size,</span>
<span class="quote">&gt;  					   out_num, in_num,</span>
<span class="quote">&gt; -					   log, log_num, &amp;desc);</span>
<span class="quote">&gt; +					   log, log_num, &amp;desc, access);</span>
<span class="quote">&gt;  			if (unlikely(ret &lt; 0)) {</span>
<span class="quote">&gt; -				vq_err(vq, &quot;Failure detected &quot;</span>
<span class="quote">&gt; -				       &quot;in indirect descriptor at idx %d\n&quot;, i);</span>
<span class="quote">&gt; +				if (ret != -EAGAIN)</span>
<span class="quote">&gt; +					vq_err(vq, &quot;Failure detected &quot;</span>
<span class="quote">&gt; +						&quot;in indirect descriptor at idx %d\n&quot;, i);</span>
<span class="quote">&gt;  				return ret;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt; @@ -1444,10 +1674,11 @@ int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),</span>
<span class="quote">&gt;  				     vhost32_to_cpu(vq, desc.len), iov + iov_count,</span>
<span class="quote">&gt; -				     iov_size - iov_count);</span>
<span class="quote">&gt; +				     iov_size - iov_count, access);</span>
<span class="quote">&gt;  		if (unlikely(ret &lt; 0)) {</span>
<span class="quote">&gt; -			vq_err(vq, &quot;Translation failure %d descriptor idx %d\n&quot;,</span>
<span class="quote">&gt; -			       ret, i);</span>
<span class="quote">&gt; +			if (ret != -EAGAIN)</span>
<span class="quote">&gt; +				vq_err(vq, &quot;Translation failure %d descriptor idx %d\n&quot;,</span>
<span class="quote">&gt; +					ret, i);</span>
<span class="quote">&gt;  			return ret;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		if (desc.flags &amp; cpu_to_vhost16(vq, VRING_DESC_F_WRITE)) {</span>
<span class="quote">&gt; diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h</span>
<span class="quote">&gt; index 5d64393..4365104 100644</span>
<span class="quote">&gt; --- a/drivers/vhost/vhost.h</span>
<span class="quote">&gt; +++ b/drivers/vhost/vhost.h</span>
<span class="quote">&gt; @@ -62,13 +62,15 @@ struct vhost_umem_node {</span>
<span class="quote">&gt;  	__u64 last;</span>
<span class="quote">&gt;  	__u64 size;</span>
<span class="quote">&gt;  	__u64 userspace_addr;</span>
<span class="quote">&gt; -	__u64 flags_padding;</span>
<span class="quote">&gt; +	__u32 perm;</span>
<span class="quote">&gt; +	__u32 flags_padding;</span>
<span class="quote">&gt;  	__u64 __subtree_last;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct vhost_umem {</span>
<span class="quote">&gt;  	struct rb_root umem_tree;</span>
<span class="quote">&gt;  	struct list_head umem_list;</span>
<span class="quote">&gt; +	int numem;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* The virtqueue structure describes a queue attached to a device. */</span>
<span class="quote">&gt; @@ -84,9 +86,13 @@ struct vhost_virtqueue {</span>
<span class="quote">&gt;  	struct file *kick;</span>
<span class="quote">&gt;  	struct file *call;</span>
<span class="quote">&gt;  	struct file *error;</span>
<span class="quote">&gt; +	struct file *iotlb_call;</span>
<span class="quote">&gt;  	struct eventfd_ctx *call_ctx;</span>
<span class="quote">&gt;  	struct eventfd_ctx *error_ctx;</span>
<span class="quote">&gt;  	struct eventfd_ctx *log_ctx;</span>
<span class="quote">&gt; +	struct eventfd_ctx *iotlb_call_ctx;</span>
<span class="quote">&gt; +	struct vhost_iotlb_entry __user *iotlb_request;</span>
<span class="quote">&gt; +	struct vhost_iotlb_entry pending_request;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	struct vhost_poll poll;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -135,6 +141,8 @@ struct vhost_virtqueue {</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define VHOST_IOTLB_SIZE 2048</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  struct vhost_dev {</span>
<span class="quote">&gt;  	struct mm_struct *mm;</span>
<span class="quote">&gt;  	struct mutex mutex;</span>
<span class="quote">&gt; @@ -146,6 +154,8 @@ struct vhost_dev {</span>
<span class="quote">&gt;  	struct list_head work_list;</span>
<span class="quote">&gt;  	struct task_struct *worker;</span>
<span class="quote">&gt;  	struct vhost_umem *umem;</span>
<span class="quote">&gt; +	struct vhost_umem *iotlb;</span>
<span class="quote">&gt; +	spinlock_t iotlb_lock;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void vhost_dev_init(struct vhost_dev *, struct vhost_virtqueue **vqs, int nvqs);</span>
<span class="quote">&gt; @@ -164,7 +174,8 @@ int vhost_log_access_ok(struct vhost_dev *);</span>
<span class="quote">&gt;  int vhost_get_vq_desc(struct vhost_virtqueue *,</span>
<span class="quote">&gt;  		      struct iovec iov[], unsigned int iov_count,</span>
<span class="quote">&gt;  		      unsigned int *out_num, unsigned int *in_num,</span>
<span class="quote">&gt; -		      struct vhost_log *log, unsigned int *log_num);</span>
<span class="quote">&gt; +		      struct vhost_log *log, unsigned int *log_num,</span>
<span class="quote">&gt; +		      int access);</span>
<span class="quote">&gt;  void vhost_discard_vq_desc(struct vhost_virtqueue *, int n);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  int vhost_init_used(struct vhost_virtqueue *);</span>
<span class="quote">&gt; @@ -183,7 +194,7 @@ int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,</span>
<span class="quote">&gt;  		    unsigned int log_num, u64 len);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define vq_err(vq, fmt, ...) do {                                  \</span>
<span class="quote">&gt; -		pr_debug(pr_fmt(fmt), ##__VA_ARGS__);       \</span>
<span class="quote">&gt; +		printk(pr_fmt(fmt), ##__VA_ARGS__);       \</span>
<span class="quote">&gt;  		if ((vq)-&gt;error_ctx)                               \</span>
<span class="quote">&gt;  				eventfd_signal((vq)-&gt;error_ctx, 1);\</span>
<span class="quote">&gt;  	} while (0)</span>
<span class="quote">&gt; diff --git a/fs/eventfd.c b/fs/eventfd.c</span>
<span class="quote">&gt; index 8d0c0df..5c0a22f 100644</span>
<span class="quote">&gt; --- a/fs/eventfd.c</span>
<span class="quote">&gt; +++ b/fs/eventfd.c</span>
<span class="quote">&gt; @@ -59,8 +59,9 @@ __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)</span>
<span class="quote">&gt;  	if (ULLONG_MAX - ctx-&gt;count &lt; n)</span>
<span class="quote">&gt;  		n = ULLONG_MAX - ctx-&gt;count;</span>
<span class="quote">&gt;  	ctx-&gt;count += n;</span>
<span class="quote">&gt; -	if (waitqueue_active(&amp;ctx-&gt;wqh))</span>
<span class="quote">&gt; +	if (waitqueue_active(&amp;ctx-&gt;wqh)) {</span>
<span class="quote">&gt;  		wake_up_locked_poll(&amp;ctx-&gt;wqh, POLLIN);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	spin_unlock_irqrestore(&amp;ctx-&gt;wqh.lock, flags);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	return n;</span>
<span class="quote">&gt; diff --git a/include/uapi/linux/vhost.h b/include/uapi/linux/vhost.h</span>
<span class="quote">&gt; index ab373191..5c35ab4 100644</span>
<span class="quote">&gt; --- a/include/uapi/linux/vhost.h</span>
<span class="quote">&gt; +++ b/include/uapi/linux/vhost.h</span>
<span class="quote">&gt; @@ -47,6 +47,32 @@ struct vhost_vring_addr {</span>
<span class="quote">&gt;  	__u64 log_guest_addr;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +struct vhost_iotlb_entry {</span>
<span class="quote">&gt; +	__u64 iova;</span>
<span class="quote">&gt; +	__u64 size;</span>
<span class="quote">&gt; +	__u64 userspace_addr;</span>

Alignment requirements?
<span class="quote">
&gt; +	struct {</span>
<span class="quote">&gt; +#define VHOST_ACCESS_RO      0x1</span>
<span class="quote">&gt; +#define VHOST_ACCESS_WO      0x2</span>
<span class="quote">&gt; +#define VHOST_ACCESS_RW      0x3</span>
<span class="quote">&gt; +		__u8  perm;</span>
<span class="quote">&gt; +#define VHOST_IOTLB_MISS           1</span>
<span class="quote">&gt; +#define VHOST_IOTLB_UPDATE         2</span>
<span class="quote">&gt; +#define VHOST_IOTLB_INVALIDATE     3</span>
<span class="quote">&gt; +		__u8  type;</span>
<span class="quote">&gt; +#define VHOST_IOTLB_INVALID        0x1</span>
<span class="quote">&gt; +#define VHOST_IOTLB_VALID          0x2</span>
<span class="quote">&gt; +		__u8  valid;</span>

why do we need this flag?
<span class="quote">
&gt; +		__u8  u8_padding;</span>
<span class="quote">&gt; +		__u32 padding;</span>
<span class="quote">&gt; +	} flags;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct vhost_vring_iotlb_entry {</span>
<span class="quote">&gt; +	unsigned int index;</span>
<span class="quote">&gt; +	__u64 userspace_addr;</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  struct vhost_memory_region {</span>
<span class="quote">&gt;  	__u64 guest_phys_addr;</span>
<span class="quote">&gt;  	__u64 memory_size; /* bytes */</span>
<span class="quote">&gt; @@ -127,6 +153,15 @@ struct vhost_memory {</span>
<span class="quote">&gt;  /* Set eventfd to signal an error */</span>
<span class="quote">&gt;  #define VHOST_SET_VRING_ERR _IOW(VHOST_VIRTIO, 0x22, struct vhost_vring_file)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +/* IOTLB */</span>
<span class="quote">&gt; +/* Specify an eventfd file descriptor to signle on IOTLB miss */</span>

typo
<span class="quote">
&gt; +#define VHOST_SET_VRING_IOTLB_CALL _IOW(VHOST_VIRTIO, 0x23, struct      \</span>
<span class="quote">&gt; +                                        vhost_vring_file)</span>
<span class="quote">&gt; +#define VHOST_SET_VRING_IOTLB_REQUEST _IOW(VHOST_VIRTIO, 0x25, struct   \</span>
<span class="quote">&gt; +                                           vhost_vring_iotlb_entry)</span>
<span class="quote">&gt; +#define VHOST_UPDATE_IOTLB _IOW(VHOST_VIRTIO, 0x24, struct vhost_iotlb_entry)</span>
<span class="quote">&gt; +#define VHOST_RUN_IOTLB _IOW(VHOST_VIRTIO, 0x26, int)</span>
<span class="quote">&gt; +</span>

Is the assumption that userspace must dedicate a thread to running the iotlb? 
I dislike this one.
Please support asynchronous APIs at least optionally to make
userspace make its own threading decisions.
<span class="quote">
&gt;  /* VHOST_NET specific defines */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* Attach virtio net ring to a raw socket, or tap device.</span>

Don&#39;t we need a feature bit for this?
Are we short on feature bits? If yes maybe it&#39;s time to add
something like PROTOCOL_FEATURES that we have in vhost-user.
<span class="quote">
&gt; -- </span>
<span class="quote">&gt; 2.5.0</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2154">Jason Wang</a> - April 28, 2016, 6:37 a.m.</div>
<pre class="content">
On 04/27/2016 07:45 PM, Michael S. Tsirkin wrote:
<span class="quote">&gt; On Fri, Mar 25, 2016 at 10:34:34AM +0800, Jason Wang wrote:</span>
<span class="quote">&gt;&gt; This patch tries to implement an device IOTLB for vhost. This could be</span>
<span class="quote">&gt;&gt; used with for co-operation with userspace(qemu) implementation of DMA</span>
<span class="quote">&gt;&gt; remapping.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The idea is simple. When vhost meets an IOTLB miss, it will request</span>
<span class="quote">&gt;&gt; the assistance of userspace to do the translation, this is done</span>
<span class="quote">&gt;&gt; through:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; - Fill the translation request in a preset userspace address (This</span>
<span class="quote">&gt;&gt;   address is set through ioctl VHOST_SET_IOTLB_REQUEST_ENTRY).</span>
<span class="quote">&gt;&gt; - Notify userspace through eventfd (This eventfd was set through ioctl</span>
<span class="quote">&gt;&gt;   VHOST_SET_IOTLB_FD).</span>
<span class="quote">&gt; Why use an eventfd for this?</span>

The aim is to implement the API all through ioctls.
<span class="quote">
&gt;  We use them for interrupts because</span>
<span class="quote">&gt; that happens to be what kvm wants, but here - why don&#39;t we</span>
<span class="quote">&gt; just add a generic support for reading out events</span>
<span class="quote">&gt; on the vhost fd itself?</span>

I&#39;ve considered this approach, but what&#39;s the advantages of this? I mean
looks like all other ioctls could be done through vhost fd
reading/writing too.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; - device IOTLB were started and stopped through VHOST_RUN_IOTLB ioctl</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; When userspace finishes the translation, it will update the vhost</span>
<span class="quote">&gt;&gt; IOTLB through VHOST_UPDATE_IOTLB ioctl. Userspace is also in charge of</span>
<span class="quote">&gt;&gt; snooping the IOTLB invalidation of IOMMU IOTLB and use</span>
<span class="quote">&gt;&gt; VHOST_UPDATE_IOTLB to invalidate the possible entry in vhost.</span>
<span class="quote">&gt; There&#39;s one problem here, and that is that VQs still do not undergo</span>
<span class="quote">&gt; translation.  In theory VQ could be mapped in such a way</span>
<span class="quote">&gt; that it&#39;s not contigious in userspace memory.</span>

I&#39;m not sure I get the issue, current vhost API support setting
desc_user_addr, used_user_addr and avail_user_addr independently. So
looks ok? If not, looks not a problem to device IOTLB API itself.
<span class="quote">
&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Jason Wang &lt;jasowang@redhat.com&gt;</span>
<span class="quote">&gt; What limits amount of entries that kernel keeps around?</span>

It depends on guest working set I think. Looking at
http://dpdk.org/doc/guides/linux_gsg/sys_reqs.html:

- For 2MB page size in guest, it suggests hugepages=1024
- For 1GB page size, it suggests a hugepages=4

So I choose 2048 to make sure it can cover this.
<span class="quote">
&gt; Do we want at least a mod parameter for this?</span>

Maybe.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;  drivers/vhost/net.c        |   6 +-</span>
<span class="quote">&gt;&gt;  drivers/vhost/vhost.c      | 301 +++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;&gt;  drivers/vhost/vhost.h      |  17 ++-</span>
<span class="quote">&gt;&gt;  fs/eventfd.c               |   3 +-</span>
<span class="quote">&gt;&gt;  include/uapi/linux/vhost.h |  35 ++++++</span>
<span class="quote">&gt;&gt;  5 files changed, 320 insertions(+), 42 deletions(-)</span>
<span class="quote">&gt;&gt;</span>

[...]
<span class="quote">
&gt;&gt; +struct vhost_iotlb_entry {</span>
<span class="quote">&gt;&gt; +	__u64 iova;</span>
<span class="quote">&gt;&gt; +	__u64 size;</span>
<span class="quote">&gt;&gt; +	__u64 userspace_addr;</span>
<span class="quote">&gt; Alignment requirements?</span>

The API does not require any alignment. Will add a comment for this.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +	struct {</span>
<span class="quote">&gt;&gt; +#define VHOST_ACCESS_RO      0x1</span>
<span class="quote">&gt;&gt; +#define VHOST_ACCESS_WO      0x2</span>
<span class="quote">&gt;&gt; +#define VHOST_ACCESS_RW      0x3</span>
<span class="quote">&gt;&gt; +		__u8  perm;</span>
<span class="quote">&gt;&gt; +#define VHOST_IOTLB_MISS           1</span>
<span class="quote">&gt;&gt; +#define VHOST_IOTLB_UPDATE         2</span>
<span class="quote">&gt;&gt; +#define VHOST_IOTLB_INVALIDATE     3</span>
<span class="quote">&gt;&gt; +		__u8  type;</span>
<span class="quote">&gt;&gt; +#define VHOST_IOTLB_INVALID        0x1</span>
<span class="quote">&gt;&gt; +#define VHOST_IOTLB_VALID          0x2</span>
<span class="quote">&gt;&gt; +		__u8  valid;</span>
<span class="quote">&gt; why do we need this flag?</span>

Useless, will remove.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +		__u8  u8_padding;</span>
<span class="quote">&gt;&gt; +		__u32 padding;</span>
<span class="quote">&gt;&gt; +	} flags;</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +struct vhost_vring_iotlb_entry {</span>
<span class="quote">&gt;&gt; +	unsigned int index;</span>
<span class="quote">&gt;&gt; +	__u64 userspace_addr;</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  struct vhost_memory_region {</span>
<span class="quote">&gt;&gt;  	__u64 guest_phys_addr;</span>
<span class="quote">&gt;&gt;  	__u64 memory_size; /* bytes */</span>
<span class="quote">&gt;&gt; @@ -127,6 +153,15 @@ struct vhost_memory {</span>
<span class="quote">&gt;&gt;  /* Set eventfd to signal an error */</span>
<span class="quote">&gt;&gt;  #define VHOST_SET_VRING_ERR _IOW(VHOST_VIRTIO, 0x22, struct vhost_vring_file)</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +/* IOTLB */</span>
<span class="quote">&gt;&gt; +/* Specify an eventfd file descriptor to signle on IOTLB miss */</span>
<span class="quote">&gt; typo</span>

Will fix it.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +#define VHOST_SET_VRING_IOTLB_CALL _IOW(VHOST_VIRTIO, 0x23, struct      \</span>
<span class="quote">&gt;&gt; +                                        vhost_vring_file)</span>
<span class="quote">&gt;&gt; +#define VHOST_SET_VRING_IOTLB_REQUEST _IOW(VHOST_VIRTIO, 0x25, struct   \</span>
<span class="quote">&gt;&gt; +                                           vhost_vring_iotlb_entry)</span>
<span class="quote">&gt;&gt; +#define VHOST_UPDATE_IOTLB _IOW(VHOST_VIRTIO, 0x24, struct vhost_iotlb_entry)</span>
<span class="quote">&gt;&gt; +#define VHOST_RUN_IOTLB _IOW(VHOST_VIRTIO, 0x26, int)</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt; Is the assumption that userspace must dedicate a thread to running the iotlb? </span>
<span class="quote">&gt; I dislike this one.</span>
<span class="quote">&gt; Please support asynchronous APIs at least optionally to make</span>
<span class="quote">&gt; userspace make its own threading decisions.</span>

Nope, my qemu patches does not use a dedicated thread. This API is used
to start or top DMAR according to e.g whether guest enable DMAR for
intel IOMMU.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;  /* VHOST_NET specific defines */</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  /* Attach virtio net ring to a raw socket, or tap device.</span>
<span class="quote">&gt; Don&#39;t we need a feature bit for this?</span>

Yes we need it. The feature bit is not considered in this patch and
looks like it was still under discussion. After we finalize it, I will add.
<span class="quote">
&gt; Are we short on feature bits? If yes maybe it&#39;s time to add</span>
<span class="quote">&gt; something like PROTOCOL_FEATURES that we have in vhost-user.</span>
<span class="quote">&gt;</span>

I believe it can just work like VERSION_1, or is there anything I missed?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1984">Michael S. Tsirkin</a> - April 28, 2016, 2:43 p.m.</div>
<pre class="content">
On Thu, Apr 28, 2016 at 02:37:16PM +0800, Jason Wang wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 04/27/2016 07:45 PM, Michael S. Tsirkin wrote:</span>
<span class="quote">&gt; &gt; On Fri, Mar 25, 2016 at 10:34:34AM +0800, Jason Wang wrote:</span>
<span class="quote">&gt; &gt;&gt; This patch tries to implement an device IOTLB for vhost. This could be</span>
<span class="quote">&gt; &gt;&gt; used with for co-operation with userspace(qemu) implementation of DMA</span>
<span class="quote">&gt; &gt;&gt; remapping.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; The idea is simple. When vhost meets an IOTLB miss, it will request</span>
<span class="quote">&gt; &gt;&gt; the assistance of userspace to do the translation, this is done</span>
<span class="quote">&gt; &gt;&gt; through:</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; - Fill the translation request in a preset userspace address (This</span>
<span class="quote">&gt; &gt;&gt;   address is set through ioctl VHOST_SET_IOTLB_REQUEST_ENTRY).</span>
<span class="quote">&gt; &gt;&gt; - Notify userspace through eventfd (This eventfd was set through ioctl</span>
<span class="quote">&gt; &gt;&gt;   VHOST_SET_IOTLB_FD).</span>
<span class="quote">&gt; &gt; Why use an eventfd for this?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The aim is to implement the API all through ioctls.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;  We use them for interrupts because</span>
<span class="quote">&gt; &gt; that happens to be what kvm wants, but here - why don&#39;t we</span>
<span class="quote">&gt; &gt; just add a generic support for reading out events</span>
<span class="quote">&gt; &gt; on the vhost fd itself?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve considered this approach, but what&#39;s the advantages of this? I mean</span>
<span class="quote">&gt; looks like all other ioctls could be done through vhost fd</span>
<span class="quote">&gt; reading/writing too.</span>

read/write have a non-blocking flag.

It&#39;s not useful for other ioctls but it&#39;s useful here.
<span class="quote">

&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; - device IOTLB were started and stopped through VHOST_RUN_IOTLB ioctl</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; When userspace finishes the translation, it will update the vhost</span>
<span class="quote">&gt; &gt;&gt; IOTLB through VHOST_UPDATE_IOTLB ioctl. Userspace is also in charge of</span>
<span class="quote">&gt; &gt;&gt; snooping the IOTLB invalidation of IOMMU IOTLB and use</span>
<span class="quote">&gt; &gt;&gt; VHOST_UPDATE_IOTLB to invalidate the possible entry in vhost.</span>
<span class="quote">&gt; &gt; There&#39;s one problem here, and that is that VQs still do not undergo</span>
<span class="quote">&gt; &gt; translation.  In theory VQ could be mapped in such a way</span>
<span class="quote">&gt; &gt; that it&#39;s not contigious in userspace memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure I get the issue, current vhost API support setting</span>
<span class="quote">&gt; desc_user_addr, used_user_addr and avail_user_addr independently. So</span>
<span class="quote">&gt; looks ok? If not, looks not a problem to device IOTLB API itself.</span>

The problem is that addresses are all HVA.

Without an iommu, we ask for them to be contigious and
since bus address == GPA, this means contigious GPA =&gt;
contigious HVA. With an IOMMU you can map contigious
bus address but non contigious GPA and non contigious HVA.

Another concern: what if guest changes the GPA while keeping bus address
constant? Normal devices will work because they only use
bus addresses, but virtio will break.
<span class="quote">


&gt; &gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; Signed-off-by: Jason Wang &lt;jasowang@redhat.com&gt;</span>
<span class="quote">&gt; &gt; What limits amount of entries that kernel keeps around?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It depends on guest working set I think. Looking at</span>
<span class="quote">&gt; http://dpdk.org/doc/guides/linux_gsg/sys_reqs.html:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - For 2MB page size in guest, it suggests hugepages=1024</span>
<span class="quote">&gt; - For 1GB page size, it suggests a hugepages=4</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I choose 2048 to make sure it can cover this.</span>

4K page size is rather common, too.
<span class="quote">
&gt; &gt; Do we want at least a mod parameter for this?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Maybe.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; ---</span>
<span class="quote">&gt; &gt;&gt;  drivers/vhost/net.c        |   6 +-</span>
<span class="quote">&gt; &gt;&gt;  drivers/vhost/vhost.c      | 301 +++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt; &gt;&gt;  drivers/vhost/vhost.h      |  17 ++-</span>
<span class="quote">&gt; &gt;&gt;  fs/eventfd.c               |   3 +-</span>
<span class="quote">&gt; &gt;&gt;  include/uapi/linux/vhost.h |  35 ++++++</span>
<span class="quote">&gt; &gt;&gt;  5 files changed, 320 insertions(+), 42 deletions(-)</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt; +struct vhost_iotlb_entry {</span>
<span class="quote">&gt; &gt;&gt; +	__u64 iova;</span>
<span class="quote">&gt; &gt;&gt; +	__u64 size;</span>
<span class="quote">&gt; &gt;&gt; +	__u64 userspace_addr;</span>
<span class="quote">&gt; &gt; Alignment requirements?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The API does not require any alignment. Will add a comment for this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +	struct {</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_ACCESS_RO      0x1</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_ACCESS_WO      0x2</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_ACCESS_RW      0x3</span>
<span class="quote">&gt; &gt;&gt; +		__u8  perm;</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_IOTLB_MISS           1</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_IOTLB_UPDATE         2</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_IOTLB_INVALIDATE     3</span>
<span class="quote">&gt; &gt;&gt; +		__u8  type;</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_IOTLB_INVALID        0x1</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_IOTLB_VALID          0x2</span>
<span class="quote">&gt; &gt;&gt; +		__u8  valid;</span>
<span class="quote">&gt; &gt; why do we need this flag?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Useless, will remove.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +		__u8  u8_padding;</span>
<span class="quote">&gt; &gt;&gt; +		__u32 padding;</span>
<span class="quote">&gt; &gt;&gt; +	} flags;</span>
<span class="quote">&gt; &gt;&gt; +};</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt; +struct vhost_vring_iotlb_entry {</span>
<span class="quote">&gt; &gt;&gt; +	unsigned int index;</span>
<span class="quote">&gt; &gt;&gt; +	__u64 userspace_addr;</span>
<span class="quote">&gt; &gt;&gt; +};</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;  struct vhost_memory_region {</span>
<span class="quote">&gt; &gt;&gt;  	__u64 guest_phys_addr;</span>
<span class="quote">&gt; &gt;&gt;  	__u64 memory_size; /* bytes */</span>
<span class="quote">&gt; &gt;&gt; @@ -127,6 +153,15 @@ struct vhost_memory {</span>
<span class="quote">&gt; &gt;&gt;  /* Set eventfd to signal an error */</span>
<span class="quote">&gt; &gt;&gt;  #define VHOST_SET_VRING_ERR _IOW(VHOST_VIRTIO, 0x22, struct vhost_vring_file)</span>
<span class="quote">&gt; &gt;&gt;  </span>
<span class="quote">&gt; &gt;&gt; +/* IOTLB */</span>
<span class="quote">&gt; &gt;&gt; +/* Specify an eventfd file descriptor to signle on IOTLB miss */</span>
<span class="quote">&gt; &gt; typo</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Will fix it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_SET_VRING_IOTLB_CALL _IOW(VHOST_VIRTIO, 0x23, struct      \</span>
<span class="quote">&gt; &gt;&gt; +                                        vhost_vring_file)</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_SET_VRING_IOTLB_REQUEST _IOW(VHOST_VIRTIO, 0x25, struct   \</span>
<span class="quote">&gt; &gt;&gt; +                                           vhost_vring_iotlb_entry)</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_UPDATE_IOTLB _IOW(VHOST_VIRTIO, 0x24, struct vhost_iotlb_entry)</span>
<span class="quote">&gt; &gt;&gt; +#define VHOST_RUN_IOTLB _IOW(VHOST_VIRTIO, 0x26, int)</span>
<span class="quote">&gt; &gt;&gt; +</span>
<span class="quote">&gt; &gt; Is the assumption that userspace must dedicate a thread to running the iotlb? </span>
<span class="quote">&gt; &gt; I dislike this one.</span>
<span class="quote">&gt; &gt; Please support asynchronous APIs at least optionally to make</span>
<span class="quote">&gt; &gt; userspace make its own threading decisions.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Nope, my qemu patches does not use a dedicated thread. This API is used</span>
<span class="quote">&gt; to start or top DMAR according to e.g whether guest enable DMAR for</span>
<span class="quote">&gt; intel IOMMU.</span>

I see. Seems rather confusing - do we need to start/stop it
while device is running?
<span class="quote">

&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;  /* VHOST_NET specific defines */</span>
<span class="quote">&gt; &gt;&gt;  </span>
<span class="quote">&gt; &gt;&gt;  /* Attach virtio net ring to a raw socket, or tap device.</span>
<span class="quote">&gt; &gt; Don&#39;t we need a feature bit for this?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes we need it. The feature bit is not considered in this patch and</span>
<span class="quote">&gt; looks like it was still under discussion. After we finalize it, I will add.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Are we short on feature bits? If yes maybe it&#39;s time to add</span>
<span class="quote">&gt; &gt; something like PROTOCOL_FEATURES that we have in vhost-user.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I believe it can just work like VERSION_1, or is there anything I missed?</span>

VERSION_1 is a virtio feature though. This one would be backend specific
...
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2154">Jason Wang</a> - April 29, 2016, 1:12 a.m.</div>
<pre class="content">
On 04/28/2016 10:43 PM, Michael S. Tsirkin wrote:
<span class="quote">&gt; On Thu, Apr 28, 2016 at 02:37:16PM +0800, Jason Wang wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On 04/27/2016 07:45 PM, Michael S. Tsirkin wrote:</span>
<span class="quote">&gt;&gt;&gt; On Fri, Mar 25, 2016 at 10:34:34AM +0800, Jason Wang wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; This patch tries to implement an device IOTLB for vhost. This could be</span>
<span class="quote">&gt;&gt;&gt;&gt; used with for co-operation with userspace(qemu) implementation of DMA</span>
<span class="quote">&gt;&gt;&gt;&gt; remapping.</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; The idea is simple. When vhost meets an IOTLB miss, it will request</span>
<span class="quote">&gt;&gt;&gt;&gt; the assistance of userspace to do the translation, this is done</span>
<span class="quote">&gt;&gt;&gt;&gt; through:</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; - Fill the translation request in a preset userspace address (This</span>
<span class="quote">&gt;&gt;&gt;&gt;   address is set through ioctl VHOST_SET_IOTLB_REQUEST_ENTRY).</span>
<span class="quote">&gt;&gt;&gt;&gt; - Notify userspace through eventfd (This eventfd was set through ioctl</span>
<span class="quote">&gt;&gt;&gt;&gt;   VHOST_SET_IOTLB_FD).</span>
<span class="quote">&gt;&gt;&gt; Why use an eventfd for this?</span>
<span class="quote">&gt;&gt; The aim is to implement the API all through ioctls.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;  We use them for interrupts because</span>
<span class="quote">&gt;&gt;&gt; that happens to be what kvm wants, but here - why don&#39;t we</span>
<span class="quote">&gt;&gt;&gt; just add a generic support for reading out events</span>
<span class="quote">&gt;&gt;&gt; on the vhost fd itself?</span>
<span class="quote">&gt;&gt; I&#39;ve considered this approach, but what&#39;s the advantages of this? I mean</span>
<span class="quote">&gt;&gt; looks like all other ioctls could be done through vhost fd</span>
<span class="quote">&gt;&gt; reading/writing too.</span>
<span class="quote">&gt; read/write have a non-blocking flag.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It&#39;s not useful for other ioctls but it&#39;s useful here.</span>
<span class="quote">&gt;</span>

Ok, this looks better.
<span class="quote">
&gt;&gt;&gt;&gt; - device IOTLB were started and stopped through VHOST_RUN_IOTLB ioctl</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; When userspace finishes the translation, it will update the vhost</span>
<span class="quote">&gt;&gt;&gt;&gt; IOTLB through VHOST_UPDATE_IOTLB ioctl. Userspace is also in charge of</span>
<span class="quote">&gt;&gt;&gt;&gt; snooping the IOTLB invalidation of IOMMU IOTLB and use</span>
<span class="quote">&gt;&gt;&gt;&gt; VHOST_UPDATE_IOTLB to invalidate the possible entry in vhost.</span>
<span class="quote">&gt;&gt;&gt; There&#39;s one problem here, and that is that VQs still do not undergo</span>
<span class="quote">&gt;&gt;&gt; translation.  In theory VQ could be mapped in such a way</span>
<span class="quote">&gt;&gt;&gt; that it&#39;s not contigious in userspace memory.</span>
<span class="quote">&gt;&gt; I&#39;m not sure I get the issue, current vhost API support setting</span>
<span class="quote">&gt;&gt; desc_user_addr, used_user_addr and avail_user_addr independently. So</span>
<span class="quote">&gt;&gt; looks ok? If not, looks not a problem to device IOTLB API itself.</span>
<span class="quote">&gt; The problem is that addresses are all HVA.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Without an iommu, we ask for them to be contigious and</span>
<span class="quote">&gt; since bus address == GPA, this means contigious GPA =&gt;</span>
<span class="quote">&gt; contigious HVA. With an IOMMU you can map contigious</span>
<span class="quote">&gt; bus address but non contigious GPA and non contigious HVA.</span>

Yes, so the issue is we should not reuse VHOST_SET_VRING_ADDR and invent
a new ioctl to set bus addr (guest iova). The access the VQ through
device IOTLB too.
<span class="quote">
&gt;</span>
<span class="quote">&gt; Another concern: what if guest changes the GPA while keeping bus address</span>
<span class="quote">&gt; constant? Normal devices will work because they only use</span>
<span class="quote">&gt; bus addresses, but virtio will break.</span>

If we access VQ through device IOTLB too, this could be solved.
<span class="quote">
&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; Signed-off-by: Jason Wang &lt;jasowang@redhat.com&gt;</span>
<span class="quote">&gt;&gt;&gt; What limits amount of entries that kernel keeps around?</span>
<span class="quote">&gt;&gt; It depends on guest working set I think. Looking at</span>
<span class="quote">&gt;&gt; http://dpdk.org/doc/guides/linux_gsg/sys_reqs.html:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; - For 2MB page size in guest, it suggests hugepages=1024</span>
<span class="quote">&gt;&gt; - For 1GB page size, it suggests a hugepages=4</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So I choose 2048 to make sure it can cover this.</span>
<span class="quote">&gt; 4K page size is rather common, too.</span>

I assume hugepages is used widely, and there&#39;s a note in the above link:

&quot;For 64-bit applications, it is recommended to use 1 GB hugepages if the
platform supports them.&quot;

For 4K case, the TLB hit rate will be very low for a large working set
even in a physical environment. Not sure we should care, if we want, we
probably can cache more translations in userspace&#39;s device IOTLB
implementation.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;&gt; Do we want at least a mod parameter for this?</span>
<span class="quote">&gt;&gt; Maybe.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt;&gt;  drivers/vhost/net.c        |   6 +-</span>
<span class="quote">&gt;&gt;&gt;&gt;  drivers/vhost/vhost.c      | 301 +++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;&gt;&gt;&gt;  drivers/vhost/vhost.h      |  17 ++-</span>
<span class="quote">&gt;&gt;&gt;&gt;  fs/eventfd.c               |   3 +-</span>
<span class="quote">&gt;&gt;&gt;&gt;  include/uapi/linux/vhost.h |  35 ++++++</span>
<span class="quote">&gt;&gt;&gt;&gt;  5 files changed, 320 insertions(+), 42 deletions(-)</span>
<span class="quote">&gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt; [...]</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +struct vhost_iotlb_entry {</span>
<span class="quote">&gt;&gt;&gt;&gt; +	__u64 iova;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	__u64 size;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	__u64 userspace_addr;</span>
<span class="quote">&gt;&gt;&gt; Alignment requirements?</span>
<span class="quote">&gt;&gt; The API does not require any alignment. Will add a comment for this.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	struct {</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_ACCESS_RO      0x1</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_ACCESS_WO      0x2</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_ACCESS_RW      0x3</span>
<span class="quote">&gt;&gt;&gt;&gt; +		__u8  perm;</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_IOTLB_MISS           1</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_IOTLB_UPDATE         2</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_IOTLB_INVALIDATE     3</span>
<span class="quote">&gt;&gt;&gt;&gt; +		__u8  type;</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_IOTLB_INVALID        0x1</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_IOTLB_VALID          0x2</span>
<span class="quote">&gt;&gt;&gt;&gt; +		__u8  valid;</span>
<span class="quote">&gt;&gt;&gt; why do we need this flag?</span>
<span class="quote">&gt;&gt; Useless, will remove.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +		__u8  u8_padding;</span>
<span class="quote">&gt;&gt;&gt;&gt; +		__u32 padding;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	} flags;</span>
<span class="quote">&gt;&gt;&gt;&gt; +};</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt; +struct vhost_vring_iotlb_entry {</span>
<span class="quote">&gt;&gt;&gt;&gt; +	unsigned int index;</span>
<span class="quote">&gt;&gt;&gt;&gt; +	__u64 userspace_addr;</span>
<span class="quote">&gt;&gt;&gt;&gt; +};</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt;&gt;  struct vhost_memory_region {</span>
<span class="quote">&gt;&gt;&gt;&gt;  	__u64 guest_phys_addr;</span>
<span class="quote">&gt;&gt;&gt;&gt;  	__u64 memory_size; /* bytes */</span>
<span class="quote">&gt;&gt;&gt;&gt; @@ -127,6 +153,15 @@ struct vhost_memory {</span>
<span class="quote">&gt;&gt;&gt;&gt;  /* Set eventfd to signal an error */</span>
<span class="quote">&gt;&gt;&gt;&gt;  #define VHOST_SET_VRING_ERR _IOW(VHOST_VIRTIO, 0x22, struct vhost_vring_file)</span>
<span class="quote">&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt; +/* IOTLB */</span>
<span class="quote">&gt;&gt;&gt;&gt; +/* Specify an eventfd file descriptor to signle on IOTLB miss */</span>
<span class="quote">&gt;&gt;&gt; typo</span>
<span class="quote">&gt;&gt; Will fix it.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_SET_VRING_IOTLB_CALL _IOW(VHOST_VIRTIO, 0x23, struct      \</span>
<span class="quote">&gt;&gt;&gt;&gt; +                                        vhost_vring_file)</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_SET_VRING_IOTLB_REQUEST _IOW(VHOST_VIRTIO, 0x25, struct   \</span>
<span class="quote">&gt;&gt;&gt;&gt; +                                           vhost_vring_iotlb_entry)</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_UPDATE_IOTLB _IOW(VHOST_VIRTIO, 0x24, struct vhost_iotlb_entry)</span>
<span class="quote">&gt;&gt;&gt;&gt; +#define VHOST_RUN_IOTLB _IOW(VHOST_VIRTIO, 0x26, int)</span>
<span class="quote">&gt;&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; Is the assumption that userspace must dedicate a thread to running the iotlb? </span>
<span class="quote">&gt;&gt;&gt; I dislike this one.</span>
<span class="quote">&gt;&gt;&gt; Please support asynchronous APIs at least optionally to make</span>
<span class="quote">&gt;&gt;&gt; userspace make its own threading decisions.</span>
<span class="quote">&gt;&gt; Nope, my qemu patches does not use a dedicated thread. This API is used</span>
<span class="quote">&gt;&gt; to start or top DMAR according to e.g whether guest enable DMAR for</span>
<span class="quote">&gt;&gt; intel IOMMU.</span>
<span class="quote">&gt; I see. Seems rather confusing - do we need to start/stop it</span>
<span class="quote">&gt; while device is running?</span>

Technically, guest driver (e.g intel ioomu) can stop DMAR at any time.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;  /* VHOST_NET specific defines */</span>
<span class="quote">&gt;&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;&gt;  /* Attach virtio net ring to a raw socket, or tap device.</span>
<span class="quote">&gt;&gt;&gt; Don&#39;t we need a feature bit for this?</span>
<span class="quote">&gt;&gt; Yes we need it. The feature bit is not considered in this patch and</span>
<span class="quote">&gt;&gt; looks like it was still under discussion. After we finalize it, I will add.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Are we short on feature bits? If yes maybe it&#39;s time to add</span>
<span class="quote">&gt;&gt;&gt; something like PROTOCOL_FEATURES that we have in vhost-user.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt; I believe it can just work like VERSION_1, or is there anything I missed?</span>
<span class="quote">&gt; VERSION_1 is a virtio feature though. This one would be backend specific</span>
<span class="quote">&gt; ...</span>

Any differences? Consider we want feature to be something like
VIRTIO_F_HOST_IOMMU, vhost could just add this to VHOST_FEATURES?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2154">Jason Wang</a> - April 29, 2016, 4:44 a.m.</div>
<pre class="content">
On 04/29/2016 09:12 AM, Jason Wang wrote:
<span class="quote">&gt; On 04/28/2016 10:43 PM, Michael S. Tsirkin wrote:</span>
<span class="quote">&gt;&gt; &gt; On Thu, Apr 28, 2016 at 02:37:16PM +0800, Jason Wang wrote:</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; On 04/27/2016 07:45 PM, Michael S. Tsirkin wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt;&gt; On Fri, Mar 25, 2016 at 10:34:34AM +0800, Jason Wang wrote:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; This patch tries to implement an device IOTLB for vhost. This could be</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; used with for co-operation with userspace(qemu) implementation of DMA</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; remapping.</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; The idea is simple. When vhost meets an IOTLB miss, it will request</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; the assistance of userspace to do the translation, this is done</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; through:</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; - Fill the translation request in a preset userspace address (This</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;   address is set through ioctl VHOST_SET_IOTLB_REQUEST_ENTRY).</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; - Notify userspace through eventfd (This eventfd was set through ioctl</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;   VHOST_SET_IOTLB_FD).</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt;&gt; Why use an eventfd for this?</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; The aim is to implement the API all through ioctls.</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt;&gt;  We use them for interrupts because</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt;&gt; that happens to be what kvm wants, but here - why don&#39;t we</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt;&gt; just add a generic support for reading out events</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt;&gt; on the vhost fd itself?</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; I&#39;ve considered this approach, but what&#39;s the advantages of this? I mean</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; looks like all other ioctls could be done through vhost fd</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; reading/writing too.</span>
<span class="quote">&gt;&gt; &gt; read/write have a non-blocking flag.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; It&#39;s not useful for other ioctls but it&#39;s useful here.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt; Ok, this looks better.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; - device IOTLB were started and stopped through VHOST_RUN_IOTLB ioctl</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; When userspace finishes the translation, it will update the vhost</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; IOTLB through VHOST_UPDATE_IOTLB ioctl. Userspace is also in charge of</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; snooping the IOTLB invalidation of IOMMU IOTLB and use</span>
<span class="quote">&gt;&gt;&gt;&gt;&gt; &gt;&gt;&gt;&gt; VHOST_UPDATE_IOTLB to invalidate the possible entry in vhost.</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt;&gt; There&#39;s one problem here, and that is that VQs still do not undergo</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt;&gt; translation.  In theory VQ could be mapped in such a way</span>
<span class="quote">&gt;&gt;&gt;&gt; &gt;&gt;&gt; that it&#39;s not contigious in userspace memory.</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; I&#39;m not sure I get the issue, current vhost API support setting</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; desc_user_addr, used_user_addr and avail_user_addr independently. So</span>
<span class="quote">&gt;&gt;&gt; &gt;&gt; looks ok? If not, looks not a problem to device IOTLB API itself.</span>
<span class="quote">&gt;&gt; &gt; The problem is that addresses are all HVA.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Without an iommu, we ask for them to be contigious and</span>
<span class="quote">&gt;&gt; &gt; since bus address == GPA, this means contigious GPA =&gt;</span>
<span class="quote">&gt;&gt; &gt; contigious HVA. With an IOMMU you can map contigious</span>
<span class="quote">&gt;&gt; &gt; bus address but non contigious GPA and non contigious HVA.</span>
<span class="quote">&gt; Yes, so the issue is we should not reuse VHOST_SET_VRING_ADDR and invent</span>
<span class="quote">&gt; a new ioctl to set bus addr (guest iova). The access the VQ through</span>
<span class="quote">&gt; device IOTLB too.</span>

Note that userspace has checked for this and fallback to userspace if it
detects non contiguous GPA. Consider this happens rare, I&#39;m not sure we
should handle this.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Another concern: what if guest changes the GPA while keeping bus address</span>
<span class="quote">&gt;&gt; &gt; constant? Normal devices will work because they only use</span>
<span class="quote">&gt;&gt; &gt; bus addresses, but virtio will break.</span>
<span class="quote">&gt; If we access VQ through device IOTLB too, this could be solved.</span>
<span class="quote">&gt;</span>

I don&#39;t see a reason why guest want change GPA during DMA. Even if it
can, it needs lots of other synchronization.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/vhost/net.c b/drivers/vhost/net.c</span>
<span class="p_header">index 481db96..7cbdeed 100644</span>
<span class="p_header">--- a/drivers/vhost/net.c</span>
<span class="p_header">+++ b/drivers/vhost/net.c</span>
<span class="p_chunk">@@ -334,7 +334,7 @@</span> <span class="p_context"> static void handle_tx(struct vhost_net *net)</span>
 		head = vhost_get_vq_desc(vq, vq-&gt;iov,
 					 ARRAY_SIZE(vq-&gt;iov),
 					 &amp;out, &amp;in,
<span class="p_del">-					 NULL, NULL);</span>
<span class="p_add">+					 NULL, NULL, VHOST_ACCESS_RO);</span>
 		/* On error, stop handling until the next kick. */
 		if (unlikely(head &lt; 0))
 			break;
<span class="p_chunk">@@ -470,7 +470,7 @@</span> <span class="p_context"> static int get_rx_bufs(struct vhost_virtqueue *vq,</span>
 		}
 		r = vhost_get_vq_desc(vq, vq-&gt;iov + seg,
 				      ARRAY_SIZE(vq-&gt;iov) - seg, &amp;out,
<span class="p_del">-				      &amp;in, log, log_num);</span>
<span class="p_add">+				      &amp;in, log, log_num, VHOST_ACCESS_WO);</span>
 		if (unlikely(r &lt; 0))
 			goto err;
 
<span class="p_chunk">@@ -1083,7 +1083,7 @@</span> <span class="p_context"> static long vhost_net_ioctl(struct file *f, unsigned int ioctl,</span>
 		r = vhost_dev_ioctl(&amp;n-&gt;dev, ioctl, argp);
 		if (r == -ENOIOCTLCMD)
 			r = vhost_vring_ioctl(&amp;n-&gt;dev, ioctl, argp);
<span class="p_del">-		else</span>
<span class="p_add">+		else if (ioctl != VHOST_UPDATE_IOTLB)</span>
 			vhost_net_flush(n);
 		mutex_unlock(&amp;n-&gt;dev.mutex);
 		return r;
<span class="p_header">diff --git a/drivers/vhost/vhost.c b/drivers/vhost/vhost.c</span>
<span class="p_header">index 32c35a9..1dd43e8 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.c</span>
<span class="p_header">+++ b/drivers/vhost/vhost.c</span>
<span class="p_chunk">@@ -280,6 +280,10 @@</span> <span class="p_context"> static void vhost_vq_reset(struct vhost_dev *dev,</span>
 	vq-&gt;call_ctx = NULL;
 	vq-&gt;call = NULL;
 	vq-&gt;log_ctx = NULL;
<span class="p_add">+	vq-&gt;iotlb_call = NULL;</span>
<span class="p_add">+	vq-&gt;iotlb_call_ctx = NULL;</span>
<span class="p_add">+	vq-&gt;iotlb_request = NULL;</span>
<span class="p_add">+	vq-&gt;pending_request.flags.type = VHOST_IOTLB_INVALIDATE;</span>
 	vq-&gt;umem = NULL;
 	vq-&gt;is_le = virtio_legacy_is_little_endian();
 	vhost_vq_reset_user_be(vq);
<span class="p_chunk">@@ -387,8 +391,10 @@</span> <span class="p_context"> void vhost_dev_init(struct vhost_dev *dev,</span>
 	dev-&gt;log_ctx = NULL;
 	dev-&gt;log_file = NULL;
 	dev-&gt;umem = NULL;
<span class="p_add">+	dev-&gt;iotlb = NULL;</span>
 	dev-&gt;mm = NULL;
 	spin_lock_init(&amp;dev-&gt;work_lock);
<span class="p_add">+	spin_lock_init(&amp;dev-&gt;iotlb_lock);</span>
 	INIT_LIST_HEAD(&amp;dev-&gt;work_list);
 	dev-&gt;worker = NULL;
 
<span class="p_chunk">@@ -537,6 +543,15 @@</span> <span class="p_context"> void vhost_dev_stop(struct vhost_dev *dev)</span>
 }
 EXPORT_SYMBOL_GPL(vhost_dev_stop);
 
<span class="p_add">+static void vhost_umem_free(struct vhost_umem *umem,</span>
<span class="p_add">+			    struct vhost_umem_node *node)</span>
<span class="p_add">+{</span>
<span class="p_add">+	vhost_umem_interval_tree_remove(node, &amp;umem-&gt;umem_tree);</span>
<span class="p_add">+	list_del(&amp;node-&gt;link);</span>
<span class="p_add">+	kfree(node);</span>
<span class="p_add">+	umem-&gt;numem--;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void vhost_umem_clean(struct vhost_umem *umem)
 {
 	struct vhost_umem_node *node, *tmp;
<span class="p_chunk">@@ -544,11 +559,9 @@</span> <span class="p_context"> static void vhost_umem_clean(struct vhost_umem *umem)</span>
 	if (!umem)
 		return;
 
<span class="p_del">-	list_for_each_entry_safe(node, tmp, &amp;umem-&gt;umem_list, link) {</span>
<span class="p_del">-		vhost_umem_interval_tree_remove(node, &amp;umem-&gt;umem_tree);</span>
<span class="p_del">-		list_del(&amp;node-&gt;link);</span>
<span class="p_del">-		kvfree(node);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	list_for_each_entry_safe(node, tmp, &amp;umem-&gt;umem_list, link)</span>
<span class="p_add">+		vhost_umem_free(umem, node);</span>
<span class="p_add">+</span>
 	kvfree(umem);
 }
 
<span class="p_chunk">@@ -580,6 +593,8 @@</span> <span class="p_context"> void vhost_dev_cleanup(struct vhost_dev *dev, bool locked)</span>
 	/* No one will access memory at this point */
 	vhost_umem_clean(dev-&gt;umem);
 	dev-&gt;umem = NULL;
<span class="p_add">+	vhost_umem_clean(dev-&gt;iotlb);</span>
<span class="p_add">+	dev-&gt;iotlb = NULL;</span>
 	WARN_ON(!list_empty(&amp;dev-&gt;work_list));
 	if (dev-&gt;worker) {
 		kthread_stop(dev-&gt;worker);
<span class="p_chunk">@@ -699,11 +714,61 @@</span> <span class="p_context"> int vhost_vq_access_ok(struct vhost_virtqueue *vq)</span>
 }
 EXPORT_SYMBOL_GPL(vhost_vq_access_ok);
 
<span class="p_add">+static int vhost_new_umem_range(struct vhost_umem *umem,</span>
<span class="p_add">+				u64 start, u64 size, u64 end,</span>
<span class="p_add">+				u64 userspace_addr, int perm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_umem_node *tmp, *node = kmalloc(sizeof(*node), GFP_ATOMIC);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!node)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (umem-&gt;numem == VHOST_IOTLB_SIZE) {</span>
<span class="p_add">+		tmp = list_last_entry(&amp;umem-&gt;umem_list, typeof(*tmp), link);</span>
<span class="p_add">+		vhost_umem_free(umem, tmp);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	node-&gt;start = start;</span>
<span class="p_add">+	node-&gt;size = size;</span>
<span class="p_add">+	node-&gt;last = end;</span>
<span class="p_add">+	node-&gt;userspace_addr = userspace_addr;</span>
<span class="p_add">+	node-&gt;perm = perm;</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;node-&gt;link);</span>
<span class="p_add">+	list_add_tail(&amp;node-&gt;link, &amp;umem-&gt;umem_list);</span>
<span class="p_add">+	vhost_umem_interval_tree_insert(node, &amp;umem-&gt;umem_tree);</span>
<span class="p_add">+	umem-&gt;numem++;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void vhost_del_umem_range(struct vhost_umem *umem,</span>
<span class="p_add">+				 u64 start, u64 end)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_umem_node *node;</span>
<span class="p_add">+</span>
<span class="p_add">+	while ((node = vhost_umem_interval_tree_iter_first(&amp;umem-&gt;umem_tree,</span>
<span class="p_add">+							   start, end)))</span>
<span class="p_add">+		vhost_umem_free(umem, node);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct vhost_umem *vhost_umem_alloc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_umem *umem = vhost_kvzalloc(sizeof(*umem));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!umem)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	umem-&gt;umem_tree = RB_ROOT;</span>
<span class="p_add">+	umem-&gt;numem = 0;</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;umem-&gt;umem_list);</span>
<span class="p_add">+</span>
<span class="p_add">+	return umem;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)
 {
 	struct vhost_memory mem, *newmem;
 	struct vhost_memory_region *region;
<span class="p_del">-	struct vhost_umem_node *node;</span>
 	struct vhost_umem *newumem, *oldumem;
 	unsigned long size = offsetof(struct vhost_memory, regions);
 	int i;
<span class="p_chunk">@@ -725,28 +790,23 @@</span> <span class="p_context"> static long vhost_set_memory(struct vhost_dev *d, struct vhost_memory __user *m)</span>
 		return -EFAULT;
 	}
 
<span class="p_del">-	newumem = vhost_kvzalloc(sizeof(*newumem));</span>
<span class="p_add">+	newumem = vhost_umem_alloc();</span>
 	if (!newumem) {
 		kvfree(newmem);
 		return -ENOMEM;
 	}
 
<span class="p_del">-	newumem-&gt;umem_tree = RB_ROOT;</span>
<span class="p_del">-	INIT_LIST_HEAD(&amp;newumem-&gt;umem_list);</span>
<span class="p_del">-</span>
 	for (region = newmem-&gt;regions;
 	     region &lt; newmem-&gt;regions + mem.nregions;
 	     region++) {
<span class="p_del">-		node = vhost_kvzalloc(sizeof(*node));</span>
<span class="p_del">-		if (!node)</span>
<span class="p_add">+		if (vhost_new_umem_range(newumem,</span>
<span class="p_add">+					 region-&gt;guest_phys_addr,</span>
<span class="p_add">+					 region-&gt;memory_size,</span>
<span class="p_add">+					 region-&gt;guest_phys_addr +</span>
<span class="p_add">+					 region-&gt;memory_size - 1,</span>
<span class="p_add">+					 region-&gt;userspace_addr,</span>
<span class="p_add">+				         VHOST_ACCESS_RW))</span>
 			goto err;
<span class="p_del">-		node-&gt;start = region-&gt;guest_phys_addr;</span>
<span class="p_del">-		node-&gt;size = region-&gt;memory_size;</span>
<span class="p_del">-		node-&gt;last = node-&gt;start + node-&gt;size - 1;</span>
<span class="p_del">-		node-&gt;userspace_addr = region-&gt;userspace_addr;</span>
<span class="p_del">-		INIT_LIST_HEAD(&amp;node-&gt;link);</span>
<span class="p_del">-		list_add_tail(&amp;node-&gt;link, &amp;newumem-&gt;umem_list);</span>
<span class="p_del">-		vhost_umem_interval_tree_insert(node, &amp;newumem-&gt;umem_tree);</span>
 	}
 
 	if (!memory_access_ok(d, newumem, 0))
<span class="p_chunk">@@ -782,6 +842,7 @@</span> <span class="p_context"> long vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)</span>
 	struct vhost_vring_state s;
 	struct vhost_vring_file f;
 	struct vhost_vring_addr a;
<span class="p_add">+	struct vhost_vring_iotlb_entry e;</span>
 	u32 idx;
 	long r;
 
<span class="p_chunk">@@ -910,6 +971,35 @@</span> <span class="p_context"> long vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)</span>
 		} else
 			filep = eventfp;
 		break;
<span class="p_add">+	case VHOST_SET_VRING_IOTLB_REQUEST:</span>
<span class="p_add">+		r = -EFAULT;</span>
<span class="p_add">+		if (copy_from_user(&amp;e, argp, sizeof e))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		if (!access_ok(VERIFY_WRITE, e.userspace_addr,</span>
<span class="p_add">+				sizeof(*vq-&gt;iotlb_request)))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		r = 0;</span>
<span class="p_add">+		vq-&gt;iotlb_request = (struct vhost_iotlb_entry __user *)e.userspace_addr;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case VHOST_SET_VRING_IOTLB_CALL:</span>
<span class="p_add">+		if (copy_from_user(&amp;f, argp, sizeof f)) {</span>
<span class="p_add">+			r = -EFAULT;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		eventfp = f.fd == -1 ? NULL : eventfd_fget(f.fd);</span>
<span class="p_add">+		if (IS_ERR(eventfp)) {</span>
<span class="p_add">+			r = PTR_ERR(eventfp);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (eventfp != vq-&gt;iotlb_call) {</span>
<span class="p_add">+			filep = vq-&gt;iotlb_call;</span>
<span class="p_add">+			ctx = vq-&gt;iotlb_call_ctx;</span>
<span class="p_add">+			vq-&gt;iotlb_call = eventfp;</span>
<span class="p_add">+			vq-&gt;iotlb_call_ctx = eventfp ?</span>
<span class="p_add">+				eventfd_ctx_fileget(eventfp) : NULL;</span>
<span class="p_add">+		} else</span>
<span class="p_add">+			filep = eventfp;</span>
<span class="p_add">+		break;</span>
 	case VHOST_SET_VRING_CALL:
 		if (copy_from_user(&amp;f, argp, sizeof f)) {
 			r = -EFAULT;
<span class="p_chunk">@@ -977,11 +1067,55 @@</span> <span class="p_context"> long vhost_vring_ioctl(struct vhost_dev *d, int ioctl, void __user *argp)</span>
 }
 EXPORT_SYMBOL_GPL(vhost_vring_ioctl);
 
<span class="p_add">+static int vhost_init_device_iotlb(struct vhost_dev *d, bool enabled)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_umem *niotlb, *oiotlb;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (enabled) {</span>
<span class="p_add">+		niotlb = vhost_umem_alloc();</span>
<span class="p_add">+		if (!niotlb)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+	} else</span>
<span class="p_add">+		niotlb = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;d-&gt;iotlb_lock);</span>
<span class="p_add">+	oiotlb = d-&gt;iotlb;</span>
<span class="p_add">+	d-&gt;iotlb = niotlb;</span>
<span class="p_add">+	spin_unlock(&amp;d-&gt;iotlb_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	vhost_umem_clean(oiotlb);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void vhost_complete_iotlb_update(struct vhost_dev *d,</span>
<span class="p_add">+					struct vhost_iotlb_entry *entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_iotlb_entry *req;</span>
<span class="p_add">+	struct vhost_virtqueue *vq;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; d-&gt;nvqs; i++) {</span>
<span class="p_add">+		vq = d-&gt;vqs[i];</span>
<span class="p_add">+		mutex_lock(&amp;vq-&gt;mutex);</span>
<span class="p_add">+		req = &amp;vq-&gt;pending_request;</span>
<span class="p_add">+		if (entry-&gt;iova &lt;= req-&gt;iova &amp;&amp;</span>
<span class="p_add">+		    entry-&gt;iova + entry-&gt;size - 1 &gt; req-&gt;iova &amp;&amp;</span>
<span class="p_add">+		    req-&gt;flags.type == VHOST_IOTLB_MISS) {</span>
<span class="p_add">+			*req = *entry;</span>
<span class="p_add">+			vhost_poll_queue(&amp;vq-&gt;poll);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		mutex_unlock(&amp;vq-&gt;mutex);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Caller must have device mutex */
 long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)
 {
 	struct file *eventfp, *filep = NULL;
 	struct eventfd_ctx *ctx = NULL;
<span class="p_add">+	struct vhost_iotlb_entry entry;</span>
 	u64 p;
 	long r;
 	int i, fd;
<span class="p_chunk">@@ -1050,6 +1184,52 @@</span> <span class="p_context"> long vhost_dev_ioctl(struct vhost_dev *d, unsigned int ioctl, void __user *argp)</span>
 		if (filep)
 			fput(filep);
 		break;
<span class="p_add">+	case VHOST_RUN_IOTLB:</span>
<span class="p_add">+		/* FIXME: enable and disabled */</span>
<span class="p_add">+		vhost_init_device_iotlb(d, true);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case VHOST_UPDATE_IOTLB:</span>
<span class="p_add">+		r = copy_from_user(&amp;entry, argp, sizeof(entry));</span>
<span class="p_add">+		if (r &lt; 0) {</span>
<span class="p_add">+			r = -EFAULT;</span>
<span class="p_add">+			goto done;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;d-&gt;iotlb_lock);</span>
<span class="p_add">+		if (!d-&gt;iotlb) {</span>
<span class="p_add">+			spin_unlock(&amp;d-&gt;iotlb_lock);</span>
<span class="p_add">+			r = -EFAULT;</span>
<span class="p_add">+			goto done;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		switch (entry.flags.type) {</span>
<span class="p_add">+		case VHOST_IOTLB_UPDATE:</span>
<span class="p_add">+			if (entry.flags.valid != VHOST_IOTLB_VALID) {</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			if (vhost_new_umem_range(d-&gt;iotlb,</span>
<span class="p_add">+						 entry.iova,</span>
<span class="p_add">+						 entry.size,</span>
<span class="p_add">+						 entry.iova + entry.size - 1,</span>
<span class="p_add">+                                                 entry.userspace_addr,</span>
<span class="p_add">+                                                 entry.flags.perm)) {</span>
<span class="p_add">+				r = -ENOMEM;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case VHOST_IOTLB_INVALIDATE:</span>
<span class="p_add">+			vhost_del_umem_range(d-&gt;iotlb,</span>
<span class="p_add">+					     entry.iova,</span>
<span class="p_add">+					     entry.iova + entry.size - 1);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			r = -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		spin_unlock(&amp;d-&gt;iotlb_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!r &amp;&amp; entry.flags.type != VHOST_IOTLB_INVALIDATE)</span>
<span class="p_add">+			vhost_complete_iotlb_update(d, &amp;entry);</span>
<span class="p_add">+</span>
<span class="p_add">+		break;</span>
 	default:
 		r = -ENOIOCTLCMD;
 		break;
<span class="p_chunk">@@ -1197,27 +1377,69 @@</span> <span class="p_context"> int vhost_init_used(struct vhost_virtqueue *vq)</span>
 }
 EXPORT_SYMBOL_GPL(vhost_init_used);
 
<span class="p_add">+static int vhost_iotlb_miss(struct vhost_virtqueue *vq, u64 iova)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vhost_iotlb_entry *pending = &amp;vq-&gt;pending_request;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!vq-&gt;iotlb_call_ctx)</span>
<span class="p_add">+		return -EOPNOTSUPP;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!vq-&gt;iotlb_request)</span>
<span class="p_add">+		return -EOPNOTSUPP;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pending-&gt;flags.type == VHOST_IOTLB_MISS) {</span>
<span class="p_add">+		return -EEXIST;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pending-&gt;iova = iova;</span>
<span class="p_add">+	pending-&gt;flags.type = VHOST_IOTLB_MISS;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = __copy_to_user(vq-&gt;iotlb_request, pending,</span>
<span class="p_add">+			     sizeof(struct vhost_iotlb_entry));</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vq-&gt;iotlb_call_ctx)</span>
<span class="p_add">+		eventfd_signal(vq-&gt;iotlb_call_ctx, 1);</span>
<span class="p_add">+err:</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,
<span class="p_del">-			  struct iovec iov[], int iov_size)</span>
<span class="p_add">+			  struct iovec iov[], int iov_size, int access)</span>
 {
 	const struct vhost_umem_node *node;
<span class="p_del">-	struct vhost_umem *umem = vq-&gt;umem;</span>
<span class="p_add">+	struct vhost_dev *dev = vq-&gt;dev;</span>
<span class="p_add">+	struct vhost_umem *umem = dev-&gt;iotlb ? dev-&gt;iotlb : dev-&gt;umem;</span>
 	struct iovec *_iov;
 	u64 s = 0;
 	int ret = 0;
 
<span class="p_add">+	spin_lock(&amp;dev-&gt;iotlb_lock);</span>
<span class="p_add">+</span>
 	while ((u64)len &gt; s) {
 		u64 size;
 		if (unlikely(ret &gt;= iov_size)) {
 			ret = -ENOBUFS;
 			break;
 		}
<span class="p_add">+</span>
 		node = vhost_umem_interval_tree_iter_first(&amp;umem-&gt;umem_tree,
 							addr, addr + len - 1);
 		if (node == NULL || node-&gt;start &gt; addr) {
<span class="p_del">-			ret = -EFAULT;</span>
<span class="p_add">+			if (umem != dev-&gt;iotlb) {</span>
<span class="p_add">+				ret = -EFAULT;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			ret = -EAGAIN;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		} else if (!(node-&gt;perm &amp; access)) {</span>
<span class="p_add">+			ret = -EPERM;</span>
 			break;
 		}
<span class="p_add">+</span>
 		_iov = iov + ret;
 		size = node-&gt;size - addr + node-&gt;start;
 		_iov-&gt;iov_len = min((u64)len - s, size);
<span class="p_chunk">@@ -1228,6 +1450,10 @@</span> <span class="p_context"> static int translate_desc(struct vhost_virtqueue *vq, u64 addr, u32 len,</span>
 		++ret;
 	}
 
<span class="p_add">+	spin_unlock(&amp;dev-&gt;iotlb_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ret == -EAGAIN)</span>
<span class="p_add">+		vhost_iotlb_miss(vq, addr);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1256,7 +1482,7 @@</span> <span class="p_context"> static int get_indirect(struct vhost_virtqueue *vq,</span>
 			struct iovec iov[], unsigned int iov_size,
 			unsigned int *out_num, unsigned int *in_num,
 			struct vhost_log *log, unsigned int *log_num,
<span class="p_del">-			struct vring_desc *indirect)</span>
<span class="p_add">+			struct vring_desc *indirect, int access)</span>
 {
 	struct vring_desc desc;
 	unsigned int i = 0, count, found = 0;
<span class="p_chunk">@@ -1274,9 +1500,10 @@</span> <span class="p_context"> static int get_indirect(struct vhost_virtqueue *vq,</span>
 	}
 
 	ret = translate_desc(vq, vhost64_to_cpu(vq, indirect-&gt;addr), len, vq-&gt;indirect,
<span class="p_del">-			     UIO_MAXIOV);</span>
<span class="p_add">+			     UIO_MAXIOV, access);</span>
 	if (unlikely(ret &lt; 0)) {
<span class="p_del">-		vq_err(vq, &quot;Translation failure %d in indirect.\n&quot;, ret);</span>
<span class="p_add">+		if (ret != -EAGAIN)</span>
<span class="p_add">+			vq_err(vq, &quot;Translation failure %d in indirect.\n&quot;, ret);</span>
 		return ret;
 	}
 	iov_iter_init(&amp;from, READ, vq-&gt;indirect, ret, len);
<span class="p_chunk">@@ -1316,10 +1543,11 @@</span> <span class="p_context"> static int get_indirect(struct vhost_virtqueue *vq,</span>
 
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
<span class="p_del">-				     iov_size - iov_count);</span>
<span class="p_add">+				     iov_size - iov_count, access);</span>
 		if (unlikely(ret &lt; 0)) {
<span class="p_del">-			vq_err(vq, &quot;Translation failure %d indirect idx %d\n&quot;,</span>
<span class="p_del">-			       ret, i);</span>
<span class="p_add">+			if (ret != -EAGAIN)</span>
<span class="p_add">+				vq_err(vq, &quot;Translation failure %d indirect idx %d\n&quot;,</span>
<span class="p_add">+					ret, i);</span>
 			return ret;
 		}
 		/* If this is an input descriptor, increment that count. */
<span class="p_chunk">@@ -1355,7 +1583,8 @@</span> <span class="p_context"> static int get_indirect(struct vhost_virtqueue *vq,</span>
 int vhost_get_vq_desc(struct vhost_virtqueue *vq,
 		      struct iovec iov[], unsigned int iov_size,
 		      unsigned int *out_num, unsigned int *in_num,
<span class="p_del">-		      struct vhost_log *log, unsigned int *log_num)</span>
<span class="p_add">+		      struct vhost_log *log, unsigned int *log_num,</span>
<span class="p_add">+		      int access)</span>
 {
 	struct vring_desc desc;
 	unsigned int i, head, found = 0;
<span class="p_chunk">@@ -1433,10 +1662,11 @@</span> <span class="p_context"> int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
 		if (desc.flags &amp; cpu_to_vhost16(vq, VRING_DESC_F_INDIRECT)) {
 			ret = get_indirect(vq, iov, iov_size,
 					   out_num, in_num,
<span class="p_del">-					   log, log_num, &amp;desc);</span>
<span class="p_add">+					   log, log_num, &amp;desc, access);</span>
 			if (unlikely(ret &lt; 0)) {
<span class="p_del">-				vq_err(vq, &quot;Failure detected &quot;</span>
<span class="p_del">-				       &quot;in indirect descriptor at idx %d\n&quot;, i);</span>
<span class="p_add">+				if (ret != -EAGAIN)</span>
<span class="p_add">+					vq_err(vq, &quot;Failure detected &quot;</span>
<span class="p_add">+						&quot;in indirect descriptor at idx %d\n&quot;, i);</span>
 				return ret;
 			}
 			continue;
<span class="p_chunk">@@ -1444,10 +1674,11 @@</span> <span class="p_context"> int vhost_get_vq_desc(struct vhost_virtqueue *vq,</span>
 
 		ret = translate_desc(vq, vhost64_to_cpu(vq, desc.addr),
 				     vhost32_to_cpu(vq, desc.len), iov + iov_count,
<span class="p_del">-				     iov_size - iov_count);</span>
<span class="p_add">+				     iov_size - iov_count, access);</span>
 		if (unlikely(ret &lt; 0)) {
<span class="p_del">-			vq_err(vq, &quot;Translation failure %d descriptor idx %d\n&quot;,</span>
<span class="p_del">-			       ret, i);</span>
<span class="p_add">+			if (ret != -EAGAIN)</span>
<span class="p_add">+				vq_err(vq, &quot;Translation failure %d descriptor idx %d\n&quot;,</span>
<span class="p_add">+					ret, i);</span>
 			return ret;
 		}
 		if (desc.flags &amp; cpu_to_vhost16(vq, VRING_DESC_F_WRITE)) {
<span class="p_header">diff --git a/drivers/vhost/vhost.h b/drivers/vhost/vhost.h</span>
<span class="p_header">index 5d64393..4365104 100644</span>
<span class="p_header">--- a/drivers/vhost/vhost.h</span>
<span class="p_header">+++ b/drivers/vhost/vhost.h</span>
<span class="p_chunk">@@ -62,13 +62,15 @@</span> <span class="p_context"> struct vhost_umem_node {</span>
 	__u64 last;
 	__u64 size;
 	__u64 userspace_addr;
<span class="p_del">-	__u64 flags_padding;</span>
<span class="p_add">+	__u32 perm;</span>
<span class="p_add">+	__u32 flags_padding;</span>
 	__u64 __subtree_last;
 };
 
 struct vhost_umem {
 	struct rb_root umem_tree;
 	struct list_head umem_list;
<span class="p_add">+	int numem;</span>
 };
 
 /* The virtqueue structure describes a queue attached to a device. */
<span class="p_chunk">@@ -84,9 +86,13 @@</span> <span class="p_context"> struct vhost_virtqueue {</span>
 	struct file *kick;
 	struct file *call;
 	struct file *error;
<span class="p_add">+	struct file *iotlb_call;</span>
 	struct eventfd_ctx *call_ctx;
 	struct eventfd_ctx *error_ctx;
 	struct eventfd_ctx *log_ctx;
<span class="p_add">+	struct eventfd_ctx *iotlb_call_ctx;</span>
<span class="p_add">+	struct vhost_iotlb_entry __user *iotlb_request;</span>
<span class="p_add">+	struct vhost_iotlb_entry pending_request;</span>
 
 	struct vhost_poll poll;
 
<span class="p_chunk">@@ -135,6 +141,8 @@</span> <span class="p_context"> struct vhost_virtqueue {</span>
 #endif
 };
 
<span class="p_add">+#define VHOST_IOTLB_SIZE 2048</span>
<span class="p_add">+</span>
 struct vhost_dev {
 	struct mm_struct *mm;
 	struct mutex mutex;
<span class="p_chunk">@@ -146,6 +154,8 @@</span> <span class="p_context"> struct vhost_dev {</span>
 	struct list_head work_list;
 	struct task_struct *worker;
 	struct vhost_umem *umem;
<span class="p_add">+	struct vhost_umem *iotlb;</span>
<span class="p_add">+	spinlock_t iotlb_lock;</span>
 };
 
 void vhost_dev_init(struct vhost_dev *, struct vhost_virtqueue **vqs, int nvqs);
<span class="p_chunk">@@ -164,7 +174,8 @@</span> <span class="p_context"> int vhost_log_access_ok(struct vhost_dev *);</span>
 int vhost_get_vq_desc(struct vhost_virtqueue *,
 		      struct iovec iov[], unsigned int iov_count,
 		      unsigned int *out_num, unsigned int *in_num,
<span class="p_del">-		      struct vhost_log *log, unsigned int *log_num);</span>
<span class="p_add">+		      struct vhost_log *log, unsigned int *log_num,</span>
<span class="p_add">+		      int access);</span>
 void vhost_discard_vq_desc(struct vhost_virtqueue *, int n);
 
 int vhost_init_used(struct vhost_virtqueue *);
<span class="p_chunk">@@ -183,7 +194,7 @@</span> <span class="p_context"> int vhost_log_write(struct vhost_virtqueue *vq, struct vhost_log *log,</span>
 		    unsigned int log_num, u64 len);
 
 #define vq_err(vq, fmt, ...) do {                                  \
<span class="p_del">-		pr_debug(pr_fmt(fmt), ##__VA_ARGS__);       \</span>
<span class="p_add">+		printk(pr_fmt(fmt), ##__VA_ARGS__);       \</span>
 		if ((vq)-&gt;error_ctx)                               \
 				eventfd_signal((vq)-&gt;error_ctx, 1);\
 	} while (0)
<span class="p_header">diff --git a/fs/eventfd.c b/fs/eventfd.c</span>
<span class="p_header">index 8d0c0df..5c0a22f 100644</span>
<span class="p_header">--- a/fs/eventfd.c</span>
<span class="p_header">+++ b/fs/eventfd.c</span>
<span class="p_chunk">@@ -59,8 +59,9 @@</span> <span class="p_context"> __u64 eventfd_signal(struct eventfd_ctx *ctx, __u64 n)</span>
 	if (ULLONG_MAX - ctx-&gt;count &lt; n)
 		n = ULLONG_MAX - ctx-&gt;count;
 	ctx-&gt;count += n;
<span class="p_del">-	if (waitqueue_active(&amp;ctx-&gt;wqh))</span>
<span class="p_add">+	if (waitqueue_active(&amp;ctx-&gt;wqh)) {</span>
 		wake_up_locked_poll(&amp;ctx-&gt;wqh, POLLIN);
<span class="p_add">+	}</span>
 	spin_unlock_irqrestore(&amp;ctx-&gt;wqh.lock, flags);
 
 	return n;
<span class="p_header">diff --git a/include/uapi/linux/vhost.h b/include/uapi/linux/vhost.h</span>
<span class="p_header">index ab373191..5c35ab4 100644</span>
<span class="p_header">--- a/include/uapi/linux/vhost.h</span>
<span class="p_header">+++ b/include/uapi/linux/vhost.h</span>
<span class="p_chunk">@@ -47,6 +47,32 @@</span> <span class="p_context"> struct vhost_vring_addr {</span>
 	__u64 log_guest_addr;
 };
 
<span class="p_add">+struct vhost_iotlb_entry {</span>
<span class="p_add">+	__u64 iova;</span>
<span class="p_add">+	__u64 size;</span>
<span class="p_add">+	__u64 userspace_addr;</span>
<span class="p_add">+	struct {</span>
<span class="p_add">+#define VHOST_ACCESS_RO      0x1</span>
<span class="p_add">+#define VHOST_ACCESS_WO      0x2</span>
<span class="p_add">+#define VHOST_ACCESS_RW      0x3</span>
<span class="p_add">+		__u8  perm;</span>
<span class="p_add">+#define VHOST_IOTLB_MISS           1</span>
<span class="p_add">+#define VHOST_IOTLB_UPDATE         2</span>
<span class="p_add">+#define VHOST_IOTLB_INVALIDATE     3</span>
<span class="p_add">+		__u8  type;</span>
<span class="p_add">+#define VHOST_IOTLB_INVALID        0x1</span>
<span class="p_add">+#define VHOST_IOTLB_VALID          0x2</span>
<span class="p_add">+		__u8  valid;</span>
<span class="p_add">+		__u8  u8_padding;</span>
<span class="p_add">+		__u32 padding;</span>
<span class="p_add">+	} flags;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+struct vhost_vring_iotlb_entry {</span>
<span class="p_add">+	unsigned int index;</span>
<span class="p_add">+	__u64 userspace_addr;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 struct vhost_memory_region {
 	__u64 guest_phys_addr;
 	__u64 memory_size; /* bytes */
<span class="p_chunk">@@ -127,6 +153,15 @@</span> <span class="p_context"> struct vhost_memory {</span>
 /* Set eventfd to signal an error */
 #define VHOST_SET_VRING_ERR _IOW(VHOST_VIRTIO, 0x22, struct vhost_vring_file)
 
<span class="p_add">+/* IOTLB */</span>
<span class="p_add">+/* Specify an eventfd file descriptor to signle on IOTLB miss */</span>
<span class="p_add">+#define VHOST_SET_VRING_IOTLB_CALL _IOW(VHOST_VIRTIO, 0x23, struct      \</span>
<span class="p_add">+                                        vhost_vring_file)</span>
<span class="p_add">+#define VHOST_SET_VRING_IOTLB_REQUEST _IOW(VHOST_VIRTIO, 0x25, struct   \</span>
<span class="p_add">+                                           vhost_vring_iotlb_entry)</span>
<span class="p_add">+#define VHOST_UPDATE_IOTLB _IOW(VHOST_VIRTIO, 0x24, struct vhost_iotlb_entry)</span>
<span class="p_add">+#define VHOST_RUN_IOTLB _IOW(VHOST_VIRTIO, 0x26, int)</span>
<span class="p_add">+</span>
 /* VHOST_NET specific defines */
 
 /* Attach virtio net ring to a raw socket, or tap device.

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



