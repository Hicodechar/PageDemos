
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,v15,13/16] mm/hmm/migrate: new memory migration helper for use with device memory v2 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,v15,13/16] mm/hmm/migrate: new memory migration helper for use with device memory v2</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 6, 2017, 4:46 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1483721203-1678-14-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9501343/mbox/"
   >mbox</a>
|
   <a href="/patch/9501343/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9501343/">/patch/9501343/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7F62A6021C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  6 Jan 2017 15:47:55 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 851A7284D4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  6 Jan 2017 15:47:55 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 797C1284E3; Fri,  6 Jan 2017 15:47:55 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4E0DE284D8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  6 Jan 2017 15:47:54 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S966179AbdAFPrp (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 6 Jan 2017 10:47:45 -0500
Received: from mx1.redhat.com ([209.132.183.28]:35518 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S935819AbdAFPqM (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 6 Jan 2017 10:46:12 -0500
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 6B8FCC05CDF3;
	Fri,  6 Jan 2017 15:46:12 +0000 (UTC)
Received: from xgl-cortex.ml2.eng.bos.redhat.com
	(xgl-cortex.ml2.eng.bos.redhat.com [10.19.160.80])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id v06FjOgY013962; Fri, 6 Jan 2017 10:46:11 -0500
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;
Subject: [HMM v15 13/16] mm/hmm/migrate: new memory migration helper for use
	with device memory v2
Date: Fri,  6 Jan 2017 11:46:40 -0500
Message-Id: &lt;1483721203-1678-14-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1483721203-1678-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1483721203-1678-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.31]);
	Fri, 06 Jan 2017 15:46:12 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171983">David Nellans</a> - Jan. 6, 2017, 4:46 p.m.</div>
<pre class="content">
On 01/06/2017 10:46 AM, Jérôme Glisse wrote:
<span class="quote">&gt; This patch add a new memory migration helpers, which migrate memory</span>
<span class="quote">&gt; backing a range of virtual address of a process to different memory</span>
<span class="quote">&gt; (which can be allocated through special allocator). It differs from</span>
<span class="quote">&gt; numa migration by working on a range of virtual address and thus by</span>
<span class="quote">&gt; doing migration in chunk that can be large enough to use DMA engine</span>
<span class="quote">&gt; or special copy offloading engine.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Expected users are any one with heterogeneous memory where different</span>
<span class="quote">&gt; memory have different characteristics (latency, bandwidth, ...). As</span>
<span class="quote">&gt; an example IBM platform with CAPI bus can make use of this feature</span>
<span class="quote">&gt; to migrate between regular memory and CAPI device memory. New CPU</span>
<span class="quote">&gt; architecture with a pool of high performance memory not manage as</span>
<span class="quote">&gt; cache but presented as regular memory (while being faster and with</span>
<span class="quote">&gt; lower latency than DDR) will also be prime user of this patch.</span>
Why should the normal page migration path (where neither src nor dest are
device private), use the hmm_migrate functionality?  11-14 are
replicating a lot of the
normal migration functionality but with special casing for HMM
requirements.  When migrating
THP&#39;s or a list of pages (your use case above), normal NUMA migration
is going to want to do this as fast as possible too (see Zi Yan&#39;s
patches for multi-threading normal
migrations &amp; prototype of using intel IOAT for transfers, he sees 3-5x
speedup).

If the intention is to provide a common interface hook for migration to
use DMA acceleration
(which is a good idea), it probably shouldn&#39;t be special cased inside
HMM functionality.
For example, using the intel IOAT for migration DMA has nothing to do
with HMM
whatsoever. We need a normal migration path interface to allow DMA that
isn&#39;t tied
to HMM.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Jan. 6, 2017, 4:46 p.m.</div>
<pre class="content">
This patch add a new memory migration helpers, which migrate memory
backing a range of virtual address of a process to different memory
(which can be allocated through special allocator). It differs from
numa migration by working on a range of virtual address and thus by
doing migration in chunk that can be large enough to use DMA engine
or special copy offloading engine.

Expected users are any one with heterogeneous memory where different
memory have different characteristics (latency, bandwidth, ...). As
an example IBM platform with CAPI bus can make use of this feature
to migrate between regular memory and CAPI device memory. New CPU
architecture with a pool of high performance memory not manage as
cache but presented as regular memory (while being faster and with
lower latency than DDR) will also be prime user of this patch.

Migration to private device memory will be usefull for device that
have large pool of such like GPU, NVidia plans to use HMM for that.

Changed since v1:
  - typos fix
  - split early unmap optimization for page with single mapping
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
---
 include/linux/hmm.h |  66 +++++++-
 mm/Kconfig          |  13 ++
 mm/migrate.c        | 460 ++++++++++++++++++++++++++++++++++++++++++++++++++++
 3 files changed, 536 insertions(+), 3 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Jan. 6, 2017, 5:13 p.m.</div>
<pre class="content">
On Fri, Jan 06, 2017 at 10:46:09AM -0600, David Nellans wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On 01/06/2017 10:46 AM, Jérôme Glisse wrote:</span>
<span class="quote">&gt; &gt; This patch add a new memory migration helpers, which migrate memory</span>
<span class="quote">&gt; &gt; backing a range of virtual address of a process to different memory</span>
<span class="quote">&gt; &gt; (which can be allocated through special allocator). It differs from</span>
<span class="quote">&gt; &gt; numa migration by working on a range of virtual address and thus by</span>
<span class="quote">&gt; &gt; doing migration in chunk that can be large enough to use DMA engine</span>
<span class="quote">&gt; &gt; or special copy offloading engine.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Expected users are any one with heterogeneous memory where different</span>
<span class="quote">&gt; &gt; memory have different characteristics (latency, bandwidth, ...). As</span>
<span class="quote">&gt; &gt; an example IBM platform with CAPI bus can make use of this feature</span>
<span class="quote">&gt; &gt; to migrate between regular memory and CAPI device memory. New CPU</span>
<span class="quote">&gt; &gt; architecture with a pool of high performance memory not manage as</span>
<span class="quote">&gt; &gt; cache but presented as regular memory (while being faster and with</span>
<span class="quote">&gt; &gt; lower latency than DDR) will also be prime user of this patch.</span>
<span class="quote">&gt; Why should the normal page migration path (where neither src nor dest</span>
<span class="quote">&gt; are device private), use the hmm_migrate functionality?  11-14 are</span>
<span class="quote">&gt; replicating a lot of the normal migration functionality but with special</span>
<span class="quote">&gt; casing for HMM requirements.</span>

You are mischaracterizing patch 11-14. Patch 11-12 adds new flags and
modify existing functions so that they can be share. Patch 13 implement
new migration helper while patch 14 optimize this new migration helper.

hmm_migrate() is different from existing migration code because it works
on virtual address range of a process. Existing migration code works
from page. The only difference with existing code is that we collect
pages from virtual address and we allow use of dma engine to perform
copy.
<span class="quote">
&gt; When migrating THP&#39;s or a list of pages (your use case above), normal</span>
<span class="quote">&gt; NUMA migration is going to want to do this as fast as possible too (see</span>
<span class="quote">&gt; Zi Yan&#39;s patches for multi-threading normal migrations &amp; prototype of</span>
<span class="quote">&gt; using intel IOAT for transfers, he sees 3-5x speedup).</span>

This is core features of HMM and as such optimization like better THP
support are defer to later patchset.
<span class="quote">
&gt; </span>
<span class="quote">&gt; If the intention is to provide a common interface hook for migration to</span>
<span class="quote">&gt; use DMA acceleration (which is a good idea), it probably shouldn&#39;t be</span>
<span class="quote">&gt; special cased inside HMM functionality. For example, using the intel IOAT</span>
<span class="quote">&gt; for migration DMA has nothing to do with HMM whatsoever. We need a normal</span>
<span class="quote">&gt; migration path interface to allow DMA that isn&#39;t tied to HMM.</span>

There is nothing that ie hmm_migrate() to HMM. If that make you feel better
i can drop the hmm_ prefix but i would need another name than migrate() as
it is already taken. I can probably name it vma_range_dma_migrate() or
something like that.

The only think that is HMM specific in this code is understanding HMM special
page table entry and handling those. Such entry can only be migrated by DMA
and not by memcpy hence why i do not modify existing code to support those.

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171983">David Nellans</a> - Jan. 10, 2017, 3:30 p.m.</div>
<pre class="content">
<span class="quote">&gt; You are mischaracterizing patch 11-14. Patch 11-12 adds new flags and</span>
<span class="quote">&gt; modify existing functions so that they can be share. Patch 13 implement</span>
<span class="quote">&gt; new migration helper while patch 14 optimize this new migration helper.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; hmm_migrate() is different from existing migration code because it works</span>
<span class="quote">&gt; on virtual address range of a process. Existing migration code works</span>
<span class="quote">&gt; from page. The only difference with existing code is that we collect</span>
<span class="quote">&gt; pages from virtual address and we allow use of dma engine to perform</span>
<span class="quote">&gt; copy.</span>
You&#39;re right, but why not just introduce a new general migration interface
that works on vma range first, then case all the normal migration paths for
HMM and then DMA?  Being able to migrate based on vma range certainly
makes user level control of memory placement/migration less complicated
than
page interfaces.
<span class="quote">
&gt; There is nothing that ie hmm_migrate() to HMM. If that make you feel better</span>
<span class="quote">&gt; i can drop the hmm_ prefix but i would need another name than migrate() as</span>
<span class="quote">&gt; it is already taken. I can probably name it vma_range_dma_migrate() or</span>
<span class="quote">&gt; something like that.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The only think that is HMM specific in this code is understanding HMM special</span>
<span class="quote">&gt; page table entry and handling those. Such entry can only be migrated by DMA</span>
<span class="quote">&gt; and not by memcpy hence why i do not modify existing code to support those.</span>
I&#39;d be happier if there was a vma_migrate proposed independently, I
think it would find
users outside the HMM sandbox. In the IBM migration case, they might
want the vma
interface but choose to use CPU based migration rather than this DMA
interface,
It certainly would make testing of the vma_migrate interface easier.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Jan. 10, 2017, 4:58 p.m.</div>
<pre class="content">
On Tue, Jan 10, 2017 at 09:30:30AM -0600, David Nellans wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; You are mischaracterizing patch 11-14. Patch 11-12 adds new flags and</span>
<span class="quote">&gt; &gt; modify existing functions so that they can be share. Patch 13 implement</span>
<span class="quote">&gt; &gt; new migration helper while patch 14 optimize this new migration helper.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; hmm_migrate() is different from existing migration code because it works</span>
<span class="quote">&gt; &gt; on virtual address range of a process. Existing migration code works</span>
<span class="quote">&gt; &gt; from page. The only difference with existing code is that we collect</span>
<span class="quote">&gt; &gt; pages from virtual address and we allow use of dma engine to perform</span>
<span class="quote">&gt; &gt; copy.</span>
<span class="quote">&gt; You&#39;re right, but why not just introduce a new general migration interface</span>
<span class="quote">&gt; that works on vma range first, then case all the normal migration paths for</span>
<span class="quote">&gt; HMM and then DMA?  Being able to migrate based on vma range certainly</span>
<span class="quote">&gt; makes user level control of memory placement/migration less complicated</span>
<span class="quote">&gt; than page interfaces.</span>

Special casing for HMM and DMA is already what those patches do. They share
as much code as doable with existing path. There is one thing to consider
here, because we are working on vma range we can easily optimize the unmap
step. This is why i do not share any of the outer loop with existing code.

Sharing more code than this will be counter-productive from optimization
point of view.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; There is nothing that ie hmm_migrate() to HMM. If that make you feel better</span>
<span class="quote">&gt; &gt; i can drop the hmm_ prefix but i would need another name than migrate() as</span>
<span class="quote">&gt; &gt; it is already taken. I can probably name it vma_range_dma_migrate() or</span>
<span class="quote">&gt; &gt; something like that.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The only think that is HMM specific in this code is understanding HMM special</span>
<span class="quote">&gt; &gt; page table entry and handling those. Such entry can only be migrated by DMA</span>
<span class="quote">&gt; &gt; and not by memcpy hence why i do not modify existing code to support those.</span>
<span class="quote">&gt; I&#39;d be happier if there was a vma_migrate proposed independently, I think</span>
<span class="quote">&gt; it would find users outside the HMM sandbox. In the IBM migration case,</span>
<span class="quote">&gt; they might want the vma interface but choose to use CPU based migration</span>
<span class="quote">&gt; rather than this DMA interface, It certainly would make testing of the</span>
<span class="quote">&gt; vma_migrate interface easier.</span>

Like i said that code is not in HMM sandbox, it seats behind its own kernel
option and do not rely on any HMM thing beside hmm_pfn_t which is pfn with
a bunch of flags. The only difference with existing code is that it does
understand HMM CPU pte. It can easily be rename without hmm_ prefix if that
is what people want. The hmm_pfn_t is harder to replace as there isn&#39;t any-
thing that match the requirement (need few flags: DEVICE,MIGRATE,EMPTY,
UNADDRESSABLE).

The DMA is a callback function the caller of hmm_migrate() provide so you can
easily provide a callback that just do memcpy (well copy_highpage()). There
is no need to make any change. I can even provide a default CPU copy call-
back.

Cheers,
Jérôme
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
<span class="p_header">index f19c2a0..b1de4e1 100644</span>
<span class="p_header">--- a/include/linux/hmm.h</span>
<span class="p_header">+++ b/include/linux/hmm.h</span>
<span class="p_chunk">@@ -88,10 +88,13 @@</span> <span class="p_context"> struct hmm;</span>
  * HMM_PFN_ERROR: corresponding CPU page table entry point to poisonous memory
  * HMM_PFN_EMPTY: corresponding CPU page table entry is none (pte_none() true)
  * HMM_PFN_DEVICE: this is device memory (ie a ZONE_DEVICE page)
<span class="p_add">+ * HMM_PFN_LOCKED: underlying struct page is lock</span>
  * HMM_PFN_SPECIAL: corresponding CPU page table entry is special ie result of
  *      vm_insert_pfn() or vm_insert_page() and thus should not be mirror by a
  *      device (the entry will never have HMM_PFN_VALID set and the pfn value
  *      is undefine)
<span class="p_add">+ * HMM_PFN_MIGRATE: use by hmm_vma_migrate() to signify which address can be</span>
<span class="p_add">+ *      migrated</span>
  * HMM_PFN_UNADDRESSABLE: unaddressable device memory (ZONE_DEVICE)
  */
 typedef unsigned long hmm_pfn_t;
<span class="p_chunk">@@ -102,9 +105,11 @@</span> <span class="p_context"> typedef unsigned long hmm_pfn_t;</span>
 #define HMM_PFN_ERROR (1 &lt;&lt; 3)
 #define HMM_PFN_EMPTY (1 &lt;&lt; 4)
 #define HMM_PFN_DEVICE (1 &lt;&lt; 5)
<span class="p_del">-#define HMM_PFN_SPECIAL (1 &lt;&lt; 6)</span>
<span class="p_del">-#define HMM_PFN_UNADDRESSABLE (1 &lt;&lt; 7)</span>
<span class="p_del">-#define HMM_PFN_SHIFT 8</span>
<span class="p_add">+#define HMM_PFN_LOCKED (1 &lt;&lt; 6)</span>
<span class="p_add">+#define HMM_PFN_SPECIAL (1 &lt;&lt; 7)</span>
<span class="p_add">+#define HMM_PFN_MIGRATE (1 &lt;&lt; 8)</span>
<span class="p_add">+#define HMM_PFN_UNADDRESSABLE (1 &lt;&lt; 9)</span>
<span class="p_add">+#define HMM_PFN_SHIFT 10</span>
 
 /*
  * hmm_pfn_to_page() - return struct page pointed to by a valid hmm_pfn_t
<span class="p_chunk">@@ -317,6 +322,61 @@</span> <span class="p_context"> int hmm_vma_fault(struct vm_area_struct *vma,</span>
 #endif /* IS_ENABLED(CONFIG_HMM_MIRROR) */
 
 
<span class="p_add">+#if IS_ENABLED(CONFIG_HMM_MIGRATE)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * struct hmm_migrate_ops - migrate operation callback</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @alloc_and_copy: alloc destination memoiry and copy source to it</span>
<span class="p_add">+ * @finalize_and_map: allow caller to inspect successfull migrated page</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The new HMM migrate helper hmm_vma_migrate() allow memory migration to use</span>
<span class="p_add">+ * device DMA engine to perform copy from source to destination memory it also</span>
<span class="p_add">+ * allow caller to use its own memory allocator for destination memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that in alloc_and_copy device driver can decide not to migrate some of</span>
<span class="p_add">+ * the entry by simply setting corresponding dst_pfns to 0.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Destination page must locked and HMM_PFN_LOCKED flag set in corresponding</span>
<span class="p_add">+ * hmm_pfn_t entry of dst_pfns array. It is expected that page allocated will</span>
<span class="p_add">+ * have an elevated refcount and that a put_page() will free the page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Device driver might want to allocate with an extra-refcount if they want to</span>
<span class="p_add">+ * control deallocation of failed migration inside finalize_and_map() callback.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Inside finalize_and_map() device driver must use the HMM_PFN_MIGRATE flag to</span>
<span class="p_add">+ * determine which page have been successfully migrated (this is set inside the</span>
<span class="p_add">+ * src_pfns array).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For migration from device memory to system memory device driver must set any</span>
<span class="p_add">+ * dst_pfns entry to HMM_PFN_ERROR for any entry it can not migrate back due to</span>
<span class="p_add">+ * hardware fatal failure that can not be recovered. Such failure will trigger</span>
<span class="p_add">+ * a SIGBUS for the process trying to access such memory.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm_migrate_ops {</span>
<span class="p_add">+	void (*alloc_and_copy)(struct vm_area_struct *vma,</span>
<span class="p_add">+			       const hmm_pfn_t *src_pfns,</span>
<span class="p_add">+			       hmm_pfn_t *dst_pfns,</span>
<span class="p_add">+			       unsigned long start,</span>
<span class="p_add">+			       unsigned long end,</span>
<span class="p_add">+			       void *private);</span>
<span class="p_add">+	void (*finalize_and_map)(struct vm_area_struct *vma,</span>
<span class="p_add">+				 const hmm_pfn_t *src_pfns,</span>
<span class="p_add">+				 hmm_pfn_t *dst_pfns,</span>
<span class="p_add">+				 unsigned long start,</span>
<span class="p_add">+				 unsigned long end,</span>
<span class="p_add">+				 void *private);</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="p_add">+		    struct vm_area_struct *vma,</span>
<span class="p_add">+		    hmm_pfn_t *src_pfns,</span>
<span class="p_add">+		    hmm_pfn_t *dst_pfns,</span>
<span class="p_add">+		    unsigned long start,</span>
<span class="p_add">+		    unsigned long end,</span>
<span class="p_add">+		    void *private);</span>
<span class="p_add">+#endif /* IS_ENABLED(CONFIG_HMM_MIGRATE) */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
 /* Below are for HMM internal use only ! Not to be used by device driver ! */
 void hmm_mm_destroy(struct mm_struct *mm);
 
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index 598c38a..3806d69 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -308,6 +308,19 @@</span> <span class="p_context"> config HMM_MIRROR</span>
 	  range of virtual address. This require careful synchronization with
 	  CPU page table update.
 
<span class="p_add">+config HMM_MIGRATE</span>
<span class="p_add">+	bool &quot;HMM migrate virtual range of process using device driver DMA&quot;</span>
<span class="p_add">+	select HMM</span>
<span class="p_add">+	select MIGRATION</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  HMM migrate is a new helper to migrate range of virtual address using</span>
<span class="p_add">+	  special page allocator and copy callback. This allow device driver to</span>
<span class="p_add">+	  migrate range of a process memory to its memory using its DMA engine.</span>
<span class="p_add">+</span>
<span class="p_add">+	  It obyes all rules of memory migration, except that it supports the</span>
<span class="p_add">+	  migration of ZONE_DEVICE page that have MEMOY_DEVICE_ALLOW_MIGRATE</span>
<span class="p_add">+	  flag set.</span>
<span class="p_add">+</span>
 config PHYS_ADDR_T_64BIT
 	def_bool 64BIT || ARCH_PHYS_ADDR_T_64BIT
 
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 36e2ed9..365b615 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -41,6 +41,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/page_idle.h&gt;
 #include &lt;linux/page_owner.h&gt;
 #include &lt;linux/memremap.h&gt;
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
 
 #include &lt;asm/tlbflush.h&gt;
 
<span class="p_chunk">@@ -421,6 +422,14 @@</span> <span class="p_context"> int migrate_page_move_mapping(struct address_space *mapping,</span>
 	int expected_count = 1 + extra_count;
 	void **pslot;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * ZONE_DEVICE pages have 1 refcount always held by their device</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that DAX memory will never reach that point as it does not have</span>
<span class="p_add">+	 * the MEMORY_DEVICE_ALLOW_MIGRATE flag set (see memory_hotplug.h).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	expected_count += is_zone_device_page(page);</span>
<span class="p_add">+</span>
 	if (!mapping) {
 		/* Anonymous page without mapping */
 		if (page_count(page) != expected_count)
<span class="p_chunk">@@ -2087,3 +2096,454 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 #endif /* CONFIG_NUMA_BALANCING */
 
 #endif /* CONFIG_NUMA */
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#if IS_ENABLED(CONFIG_HMM_MIGRATE)</span>
<span class="p_add">+struct hmm_migrate {</span>
<span class="p_add">+	struct vm_area_struct	*vma;</span>
<span class="p_add">+	hmm_pfn_t		*dst_pfns;</span>
<span class="p_add">+	hmm_pfn_t		*src_pfns;</span>
<span class="p_add">+	unsigned long		npages;</span>
<span class="p_add">+	unsigned long		start;</span>
<span class="p_add">+	unsigned long		end;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static int hmm_collect_walk_pmd(pmd_t *pmdp,</span>
<span class="p_add">+				unsigned long start,</span>
<span class="p_add">+				unsigned long end,</span>
<span class="p_add">+				struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_migrate *migrate = walk-&gt;private;</span>
<span class="p_add">+	struct mm_struct *mm = walk-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long addr = start;</span>
<span class="p_add">+	hmm_pfn_t *src_pfns;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+again:</span>
<span class="p_add">+	if (pmd_none(*pmdp))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	split_huge_pmd(walk-&gt;vma, pmdp, addr);</span>
<span class="p_add">+	if (pmd_trans_unstable(pmdp))</span>
<span class="p_add">+		goto again;</span>
<span class="p_add">+</span>
<span class="p_add">+	src_pfns = &amp;migrate-&gt;src_pfns[(addr - migrate-&gt;start) &gt;&gt; PAGE_SHIFT];</span>
<span class="p_add">+	ptep = pte_offset_map_lock(mm, pmdp, addr, &amp;ptl);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; end; addr += PAGE_SIZE, src_pfns++, ptep++) {</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+		swp_entry_t entry;</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		hmm_pfn_t flags;</span>
<span class="p_add">+		bool write;</span>
<span class="p_add">+		pte_t pte;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_present(pte)) {</span>
<span class="p_add">+			if (pte_none(pte))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Only care about un-addressable device page special</span>
<span class="p_add">+			 * page table entry. Other special swap entry are not</span>
<span class="p_add">+			 * migratable and we ignore regular swaped page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			entry = pte_to_swp_entry(pte);</span>
<span class="p_add">+			if (!is_device_entry(entry))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			flags = HMM_PFN_DEVICE | HMM_PFN_UNADDRESSABLE;</span>
<span class="p_add">+			write = is_write_device_entry(entry);</span>
<span class="p_add">+			page = device_entry_to_page(entry);</span>
<span class="p_add">+			pfn = page_to_pfn(page);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!dev_page_allow_migrate(page))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			pfn = pte_pfn(pte);</span>
<span class="p_add">+			write = pte_write(pte);</span>
<span class="p_add">+			page = pfn_to_page(pfn);</span>
<span class="p_add">+			flags = is_zone_device_page(page) ? HMM_PFN_DEVICE : 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* FIXME support THP see hmm_migrate_page_check() */</span>
<span class="p_add">+		if (PageTransCompound(page))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Corner case handling:</span>
<span class="p_add">+		 * 1. When a new swap-cache page is read into, it is added to</span>
<span class="p_add">+		 * the LRU and treated as swapcache but it has no rmap yet. Skip</span>
<span class="p_add">+		 * those.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!page-&gt;mapping)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		*src_pfns = hmm_pfn_from_pfn(pfn) | HMM_PFN_MIGRATE | flags;</span>
<span class="p_add">+		*src_pfns |= write ? HMM_PFN_WRITE : 0;</span>
<span class="p_add">+		migrate-&gt;npages++;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * By getting a reference on the page we pin it and blocks any</span>
<span class="p_add">+		 * kind of migration. Side effect is that it &quot;freeze&quot; the pte.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * We drop this reference after isolating the page from the lru</span>
<span class="p_add">+		 * for non device page (device page are not on the lru and thus</span>
<span class="p_add">+		 * can&#39;t be drop from it).</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		get_page(page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	pte_unmap_unlock(ptep - 1, ptl);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_migrate_collect() - collect page over range of virtual address</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This will go over the CPU page table and for each virtual address back by a</span>
<span class="p_add">+ * valid page it update the src_pfns array and take a reference on the page in</span>
<span class="p_add">+ * order to pin the page until we lock it and unmap it.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void hmm_migrate_collect(struct hmm_migrate *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_walk mm_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+	mm_walk.pmd_entry = hmm_collect_walk_pmd;</span>
<span class="p_add">+	mm_walk.pte_entry = NULL;</span>
<span class="p_add">+	mm_walk.pte_hole = NULL;</span>
<span class="p_add">+	mm_walk.hugetlb_entry = NULL;</span>
<span class="p_add">+	mm_walk.test_walk = NULL;</span>
<span class="p_add">+	mm_walk.vma = migrate-&gt;vma;</span>
<span class="p_add">+	mm_walk.mm = migrate-&gt;vma-&gt;vm_mm;</span>
<span class="p_add">+	mm_walk.private = migrate;</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_invalidate_range_start(mm_walk.mm,</span>
<span class="p_add">+					    migrate-&gt;start,</span>
<span class="p_add">+					    migrate-&gt;end);</span>
<span class="p_add">+	walk_page_range(migrate-&gt;start, migrate-&gt;end, &amp;mm_walk);</span>
<span class="p_add">+	mmu_notifier_invalidate_range_end(mm_walk.mm,</span>
<span class="p_add">+					  migrate-&gt;start,</span>
<span class="p_add">+					  migrate-&gt;end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_migrate_page_check() - check if page is pin or not</span>
<span class="p_add">+ * @page: struct page to check</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Pinned page can not be migrated. Same test in migrate_page_move_mapping()</span>
<span class="p_add">+ * except that here we allow migration of ZONE_DEVICE page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool hmm_migrate_page_check(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * One extra ref because caller hold an extra reference either from</span>
<span class="p_add">+	 * either isolate_lru_page() for regular page or hmm_migrate_collect()</span>
<span class="p_add">+	 * for device page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int extra = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * FIXME support THP (transparent huge page), it is bit more complex to</span>
<span class="p_add">+	 * check them then regular page because they can be map with a pmd or</span>
<span class="p_add">+	 * with a pte (split pte mapping).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (PageCompound(page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Page from ZONE_DEVICE have one extra reference */</span>
<span class="p_add">+	if (is_zone_device_page(page)) {</span>
<span class="p_add">+		if (!dev_page_allow_migrate(page))</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		extra++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((page_count(page) - extra) &gt; page_mapcount(page))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_migrate_lock_and_isolate() - lock pages and isolate them from the lru</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This lock pages that have been collected by hmm_migrate_collect(). Once page</span>
<span class="p_add">+ * is locked it is isolated from the lru (for non device page). Finaly the ref</span>
<span class="p_add">+ * taken by hmm_migrate_collect() is drop as locked page can not be migrated by</span>
<span class="p_add">+ * concurrent kernel thread.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void hmm_migrate_lock_and_isolate(struct hmm_migrate *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="p_add">+	bool allow_drain = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	lru_add_drain();</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; (addr&lt;migrate-&gt;end) &amp;&amp; migrate-&gt;npages; addr+=PAGE_SIZE, i++) {</span>
<span class="p_add">+		struct page *page = hmm_pfn_to_page(migrate-&gt;src_pfns[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		lock_page(page);</span>
<span class="p_add">+		migrate-&gt;src_pfns[i] |= HMM_PFN_LOCKED;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* ZONE_DEVICE page are not on LRU */</span>
<span class="p_add">+		if (!is_zone_device_page(page)) {</span>
<span class="p_add">+			if (!PageLRU(page) &amp;&amp; allow_drain) {</span>
<span class="p_add">+				/* Drain CPU&#39;s pagevec */</span>
<span class="p_add">+				lru_add_drain_all();</span>
<span class="p_add">+				allow_drain = false;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			if (isolate_lru_page(page)) {</span>
<span class="p_add">+				migrate-&gt;src_pfns[i] = 0;</span>
<span class="p_add">+				migrate-&gt;npages--;</span>
<span class="p_add">+				unlock_page(page);</span>
<span class="p_add">+				put_page(page);</span>
<span class="p_add">+			} else</span>
<span class="p_add">+				/* Drop the reference we took in collect */</span>
<span class="p_add">+				put_page(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!hmm_migrate_page_check(page)) {</span>
<span class="p_add">+			migrate-&gt;src_pfns[i] = 0;</span>
<span class="p_add">+			migrate-&gt;npages--;</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_migrate_unmap() - replace page mapping with special migration pte entry</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Replace page mapping (CPU page table pte) with special migration pte entry</span>
<span class="p_add">+ * and check again if it has be pin. Pin page are restore because we can not</span>
<span class="p_add">+ * migrate them.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is the last step before we call the device driver callback to allocate</span>
<span class="p_add">+ * destination memory and copy content of original page over to new page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void hmm_migrate_unmap(struct hmm_migrate *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int flags = TTU_MIGRATION | TTU_IGNORE_MLOCK | TTU_IGNORE_ACCESS;</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0, restore = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="p_add">+		struct page *page = hmm_pfn_to_page(migrate-&gt;src_pfns[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || !(migrate-&gt;src_pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		try_to_unmap(page, flags);</span>
<span class="p_add">+		if (page_mapped(page) || !hmm_migrate_page_check(page)) {</span>
<span class="p_add">+			migrate-&gt;src_pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="p_add">+			migrate-&gt;npages--;</span>
<span class="p_add">+			restore++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; (addr &lt; migrate-&gt;end) &amp;&amp; restore; addr += PAGE_SIZE, i++) {</span>
<span class="p_add">+		struct page *page = hmm_pfn_to_page(migrate-&gt;src_pfns[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || (migrate-&gt;src_pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		remove_migration_ptes(page, page, false);</span>
<span class="p_add">+</span>
<span class="p_add">+		migrate-&gt;src_pfns[i] = 0;</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		restore--;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (is_zone_device_page(page))</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			putback_lru_page(page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_migrate_struct_page() - migrate meta-data from src page to dst page</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This migrate struct page meta-data from source struct page to destination</span>
<span class="p_add">+ * struct page. This effectively finish the migration from source page to the</span>
<span class="p_add">+ * destination page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void hmm_migrate_struct_page(struct hmm_migrate *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; addr &lt; migrate-&gt;end; addr += PAGE_SIZE, i++) {</span>
<span class="p_add">+		struct page *newpage = hmm_pfn_to_page(migrate-&gt;dst_pfns[i]);</span>
<span class="p_add">+		struct page *page = hmm_pfn_to_page(migrate-&gt;src_pfns[i]);</span>
<span class="p_add">+		struct address_space *mapping;</span>
<span class="p_add">+		int r;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page || !newpage)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (!(migrate-&gt;src_pfns[i] &amp; HMM_PFN_MIGRATE))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		mapping = page_mapping(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * For now only support private anonymous when migrating</span>
<span class="p_add">+		 * to un-addressable device memory.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (mapping &amp;&amp; is_zone_device_page(newpage) &amp;&amp;</span>
<span class="p_add">+		    !is_addressable_page(newpage)) {</span>
<span class="p_add">+			migrate-&gt;src_pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		r = migrate_page(mapping, newpage, page, MIGRATE_SYNC, false);</span>
<span class="p_add">+		if (r != MIGRATEPAGE_SUCCESS)</span>
<span class="p_add">+			migrate-&gt;src_pfns[i] &amp;= ~HMM_PFN_MIGRATE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_migrate_remove_migration_pte() - restore CPU page table entry</span>
<span class="p_add">+ * @migrate: migrate struct containing all migration informations</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This replace the special migration pte entry with either a mapping to the</span>
<span class="p_add">+ * new page if migration was successful for that page or to the original page</span>
<span class="p_add">+ * otherwise.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This also unlock the page and put them back on the lru or drop the extra</span>
<span class="p_add">+ * ref for device page.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void hmm_migrate_remove_migration_pte(struct hmm_migrate *migrate)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = migrate-&gt;start, i = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; (addr&lt;migrate-&gt;end) &amp;&amp; migrate-&gt;npages; addr+=PAGE_SIZE, i++) {</span>
<span class="p_add">+		struct page *newpage = hmm_pfn_to_page(migrate-&gt;dst_pfns[i]);</span>
<span class="p_add">+		struct page *page = hmm_pfn_to_page(migrate-&gt;src_pfns[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		newpage = newpage ? newpage : page;</span>
<span class="p_add">+</span>
<span class="p_add">+		remove_migration_ptes(page, newpage, false);</span>
<span class="p_add">+		unlock_page(page);</span>
<span class="p_add">+		migrate-&gt;npages--;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (is_zone_device_page(page))</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			putback_lru_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (newpage != page) {</span>
<span class="p_add">+			unlock_page(newpage);</span>
<span class="p_add">+			if (is_zone_device_page(newpage))</span>
<span class="p_add">+				put_page(newpage);</span>
<span class="p_add">+			else</span>
<span class="p_add">+				putback_lru_page(newpage);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_vma_migrate() - migrate a range of memory inside vma using accel copy</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @ops: migration callback for allocating destination memory and copying</span>
<span class="p_add">+ * @vma: virtual memory area containing the range to be migrated</span>
<span class="p_add">+ * @src_pfns: array of hmm_pfn_t containing source pfns</span>
<span class="p_add">+ * @dst_pfns: array of hmm_pfn_t containing destination pfns</span>
<span class="p_add">+ * @start: start address of the range to migrate (inclusive)</span>
<span class="p_add">+ * @end: end address of the range to migrate (exclusive)</span>
<span class="p_add">+ * @private: pointer passed back to each of the callback</span>
<span class="p_add">+ * Returns: 0 on success, error code otherwise</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This will try to migrate a range of memory using callback to allocate and</span>
<span class="p_add">+ * copy memory from source to destination. This function will first collect,</span>
<span class="p_add">+ * lock and unmap pages in the range and then call alloc_and_copy() callback</span>
<span class="p_add">+ * for device driver to allocate destination memory and copy from source.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Then it will proceed and try to effectively migrate the page (struct page</span>
<span class="p_add">+ * metadata) a step that can fail for various reasons. Before updating CPU page</span>
<span class="p_add">+ * table it will call finalize_and_map() callback so that device driver can</span>
<span class="p_add">+ * inspect what have been successfully migrated and update its own page table</span>
<span class="p_add">+ * (this latter aspect is not mandatory and only make sense for some user of</span>
<span class="p_add">+ * this API).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Finaly the function update CPU page table and unlock the pages before</span>
<span class="p_add">+ * returning 0.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * It will return an error code only if one of the argument is invalid.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int hmm_vma_migrate(const struct hmm_migrate_ops *ops,</span>
<span class="p_add">+		    struct vm_area_struct *vma,</span>
<span class="p_add">+		    hmm_pfn_t *src_pfns,</span>
<span class="p_add">+		    hmm_pfn_t *dst_pfns,</span>
<span class="p_add">+		    unsigned long start,</span>
<span class="p_add">+		    unsigned long end,</span>
<span class="p_add">+		    void *private)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_migrate migrate;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Sanity check the arguments */</span>
<span class="p_add">+	start &amp;= PAGE_MASK;</span>
<span class="p_add">+	end &amp;= PAGE_MASK;</span>
<span class="p_add">+	if (is_vm_hugetlb_page(vma) || (vma-&gt;vm_flags &amp; VM_SPECIAL))</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (!vma || !ops || !src_pfns || !dst_pfns || start &gt;= end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (start &lt; vma-&gt;vm_start || start &gt;= vma-&gt;vm_end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	if (end &lt;= vma-&gt;vm_start || end &gt; vma-&gt;vm_end)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	memset(src_pfns, 0, sizeof(*src_pfns) * ((end - start) &gt;&gt; PAGE_SHIFT));</span>
<span class="p_add">+	migrate.src_pfns = src_pfns;</span>
<span class="p_add">+	migrate.dst_pfns = dst_pfns;</span>
<span class="p_add">+	migrate.start = start;</span>
<span class="p_add">+	migrate.npages = 0;</span>
<span class="p_add">+	migrate.end = end;</span>
<span class="p_add">+	migrate.vma = vma;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Collect, and try to unmap source pages */</span>
<span class="p_add">+	hmm_migrate_collect(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.npages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Lock and isolate page */</span>
<span class="p_add">+	hmm_migrate_lock_and_isolate(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.npages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unmap pages */</span>
<span class="p_add">+	hmm_migrate_unmap(&amp;migrate);</span>
<span class="p_add">+	if (!migrate.npages)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At this point pages are lock and unmap and thus they have stable</span>
<span class="p_add">+	 * content and can safely be copied to destination memory that is</span>
<span class="p_add">+	 * allocated by the callback.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note that migration can fail in hmm_migrate_struct_page() for each</span>
<span class="p_add">+	 * individual page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ops-&gt;alloc_and_copy(vma, src_pfns, dst_pfns, start, end, private);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* This does the real migration of struct page */</span>
<span class="p_add">+	hmm_migrate_struct_page(&amp;migrate);</span>
<span class="p_add">+</span>
<span class="p_add">+	ops-&gt;finalize_and_map(vma, src_pfns, dst_pfns, start, end, private);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Unlock and remap pages */</span>
<span class="p_add">+	hmm_migrate_remove_migration_pte(&amp;migrate);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_vma_migrate);</span>
<span class="p_add">+#endif /* IS_ENABLED(CONFIG_HMM_MIGRATE) */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



