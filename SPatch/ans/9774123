
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,4/4] hugetlb: add support for preferred node to alloc_huge_page_nodemask - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,4/4] hugetlb: add support for preferred node to alloc_huge_page_nodemask</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 8, 2017, 7:45 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170608074553.22152-5-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9774123/mbox/"
   >mbox</a>
|
   <a href="/patch/9774123/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9774123/">/patch/9774123/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7193960393 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  8 Jun 2017 07:46:29 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5FAA422B39
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  8 Jun 2017 07:46:29 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 545E627C05; Thu,  8 Jun 2017 07:46:29 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C172E22B39
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  8 Jun 2017 07:46:28 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751905AbdFHHqT (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 8 Jun 2017 03:46:19 -0400
Received: from mail-wm0-f67.google.com ([74.125.82.67]:33341 &quot;EHLO
	mail-wm0-f67.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751887AbdFHHqQ (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 8 Jun 2017 03:46:16 -0400
Received: by mail-wm0-f67.google.com with SMTP id x3so5907191wme.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu, 08 Jun 2017 00:46:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=8TsewqwCUy48BmBHzPPUZBymBPlmHklRGqnfyD+J5IQ=;
	b=C21ERB//RlXU7AANx+wEmFqkC74B1VYIpt+/7FijqpkzvTp7T1zfv7A1C+x8xEoQYn
	tUyTDoUsHFAbb0HRMbOAC7tuUhiLBlT70B2Msyeud5yIygRF04ZOA1xYjwbdSZ05EQvn
	kDW8e5yK1AcOigfnhv6qhKqCkfiU7ngtR07rJnFMNOT96F6vJlhF6yOmzqIx9oXYc8Mr
	GxyJNP24L0AWHyxynmvlXPQO72o5xkC8eCgtjLMnwbz8LTSoOA2ARvbGdjtBnvb82QR5
	NoRcuNRzR2Ps4sr/iOSAHDyOiX/C8XEk71JgiSbu+WUrWx7Yvj4ykv58FMew6bpbJCFs
	2j6w==
X-Gm-Message-State: AODbwcAFp22JzypCwMCQp6y6exhp5FioJgvAJDxyF4p5j7419BHB4z9V
	vDUJaKNY52ylXQ==
X-Received: by 10.28.46.14 with SMTP id u14mr2473293wmu.82.1496907970168;
	Thu, 08 Jun 2017 00:46:10 -0700 (PDT)
Received: from tiehlicka.suse.cz (prg-ext-pat.suse.com. [213.151.95.130])
	by smtp.gmail.com with ESMTPSA id
	i64sm5834505wmd.33.2017.06.08.00.46.09
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Thu, 08 Jun 2017 00:46:09 -0700 (PDT)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: &lt;linux-mm@kvack.org&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Xishi Qiu &lt;qiuxishi@huawei.com&gt;, zhong jiang &lt;zhongjiang@huawei.com&gt;,
	Joonsoo Kim &lt;js1304@gmail.com&gt;, LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [RFC PATCH 4/4] hugetlb: add support for preferred node to
	alloc_huge_page_nodemask
Date: Thu,  8 Jun 2017 09:45:53 +0200
Message-Id: &lt;20170608074553.22152-5-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170608074553.22152-1-mhocko@kernel.org&gt;
References: &lt;20170608074553.22152-1-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 8, 2017, 7:45 a.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

alloc_huge_page_nodemask tries to allocate from any numa node in the
allowed node mask. This might lead to filling up low NUMA nodes while
others are not used. We can reduce this risk by introducing a concept
of the preferred node similar to what we have in the regular page
allocator. We will start allocating from the preferred nid and then
iterate over all allowed nodes until we try them all. Introduce
for_each_node_mask_preferred helper which does the iteration and reuse
the available preferred node in new_page_nodemask which is currently
the only caller of alloc_huge_page_nodemask.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 include/linux/hugetlb.h  |  3 ++-
 include/linux/migrate.h  |  2 +-
 include/linux/nodemask.h | 20 ++++++++++++++++++++
 mm/hugetlb.c             |  9 ++++++---
 4 files changed, 29 insertions(+), 5 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - June 8, 2017, 8:38 a.m.</div>
<pre class="content">
On 06/08/2017 09:45 AM, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; alloc_huge_page_nodemask tries to allocate from any numa node in the</span>
<span class="quote">&gt; allowed node mask. This might lead to filling up low NUMA nodes while</span>
<span class="quote">&gt; others are not used. We can reduce this risk by introducing a concept</span>
<span class="quote">&gt; of the preferred node similar to what we have in the regular page</span>
<span class="quote">&gt; allocator. We will start allocating from the preferred nid and then</span>
<span class="quote">&gt; iterate over all allowed nodes until we try them all. Introduce</span>
<span class="quote">&gt; for_each_node_mask_preferred helper which does the iteration and reuse</span>
<span class="quote">&gt; the available preferred node in new_page_nodemask which is currently</span>
<span class="quote">&gt; the only caller of alloc_huge_page_nodemask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>

That&#39;s better, yeah. I don&#39;t think it would be too hard to use a
zonelist though. What do others think?
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  include/linux/hugetlb.h  |  3 ++-</span>
<span class="quote">&gt;  include/linux/migrate.h  |  2 +-</span>
<span class="quote">&gt;  include/linux/nodemask.h | 20 ++++++++++++++++++++</span>
<span class="quote">&gt;  mm/hugetlb.c             |  9 ++++++---</span>
<span class="quote">&gt;  4 files changed, 29 insertions(+), 5 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="quote">&gt; index c469191bb13b..9831a4434dd7 100644</span>
<span class="quote">&gt; --- a/include/linux/hugetlb.h</span>
<span class="quote">&gt; +++ b/include/linux/hugetlb.h</span>
<span class="quote">&gt; @@ -349,7 +349,8 @@ struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  struct page *alloc_huge_page_node(struct hstate *h, int nid);</span>
<span class="quote">&gt;  struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				unsigned long addr, int avoid_reserve);</span>
<span class="quote">&gt; -struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask);</span>
<span class="quote">&gt; +struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="quote">&gt; +				const nodemask_t *nmask);</span>
<span class="quote">&gt;  int huge_add_to_page_cache(struct page *page, struct address_space *mapping,</span>
<span class="quote">&gt;  			pgoff_t idx);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="quote">&gt; index f80c9882403a..af3ccf93efaa 100644</span>
<span class="quote">&gt; --- a/include/linux/migrate.h</span>
<span class="quote">&gt; +++ b/include/linux/migrate.h</span>
<span class="quote">&gt; @@ -38,7 +38,7 @@ static inline struct page *new_page_nodemask(struct page *page, int preferred_ni</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (PageHuge(page))</span>
<span class="quote">&gt;  		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),</span>
<span class="quote">&gt; -				nodemask);</span>
<span class="quote">&gt; +				preferred_nid, nodemask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (PageHighMem(page)</span>
<span class="quote">&gt;  	    || (zone_idx(page_zone(page)) == ZONE_MOVABLE))</span>
<span class="quote">&gt; diff --git a/include/linux/nodemask.h b/include/linux/nodemask.h</span>
<span class="quote">&gt; index cf0b91c3ec12..797aa74392bc 100644</span>
<span class="quote">&gt; --- a/include/linux/nodemask.h</span>
<span class="quote">&gt; +++ b/include/linux/nodemask.h</span>
<span class="quote">&gt; @@ -42,6 +42,8 @@</span>
<span class="quote">&gt;   * void nodes_shift_left(dst, src, n)	Shift left</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * int first_node(mask)			Number lowest set bit, or MAX_NUMNODES</span>
<span class="quote">&gt; + * int first_node_from(nid, mask)	First node starting from nid, or wrap</span>
<span class="quote">&gt; + * 					from first or MAX_NUMNODES</span>
<span class="quote">&gt;   * int next_node(node, mask)		Next node past &#39;node&#39;, or MAX_NUMNODES</span>
<span class="quote">&gt;   * int next_node_in(node, mask)		Next node past &#39;node&#39;, or wrap to first,</span>
<span class="quote">&gt;   *					or MAX_NUMNODES</span>
<span class="quote">&gt; @@ -268,6 +270,15 @@ static inline int __next_node(int n, const nodemask_t *srcp)</span>
<span class="quote">&gt;  #define next_node_in(n, src) __next_node_in((n), &amp;(src))</span>
<span class="quote">&gt;  int __next_node_in(int node, const nodemask_t *srcp);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define first_node_from(nid, mask) __first_node_from(nid, &amp;(mask))</span>
<span class="quote">&gt; +static inline int __first_node_from(int nid, const nodemask_t *mask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (test_bit(nid, mask-&gt;bits))</span>
<span class="quote">&gt; +		return nid;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return __next_node_in(nid, mask);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline void init_nodemask_of_node(nodemask_t *mask, int node)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	nodes_clear(*mask);</span>
<span class="quote">&gt; @@ -369,10 +380,19 @@ static inline void __nodes_fold(nodemask_t *dstp, const nodemask_t *origp,</span>
<span class="quote">&gt;  	for ((node) = first_node(mask);			\</span>
<span class="quote">&gt;  		(node) &lt; MAX_NUMNODES;			\</span>
<span class="quote">&gt;  		(node) = next_node((node), (mask)))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define for_each_node_mask_preferred(node, iter, preferred, mask)	\</span>
<span class="quote">&gt; +	for ((node) = first_node_from((preferred), (mask)), iter = 0;	\</span>
<span class="quote">&gt; +		(iter) &lt; nodes_weight((mask));				\</span>
<span class="quote">&gt; +		(node) = next_node_in((node), (mask)), (iter)++)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #else /* MAX_NUMNODES == 1 */</span>
<span class="quote">&gt;  #define for_each_node_mask(node, mask)			\</span>
<span class="quote">&gt;  	if (!nodes_empty(mask))				\</span>
<span class="quote">&gt;  		for ((node) = 0; (node) &lt; 1; (node)++)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define for_each_node_mask_preferred(node, iter, preferred, mask) \</span>
<span class="quote">&gt; +	for_each_node_mask(node, mask)</span>
<span class="quote">&gt;  #endif /* MAX_NUMNODES */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 01c11ceb47d6..ebf5c9b890d5 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -1723,14 +1723,17 @@ struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="quote">&gt; +struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="quote">&gt; +		const nodemask_t *nmask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt; +	int iter;</span>
<span class="quote">&gt;  	int node;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {</span>
<span class="quote">&gt; -		for_each_node_mask(node, *nmask) {</span>
<span class="quote">&gt; +		/* It would be nicer to iterate in the node distance order */</span>
<span class="quote">&gt; +		for_each_node_mask_preferred(node, iter, preferred_nid, *nmask) {</span>
<span class="quote">&gt;  			page = dequeue_huge_page_node_exact(h, node);</span>
<span class="quote">&gt;  			if (page)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt; @@ -1741,7 +1744,7 @@ struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="quote">&gt;  		return page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* No reservations, try to overcommit */</span>
<span class="quote">&gt; -	for_each_node_mask(node, *nmask) {</span>
<span class="quote">&gt; +	for_each_node_mask_preferred(node, iter, preferred_nid, *nmask) {</span>
<span class="quote">&gt;  		page = __alloc_buddy_huge_page_no_mpol(h, node);</span>
<span class="quote">&gt;  		if (page)</span>
<span class="quote">&gt;  			return page;</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 12, 2017, 3:21 p.m.</div>
<pre class="content">
JFTR, I am dropping this patch and will follow up with a series which
will make the hugetlb allocation reflect node/zone ordering. It doesn&#39;t
make much sense to wait for those with this series because it doesn&#39;t
depend  on it.

I have some preliminary work but I would like to give it few days before
I post it.

On Thu 08-06-17 09:45:53, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; alloc_huge_page_nodemask tries to allocate from any numa node in the</span>
<span class="quote">&gt; allowed node mask. This might lead to filling up low NUMA nodes while</span>
<span class="quote">&gt; others are not used. We can reduce this risk by introducing a concept</span>
<span class="quote">&gt; of the preferred node similar to what we have in the regular page</span>
<span class="quote">&gt; allocator. We will start allocating from the preferred nid and then</span>
<span class="quote">&gt; iterate over all allowed nodes until we try them all. Introduce</span>
<span class="quote">&gt; for_each_node_mask_preferred helper which does the iteration and reuse</span>
<span class="quote">&gt; the available preferred node in new_page_nodemask which is currently</span>
<span class="quote">&gt; the only caller of alloc_huge_page_nodemask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/hugetlb.h  |  3 ++-</span>
<span class="quote">&gt;  include/linux/migrate.h  |  2 +-</span>
<span class="quote">&gt;  include/linux/nodemask.h | 20 ++++++++++++++++++++</span>
<span class="quote">&gt;  mm/hugetlb.c             |  9 ++++++---</span>
<span class="quote">&gt;  4 files changed, 29 insertions(+), 5 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="quote">&gt; index c469191bb13b..9831a4434dd7 100644</span>
<span class="quote">&gt; --- a/include/linux/hugetlb.h</span>
<span class="quote">&gt; +++ b/include/linux/hugetlb.h</span>
<span class="quote">&gt; @@ -349,7 +349,8 @@ struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  struct page *alloc_huge_page_node(struct hstate *h, int nid);</span>
<span class="quote">&gt;  struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				unsigned long addr, int avoid_reserve);</span>
<span class="quote">&gt; -struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask);</span>
<span class="quote">&gt; +struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="quote">&gt; +				const nodemask_t *nmask);</span>
<span class="quote">&gt;  int huge_add_to_page_cache(struct page *page, struct address_space *mapping,</span>
<span class="quote">&gt;  			pgoff_t idx);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="quote">&gt; index f80c9882403a..af3ccf93efaa 100644</span>
<span class="quote">&gt; --- a/include/linux/migrate.h</span>
<span class="quote">&gt; +++ b/include/linux/migrate.h</span>
<span class="quote">&gt; @@ -38,7 +38,7 @@ static inline struct page *new_page_nodemask(struct page *page, int preferred_ni</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (PageHuge(page))</span>
<span class="quote">&gt;  		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),</span>
<span class="quote">&gt; -				nodemask);</span>
<span class="quote">&gt; +				preferred_nid, nodemask);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (PageHighMem(page)</span>
<span class="quote">&gt;  	    || (zone_idx(page_zone(page)) == ZONE_MOVABLE))</span>
<span class="quote">&gt; diff --git a/include/linux/nodemask.h b/include/linux/nodemask.h</span>
<span class="quote">&gt; index cf0b91c3ec12..797aa74392bc 100644</span>
<span class="quote">&gt; --- a/include/linux/nodemask.h</span>
<span class="quote">&gt; +++ b/include/linux/nodemask.h</span>
<span class="quote">&gt; @@ -42,6 +42,8 @@</span>
<span class="quote">&gt;   * void nodes_shift_left(dst, src, n)	Shift left</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * int first_node(mask)			Number lowest set bit, or MAX_NUMNODES</span>
<span class="quote">&gt; + * int first_node_from(nid, mask)	First node starting from nid, or wrap</span>
<span class="quote">&gt; + * 					from first or MAX_NUMNODES</span>
<span class="quote">&gt;   * int next_node(node, mask)		Next node past &#39;node&#39;, or MAX_NUMNODES</span>
<span class="quote">&gt;   * int next_node_in(node, mask)		Next node past &#39;node&#39;, or wrap to first,</span>
<span class="quote">&gt;   *					or MAX_NUMNODES</span>
<span class="quote">&gt; @@ -268,6 +270,15 @@ static inline int __next_node(int n, const nodemask_t *srcp)</span>
<span class="quote">&gt;  #define next_node_in(n, src) __next_node_in((n), &amp;(src))</span>
<span class="quote">&gt;  int __next_node_in(int node, const nodemask_t *srcp);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#define first_node_from(nid, mask) __first_node_from(nid, &amp;(mask))</span>
<span class="quote">&gt; +static inline int __first_node_from(int nid, const nodemask_t *mask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (test_bit(nid, mask-&gt;bits))</span>
<span class="quote">&gt; +		return nid;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return __next_node_in(nid, mask);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static inline void init_nodemask_of_node(nodemask_t *mask, int node)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	nodes_clear(*mask);</span>
<span class="quote">&gt; @@ -369,10 +380,19 @@ static inline void __nodes_fold(nodemask_t *dstp, const nodemask_t *origp,</span>
<span class="quote">&gt;  	for ((node) = first_node(mask);			\</span>
<span class="quote">&gt;  		(node) &lt; MAX_NUMNODES;			\</span>
<span class="quote">&gt;  		(node) = next_node((node), (mask)))</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define for_each_node_mask_preferred(node, iter, preferred, mask)	\</span>
<span class="quote">&gt; +	for ((node) = first_node_from((preferred), (mask)), iter = 0;	\</span>
<span class="quote">&gt; +		(iter) &lt; nodes_weight((mask));				\</span>
<span class="quote">&gt; +		(node) = next_node_in((node), (mask)), (iter)++)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #else /* MAX_NUMNODES == 1 */</span>
<span class="quote">&gt;  #define for_each_node_mask(node, mask)			\</span>
<span class="quote">&gt;  	if (!nodes_empty(mask))				\</span>
<span class="quote">&gt;  		for ((node) = 0; (node) &lt; 1; (node)++)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#define for_each_node_mask_preferred(node, iter, preferred, mask) \</span>
<span class="quote">&gt; +	for_each_node_mask(node, mask)</span>
<span class="quote">&gt;  #endif /* MAX_NUMNODES */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 01c11ceb47d6..ebf5c9b890d5 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -1723,14 +1723,17 @@ struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="quote">&gt; +struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="quote">&gt; +		const nodemask_t *nmask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt; +	int iter;</span>
<span class="quote">&gt;  	int node;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {</span>
<span class="quote">&gt; -		for_each_node_mask(node, *nmask) {</span>
<span class="quote">&gt; +		/* It would be nicer to iterate in the node distance order */</span>
<span class="quote">&gt; +		for_each_node_mask_preferred(node, iter, preferred_nid, *nmask) {</span>
<span class="quote">&gt;  			page = dequeue_huge_page_node_exact(h, node);</span>
<span class="quote">&gt;  			if (page)</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt; @@ -1741,7 +1744,7 @@ struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="quote">&gt;  		return page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* No reservations, try to overcommit */</span>
<span class="quote">&gt; -	for_each_node_mask(node, *nmask) {</span>
<span class="quote">&gt; +	for_each_node_mask_preferred(node, iter, preferred_nid, *nmask) {</span>
<span class="quote">&gt;  		page = __alloc_buddy_huge_page_no_mpol(h, node);</span>
<span class="quote">&gt;  		if (page)</span>
<span class="quote">&gt;  			return page;</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.11.0</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index c469191bb13b..9831a4434dd7 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -349,7 +349,8 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask);</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="p_add">+				const nodemask_t *nmask);</span>
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
<span class="p_header">diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="p_header">index f80c9882403a..af3ccf93efaa 100644</span>
<span class="p_header">--- a/include/linux/migrate.h</span>
<span class="p_header">+++ b/include/linux/migrate.h</span>
<span class="p_chunk">@@ -38,7 +38,7 @@</span> <span class="p_context"> static inline struct page *new_page_nodemask(struct page *page, int preferred_ni</span>
 
 	if (PageHuge(page))
 		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),
<span class="p_del">-				nodemask);</span>
<span class="p_add">+				preferred_nid, nodemask);</span>
 
 	if (PageHighMem(page)
 	    || (zone_idx(page_zone(page)) == ZONE_MOVABLE))
<span class="p_header">diff --git a/include/linux/nodemask.h b/include/linux/nodemask.h</span>
<span class="p_header">index cf0b91c3ec12..797aa74392bc 100644</span>
<span class="p_header">--- a/include/linux/nodemask.h</span>
<span class="p_header">+++ b/include/linux/nodemask.h</span>
<span class="p_chunk">@@ -42,6 +42,8 @@</span> <span class="p_context"></span>
  * void nodes_shift_left(dst, src, n)	Shift left
  *
  * int first_node(mask)			Number lowest set bit, or MAX_NUMNODES
<span class="p_add">+ * int first_node_from(nid, mask)	First node starting from nid, or wrap</span>
<span class="p_add">+ * 					from first or MAX_NUMNODES</span>
  * int next_node(node, mask)		Next node past &#39;node&#39;, or MAX_NUMNODES
  * int next_node_in(node, mask)		Next node past &#39;node&#39;, or wrap to first,
  *					or MAX_NUMNODES
<span class="p_chunk">@@ -268,6 +270,15 @@</span> <span class="p_context"> static inline int __next_node(int n, const nodemask_t *srcp)</span>
 #define next_node_in(n, src) __next_node_in((n), &amp;(src))
 int __next_node_in(int node, const nodemask_t *srcp);
 
<span class="p_add">+#define first_node_from(nid, mask) __first_node_from(nid, &amp;(mask))</span>
<span class="p_add">+static inline int __first_node_from(int nid, const nodemask_t *mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (test_bit(nid, mask-&gt;bits))</span>
<span class="p_add">+		return nid;</span>
<span class="p_add">+</span>
<span class="p_add">+	return __next_node_in(nid, mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void init_nodemask_of_node(nodemask_t *mask, int node)
 {
 	nodes_clear(*mask);
<span class="p_chunk">@@ -369,10 +380,19 @@</span> <span class="p_context"> static inline void __nodes_fold(nodemask_t *dstp, const nodemask_t *origp,</span>
 	for ((node) = first_node(mask);			\
 		(node) &lt; MAX_NUMNODES;			\
 		(node) = next_node((node), (mask)))
<span class="p_add">+</span>
<span class="p_add">+#define for_each_node_mask_preferred(node, iter, preferred, mask)	\</span>
<span class="p_add">+	for ((node) = first_node_from((preferred), (mask)), iter = 0;	\</span>
<span class="p_add">+		(iter) &lt; nodes_weight((mask));				\</span>
<span class="p_add">+		(node) = next_node_in((node), (mask)), (iter)++)</span>
<span class="p_add">+</span>
 #else /* MAX_NUMNODES == 1 */
 #define for_each_node_mask(node, mask)			\
 	if (!nodes_empty(mask))				\
 		for ((node) = 0; (node) &lt; 1; (node)++)
<span class="p_add">+</span>
<span class="p_add">+#define for_each_node_mask_preferred(node, iter, preferred, mask) \</span>
<span class="p_add">+	for_each_node_mask(node, mask)</span>
 #endif /* MAX_NUMNODES */
 
 /*
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 01c11ceb47d6..ebf5c9b890d5 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1723,14 +1723,17 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
 	return page;
 }
 
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="p_add">+		const nodemask_t *nmask)</span>
 {
 	struct page *page = NULL;
<span class="p_add">+	int iter;</span>
 	int node;
 
 	spin_lock(&amp;hugetlb_lock);
 	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {
<span class="p_del">-		for_each_node_mask(node, *nmask) {</span>
<span class="p_add">+		/* It would be nicer to iterate in the node distance order */</span>
<span class="p_add">+		for_each_node_mask_preferred(node, iter, preferred_nid, *nmask) {</span>
 			page = dequeue_huge_page_node_exact(h, node);
 			if (page)
 				break;
<span class="p_chunk">@@ -1741,7 +1744,7 @@</span> <span class="p_context"> struct page *alloc_huge_page_nodemask(struct hstate *h, const nodemask_t *nmask)</span>
 		return page;
 
 	/* No reservations, try to overcommit */
<span class="p_del">-	for_each_node_mask(node, *nmask) {</span>
<span class="p_add">+	for_each_node_mask_preferred(node, iter, preferred_nid, *nmask) {</span>
 		page = __alloc_buddy_huge_page_no_mpol(h, node);
 		if (page)
 			return page;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



