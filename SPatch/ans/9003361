
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm/numa/thp: fix assumptions of migrate_misplaced_transhuge_page() - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm/numa/thp: fix assumptions of migrate_misplaced_transhuge_page()</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 3, 2016, 12:33 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1462278831-1959-1-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9003361/mbox/"
   >mbox</a>
|
   <a href="/patch/9003361/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9003361/">/patch/9003361/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id AF696BF29F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 May 2016 12:34:15 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 5C96520172
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 May 2016 12:34:13 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id EC88C200E5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 May 2016 12:34:10 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933255AbcECMeH (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 3 May 2016 08:34:07 -0400
Received: from mx1.redhat.com ([209.132.183.28]:47862 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932687AbcECMeF (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 3 May 2016 08:34:05 -0400
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id B2727C000755;
	Tue,  3 May 2016 12:34:03 +0000 (UTC)
Received: from localhost.localdomain.com (vpn-48-202.rdu2.redhat.com
	[10.10.48.202])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id u43CY1bd023890; Tue, 3 May 2016 08:34:02 -0400
From: jglisse@redhat.com
To: linux-mm@kvack.org
Cc: linux-kernel@vger.kernel.org,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Subject: [PATCH] mm/numa/thp: fix assumptions of
	migrate_misplaced_transhuge_page()
Date: Tue,  3 May 2016 14:33:51 +0200
Message-Id: &lt;1462278831-1959-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - May 3, 2016, 12:33 p.m.</div>
<pre class="content">
<span class="from">From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>

Fix assumptions in migrate_misplaced_transhuge_page() which is only
call by do_huge_pmd_numa_page() itself only call by __handle_mm_fault()
for pmd with PROT_NONE. This means that if the pmd stays the same
then there can be no concurrent get_user_pages / get_user_pages_fast
(GUP/GUP_fast). More over because migrate_misplaced_transhuge_page()
abort if page is mapped more than once then there can be no GUP from
a different process. Finaly, holding the pmd lock assure us that no
other part of the kernel can take an extra reference on the page.

In the end this means that the failure code path should never be
taken unless something is horribly wrong, so convert it to BUG_ON().
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="acked-by">Acked-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
Cc: Mel Gorman &lt;mgorman@suse.de&gt;
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
---
 mm/migrate.c | 38 +++++++++++++++++++++++---------------
 1 file changed, 23 insertions(+), 15 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - May 5, 2016, 9:53 a.m.</div>
<pre class="content">
On Tue, May 03, 2016 at 02:33:51PM +0200, jglisse@redhat.com wrote:
<span class="quote">&gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Fix assumptions in migrate_misplaced_transhuge_page() which is only</span>
<span class="quote">&gt; call by do_huge_pmd_numa_page() itself only call by __handle_mm_fault()</span>
<span class="quote">&gt; for pmd with PROT_NONE. This means that if the pmd stays the same</span>
<span class="quote">&gt; then there can be no concurrent get_user_pages / get_user_pages_fast</span>
<span class="quote">&gt; (GUP/GUP_fast). More over because migrate_misplaced_transhuge_page()</span>
<span class="quote">&gt; abort if page is mapped more than once then there can be no GUP from</span>
<span class="quote">&gt; a different process. Finaly, holding the pmd lock assure us that no</span>
<span class="quote">&gt; other part of the kernel can take an extra reference on the page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In the end this means that the failure code path should never be</span>
<span class="quote">&gt; taken unless something is horribly wrong, so convert it to BUG_ON().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Acked-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="quote">&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
<span class="acked-by">
Acked-by: Mel Gorman &lt;mgorman@suse.de&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index f9dfb18..07148be 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -1760,7 +1760,13 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 	int page_lru = page_is_file_cache(page);
 	unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;
 	unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;
<span class="p_del">-	pmd_t orig_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * What we do here is only valid if pmd_protnone(entry) is true and thp</span>
<span class="p_add">+	 * page is map in only once, which numamigrate_isolate_page() checks.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!pmd_protnone(entry))</span>
<span class="p_add">+		goto out_unlock;</span>
 
 	/*
 	 * Rate-limit the amount of data that is being migrated to a node.
<span class="p_chunk">@@ -1803,7 +1809,6 @@</span> <span class="p_context"> int migrate_misplaced_transhuge_page(struct mm_struct *mm,</span>
 	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);
 	ptl = pmd_lock(mm, pmd);
 	if (unlikely(!pmd_same(*pmd, entry) || page_count(page) != 2)) {
<span class="p_del">-fail_putback:</span>
 		spin_unlock(ptl);
 		mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
 
<span class="p_chunk">@@ -1825,17 +1830,21 @@</span> <span class="p_context"> fail_putback:</span>
 		goto out_unlock;
 	}
 
<span class="p_del">-	orig_entry = *pmd;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We are holding the lock so no one can set a new pmd and original pmd</span>
<span class="p_add">+	 * is PROT_NONE thus no one can get_user_pages or get_user_pages_fast</span>
<span class="p_add">+	 * (GUP or GUP_fast) from this point on we can not fail.</span>
<span class="p_add">+	 */</span>
 	entry = mk_pmd(new_page, vma-&gt;vm_page_prot);
 	entry = pmd_mkhuge(entry);
 	entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
 
 	/*
 	 * Clear the old entry under pagetable lock and establish the new PTE.
<span class="p_del">-	 * Any parallel GUP will either observe the old page blocking on the</span>
<span class="p_del">-	 * page lock, block on the page table lock or observe the new page.</span>
<span class="p_del">-	 * The SetPageUptodate on the new page and page_add_new_anon_rmap</span>
<span class="p_del">-	 * guarantee the copy is visible before the pagetable update.</span>
<span class="p_add">+	 * Any parallel GUP can only observe the new page as old page only had</span>
<span class="p_add">+	 * one mapping with PROT_NONE (GUP/GUP_fast fails if pmd_protnone() is</span>
<span class="p_add">+	 * true). However a concurrent GUP might see the new page as soon as</span>
<span class="p_add">+	 * we set the pmd to the new entry.</span>
 	 */
 	flush_cache_range(vma, mmun_start, mmun_end);
 	page_add_anon_rmap(new_page, vma, mmun_start, true);
<span class="p_chunk">@@ -1843,14 +1852,13 @@</span> <span class="p_context"> fail_putback:</span>
 	set_pmd_at(mm, mmun_start, pmd, entry);
 	update_mmu_cache_pmd(vma, address, &amp;entry);
 
<span class="p_del">-	if (page_count(page) != 2) {</span>
<span class="p_del">-		set_pmd_at(mm, mmun_start, pmd, orig_entry);</span>
<span class="p_del">-		flush_pmd_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="p_del">-		mmu_notifier_invalidate_range(mm, mmun_start, mmun_end);</span>
<span class="p_del">-		update_mmu_cache_pmd(vma, address, &amp;entry);</span>
<span class="p_del">-		page_remove_rmap(new_page, true);</span>
<span class="p_del">-		goto fail_putback;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	/* As said above no one can get reference on the old page nor through</span>
<span class="p_add">+	 * get_user_pages or get_user_pages_fast (GUP/GUP_fast) or through</span>
<span class="p_add">+	 * any other means. To get reference on huge page you need to hold</span>
<span class="p_add">+	 * pmd_lock and we are already holding that lock here and the page</span>
<span class="p_add">+	 * is only mapped once.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUG_ON(page_count(page) != 2);</span>
 
 	mlock_migrate_page(new_page, page);
 	page_remove_rmap(page, true);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



