
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,6/7] arm64: KVM: Handle trappable TLB instructions - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,6/7] arm64: KVM: Handle trappable TLB instructions</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=58591">Punit Agrawal</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 16, 2016, 10:45 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1471344312-26685-7-git-send-email-punit.agrawal@arm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9283313/mbox/"
   >mbox</a>
|
   <a href="/patch/9283313/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9283313/">/patch/9283313/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C54126086A for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 Aug 2016 10:47:21 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B3E201FFBD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 Aug 2016 10:47:21 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A7FFF285DE; Tue, 16 Aug 2016 10:47:21 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B5F0F285DF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 Aug 2016 10:47:20 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932339AbcHPKrI (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 16 Aug 2016 06:47:08 -0400
Received: from fw-tnat.cambridge.arm.com ([217.140.96.140]:21686 &quot;EHLO
	cam-smtp0.cambridge.arm.com&quot; rhost-flags-OK-OK-OK-FAIL)
	by vger.kernel.org with ESMTP id S1753590AbcHPKrE (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 16 Aug 2016 06:47:04 -0400
Received: from e105922-lin.cambridge.arm.com (e105922-lin.cambridge.arm.com
	[10.1.194.52])
	by cam-smtp0.cambridge.arm.com (8.13.8/8.13.8) with SMTP id
	u7GAkNIr032342; Tue, 16 Aug 2016 11:46:23 +0100
Received: by e105922-lin.cambridge.arm.com (sSMTP sendmail emulation);
	Tue, 16 Aug 2016 11:46:23 +0100
From: Punit Agrawal &lt;punit.agrawal@arm.com&gt;
To: linux-kernel@vger.kernel.org, kvm@vger.kernel.org,
	kvmarm@lists.cs.columbia.edu, linux-arm-kernel@lists.infradead.org
Cc: Punit Agrawal &lt;punit.agrawal@arm.com&gt;,
	Christoffer Dall &lt;christoffer.dall@linaro.org&gt;,
	Marc Zyngier &lt;marc.zyngier@arm.com&gt;,
	Steven Rostedt &lt;rostedt@goodmis.org&gt;, Ingo Molnar &lt;mingo@redhat.com&gt;,
	Will Deacon &lt;will.deacon@arm.com&gt;
Subject: [RFC PATCH 6/7] arm64: KVM: Handle trappable TLB instructions
Date: Tue, 16 Aug 2016 11:45:11 +0100
Message-Id: &lt;1471344312-26685-7-git-send-email-punit.agrawal@arm.com&gt;
X-Mailer: git-send-email 2.8.1
In-Reply-To: &lt;1471344312-26685-1-git-send-email-punit.agrawal@arm.com&gt;
References: &lt;1471344312-26685-1-git-send-email-punit.agrawal@arm.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=58591">Punit Agrawal</a> - Aug. 16, 2016, 10:45 a.m.</div>
<pre class="content">
The ARMv8 architecture allows trapping of TLB maintenane instructions
from EL0/EL1 to higher exception levels. On encountering a trappable TLB
instruction in a guest, an exception is taken to EL2.

Add functionality to handle emulating the TLB instructions.
<span class="signed-off-by">
Signed-off-by: Punit Agrawal &lt;punit.agrawal@arm.com&gt;</span>
Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;
Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;
---
 arch/arm64/include/asm/kvm_asm.h |   1 +
 arch/arm64/kvm/hyp/tlb.c         | 146 +++++++++++++++++++++++++++++++++++++++
 arch/arm64/kvm/sys_regs.c        |  81 ++++++++++++++++++++++
 arch/arm64/kvm/trace.h           |  16 +++++
 4 files changed, 244 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Aug. 19, 2016, 3:18 p.m.</div>
<pre class="content">
Hi Punit,

On Tue, Aug 16, 2016 at 11:45:11AM +0100, Punit Agrawal wrote:
<span class="quote">&gt; The ARMv8 architecture allows trapping of TLB maintenane instructions</span>
<span class="quote">&gt; from EL0/EL1 to higher exception levels. On encountering a trappable TLB</span>
<span class="quote">&gt; instruction in a guest, an exception is taken to EL2.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Add functionality to handle emulating the TLB instructions.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Punit Agrawal &lt;punit.agrawal@arm.com&gt;</span>
<span class="quote">&gt; Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="quote">&gt; Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;</span>

[...]
<span class="quote">
&gt; +void __hyp_text</span>
<span class="quote">&gt; +__kvm_emulate_tlb_invalidate(struct kvm *kvm, u32 sys_op, u64 regval)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	kvm = kern_hyp_va(kvm);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Switch to the guest before performing any TLB operations to</span>
<span class="quote">&gt; +	 * target the appropriate VMID</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	__switch_to_guest_regime(kvm);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 *  TLB maintenance operations broadcast to inner-shareable</span>
<span class="quote">&gt; +	 *  domain when HCR_FB is set (default for KVM).</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	switch (sys_op) {</span>
<span class="quote">&gt; +	case TLBIALL:</span>
<span class="quote">&gt; +	case TLBIALLIS:</span>
<span class="quote">&gt; +	case ITLBIALL:</span>
<span class="quote">&gt; +	case DTLBIALL:</span>
<span class="quote">&gt; +	case TLBI_VMALLE1:</span>
<span class="quote">&gt; +	case TLBI_VMALLE1IS:</span>
<span class="quote">&gt; +		__tlbi(vmalle1is);</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case TLBIMVA:</span>
<span class="quote">&gt; +	case TLBIMVAIS:</span>
<span class="quote">&gt; +	case ITLBIMVA:</span>
<span class="quote">&gt; +	case DTLBIMVA:</span>
<span class="quote">&gt; +	case TLBI_VAE1:</span>
<span class="quote">&gt; +	case TLBI_VAE1IS:</span>
<span class="quote">&gt; +		__tlbi(vae1is, regval);</span>

I&#39;m pretty nervous about this. Although you&#39;ve switched in the guest stage-2
page table before the TLB maintenance, we&#39;re still running on a host stage-1
and it&#39;s not clear to me that the stage-1 context is completely ignored for
the purposes of a stage-1 TLBI executed at EL2.

For example, if TCR_EL1.TBI0 is set in the guest but cleared in the host,
my reading of the architecture is that it will be treated as zero when
we perform this invalidation operation. I worry that we have similar
problems with the granule size, where bits become RES0 in the TLBI VA
ops.

Finally, we should probably be masking out the RES0 bits in the TLBI
ops, just in case some future extension to the architecture defines them
in such a way where they have different meanings when executed at EL2
or EL1.

The easiest thing to do is just TLBI VMALLE1IS for all trapped operations,
but you might want to see how that performs.

Will
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=58591">Punit Agrawal</a> - Aug. 24, 2016, 10:40 a.m.</div>
<pre class="content">
Will Deacon &lt;will.deacon@arm.com&gt; writes:
<span class="quote">
&gt; Hi Punit,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; On Tue, Aug 16, 2016 at 11:45:11AM +0100, Punit Agrawal wrote:</span>
<span class="quote">&gt;&gt; The ARMv8 architecture allows trapping of TLB maintenane instructions</span>
<span class="quote">&gt;&gt; from EL0/EL1 to higher exception levels. On encountering a trappable TLB</span>
<span class="quote">&gt;&gt; instruction in a guest, an exception is taken to EL2.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Add functionality to handle emulating the TLB instructions.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Signed-off-by: Punit Agrawal &lt;punit.agrawal@arm.com&gt;</span>
<span class="quote">&gt;&gt; Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="quote">&gt;&gt; Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +void __hyp_text</span>
<span class="quote">&gt;&gt; +__kvm_emulate_tlb_invalidate(struct kvm *kvm, u32 sys_op, u64 regval)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +	kvm = kern_hyp_va(kvm);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 * Switch to the guest before performing any TLB operations to</span>
<span class="quote">&gt;&gt; +	 * target the appropriate VMID</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	__switch_to_guest_regime(kvm);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt; +	 *  TLB maintenance operations broadcast to inner-shareable</span>
<span class="quote">&gt;&gt; +	 *  domain when HCR_FB is set (default for KVM).</span>
<span class="quote">&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt; +	switch (sys_op) {</span>
<span class="quote">&gt;&gt; +	case TLBIALL:</span>
<span class="quote">&gt;&gt; +	case TLBIALLIS:</span>
<span class="quote">&gt;&gt; +	case ITLBIALL:</span>
<span class="quote">&gt;&gt; +	case DTLBIALL:</span>
<span class="quote">&gt;&gt; +	case TLBI_VMALLE1:</span>
<span class="quote">&gt;&gt; +	case TLBI_VMALLE1IS:</span>
<span class="quote">&gt;&gt; +		__tlbi(vmalle1is);</span>
<span class="quote">&gt;&gt; +		break;</span>
<span class="quote">&gt;&gt; +	case TLBIMVA:</span>
<span class="quote">&gt;&gt; +	case TLBIMVAIS:</span>
<span class="quote">&gt;&gt; +	case ITLBIMVA:</span>
<span class="quote">&gt;&gt; +	case DTLBIMVA:</span>
<span class="quote">&gt;&gt; +	case TLBI_VAE1:</span>
<span class="quote">&gt;&gt; +	case TLBI_VAE1IS:</span>
<span class="quote">&gt;&gt; +		__tlbi(vae1is, regval);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I&#39;m pretty nervous about this. Although you&#39;ve switched in the guest stage-2</span>
<span class="quote">&gt; page table before the TLB maintenance, we&#39;re still running on a host stage-1</span>
<span class="quote">&gt; and it&#39;s not clear to me that the stage-1 context is completely ignored for</span>
<span class="quote">&gt; the purposes of a stage-1 TLBI executed at EL2.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; For example, if TCR_EL1.TBI0 is set in the guest but cleared in the host,</span>
<span class="quote">&gt; my reading of the architecture is that it will be treated as zero when</span>
<span class="quote">&gt; we perform this invalidation operation. I worry that we have similar</span>
<span class="quote">&gt; problems with the granule size, where bits become RES0 in the TLBI VA</span>
<span class="quote">&gt; ops.</span>

Some control bits seem to be explicitly called out to not affect TLB
maintenance operations[0] but I hadn&#39;t considered the ones you highlight.

[0] ARMv8 ARM DDI 0487A.j D4.7, Pg D4-1814
<span class="quote">
&gt;</span>
<span class="quote">&gt; Finally, we should probably be masking out the RES0 bits in the TLBI</span>
<span class="quote">&gt; ops, just in case some future extension to the architecture defines them</span>
<span class="quote">&gt; in such a way where they have different meanings when executed at EL2</span>
<span class="quote">&gt; or EL1.</span>

Although, the RES0 bits for TLBI VA ops are currently ignored, I agree
that masking them out based on granule size protects against future
incompatible changes.
<span class="quote">
&gt;</span>
<span class="quote">&gt; The easiest thing to do is just TLBI VMALLE1IS for all trapped operations,</span>
<span class="quote">&gt; but you might want to see how that performs.</span>

That sounds reasonable for correctness. But I suspect we&#39;ll have to do
more to claw back some performance. Let me run a few tests and come back
on this.

Thanks for having a look.

Punit
<span class="quote">
&gt;</span>
<span class="quote">&gt; Will</span>
<span class="quote">&gt; _______________________________________________</span>
<span class="quote">&gt; kvmarm mailing list</span>
<span class="quote">&gt; kvmarm@lists.cs.columbia.edu</span>
<span class="quote">&gt; https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=58591">Punit Agrawal</a> - Aug. 26, 2016, 9:37 a.m.</div>
<pre class="content">
Punit Agrawal &lt;punit.agrawal@arm.com&gt; writes:
<span class="quote">
&gt; Will Deacon &lt;will.deacon@arm.com&gt; writes:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; Hi Punit,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On Tue, Aug 16, 2016 at 11:45:11AM +0100, Punit Agrawal wrote:</span>
<span class="quote">&gt;&gt;&gt; The ARMv8 architecture allows trapping of TLB maintenane instructions</span>
<span class="quote">&gt;&gt;&gt; from EL0/EL1 to higher exception levels. On encountering a trappable TLB</span>
<span class="quote">&gt;&gt;&gt; instruction in a guest, an exception is taken to EL2.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Add functionality to handle emulating the TLB instructions.</span>
<span class="quote">&gt;&gt;&gt; </span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Punit Agrawal &lt;punit.agrawal@arm.com&gt;</span>
<span class="quote">&gt;&gt;&gt; Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="quote">&gt;&gt;&gt; Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; [...]</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; +void __hyp_text</span>
<span class="quote">&gt;&gt;&gt; +__kvm_emulate_tlb_invalidate(struct kvm *kvm, u32 sys_op, u64 regval)</span>
<span class="quote">&gt;&gt;&gt; +{</span>
<span class="quote">&gt;&gt;&gt; +	kvm = kern_hyp_va(kvm);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt;&gt; +	 * Switch to the guest before performing any TLB operations to</span>
<span class="quote">&gt;&gt;&gt; +	 * target the appropriate VMID</span>
<span class="quote">&gt;&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt;&gt; +	__switch_to_guest_regime(kvm);</span>
<span class="quote">&gt;&gt;&gt; +</span>
<span class="quote">&gt;&gt;&gt; +	/*</span>
<span class="quote">&gt;&gt;&gt; +	 *  TLB maintenance operations broadcast to inner-shareable</span>
<span class="quote">&gt;&gt;&gt; +	 *  domain when HCR_FB is set (default for KVM).</span>
<span class="quote">&gt;&gt;&gt; +	 */</span>
<span class="quote">&gt;&gt;&gt; +	switch (sys_op) {</span>
<span class="quote">&gt;&gt;&gt; +	case TLBIALL:</span>
<span class="quote">&gt;&gt;&gt; +	case TLBIALLIS:</span>
<span class="quote">&gt;&gt;&gt; +	case ITLBIALL:</span>
<span class="quote">&gt;&gt;&gt; +	case DTLBIALL:</span>
<span class="quote">&gt;&gt;&gt; +	case TLBI_VMALLE1:</span>
<span class="quote">&gt;&gt;&gt; +	case TLBI_VMALLE1IS:</span>
<span class="quote">&gt;&gt;&gt; +		__tlbi(vmalle1is);</span>
<span class="quote">&gt;&gt;&gt; +		break;</span>
<span class="quote">&gt;&gt;&gt; +	case TLBIMVA:</span>
<span class="quote">&gt;&gt;&gt; +	case TLBIMVAIS:</span>
<span class="quote">&gt;&gt;&gt; +	case ITLBIMVA:</span>
<span class="quote">&gt;&gt;&gt; +	case DTLBIMVA:</span>
<span class="quote">&gt;&gt;&gt; +	case TLBI_VAE1:</span>
<span class="quote">&gt;&gt;&gt; +	case TLBI_VAE1IS:</span>
<span class="quote">&gt;&gt;&gt; +		__tlbi(vae1is, regval);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;m pretty nervous about this. Although you&#39;ve switched in the guest stage-2</span>
<span class="quote">&gt;&gt; page table before the TLB maintenance, we&#39;re still running on a host stage-1</span>
<span class="quote">&gt;&gt; and it&#39;s not clear to me that the stage-1 context is completely ignored for</span>
<span class="quote">&gt;&gt; the purposes of a stage-1 TLBI executed at EL2.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; For example, if TCR_EL1.TBI0 is set in the guest but cleared in the host,</span>
<span class="quote">&gt;&gt; my reading of the architecture is that it will be treated as zero when</span>
<span class="quote">&gt;&gt; we perform this invalidation operation. I worry that we have similar</span>
<span class="quote">&gt;&gt; problems with the granule size, where bits become RES0 in the TLBI VA</span>
<span class="quote">&gt;&gt; ops.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Some control bits seem to be explicitly called out to not affect TLB</span>
<span class="quote">&gt; maintenance operations[0] but I hadn&#39;t considered the ones you highlight.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; [0] ARMv8 ARM DDI 0487A.j D4.7, Pg D4-1814</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Finally, we should probably be masking out the RES0 bits in the TLBI</span>
<span class="quote">&gt;&gt; ops, just in case some future extension to the architecture defines them</span>
<span class="quote">&gt;&gt; in such a way where they have different meanings when executed at EL2</span>
<span class="quote">&gt;&gt; or EL1.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Although, the RES0 bits for TLBI VA ops are currently ignored, I agree</span>
<span class="quote">&gt; that masking them out based on granule size protects against future</span>
<span class="quote">&gt; incompatible changes.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The easiest thing to do is just TLBI VMALLE1IS for all trapped operations,</span>
<span class="quote">&gt;&gt; but you might want to see how that performs.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That sounds reasonable for correctness. But I suspect we&#39;ll have to do</span>
<span class="quote">&gt; more to claw back some performance. Let me run a few tests and come back</span>
<span class="quote">&gt; on this.</span>

Assuming I&#39;ve correctly switched in TCR and replacing the various TLB
operations in this patch with TLBI VMALLE1IS, there is a drop in kernel
build times of ~5% (384s vs 363s).

For the next version, I&#39;ll use this as a starting point and try clawing
back the loss by using the appropriate TLB instructions albeit with
additional sanity checking based on context.
<span class="quote">
&gt;</span>
<span class="quote">&gt; Thanks for having a look.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Punit</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Will</span>
<span class="quote">&gt;&gt; _______________________________________________</span>
<span class="quote">&gt;&gt; kvmarm mailing list</span>
<span class="quote">&gt;&gt; kvmarm@lists.cs.columbia.edu</span>
<span class="quote">&gt;&gt; https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</span>
<span class="quote">&gt; _______________________________________________</span>
<span class="quote">&gt; kvmarm mailing list</span>
<span class="quote">&gt; kvmarm@lists.cs.columbia.edu</span>
<span class="quote">&gt; https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=28962">Marc Zyngier</a> - Aug. 26, 2016, 12:21 p.m.</div>
<pre class="content">
On Fri, 26 Aug 2016 10:37:08 +0100
Punit Agrawal &lt;punit.agrawal@arm.com&gt; wrote:
<span class="quote">
&gt; Punit Agrawal &lt;punit.agrawal@arm.com&gt; writes:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; Will Deacon &lt;will.deacon@arm.com&gt; writes:</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;&gt; Hi Punit,</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; On Tue, Aug 16, 2016 at 11:45:11AM +0100, Punit Agrawal wrote:  </span>
<span class="quote">&gt; &gt;&gt;&gt; The ARMv8 architecture allows trapping of TLB maintenane instructions</span>
<span class="quote">&gt; &gt;&gt;&gt; from EL0/EL1 to higher exception levels. On encountering a trappable TLB</span>
<span class="quote">&gt; &gt;&gt;&gt; instruction in a guest, an exception is taken to EL2.</span>
<span class="quote">&gt; &gt;&gt;&gt; </span>
<span class="quote">&gt; &gt;&gt;&gt; Add functionality to handle emulating the TLB instructions.</span>
<span class="quote">&gt; &gt;&gt;&gt; </span>
<span class="quote">&gt; &gt;&gt;&gt; Signed-off-by: Punit Agrawal &lt;punit.agrawal@arm.com&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Cc: Christoffer Dall &lt;christoffer.dall@linaro.org&gt;</span>
<span class="quote">&gt; &gt;&gt;&gt; Cc: Marc Zyngier &lt;marc.zyngier@arm.com&gt;  </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; [...]</span>
<span class="quote">&gt; &gt;&gt;  </span>
<span class="quote">&gt; &gt;&gt;&gt; +void __hyp_text</span>
<span class="quote">&gt; &gt;&gt;&gt; +__kvm_emulate_tlb_invalidate(struct kvm *kvm, u32 sys_op, u64 regval)</span>
<span class="quote">&gt; &gt;&gt;&gt; +{</span>
<span class="quote">&gt; &gt;&gt;&gt; +	kvm = kern_hyp_va(kvm);</span>
<span class="quote">&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt; +	/*</span>
<span class="quote">&gt; &gt;&gt;&gt; +	 * Switch to the guest before performing any TLB operations to</span>
<span class="quote">&gt; &gt;&gt;&gt; +	 * target the appropriate VMID</span>
<span class="quote">&gt; &gt;&gt;&gt; +	 */</span>
<span class="quote">&gt; &gt;&gt;&gt; +	__switch_to_guest_regime(kvm);</span>
<span class="quote">&gt; &gt;&gt;&gt; +</span>
<span class="quote">&gt; &gt;&gt;&gt; +	/*</span>
<span class="quote">&gt; &gt;&gt;&gt; +	 *  TLB maintenance operations broadcast to inner-shareable</span>
<span class="quote">&gt; &gt;&gt;&gt; +	 *  domain when HCR_FB is set (default for KVM).</span>
<span class="quote">&gt; &gt;&gt;&gt; +	 */</span>
<span class="quote">&gt; &gt;&gt;&gt; +	switch (sys_op) {</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case TLBIALL:</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case TLBIALLIS:</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case ITLBIALL:</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case DTLBIALL:</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case TLBI_VMALLE1:</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case TLBI_VMALLE1IS:</span>
<span class="quote">&gt; &gt;&gt;&gt; +		__tlbi(vmalle1is);</span>
<span class="quote">&gt; &gt;&gt;&gt; +		break;</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case TLBIMVA:</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case TLBIMVAIS:</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case ITLBIMVA:</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case DTLBIMVA:</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case TLBI_VAE1:</span>
<span class="quote">&gt; &gt;&gt;&gt; +	case TLBI_VAE1IS:</span>
<span class="quote">&gt; &gt;&gt;&gt; +		__tlbi(vae1is, regval);  </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I&#39;m pretty nervous about this. Although you&#39;ve switched in the guest stage-2</span>
<span class="quote">&gt; &gt;&gt; page table before the TLB maintenance, we&#39;re still running on a host stage-1</span>
<span class="quote">&gt; &gt;&gt; and it&#39;s not clear to me that the stage-1 context is completely ignored for</span>
<span class="quote">&gt; &gt;&gt; the purposes of a stage-1 TLBI executed at EL2.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; For example, if TCR_EL1.TBI0 is set in the guest but cleared in the host,</span>
<span class="quote">&gt; &gt;&gt; my reading of the architecture is that it will be treated as zero when</span>
<span class="quote">&gt; &gt;&gt; we perform this invalidation operation. I worry that we have similar</span>
<span class="quote">&gt; &gt;&gt; problems with the granule size, where bits become RES0 in the TLBI VA</span>
<span class="quote">&gt; &gt;&gt; ops.  </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Some control bits seem to be explicitly called out to not affect TLB</span>
<span class="quote">&gt; &gt; maintenance operations[0] but I hadn&#39;t considered the ones you highlight.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; [0] ARMv8 ARM DDI 0487A.j D4.7, Pg D4-1814</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Finally, we should probably be masking out the RES0 bits in the TLBI</span>
<span class="quote">&gt; &gt;&gt; ops, just in case some future extension to the architecture defines them</span>
<span class="quote">&gt; &gt;&gt; in such a way where they have different meanings when executed at EL2</span>
<span class="quote">&gt; &gt;&gt; or EL1.  </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Although, the RES0 bits for TLBI VA ops are currently ignored, I agree</span>
<span class="quote">&gt; &gt; that masking them out based on granule size protects against future</span>
<span class="quote">&gt; &gt; incompatible changes.</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; The easiest thing to do is just TLBI VMALLE1IS for all trapped operations,</span>
<span class="quote">&gt; &gt;&gt; but you might want to see how that performs.  </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; That sounds reasonable for correctness. But I suspect we&#39;ll have to do</span>
<span class="quote">&gt; &gt; more to claw back some performance. Let me run a few tests and come back</span>
<span class="quote">&gt; &gt; on this.  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Assuming I&#39;ve correctly switched in TCR and replacing the various TLB</span>
<span class="quote">&gt; operations in this patch with TLBI VMALLE1IS, there is a drop in kernel</span>
<span class="quote">&gt; build times of ~5% (384s vs 363s).</span>

Note that if all you&#39;re doing is a VMALLE1IS, switching TCR_EL1 should
not be necessary, as all that is required for this invalidation is the
VMID.
<span class="quote">
&gt; For the next version, I&#39;ll use this as a starting point and try clawing</span>
<span class="quote">&gt; back the loss by using the appropriate TLB instructions albeit with</span>
<span class="quote">&gt; additional sanity checking based on context.</span>

Great, thanks!

	M.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=7096">Will Deacon</a> - Sept. 1, 2016, 2:55 p.m.</div>
<pre class="content">
On Fri, Aug 26, 2016 at 10:37:08AM +0100, Punit Agrawal wrote:
<span class="quote">&gt; &gt; Will Deacon &lt;will.deacon@arm.com&gt; writes:</span>
<span class="quote">&gt; &gt;&gt; The easiest thing to do is just TLBI VMALLE1IS for all trapped operations,</span>
<span class="quote">&gt; &gt;&gt; but you might want to see how that performs.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; That sounds reasonable for correctness. But I suspect we&#39;ll have to do</span>
<span class="quote">&gt; &gt; more to claw back some performance. Let me run a few tests and come back</span>
<span class="quote">&gt; &gt; on this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Assuming I&#39;ve correctly switched in TCR and replacing the various TLB</span>
<span class="quote">&gt; operations in this patch with TLBI VMALLE1IS, there is a drop in kernel</span>
<span class="quote">&gt; build times of ~5% (384s vs 363s).</span>

What do you mean by &quot;switched in TCR&quot;? Why is that necessary if you just
nuke the whole thing? Is the ~5% relative to no trapping at all, or
trapping, but being selective about the operation?

Will
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=58591">Punit Agrawal</a> - Sept. 1, 2016, 6:29 p.m.</div>
<pre class="content">
Will Deacon &lt;will.deacon@arm.com&gt; writes:
<span class="quote">
&gt; On Fri, Aug 26, 2016 at 10:37:08AM +0100, Punit Agrawal wrote:</span>
<span class="quote">&gt;&gt; &gt; Will Deacon &lt;will.deacon@arm.com&gt; writes:</span>
<span class="quote">&gt;&gt; &gt;&gt; The easiest thing to do is just TLBI VMALLE1IS for all trapped operations,</span>
<span class="quote">&gt;&gt; &gt;&gt; but you might want to see how that performs.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; That sounds reasonable for correctness. But I suspect we&#39;ll have to do</span>
<span class="quote">&gt;&gt; &gt; more to claw back some performance. Let me run a few tests and come back</span>
<span class="quote">&gt;&gt; &gt; on this.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Assuming I&#39;ve correctly switched in TCR and replacing the various TLB</span>
<span class="quote">&gt;&gt; operations in this patch with TLBI VMALLE1IS, there is a drop in kernel</span>
<span class="quote">&gt;&gt; build times of ~5% (384s vs 363s).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; What do you mean by &quot;switched in TCR&quot;? Why is that necessary if you just</span>
<span class="quote">&gt; nuke the whole thing?</span>

You&#39;re right. it&#39;s not necessary. I&#39;d misunderstood how TCR affects
things and was switching it in the above tests.
<span class="quote">
&gt; Is the ~5% relative to no trapping at all, or</span>
<span class="quote">&gt; trapping, but being selective about the operation?</span>

The reported number was relative to trapping and being selective about
the operation. But I hadn&#39;t been careful in ensuring identical
conditions (page caches, etc.) when running the numbers.

So I&#39;ve done a fresh set of identical measurements by running &quot;time make
-j 7&quot; in a VM booted with 7 vcpus and see the following results

1. no trapping ~ 365s
2. traps using selective tlb operations ~ 371s
3. traps that nuke all stage 1 (tlbi vmalle1is) ~ 393s

So based on these measurements there is ~1% and ~7.5% drop in comparison
between 2. and 3. compared to the base case of no trapping at all.

Thanks,
Punit
<span class="quote">
&gt;</span>
<span class="quote">&gt; Will</span>
<span class="quote">&gt; _______________________________________________</span>
<span class="quote">&gt; kvmarm mailing list</span>
<span class="quote">&gt; kvmarm@lists.cs.columbia.edu</span>
<span class="quote">&gt; https://lists.cs.columbia.edu/mailman/listinfo/kvmarm</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/include/asm/kvm_asm.h b/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_header">index 7561f63..1ac1cc3 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/kvm_asm.h</span>
<span class="p_chunk">@@ -49,6 +49,7 @@</span> <span class="p_context"> extern char __kvm_hyp_vector[];</span>
 extern void __kvm_flush_vm_context(void);
 extern void __kvm_tlb_flush_vmid_ipa(struct kvm *kvm, phys_addr_t ipa);
 extern void __kvm_tlb_flush_vmid(struct kvm *kvm);
<span class="p_add">+extern void __kvm_emulate_tlb_invalidate(struct kvm *kvm, u32 sysreg, u64 regval);</span>
 
 extern int __kvm_vcpu_run(struct kvm_vcpu *vcpu);
 
<span class="p_header">diff --git a/arch/arm64/kvm/hyp/tlb.c b/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_header">index 4cda100..e0a0309 100644</span>
<span class="p_header">--- a/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_header">+++ b/arch/arm64/kvm/hyp/tlb.c</span>
<span class="p_chunk">@@ -78,3 +78,149 @@</span> <span class="p_context"> static void __hyp_text __tlb_flush_vm_context(void)</span>
 }
 
 __alias(__tlb_flush_vm_context) void __kvm_flush_vm_context(void);
<span class="p_add">+</span>
<span class="p_add">+/* Intentionally empty functions */</span>
<span class="p_add">+static void __hyp_text __switch_to_hyp_role_nvhe(void) { }</span>
<span class="p_add">+static void __hyp_text __switch_to_host_role_nvhe(void) { }</span>
<span class="p_add">+</span>
<span class="p_add">+static void __hyp_text __switch_to_hyp_role_vhe(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 hcr = read_sysreg(hcr_el2);</span>
<span class="p_add">+</span>
<span class="p_add">+	hcr &amp;= ~HCR_TGE;</span>
<span class="p_add">+	write_sysreg(hcr, hcr_el2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __hyp_text __switch_to_host_role_vhe(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 hcr = read_sysreg(hcr_el2);</span>
<span class="p_add">+</span>
<span class="p_add">+	hcr |= HCR_TGE;</span>
<span class="p_add">+	write_sysreg(hcr, hcr_el2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static hyp_alternate_select(__switch_to_hyp_role,</span>
<span class="p_add">+			    __switch_to_hyp_role_nvhe,</span>
<span class="p_add">+			    __switch_to_hyp_role_vhe,</span>
<span class="p_add">+			    ARM64_HAS_VIRT_HOST_EXTN);</span>
<span class="p_add">+</span>
<span class="p_add">+static hyp_alternate_select(__switch_to_host_role,</span>
<span class="p_add">+			    __switch_to_host_role_nvhe,</span>
<span class="p_add">+			    __switch_to_host_role_vhe,</span>
<span class="p_add">+			    ARM64_HAS_VIRT_HOST_EXTN);</span>
<span class="p_add">+</span>
<span class="p_add">+static void __hyp_text __switch_to_guest_regime(struct kvm *kvm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	write_sysreg(kvm-&gt;arch.vttbr, vttbr_el2);</span>
<span class="p_add">+	__switch_to_hyp_role();</span>
<span class="p_add">+	isb();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __hyp_text __switch_to_host_regime(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__switch_to_host_role();</span>
<span class="p_add">+	write_sysreg(0, vttbr_el2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ *  AArch32 TLB maintenance instructions trapping to EL2</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define TLBIALLIS			sys_reg(0, 0, 8, 3, 0)</span>
<span class="p_add">+#define TLBIMVAIS			sys_reg(0, 0, 8, 3, 1)</span>
<span class="p_add">+#define TLBIASIDIS			sys_reg(0, 0, 8, 3, 2)</span>
<span class="p_add">+#define TLBIMVAAIS			sys_reg(0, 0, 8, 3, 3)</span>
<span class="p_add">+#define TLBIMVALIS			sys_reg(0, 0, 8, 3, 5)</span>
<span class="p_add">+#define TLBIMVAALIS			sys_reg(0, 0, 8, 3, 7)</span>
<span class="p_add">+#define ITLBIALL			sys_reg(0, 0, 8, 5, 0)</span>
<span class="p_add">+#define ITLBIMVA			sys_reg(0, 0, 8, 5, 1)</span>
<span class="p_add">+#define ITLBIASID			sys_reg(0, 0, 8, 5, 2)</span>
<span class="p_add">+#define DTLBIALL			sys_reg(0, 0, 8, 6, 0)</span>
<span class="p_add">+#define DTLBIMVA			sys_reg(0, 0, 8, 6, 1)</span>
<span class="p_add">+#define DTLBIASID			sys_reg(0, 0, 8, 6, 2)</span>
<span class="p_add">+#define TLBIALL				sys_reg(0, 0, 8, 7, 0)</span>
<span class="p_add">+#define TLBIMVA				sys_reg(0, 0, 8, 7, 1)</span>
<span class="p_add">+#define TLBIASID			sys_reg(0, 0, 8, 7, 2)</span>
<span class="p_add">+#define TLBIMVAA			sys_reg(0, 0, 8, 7, 3)</span>
<span class="p_add">+#define TLBIMVAL			sys_reg(0, 0, 8, 7, 5)</span>
<span class="p_add">+#define TLBIMVAAL			sys_reg(0, 0, 8, 7, 7)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * ARMv8 ARM: Table C5-4 TLB maintenance instructions</span>
<span class="p_add">+ * (Ref: ARMv8 ARM C5.1 version: ARM DDI 0487A.j)</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define TLBI_VMALLE1IS			sys_reg(1, 0, 8, 3, 0)</span>
<span class="p_add">+#define TLBI_VAE1IS			sys_reg(1, 0, 8, 3, 1)</span>
<span class="p_add">+#define TLBI_ASIDE1IS			sys_reg(1, 0, 8, 3, 2)</span>
<span class="p_add">+#define TLBI_VAAE1IS			sys_reg(1, 0, 8, 3, 3)</span>
<span class="p_add">+#define TLBI_VALE1IS			sys_reg(1, 0, 8, 3, 5)</span>
<span class="p_add">+#define TLBI_VAALE1IS			sys_reg(1, 0, 8, 3, 7)</span>
<span class="p_add">+#define TLBI_VMALLE1			sys_reg(1, 0, 8, 7, 0)</span>
<span class="p_add">+#define TLBI_VAE1			sys_reg(1, 0, 8, 7, 1)</span>
<span class="p_add">+#define TLBI_ASIDE1			sys_reg(1, 0, 8, 7, 2)</span>
<span class="p_add">+#define TLBI_VAAE1			sys_reg(1, 0, 8, 7, 3)</span>
<span class="p_add">+#define TLBI_VALE1			sys_reg(1, 0, 8, 7, 5)</span>
<span class="p_add">+#define TLBI_VAALE1			sys_reg(1, 0, 8, 7, 7)</span>
<span class="p_add">+</span>
<span class="p_add">+void __hyp_text</span>
<span class="p_add">+__kvm_emulate_tlb_invalidate(struct kvm *kvm, u32 sys_op, u64 regval)</span>
<span class="p_add">+{</span>
<span class="p_add">+	kvm = kern_hyp_va(kvm);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Switch to the guest before performing any TLB operations to</span>
<span class="p_add">+	 * target the appropriate VMID</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	__switch_to_guest_regime(kvm);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 *  TLB maintenance operations broadcast to inner-shareable</span>
<span class="p_add">+	 *  domain when HCR_FB is set (default for KVM).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	switch (sys_op) {</span>
<span class="p_add">+	case TLBIALL:</span>
<span class="p_add">+	case TLBIALLIS:</span>
<span class="p_add">+	case ITLBIALL:</span>
<span class="p_add">+	case DTLBIALL:</span>
<span class="p_add">+	case TLBI_VMALLE1:</span>
<span class="p_add">+	case TLBI_VMALLE1IS:</span>
<span class="p_add">+		__tlbi(vmalle1is);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case TLBIMVA:</span>
<span class="p_add">+	case TLBIMVAIS:</span>
<span class="p_add">+	case ITLBIMVA:</span>
<span class="p_add">+	case DTLBIMVA:</span>
<span class="p_add">+	case TLBI_VAE1:</span>
<span class="p_add">+	case TLBI_VAE1IS:</span>
<span class="p_add">+		__tlbi(vae1is, regval);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case TLBIASID:</span>
<span class="p_add">+	case TLBIASIDIS:</span>
<span class="p_add">+	case ITLBIASID:</span>
<span class="p_add">+	case DTLBIASID:</span>
<span class="p_add">+	case TLBI_ASIDE1:</span>
<span class="p_add">+	case TLBI_ASIDE1IS:</span>
<span class="p_add">+		__tlbi(aside1is, regval);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case TLBIMVAA:</span>
<span class="p_add">+	case TLBIMVAAIS:</span>
<span class="p_add">+	case TLBI_VAAE1:</span>
<span class="p_add">+	case TLBI_VAAE1IS:</span>
<span class="p_add">+		__tlbi(vaae1is, regval);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case TLBIMVAL:</span>
<span class="p_add">+	case TLBIMVALIS:</span>
<span class="p_add">+	case TLBI_VALE1:</span>
<span class="p_add">+	case TLBI_VALE1IS:</span>
<span class="p_add">+		__tlbi(vale1is, regval);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case TLBIMVAAL:</span>
<span class="p_add">+	case TLBIMVAALIS:</span>
<span class="p_add">+	case TLBI_VAALE1:</span>
<span class="p_add">+	case TLBI_VAALE1IS:</span>
<span class="p_add">+		__tlbi(vaale1is, regval);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	isb();</span>
<span class="p_add">+</span>
<span class="p_add">+	__switch_to_host_regime();</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/arm64/kvm/sys_regs.c b/arch/arm64/kvm/sys_regs.c</span>
<span class="p_header">index b0b225c..ca0b80f 100644</span>
<span class="p_header">--- a/arch/arm64/kvm/sys_regs.c</span>
<span class="p_header">+++ b/arch/arm64/kvm/sys_regs.c</span>
<span class="p_chunk">@@ -790,6 +790,18 @@</span> <span class="p_context"> static bool access_pmuserenr(struct kvm_vcpu *vcpu, struct sys_reg_params *p,</span>
 	return true;
 }
 
<span class="p_add">+static bool emulate_tlb_invalidate(struct kvm_vcpu *vcpu, struct sys_reg_params *p,</span>
<span class="p_add">+				  const struct sys_reg_desc *r)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 opcode = sys_reg(p-&gt;Op0, p-&gt;Op1, p-&gt;CRn, p-&gt;CRm, p-&gt;Op2);</span>
<span class="p_add">+</span>
<span class="p_add">+	kvm_call_hyp(__kvm_emulate_tlb_invalidate,</span>
<span class="p_add">+		     vcpu-&gt;kvm, opcode, p-&gt;regval);</span>
<span class="p_add">+	trace_kvm_tlb_invalidate(*vcpu_pc(vcpu), opcode);</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Silly macro to expand the DBG{BCR,BVR,WVR,WCR}n_EL1 registers in one go */
 #define DBG_BCR_BVR_WCR_WVR_EL1(n)					\
 	/* DBGBVRn_EL1 */						\
<span class="p_chunk">@@ -849,6 +861,35 @@</span> <span class="p_context"> static const struct sys_reg_desc sys_reg_descs[] = {</span>
 	{ Op0(0b01), Op1(0b000), CRn(0b0111), CRm(0b1110), Op2(0b010),
 	  access_dcsw },
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * ARMv8 ARM: Table C5-4 TLB maintenance instructions</span>
<span class="p_add">+	 * (Ref: ARMv8 ARM C5.1 version: ARM DDI 0487A.j)</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	/* TLBI VMALLE1IS */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(3), Op2(0), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI VAE1IS */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(3), Op2(1), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI ASIDE1IS */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(3), Op2(2), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI VAAE1IS */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(3), Op2(3), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI VALE1IS */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(3), Op2(5), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI VAALE1IS */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(3), Op2(7), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI VMALLE1 */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(7), Op2(0), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI VAE1 */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(7), Op2(1), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI ASIDE1 */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(7), Op2(2), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI VAAE1 */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(7), Op2(3), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI VALE1 */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(7), Op2(5), emulate_tlb_invalidate },</span>
<span class="p_add">+	/* TLBI VAALE1 */</span>
<span class="p_add">+	{ Op0(1), Op1(0), CRn(8), CRm(7), Op2(7), emulate_tlb_invalidate },</span>
<span class="p_add">+</span>
 	DBG_BCR_BVR_WCR_WVR_EL1(0),
 	DBG_BCR_BVR_WCR_WVR_EL1(1),
 	/* MDCCINT_EL1 */
<span class="p_chunk">@@ -1337,6 +1378,46 @@</span> <span class="p_context"> static const struct sys_reg_desc cp15_regs[] = {</span>
 	{ Op1( 0), CRn( 7), CRm(10), Op2( 2), access_dcsw },
 	{ Op1( 0), CRn( 7), CRm(14), Op2( 2), access_dcsw },
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * TLB operations</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	/* TLBIALLIS */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 3), Op2( 0), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIMVAIS */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 3), Op2( 1), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIASIDIS */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 3), Op2( 2), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIMVAAIS */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 3), Op2( 3), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIMVALIS */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 3), Op2( 5), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIMVAALIS */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 3), Op2( 7), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* ITLBIALL */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 5), Op2( 0), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* ITLBIMVA */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 5), Op2( 1), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* ITLBIASID */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 5), Op2( 2), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* DTLBIALL */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 6), Op2( 0), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* DTLBIMVA */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 6), Op2( 1), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* DTLBIASID */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 6), Op2( 2), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIALL */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 7), Op2( 0), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIMVA */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 7), Op2( 1), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIASID */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 7), Op2( 2), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIMVAA */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 7), Op2( 3), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIMVAL */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 7), Op2( 5), emulate_tlb_invalidate},</span>
<span class="p_add">+	/* TLBIMVAAL */</span>
<span class="p_add">+	{ Op1( 0), CRn( 8), CRm( 7), Op2( 7), emulate_tlb_invalidate},</span>
<span class="p_add">+</span>
 	/* PMU */
 	{ Op1( 0), CRn( 9), CRm(12), Op2( 0), access_pmcr },
 	{ Op1( 0), CRn( 9), CRm(12), Op2( 1), access_pmcnten },
<span class="p_header">diff --git a/arch/arm64/kvm/trace.h b/arch/arm64/kvm/trace.h</span>
<span class="p_header">index 7fb0008..c4d577f 100644</span>
<span class="p_header">--- a/arch/arm64/kvm/trace.h</span>
<span class="p_header">+++ b/arch/arm64/kvm/trace.h</span>
<span class="p_chunk">@@ -166,6 +166,22 @@</span> <span class="p_context"> TRACE_EVENT(kvm_set_guest_debug,</span>
 	TP_printk(&quot;vcpu: %p, flags: 0x%08x&quot;, __entry-&gt;vcpu, __entry-&gt;guest_debug)
 );
 
<span class="p_add">+TRACE_EVENT(kvm_tlb_invalidate,</span>
<span class="p_add">+	TP_PROTO(unsigned long vcpu_pc, u32 opcode),</span>
<span class="p_add">+	TP_ARGS(vcpu_pc, opcode),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_STRUCT__entry(</span>
<span class="p_add">+		__field(unsigned long, vcpu_pc)</span>
<span class="p_add">+		__field(u32, opcode)</span>
<span class="p_add">+	),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_fast_assign(</span>
<span class="p_add">+		__entry-&gt;vcpu_pc = vcpu_pc;</span>
<span class="p_add">+		__entry-&gt;opcode = opcode;</span>
<span class="p_add">+	),</span>
<span class="p_add">+</span>
<span class="p_add">+	TP_printk(&quot;vcpu_pc=0x%16lx opcode=%08x&quot;, __entry-&gt;vcpu_pc, __entry-&gt;opcode)</span>
<span class="p_add">+);</span>
 
 #endif /* _TRACE_ARM64_KVM_H */
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



