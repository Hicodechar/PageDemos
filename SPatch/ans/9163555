
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>arm64: mm: only initialize swiotlb when necessary - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    arm64: mm: only initialize swiotlb when necessary</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=72784">Jisheng Zhang</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 8, 2016, 7:53 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1465372426-4077-1-git-send-email-jszhang@marvell.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9163555/mbox/"
   >mbox</a>
|
   <a href="/patch/9163555/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9163555/">/patch/9163555/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	6A70960572 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  8 Jun 2016 07:58:19 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 53E6628384
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  8 Jun 2016 07:58:19 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 48CE92838F; Wed,  8 Jun 2016 07:58:19 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B801628384
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  8 Jun 2016 07:58:18 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753655AbcFHH6O (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 8 Jun 2016 03:58:14 -0400
Received: from mx0b-0016f401.pphosted.com ([67.231.156.173]:5453 &quot;EHLO
	mx0b-0016f401.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1750756AbcFHH6L (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 8 Jun 2016 03:58:11 -0400
Received: from pps.filterd (m0045851.ppops.net [127.0.0.1])
	by mx0b-0016f401.pphosted.com (8.16.0.17/8.16.0.17) with SMTP id
	u587uwaa000411; Wed, 8 Jun 2016 00:57:43 -0700
Received: from sc-exch02.marvell.com ([199.233.58.182])
	by mx0b-0016f401.pphosted.com with ESMTP id 23e39u25sk-1
	(version=TLSv1.2 cipher=ECDHE-RSA-AES256-SHA384 bits=256 verify=NOT); 
	Wed, 08 Jun 2016 00:57:43 -0700
Received: from SC-EXCH04.marvell.com (10.93.176.84) by SC-EXCH02.marvell.com
	(10.93.176.82) with Microsoft SMTP Server (TLS) id 15.0.1104.5;
	Wed, 8 Jun 2016 00:57:41 -0700
Received: from maili.marvell.com (10.93.176.43) by SC-EXCH04.marvell.com
	(10.93.176.84) with Microsoft SMTP Server id 15.0.1104.5 via Frontend
	Transport; Wed, 8 Jun 2016 00:57:41 -0700
Received: from xhacker.marvell.com (unknown [10.37.130.136])
	by maili.marvell.com (Postfix) with ESMTP id 06FE53F7041;
	Wed,  8 Jun 2016 00:57:40 -0700 (PDT)
From: Jisheng Zhang &lt;jszhang@marvell.com&gt;
To: &lt;catalin.marinas@arm.com&gt;, &lt;will.deacon@arm.com&gt;
CC: &lt;linux-arm-kernel@lists.infradead.org&gt;,
	&lt;linux-kernel@vger.kernel.org&gt;, Jisheng Zhang &lt;jszhang@marvell.com&gt;
Subject: [PATCH] arm64: mm: only initialize swiotlb when necessary
Date: Wed, 8 Jun 2016 15:53:46 +0800
Message-ID: &lt;1465372426-4077-1-git-send-email-jszhang@marvell.com&gt;
X-Mailer: git-send-email 2.8.1
MIME-Version: 1.0
Content-Type: text/plain
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2016-06-08_02:, , signatures=0
X-Proofpoint-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=2
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1604210000
	definitions=main-1606080094
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72784">Jisheng Zhang</a> - June 8, 2016, 7:53 a.m.</div>
<pre class="content">
we only initialize swiotlb when swiotlb_force is true or not all system
memory is DMA-able, this trivial optimization saves us 64MB when
swiotlb is not necessary.
<span class="signed-off-by">
Signed-off-by: Jisheng Zhang &lt;jszhang@marvell.com&gt;</span>
---
 arch/arm64/mm/dma-mapping.c | 15 ++++++++++++++-
 arch/arm64/mm/init.c        |  3 ++-
 2 files changed, 16 insertions(+), 2 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72784">Jisheng Zhang</a> - June 8, 2016, 8:01 a.m.</div>
<pre class="content">
Dear all,

On Wed, 8 Jun 2016 15:53:46 +0800 Jisheng Zhang wrote:
<span class="quote">
&gt; we only initialize swiotlb when swiotlb_force is true or not all system</span>
<span class="quote">&gt; memory is DMA-able, this trivial optimization saves us 64MB when</span>
<span class="quote">&gt; swiotlb is not necessary.</span>

another solution is to call swiotlb_free() as ppc does. Either solution can
solve my problem. If maintainers prefer that solution, I can send a v2 patch.

Thanks,
Jisheng
<span class="quote">
&gt; </span>
<span class="quote">&gt; Signed-off-by: Jisheng Zhang &lt;jszhang@marvell.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm64/mm/dma-mapping.c | 15 ++++++++++++++-</span>
<span class="quote">&gt;  arch/arm64/mm/init.c        |  3 ++-</span>
<span class="quote">&gt;  2 files changed, 16 insertions(+), 2 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; index c566ec8..46a4157 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -19,6 +19,7 @@</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;linux/gfp.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/acpi.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/bootmem.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/export.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/genalloc.h&gt;</span>
<span class="quote">&gt; @@ -29,6 +30,8 @@</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/cacheflush.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static int swiotlb __read_mostly;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static pgprot_t __get_dma_pgprot(struct dma_attrs *attrs, pgprot_t prot,</span>
<span class="quote">&gt;  				 bool coherent)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -341,6 +344,13 @@ static int __swiotlb_get_sgtable(struct device *dev, struct sg_table *sgt,</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static int __swiotlb_dma_supported(struct device *hwdev, u64 mask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (swiotlb)</span>
<span class="quote">&gt; +		return swiotlb_dma_supported(hwdev, mask);</span>
<span class="quote">&gt; +	return 1;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static struct dma_map_ops swiotlb_dma_ops = {</span>
<span class="quote">&gt;  	.alloc = __dma_alloc,</span>
<span class="quote">&gt;  	.free = __dma_free,</span>
<span class="quote">&gt; @@ -354,7 +364,7 @@ static struct dma_map_ops swiotlb_dma_ops = {</span>
<span class="quote">&gt;  	.sync_single_for_device = __swiotlb_sync_single_for_device,</span>
<span class="quote">&gt;  	.sync_sg_for_cpu = __swiotlb_sync_sg_for_cpu,</span>
<span class="quote">&gt;  	.sync_sg_for_device = __swiotlb_sync_sg_for_device,</span>
<span class="quote">&gt; -	.dma_supported = swiotlb_dma_supported,</span>
<span class="quote">&gt; +	.dma_supported = __swiotlb_dma_supported,</span>
<span class="quote">&gt;  	.mapping_error = swiotlb_dma_mapping_error,</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -513,6 +523,9 @@ EXPORT_SYMBOL(dummy_dma_ops);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int __init arm64_dma_init(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	if (swiotlb_force || max_pfn &gt; (arm64_dma_phys_limit &gt;&gt; PAGE_SHIFT))</span>
<span class="quote">&gt; +		swiotlb = 1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	return atomic_pool_init();</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  arch_initcall(arm64_dma_init);</span>
<span class="quote">&gt; diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c</span>
<span class="quote">&gt; index d45f862..7d25b4d 100644</span>
<span class="quote">&gt; --- a/arch/arm64/mm/init.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/init.c</span>
<span class="quote">&gt; @@ -403,7 +403,8 @@ static void __init free_unused_memmap(void)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  void __init mem_init(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	swiotlb_init(1);</span>
<span class="quote">&gt; +	if (swiotlb_force || max_pfn &gt; (arm64_dma_phys_limit &gt;&gt; PAGE_SHIFT))</span>
<span class="quote">&gt; +		swiotlb_init(1);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	set_max_mapnr(pfn_to_page(max_pfn) - mem_map);</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - June 8, 2016, 12:08 p.m.</div>
<pre class="content">
On Wed, Jun 08, 2016 at 03:53:46PM +0800, Jisheng Zhang wrote:
<span class="quote">&gt;  static int __init arm64_dma_init(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; +	if (swiotlb_force || max_pfn &gt; (arm64_dma_phys_limit &gt;&gt; PAGE_SHIFT))</span>
<span class="quote">&gt; +		swiotlb = 1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	return atomic_pool_init();</span>
<span class="quote">&gt;  }</span>

So any platform with RAM larger than 4GB would still initialise swiotlb.
I wouldn&#39;t say it&#39;s an issue, 64MB is not a significant loss on such
systems.

An alternative would be to defer the freeing until we are aware of all
possible dma masks for the populated devices (e.g. from DT), though I&#39;m
not sure that&#39;s enough, drivers may try to change such masks when
probed.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - June 8, 2016, 12:13 p.m.</div>
<pre class="content">
On Wed, Jun 08, 2016 at 03:53:46PM +0800, Jisheng Zhang wrote:
<span class="quote">&gt; --- a/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; +++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; @@ -19,6 +19,7 @@</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;linux/gfp.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/acpi.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/bootmem.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/export.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/genalloc.h&gt;</span>

Why is this include needed?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - June 8, 2016, 3:49 p.m.</div>
<pre class="content">
On Wednesday, June 8, 2016 1:08:29 PM CEST Catalin Marinas wrote:
<span class="quote">&gt; On Wed, Jun 08, 2016 at 03:53:46PM +0800, Jisheng Zhang wrote:</span>
<span class="quote">&gt; &gt;  static int __init arm64_dma_init(void)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +     if (swiotlb_force || max_pfn &gt; (arm64_dma_phys_limit &gt;&gt; PAGE_SHIFT))</span>
<span class="quote">&gt; &gt; +             swiotlb = 1;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;       return atomic_pool_init();</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So any platform with RAM larger than 4GB would still initialise swiotlb.</span>
<span class="quote">&gt; I wouldn&#39;t say it&#39;s an issue, 64MB is not a significant loss on such</span>
<span class="quote">&gt; systems.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; An alternative would be to defer the freeing until we are aware of all</span>
<span class="quote">&gt; possible dma masks for the populated devices (e.g. from DT), though I&#39;m</span>
<span class="quote">&gt; not sure that&#39;s enough, drivers may try to change such masks when</span>
<span class="quote">&gt; probed.</span>

Right, this is a hard problem, because you can in theory have odd devices
that require a DMA mask lower than the limit of ZONE_DMA.

In the kernel sources, I find these:

# git grep DMA_BIT_MASK | grep -v &#39;DMA_BIT_MASK(\(3[2-9]\|[456][0-9]\))&#39;
arch/arm/mach-ixp4xx/common.c:		dev-&gt;coherent_dma_mask = DMA_BIT_MASK(28); /* 64 MB */
drivers/base/isa.c:		isa_dev-&gt;dev.coherent_dma_mask = DMA_BIT_MASK(24);
drivers/media/pci/sta2x11/sta2x11_vip.c:	err = dma_set_coherent_mask(&amp;vip-&gt;pdev-&gt;dev, DMA_BIT_MASK(29));
drivers/media/pci/sta2x11/sta2x11_vip.c:	if (dma_set_mask(&amp;pdev-&gt;dev, DMA_BIT_MASK(26))) {
drivers/net/ethernet/broadcom/b44.c:		mapping + RX_PKT_BUF_SZ &gt; DMA_BIT_MASK(30)) {
drivers/net/ethernet/broadcom/b44.c:		    mapping + RX_PKT_BUF_SZ &gt; DMA_BIT_MASK(30)) {
drivers/net/ethernet/broadcom/b44.c:	if (dma_mapping_error(bp-&gt;sdev-&gt;dma_dev, mapping) || mapping + len &gt; DMA_BIT_MASK(30)) {
drivers/net/ethernet/broadcom/b44.c:		if (dma_mapping_error(bp-&gt;sdev-&gt;dma_dev, mapping) || mapping + len &gt; DMA_BIT_MASK(30)) {
drivers/net/ethernet/broadcom/b44.c:			rx_ring_dma + size &gt; DMA_BIT_MASK(30)) {
drivers/net/ethernet/broadcom/b44.c:			tx_ring_dma + size &gt; DMA_BIT_MASK(30)) {
drivers/net/ethernet/broadcom/b44.c:	if (dma_set_mask_and_coherent(sdev-&gt;dma_dev, DMA_BIT_MASK(30))) {
drivers/net/wan/wanxl.c:	if (pci_set_consistent_dma_mask(pdev, DMA_BIT_MASK(28)) ||
drivers/net/wan/wanxl.c:	    pci_set_dma_mask(pdev, DMA_BIT_MASK(28))) {
drivers/net/wireless/broadcom/b43/dma.c:	return DMA_BIT_MASK(30);
drivers/net/wireless/broadcom/b43/dma.c:	if (dmamask == DMA_BIT_MASK(30))
drivers/net/wireless/broadcom/b43/dma.c:			mask = DMA_BIT_MASK(30);
drivers/net/wireless/broadcom/b43legacy/dma.c:	return DMA_BIT_MASK(30);
drivers/net/wireless/broadcom/b43legacy/dma.c:	if (dmamask == DMA_BIT_MASK(30))
drivers/net/wireless/broadcom/b43legacy/dma.c:			mask = DMA_BIT_MASK(30);
drivers/parport/parport_pc.c:		ret = dma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(24));
drivers/pnp/card.c:	card-&gt;dev.coherent_dma_mask = DMA_BIT_MASK(24);
drivers/pnp/core.c:	dev-&gt;dma_mask = DMA_BIT_MASK(24);
drivers/scsi/aacraid/commsup.c:		if (((retval = pci_set_dma_mask(aac-&gt;pdev, DMA_BIT_MASK(31)))) ||
drivers/scsi/aacraid/commsup.c:		  ((retval = pci_set_consistent_dma_mask(aac-&gt;pdev, DMA_BIT_MASK(31)))))
drivers/scsi/aacraid/linit.c:		dmamask = DMA_BIT_MASK(31);
drivers/usb/host/ehci-pci.c:						DMA_BIT_MASK(31)) &lt; 0)
include/linux/blkdev.h:#define BLK_BOUNCE_ISA		(DMA_BIT_MASK(24))
sound/pci/ali5451/ali5451.c:	if (dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(31)) &lt; 0 ||
sound/pci/als300.c:	if (dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(28)) &lt; 0 ||
sound/pci/als4000.c:	if (dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(24)) &lt; 0 ||
sound/pci/azt3328.c:	if (dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(24)) &lt; 0 ||
sound/pci/emu10k1/emu10k1x.c:	if (pci_set_dma_mask(pci, DMA_BIT_MASK(28)) &lt; 0 ||
sound/pci/es1938.c:	if (dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(24)) &lt; 0 ||
sound/pci/es1968.c:	if (dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(28)) &lt; 0 ||
sound/pci/ice1712/ice1712.c:	if (dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(28)) &lt; 0 ||
sound/pci/maestro3.c:	if (dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(28)) &lt; 0 ||
sound/pci/sis7019.c:	rc = dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(30));
sound/pci/sonicvibes.c:	if (dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(24)) &lt; 0 ||
sound/pci/trident/trident_main.c:	if (dma_set_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(30)) &lt; 0 ||
sound/pci/trident/trident_main.c:	    dma_set_coherent_mask(&amp;pci-&gt;dev, DMA_BIT_MASK(30)) &lt; 0) {
sound/soc/intel/common/sst-firmware.c:	err = dma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(31));
sound/soc/intel/haswell/sst-haswell-dsp.c:	ret = dma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(31));

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72784">Jisheng Zhang</a> - June 12, 2016, 7:14 a.m.</div>
<pre class="content">
Dear Catalin,

On Wed, 8 Jun 2016 13:13:56 +0100 Catalin Marinas wrote:
<span class="quote">
&gt; On Wed, Jun 08, 2016 at 03:53:46PM +0800, Jisheng Zhang wrote:</span>
<span class="quote">&gt; &gt; --- a/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; &gt; +++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="quote">&gt; &gt; @@ -19,6 +19,7 @@</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  #include &lt;linux/gfp.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/acpi.h&gt;</span>
<span class="quote">&gt; &gt; +#include &lt;linux/bootmem.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/export.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/slab.h&gt;</span>
<span class="quote">&gt; &gt;  #include &lt;linux/genalloc.h&gt;  </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why is this include needed?</span>
<span class="quote">&gt; </span>

This is for max_pfn

Thanks,
Jisheng
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - June 21, 2016, 4:06 p.m.</div>
<pre class="content">
On Wed, Jun 08, 2016 at 05:49:59PM +0200, Arnd Bergmann wrote:
<span class="quote">&gt; On Wednesday, June 8, 2016 1:08:29 PM CEST Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; On Wed, Jun 08, 2016 at 03:53:46PM +0800, Jisheng Zhang wrote:</span>
<span class="quote">&gt; &gt; &gt;  static int __init arm64_dma_init(void)</span>
<span class="quote">&gt; &gt; &gt;  {</span>
<span class="quote">&gt; &gt; &gt; +     if (swiotlb_force || max_pfn &gt; (arm64_dma_phys_limit &gt;&gt; PAGE_SHIFT))</span>
<span class="quote">&gt; &gt; &gt; +             swiotlb = 1;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt;       return atomic_pool_init();</span>
<span class="quote">&gt; &gt; &gt;  }</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; So any platform with RAM larger than 4GB would still initialise swiotlb.</span>
<span class="quote">&gt; &gt; I wouldn&#39;t say it&#39;s an issue, 64MB is not a significant loss on such</span>
<span class="quote">&gt; &gt; systems.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; An alternative would be to defer the freeing until we are aware of all</span>
<span class="quote">&gt; &gt; possible dma masks for the populated devices (e.g. from DT), though I&#39;m</span>
<span class="quote">&gt; &gt; not sure that&#39;s enough, drivers may try to change such masks when</span>
<span class="quote">&gt; &gt; probed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Right, this is a hard problem, because you can in theory have odd devices</span>
<span class="quote">&gt; that require a DMA mask lower than the limit of ZONE_DMA.</span>

I&#39;m not sure what we can do about such devices even with swiotlb. The
bounce buffer is allocated from ZONE_DMA and currently it assumes a
32-bit mask from the start of RAM, so it is not guaranteed to support a
smaller mask. We may need to come up with some DT memory node attribute
to define the minimum DMA-able memory and restrict ZONE_DMA during early
boot but I would rather wait until we hit a real issue in practice.

Anyway, I plan to merge this patch as is, we can improve the logic in
the future if we have a better idea.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - June 21, 2016, 8:13 p.m.</div>
<pre class="content">
On Tuesday, June 21, 2016 5:06:25 PM CEST Catalin Marinas wrote:
<span class="quote">&gt; On Wed, Jun 08, 2016 at 05:49:59PM +0200, Arnd Bergmann wrote:</span>
<span class="quote">&gt; &gt; On Wednesday, June 8, 2016 1:08:29 PM CEST Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; &gt; On Wed, Jun 08, 2016 at 03:53:46PM +0800, Jisheng Zhang wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt;  static int __init arm64_dma_init(void)</span>
<span class="quote">&gt; &gt; &gt; &gt;  {</span>
<span class="quote">&gt; &gt; &gt; &gt; +     if (swiotlb_force || max_pfn &gt; (arm64_dma_phys_limit &gt;&gt; PAGE_SHIFT))</span>
<span class="quote">&gt; &gt; &gt; &gt; +             swiotlb = 1;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt;       return atomic_pool_init();</span>
<span class="quote">&gt; &gt; &gt; &gt;  }</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; So any platform with RAM larger than 4GB would still initialise swiotlb.</span>
<span class="quote">&gt; &gt; &gt; I wouldn&#39;t say it&#39;s an issue, 64MB is not a significant loss on such</span>
<span class="quote">&gt; &gt; &gt; systems.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; An alternative would be to defer the freeing until we are aware of all</span>
<span class="quote">&gt; &gt; &gt; possible dma masks for the populated devices (e.g. from DT), though I&#39;m</span>
<span class="quote">&gt; &gt; &gt; not sure that&#39;s enough, drivers may try to change such masks when</span>
<span class="quote">&gt; &gt; &gt; probed.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Right, this is a hard problem, because you can in theory have odd devices</span>
<span class="quote">&gt; &gt; that require a DMA mask lower than the limit of ZONE_DMA.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not sure what we can do about such devices even with swiotlb. The</span>
<span class="quote">&gt; bounce buffer is allocated from ZONE_DMA and currently it assumes a</span>
<span class="quote">&gt; 32-bit mask from the start of RAM, so it is not guaranteed to support a</span>
<span class="quote">&gt; smaller mask. We may need to come up with some DT memory node attribute</span>
<span class="quote">&gt; to define the minimum DMA-able memory and restrict ZONE_DMA during early</span>
<span class="quote">&gt; boot but I would rather wait until we hit a real issue in practice.</span>

The bounce buffer is allocated at early boot time in order to get an address
as low as possible, in the hope that it is small enough for those cases.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=352">Catalin Marinas</a> - June 22, 2016, 4:56 p.m.</div>
<pre class="content">
On Tue, Jun 21, 2016 at 10:13:45PM +0200, Arnd Bergmann wrote:
<span class="quote">&gt; On Tuesday, June 21, 2016 5:06:25 PM CEST Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; On Wed, Jun 08, 2016 at 05:49:59PM +0200, Arnd Bergmann wrote:</span>
<span class="quote">&gt; &gt; &gt; On Wednesday, June 8, 2016 1:08:29 PM CEST Catalin Marinas wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Wed, Jun 08, 2016 at 03:53:46PM +0800, Jisheng Zhang wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  static int __init arm64_dma_init(void)</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +     if (swiotlb_force || max_pfn &gt; (arm64_dma_phys_limit &gt;&gt; PAGE_SHIFT))</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +             swiotlb = 1;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;       return atomic_pool_init();</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  }</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; So any platform with RAM larger than 4GB would still initialise swiotlb.</span>
<span class="quote">&gt; &gt; &gt; &gt; I wouldn&#39;t say it&#39;s an issue, 64MB is not a significant loss on such</span>
<span class="quote">&gt; &gt; &gt; &gt; systems.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; An alternative would be to defer the freeing until we are aware of all</span>
<span class="quote">&gt; &gt; &gt; &gt; possible dma masks for the populated devices (e.g. from DT), though I&#39;m</span>
<span class="quote">&gt; &gt; &gt; &gt; not sure that&#39;s enough, drivers may try to change such masks when</span>
<span class="quote">&gt; &gt; &gt; &gt; probed.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Right, this is a hard problem, because you can in theory have odd devices</span>
<span class="quote">&gt; &gt; &gt; that require a DMA mask lower than the limit of ZONE_DMA.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;m not sure what we can do about such devices even with swiotlb. The</span>
<span class="quote">&gt; &gt; bounce buffer is allocated from ZONE_DMA and currently it assumes a</span>
<span class="quote">&gt; &gt; 32-bit mask from the start of RAM, so it is not guaranteed to support a</span>
<span class="quote">&gt; &gt; smaller mask. We may need to come up with some DT memory node attribute</span>
<span class="quote">&gt; &gt; to define the minimum DMA-able memory and restrict ZONE_DMA during early</span>
<span class="quote">&gt; &gt; boot but I would rather wait until we hit a real issue in practice.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The bounce buffer is allocated at early boot time in order to get an address</span>
<span class="quote">&gt; as low as possible, in the hope that it is small enough for those cases.</span>

The swiotlb allocation is triggered from mem_init() and that&#39;s well
after the memblock has been initialised. The swiotlb_init() function
uses memblock_virt_alloc_low_nopanic() which is a top-down allocator
with an upper bound of ARCH_LOW_ADDRESS_LIMIT. The latter is set by the
arm64 code to the top 32-bit of RAM (plus some offset), so it&#39;s likely
that the swiotlb bounce buffer will be placed in the upper range of the
32-bit mask.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">index c566ec8..46a4157 100644</span>
<span class="p_header">--- a/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -19,6 +19,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/gfp.h&gt;
 #include &lt;linux/acpi.h&gt;
<span class="p_add">+#include &lt;linux/bootmem.h&gt;</span>
 #include &lt;linux/export.h&gt;
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/genalloc.h&gt;
<span class="p_chunk">@@ -29,6 +30,8 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/cacheflush.h&gt;
 
<span class="p_add">+static int swiotlb __read_mostly;</span>
<span class="p_add">+</span>
 static pgprot_t __get_dma_pgprot(struct dma_attrs *attrs, pgprot_t prot,
 				 bool coherent)
 {
<span class="p_chunk">@@ -341,6 +344,13 @@</span> <span class="p_context"> static int __swiotlb_get_sgtable(struct device *dev, struct sg_table *sgt,</span>
 	return ret;
 }
 
<span class="p_add">+static int __swiotlb_dma_supported(struct device *hwdev, u64 mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (swiotlb)</span>
<span class="p_add">+		return swiotlb_dma_supported(hwdev, mask);</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static struct dma_map_ops swiotlb_dma_ops = {
 	.alloc = __dma_alloc,
 	.free = __dma_free,
<span class="p_chunk">@@ -354,7 +364,7 @@</span> <span class="p_context"> static struct dma_map_ops swiotlb_dma_ops = {</span>
 	.sync_single_for_device = __swiotlb_sync_single_for_device,
 	.sync_sg_for_cpu = __swiotlb_sync_sg_for_cpu,
 	.sync_sg_for_device = __swiotlb_sync_sg_for_device,
<span class="p_del">-	.dma_supported = swiotlb_dma_supported,</span>
<span class="p_add">+	.dma_supported = __swiotlb_dma_supported,</span>
 	.mapping_error = swiotlb_dma_mapping_error,
 };
 
<span class="p_chunk">@@ -513,6 +523,9 @@</span> <span class="p_context"> EXPORT_SYMBOL(dummy_dma_ops);</span>
 
 static int __init arm64_dma_init(void)
 {
<span class="p_add">+	if (swiotlb_force || max_pfn &gt; (arm64_dma_phys_limit &gt;&gt; PAGE_SHIFT))</span>
<span class="p_add">+		swiotlb = 1;</span>
<span class="p_add">+</span>
 	return atomic_pool_init();
 }
 arch_initcall(arm64_dma_init);
<span class="p_header">diff --git a/arch/arm64/mm/init.c b/arch/arm64/mm/init.c</span>
<span class="p_header">index d45f862..7d25b4d 100644</span>
<span class="p_header">--- a/arch/arm64/mm/init.c</span>
<span class="p_header">+++ b/arch/arm64/mm/init.c</span>
<span class="p_chunk">@@ -403,7 +403,8 @@</span> <span class="p_context"> static void __init free_unused_memmap(void)</span>
  */
 void __init mem_init(void)
 {
<span class="p_del">-	swiotlb_init(1);</span>
<span class="p_add">+	if (swiotlb_force || max_pfn &gt; (arm64_dma_phys_limit &gt;&gt; PAGE_SHIFT))</span>
<span class="p_add">+		swiotlb_init(1);</span>
 
 	set_max_mapnr(pfn_to_page(max_pfn) - mem_map);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



