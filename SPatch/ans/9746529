
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,02/15] mm/hmm: heterogeneous memory management (HMM for short) v4 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,02/15] mm/hmm: heterogeneous memory management (HMM for short) v4</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 24, 2017, 5:20 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170524172024.30810-3-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9746529/mbox/"
   >mbox</a>
|
   <a href="/patch/9746529/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9746529/">/patch/9746529/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	66271601C2 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 May 2017 17:20:53 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 48502289A1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 May 2017 17:20:53 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 3BF7D289C6; Wed, 24 May 2017 17:20:53 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 644D2289A1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 May 2017 17:20:51 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752868AbdEXRUi (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 24 May 2017 13:20:38 -0400
Received: from mx1.redhat.com ([209.132.183.28]:38340 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752721AbdEXRUb (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 24 May 2017 13:20:31 -0400
Received: from smtp.corp.redhat.com
	(int-mx05.intmail.prod.int.phx2.redhat.com [10.5.11.15])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 7AE8112BA3;
	Wed, 24 May 2017 17:20:30 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com 7AE8112BA3
Authentication-Results: ext-mx06.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx06.extmail.prod.ext.phx2.redhat.com;
	spf=pass smtp.mailfrom=jglisse@redhat.com
DKIM-Filter: OpenDKIM Filter v2.11.0 mx1.redhat.com 7AE8112BA3
Received: from localhost.localdomain.com (ovpn-122-80.rdu2.redhat.com
	[10.10.122.80])
	by smtp.corp.redhat.com (Postfix) with ESMTP id 018E817140;
	Wed, 24 May 2017 17:20:28 +0000 (UTC)
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, linux-kernel@vger.kernel.org,
	linux-mm@kvack.org
Cc: Dan Williams &lt;dan.j.williams@intel.com&gt;,
	&quot;Kirill A . Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	John Hubbard &lt;jhubbard@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;
Subject: [HMM 02/15] mm/hmm: heterogeneous memory management (HMM for short)
	v4
Date: Wed, 24 May 2017 13:20:11 -0400
Message-Id: &lt;20170524172024.30810-3-jglisse@redhat.com&gt;
In-Reply-To: &lt;20170524172024.30810-1-jglisse@redhat.com&gt;
References: &lt;20170524172024.30810-1-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.15
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.30]);
	Wed, 24 May 2017 17:20:31 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - May 24, 2017, 5:20 p.m.</div>
<pre class="content">
HMM provides 3 separate types of functionality:
    - Mirroring: synchronize CPU page table and device page table
    - Device memory: allocating struct page for device memory
    - Migration: migrating regular memory to device memory

This patch introduces some common helpers and definitions to all of
those 3 functionality.

Changed since v3:
  - Unconditionaly build hmm.c for static keys
Changed since v2:
  - s/device unaddressable/device private
Changed since v1:
  - Kconfig logic (depend on x86-64 and use ARCH_HAS pattern)
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
---
 include/linux/hmm.h      | 146 +++++++++++++++++++++++++++++++++++++++++++++++
 include/linux/mm_types.h |   5 ++
 kernel/fork.c            |   2 +
 mm/Kconfig               |  13 +++++
 mm/Makefile              |   2 +-
 mm/hmm.c                 |  74 ++++++++++++++++++++++++
 6 files changed, 241 insertions(+), 1 deletion(-)
 create mode 100644 include/linux/hmm.h
 create mode 100644 mm/hmm.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - May 31, 2017, 2:10 a.m.</div>
<pre class="content">
On Wed, 24 May 2017 13:20:11 -0400
Jérôme Glisse &lt;jglisse@redhat.com&gt; wrote:
<span class="quote">
&gt; HMM provides 3 separate types of functionality:</span>
<span class="quote">&gt;     - Mirroring: synchronize CPU page table and device page table</span>
<span class="quote">&gt;     - Device memory: allocating struct page for device memory</span>
<span class="quote">&gt;     - Migration: migrating regular memory to device memory</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch introduces some common helpers and definitions to all of</span>
<span class="quote">&gt; those 3 functionality.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Changed since v3:</span>
<span class="quote">&gt;   - Unconditionaly build hmm.c for static keys</span>
<span class="quote">&gt; Changed since v2:</span>
<span class="quote">&gt;   - s/device unaddressable/device private</span>
<span class="quote">&gt; Changed since v1:</span>
<span class="quote">&gt;   - Kconfig logic (depend on x86-64 and use ARCH_HAS pattern)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
<span class="quote">&gt; ---</span>

It would be nice to explain a bit of how hmm_pfn_t bits work with pfn
and find out what we need from an arch to support HMM.


Balbir Singh.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - June 1, 2017, 10:35 p.m.</div>
<pre class="content">
On Wed, May 31, 2017 at 12:10:24PM +1000, Balbir Singh wrote:
<span class="quote">&gt; On Wed, 24 May 2017 13:20:11 -0400</span>
<span class="quote">&gt; Jérôme Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; HMM provides 3 separate types of functionality:</span>
<span class="quote">&gt; &gt;     - Mirroring: synchronize CPU page table and device page table</span>
<span class="quote">&gt; &gt;     - Device memory: allocating struct page for device memory</span>
<span class="quote">&gt; &gt;     - Migration: migrating regular memory to device memory</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This patch introduces some common helpers and definitions to all of</span>
<span class="quote">&gt; &gt; those 3 functionality.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Changed since v3:</span>
<span class="quote">&gt; &gt;   - Unconditionaly build hmm.c for static keys</span>
<span class="quote">&gt; &gt; Changed since v2:</span>
<span class="quote">&gt; &gt;   - s/device unaddressable/device private</span>
<span class="quote">&gt; &gt; Changed since v1:</span>
<span class="quote">&gt; &gt;   - Kconfig logic (depend on x86-64 and use ARCH_HAS pattern)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It would be nice to explain a bit of how hmm_pfn_t bits work with pfn</span>
<span class="quote">&gt; and find out what we need from an arch to support HMM.</span>
<span class="quote">&gt; </span>

This is only needed for HMM_MIRROR feature so you do not care about it
for powerpc

Jérôme
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
new file mode 100644
<span class="p_header">index 0000000..e24c7a7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/hmm.h</span>
<span class="p_chunk">@@ -0,0 +1,146 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright 2013 Red Hat Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Heterogeneous Memory Management (HMM)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * See Documentation/vm/hmm.txt for reasons and overview of what HMM is and it</span>
<span class="p_add">+ * is for. Here we focus on the HMM API description, with some explanation of</span>
<span class="p_add">+ * the underlying implementation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Short description: HMM provides a set of helpers to share a virtual address</span>
<span class="p_add">+ * space between CPU and a device, so that the device can access any valid</span>
<span class="p_add">+ * address of the process (while still obeying memory protection). HMM also</span>
<span class="p_add">+ * provides helpers to migrate process memory to device memory, and back. Each</span>
<span class="p_add">+ * set of functionality (address space mirroring, and migration to and from</span>
<span class="p_add">+ * device memory) can be used independently of the other.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM address space mirroring API:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Use HMM address space mirroring if you want to mirror range of the CPU page</span>
<span class="p_add">+ * table of a process into a device page table. Here, &quot;mirror&quot; means &quot;keep</span>
<span class="p_add">+ * synchronized&quot;. Prerequisites: the device must provide the ability to write-</span>
<span class="p_add">+ * protect its page tables (at PAGE_SIZE granularity), and must be able to</span>
<span class="p_add">+ * recover from the resulting potential page faults.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM guarantees that at any point in time, a given virtual address points to</span>
<span class="p_add">+ * either the same memory in both CPU and device page tables (that is: CPU and</span>
<span class="p_add">+ * device page tables each point to the same pages), or that one page table (CPU</span>
<span class="p_add">+ * or device) points to no entry, while the other still points to the old page</span>
<span class="p_add">+ * for the address. The latter case happens when the CPU page table update</span>
<span class="p_add">+ * happens first, and then the update is mirrored over to the device page table.</span>
<span class="p_add">+ * This does not cause any issue, because the CPU page table cannot start</span>
<span class="p_add">+ * pointing to a new page until the device page table is invalidated.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM uses mmu_notifiers to monitor the CPU page tables, and forwards any</span>
<span class="p_add">+ * updates to each device driver that has registered a mirror. It also provides</span>
<span class="p_add">+ * some API calls to help with taking a snapshot of the CPU page table, and to</span>
<span class="p_add">+ * synchronize with any updates that might happen concurrently.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM migration to and from device memory:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM provides a set of helpers to hotplug device memory as ZONE_DEVICE, with</span>
<span class="p_add">+ * a new MEMORY_DEVICE_PRIVATE type. This provides a struct page for each page</span>
<span class="p_add">+ * of the device memory, and allows the device driver to manage its memory</span>
<span class="p_add">+ * using those struct pages. Having struct pages for device memory makes</span>
<span class="p_add">+ * migration easier. Because that memory is not addressable by the CPU it must</span>
<span class="p_add">+ * never be pinned to the device; in other words, any CPU page fault can always</span>
<span class="p_add">+ * cause the device memory to be migrated (copied/moved) back to regular memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * A new migrate helper (migrate_vma()) has been added (see mm/migrate.c) that</span>
<span class="p_add">+ * allows use of a device DMA engine to perform the copy operation between</span>
<span class="p_add">+ * regular system memory and device memory.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef LINUX_HMM_H</span>
<span class="p_add">+#define LINUX_HMM_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kconfig.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#if IS_ENABLED(CONFIG_HMM)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_t - HMM uses its own pfn type to keep several flags per page</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Flags:</span>
<span class="p_add">+ * HMM_PFN_VALID: pfn is valid</span>
<span class="p_add">+ * HMM_PFN_WRITE: CPU page table has write permission set</span>
<span class="p_add">+ */</span>
<span class="p_add">+typedef unsigned long hmm_pfn_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define HMM_PFN_VALID (1 &lt;&lt; 0)</span>
<span class="p_add">+#define HMM_PFN_WRITE (1 &lt;&lt; 1)</span>
<span class="p_add">+#define HMM_PFN_SHIFT 2</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_t_to_page() - return struct page pointed to by a valid hmm_pfn_t</span>
<span class="p_add">+ * @pfn: hmm_pfn_t to convert to struct page</span>
<span class="p_add">+ * Returns: struct page pointer if pfn is a valid hmm_pfn_t, NULL otherwise</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If the hmm_pfn_t is valid (ie valid flag set) then return the struct page</span>
<span class="p_add">+ * matching the pfn value stored in the hmm_pfn_t. Otherwise return NULL.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline struct page *hmm_pfn_t_to_page(hmm_pfn_t pfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	return pfn_to_page(pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_t_to_pfn() - return pfn value store in a hmm_pfn_t</span>
<span class="p_add">+ * @pfn: hmm_pfn_t to extract pfn from</span>
<span class="p_add">+ * Returns: pfn value if hmm_pfn_t is valid, -1UL otherwise</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline unsigned long hmm_pfn_t_to_pfn(hmm_pfn_t pfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="p_add">+		return -1UL;</span>
<span class="p_add">+	return (pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_t_from_page() - create a valid hmm_pfn_t value from struct page</span>
<span class="p_add">+ * @page: struct page pointer for which to create the hmm_pfn_t</span>
<span class="p_add">+ * Returns: valid hmm_pfn_t for the page</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline hmm_pfn_t hmm_pfn_t_from_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (page_to_pfn(page) &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_t_from_pfn() - create a valid hmm_pfn_t value from pfn</span>
<span class="p_add">+ * @pfn: pfn value for which to create the hmm_pfn_t</span>
<span class="p_add">+ * Returns: valid hmm_pfn_t for the pfn</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline hmm_pfn_t hmm_pfn_t_from_pfn(unsigned long pfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pfn &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Below are for HMM internal use only! Not to be used by device driver! */</span>
<span class="p_add">+void hmm_mm_destroy(struct mm_struct *mm);</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Below are for HMM internal use only! Not to be used by device driver! */</span>
<span class="p_add">+static inline void hmm_mm_destroy(struct mm_struct *mm) {}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="p_add">+#endif /* LINUX_HMM_H */</span>
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index 45cdb27..5ddeacc7 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -23,6 +23,7 @@</span> <span class="p_context"></span>
 
 struct address_space;
 struct mem_cgroup;
<span class="p_add">+struct hmm;</span>
 
 /*
  * Each physical page in the system has a struct page associated with
<span class="p_chunk">@@ -500,6 +501,10 @@</span> <span class="p_context"> struct mm_struct {</span>
 	atomic_long_t hugetlb_usage;
 #endif
 	struct work_struct async_put_work;
<span class="p_add">+#if IS_ENABLED(CONFIG_HMM)</span>
<span class="p_add">+	/* HMM needs to track a few things per mm */</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+#endif</span>
 };
 
 extern struct mm_struct init_mm;
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 6210bb2..9843b16 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -37,6 +37,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/binfmts.h&gt;
 #include &lt;linux/mman.h&gt;
 #include &lt;linux/mmu_notifier.h&gt;
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
 #include &lt;linux/fs.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/vmacache.h&gt;
<span class="p_chunk">@@ -885,6 +886,7 @@</span> <span class="p_context"> void __mmdrop(struct mm_struct *mm)</span>
 	BUG_ON(mm == &amp;init_mm);
 	mm_free_pgd(mm);
 	destroy_context(mm);
<span class="p_add">+	hmm_mm_destroy(mm);</span>
 	mmu_notifier_mm_destroy(mm);
 	check_mm(mm);
 	put_user_ns(mm-&gt;user_ns);
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index b2356fe..8059ad6 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -289,6 +289,19 @@</span> <span class="p_context"> config MIGRATION</span>
 config ARCH_ENABLE_HUGEPAGE_MIGRATION
 	bool
 
<span class="p_add">+config ARCH_HAS_HMM</span>
<span class="p_add">+	bool</span>
<span class="p_add">+	default y</span>
<span class="p_add">+	depends on X86_64</span>
<span class="p_add">+	depends on ZONE_DEVICE</span>
<span class="p_add">+	depends on MMU &amp;&amp; 64BIT</span>
<span class="p_add">+	depends on MEMORY_HOTPLUG</span>
<span class="p_add">+	depends on MEMORY_HOTREMOVE</span>
<span class="p_add">+	depends on SPARSEMEM_VMEMMAP</span>
<span class="p_add">+</span>
<span class="p_add">+config HMM</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
 config PHYS_ADDR_T_64BIT
 	def_bool 64BIT || ARCH_PHYS_ADDR_T_64BIT
 
<span class="p_header">diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="p_header">index 026f6a8..b964848 100644</span>
<span class="p_header">--- a/mm/Makefile</span>
<span class="p_header">+++ b/mm/Makefile</span>
<span class="p_chunk">@@ -39,7 +39,7 @@</span> <span class="p_context"> obj-y			:= filemap.o mempool.o oom_kill.o \</span>
 			   mm_init.o mmu_context.o percpu.o slab_common.o \
 			   compaction.o vmacache.o swap_slots.o \
 			   interval_tree.o list_lru.o workingset.o \
<span class="p_del">-			   debug.o $(mmu-y)</span>
<span class="p_add">+			   debug.o hmm.o $(mmu-y)</span>
 
 obj-y += init-mm.o
 
<span class="p_header">diff --git a/mm/hmm.c b/mm/hmm.c</span>
new file mode 100644
<span class="p_header">index 0000000..88a7e10</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/hmm.c</span>
<span class="p_chunk">@@ -0,0 +1,74 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright 2013 Red Hat Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Refer to include/linux/hmm.h for information about heterogeneous memory</span>
<span class="p_add">+ * management or HMM for short.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_HMM</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * struct hmm - HMM per mm struct</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mm: mm struct this HMM struct is bound to</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm {</span>
<span class="p_add">+	struct mm_struct	*mm;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_register - register HMM against an mm (HMM internal)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mm: mm struct to attach to</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is not intended to be used directly by device drivers. It allocates an</span>
<span class="p_add">+ * HMM struct if mm does not have one, and initializes it.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct hmm *hmm_register(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!mm-&gt;hmm) {</span>
<span class="p_add">+		struct hmm *hmm = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		hmm = kmalloc(sizeof(*hmm), GFP_KERNEL);</span>
<span class="p_add">+		if (!hmm)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+		hmm-&gt;mm = mm;</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+		if (!mm-&gt;hmm)</span>
<span class="p_add">+			mm-&gt;hmm = hmm;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			kfree(hmm);</span>
<span class="p_add">+		spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The hmm struct can only be freed once the mm_struct goes away,</span>
<span class="p_add">+	 * hence we should always have pre-allocated an new hmm struct</span>
<span class="p_add">+	 * above.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return mm-&gt;hmm;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void hmm_mm_destroy(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	kfree(mm-&gt;hmm);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_HMM */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



