
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[13/27] mm, memcg: Move memcg limit enforcement from zones to nodes - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [13/27] mm, memcg: Move memcg limit enforcement from zones to nodes</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 21, 2016, 2:15 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1466518566-30034-14-git-send-email-mgorman@techsingularity.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9190701/mbox/"
   >mbox</a>
|
   <a href="/patch/9190701/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9190701/">/patch/9190701/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	66DB0601C0 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jun 2016 14:25:58 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 574A528173
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jun 2016 14:25:58 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 4C1A32818B; Tue, 21 Jun 2016 14:25:58 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id D49D528173
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jun 2016 14:25:56 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752038AbcFUOZo (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 21 Jun 2016 10:25:44 -0400
Received: from outbound-smtp09.blacknight.com ([46.22.139.14]:60174 &quot;EHLO
	outbound-smtp09.blacknight.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1751088AbcFUOZn (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 21 Jun 2016 10:25:43 -0400
Received: from mail.blacknight.com (pemlinmail01.blacknight.ie
	[81.17.254.10])
	by outbound-smtp09.blacknight.com (Postfix) with ESMTPS id
	D72DF1C19B2 for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 21 Jun 2016 15:18:29 +0100 (IST)
Received: (qmail 17062 invoked from network); 21 Jun 2016 14:18:29 -0000
Received: from unknown (HELO stampy.163woodhaven.lan)
	(mgorman@techsingularity.net@[37.228.231.136])
	by 81.17.254.9 with ESMTPA; 21 Jun 2016 14:18:29 -0000
From: Mel Gorman &lt;mgorman@techsingularity.net&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;, Linux-MM &lt;linux-mm@kvack.org&gt;
Cc: Rik van Riel &lt;riel@surriel.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;
Subject: [PATCH 13/27] mm,
	memcg: Move memcg limit enforcement from zones to nodes
Date: Tue, 21 Jun 2016 15:15:52 +0100
Message-Id: &lt;1466518566-30034-14-git-send-email-mgorman@techsingularity.net&gt;
X-Mailer: git-send-email 2.6.4
In-Reply-To: &lt;1466518566-30034-1-git-send-email-mgorman@techsingularity.net&gt;
References: &lt;1466518566-30034-1-git-send-email-mgorman@techsingularity.net&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - June 21, 2016, 2:15 p.m.</div>
<pre class="content">
Memcg was broken by the move of all LRUs to nodes because it is tracking
limits on a per-zone basis while receiving reclaim requests on a per-node
basis. This patch moves limit enforcement to the nodes. Technically, all
the variable names should also change but people are already familiar by
the meaning of &quot;mz&quot; even if &quot;mn&quot; would be a more appropriate name now.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
---
 include/linux/memcontrol.h |  21 ++---
 include/linux/swap.h       |   2 +-
 mm/memcontrol.c            | 210 ++++++++++++++++++++-------------------------
 mm/vmscan.c                |  22 ++---
 mm/workingset.c            |   6 +-
 5 files changed, 114 insertions(+), 147 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 22, 2016, 1:15 p.m.</div>
<pre class="content">
On Tue 21-06-16 15:15:52, Mel Gorman wrote:
<span class="quote">&gt; Memcg was broken by the move of all LRUs to nodes because it is tracking</span>
<span class="quote">&gt; limits on a per-zone basis while receiving reclaim requests on a per-node</span>
<span class="quote">&gt; basis.</span>

This might be a bit misleading/confusing. First of all the limit is
tracked per memcg not zones/nodes. What you are talking about is
tracking soft limit excess and that was really tracked per zone.
Then I do not think the zones-&gt;nodes LRUs should lead to any noticeable
differences. All we care about is to reclaim some memory to get down to
the soft limit.
<span class="quote">
&gt; This patch moves limit enforcement to the nodes. Technically, all</span>
<span class="quote">&gt; the variable names should also change but people are already familiar by</span>
<span class="quote">&gt; the meaning of &quot;mz&quot; even if &quot;mn&quot; would be a more appropriate name now.</span>

I wouldn&#39;t bother with those.
<span class="quote"> 
&gt; Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>

This simplifies the code so it is definitely welcome! I would appreciate
a more precise changelog.
<span class="acked-by">
Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  include/linux/memcontrol.h |  21 ++---</span>
<span class="quote">&gt;  include/linux/swap.h       |   2 +-</span>
<span class="quote">&gt;  mm/memcontrol.c            | 210 ++++++++++++++++++++-------------------------</span>
<span class="quote">&gt;  mm/vmscan.c                |  22 ++---</span>
<span class="quote">&gt;  mm/workingset.c            |   6 +-</span>
<span class="quote">&gt;  5 files changed, 114 insertions(+), 147 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="quote">&gt; index a13328851fea..ea7e249cde6d 100644</span>
<span class="quote">&gt; --- a/include/linux/memcontrol.h</span>
<span class="quote">&gt; +++ b/include/linux/memcontrol.h</span>
<span class="quote">&gt; @@ -60,7 +60,7 @@ enum mem_cgroup_stat_index {</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct mem_cgroup_reclaim_cookie {</span>
<span class="quote">&gt; -	struct zone *zone;</span>
<span class="quote">&gt; +	pg_data_t *pgdat;</span>
<span class="quote">&gt;  	int priority;</span>
<span class="quote">&gt;  	unsigned int generation;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt; @@ -113,7 +113,7 @@ struct mem_cgroup_reclaim_iter {</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * per-zone information in memory controller.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; -struct mem_cgroup_per_zone {</span>
<span class="quote">&gt; +struct mem_cgroup_per_node {</span>
<span class="quote">&gt;  	struct lruvec		lruvec;</span>
<span class="quote">&gt;  	unsigned long		lru_size[NR_LRU_LISTS];</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -127,10 +127,6 @@ struct mem_cgroup_per_zone {</span>
<span class="quote">&gt;  						/* use container_of	   */</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct mem_cgroup_per_node {</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone zoneinfo[MAX_NR_ZONES];</span>
<span class="quote">&gt; -};</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  struct mem_cgroup_threshold {</span>
<span class="quote">&gt;  	struct eventfd_ctx *eventfd;</span>
<span class="quote">&gt;  	unsigned long threshold;</span>
<span class="quote">&gt; @@ -306,8 +302,7 @@ void mem_cgroup_uncharge_list(struct list_head *page_list);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void mem_cgroup_migrate(struct page *oldpage, struct page *newpage);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct lruvec *mem_cgroup_lruvec(struct pglist_data *, struct zone *zone,</span>
<span class="quote">&gt; -				 struct mem_cgroup *);</span>
<span class="quote">&gt; +struct lruvec *mem_cgroup_lruvec(struct pglist_data *, struct mem_cgroup *);</span>
<span class="quote">&gt;  struct lruvec *mem_cgroup_page_lruvec(struct page *, struct pglist_data *);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  bool task_in_mem_cgroup(struct task_struct *task, struct mem_cgroup *memcg);</span>
<span class="quote">&gt; @@ -410,9 +405,9 @@ unsigned long mem_cgroup_node_nr_lru_pages(struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  static inline</span>
<span class="quote">&gt;  unsigned long mem_cgroup_get_lru_size(struct lruvec *lruvec, enum lru_list lru)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mz = container_of(lruvec, struct mem_cgroup_per_zone, lruvec);</span>
<span class="quote">&gt; +	mz = container_of(lruvec, struct mem_cgroup_per_node, lruvec);</span>
<span class="quote">&gt;  	return mz-&gt;lru_size[lru];</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -483,7 +478,7 @@ static inline void mem_cgroup_dec_page_stat(struct page *page,</span>
<span class="quote">&gt;  	mem_cgroup_update_page_stat(page, idx, -1);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
<span class="quote">&gt; +unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,</span>
<span class="quote">&gt;  						gfp_t gfp_mask,</span>
<span class="quote">&gt;  						unsigned long *total_scanned);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -575,7 +570,7 @@ static inline void mem_cgroup_migrate(struct page *old, struct page *new)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
<span class="quote">&gt; -				struct zone *zone, struct mem_cgroup *memcg)</span>
<span class="quote">&gt; +				struct mem_cgroup *memcg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return node_lruvec(pgdat);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -687,7 +682,7 @@ static inline void mem_cgroup_dec_page_stat(struct page *page,</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline</span>
<span class="quote">&gt; -unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
<span class="quote">&gt; +unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,</span>
<span class="quote">&gt;  					    gfp_t gfp_mask,</span>
<span class="quote">&gt;  					    unsigned long *total_scanned)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="quote">&gt; index 0ad616d7c381..2a23ddc96edd 100644</span>
<span class="quote">&gt; --- a/include/linux/swap.h</span>
<span class="quote">&gt; +++ b/include/linux/swap.h</span>
<span class="quote">&gt; @@ -318,7 +318,7 @@ extern unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  						  bool may_swap);</span>
<span class="quote">&gt;  extern unsigned long mem_cgroup_shrink_node(struct mem_cgroup *mem,</span>
<span class="quote">&gt;  						gfp_t gfp_mask, bool noswap,</span>
<span class="quote">&gt; -						struct zone *zone,</span>
<span class="quote">&gt; +						pg_data_t *pgdat,</span>
<span class="quote">&gt;  						unsigned long *nr_scanned);</span>
<span class="quote">&gt;  extern unsigned long shrink_all_memory(unsigned long nr_pages);</span>
<span class="quote">&gt;  extern int vm_swappiness;</span>
<span class="quote">&gt; diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="quote">&gt; index aac5fae56ea4..b09a17e4f2ff 100644</span>
<span class="quote">&gt; --- a/mm/memcontrol.c</span>
<span class="quote">&gt; +++ b/mm/memcontrol.c</span>
<span class="quote">&gt; @@ -132,15 +132,11 @@ static const char * const mem_cgroup_lru_names[] = {</span>
<span class="quote">&gt;   * their hierarchy representation</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct mem_cgroup_tree_per_zone {</span>
<span class="quote">&gt; +struct mem_cgroup_tree_per_node {</span>
<span class="quote">&gt;  	struct rb_root rb_root;</span>
<span class="quote">&gt;  	spinlock_t lock;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct mem_cgroup_tree_per_node {</span>
<span class="quote">&gt; -	struct mem_cgroup_tree_per_zone rb_tree_per_zone[MAX_NR_ZONES];</span>
<span class="quote">&gt; -};</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  struct mem_cgroup_tree {</span>
<span class="quote">&gt;  	struct mem_cgroup_tree_per_node *rb_tree_per_node[MAX_NUMNODES];</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt; @@ -323,13 +319,10 @@ EXPORT_SYMBOL(memcg_kmem_enabled_key);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #endif /* !CONFIG_SLOB */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct mem_cgroup_per_zone *</span>
<span class="quote">&gt; -mem_cgroup_zone_zoneinfo(struct mem_cgroup *memcg, struct zone *zone)</span>
<span class="quote">&gt; +static struct mem_cgroup_per_node *</span>
<span class="quote">&gt; +mem_cgroup_nodeinfo(struct mem_cgroup *memcg, int nid)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	int nid = zone_to_nid(zone);</span>
<span class="quote">&gt; -	int zid = zone_idx(zone);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="quote">&gt; +	return memcg-&gt;nodeinfo[nid];</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt; @@ -383,37 +376,35 @@ ino_t page_cgroup_ino(struct page *page)</span>
<span class="quote">&gt;  	return ino;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct mem_cgroup_per_zone *</span>
<span class="quote">&gt; -mem_cgroup_page_zoneinfo(struct mem_cgroup *memcg, struct page *page)</span>
<span class="quote">&gt; +static struct mem_cgroup_per_node *</span>
<span class="quote">&gt; +mem_cgroup_page_nodeinfo(struct mem_cgroup *memcg, struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int nid = page_to_nid(page);</span>
<span class="quote">&gt; -	int zid = page_zonenum(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	return &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="quote">&gt; +	return memcg-&gt;nodeinfo[nid];</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct mem_cgroup_tree_per_zone *</span>
<span class="quote">&gt; -soft_limit_tree_node_zone(int nid, int zid)</span>
<span class="quote">&gt; +static struct mem_cgroup_tree_per_node *</span>
<span class="quote">&gt; +soft_limit_tree_node(int nid)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	return &amp;soft_limit_tree.rb_tree_per_node[nid]-&gt;rb_tree_per_zone[zid];</span>
<span class="quote">&gt; +	return soft_limit_tree.rb_tree_per_node[nid];</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct mem_cgroup_tree_per_zone *</span>
<span class="quote">&gt; +static struct mem_cgroup_tree_per_node *</span>
<span class="quote">&gt;  soft_limit_tree_from_page(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int nid = page_to_nid(page);</span>
<span class="quote">&gt; -	int zid = page_zonenum(page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	return &amp;soft_limit_tree.rb_tree_per_node[nid]-&gt;rb_tree_per_zone[zid];</span>
<span class="quote">&gt; +	return soft_limit_tree.rb_tree_per_node[nid];</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="quote">&gt; -					 struct mem_cgroup_tree_per_zone *mctz,</span>
<span class="quote">&gt; +static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_node *mz,</span>
<span class="quote">&gt; +					 struct mem_cgroup_tree_per_node *mctz,</span>
<span class="quote">&gt;  					 unsigned long new_usage_in_excess)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct rb_node **p = &amp;mctz-&gt;rb_root.rb_node;</span>
<span class="quote">&gt;  	struct rb_node *parent = NULL;</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz_node;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz_node;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (mz-&gt;on_tree)</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; @@ -423,7 +414,7 @@ static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  	while (*p) {</span>
<span class="quote">&gt;  		parent = *p;</span>
<span class="quote">&gt; -		mz_node = rb_entry(parent, struct mem_cgroup_per_zone,</span>
<span class="quote">&gt; +		mz_node = rb_entry(parent, struct mem_cgroup_per_node,</span>
<span class="quote">&gt;  					tree_node);</span>
<span class="quote">&gt;  		if (mz-&gt;usage_in_excess &lt; mz_node-&gt;usage_in_excess)</span>
<span class="quote">&gt;  			p = &amp;(*p)-&gt;rb_left;</span>
<span class="quote">&gt; @@ -439,8 +430,8 @@ static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="quote">&gt;  	mz-&gt;on_tree = true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="quote">&gt; -					 struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="quote">&gt; +static void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_node *mz,</span>
<span class="quote">&gt; +					 struct mem_cgroup_tree_per_node *mctz)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	if (!mz-&gt;on_tree)</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; @@ -448,8 +439,8 @@ static void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="quote">&gt;  	mz-&gt;on_tree = false;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void mem_cgroup_remove_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="quote">&gt; -				       struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="quote">&gt; +static void mem_cgroup_remove_exceeded(struct mem_cgroup_per_node *mz,</span>
<span class="quote">&gt; +				       struct mem_cgroup_tree_per_node *mctz)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long flags;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -473,8 +464,8 @@ static unsigned long soft_limit_excess(struct mem_cgroup *memcg)</span>
<span class="quote">&gt;  static void mem_cgroup_update_tree(struct mem_cgroup *memcg, struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long excess;</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; -	struct mem_cgroup_tree_per_zone *mctz;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt; +	struct mem_cgroup_tree_per_node *mctz;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mctz = soft_limit_tree_from_page(page);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -482,7 +473,7 @@ static void mem_cgroup_update_tree(struct mem_cgroup *memcg, struct page *page)</span>
<span class="quote">&gt;  	 * because their event counter is not touched.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	for (; memcg; memcg = parent_mem_cgroup(memcg)) {</span>
<span class="quote">&gt; -		mz = mem_cgroup_page_zoneinfo(memcg, page);</span>
<span class="quote">&gt; +		mz = mem_cgroup_page_nodeinfo(memcg, page);</span>
<span class="quote">&gt;  		excess = soft_limit_excess(memcg);</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * We have to update the tree if mz is on RB-tree or</span>
<span class="quote">&gt; @@ -507,24 +498,22 @@ static void mem_cgroup_update_tree(struct mem_cgroup *memcg, struct page *page)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void mem_cgroup_remove_from_trees(struct mem_cgroup *memcg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct mem_cgroup_tree_per_zone *mctz;</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; -	int nid, zid;</span>
<span class="quote">&gt; +	struct mem_cgroup_tree_per_node *mctz;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt; +	int nid;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_node(nid) {</span>
<span class="quote">&gt; -		for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="quote">&gt; -			mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="quote">&gt; -			mctz = soft_limit_tree_node_zone(nid, zid);</span>
<span class="quote">&gt; -			mem_cgroup_remove_exceeded(mz, mctz);</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; +		mz = mem_cgroup_nodeinfo(memcg, nid);</span>
<span class="quote">&gt; +		mctz = soft_limit_tree_node(nid);</span>
<span class="quote">&gt; +		mem_cgroup_remove_exceeded(mz, mctz);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct mem_cgroup_per_zone *</span>
<span class="quote">&gt; -__mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="quote">&gt; +static struct mem_cgroup_per_node *</span>
<span class="quote">&gt; +__mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_node *mctz)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct rb_node *rightmost = NULL;</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  retry:</span>
<span class="quote">&gt;  	mz = NULL;</span>
<span class="quote">&gt; @@ -532,7 +521,7 @@ __mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="quote">&gt;  	if (!rightmost)</span>
<span class="quote">&gt;  		goto done;		/* Nothing to reclaim from */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mz = rb_entry(rightmost, struct mem_cgroup_per_zone, tree_node);</span>
<span class="quote">&gt; +	mz = rb_entry(rightmost, struct mem_cgroup_per_node, tree_node);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Remove the node now but someone else can add it back,</span>
<span class="quote">&gt;  	 * we will to add it back at the end of reclaim to its correct</span>
<span class="quote">&gt; @@ -546,10 +535,10 @@ __mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="quote">&gt;  	return mz;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct mem_cgroup_per_zone *</span>
<span class="quote">&gt; -mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="quote">&gt; +static struct mem_cgroup_per_node *</span>
<span class="quote">&gt; +mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_node *mctz)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock_irq(&amp;mctz-&gt;lock);</span>
<span class="quote">&gt;  	mz = __mem_cgroup_largest_soft_limit_node(mctz);</span>
<span class="quote">&gt; @@ -643,20 +632,16 @@ unsigned long mem_cgroup_node_nr_lru_pages(struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  					   int nid, unsigned int lru_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long nr = 0;</span>
<span class="quote">&gt; -	int zid;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt; +	enum lru_list lru;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	VM_BUG_ON((unsigned)nid &gt;= nr_node_ids);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="quote">&gt; -		struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; -		enum lru_list lru;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		for_each_lru(lru) {</span>
<span class="quote">&gt; -			if (!(BIT(lru) &amp; lru_mask))</span>
<span class="quote">&gt; -				continue;</span>
<span class="quote">&gt; -			mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="quote">&gt; -			nr += mz-&gt;lru_size[lru];</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; +	for_each_lru(lru) {</span>
<span class="quote">&gt; +		if (!(BIT(lru) &amp; lru_mask))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		mz = mem_cgroup_nodeinfo(memcg, nid);</span>
<span class="quote">&gt; +		nr += mz-&gt;lru_size[lru];</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return nr;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -809,9 +794,9 @@ struct mem_cgroup *mem_cgroup_iter(struct mem_cgroup *root,</span>
<span class="quote">&gt;  	rcu_read_lock();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (reclaim) {</span>
<span class="quote">&gt; -		struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; +		struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		mz = mem_cgroup_zone_zoneinfo(root, reclaim-&gt;zone);</span>
<span class="quote">&gt; +		mz = mem_cgroup_nodeinfo(root, reclaim-&gt;pgdat-&gt;node_id);</span>
<span class="quote">&gt;  		iter = &amp;mz-&gt;iter[reclaim-&gt;priority];</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (prev &amp;&amp; reclaim-&gt;generation != iter-&gt;generation)</span>
<span class="quote">&gt; @@ -910,19 +895,17 @@ static void invalidate_reclaim_iterators(struct mem_cgroup *dead_memcg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mem_cgroup *memcg = dead_memcg;</span>
<span class="quote">&gt;  	struct mem_cgroup_reclaim_iter *iter;</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; -	int nid, zid;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt; +	int nid;</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	while ((memcg = parent_mem_cgroup(memcg))) {</span>
<span class="quote">&gt;  		for_each_node(nid) {</span>
<span class="quote">&gt; -			for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="quote">&gt; -				mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="quote">&gt; -				for (i = 0; i &lt;= DEF_PRIORITY; i++) {</span>
<span class="quote">&gt; -					iter = &amp;mz-&gt;iter[i];</span>
<span class="quote">&gt; -					cmpxchg(&amp;iter-&gt;position,</span>
<span class="quote">&gt; -						dead_memcg, NULL);</span>
<span class="quote">&gt; -				}</span>
<span class="quote">&gt; +			mz = mem_cgroup_nodeinfo(memcg, nid);</span>
<span class="quote">&gt; +			for (i = 0; i &lt;= DEF_PRIORITY; i++) {</span>
<span class="quote">&gt; +				iter = &amp;mz-&gt;iter[i];</span>
<span class="quote">&gt; +				cmpxchg(&amp;iter-&gt;position,</span>
<span class="quote">&gt; +					dead_memcg, NULL);</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; @@ -946,7 +929,6 @@ static void invalidate_reclaim_iterators(struct mem_cgroup *dead_memcg)</span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt;   * mem_cgroup_lruvec - get the lru list vector for a node or a memcg zone</span>
<span class="quote">&gt;   * @node: node of the wanted lruvec</span>
<span class="quote">&gt; - * @zone: zone of the wanted lruvec</span>
<span class="quote">&gt;   * @memcg: memcg of the wanted lruvec</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Returns the lru list vector holding pages for a given @node or a given</span>
<span class="quote">&gt; @@ -954,9 +936,9 @@ static void invalidate_reclaim_iterators(struct mem_cgroup *dead_memcg)</span>
<span class="quote">&gt;   * is disabled.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
<span class="quote">&gt; -				 struct zone *zone, struct mem_cgroup *memcg)</span>
<span class="quote">&gt; +				 struct mem_cgroup *memcg)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt;  	struct lruvec *lruvec;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (mem_cgroup_disabled()) {</span>
<span class="quote">&gt; @@ -964,7 +946,7 @@ struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mz = mem_cgroup_zone_zoneinfo(memcg, zone);</span>
<span class="quote">&gt; +	mz = mem_cgroup_nodeinfo(memcg, pgdat-&gt;node_id);</span>
<span class="quote">&gt;  	lruvec = &amp;mz-&gt;lruvec;</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -972,8 +954,8 @@ struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
<span class="quote">&gt;  	 * we have to be prepared to initialize lruvec-&gt;zone here;</span>
<span class="quote">&gt;  	 * and if offlined then reonlined, we need to reinitialize it.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (unlikely(lruvec-&gt;pgdat != zone-&gt;zone_pgdat))</span>
<span class="quote">&gt; -		lruvec-&gt;pgdat = zone-&gt;zone_pgdat;</span>
<span class="quote">&gt; +	if (unlikely(lruvec-&gt;pgdat != pgdat))</span>
<span class="quote">&gt; +		lruvec-&gt;pgdat = pgdat;</span>
<span class="quote">&gt;  	return lruvec;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -988,7 +970,7 @@ struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  struct lruvec *mem_cgroup_page_lruvec(struct page *page, struct pglist_data *pgdat)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt;  	struct mem_cgroup *memcg;</span>
<span class="quote">&gt;  	struct lruvec *lruvec;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1005,7 +987,7 @@ struct lruvec *mem_cgroup_page_lruvec(struct page *page, struct pglist_data *pgd</span>
<span class="quote">&gt;  	if (!memcg)</span>
<span class="quote">&gt;  		memcg = root_mem_cgroup;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mz = mem_cgroup_page_zoneinfo(memcg, page);</span>
<span class="quote">&gt; +	mz = mem_cgroup_page_nodeinfo(memcg, page);</span>
<span class="quote">&gt;  	lruvec = &amp;mz-&gt;lruvec;</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt; @@ -1032,7 +1014,7 @@ struct lruvec *mem_cgroup_page_lruvec(struct page *page, struct pglist_data *pgd</span>
<span class="quote">&gt;  void mem_cgroup_update_lru_size(struct lruvec *lruvec, enum lru_list lru,</span>
<span class="quote">&gt;  				enum zone_type zid, int nr_pages)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt;  	unsigned long *lru_size;</span>
<span class="quote">&gt;  	long size;</span>
<span class="quote">&gt;  	bool empty;</span>
<span class="quote">&gt; @@ -1042,7 +1024,7 @@ void mem_cgroup_update_lru_size(struct lruvec *lruvec, enum lru_list lru,</span>
<span class="quote">&gt;  	if (mem_cgroup_disabled())</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mz = container_of(lruvec, struct mem_cgroup_per_zone, lruvec);</span>
<span class="quote">&gt; +	mz = container_of(lruvec, struct mem_cgroup_per_node, lruvec);</span>
<span class="quote">&gt;  	lru_size = mz-&gt;lru_size + lru;</span>
<span class="quote">&gt;  	empty = list_empty(lruvec-&gt;lists + lru);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1435,7 +1417,7 @@ int mem_cgroup_select_victim_node(struct mem_cgroup *memcg)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,</span>
<span class="quote">&gt; -				   struct zone *zone,</span>
<span class="quote">&gt; +				   pg_data_t *pgdat,</span>
<span class="quote">&gt;  				   gfp_t gfp_mask,</span>
<span class="quote">&gt;  				   unsigned long *total_scanned)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -1445,7 +1427,7 @@ static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,</span>
<span class="quote">&gt;  	unsigned long excess;</span>
<span class="quote">&gt;  	unsigned long nr_scanned;</span>
<span class="quote">&gt;  	struct mem_cgroup_reclaim_cookie reclaim = {</span>
<span class="quote">&gt; -		.zone = zone,</span>
<span class="quote">&gt; +		.pgdat = pgdat,</span>
<span class="quote">&gt;  		.priority = 0,</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1476,7 +1458,7 @@ static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,</span>
<span class="quote">&gt;  			continue;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		total += mem_cgroup_shrink_node(victim, gfp_mask, false,</span>
<span class="quote">&gt; -					zone, &amp;nr_scanned);</span>
<span class="quote">&gt; +					pgdat, &amp;nr_scanned);</span>
<span class="quote">&gt;  		*total_scanned += nr_scanned;</span>
<span class="quote">&gt;  		if (!soft_limit_excess(root_memcg))</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt; @@ -2603,22 +2585,22 @@ static int mem_cgroup_resize_memsw_limit(struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
<span class="quote">&gt; +unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,</span>
<span class="quote">&gt;  					    gfp_t gfp_mask,</span>
<span class="quote">&gt;  					    unsigned long *total_scanned)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	unsigned long nr_reclaimed = 0;</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz, *next_mz = NULL;</span>
<span class="quote">&gt; +	struct mem_cgroup_per_node *mz, *next_mz = NULL;</span>
<span class="quote">&gt;  	unsigned long reclaimed;</span>
<span class="quote">&gt;  	int loop = 0;</span>
<span class="quote">&gt; -	struct mem_cgroup_tree_per_zone *mctz;</span>
<span class="quote">&gt; +	struct mem_cgroup_tree_per_node *mctz;</span>
<span class="quote">&gt;  	unsigned long excess;</span>
<span class="quote">&gt;  	unsigned long nr_scanned;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (order &gt; 0)</span>
<span class="quote">&gt;  		return 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	mctz = soft_limit_tree_node_zone(zone_to_nid(zone), zone_idx(zone));</span>
<span class="quote">&gt; +	mctz = soft_limit_tree_node(pgdat-&gt;node_id);</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * This loop can run a while, specially if mem_cgroup&#39;s continuously</span>
<span class="quote">&gt;  	 * keep exceeding their soft limit and putting the system under</span>
<span class="quote">&gt; @@ -2633,7 +2615,7 @@ unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		nr_scanned = 0;</span>
<span class="quote">&gt; -		reclaimed = mem_cgroup_soft_reclaim(mz-&gt;memcg, zone,</span>
<span class="quote">&gt; +		reclaimed = mem_cgroup_soft_reclaim(mz-&gt;memcg, pgdat,</span>
<span class="quote">&gt;  						    gfp_mask, &amp;nr_scanned);</span>
<span class="quote">&gt;  		nr_reclaimed += reclaimed;</span>
<span class="quote">&gt;  		*total_scanned += nr_scanned;</span>
<span class="quote">&gt; @@ -3254,22 +3236,21 @@ static int memcg_stat_show(struct seq_file *m, void *v)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_DEBUG_VM</span>
<span class="quote">&gt;  	{</span>
<span class="quote">&gt; -		int nid, zid;</span>
<span class="quote">&gt; -		struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; +		pg_data_t *pgdat;</span>
<span class="quote">&gt; +		struct mem_cgroup_per_node *mz;</span>
<span class="quote">&gt;  		struct zone_reclaim_stat *rstat;</span>
<span class="quote">&gt;  		unsigned long recent_rotated[2] = {0, 0};</span>
<span class="quote">&gt;  		unsigned long recent_scanned[2] = {0, 0};</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		for_each_online_node(nid)</span>
<span class="quote">&gt; -			for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="quote">&gt; -				mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="quote">&gt; -				rstat = &amp;mz-&gt;lruvec.reclaim_stat;</span>
<span class="quote">&gt; +		for_each_online_pgdat(pgdat) {</span>
<span class="quote">&gt; +			mz = mem_cgroup_nodeinfo(memcg, pgdat-&gt;node_id);</span>
<span class="quote">&gt; +			rstat = &amp;mz-&gt;lruvec.reclaim_stat;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -				recent_rotated[0] += rstat-&gt;recent_rotated[0];</span>
<span class="quote">&gt; -				recent_rotated[1] += rstat-&gt;recent_rotated[1];</span>
<span class="quote">&gt; -				recent_scanned[0] += rstat-&gt;recent_scanned[0];</span>
<span class="quote">&gt; -				recent_scanned[1] += rstat-&gt;recent_scanned[1];</span>
<span class="quote">&gt; -			}</span>
<span class="quote">&gt; +			recent_rotated[0] += rstat-&gt;recent_rotated[0];</span>
<span class="quote">&gt; +			recent_rotated[1] += rstat-&gt;recent_rotated[1];</span>
<span class="quote">&gt; +			recent_scanned[0] += rstat-&gt;recent_scanned[0];</span>
<span class="quote">&gt; +			recent_scanned[1] += rstat-&gt;recent_scanned[1];</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		seq_printf(m, &quot;recent_rotated_anon %lu\n&quot;, recent_rotated[0]);</span>
<span class="quote">&gt;  		seq_printf(m, &quot;recent_rotated_file %lu\n&quot;, recent_rotated[1]);</span>
<span class="quote">&gt;  		seq_printf(m, &quot;recent_scanned_anon %lu\n&quot;, recent_scanned[0]);</span>
<span class="quote">&gt; @@ -4095,11 +4076,10 @@ static struct cftype mem_cgroup_legacy_files[] = {</span>
<span class="quote">&gt;  	{ },	/* terminate */</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static int alloc_mem_cgroup_per_zone_info(struct mem_cgroup *memcg, int node)</span>
<span class="quote">&gt; +static int alloc_mem_cgroup_per_node_info(struct mem_cgroup *memcg, int node)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mem_cgroup_per_node *pn;</span>
<span class="quote">&gt; -	struct mem_cgroup_per_zone *mz;</span>
<span class="quote">&gt; -	int zone, tmp = node;</span>
<span class="quote">&gt; +	int tmp = node;</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * This routine is called against possible nodes.</span>
<span class="quote">&gt;  	 * But it&#39;s BUG to call kmalloc() against offline node.</span>
<span class="quote">&gt; @@ -4114,18 +4094,16 @@ static int alloc_mem_cgroup_per_zone_info(struct mem_cgroup *memcg, int node)</span>
<span class="quote">&gt;  	if (!pn)</span>
<span class="quote">&gt;  		return 1;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	for (zone = 0; zone &lt; MAX_NR_ZONES; zone++) {</span>
<span class="quote">&gt; -		mz = &amp;pn-&gt;zoneinfo[zone];</span>
<span class="quote">&gt; -		lruvec_init(&amp;mz-&gt;lruvec);</span>
<span class="quote">&gt; -		mz-&gt;usage_in_excess = 0;</span>
<span class="quote">&gt; -		mz-&gt;on_tree = false;</span>
<span class="quote">&gt; -		mz-&gt;memcg = memcg;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; +	lruvec_init(&amp;pn-&gt;lruvec);</span>
<span class="quote">&gt; +	pn-&gt;usage_in_excess = 0;</span>
<span class="quote">&gt; +	pn-&gt;on_tree = false;</span>
<span class="quote">&gt; +	pn-&gt;memcg = memcg;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	memcg-&gt;nodeinfo[node] = pn;</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static void free_mem_cgroup_per_zone_info(struct mem_cgroup *memcg, int node)</span>
<span class="quote">&gt; +static void free_mem_cgroup_per_node_info(struct mem_cgroup *memcg, int node)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	kfree(memcg-&gt;nodeinfo[node]);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -4136,7 +4114,7 @@ static void mem_cgroup_free(struct mem_cgroup *memcg)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	memcg_wb_domain_exit(memcg);</span>
<span class="quote">&gt;  	for_each_node(node)</span>
<span class="quote">&gt; -		free_mem_cgroup_per_zone_info(memcg, node);</span>
<span class="quote">&gt; +		free_mem_cgroup_per_node_info(memcg, node);</span>
<span class="quote">&gt;  	free_percpu(memcg-&gt;stat);</span>
<span class="quote">&gt;  	kfree(memcg);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -4159,7 +4137,7 @@ static struct mem_cgroup *mem_cgroup_alloc(void)</span>
<span class="quote">&gt;  		goto fail;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_node(node)</span>
<span class="quote">&gt; -		if (alloc_mem_cgroup_per_zone_info(memcg, node))</span>
<span class="quote">&gt; +		if (alloc_mem_cgroup_per_node_info(memcg, node))</span>
<span class="quote">&gt;  			goto fail;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (memcg_wb_domain_init(memcg, GFP_KERNEL))</span>
<span class="quote">&gt; @@ -5757,18 +5735,12 @@ static int __init mem_cgroup_init(void)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	for_each_node(node) {</span>
<span class="quote">&gt;  		struct mem_cgroup_tree_per_node *rtpn;</span>
<span class="quote">&gt; -		int zone;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		rtpn = kzalloc_node(sizeof(*rtpn), GFP_KERNEL,</span>
<span class="quote">&gt;  				    node_online(node) ? node : NUMA_NO_NODE);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		for (zone = 0; zone &lt; MAX_NR_ZONES; zone++) {</span>
<span class="quote">&gt; -			struct mem_cgroup_tree_per_zone *rtpz;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -			rtpz = &amp;rtpn-&gt;rb_tree_per_zone[zone];</span>
<span class="quote">&gt; -			rtpz-&gt;rb_root = RB_ROOT;</span>
<span class="quote">&gt; -			spin_lock_init(&amp;rtpz-&gt;lock);</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; +		rtpn-&gt;rb_root = RB_ROOT;</span>
<span class="quote">&gt; +		spin_lock_init(&amp;rtpn-&gt;lock);</span>
<span class="quote">&gt;  		soft_limit_tree.rb_tree_per_node[node] = rtpn;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt; index 3774ebf19f63..cf73bf4ebd06 100644</span>
<span class="quote">&gt; --- a/mm/vmscan.c</span>
<span class="quote">&gt; +++ b/mm/vmscan.c</span>
<span class="quote">&gt; @@ -2223,8 +2223,7 @@ static inline void init_tlb_ubc(void)</span>
<span class="quote">&gt;  static void shrink_node_memcg(struct pglist_data *pgdat, struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  			      struct scan_control *sc, unsigned long *lru_pages)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct zone *zone = &amp;pgdat-&gt;node_zones[sc-&gt;reclaim_idx];</span>
<span class="quote">&gt; -	struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, zone, memcg);</span>
<span class="quote">&gt; +	struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, memcg);</span>
<span class="quote">&gt;  	unsigned long nr[NR_LRU_LISTS];</span>
<span class="quote">&gt;  	unsigned long targets[NR_LRU_LISTS];</span>
<span class="quote">&gt;  	unsigned long nr_to_scan;</span>
<span class="quote">&gt; @@ -2431,7 +2430,7 @@ static bool shrink_node(pg_data_t *pgdat, struct scan_control *sc,</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt;  		struct mem_cgroup *root = sc-&gt;target_mem_cgroup;</span>
<span class="quote">&gt;  		struct mem_cgroup_reclaim_cookie reclaim = {</span>
<span class="quote">&gt; -			.zone = &amp;pgdat-&gt;node_zones[classzone_idx],</span>
<span class="quote">&gt; +			.pgdat = pgdat,</span>
<span class="quote">&gt;  			.priority = sc-&gt;priority,</span>
<span class="quote">&gt;  		};</span>
<span class="quote">&gt;  		unsigned long node_lru_pages = 0;</span>
<span class="quote">&gt; @@ -2638,7 +2637,7 @@ static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc,</span>
<span class="quote">&gt;  			 * and balancing, not for a memcg&#39;s limit.</span>
<span class="quote">&gt;  			 */</span>
<span class="quote">&gt;  			nr_soft_scanned = 0;</span>
<span class="quote">&gt; -			nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone,</span>
<span class="quote">&gt; +			nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone-&gt;zone_pgdat,</span>
<span class="quote">&gt;  						sc-&gt;order, sc-&gt;gfp_mask,</span>
<span class="quote">&gt;  						&amp;nr_soft_scanned);</span>
<span class="quote">&gt;  			sc-&gt;nr_reclaimed += nr_soft_reclaimed;</span>
<span class="quote">&gt; @@ -2905,7 +2904,7 @@ unsigned long try_to_free_pages(struct zonelist *zonelist, int order,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  						gfp_t gfp_mask, bool noswap,</span>
<span class="quote">&gt; -						struct zone *zone,</span>
<span class="quote">&gt; +						pg_data_t *pgdat,</span>
<span class="quote">&gt;  						unsigned long *nr_scanned)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct scan_control sc = {</span>
<span class="quote">&gt; @@ -2913,7 +2912,7 @@ unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  		.target_mem_cgroup = memcg,</span>
<span class="quote">&gt;  		.may_writepage = !laptop_mode,</span>
<span class="quote">&gt;  		.may_unmap = 1,</span>
<span class="quote">&gt; -		.reclaim_idx = zone_idx(zone),</span>
<span class="quote">&gt; +		.reclaim_idx = MAX_NR_ZONES - 1,</span>
<span class="quote">&gt;  		.may_swap = !noswap,</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt;  	unsigned long lru_pages;</span>
<span class="quote">&gt; @@ -2932,7 +2931,7 @@ unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  	 * will pick up pages from other mem cgroup&#39;s as well. We hack</span>
<span class="quote">&gt;  	 * the priority and make it zero.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	shrink_node_memcg(zone-&gt;zone_pgdat, memcg, &amp;sc, &amp;lru_pages);</span>
<span class="quote">&gt; +	shrink_node_memcg(pgdat, memcg, &amp;sc, &amp;lru_pages);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	trace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -2952,6 +2951,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  		.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),</span>
<span class="quote">&gt;  		.gfp_mask = (gfp_mask &amp; GFP_RECLAIM_MASK) |</span>
<span class="quote">&gt;  				(GFP_HIGHUSER_MOVABLE &amp; ~GFP_RECLAIM_MASK),</span>
<span class="quote">&gt; +		.reclaim_idx = MAX_NR_ZONES - 1,</span>
<span class="quote">&gt;  		.target_mem_cgroup = memcg,</span>
<span class="quote">&gt;  		.priority = DEF_PRIORITY,</span>
<span class="quote">&gt;  		.may_writepage = !laptop_mode,</span>
<span class="quote">&gt; @@ -2981,7 +2981,7 @@ unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void age_active_anon(struct pglist_data *pgdat,</span>
<span class="quote">&gt; -				struct zone *zone, struct scan_control *sc)</span>
<span class="quote">&gt; +				struct scan_control *sc)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mem_cgroup *memcg;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -2990,7 +2990,7 @@ static void age_active_anon(struct pglist_data *pgdat,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	memcg = mem_cgroup_iter(NULL, NULL, NULL);</span>
<span class="quote">&gt;  	do {</span>
<span class="quote">&gt; -		struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, zone, memcg);</span>
<span class="quote">&gt; +		struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, memcg);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (inactive_list_is_low(lruvec, false))</span>
<span class="quote">&gt;  			shrink_active_list(SWAP_CLUSTER_MAX, lruvec,</span>
<span class="quote">&gt; @@ -3178,7 +3178,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)</span>
<span class="quote">&gt;  		 * pages are rotated regardless of classzone as this is</span>
<span class="quote">&gt;  		 * about consistent aging.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		age_active_anon(pgdat, &amp;pgdat-&gt;node_zones[MAX_NR_ZONES - 1], &amp;sc);</span>
<span class="quote">&gt; +		age_active_anon(pgdat, &amp;sc);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * If we&#39;re getting trouble reclaiming, start doing writepage</span>
<span class="quote">&gt; @@ -3190,7 +3190,7 @@ static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)</span>
<span class="quote">&gt;  		/* Call soft limit reclaim before calling shrink_node. */</span>
<span class="quote">&gt;  		sc.nr_scanned = 0;</span>
<span class="quote">&gt;  		nr_soft_scanned = 0;</span>
<span class="quote">&gt; -		nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone, sc.order,</span>
<span class="quote">&gt; +		nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(pgdat, sc.order,</span>
<span class="quote">&gt;  						sc.gfp_mask, &amp;nr_soft_scanned);</span>
<span class="quote">&gt;  		sc.nr_reclaimed += nr_soft_reclaimed;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/workingset.c b/mm/workingset.c</span>
<span class="quote">&gt; index 2d81ca11317d..5b479890cc18 100644</span>
<span class="quote">&gt; --- a/mm/workingset.c</span>
<span class="quote">&gt; +++ b/mm/workingset.c</span>
<span class="quote">&gt; @@ -218,7 +218,7 @@ void *workingset_eviction(struct address_space *mapping, struct page *page)</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(page_count(page), page);</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, zone, memcg);</span>
<span class="quote">&gt; +	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, memcg);</span>
<span class="quote">&gt;  	eviction = atomic_long_inc_return(&amp;lruvec-&gt;inactive_age);</span>
<span class="quote">&gt;  	return pack_shadow(memcgid, zone, eviction);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -267,7 +267,7 @@ bool workingset_refault(void *shadow)</span>
<span class="quote">&gt;  		rcu_read_unlock();</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; -	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, zone, memcg);</span>
<span class="quote">&gt; +	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, memcg);</span>
<span class="quote">&gt;  	refault = atomic_long_read(&amp;lruvec-&gt;inactive_age);</span>
<span class="quote">&gt;  	active_file = lruvec_lru_size(lruvec, LRU_ACTIVE_FILE);</span>
<span class="quote">&gt;  	rcu_read_unlock();</span>
<span class="quote">&gt; @@ -317,7 +317,7 @@ void workingset_activation(struct page *page)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (!mem_cgroup_disabled() &amp;&amp; !page_memcg(page))</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt; -	lruvec = mem_cgroup_lruvec(page_pgdat(page), page_zone(page), page_memcg(page));</span>
<span class="quote">&gt; +	lruvec = mem_cgroup_lruvec(page_pgdat(page), page_memcg(page));</span>
<span class="quote">&gt;  	atomic_long_inc(&amp;lruvec-&gt;inactive_age);</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	unlock_page_memcg(page);</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.6.4</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="p_header">index a13328851fea..ea7e249cde6d 100644</span>
<span class="p_header">--- a/include/linux/memcontrol.h</span>
<span class="p_header">+++ b/include/linux/memcontrol.h</span>
<span class="p_chunk">@@ -60,7 +60,7 @@</span> <span class="p_context"> enum mem_cgroup_stat_index {</span>
 };
 
 struct mem_cgroup_reclaim_cookie {
<span class="p_del">-	struct zone *zone;</span>
<span class="p_add">+	pg_data_t *pgdat;</span>
 	int priority;
 	unsigned int generation;
 };
<span class="p_chunk">@@ -113,7 +113,7 @@</span> <span class="p_context"> struct mem_cgroup_reclaim_iter {</span>
 /*
  * per-zone information in memory controller.
  */
<span class="p_del">-struct mem_cgroup_per_zone {</span>
<span class="p_add">+struct mem_cgroup_per_node {</span>
 	struct lruvec		lruvec;
 	unsigned long		lru_size[NR_LRU_LISTS];
 
<span class="p_chunk">@@ -127,10 +127,6 @@</span> <span class="p_context"> struct mem_cgroup_per_zone {</span>
 						/* use container_of	   */
 };
 
<span class="p_del">-struct mem_cgroup_per_node {</span>
<span class="p_del">-	struct mem_cgroup_per_zone zoneinfo[MAX_NR_ZONES];</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 struct mem_cgroup_threshold {
 	struct eventfd_ctx *eventfd;
 	unsigned long threshold;
<span class="p_chunk">@@ -306,8 +302,7 @@</span> <span class="p_context"> void mem_cgroup_uncharge_list(struct list_head *page_list);</span>
 
 void mem_cgroup_migrate(struct page *oldpage, struct page *newpage);
 
<span class="p_del">-struct lruvec *mem_cgroup_lruvec(struct pglist_data *, struct zone *zone,</span>
<span class="p_del">-				 struct mem_cgroup *);</span>
<span class="p_add">+struct lruvec *mem_cgroup_lruvec(struct pglist_data *, struct mem_cgroup *);</span>
 struct lruvec *mem_cgroup_page_lruvec(struct page *, struct pglist_data *);
 
 bool task_in_mem_cgroup(struct task_struct *task, struct mem_cgroup *memcg);
<span class="p_chunk">@@ -410,9 +405,9 @@</span> <span class="p_context"> unsigned long mem_cgroup_node_nr_lru_pages(struct mem_cgroup *memcg,</span>
 static inline
 unsigned long mem_cgroup_get_lru_size(struct lruvec *lruvec, enum lru_list lru)
 {
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 
<span class="p_del">-	mz = container_of(lruvec, struct mem_cgroup_per_zone, lruvec);</span>
<span class="p_add">+	mz = container_of(lruvec, struct mem_cgroup_per_node, lruvec);</span>
 	return mz-&gt;lru_size[lru];
 }
 
<span class="p_chunk">@@ -483,7 +478,7 @@</span> <span class="p_context"> static inline void mem_cgroup_dec_page_stat(struct page *page,</span>
 	mem_cgroup_update_page_stat(page, idx, -1);
 }
 
<span class="p_del">-unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
<span class="p_add">+unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,</span>
 						gfp_t gfp_mask,
 						unsigned long *total_scanned);
 
<span class="p_chunk">@@ -575,7 +570,7 @@</span> <span class="p_context"> static inline void mem_cgroup_migrate(struct page *old, struct page *new)</span>
 }
 
 static inline struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,
<span class="p_del">-				struct zone *zone, struct mem_cgroup *memcg)</span>
<span class="p_add">+				struct mem_cgroup *memcg)</span>
 {
 	return node_lruvec(pgdat);
 }
<span class="p_chunk">@@ -687,7 +682,7 @@</span> <span class="p_context"> static inline void mem_cgroup_dec_page_stat(struct page *page,</span>
 }
 
 static inline
<span class="p_del">-unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
<span class="p_add">+unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,</span>
 					    gfp_t gfp_mask,
 					    unsigned long *total_scanned)
 {
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index 0ad616d7c381..2a23ddc96edd 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -318,7 +318,7 @@</span> <span class="p_context"> extern unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,</span>
 						  bool may_swap);
 extern unsigned long mem_cgroup_shrink_node(struct mem_cgroup *mem,
 						gfp_t gfp_mask, bool noswap,
<span class="p_del">-						struct zone *zone,</span>
<span class="p_add">+						pg_data_t *pgdat,</span>
 						unsigned long *nr_scanned);
 extern unsigned long shrink_all_memory(unsigned long nr_pages);
 extern int vm_swappiness;
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index aac5fae56ea4..b09a17e4f2ff 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -132,15 +132,11 @@</span> <span class="p_context"> static const char * const mem_cgroup_lru_names[] = {</span>
  * their hierarchy representation
  */
 
<span class="p_del">-struct mem_cgroup_tree_per_zone {</span>
<span class="p_add">+struct mem_cgroup_tree_per_node {</span>
 	struct rb_root rb_root;
 	spinlock_t lock;
 };
 
<span class="p_del">-struct mem_cgroup_tree_per_node {</span>
<span class="p_del">-	struct mem_cgroup_tree_per_zone rb_tree_per_zone[MAX_NR_ZONES];</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 struct mem_cgroup_tree {
 	struct mem_cgroup_tree_per_node *rb_tree_per_node[MAX_NUMNODES];
 };
<span class="p_chunk">@@ -323,13 +319,10 @@</span> <span class="p_context"> EXPORT_SYMBOL(memcg_kmem_enabled_key);</span>
 
 #endif /* !CONFIG_SLOB */
 
<span class="p_del">-static struct mem_cgroup_per_zone *</span>
<span class="p_del">-mem_cgroup_zone_zoneinfo(struct mem_cgroup *memcg, struct zone *zone)</span>
<span class="p_add">+static struct mem_cgroup_per_node *</span>
<span class="p_add">+mem_cgroup_nodeinfo(struct mem_cgroup *memcg, int nid)</span>
 {
<span class="p_del">-	int nid = zone_to_nid(zone);</span>
<span class="p_del">-	int zid = zone_idx(zone);</span>
<span class="p_del">-</span>
<span class="p_del">-	return &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_add">+	return memcg-&gt;nodeinfo[nid];</span>
 }
 
 /**
<span class="p_chunk">@@ -383,37 +376,35 @@</span> <span class="p_context"> ino_t page_cgroup_ino(struct page *page)</span>
 	return ino;
 }
 
<span class="p_del">-static struct mem_cgroup_per_zone *</span>
<span class="p_del">-mem_cgroup_page_zoneinfo(struct mem_cgroup *memcg, struct page *page)</span>
<span class="p_add">+static struct mem_cgroup_per_node *</span>
<span class="p_add">+mem_cgroup_page_nodeinfo(struct mem_cgroup *memcg, struct page *page)</span>
 {
 	int nid = page_to_nid(page);
<span class="p_del">-	int zid = page_zonenum(page);</span>
 
<span class="p_del">-	return &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_add">+	return memcg-&gt;nodeinfo[nid];</span>
 }
 
<span class="p_del">-static struct mem_cgroup_tree_per_zone *</span>
<span class="p_del">-soft_limit_tree_node_zone(int nid, int zid)</span>
<span class="p_add">+static struct mem_cgroup_tree_per_node *</span>
<span class="p_add">+soft_limit_tree_node(int nid)</span>
 {
<span class="p_del">-	return &amp;soft_limit_tree.rb_tree_per_node[nid]-&gt;rb_tree_per_zone[zid];</span>
<span class="p_add">+	return soft_limit_tree.rb_tree_per_node[nid];</span>
 }
 
<span class="p_del">-static struct mem_cgroup_tree_per_zone *</span>
<span class="p_add">+static struct mem_cgroup_tree_per_node *</span>
 soft_limit_tree_from_page(struct page *page)
 {
 	int nid = page_to_nid(page);
<span class="p_del">-	int zid = page_zonenum(page);</span>
 
<span class="p_del">-	return &amp;soft_limit_tree.rb_tree_per_node[nid]-&gt;rb_tree_per_zone[zid];</span>
<span class="p_add">+	return soft_limit_tree.rb_tree_per_node[nid];</span>
 }
 
<span class="p_del">-static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="p_del">-					 struct mem_cgroup_tree_per_zone *mctz,</span>
<span class="p_add">+static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_node *mz,</span>
<span class="p_add">+					 struct mem_cgroup_tree_per_node *mctz,</span>
 					 unsigned long new_usage_in_excess)
 {
 	struct rb_node **p = &amp;mctz-&gt;rb_root.rb_node;
 	struct rb_node *parent = NULL;
<span class="p_del">-	struct mem_cgroup_per_zone *mz_node;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz_node;</span>
 
 	if (mz-&gt;on_tree)
 		return;
<span class="p_chunk">@@ -423,7 +414,7 @@</span> <span class="p_context"> static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_zone *mz,</span>
 		return;
 	while (*p) {
 		parent = *p;
<span class="p_del">-		mz_node = rb_entry(parent, struct mem_cgroup_per_zone,</span>
<span class="p_add">+		mz_node = rb_entry(parent, struct mem_cgroup_per_node,</span>
 					tree_node);
 		if (mz-&gt;usage_in_excess &lt; mz_node-&gt;usage_in_excess)
 			p = &amp;(*p)-&gt;rb_left;
<span class="p_chunk">@@ -439,8 +430,8 @@</span> <span class="p_context"> static void __mem_cgroup_insert_exceeded(struct mem_cgroup_per_zone *mz,</span>
 	mz-&gt;on_tree = true;
 }
 
<span class="p_del">-static void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="p_del">-					 struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="p_add">+static void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_node *mz,</span>
<span class="p_add">+					 struct mem_cgroup_tree_per_node *mctz)</span>
 {
 	if (!mz-&gt;on_tree)
 		return;
<span class="p_chunk">@@ -448,8 +439,8 @@</span> <span class="p_context"> static void __mem_cgroup_remove_exceeded(struct mem_cgroup_per_zone *mz,</span>
 	mz-&gt;on_tree = false;
 }
 
<span class="p_del">-static void mem_cgroup_remove_exceeded(struct mem_cgroup_per_zone *mz,</span>
<span class="p_del">-				       struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="p_add">+static void mem_cgroup_remove_exceeded(struct mem_cgroup_per_node *mz,</span>
<span class="p_add">+				       struct mem_cgroup_tree_per_node *mctz)</span>
 {
 	unsigned long flags;
 
<span class="p_chunk">@@ -473,8 +464,8 @@</span> <span class="p_context"> static unsigned long soft_limit_excess(struct mem_cgroup *memcg)</span>
 static void mem_cgroup_update_tree(struct mem_cgroup *memcg, struct page *page)
 {
 	unsigned long excess;
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_del">-	struct mem_cgroup_tree_per_zone *mctz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
<span class="p_add">+	struct mem_cgroup_tree_per_node *mctz;</span>
 
 	mctz = soft_limit_tree_from_page(page);
 	/*
<span class="p_chunk">@@ -482,7 +473,7 @@</span> <span class="p_context"> static void mem_cgroup_update_tree(struct mem_cgroup *memcg, struct page *page)</span>
 	 * because their event counter is not touched.
 	 */
 	for (; memcg; memcg = parent_mem_cgroup(memcg)) {
<span class="p_del">-		mz = mem_cgroup_page_zoneinfo(memcg, page);</span>
<span class="p_add">+		mz = mem_cgroup_page_nodeinfo(memcg, page);</span>
 		excess = soft_limit_excess(memcg);
 		/*
 		 * We have to update the tree if mz is on RB-tree or
<span class="p_chunk">@@ -507,24 +498,22 @@</span> <span class="p_context"> static void mem_cgroup_update_tree(struct mem_cgroup *memcg, struct page *page)</span>
 
 static void mem_cgroup_remove_from_trees(struct mem_cgroup *memcg)
 {
<span class="p_del">-	struct mem_cgroup_tree_per_zone *mctz;</span>
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_del">-	int nid, zid;</span>
<span class="p_add">+	struct mem_cgroup_tree_per_node *mctz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
<span class="p_add">+	int nid;</span>
 
 	for_each_node(nid) {
<span class="p_del">-		for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="p_del">-			mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_del">-			mctz = soft_limit_tree_node_zone(nid, zid);</span>
<span class="p_del">-			mem_cgroup_remove_exceeded(mz, mctz);</span>
<span class="p_del">-		}</span>
<span class="p_add">+		mz = mem_cgroup_nodeinfo(memcg, nid);</span>
<span class="p_add">+		mctz = soft_limit_tree_node(nid);</span>
<span class="p_add">+		mem_cgroup_remove_exceeded(mz, mctz);</span>
 	}
 }
 
<span class="p_del">-static struct mem_cgroup_per_zone *</span>
<span class="p_del">-__mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="p_add">+static struct mem_cgroup_per_node *</span>
<span class="p_add">+__mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_node *mctz)</span>
 {
 	struct rb_node *rightmost = NULL;
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 
 retry:
 	mz = NULL;
<span class="p_chunk">@@ -532,7 +521,7 @@</span> <span class="p_context"> __mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
 	if (!rightmost)
 		goto done;		/* Nothing to reclaim from */
 
<span class="p_del">-	mz = rb_entry(rightmost, struct mem_cgroup_per_zone, tree_node);</span>
<span class="p_add">+	mz = rb_entry(rightmost, struct mem_cgroup_per_node, tree_node);</span>
 	/*
 	 * Remove the node now but someone else can add it back,
 	 * we will to add it back at the end of reclaim to its correct
<span class="p_chunk">@@ -546,10 +535,10 @@</span> <span class="p_context"> __mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
 	return mz;
 }
 
<span class="p_del">-static struct mem_cgroup_per_zone *</span>
<span class="p_del">-mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_zone *mctz)</span>
<span class="p_add">+static struct mem_cgroup_per_node *</span>
<span class="p_add">+mem_cgroup_largest_soft_limit_node(struct mem_cgroup_tree_per_node *mctz)</span>
 {
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 
 	spin_lock_irq(&amp;mctz-&gt;lock);
 	mz = __mem_cgroup_largest_soft_limit_node(mctz);
<span class="p_chunk">@@ -643,20 +632,16 @@</span> <span class="p_context"> unsigned long mem_cgroup_node_nr_lru_pages(struct mem_cgroup *memcg,</span>
 					   int nid, unsigned int lru_mask)
 {
 	unsigned long nr = 0;
<span class="p_del">-	int zid;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
<span class="p_add">+	enum lru_list lru;</span>
 
 	VM_BUG_ON((unsigned)nid &gt;= nr_node_ids);
 
<span class="p_del">-	for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="p_del">-		struct mem_cgroup_per_zone *mz;</span>
<span class="p_del">-		enum lru_list lru;</span>
<span class="p_del">-</span>
<span class="p_del">-		for_each_lru(lru) {</span>
<span class="p_del">-			if (!(BIT(lru) &amp; lru_mask))</span>
<span class="p_del">-				continue;</span>
<span class="p_del">-			mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_del">-			nr += mz-&gt;lru_size[lru];</span>
<span class="p_del">-		}</span>
<span class="p_add">+	for_each_lru(lru) {</span>
<span class="p_add">+		if (!(BIT(lru) &amp; lru_mask))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		mz = mem_cgroup_nodeinfo(memcg, nid);</span>
<span class="p_add">+		nr += mz-&gt;lru_size[lru];</span>
 	}
 	return nr;
 }
<span class="p_chunk">@@ -809,9 +794,9 @@</span> <span class="p_context"> struct mem_cgroup *mem_cgroup_iter(struct mem_cgroup *root,</span>
 	rcu_read_lock();
 
 	if (reclaim) {
<span class="p_del">-		struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+		struct mem_cgroup_per_node *mz;</span>
 
<span class="p_del">-		mz = mem_cgroup_zone_zoneinfo(root, reclaim-&gt;zone);</span>
<span class="p_add">+		mz = mem_cgroup_nodeinfo(root, reclaim-&gt;pgdat-&gt;node_id);</span>
 		iter = &amp;mz-&gt;iter[reclaim-&gt;priority];
 
 		if (prev &amp;&amp; reclaim-&gt;generation != iter-&gt;generation)
<span class="p_chunk">@@ -910,19 +895,17 @@</span> <span class="p_context"> static void invalidate_reclaim_iterators(struct mem_cgroup *dead_memcg)</span>
 {
 	struct mem_cgroup *memcg = dead_memcg;
 	struct mem_cgroup_reclaim_iter *iter;
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_del">-	int nid, zid;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
<span class="p_add">+	int nid;</span>
 	int i;
 
 	while ((memcg = parent_mem_cgroup(memcg))) {
 		for_each_node(nid) {
<span class="p_del">-			for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="p_del">-				mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_del">-				for (i = 0; i &lt;= DEF_PRIORITY; i++) {</span>
<span class="p_del">-					iter = &amp;mz-&gt;iter[i];</span>
<span class="p_del">-					cmpxchg(&amp;iter-&gt;position,</span>
<span class="p_del">-						dead_memcg, NULL);</span>
<span class="p_del">-				}</span>
<span class="p_add">+			mz = mem_cgroup_nodeinfo(memcg, nid);</span>
<span class="p_add">+			for (i = 0; i &lt;= DEF_PRIORITY; i++) {</span>
<span class="p_add">+				iter = &amp;mz-&gt;iter[i];</span>
<span class="p_add">+				cmpxchg(&amp;iter-&gt;position,</span>
<span class="p_add">+					dead_memcg, NULL);</span>
 			}
 		}
 	}
<span class="p_chunk">@@ -946,7 +929,6 @@</span> <span class="p_context"> static void invalidate_reclaim_iterators(struct mem_cgroup *dead_memcg)</span>
 /**
  * mem_cgroup_lruvec - get the lru list vector for a node or a memcg zone
  * @node: node of the wanted lruvec
<span class="p_del">- * @zone: zone of the wanted lruvec</span>
  * @memcg: memcg of the wanted lruvec
  *
  * Returns the lru list vector holding pages for a given @node or a given
<span class="p_chunk">@@ -954,9 +936,9 @@</span> <span class="p_context"> static void invalidate_reclaim_iterators(struct mem_cgroup *dead_memcg)</span>
  * is disabled.
  */
 struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,
<span class="p_del">-				 struct zone *zone, struct mem_cgroup *memcg)</span>
<span class="p_add">+				 struct mem_cgroup *memcg)</span>
 {
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 	struct lruvec *lruvec;
 
 	if (mem_cgroup_disabled()) {
<span class="p_chunk">@@ -964,7 +946,7 @@</span> <span class="p_context"> struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
 		goto out;
 	}
 
<span class="p_del">-	mz = mem_cgroup_zone_zoneinfo(memcg, zone);</span>
<span class="p_add">+	mz = mem_cgroup_nodeinfo(memcg, pgdat-&gt;node_id);</span>
 	lruvec = &amp;mz-&gt;lruvec;
 out:
 	/*
<span class="p_chunk">@@ -972,8 +954,8 @@</span> <span class="p_context"> struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
 	 * we have to be prepared to initialize lruvec-&gt;zone here;
 	 * and if offlined then reonlined, we need to reinitialize it.
 	 */
<span class="p_del">-	if (unlikely(lruvec-&gt;pgdat != zone-&gt;zone_pgdat))</span>
<span class="p_del">-		lruvec-&gt;pgdat = zone-&gt;zone_pgdat;</span>
<span class="p_add">+	if (unlikely(lruvec-&gt;pgdat != pgdat))</span>
<span class="p_add">+		lruvec-&gt;pgdat = pgdat;</span>
 	return lruvec;
 }
 
<span class="p_chunk">@@ -988,7 +970,7 @@</span> <span class="p_context"> struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
  */
 struct lruvec *mem_cgroup_page_lruvec(struct page *page, struct pglist_data *pgdat)
 {
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 	struct mem_cgroup *memcg;
 	struct lruvec *lruvec;
 
<span class="p_chunk">@@ -1005,7 +987,7 @@</span> <span class="p_context"> struct lruvec *mem_cgroup_page_lruvec(struct page *page, struct pglist_data *pgd</span>
 	if (!memcg)
 		memcg = root_mem_cgroup;
 
<span class="p_del">-	mz = mem_cgroup_page_zoneinfo(memcg, page);</span>
<span class="p_add">+	mz = mem_cgroup_page_nodeinfo(memcg, page);</span>
 	lruvec = &amp;mz-&gt;lruvec;
 out:
 	/*
<span class="p_chunk">@@ -1032,7 +1014,7 @@</span> <span class="p_context"> struct lruvec *mem_cgroup_page_lruvec(struct page *page, struct pglist_data *pgd</span>
 void mem_cgroup_update_lru_size(struct lruvec *lruvec, enum lru_list lru,
 				enum zone_type zid, int nr_pages)
 {
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz;</span>
 	unsigned long *lru_size;
 	long size;
 	bool empty;
<span class="p_chunk">@@ -1042,7 +1024,7 @@</span> <span class="p_context"> void mem_cgroup_update_lru_size(struct lruvec *lruvec, enum lru_list lru,</span>
 	if (mem_cgroup_disabled())
 		return;
 
<span class="p_del">-	mz = container_of(lruvec, struct mem_cgroup_per_zone, lruvec);</span>
<span class="p_add">+	mz = container_of(lruvec, struct mem_cgroup_per_node, lruvec);</span>
 	lru_size = mz-&gt;lru_size + lru;
 	empty = list_empty(lruvec-&gt;lists + lru);
 
<span class="p_chunk">@@ -1435,7 +1417,7 @@</span> <span class="p_context"> int mem_cgroup_select_victim_node(struct mem_cgroup *memcg)</span>
 #endif
 
 static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,
<span class="p_del">-				   struct zone *zone,</span>
<span class="p_add">+				   pg_data_t *pgdat,</span>
 				   gfp_t gfp_mask,
 				   unsigned long *total_scanned)
 {
<span class="p_chunk">@@ -1445,7 +1427,7 @@</span> <span class="p_context"> static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,</span>
 	unsigned long excess;
 	unsigned long nr_scanned;
 	struct mem_cgroup_reclaim_cookie reclaim = {
<span class="p_del">-		.zone = zone,</span>
<span class="p_add">+		.pgdat = pgdat,</span>
 		.priority = 0,
 	};
 
<span class="p_chunk">@@ -1476,7 +1458,7 @@</span> <span class="p_context"> static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,</span>
 			continue;
 		}
 		total += mem_cgroup_shrink_node(victim, gfp_mask, false,
<span class="p_del">-					zone, &amp;nr_scanned);</span>
<span class="p_add">+					pgdat, &amp;nr_scanned);</span>
 		*total_scanned += nr_scanned;
 		if (!soft_limit_excess(root_memcg))
 			break;
<span class="p_chunk">@@ -2603,22 +2585,22 @@</span> <span class="p_context"> static int mem_cgroup_resize_memsw_limit(struct mem_cgroup *memcg,</span>
 	return ret;
 }
 
<span class="p_del">-unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
<span class="p_add">+unsigned long mem_cgroup_soft_limit_reclaim(pg_data_t *pgdat, int order,</span>
 					    gfp_t gfp_mask,
 					    unsigned long *total_scanned)
 {
 	unsigned long nr_reclaimed = 0;
<span class="p_del">-	struct mem_cgroup_per_zone *mz, *next_mz = NULL;</span>
<span class="p_add">+	struct mem_cgroup_per_node *mz, *next_mz = NULL;</span>
 	unsigned long reclaimed;
 	int loop = 0;
<span class="p_del">-	struct mem_cgroup_tree_per_zone *mctz;</span>
<span class="p_add">+	struct mem_cgroup_tree_per_node *mctz;</span>
 	unsigned long excess;
 	unsigned long nr_scanned;
 
 	if (order &gt; 0)
 		return 0;
 
<span class="p_del">-	mctz = soft_limit_tree_node_zone(zone_to_nid(zone), zone_idx(zone));</span>
<span class="p_add">+	mctz = soft_limit_tree_node(pgdat-&gt;node_id);</span>
 	/*
 	 * This loop can run a while, specially if mem_cgroup&#39;s continuously
 	 * keep exceeding their soft limit and putting the system under
<span class="p_chunk">@@ -2633,7 +2615,7 @@</span> <span class="p_context"> unsigned long mem_cgroup_soft_limit_reclaim(struct zone *zone, int order,</span>
 			break;
 
 		nr_scanned = 0;
<span class="p_del">-		reclaimed = mem_cgroup_soft_reclaim(mz-&gt;memcg, zone,</span>
<span class="p_add">+		reclaimed = mem_cgroup_soft_reclaim(mz-&gt;memcg, pgdat,</span>
 						    gfp_mask, &amp;nr_scanned);
 		nr_reclaimed += reclaimed;
 		*total_scanned += nr_scanned;
<span class="p_chunk">@@ -3254,22 +3236,21 @@</span> <span class="p_context"> static int memcg_stat_show(struct seq_file *m, void *v)</span>
 
 #ifdef CONFIG_DEBUG_VM
 	{
<span class="p_del">-		int nid, zid;</span>
<span class="p_del">-		struct mem_cgroup_per_zone *mz;</span>
<span class="p_add">+		pg_data_t *pgdat;</span>
<span class="p_add">+		struct mem_cgroup_per_node *mz;</span>
 		struct zone_reclaim_stat *rstat;
 		unsigned long recent_rotated[2] = {0, 0};
 		unsigned long recent_scanned[2] = {0, 0};
 
<span class="p_del">-		for_each_online_node(nid)</span>
<span class="p_del">-			for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {</span>
<span class="p_del">-				mz = &amp;memcg-&gt;nodeinfo[nid]-&gt;zoneinfo[zid];</span>
<span class="p_del">-				rstat = &amp;mz-&gt;lruvec.reclaim_stat;</span>
<span class="p_add">+		for_each_online_pgdat(pgdat) {</span>
<span class="p_add">+			mz = mem_cgroup_nodeinfo(memcg, pgdat-&gt;node_id);</span>
<span class="p_add">+			rstat = &amp;mz-&gt;lruvec.reclaim_stat;</span>
 
<span class="p_del">-				recent_rotated[0] += rstat-&gt;recent_rotated[0];</span>
<span class="p_del">-				recent_rotated[1] += rstat-&gt;recent_rotated[1];</span>
<span class="p_del">-				recent_scanned[0] += rstat-&gt;recent_scanned[0];</span>
<span class="p_del">-				recent_scanned[1] += rstat-&gt;recent_scanned[1];</span>
<span class="p_del">-			}</span>
<span class="p_add">+			recent_rotated[0] += rstat-&gt;recent_rotated[0];</span>
<span class="p_add">+			recent_rotated[1] += rstat-&gt;recent_rotated[1];</span>
<span class="p_add">+			recent_scanned[0] += rstat-&gt;recent_scanned[0];</span>
<span class="p_add">+			recent_scanned[1] += rstat-&gt;recent_scanned[1];</span>
<span class="p_add">+		}</span>
 		seq_printf(m, &quot;recent_rotated_anon %lu\n&quot;, recent_rotated[0]);
 		seq_printf(m, &quot;recent_rotated_file %lu\n&quot;, recent_rotated[1]);
 		seq_printf(m, &quot;recent_scanned_anon %lu\n&quot;, recent_scanned[0]);
<span class="p_chunk">@@ -4095,11 +4076,10 @@</span> <span class="p_context"> static struct cftype mem_cgroup_legacy_files[] = {</span>
 	{ },	/* terminate */
 };
 
<span class="p_del">-static int alloc_mem_cgroup_per_zone_info(struct mem_cgroup *memcg, int node)</span>
<span class="p_add">+static int alloc_mem_cgroup_per_node_info(struct mem_cgroup *memcg, int node)</span>
 {
 	struct mem_cgroup_per_node *pn;
<span class="p_del">-	struct mem_cgroup_per_zone *mz;</span>
<span class="p_del">-	int zone, tmp = node;</span>
<span class="p_add">+	int tmp = node;</span>
 	/*
 	 * This routine is called against possible nodes.
 	 * But it&#39;s BUG to call kmalloc() against offline node.
<span class="p_chunk">@@ -4114,18 +4094,16 @@</span> <span class="p_context"> static int alloc_mem_cgroup_per_zone_info(struct mem_cgroup *memcg, int node)</span>
 	if (!pn)
 		return 1;
 
<span class="p_del">-	for (zone = 0; zone &lt; MAX_NR_ZONES; zone++) {</span>
<span class="p_del">-		mz = &amp;pn-&gt;zoneinfo[zone];</span>
<span class="p_del">-		lruvec_init(&amp;mz-&gt;lruvec);</span>
<span class="p_del">-		mz-&gt;usage_in_excess = 0;</span>
<span class="p_del">-		mz-&gt;on_tree = false;</span>
<span class="p_del">-		mz-&gt;memcg = memcg;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	lruvec_init(&amp;pn-&gt;lruvec);</span>
<span class="p_add">+	pn-&gt;usage_in_excess = 0;</span>
<span class="p_add">+	pn-&gt;on_tree = false;</span>
<span class="p_add">+	pn-&gt;memcg = memcg;</span>
<span class="p_add">+</span>
 	memcg-&gt;nodeinfo[node] = pn;
 	return 0;
 }
 
<span class="p_del">-static void free_mem_cgroup_per_zone_info(struct mem_cgroup *memcg, int node)</span>
<span class="p_add">+static void free_mem_cgroup_per_node_info(struct mem_cgroup *memcg, int node)</span>
 {
 	kfree(memcg-&gt;nodeinfo[node]);
 }
<span class="p_chunk">@@ -4136,7 +4114,7 @@</span> <span class="p_context"> static void mem_cgroup_free(struct mem_cgroup *memcg)</span>
 
 	memcg_wb_domain_exit(memcg);
 	for_each_node(node)
<span class="p_del">-		free_mem_cgroup_per_zone_info(memcg, node);</span>
<span class="p_add">+		free_mem_cgroup_per_node_info(memcg, node);</span>
 	free_percpu(memcg-&gt;stat);
 	kfree(memcg);
 }
<span class="p_chunk">@@ -4159,7 +4137,7 @@</span> <span class="p_context"> static struct mem_cgroup *mem_cgroup_alloc(void)</span>
 		goto fail;
 
 	for_each_node(node)
<span class="p_del">-		if (alloc_mem_cgroup_per_zone_info(memcg, node))</span>
<span class="p_add">+		if (alloc_mem_cgroup_per_node_info(memcg, node))</span>
 			goto fail;
 
 	if (memcg_wb_domain_init(memcg, GFP_KERNEL))
<span class="p_chunk">@@ -5757,18 +5735,12 @@</span> <span class="p_context"> static int __init mem_cgroup_init(void)</span>
 
 	for_each_node(node) {
 		struct mem_cgroup_tree_per_node *rtpn;
<span class="p_del">-		int zone;</span>
 
 		rtpn = kzalloc_node(sizeof(*rtpn), GFP_KERNEL,
 				    node_online(node) ? node : NUMA_NO_NODE);
 
<span class="p_del">-		for (zone = 0; zone &lt; MAX_NR_ZONES; zone++) {</span>
<span class="p_del">-			struct mem_cgroup_tree_per_zone *rtpz;</span>
<span class="p_del">-</span>
<span class="p_del">-			rtpz = &amp;rtpn-&gt;rb_tree_per_zone[zone];</span>
<span class="p_del">-			rtpz-&gt;rb_root = RB_ROOT;</span>
<span class="p_del">-			spin_lock_init(&amp;rtpz-&gt;lock);</span>
<span class="p_del">-		}</span>
<span class="p_add">+		rtpn-&gt;rb_root = RB_ROOT;</span>
<span class="p_add">+		spin_lock_init(&amp;rtpn-&gt;lock);</span>
 		soft_limit_tree.rb_tree_per_node[node] = rtpn;
 	}
 
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 3774ebf19f63..cf73bf4ebd06 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -2223,8 +2223,7 @@</span> <span class="p_context"> static inline void init_tlb_ubc(void)</span>
 static void shrink_node_memcg(struct pglist_data *pgdat, struct mem_cgroup *memcg,
 			      struct scan_control *sc, unsigned long *lru_pages)
 {
<span class="p_del">-	struct zone *zone = &amp;pgdat-&gt;node_zones[sc-&gt;reclaim_idx];</span>
<span class="p_del">-	struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, zone, memcg);</span>
<span class="p_add">+	struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, memcg);</span>
 	unsigned long nr[NR_LRU_LISTS];
 	unsigned long targets[NR_LRU_LISTS];
 	unsigned long nr_to_scan;
<span class="p_chunk">@@ -2431,7 +2430,7 @@</span> <span class="p_context"> static bool shrink_node(pg_data_t *pgdat, struct scan_control *sc,</span>
 	do {
 		struct mem_cgroup *root = sc-&gt;target_mem_cgroup;
 		struct mem_cgroup_reclaim_cookie reclaim = {
<span class="p_del">-			.zone = &amp;pgdat-&gt;node_zones[classzone_idx],</span>
<span class="p_add">+			.pgdat = pgdat,</span>
 			.priority = sc-&gt;priority,
 		};
 		unsigned long node_lru_pages = 0;
<span class="p_chunk">@@ -2638,7 +2637,7 @@</span> <span class="p_context"> static void shrink_zones(struct zonelist *zonelist, struct scan_control *sc,</span>
 			 * and balancing, not for a memcg&#39;s limit.
 			 */
 			nr_soft_scanned = 0;
<span class="p_del">-			nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone,</span>
<span class="p_add">+			nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone-&gt;zone_pgdat,</span>
 						sc-&gt;order, sc-&gt;gfp_mask,
 						&amp;nr_soft_scanned);
 			sc-&gt;nr_reclaimed += nr_soft_reclaimed;
<span class="p_chunk">@@ -2905,7 +2904,7 @@</span> <span class="p_context"> unsigned long try_to_free_pages(struct zonelist *zonelist, int order,</span>
 
 unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,
 						gfp_t gfp_mask, bool noswap,
<span class="p_del">-						struct zone *zone,</span>
<span class="p_add">+						pg_data_t *pgdat,</span>
 						unsigned long *nr_scanned)
 {
 	struct scan_control sc = {
<span class="p_chunk">@@ -2913,7 +2912,7 @@</span> <span class="p_context"> unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,</span>
 		.target_mem_cgroup = memcg,
 		.may_writepage = !laptop_mode,
 		.may_unmap = 1,
<span class="p_del">-		.reclaim_idx = zone_idx(zone),</span>
<span class="p_add">+		.reclaim_idx = MAX_NR_ZONES - 1,</span>
 		.may_swap = !noswap,
 	};
 	unsigned long lru_pages;
<span class="p_chunk">@@ -2932,7 +2931,7 @@</span> <span class="p_context"> unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,</span>
 	 * will pick up pages from other mem cgroup&#39;s as well. We hack
 	 * the priority and make it zero.
 	 */
<span class="p_del">-	shrink_node_memcg(zone-&gt;zone_pgdat, memcg, &amp;sc, &amp;lru_pages);</span>
<span class="p_add">+	shrink_node_memcg(pgdat, memcg, &amp;sc, &amp;lru_pages);</span>
 
 	trace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);
 
<span class="p_chunk">@@ -2952,6 +2951,7 @@</span> <span class="p_context"> unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,</span>
 		.nr_to_reclaim = max(nr_pages, SWAP_CLUSTER_MAX),
 		.gfp_mask = (gfp_mask &amp; GFP_RECLAIM_MASK) |
 				(GFP_HIGHUSER_MOVABLE &amp; ~GFP_RECLAIM_MASK),
<span class="p_add">+		.reclaim_idx = MAX_NR_ZONES - 1,</span>
 		.target_mem_cgroup = memcg,
 		.priority = DEF_PRIORITY,
 		.may_writepage = !laptop_mode,
<span class="p_chunk">@@ -2981,7 +2981,7 @@</span> <span class="p_context"> unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,</span>
 #endif
 
 static void age_active_anon(struct pglist_data *pgdat,
<span class="p_del">-				struct zone *zone, struct scan_control *sc)</span>
<span class="p_add">+				struct scan_control *sc)</span>
 {
 	struct mem_cgroup *memcg;
 
<span class="p_chunk">@@ -2990,7 +2990,7 @@</span> <span class="p_context"> static void age_active_anon(struct pglist_data *pgdat,</span>
 
 	memcg = mem_cgroup_iter(NULL, NULL, NULL);
 	do {
<span class="p_del">-		struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, zone, memcg);</span>
<span class="p_add">+		struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, memcg);</span>
 
 		if (inactive_list_is_low(lruvec, false))
 			shrink_active_list(SWAP_CLUSTER_MAX, lruvec,
<span class="p_chunk">@@ -3178,7 +3178,7 @@</span> <span class="p_context"> static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)</span>
 		 * pages are rotated regardless of classzone as this is
 		 * about consistent aging.
 		 */
<span class="p_del">-		age_active_anon(pgdat, &amp;pgdat-&gt;node_zones[MAX_NR_ZONES - 1], &amp;sc);</span>
<span class="p_add">+		age_active_anon(pgdat, &amp;sc);</span>
 
 		/*
 		 * If we&#39;re getting trouble reclaiming, start doing writepage
<span class="p_chunk">@@ -3190,7 +3190,7 @@</span> <span class="p_context"> static int balance_pgdat(pg_data_t *pgdat, int order, int classzone_idx)</span>
 		/* Call soft limit reclaim before calling shrink_node. */
 		sc.nr_scanned = 0;
 		nr_soft_scanned = 0;
<span class="p_del">-		nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(zone, sc.order,</span>
<span class="p_add">+		nr_soft_reclaimed = mem_cgroup_soft_limit_reclaim(pgdat, sc.order,</span>
 						sc.gfp_mask, &amp;nr_soft_scanned);
 		sc.nr_reclaimed += nr_soft_reclaimed;
 
<span class="p_header">diff --git a/mm/workingset.c b/mm/workingset.c</span>
<span class="p_header">index 2d81ca11317d..5b479890cc18 100644</span>
<span class="p_header">--- a/mm/workingset.c</span>
<span class="p_header">+++ b/mm/workingset.c</span>
<span class="p_chunk">@@ -218,7 +218,7 @@</span> <span class="p_context"> void *workingset_eviction(struct address_space *mapping, struct page *page)</span>
 	VM_BUG_ON_PAGE(page_count(page), page);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 
<span class="p_del">-	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, zone, memcg);</span>
<span class="p_add">+	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, memcg);</span>
 	eviction = atomic_long_inc_return(&amp;lruvec-&gt;inactive_age);
 	return pack_shadow(memcgid, zone, eviction);
 }
<span class="p_chunk">@@ -267,7 +267,7 @@</span> <span class="p_context"> bool workingset_refault(void *shadow)</span>
 		rcu_read_unlock();
 		return false;
 	}
<span class="p_del">-	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, zone, memcg);</span>
<span class="p_add">+	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, memcg);</span>
 	refault = atomic_long_read(&amp;lruvec-&gt;inactive_age);
 	active_file = lruvec_lru_size(lruvec, LRU_ACTIVE_FILE);
 	rcu_read_unlock();
<span class="p_chunk">@@ -317,7 +317,7 @@</span> <span class="p_context"> void workingset_activation(struct page *page)</span>
 	 */
 	if (!mem_cgroup_disabled() &amp;&amp; !page_memcg(page))
 		goto out;
<span class="p_del">-	lruvec = mem_cgroup_lruvec(page_pgdat(page), page_zone(page), page_memcg(page));</span>
<span class="p_add">+	lruvec = mem_cgroup_lruvec(page_pgdat(page), page_memcg(page));</span>
 	atomic_long_inc(&amp;lruvec-&gt;inactive_age);
 out:
 	unlock_page_memcg(page);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



