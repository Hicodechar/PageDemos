
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv7,02/36] rmap: add argument to charge compound page - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv7,02/36] rmap: add argument to charge compound page</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 23, 2015, 1:46 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1435067206-92901-3-git-send-email-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6661201/mbox/"
   >mbox</a>
|
   <a href="/patch/6661201/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6661201/">/patch/6661201/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 050B19F39B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 23 Jun 2015 13:48:20 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 6ED71203A4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 23 Jun 2015 13:48:17 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 8AD6B2049C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 23 Jun 2015 13:48:15 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933347AbbFWNsG (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 23 Jun 2015 09:48:06 -0400
Received: from mga01.intel.com ([192.55.52.88]:7983 &quot;EHLO mga01.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754741AbbFWNrG (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 23 Jun 2015 09:47:06 -0400
Received: from orsmga001.jf.intel.com ([10.7.209.18])
	by fmsmga101.fm.intel.com with ESMTP; 23 Jun 2015 06:47:04 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.13,665,1427785200&quot;; d=&quot;scan&#39;208&quot;;a=&quot;716165522&quot;
Received: from black.fi.intel.com ([10.237.72.82])
	by orsmga001.jf.intel.com with ESMTP; 23 Jun 2015 06:46:59 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id CBE2F610; Tue, 23 Jun 2015 16:46:58 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;
Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Christoph Lameter &lt;cl@gentwo.org&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Steve Capper &lt;steve.capper@linaro.org&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	Jerome Marchand &lt;jmarchan@redhat.com&gt;,
	Sasha Levin &lt;sasha.levin@oracle.com&gt;,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCHv7 02/36] rmap: add argument to charge compound page
Date: Tue, 23 Jun 2015 16:46:12 +0300
Message-Id: &lt;1435067206-92901-3-git-send-email-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.1.4
In-Reply-To: &lt;1435067206-92901-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
References: &lt;1435067206-92901-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.3 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - June 23, 2015, 1:46 p.m.</div>
<pre class="content">
We&#39;re going to allow mapping of individual 4k pages of THP compound
page. It means we cannot rely on PageTransHuge() check to decide if
map/unmap small page or THP.

The patch adds new argument to rmap functions to indicate whether we want
to operate on whole compound page or only the small page.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="tested-by">Tested-by: Sasha Levin &lt;sasha.levin@oracle.com&gt;</span>
<span class="acked-by">Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
---
 include/linux/rmap.h    | 12 +++++++++---
 kernel/events/uprobes.c |  4 ++--
 mm/huge_memory.c        | 16 ++++++++--------
 mm/hugetlb.c            |  4 ++--
 mm/ksm.c                |  4 ++--
 mm/memory.c             | 14 +++++++-------
 mm/migrate.c            |  8 ++++----
 mm/rmap.c               | 48 +++++++++++++++++++++++++++++++-----------------
 mm/swapfile.c           |  4 ++--
 mm/userfaultfd.c        |  2 +-
 10 files changed, 68 insertions(+), 48 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index bf36b6e644c4..2bc86dc14305 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -159,16 +159,22 @@</span> <span class="p_context"> static inline void anon_vma_merge(struct vm_area_struct *vma,</span>
 
 struct anon_vma *page_get_anon_vma(struct page *page);
 
<span class="p_add">+/* bitflags for do_page_add_anon_rmap() */</span>
<span class="p_add">+#define RMAP_EXCLUSIVE 0x01</span>
<span class="p_add">+#define RMAP_COMPOUND 0x02</span>
<span class="p_add">+</span>
 /*
  * rmap interfaces called when adding or removing pte of page
  */
 void page_move_anon_rmap(struct page *, struct vm_area_struct *, unsigned long);
<span class="p_del">-void page_add_anon_rmap(struct page *, struct vm_area_struct *, unsigned long);</span>
<span class="p_add">+void page_add_anon_rmap(struct page *, struct vm_area_struct *,</span>
<span class="p_add">+		unsigned long, bool);</span>
 void do_page_add_anon_rmap(struct page *, struct vm_area_struct *,
 			   unsigned long, int);
<span class="p_del">-void page_add_new_anon_rmap(struct page *, struct vm_area_struct *, unsigned long);</span>
<span class="p_add">+void page_add_new_anon_rmap(struct page *, struct vm_area_struct *,</span>
<span class="p_add">+		unsigned long, bool);</span>
 void page_add_file_rmap(struct page *);
<span class="p_del">-void page_remove_rmap(struct page *);</span>
<span class="p_add">+void page_remove_rmap(struct page *, bool);</span>
 
 void hugepage_add_anon_rmap(struct page *, struct vm_area_struct *,
 			    unsigned long);
<span class="p_header">diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="p_header">index cb346f26a22d..5523daf59953 100644</span>
<span class="p_header">--- a/kernel/events/uprobes.c</span>
<span class="p_header">+++ b/kernel/events/uprobes.c</span>
<span class="p_chunk">@@ -183,7 +183,7 @@</span> <span class="p_context"> static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
 		goto unlock;
 
 	get_page(kpage);
<span class="p_del">-	page_add_new_anon_rmap(kpage, vma, addr);</span>
<span class="p_add">+	page_add_new_anon_rmap(kpage, vma, addr, false);</span>
 	mem_cgroup_commit_charge(kpage, memcg, false);
 	lru_cache_add_active_or_unevictable(kpage, vma);
 
<span class="p_chunk">@@ -196,7 +196,7 @@</span> <span class="p_context"> static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
 	ptep_clear_flush_notify(vma, addr, ptep);
 	set_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma-&gt;vm_page_prot));
 
<span class="p_del">-	page_remove_rmap(page);</span>
<span class="p_add">+	page_remove_rmap(page, false);</span>
 	if (!page_mapped(page))
 		try_to_free_swap(page);
 	pte_unmap_unlock(ptep, ptl);
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 9671f51e954d..310b0650abe0 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -773,7 +773,7 @@</span> <span class="p_context"> static int __do_huge_pmd_anonymous_page(struct mm_struct *mm,</span>
 
 		entry = mk_huge_pmd(page, vma-&gt;vm_page_prot);
 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
<span class="p_del">-		page_add_new_anon_rmap(page, vma, haddr);</span>
<span class="p_add">+		page_add_new_anon_rmap(page, vma, haddr, true);</span>
 		mem_cgroup_commit_charge(page, memcg, false);
 		lru_cache_add_active_or_unevictable(page, vma);
 		pgtable_trans_huge_deposit(mm, pmd, pgtable);
<span class="p_chunk">@@ -1068,7 +1068,7 @@</span> <span class="p_context"> static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 		memcg = (void *)page_private(pages[i]);
 		set_page_private(pages[i], 0);
<span class="p_del">-		page_add_new_anon_rmap(pages[i], vma, haddr);</span>
<span class="p_add">+		page_add_new_anon_rmap(pages[i], vma, haddr, false);</span>
 		mem_cgroup_commit_charge(pages[i], memcg, false);
 		lru_cache_add_active_or_unevictable(pages[i], vma);
 		pte = pte_offset_map(&amp;_pmd, haddr);
<span class="p_chunk">@@ -1080,7 +1080,7 @@</span> <span class="p_context"> static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
 
 	smp_wmb(); /* make pte visible before pmd */
 	pmd_populate(mm, pmd, pgtable);
<span class="p_del">-	page_remove_rmap(page);</span>
<span class="p_add">+	page_remove_rmap(page, true);</span>
 	spin_unlock(ptl);
 
 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
<span class="p_chunk">@@ -1200,7 +1200,7 @@</span> <span class="p_context"> alloc:</span>
 		entry = mk_huge_pmd(new_page, vma-&gt;vm_page_prot);
 		entry = maybe_pmd_mkwrite(pmd_mkdirty(entry), vma);
 		pmdp_huge_clear_flush_notify(vma, haddr, pmd);
<span class="p_del">-		page_add_new_anon_rmap(new_page, vma, haddr);</span>
<span class="p_add">+		page_add_new_anon_rmap(new_page, vma, haddr, true);</span>
 		mem_cgroup_commit_charge(new_page, memcg, false);
 		lru_cache_add_active_or_unevictable(new_page, vma);
 		set_pmd_at(mm, haddr, pmd, entry);
<span class="p_chunk">@@ -1210,7 +1210,7 @@</span> <span class="p_context"> alloc:</span>
 			put_huge_zero_page();
 		} else {
 			VM_BUG_ON_PAGE(!PageHead(page), page);
<span class="p_del">-			page_remove_rmap(page);</span>
<span class="p_add">+			page_remove_rmap(page, true);</span>
 			put_page(page);
 		}
 		ret |= VM_FAULT_WRITE;
<span class="p_chunk">@@ -1465,7 +1465,7 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 			put_huge_zero_page();
 		} else {
 			page = pmd_page(orig_pmd);
<span class="p_del">-			page_remove_rmap(page);</span>
<span class="p_add">+			page_remove_rmap(page, true);</span>
 			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);
 			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);
 			VM_BUG_ON_PAGE(!PageHead(page), page);
<span class="p_chunk">@@ -2311,7 +2311,7 @@</span> <span class="p_context"> static void __collapse_huge_page_copy(pte_t *pte, struct page *page,</span>
 			 * superfluous.
 			 */
 			pte_clear(vma-&gt;vm_mm, address, _pte);
<span class="p_del">-			page_remove_rmap(src_page);</span>
<span class="p_add">+			page_remove_rmap(src_page, false);</span>
 			spin_unlock(ptl);
 			free_page_and_swap_cache(src_page);
 		}
<span class="p_chunk">@@ -2606,7 +2606,7 @@</span> <span class="p_context"> static void collapse_huge_page(struct mm_struct *mm,</span>
 
 	spin_lock(pmd_ptl);
 	BUG_ON(!pmd_none(*pmd));
<span class="p_del">-	page_add_new_anon_rmap(new_page, vma, address);</span>
<span class="p_add">+	page_add_new_anon_rmap(new_page, vma, address, true);</span>
 	mem_cgroup_commit_charge(new_page, memcg, false);
 	lru_cache_add_active_or_unevictable(new_page, vma);
 	pgtable_trans_huge_deposit(mm, pmd, pgtable);
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index a8c3087089d8..29a8207a79df 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -2877,7 +2877,7 @@</span> <span class="p_context"> again:</span>
 		if (huge_pte_dirty(pte))
 			set_page_dirty(page);
 
<span class="p_del">-		page_remove_rmap(page);</span>
<span class="p_add">+		page_remove_rmap(page, true);</span>
 		force_flush = !__tlb_remove_page(tlb, page);
 		if (force_flush) {
 			address += sz;
<span class="p_chunk">@@ -3098,7 +3098,7 @@</span> <span class="p_context"> retry_avoidcopy:</span>
 		mmu_notifier_invalidate_range(mm, mmun_start, mmun_end);
 		set_huge_pte_at(mm, address, ptep,
 				make_huge_pte(vma, new_page, 1));
<span class="p_del">-		page_remove_rmap(old_page);</span>
<span class="p_add">+		page_remove_rmap(old_page, true);</span>
 		hugepage_add_new_anon_rmap(new_page, vma, address);
 		/* Make the old page be freed below */
 		new_page = old_page;
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index bc7be0ee2080..fe09f3ddc912 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -957,13 +957,13 @@</span> <span class="p_context"> static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
 	}
 
 	get_page(kpage);
<span class="p_del">-	page_add_anon_rmap(kpage, vma, addr);</span>
<span class="p_add">+	page_add_anon_rmap(kpage, vma, addr, false);</span>
 
 	flush_cache_page(vma, addr, pte_pfn(*ptep));
 	ptep_clear_flush_notify(vma, addr, ptep);
 	set_pte_at_notify(mm, addr, ptep, mk_pte(kpage, vma-&gt;vm_page_prot));
 
<span class="p_del">-	page_remove_rmap(page);</span>
<span class="p_add">+	page_remove_rmap(page, false);</span>
 	if (!page_mapped(page))
 		try_to_free_swap(page);
 	put_page(page);
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 8a2fc9945b46..4cd6d9392004 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -1125,7 +1125,7 @@</span> <span class="p_context"> again:</span>
 					mark_page_accessed(page);
 				rss[MM_FILEPAGES]--;
 			}
<span class="p_del">-			page_remove_rmap(page);</span>
<span class="p_add">+			page_remove_rmap(page, false);</span>
 			if (unlikely(page_mapcount(page) &lt; 0))
 				print_bad_pte(vma, addr, ptent, page);
 			if (unlikely(!__tlb_remove_page(tlb, page))) {
<span class="p_chunk">@@ -2113,7 +2113,7 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		 * thread doing COW.
 		 */
 		ptep_clear_flush_notify(vma, address, page_table);
<span class="p_del">-		page_add_new_anon_rmap(new_page, vma, address);</span>
<span class="p_add">+		page_add_new_anon_rmap(new_page, vma, address, false);</span>
 		mem_cgroup_commit_charge(new_page, memcg, false);
 		lru_cache_add_active_or_unevictable(new_page, vma);
 		/*
<span class="p_chunk">@@ -2146,7 +2146,7 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 			 * mapcount is visible. So transitively, TLBs to
 			 * old page will be flushed before it can be reused.
 			 */
<span class="p_del">-			page_remove_rmap(old_page);</span>
<span class="p_add">+			page_remove_rmap(old_page, false);</span>
 		}
 
 		/* Free the old page.. */
<span class="p_chunk">@@ -2562,7 +2562,7 @@</span> <span class="p_context"> static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		pte = maybe_mkwrite(pte_mkdirty(pte), vma);
 		flags &amp;= ~FAULT_FLAG_WRITE;
 		ret |= VM_FAULT_WRITE;
<span class="p_del">-		exclusive = 1;</span>
<span class="p_add">+		exclusive = RMAP_EXCLUSIVE;</span>
 	}
 	flush_icache_page(vma, page);
 	if (pte_swp_soft_dirty(orig_pte))
<span class="p_chunk">@@ -2572,7 +2572,7 @@</span> <span class="p_context"> static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		do_page_add_anon_rmap(page, vma, address, exclusive);
 		mem_cgroup_commit_charge(page, memcg, true);
 	} else { /* ksm created a completely new copy */
<span class="p_del">-		page_add_new_anon_rmap(page, vma, address);</span>
<span class="p_add">+		page_add_new_anon_rmap(page, vma, address, false);</span>
 		mem_cgroup_commit_charge(page, memcg, false);
 		lru_cache_add_active_or_unevictable(page, vma);
 	}
<span class="p_chunk">@@ -2726,7 +2726,7 @@</span> <span class="p_context"> static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	}
 
 	inc_mm_counter_fast(mm, MM_ANONPAGES);
<span class="p_del">-	page_add_new_anon_rmap(page, vma, address);</span>
<span class="p_add">+	page_add_new_anon_rmap(page, vma, address, false);</span>
 	mem_cgroup_commit_charge(page, memcg, false);
 	lru_cache_add_active_or_unevictable(page, vma);
 setpte:
<span class="p_chunk">@@ -2814,7 +2814,7 @@</span> <span class="p_context"> void do_set_pte(struct vm_area_struct *vma, unsigned long address,</span>
 		entry = maybe_mkwrite(pte_mkdirty(entry), vma);
 	if (anon) {
 		inc_mm_counter_fast(vma-&gt;vm_mm, MM_ANONPAGES);
<span class="p_del">-		page_add_new_anon_rmap(page, vma, address);</span>
<span class="p_add">+		page_add_new_anon_rmap(page, vma, address, false);</span>
 	} else {
 		inc_mm_counter_fast(vma-&gt;vm_mm, MM_FILEPAGES);
 		page_add_file_rmap(page);
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 236ee25e79d9..9cabb25fb16e 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -166,7 +166,7 @@</span> <span class="p_context"> static int remove_migration_pte(struct page *new, struct vm_area_struct *vma,</span>
 		else
 			page_dup_rmap(new);
 	} else if (PageAnon(new))
<span class="p_del">-		page_add_anon_rmap(new, vma, addr);</span>
<span class="p_add">+		page_add_anon_rmap(new, vma, addr, false);</span>
 	else
 		page_add_file_rmap(new);
 
<span class="p_chunk">@@ -1798,7 +1798,7 @@</span> <span class="p_context"> fail_putback:</span>
 	 * guarantee the copy is visible before the pagetable update.
 	 */
 	flush_cache_range(vma, mmun_start, mmun_end);
<span class="p_del">-	page_add_anon_rmap(new_page, vma, mmun_start);</span>
<span class="p_add">+	page_add_anon_rmap(new_page, vma, mmun_start, true);</span>
 	pmdp_huge_clear_flush_notify(vma, mmun_start, pmd);
 	set_pmd_at(mm, mmun_start, pmd, entry);
 	flush_tlb_range(vma, mmun_start, mmun_end);
<span class="p_chunk">@@ -1809,13 +1809,13 @@</span> <span class="p_context"> fail_putback:</span>
 		flush_tlb_range(vma, mmun_start, mmun_end);
 		mmu_notifier_invalidate_range(mm, mmun_start, mmun_end);
 		update_mmu_cache_pmd(vma, address, &amp;entry);
<span class="p_del">-		page_remove_rmap(new_page);</span>
<span class="p_add">+		page_remove_rmap(new_page, true);</span>
 		goto fail_putback;
 	}
 
 	mem_cgroup_migrate(page, new_page, false);
 
<span class="p_del">-	page_remove_rmap(page);</span>
<span class="p_add">+	page_remove_rmap(page, true);</span>
 
 	spin_unlock(ptl);
 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 9c045940ed10..51807d605ebf 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1046,6 +1046,7 @@</span> <span class="p_context"> static void __page_check_anon_rmap(struct page *page,</span>
  * @page:	the page to add the mapping to
  * @vma:	the vm area in which the mapping is added
  * @address:	the user virtual address mapped
<span class="p_add">+ * @compound:	charge the page as compound or small page</span>
  *
  * The caller needs to hold the pte lock, and the page must be locked in
  * the anon_vma case: to serialize mapping,index checking after setting,
<span class="p_chunk">@@ -1053,9 +1054,9 @@</span> <span class="p_context"> static void __page_check_anon_rmap(struct page *page,</span>
  * (but PageKsm is never downgraded to PageAnon).
  */
 void page_add_anon_rmap(struct page *page,
<span class="p_del">-	struct vm_area_struct *vma, unsigned long address)</span>
<span class="p_add">+	struct vm_area_struct *vma, unsigned long address, bool compound)</span>
 {
<span class="p_del">-	do_page_add_anon_rmap(page, vma, address, 0);</span>
<span class="p_add">+	do_page_add_anon_rmap(page, vma, address, compound ? RMAP_COMPOUND : 0);</span>
 }
 
 /*
<span class="p_chunk">@@ -1064,21 +1065,24 @@</span> <span class="p_context"> void page_add_anon_rmap(struct page *page,</span>
  * Everybody else should continue to use page_add_anon_rmap above.
  */
 void do_page_add_anon_rmap(struct page *page,
<span class="p_del">-	struct vm_area_struct *vma, unsigned long address, int exclusive)</span>
<span class="p_add">+	struct vm_area_struct *vma, unsigned long address, int flags)</span>
 {
 	int first = atomic_inc_and_test(&amp;page-&gt;_mapcount);
 	if (first) {
<span class="p_add">+		bool compound = flags &amp; RMAP_COMPOUND;</span>
<span class="p_add">+		int nr = compound ? hpage_nr_pages(page) : 1;</span>
 		/*
 		 * We use the irq-unsafe __{inc|mod}_zone_page_stat because
 		 * these counters are not modified in interrupt context, and
 		 * pte lock(a spinlock) is held, which implies preemption
 		 * disabled.
 		 */
<span class="p_del">-		if (PageTransHuge(page))</span>
<span class="p_add">+		if (compound) {</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageTransHuge(page), page);</span>
 			__inc_zone_page_state(page,
 					      NR_ANON_TRANSPARENT_HUGEPAGES);
<span class="p_del">-		__mod_zone_page_state(page_zone(page), NR_ANON_PAGES,</span>
<span class="p_del">-				hpage_nr_pages(page));</span>
<span class="p_add">+		}</span>
<span class="p_add">+		__mod_zone_page_state(page_zone(page), NR_ANON_PAGES, nr);</span>
 	}
 	if (unlikely(PageKsm(page)))
 		return;
<span class="p_chunk">@@ -1086,7 +1090,8 @@</span> <span class="p_context"> void do_page_add_anon_rmap(struct page *page,</span>
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 	/* address might be in next vma when migration races vma_adjust */
 	if (first)
<span class="p_del">-		__page_set_anon_rmap(page, vma, address, exclusive);</span>
<span class="p_add">+		__page_set_anon_rmap(page, vma, address,</span>
<span class="p_add">+				flags &amp; RMAP_EXCLUSIVE);</span>
 	else
 		__page_check_anon_rmap(page, vma, address);
 }
<span class="p_chunk">@@ -1096,21 +1101,25 @@</span> <span class="p_context"> void do_page_add_anon_rmap(struct page *page,</span>
  * @page:	the page to add the mapping to
  * @vma:	the vm area in which the mapping is added
  * @address:	the user virtual address mapped
<span class="p_add">+ * @compound:	charge the page as compound or small page</span>
  *
  * Same as page_add_anon_rmap but must only be called on *new* pages.
  * This means the inc-and-test can be bypassed.
  * Page does not have to be locked.
  */
 void page_add_new_anon_rmap(struct page *page,
<span class="p_del">-	struct vm_area_struct *vma, unsigned long address)</span>
<span class="p_add">+	struct vm_area_struct *vma, unsigned long address, bool compound)</span>
 {
<span class="p_add">+	int nr = compound ? hpage_nr_pages(page) : 1;</span>
<span class="p_add">+</span>
 	VM_BUG_ON_VMA(address &lt; vma-&gt;vm_start || address &gt;= vma-&gt;vm_end, vma);
 	SetPageSwapBacked(page);
 	atomic_set(&amp;page-&gt;_mapcount, 0); /* increment count (starts at -1) */
<span class="p_del">-	if (PageTransHuge(page))</span>
<span class="p_add">+	if (compound) {</span>
<span class="p_add">+		VM_BUG_ON_PAGE(!PageTransHuge(page), page);</span>
 		__inc_zone_page_state(page, NR_ANON_TRANSPARENT_HUGEPAGES);
<span class="p_del">-	__mod_zone_page_state(page_zone(page), NR_ANON_PAGES,</span>
<span class="p_del">-			hpage_nr_pages(page));</span>
<span class="p_add">+	}</span>
<span class="p_add">+	__mod_zone_page_state(page_zone(page), NR_ANON_PAGES, nr);</span>
 	__page_set_anon_rmap(page, vma, address, 1);
 }
 
<span class="p_chunk">@@ -1162,13 +1171,17 @@</span> <span class="p_context"> out:</span>
 
 /**
  * page_remove_rmap - take down pte mapping from a page
<span class="p_del">- * @page: page to remove mapping from</span>
<span class="p_add">+ * @page:	page to remove mapping from</span>
<span class="p_add">+ * @compound:	uncharge the page as compound or small page</span>
  *
  * The caller needs to hold the pte lock.
  */
<span class="p_del">-void page_remove_rmap(struct page *page)</span>
<span class="p_add">+void page_remove_rmap(struct page *page, bool compound)</span>
 {
<span class="p_add">+	int nr = compound ? hpage_nr_pages(page) : 1;</span>
<span class="p_add">+</span>
 	if (!PageAnon(page)) {
<span class="p_add">+		VM_BUG_ON_PAGE(compound &amp;&amp; !PageHuge(page), page);</span>
 		page_remove_file_rmap(page);
 		return;
 	}
<span class="p_chunk">@@ -1186,11 +1199,12 @@</span> <span class="p_context"> void page_remove_rmap(struct page *page)</span>
 	 * these counters are not modified in interrupt context, and
 	 * pte lock(a spinlock) is held, which implies preemption disabled.
 	 */
<span class="p_del">-	if (PageTransHuge(page))</span>
<span class="p_add">+	if (compound) {</span>
<span class="p_add">+		VM_BUG_ON_PAGE(!PageTransHuge(page), page);</span>
 		__dec_zone_page_state(page, NR_ANON_TRANSPARENT_HUGEPAGES);
<span class="p_add">+	}</span>
 
<span class="p_del">-	__mod_zone_page_state(page_zone(page), NR_ANON_PAGES,</span>
<span class="p_del">-			      -hpage_nr_pages(page));</span>
<span class="p_add">+	__mod_zone_page_state(page_zone(page), NR_ANON_PAGES, -nr);</span>
 
 	if (unlikely(PageMlocked(page)))
 		clear_page_mlock(page);
<span class="p_chunk">@@ -1332,7 +1346,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 		dec_mm_counter(mm, MM_FILEPAGES);
 
 discard:
<span class="p_del">-	page_remove_rmap(page);</span>
<span class="p_add">+	page_remove_rmap(page, false);</span>
 	page_cache_release(page);
 
 out_unmap:
<span class="p_header">diff --git a/mm/swapfile.c b/mm/swapfile.c</span>
<span class="p_header">index a7e72103f23b..65825c2687f5 100644</span>
<span class="p_header">--- a/mm/swapfile.c</span>
<span class="p_header">+++ b/mm/swapfile.c</span>
<span class="p_chunk">@@ -1121,10 +1121,10 @@</span> <span class="p_context"> static int unuse_pte(struct vm_area_struct *vma, pmd_t *pmd,</span>
 	set_pte_at(vma-&gt;vm_mm, addr, pte,
 		   pte_mkold(mk_pte(page, vma-&gt;vm_page_prot)));
 	if (page == swapcache) {
<span class="p_del">-		page_add_anon_rmap(page, vma, addr);</span>
<span class="p_add">+		page_add_anon_rmap(page, vma, addr, false);</span>
 		mem_cgroup_commit_charge(page, memcg, true);
 	} else { /* ksm created a completely new copy */
<span class="p_del">-		page_add_new_anon_rmap(page, vma, addr);</span>
<span class="p_add">+		page_add_new_anon_rmap(page, vma, addr, false);</span>
 		mem_cgroup_commit_charge(page, memcg, false);
 		lru_cache_add_active_or_unevictable(page, vma);
 	}
<span class="p_header">diff --git a/mm/userfaultfd.c b/mm/userfaultfd.c</span>
<span class="p_header">index 77fee9325a57..ae21a1f309c2 100644</span>
<span class="p_header">--- a/mm/userfaultfd.c</span>
<span class="p_header">+++ b/mm/userfaultfd.c</span>
<span class="p_chunk">@@ -76,7 +76,7 @@</span> <span class="p_context"> static int mcopy_atomic_pte(struct mm_struct *dst_mm,</span>
 		goto out_release_uncharge_unlock;
 
 	inc_mm_counter(dst_mm, MM_ANONPAGES);
<span class="p_del">-	page_add_new_anon_rmap(page, dst_vma, dst_addr);</span>
<span class="p_add">+	page_add_new_anon_rmap(page, dst_vma, dst_addr, false);</span>
 	mem_cgroup_commit_charge(page, memcg, false);
 	lru_cache_add_active_or_unevictable(page, dst_vma);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



