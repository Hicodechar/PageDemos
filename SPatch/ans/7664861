
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v4,14/16] mm: introduce lazyfree LRU list - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v4,14/16] mm: introduce lazyfree LRU list</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 20, 2015, 8:02 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1448006568-16031-15-git-send-email-minchan@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7664861/mbox/"
   >mbox</a>
|
   <a href="/patch/7664861/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7664861/">/patch/7664861/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 2FDAE9F2EC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 20 Nov 2015 08:04:04 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id B8AFA20483
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 20 Nov 2015 08:04:01 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 06E8620494
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 20 Nov 2015 08:03:59 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1162060AbbKTIDH (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 20 Nov 2015 03:03:07 -0500
Received: from LGEAMRELO12.lge.com ([156.147.23.52]:43164 &quot;EHLO
	lgeamrelo12.lge.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1162005AbbKTIC6 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 20 Nov 2015 03:02:58 -0500
Received: from unknown (HELO lgemrelse6q.lge.com) (156.147.1.121)
	by 156.147.23.52 with ESMTP; 20 Nov 2015 17:02:56 +0900
X-Original-SENDERIP: 156.147.1.121
X-Original-MAILFROM: minchan@kernel.org
Received: from unknown (HELO localhost.localdomain) (10.177.223.161)
	by 156.147.1.121 with ESMTP; 20 Nov 2015 17:02:56 +0900
X-Original-SENDERIP: 10.177.223.161
X-Original-MAILFROM: minchan@kernel.org
From: Minchan Kim &lt;minchan@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;,
	linux-api@vger.kernel.org, Hugh Dickins &lt;hughd@google.com&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	KOSAKI Motohiro &lt;kosaki.motohiro@jp.fujitsu.com&gt;,
	Jason Evans &lt;je@fb.com&gt;, Daniel Micay &lt;danielmicay@gmail.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Shaohua Li &lt;shli@kernel.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	yalin.wang2010@gmail.com, Andy Lutomirski &lt;luto@amacapital.net&gt;,
	Minchan Kim &lt;minchan@kernel.org&gt;
Subject: [PATCH v4 14/16] mm: introduce lazyfree LRU list
Date: Fri, 20 Nov 2015 17:02:46 +0900
Message-Id: &lt;1448006568-16031-15-git-send-email-minchan@kernel.org&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1448006568-16031-1-git-send-email-minchan@kernel.org&gt;
References: &lt;1448006568-16031-1-git-send-email-minchan@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.5 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 20, 2015, 8:02 a.m.</div>
<pre class="content">
There are issues to support MADV_FREE.

* MADV_FREE pages&#39;s hotness

It&#39;s really arguable. Someone think it&#39;s cold while others are not.
It&#39;s matter of workload dependent so I think no one could have
a one way. IOW, we need tunable knob.

* MADV_FREE on swapless system

Now, we instantly free MADV_FREEed pages on swapless system
because we don&#39;t have aged anonymous LRU list on swapless system
so there is no chance to discard them.

I tried to solve it with inactive anonymous LRU list without
introducing new LRU list but it needs a few hooks in reclaim
path to fix old behavior witch was not good to me. Moreover,
it makes implement tuning konb hard.

For addressing issues, this patch adds new LazyFree LRU list and
functions for the stat. Pages on the list have PG_lazyfree flag
which overrides PG_mappedtodisk(It should be safe because
no anonymous page can have the flag).

If user calls madvise(start, len, MADV_FREE), pages in the range
moves to lazyfree LRU from anonymous LRU. When memory pressure
happens, they can be discarded since there is no more store
opeartion since then. If there is store operation, they can move
to active anonymous LRU list.

In this patch, how to age lazyfree pages is very basic, which just
discards all pages in the list whenever memory pressure happens.
It&#39;s enough to prove working. Later patch will implement the policy.
<span class="signed-off-by">
Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
---
 drivers/base/node.c                       |  2 +
 drivers/staging/android/lowmemorykiller.c |  3 +-
 fs/proc/meminfo.c                         |  2 +
 include/linux/huge_mm.h                   |  5 +-
 include/linux/mm_inline.h                 | 25 +++++++--
 include/linux/mmzone.h                    | 11 ++--
 include/linux/page-flags.h                |  5 ++
 include/linux/rmap.h                      |  2 +-
 include/linux/swap.h                      |  5 +-
 include/linux/vm_event_item.h             |  4 +-
 include/trace/events/vmscan.h             | 18 ++++---
 mm/compaction.c                           | 12 +++--
 mm/huge_memory.c                          | 11 ++--
 mm/madvise.c                              | 40 ++++++++------
 mm/memcontrol.c                           | 14 ++++-
 mm/migrate.c                              |  2 +
 mm/page_alloc.c                           |  7 +++
 mm/rmap.c                                 | 15 ++++--
 mm/swap.c                                 | 87 ++++++++++++++++++-------------
 mm/vmscan.c                               | 78 +++++++++++++++++++--------
 mm/vmstat.c                               |  3 ++
 21 files changed, 244 insertions(+), 107 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="p_header">index 560751bad294..f7a1f2107b43 100644</span>
<span class="p_header">--- a/drivers/base/node.c</span>
<span class="p_header">+++ b/drivers/base/node.c</span>
<span class="p_chunk">@@ -70,6 +70,7 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       &quot;Node %d Active(file):   %8lu kB\n&quot;
 		       &quot;Node %d Inactive(file): %8lu kB\n&quot;
 		       &quot;Node %d Unevictable:    %8lu kB\n&quot;
<span class="p_add">+		       &quot;Node %d LazyFree:	%8lu kB\n&quot;</span>
 		       &quot;Node %d Mlocked:        %8lu kB\n&quot;,
 		       nid, K(i.totalram),
 		       nid, K(i.freeram),
<span class="p_chunk">@@ -83,6 +84,7 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       nid, K(node_page_state(nid, NR_ACTIVE_FILE)),
 		       nid, K(node_page_state(nid, NR_INACTIVE_FILE)),
 		       nid, K(node_page_state(nid, NR_UNEVICTABLE)),
<span class="p_add">+		       nid, K(node_page_state(nid, NR_LZFREE)),</span>
 		       nid, K(node_page_state(nid, NR_MLOCK)));
 
 #ifdef CONFIG_HIGHMEM
<span class="p_header">diff --git a/drivers/staging/android/lowmemorykiller.c b/drivers/staging/android/lowmemorykiller.c</span>
<span class="p_header">index 872bd603fd0d..658c16a653c2 100644</span>
<span class="p_header">--- a/drivers/staging/android/lowmemorykiller.c</span>
<span class="p_header">+++ b/drivers/staging/android/lowmemorykiller.c</span>
<span class="p_chunk">@@ -72,7 +72,8 @@</span> <span class="p_context"> static unsigned long lowmem_count(struct shrinker *s,</span>
 	return global_page_state(NR_ACTIVE_ANON) +
 		global_page_state(NR_ACTIVE_FILE) +
 		global_page_state(NR_INACTIVE_ANON) +
<span class="p_del">-		global_page_state(NR_INACTIVE_FILE);</span>
<span class="p_add">+		global_page_state(NR_INACTIVE_FILE) +</span>
<span class="p_add">+		global_page_state(NR_LZFREE);</span>
 }
 
 static unsigned long lowmem_scan(struct shrinker *s, struct shrink_control *sc)
<span class="p_header">diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c</span>
<span class="p_header">index d3ebf2e61853..3444f7c4e0b6 100644</span>
<span class="p_header">--- a/fs/proc/meminfo.c</span>
<span class="p_header">+++ b/fs/proc/meminfo.c</span>
<span class="p_chunk">@@ -102,6 +102,7 @@</span> <span class="p_context"> static int meminfo_proc_show(struct seq_file *m, void *v)</span>
 		&quot;Active(file):   %8lu kB\n&quot;
 		&quot;Inactive(file): %8lu kB\n&quot;
 		&quot;Unevictable:    %8lu kB\n&quot;
<span class="p_add">+		&quot;LazyFree:	 %8lu kB\n&quot;</span>
 		&quot;Mlocked:        %8lu kB\n&quot;
 #ifdef CONFIG_HIGHMEM
 		&quot;HighTotal:      %8lu kB\n&quot;
<span class="p_chunk">@@ -159,6 +160,7 @@</span> <span class="p_context"> static int meminfo_proc_show(struct seq_file *m, void *v)</span>
 		K(pages[LRU_ACTIVE_FILE]),
 		K(pages[LRU_INACTIVE_FILE]),
 		K(pages[LRU_UNEVICTABLE]),
<span class="p_add">+		K(pages[LRU_LZFREE]),</span>
 		K(global_page_state(NR_MLOCK)),
 #ifdef CONFIG_HIGHMEM
 		K(i.totalhigh),
<span class="p_header">diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="p_header">index e9db238a75c1..6eb54a6ed5d0 100644</span>
<span class="p_header">--- a/include/linux/huge_mm.h</span>
<span class="p_header">+++ b/include/linux/huge_mm.h</span>
<span class="p_chunk">@@ -1,6 +1,8 @@</span> <span class="p_context"></span>
 #ifndef _LINUX_HUGE_MM_H
 #define _LINUX_HUGE_MM_H
 
<span class="p_add">+struct pagevec;</span>
<span class="p_add">+</span>
 extern int do_huge_pmd_anonymous_page(struct mm_struct *mm,
 				      struct vm_area_struct *vma,
 				      unsigned long address, pmd_t *pmd,
<span class="p_chunk">@@ -21,7 +23,8 @@</span> <span class="p_context"> extern struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
 					  unsigned int flags);
 extern int madvise_free_huge_pmd(struct mmu_gather *tlb,
 			struct vm_area_struct *vma,
<span class="p_del">-			pmd_t *pmd, unsigned long addr);</span>
<span class="p_add">+			pmd_t *pmd, unsigned long addr,</span>
<span class="p_add">+			struct pagevec *pvec);</span>
 extern int zap_huge_pmd(struct mmu_gather *tlb,
 			struct vm_area_struct *vma,
 			pmd_t *pmd, unsigned long addr);
<span class="p_header">diff --git a/include/linux/mm_inline.h b/include/linux/mm_inline.h</span>
<span class="p_header">index 5e08a354f936..b34e511e90ae 100644</span>
<span class="p_header">--- a/include/linux/mm_inline.h</span>
<span class="p_header">+++ b/include/linux/mm_inline.h</span>
<span class="p_chunk">@@ -26,6 +26,10 @@</span> <span class="p_context"> static __always_inline void add_page_to_lru_list(struct page *page,</span>
 				struct lruvec *lruvec, enum lru_list lru)
 {
 	int nr_pages = hpage_nr_pages(page);
<span class="p_add">+</span>
<span class="p_add">+	if (lru == LRU_LZFREE)</span>
<span class="p_add">+		VM_BUG_ON_PAGE(PageActive(page), page);</span>
<span class="p_add">+</span>
 	mem_cgroup_update_lru_size(lruvec, lru, nr_pages);
 	list_add(&amp;page-&gt;lru, &amp;lruvec-&gt;lists[lru]);
 	__mod_zone_page_state(lruvec_zone(lruvec), NR_LRU_BASE + lru, nr_pages);
<span class="p_chunk">@@ -35,6 +39,10 @@</span> <span class="p_context"> static __always_inline void del_page_from_lru_list(struct page *page,</span>
 				struct lruvec *lruvec, enum lru_list lru)
 {
 	int nr_pages = hpage_nr_pages(page);
<span class="p_add">+</span>
<span class="p_add">+	if (lru == LRU_LZFREE)</span>
<span class="p_add">+		VM_BUG_ON_PAGE(!PageLazyFree(page), page);</span>
<span class="p_add">+</span>
 	mem_cgroup_update_lru_size(lruvec, lru, -nr_pages);
 	list_del(&amp;page-&gt;lru);
 	__mod_zone_page_state(lruvec_zone(lruvec), NR_LRU_BASE + lru, -nr_pages);
<span class="p_chunk">@@ -46,12 +54,14 @@</span> <span class="p_context"> static __always_inline void del_page_from_lru_list(struct page *page,</span>
  *
  * Used for LRU list index arithmetic.
  *
<span class="p_del">- * Returns the base LRU type - file or anon - @page should be on.</span>
<span class="p_add">+ * Returns the base LRU type - file or anon or lazyfree - @page should be on.</span>
  */
 static inline enum lru_list page_lru_base_type(struct page *page)
 {
 	if (page_is_file_cache(page))
 		return LRU_INACTIVE_FILE;
<span class="p_add">+	if (PageLazyFree(page))</span>
<span class="p_add">+		return LRU_LZFREE;</span>
 	return LRU_INACTIVE_ANON;
 }
 
<span class="p_chunk">@@ -60,7 +70,7 @@</span> <span class="p_context"> static inline enum lru_list page_lru_base_type(struct page *page)</span>
  *
  * Used for LRU list index arithmetic.
  *
<span class="p_del">- * Returns 0 if @lru is anon, 1 if it is file.</span>
<span class="p_add">+ * Returns 0 if @lru is anon, 1 if it is file, 2 if it is lazyfree</span>
  */
 static inline int lru_index(enum lru_list lru)
 {
<span class="p_chunk">@@ -75,6 +85,9 @@</span> <span class="p_context"> static inline int lru_index(enum lru_list lru)</span>
 	case LRU_ACTIVE_FILE:
 		base = 1;
 		break;
<span class="p_add">+	case LRU_LZFREE:</span>
<span class="p_add">+		base = 2;</span>
<span class="p_add">+		break;</span>
 	default:
 		BUG();
 	}
<span class="p_chunk">@@ -94,6 +107,8 @@</span> <span class="p_context"> static inline int page_off_isolate(struct page *page)</span>
 
 	if (!PageSwapBacked(page))
 		lru = NR_ISOLATED_FILE;
<span class="p_add">+	else if (PageLazyFree(page))</span>
<span class="p_add">+		lru = NR_ISOLATED_LZFREE;</span>
 	return lru;
 }
 
<span class="p_chunk">@@ -106,10 +121,12 @@</span> <span class="p_context"> static inline int page_off_isolate(struct page *page)</span>
  */
 static inline int lru_off_isolate(enum lru_list lru)
 {
<span class="p_del">-	int base = NR_ISOLATED_FILE;</span>
<span class="p_add">+	int base = NR_ISOLATED_LZFREE;</span>
 
 	if (lru &lt;= LRU_ACTIVE_ANON)
 		base = NR_ISOLATED_ANON;
<span class="p_add">+	else if (lru &lt;= LRU_ACTIVE_FILE)</span>
<span class="p_add">+		base = NR_ISOLATED_FILE;</span>
 	return base;
 }
 
<span class="p_chunk">@@ -154,6 +171,8 @@</span> <span class="p_context"> static __always_inline enum lru_list page_lru(struct page *page)</span>
 		lru = page_lru_base_type(page);
 		if (PageActive(page))
 			lru += LRU_ACTIVE;
<span class="p_add">+		if (lru == LRU_LZFREE + LRU_ACTIVE)</span>
<span class="p_add">+			lru = LRU_ACTIVE_ANON;</span>
 	}
 	return lru;
 }
<span class="p_header">diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="p_header">index d94347737292..1aaa436da0d5 100644</span>
<span class="p_header">--- a/include/linux/mmzone.h</span>
<span class="p_header">+++ b/include/linux/mmzone.h</span>
<span class="p_chunk">@@ -121,6 +121,7 @@</span> <span class="p_context"> enum zone_stat_item {</span>
 	NR_INACTIVE_FILE,	/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
 	NR_ACTIVE_FILE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
 	NR_UNEVICTABLE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
<span class="p_add">+	NR_LZFREE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
<span class="p_chunk">@@ -140,6 +141,7 @@</span> <span class="p_context"> enum zone_stat_item {</span>
 	NR_WRITEBACK_TEMP,	/* Writeback using temporary buffers */
 	NR_ISOLATED_ANON,	/* Temporary isolated pages from anon lru */
 	NR_ISOLATED_FILE,	/* Temporary isolated pages from file lru */
<span class="p_add">+	NR_ISOLATED_LZFREE,	/* Temporary isolated pages from lzfree lru */</span>
 	NR_SHMEM,		/* shmem pages (included tmpfs/GEM pages) */
 	NR_DIRTIED,		/* page dirtyings since bootup */
 	NR_WRITTEN,		/* page writings since bootup */
<span class="p_chunk">@@ -178,6 +180,7 @@</span> <span class="p_context"> enum lru_list {</span>
 	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
 	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
 	LRU_UNEVICTABLE,
<span class="p_add">+	LRU_LZFREE,</span>
 	NR_LRU_LISTS
 };
 
<span class="p_chunk">@@ -207,10 +210,11 @@</span> <span class="p_context"> struct zone_reclaim_stat {</span>
 	 * The higher the rotated/scanned ratio, the more valuable
 	 * that cache is.
 	 *
<span class="p_del">-	 * The anon LRU stats live in [0], file LRU stats in [1]</span>
<span class="p_add">+	 * The anon LRU stats live in [0], file LRU stats in [1],</span>
<span class="p_add">+	 * lazyfree LRU stats in [2]</span>
 	 */
<span class="p_del">-	unsigned long		recent_rotated[2];</span>
<span class="p_del">-	unsigned long		recent_scanned[2];</span>
<span class="p_add">+	unsigned long		recent_rotated[3];</span>
<span class="p_add">+	unsigned long		recent_scanned[3];</span>
 };
 
 struct lruvec {
<span class="p_chunk">@@ -224,6 +228,7 @@</span> <span class="p_context"> struct lruvec {</span>
 /* Mask used at gathering information at once (see memcontrol.c) */
 #define LRU_ALL_FILE (BIT(LRU_INACTIVE_FILE) | BIT(LRU_ACTIVE_FILE))
 #define LRU_ALL_ANON (BIT(LRU_INACTIVE_ANON) | BIT(LRU_ACTIVE_ANON))
<span class="p_add">+#define LRU_ALL_LZFREE (BIT(LRU_LZFREE))</span>
 #define LRU_ALL	     ((1 &lt;&lt; NR_LRU_LISTS) - 1)
 
 /* Isolate clean file */
<span class="p_header">diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h</span>
<span class="p_header">index 416509e26d6d..14f0643af5c4 100644</span>
<span class="p_header">--- a/include/linux/page-flags.h</span>
<span class="p_header">+++ b/include/linux/page-flags.h</span>
<span class="p_chunk">@@ -115,6 +115,9 @@</span> <span class="p_context"> enum pageflags {</span>
 #endif
 	__NR_PAGEFLAGS,
 
<span class="p_add">+	/* MADV_FREE */</span>
<span class="p_add">+	PG_lazyfree = PG_mappedtodisk,</span>
<span class="p_add">+</span>
 	/* Filesystems */
 	PG_checked = PG_owner_priv_1,
 
<span class="p_chunk">@@ -343,6 +346,8 @@</span> <span class="p_context"> TESTPAGEFLAG_FALSE(Ksm)</span>
 
 u64 stable_page_flags(struct page *page);
 
<span class="p_add">+PAGEFLAG(LazyFree, lazyfree);</span>
<span class="p_add">+</span>
 static inline int PageUptodate(struct page *page)
 {
 	int ret = test_bit(PG_uptodate, &amp;(page)-&gt;flags);
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index f4c992826242..edace84b45d5 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -85,7 +85,7 @@</span> <span class="p_context"> enum ttu_flags {</span>
 	TTU_UNMAP = 1,			/* unmap mode */
 	TTU_MIGRATION = 2,		/* migration mode */
 	TTU_MUNLOCK = 4,		/* munlock mode */
<span class="p_del">-	TTU_FREE = 8,			/* free mode */</span>
<span class="p_add">+	TTU_LZFREE = 8,			/* lazyfree mode */</span>
 
 	TTU_IGNORE_MLOCK = (1 &lt;&lt; 8),	/* ignore mlock */
 	TTU_IGNORE_ACCESS = (1 &lt;&lt; 9),	/* don&#39;t age */
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index f629df4cc13d..c484339b46b6 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -14,8 +14,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/page.h&gt;
 
 struct notifier_block;
<span class="p_del">-</span>
 struct bio;
<span class="p_add">+struct pagevec;</span>
 
 #define SWAP_FLAG_PREFER	0x8000	/* set if swap priority specified */
 #define SWAP_FLAG_PRIO_MASK	0x7fff
<span class="p_chunk">@@ -308,7 +308,8 @@</span> <span class="p_context"> extern void lru_add_drain_cpu(int cpu);</span>
 extern void lru_add_drain_all(void);
 extern void rotate_reclaimable_page(struct page *page);
 extern void deactivate_file_page(struct page *page);
<span class="p_del">-extern void deactivate_page(struct page *page);</span>
<span class="p_add">+extern void drain_lazyfree_pagevec(struct pagevec *pvec);</span>
<span class="p_add">+extern int add_page_to_lazyfree_list(struct page *page, struct pagevec *pvec);</span>
 extern void swap_setup(void);
 
 extern void add_page_to_unevictable_list(struct page *page);
<span class="p_header">diff --git a/include/linux/vm_event_item.h b/include/linux/vm_event_item.h</span>
<span class="p_header">index 2b1cef88b827..7ebfd7ca992d 100644</span>
<span class="p_header">--- a/include/linux/vm_event_item.h</span>
<span class="p_header">+++ b/include/linux/vm_event_item.h</span>
<span class="p_chunk">@@ -23,9 +23,9 @@</span> <span class="p_context"></span>
 
 enum vm_event_item { PGPGIN, PGPGOUT, PSWPIN, PSWPOUT,
 		FOR_ALL_ZONES(PGALLOC),
<span class="p_del">-		PGFREE, PGACTIVATE, PGDEACTIVATE,</span>
<span class="p_add">+		PGFREE, PGACTIVATE, PGDEACTIVATE, PGLZFREE,</span>
 		PGFAULT, PGMAJFAULT,
<span class="p_del">-		PGLAZYFREED,</span>
<span class="p_add">+		PGLZFREED,</span>
 		FOR_ALL_ZONES(PGREFILL),
 		FOR_ALL_ZONES(PGSTEAL_KSWAPD),
 		FOR_ALL_ZONES(PGSTEAL_DIRECT),
<span class="p_header">diff --git a/include/trace/events/vmscan.h b/include/trace/events/vmscan.h</span>
<span class="p_header">index 4e9e86733849..a7ce9169b0fa 100644</span>
<span class="p_header">--- a/include/trace/events/vmscan.h</span>
<span class="p_header">+++ b/include/trace/events/vmscan.h</span>
<span class="p_chunk">@@ -12,28 +12,32 @@</span> <span class="p_context"></span>
 
 #define RECLAIM_WB_ANON		0x0001u
 #define RECLAIM_WB_FILE		0x0002u
<span class="p_add">+#define RECLAIM_WB_LZFREE	0x0004u</span>
 #define RECLAIM_WB_MIXED	0x0010u
<span class="p_del">-#define RECLAIM_WB_SYNC		0x0004u /* Unused, all reclaim async */</span>
<span class="p_del">-#define RECLAIM_WB_ASYNC	0x0008u</span>
<span class="p_add">+#define RECLAIM_WB_SYNC		0x0040u /* Unused, all reclaim async */</span>
<span class="p_add">+#define RECLAIM_WB_ASYNC	0x0080u</span>
 
 #define show_reclaim_flags(flags)				\
 	(flags) ? __print_flags(flags, &quot;|&quot;,			\
 		{RECLAIM_WB_ANON,	&quot;RECLAIM_WB_ANON&quot;},	\
 		{RECLAIM_WB_FILE,	&quot;RECLAIM_WB_FILE&quot;},	\
<span class="p_add">+		{RECLAIM_WB_LZFREE,	&quot;RECLAIM_WB_LZFREE&quot;},	\</span>
 		{RECLAIM_WB_MIXED,	&quot;RECLAIM_WB_MIXED&quot;},	\
 		{RECLAIM_WB_SYNC,	&quot;RECLAIM_WB_SYNC&quot;},	\
 		{RECLAIM_WB_ASYNC,	&quot;RECLAIM_WB_ASYNC&quot;}	\
 		) : &quot;RECLAIM_WB_NONE&quot;
 
 #define trace_reclaim_flags(page) ( \
<span class="p_del">-	(page_is_file_cache(page) ? RECLAIM_WB_FILE : RECLAIM_WB_ANON) | \</span>
<span class="p_del">-	(RECLAIM_WB_ASYNC) \</span>
<span class="p_add">+	(page_is_file_cache(page) ? RECLAIM_WB_FILE : \</span>
<span class="p_add">+		(PageLazyFree(page) ? RECLAIM_WB_LZFREE : \</span>
<span class="p_add">+		RECLAIM_WB_ANON)) | (RECLAIM_WB_ASYNC) \</span>
 	)
 
<span class="p_del">-#define trace_shrink_flags(lru) \</span>
<span class="p_add">+#define trace_shrink_flags(lru_idx) \</span>
 	( \
<span class="p_del">-		(lru ? RECLAIM_WB_FILE : RECLAIM_WB_ANON) | \</span>
<span class="p_del">-		(RECLAIM_WB_ASYNC) \</span>
<span class="p_add">+		(lru_idx == 1 ? RECLAIM_WB_FILE : (lru_idx == 0 ? \</span>
<span class="p_add">+			RECLAIM_WB_ANON : RECLAIM_WB_LZFREE)) | \</span>
<span class="p_add">+			(RECLAIM_WB_ASYNC) \</span>
 	)
 
 TRACE_EVENT(mm_vmscan_kswapd_sleep,
<span class="p_header">diff --git a/mm/compaction.c b/mm/compaction.c</span>
<span class="p_header">index d888fa248ebb..cc40c766de38 100644</span>
<span class="p_header">--- a/mm/compaction.c</span>
<span class="p_header">+++ b/mm/compaction.c</span>
<span class="p_chunk">@@ -626,7 +626,7 @@</span> <span class="p_context"> isolate_freepages_range(struct compact_control *cc,</span>
 static void acct_isolated(struct zone *zone, struct compact_control *cc)
 {
 	struct page *page;
<span class="p_del">-	unsigned int count[2] = { 0, };</span>
<span class="p_add">+	unsigned int count[3] = { 0, };</span>
 
 	if (list_empty(&amp;cc-&gt;migratepages))
 		return;
<span class="p_chunk">@@ -636,21 +636,25 @@</span> <span class="p_context"> static void acct_isolated(struct zone *zone, struct compact_control *cc)</span>
 
 	mod_zone_page_state(zone, NR_ISOLATED_ANON, count[0]);
 	mod_zone_page_state(zone, NR_ISOLATED_FILE, count[1]);
<span class="p_add">+	mod_zone_page_state(zone, NR_ISOLATED_LZFREE, count[2]);</span>
 }
 
 /* Similar to reclaim, but different enough that they don&#39;t share logic */
 static bool too_many_isolated(struct zone *zone)
 {
<span class="p_del">-	unsigned long active, inactive, isolated;</span>
<span class="p_add">+	unsigned long active, inactive, lzfree, isolated;</span>
 
 	inactive = zone_page_state(zone, NR_INACTIVE_FILE) +
 					zone_page_state(zone, NR_INACTIVE_ANON);
 	active = zone_page_state(zone, NR_ACTIVE_FILE) +
 					zone_page_state(zone, NR_ACTIVE_ANON);
<span class="p_add">+	lzfree = zone_page_state(zone, NR_LZFREE);</span>
<span class="p_add">+</span>
 	isolated = zone_page_state(zone, NR_ISOLATED_FILE) +
<span class="p_del">-					zone_page_state(zone, NR_ISOLATED_ANON);</span>
<span class="p_add">+			zone_page_state(zone, NR_ISOLATED_ANON) +</span>
<span class="p_add">+			zone_page_state(zone, NR_ISOLATED_LZFREE);</span>
 
<span class="p_del">-	return isolated &gt; (inactive + active) / 2;</span>
<span class="p_add">+	return isolated &gt; (inactive + active + lzfree) / 2;</span>
 }
 
 /**
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 7a48c3d4f92e..4277740494a0 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1454,7 +1454,7 @@</span> <span class="p_context"> int do_huge_pmd_numa_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 }
 
 int madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
<span class="p_del">-		pmd_t *pmd, unsigned long addr)</span>
<span class="p_add">+		pmd_t *pmd, unsigned long addr, struct pagevec *pvec)</span>
 
 {
 	spinlock_t *ptl;
<span class="p_chunk">@@ -1478,18 +1478,18 @@</span> <span class="p_context"> int madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		ClearPageDirty(page);
 	unlock_page(page);
 
<span class="p_del">-	if (PageActive(page))</span>
<span class="p_del">-		deactivate_page(page);</span>
<span class="p_del">-</span>
 	if (pmd_young(orig_pmd) || pmd_dirty(orig_pmd)) {
 		orig_pmd = pmdp_huge_get_and_clear_full(tlb-&gt;mm, addr, pmd,
 			tlb-&gt;fullmm);
 		orig_pmd = pmd_mkold(orig_pmd);
 		orig_pmd = pmd_mkclean(orig_pmd);
<span class="p_del">-</span>
<span class="p_add">+		SetPageDirty(page);</span>
 		set_pmd_at(mm, addr, pmd, orig_pmd);
 		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);
 	}
<span class="p_add">+</span>
<span class="p_add">+	add_page_to_lazyfree_list(page, pvec);</span>
<span class="p_add">+	drain_lazyfree_pagevec(pvec);</span>
 out:
 	spin_unlock(ptl);
 
<span class="p_chunk">@@ -1795,6 +1795,7 @@</span> <span class="p_context"> static void __split_huge_page_refcount(struct page *page,</span>
 				      (1L &lt;&lt; PG_mlocked) |
 				      (1L &lt;&lt; PG_uptodate) |
 				      (1L &lt;&lt; PG_active) |
<span class="p_add">+				      (1L &lt;&lt; PG_lazyfree) |</span>
 				      (1L &lt;&lt; PG_unevictable) |
 				      (1L &lt;&lt; PG_dirty)));
 
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 982484fb44ca..e0836c870980 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -21,6 +21,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/swap.h&gt;
 #include &lt;linux/swapops.h&gt;
 #include &lt;linux/mmu_notifier.h&gt;
<span class="p_add">+#include &lt;linux/pagevec.h&gt;</span>
 
 #include &lt;asm/tlb.h&gt;
 
<span class="p_chunk">@@ -272,12 +273,15 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 	struct page *page;
 	int nr_swap = 0;
 	unsigned long next;
<span class="p_add">+	struct pagevec pvec;</span>
<span class="p_add">+</span>
<span class="p_add">+	pagevec_init(&amp;pvec, 0);</span>
 
 	next = pmd_addr_end(addr, end);
 	if (pmd_trans_huge(*pmd)) {
 		if (next - addr != HPAGE_PMD_SIZE)
 			split_huge_page_pmd(vma, addr, pmd);
<span class="p_del">-		else if (!madvise_free_huge_pmd(tlb, vma, pmd, addr))</span>
<span class="p_add">+		else if (!madvise_free_huge_pmd(tlb, vma, pmd, addr, &amp;pvec))</span>
 			goto next;
 		/* fall through */
 	}
<span class="p_chunk">@@ -315,24 +319,17 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 
 		VM_BUG_ON_PAGE(PageTransCompound(page), page);
 
<span class="p_del">-		if (PageSwapCache(page) || PageDirty(page)) {</span>
<span class="p_add">+		if (page_mapcount(page) != 1)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (PageSwapCache(page)) {</span>
 			if (!trylock_page(page))
 				continue;
<span class="p_del">-			/*</span>
<span class="p_del">-			 * If page is shared with others, we couldn&#39;t clear</span>
<span class="p_del">-			 * PG_dirty of the page.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			if (page_mapcount(page) != 1) {</span>
<span class="p_add">+			if (PageSwapCache(page) &amp;&amp;</span>
<span class="p_add">+					!try_to_free_swap(page)) {</span>
 				unlock_page(page);
 				continue;
 			}
<span class="p_del">-</span>
<span class="p_del">-			if (PageSwapCache(page) &amp;&amp; !try_to_free_swap(page)) {</span>
<span class="p_del">-				unlock_page(page);</span>
<span class="p_del">-				continue;</span>
<span class="p_del">-			}</span>
<span class="p_del">-</span>
<span class="p_del">-			ClearPageDirty(page);</span>
 			unlock_page(page);
 		}
 
<span class="p_chunk">@@ -348,11 +345,21 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 
 			ptent = pte_mkold(ptent);
 			ptent = pte_mkclean(ptent);
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Page could lost dirty bit without moving</span>
<span class="p_add">+			 * lazyfree LRU list so the result causes</span>
<span class="p_add">+			 * freeing the page without paging out.</span>
<span class="p_add">+			 * So let&#39;s move the dirtiness to page-&gt;flags.</span>
<span class="p_add">+			 * If it is moved to lazyfree successfully,</span>
<span class="p_add">+			 * lru_lazyfree_fn will clear it.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			SetPageDirty(page);</span>
 			set_pte_at(mm, addr, pte, ptent);
<span class="p_del">-			if (PageActive(page))</span>
<span class="p_del">-				deactivate_page(page);</span>
 			tlb_remove_tlb_entry(tlb, pte, addr);
 		}
<span class="p_add">+</span>
<span class="p_add">+		if (add_page_to_lazyfree_list(page, &amp;pvec) == 0)</span>
<span class="p_add">+			drain_lazyfree_pagevec(&amp;pvec);</span>
 	}
 
 	if (nr_swap) {
<span class="p_chunk">@@ -362,6 +369,7 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 		add_mm_counter(mm, MM_SWAPENTS, nr_swap);
 	}
 
<span class="p_add">+	drain_lazyfree_pagevec(&amp;pvec);</span>
 	arch_leave_lazy_mmu_mode();
 	pte_unmap_unlock(pte - 1, ptl);
 	cond_resched();
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index c57c4423c688..1dc599ce1bcb 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -109,6 +109,7 @@</span> <span class="p_context"> static const char * const mem_cgroup_lru_names[] = {</span>
 	&quot;inactive_file&quot;,
 	&quot;active_file&quot;,
 	&quot;unevictable&quot;,
<span class="p_add">+	&quot;lazyfree&quot;,</span>
 };
 
 #define THRESHOLDS_EVENTS_TARGET 128
<span class="p_chunk">@@ -1402,6 +1403,8 @@</span> <span class="p_context"> static void mem_cgroup_out_of_memory(struct mem_cgroup *memcg, gfp_t gfp_mask,</span>
 static bool test_mem_cgroup_node_reclaimable(struct mem_cgroup *memcg,
 		int nid, bool noswap)
 {
<span class="p_add">+	if (mem_cgroup_node_nr_lru_pages(memcg, nid, LRU_ALL_LZFREE))</span>
<span class="p_add">+		return true;</span>
 	if (mem_cgroup_node_nr_lru_pages(memcg, nid, LRU_ALL_FILE))
 		return true;
 	if (noswap || !total_swap_pages)
<span class="p_chunk">@@ -3120,6 +3123,7 @@</span> <span class="p_context"> static int memcg_numa_stat_show(struct seq_file *m, void *v)</span>
 		{ &quot;total&quot;, LRU_ALL },
 		{ &quot;file&quot;, LRU_ALL_FILE },
 		{ &quot;anon&quot;, LRU_ALL_ANON },
<span class="p_add">+		{ &quot;lazyfree&quot;, LRU_ALL_LZFREE },</span>
 		{ &quot;unevictable&quot;, BIT(LRU_UNEVICTABLE) },
 	};
 	const struct numa_stat *stat;
<span class="p_chunk">@@ -3231,8 +3235,8 @@</span> <span class="p_context"> static int memcg_stat_show(struct seq_file *m, void *v)</span>
 		int nid, zid;
 		struct mem_cgroup_per_zone *mz;
 		struct zone_reclaim_stat *rstat;
<span class="p_del">-		unsigned long recent_rotated[2] = {0, 0};</span>
<span class="p_del">-		unsigned long recent_scanned[2] = {0, 0};</span>
<span class="p_add">+		unsigned long recent_rotated[3] = {0, 0};</span>
<span class="p_add">+		unsigned long recent_scanned[3] = {0, 0};</span>
 
 		for_each_online_node(nid)
 			for (zid = 0; zid &lt; MAX_NR_ZONES; zid++) {
<span class="p_chunk">@@ -3241,13 +3245,19 @@</span> <span class="p_context"> static int memcg_stat_show(struct seq_file *m, void *v)</span>
 
 				recent_rotated[0] += rstat-&gt;recent_rotated[0];
 				recent_rotated[1] += rstat-&gt;recent_rotated[1];
<span class="p_add">+				recent_rotated[2] += rstat-&gt;recent_rotated[2];</span>
 				recent_scanned[0] += rstat-&gt;recent_scanned[0];
 				recent_scanned[1] += rstat-&gt;recent_scanned[1];
<span class="p_add">+				recent_scanned[2] += rstat-&gt;recent_scanned[2];</span>
 			}
 		seq_printf(m, &quot;recent_rotated_anon %lu\n&quot;, recent_rotated[0]);
 		seq_printf(m, &quot;recent_rotated_file %lu\n&quot;, recent_rotated[1]);
<span class="p_add">+		seq_printf(m, &quot;recent_rotated_lzfree %lu\n&quot;,</span>
<span class="p_add">+						recent_rotated[2]);</span>
 		seq_printf(m, &quot;recent_scanned_anon %lu\n&quot;, recent_scanned[0]);
 		seq_printf(m, &quot;recent_scanned_file %lu\n&quot;, recent_scanned[1]);
<span class="p_add">+		seq_printf(m, &quot;recent_scanned_lzfree %lu\n&quot;,</span>
<span class="p_add">+						recent_scanned[2]);</span>
 	}
 #endif
 
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 87ebf0833b84..945e5655cd69 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -508,6 +508,8 @@</span> <span class="p_context"> void migrate_page_copy(struct page *newpage, struct page *page)</span>
 		SetPageChecked(newpage);
 	if (PageMappedToDisk(page))
 		SetPageMappedToDisk(newpage);
<span class="p_add">+	if (PageLazyFree(page))</span>
<span class="p_add">+		SetPageLazyFree(newpage);</span>
 
 	if (PageDirty(page)) {
 		clear_page_dirty_for_io(page);
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 48aaf7b9f253..d6a42c905b0b 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -3712,6 +3712,7 @@</span> <span class="p_context"> void show_free_areas(unsigned int filter)</span>
 
 	printk(&quot;active_anon:%lu inactive_anon:%lu isolated_anon:%lu\n&quot;
 		&quot; active_file:%lu inactive_file:%lu isolated_file:%lu\n&quot;
<span class="p_add">+		&quot; lazy_free:%lu isolated_lazyfree:%lu\n&quot;</span>
 		&quot; unevictable:%lu dirty:%lu writeback:%lu unstable:%lu\n&quot;
 		&quot; slab_reclaimable:%lu slab_unreclaimable:%lu\n&quot;
 		&quot; mapped:%lu shmem:%lu pagetables:%lu bounce:%lu\n&quot;
<span class="p_chunk">@@ -3722,6 +3723,8 @@</span> <span class="p_context"> void show_free_areas(unsigned int filter)</span>
 		global_page_state(NR_ACTIVE_FILE),
 		global_page_state(NR_INACTIVE_FILE),
 		global_page_state(NR_ISOLATED_FILE),
<span class="p_add">+		global_page_state(NR_LZFREE),</span>
<span class="p_add">+		global_page_state(NR_ISOLATED_LZFREE),</span>
 		global_page_state(NR_UNEVICTABLE),
 		global_page_state(NR_FILE_DIRTY),
 		global_page_state(NR_WRITEBACK),
<span class="p_chunk">@@ -3756,9 +3759,11 @@</span> <span class="p_context"> void show_free_areas(unsigned int filter)</span>
 			&quot; inactive_anon:%lukB&quot;
 			&quot; active_file:%lukB&quot;
 			&quot; inactive_file:%lukB&quot;
<span class="p_add">+			&quot; lazyfree:%lukB&quot;</span>
 			&quot; unevictable:%lukB&quot;
 			&quot; isolated(anon):%lukB&quot;
 			&quot; isolated(file):%lukB&quot;
<span class="p_add">+			&quot; isolated(lazy):%lukB&quot;</span>
 			&quot; present:%lukB&quot;
 			&quot; managed:%lukB&quot;
 			&quot; mlocked:%lukB&quot;
<span class="p_chunk">@@ -3788,9 +3793,11 @@</span> <span class="p_context"> void show_free_areas(unsigned int filter)</span>
 			K(zone_page_state(zone, NR_INACTIVE_ANON)),
 			K(zone_page_state(zone, NR_ACTIVE_FILE)),
 			K(zone_page_state(zone, NR_INACTIVE_FILE)),
<span class="p_add">+			K(zone_page_state(zone, NR_LZFREE)),</span>
 			K(zone_page_state(zone, NR_UNEVICTABLE)),
 			K(zone_page_state(zone, NR_ISOLATED_ANON)),
 			K(zone_page_state(zone, NR_ISOLATED_FILE)),
<span class="p_add">+			K(zone_page_state(zone, NR_ISOLATED_LZFREE)),</span>
 			K(zone-&gt;present_pages),
 			K(zone-&gt;managed_pages),
 			K(zone_page_state(zone, NR_MLOCK)),
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 9449e91839ab..75bd68bc8abc 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1374,10 +1374,17 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 		swp_entry_t entry = { .val = page_private(page) };
 		pte_t swp_pte;
 
<span class="p_del">-		if (!PageDirty(page) &amp;&amp; (flags &amp; TTU_FREE)) {</span>
<span class="p_del">-			/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="p_del">-			dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="p_del">-			goto discard;</span>
<span class="p_add">+		if ((flags &amp; TTU_LZFREE)) {</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageLazyFree(page), page);</span>
<span class="p_add">+			if (!PageDirty(page)) {</span>
<span class="p_add">+				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="p_add">+				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="p_add">+				goto discard;</span>
<span class="p_add">+			} else {</span>
<span class="p_add">+				set_pte_at(mm, address, pte, pteval);</span>
<span class="p_add">+				ret = SWAP_FAIL;</span>
<span class="p_add">+				goto out_unmap;</span>
<span class="p_add">+			}</span>
 		}
 
 		if (PageSwapCache(page)) {
<span class="p_header">diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="p_header">index ac1c6be4381f..b88c59c2f1e8 100644</span>
<span class="p_header">--- a/mm/swap.c</span>
<span class="p_header">+++ b/mm/swap.c</span>
<span class="p_chunk">@@ -45,7 +45,6 @@</span> <span class="p_context"> int page_cluster;</span>
 static DEFINE_PER_CPU(struct pagevec, lru_add_pvec);
 static DEFINE_PER_CPU(struct pagevec, lru_rotate_pvecs);
 static DEFINE_PER_CPU(struct pagevec, lru_deactivate_file_pvecs);
<span class="p_del">-static DEFINE_PER_CPU(struct pagevec, lru_deactivate_pvecs);</span>
 
 /*
  * This path almost never happens for VM activity - pages are normally
<span class="p_chunk">@@ -508,6 +507,10 @@</span> <span class="p_context"> static void __activate_page(struct page *page, struct lruvec *lruvec,</span>
 
 		del_page_from_lru_list(page, lruvec, lru);
 		SetPageActive(page);
<span class="p_add">+		if (lru == LRU_LZFREE) {</span>
<span class="p_add">+			ClearPageLazyFree(page);</span>
<span class="p_add">+			lru = LRU_INACTIVE_ANON;</span>
<span class="p_add">+		}</span>
 		lru += LRU_ACTIVE;
 		add_page_to_lru_list(page, lruvec, lru);
 		trace_mm_lru_activate(page);
<span class="p_chunk">@@ -799,20 +802,41 @@</span> <span class="p_context"> static void lru_deactivate_file_fn(struct page *page, struct lruvec *lruvec,</span>
 	update_page_reclaim_stat(lruvec, lru_index(lru), 0);
 }
 
<span class="p_del">-</span>
<span class="p_del">-static void lru_deactivate_fn(struct page *page, struct lruvec *lruvec,</span>
<span class="p_del">-			    void *arg)</span>
<span class="p_add">+static void lru_lazyfree_fn(struct page *page, struct lruvec *lruvec,</span>
<span class="p_add">+		void *arg)</span>
 {
<span class="p_del">-	if (PageLRU(page) &amp;&amp; PageActive(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="p_del">-		enum lru_list lru = page_lru_base_type(page);</span>
<span class="p_add">+	VM_BUG_ON_PAGE(!PageAnon(page), page);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (PageLRU(page) &amp;&amp; !PageLazyFree(page) &amp;&amp;</span>
<span class="p_add">+				!PageUnevictable(page)) {</span>
<span class="p_add">+		unsigned int nr_pages = PageTransHuge(page) ? HPAGE_PMD_NR : 1;</span>
<span class="p_add">+		bool active = PageActive(page);</span>
 
<span class="p_del">-		del_page_from_lru_list(page, lruvec, lru + LRU_ACTIVE);</span>
<span class="p_add">+		if (!trylock_page(page))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		if (PageSwapCache(page))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		if (PageDirty(page)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If page is shared with others, we couldn&#39;t clear</span>
<span class="p_add">+			 * PG_dirty of the page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (page_count(page) != 2) {</span>
<span class="p_add">+				unlock_page(page);</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			ClearPageDirty(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		del_page_from_lru_list(page, lruvec,</span>
<span class="p_add">+			       LRU_INACTIVE_ANON + active);</span>
 		ClearPageActive(page);
<span class="p_del">-		ClearPageReferenced(page);</span>
<span class="p_del">-		add_page_to_lru_list(page, lruvec, lru);</span>
<span class="p_add">+		SetPageLazyFree(page);</span>
<span class="p_add">+		add_page_to_lru_list(page, lruvec, LRU_LZFREE);</span>
<span class="p_add">+		unlock_page(page);</span>
 
<span class="p_del">-		__count_vm_event(PGDEACTIVATE);</span>
<span class="p_del">-		update_page_reclaim_stat(lruvec, lru_index(lru), 0);</span>
<span class="p_add">+		count_vm_events(PGLZFREE, nr_pages);</span>
<span class="p_add">+		update_page_reclaim_stat(lruvec, 2, 0);</span>
 	}
 }
 
<span class="p_chunk">@@ -842,11 +866,25 @@</span> <span class="p_context"> void lru_add_drain_cpu(int cpu)</span>
 	if (pagevec_count(pvec))
 		pagevec_lru_move_fn(pvec, lru_deactivate_file_fn, NULL);
 
<span class="p_del">-	pvec = &amp;per_cpu(lru_deactivate_pvecs, cpu);</span>
<span class="p_add">+	activate_page_drain(cpu);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Drain lazyfree pages out of the cpu&#39;s pagevec before release pte lock</span>
<span class="p_add">+ */</span>
<span class="p_add">+void drain_lazyfree_pagevec(struct pagevec *pvec)</span>
<span class="p_add">+{</span>
 	if (pagevec_count(pvec))
<span class="p_del">-		pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);</span>
<span class="p_add">+		pagevec_lru_move_fn(pvec, lru_lazyfree_fn, NULL);</span>
<span class="p_add">+}</span>
 
<span class="p_del">-	activate_page_drain(cpu);</span>
<span class="p_add">+int add_page_to_lazyfree_list(struct page *page, struct pagevec *pvec)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (PageLRU(page) &amp;&amp; !PageLazyFree(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="p_add">+		page_cache_get(page);</span>
<span class="p_add">+		pagevec_add(pvec, page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pagevec_space(pvec);</span>
 }
 
 /**
<span class="p_chunk">@@ -875,26 +913,6 @@</span> <span class="p_context"> void deactivate_file_page(struct page *page)</span>
 	}
 }
 
<span class="p_del">-/**</span>
<span class="p_del">- * deactivate_page - deactivate a page</span>
<span class="p_del">- * @page: page to deactivate</span>
<span class="p_del">- *</span>
<span class="p_del">- * deactivate_page() moves @page to the inactive list if @page was on the active</span>
<span class="p_del">- * list and was not an unevictable page.  This is done to accelerate the reclaim</span>
<span class="p_del">- * of @page.</span>
<span class="p_del">- */</span>
<span class="p_del">-void deactivate_page(struct page *page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (PageLRU(page) &amp;&amp; PageActive(page) &amp;&amp; !PageUnevictable(page)) {</span>
<span class="p_del">-		struct pagevec *pvec = &amp;get_cpu_var(lru_deactivate_pvecs);</span>
<span class="p_del">-</span>
<span class="p_del">-		page_cache_get(page);</span>
<span class="p_del">-		if (!pagevec_add(pvec, page))</span>
<span class="p_del">-			pagevec_lru_move_fn(pvec, lru_deactivate_fn, NULL);</span>
<span class="p_del">-		put_cpu_var(lru_deactivate_pvecs);</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 void lru_add_drain(void)
 {
 	lru_add_drain_cpu(get_cpu());
<span class="p_chunk">@@ -924,7 +942,6 @@</span> <span class="p_context"> void lru_add_drain_all(void)</span>
 		if (pagevec_count(&amp;per_cpu(lru_add_pvec, cpu)) ||
 		    pagevec_count(&amp;per_cpu(lru_rotate_pvecs, cpu)) ||
 		    pagevec_count(&amp;per_cpu(lru_deactivate_file_pvecs, cpu)) ||
<span class="p_del">-		    pagevec_count(&amp;per_cpu(lru_deactivate_pvecs, cpu)) ||</span>
 		    need_activate_page_drain(cpu)) {
 			INIT_WORK(work, lru_add_drain_per_cpu);
 			schedule_work_on(cpu, work);
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 80dff84ba673..8efe30ceec3a 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -197,7 +197,8 @@</span> <span class="p_context"> static unsigned long zone_reclaimable_pages(struct zone *zone)</span>
 	int nr;
 
 	nr = zone_page_state(zone, NR_ACTIVE_FILE) +
<span class="p_del">-	     zone_page_state(zone, NR_INACTIVE_FILE);</span>
<span class="p_add">+		zone_page_state(zone, NR_INACTIVE_FILE) +</span>
<span class="p_add">+		zone_page_state(zone, NR_LZFREE);</span>
 
 	if (get_nr_swap_pages() &gt; 0)
 		nr += zone_page_state(zone, NR_ACTIVE_ANON) +
<span class="p_chunk">@@ -918,6 +919,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 
 		VM_BUG_ON_PAGE(PageActive(page), page);
 		VM_BUG_ON_PAGE(page_zone(page) != zone, page);
<span class="p_add">+		VM_BUG_ON_PAGE((ttu_flags &amp; TTU_LZFREE) &amp;&amp;</span>
<span class="p_add">+				!PageLazyFree(page), page);</span>
 
 		sc-&gt;nr_scanned++;
 
<span class="p_chunk">@@ -1050,7 +1053,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 				goto keep_locked;
 			if (!add_to_swap(page, page_list))
 				goto activate_locked;
<span class="p_del">-			freeable = true;</span>
<span class="p_add">+			if (ttu_flags &amp; TTU_LZFREE)</span>
<span class="p_add">+				freeable = true;</span>
 			may_enter_fs = 1;
 
 			/* Adding to swap updated mapping */
<span class="p_chunk">@@ -1063,8 +1067,9 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 */
 		if (page_mapped(page) &amp;&amp; mapping) {
 			switch (try_to_unmap(page, freeable ?
<span class="p_del">-				(ttu_flags | TTU_BATCH_FLUSH | TTU_FREE) :</span>
<span class="p_del">-				(ttu_flags | TTU_BATCH_FLUSH))) {</span>
<span class="p_add">+				(ttu_flags | TTU_BATCH_FLUSH) :</span>
<span class="p_add">+				((ttu_flags &amp; ~TTU_LZFREE) |</span>
<span class="p_add">+						TTU_BATCH_FLUSH))) {</span>
 			case SWAP_FAIL:
 				goto activate_locked;
 			case SWAP_AGAIN:
<span class="p_chunk">@@ -1190,7 +1195,7 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		__clear_page_locked(page);
 free_it:
 		if (freeable &amp;&amp; !PageDirty(page))
<span class="p_del">-			count_vm_event(PGLAZYFREED);</span>
<span class="p_add">+			count_vm_event(PGLZFREED);</span>
 
 		nr_reclaimed++;
 
<span class="p_chunk">@@ -1458,7 +1463,7 @@</span> <span class="p_context"> int isolate_lru_page(struct page *page)</span>
  * the LRU list will go small and be scanned faster than necessary, leading to
  * unnecessary swapping, thrashing and OOM.
  */
<span class="p_del">-static int too_many_isolated(struct zone *zone, int file,</span>
<span class="p_add">+static int too_many_isolated(struct zone *zone, int lru_index,</span>
 		struct scan_control *sc)
 {
 	unsigned long inactive, isolated;
<span class="p_chunk">@@ -1469,12 +1474,21 @@</span> <span class="p_context"> static int too_many_isolated(struct zone *zone, int file,</span>
 	if (!sane_reclaim(sc))
 		return 0;
 
<span class="p_del">-	if (file) {</span>
<span class="p_del">-		inactive = zone_page_state(zone, NR_INACTIVE_FILE);</span>
<span class="p_del">-		isolated = zone_page_state(zone, NR_ISOLATED_FILE);</span>
<span class="p_del">-	} else {</span>
<span class="p_add">+	switch (lru_index) {</span>
<span class="p_add">+	case 0:</span>
 		inactive = zone_page_state(zone, NR_INACTIVE_ANON);
 		isolated = zone_page_state(zone, NR_ISOLATED_ANON);
<span class="p_add">+		break;</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		inactive = zone_page_state(zone, NR_INACTIVE_FILE);</span>
<span class="p_add">+		isolated = zone_page_state(zone, NR_ISOLATED_FILE);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		inactive = zone_page_state(zone, NR_LZFREE);</span>
<span class="p_add">+		isolated = zone_page_state(zone, NR_ISOLATED_LZFREE);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		BUG_ON(lru_index);</span>
 	}
 
 	/*
<span class="p_chunk">@@ -1489,7 +1503,8 @@</span> <span class="p_context"> static int too_many_isolated(struct zone *zone, int file,</span>
 }
 
 static noinline_for_stack void
<span class="p_del">-putback_inactive_pages(struct lruvec *lruvec, struct list_head *page_list)</span>
<span class="p_add">+putback_inactive_pages(struct lruvec *lruvec, enum lru_list old_lru,</span>
<span class="p_add">+			struct list_head *page_list)</span>
 {
 	struct zone_reclaim_stat *reclaim_stat = &amp;lruvec-&gt;reclaim_stat;
 	struct zone *zone = lruvec_zone(lruvec);
<span class="p_chunk">@@ -1500,7 +1515,7 @@</span> <span class="p_context"> putback_inactive_pages(struct lruvec *lruvec, struct list_head *page_list)</span>
 	 */
 	while (!list_empty(page_list)) {
 		struct page *page = lru_to_page(page_list);
<span class="p_del">-		int lru;</span>
<span class="p_add">+		int new_lru;</span>
 
 		VM_BUG_ON_PAGE(PageLRU(page), page);
 		list_del(&amp;page-&gt;lru);
<span class="p_chunk">@@ -1514,18 +1529,20 @@</span> <span class="p_context"> putback_inactive_pages(struct lruvec *lruvec, struct list_head *page_list)</span>
 		lruvec = mem_cgroup_page_lruvec(page, zone);
 
 		SetPageLRU(page);
<span class="p_del">-		lru = page_lru(page);</span>
<span class="p_del">-		add_page_to_lru_list(page, lruvec, lru);</span>
<span class="p_add">+		new_lru = page_lru(page);</span>
<span class="p_add">+		if (old_lru == LRU_LZFREE &amp;&amp; new_lru == LRU_ACTIVE_ANON)</span>
<span class="p_add">+			ClearPageLazyFree(page);</span>
 
<span class="p_del">-		if (is_active_lru(lru)) {</span>
<span class="p_add">+		add_page_to_lru_list(page, lruvec, new_lru);</span>
<span class="p_add">+		if (PageActive(page)) {</span>
 			int numpages = hpage_nr_pages(page);
<span class="p_del">-			reclaim_stat-&gt;recent_rotated[lru_index(lru)]</span>
<span class="p_add">+			reclaim_stat-&gt;recent_rotated[lru_index(old_lru)]</span>
 				+= numpages;
 		}
 		if (put_page_testzero(page)) {
 			__ClearPageLRU(page);
 			__ClearPageActive(page);
<span class="p_del">-			del_page_from_lru_list(page, lruvec, lru);</span>
<span class="p_add">+			del_page_from_lru_list(page, lruvec, new_lru);</span>
 
 			if (unlikely(PageCompound(page))) {
 				spin_unlock_irq(&amp;zone-&gt;lru_lock);
<span class="p_chunk">@@ -1578,7 +1595,7 @@</span> <span class="p_context"> shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,</span>
 	struct zone *zone = lruvec_zone(lruvec);
 	struct zone_reclaim_stat *reclaim_stat = &amp;lruvec-&gt;reclaim_stat;
 
<span class="p_del">-	while (unlikely(too_many_isolated(zone, file, sc))) {</span>
<span class="p_add">+	while (unlikely(too_many_isolated(zone, lru_index(lru), sc))) {</span>
 		congestion_wait(BLK_RW_ASYNC, HZ/10);
 
 		/* We are about to die and free our memory. Return now. */
<span class="p_chunk">@@ -1613,7 +1630,10 @@</span> <span class="p_context"> shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,</span>
 	if (nr_taken == 0)
 		return 0;
 
<span class="p_del">-	nr_reclaimed = shrink_page_list(&amp;page_list, zone, sc, TTU_UNMAP,</span>
<span class="p_add">+	nr_reclaimed = shrink_page_list(&amp;page_list, zone, sc,</span>
<span class="p_add">+				(lru != LRU_LZFREE) ?</span>
<span class="p_add">+				TTU_UNMAP :</span>
<span class="p_add">+				TTU_UNMAP|TTU_LZFREE,</span>
 				&amp;nr_dirty, &amp;nr_unqueued_dirty, &amp;nr_congested,
 				&amp;nr_writeback, &amp;nr_immediate,
 				false);
<span class="p_chunk">@@ -1631,7 +1651,7 @@</span> <span class="p_context"> shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,</span>
 					       nr_reclaimed);
 	}
 
<span class="p_del">-	putback_inactive_pages(lruvec, &amp;page_list);</span>
<span class="p_add">+	putback_inactive_pages(lruvec, lru, &amp;page_list);</span>
 
 	__mod_zone_page_state(zone, lru_off_isolate(lru), -nr_taken);
 
<span class="p_chunk">@@ -1701,7 +1721,7 @@</span> <span class="p_context"> shrink_inactive_list(unsigned long nr_to_scan, struct lruvec *lruvec,</span>
 		zone_idx(zone),
 		nr_scanned, nr_reclaimed,
 		sc-&gt;priority,
<span class="p_del">-		trace_shrink_flags(lru));</span>
<span class="p_add">+		trace_shrink_flags(lru_index(lru)));</span>
 	return nr_reclaimed;
 }
 
<span class="p_chunk">@@ -2194,6 +2214,7 @@</span> <span class="p_context"> static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
 	unsigned long nr[NR_LRU_LISTS];
 	unsigned long targets[NR_LRU_LISTS];
 	unsigned long nr_to_scan;
<span class="p_add">+	unsigned long nr_to_scan_lzfree;</span>
 	enum lru_list lru;
 	unsigned long nr_reclaimed = 0;
 	unsigned long nr_to_reclaim = sc-&gt;nr_to_reclaim;
<span class="p_chunk">@@ -2204,6 +2225,7 @@</span> <span class="p_context"> static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
 
 	/* Record the original scan target for proportional adjustments later */
 	memcpy(targets, nr, sizeof(nr));
<span class="p_add">+	nr_to_scan_lzfree = get_lru_size(lruvec, LRU_LZFREE);</span>
 
 	/*
 	 * Global reclaiming within direct reclaim at DEF_PRIORITY is a normal
<span class="p_chunk">@@ -2221,6 +2243,19 @@</span> <span class="p_context"> static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
 
 	init_tlb_ubc();
 
<span class="p_add">+	while (nr_to_scan_lzfree) {</span>
<span class="p_add">+		nr_to_scan = min(nr_to_scan_lzfree, SWAP_CLUSTER_MAX);</span>
<span class="p_add">+		nr_to_scan_lzfree -= nr_to_scan;</span>
<span class="p_add">+</span>
<span class="p_add">+		nr_reclaimed += shrink_inactive_list(nr_to_scan, lruvec,</span>
<span class="p_add">+						sc, LRU_LZFREE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (nr_reclaimed &gt;= nr_to_reclaim) {</span>
<span class="p_add">+		sc-&gt;nr_reclaimed += nr_reclaimed;</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	blk_start_plug(&amp;plug);
 	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
 					nr[LRU_INACTIVE_FILE]) {
<span class="p_chunk">@@ -2364,6 +2399,7 @@</span> <span class="p_context"> static inline bool should_continue_reclaim(struct zone *zone,</span>
 	 */
 	pages_for_compaction = (2UL &lt;&lt; sc-&gt;order);
 	inactive_lru_pages = zone_page_state(zone, NR_INACTIVE_FILE);
<span class="p_add">+	inactive_lru_pages += zone_page_state(zone, NR_LZFREE);</span>
 	if (get_nr_swap_pages() &gt; 0)
 		inactive_lru_pages += zone_page_state(zone, NR_INACTIVE_ANON);
 	if (sc-&gt;nr_reclaimed &lt; pages_for_compaction &amp;&amp;
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index 59d45b22355f..df95d9473bba 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -704,6 +704,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;nr_inactive_file&quot;,
 	&quot;nr_active_file&quot;,
 	&quot;nr_unevictable&quot;,
<span class="p_add">+	&quot;nr_lazyfree&quot;,</span>
 	&quot;nr_mlock&quot;,
 	&quot;nr_anon_pages&quot;,
 	&quot;nr_mapped&quot;,
<span class="p_chunk">@@ -721,6 +722,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;nr_writeback_temp&quot;,
 	&quot;nr_isolated_anon&quot;,
 	&quot;nr_isolated_file&quot;,
<span class="p_add">+	&quot;nr_isolated_lazyfree&quot;,</span>
 	&quot;nr_shmem&quot;,
 	&quot;nr_dirtied&quot;,
 	&quot;nr_written&quot;,
<span class="p_chunk">@@ -756,6 +758,7 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;pgfree&quot;,
 	&quot;pgactivate&quot;,
 	&quot;pgdeactivate&quot;,
<span class="p_add">+	&quot;pglazyfree&quot;,</span>
 
 	&quot;pgfault&quot;,
 	&quot;pgmajfault&quot;,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



