
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] locking changes for v4.9 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] locking changes for v4.9</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 3, 2016, 7:09 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20161003070906.GA5499@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9360087/mbox/"
   >mbox</a>
|
   <a href="/patch/9360087/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9360087/">/patch/9360087/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	EAC21601C0 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Oct 2016 07:09:45 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BF0A728999
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Oct 2016 07:09:45 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A673A289A2; Mon,  3 Oct 2016 07:09:45 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.3 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI, RCVD_IN_SORBS_SPAM,
	T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AA85128999
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Oct 2016 07:09:36 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752305AbcJCHJZ (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 3 Oct 2016 03:09:25 -0400
Received: from mail-wm0-f67.google.com ([74.125.82.67]:36335 &quot;EHLO
	mail-wm0-f67.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751099AbcJCHJN (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 3 Oct 2016 03:09:13 -0400
Received: by mail-wm0-f67.google.com with SMTP id b184so13163283wma.3
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 03 Oct 2016 00:09:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:content-transfer-encoding:user-agent;
	bh=COzgV78vfhZpdsiW38WgD4P6n1ocQCC0Kj+45faCeWA=;
	b=uerDNnzTsmA/cGpSM5/lZScyI0clyYGjuociS2/jvSWLN1VAoV+sEubtYSZusNvdf9
	8222KNPxYiQ7dzFk3eoi47DiB0xcrwu4Vp0yK2iWaSClgUb+YuXtGQadYXIZIhXz7g/X
	O0/YEGi4/clzP9wdfLSD3JOHk7V86/SS/TZvdH3oIKI8LKEq8Ij7/xCFErRLWSHr6HZm
	7bZLFuY1CSRnor4n+jUJCtgGNEYUnhdPPdvBYcu16gChWxk6EZ+vY7F9MaeSAbZIO1LU
	Xnwfi5zGvA+Gq59A4597GNY+MiaLHqAsGTaqE+d7ZUUhptDgffVX7tmHu+/j5xmHJgca
	aPJg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:content-transfer-encoding
	:user-agent;
	bh=COzgV78vfhZpdsiW38WgD4P6n1ocQCC0Kj+45faCeWA=;
	b=OyV1tLjd27gjQaDLYOdOIcWVZ3utJntaTWh/oVMoIciBAJgVgML0jIUNtawLMJ0aWh
	pC1rlcFk5WZXOPusG9erNj7w9RF0Blsn57mFJSTaWfRxAp3ZN19GyOu0is8AOjjEpmfp
	vIxKAajwkAgtIv4U+EhGzShXBhFc+YCgyuV+AYxPm3au8RGJi249gdhpMq5hkBNaENqv
	Dg4eVv63IOhOv0AgJmsiretHgIDFzx7p1xPAZbkyUPzDN7ZnwAilNPYQX8Wc8ziM1DDc
	Gs9lIXGUCW/3mmRdvQ0AtDX0CEKdA/1bLqAZDR/chHP5kOID9/liFojC79Fcip+TcXcW
	SPzw==
X-Gm-Message-State: AA6/9RmF28xDMRsYkyAX21egvN8+ZB8MjL3NqVlBYuj4ibNJpVUzIJSFbZ7BrPt1agVPwQ==
X-Received: by 10.194.227.38 with SMTP id rx6mr19031125wjc.40.1475478550391; 
	Mon, 03 Oct 2016 00:09:10 -0700 (PDT)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	jr3sm24798625wjb.13.2016.10.03.00.09.08
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 03 Oct 2016 00:09:09 -0700 (PDT)
Date: Mon, 3 Oct 2016 09:09:06 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;Paul E. McKenney&quot; &lt;paulmck@us.ibm.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] locking changes for v4.9
Message-ID: &lt;20161003070906.GA5499@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
Content-Transfer-Encoding: 8bit
User-Agent: Mutt/1.5.24 (2015-08-30)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Oct. 3, 2016, 7:09 a.m.</div>
<pre class="content">
Linus,

Please pull the latest locking-core-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git locking-core-for-linus

   # HEAD: 08645077b7f9f7824dbaf1959b0e014a894c8acc x86/cmpxchg, locking/atomics: Remove superfluous definitions

The main changes in this cycle were:

 - rwsem micro-optimizations (Davidlohr Bueso)

 - Improve the implementation and optimize the performance of percpu-rwsems. 
   (Peter Zijlstra.)

 - Convert all lglock users to better facilities such as percpu-rwsems or 
   percpu-spinlocks and remove lglocks. (Peter Zijlstra)

 - Remove the ticket (spin)lock implementation. (Peter Zijlstra)

 - Korean translation of memory-barriers.txt and related fixes to the 
   English document. (SeongJae Park)

 - misc fixes and cleanups

 Thanks,

	Ingo

------------------&gt;
Davidlohr Bueso (3):
      locking/rwsem: Return void in __rwsem_mark_wake()
      locking/rwsem: Remove a few useless comments
      locking/rwsem: Scan the wait_list for readers only once

Jan Beulich (1):
      locking/rwsem, x86: Drop a bogus cc clobber

Nikolay Borisov (1):
      x86/cmpxchg, locking/atomics: Remove superfluous definitions

Oleg Nesterov (1):
      stop_machine: Remove stop_cpus_lock and lg_double_lock/unlock()

Pan Xinhui (1):
      locking/pv-qspinlock: Use cmpxchg_release() in __pv_queued_spin_unlock()

Peter Zijlstra (10):
      locking/qspinlock: Improve readability
      locking/percpu-rwsem: Optimize readers and reduce global impact
      locking, rcu, cgroup: Avoid synchronize_sched() in __cgroup_procs_write()
      locking/percpu-rwsem: Add DEFINE_STATIC_PERCPU_RWSEMand percpu_rwsem_assert_held()
      fs/locks: Replace lg_global with a percpu-rwsem
      fs/locks: Replace lg_local with a per-cpu spinlock
      locking/percpu-rwsem: Add down_read_preempt_disable()
      fs/locks: Use percpu_down_read_preempt_disable()
      locking/lglock: Remove lglock implementation
      x86, locking/spinlocks: Remove ticket (spin)lock implementation

SeongJae Park (4):
      locking/Documentation: Maintain consistent blank line
      locking/Documentation: Fix wrong section reference
      locking/Documentation: Fix a typo of example result
      locking/Documentation: Add Korean translation

Thomas Gleixner (1):
      futex: Add some more function commentry

Vegard Nossum (1):
      locking/hung_task: Show all locks

Waiman Long (1):
      locking/pvstat: Separate wait_again and spurious wakeup stats


 Documentation/ko_KR/memory-barriers.txt | 3135 +++++++++++++++++++++++++++++++
 Documentation/locking/lglock.txt        |  166 --
 Documentation/memory-barriers.txt       |    5 +-
 arch/x86/Kconfig                        |    3 +-
 arch/x86/include/asm/cmpxchg.h          |   44 -
 arch/x86/include/asm/paravirt.h         |   18 -
 arch/x86/include/asm/paravirt_types.h   |    7 -
 arch/x86/include/asm/rwsem.h            |    2 +-
 arch/x86/include/asm/spinlock.h         |  174 --
 arch/x86/include/asm/spinlock_types.h   |   13 -
 arch/x86/kernel/kvm.c                   |  245 ---
 arch/x86/kernel/paravirt-spinlocks.c    |    7 -
 arch/x86/kernel/paravirt_patch_32.c     |    4 +-
 arch/x86/kernel/paravirt_patch_64.c     |    4 +-
 arch/x86/xen/spinlock.c                 |  250 +--
 fs/Kconfig                              |    1 +
 fs/locks.c                              |   68 +-
 include/linux/lglock.h                  |   81 -
 include/linux/percpu-rwsem.h            |  108 +-
 include/linux/rcu_sync.h                |    1 +
 kernel/cgroup.c                         |    6 +
 kernel/futex.c                          |   15 +-
 kernel/hung_task.c                      |    2 +-
 kernel/locking/Makefile                 |    1 -
 kernel/locking/lglock.c                 |  111 --
 kernel/locking/percpu-rwsem.c           |  228 ++-
 kernel/locking/qspinlock_paravirt.h     |   26 +-
 kernel/locking/qspinlock_stat.h         |    4 +-
 kernel/locking/rwsem-xadd.c             |   92 +-
 kernel/rcu/sync.c                       |   14 +
 kernel/stop_machine.c                   |   42 +-
 31 files changed, 3540 insertions(+), 1337 deletions(-)
 create mode 100644 Documentation/ko_KR/memory-barriers.txt
 delete mode 100644 Documentation/locking/lglock.txt
 delete mode 100644 include/linux/lglock.h
 delete mode 100644 kernel/locking/lglock.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/ko_KR/memory-barriers.txt b/Documentation/ko_KR/memory-barriers.txt</span>
new file mode 100644
<span class="p_header">index 000000000000..34d3d380893d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/Documentation/ko_KR/memory-barriers.txt</span>
<span class="p_chunk">@@ -0,0 +1,3135 @@</span> <span class="p_context"></span>
<span class="p_add">+NOTE:</span>
<span class="p_add">+This is a version of Documentation/memory-barriers.txt translated into Korean.</span>
<span class="p_add">+This document is maintained by SeongJae Park &lt;sj38.park@gmail.com&gt;.</span>
<span class="p_add">+If you find any difference between this document and the original file or</span>
<span class="p_add">+a problem with the translation, please contact the maintainer of this file.</span>
<span class="p_add">+</span>
<span class="p_add">+Please also note that the purpose of this file is to be easier to</span>
<span class="p_add">+read for non English (read: Korean) speakers and is not intended as</span>
<span class="p_add">+a fork.  So if you have any comments or updates for this file please</span>
<span class="p_add">+update the original English file first.  The English version is</span>
<span class="p_add">+definitive, and readers should look there if they have any doubt.</span>
<span class="p_add">+</span>
<span class="p_add">+===================================</span>
<span class="p_add">+이 문서는</span>
<span class="p_add">+Documentation/memory-barriers.txt</span>
<span class="p_add">+의 한글 번역입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+역자： 박성재 &lt;sj38.park@gmail.com&gt;</span>
<span class="p_add">+===================================</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+			 =========================</span>
<span class="p_add">+			 리눅스 커널 메모리 배리어</span>
<span class="p_add">+			 =========================</span>
<span class="p_add">+</span>
<span class="p_add">+저자: David Howells &lt;dhowells@redhat.com&gt;</span>
<span class="p_add">+      Paul E. McKenney &lt;paulmck@linux.vnet.ibm.com&gt;</span>
<span class="p_add">+      Will Deacon &lt;will.deacon@arm.com&gt;</span>
<span class="p_add">+      Peter Zijlstra &lt;peterz@infradead.org&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+========</span>
<span class="p_add">+면책조항</span>
<span class="p_add">+========</span>
<span class="p_add">+</span>
<span class="p_add">+이 문서는 명세서가 아닙니다; 이 문서는 완벽하지 않은데, 간결성을 위해 의도된</span>
<span class="p_add">+부분도 있고, 의도하진 않았지만 사람에 의해 쓰였다보니 불완전한 부분도 있습니다.</span>
<span class="p_add">+이 문서는 리눅스에서 제공하는 다양한 메모리 배리어들을 사용하기 위한</span>
<span class="p_add">+안내서입니다만, 뭔가 이상하다 싶으면 (그런게 많을 겁니다) 질문을 부탁드립니다.</span>
<span class="p_add">+</span>
<span class="p_add">+다시 말하지만, 이 문서는 리눅스가 하드웨어에 기대하는 사항에 대한 명세서가</span>
<span class="p_add">+아닙니다.</span>
<span class="p_add">+</span>
<span class="p_add">+이 문서의 목적은 두가지입니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (1) 어떤 특정 배리어에 대해 기대할 수 있는 최소한의 기능을 명세하기 위해서,</span>
<span class="p_add">+     그리고</span>
<span class="p_add">+</span>
<span class="p_add">+ (2) 사용 가능한 배리어들에 대해 어떻게 사용해야 하는지에 대한 안내를 제공하기</span>
<span class="p_add">+     위해서.</span>
<span class="p_add">+</span>
<span class="p_add">+어떤 아키텍쳐는 특정한 배리어들에 대해서는 여기서 이야기하는 최소한의</span>
<span class="p_add">+요구사항들보다 많은 기능을 제공할 수도 있습니다만, 여기서 이야기하는</span>
<span class="p_add">+요구사항들을 충족하지 않는 아키텍쳐가 있다면 그 아키텍쳐가 잘못된 것이란 점을</span>
<span class="p_add">+알아두시기 바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+또한, 특정 아키텍쳐에서 일부 배리어는 해당 아키텍쳐의 특수한 동작 방식으로 인해</span>
<span class="p_add">+해당 배리어의 명시적 사용이 불필요해서 no-op 이 될수도 있음을 알아두시기</span>
<span class="p_add">+바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+역자: 본 번역 역시 완벽하지 않은데, 이 역시 부분적으로는 의도된 것이기도</span>
<span class="p_add">+합니다.  여타 기술 문서들이 그렇듯 완벽한 이해를 위해서는 번역문과 원문을 함께</span>
<span class="p_add">+읽으시되 번역문을 하나의 가이드로 활용하시길 추천드리며, 발견되는 오역 등에</span>
<span class="p_add">+대해서는 언제든 의견을 부탁드립니다.  과한 번역으로 인한 오해를 최소화하기 위해</span>
<span class="p_add">+애매한 부분이 있을 경우에는 어색함이 있더라도 원래의 용어를 차용합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+=====</span>
<span class="p_add">+목차:</span>
<span class="p_add">+=====</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 추상 메모리 액세스 모델.</span>
<span class="p_add">+</span>
<span class="p_add">+     - 디바이스 오퍼레이션.</span>
<span class="p_add">+     - 보장사항.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 메모리 배리어란 무엇인가?</span>
<span class="p_add">+</span>
<span class="p_add">+     - 메모리 배리어의 종류.</span>
<span class="p_add">+     - 메모리 배리어에 대해 가정해선 안될 것.</span>
<span class="p_add">+     - 데이터 의존성 배리어.</span>
<span class="p_add">+     - 컨트롤 의존성.</span>
<span class="p_add">+     - SMP 배리어 짝맞추기.</span>
<span class="p_add">+     - 메모리 배리어 시퀀스의 예.</span>
<span class="p_add">+     - 읽기 메모리 배리어 vs 로드 예측.</span>
<span class="p_add">+     - 이행성</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 명시적 커널 배리어.</span>
<span class="p_add">+</span>
<span class="p_add">+     - 컴파일러 배리어.</span>
<span class="p_add">+     - CPU 메모리 배리어.</span>
<span class="p_add">+     - MMIO 쓰기 배리어.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 암묵적 커널 메모리 배리어.</span>
<span class="p_add">+</span>
<span class="p_add">+     - 락 Acquisition 함수.</span>
<span class="p_add">+     - 인터럽트 비활성화 함수.</span>
<span class="p_add">+     - 슬립과 웨이크업 함수.</span>
<span class="p_add">+     - 그외의 함수들.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) CPU 간 ACQUIRING 배리어의 효과.</span>
<span class="p_add">+</span>
<span class="p_add">+     - Acquire vs 메모리 액세스.</span>
<span class="p_add">+     - Acquire vs I/O 액세스.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 메모리 배리어가 필요한 곳</span>
<span class="p_add">+</span>
<span class="p_add">+     - 프로세서간 상호 작용.</span>
<span class="p_add">+     - 어토믹 오퍼레이션.</span>
<span class="p_add">+     - 디바이스 액세스.</span>
<span class="p_add">+     - 인터럽트.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 커널 I/O 배리어의 효과.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 가정되는 가장 완화된 실행 순서 모델.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) CPU 캐시의 영향.</span>
<span class="p_add">+</span>
<span class="p_add">+     - 캐시 일관성.</span>
<span class="p_add">+     - 캐시 일관성 vs DMA.</span>
<span class="p_add">+     - 캐시 일관성 vs MMIO.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) CPU 들이 저지르는 일들.</span>
<span class="p_add">+</span>
<span class="p_add">+     - 그리고, Alpha 가 있다.</span>
<span class="p_add">+     - 가상 머신 게스트.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 사용 예.</span>
<span class="p_add">+</span>
<span class="p_add">+     - 순환식 버퍼.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 참고 문헌.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+=======================</span>
<span class="p_add">+추상 메모리 액세스 모델</span>
<span class="p_add">+=======================</span>
<span class="p_add">+</span>
<span class="p_add">+다음과 같이 추상화된 시스템 모델을 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+		            :                :</span>
<span class="p_add">+		            :                :</span>
<span class="p_add">+		            :                :</span>
<span class="p_add">+		+-------+   :   +--------+   :   +-------+</span>
<span class="p_add">+		|       |   :   |        |   :   |       |</span>
<span class="p_add">+		|       |   :   |        |   :   |       |</span>
<span class="p_add">+		| CPU 1 |&lt;-----&gt;| Memory |&lt;-----&gt;| CPU 2 |</span>
<span class="p_add">+		|       |   :   |        |   :   |       |</span>
<span class="p_add">+		|       |   :   |        |   :   |       |</span>
<span class="p_add">+		+-------+   :   +--------+   :   +-------+</span>
<span class="p_add">+		    ^       :       ^        :       ^</span>
<span class="p_add">+		    |       :       |        :       |</span>
<span class="p_add">+		    |       :       |        :       |</span>
<span class="p_add">+		    |       :       v        :       |</span>
<span class="p_add">+		    |       :   +--------+   :       |</span>
<span class="p_add">+		    |       :   |        |   :       |</span>
<span class="p_add">+		    |       :   |        |   :       |</span>
<span class="p_add">+		    +----------&gt;| Device |&lt;----------+</span>
<span class="p_add">+		            :   |        |   :</span>
<span class="p_add">+		            :   |        |   :</span>
<span class="p_add">+		            :   +--------+   :</span>
<span class="p_add">+		            :                :</span>
<span class="p_add">+</span>
<span class="p_add">+프로그램은 여러 메모리 액세스 오퍼레이션을 발생시키고, 각각의 CPU 는 그런</span>
<span class="p_add">+프로그램들을 실행합니다.  추상화된 CPU 모델에서 메모리 오퍼레이션들의 순서는</span>
<span class="p_add">+매우 완화되어 있고, CPU 는 프로그램이 인과관계를 어기지 않는 상태로 관리된다고</span>
<span class="p_add">+보일 수만 있다면 메모리 오퍼레이션을 자신이 원하는 어떤 순서대로든 재배치해</span>
<span class="p_add">+동작시킬 수 있습니다.  비슷하게, 컴파일러 또한 프로그램의 정상적 동작을 해치지</span>
<span class="p_add">+않는 한도 내에서는 어떤 순서로든 자신이 원하는 대로 인스트럭션을 재배치 할 수</span>
<span class="p_add">+있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+따라서 위의 다이어그램에서 한 CPU가 동작시키는 메모리 오퍼레이션이 만들어내는</span>
<span class="p_add">+변화는 해당 오퍼레이션이 CPU 와 시스템의 다른 부분들 사이의 인터페이스(점선)를</span>
<span class="p_add">+지나가면서 시스템의 나머지 부분들에 인지됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+예를 들어, 다음의 일련의 이벤트들을 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		CPU 2</span>
<span class="p_add">+	===============	===============</span>
<span class="p_add">+	{ A == 1; B == 2 }</span>
<span class="p_add">+	A = 3;		x = B;</span>
<span class="p_add">+	B = 4;		y = A;</span>
<span class="p_add">+</span>
<span class="p_add">+다이어그램의 가운데에 위치한 메모리 시스템에 보여지게 되는 액세스들은 다음의 총</span>
<span class="p_add">+24개의 조합으로 재구성될 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	STORE A=3,	STORE B=4,	y=LOAD A-&gt;3,	x=LOAD B-&gt;4</span>
<span class="p_add">+	STORE A=3,	STORE B=4,	x=LOAD B-&gt;4,	y=LOAD A-&gt;3</span>
<span class="p_add">+	STORE A=3,	y=LOAD A-&gt;3,	STORE B=4,	x=LOAD B-&gt;4</span>
<span class="p_add">+	STORE A=3,	y=LOAD A-&gt;3,	x=LOAD B-&gt;2,	STORE B=4</span>
<span class="p_add">+	STORE A=3,	x=LOAD B-&gt;2,	STORE B=4,	y=LOAD A-&gt;3</span>
<span class="p_add">+	STORE A=3,	x=LOAD B-&gt;2,	y=LOAD A-&gt;3,	STORE B=4</span>
<span class="p_add">+	STORE B=4,	STORE A=3,	y=LOAD A-&gt;3,	x=LOAD B-&gt;4</span>
<span class="p_add">+	STORE B=4, ...</span>
<span class="p_add">+	...</span>
<span class="p_add">+</span>
<span class="p_add">+따라서 다음의 네가지 조합의 값들이 나올 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	x == 2, y == 1</span>
<span class="p_add">+	x == 2, y == 3</span>
<span class="p_add">+	x == 4, y == 1</span>
<span class="p_add">+	x == 4, y == 3</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+한발 더 나아가서, 한 CPU 가 메모리 시스템에 반영한 스토어 오퍼레이션들의 결과는</span>
<span class="p_add">+다른 CPU 에서의 로드 오퍼레이션을 통해 인지되는데, 이 때 스토어가 반영된 순서와</span>
<span class="p_add">+다른 순서로 인지될 수도 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+예로, 아래의 일련의 이벤트들을 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		CPU 2</span>
<span class="p_add">+	===============	===============</span>
<span class="p_add">+	{ A == 1, B == 2, C == 3, P == &amp;A, Q == &amp;C }</span>
<span class="p_add">+	B = 4;		Q = P;</span>
<span class="p_add">+	P = &amp;B		D = *Q;</span>
<span class="p_add">+</span>
<span class="p_add">+D 로 읽혀지는 값은 CPU 2 에서 P 로부터 읽혀진 주소값에 의존적이기 때문에 여기엔</span>
<span class="p_add">+분명한 데이터 의존성이 있습니다.  하지만 이 이벤트들의 실행 결과로는 아래의</span>
<span class="p_add">+결과들이 모두 나타날 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	(Q == &amp;A) and (D == 1)</span>
<span class="p_add">+	(Q == &amp;B) and (D == 2)</span>
<span class="p_add">+	(Q == &amp;B) and (D == 4)</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 2 는 *Q 의 로드를 요청하기 전에 P 를 Q 에 넣기 때문에 D 에 C 를 집어넣는</span>
<span class="p_add">+일은 없음을 알아두세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+디바이스 오퍼레이션</span>
<span class="p_add">+-------------------</span>
<span class="p_add">+</span>
<span class="p_add">+일부 디바이스는 자신의 컨트롤 인터페이스를 메모리의 특정 영역으로 매핑해서</span>
<span class="p_add">+제공하는데(Memory mapped I/O), 해당 컨트롤 레지스터에 접근하는 순서는 매우</span>
<span class="p_add">+중요합니다.  예를 들어, 어드레스 포트 레지스터 (A) 와 데이터 포트 레지스터 (D)</span>
<span class="p_add">+를 통해 접근되는 내부 레지스터 집합을 갖는 이더넷 카드를 생각해 봅시다.  내부의</span>
<span class="p_add">+5번 레지스터를 읽기 위해 다음의 코드가 사용될 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	*A = 5;</span>
<span class="p_add">+	x = *D;</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, 이건 다음의 두 조합 중 하나로 만들어질 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	STORE *A = 5, x = LOAD *D</span>
<span class="p_add">+	x = LOAD *D, STORE *A = 5</span>
<span class="p_add">+</span>
<span class="p_add">+두번째 조합은 데이터를 읽어온 _후에_ 주소를 설정하므로, 오동작을 일으킬 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+보장사항</span>
<span class="p_add">+--------</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 에게 기대할 수 있는 최소한의 보장사항 몇가지가 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 어떤 CPU 든, 의존성이 존재하는 메모리 액세스들은 해당 CPU 자신에게</span>
<span class="p_add">+     있어서는 순서대로 메모리 시스템에 수행 요청됩니다. 즉, 다음에 대해서:</span>
<span class="p_add">+</span>
<span class="p_add">+	Q = READ_ONCE(P); smp_read_barrier_depends(); D = READ_ONCE(*Q);</span>
<span class="p_add">+</span>
<span class="p_add">+     CPU 는 다음과 같은 메모리 오퍼레이션 시퀀스를 수행 요청합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	Q = LOAD P, D = LOAD *Q</span>
<span class="p_add">+</span>
<span class="p_add">+     그리고 그 시퀀스 내에서의 순서는 항상 지켜집니다.  대부분의 시스템에서</span>
<span class="p_add">+     smp_read_barrier_depends() 는 아무일도 안하지만 DEC Alpha 에서는</span>
<span class="p_add">+     명시적으로 사용되어야 합니다.  보통의 경우에는 smp_read_barrier_depends()</span>
<span class="p_add">+     를 직접 사용하는 대신 rcu_dereference() 같은 것들을 사용해야 함을</span>
<span class="p_add">+     알아두세요.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 특정 CPU 내에서 겹치는 영역의 메모리에 행해지는 로드와 스토어 들은 해당</span>
<span class="p_add">+     CPU 안에서는 순서가 바뀌지 않은 것으로 보여집니다.  즉, 다음에 대해서:</span>
<span class="p_add">+</span>
<span class="p_add">+	a = READ_ONCE(*X); WRITE_ONCE(*X, b);</span>
<span class="p_add">+</span>
<span class="p_add">+     CPU 는 다음의 메모리 오퍼레이션 시퀀스만을 메모리에 요청할 겁니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	a = LOAD *X, STORE *X = b</span>
<span class="p_add">+</span>
<span class="p_add">+     그리고 다음에 대해서는:</span>
<span class="p_add">+</span>
<span class="p_add">+	WRITE_ONCE(*X, c); d = READ_ONCE(*X);</span>
<span class="p_add">+</span>
<span class="p_add">+     CPU 는 다음의 수행 요청만을 만들어 냅니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	STORE *X = c, d = LOAD *X</span>
<span class="p_add">+</span>
<span class="p_add">+     (로드 오퍼레이션과 스토어 오퍼레이션이 겹치는 메모리 영역에 대해</span>
<span class="p_add">+     수행된다면 해당 오퍼레이션들은 겹친다고 표현됩니다).</span>
<span class="p_add">+</span>
<span class="p_add">+그리고 _반드시_ 또는 _절대로_ 가정하거나 가정하지 말아야 하는 것들이 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 컴파일러가 READ_ONCE() 나 WRITE_ONCE() 로 보호되지 않은 메모리 액세스를</span>
<span class="p_add">+     당신이 원하는 대로 할 것이라는 가정은 _절대로_ 해선 안됩니다.  그것들이</span>
<span class="p_add">+     없다면, 컴파일러는 컴파일러 배리어 섹션에서 다루게 될, 모든 &quot;창의적인&quot;</span>
<span class="p_add">+     변경들을 만들어낼 권한을 갖게 됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 개별적인 로드와 스토어들이 주어진 순서대로 요청될 것이라는 가정은 _절대로_</span>
<span class="p_add">+     하지 말아야 합니다.  이 말은 곧:</span>
<span class="p_add">+</span>
<span class="p_add">+	X = *A; Y = *B; *D = Z;</span>
<span class="p_add">+</span>
<span class="p_add">+     는 다음의 것들 중 어느 것으로든 만들어질 수 있다는 의미입니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	X = LOAD *A,  Y = LOAD *B,  STORE *D = Z</span>
<span class="p_add">+	X = LOAD *A,  STORE *D = Z, Y = LOAD *B</span>
<span class="p_add">+	Y = LOAD *B,  X = LOAD *A,  STORE *D = Z</span>
<span class="p_add">+	Y = LOAD *B,  STORE *D = Z, X = LOAD *A</span>
<span class="p_add">+	STORE *D = Z, X = LOAD *A,  Y = LOAD *B</span>
<span class="p_add">+	STORE *D = Z, Y = LOAD *B,  X = LOAD *A</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 겹치는 메모리 액세스들은 합쳐지거나 버려질 수 있음을 _반드시_ 가정해야</span>
<span class="p_add">+     합니다.  다음의 코드는:</span>
<span class="p_add">+</span>
<span class="p_add">+	X = *A; Y = *(A + 4);</span>
<span class="p_add">+</span>
<span class="p_add">+     다음의 것들 중 뭐든 될 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	X = LOAD *A; Y = LOAD *(A + 4);</span>
<span class="p_add">+	Y = LOAD *(A + 4); X = LOAD *A;</span>
<span class="p_add">+	{X, Y} = LOAD {*A, *(A + 4) };</span>
<span class="p_add">+</span>
<span class="p_add">+     그리고:</span>
<span class="p_add">+</span>
<span class="p_add">+	*A = X; *(A + 4) = Y;</span>
<span class="p_add">+</span>
<span class="p_add">+     는 다음 중 뭐든 될 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	STORE *A = X; STORE *(A + 4) = Y;</span>
<span class="p_add">+	STORE *(A + 4) = Y; STORE *A = X;</span>
<span class="p_add">+	STORE {*A, *(A + 4) } = {X, Y};</span>
<span class="p_add">+</span>
<span class="p_add">+그리고 보장사항에 반대되는 것들(anti-guarantees)이 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 이 보장사항들은 bitfield 에는 적용되지 않는데, 컴파일러들은 bitfield 를</span>
<span class="p_add">+     수정하는 코드를 생성할 때 원자성 없는(non-atomic) 읽고-수정하고-쓰는</span>
<span class="p_add">+     인스트럭션들의 조합을 만드는 경우가 많기 때문입니다.  병렬 알고리즘의</span>
<span class="p_add">+     동기화에 bitfield 를 사용하려 하지 마십시오.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) bitfield 들이 여러 락으로 보호되는 경우라 하더라도, 하나의 bitfield 의</span>
<span class="p_add">+     모든 필드들은 하나의 락으로 보호되어야 합니다.  만약 한 bitfield 의 두</span>
<span class="p_add">+     필드가 서로 다른 락으로 보호된다면, 컴파일러의 원자성 없는</span>
<span class="p_add">+     읽고-수정하고-쓰는 인스트럭션 조합은 한 필드에의 업데이트가 근처의</span>
<span class="p_add">+     필드에도 영향을 끼치게 할 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 이 보장사항들은 적절하게 정렬되고 크기가 잡힌 스칼라 변수들에 대해서만</span>
<span class="p_add">+     적용됩니다.  &quot;적절하게 크기가 잡힌&quot; 이라함은 현재로써는 &quot;char&quot;, &quot;short&quot;,</span>
<span class="p_add">+     &quot;int&quot; 그리고 &quot;long&quot; 과 같은 크기의 변수들을 의미합니다.  &quot;적절하게 정렬된&quot;</span>
<span class="p_add">+     은 자연스런 정렬을 의미하는데, 따라서 &quot;char&quot; 에 대해서는 아무 제약이 없고,</span>
<span class="p_add">+     &quot;short&quot; 에 대해서는 2바이트 정렬을, &quot;int&quot; 에는 4바이트 정렬을, 그리고</span>
<span class="p_add">+     &quot;long&quot; 에 대해서는 32-bit 시스템인지 64-bit 시스템인지에 따라 4바이트 또는</span>
<span class="p_add">+     8바이트 정렬을 의미합니다.  이 보장사항들은 C11 표준에서 소개되었으므로,</span>
<span class="p_add">+     C11 전의 오래된 컴파일러(예를 들어, gcc 4.6) 를 사용할 때엔 주의하시기</span>
<span class="p_add">+     바랍니다.  표준에 이 보장사항들은 &quot;memory location&quot; 을 정의하는 3.14</span>
<span class="p_add">+     섹션에 다음과 같이 설명되어 있습니다:</span>
<span class="p_add">+     (역자: 인용문이므로 번역하지 않습니다)</span>
<span class="p_add">+</span>
<span class="p_add">+	memory location</span>
<span class="p_add">+		either an object of scalar type, or a maximal sequence</span>
<span class="p_add">+		of adjacent bit-fields all having nonzero width</span>
<span class="p_add">+</span>
<span class="p_add">+		NOTE 1: Two threads of execution can update and access</span>
<span class="p_add">+		separate memory locations without interfering with</span>
<span class="p_add">+		each other.</span>
<span class="p_add">+</span>
<span class="p_add">+		NOTE 2: A bit-field and an adjacent non-bit-field member</span>
<span class="p_add">+		are in separate memory locations. The same applies</span>
<span class="p_add">+		to two bit-fields, if one is declared inside a nested</span>
<span class="p_add">+		structure declaration and the other is not, or if the two</span>
<span class="p_add">+		are separated by a zero-length bit-field declaration,</span>
<span class="p_add">+		or if they are separated by a non-bit-field member</span>
<span class="p_add">+		declaration. It is not safe to concurrently update two</span>
<span class="p_add">+		bit-fields in the same structure if all members declared</span>
<span class="p_add">+		between them are also bit-fields, no matter what the</span>
<span class="p_add">+		sizes of those intervening bit-fields happen to be.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+=========================</span>
<span class="p_add">+메모리 배리어란 무엇인가?</span>
<span class="p_add">+=========================</span>
<span class="p_add">+</span>
<span class="p_add">+앞에서 봤듯이, 상호간 의존성이 없는 메모리 오퍼레이션들은 실제로는 무작위적</span>
<span class="p_add">+순서로 수행될 수 있으며, 이는 CPU 와 CPU 간의 상호작용이나 I/O 에 문제가 될 수</span>
<span class="p_add">+있습니다.  따라서 컴파일러와 CPU 가 순서를 바꾸는데 제약을 걸 수 있도록 개입할</span>
<span class="p_add">+수 있는 어떤 방법이 필요합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+메모리 배리어는 그런 개입 수단입니다.  메모리 배리어는 배리어를 사이에 둔 앞과</span>
<span class="p_add">+뒤 양측의 메모리 오퍼레이션들 간에 부분적 순서가 존재하도록 하는 효과를 줍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+시스템의 CPU 들과 여러 디바이스들은 성능을 올리기 위해 명령어 재배치, 실행</span>
<span class="p_add">+유예, 메모리 오퍼레이션들의 조합, 예측적 로드(speculative load), 브랜치</span>
<span class="p_add">+예측(speculative branch prediction), 다양한 종류의 캐싱(caching) 등의 다양한</span>
<span class="p_add">+트릭을 사용할 수 있기 때문에 이런 강제력은 중요합니다.  메모리 배리어들은 이런</span>
<span class="p_add">+트릭들을 무효로 하거나 억제하는 목적으로 사용되어져서 코드가 여러 CPU 와</span>
<span class="p_add">+디바이스들 간의 상호작용을 정상적으로 제어할 수 있게 해줍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+메모리 배리어의 종류</span>
<span class="p_add">+--------------------</span>
<span class="p_add">+</span>
<span class="p_add">+메모리 배리어는 네개의 기본 타입으로 분류됩니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (1) 쓰기 (또는 스토어) 메모리 배리어.</span>
<span class="p_add">+</span>
<span class="p_add">+     쓰기 메모리 배리어는 시스템의 다른 컴포넌트들에 해당 배리어보다 앞서</span>
<span class="p_add">+     명시된 모든 STORE 오퍼레이션들이 해당 배리어 뒤에 명시된 모든 STORE</span>
<span class="p_add">+     오퍼레이션들보다 먼저 수행된 것으로 보일 것을 보장합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     쓰기 배리어는 스토어 오퍼레이션들에 대한 부분적 순서 세우기입니다; 로드</span>
<span class="p_add">+     오퍼레이션들에 대해서는 어떤 영향도 끼치지 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     CPU 는 시간의 흐름에 따라 메모리 시스템에 일련의 스토어 오퍼레이션들을</span>
<span class="p_add">+     하나씩 요청해 집어넣습니다.  쓰기 배리어 앞의 모든 스토어 오퍼레이션들은</span>
<span class="p_add">+     쓰기 배리어 뒤의 모든 스토어 오퍼레이션들보다 _앞서_ 수행될 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     [!] 쓰기 배리어들은 읽기 또는 데이터 의존성 배리어와 함께 짝을 맞춰</span>
<span class="p_add">+     사용되어야만 함을 알아두세요; &quot;SMP 배리어 짝맞추기&quot; 서브섹션을 참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+ (2) 데이터 의존성 배리어.</span>
<span class="p_add">+</span>
<span class="p_add">+     데이터 의존성 배리어는 읽기 배리어의 보다 완화된 형태입니다.  두개의 로드</span>
<span class="p_add">+     오퍼레이션이 있고 두번째 것이 첫번째 것의 결과에 의존하고 있을 때(예:</span>
<span class="p_add">+     두번째 로드가 참조할 주소를 첫번째 로드가 읽는 경우), 두번째 로드가 읽어올</span>
<span class="p_add">+     데이터는 첫번째 로드에 의해 그 주소가 얻어지기 전에 업데이트 되어 있음을</span>
<span class="p_add">+     보장하기 위해서 데이터 의존성 배리어가 필요할 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     데이터 의존성 배리어는 상호 의존적인 로드 오퍼레이션들 사이의 부분적 순서</span>
<span class="p_add">+     세우기입니다; 스토어 오퍼레이션들이나 독립적인 로드들, 또는 중복되는</span>
<span class="p_add">+     로드들에 대해서는 어떤 영향도 끼치지 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     (1) 에서 언급했듯이, 시스템의 CPU 들은 메모리 시스템에 일련의 스토어</span>
<span class="p_add">+     오퍼레이션들을 던져 넣고 있으며, 거기에 관심이 있는 다른 CPU 는 그</span>
<span class="p_add">+     오퍼레이션들을 메모리 시스템이 실행한 결과를 인지할 수 있습니다.  이처럼</span>
<span class="p_add">+     다른 CPU 의 스토어 오퍼레이션의 결과에 관심을 두고 있는 CPU 가 수행 요청한</span>
<span class="p_add">+     데이터 의존성 배리어는, 배리어 앞의 어떤 로드 오퍼레이션이 다른 CPU 에서</span>
<span class="p_add">+     던져 넣은 스토어 오퍼레이션과 같은 영역을 향했다면, 그런 스토어</span>
<span class="p_add">+     오퍼레이션들이 만들어내는 결과가 데이터 의존성 배리어 뒤의 로드</span>
<span class="p_add">+     오퍼레이션들에게는 보일 것을 보장합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     이 순서 세우기 제약에 대한 그림을 보기 위해선 &quot;메모리 배리어 시퀀스의 예&quot;</span>
<span class="p_add">+     서브섹션을 참고하시기 바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     [!] 첫번째 로드는 반드시 _데이터_ 의존성을 가져야지 컨트롤 의존성을 가져야</span>
<span class="p_add">+     하는게 아님을 알아두십시오.  만약 두번째 로드를 위한 주소가 첫번째 로드에</span>
<span class="p_add">+     의존적이지만 그 의존성은 조건적이지 그 주소 자체를 가져오는게 아니라면,</span>
<span class="p_add">+     그것은 _컨트롤_ 의존성이고, 이 경우에는 읽기 배리어나 그보다 강력한</span>
<span class="p_add">+     무언가가 필요합니다.  더 자세한 내용을 위해서는 &quot;컨트롤 의존성&quot; 서브섹션을</span>
<span class="p_add">+     참고하시기 바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     [!] 데이터 의존성 배리어는 보통 쓰기 배리어들과 함께 짝을 맞춰 사용되어야</span>
<span class="p_add">+     합니다; &quot;SMP 배리어 짝맞추기&quot; 서브섹션을 참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+ (3) 읽기 (또는 로드) 메모리 배리어.</span>
<span class="p_add">+</span>
<span class="p_add">+     읽기 배리어는 데이터 의존성 배리어 기능의 보장사항에 더해서 배리어보다</span>
<span class="p_add">+     앞서 명시된 모든 LOAD 오퍼레이션들이 배리어 뒤에 명시되는 모든 LOAD</span>
<span class="p_add">+     오퍼레이션들보다 먼저 행해진 것으로 시스템의 다른 컴포넌트들에 보여질 것을</span>
<span class="p_add">+     보장합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     읽기 배리어는 로드 오퍼레이션에 행해지는 부분적 순서 세우기입니다; 스토어</span>
<span class="p_add">+     오퍼레이션에 대해서는 어떤 영향도 끼치지 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     읽기 메모리 배리어는 데이터 의존성 배리어를 내장하므로 데이터 의존성</span>
<span class="p_add">+     배리어를 대신할 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     [!] 읽기 배리어는 일반적으로 쓰기 배리어들과 함께 짝을 맞춰 사용되어야</span>
<span class="p_add">+     합니다; &quot;SMP 배리어 짝맞추기&quot; 서브섹션을 참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+ (4) 범용 메모리 배리어.</span>
<span class="p_add">+</span>
<span class="p_add">+     범용(general) 메모리 배리어는 배리어보다 앞서 명시된 모든 LOAD 와 STORE</span>
<span class="p_add">+     오퍼레이션들이 배리어 뒤에 명시된 모든 LOAD 와 STORE 오퍼레이션들보다</span>
<span class="p_add">+     먼저 수행된 것으로 시스템의 나머지 컴포넌트들에 보이게 됨을 보장합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     범용 메모리 배리어는 로드와 스토어 모두에 대한 부분적 순서 세우기입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     범용 메모리 배리어는 읽기 메모리 배리어, 쓰기 메모리 배리어 모두를</span>
<span class="p_add">+     내장하므로, 두 배리어를 모두 대신할 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+그리고 두개의 명시적이지 않은 타입이 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (5) ACQUIRE 오퍼레이션.</span>
<span class="p_add">+</span>
<span class="p_add">+     이 타입의 오퍼레이션은 단방향의 투과성 배리어처럼 동작합니다.  ACQUIRE</span>
<span class="p_add">+     오퍼레이션 뒤의 모든 메모리 오퍼레이션들이 ACQUIRE 오퍼레이션 후에</span>
<span class="p_add">+     일어난 것으로 시스템의 나머지 컴포넌트들에 보이게 될 것이 보장됩니다.</span>
<span class="p_add">+     LOCK 오퍼레이션과 smp_load_acquire(), smp_cond_acquire() 오퍼레이션도</span>
<span class="p_add">+     ACQUIRE 오퍼레이션에 포함됩니다.  smp_cond_acquire() 오퍼레이션은 컨트롤</span>
<span class="p_add">+     의존성과 smp_rmb() 를 사용해서 ACQUIRE 의 의미적 요구사항(semantic)을</span>
<span class="p_add">+     충족시킵니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     ACQUIRE 오퍼레이션 앞의 메모리 오퍼레이션들은 ACQUIRE 오퍼레이션 완료 후에</span>
<span class="p_add">+     수행된 것처럼 보일 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     ACQUIRE 오퍼레이션은 거의 항상 RELEASE 오퍼레이션과 짝을 지어 사용되어야</span>
<span class="p_add">+     합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+ (6) RELEASE 오퍼레이션.</span>
<span class="p_add">+</span>
<span class="p_add">+     이 타입의 오퍼레이션들도 단방향 투과성 배리어처럼 동작합니다.  RELEASE</span>
<span class="p_add">+     오퍼레이션 앞의 모든 메모리 오퍼레이션들은 RELEASE 오퍼레이션 전에 완료된</span>
<span class="p_add">+     것으로 시스템의 다른 컴포넌트들에 보여질 것이 보장됩니다.  UNLOCK 류의</span>
<span class="p_add">+     오퍼레이션들과 smp_store_release() 오퍼레이션도 RELEASE 오퍼레이션의</span>
<span class="p_add">+     일종입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     RELEASE 오퍼레이션 뒤의 메모리 오퍼레이션들은 RELEASE 오퍼레이션이</span>
<span class="p_add">+     완료되기 전에 행해진 것처럼 보일 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     ACQUIRE 와 RELEASE 오퍼레이션의 사용은 일반적으로 다른 메모리 배리어의</span>
<span class="p_add">+     필요성을 없앱니다 (하지만 &quot;MMIO 쓰기 배리어&quot; 서브섹션에서 설명되는 예외를</span>
<span class="p_add">+     알아두세요).  또한, RELEASE+ACQUIRE 조합은 범용 메모리 배리어처럼 동작할</span>
<span class="p_add">+     것을 보장하지 -않습니다-.  하지만, 어떤 변수에 대한 RELEASE 오퍼레이션을</span>
<span class="p_add">+     앞서는 메모리 액세스들의 수행 결과는 이 RELEASE 오퍼레이션을 뒤이어 같은</span>
<span class="p_add">+     변수에 대해 수행된 ACQUIRE 오퍼레이션을 뒤따르는 메모리 액세스에는 보여질</span>
<span class="p_add">+     것이 보장됩니다.  다르게 말하자면, 주어진 변수의 크리티컬 섹션에서는, 해당</span>
<span class="p_add">+     변수에 대한 앞의 크리티컬 섹션에서의 모든 액세스들이 완료되었을 것을</span>
<span class="p_add">+     보장합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     즉, ACQUIRE 는 최소한의 &quot;취득&quot; 동작처럼, 그리고 RELEASE 는 최소한의 &quot;공개&quot;</span>
<span class="p_add">+     처럼 동작한다는 의미입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+atomic_ops.txt 에서 설명되는 어토믹 오퍼레이션들 중에는 완전히 순서잡힌 것들과</span>
<span class="p_add">+(배리어를 사용하지 않는) 완화된 순서의 것들 외에 ACQUIRE 와 RELEASE 부류의</span>
<span class="p_add">+것들도 존재합니다.  로드와 스토어를 모두 수행하는 조합된 어토믹 오퍼레이션에서,</span>
<span class="p_add">+ACQUIRE 는 해당 오퍼레이션의 로드 부분에만 적용되고 RELEASE 는 해당</span>
<span class="p_add">+오퍼레이션의 스토어 부분에만 적용됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+메모리 배리어들은 두 CPU 간, 또는 CPU 와 디바이스 간에 상호작용의 가능성이 있을</span>
<span class="p_add">+때에만 필요합니다.  만약 어떤 코드에 그런 상호작용이 없을 것이 보장된다면, 해당</span>
<span class="p_add">+코드에서는 메모리 배리어를 사용할 필요가 없습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+이것들은 _최소한의_ 보장사항들임을 알아두세요.  다른 아키텍쳐에서는 더 강력한</span>
<span class="p_add">+보장사항을 제공할 수도 있습니다만, 그런 보장사항은 아키텍쳐 종속적 코드 이외의</span>
<span class="p_add">+부분에서는 신뢰되지 _않을_ 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+메모리 배리어에 대해 가정해선 안될 것</span>
<span class="p_add">+-------------------------------------</span>
<span class="p_add">+</span>
<span class="p_add">+리눅스 커널 메모리 배리어들이 보장하지 않는 것들이 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 메모리 배리어 앞에서 명시된 어떤 메모리 액세스도 메모리 배리어 명령의 수행</span>
<span class="p_add">+     완료 시점까지 _완료_ 될 것이란 보장은 없습니다; 배리어가 하는 일은 CPU 의</span>
<span class="p_add">+     액세스 큐에 특정 타입의 액세스들은 넘을 수 없는 선을 긋는 것으로 생각될 수</span>
<span class="p_add">+     있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 한 CPU 에서 메모리 배리어를 수행하는게 시스템의 다른 CPU 나 하드웨어에</span>
<span class="p_add">+     어떤 직접적인 영향을 끼친다는 보장은 존재하지 않습니다.  배리어 수행이</span>
<span class="p_add">+     만드는 간접적 영향은 두번째 CPU 가 첫번째 CPU 의 액세스들의 결과를</span>
<span class="p_add">+     바라보는 순서가 됩니다만, 다음 항목을 보세요:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 첫번째 CPU 가 두번째 CPU 의 메모리 액세스들의 결과를 바라볼 때, _설령_</span>
<span class="p_add">+     두번째 CPU 가 메모리 배리어를 사용한다 해도, 첫번째 CPU _또한_ 그에 맞는</span>
<span class="p_add">+     메모리 배리어를 사용하지 않는다면 (&quot;SMP 배리어 짝맞추기&quot; 서브섹션을</span>
<span class="p_add">+     참고하세요) 그 결과가 올바른 순서로 보여진다는 보장은 없습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) CPU 바깥의 하드웨어[*] 가 메모리 액세스들의 순서를 바꾸지 않는다는 보장은</span>
<span class="p_add">+     존재하지 않습니다.  CPU 캐시 일관성 메커니즘은 메모리 배리어의 간접적</span>
<span class="p_add">+     영향을 CPU 사이에 전파하긴 하지만, 순서대로 전파하지는 않을 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+	[*] 버스 마스터링 DMA 와 일관성에 대해서는 다음을 참고하시기 바랍니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	    Documentation/PCI/pci.txt</span>
<span class="p_add">+	    Documentation/DMA-API-HOWTO.txt</span>
<span class="p_add">+	    Documentation/DMA-API.txt</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+데이터 의존성 배리어</span>
<span class="p_add">+--------------------</span>
<span class="p_add">+</span>
<span class="p_add">+데이터 의존성 배리어의 사용에 있어 지켜야 하는 사항들은 약간 미묘하고, 데이터</span>
<span class="p_add">+의존성 배리어가 사용되어야 하는 상황도 항상 명백하지는 않습니다.  설명을 위해</span>
<span class="p_add">+다음의 이벤트 시퀀스를 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		      CPU 2</span>
<span class="p_add">+	===============	      ===============</span>
<span class="p_add">+	{ A == 1, B == 2, C == 3, P == &amp;A, Q == &amp;C }</span>
<span class="p_add">+	B = 4;</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	WRITE_ONCE(P, &amp;B)</span>
<span class="p_add">+			      Q = READ_ONCE(P);</span>
<span class="p_add">+			      D = *Q;</span>
<span class="p_add">+</span>
<span class="p_add">+여기엔 분명한 데이터 의존성이 존재하므로, 이 시퀀스가 끝났을 때 Q 는 &amp;A 또는 &amp;B</span>
<span class="p_add">+일 것이고, 따라서:</span>
<span class="p_add">+</span>
<span class="p_add">+	(Q == &amp;A) 는 (D == 1) 를,</span>
<span class="p_add">+	(Q == &amp;B) 는 (D == 4) 를 의미합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+하지만!  CPU 2 는 B 의 업데이트를 인식하기 전에 P 의 업데이트를 인식할 수 있고,</span>
<span class="p_add">+따라서 다음의 결과가 가능합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	(Q == &amp;B) and (D == 2) ????</span>
<span class="p_add">+</span>
<span class="p_add">+이런 결과는 일관성이나 인과 관계 유지가 실패한 것처럼 보일 수도 있겠지만,</span>
<span class="p_add">+그렇지 않습니다, 그리고 이 현상은 (DEC Alpha 와 같은) 여러 CPU 에서 실제로</span>
<span class="p_add">+발견될 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+이 문제 상황을 제대로 해결하기 위해, 데이터 의존성 배리어나 그보다 강화된</span>
<span class="p_add">+무언가가 주소를 읽어올 때와 데이터를 읽어올 때 사이에 추가되어야만 합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		      CPU 2</span>
<span class="p_add">+	===============	      ===============</span>
<span class="p_add">+	{ A == 1, B == 2, C == 3, P == &amp;A, Q == &amp;C }</span>
<span class="p_add">+	B = 4;</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	WRITE_ONCE(P, &amp;B);</span>
<span class="p_add">+			      Q = READ_ONCE(P);</span>
<span class="p_add">+			      &lt;데이터 의존성 배리어&gt;</span>
<span class="p_add">+			      D = *Q;</span>
<span class="p_add">+</span>
<span class="p_add">+이 변경은 앞의 처음 두가지 결과 중 하나만이 발생할 수 있고, 세번째의 결과는</span>
<span class="p_add">+발생할 수 없도록 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+데이터 의존성 배리어는 의존적 쓰기에 대해서도 순서를 잡아줍니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		      CPU 2</span>
<span class="p_add">+	===============	      ===============</span>
<span class="p_add">+	{ A == 1, B == 2, C = 3, P == &amp;A, Q == &amp;C }</span>
<span class="p_add">+	B = 4;</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	WRITE_ONCE(P, &amp;B);</span>
<span class="p_add">+			      Q = READ_ONCE(P);</span>
<span class="p_add">+			      &lt;데이터 의존성 배리어&gt;</span>
<span class="p_add">+			      *Q = 5;</span>
<span class="p_add">+</span>
<span class="p_add">+이 데이터 의존성 배리어는 Q 로의 읽기가 *Q 로의 스토어와 순서를 맞추게</span>
<span class="p_add">+해줍니다.  이는 다음과 같은 결과를 막습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	(Q == &amp;B) &amp;&amp; (B == 4)</span>
<span class="p_add">+</span>
<span class="p_add">+이런 패턴은 드물게 사용되어야 함을 알아 두시기 바랍니다.  무엇보다도, 의존성</span>
<span class="p_add">+순서 규칙의 의도는 쓰기 작업을 -예방- 해서 그로 인해 발생하는 비싼 캐시 미스도</span>
<span class="p_add">+없애려는 것입니다.  이 패턴은 드물게 발생하는 에러 조건 같은것들을 기록하는데</span>
<span class="p_add">+사용될 수 있고, 이렇게 배리어를 사용해 순서를 지키게 함으로써 그런 기록이</span>
<span class="p_add">+사라지는 것을 막습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+[!] 상당히 비직관적인 이 상황은 분리된 캐시를 가진 기계, 예를 들어 한 캐시</span>
<span class="p_add">+뱅크가 짝수번 캐시 라인을 처리하고 다른 뱅크는 홀수번 캐시 라인을 처리하는 기계</span>
<span class="p_add">+등에서 가장 잘 발생합니다.  포인터 P 는 홀수 번호의 캐시 라인에 있고, 변수 B 는</span>
<span class="p_add">+짝수 번호 캐시 라인에 있다고 생각해 봅시다.  그런 상태에서 읽기 작업을 하는 CPU</span>
<span class="p_add">+의 짝수번 뱅크는 할 일이 쌓여 매우 바쁘지만 홀수번 뱅크는 할 일이 없어 아무</span>
<span class="p_add">+일도 하지 않고  있었다면, 포인터 P 는 새 값 (&amp;B) 을, 그리고 변수 B 는 옛날 값</span>
<span class="p_add">+(2) 을 가지고 있는 상태가 보여질 수도 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+데이터 의존성 배리어는 매우 중요한데, 예를 들어 RCU 시스템에서 그렇습니다.</span>
<span class="p_add">+include/linux/rcupdate.h 의 rcu_assign_pointer() 와 rcu_dereference() 를</span>
<span class="p_add">+참고하세요.  여기서 데이터 의존성 배리어는 RCU 로 관리되는 포인터의 타겟을 현재</span>
<span class="p_add">+타겟에서 수정된 새로운 타겟으로 바꾸는 작업에서 새로 수정된 타겟이 초기화가</span>
<span class="p_add">+완료되지 않은 채로 보여지는 일이 일어나지 않게 해줍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+더 많은 예를 위해선 &quot;캐시 일관성&quot; 서브섹션을 참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+컨트롤 의존성</span>
<span class="p_add">+-------------</span>
<span class="p_add">+</span>
<span class="p_add">+로드-로드 컨트롤 의존성은 데이터 의존성 배리어만으로는 정확히 동작할 수가</span>
<span class="p_add">+없어서 읽기 메모리 배리어를 필요로 합니다.  아래의 코드를 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	if (q) {</span>
<span class="p_add">+		&lt;데이터 의존성 배리어&gt;  /* BUG: No data dependency!!! */</span>
<span class="p_add">+		p = READ_ONCE(b);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+이 코드는 원하는 대로의 효과를 내지 못할 수 있는데, 이 코드에는 데이터 의존성이</span>
<span class="p_add">+아니라 컨트롤 의존성이 존재하기 때문으로, 이런 상황에서 CPU 는 실행 속도를 더</span>
<span class="p_add">+빠르게 하기 위해 분기 조건의 결과를 예측하고 코드를 재배치 할 수 있어서 다른</span>
<span class="p_add">+CPU 는 b 로부터의 로드 오퍼레이션이 a 로부터의 로드 오퍼레이션보다 먼저 발생한</span>
<span class="p_add">+걸로 인식할 수 있습니다.  여기에 정말로 필요했던 건 다음과 같습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	if (q) {</span>
<span class="p_add">+		&lt;읽기 배리어&gt;</span>
<span class="p_add">+		p = READ_ONCE(b);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, 스토어 오퍼레이션은 예측적으로 수행되지 않습니다.  즉, 다음 예에서와</span>
<span class="p_add">+같이 로드-스토어 컨트롤 의존성이 존재하는 경우에는 순서가 -지켜진다-는</span>
<span class="p_add">+의미입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	if (q) {</span>
<span class="p_add">+		WRITE_ONCE(b, p);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+컨트롤 의존성은 보통 다른 타입의 배리어들과 짝을 맞춰 사용됩니다.  그렇다곤</span>
<span class="p_add">+하나, READ_ONCE() 는 반드시 사용해야 함을 부디 명심하세요!  READ_ONCE() 가</span>
<span class="p_add">+없다면, 컴파일러가 &#39;a&#39; 로부터의 로드를 &#39;a&#39; 로부터의 또다른 로드와, &#39;b&#39; 로의</span>
<span class="p_add">+스토어를 &#39;b&#39; 로의 또다른 스토어와 조합해 버려 매우 비직관적인 결과를 초래할 수</span>
<span class="p_add">+있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+이걸로 끝이 아닌게, 컴파일러가 변수 &#39;a&#39; 의 값이 항상 0이 아니라고 증명할 수</span>
<span class="p_add">+있다면, 앞의 예에서 &quot;if&quot; 문을 없애서 다음과 같이 최적화 할 수도 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = a;</span>
<span class="p_add">+	b = p;  /* BUG: Compiler and CPU can both reorder!!! */</span>
<span class="p_add">+</span>
<span class="p_add">+그러니 READ_ONCE() 를 반드시 사용하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+다음과 같이 &quot;if&quot; 문의 양갈래 브랜치에 모두 존재하는 동일한 스토어에 대해 순서를</span>
<span class="p_add">+강제하고 싶은 경우가 있을 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	if (q) {</span>
<span class="p_add">+		barrier();</span>
<span class="p_add">+		WRITE_ONCE(b, p);</span>
<span class="p_add">+		do_something();</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		barrier();</span>
<span class="p_add">+		WRITE_ONCE(b, p);</span>
<span class="p_add">+		do_something_else();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+안타깝게도, 현재의 컴파일러들은 높은 최적화 레벨에서는 이걸 다음과 같이</span>
<span class="p_add">+바꿔버립니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	barrier();</span>
<span class="p_add">+	WRITE_ONCE(b, p);  /* BUG: No ordering vs. load from a!!! */</span>
<span class="p_add">+	if (q) {</span>
<span class="p_add">+		/* WRITE_ONCE(b, p); -- moved up, BUG!!! */</span>
<span class="p_add">+		do_something();</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* WRITE_ONCE(b, p); -- moved up, BUG!!! */</span>
<span class="p_add">+		do_something_else();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+이제 &#39;a&#39; 에서의 로드와 &#39;b&#39; 로의 스토어 사이에는 조건적 관계가 없기 때문에 CPU</span>
<span class="p_add">+는 이들의 순서를 바꿀 수 있게 됩니다: 이런 경우에 조건적 관계는 반드시</span>
<span class="p_add">+필요한데, 모든 컴파일러 최적화가 이루어지고 난 후의 어셈블리 코드에서도</span>
<span class="p_add">+마찬가지입니다.  따라서, 이 예에서 순서를 지키기 위해서는 smp_store_release()</span>
<span class="p_add">+와 같은 명시적 메모리 배리어가 필요합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	if (q) {</span>
<span class="p_add">+		smp_store_release(&amp;b, p);</span>
<span class="p_add">+		do_something();</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		smp_store_release(&amp;b, p);</span>
<span class="p_add">+		do_something_else();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+반면에 명시적 메모리 배리어가 없다면, 이런 경우의 순서는 스토어 오퍼레이션들이</span>
<span class="p_add">+서로 다를 때에만 보장되는데, 예를 들면 다음과 같은 경우입니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	if (q) {</span>
<span class="p_add">+		WRITE_ONCE(b, p);</span>
<span class="p_add">+		do_something();</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		WRITE_ONCE(b, r);</span>
<span class="p_add">+		do_something_else();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+처음의 READ_ONCE() 는 컴파일러가 &#39;a&#39; 의 값을 증명해내는 것을 막기 위해 여전히</span>
<span class="p_add">+필요합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+또한, 로컬 변수 &#39;q&#39; 를 가지고 하는 일에 대해 주의해야 하는데, 그러지 않으면</span>
<span class="p_add">+컴파일러는 그 값을 추측하고 또다시 필요한 조건관계를 없애버릴 수 있습니다.</span>
<span class="p_add">+예를 들면:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	if (q % MAX) {</span>
<span class="p_add">+		WRITE_ONCE(b, p);</span>
<span class="p_add">+		do_something();</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		WRITE_ONCE(b, r);</span>
<span class="p_add">+		do_something_else();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+만약 MAX 가 1 로 정의된 상수라면, 컴파일러는 (q % MAX) 는 0이란 것을 알아채고,</span>
<span class="p_add">+위의 코드를 아래와 같이 바꿔버릴 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	WRITE_ONCE(b, p);</span>
<span class="p_add">+	do_something_else();</span>
<span class="p_add">+</span>
<span class="p_add">+이렇게 되면, CPU 는 변수 &#39;a&#39; 로부터의 로드와 변수 &#39;b&#39; 로의 스토어 사이의 순서를</span>
<span class="p_add">+지켜줄 필요가 없어집니다.  barrier() 를 추가해 해결해 보고 싶겠지만, 그건</span>
<span class="p_add">+도움이 안됩니다.  조건 관계는 사라졌고, barrier() 는 이를 되돌리지 못합니다.</span>
<span class="p_add">+따라서, 이 순서를 지켜야 한다면, MAX 가 1 보다 크다는 것을, 다음과 같은 방법을</span>
<span class="p_add">+사용해 분명히 해야 합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	BUILD_BUG_ON(MAX &lt;= 1); /* Order load from a with store to b. */</span>
<span class="p_add">+	if (q % MAX) {</span>
<span class="p_add">+		WRITE_ONCE(b, p);</span>
<span class="p_add">+		do_something();</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		WRITE_ONCE(b, r);</span>
<span class="p_add">+		do_something_else();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+&#39;b&#39; 로의 스토어들은 여전히 서로 다름을 알아두세요.  만약 그것들이 동일하면,</span>
<span class="p_add">+앞에서 이야기했듯, 컴파일러가 그 스토어 오퍼레이션들을 &#39;if&#39; 문 바깥으로</span>
<span class="p_add">+끄집어낼 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+또한 이진 조건문 평가에 너무 의존하지 않도록 조심해야 합니다.  다음의 예를</span>
<span class="p_add">+봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	if (q || 1 &gt; 0)</span>
<span class="p_add">+		WRITE_ONCE(b, 1);</span>
<span class="p_add">+</span>
<span class="p_add">+첫번째 조건만으로는 브랜치 조건 전체를 거짓으로 만들 수 없고 두번째 조건은 항상</span>
<span class="p_add">+참이기 때문에, 컴파일러는 이 예를 다음과 같이 바꿔서 컨트롤 의존성을 없애버릴</span>
<span class="p_add">+수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	q = READ_ONCE(a);</span>
<span class="p_add">+	WRITE_ONCE(b, 1);</span>
<span class="p_add">+</span>
<span class="p_add">+이 예는 컴파일러가 코드를 추측으로 수정할 수 없도록 분명히 해야 한다는 점을</span>
<span class="p_add">+강조합니다.  조금 더 일반적으로 말해서, READ_ONCE() 는 컴파일러에게 주어진 로드</span>
<span class="p_add">+오퍼레이션을 위한 코드를 정말로 만들도록 하지만, 컴파일러가 그렇게 만들어진</span>
<span class="p_add">+코드의 수행 결과를 사용하도록 강제하지는 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+마지막으로, 컨트롤 의존성은 이행성 (transitivity) 을 제공하지 -않습니다-.  이건</span>
<span class="p_add">+x 와 y 가 둘 다 0 이라는 초기값을 가졌다는 가정 하의 두개의 예제로</span>
<span class="p_add">+보이겠습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 0                     CPU 1</span>
<span class="p_add">+	=======================   =======================</span>
<span class="p_add">+	r1 = READ_ONCE(x);        r2 = READ_ONCE(y);</span>
<span class="p_add">+	if (r1 &gt; 0)               if (r2 &gt; 0)</span>
<span class="p_add">+	  WRITE_ONCE(y, 1);         WRITE_ONCE(x, 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	assert(!(r1 == 1 &amp;&amp; r2 == 1));</span>
<span class="p_add">+</span>
<span class="p_add">+이 두 CPU 예제에서 assert() 의 조건은 항상 참일 것입니다.  그리고, 만약 컨트롤</span>
<span class="p_add">+의존성이 이행성을 (실제로는 그러지 않지만) 보장한다면, 다음의 CPU 가 추가되어도</span>
<span class="p_add">+아래의 assert() 조건은 참이 될것입니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 2</span>
<span class="p_add">+	=====================</span>
<span class="p_add">+	WRITE_ONCE(x, 2);</span>
<span class="p_add">+</span>
<span class="p_add">+	assert(!(r1 == 2 &amp;&amp; r2 == 1 &amp;&amp; x == 2)); /* FAILS!!! */</span>
<span class="p_add">+</span>
<span class="p_add">+하지만 컨트롤 의존성은 이행성을 제공하지 -않기- 때문에, 세개의 CPU 예제가 실행</span>
<span class="p_add">+완료된 후에 위의 assert() 의 조건은 거짓으로 평가될 수 있습니다.  세개의 CPU</span>
<span class="p_add">+예제가 순서를 지키길 원한다면, CPU 0 와 CPU 1 코드의 로드와 스토어 사이, &quot;if&quot;</span>
<span class="p_add">+문 바로 다음에 smp_mb()를 넣어야 합니다.  더 나아가서, 최초의 두 CPU 예제는</span>
<span class="p_add">+매우 위험하므로 사용되지 않아야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+이 두개의 예제는 다음 논문:</span>
<span class="p_add">+http://www.cl.cam.ac.uk/users/pes20/ppc-supplemental/test6.pdf 와</span>
<span class="p_add">+이 사이트: https://www.cl.cam.ac.uk/~pes20/ppcmem/index.html 에 나온 LB 와 WWC</span>
<span class="p_add">+리트머스 테스트입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+요약하자면:</span>
<span class="p_add">+</span>
<span class="p_add">+  (*) 컨트롤 의존성은 앞의 로드들을 뒤의 스토어들에 대해 순서를 맞춰줍니다.</span>
<span class="p_add">+      하지만, 그 외의 어떤 순서도 보장하지 -않습니다-: 앞의 로드와 뒤의 로드들</span>
<span class="p_add">+      사이에도, 앞의 스토어와 뒤의 스토어들 사이에도요.  이런 다른 형태의</span>
<span class="p_add">+      순서가 필요하다면 smp_rmb() 나 smp_wmb()를, 또는, 앞의 스토어들과 뒤의</span>
<span class="p_add">+      로드들 사이의 순서를 위해서는 smp_mb() 를 사용하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+  (*) &quot;if&quot; 문의 양갈래 브랜치가 같은 변수에의 동일한 스토어로 시작한다면, 그</span>
<span class="p_add">+      스토어들은 각 스토어 앞에 smp_mb() 를 넣거나 smp_store_release() 를</span>
<span class="p_add">+      사용해서 스토어를 하는 식으로 순서를 맞춰줘야 합니다.  이 문제를 해결하기</span>
<span class="p_add">+      위해 &quot;if&quot; 문의 양갈래 브랜치의 시작 지점에 barrier() 를 넣는 것만으로는</span>
<span class="p_add">+      충분한 해결이 되지 않는데, 이는 앞의 예에서 본것과 같이, 컴파일러의</span>
<span class="p_add">+      최적화는 barrier() 가 의미하는 바를 지키면서도 컨트롤 의존성을 손상시킬</span>
<span class="p_add">+      수 있기 때문이라는 점을 부디 알아두시기 바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+  (*) 컨트롤 의존성은 앞의 로드와 뒤의 스토어 사이에 최소 하나의, 실행</span>
<span class="p_add">+      시점에서의 조건관계를 필요로 하며, 이 조건관계는 앞의 로드와 관계되어야</span>
<span class="p_add">+      합니다.  만약 컴파일러가 조건 관계를 최적화로 없앨수 있다면, 순서도</span>
<span class="p_add">+      최적화로 없애버렸을 겁니다.  READ_ONCE() 와 WRITE_ONCE() 의 주의 깊은</span>
<span class="p_add">+      사용은 주어진 조건 관계를 유지하는데 도움이 될 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+  (*) 컨트롤 의존성을 위해선 컴파일러가 조건관계를 없애버리는 것을 막아야</span>
<span class="p_add">+      합니다.  주의 깊은 READ_ONCE() 나 atomic{,64}_read() 의 사용이 컨트롤</span>
<span class="p_add">+      의존성이 사라지지 않게 하는데 도움을 줄 수 있습니다.  더 많은 정보를</span>
<span class="p_add">+      위해선 &quot;컴파일러 배리어&quot; 섹션을 참고하시기 바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+  (*) 컨트롤 의존성은 보통 다른 타입의 배리어들과 짝을 맞춰 사용됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+  (*) 컨트롤 의존성은 이행성을 제공하지 -않습니다-.  이행성이 필요하다면,</span>
<span class="p_add">+      smp_mb() 를 사용하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+SMP 배리어 짝맞추기</span>
<span class="p_add">+--------------------</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 간 상호작용을 다룰 때에 일부 타입의 메모리 배리어는 항상 짝을 맞춰</span>
<span class="p_add">+사용되어야 합니다.  적절하게 짝을 맞추지 않은 코드는 사실상 에러에 가깝습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+범용 배리어들은 범용 배리어끼리도 짝을 맞추지만 이행성이 없는 대부분의 다른</span>
<span class="p_add">+타입의 배리어들과도 짝을 맞춥니다.  ACQUIRE 배리어는 RELEASE 배리어와 짝을</span>
<span class="p_add">+맞춥니다만, 둘 다 범용 배리어를 포함해 다른 배리어들과도 짝을 맞출 수 있습니다.</span>
<span class="p_add">+쓰기 배리어는 데이터 의존성 배리어나 컨트롤 의존성, ACQUIRE 배리어, RELEASE</span>
<span class="p_add">+배리어, 읽기 배리어, 또는 범용 배리어와 짝을 맞춥니다.  비슷하게 읽기 배리어나</span>
<span class="p_add">+컨트롤 의존성, 또는 데이터 의존성 배리어는 쓰기 배리어나 ACQUIRE 배리어,</span>
<span class="p_add">+RELEASE 배리어, 또는 범용 배리어와 짝을 맞추는데, 다음과 같습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		      CPU 2</span>
<span class="p_add">+	===============	      ===============</span>
<span class="p_add">+	WRITE_ONCE(a, 1);</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	WRITE_ONCE(b, 2);     x = READ_ONCE(b);</span>
<span class="p_add">+			      &lt;읽기 배리어&gt;</span>
<span class="p_add">+			      y = READ_ONCE(a);</span>
<span class="p_add">+</span>
<span class="p_add">+또는:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		      CPU 2</span>
<span class="p_add">+	===============	      ===============================</span>
<span class="p_add">+	a = 1;</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	WRITE_ONCE(b, &amp;a);    x = READ_ONCE(b);</span>
<span class="p_add">+			      &lt;데이터 의존성 배리어&gt;</span>
<span class="p_add">+			      y = *x;</span>
<span class="p_add">+</span>
<span class="p_add">+또는:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		      CPU 2</span>
<span class="p_add">+	===============	      ===============================</span>
<span class="p_add">+	r1 = READ_ONCE(y);</span>
<span class="p_add">+	&lt;범용 배리어&gt;</span>
<span class="p_add">+	WRITE_ONCE(y, 1);     if (r2 = READ_ONCE(x)) {</span>
<span class="p_add">+			         &lt;묵시적 컨트롤 의존성&gt;</span>
<span class="p_add">+			         WRITE_ONCE(y, 1);</span>
<span class="p_add">+			      }</span>
<span class="p_add">+</span>
<span class="p_add">+	assert(r1 == 0 || r2 == 0);</span>
<span class="p_add">+</span>
<span class="p_add">+기본적으로, 여기서의 읽기 배리어는 &quot;더 완화된&quot; 타입일 순 있어도 항상 존재해야</span>
<span class="p_add">+합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+[!] 쓰기 배리어 앞의 스토어 오퍼레이션은 일반적으로 읽기 배리어나 데이터</span>
<span class="p_add">+의존성 배리어 뒤의 로드 오퍼레이션과 매치될 것이고, 반대도 마찬가지입니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1                               CPU 2</span>
<span class="p_add">+	===================                 ===================</span>
<span class="p_add">+	WRITE_ONCE(a, 1);    }----   ---&gt;{  v = READ_ONCE(c);</span>
<span class="p_add">+	WRITE_ONCE(b, 2);    }    \ /    {  w = READ_ONCE(d);</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;              \        &lt;읽기 배리어&gt;</span>
<span class="p_add">+	WRITE_ONCE(c, 3);    }    / \    {  x = READ_ONCE(a);</span>
<span class="p_add">+	WRITE_ONCE(d, 4);    }----   ---&gt;{  y = READ_ONCE(b);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+메모리 배리어 시퀀스의 예</span>
<span class="p_add">+-------------------------</span>
<span class="p_add">+</span>
<span class="p_add">+첫째, 쓰기 배리어는 스토어 오퍼레이션들의 부분적 순서 세우기로 동작합니다.</span>
<span class="p_add">+아래의 이벤트 시퀀스를 보세요:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1</span>
<span class="p_add">+	=======================</span>
<span class="p_add">+	STORE A = 1</span>
<span class="p_add">+	STORE B = 2</span>
<span class="p_add">+	STORE C = 3</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	STORE D = 4</span>
<span class="p_add">+	STORE E = 5</span>
<span class="p_add">+</span>
<span class="p_add">+이 이벤트 시퀀스는 메모리 일관성 시스템에 원소끼리의 순서가 존재하지 않는 집합</span>
<span class="p_add">+{ STORE A, STORE B, STORE C } 가 역시 원소끼리의 순서가 존재하지 않는 집합</span>
<span class="p_add">+{ STORE D, STORE E } 보다 먼저 일어난 것으로 시스템의 나머지 요소들에 보이도록</span>
<span class="p_add">+전달됩니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	+-------+       :      :</span>
<span class="p_add">+	|       |       +------+</span>
<span class="p_add">+	|       |------&gt;| C=3  |     }     /\</span>
<span class="p_add">+	|       |  :    +------+     }-----  \  -----&gt; 시스템의 나머지 요소에</span>
<span class="p_add">+	|       |  :    | A=1  |     }        \/       보여질 수 있는 이벤트들</span>
<span class="p_add">+	|       |  :    +------+     }</span>
<span class="p_add">+	| CPU 1 |  :    | B=2  |     }</span>
<span class="p_add">+	|       |       +------+     }</span>
<span class="p_add">+	|       |   wwwwwwwwwwwwwwww }   &lt;--- 여기서 쓰기 배리어는 배리어 앞의</span>
<span class="p_add">+	|       |       +------+     }        모든 스토어가 배리어 뒤의 스토어</span>
<span class="p_add">+	|       |  :    | E=5  |     }        전에 메모리 시스템에 전달되도록</span>
<span class="p_add">+	|       |  :    +------+     }        합니다</span>
<span class="p_add">+	|       |------&gt;| D=4  |     }</span>
<span class="p_add">+	|       |       +------+</span>
<span class="p_add">+	+-------+       :      :</span>
<span class="p_add">+	                   |</span>
<span class="p_add">+	                   | CPU 1 에 의해 메모리 시스템에 전달되는</span>
<span class="p_add">+	                   | 일련의 스토어 오퍼레이션들</span>
<span class="p_add">+	                   V</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+둘째, 데이터 의존성 배리어는 데이터 의존적 로드 오퍼레이션들의 부분적 순서</span>
<span class="p_add">+세우기로 동작합니다.  다음 일련의 이벤트들을 보세요:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1			CPU 2</span>
<span class="p_add">+	=======================	=======================</span>
<span class="p_add">+		{ B = 7; X = 9; Y = 8; C = &amp;Y }</span>
<span class="p_add">+	STORE A = 1</span>
<span class="p_add">+	STORE B = 2</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	STORE C = &amp;B		LOAD X</span>
<span class="p_add">+	STORE D = 4		LOAD C (gets &amp;B)</span>
<span class="p_add">+				LOAD *C (reads B)</span>
<span class="p_add">+</span>
<span class="p_add">+여기에 별다른 개입이 없다면, CPU 1 의 쓰기 배리어에도 불구하고 CPU 2 는 CPU 1</span>
<span class="p_add">+의 이벤트들을 완전히 무작위적 순서로 인지하게 됩니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	+-------+       :      :                :       :</span>
<span class="p_add">+	|       |       +------+                +-------+  | CPU 2 에 인지되는</span>
<span class="p_add">+	|       |------&gt;| B=2  |-----       ---&gt;| Y-&gt;8  |  | 업데이트 이벤트</span>
<span class="p_add">+	|       |  :    +------+     \          +-------+  | 시퀀스</span>
<span class="p_add">+	| CPU 1 |  :    | A=1  |      \     ---&gt;| C-&gt;&amp;Y |  V</span>
<span class="p_add">+	|       |       +------+       |        +-------+</span>
<span class="p_add">+	|       |   wwwwwwwwwwwwwwww   |        :       :</span>
<span class="p_add">+	|       |       +------+       |        :       :</span>
<span class="p_add">+	|       |  :    | C=&amp;B |---    |        :       :       +-------+</span>
<span class="p_add">+	|       |  :    +------+   \   |        +-------+       |       |</span>
<span class="p_add">+	|       |------&gt;| D=4  |    -----------&gt;| C-&gt;&amp;B |------&gt;|       |</span>
<span class="p_add">+	|       |       +------+       |        +-------+       |       |</span>
<span class="p_add">+	+-------+       :      :       |        :       :       |       |</span>
<span class="p_add">+	                               |        :       :       |       |</span>
<span class="p_add">+	                               |        :       :       | CPU 2 |</span>
<span class="p_add">+	                               |        +-------+       |       |</span>
<span class="p_add">+	    분명히 잘못된        ---&gt;  |        | B-&gt;7  |------&gt;|       |</span>
<span class="p_add">+	    B 의 값 인지 (!)           |        +-------+       |       |</span>
<span class="p_add">+	                               |        :       :       |       |</span>
<span class="p_add">+	                               |        +-------+       |       |</span>
<span class="p_add">+	    X 의 로드가 B 의    ---&gt;    \       | X-&gt;9  |------&gt;|       |</span>
<span class="p_add">+	    일관성 유지를                \      +-------+       |       |</span>
<span class="p_add">+	    지연시킴                      -----&gt;| B-&gt;2  |       +-------+</span>
<span class="p_add">+	                                        +-------+</span>
<span class="p_add">+	                                        :       :</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+앞의 예에서, CPU 2 는 (B 의 값이 될) *C 의 값 읽기가 C 의 LOAD 뒤에 이어짐에도</span>
<span class="p_add">+B 가 7 이라는 결과를 얻습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, 만약 데이터 의존성 배리어가 C 의 로드와 *C (즉, B) 의 로드 사이에</span>
<span class="p_add">+있었다면:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1			CPU 2</span>
<span class="p_add">+	=======================	=======================</span>
<span class="p_add">+		{ B = 7; X = 9; Y = 8; C = &amp;Y }</span>
<span class="p_add">+	STORE A = 1</span>
<span class="p_add">+	STORE B = 2</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	STORE C = &amp;B		LOAD X</span>
<span class="p_add">+	STORE D = 4		LOAD C (gets &amp;B)</span>
<span class="p_add">+				&lt;데이터 의존성 배리어&gt;</span>
<span class="p_add">+				LOAD *C (reads B)</span>
<span class="p_add">+</span>
<span class="p_add">+다음과 같이 됩니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	+-------+       :      :                :       :</span>
<span class="p_add">+	|       |       +------+                +-------+</span>
<span class="p_add">+	|       |------&gt;| B=2  |-----       ---&gt;| Y-&gt;8  |</span>
<span class="p_add">+	|       |  :    +------+     \          +-------+</span>
<span class="p_add">+	| CPU 1 |  :    | A=1  |      \     ---&gt;| C-&gt;&amp;Y |</span>
<span class="p_add">+	|       |       +------+       |        +-------+</span>
<span class="p_add">+	|       |   wwwwwwwwwwwwwwww   |        :       :</span>
<span class="p_add">+	|       |       +------+       |        :       :</span>
<span class="p_add">+	|       |  :    | C=&amp;B |---    |        :       :       +-------+</span>
<span class="p_add">+	|       |  :    +------+   \   |        +-------+       |       |</span>
<span class="p_add">+	|       |------&gt;| D=4  |    -----------&gt;| C-&gt;&amp;B |------&gt;|       |</span>
<span class="p_add">+	|       |       +------+       |        +-------+       |       |</span>
<span class="p_add">+	+-------+       :      :       |        :       :       |       |</span>
<span class="p_add">+	                               |        :       :       |       |</span>
<span class="p_add">+	                               |        :       :       | CPU 2 |</span>
<span class="p_add">+	                               |        +-------+       |       |</span>
<span class="p_add">+	                               |        | X-&gt;9  |------&gt;|       |</span>
<span class="p_add">+	                               |        +-------+       |       |</span>
<span class="p_add">+	  C 로의 스토어 앞의     ---&gt;   \   ddddddddddddddddd   |       |</span>
<span class="p_add">+	  모든 이벤트 결과가             \      +-------+       |       |</span>
<span class="p_add">+	  뒤의 로드에게                   -----&gt;| B-&gt;2  |------&gt;|       |</span>
<span class="p_add">+	  보이게 강제한다                       +-------+       |       |</span>
<span class="p_add">+	                                        :       :       +-------+</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+셋째, 읽기 배리어는 로드 오퍼레이션들에의 부분적 순서 세우기로 동작합니다.</span>
<span class="p_add">+아래의 일련의 이벤트를 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1			CPU 2</span>
<span class="p_add">+	=======================	=======================</span>
<span class="p_add">+		{ A = 0, B = 9 }</span>
<span class="p_add">+	STORE A=1</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	STORE B=2</span>
<span class="p_add">+				LOAD B</span>
<span class="p_add">+				LOAD A</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 1 은 쓰기 배리어를 쳤지만, 별다른 개입이 없다면 CPU 2 는 CPU 1 에서 행해진</span>
<span class="p_add">+이벤트의 결과를 무작위적 순서로 인지하게 됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+	+-------+       :      :                :       :</span>
<span class="p_add">+	|       |       +------+                +-------+</span>
<span class="p_add">+	|       |------&gt;| A=1  |------      ---&gt;| A-&gt;0  |</span>
<span class="p_add">+	|       |       +------+      \         +-------+</span>
<span class="p_add">+	| CPU 1 |   wwwwwwwwwwwwwwww   \    ---&gt;| B-&gt;9  |</span>
<span class="p_add">+	|       |       +------+        |       +-------+</span>
<span class="p_add">+	|       |------&gt;| B=2  |---     |       :       :</span>
<span class="p_add">+	|       |       +------+   \    |       :       :       +-------+</span>
<span class="p_add">+	+-------+       :      :    \   |       +-------+       |       |</span>
<span class="p_add">+	                             ----------&gt;| B-&gt;2  |------&gt;|       |</span>
<span class="p_add">+	                                |       +-------+       | CPU 2 |</span>
<span class="p_add">+	                                |       | A-&gt;0  |------&gt;|       |</span>
<span class="p_add">+	                                |       +-------+       |       |</span>
<span class="p_add">+	                                |       :       :       +-------+</span>
<span class="p_add">+	                                 \      :       :</span>
<span class="p_add">+	                                  \     +-------+</span>
<span class="p_add">+	                                   ----&gt;| A-&gt;1  |</span>
<span class="p_add">+	                                        +-------+</span>
<span class="p_add">+	                                        :       :</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, 만약 읽기 배리어가 B 의 로드와 A 의 로드 사이에 존재한다면:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1			CPU 2</span>
<span class="p_add">+	=======================	=======================</span>
<span class="p_add">+		{ A = 0, B = 9 }</span>
<span class="p_add">+	STORE A=1</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	STORE B=2</span>
<span class="p_add">+				LOAD B</span>
<span class="p_add">+				&lt;읽기 배리어&gt;</span>
<span class="p_add">+				LOAD A</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 1 에 의해 만들어진 부분적 순서가 CPU 2 에도 그대로 인지됩니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	+-------+       :      :                :       :</span>
<span class="p_add">+	|       |       +------+                +-------+</span>
<span class="p_add">+	|       |------&gt;| A=1  |------      ---&gt;| A-&gt;0  |</span>
<span class="p_add">+	|       |       +------+      \         +-------+</span>
<span class="p_add">+	| CPU 1 |   wwwwwwwwwwwwwwww   \    ---&gt;| B-&gt;9  |</span>
<span class="p_add">+	|       |       +------+        |       +-------+</span>
<span class="p_add">+	|       |------&gt;| B=2  |---     |       :       :</span>
<span class="p_add">+	|       |       +------+   \    |       :       :       +-------+</span>
<span class="p_add">+	+-------+       :      :    \   |       +-------+       |       |</span>
<span class="p_add">+	                             ----------&gt;| B-&gt;2  |------&gt;|       |</span>
<span class="p_add">+	                                |       +-------+       | CPU 2 |</span>
<span class="p_add">+	                                |       :       :       |       |</span>
<span class="p_add">+	                                |       :       :       |       |</span>
<span class="p_add">+	  여기서 읽기 배리어는   ----&gt;   \  rrrrrrrrrrrrrrrrr   |       |</span>
<span class="p_add">+	  B 로의 스토어 전의              \     +-------+       |       |</span>
<span class="p_add">+	  모든 결과를 CPU 2 에             ----&gt;| A-&gt;1  |------&gt;|       |</span>
<span class="p_add">+	  보이도록 한다                         +-------+       |       |</span>
<span class="p_add">+	                                        :       :       +-------+</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+더 완벽한 설명을 위해, A 의 로드가 읽기 배리어 앞과 뒤에 있으면 어떻게 될지</span>
<span class="p_add">+생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1			CPU 2</span>
<span class="p_add">+	=======================	=======================</span>
<span class="p_add">+		{ A = 0, B = 9 }</span>
<span class="p_add">+	STORE A=1</span>
<span class="p_add">+	&lt;쓰기 배리어&gt;</span>
<span class="p_add">+	STORE B=2</span>
<span class="p_add">+				LOAD B</span>
<span class="p_add">+				LOAD A [first load of A]</span>
<span class="p_add">+				&lt;읽기 배리어&gt;</span>
<span class="p_add">+				LOAD A [second load of A]</span>
<span class="p_add">+</span>
<span class="p_add">+A 의 로드 두개가 모두 B 의 로드 뒤에 있지만, 서로 다른 값을 얻어올 수</span>
<span class="p_add">+있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	+-------+       :      :                :       :</span>
<span class="p_add">+	|       |       +------+                +-------+</span>
<span class="p_add">+	|       |------&gt;| A=1  |------      ---&gt;| A-&gt;0  |</span>
<span class="p_add">+	|       |       +------+      \         +-------+</span>
<span class="p_add">+	| CPU 1 |   wwwwwwwwwwwwwwww   \    ---&gt;| B-&gt;9  |</span>
<span class="p_add">+	|       |       +------+        |       +-------+</span>
<span class="p_add">+	|       |------&gt;| B=2  |---     |       :       :</span>
<span class="p_add">+	|       |       +------+   \    |       :       :       +-------+</span>
<span class="p_add">+	+-------+       :      :    \   |       +-------+       |       |</span>
<span class="p_add">+	                             ----------&gt;| B-&gt;2  |------&gt;|       |</span>
<span class="p_add">+	                                |       +-------+       | CPU 2 |</span>
<span class="p_add">+	                                |       :       :       |       |</span>
<span class="p_add">+	                                |       :       :       |       |</span>
<span class="p_add">+	                                |       +-------+       |       |</span>
<span class="p_add">+	                                |       | A-&gt;0  |------&gt;| 1st   |</span>
<span class="p_add">+	                                |       +-------+       |       |</span>
<span class="p_add">+	  여기서 읽기 배리어는   ----&gt;   \  rrrrrrrrrrrrrrrrr   |       |</span>
<span class="p_add">+	  B 로의 스토어 전의              \     +-------+       |       |</span>
<span class="p_add">+	  모든 결과를 CPU 2 에             ----&gt;| A-&gt;1  |------&gt;| 2nd   |</span>
<span class="p_add">+	  보이도록 한다                         +-------+       |       |</span>
<span class="p_add">+	                                        :       :       +-------+</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+하지만 CPU 1 에서의 A 업데이트는 읽기 배리어가 완료되기 전에도 보일 수도</span>
<span class="p_add">+있긴 합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	+-------+       :      :                :       :</span>
<span class="p_add">+	|       |       +------+                +-------+</span>
<span class="p_add">+	|       |------&gt;| A=1  |------      ---&gt;| A-&gt;0  |</span>
<span class="p_add">+	|       |       +------+      \         +-------+</span>
<span class="p_add">+	| CPU 1 |   wwwwwwwwwwwwwwww   \    ---&gt;| B-&gt;9  |</span>
<span class="p_add">+	|       |       +------+        |       +-------+</span>
<span class="p_add">+	|       |------&gt;| B=2  |---     |       :       :</span>
<span class="p_add">+	|       |       +------+   \    |       :       :       +-------+</span>
<span class="p_add">+	+-------+       :      :    \   |       +-------+       |       |</span>
<span class="p_add">+	                             ----------&gt;| B-&gt;2  |------&gt;|       |</span>
<span class="p_add">+	                                |       +-------+       | CPU 2 |</span>
<span class="p_add">+	                                |       :       :       |       |</span>
<span class="p_add">+	                                 \      :       :       |       |</span>
<span class="p_add">+	                                  \     +-------+       |       |</span>
<span class="p_add">+	                                   ----&gt;| A-&gt;1  |------&gt;| 1st   |</span>
<span class="p_add">+	                                        +-------+       |       |</span>
<span class="p_add">+	                                    rrrrrrrrrrrrrrrrr   |       |</span>
<span class="p_add">+	                                        +-------+       |       |</span>
<span class="p_add">+	                                        | A-&gt;1  |------&gt;| 2nd   |</span>
<span class="p_add">+	                                        +-------+       |       |</span>
<span class="p_add">+	                                        :       :       +-------+</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+여기서 보장되는 건, 만약 B 의 로드가 B == 2 라는 결과를 봤다면, A 에의 두번째</span>
<span class="p_add">+로드는 항상 A == 1 을 보게 될 것이라는 겁니다.  A 에의 첫번째 로드에는 그런</span>
<span class="p_add">+보장이 없습니다; A == 0 이거나 A == 1 이거나 둘 중 하나의 결과를 보게 될겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+읽기 메모리 배리어 VS 로드 예측</span>
<span class="p_add">+-------------------------------</span>
<span class="p_add">+</span>
<span class="p_add">+많은 CPU들이 로드를 예측적으로 (speculatively) 합니다: 어떤 데이터를 메모리에서</span>
<span class="p_add">+로드해야 하게 될지 예측을 했다면, 해당 데이터를 로드하는 인스트럭션을 실제로는</span>
<span class="p_add">+아직 만나지 않았더라도 다른 로드 작업이 없어 버스 (bus) 가 아무 일도 하고 있지</span>
<span class="p_add">+않다면, 그 데이터를 로드합니다.  이후에 실제 로드 인스트럭션이 실행되면 CPU 가</span>
<span class="p_add">+이미 그 값을 가지고 있기 때문에 그 로드 인스트럭션은 즉시 완료됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+해당 CPU 는 실제로는 그 값이 필요치 않았다는 사실이 나중에 드러날 수도 있는데 -</span>
<span class="p_add">+해당 로드 인스트럭션이 브랜치로 우회되거나 했을 수 있겠죠 - , 그렇게 되면 앞서</span>
<span class="p_add">+읽어둔 값을 버리거나 나중의 사용을 위해 캐시에 넣어둘 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+다음을 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1			CPU 2</span>
<span class="p_add">+	=======================	=======================</span>
<span class="p_add">+				LOAD B</span>
<span class="p_add">+				DIVIDE		} 나누기 명령은 일반적으로</span>
<span class="p_add">+				DIVIDE		} 긴 시간을 필요로 합니다</span>
<span class="p_add">+				LOAD A</span>
<span class="p_add">+</span>
<span class="p_add">+는 이렇게 될 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	                                        :       :       +-------+</span>
<span class="p_add">+	                                        +-------+       |       |</span>
<span class="p_add">+	                                    ---&gt;| B-&gt;2  |------&gt;|       |</span>
<span class="p_add">+	                                        +-------+       | CPU 2 |</span>
<span class="p_add">+	                                        :       :DIVIDE |       |</span>
<span class="p_add">+	                                        +-------+       |       |</span>
<span class="p_add">+	나누기 하느라 바쁜       ---&gt;       ---&gt;| A-&gt;0  |~~~~   |       |</span>
<span class="p_add">+	CPU 는 A 의 LOAD 를                     +-------+   ~   |       |</span>
<span class="p_add">+	예측해서 수행한다                       :       :   ~   |       |</span>
<span class="p_add">+	                                        :       :DIVIDE |       |</span>
<span class="p_add">+	                                        :       :   ~   |       |</span>
<span class="p_add">+	나누기가 끝나면       ---&gt;     ---&gt;     :       :   ~--&gt;|       |</span>
<span class="p_add">+	CPU 는 해당 LOAD 를                     :       :       |       |</span>
<span class="p_add">+	즉각 완료한다                           :       :       +-------+</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+읽기 배리어나 데이터 의존성 배리어를 두번째 로드 직전에 놓는다면:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1			CPU 2</span>
<span class="p_add">+	=======================	=======================</span>
<span class="p_add">+				LOAD B</span>
<span class="p_add">+				DIVIDE</span>
<span class="p_add">+				DIVIDE</span>
<span class="p_add">+				&lt;읽기 배리어&gt;</span>
<span class="p_add">+				LOAD A</span>
<span class="p_add">+</span>
<span class="p_add">+예측으로 얻어진 값은 사용된 배리어의 타입에 따라서 해당 값이 옳은지 검토되게</span>
<span class="p_add">+됩니다.  만약 해당 메모리 영역에 변화가 없었다면, 예측으로 얻어두었던 값이</span>
<span class="p_add">+사용됩니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	                                        :       :       +-------+</span>
<span class="p_add">+	                                        +-------+       |       |</span>
<span class="p_add">+	                                    ---&gt;| B-&gt;2  |------&gt;|       |</span>
<span class="p_add">+	                                        +-------+       | CPU 2 |</span>
<span class="p_add">+	                                        :       :DIVIDE |       |</span>
<span class="p_add">+	                                        +-------+       |       |</span>
<span class="p_add">+	나누기 하느라 바쁜       ---&gt;       ---&gt;| A-&gt;0  |~~~~   |       |</span>
<span class="p_add">+	CPU 는 A 의 LOAD 를                     +-------+   ~   |       |</span>
<span class="p_add">+	예측한다                                :       :   ~   |       |</span>
<span class="p_add">+	                                        :       :DIVIDE |       |</span>
<span class="p_add">+	                                        :       :   ~   |       |</span>
<span class="p_add">+	                                        :       :   ~   |       |</span>
<span class="p_add">+	                                    rrrrrrrrrrrrrrrr~   |       |</span>
<span class="p_add">+	                                        :       :   ~   |       |</span>
<span class="p_add">+	                                        :       :   ~--&gt;|       |</span>
<span class="p_add">+	                                        :       :       |       |</span>
<span class="p_add">+	                                        :       :       +-------+</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+하지만 다른 CPU 에서 업데이트나 무효화가 있었다면, 그 예측은 무효화되고 그 값은</span>
<span class="p_add">+다시 읽혀집니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	                                        :       :       +-------+</span>
<span class="p_add">+	                                        +-------+       |       |</span>
<span class="p_add">+	                                    ---&gt;| B-&gt;2  |------&gt;|       |</span>
<span class="p_add">+	                                        +-------+       | CPU 2 |</span>
<span class="p_add">+	                                        :       :DIVIDE |       |</span>
<span class="p_add">+	                                        +-------+       |       |</span>
<span class="p_add">+	나누기 하느라 바쁜       ---&gt;       ---&gt;| A-&gt;0  |~~~~   |       |</span>
<span class="p_add">+	CPU 는 A 의 LOAD 를                     +-------+   ~   |       |</span>
<span class="p_add">+	예측한다                                :       :   ~   |       |</span>
<span class="p_add">+	                                        :       :DIVIDE |       |</span>
<span class="p_add">+	                                        :       :   ~   |       |</span>
<span class="p_add">+	                                        :       :   ~   |       |</span>
<span class="p_add">+	                                    rrrrrrrrrrrrrrrrr   |       |</span>
<span class="p_add">+	                                        +-------+       |       |</span>
<span class="p_add">+	예측성 동작은 무효화 되고    ---&gt;   ---&gt;| A-&gt;1  |------&gt;|       |</span>
<span class="p_add">+	업데이트된 값이 다시 읽혀진다           +-------+       |       |</span>
<span class="p_add">+	                                        :       :       +-------+</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+이행성</span>
<span class="p_add">+------</span>
<span class="p_add">+</span>
<span class="p_add">+이행성(transitivity)은 실제의 컴퓨터 시스템에서 항상 제공되지는 않는, 순서</span>
<span class="p_add">+맞추기에 대한 상당히 직관적인 개념입니다.  다음의 예가 이행성을 보여줍니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1			CPU 2			CPU 3</span>
<span class="p_add">+	=======================	=======================	=======================</span>
<span class="p_add">+		{ X = 0, Y = 0 }</span>
<span class="p_add">+	STORE X=1		LOAD X			STORE Y=1</span>
<span class="p_add">+				&lt;범용 배리어&gt;		&lt;범용 배리어&gt;</span>
<span class="p_add">+				LOAD Y			LOAD X</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 2 의 X 로드가 1을 리턴했고 Y 로드가 0을 리턴했다고 해봅시다.  이는 CPU 2 의</span>
<span class="p_add">+X 로드가 CPU 1 의 X 스토어 뒤에 이루어졌고 CPU 2 의 Y 로드는 CPU 3 의 Y 스토어</span>
<span class="p_add">+전에 이루어졌음을 의미합니다.  그럼 &quot;CPU 3 의 X 로드는 0을 리턴할 수 있나요?&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 2 의 X 로드는 CPU 1 의 스토어 후에 이루어졌으니, CPU 3 의 X 로드는 1을</span>
<span class="p_add">+리턴하는게 자연스럽습니다.  이런 생각이 이행성의 한 예입니다: CPU A 에서 실행된</span>
<span class="p_add">+로드가 CPU B 에서의 같은 변수에 대한 로드를 뒤따른다면, CPU A 의 로드는 CPU B</span>
<span class="p_add">+의 로드가 내놓은 값과 같거나 그 후의 값을 내놓아야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+리눅스 커널에서 범용 배리어의 사용은 이행성을 보장합니다.  따라서, 앞의 예에서</span>
<span class="p_add">+CPU 2 의 X 로드가 1을, Y 로드는 0을 리턴했다면, CPU 3 의 X 로드는 반드시 1을</span>
<span class="p_add">+리턴합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, 읽기나 쓰기 배리어에 대해서는 이행성이 보장되지 -않습니다-.  예를 들어,</span>
<span class="p_add">+앞의 예에서 CPU 2 의 범용 배리어가 아래처럼 읽기 배리어로 바뀐 경우를 생각해</span>
<span class="p_add">+봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1			CPU 2			CPU 3</span>
<span class="p_add">+	=======================	=======================	=======================</span>
<span class="p_add">+		{ X = 0, Y = 0 }</span>
<span class="p_add">+	STORE X=1		LOAD X			STORE Y=1</span>
<span class="p_add">+				&lt;읽기 배리어&gt;		&lt;범용 배리어&gt;</span>
<span class="p_add">+				LOAD Y			LOAD X</span>
<span class="p_add">+</span>
<span class="p_add">+이 코드는 이행성을 갖지 않습니다: 이 예에서는, CPU 2 의 X 로드가 1을</span>
<span class="p_add">+리턴하고, Y 로드는 0을 리턴하지만 CPU 3 의 X 로드가 0을 리턴하는 것도 완전히</span>
<span class="p_add">+합법적입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 2 의 읽기 배리어가 자신의 읽기는 순서를 맞춰줘도, CPU 1 의 스토어와의</span>
<span class="p_add">+순서를 맞춰준다고는 보장할 수 없다는게 핵심입니다.  따라서, CPU 1 과 CPU 2 가</span>
<span class="p_add">+버퍼나 캐시를 공유하는 시스템에서 이 예제 코드가 실행된다면, CPU 2 는 CPU 1 이</span>
<span class="p_add">+쓴 값에 좀 빨리 접근할 수 있을 것입니다.  따라서 CPU 1 과 CPU 2 의 접근으로</span>
<span class="p_add">+조합된 순서를 모든 CPU 가 동의할 수 있도록 하기 위해 범용 배리어가 필요합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+범용 배리어는 &quot;글로벌 이행성&quot;을 제공해서, 모든 CPU 들이 오퍼레이션들의 순서에</span>
<span class="p_add">+동의하게 할 것입니다.  반면, release-acquire 조합은 &quot;로컬 이행성&quot; 만을</span>
<span class="p_add">+제공해서, 해당 조합이 사용된 CPU 들만이 해당 액세스들의 조합된 순서에 동의함이</span>
<span class="p_add">+보장됩니다.  예를 들어, 존경스런 Herman Hollerith 의 C 코드로 보면:</span>
<span class="p_add">+</span>
<span class="p_add">+	int u, v, x, y, z;</span>
<span class="p_add">+</span>
<span class="p_add">+	void cpu0(void)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		r0 = smp_load_acquire(&amp;x);</span>
<span class="p_add">+		WRITE_ONCE(u, 1);</span>
<span class="p_add">+		smp_store_release(&amp;y, 1);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	void cpu1(void)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		r1 = smp_load_acquire(&amp;y);</span>
<span class="p_add">+		r4 = READ_ONCE(v);</span>
<span class="p_add">+		r5 = READ_ONCE(u);</span>
<span class="p_add">+		smp_store_release(&amp;z, 1);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	void cpu2(void)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		r2 = smp_load_acquire(&amp;z);</span>
<span class="p_add">+		smp_store_release(&amp;x, 1);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	void cpu3(void)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		WRITE_ONCE(v, 1);</span>
<span class="p_add">+		smp_mb();</span>
<span class="p_add">+		r3 = READ_ONCE(u);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+cpu0(), cpu1(), 그리고 cpu2() 는 smp_store_release()/smp_load_acquire() 쌍의</span>
<span class="p_add">+연결을 통한 로컬 이행성에 동참하고 있으므로, 다음과 같은 결과는 나오지 않을</span>
<span class="p_add">+겁니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	r0 == 1 &amp;&amp; r1 == 1 &amp;&amp; r2 == 1</span>
<span class="p_add">+</span>
<span class="p_add">+더 나아가서, cpu0() 와 cpu1() 사이의 release-acquire 관계로 인해, cpu1() 은</span>
<span class="p_add">+cpu0() 의 쓰기를 봐야만 하므로, 다음과 같은 결과도 없을 겁니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	r1 == 1 &amp;&amp; r5 == 0</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, release-acquire 타동성은 동참한 CPU 들에만 적용되므로 cpu3() 에는</span>
<span class="p_add">+적용되지 않습니다.  따라서, 다음과 같은 결과가 가능합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	r0 == 0 &amp;&amp; r1 == 1 &amp;&amp; r2 == 1 &amp;&amp; r3 == 0 &amp;&amp; r4 == 0</span>
<span class="p_add">+</span>
<span class="p_add">+비슷하게, 다음과 같은 결과도 가능합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	r0 == 0 &amp;&amp; r1 == 1 &amp;&amp; r2 == 1 &amp;&amp; r3 == 0 &amp;&amp; r4 == 0 &amp;&amp; r5 == 1</span>
<span class="p_add">+</span>
<span class="p_add">+cpu0(), cpu1(), 그리고 cpu2() 는 그들의 읽기와 쓰기를 순서대로 보게 되지만,</span>
<span class="p_add">+release-acquire 체인에 관여되지 않은 CPU 들은 그 순서에 이견을 가질 수</span>
<span class="p_add">+있습니다.  이런 이견은 smp_load_acquire() 와 smp_store_release() 의 구현에</span>
<span class="p_add">+사용되는 완화된 메모리 배리어 인스트럭션들은 항상 배리어 앞의 스토어들을 뒤의</span>
<span class="p_add">+로드들에 앞세울 필요는 없다는 사실에서 기인합니다.  이 말은 cpu3() 는 cpu0() 의</span>
<span class="p_add">+u 로의 스토어를 cpu1() 의 v 로부터의 로드 뒤에 일어난 것으로 볼 수 있다는</span>
<span class="p_add">+뜻입니다, cpu0() 와 cpu1() 은 이 두 오퍼레이션이 의도된 순서대로 일어났음에</span>
<span class="p_add">+모두 동의하는데도 말입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, smp_load_acquire() 는 마술이 아님을 명심하시기 바랍니다.  구체적으로,</span>
<span class="p_add">+이 함수는 단순히 순서 규칙을 지키며 인자로부터의 읽기를 수행합니다.  이것은</span>
<span class="p_add">+어떤 특정한 값이 읽힐 것인지는 보장하지 -않습니다-.  따라서, 다음과 같은 결과도</span>
<span class="p_add">+가능합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	r0 == 0 &amp;&amp; r1 == 0 &amp;&amp; r2 == 0 &amp;&amp; r5 == 0</span>
<span class="p_add">+</span>
<span class="p_add">+이런 결과는 어떤 것도 재배치 되지 않는, 순차적 일관성을 가진 가상의</span>
<span class="p_add">+시스템에서도 일어날 수 있음을 기억해 두시기 바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+다시 말하지만, 당신의 코드가 글로벌 이행성을 필요로 한다면, 범용 배리어를</span>
<span class="p_add">+사용하십시오.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+==================</span>
<span class="p_add">+명시적 커널 배리어</span>
<span class="p_add">+==================</span>
<span class="p_add">+</span>
<span class="p_add">+리눅스 커널은 서로 다른 단계에서 동작하는 다양한 배리어들을 가지고 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+  (*) 컴파일러 배리어.</span>
<span class="p_add">+</span>
<span class="p_add">+  (*) CPU 메모리 배리어.</span>
<span class="p_add">+</span>
<span class="p_add">+  (*) MMIO 쓰기 배리어.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+컴파일러 배리어</span>
<span class="p_add">+---------------</span>
<span class="p_add">+</span>
<span class="p_add">+리눅스 커널은 컴파일러가 메모리 액세스를 재배치 하는 것을 막아주는 명시적인</span>
<span class="p_add">+컴파일러 배리어를 가지고 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	barrier();</span>
<span class="p_add">+</span>
<span class="p_add">+이건 범용 배리어입니다 -- barrier() 의 읽기-읽기 나 쓰기-쓰기 변종은 없습니다.</span>
<span class="p_add">+하지만, READ_ONCE() 와 WRITE_ONCE() 는 특정 액세스들에 대해서만 동작하는</span>
<span class="p_add">+barrier() 의 완화된 형태로 볼 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+barrier() 함수는 다음과 같은 효과를 갖습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 컴파일러가 barrier() 뒤의 액세스들이 barrier() 앞의 액세스보다 앞으로</span>
<span class="p_add">+     재배치되지 못하게 합니다.  예를 들어, 인터럽트 핸들러 코드와 인터럽트 당한</span>
<span class="p_add">+     코드 사이의 통신을 신중히 하기 위해 사용될 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 루프에서, 컴파일러가 루프 조건에 사용된 변수를 매 이터레이션마다</span>
<span class="p_add">+     메모리에서 로드하지 않아도 되도록 최적화 하는걸 방지합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+READ_ONCE() 와 WRITE_ONCE() 함수는 싱글 쓰레드 코드에서는 문제 없지만 동시성이</span>
<span class="p_add">+있는 코드에서는 문제가 될 수 있는 모든 최적화를 막습니다.  이런 류의 최적화에</span>
<span class="p_add">+대한 예를 몇가지 들어보면 다음과 같습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 컴파일러는 같은 변수에 대한 로드와 스토어를 재배치 할 수 있고, 어떤</span>
<span class="p_add">+     경우에는 CPU가 같은 변수로부터의 로드들을 재배치할 수도 있습니다.  이는</span>
<span class="p_add">+     다음의 코드가:</span>
<span class="p_add">+</span>
<span class="p_add">+	a[0] = x;</span>
<span class="p_add">+	a[1] = x;</span>
<span class="p_add">+</span>
<span class="p_add">+     x 의 예전 값이 a[1] 에, 새 값이 a[0] 에 있게 할 수 있다는 뜻입니다.</span>
<span class="p_add">+     컴파일러와 CPU가 이런 일을 못하게 하려면 다음과 같이 해야 합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	a[0] = READ_ONCE(x);</span>
<span class="p_add">+	a[1] = READ_ONCE(x);</span>
<span class="p_add">+</span>
<span class="p_add">+     즉, READ_ONCE() 와 WRITE_ONCE() 는 여러 CPU 에서 하나의 변수에 가해지는</span>
<span class="p_add">+     액세스들에 캐시 일관성을 제공합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 컴파일러는 같은 변수에 대한 연속적인 로드들을 병합할 수 있습니다.  그런</span>
<span class="p_add">+     병합 작업으로 컴파일러는 다음의 코드를:</span>
<span class="p_add">+</span>
<span class="p_add">+	while (tmp = a)</span>
<span class="p_add">+		do_something_with(tmp);</span>
<span class="p_add">+</span>
<span class="p_add">+     다음과 같이, 싱글 쓰레드 코드에서는 말이 되지만 개발자의 의도와 전혀 맞지</span>
<span class="p_add">+     않는 방향으로 &quot;최적화&quot; 할 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tmp = a)</span>
<span class="p_add">+		for (;;)</span>
<span class="p_add">+			do_something_with(tmp);</span>
<span class="p_add">+</span>
<span class="p_add">+     컴파일러가 이런 짓을 하지 못하게 하려면 READ_ONCE() 를 사용하세요:</span>
<span class="p_add">+</span>
<span class="p_add">+	while (tmp = READ_ONCE(a))</span>
<span class="p_add">+		do_something_with(tmp);</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 예컨대 레지스터 사용량이 많아 컴파일러가 모든 데이터를 레지스터에 담을 수</span>
<span class="p_add">+     없는 경우, 컴파일러는 변수를 다시 로드할 수 있습니다.  따라서 컴파일러는</span>
<span class="p_add">+     앞의 예에서 변수 &#39;tmp&#39; 사용을 최적화로 없애버릴 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	while (tmp = a)</span>
<span class="p_add">+		do_something_with(tmp);</span>
<span class="p_add">+</span>
<span class="p_add">+     이 코드는 다음과 같이 싱글 쓰레드에서는 완벽하지만 동시성이 존재하는</span>
<span class="p_add">+     경우엔 치명적인 코드로 바뀔 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	while (a)</span>
<span class="p_add">+		do_something_with(a);</span>
<span class="p_add">+</span>
<span class="p_add">+     예를 들어, 최적화된 이 코드는 변수 a 가 다른 CPU 에 의해 &quot;while&quot; 문과</span>
<span class="p_add">+     do_something_with() 호출 사이에 바뀌어 do_something_with() 에 0을 넘길</span>
<span class="p_add">+     수도 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     이번에도, 컴파일러가 그런 짓을 하는걸 막기 위해 READ_ONCE() 를 사용하세요:</span>
<span class="p_add">+</span>
<span class="p_add">+	while (tmp = READ_ONCE(a))</span>
<span class="p_add">+		do_something_with(tmp);</span>
<span class="p_add">+</span>
<span class="p_add">+     레지스터가 부족한 상황을 겪는 경우, 컴파일러는 tmp 를 스택에 저장해둘 수도</span>
<span class="p_add">+     있습니다.  컴파일러가 변수를 다시 읽어들이는건 이렇게 저장해두고 후에 다시</span>
<span class="p_add">+     읽어들이는데 드는 오버헤드 때문입니다.  그렇게 하는게 싱글 쓰레드</span>
<span class="p_add">+     코드에서는 안전하므로, 안전하지 않은 경우에는 컴파일러에게 직접 알려줘야</span>
<span class="p_add">+     합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 컴파일러는 그 값이 무엇일지 알고 있다면 로드를 아예 안할 수도 있습니다.</span>
<span class="p_add">+     예를 들어, 다음의 코드는 변수 &#39;a&#39; 의 값이 항상 0임을 증명할 수 있다면:</span>
<span class="p_add">+</span>
<span class="p_add">+	while (tmp = a)</span>
<span class="p_add">+		do_something_with(tmp);</span>
<span class="p_add">+</span>
<span class="p_add">+     이렇게 최적화 되어버릴 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	do { } while (0);</span>
<span class="p_add">+</span>
<span class="p_add">+     이 변환은 싱글 쓰레드 코드에서는 도움이 되는데 로드와 브랜치를 제거했기</span>
<span class="p_add">+     때문입니다.  문제는 컴파일러가 &#39;a&#39; 의 값을 업데이트 하는건 현재의 CPU 하나</span>
<span class="p_add">+     뿐이라는 가정 위에서 증명을 했다는데 있습니다.  만약 변수 &#39;a&#39; 가 공유되어</span>
<span class="p_add">+     있다면, 컴파일러의 증명은 틀린 것이 될겁니다.  컴파일러는 그 자신이</span>
<span class="p_add">+     생각하는 것만큼 많은 것을 알고 있지 못함을 컴파일러에게 알리기 위해</span>
<span class="p_add">+     READ_ONCE() 를 사용하세요:</span>
<span class="p_add">+</span>
<span class="p_add">+	while (tmp = READ_ONCE(a))</span>
<span class="p_add">+		do_something_with(tmp);</span>
<span class="p_add">+</span>
<span class="p_add">+     하지만 컴파일러는 READ_ONCE() 뒤에 나오는 값에 대해서도 눈길을 두고 있음을</span>
<span class="p_add">+     기억하세요.  예를 들어, 다음의 코드에서 MAX 는 전처리기 매크로로, 1의 값을</span>
<span class="p_add">+     갖는다고 해봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	while ((tmp = READ_ONCE(a)) % MAX)</span>
<span class="p_add">+		do_something_with(tmp);</span>
<span class="p_add">+</span>
<span class="p_add">+     이렇게 되면 컴파일러는 MAX 를 가지고 수행되는 &quot;%&quot; 오퍼레이터의 결과가 항상</span>
<span class="p_add">+     0이라는 것을 알게 되고, 컴파일러가 코드를 실질적으로는 존재하지 않는</span>
<span class="p_add">+     것처럼 최적화 하는 것이 허용되어 버립니다.  (&#39;a&#39; 변수의 로드는 여전히</span>
<span class="p_add">+     행해질 겁니다.)</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 비슷하게, 컴파일러는 변수가 저장하려 하는 값을 이미 가지고 있다는 것을</span>
<span class="p_add">+     알면 스토어 자체를 제거할 수 있습니다.  이번에도, 컴파일러는 현재의 CPU</span>
<span class="p_add">+     만이 그 변수에 값을 쓰는 오로지 하나의 존재라고 생각하여 공유된 변수에</span>
<span class="p_add">+     대해서는 잘못된 일을 하게 됩니다.  예를 들어, 다음과 같은 경우가 있을 수</span>
<span class="p_add">+     있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	a = 0;</span>
<span class="p_add">+	... 변수 a 에 스토어를 하지 않는 코드 ...</span>
<span class="p_add">+	a = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+     컴파일러는 변수 &#39;a&#39; 의 값은 이미 0이라는 것을 알고, 따라서 두번째 스토어를</span>
<span class="p_add">+     삭제할 겁니다.  만약 다른 CPU 가 그 사이 변수 &#39;a&#39; 에 다른 값을 썼다면</span>
<span class="p_add">+     황당한 결과가 나올 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     컴파일러가 그런 잘못된 추측을 하지 않도록 WRITE_ONCE() 를 사용하세요:</span>
<span class="p_add">+</span>
<span class="p_add">+	WRITE_ONCE(a, 0);</span>
<span class="p_add">+	... 변수 a 에 스토어를 하지 않는 코드 ...</span>
<span class="p_add">+	WRITE_ONCE(a, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 컴파일러는 하지 말라고 하지 않으면 메모리 액세스들을 재배치 할 수</span>
<span class="p_add">+     있습니다.  예를 들어, 다음의 프로세스 레벨 코드와 인터럽트 핸들러 사이의</span>
<span class="p_add">+     상호작용을 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	void process_level(void)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		msg = get_message();</span>
<span class="p_add">+		flag = true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	void interrupt_handler(void)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		if (flag)</span>
<span class="p_add">+			process_message(msg);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+     이 코드에는 컴파일러가 process_level() 을 다음과 같이 변환하는 것을 막을</span>
<span class="p_add">+     수단이 없고, 이런 변환은 싱글쓰레드에서라면 실제로 훌륭한 선택일 수</span>
<span class="p_add">+     있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	void process_level(void)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		flag = true;</span>
<span class="p_add">+		msg = get_message();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+     이 두개의 문장 사이에 인터럽트가 발생한다면, interrupt_handler() 는 의미를</span>
<span class="p_add">+     알 수 없는 메세지를 받을 수도 있습니다.  이걸 막기 위해 다음과 같이</span>
<span class="p_add">+     WRITE_ONCE() 를 사용하세요:</span>
<span class="p_add">+</span>
<span class="p_add">+	void process_level(void)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		WRITE_ONCE(msg, get_message());</span>
<span class="p_add">+		WRITE_ONCE(flag, true);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	void interrupt_handler(void)</span>
<span class="p_add">+	{</span>
<span class="p_add">+		if (READ_ONCE(flag))</span>
<span class="p_add">+			process_message(READ_ONCE(msg));</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+     interrupt_handler() 안에서도 중첩된 인터럽트나 NMI 와 같이 인터럽트 핸들러</span>
<span class="p_add">+     역시 &#39;flag&#39; 와 &#39;msg&#39; 에 접근하는 또다른 무언가에 인터럽트 될 수 있다면</span>
<span class="p_add">+     READ_ONCE() 와 WRITE_ONCE() 를 사용해야 함을 기억해 두세요.  만약 그런</span>
<span class="p_add">+     가능성이 없다면, interrupt_handler() 안에서는 문서화 목적이 아니라면</span>
<span class="p_add">+     READ_ONCE() 와 WRITE_ONCE() 는 필요치 않습니다.  (근래의 리눅스 커널에서</span>
<span class="p_add">+     중첩된 인터럽트는 보통 잘 일어나지 않음도 기억해 두세요, 실제로, 어떤</span>
<span class="p_add">+     인터럽트 핸들러가 인터럽트가 활성화된 채로 리턴하면 WARN_ONCE() 가</span>
<span class="p_add">+     실행됩니다.)</span>
<span class="p_add">+</span>
<span class="p_add">+     컴파일러는 READ_ONCE() 와 WRITE_ONCE() 뒤의 READ_ONCE() 나 WRITE_ONCE(),</span>
<span class="p_add">+     barrier(), 또는 비슷한 것들을 담고 있지 않은 코드를 움직일 수 있을 것으로</span>
<span class="p_add">+     가정되어야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     이 효과는 barrier() 를 통해서도 만들 수 있지만, READ_ONCE() 와</span>
<span class="p_add">+     WRITE_ONCE() 가 좀 더 안목 높은 선택입니다: READ_ONCE() 와 WRITE_ONCE()는</span>
<span class="p_add">+     컴파일러에 주어진 메모리 영역에 대해서만 최적화 가능성을 포기하도록</span>
<span class="p_add">+     하지만, barrier() 는 컴파일러가 지금까지 기계의 레지스터에 캐시해 놓은</span>
<span class="p_add">+     모든 메모리 영역의 값을 버려야 하게 하기 때문입니다.  물론, 컴파일러는</span>
<span class="p_add">+     READ_ONCE() 와 WRITE_ONCE() 가 일어난 순서도 지켜줍니다, CPU 는 당연히</span>
<span class="p_add">+     그 순서를 지킬 의무가 없지만요.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 컴파일러는 다음의 예에서와 같이 변수에의 스토어를 날조해낼 수도 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	if (a)</span>
<span class="p_add">+		b = a;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		b = 42;</span>
<span class="p_add">+</span>
<span class="p_add">+     컴파일러는 아래와 같은 최적화로 브랜치를 줄일 겁니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	b = 42;</span>
<span class="p_add">+	if (a)</span>
<span class="p_add">+		b = a;</span>
<span class="p_add">+</span>
<span class="p_add">+     싱글 쓰레드 코드에서 이 최적화는 안전할 뿐 아니라 브랜치 갯수를</span>
<span class="p_add">+     줄여줍니다.  하지만 안타깝게도, 동시성이 있는 코드에서는 이 최적화는 다른</span>
<span class="p_add">+     CPU 가 &#39;b&#39; 를 로드할 때, -- &#39;a&#39; 가 0이 아닌데도 -- 가짜인 값, 42를 보게</span>
<span class="p_add">+     되는 경우를 가능하게 합니다.  이걸 방지하기 위해 WRITE_ONCE() 를</span>
<span class="p_add">+     사용하세요:</span>
<span class="p_add">+</span>
<span class="p_add">+	if (a)</span>
<span class="p_add">+		WRITE_ONCE(b, a);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		WRITE_ONCE(b, 42);</span>
<span class="p_add">+</span>
<span class="p_add">+     컴파일러는 로드를 만들어낼 수도 있습니다.  일반적으로는 문제를 일으키지</span>
<span class="p_add">+     않지만, 캐시 라인 바운싱을 일으켜 성능과 확장성을 떨어뜨릴 수 있습니다.</span>
<span class="p_add">+     날조된 로드를 막기 위해선 READ_ONCE() 를 사용하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 정렬된 메모리 주소에 위치한, 한번의 메모리 참조 인스트럭션으로 액세스</span>
<span class="p_add">+     가능한 크기의 데이터는 하나의 큰 액세스가 여러개의 작은 액세스들로</span>
<span class="p_add">+     대체되는 &quot;로드 티어링(load tearing)&quot; 과 &quot;스토어 티어링(store tearing)&quot; 을</span>
<span class="p_add">+     방지합니다.  예를 들어, 주어진 아키텍쳐가 7-bit imeediate field 를 갖는</span>
<span class="p_add">+     16-bit 스토어 인스트럭션을 제공한다면, 컴파일러는 다음의 32-bit 스토어를</span>
<span class="p_add">+     구현하는데에 두개의 16-bit store-immediate 명령을 사용하려 할겁니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	p = 0x00010002;</span>
<span class="p_add">+</span>
<span class="p_add">+     스토어 할 상수를 만들고 그 값을 스토어 하기 위해 두개가 넘는 인스트럭션을</span>
<span class="p_add">+     사용하게 되는, 이런 종류의 최적화를 GCC 는 실제로 함을 부디 알아 두십시오.</span>
<span class="p_add">+     이 최적화는 싱글 쓰레드 코드에서는 성공적인 최적화 입니다.  실제로, 근래에</span>
<span class="p_add">+     발생한 (그리고 고쳐진) 버그는 GCC 가 volatile 스토어에 비정상적으로 이</span>
<span class="p_add">+     최적화를 사용하게 했습니다.  그런 버그가 없다면, 다음의 예에서</span>
<span class="p_add">+     WRITE_ONCE() 의 사용은 스토어 티어링을 방지합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	WRITE_ONCE(p, 0x00010002);</span>
<span class="p_add">+</span>
<span class="p_add">+     Packed 구조체의 사용 역시 다음의 예처럼  로드 / 스토어 티어링을 유발할 수</span>
<span class="p_add">+     있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	struct __attribute__((__packed__)) foo {</span>
<span class="p_add">+		short a;</span>
<span class="p_add">+		int b;</span>
<span class="p_add">+		short c;</span>
<span class="p_add">+	};</span>
<span class="p_add">+	struct foo foo1, foo2;</span>
<span class="p_add">+	...</span>
<span class="p_add">+</span>
<span class="p_add">+	foo2.a = foo1.a;</span>
<span class="p_add">+	foo2.b = foo1.b;</span>
<span class="p_add">+	foo2.c = foo1.c;</span>
<span class="p_add">+</span>
<span class="p_add">+     READ_ONCE() 나 WRITE_ONCE() 도 없고 volatile 마킹도 없기 때문에,</span>
<span class="p_add">+     컴파일러는 이 세개의 대입문을 두개의 32-bit 로드와 두개의 32-bit 스토어로</span>
<span class="p_add">+     변환할 수 있습니다.  이는 &#39;foo1.b&#39; 의 값의 로드 티어링과 &#39;foo2.b&#39; 의</span>
<span class="p_add">+     스토어 티어링을 초래할 겁니다.  이 예에서도 READ_ONCE() 와 WRITE_ONCE()</span>
<span class="p_add">+     가 티어링을 막을 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	foo2.a = foo1.a;</span>
<span class="p_add">+	WRITE_ONCE(foo2.b, READ_ONCE(foo1.b));</span>
<span class="p_add">+	foo2.c = foo1.c;</span>
<span class="p_add">+</span>
<span class="p_add">+그렇지만, volatile 로 마크된 변수에 대해서는 READ_ONCE() 와 WRITE_ONCE() 가</span>
<span class="p_add">+필요치 않습니다.  예를 들어, &#39;jiffies&#39; 는 volatile 로 마크되어 있기 때문에,</span>
<span class="p_add">+READ_ONCE(jiffies) 라고 할 필요가 없습니다.  READ_ONCE() 와 WRITE_ONCE() 가</span>
<span class="p_add">+실은 volatile 캐스팅으로 구현되어 있어서 인자가 이미 volatile 로 마크되어</span>
<span class="p_add">+있다면 또다른 효과를 내지는 않기 때문입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+이 컴파일러 배리어들은 CPU 에는 직접적 효과를 전혀 만들지 않기 때문에, 결국은</span>
<span class="p_add">+재배치가 일어날 수도 있음을 부디 기억해 두십시오.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 메모리 배리어</span>
<span class="p_add">+-----------------</span>
<span class="p_add">+</span>
<span class="p_add">+리눅스 커널은 다음의 여덟개 기본 CPU 메모리 배리어를 가지고 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	TYPE		MANDATORY		SMP CONDITIONAL</span>
<span class="p_add">+	===============	=======================	===========================</span>
<span class="p_add">+	범용		mb()			smp_mb()</span>
<span class="p_add">+	쓰기		wmb()			smp_wmb()</span>
<span class="p_add">+	읽기		rmb()			smp_rmb()</span>
<span class="p_add">+	데이터 의존성	read_barrier_depends()	smp_read_barrier_depends()</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+데이터 의존성 배리어를 제외한 모든 메모리 배리어는 컴파일러 배리어를</span>
<span class="p_add">+포함합니다.  데이터 의존성은 컴파일러에의 추가적인 순서 보장을 포함하지</span>
<span class="p_add">+않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+방백: 데이터 의존성이 있는 경우, 컴파일러는 해당 로드를 올바른 순서로 일으킬</span>
<span class="p_add">+것으로 (예: `a[b]` 는 a[b] 를 로드 하기 전에 b 의 값을 먼저 로드한다)</span>
<span class="p_add">+기대되지만, C 언어 사양에는 컴파일러가 b 의 값을 추측 (예: 1 과 같음) 해서</span>
<span class="p_add">+b  로드 전에 a 로드를 하는 코드 (예: tmp = a[1]; if (b != 1) tmp = a[b]; ) 를</span>
<span class="p_add">+만들지 않아야 한다는 내용 같은 건 없습니다.  또한 컴파일러는 a[b] 를 로드한</span>
<span class="p_add">+후에 b 를 또다시 로드할 수도 있어서, a[b] 보다 최신 버전의 b 값을 가질 수도</span>
<span class="p_add">+있습니다.  이런 문제들의 해결책에 대한 의견 일치는 아직 없습니다만, 일단</span>
<span class="p_add">+READ_ONCE() 매크로부터 보기 시작하는게 좋은 시작이 될겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+SMP 메모리 배리어들은 유니프로세서로 컴파일된 시스템에서는 컴파일러 배리어로</span>
<span class="p_add">+바뀌는데, 하나의 CPU 는 스스로 일관성을 유지하고, 겹치는 액세스들 역시 올바른</span>
<span class="p_add">+순서로 행해질 것으로 생각되기 때문입니다.  하지만, 아래의 &quot;Virtual Machine</span>
<span class="p_add">+Guests&quot; 서브섹션을 참고하십시오.</span>
<span class="p_add">+</span>
<span class="p_add">+[!] SMP 시스템에서 공유메모리로의 접근들을 순서 세워야 할 때, SMP 메모리</span>
<span class="p_add">+배리어는 _반드시_ 사용되어야 함을 기억하세요, 그대신 락을 사용하는 것으로도</span>
<span class="p_add">+충분하긴 하지만 말이죠.</span>
<span class="p_add">+</span>
<span class="p_add">+Mandatory 배리어들은 SMP 시스템에서도 UP 시스템에서도 SMP 효과만 통제하기에는</span>
<span class="p_add">+불필요한 오버헤드를 갖기 때문에 SMP 효과만 통제하면 되는 곳에는 사용되지 않아야</span>
<span class="p_add">+합니다.  하지만, 느슨한 순서 규칙의 메모리 I/O 윈도우를 통한 MMIO 의 효과를</span>
<span class="p_add">+통제할 때에는 mandatory 배리어들이 사용될 수 있습니다.  이 배리어들은</span>
<span class="p_add">+컴파일러와 CPU 모두 재배치를 못하도록 함으로써 메모리 오퍼레이션들이 디바이스에</span>
<span class="p_add">+보여지는 순서에도 영향을 주기 때문에, SMP 가 아닌 시스템이라 할지라도 필요할 수</span>
<span class="p_add">+있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+일부 고급 배리어 함수들도 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) smp_store_mb(var, value)</span>
<span class="p_add">+</span>
<span class="p_add">+     이 함수는 특정 변수에 특정 값을 대입하고 범용 메모리 배리어를 칩니다.</span>
<span class="p_add">+     UP 컴파일에서는 컴파일러 배리어보다 더한 것을 친다고는 보장되지 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) smp_mb__before_atomic();</span>
<span class="p_add">+ (*) smp_mb__after_atomic();</span>
<span class="p_add">+</span>
<span class="p_add">+     이것들은 값을 리턴하지 않는 (더하기, 빼기, 증가, 감소와 같은) 어토믹</span>
<span class="p_add">+     함수들을 위한, 특히 그것들이 레퍼런스 카운팅에 사용될 때를 위한</span>
<span class="p_add">+     함수들입니다.  이 함수들은 메모리 배리어를 내포하고 있지는 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     이것들은 값을 리턴하지 않으며 어토믹한 (set_bit 과 clear_bit 같은) 비트</span>
<span class="p_add">+     연산에도 사용될 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     한 예로, 객체 하나를 무효한 것으로 표시하고 그 객체의 레퍼런스 카운트를</span>
<span class="p_add">+     감소시키는 다음 코드를 보세요:</span>
<span class="p_add">+</span>
<span class="p_add">+	obj-&gt;dead = 1;</span>
<span class="p_add">+	smp_mb__before_atomic();</span>
<span class="p_add">+	atomic_dec(&amp;obj-&gt;ref_count);</span>
<span class="p_add">+</span>
<span class="p_add">+     이 코드는 객체의 업데이트된 death 마크가 레퍼런스 카운터 감소 동작</span>
<span class="p_add">+     *전에* 보일 것을 보장합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     더 많은 정보를 위해선 Documentation/atomic_ops.txt 문서를 참고하세요.</span>
<span class="p_add">+     어디서 이것들을 사용해야 할지 궁금하다면 &quot;어토믹 오퍼레이션&quot; 서브섹션을</span>
<span class="p_add">+     참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) lockless_dereference();</span>
<span class="p_add">+</span>
<span class="p_add">+     이 함수는 smp_read_barrier_depends() 데이터 의존성 배리어를 사용하는</span>
<span class="p_add">+     포인터 읽어오기 래퍼(wrapper) 함수로 생각될 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     객체의 라이프타임이 RCU 외의 메커니즘으로 관리된다는 점을 제외하면</span>
<span class="p_add">+     rcu_dereference() 와도 유사한데, 예를 들면 객체가 시스템이 꺼질 때에만</span>
<span class="p_add">+     제거되는 경우 등입니다.  또한, lockless_dereference() 은 RCU 와 함께</span>
<span class="p_add">+     사용될수도, RCU 없이 사용될 수도 있는 일부 데이터 구조에 사용되고</span>
<span class="p_add">+     있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) dma_wmb();</span>
<span class="p_add">+ (*) dma_rmb();</span>
<span class="p_add">+</span>
<span class="p_add">+     이것들은 CPU 와 DMA 가능한 디바이스에서 모두 액세스 가능한 공유 메모리의</span>
<span class="p_add">+     읽기, 쓰기 작업들의 순서를 보장하기 위해 consistent memory 에서 사용하기</span>
<span class="p_add">+     위한 것들입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     예를 들어, 디바이스와 메모리를 공유하며, 디스크립터 상태 값을 사용해</span>
<span class="p_add">+     디스크립터가 디바이스에 속해 있는지 아니면 CPU 에 속해 있는지 표시하고,</span>
<span class="p_add">+     공지용 초인종(doorbell) 을 사용해 업데이트된 디스크립터가 디바이스에 사용</span>
<span class="p_add">+     가능해졌음을 공지하는 디바이스 드라이버를 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	if (desc-&gt;status != DEVICE_OWN) {</span>
<span class="p_add">+		/* 디스크립터를 소유하기 전에는 데이터를 읽지 않음 */</span>
<span class="p_add">+		dma_rmb();</span>
<span class="p_add">+</span>
<span class="p_add">+		/* 데이터를 읽고 씀 */</span>
<span class="p_add">+		read_data = desc-&gt;data;</span>
<span class="p_add">+		desc-&gt;data = write_data;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* 상태 업데이트 전 수정사항을 반영 */</span>
<span class="p_add">+		dma_wmb();</span>
<span class="p_add">+</span>
<span class="p_add">+		/* 소유권을 수정 */</span>
<span class="p_add">+		desc-&gt;status = DEVICE_OWN;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* MMIO 를 통해 디바이스에 공지를 하기 전에 메모리를 동기화 */</span>
<span class="p_add">+		wmb();</span>
<span class="p_add">+</span>
<span class="p_add">+		/* 업데이트된 디스크립터의 디바이스에 공지 */</span>
<span class="p_add">+		writel(DESC_NOTIFY, doorbell);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+     dma_rmb() 는 디스크립터로부터 데이터를 읽어오기 전에 디바이스가 소유권을</span>
<span class="p_add">+     내놓았음을 보장하게 하고, dma_wmb() 는 디바이스가 자신이 소유권을 다시</span>
<span class="p_add">+     가졌음을 보기 전에 디스크립터에 데이터가 쓰였음을 보장합니다.  wmb() 는</span>
<span class="p_add">+     캐시 일관성이 없는 (cache incoherent) MMIO 영역에 쓰기를 시도하기 전에</span>
<span class="p_add">+     캐시 일관성이 있는 메모리 (cache coherent memory) 쓰기가 완료되었음을</span>
<span class="p_add">+     보장해주기 위해 필요합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     consistent memory 에 대한 자세한 내용을 위해선 Documentation/DMA-API.txt</span>
<span class="p_add">+     문서를 참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+MMIO 쓰기 배리어</span>
<span class="p_add">+----------------</span>
<span class="p_add">+</span>
<span class="p_add">+리눅스 커널은 또한 memory-mapped I/O 쓰기를 위한 특별한 배리어도 가지고</span>
<span class="p_add">+있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	mmiowb();</span>
<span class="p_add">+</span>
<span class="p_add">+이것은 mandatory 쓰기 배리어의 변종으로, 완화된 순서 규칙의 I/O 영역에으로의</span>
<span class="p_add">+쓰기가 부분적으로 순서를 맞추도록 해줍니다.  이 함수는 CPU-&gt;하드웨어 사이를</span>
<span class="p_add">+넘어서 실제 하드웨어에까지 일부 수준의 영향을 끼칩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+더 많은 정보를 위해선 &quot;Acquire vs I/O 액세스&quot; 서브섹션을 참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+=========================</span>
<span class="p_add">+암묵적 커널 메모리 배리어</span>
<span class="p_add">+=========================</span>
<span class="p_add">+</span>
<span class="p_add">+리눅스 커널의 일부 함수들은 메모리 배리어를 내장하고 있는데, 락(lock)과</span>
<span class="p_add">+스케쥴링 관련 함수들이 대부분입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+여기선 _최소한의_ 보장을 설명합니다; 특정 아키텍쳐에서는 이 설명보다 더 많은</span>
<span class="p_add">+보장을 제공할 수도 있습니다만 해당 아키텍쳐에 종속적인 코드 외의 부분에서는</span>
<span class="p_add">+그런 보장을 기대해선 안될겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+락 ACQUISITION 함수</span>
<span class="p_add">+-------------------</span>
<span class="p_add">+</span>
<span class="p_add">+리눅스 커널은 다양한 락 구성체를 가지고 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 스핀 락</span>
<span class="p_add">+ (*) R/W 스핀 락</span>
<span class="p_add">+ (*) 뮤텍스</span>
<span class="p_add">+ (*) 세마포어</span>
<span class="p_add">+ (*) R/W 세마포어</span>
<span class="p_add">+</span>
<span class="p_add">+각 구성체마다 모든 경우에 &quot;ACQUIRE&quot; 오퍼레이션과 &quot;RELEASE&quot; 오퍼레이션의 변종이</span>
<span class="p_add">+존재합니다.  이 오퍼레이션들은 모두 적절한 배리어를 내포하고 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (1) ACQUIRE 오퍼레이션의 영향:</span>
<span class="p_add">+</span>
<span class="p_add">+     ACQUIRE 뒤에서 요청된 메모리 오퍼레이션은 ACQUIRE 오퍼레이션이 완료된</span>
<span class="p_add">+     뒤에 완료됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     ACQUIRE 앞에서 요청된 메모리 오퍼레이션은 ACQUIRE 오퍼레이션이 완료된 후에</span>
<span class="p_add">+     완료될 수 있습니다.  smp_mb__before_spinlock() 뒤에 ACQUIRE 가 실행되는</span>
<span class="p_add">+     코드 블록은 블록 앞의 스토어를 블록 뒤의 로드와 스토어에 대해 순서</span>
<span class="p_add">+     맞춥니다.  이건 smp_mb() 보다 완화된 것임을 기억하세요!  많은 아키텍쳐에서</span>
<span class="p_add">+     smp_mb__before_spinlock() 은 사실 아무일도 하지 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (2) RELEASE 오퍼레이션의 영향:</span>
<span class="p_add">+</span>
<span class="p_add">+     RELEASE 앞에서 요청된 메모리 오퍼레이션은 RELEASE 오퍼레이션이 완료되기</span>
<span class="p_add">+     전에 완료됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     RELEASE 뒤에서 요청된 메모리 오퍼레이션은 RELEASE 오퍼레이션 완료 전에</span>
<span class="p_add">+     완료될 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (3) ACQUIRE vs ACQUIRE 영향:</span>
<span class="p_add">+</span>
<span class="p_add">+     어떤 ACQUIRE 오퍼레이션보다 앞에서 요청된 모든 ACQUIRE 오퍼레이션은 그</span>
<span class="p_add">+     ACQUIRE 오퍼레이션 전에 완료됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (4) ACQUIRE vs RELEASE implication:</span>
<span class="p_add">+</span>
<span class="p_add">+     어떤 RELEASE 오퍼레이션보다 앞서 요청된 ACQUIRE 오퍼레이션은 그 RELEASE</span>
<span class="p_add">+     오퍼레이션보다 먼저 완료됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (5) 실패한 조건적 ACQUIRE 영향:</span>
<span class="p_add">+</span>
<span class="p_add">+     ACQUIRE 오퍼레이션의 일부 락(lock) 변종은 락이 곧바로 획득하기에는</span>
<span class="p_add">+     불가능한 상태이거나 락이 획득 가능해지도록 기다리는 도중 시그널을 받거나</span>
<span class="p_add">+     해서 실패할 수 있습니다.  실패한 락은 어떤 배리어도 내포하지 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+[!] 참고: 락 ACQUIRE 와 RELEASE 가 단방향 배리어여서 나타나는 현상 중 하나는</span>
<span class="p_add">+크리티컬 섹션 바깥의 인스트럭션의 영향이 크리티컬 섹션 내부로도 들어올 수</span>
<span class="p_add">+있다는 것입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+RELEASE 후에 요청되는 ACQUIRE 는 전체 메모리 배리어라 여겨지면 안되는데,</span>
<span class="p_add">+ACQUIRE 앞의 액세스가 ACQUIRE 후에 수행될 수 있고, RELEASE 후의 액세스가</span>
<span class="p_add">+RELEASE 전에 수행될 수도 있으며, 그 두개의 액세스가 서로를 지나칠 수도 있기</span>
<span class="p_add">+때문입니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	*A = a;</span>
<span class="p_add">+	ACQUIRE M</span>
<span class="p_add">+	RELEASE M</span>
<span class="p_add">+	*B = b;</span>
<span class="p_add">+</span>
<span class="p_add">+는 다음과 같이 될 수도 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	ACQUIRE M, STORE *B, STORE *A, RELEASE M</span>
<span class="p_add">+</span>
<span class="p_add">+ACQUIRE 와 RELEASE 가 락 획득과 해제라면, 그리고 락의 ACQUIRE 와 RELEASE 가</span>
<span class="p_add">+같은 락 변수에 대한 것이라면, 해당 락을 쥐고 있지 않은 다른 CPU 의 시야에는</span>
<span class="p_add">+이와 같은 재배치가 일어나는 것으로 보일 수 있습니다.  요약하자면, ACQUIRE 에</span>
<span class="p_add">+이어 RELEASE 오퍼레이션을 순차적으로 실행하는 행위가 전체 메모리 배리어로</span>
<span class="p_add">+생각되어선 -안됩니다-.</span>
<span class="p_add">+</span>
<span class="p_add">+비슷하게, 앞의 반대 케이스인 RELEASE 와 ACQUIRE 두개 오퍼레이션의 순차적 실행</span>
<span class="p_add">+역시 전체 메모리 배리어를 내포하지 않습니다.  따라서, RELEASE, ACQUIRE 로</span>
<span class="p_add">+규정되는 크리티컬 섹션의 CPU 수행은 RELEASE 와 ACQUIRE 를 가로지를 수 있으므로,</span>
<span class="p_add">+다음과 같은 코드는:</span>
<span class="p_add">+</span>
<span class="p_add">+	*A = a;</span>
<span class="p_add">+	RELEASE M</span>
<span class="p_add">+	ACQUIRE N</span>
<span class="p_add">+	*B = b;</span>
<span class="p_add">+</span>
<span class="p_add">+다음과 같이 수행될 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	ACQUIRE N, STORE *B, STORE *A, RELEASE M</span>
<span class="p_add">+</span>
<span class="p_add">+이런 재배치는 데드락을 일으킬 수도 있을 것처럼 보일 수 있습니다.  하지만, 그런</span>
<span class="p_add">+데드락의 조짐이 있다면 RELEASE 는 단순히 완료될 것이므로 데드락은 존재할 수</span>
<span class="p_add">+없습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+	이게 어떻게 올바른 동작을 할 수 있을까요?</span>
<span class="p_add">+</span>
<span class="p_add">+	우리가 이야기 하고 있는건 재배치를 하는 CPU 에 대한 이야기이지,</span>
<span class="p_add">+	컴파일러에 대한 것이 아니란 점이 핵심입니다.  컴파일러 (또는, 개발자)</span>
<span class="p_add">+	가 오퍼레이션들을 이렇게 재배치하면, 데드락이 일어날 수 -있습-니다.</span>
<span class="p_add">+</span>
<span class="p_add">+	하지만 CPU 가 오퍼레이션들을 재배치 했다는걸 생각해 보세요.  이 예에서,</span>
<span class="p_add">+	어셈블리 코드 상으로는 언락이 락을 앞서게 되어 있습니다.  CPU 가 이를</span>
<span class="p_add">+	재배치해서 뒤의 락 오퍼레이션을 먼저 실행하게 됩니다.  만약 데드락이</span>
<span class="p_add">+	존재한다면, 이 락 오퍼레이션은 그저 스핀을 하며 계속해서 락을</span>
<span class="p_add">+	시도합니다 (또는, 한참 후에겠지만, 잠듭니다).  CPU 는 언젠가는</span>
<span class="p_add">+	(어셈블리 코드에서는 락을 앞서는) 언락 오퍼레이션을 실행하는데, 이 언락</span>
<span class="p_add">+	오퍼레이션이 잠재적 데드락을 해결하고, 락 오퍼레이션도 뒤이어 성공하게</span>
<span class="p_add">+	됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+	하지만 만약 락이 잠을 자는 타입이었다면요?  그런 경우에 코드는</span>
<span class="p_add">+	스케쥴러로 들어가려 할 거고, 여기서 결국은 메모리 배리어를 만나게</span>
<span class="p_add">+	되는데, 이 메모리 배리어는 앞의 언락 오퍼레이션이 완료되도록 만들고,</span>
<span class="p_add">+	데드락은 이번에도 해결됩니다.  잠을 자는 행위와 언락 사이의 경주 상황</span>
<span class="p_add">+	(race) 도 있을 수 있겠습니다만, 락 관련 기능들은 그런 경주 상황을 모든</span>
<span class="p_add">+	경우에 제대로 해결할 수 있어야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+락과 세마포어는 UP 컴파일된 시스템에서의 순서에 대해 보장을 하지 않기 때문에,</span>
<span class="p_add">+그런 상황에서 인터럽트 비활성화 오퍼레이션과 함께가 아니라면 어떤 일에도 - 특히</span>
<span class="p_add">+I/O 액세스와 관련해서는 - 제대로 사용될 수 없을 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+&quot;CPU 간 ACQUIRING 배리어 효과&quot; 섹션도 참고하시기 바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+예를 들어, 다음과 같은 코드를 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	*A = a;</span>
<span class="p_add">+	*B = b;</span>
<span class="p_add">+	ACQUIRE</span>
<span class="p_add">+	*C = c;</span>
<span class="p_add">+	*D = d;</span>
<span class="p_add">+	RELEASE</span>
<span class="p_add">+	*E = e;</span>
<span class="p_add">+	*F = f;</span>
<span class="p_add">+</span>
<span class="p_add">+여기선 다음의 이벤트 시퀀스가 생길 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	ACQUIRE, {*F,*A}, *E, {*C,*D}, *B, RELEASE</span>
<span class="p_add">+</span>
<span class="p_add">+	[+] {*F,*A} 는 조합된 액세스를 의미합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+하지만 다음과 같은 건 불가능하죠:</span>
<span class="p_add">+</span>
<span class="p_add">+	{*F,*A}, *B,	ACQUIRE, *C, *D,	RELEASE, *E</span>
<span class="p_add">+	*A, *B, *C,	ACQUIRE, *D,		RELEASE, *E, *F</span>
<span class="p_add">+	*A, *B,		ACQUIRE, *C,		RELEASE, *D, *E, *F</span>
<span class="p_add">+	*B,		ACQUIRE, *C, *D,	RELEASE, {*F,*A}, *E</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+인터럽트 비활성화 함수</span>
<span class="p_add">+----------------------</span>
<span class="p_add">+</span>
<span class="p_add">+인터럽트를 비활성화 하는 함수 (ACQUIRE 와 동일) 와 인터럽트를 활성화 하는 함수</span>
<span class="p_add">+(RELEASE 와 동일) 는 컴파일러 배리어처럼만 동작합니다.  따라서, 별도의 메모리</span>
<span class="p_add">+배리어나 I/O 배리어가 필요한 상황이라면 그 배리어들은 인터럽트 비활성화 함수</span>
<span class="p_add">+외의 방법으로 제공되어야만 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+슬립과 웨이크업 함수</span>
<span class="p_add">+--------------------</span>
<span class="p_add">+</span>
<span class="p_add">+글로벌 데이터에 표시된 이벤트에 의해 프로세스를 잠에 빠트리는 것과 깨우는 것은</span>
<span class="p_add">+해당 이벤트를 기다리는 태스크의 태스크 상태와 그 이벤트를 알리기 위해 사용되는</span>
<span class="p_add">+글로벌 데이터, 두 데이터간의 상호작용으로 볼 수 있습니다.  이것이 옳은 순서대로</span>
<span class="p_add">+일어남을 분명히 하기 위해, 프로세스를 잠에 들게 하는 기능과 깨우는 기능은</span>
<span class="p_add">+몇가지 배리어를 내포합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+먼저, 잠을 재우는 쪽은 일반적으로 다음과 같은 이벤트 시퀀스를 따릅니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	for (;;) {</span>
<span class="p_add">+		set_current_state(TASK_UNINTERRUPTIBLE);</span>
<span class="p_add">+		if (event_indicated)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		schedule();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+set_current_state() 에 의해, 태스크 상태가 바뀐 후 범용 메모리 배리어가</span>
<span class="p_add">+자동으로 삽입됩니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1</span>
<span class="p_add">+	===============================</span>
<span class="p_add">+	set_current_state();</span>
<span class="p_add">+	  smp_store_mb();</span>
<span class="p_add">+	    STORE current-&gt;state</span>
<span class="p_add">+	    &lt;범용 배리어&gt;</span>
<span class="p_add">+	LOAD event_indicated</span>
<span class="p_add">+</span>
<span class="p_add">+set_current_state() 는 다음의 것들로 감싸질 수도 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	prepare_to_wait();</span>
<span class="p_add">+	prepare_to_wait_exclusive();</span>
<span class="p_add">+</span>
<span class="p_add">+이것들 역시 상태를 설정한 후 범용 메모리 배리어를 삽입합니다.</span>
<span class="p_add">+앞의 전체 시퀀스는 다음과 같은 함수들로 한번에 수행 가능한데, 이것들은 모두</span>
<span class="p_add">+올바른 장소에 메모리 배리어를 삽입합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	wait_event();</span>
<span class="p_add">+	wait_event_interruptible();</span>
<span class="p_add">+	wait_event_interruptible_exclusive();</span>
<span class="p_add">+	wait_event_interruptible_timeout();</span>
<span class="p_add">+	wait_event_killable();</span>
<span class="p_add">+	wait_event_timeout();</span>
<span class="p_add">+	wait_on_bit();</span>
<span class="p_add">+	wait_on_bit_lock();</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+두번째로, 깨우기를 수행하는 코드는 일반적으로 다음과 같을 겁니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	event_indicated = 1;</span>
<span class="p_add">+	wake_up(&amp;event_wait_queue);</span>
<span class="p_add">+</span>
<span class="p_add">+또는:</span>
<span class="p_add">+</span>
<span class="p_add">+	event_indicated = 1;</span>
<span class="p_add">+	wake_up_process(event_daemon);</span>
<span class="p_add">+</span>
<span class="p_add">+wake_up() 류에 의해 쓰기 메모리 배리어가 내포됩니다.  만약 그것들이 뭔가를</span>
<span class="p_add">+깨운다면요.  이 배리어는 태스크 상태가 지워지기 전에 수행되므로, 이벤트를</span>
<span class="p_add">+알리기 위한 STORE 와 태스크 상태를 TASK_RUNNING 으로 설정하는 STORE 사이에</span>
<span class="p_add">+위치하게 됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1				CPU 2</span>
<span class="p_add">+	===============================	===============================</span>
<span class="p_add">+	set_current_state();		STORE event_indicated</span>
<span class="p_add">+	  smp_store_mb();		wake_up();</span>
<span class="p_add">+	    STORE current-&gt;state	  &lt;쓰기 배리어&gt;</span>
<span class="p_add">+	    &lt;범용 배리어&gt;		  STORE current-&gt;state</span>
<span class="p_add">+	LOAD event_indicated</span>
<span class="p_add">+</span>
<span class="p_add">+한번더 말합니다만, 이 쓰기 메모리 배리어는 이 코드가 정말로 뭔가를 깨울 때에만</span>
<span class="p_add">+실행됩니다.  이걸 설명하기 위해, X 와 Y 는 모두 0 으로 초기화 되어 있다는 가정</span>
<span class="p_add">+하에 아래의 이벤트 시퀀스를 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1				CPU 2</span>
<span class="p_add">+	===============================	===============================</span>
<span class="p_add">+	X = 1;				STORE event_indicated</span>
<span class="p_add">+	smp_mb();			wake_up();</span>
<span class="p_add">+	Y = 1;				wait_event(wq, Y == 1);</span>
<span class="p_add">+	wake_up();			  load from Y sees 1, no memory barrier</span>
<span class="p_add">+					load from X might see 0</span>
<span class="p_add">+</span>
<span class="p_add">+위 예제에서의 경우와 달리 깨우기가 정말로 행해졌다면, CPU 2 의 X 로드는 1 을</span>
<span class="p_add">+본다고 보장될 수 있을 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+사용 가능한 깨우기류 함수들로 다음과 같은 것들이 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	complete();</span>
<span class="p_add">+	wake_up();</span>
<span class="p_add">+	wake_up_all();</span>
<span class="p_add">+	wake_up_bit();</span>
<span class="p_add">+	wake_up_interruptible();</span>
<span class="p_add">+	wake_up_interruptible_all();</span>
<span class="p_add">+	wake_up_interruptible_nr();</span>
<span class="p_add">+	wake_up_interruptible_poll();</span>
<span class="p_add">+	wake_up_interruptible_sync();</span>
<span class="p_add">+	wake_up_interruptible_sync_poll();</span>
<span class="p_add">+	wake_up_locked();</span>
<span class="p_add">+	wake_up_locked_poll();</span>
<span class="p_add">+	wake_up_nr();</span>
<span class="p_add">+	wake_up_poll();</span>
<span class="p_add">+	wake_up_process();</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+[!] 잠재우는 코드와 깨우는 코드에 내포되는 메모리 배리어들은 깨우기 전에</span>
<span class="p_add">+이루어진 스토어를 잠재우는 코드가 set_current_state() 를 호출한 후에 행하는</span>
<span class="p_add">+로드에 대해 순서를 맞추지 _않는다는_ 점을 기억하세요.  예를 들어, 잠재우는</span>
<span class="p_add">+코드가 다음과 같고:</span>
<span class="p_add">+</span>
<span class="p_add">+	set_current_state(TASK_INTERRUPTIBLE);</span>
<span class="p_add">+	if (event_indicated)</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	__set_current_state(TASK_RUNNING);</span>
<span class="p_add">+	do_something(my_data);</span>
<span class="p_add">+</span>
<span class="p_add">+깨우는 코드는 다음과 같다면:</span>
<span class="p_add">+</span>
<span class="p_add">+	my_data = value;</span>
<span class="p_add">+	event_indicated = 1;</span>
<span class="p_add">+	wake_up(&amp;event_wait_queue);</span>
<span class="p_add">+</span>
<span class="p_add">+event_indecated 에의 변경이 잠재우는 코드에게 my_data 에의 변경 후에 이루어진</span>
<span class="p_add">+것으로 인지될 것이라는 보장이 없습니다.  이런 경우에는 양쪽 코드 모두 각각의</span>
<span class="p_add">+데이터 액세스 사이에 메모리 배리어를 직접 쳐야 합니다.  따라서 앞의 재우는</span>
<span class="p_add">+코드는 다음과 같이:</span>
<span class="p_add">+</span>
<span class="p_add">+	set_current_state(TASK_INTERRUPTIBLE);</span>
<span class="p_add">+	if (event_indicated) {</span>
<span class="p_add">+		smp_rmb();</span>
<span class="p_add">+		do_something(my_data);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+그리고 깨우는 코드는 다음과 같이 되어야 합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	my_data = value;</span>
<span class="p_add">+	smp_wmb();</span>
<span class="p_add">+	event_indicated = 1;</span>
<span class="p_add">+	wake_up(&amp;event_wait_queue);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+그외의 함수들</span>
<span class="p_add">+-------------</span>
<span class="p_add">+</span>
<span class="p_add">+그외의 배리어를 내포하는 함수들은 다음과 같습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) schedule() 과 그 유사한 것들이 완전한 메모리 배리어를 내포합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+==============================</span>
<span class="p_add">+CPU 간 ACQUIRING 배리어의 효과</span>
<span class="p_add">+==============================</span>
<span class="p_add">+</span>
<span class="p_add">+SMP 시스템에서의 락 기능들은 더욱 강력한 형태의 배리어를 제공합니다: 이</span>
<span class="p_add">+배리어는 동일한 락을 사용하는 다른 CPU 들의 메모리 액세스 순서에도 영향을</span>
<span class="p_add">+끼칩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+ACQUIRE VS 메모리 액세스</span>
<span class="p_add">+------------------------</span>
<span class="p_add">+</span>
<span class="p_add">+다음의 예를 생각해 봅시다: 시스템은 두개의 스핀락 (M) 과 (Q), 그리고 세개의 CPU</span>
<span class="p_add">+를 가지고 있습니다; 여기에 다음의 이벤트 시퀀스가 발생합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1				CPU 2</span>
<span class="p_add">+	===============================	===============================</span>
<span class="p_add">+	WRITE_ONCE(*A, a);		WRITE_ONCE(*E, e);</span>
<span class="p_add">+	ACQUIRE M			ACQUIRE Q</span>
<span class="p_add">+	WRITE_ONCE(*B, b);		WRITE_ONCE(*F, f);</span>
<span class="p_add">+	WRITE_ONCE(*C, c);		WRITE_ONCE(*G, g);</span>
<span class="p_add">+	RELEASE M			RELEASE Q</span>
<span class="p_add">+	WRITE_ONCE(*D, d);		WRITE_ONCE(*H, h);</span>
<span class="p_add">+</span>
<span class="p_add">+*A 로의 액세스부터 *H 로의 액세스까지가 어떤 순서로 CPU 3 에게 보여질지에</span>
<span class="p_add">+대해서는 각 CPU 에서의 락 사용에 의해 내포되어 있는 제약을 제외하고는 어떤</span>
<span class="p_add">+보장도 존재하지 않습니다.  예를 들어, CPU 3 에게 다음과 같은 순서로 보여지는</span>
<span class="p_add">+것이 가능합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	*E, ACQUIRE M, ACQUIRE Q, *G, *C, *F, *A, *B, RELEASE Q, *D, *H, RELEASE M</span>
<span class="p_add">+</span>
<span class="p_add">+하지만 다음과 같이 보이지는 않을 겁니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	*B, *C or *D preceding ACQUIRE M</span>
<span class="p_add">+	*A, *B or *C following RELEASE M</span>
<span class="p_add">+	*F, *G or *H preceding ACQUIRE Q</span>
<span class="p_add">+	*E, *F or *G following RELEASE Q</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+ACQUIRE VS I/O 액세스</span>
<span class="p_add">+----------------------</span>
<span class="p_add">+</span>
<span class="p_add">+특정한 (특히 NUMA 가 관련된) 환경 하에서 두개의 CPU 에서 동일한 스핀락으로</span>
<span class="p_add">+보호되는 두개의 크리티컬 섹션 안의 I/O 액세스는 PCI 브릿지에 겹쳐진 I/O</span>
<span class="p_add">+액세스로 보일 수 있는데, PCI 브릿지는 캐시 일관성 프로토콜과 합을 맞춰야 할</span>
<span class="p_add">+의무가 없으므로, 필요한 읽기 메모리 배리어가 요청되지 않기 때문입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+예를 들어서:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1				CPU 2</span>
<span class="p_add">+	===============================	===============================</span>
<span class="p_add">+	spin_lock(Q)</span>
<span class="p_add">+	writel(0, ADDR)</span>
<span class="p_add">+	writel(1, DATA);</span>
<span class="p_add">+	spin_unlock(Q);</span>
<span class="p_add">+					spin_lock(Q);</span>
<span class="p_add">+					writel(4, ADDR);</span>
<span class="p_add">+					writel(5, DATA);</span>
<span class="p_add">+					spin_unlock(Q);</span>
<span class="p_add">+</span>
<span class="p_add">+는 PCI 브릿지에 다음과 같이 보일 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	STORE *ADDR = 0, STORE *ADDR = 4, STORE *DATA = 1, STORE *DATA = 5</span>
<span class="p_add">+</span>
<span class="p_add">+이렇게 되면 하드웨어의 오동작을 일으킬 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+이런 경우엔 잡아둔 스핀락을 내려놓기 전에 mmiowb() 를 수행해야 하는데, 예를</span>
<span class="p_add">+들면 다음과 같습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1				CPU 2</span>
<span class="p_add">+	===============================	===============================</span>
<span class="p_add">+	spin_lock(Q)</span>
<span class="p_add">+	writel(0, ADDR)</span>
<span class="p_add">+	writel(1, DATA);</span>
<span class="p_add">+	mmiowb();</span>
<span class="p_add">+	spin_unlock(Q);</span>
<span class="p_add">+					spin_lock(Q);</span>
<span class="p_add">+					writel(4, ADDR);</span>
<span class="p_add">+					writel(5, DATA);</span>
<span class="p_add">+					mmiowb();</span>
<span class="p_add">+					spin_unlock(Q);</span>
<span class="p_add">+</span>
<span class="p_add">+이 코드는 CPU 1 에서 요청된 두개의 스토어가 PCI 브릿지에 CPU 2 에서 요청된</span>
<span class="p_add">+스토어들보다 먼저 보여짐을 보장합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+또한, 같은 디바이스에서 스토어를 이어 로드가 수행되면 이 로드는 로드가 수행되기</span>
<span class="p_add">+전에 스토어가 완료되기를 강제하므로 mmiowb() 의 필요가 없어집니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1				CPU 2</span>
<span class="p_add">+	===============================	===============================</span>
<span class="p_add">+	spin_lock(Q)</span>
<span class="p_add">+	writel(0, ADDR)</span>
<span class="p_add">+	a = readl(DATA);</span>
<span class="p_add">+	spin_unlock(Q);</span>
<span class="p_add">+					spin_lock(Q);</span>
<span class="p_add">+					writel(4, ADDR);</span>
<span class="p_add">+					b = readl(DATA);</span>
<span class="p_add">+					spin_unlock(Q);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+더 많은 정보를 위해선 Documenataion/DocBook/deviceiobook.tmpl 을 참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+=========================</span>
<span class="p_add">+메모리 배리어가 필요한 곳</span>
<span class="p_add">+=========================</span>
<span class="p_add">+</span>
<span class="p_add">+설령 SMP 커널을 사용하더라도 싱글 쓰레드로 동작하는 코드는 올바르게 동작하는</span>
<span class="p_add">+것으로 보여질 것이기 때문에, 평범한 시스템 운영중에 메모리 오퍼레이션 재배치는</span>
<span class="p_add">+일반적으로 문제가 되지 않습니다.  하지만, 재배치가 문제가 _될 수 있는_ 네가지</span>
<span class="p_add">+환경이 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 프로세서간 상호 작용.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 어토믹 오퍼레이션.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 디바이스 액세스.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 인터럽트.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+프로세서간 상호 작용</span>
<span class="p_add">+--------------------</span>
<span class="p_add">+</span>
<span class="p_add">+두개 이상의 프로세서를 가진 시스템이 있다면, 시스템의 두개 이상의 CPU 는 동시에</span>
<span class="p_add">+같은 데이터에 대한 작업을 할 수 있습니다.  이는 동기화 문제를 일으킬 수 있고,</span>
<span class="p_add">+이 문제를 해결하는 일반적 방법은 락을 사용하는 것입니다.  하지만, 락은 상당히</span>
<span class="p_add">+비용이 비싸서 가능하면 락을 사용하지 않고 일을 처리하는 것이 낫습니다.  이런</span>
<span class="p_add">+경우, 두 CPU 모두에 영향을 끼치는 오퍼레이션들은 오동작을 막기 위해 신중하게</span>
<span class="p_add">+순서가 맞춰져야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+예를 들어, R/W 세마포어의 느린 수행경로 (slow path) 를 생각해 봅시다.</span>
<span class="p_add">+세마포어를 위해 대기를 하는 하나의 프로세스가 자신의 스택 중 일부를 이</span>
<span class="p_add">+세마포어의 대기 프로세스 리스트에 링크한 채로 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	struct rw_semaphore {</span>
<span class="p_add">+		...</span>
<span class="p_add">+		spinlock_t lock;</span>
<span class="p_add">+		struct list_head waiters;</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	struct rwsem_waiter {</span>
<span class="p_add">+		struct list_head list;</span>
<span class="p_add">+		struct task_struct *task;</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+특정 대기 상태 프로세스를 깨우기 위해, up_read() 나 up_write() 함수는 다음과</span>
<span class="p_add">+같은 일을 합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (1) 다음 대기 상태 프로세스 레코드는 어디있는지 알기 위해 이 대기 상태</span>
<span class="p_add">+     프로세스 레코드의 next 포인터를 읽습니다;</span>
<span class="p_add">+</span>
<span class="p_add">+ (2) 이 대기 상태 프로세스의 task 구조체로의 포인터를 읽습니다;</span>
<span class="p_add">+</span>
<span class="p_add">+ (3) 이 대기 상태 프로세스가 세마포어를 획득했음을 알리기 위해 task</span>
<span class="p_add">+     포인터를 초기화 합니다;</span>
<span class="p_add">+</span>
<span class="p_add">+ (4) 해당 태스크에 대해 wake_up_process() 를 호출합니다; 그리고</span>
<span class="p_add">+</span>
<span class="p_add">+ (5) 해당 대기 상태 프로세스의 task 구조체를 잡고 있던 레퍼런스를 해제합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+달리 말하자면, 다음 이벤트 시퀀스를 수행해야 합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	LOAD waiter-&gt;list.next;</span>
<span class="p_add">+	LOAD waiter-&gt;task;</span>
<span class="p_add">+	STORE waiter-&gt;task;</span>
<span class="p_add">+	CALL wakeup</span>
<span class="p_add">+	RELEASE task</span>
<span class="p_add">+</span>
<span class="p_add">+그리고 이 이벤트들이 다른 순서로 수행된다면, 오동작이 일어날 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+한번 세마포어의 대기줄에 들어갔고 세마포어 락을 놓았다면, 해당 대기 프로세스는</span>
<span class="p_add">+락을 다시는 잡지 않습니다; 대신 자신의 task 포인터가 초기화 되길 기다립니다.</span>
<span class="p_add">+그 레코드는 대기 프로세스의 스택에 있기 때문에, 리스트의 next 포인터가 읽혀지기</span>
<span class="p_add">+_전에_ task 포인터가 지워진다면, 다른 CPU 는 해당 대기 프로세스를 시작해 버리고</span>
<span class="p_add">+up*() 함수가 next 포인터를 읽기 전에 대기 프로세스의 스택을 마구 건드릴 수</span>
<span class="p_add">+있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+그렇게 되면 위의 이벤트 시퀀스에 어떤 일이 일어나는지 생각해 보죠:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1				CPU 2</span>
<span class="p_add">+	===============================	===============================</span>
<span class="p_add">+					down_xxx()</span>
<span class="p_add">+					Queue waiter</span>
<span class="p_add">+					Sleep</span>
<span class="p_add">+	up_yyy()</span>
<span class="p_add">+	LOAD waiter-&gt;task;</span>
<span class="p_add">+	STORE waiter-&gt;task;</span>
<span class="p_add">+					Woken up by other event</span>
<span class="p_add">+	&lt;preempt&gt;</span>
<span class="p_add">+					Resume processing</span>
<span class="p_add">+					down_xxx() returns</span>
<span class="p_add">+					call foo()</span>
<span class="p_add">+					foo() clobbers *waiter</span>
<span class="p_add">+	&lt;/preempt&gt;</span>
<span class="p_add">+	LOAD waiter-&gt;list.next;</span>
<span class="p_add">+	--- OOPS ---</span>
<span class="p_add">+</span>
<span class="p_add">+이 문제는 세마포어 락의 사용으로 해결될 수도 있겠지만, 그렇게 되면 깨어난 후에</span>
<span class="p_add">+down_xxx() 함수가 불필요하게 스핀락을 또다시 얻어야만 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+이 문제를 해결하는 방법은 범용 SMP 메모리 배리어를 추가하는 겁니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	LOAD waiter-&gt;list.next;</span>
<span class="p_add">+	LOAD waiter-&gt;task;</span>
<span class="p_add">+	smp_mb();</span>
<span class="p_add">+	STORE waiter-&gt;task;</span>
<span class="p_add">+	CALL wakeup</span>
<span class="p_add">+	RELEASE task</span>
<span class="p_add">+</span>
<span class="p_add">+이 경우에, 배리어는 시스템의 나머지 CPU 들에게 모든 배리어 앞의 메모리 액세스가</span>
<span class="p_add">+배리어 뒤의 메모리 액세스보다 앞서 일어난 것으로 보이게 만듭니다.  배리어 앞의</span>
<span class="p_add">+메모리 액세스들이 배리어 명령 자체가 완료되는 시점까지 완료된다고는 보장하지</span>
<span class="p_add">+_않습니다_.</span>
<span class="p_add">+</span>
<span class="p_add">+(이게 문제가 되지 않을) 단일 프로세서 시스템에서 smp_mb() 는 실제로는 그저</span>
<span class="p_add">+컴파일러가 CPU 안에서의 순서를 바꾸거나 하지 않고 주어진 순서대로 명령을</span>
<span class="p_add">+내리도록 하는 컴파일러 배리어일 뿐입니다.  오직 하나의 CPU 만 있으니, CPU 의</span>
<span class="p_add">+의존성 순서 로직이 그 외의 모든것을 알아서 처리할 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+어토믹 오퍼레이션</span>
<span class="p_add">+-----------------</span>
<span class="p_add">+</span>
<span class="p_add">+어토믹 오퍼레이션은 기술적으로 프로세서간 상호작용으로 분류되며 그 중 일부는</span>
<span class="p_add">+전체 메모리 배리어를 내포하고 또 일부는 내포하지 않지만, 커널에서 상당히</span>
<span class="p_add">+의존적으로 사용하는 기능 중 하나입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+메모리의 어떤 상태를 수정하고 해당 상태에 대한 (예전의 또는 최신의) 정보를</span>
<span class="p_add">+리턴하는 어토믹 오퍼레이션은 모두 SMP-조건적 범용 메모리 배리어(smp_mb())를</span>
<span class="p_add">+실제 오퍼레이션의 앞과 뒤에 내포합니다.  이런 오퍼레이션은 다음의 것들을</span>
<span class="p_add">+포함합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	xchg();</span>
<span class="p_add">+	atomic_xchg();			atomic_long_xchg();</span>
<span class="p_add">+	atomic_inc_return();		atomic_long_inc_return();</span>
<span class="p_add">+	atomic_dec_return();		atomic_long_dec_return();</span>
<span class="p_add">+	atomic_add_return();		atomic_long_add_return();</span>
<span class="p_add">+	atomic_sub_return();		atomic_long_sub_return();</span>
<span class="p_add">+	atomic_inc_and_test();		atomic_long_inc_and_test();</span>
<span class="p_add">+	atomic_dec_and_test();		atomic_long_dec_and_test();</span>
<span class="p_add">+	atomic_sub_and_test();		atomic_long_sub_and_test();</span>
<span class="p_add">+	atomic_add_negative();		atomic_long_add_negative();</span>
<span class="p_add">+	test_and_set_bit();</span>
<span class="p_add">+	test_and_clear_bit();</span>
<span class="p_add">+	test_and_change_bit();</span>
<span class="p_add">+</span>
<span class="p_add">+	/* exchange 조건이 성공할 때 */</span>
<span class="p_add">+	cmpxchg();</span>
<span class="p_add">+	atomic_cmpxchg();		atomic_long_cmpxchg();</span>
<span class="p_add">+	atomic_add_unless();		atomic_long_add_unless();</span>
<span class="p_add">+</span>
<span class="p_add">+이것들은 메모리 배리어 효과가 필요한 ACQUIRE 부류와 RELEASE 부류 오퍼레이션들을</span>
<span class="p_add">+구현할 때, 그리고 객체 해제를 위해 레퍼런스 카운터를 조정할 때, 암묵적 메모리</span>
<span class="p_add">+배리어 효과가 필요한 곳 등에 사용됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+다음의 오퍼레이션들은 메모리 배리어를 내포하지 _않기_ 때문에 문제가 될 수</span>
<span class="p_add">+있지만, RELEASE 부류의 오퍼레이션들과 같은 것들을 구현할 때 사용될 수도</span>
<span class="p_add">+있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	atomic_set();</span>
<span class="p_add">+	set_bit();</span>
<span class="p_add">+	clear_bit();</span>
<span class="p_add">+	change_bit();</span>
<span class="p_add">+</span>
<span class="p_add">+이것들을 사용할 때에는 필요하다면 적절한 (예를 들면 smp_mb__before_atomic()</span>
<span class="p_add">+같은) 메모리 배리어가 명시적으로 함께 사용되어야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+아래의 것들도 메모리 배리어를 내포하지 _않기_ 때문에, 일부 환경에서는 (예를</span>
<span class="p_add">+들면 smp_mb__before_atomic() 과 같은) 명시적인 메모리 배리어 사용이 필요합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+	atomic_add();</span>
<span class="p_add">+	atomic_sub();</span>
<span class="p_add">+	atomic_inc();</span>
<span class="p_add">+	atomic_dec();</span>
<span class="p_add">+</span>
<span class="p_add">+이것들이 통계 생성을 위해 사용된다면, 그리고 통계 데이터 사이에 관계가 존재하지</span>
<span class="p_add">+않는다면 메모리 배리어는 필요치 않을 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+객체의 수명을 관리하기 위해 레퍼런스 카운팅 목적으로 사용된다면, 레퍼런스</span>
<span class="p_add">+카운터는 락으로 보호되는 섹션에서만 조정되거나 호출하는 쪽이 이미 충분한</span>
<span class="p_add">+레퍼런스를 잡고 있을 것이기 때문에 메모리 배리어는 아마 필요 없을 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+만약 어떤 락을 구성하기 위해 사용된다면, 락 관련 동작은 일반적으로 작업을 특정</span>
<span class="p_add">+순서대로 진행해야 하므로 메모리 배리어가 필요할 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+기본적으로, 각 사용처에서는 메모리 배리어가 필요한지 아닌지 충분히 고려해야</span>
<span class="p_add">+합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+아래의 오퍼레이션들은 특별한 락 관련 동작들입니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	test_and_set_bit_lock();</span>
<span class="p_add">+	clear_bit_unlock();</span>
<span class="p_add">+	__clear_bit_unlock();</span>
<span class="p_add">+</span>
<span class="p_add">+이것들은 ACQUIRE 류와 RELEASE 류의 오퍼레이션들을 구현합니다.  락 관련 도구를</span>
<span class="p_add">+구현할 때에는 이것들을 좀 더 선호하는 편이 나은데, 이것들의 구현은 많은</span>
<span class="p_add">+아키텍쳐에서 최적화 될 수 있기 때문입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+[!] 이런 상황에 사용할 수 있는 특수한 메모리 배리어 도구들이 있습니다만, 일부</span>
<span class="p_add">+CPU 에서는 사용되는 어토믹 인스트럭션 자체에 메모리 배리어가 내포되어 있어서</span>
<span class="p_add">+어토믹 오퍼레이션과 메모리 배리어를 함께 사용하는 게 불필요한 일이 될 수</span>
<span class="p_add">+있는데, 그런 경우에 이 특수 메모리 배리어 도구들은 no-op 이 되어 실질적으로</span>
<span class="p_add">+아무일도 하지 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+더 많은 내용을 위해선 Documentation/atomic_ops.txt 를 참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+디바이스 액세스</span>
<span class="p_add">+---------------</span>
<span class="p_add">+</span>
<span class="p_add">+많은 디바이스가 메모리 매핑 기법으로 제어될 수 있는데, 그렇게 제어되는</span>
<span class="p_add">+디바이스는 CPU 에는 단지 특정 메모리 영역의 집합처럼 보이게 됩니다.  드라이버는</span>
<span class="p_add">+그런 디바이스를 제어하기 위해 정확히 올바른 순서로 올바른 메모리 액세스를</span>
<span class="p_add">+만들어야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, 액세스들을 재배치 하거나 조합하거나 병합하는게 더 효율적이라 판단하는</span>
<span class="p_add">+영리한 CPU 나 컴파일러들을 사용하면 드라이버 코드의 조심스럽게 순서 맞춰진</span>
<span class="p_add">+액세스들이 디바이스에는 요청된 순서대로 도착하지 못하게 할 수 있는 - 디바이스가</span>
<span class="p_add">+오동작을 하게 할 - 잠재적 문제가 생길 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+리눅스 커널 내부에서, I/O 는 어떻게 액세스들을 적절히 순차적이게 만들 수 있는지</span>
<span class="p_add">+알고 있는, - inb() 나 writel() 과 같은 - 적절한 액세스 루틴을 통해 이루어져야만</span>
<span class="p_add">+합니다.  이것들은 대부분의 경우에는 명시적 메모리 배리어 와 함께 사용될 필요가</span>
<span class="p_add">+없습니다만, 다음의 두가지 상황에서는 명시적 메모리 배리어가 필요할 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (1) 일부 시스템에서 I/O 스토어는 모든 CPU 에 일관되게 순서 맞춰지지 않는데,</span>
<span class="p_add">+     따라서 _모든_ 일반적인 드라이버들에 락이 사용되어야만 하고 이 크리티컬</span>
<span class="p_add">+     섹션을 빠져나오기 전에 mmiowb() 가 꼭 호출되어야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (2) 만약 액세스 함수들이 완화된 메모리 액세스 속성을 갖는 I/O 메모리 윈도우를</span>
<span class="p_add">+     사용한다면, 순서를 강제하기 위해선 _mandatory_ 메모리 배리어가 필요합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+더 많은 정보를 위해선 Documentation/DocBook/deviceiobook.tmpl 을 참고하십시오.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+인터럽트</span>
<span class="p_add">+--------</span>
<span class="p_add">+</span>
<span class="p_add">+드라이버는 자신의 인터럽트 서비스 루틴에 의해 인터럽트 당할 수 있기 때문에</span>
<span class="p_add">+드라이버의 이 두 부분은 서로의 디바이스 제어 또는 액세스 부분과 상호 간섭할 수</span>
<span class="p_add">+있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+스스로에게 인터럽트 당하는 걸 불가능하게 하고, 드라이버의 크리티컬한</span>
<span class="p_add">+오퍼레이션들을 모두 인터럽트가 불가능하게 된 영역에 집어넣거나 하는 방법 (락의</span>
<span class="p_add">+한 형태) 으로 이런 상호 간섭을 - 최소한 부분적으로라도 - 줄일 수 있습니다.</span>
<span class="p_add">+드라이버의 인터럽트 루틴이 실행 중인 동안, 해당 드라이버의 코어는 같은 CPU 에서</span>
<span class="p_add">+수행되지 않을 것이며, 현재의 인터럽트가 처리되는 중에는 또다시 인터럽트가</span>
<span class="p_add">+일어나지 못하도록 되어 있으니 인터럽트 핸들러는 그에 대해서는 락을 잡지 않아도</span>
<span class="p_add">+됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, 어드레스 레지스터와 데이터 레지스터를 갖는 이더넷 카드를 다루는</span>
<span class="p_add">+드라이버를 생각해 봅시다.  만약 이 드라이버의 코어가 인터럽트를 비활성화시킨</span>
<span class="p_add">+채로 이더넷 카드와 대화하고 드라이버의 인터럽트 핸들러가 호출되었다면:</span>
<span class="p_add">+</span>
<span class="p_add">+	LOCAL IRQ DISABLE</span>
<span class="p_add">+	writew(ADDR, 3);</span>
<span class="p_add">+	writew(DATA, y);</span>
<span class="p_add">+	LOCAL IRQ ENABLE</span>
<span class="p_add">+	&lt;interrupt&gt;</span>
<span class="p_add">+	writew(ADDR, 4);</span>
<span class="p_add">+	q = readw(DATA);</span>
<span class="p_add">+	&lt;/interrupt&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+만약 순서 규칙이 충분히 완화되어 있다면 데이터 레지스터에의 스토어는 어드레스</span>
<span class="p_add">+레지스터에 두번째로 행해지는 스토어 뒤에 일어날 수도 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	STORE *ADDR = 3, STORE *ADDR = 4, STORE *DATA = y, q = LOAD *DATA</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+만약 순서 규칙이 충분히 완화되어 있고 묵시적으로든 명시적으로든 배리어가</span>
<span class="p_add">+사용되지 않았다면 인터럽트 비활성화 섹션에서 일어난 액세스가 바깥으로 새어서</span>
<span class="p_add">+인터럽트 내에서 일어난 액세스와 섞일 수 있다고 - 그리고 그 반대도 - 가정해야만</span>
<span class="p_add">+합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+그런 영역 안에서 일어나는 I/O 액세스들은 엄격한 순서 규칙의 I/O 레지스터에</span>
<span class="p_add">+묵시적 I/O 배리어를 형성하는 동기적 (synchronous) 로드 오퍼레이션을 포함하기</span>
<span class="p_add">+때문에 일반적으로는 이런게 문제가 되지 않습니다.  만약 이걸로는 충분치 않다면</span>
<span class="p_add">+mmiowb() 가 명시적으로 사용될 필요가 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+하나의 인터럽트 루틴과 별도의 CPU 에서 수행중이며 서로 통신을 하는 두 루틴</span>
<span class="p_add">+사이에도 비슷한 상황이 일어날 수 있습니다.  만약 그런 경우가 발생할 가능성이</span>
<span class="p_add">+있다면, 순서를 보장하기 위해 인터럽트 비활성화 락이 사용되어져야만 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+======================</span>
<span class="p_add">+커널 I/O 배리어의 효과</span>
<span class="p_add">+======================</span>
<span class="p_add">+</span>
<span class="p_add">+I/O 메모리에 액세스할 때, 드라이버는 적절한 액세스 함수를 사용해야 합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) inX(), outX():</span>
<span class="p_add">+</span>
<span class="p_add">+     이것들은 메모리 공간보다는 I/O 공간에 이야기를 하려는 의도로</span>
<span class="p_add">+     만들어졌습니다만, 그건 기본적으로 CPU 마다 다른 컨셉입니다.  i386 과</span>
<span class="p_add">+     x86_64 프로세서들은 특별한 I/O 공간 액세스 사이클과 명령어를 실제로 가지고</span>
<span class="p_add">+     있지만, 다른 많은 CPU 들에는 그런 컨셉이 존재하지 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     다른 것들 중에서도 PCI 버스가 I/O 공간 컨셉을 정의하는데, 이는 - i386 과</span>
<span class="p_add">+     x86_64 같은 CPU 에서 - CPU 의 I/O 공간 컨셉으로 쉽게 매치됩니다.  하지만,</span>
<span class="p_add">+     대체할 I/O 공간이 없는 CPU 에서는 CPU 의 메모리 맵의 가상 I/O 공간으로</span>
<span class="p_add">+     매핑될 수도 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     이 공간으로의 액세스는 (i386 등에서는) 완전하게 동기화 됩니다만, 중간의</span>
<span class="p_add">+     (PCI 호스트 브리지와 같은) 브리지들은 이를 완전히 보장하진 않을수도</span>
<span class="p_add">+     있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     이것들의 상호간의 순서는 완전하게 보장됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     다른 타입의 메모리 오퍼레이션, I/O 오퍼레이션에 대한 순서는 완전하게</span>
<span class="p_add">+     보장되지는 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) readX(), writeX():</span>
<span class="p_add">+</span>
<span class="p_add">+     이것들이 수행 요청되는 CPU 에서 서로에게 완전히 순서가 맞춰지고 독립적으로</span>
<span class="p_add">+     수행되는지에 대한 보장 여부는 이들이 액세스 하는 메모리 윈도우에 정의된</span>
<span class="p_add">+     특성에 의해 결정됩니다.  예를 들어, 최신의 i386 아키텍쳐 머신에서는 MTRR</span>
<span class="p_add">+     레지스터로 이 특성이 조정됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     일반적으로는, 프리페치 (prefetch) 가능한 디바이스를 액세스 하는게</span>
<span class="p_add">+     아니라면, 이것들은 완전히 순서가 맞춰지고 결합되지 않게 보장될 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     하지만, (PCI 브리지와 같은) 중간의 하드웨어는 자신이 원한다면 집행을</span>
<span class="p_add">+     연기시킬 수 있습니다; 스토어 명령을 실제로 하드웨어로 내려보내기(flush)</span>
<span class="p_add">+     위해서는 같은 위치로부터 로드를 하는 방법이 있습니다만[*], PCI 의 경우는</span>
<span class="p_add">+     같은 디바이스나 환경 구성 영역에서의 로드만으로도 충분할 겁니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     [*] 주의! 쓰여진 것과 같은 위치로부터의 로드를 시도하는 것은 오동작을</span>
<span class="p_add">+	 일으킬 수도 있습니다 - 예로 16650 Rx/Tx 시리얼 레지스터를 생각해</span>
<span class="p_add">+	 보세요.</span>
<span class="p_add">+</span>
<span class="p_add">+     프리페치 가능한 I/O 메모리가 사용되면, 스토어 명령들이 순서를 지키도록</span>
<span class="p_add">+     하기 위해 mmiowb() 배리어가 필요할 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+     PCI 트랜잭션 사이의 상호작용에 대해 더 많은 정보를 위해선 PCI 명세서를</span>
<span class="p_add">+     참고하시기 바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) readX_relaxed(), writeX_relaxed()</span>
<span class="p_add">+</span>
<span class="p_add">+     이것들은 readX() 와 writeX() 랑 비슷하지만, 더 완화된 메모리 순서 보장을</span>
<span class="p_add">+     제공합니다.  구체적으로, 이것들은 일반적 메모리 액세스 (예: DMA 버퍼) 에도</span>
<span class="p_add">+     LOCK 이나 UNLOCK 오퍼레이션들에도 순서를 보장하지 않습니다.  LOCK 이나</span>
<span class="p_add">+     UNLOCK 오퍼레이션들에 맞춰지는 순서가 필요하다면, mmiowb() 배리어가 사용될</span>
<span class="p_add">+     수 있습니다.  같은 주변 장치에의 완화된 액세스끼리는 순서가 지켜짐을 알아</span>
<span class="p_add">+     두시기 바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) ioreadX(), iowriteX()</span>
<span class="p_add">+</span>
<span class="p_add">+     이것들은 inX()/outX() 나 readX()/writeX() 처럼 실제로 수행하는 액세스의</span>
<span class="p_add">+     종류에 따라 적절하게 수행될 것입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+===================================</span>
<span class="p_add">+가정되는 가장 완화된 실행 순서 모델</span>
<span class="p_add">+===================================</span>
<span class="p_add">+</span>
<span class="p_add">+컨셉적으로 CPU 는 주어진 프로그램에 대해 프로그램 그 자체에는 인과성 (program</span>
<span class="p_add">+causality) 을 지키는 것처럼 보이게 하지만 일반적으로는 순서를 거의 지켜주지</span>
<span class="p_add">+않는다고 가정되어야만 합니다.  (i386 이나 x86_64 같은) 일부 CPU 들은 코드</span>
<span class="p_add">+재배치에 (powerpc 나 frv 와 같은) 다른 것들에 비해 강한 제약을 갖지만, 아키텍쳐</span>
<span class="p_add">+종속적 코드 이외의 코드에서는 순서에 대한 제약이 가장 완화된 경우 (DEC Alpha)</span>
<span class="p_add">+를 가정해야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+이 말은, CPU 에게 주어지는 인스트럭션 스트림 내의 한 인스트럭션이 앞의</span>
<span class="p_add">+인스트럭션에 종속적이라면 앞의 인스트럭션은 뒤의 종속적 인스트럭션이 실행되기</span>
<span class="p_add">+전에 완료[*]될 수 있어야 한다는 제약 (달리 말해서, 인과성이 지켜지는 것으로</span>
<span class="p_add">+보이게 함) 외에는 자신이 원하는 순서대로 - 심지어 병렬적으로도 - 그 스트림을</span>
<span class="p_add">+실행할 수 있음을 의미합니다</span>
<span class="p_add">+</span>
<span class="p_add">+ [*] 일부 인스트럭션은 하나 이상의 영향 - 조건 코드를 바꾼다던지, 레지스터나</span>
<span class="p_add">+     메모리를 바꾼다던지 - 을 만들어내며, 다른 인스트럭션은 다른 효과에</span>
<span class="p_add">+     종속적일 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 는 최종적으로 아무 효과도 만들지 않는 인스트럭션 시퀀스는 없애버릴 수도</span>
<span class="p_add">+있습니다.  예를 들어, 만약 두개의 연속되는 인스트럭션이 둘 다 같은 레지스터에</span>
<span class="p_add">+직접적인 값 (immediate value) 을 집어넣는다면, 첫번째 인스트럭션은 버려질 수도</span>
<span class="p_add">+있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+비슷하게, 컴파일러 역시 프로그램의 인과성만 지켜준다면 인스트럭션 스트림을</span>
<span class="p_add">+자신이 보기에 올바르다 생각되는대로 재배치 할 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+===============</span>
<span class="p_add">+CPU 캐시의 영향</span>
<span class="p_add">+===============</span>
<span class="p_add">+</span>
<span class="p_add">+캐시된 메모리 오퍼레이션들이 시스템 전체에 어떻게 인지되는지는 CPU 와 메모리</span>
<span class="p_add">+사이에 존재하는 캐시들, 그리고 시스템 상태의 일관성을 관리하는 메모리 일관성</span>
<span class="p_add">+시스템에 상당 부분 영향을 받습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+한 CPU 가 시스템의 다른 부분들과 캐시를 통해 상호작용한다면, 메모리 시스템은</span>
<span class="p_add">+CPU 의 캐시들을 포함해야 하며, CPU 와 CPU 자신의 캐시 사이에서의 동작을 위한</span>
<span class="p_add">+메모리 배리어를 가져야 합니다. (메모리 배리어는 논리적으로는 다음 그림의</span>
<span class="p_add">+점선에서 동작합니다):</span>
<span class="p_add">+</span>
<span class="p_add">+	    &lt;--- CPU ---&gt;         :       &lt;----------- Memory -----------&gt;</span>
<span class="p_add">+	                          :</span>
<span class="p_add">+	+--------+    +--------+  :   +--------+    +-----------+</span>
<span class="p_add">+	|        |    |        |  :   |        |    |           |    +--------+</span>
<span class="p_add">+	|  CPU   |    | Memory |  :   | CPU    |    |           |    |        |</span>
<span class="p_add">+	|  Core  |---&gt;| Access |-----&gt;| Cache  |&lt;--&gt;|           |    |        |</span>
<span class="p_add">+	|        |    | Queue  |  :   |        |    |           |---&gt;| Memory |</span>
<span class="p_add">+	|        |    |        |  :   |        |    |           |    |        |</span>
<span class="p_add">+	+--------+    +--------+  :   +--------+    |           |    |        |</span>
<span class="p_add">+	                          :                 | Cache     |    +--------+</span>
<span class="p_add">+	                          :                 | Coherency |</span>
<span class="p_add">+	                          :                 | Mechanism |    +--------+</span>
<span class="p_add">+	+--------+    +--------+  :   +--------+    |           |    |	      |</span>
<span class="p_add">+	|        |    |        |  :   |        |    |           |    |        |</span>
<span class="p_add">+	|  CPU   |    | Memory |  :   | CPU    |    |           |---&gt;| Device |</span>
<span class="p_add">+	|  Core  |---&gt;| Access |-----&gt;| Cache  |&lt;--&gt;|           |    |        |</span>
<span class="p_add">+	|        |    | Queue  |  :   |        |    |           |    |        |</span>
<span class="p_add">+	|        |    |        |  :   |        |    |           |    +--------+</span>
<span class="p_add">+	+--------+    +--------+  :   +--------+    +-----------+</span>
<span class="p_add">+	                          :</span>
<span class="p_add">+	                          :</span>
<span class="p_add">+</span>
<span class="p_add">+특정 로드나 스토어는 해당 오퍼레이션을 요청한 CPU 의 캐시 내에서 동작을 완료할</span>
<span class="p_add">+수도 있기 때문에 해당 CPU 의 바깥에는 보이지 않을 수 있지만, 다른 CPU 가 관심을</span>
<span class="p_add">+갖는다면 캐시 일관성 메커니즘이 해당 캐시라인을 해당 CPU 에게 전달하고, 해당</span>
<span class="p_add">+메모리 영역에 대한 오퍼레이션이 발생할 때마다 그 영향을 전파시키기 때문에, 해당</span>
<span class="p_add">+오퍼레이션은 메모리에 실제로 액세스를 한것처럼 나타날 것입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 코어는 프로그램의 인과성이 유지된다고만 여겨진다면 인스트럭션들을 어떤</span>
<span class="p_add">+순서로든 재배치해서 수행할 수 있습니다.  일부 인스트럭션들은 로드나 스토어</span>
<span class="p_add">+오퍼레이션을 만드는데 이 오퍼레이션들은 이후 수행될 메모리 액세스 큐에 들어가게</span>
<span class="p_add">+됩니다.  코어는 이 오퍼레이션들을 해당 큐에 어떤 순서로든 원하는대로 넣을 수</span>
<span class="p_add">+있고, 다른 인스트럭션의 완료를 기다리도록 강제되기 전까지는 수행을 계속합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+메모리 배리어가 하는 일은 CPU 쪽에서 메모리 쪽으로 넘어가는 액세스들의 순서,</span>
<span class="p_add">+그리고 그 액세스의 결과가 시스템의 다른 관찰자들에게 인지되는 순서를 제어하는</span>
<span class="p_add">+것입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+[!] CPU 들은 항상 그들 자신의 로드와 스토어는 프로그램 순서대로 일어난 것으로</span>
<span class="p_add">+보기 때문에, 주어진 CPU 내에서는 메모리 배리어를 사용할 필요가 _없습니다_.</span>
<span class="p_add">+</span>
<span class="p_add">+[!] MMIO 나 다른 디바이스 액세스들은 캐시 시스템을 우회할 수도 있습니다.  우회</span>
<span class="p_add">+여부는 디바이스가 액세스 되는 메모리 윈도우의 특성에 의해 결정될 수도 있고, CPU</span>
<span class="p_add">+가 가지고 있을 수 있는 특수한 디바이스 통신 인스트럭션의 사용에 의해서 결정될</span>
<span class="p_add">+수도 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+캐시 일관성</span>
<span class="p_add">+-----------</span>
<span class="p_add">+</span>
<span class="p_add">+하지만 삶은 앞에서 이야기한 것처럼 단순하지 않습니다: 캐시들은 일관적일 것으로</span>
<span class="p_add">+기대되지만, 그 일관성이 순서에도 적용될 거라는 보장은 없습니다.  한 CPU 에서</span>
<span class="p_add">+만들어진 변경 사항은 최종적으로는 시스템의 모든 CPU 에게 보여지게 되지만, 다른</span>
<span class="p_add">+CPU 들에게도 같은 순서로 보이게 될 거라는 보장은 없다는 뜻입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+두개의 CPU (1 &amp; 2) 가 달려 있고, 각 CPU 에 두개의 데이터 캐시(CPU 1 은 A/B 를,</span>
<span class="p_add">+CPU 2 는 C/D 를 갖습니다)가 병렬로 연결되어 있는 시스템을 다룬다고 생각해</span>
<span class="p_add">+봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	            :</span>
<span class="p_add">+	            :                          +--------+</span>
<span class="p_add">+	            :      +---------+         |        |</span>
<span class="p_add">+	+--------+  : +---&gt;| Cache A |&lt;-------&gt;|        |</span>
<span class="p_add">+	|        |  : |    +---------+         |        |</span>
<span class="p_add">+	|  CPU 1 |&lt;---+                        |        |</span>
<span class="p_add">+	|        |  : |    +---------+         |        |</span>
<span class="p_add">+	+--------+  : +---&gt;| Cache B |&lt;-------&gt;|        |</span>
<span class="p_add">+	            :      +---------+         |        |</span>
<span class="p_add">+	            :                          | Memory |</span>
<span class="p_add">+	            :      +---------+         | System |</span>
<span class="p_add">+	+--------+  : +---&gt;| Cache C |&lt;-------&gt;|        |</span>
<span class="p_add">+	|        |  : |    +---------+         |        |</span>
<span class="p_add">+	|  CPU 2 |&lt;---+                        |        |</span>
<span class="p_add">+	|        |  : |    +---------+         |        |</span>
<span class="p_add">+	+--------+  : +---&gt;| Cache D |&lt;-------&gt;|        |</span>
<span class="p_add">+	            :      +---------+         |        |</span>
<span class="p_add">+	            :                          +--------+</span>
<span class="p_add">+	            :</span>
<span class="p_add">+</span>
<span class="p_add">+이 시스템이 다음과 같은 특성을 갖는다 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 홀수번 캐시라인은 캐시 A, 캐시 C 또는 메모리에 위치할 수 있음;</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 짝수번 캐시라인은 캐시 B, 캐시 D 또는 메모리에 위치할 수 있음;</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) CPU 코어가 한개의 캐시에 접근하는 동안, 다른 캐시는 - 더티 캐시라인을</span>
<span class="p_add">+     메모리에 내리거나 추측성 로드를 하거나 하기 위해 - 시스템의 다른 부분에</span>
<span class="p_add">+     액세스 하기 위해 버스를 사용할 수 있음;</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 각 캐시는 시스템의 나머지 부분들과 일관성을 맞추기 위해 해당 캐시에</span>
<span class="p_add">+     적용되어야 할 오퍼레이션들의 큐를 가짐;</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 이 일관성 큐는 캐시에 이미 존재하는 라인에 가해지는 평범한 로드에 의해서는</span>
<span class="p_add">+     비워지지 않는데, 큐의 오퍼레이션들이 이 로드의 결과에 영향을 끼칠 수 있다</span>
<span class="p_add">+     할지라도 그러함.</span>
<span class="p_add">+</span>
<span class="p_add">+이제, 첫번째 CPU 에서 두개의 쓰기 오퍼레이션을 만드는데, 해당 CPU 의 캐시에</span>
<span class="p_add">+요청된 순서로 오퍼레이션이 도달됨을 보장하기 위해 두 오퍼레이션 사이에 쓰기</span>
<span class="p_add">+배리어를 사용하는 상황을 상상해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		CPU 2		COMMENT</span>
<span class="p_add">+	===============	===============	=======================================</span>
<span class="p_add">+					u == 0, v == 1 and p == &amp;u, q == &amp;u</span>
<span class="p_add">+	v = 2;</span>
<span class="p_add">+	smp_wmb();			v 의 변경이 p 의 변경 전에 보일 것을</span>
<span class="p_add">+					 분명히 함</span>
<span class="p_add">+	&lt;A:modify v=2&gt;			v 는 이제 캐시 A 에 독점적으로 존재함</span>
<span class="p_add">+	p = &amp;v;</span>
<span class="p_add">+	&lt;B:modify p=&amp;v&gt;			p 는 이제 캐시 B 에 독점적으로 존재함</span>
<span class="p_add">+</span>
<span class="p_add">+여기서의 쓰기 메모리 배리어는 CPU 1 의 캐시가 올바른 순서로 업데이트 된 것으로</span>
<span class="p_add">+시스템의 다른 CPU 들이 인지하게 만듭니다.  하지만, 이제 두번째 CPU 가 그 값들을</span>
<span class="p_add">+읽으려 하는 상황을 생각해 봅시다:</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		CPU 2		COMMENT</span>
<span class="p_add">+	===============	===============	=======================================</span>
<span class="p_add">+	...</span>
<span class="p_add">+			q = p;</span>
<span class="p_add">+			x = *q;</span>
<span class="p_add">+</span>
<span class="p_add">+위의 두개의 읽기 오퍼레이션은 예상된 순서로 일어나지 못할 수 있는데, 두번째 CPU</span>
<span class="p_add">+의 한 캐시에 다른 캐시 이벤트가 발생해 v 를 담고 있는 캐시라인의 해당 캐시에의</span>
<span class="p_add">+업데이트가 지연되는 사이, p 를 담고 있는 캐시라인은 두번째 CPU 의 다른 캐시에</span>
<span class="p_add">+업데이트 되어버렸을 수 있기 때문입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		CPU 2		COMMENT</span>
<span class="p_add">+	===============	===============	=======================================</span>
<span class="p_add">+					u == 0, v == 1 and p == &amp;u, q == &amp;u</span>
<span class="p_add">+	v = 2;</span>
<span class="p_add">+	smp_wmb();</span>
<span class="p_add">+	&lt;A:modify v=2&gt;	&lt;C:busy&gt;</span>
<span class="p_add">+			&lt;C:queue v=2&gt;</span>
<span class="p_add">+	p = &amp;v;		q = p;</span>
<span class="p_add">+			&lt;D:request p&gt;</span>
<span class="p_add">+	&lt;B:modify p=&amp;v&gt;	&lt;D:commit p=&amp;v&gt;</span>
<span class="p_add">+			&lt;D:read p&gt;</span>
<span class="p_add">+			x = *q;</span>
<span class="p_add">+			&lt;C:read *q&gt;	캐시에 업데이트 되기 전의 v 를 읽음</span>
<span class="p_add">+			&lt;C:unbusy&gt;</span>
<span class="p_add">+			&lt;C:commit v=2&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+기본적으로, 두개의 캐시라인 모두 CPU 2 에 최종적으로는 업데이트 될 것이지만,</span>
<span class="p_add">+별도의 개입 없이는, 업데이트의 순서가 CPU 1 에서 만들어진 순서와 동일할</span>
<span class="p_add">+것이라는 보장이 없습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+여기에 개입하기 위해선, 데이터 의존성 배리어나 읽기 배리어를 로드 오퍼레이션들</span>
<span class="p_add">+사이에 넣어야 합니다.  이렇게 함으로써 캐시가 다음 요청을 처리하기 전에 일관성</span>
<span class="p_add">+큐를 처리하도록 강제하게 됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+	CPU 1		CPU 2		COMMENT</span>
<span class="p_add">+	===============	===============	=======================================</span>
<span class="p_add">+					u == 0, v == 1 and p == &amp;u, q == &amp;u</span>
<span class="p_add">+	v = 2;</span>
<span class="p_add">+	smp_wmb();</span>
<span class="p_add">+	&lt;A:modify v=2&gt;	&lt;C:busy&gt;</span>
<span class="p_add">+			&lt;C:queue v=2&gt;</span>
<span class="p_add">+	p = &amp;v;		q = p;</span>
<span class="p_add">+			&lt;D:request p&gt;</span>
<span class="p_add">+	&lt;B:modify p=&amp;v&gt;	&lt;D:commit p=&amp;v&gt;</span>
<span class="p_add">+			&lt;D:read p&gt;</span>
<span class="p_add">+			smp_read_barrier_depends()</span>
<span class="p_add">+			&lt;C:unbusy&gt;</span>
<span class="p_add">+			&lt;C:commit v=2&gt;</span>
<span class="p_add">+			x = *q;</span>
<span class="p_add">+			&lt;C:read *q&gt;	캐시에 업데이트 된 v 를 읽음</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+이런 부류의 문제는 DEC Alpha 계열 프로세서들에서 발견될 수 있는데, 이들은</span>
<span class="p_add">+데이터 버스를 좀 더 잘 사용해 성능을 개선할 수 있는, 분할된 캐시를 가지고 있기</span>
<span class="p_add">+때문입니다.  대부분의 CPU 는 하나의 읽기 오퍼레이션의 메모리 액세스가 다른 읽기</span>
<span class="p_add">+오퍼레이션에 의존적이라면 데이터 의존성 배리어를 내포시킵니다만, 모두가 그런건</span>
<span class="p_add">+아니기 때문에 이점에 의존해선 안됩니다.</span>
<span class="p_add">+</span>
<span class="p_add">+다른 CPU 들도 분할된 캐시를 가지고 있을 수 있지만, 그런 CPU 들은 평범한 메모리</span>
<span class="p_add">+액세스를 위해서도 이 분할된 캐시들 사이의 조정을 해야만 합니다.  Alpha 는 가장</span>
<span class="p_add">+약한 메모리 순서 시맨틱 (semantic) 을 선택함으로써 메모리 배리어가 명시적으로</span>
<span class="p_add">+사용되지 않았을 때에는 그런 조정이 필요하지 않게 했습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+캐시 일관성 VS DMA</span>
<span class="p_add">+------------------</span>
<span class="p_add">+</span>
<span class="p_add">+모든 시스템이 DMA 를 하는 디바이스에 대해서까지 캐시 일관성을 유지하지는</span>
<span class="p_add">+않습니다.  그런 경우, DMA 를 시도하는 디바이스는 RAM 으로부터 잘못된 데이터를</span>
<span class="p_add">+읽을 수 있는데, 더티 캐시 라인이 CPU 의 캐시에 머무르고 있고, 바뀐 값이 아직</span>
<span class="p_add">+RAM 에 써지지 않았을 수 있기 때문입니다.  이 문제를 해결하기 위해선, 커널의</span>
<span class="p_add">+적절한 부분에서 각 CPU 캐시의 문제되는 비트들을 플러시 (flush) 시켜야만 합니다</span>
<span class="p_add">+(그리고 그것들을 무효화 - invalidation - 시킬 수도 있겠죠).</span>
<span class="p_add">+</span>
<span class="p_add">+또한, 디바이스에 의해 RAM 에 DMA 로 쓰여진 값은 디바이스가 쓰기를 완료한 후에</span>
<span class="p_add">+CPU 의 캐시에서 RAM 으로 쓰여지는 더티 캐시 라인에 의해 덮어써질 수도 있고, CPU</span>
<span class="p_add">+의 캐시에 존재하는 캐시 라인이 해당 캐시에서 삭제되고 다시 값을 읽어들이기</span>
<span class="p_add">+전까지는 RAM 이 업데이트 되었다는 사실 자체가 숨겨져 버릴 수도 있습니다.  이</span>
<span class="p_add">+문제를 해결하기 위해선, 커널의 적절한 부분에서 각 CPU 의 캐시 안의 문제가 되는</span>
<span class="p_add">+비트들을 무효화 시켜야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+캐시 관리에 대한 더 많은 정보를 위해선 Documentation/cachetlb.txt 를</span>
<span class="p_add">+참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+캐시 일관성 VS MMIO</span>
<span class="p_add">+-------------------</span>
<span class="p_add">+</span>
<span class="p_add">+Memory mapped I/O 는 일반적으로 CPU 의 메모리 공간 내의 한 윈도우의 특정 부분</span>
<span class="p_add">+내의 메모리 지역에 이루어지는데, 이 윈도우는 일반적인, RAM 으로 향하는</span>
<span class="p_add">+윈도우와는 다른 특성을 갖습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+그런 특성 가운데 하나는, 일반적으로 그런 액세스는 캐시를 완전히 우회하고</span>
<span class="p_add">+디바이스 버스로 곧바로 향한다는 것입니다.  이 말은 MMIO 액세스는 먼저</span>
<span class="p_add">+시작되어서 캐시에서 완료된 메모리 액세스를 추월할 수 있다는 뜻입니다.  이런</span>
<span class="p_add">+경우엔 메모리 배리어만으로는 충분치 않고, 만약 캐시된 메모리 쓰기 오퍼레이션과</span>
<span class="p_add">+MMIO 액세스가 어떤 방식으로든 의존적이라면 해당 캐시는 두 오퍼레이션 사이에</span>
<span class="p_add">+비워져(flush)야만 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+======================</span>
<span class="p_add">+CPU 들이 저지르는 일들</span>
<span class="p_add">+======================</span>
<span class="p_add">+</span>
<span class="p_add">+프로그래머는 CPU 가 메모리 오퍼레이션들을 정확히 요청한대로 수행해 줄 것이라고</span>
<span class="p_add">+생각하는데, 예를 들어 다음과 같은 코드를 CPU 에게 넘긴다면:</span>
<span class="p_add">+</span>
<span class="p_add">+	a = READ_ONCE(*A);</span>
<span class="p_add">+	WRITE_ONCE(*B, b);</span>
<span class="p_add">+	c = READ_ONCE(*C);</span>
<span class="p_add">+	d = READ_ONCE(*D);</span>
<span class="p_add">+	WRITE_ONCE(*E, e);</span>
<span class="p_add">+</span>
<span class="p_add">+CPU 는 다음 인스트럭션을 처리하기 전에 현재의 인스트럭션을 위한 메모리</span>
<span class="p_add">+오퍼레이션을 완료할 것이라 생각하고, 따라서 시스템 외부에서 관찰하기에도 정해진</span>
<span class="p_add">+순서대로 오퍼레이션이 수행될 것으로 예상합니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	LOAD *A, STORE *B, LOAD *C, LOAD *D, STORE *E.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+당연하지만, 실제로는 훨씬 엉망입니다.  많은 CPU 와 컴파일러에서 앞의 가정은</span>
<span class="p_add">+성립하지 못하는데 그 이유는 다음과 같습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 로드 오퍼레이션들은 실행을 계속 해나가기 위해 곧바로 완료될 필요가 있는</span>
<span class="p_add">+     경우가 많은 반면, 스토어 오퍼레이션들은 종종 별다른 문제 없이 유예될 수</span>
<span class="p_add">+     있습니다;</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 로드 오퍼레이션들은 예측적으로 수행될 수 있으며, 필요없는 로드였다고</span>
<span class="p_add">+     증명된 예측적 로드의 결과는 버려집니다;</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 로드 오퍼레이션들은 예측적으로 수행될 수 있으므로, 예상된 이벤트의</span>
<span class="p_add">+     시퀀스와 다른 시간에 로드가 이뤄질 수 있습니다;</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 메모리 액세스 순서는 CPU 버스와 캐시를 좀 더 잘 사용할 수 있도록 재배치</span>
<span class="p_add">+     될 수 있습니다;</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 로드와 스토어는 인접한 위치에의 액세스들을 일괄적으로 처리할 수 있는</span>
<span class="p_add">+     메모리나 I/O 하드웨어 (메모리와 PCI 디바이스 둘 다 이게 가능할 수</span>
<span class="p_add">+     있습니다) 에 대해 요청되는 경우, 개별 오퍼레이션을 위한 트랜잭션 설정</span>
<span class="p_add">+     비용을 아끼기 위해 조합되어 실행될 수 있습니다; 그리고</span>
<span class="p_add">+</span>
<span class="p_add">+ (*) 해당 CPU 의 데이터 캐시가 순서에 영향을 끼칠 수도 있고, 캐시 일관성</span>
<span class="p_add">+     메커니즘이 - 스토어가 실제로 캐시에 도달한다면 - 이 문제를 완화시킬 수는</span>
<span class="p_add">+     있지만 이 일관성 관리가 다른 CPU 들에도 같은 순서로 전달된다는 보장은</span>
<span class="p_add">+     없습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+따라서, 앞의 코드에 대해 다른 CPU 가 보는 결과는 다음과 같을 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	LOAD *A, ..., LOAD {*C,*D}, STORE *E, STORE *B</span>
<span class="p_add">+</span>
<span class="p_add">+	(&quot;LOAD {*C,*D}&quot; 는 조합된 로드입니다)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, CPU 는 스스로는 일관적일 것을 보장합니다: CPU _자신_ 의 액세스들은</span>
<span class="p_add">+자신에게는 메모리 배리어가 없음에도 불구하고 정확히 순서 세워진 것으로 보여질</span>
<span class="p_add">+것입니다.  예를 들어 다음의 코드가 주어졌다면:</span>
<span class="p_add">+</span>
<span class="p_add">+	U = READ_ONCE(*A);</span>
<span class="p_add">+	WRITE_ONCE(*A, V);</span>
<span class="p_add">+	WRITE_ONCE(*A, W);</span>
<span class="p_add">+	X = READ_ONCE(*A);</span>
<span class="p_add">+	WRITE_ONCE(*A, Y);</span>
<span class="p_add">+	Z = READ_ONCE(*A);</span>
<span class="p_add">+</span>
<span class="p_add">+그리고 외부의 영향에 의한 간섭이 없다고 가정하면, 최종 결과는 다음과 같이</span>
<span class="p_add">+나타날 것이라고 예상될 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	U == *A 의 최초 값</span>
<span class="p_add">+	X == W</span>
<span class="p_add">+	Z == Y</span>
<span class="p_add">+	*A == Y</span>
<span class="p_add">+</span>
<span class="p_add">+앞의 코드는 CPU 가 다음의 메모리 액세스 시퀀스를 만들도록 할겁니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	U=LOAD *A, STORE *A=V, STORE *A=W, X=LOAD *A, STORE *A=Y, Z=LOAD *A</span>
<span class="p_add">+</span>
<span class="p_add">+하지만, 별다른 개입이 없고 프로그램의 시야에 이 세상이 여전히 일관적이라고</span>
<span class="p_add">+보인다는 보장만 지켜진다면 이 시퀀스는 어떤 조합으로든 재구성될 수 있으며, 각</span>
<span class="p_add">+액세스들은 합쳐지거나 버려질 수 있습니다.  일부 아키텍쳐에서 CPU 는 같은 위치에</span>
<span class="p_add">+대한 연속적인 로드 오퍼레이션들을 재배치 할 수 있기 때문에 앞의 예에서의</span>
<span class="p_add">+READ_ONCE() 와 WRITE_ONCE() 는 반드시 존재해야 함을 알아두세요.  그런 종류의</span>
<span class="p_add">+아키텍쳐에서 READ_ONCE() 와 WRITE_ONCE() 는 이 문제를 막기 위해 필요한 일을</span>
<span class="p_add">+뭐가 됐든지 하게 되는데, 예를 들어 Itanium 에서는 READ_ONCE() 와 WRITE_ONCE()</span>
<span class="p_add">+가 사용하는 volatile 캐스팅은 GCC 가 그런 재배치를 방지하는 특수 인스트럭션인</span>
<span class="p_add">+ld.acq 와 stl.rel 인스트럭션을 각각 만들어 내도록 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+컴파일러 역시 이 시퀀스의 액세스들을 CPU 가 보기도 전에 합치거나 버리거나 뒤로</span>
<span class="p_add">+미뤄버릴 수 있습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+예를 들어:</span>
<span class="p_add">+</span>
<span class="p_add">+	*A = V;</span>
<span class="p_add">+	*A = W;</span>
<span class="p_add">+</span>
<span class="p_add">+는 다음과 같이 변형될 수 있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	*A = W;</span>
<span class="p_add">+</span>
<span class="p_add">+따라서, 쓰기 배리어나 WRITE_ONCE() 가 없다면 *A 로의 V 값의 저장의 효과는</span>
<span class="p_add">+사라진다고 가정될 수 있습니다.  비슷하게:</span>
<span class="p_add">+</span>
<span class="p_add">+	*A = Y;</span>
<span class="p_add">+	Z = *A;</span>
<span class="p_add">+</span>
<span class="p_add">+는, 메모리 배리어나 READ_ONCE() 와 WRITE_ONCE() 없이는 다음과 같이 변형될 수</span>
<span class="p_add">+있습니다:</span>
<span class="p_add">+</span>
<span class="p_add">+	*A = Y;</span>
<span class="p_add">+	Z = Y;</span>
<span class="p_add">+</span>
<span class="p_add">+그리고 이 LOAD 오퍼레이션은 CPU 바깥에는 아예 보이지 않습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+그리고, ALPHA 가 있다</span>
<span class="p_add">+---------------------</span>
<span class="p_add">+</span>
<span class="p_add">+DEC Alpha CPU 는 가장 완화된 메모리 순서의 CPU 중 하나입니다.  뿐만 아니라,</span>
<span class="p_add">+Alpha CPU 의 일부 버전은 분할된 데이터 캐시를 가지고 있어서, 의미적으로</span>
<span class="p_add">+관계되어 있는 두개의 캐시 라인이 서로 다른 시간에 업데이트 되는게 가능합니다.</span>
<span class="p_add">+이게 데이터 의존성 배리어가 정말 필요해지는 부분인데, 데이터 의존성 배리어는</span>
<span class="p_add">+메모리 일관성 시스템과 함께 두개의 캐시를 동기화 시켜서, 포인터 변경과 새로운</span>
<span class="p_add">+데이터의 발견을 올바른 순서로 일어나게 하기 때문입니다.</span>
<span class="p_add">+</span>
<span class="p_add">+리눅스 커널의 메모리 배리어 모델은 Alpha 에 기초해서 정의되었습니다.</span>
<span class="p_add">+</span>
<span class="p_add">+위의 &quot;캐시 일관성&quot; 서브섹션을 참고하세요.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+가상 머신 게스트</span>
<span class="p_add">+----------------</span>
<span class="p_add">+</span>
<span class="p_add">+가상 머신에서 동작하는 게스트들은 게스트 자체는 SMP 지원 없이 컴파일 되었다</span>
<span class="p_add">+해도 SMP 영향을 받을 수 있습니다.  이건 UP 커널을 사용하면서 SMP 호스트와</span>
<span class="p_add">+결부되어 발생하는 부작용입니다.  이 경우에는 mandatory 배리어를 사용해서 문제를</span>
<span class="p_add">+해결할 수 있겠지만 그런 해결은 대부분의 경우 최적의 해결책이 아닙니다.</span>
<span class="p_add">+</span>
<span class="p_add">+이 문제를 완벽하게 해결하기 위해, 로우 레벨의 virt_mb() 등의 매크로를 사용할 수</span>
<span class="p_add">+있습니다. 이것들은 SMP 가 활성화 되어 있다면 smp_mb() 등과 동일한 효과를</span>
<span class="p_add">+갖습니다만, SMP 와 SMP 아닌 시스템 모두에 대해 동일한 코드를 만들어냅니다.</span>
<span class="p_add">+예를 들어, 가상 머신 게스트들은 (SMP 일 수 있는) 호스트와 동기화를 할 때에는</span>
<span class="p_add">+smp_mb() 가 아니라 virt_mb() 를 사용해야 합니다.</span>
<span class="p_add">+</span>
<span class="p_add">+이것들은 smp_mb() 류의 것들과 모든 부분에서 동일하며, 특히, MMIO 의 영향에</span>
<span class="p_add">+대해서는 간여하지 않습니다: MMIO 의 영향을 제어하려면, mandatory 배리어를</span>
<span class="p_add">+사용하시기 바랍니다.</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+=======</span>
<span class="p_add">+사용 예</span>
<span class="p_add">+=======</span>
<span class="p_add">+</span>
<span class="p_add">+순환식 버퍼</span>
<span class="p_add">+-----------</span>
<span class="p_add">+</span>
<span class="p_add">+메모리 배리어는 순환식 버퍼를 생성자(producer)와 소비자(consumer) 사이의</span>
<span class="p_add">+동기화에 락을 사용하지 않고 구현하는데에 사용될 수 있습니다.  더 자세한 내용을</span>
<span class="p_add">+위해선 다음을 참고하세요:</span>
<span class="p_add">+</span>
<span class="p_add">+	Documentation/circular-buffers.txt</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+=========</span>
<span class="p_add">+참고 문헌</span>
<span class="p_add">+=========</span>
<span class="p_add">+</span>
<span class="p_add">+Alpha AXP Architecture Reference Manual, Second Edition (Sites &amp; Witek,</span>
<span class="p_add">+Digital Press)</span>
<span class="p_add">+	Chapter 5.2: Physical Address Space Characteristics</span>
<span class="p_add">+	Chapter 5.4: Caches and Write Buffers</span>
<span class="p_add">+	Chapter 5.5: Data Sharing</span>
<span class="p_add">+	Chapter 5.6: Read/Write Ordering</span>
<span class="p_add">+</span>
<span class="p_add">+AMD64 Architecture Programmer&#39;s Manual Volume 2: System Programming</span>
<span class="p_add">+	Chapter 7.1: Memory-Access Ordering</span>
<span class="p_add">+	Chapter 7.4: Buffering and Combining Memory Writes</span>
<span class="p_add">+</span>
<span class="p_add">+IA-32 Intel Architecture Software Developer&#39;s Manual, Volume 3:</span>
<span class="p_add">+System Programming Guide</span>
<span class="p_add">+	Chapter 7.1: Locked Atomic Operations</span>
<span class="p_add">+	Chapter 7.2: Memory Ordering</span>
<span class="p_add">+	Chapter 7.4: Serializing Instructions</span>
<span class="p_add">+</span>
<span class="p_add">+The SPARC Architecture Manual, Version 9</span>
<span class="p_add">+	Chapter 8: Memory Models</span>
<span class="p_add">+	Appendix D: Formal Specification of the Memory Models</span>
<span class="p_add">+	Appendix J: Programming with the Memory Models</span>
<span class="p_add">+</span>
<span class="p_add">+UltraSPARC Programmer Reference Manual</span>
<span class="p_add">+	Chapter 5: Memory Accesses and Cacheability</span>
<span class="p_add">+	Chapter 15: Sparc-V9 Memory Models</span>
<span class="p_add">+</span>
<span class="p_add">+UltraSPARC III Cu User&#39;s Manual</span>
<span class="p_add">+	Chapter 9: Memory Models</span>
<span class="p_add">+</span>
<span class="p_add">+UltraSPARC IIIi Processor User&#39;s Manual</span>
<span class="p_add">+	Chapter 8: Memory Models</span>
<span class="p_add">+</span>
<span class="p_add">+UltraSPARC Architecture 2005</span>
<span class="p_add">+	Chapter 9: Memory</span>
<span class="p_add">+	Appendix D: Formal Specifications of the Memory Models</span>
<span class="p_add">+</span>
<span class="p_add">+UltraSPARC T1 Supplement to the UltraSPARC Architecture 2005</span>
<span class="p_add">+	Chapter 8: Memory Models</span>
<span class="p_add">+	Appendix F: Caches and Cache Coherency</span>
<span class="p_add">+</span>
<span class="p_add">+Solaris Internals, Core Kernel Architecture, p63-68:</span>
<span class="p_add">+	Chapter 3.3: Hardware Considerations for Locks and</span>
<span class="p_add">+			Synchronization</span>
<span class="p_add">+</span>
<span class="p_add">+Unix Systems for Modern Architectures, Symmetric Multiprocessing and Caching</span>
<span class="p_add">+for Kernel Programmers:</span>
<span class="p_add">+	Chapter 13: Other Memory Models</span>
<span class="p_add">+</span>
<span class="p_add">+Intel Itanium Architecture Software Developer&#39;s Manual: Volume 1:</span>
<span class="p_add">+	Section 2.6: Speculation</span>
<span class="p_add">+	Section 4.4: Memory Access</span>
<span class="p_header">diff --git a/Documentation/locking/lglock.txt b/Documentation/locking/lglock.txt</span>
deleted file mode 100644
<span class="p_header">index a6971e34fabe..000000000000</span>
<span class="p_header">--- a/Documentation/locking/lglock.txt</span>
<span class="p_header">+++ /dev/null</span>
<span class="p_chunk">@@ -1,166 +0,0 @@</span> <span class="p_context"></span>
<span class="p_del">-lglock - local/global locks for mostly local access patterns</span>
<span class="p_del">-------------------------------------------------------------</span>
<span class="p_del">-</span>
<span class="p_del">-Origin: Nick Piggin&#39;s VFS scalability series introduced during</span>
<span class="p_del">-	2.6.35++ [1] [2]</span>
<span class="p_del">-Location: kernel/locking/lglock.c</span>
<span class="p_del">-	include/linux/lglock.h</span>
<span class="p_del">-Users: currently only the VFS and stop_machine related code</span>
<span class="p_del">-</span>
<span class="p_del">-Design Goal:</span>
<span class="p_del">-------------</span>
<span class="p_del">-</span>
<span class="p_del">-Improve scalability of globally used large data sets that are</span>
<span class="p_del">-distributed over all CPUs as per_cpu elements.</span>
<span class="p_del">-</span>
<span class="p_del">-To manage global data structures that are partitioned over all CPUs</span>
<span class="p_del">-as per_cpu elements but can be mostly handled by CPU local actions</span>
<span class="p_del">-lglock will be used where the majority of accesses are cpu local</span>
<span class="p_del">-reading and occasional cpu local writing with very infrequent</span>
<span class="p_del">-global write access.</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-* deal with things locally whenever possible</span>
<span class="p_del">-	- very fast access to the local per_cpu data</span>
<span class="p_del">-	- reasonably fast access to specific per_cpu data on a different</span>
<span class="p_del">-	  CPU</span>
<span class="p_del">-* while making global action possible when needed</span>
<span class="p_del">-	- by expensive access to all CPUs locks - effectively</span>
<span class="p_del">-	  resulting in a globally visible critical section.</span>
<span class="p_del">-</span>
<span class="p_del">-Design:</span>
<span class="p_del">--------</span>
<span class="p_del">-</span>
<span class="p_del">-Basically it is an array of per_cpu spinlocks with the</span>
<span class="p_del">-lg_local_lock/unlock accessing the local CPUs lock object and the</span>
<span class="p_del">-lg_local_lock_cpu/unlock_cpu accessing a remote CPUs lock object</span>
<span class="p_del">-the lg_local_lock has to disable preemption as migration protection so</span>
<span class="p_del">-that the reference to the local CPUs lock does not go out of scope.</span>
<span class="p_del">-Due to the lg_local_lock/unlock only touching cpu-local resources it</span>
<span class="p_del">-is fast. Taking the local lock on a different CPU will be more</span>
<span class="p_del">-expensive but still relatively cheap.</span>
<span class="p_del">-</span>
<span class="p_del">-One can relax the migration constraints by acquiring the current</span>
<span class="p_del">-CPUs lock with lg_local_lock_cpu, remember the cpu, and release that</span>
<span class="p_del">-lock at the end of the critical section even if migrated. This should</span>
<span class="p_del">-give most of the performance benefits without inhibiting migration</span>
<span class="p_del">-though needs careful considerations for nesting of lglocks and</span>
<span class="p_del">-consideration of deadlocks with lg_global_lock.</span>
<span class="p_del">-</span>
<span class="p_del">-The lg_global_lock/unlock locks all underlying spinlocks of all</span>
<span class="p_del">-possible CPUs (including those off-line). The preemption disable/enable</span>
<span class="p_del">-are needed in the non-RT kernels to prevent deadlocks like:</span>
<span class="p_del">-</span>
<span class="p_del">-                     on cpu 1</span>
<span class="p_del">-</span>
<span class="p_del">-              task A          task B</span>
<span class="p_del">-         lg_global_lock</span>
<span class="p_del">-           got cpu 0 lock</span>
<span class="p_del">-                 &lt;&lt;&lt;&lt; preempt &lt;&lt;&lt;&lt;</span>
<span class="p_del">-                         lg_local_lock_cpu for cpu 0</span>
<span class="p_del">-                           spin on cpu 0 lock</span>
<span class="p_del">-</span>
<span class="p_del">-On -RT this deadlock scenario is resolved by the arch_spin_locks in the</span>
<span class="p_del">-lglocks being replaced by rt_mutexes which resolve the above deadlock</span>
<span class="p_del">-by boosting the lock-holder.</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-Implementation:</span>
<span class="p_del">----------------</span>
<span class="p_del">-</span>
<span class="p_del">-The initial lglock implementation from Nick Piggin used some complex</span>
<span class="p_del">-macros to generate the lglock/brlock in lglock.h - they were later</span>
<span class="p_del">-turned into a set of functions by Andi Kleen [7]. The change to functions</span>
<span class="p_del">-was motivated by the presence of multiple lock users and also by them</span>
<span class="p_del">-being easier to maintain than the generating macros. This change to</span>
<span class="p_del">-functions is also the basis to eliminated the restriction of not</span>
<span class="p_del">-being initializeable in kernel modules (the remaining problem is that</span>
<span class="p_del">-locks are not explicitly initialized - see lockdep-design.txt)</span>
<span class="p_del">-</span>
<span class="p_del">-Declaration and initialization:</span>
<span class="p_del">--------------------------------</span>
<span class="p_del">-</span>
<span class="p_del">-  #include &lt;linux/lglock.h&gt;</span>
<span class="p_del">-</span>
<span class="p_del">-  DEFINE_LGLOCK(name)</span>
<span class="p_del">-  or:</span>
<span class="p_del">-  DEFINE_STATIC_LGLOCK(name);</span>
<span class="p_del">-</span>
<span class="p_del">-  lg_lock_init(&amp;name, &quot;lockdep_name_string&quot;);</span>
<span class="p_del">-</span>
<span class="p_del">-  on UP this is mapped to DEFINE_SPINLOCK(name) in both cases, note</span>
<span class="p_del">-  also that as of 3.18-rc6 all declaration in use are of the _STATIC_</span>
<span class="p_del">-  variant (and it seems that the non-static was never in use).</span>
<span class="p_del">-  lg_lock_init is initializing the lockdep map only.</span>
<span class="p_del">-</span>
<span class="p_del">-Usage:</span>
<span class="p_del">-------</span>
<span class="p_del">-</span>
<span class="p_del">-From the locking semantics it is a spinlock. It could be called a</span>
<span class="p_del">-locality aware spinlock. lg_local_* behaves like a per_cpu</span>
<span class="p_del">-spinlock and lg_global_* like a global spinlock.</span>
<span class="p_del">-No surprises in the API.</span>
<span class="p_del">-</span>
<span class="p_del">-  lg_local_lock(*lglock);</span>
<span class="p_del">-     access to protected per_cpu object on this CPU</span>
<span class="p_del">-  lg_local_unlock(*lglock);</span>
<span class="p_del">-</span>
<span class="p_del">-  lg_local_lock_cpu(*lglock, cpu);</span>
<span class="p_del">-     access to protected per_cpu object on other CPU cpu</span>
<span class="p_del">-  lg_local_unlock_cpu(*lglock, cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-  lg_global_lock(*lglock);</span>
<span class="p_del">-     access all protected per_cpu objects on all CPUs</span>
<span class="p_del">-  lg_global_unlock(*lglock);</span>
<span class="p_del">-</span>
<span class="p_del">-  There are no _trylock variants of the lglocks.</span>
<span class="p_del">-</span>
<span class="p_del">-Note that the lg_global_lock/unlock has to iterate over all possible</span>
<span class="p_del">-CPUs rather than the actually present CPUs or a CPU could go off-line</span>
<span class="p_del">-with a held lock [4] and that makes it very expensive. A discussion on</span>
<span class="p_del">-these issues can be found at [5]</span>
<span class="p_del">-</span>
<span class="p_del">-Constraints:</span>
<span class="p_del">-------------</span>
<span class="p_del">-</span>
<span class="p_del">-  * currently the declaration of lglocks in kernel modules is not</span>
<span class="p_del">-    possible, though this should be doable with little change.</span>
<span class="p_del">-  * lglocks are not recursive.</span>
<span class="p_del">-  * suitable for code that can do most operations on the CPU local</span>
<span class="p_del">-    data and will very rarely need the global lock</span>
<span class="p_del">-  * lg_global_lock/unlock is *very* expensive and does not scale</span>
<span class="p_del">-  * on UP systems all lg_* primitives are simply spinlocks</span>
<span class="p_del">-  * in PREEMPT_RT the spinlock becomes an rt-mutex and can sleep but</span>
<span class="p_del">-    does not change the tasks state while sleeping [6].</span>
<span class="p_del">-  * in PREEMPT_RT the preempt_disable/enable in lg_local_lock/unlock</span>
<span class="p_del">-    is downgraded to a migrate_disable/enable, the other</span>
<span class="p_del">-    preempt_disable/enable are downgraded to barriers [6].</span>
<span class="p_del">-    The deadlock noted for non-RT above is resolved due to rt_mutexes</span>
<span class="p_del">-    boosting the lock-holder in this case which arch_spin_locks do</span>
<span class="p_del">-    not do.</span>
<span class="p_del">-</span>
<span class="p_del">-lglocks were designed for very specific problems in the VFS and probably</span>
<span class="p_del">-only are the right answer in these corner cases. Any new user that looks</span>
<span class="p_del">-at lglocks probably wants to look at the seqlock and RCU alternatives as</span>
<span class="p_del">-her first choice. There are also efforts to resolve the RCU issues that</span>
<span class="p_del">-currently prevent using RCU in place of view remaining lglocks.</span>
<span class="p_del">-</span>
<span class="p_del">-Note on brlock history:</span>
<span class="p_del">------------------------</span>
<span class="p_del">-</span>
<span class="p_del">-The &#39;Big Reader&#39; read-write spinlocks were originally introduced by</span>
<span class="p_del">-Ingo Molnar in 2000 (2.4/2.5 kernel series) and removed in 2003. They</span>
<span class="p_del">-later were introduced by the VFS scalability patch set in 2.6 series</span>
<span class="p_del">-again as the &quot;big reader lock&quot; brlock [2] variant of lglock which has</span>
<span class="p_del">-been replaced by seqlock primitives or by RCU based primitives in the</span>
<span class="p_del">-3.13 kernel series as was suggested in [3] in 2003. The brlock was</span>
<span class="p_del">-entirely removed in the 3.13 kernel series.</span>
<span class="p_del">-</span>
<span class="p_del">-Link: 1 http://lkml.org/lkml/2010/8/2/81</span>
<span class="p_del">-Link: 2 http://lwn.net/Articles/401738/</span>
<span class="p_del">-Link: 3 http://lkml.org/lkml/2003/3/9/205</span>
<span class="p_del">-Link: 4 https://lkml.org/lkml/2011/8/24/185</span>
<span class="p_del">-Link: 5 http://lkml.org/lkml/2011/12/18/189</span>
<span class="p_del">-Link: 6 https://www.kernel.org/pub/linux/kernel/projects/rt/</span>
<span class="p_del">-        patch series - lglocks-rt.patch.patch</span>
<span class="p_del">-Link: 7 http://lkml.org/lkml/2012/3/5/26</span>
<span class="p_header">diff --git a/Documentation/memory-barriers.txt b/Documentation/memory-barriers.txt</span>
<span class="p_header">index a4d0a99de04d..ba818ecce6f9 100644</span>
<span class="p_header">--- a/Documentation/memory-barriers.txt</span>
<span class="p_header">+++ b/Documentation/memory-barriers.txt</span>
<span class="p_chunk">@@ -609,7 +609,7 @@</span> <span class="p_context"> third possibility from arising.</span>
 The data-dependency barrier must order the read into Q with the store
 into *Q.  This prohibits this outcome:
 
<span class="p_del">-	(Q == B) &amp;&amp; (B == 4)</span>
<span class="p_add">+	(Q == &amp;B) &amp;&amp; (B == 4)</span>
 
 Please note that this pattern should be rare.  After all, the whole point
 of dependency ordering is to -prevent- writes to the data structure, along
<span class="p_chunk">@@ -1928,6 +1928,7 @@</span> <span class="p_context"> compiler and the CPU from reordering them.</span>
 
      See Documentation/DMA-API.txt for more information on consistent memory.
 
<span class="p_add">+</span>
 MMIO WRITE BARRIER
 ------------------
 
<span class="p_chunk">@@ -2075,7 +2076,7 @@</span> <span class="p_context"> systems, and so cannot be counted on in such a situation to actually achieve</span>
 anything at all - especially with respect to I/O accesses - unless combined
 with interrupt disabling operations.
 
<span class="p_del">-See also the section on &quot;Inter-CPU locking barrier effects&quot;.</span>
<span class="p_add">+See also the section on &quot;Inter-CPU acquiring barrier effects&quot;.</span>
 
 
 As an example, consider the following:
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 2a1f0ce7c59a..0cc8811af4e0 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -705,7 +705,6 @@</span> <span class="p_context"> config PARAVIRT_DEBUG</span>
 config PARAVIRT_SPINLOCKS
 	bool &quot;Paravirtualization layer for spinlocks&quot;
 	depends on PARAVIRT &amp;&amp; SMP
<span class="p_del">-	select UNINLINE_SPIN_UNLOCK if !QUEUED_SPINLOCKS</span>
 	---help---
 	  Paravirtualized spinlocks allow a pvops backend to replace the
 	  spinlock implementation with something virtualization-friendly
<span class="p_chunk">@@ -718,7 +717,7 @@</span> <span class="p_context"> config PARAVIRT_SPINLOCKS</span>
 
 config QUEUED_LOCK_STAT
 	bool &quot;Paravirt queued spinlock statistics&quot;
<span class="p_del">-	depends on PARAVIRT_SPINLOCKS &amp;&amp; DEBUG_FS &amp;&amp; QUEUED_SPINLOCKS</span>
<span class="p_add">+	depends on PARAVIRT_SPINLOCKS &amp;&amp; DEBUG_FS</span>
 	---help---
 	  Enable the collection of statistical data on the slowpath
 	  behavior of paravirtualized queued spinlocks and report
<span class="p_header">diff --git a/arch/x86/include/asm/cmpxchg.h b/arch/x86/include/asm/cmpxchg.h</span>
<span class="p_header">index 9733361fed6f..97848cdfcb1a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cmpxchg.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cmpxchg.h</span>
<span class="p_chunk">@@ -158,53 +158,9 @@</span> <span class="p_context"> extern void __add_wrong_size(void)</span>
  * value of &quot;*ptr&quot;.
  *
  * xadd() is locked when multiple CPUs are online
<span class="p_del">- * xadd_sync() is always locked</span>
<span class="p_del">- * xadd_local() is never locked</span>
  */
 #define __xadd(ptr, inc, lock)	__xchg_op((ptr), (inc), xadd, lock)
 #define xadd(ptr, inc)		__xadd((ptr), (inc), LOCK_PREFIX)
<span class="p_del">-#define xadd_sync(ptr, inc)	__xadd((ptr), (inc), &quot;lock; &quot;)</span>
<span class="p_del">-#define xadd_local(ptr, inc)	__xadd((ptr), (inc), &quot;&quot;)</span>
<span class="p_del">-</span>
<span class="p_del">-#define __add(ptr, inc, lock)						\</span>
<span class="p_del">-	({								\</span>
<span class="p_del">-	        __typeof__ (*(ptr)) __ret = (inc);			\</span>
<span class="p_del">-		switch (sizeof(*(ptr))) {				\</span>
<span class="p_del">-		case __X86_CASE_B:					\</span>
<span class="p_del">-			asm volatile (lock &quot;addb %b1, %0\n&quot;		\</span>
<span class="p_del">-				      : &quot;+m&quot; (*(ptr)) : &quot;qi&quot; (inc)	\</span>
<span class="p_del">-				      : &quot;memory&quot;, &quot;cc&quot;);		\</span>
<span class="p_del">-			break;						\</span>
<span class="p_del">-		case __X86_CASE_W:					\</span>
<span class="p_del">-			asm volatile (lock &quot;addw %w1, %0\n&quot;		\</span>
<span class="p_del">-				      : &quot;+m&quot; (*(ptr)) : &quot;ri&quot; (inc)	\</span>
<span class="p_del">-				      : &quot;memory&quot;, &quot;cc&quot;);		\</span>
<span class="p_del">-			break;						\</span>
<span class="p_del">-		case __X86_CASE_L:					\</span>
<span class="p_del">-			asm volatile (lock &quot;addl %1, %0\n&quot;		\</span>
<span class="p_del">-				      : &quot;+m&quot; (*(ptr)) : &quot;ri&quot; (inc)	\</span>
<span class="p_del">-				      : &quot;memory&quot;, &quot;cc&quot;);		\</span>
<span class="p_del">-			break;						\</span>
<span class="p_del">-		case __X86_CASE_Q:					\</span>
<span class="p_del">-			asm volatile (lock &quot;addq %1, %0\n&quot;		\</span>
<span class="p_del">-				      : &quot;+m&quot; (*(ptr)) : &quot;ri&quot; (inc)	\</span>
<span class="p_del">-				      : &quot;memory&quot;, &quot;cc&quot;);		\</span>
<span class="p_del">-			break;						\</span>
<span class="p_del">-		default:						\</span>
<span class="p_del">-			__add_wrong_size();				\</span>
<span class="p_del">-		}							\</span>
<span class="p_del">-		__ret;							\</span>
<span class="p_del">-	})</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * add_*() adds &quot;inc&quot; to &quot;*ptr&quot;</span>
<span class="p_del">- *</span>
<span class="p_del">- * __add() takes a lock prefix</span>
<span class="p_del">- * add_smp() is locked when multiple CPUs are online</span>
<span class="p_del">- * add_sync() is always locked</span>
<span class="p_del">- */</span>
<span class="p_del">-#define add_smp(ptr, inc)	__add((ptr), (inc), LOCK_PREFIX)</span>
<span class="p_del">-#define add_sync(ptr, inc)	__add((ptr), (inc), &quot;lock; &quot;)</span>
 
 #define __cmpxchg_double(pfx, p1, p2, o1, o2, n1, n2)			\
 ({									\
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">index 2970d22d7766..4cd8db05301f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt.h</span>
<span class="p_chunk">@@ -661,8 +661,6 @@</span> <span class="p_context"> static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,</span>
 
 #if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_PARAVIRT_SPINLOCKS)
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_del">-</span>
 static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
 							u32 val)
 {
<span class="p_chunk">@@ -684,22 +682,6 @@</span> <span class="p_context"> static __always_inline void pv_kick(int cpu)</span>
 	PVOP_VCALL1(pv_lock_ops.kick, cpu);
 }
 
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void __ticket_lock_spinning(struct arch_spinlock *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-	PVOP_VCALLEE2(pv_lock_ops.lock_spinning, lock, ticket);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void __ticket_unlock_kick(struct arch_spinlock *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-	PVOP_VCALL2(pv_lock_ops.unlock_kick, lock, ticket);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
 #endif /* SMP &amp;&amp; PARAVIRT_SPINLOCKS */
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">index 7fa9e7740ba3..60aac60ba25f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_chunk">@@ -301,23 +301,16 @@</span> <span class="p_context"> struct pv_mmu_ops {</span>
 struct arch_spinlock;
 #ifdef CONFIG_SMP
 #include &lt;asm/spinlock_types.h&gt;
<span class="p_del">-#else</span>
<span class="p_del">-typedef u16 __ticket_t;</span>
 #endif
 
 struct qspinlock;
 
 struct pv_lock_ops {
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 	void (*queued_spin_lock_slowpath)(struct qspinlock *lock, u32 val);
 	struct paravirt_callee_save queued_spin_unlock;
 
 	void (*wait)(u8 *ptr, u8 val);
 	void (*kick)(int cpu);
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-	struct paravirt_callee_save lock_spinning;</span>
<span class="p_del">-	void (*unlock_kick)(struct arch_spinlock *lock, __ticket_t ticket);</span>
<span class="p_del">-#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
 };
 
 /* This contains all the paravirt structures: we get a convenient
<span class="p_header">diff --git a/arch/x86/include/asm/rwsem.h b/arch/x86/include/asm/rwsem.h</span>
<span class="p_header">index 8dbc762ad132..3d33a719f5c1 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/rwsem.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/rwsem.h</span>
<span class="p_chunk">@@ -154,7 +154,7 @@</span> <span class="p_context"> static inline bool __down_write_trylock(struct rw_semaphore *sem)</span>
 		     : &quot;+m&quot; (sem-&gt;count), &quot;=&amp;a&quot; (tmp0), &quot;=&amp;r&quot; (tmp1),
 		       CC_OUT(e) (result)
 		     : &quot;er&quot; (RWSEM_ACTIVE_WRITE_BIAS)
<span class="p_del">-		     : &quot;memory&quot;, &quot;cc&quot;);</span>
<span class="p_add">+		     : &quot;memory&quot;);</span>
 	return result;
 }
 
<span class="p_header">diff --git a/arch/x86/include/asm/spinlock.h b/arch/x86/include/asm/spinlock.h</span>
<span class="p_header">index be0a05913b91..921bea7a2708 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/spinlock.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/spinlock.h</span>
<span class="p_chunk">@@ -20,187 +20,13 @@</span> <span class="p_context"></span>
  * (the type definitions are in asm/spinlock_types.h)
  */
 
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-# define LOCK_PTR_REG &quot;a&quot;</span>
<span class="p_del">-#else</span>
<span class="p_del">-# define LOCK_PTR_REG &quot;D&quot;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#if defined(CONFIG_X86_32) &amp;&amp; (defined(CONFIG_X86_PPRO_FENCE))</span>
<span class="p_del">-/*</span>
<span class="p_del">- * On PPro SMP, we use a locked operation to unlock</span>
<span class="p_del">- * (PPro errata 66, 92)</span>
<span class="p_del">- */</span>
<span class="p_del">-# define UNLOCK_LOCK_PREFIX LOCK_PREFIX</span>
<span class="p_del">-#else</span>
<span class="p_del">-# define UNLOCK_LOCK_PREFIX</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 /* How long a lock should spin before we consider blocking */
 #define SPIN_THRESHOLD	(1 &lt;&lt; 15)
 
 extern struct static_key paravirt_ticketlocks_enabled;
 static __always_inline bool static_key_false(struct static_key *key);
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 #include &lt;asm/qspinlock.h&gt;
<span class="p_del">-#else</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_PARAVIRT_SPINLOCKS</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void __ticket_enter_slowpath(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	set_bit(0, (volatile unsigned long *)&amp;lock-&gt;tickets.head);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#else  /* !CONFIG_PARAVIRT_SPINLOCKS */</span>
<span class="p_del">-static __always_inline void __ticket_lock_spinning(arch_spinlock_t *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-static inline void __ticket_unlock_kick(arch_spinlock_t *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* CONFIG_PARAVIRT_SPINLOCKS */</span>
<span class="p_del">-static inline int  __tickets_equal(__ticket_t one, __ticket_t two)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return !((one ^ two) &amp; ~TICKET_SLOWPATH_FLAG);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void __ticket_check_and_clear_slowpath(arch_spinlock_t *lock,</span>
<span class="p_del">-							__ticket_t head)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (head &amp; TICKET_SLOWPATH_FLAG) {</span>
<span class="p_del">-		arch_spinlock_t old, new;</span>
<span class="p_del">-</span>
<span class="p_del">-		old.tickets.head = head;</span>
<span class="p_del">-		new.tickets.head = head &amp; ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-		old.tickets.tail = new.tickets.head + TICKET_LOCK_INC;</span>
<span class="p_del">-		new.tickets.tail = old.tickets.tail;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* try to clear slowpath flag when there are no contenders */</span>
<span class="p_del">-		cmpxchg(&amp;lock-&gt;head_tail, old.head_tail, new.head_tail);</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline int arch_spin_value_unlocked(arch_spinlock_t lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return __tickets_equal(lock.tickets.head, lock.tickets.tail);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Ticket locks are conceptually two parts, one indicating the current head of</span>
<span class="p_del">- * the queue, and the other indicating the current tail. The lock is acquired</span>
<span class="p_del">- * by atomically noting the tail and incrementing it by one (thus adding</span>
<span class="p_del">- * ourself to the queue and noting our position), then waiting until the head</span>
<span class="p_del">- * becomes equal to the the initial value of the tail.</span>
<span class="p_del">- *</span>
<span class="p_del">- * We use an xadd covering *both* parts of the lock, to increment the tail and</span>
<span class="p_del">- * also load the position of the head, which takes care of memory ordering</span>
<span class="p_del">- * issues and should be optimal for the uncontended case. Note the tail must be</span>
<span class="p_del">- * in the high part, because a wide xadd increment of the low part would carry</span>
<span class="p_del">- * up and contaminate the high part.</span>
<span class="p_del">- */</span>
<span class="p_del">-static __always_inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	register struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };</span>
<span class="p_del">-</span>
<span class="p_del">-	inc = xadd(&amp;lock-&gt;tickets, inc);</span>
<span class="p_del">-	if (likely(inc.head == inc.tail))</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-</span>
<span class="p_del">-	for (;;) {</span>
<span class="p_del">-		unsigned count = SPIN_THRESHOLD;</span>
<span class="p_del">-</span>
<span class="p_del">-		do {</span>
<span class="p_del">-			inc.head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-			if (__tickets_equal(inc.head, inc.tail))</span>
<span class="p_del">-				goto clear_slowpath;</span>
<span class="p_del">-			cpu_relax();</span>
<span class="p_del">-		} while (--count);</span>
<span class="p_del">-		__ticket_lock_spinning(lock, inc.tail);</span>
<span class="p_del">-	}</span>
<span class="p_del">-clear_slowpath:</span>
<span class="p_del">-	__ticket_check_and_clear_slowpath(lock, inc.head);</span>
<span class="p_del">-out:</span>
<span class="p_del">-	barrier();	/* make sure nothing creeps before the lock is taken */</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	arch_spinlock_t old, new;</span>
<span class="p_del">-</span>
<span class="p_del">-	old.tickets = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-	if (!__tickets_equal(old.tickets.head, old.tickets.tail))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	new.head_tail = old.head_tail + (TICKET_LOCK_INC &lt;&lt; TICKET_SHIFT);</span>
<span class="p_del">-	new.head_tail &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* cmpxchg is a full barrier, so nothing can move before it */</span>
<span class="p_del">-	return cmpxchg(&amp;lock-&gt;head_tail, old.head_tail, new.head_tail) == old.head_tail;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (TICKET_SLOWPATH_FLAG &amp;&amp;</span>
<span class="p_del">-		static_key_false(&amp;paravirt_ticketlocks_enabled)) {</span>
<span class="p_del">-		__ticket_t head;</span>
<span class="p_del">-</span>
<span class="p_del">-		BUILD_BUG_ON(((__ticket_t)NR_CPUS) != NR_CPUS);</span>
<span class="p_del">-</span>
<span class="p_del">-		head = xadd(&amp;lock-&gt;tickets.head, TICKET_LOCK_INC);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (unlikely(head &amp; TICKET_SLOWPATH_FLAG)) {</span>
<span class="p_del">-			head &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-			__ticket_unlock_kick(lock, (head + TICKET_LOCK_INC));</span>
<span class="p_del">-		}</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		__add(&amp;lock-&gt;tickets.head, TICKET_LOCK_INC, UNLOCK_LOCK_PREFIX);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int arch_spin_is_locked(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-</span>
<span class="p_del">-	return !__tickets_equal(tmp.tail, tmp.head);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int arch_spin_is_contended(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-</span>
<span class="p_del">-	tmp.head &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-	return (__ticket_t)(tmp.tail - tmp.head) &gt; TICKET_LOCK_INC;</span>
<span class="p_del">-}</span>
<span class="p_del">-#define arch_spin_is_contended	arch_spin_is_contended</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void arch_spin_lock_flags(arch_spinlock_t *lock,</span>
<span class="p_del">-						  unsigned long flags)</span>
<span class="p_del">-{</span>
<span class="p_del">-	arch_spin_lock(lock);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void arch_spin_unlock_wait(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__ticket_t head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-</span>
<span class="p_del">-	for (;;) {</span>
<span class="p_del">-		struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * We need to check &quot;unlocked&quot; in a loop, tmp.head == head</span>
<span class="p_del">-		 * can be false positive because of overflow.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (__tickets_equal(tmp.head, tmp.tail) ||</span>
<span class="p_del">-				!__tickets_equal(tmp.head, head))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-</span>
<span class="p_del">-		cpu_relax();</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
 
 /*
  * Read-write spinlocks, allowing multiple readers
<span class="p_header">diff --git a/arch/x86/include/asm/spinlock_types.h b/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_header">index 65c3e37f879a..25311ebb446c 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_chunk">@@ -23,20 +23,7 @@</span> <span class="p_context"> typedef u32 __ticketpair_t;</span>
 
 #define TICKET_SHIFT	(sizeof(__ticket_t) * 8)
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 #include &lt;asm-generic/qspinlock_types.h&gt;
<span class="p_del">-#else</span>
<span class="p_del">-typedef struct arch_spinlock {</span>
<span class="p_del">-	union {</span>
<span class="p_del">-		__ticketpair_t head_tail;</span>
<span class="p_del">-		struct __raw_tickets {</span>
<span class="p_del">-			__ticket_t head, tail;</span>
<span class="p_del">-		} tickets;</span>
<span class="p_del">-	};</span>
<span class="p_del">-} arch_spinlock_t;</span>
<span class="p_del">-</span>
<span class="p_del">-#define __ARCH_SPIN_LOCK_UNLOCKED	{ { 0 } }</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
 
 #include &lt;asm-generic/qrwlock_types.h&gt;
 
<span class="p_header">diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c</span>
<span class="p_header">index 1726c4c12336..865058d087ac 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvm.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvm.c</span>
<span class="p_chunk">@@ -575,9 +575,6 @@</span> <span class="p_context"> static void kvm_kick_cpu(int cpu)</span>
 	kvm_hypercall2(KVM_HC_KICK_CPU, flags, apicid);
 }
 
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_del">-</span>
 #include &lt;asm/qspinlock.h&gt;
 
 static void kvm_wait(u8 *ptr, u8 val)
<span class="p_chunk">@@ -606,243 +603,6 @@</span> <span class="p_context"> static void kvm_wait(u8 *ptr, u8 val)</span>
 	local_irq_restore(flags);
 }
 
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
<span class="p_del">-enum kvm_contention_stat {</span>
<span class="p_del">-	TAKEN_SLOW,</span>
<span class="p_del">-	TAKEN_SLOW_PICKUP,</span>
<span class="p_del">-	RELEASED_SLOW,</span>
<span class="p_del">-	RELEASED_SLOW_KICKED,</span>
<span class="p_del">-	NR_CONTENTION_STATS</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_KVM_DEBUG_FS</span>
<span class="p_del">-#define HISTO_BUCKETS	30</span>
<span class="p_del">-</span>
<span class="p_del">-static struct kvm_spinlock_stats</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 contention_stats[NR_CONTENTION_STATS];</span>
<span class="p_del">-	u32 histo_spin_blocked[HISTO_BUCKETS+1];</span>
<span class="p_del">-	u64 time_blocked;</span>
<span class="p_del">-} spinlock_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static u8 zero_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void check_zero(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u8 ret;</span>
<span class="p_del">-	u8 old;</span>
<span class="p_del">-</span>
<span class="p_del">-	old = READ_ONCE(zero_stats);</span>
<span class="p_del">-	if (unlikely(old)) {</span>
<span class="p_del">-		ret = cmpxchg(&amp;zero_stats, old, 0);</span>
<span class="p_del">-		/* This ensures only one fellow resets the stat */</span>
<span class="p_del">-		if (ret == old)</span>
<span class="p_del">-			memset(&amp;spinlock_stats, 0, sizeof(spinlock_stats));</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void add_stats(enum kvm_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-	spinlock_stats.contention_stats[var] += val;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return sched_clock();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __spin_time_accum(u64 delta, u32 *array)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned index;</span>
<span class="p_del">-</span>
<span class="p_del">-	index = ilog2(delta);</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (index &lt; HISTO_BUCKETS)</span>
<span class="p_del">-		array[index]++;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		array[HISTO_BUCKETS]++;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 delta;</span>
<span class="p_del">-</span>
<span class="p_del">-	delta = sched_clock() - start;</span>
<span class="p_del">-	__spin_time_accum(delta, spinlock_stats.histo_spin_blocked);</span>
<span class="p_del">-	spinlock_stats.time_blocked += delta;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static struct dentry *d_spin_debug;</span>
<span class="p_del">-static struct dentry *d_kvm_debug;</span>
<span class="p_del">-</span>
<span class="p_del">-static struct dentry *kvm_init_debugfs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	d_kvm_debug = debugfs_create_dir(&quot;kvm-guest&quot;, NULL);</span>
<span class="p_del">-	if (!d_kvm_debug)</span>
<span class="p_del">-		printk(KERN_WARNING &quot;Could not create &#39;kvm&#39; debugfs directory\n&quot;);</span>
<span class="p_del">-</span>
<span class="p_del">-	return d_kvm_debug;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init kvm_spinlock_debugfs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct dentry *d_kvm;</span>
<span class="p_del">-</span>
<span class="p_del">-	d_kvm = kvm_init_debugfs();</span>
<span class="p_del">-	if (d_kvm == NULL)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-	d_spin_debug = debugfs_create_dir(&quot;spinlocks&quot;, d_kvm);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u8(&quot;zero_stats&quot;, 0644, d_spin_debug, &amp;zero_stats);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[TAKEN_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow_pickup&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_PICKUP]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[RELEASED_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow_kicked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[RELEASED_SLOW_KICKED]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u64(&quot;time_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.time_blocked);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32_array(&quot;histo_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		     spinlock_stats.histo_spin_blocked, HISTO_BUCKETS + 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-fs_initcall(kvm_spinlock_debugfs);</span>
<span class="p_del">-#else  /* !CONFIG_KVM_DEBUG_FS */</span>
<span class="p_del">-static inline void add_stats(enum kvm_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif  /* CONFIG_KVM_DEBUG_FS */</span>
<span class="p_del">-</span>
<span class="p_del">-struct kvm_lock_waiting {</span>
<span class="p_del">-	struct arch_spinlock *lock;</span>
<span class="p_del">-	__ticket_t want;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-/* cpus &#39;waiting&#39; on a spinlock to become available */</span>
<span class="p_del">-static cpumask_t waiting_cpus;</span>
<span class="p_del">-</span>
<span class="p_del">-/* Track spinlock on which a cpu is waiting */</span>
<span class="p_del">-static DEFINE_PER_CPU(struct kvm_lock_waiting, klock_waiting);</span>
<span class="p_del">-</span>
<span class="p_del">-__visible void kvm_lock_spinning(struct arch_spinlock *lock, __ticket_t want)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct kvm_lock_waiting *w;</span>
<span class="p_del">-	int cpu;</span>
<span class="p_del">-	u64 start;</span>
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-	__ticket_t head;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (in_nmi())</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	w = this_cpu_ptr(&amp;klock_waiting);</span>
<span class="p_del">-	cpu = smp_processor_id();</span>
<span class="p_del">-	start = spin_time_start();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Make sure an interrupt handler can&#39;t upset things in a</span>
<span class="p_del">-	 * partially setup state.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The ordering protocol on this is that the &quot;lock&quot; pointer</span>
<span class="p_del">-	 * may only be set non-NULL if the &quot;want&quot; ticket is correct.</span>
<span class="p_del">-	 * If we&#39;re updating &quot;want&quot;, we must first clear &quot;lock&quot;.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;want = want;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;lock = lock;</span>
<span class="p_del">-</span>
<span class="p_del">-	add_stats(TAKEN_SLOW, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This uses set_bit, which is atomic but we should not rely on its</span>
<span class="p_del">-	 * reordering gurantees. So barrier is needed after this call.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	cpumask_set_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-</span>
<span class="p_del">-	barrier();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Mark entry to slowpath before doing the pickup test to make</span>
<span class="p_del">-	 * sure we don&#39;t deadlock with an unlocker.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	__ticket_enter_slowpath(lock);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* make sure enter_slowpath, which is atomic does not cross the read */</span>
<span class="p_del">-	smp_mb__after_atomic();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * check again make sure it didn&#39;t become free while</span>
<span class="p_del">-	 * we weren&#39;t looking.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-	if (__tickets_equal(head, want)) {</span>
<span class="p_del">-		add_stats(TAKEN_SLOW_PICKUP, 1);</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * halt until it&#39;s our turn and kicked. Note that we do safe halt</span>
<span class="p_del">-	 * for irq enabled case to avoid hang when lock info is overwritten</span>
<span class="p_del">-	 * in irq spinlock slowpath and no spurious interrupt occur to save us.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (arch_irqs_disabled_flags(flags))</span>
<span class="p_del">-		halt();</span>
<span class="p_del">-	else</span>
<span class="p_del">-		safe_halt();</span>
<span class="p_del">-</span>
<span class="p_del">-out:</span>
<span class="p_del">-	cpumask_clear_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-	spin_time_accum_blocked(start);</span>
<span class="p_del">-}</span>
<span class="p_del">-PV_CALLEE_SAVE_REGS_THUNK(kvm_lock_spinning);</span>
<span class="p_del">-</span>
<span class="p_del">-/* Kick vcpu waiting on @lock-&gt;head to reach value @ticket */</span>
<span class="p_del">-static void kvm_unlock_kick(struct arch_spinlock *lock, __ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	add_stats(RELEASED_SLOW, 1);</span>
<span class="p_del">-	for_each_cpu(cpu, &amp;waiting_cpus) {</span>
<span class="p_del">-		const struct kvm_lock_waiting *w = &amp;per_cpu(klock_waiting, cpu);</span>
<span class="p_del">-		if (READ_ONCE(w-&gt;lock) == lock &amp;&amp;</span>
<span class="p_del">-		    READ_ONCE(w-&gt;want) == ticket) {</span>
<span class="p_del">-			add_stats(RELEASED_SLOW_KICKED, 1);</span>
<span class="p_del">-			kvm_kick_cpu(cpu);</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
 /*
  * Setup pv_lock_ops to exploit KVM_FEATURE_PV_UNHALT if present.
  */
<span class="p_chunk">@@ -854,16 +614,11 @@</span> <span class="p_context"> void __init kvm_spinlock_init(void)</span>
 	if (!kvm_para_has_feature(KVM_FEATURE_PV_UNHALT))
 		return;
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 	__pv_init_lock_hash();
 	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
 	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
 	pv_lock_ops.wait = kvm_wait;
 	pv_lock_ops.kick = kvm_kick_cpu;
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(kvm_lock_spinning);</span>
<span class="p_del">-	pv_lock_ops.unlock_kick = kvm_unlock_kick;</span>
<span class="p_del">-#endif</span>
 }
 
 static __init int kvm_spinlock_init_jump(void)
<span class="p_header">diff --git a/arch/x86/kernel/paravirt-spinlocks.c b/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_header">index 1939a0269377..2c55a003b793 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_chunk">@@ -8,7 +8,6 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/paravirt.h&gt;
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 __visible void __native_queued_spin_unlock(struct qspinlock *lock)
 {
 	native_queued_spin_unlock(lock);
<span class="p_chunk">@@ -21,19 +20,13 @@</span> <span class="p_context"> bool pv_is_native_spin_unlock(void)</span>
 	return pv_lock_ops.queued_spin_unlock.func ==
 		__raw_callee_save___native_queued_spin_unlock;
 }
<span class="p_del">-#endif</span>
 
 struct pv_lock_ops pv_lock_ops = {
 #ifdef CONFIG_SMP
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 	.queued_spin_lock_slowpath = native_queued_spin_lock_slowpath,
 	.queued_spin_unlock = PV_CALLEE_SAVE(__native_queued_spin_unlock),
 	.wait = paravirt_nop,
 	.kick = paravirt_nop,
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-	.lock_spinning = __PV_IS_CALLEE_SAVE(paravirt_nop),</span>
<span class="p_del">-	.unlock_kick = paravirt_nop,</span>
<span class="p_del">-#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
 #endif /* SMP */
 };
 EXPORT_SYMBOL(pv_lock_ops);
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_32.c b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">index 158dc0650d5d..920c6ae08592 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_chunk">@@ -10,7 +10,7 @@</span> <span class="p_context"> DEF_NATIVE(pv_mmu_ops, write_cr3, &quot;mov %eax, %cr3&quot;);</span>
 DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;mov %cr3, %eax&quot;);
 DEF_NATIVE(pv_cpu_ops, clts, &quot;clts&quot;);
 
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%eax)&quot;);
 #endif
 
<span class="p_chunk">@@ -49,7 +49,7 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_mmu_ops, read_cr3);
 		PATCH_SITE(pv_mmu_ops, write_cr3);
 		PATCH_SITE(pv_cpu_ops, clts);
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):
 			if (pv_is_native_spin_unlock()) {
 				start = start_pv_lock_ops_queued_spin_unlock;
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_64.c b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">index e70087a04cc8..bb3840cedb4f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_chunk">@@ -19,7 +19,7 @@</span> <span class="p_context"> DEF_NATIVE(pv_cpu_ops, swapgs, &quot;swapgs&quot;);</span>
 DEF_NATIVE(, mov32, &quot;mov %edi, %eax&quot;);
 DEF_NATIVE(, mov64, &quot;mov %rdi, %rax&quot;);
 
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%rdi)&quot;);
 #endif
 
<span class="p_chunk">@@ -61,7 +61,7 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_cpu_ops, clts);
 		PATCH_SITE(pv_mmu_ops, flush_tlb_single);
 		PATCH_SITE(pv_cpu_ops, wbinvd);
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):
 			if (pv_is_native_spin_unlock()) {
 				start = start_pv_lock_ops_queued_spin_unlock;
<span class="p_header">diff --git a/arch/x86/xen/spinlock.c b/arch/x86/xen/spinlock.c</span>
<span class="p_header">index f42e78de1e10..3d6e0064cbfc 100644</span>
<span class="p_header">--- a/arch/x86/xen/spinlock.c</span>
<span class="p_header">+++ b/arch/x86/xen/spinlock.c</span>
<span class="p_chunk">@@ -21,8 +21,6 @@</span> <span class="p_context"> static DEFINE_PER_CPU(int, lock_kicker_irq) = -1;</span>
 static DEFINE_PER_CPU(char *, irq_name);
 static bool xen_pvspin = true;
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_del">-</span>
 #include &lt;asm/qspinlock.h&gt;
 
 static void xen_qlock_kick(int cpu)
<span class="p_chunk">@@ -71,207 +69,6 @@</span> <span class="p_context"> static void xen_qlock_wait(u8 *byte, u8 val)</span>
 	xen_poll_irq(irq);
 }
 
<span class="p_del">-#else /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
<span class="p_del">-enum xen_contention_stat {</span>
<span class="p_del">-	TAKEN_SLOW,</span>
<span class="p_del">-	TAKEN_SLOW_PICKUP,</span>
<span class="p_del">-	TAKEN_SLOW_SPURIOUS,</span>
<span class="p_del">-	RELEASED_SLOW,</span>
<span class="p_del">-	RELEASED_SLOW_KICKED,</span>
<span class="p_del">-	NR_CONTENTION_STATS</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_XEN_DEBUG_FS</span>
<span class="p_del">-#define HISTO_BUCKETS	30</span>
<span class="p_del">-static struct xen_spinlock_stats</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 contention_stats[NR_CONTENTION_STATS];</span>
<span class="p_del">-	u32 histo_spin_blocked[HISTO_BUCKETS+1];</span>
<span class="p_del">-	u64 time_blocked;</span>
<span class="p_del">-} spinlock_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static u8 zero_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void check_zero(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u8 ret;</span>
<span class="p_del">-	u8 old = READ_ONCE(zero_stats);</span>
<span class="p_del">-	if (unlikely(old)) {</span>
<span class="p_del">-		ret = cmpxchg(&amp;zero_stats, old, 0);</span>
<span class="p_del">-		/* This ensures only one fellow resets the stat */</span>
<span class="p_del">-		if (ret == old)</span>
<span class="p_del">-			memset(&amp;spinlock_stats, 0, sizeof(spinlock_stats));</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void add_stats(enum xen_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-	spinlock_stats.contention_stats[var] += val;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return xen_clocksource_read();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __spin_time_accum(u64 delta, u32 *array)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned index = ilog2(delta);</span>
<span class="p_del">-</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (index &lt; HISTO_BUCKETS)</span>
<span class="p_del">-		array[index]++;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		array[HISTO_BUCKETS]++;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 delta = xen_clocksource_read() - start;</span>
<span class="p_del">-</span>
<span class="p_del">-	__spin_time_accum(delta, spinlock_stats.histo_spin_blocked);</span>
<span class="p_del">-	spinlock_stats.time_blocked += delta;</span>
<span class="p_del">-}</span>
<span class="p_del">-#else  /* !CONFIG_XEN_DEBUG_FS */</span>
<span class="p_del">-static inline void add_stats(enum xen_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif  /* CONFIG_XEN_DEBUG_FS */</span>
<span class="p_del">-</span>
<span class="p_del">-struct xen_lock_waiting {</span>
<span class="p_del">-	struct arch_spinlock *lock;</span>
<span class="p_del">-	__ticket_t want;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-static DEFINE_PER_CPU(struct xen_lock_waiting, lock_waiting);</span>
<span class="p_del">-static cpumask_t waiting_cpus;</span>
<span class="p_del">-</span>
<span class="p_del">-__visible void xen_lock_spinning(struct arch_spinlock *lock, __ticket_t want)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int irq = __this_cpu_read(lock_kicker_irq);</span>
<span class="p_del">-	struct xen_lock_waiting *w = this_cpu_ptr(&amp;lock_waiting);</span>
<span class="p_del">-	int cpu = smp_processor_id();</span>
<span class="p_del">-	u64 start;</span>
<span class="p_del">-	__ticket_t head;</span>
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* If kicker interrupts not initialized yet, just spin */</span>
<span class="p_del">-	if (irq == -1)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	start = spin_time_start();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Make sure an interrupt handler can&#39;t upset things in a</span>
<span class="p_del">-	 * partially setup state.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We don&#39;t really care if we&#39;re overwriting some other</span>
<span class="p_del">-	 * (lock,want) pair, as that would mean that we&#39;re currently</span>
<span class="p_del">-	 * in an interrupt context, and the outer context had</span>
<span class="p_del">-	 * interrupts enabled.  That has already kicked the VCPU out</span>
<span class="p_del">-	 * of xen_poll_irq(), so it will just return spuriously and</span>
<span class="p_del">-	 * retry with newly setup (lock,want).</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * The ordering protocol on this is that the &quot;lock&quot; pointer</span>
<span class="p_del">-	 * may only be set non-NULL if the &quot;want&quot; ticket is correct.</span>
<span class="p_del">-	 * If we&#39;re updating &quot;want&quot;, we must first clear &quot;lock&quot;.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;want = want;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;lock = lock;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* This uses set_bit, which atomic and therefore a barrier */</span>
<span class="p_del">-	cpumask_set_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-	add_stats(TAKEN_SLOW, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* clear pending */</span>
<span class="p_del">-	xen_clear_irq_pending(irq);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Only check lock once pending cleared */</span>
<span class="p_del">-	barrier();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Mark entry to slowpath before doing the pickup test to make</span>
<span class="p_del">-	 * sure we don&#39;t deadlock with an unlocker.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	__ticket_enter_slowpath(lock);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* make sure enter_slowpath, which is atomic does not cross the read */</span>
<span class="p_del">-	smp_mb__after_atomic();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * check again make sure it didn&#39;t become free while</span>
<span class="p_del">-	 * we weren&#39;t looking</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-	if (__tickets_equal(head, want)) {</span>
<span class="p_del">-		add_stats(TAKEN_SLOW_PICKUP, 1);</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Allow interrupts while blocked */</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If an interrupt happens here, it will leave the wakeup irq</span>
<span class="p_del">-	 * pending, which will cause xen_poll_irq() to return</span>
<span class="p_del">-	 * immediately.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Block until irq becomes pending (or perhaps a spurious wakeup) */</span>
<span class="p_del">-	xen_poll_irq(irq);</span>
<span class="p_del">-	add_stats(TAKEN_SLOW_SPURIOUS, !xen_test_irq_pending(irq));</span>
<span class="p_del">-</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	kstat_incr_irq_this_cpu(irq);</span>
<span class="p_del">-out:</span>
<span class="p_del">-	cpumask_clear_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	spin_time_accum_blocked(start);</span>
<span class="p_del">-}</span>
<span class="p_del">-PV_CALLEE_SAVE_REGS_THUNK(xen_lock_spinning);</span>
<span class="p_del">-</span>
<span class="p_del">-static void xen_unlock_kick(struct arch_spinlock *lock, __ticket_t next)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	add_stats(RELEASED_SLOW, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	for_each_cpu(cpu, &amp;waiting_cpus) {</span>
<span class="p_del">-		const struct xen_lock_waiting *w = &amp;per_cpu(lock_waiting, cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Make sure we read lock before want */</span>
<span class="p_del">-		if (READ_ONCE(w-&gt;lock) == lock &amp;&amp;</span>
<span class="p_del">-		    READ_ONCE(w-&gt;want) == next) {</span>
<span class="p_del">-			add_stats(RELEASED_SLOW_KICKED, 1);</span>
<span class="p_del">-			xen_send_IPI_one(cpu, XEN_SPIN_UNLOCK_VECTOR);</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
 static irqreturn_t dummy_handler(int irq, void *dev_id)
 {
 	BUG();
<span class="p_chunk">@@ -334,16 +131,12 @@</span> <span class="p_context"> void __init xen_init_spinlocks(void)</span>
 		return;
 	}
 	printk(KERN_DEBUG &quot;xen: PV spinlocks enabled\n&quot;);
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+</span>
 	__pv_init_lock_hash();
 	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
 	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
 	pv_lock_ops.wait = xen_qlock_wait;
 	pv_lock_ops.kick = xen_qlock_kick;
<span class="p_del">-#else</span>
<span class="p_del">-	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(xen_lock_spinning);</span>
<span class="p_del">-	pv_lock_ops.unlock_kick = xen_unlock_kick;</span>
<span class="p_del">-#endif</span>
 }
 
 /*
<span class="p_chunk">@@ -372,44 +165,3 @@</span> <span class="p_context"> static __init int xen_parse_nopvspin(char *arg)</span>
 }
 early_param(&quot;xen_nopvspin&quot;, xen_parse_nopvspin);
 
<span class="p_del">-#if defined(CONFIG_XEN_DEBUG_FS) &amp;&amp; !defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_del">-</span>
<span class="p_del">-static struct dentry *d_spin_debug;</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init xen_spinlock_debugfs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct dentry *d_xen = xen_init_debugfs();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (d_xen == NULL)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!xen_pvspin)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	d_spin_debug = debugfs_create_dir(&quot;spinlocks&quot;, d_xen);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u8(&quot;zero_stats&quot;, 0644, d_spin_debug, &amp;zero_stats);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow_pickup&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_PICKUP]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow_spurious&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_SPURIOUS]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[RELEASED_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow_kicked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[RELEASED_SLOW_KICKED]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u64(&quot;time_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.time_blocked);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32_array(&quot;histo_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-				spinlock_stats.histo_spin_blocked, HISTO_BUCKETS + 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-fs_initcall(xen_spinlock_debugfs);</span>
<span class="p_del">-</span>
<span class="p_del">-#endif	/* CONFIG_XEN_DEBUG_FS */</span>
<span class="p_header">diff --git a/fs/Kconfig b/fs/Kconfig</span>
<span class="p_header">index 2bc7ad775842..3ef62bad8f2b 100644</span>
<span class="p_header">--- a/fs/Kconfig</span>
<span class="p_header">+++ b/fs/Kconfig</span>
<span class="p_chunk">@@ -79,6 +79,7 @@</span> <span class="p_context"> config EXPORTFS_BLOCK_OPS</span>
 config FILE_LOCKING
 	bool &quot;Enable POSIX file locking API&quot; if EXPERT
 	default y
<span class="p_add">+	select PERCPU_RWSEM</span>
 	help
 	  This option enables standard file locking support, required
           for filesystems like NFS and for the flock() system
<span class="p_header">diff --git a/fs/locks.c b/fs/locks.c</span>
<span class="p_header">index ee1b15f6fc13..133fb2543d21 100644</span>
<span class="p_header">--- a/fs/locks.c</span>
<span class="p_header">+++ b/fs/locks.c</span>
<span class="p_chunk">@@ -127,7 +127,6 @@</span> <span class="p_context"></span>
 #include &lt;linux/pid_namespace.h&gt;
 #include &lt;linux/hashtable.h&gt;
 #include &lt;linux/percpu.h&gt;
<span class="p_del">-#include &lt;linux/lglock.h&gt;</span>
 
 #define CREATE_TRACE_POINTS
 #include &lt;trace/events/filelock.h&gt;
<span class="p_chunk">@@ -158,12 +157,18 @@</span> <span class="p_context"> int lease_break_time = 45;</span>
 
 /*
  * The global file_lock_list is only used for displaying /proc/locks, so we
<span class="p_del">- * keep a list on each CPU, with each list protected by its own spinlock via</span>
<span class="p_del">- * the file_lock_lglock. Note that alterations to the list also require that</span>
<span class="p_del">- * the relevant flc_lock is held.</span>
<span class="p_add">+ * keep a list on each CPU, with each list protected by its own spinlock.</span>
<span class="p_add">+ * Global serialization is done using file_rwsem.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that alterations to the list also require that the relevant flc_lock is</span>
<span class="p_add">+ * held.</span>
  */
<span class="p_del">-DEFINE_STATIC_LGLOCK(file_lock_lglock);</span>
<span class="p_del">-static DEFINE_PER_CPU(struct hlist_head, file_lock_list);</span>
<span class="p_add">+struct file_lock_list_struct {</span>
<span class="p_add">+	spinlock_t		lock;</span>
<span class="p_add">+	struct hlist_head	hlist;</span>
<span class="p_add">+};</span>
<span class="p_add">+static DEFINE_PER_CPU(struct file_lock_list_struct, file_lock_list);</span>
<span class="p_add">+DEFINE_STATIC_PERCPU_RWSEM(file_rwsem);</span>
 
 /*
  * The blocked_hash is used to find POSIX lock loops for deadlock detection.
<span class="p_chunk">@@ -587,15 +592,23 @@</span> <span class="p_context"> static int posix_same_owner(struct file_lock *fl1, struct file_lock *fl2)</span>
 /* Must be called with the flc_lock held! */
 static void locks_insert_global_locks(struct file_lock *fl)
 {
<span class="p_del">-	lg_local_lock(&amp;file_lock_lglock);</span>
<span class="p_add">+	struct file_lock_list_struct *fll = this_cpu_ptr(&amp;file_lock_list);</span>
<span class="p_add">+</span>
<span class="p_add">+	percpu_rwsem_assert_held(&amp;file_rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;fll-&gt;lock);</span>
 	fl-&gt;fl_link_cpu = smp_processor_id();
<span class="p_del">-	hlist_add_head(&amp;fl-&gt;fl_link, this_cpu_ptr(&amp;file_lock_list));</span>
<span class="p_del">-	lg_local_unlock(&amp;file_lock_lglock);</span>
<span class="p_add">+	hlist_add_head(&amp;fl-&gt;fl_link, &amp;fll-&gt;hlist);</span>
<span class="p_add">+	spin_unlock(&amp;fll-&gt;lock);</span>
 }
 
 /* Must be called with the flc_lock held! */
 static void locks_delete_global_locks(struct file_lock *fl)
 {
<span class="p_add">+	struct file_lock_list_struct *fll;</span>
<span class="p_add">+</span>
<span class="p_add">+	percpu_rwsem_assert_held(&amp;file_rwsem);</span>
<span class="p_add">+</span>
 	/*
 	 * Avoid taking lock if already unhashed. This is safe since this check
 	 * is done while holding the flc_lock, and new insertions into the list
<span class="p_chunk">@@ -603,9 +616,11 @@</span> <span class="p_context"> static void locks_delete_global_locks(struct file_lock *fl)</span>
 	 */
 	if (hlist_unhashed(&amp;fl-&gt;fl_link))
 		return;
<span class="p_del">-	lg_local_lock_cpu(&amp;file_lock_lglock, fl-&gt;fl_link_cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	fll = per_cpu_ptr(&amp;file_lock_list, fl-&gt;fl_link_cpu);</span>
<span class="p_add">+	spin_lock(&amp;fll-&gt;lock);</span>
 	hlist_del_init(&amp;fl-&gt;fl_link);
<span class="p_del">-	lg_local_unlock_cpu(&amp;file_lock_lglock, fl-&gt;fl_link_cpu);</span>
<span class="p_add">+	spin_unlock(&amp;fll-&gt;lock);</span>
 }
 
 static unsigned long
<span class="p_chunk">@@ -915,6 +930,7 @@</span> <span class="p_context"> static int flock_lock_inode(struct inode *inode, struct file_lock *request)</span>
 			return -ENOMEM;
 	}
 
<span class="p_add">+	percpu_down_read_preempt_disable(&amp;file_rwsem);</span>
 	spin_lock(&amp;ctx-&gt;flc_lock);
 	if (request-&gt;fl_flags &amp; FL_ACCESS)
 		goto find_conflict;
<span class="p_chunk">@@ -955,6 +971,7 @@</span> <span class="p_context"> static int flock_lock_inode(struct inode *inode, struct file_lock *request)</span>
 
 out:
 	spin_unlock(&amp;ctx-&gt;flc_lock);
<span class="p_add">+	percpu_up_read_preempt_enable(&amp;file_rwsem);</span>
 	if (new_fl)
 		locks_free_lock(new_fl);
 	locks_dispose_list(&amp;dispose);
<span class="p_chunk">@@ -991,6 +1008,7 @@</span> <span class="p_context"> static int posix_lock_inode(struct inode *inode, struct file_lock *request,</span>
 		new_fl2 = locks_alloc_lock();
 	}
 
<span class="p_add">+	percpu_down_read_preempt_disable(&amp;file_rwsem);</span>
 	spin_lock(&amp;ctx-&gt;flc_lock);
 	/*
 	 * New lock request. Walk all POSIX locks and look for conflicts. If
<span class="p_chunk">@@ -1162,6 +1180,7 @@</span> <span class="p_context"> static int posix_lock_inode(struct inode *inode, struct file_lock *request,</span>
 	}
  out:
 	spin_unlock(&amp;ctx-&gt;flc_lock);
<span class="p_add">+	percpu_up_read_preempt_enable(&amp;file_rwsem);</span>
 	/*
 	 * Free any unused locks.
 	 */
<span class="p_chunk">@@ -1436,6 +1455,7 @@</span> <span class="p_context"> int __break_lease(struct inode *inode, unsigned int mode, unsigned int type)</span>
 		return error;
 	}
 
<span class="p_add">+	percpu_down_read_preempt_disable(&amp;file_rwsem);</span>
 	spin_lock(&amp;ctx-&gt;flc_lock);
 
 	time_out_leases(inode, &amp;dispose);
<span class="p_chunk">@@ -1487,9 +1507,13 @@</span> <span class="p_context"> int __break_lease(struct inode *inode, unsigned int mode, unsigned int type)</span>
 	locks_insert_block(fl, new_fl);
 	trace_break_lease_block(inode, new_fl);
 	spin_unlock(&amp;ctx-&gt;flc_lock);
<span class="p_add">+	percpu_up_read_preempt_enable(&amp;file_rwsem);</span>
<span class="p_add">+</span>
 	locks_dispose_list(&amp;dispose);
 	error = wait_event_interruptible_timeout(new_fl-&gt;fl_wait,
 						!new_fl-&gt;fl_next, break_time);
<span class="p_add">+</span>
<span class="p_add">+	percpu_down_read_preempt_disable(&amp;file_rwsem);</span>
 	spin_lock(&amp;ctx-&gt;flc_lock);
 	trace_break_lease_unblock(inode, new_fl);
 	locks_delete_block(new_fl);
<span class="p_chunk">@@ -1506,6 +1530,7 @@</span> <span class="p_context"> int __break_lease(struct inode *inode, unsigned int mode, unsigned int type)</span>
 	}
 out:
 	spin_unlock(&amp;ctx-&gt;flc_lock);
<span class="p_add">+	percpu_up_read_preempt_enable(&amp;file_rwsem);</span>
 	locks_dispose_list(&amp;dispose);
 	locks_free_lock(new_fl);
 	return error;
<span class="p_chunk">@@ -1660,6 +1685,7 @@</span> <span class="p_context"> generic_add_lease(struct file *filp, long arg, struct file_lock **flp, void **pr</span>
 		return -EINVAL;
 	}
 
<span class="p_add">+	percpu_down_read_preempt_disable(&amp;file_rwsem);</span>
 	spin_lock(&amp;ctx-&gt;flc_lock);
 	time_out_leases(inode, &amp;dispose);
 	error = check_conflicting_open(dentry, arg, lease-&gt;fl_flags);
<span class="p_chunk">@@ -1730,6 +1756,7 @@</span> <span class="p_context"> generic_add_lease(struct file *filp, long arg, struct file_lock **flp, void **pr</span>
 		lease-&gt;fl_lmops-&gt;lm_setup(lease, priv);
 out:
 	spin_unlock(&amp;ctx-&gt;flc_lock);
<span class="p_add">+	percpu_up_read_preempt_enable(&amp;file_rwsem);</span>
 	locks_dispose_list(&amp;dispose);
 	if (is_deleg)
 		inode_unlock(inode);
<span class="p_chunk">@@ -1752,6 +1779,7 @@</span> <span class="p_context"> static int generic_delete_lease(struct file *filp, void *owner)</span>
 		return error;
 	}
 
<span class="p_add">+	percpu_down_read_preempt_disable(&amp;file_rwsem);</span>
 	spin_lock(&amp;ctx-&gt;flc_lock);
 	list_for_each_entry(fl, &amp;ctx-&gt;flc_lease, fl_list) {
 		if (fl-&gt;fl_file == filp &amp;&amp;
<span class="p_chunk">@@ -1764,6 +1792,7 @@</span> <span class="p_context"> static int generic_delete_lease(struct file *filp, void *owner)</span>
 	if (victim)
 		error = fl-&gt;fl_lmops-&gt;lm_change(victim, F_UNLCK, &amp;dispose);
 	spin_unlock(&amp;ctx-&gt;flc_lock);
<span class="p_add">+	percpu_up_read_preempt_enable(&amp;file_rwsem);</span>
 	locks_dispose_list(&amp;dispose);
 	return error;
 }
<span class="p_chunk">@@ -2703,9 +2732,9 @@</span> <span class="p_context"> static void *locks_start(struct seq_file *f, loff_t *pos)</span>
 	struct locks_iterator *iter = f-&gt;private;
 
 	iter-&gt;li_pos = *pos + 1;
<span class="p_del">-	lg_global_lock(&amp;file_lock_lglock);</span>
<span class="p_add">+	percpu_down_write(&amp;file_rwsem);</span>
 	spin_lock(&amp;blocked_lock_lock);
<span class="p_del">-	return seq_hlist_start_percpu(&amp;file_lock_list, &amp;iter-&gt;li_cpu, *pos);</span>
<span class="p_add">+	return seq_hlist_start_percpu(&amp;file_lock_list.hlist, &amp;iter-&gt;li_cpu, *pos);</span>
 }
 
 static void *locks_next(struct seq_file *f, void *v, loff_t *pos)
<span class="p_chunk">@@ -2713,14 +2742,14 @@</span> <span class="p_context"> static void *locks_next(struct seq_file *f, void *v, loff_t *pos)</span>
 	struct locks_iterator *iter = f-&gt;private;
 
 	++iter-&gt;li_pos;
<span class="p_del">-	return seq_hlist_next_percpu(v, &amp;file_lock_list, &amp;iter-&gt;li_cpu, pos);</span>
<span class="p_add">+	return seq_hlist_next_percpu(v, &amp;file_lock_list.hlist, &amp;iter-&gt;li_cpu, pos);</span>
 }
 
 static void locks_stop(struct seq_file *f, void *v)
 	__releases(&amp;blocked_lock_lock)
 {
 	spin_unlock(&amp;blocked_lock_lock);
<span class="p_del">-	lg_global_unlock(&amp;file_lock_lglock);</span>
<span class="p_add">+	percpu_up_write(&amp;file_rwsem);</span>
 }
 
 static const struct seq_operations locks_seq_operations = {
<span class="p_chunk">@@ -2761,10 +2790,13 @@</span> <span class="p_context"> static int __init filelock_init(void)</span>
 	filelock_cache = kmem_cache_create(&quot;file_lock_cache&quot;,
 			sizeof(struct file_lock), 0, SLAB_PANIC, NULL);
 
<span class="p_del">-	lg_lock_init(&amp;file_lock_lglock, &quot;file_lock_lglock&quot;);</span>
 
<span class="p_del">-	for_each_possible_cpu(i)</span>
<span class="p_del">-		INIT_HLIST_HEAD(per_cpu_ptr(&amp;file_lock_list, i));</span>
<span class="p_add">+	for_each_possible_cpu(i) {</span>
<span class="p_add">+		struct file_lock_list_struct *fll = per_cpu_ptr(&amp;file_lock_list, i);</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock_init(&amp;fll-&gt;lock);</span>
<span class="p_add">+		INIT_HLIST_HEAD(&amp;fll-&gt;hlist);</span>
<span class="p_add">+	}</span>
 
 	return 0;
 }
<span class="p_header">diff --git a/include/linux/lglock.h b/include/linux/lglock.h</span>
deleted file mode 100644
<span class="p_header">index c92ebd100d9b..000000000000</span>
<span class="p_header">--- a/include/linux/lglock.h</span>
<span class="p_header">+++ /dev/null</span>
<span class="p_chunk">@@ -1,81 +0,0 @@</span> <span class="p_context"></span>
<span class="p_del">-/*</span>
<span class="p_del">- * Specialised local-global spinlock. Can only be declared as global variables</span>
<span class="p_del">- * to avoid overhead and keep things simple (and we don&#39;t want to start using</span>
<span class="p_del">- * these inside dynamically allocated structures).</span>
<span class="p_del">- *</span>
<span class="p_del">- * &quot;local/global locks&quot; (lglocks) can be used to:</span>
<span class="p_del">- *</span>
<span class="p_del">- * - Provide fast exclusive access to per-CPU data, with exclusive access to</span>
<span class="p_del">- *   another CPU&#39;s data allowed but possibly subject to contention, and to</span>
<span class="p_del">- *   provide very slow exclusive access to all per-CPU data.</span>
<span class="p_del">- * - Or to provide very fast and scalable read serialisation, and to provide</span>
<span class="p_del">- *   very slow exclusive serialisation of data (not necessarily per-CPU data).</span>
<span class="p_del">- *</span>
<span class="p_del">- * Brlocks are also implemented as a short-hand notation for the latter use</span>
<span class="p_del">- * case.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Copyright 2009, 2010, Nick Piggin, Novell Inc.</span>
<span class="p_del">- */</span>
<span class="p_del">-#ifndef __LINUX_LGLOCK_H</span>
<span class="p_del">-#define __LINUX_LGLOCK_H</span>
<span class="p_del">-</span>
<span class="p_del">-#include &lt;linux/spinlock.h&gt;</span>
<span class="p_del">-#include &lt;linux/lockdep.h&gt;</span>
<span class="p_del">-#include &lt;linux/percpu.h&gt;</span>
<span class="p_del">-#include &lt;linux/cpu.h&gt;</span>
<span class="p_del">-#include &lt;linux/notifier.h&gt;</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_SMP</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_DEBUG_LOCK_ALLOC</span>
<span class="p_del">-#define LOCKDEP_INIT_MAP lockdep_init_map</span>
<span class="p_del">-#else</span>
<span class="p_del">-#define LOCKDEP_INIT_MAP(a, b, c, d)</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-struct lglock {</span>
<span class="p_del">-	arch_spinlock_t __percpu *lock;</span>
<span class="p_del">-#ifdef CONFIG_DEBUG_LOCK_ALLOC</span>
<span class="p_del">-	struct lock_class_key lock_key;</span>
<span class="p_del">-	struct lockdep_map    lock_dep_map;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-#define DEFINE_LGLOCK(name)						\</span>
<span class="p_del">-	static DEFINE_PER_CPU(arch_spinlock_t, name ## _lock)		\</span>
<span class="p_del">-	= __ARCH_SPIN_LOCK_UNLOCKED;					\</span>
<span class="p_del">-	struct lglock name = { .lock = &amp;name ## _lock }</span>
<span class="p_del">-</span>
<span class="p_del">-#define DEFINE_STATIC_LGLOCK(name)					\</span>
<span class="p_del">-	static DEFINE_PER_CPU(arch_spinlock_t, name ## _lock)		\</span>
<span class="p_del">-	= __ARCH_SPIN_LOCK_UNLOCKED;					\</span>
<span class="p_del">-	static struct lglock name = { .lock = &amp;name ## _lock }</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_lock_init(struct lglock *lg, char *name);</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_local_lock(struct lglock *lg);</span>
<span class="p_del">-void lg_local_unlock(struct lglock *lg);</span>
<span class="p_del">-void lg_local_lock_cpu(struct lglock *lg, int cpu);</span>
<span class="p_del">-void lg_local_unlock_cpu(struct lglock *lg, int cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_double_lock(struct lglock *lg, int cpu1, int cpu2);</span>
<span class="p_del">-void lg_double_unlock(struct lglock *lg, int cpu1, int cpu2);</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_global_lock(struct lglock *lg);</span>
<span class="p_del">-void lg_global_unlock(struct lglock *lg);</span>
<span class="p_del">-</span>
<span class="p_del">-#else</span>
<span class="p_del">-/* When !CONFIG_SMP, map lglock to spinlock */</span>
<span class="p_del">-#define lglock spinlock</span>
<span class="p_del">-#define DEFINE_LGLOCK(name) DEFINE_SPINLOCK(name)</span>
<span class="p_del">-#define DEFINE_STATIC_LGLOCK(name) static DEFINE_SPINLOCK(name)</span>
<span class="p_del">-#define lg_lock_init(lg, name) spin_lock_init(lg)</span>
<span class="p_del">-#define lg_local_lock spin_lock</span>
<span class="p_del">-#define lg_local_unlock spin_unlock</span>
<span class="p_del">-#define lg_local_lock_cpu(lg, cpu) spin_lock(lg)</span>
<span class="p_del">-#define lg_local_unlock_cpu(lg, cpu) spin_unlock(lg)</span>
<span class="p_del">-#define lg_global_lock spin_lock</span>
<span class="p_del">-#define lg_global_unlock spin_unlock</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#endif</span>
<span class="p_header">diff --git a/include/linux/percpu-rwsem.h b/include/linux/percpu-rwsem.h</span>
<span class="p_header">index c2fa3ecb0dce..5b2e6159b744 100644</span>
<span class="p_header">--- a/include/linux/percpu-rwsem.h</span>
<span class="p_header">+++ b/include/linux/percpu-rwsem.h</span>
<span class="p_chunk">@@ -10,32 +10,122 @@</span> <span class="p_context"></span>
 
 struct percpu_rw_semaphore {
 	struct rcu_sync		rss;
<span class="p_del">-	unsigned int __percpu	*fast_read_ctr;</span>
<span class="p_add">+	unsigned int __percpu	*read_count;</span>
 	struct rw_semaphore	rw_sem;
<span class="p_del">-	atomic_t		slow_read_ctr;</span>
<span class="p_del">-	wait_queue_head_t	write_waitq;</span>
<span class="p_add">+	wait_queue_head_t	writer;</span>
<span class="p_add">+	int			readers_block;</span>
 };
 
<span class="p_del">-extern void percpu_down_read(struct percpu_rw_semaphore *);</span>
<span class="p_del">-extern int  percpu_down_read_trylock(struct percpu_rw_semaphore *);</span>
<span class="p_del">-extern void percpu_up_read(struct percpu_rw_semaphore *);</span>
<span class="p_add">+#define DEFINE_STATIC_PERCPU_RWSEM(name)				\</span>
<span class="p_add">+static DEFINE_PER_CPU(unsigned int, __percpu_rwsem_rc_##name);		\</span>
<span class="p_add">+static struct percpu_rw_semaphore name = {				\</span>
<span class="p_add">+	.rss = __RCU_SYNC_INITIALIZER(name.rss, RCU_SCHED_SYNC),	\</span>
<span class="p_add">+	.read_count = &amp;__percpu_rwsem_rc_##name,			\</span>
<span class="p_add">+	.rw_sem = __RWSEM_INITIALIZER(name.rw_sem),			\</span>
<span class="p_add">+	.writer = __WAIT_QUEUE_HEAD_INITIALIZER(name.writer),		\</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern int __percpu_down_read(struct percpu_rw_semaphore *, int);</span>
<span class="p_add">+extern void __percpu_up_read(struct percpu_rw_semaphore *);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void percpu_down_read_preempt_disable(struct percpu_rw_semaphore *sem)</span>
<span class="p_add">+{</span>
<span class="p_add">+	might_sleep();</span>
<span class="p_add">+</span>
<span class="p_add">+	rwsem_acquire_read(&amp;sem-&gt;rw_sem.dep_map, 0, 0, _RET_IP_);</span>
<span class="p_add">+</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We are in an RCU-sched read-side critical section, so the writer</span>
<span class="p_add">+	 * cannot both change sem-&gt;state from readers_fast and start checking</span>
<span class="p_add">+	 * counters while we are here. So if we see !sem-&gt;state, we know that</span>
<span class="p_add">+	 * the writer won&#39;t be checking until we&#39;re past the preempt_enable()</span>
<span class="p_add">+	 * and that one the synchronize_sched() is done, the writer will see</span>
<span class="p_add">+	 * anything we did within this RCU-sched read-size critical section.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	__this_cpu_inc(*sem-&gt;read_count);</span>
<span class="p_add">+	if (unlikely(!rcu_sync_is_idle(&amp;sem-&gt;rss)))</span>
<span class="p_add">+		__percpu_down_read(sem, false); /* Unconditional memory barrier */</span>
<span class="p_add">+	barrier();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The barrier() prevents the compiler from</span>
<span class="p_add">+	 * bleeding the critical section out.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void percpu_down_read(struct percpu_rw_semaphore *sem)</span>
<span class="p_add">+{</span>
<span class="p_add">+	percpu_down_read_preempt_disable(sem);</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int percpu_down_read_trylock(struct percpu_rw_semaphore *sem)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Same as in percpu_down_read().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	__this_cpu_inc(*sem-&gt;read_count);</span>
<span class="p_add">+	if (unlikely(!rcu_sync_is_idle(&amp;sem-&gt;rss)))</span>
<span class="p_add">+		ret = __percpu_down_read(sem, true); /* Unconditional memory barrier */</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The barrier() from preempt_enable() prevents the compiler from</span>
<span class="p_add">+	 * bleeding the critical section out.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		rwsem_acquire_read(&amp;sem-&gt;rw_sem.dep_map, 0, 1, _RET_IP_);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void percpu_up_read_preempt_enable(struct percpu_rw_semaphore *sem)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The barrier() prevents the compiler from</span>
<span class="p_add">+	 * bleeding the critical section out.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	barrier();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Same as in percpu_down_read().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (likely(rcu_sync_is_idle(&amp;sem-&gt;rss)))</span>
<span class="p_add">+		__this_cpu_dec(*sem-&gt;read_count);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		__percpu_up_read(sem); /* Unconditional memory barrier */</span>
<span class="p_add">+	preempt_enable();</span>
<span class="p_add">+</span>
<span class="p_add">+	rwsem_release(&amp;sem-&gt;rw_sem.dep_map, 1, _RET_IP_);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void percpu_up_read(struct percpu_rw_semaphore *sem)</span>
<span class="p_add">+{</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	percpu_up_read_preempt_enable(sem);</span>
<span class="p_add">+}</span>
 
 extern void percpu_down_write(struct percpu_rw_semaphore *);
 extern void percpu_up_write(struct percpu_rw_semaphore *);
 
 extern int __percpu_init_rwsem(struct percpu_rw_semaphore *,
 				const char *, struct lock_class_key *);
<span class="p_add">+</span>
 extern void percpu_free_rwsem(struct percpu_rw_semaphore *);
 
<span class="p_del">-#define percpu_init_rwsem(brw)	\</span>
<span class="p_add">+#define percpu_init_rwsem(sem)					\</span>
 ({								\
 	static struct lock_class_key rwsem_key;			\
<span class="p_del">-	__percpu_init_rwsem(brw, #brw, &amp;rwsem_key);		\</span>
<span class="p_add">+	__percpu_init_rwsem(sem, #sem, &amp;rwsem_key);		\</span>
 })
 
<span class="p_del">-</span>
 #define percpu_rwsem_is_held(sem) lockdep_is_held(&amp;(sem)-&gt;rw_sem)
 
<span class="p_add">+#define percpu_rwsem_assert_held(sem)				\</span>
<span class="p_add">+	lockdep_assert_held(&amp;(sem)-&gt;rw_sem)</span>
<span class="p_add">+</span>
 static inline void percpu_rwsem_release(struct percpu_rw_semaphore *sem,
 					bool read, unsigned long ip)
 {
<span class="p_header">diff --git a/include/linux/rcu_sync.h b/include/linux/rcu_sync.h</span>
<span class="p_header">index a63a33e6196e..ece7ed9a4a70 100644</span>
<span class="p_header">--- a/include/linux/rcu_sync.h</span>
<span class="p_header">+++ b/include/linux/rcu_sync.h</span>
<span class="p_chunk">@@ -59,6 +59,7 @@</span> <span class="p_context"> static inline bool rcu_sync_is_idle(struct rcu_sync *rsp)</span>
 }
 
 extern void rcu_sync_init(struct rcu_sync *, enum rcu_sync_type);
<span class="p_add">+extern void rcu_sync_enter_start(struct rcu_sync *);</span>
 extern void rcu_sync_enter(struct rcu_sync *);
 extern void rcu_sync_exit(struct rcu_sync *);
 extern void rcu_sync_dtor(struct rcu_sync *);
<span class="p_header">diff --git a/kernel/cgroup.c b/kernel/cgroup.c</span>
<span class="p_header">index d6b729beba49..9ba28310eab6 100644</span>
<span class="p_header">--- a/kernel/cgroup.c</span>
<span class="p_header">+++ b/kernel/cgroup.c</span>
<span class="p_chunk">@@ -5627,6 +5627,12 @@</span> <span class="p_context"> int __init cgroup_init(void)</span>
 	BUG_ON(cgroup_init_cftypes(NULL, cgroup_dfl_base_files));
 	BUG_ON(cgroup_init_cftypes(NULL, cgroup_legacy_base_files));
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The latency of the synchronize_sched() is too high for cgroups,</span>
<span class="p_add">+	 * avoid it at the cost of forcing all readers into the slow path.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	rcu_sync_enter_start(&amp;cgroup_threadgroup_rwsem.rss);</span>
<span class="p_add">+</span>
 	get_user_ns(init_cgroup_ns.user_ns);
 
 	mutex_lock(&amp;cgroup_mutex);
<span class="p_header">diff --git a/kernel/futex.c b/kernel/futex.c</span>
<span class="p_header">index 46cb3a301bc1..2c4be467fecd 100644</span>
<span class="p_header">--- a/kernel/futex.c</span>
<span class="p_header">+++ b/kernel/futex.c</span>
<span class="p_chunk">@@ -381,8 +381,12 @@</span> <span class="p_context"> static inline int hb_waiters_pending(struct futex_hash_bucket *hb)</span>
 #endif
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * We hash on the keys returned from get_futex_key (see below).</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * hash_futex - Return the hash bucket in the global hash</span>
<span class="p_add">+ * @key:	Pointer to the futex key for which the hash is calculated</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We hash on the keys returned from get_futex_key (see below) and return the</span>
<span class="p_add">+ * corresponding hash bucket in the global hash.</span>
  */
 static struct futex_hash_bucket *hash_futex(union futex_key *key)
 {
<span class="p_chunk">@@ -392,7 +396,12 @@</span> <span class="p_context"> static struct futex_hash_bucket *hash_futex(union futex_key *key)</span>
 	return &amp;futex_queues[hash &amp; (futex_hashsize - 1)];
 }
 
<span class="p_del">-/*</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * match_futex - Check whether two futex keys are equal</span>
<span class="p_add">+ * @key1:	Pointer to key1</span>
<span class="p_add">+ * @key2:	Pointer to key2</span>
<span class="p_add">+ *</span>
  * Return 1 if two futex_keys are equal, 0 otherwise.
  */
 static inline int match_futex(union futex_key *key1, union futex_key *key2)
<span class="p_header">diff --git a/kernel/hung_task.c b/kernel/hung_task.c</span>
<span class="p_header">index d234022805dc..432c3d71d195 100644</span>
<span class="p_header">--- a/kernel/hung_task.c</span>
<span class="p_header">+++ b/kernel/hung_task.c</span>
<span class="p_chunk">@@ -117,7 +117,7 @@</span> <span class="p_context"> static void check_hung_task(struct task_struct *t, unsigned long timeout)</span>
 	pr_err(&quot;\&quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs\&quot;&quot;
 		&quot; disables this message.\n&quot;);
 	sched_show_task(t);
<span class="p_del">-	debug_show_held_locks(t);</span>
<span class="p_add">+	debug_show_all_locks();</span>
 
 	touch_nmi_watchdog();
 
<span class="p_header">diff --git a/kernel/locking/Makefile b/kernel/locking/Makefile</span>
<span class="p_header">index 31322a4275cd..6f88e352cd4f 100644</span>
<span class="p_header">--- a/kernel/locking/Makefile</span>
<span class="p_header">+++ b/kernel/locking/Makefile</span>
<span class="p_chunk">@@ -18,7 +18,6 @@</span> <span class="p_context"> obj-$(CONFIG_LOCKDEP) += lockdep_proc.o</span>
 endif
 obj-$(CONFIG_SMP) += spinlock.o
 obj-$(CONFIG_LOCK_SPIN_ON_OWNER) += osq_lock.o
<span class="p_del">-obj-$(CONFIG_SMP) += lglock.o</span>
 obj-$(CONFIG_PROVE_LOCKING) += spinlock.o
 obj-$(CONFIG_QUEUED_SPINLOCKS) += qspinlock.o
 obj-$(CONFIG_RT_MUTEXES) += rtmutex.o
<span class="p_header">diff --git a/kernel/locking/lglock.c b/kernel/locking/lglock.c</span>
deleted file mode 100644
<span class="p_header">index 951cfcd10b4a..000000000000</span>
<span class="p_header">--- a/kernel/locking/lglock.c</span>
<span class="p_header">+++ /dev/null</span>
<span class="p_chunk">@@ -1,111 +0,0 @@</span> <span class="p_context"></span>
<span class="p_del">-/* See include/linux/lglock.h for description */</span>
<span class="p_del">-#include &lt;linux/module.h&gt;</span>
<span class="p_del">-#include &lt;linux/lglock.h&gt;</span>
<span class="p_del">-#include &lt;linux/cpu.h&gt;</span>
<span class="p_del">-#include &lt;linux/string.h&gt;</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Note there is no uninit, so lglocks cannot be defined in</span>
<span class="p_del">- * modules (but it&#39;s fine to use them from there)</span>
<span class="p_del">- * Could be added though, just undo lg_lock_init</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_lock_init(struct lglock *lg, char *name)</span>
<span class="p_del">-{</span>
<span class="p_del">-	LOCKDEP_INIT_MAP(&amp;lg-&gt;lock_dep_map, name, &amp;lg-&gt;lock_key, 0);</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL(lg_lock_init);</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_local_lock(struct lglock *lg)</span>
<span class="p_del">-{</span>
<span class="p_del">-	arch_spinlock_t *lock;</span>
<span class="p_del">-</span>
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-	lock_acquire_shared(&amp;lg-&gt;lock_dep_map, 0, 0, NULL, _RET_IP_);</span>
<span class="p_del">-	lock = this_cpu_ptr(lg-&gt;lock);</span>
<span class="p_del">-	arch_spin_lock(lock);</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL(lg_local_lock);</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_local_unlock(struct lglock *lg)</span>
<span class="p_del">-{</span>
<span class="p_del">-	arch_spinlock_t *lock;</span>
<span class="p_del">-</span>
<span class="p_del">-	lock_release(&amp;lg-&gt;lock_dep_map, 1, _RET_IP_);</span>
<span class="p_del">-	lock = this_cpu_ptr(lg-&gt;lock);</span>
<span class="p_del">-	arch_spin_unlock(lock);</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL(lg_local_unlock);</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_local_lock_cpu(struct lglock *lg, int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	arch_spinlock_t *lock;</span>
<span class="p_del">-</span>
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-	lock_acquire_shared(&amp;lg-&gt;lock_dep_map, 0, 0, NULL, _RET_IP_);</span>
<span class="p_del">-	lock = per_cpu_ptr(lg-&gt;lock, cpu);</span>
<span class="p_del">-	arch_spin_lock(lock);</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL(lg_local_lock_cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_local_unlock_cpu(struct lglock *lg, int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	arch_spinlock_t *lock;</span>
<span class="p_del">-</span>
<span class="p_del">-	lock_release(&amp;lg-&gt;lock_dep_map, 1, _RET_IP_);</span>
<span class="p_del">-	lock = per_cpu_ptr(lg-&gt;lock, cpu);</span>
<span class="p_del">-	arch_spin_unlock(lock);</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL(lg_local_unlock_cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_double_lock(struct lglock *lg, int cpu1, int cpu2)</span>
<span class="p_del">-{</span>
<span class="p_del">-	BUG_ON(cpu1 == cpu2);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* lock in cpu order, just like lg_global_lock */</span>
<span class="p_del">-	if (cpu2 &lt; cpu1)</span>
<span class="p_del">-		swap(cpu1, cpu2);</span>
<span class="p_del">-</span>
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-	lock_acquire_shared(&amp;lg-&gt;lock_dep_map, 0, 0, NULL, _RET_IP_);</span>
<span class="p_del">-	arch_spin_lock(per_cpu_ptr(lg-&gt;lock, cpu1));</span>
<span class="p_del">-	arch_spin_lock(per_cpu_ptr(lg-&gt;lock, cpu2));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_double_unlock(struct lglock *lg, int cpu1, int cpu2)</span>
<span class="p_del">-{</span>
<span class="p_del">-	lock_release(&amp;lg-&gt;lock_dep_map, 1, _RET_IP_);</span>
<span class="p_del">-	arch_spin_unlock(per_cpu_ptr(lg-&gt;lock, cpu1));</span>
<span class="p_del">-	arch_spin_unlock(per_cpu_ptr(lg-&gt;lock, cpu2));</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_global_lock(struct lglock *lg)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int i;</span>
<span class="p_del">-</span>
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-	lock_acquire_exclusive(&amp;lg-&gt;lock_dep_map, 0, 0, NULL, _RET_IP_);</span>
<span class="p_del">-	for_each_possible_cpu(i) {</span>
<span class="p_del">-		arch_spinlock_t *lock;</span>
<span class="p_del">-		lock = per_cpu_ptr(lg-&gt;lock, i);</span>
<span class="p_del">-		arch_spin_lock(lock);</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL(lg_global_lock);</span>
<span class="p_del">-</span>
<span class="p_del">-void lg_global_unlock(struct lglock *lg)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int i;</span>
<span class="p_del">-</span>
<span class="p_del">-	lock_release(&amp;lg-&gt;lock_dep_map, 1, _RET_IP_);</span>
<span class="p_del">-	for_each_possible_cpu(i) {</span>
<span class="p_del">-		arch_spinlock_t *lock;</span>
<span class="p_del">-		lock = per_cpu_ptr(lg-&gt;lock, i);</span>
<span class="p_del">-		arch_spin_unlock(lock);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL(lg_global_unlock);</span>
<span class="p_header">diff --git a/kernel/locking/percpu-rwsem.c b/kernel/locking/percpu-rwsem.c</span>
<span class="p_header">index bec0b647f9cc..ce182599cf2e 100644</span>
<span class="p_header">--- a/kernel/locking/percpu-rwsem.c</span>
<span class="p_header">+++ b/kernel/locking/percpu-rwsem.c</span>
<span class="p_chunk">@@ -8,152 +8,186 @@</span> <span class="p_context"></span>
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/errno.h&gt;
 
<span class="p_del">-int __percpu_init_rwsem(struct percpu_rw_semaphore *brw,</span>
<span class="p_add">+int __percpu_init_rwsem(struct percpu_rw_semaphore *sem,</span>
 			const char *name, struct lock_class_key *rwsem_key)
 {
<span class="p_del">-	brw-&gt;fast_read_ctr = alloc_percpu(int);</span>
<span class="p_del">-	if (unlikely(!brw-&gt;fast_read_ctr))</span>
<span class="p_add">+	sem-&gt;read_count = alloc_percpu(int);</span>
<span class="p_add">+	if (unlikely(!sem-&gt;read_count))</span>
 		return -ENOMEM;
 
 	/* -&gt;rw_sem represents the whole percpu_rw_semaphore for lockdep */
<span class="p_del">-	__init_rwsem(&amp;brw-&gt;rw_sem, name, rwsem_key);</span>
<span class="p_del">-	rcu_sync_init(&amp;brw-&gt;rss, RCU_SCHED_SYNC);</span>
<span class="p_del">-	atomic_set(&amp;brw-&gt;slow_read_ctr, 0);</span>
<span class="p_del">-	init_waitqueue_head(&amp;brw-&gt;write_waitq);</span>
<span class="p_add">+	rcu_sync_init(&amp;sem-&gt;rss, RCU_SCHED_SYNC);</span>
<span class="p_add">+	__init_rwsem(&amp;sem-&gt;rw_sem, name, rwsem_key);</span>
<span class="p_add">+	init_waitqueue_head(&amp;sem-&gt;writer);</span>
<span class="p_add">+	sem-&gt;readers_block = 0;</span>
 	return 0;
 }
 EXPORT_SYMBOL_GPL(__percpu_init_rwsem);
 
<span class="p_del">-void percpu_free_rwsem(struct percpu_rw_semaphore *brw)</span>
<span class="p_add">+void percpu_free_rwsem(struct percpu_rw_semaphore *sem)</span>
 {
 	/*
 	 * XXX: temporary kludge. The error path in alloc_super()
 	 * assumes that percpu_free_rwsem() is safe after kzalloc().
 	 */
<span class="p_del">-	if (!brw-&gt;fast_read_ctr)</span>
<span class="p_add">+	if (!sem-&gt;read_count)</span>
 		return;
 
<span class="p_del">-	rcu_sync_dtor(&amp;brw-&gt;rss);</span>
<span class="p_del">-	free_percpu(brw-&gt;fast_read_ctr);</span>
<span class="p_del">-	brw-&gt;fast_read_ctr = NULL; /* catch use after free bugs */</span>
<span class="p_add">+	rcu_sync_dtor(&amp;sem-&gt;rss);</span>
<span class="p_add">+	free_percpu(sem-&gt;read_count);</span>
<span class="p_add">+	sem-&gt;read_count = NULL; /* catch use after free bugs */</span>
 }
 EXPORT_SYMBOL_GPL(percpu_free_rwsem);
 
<span class="p_del">-/*</span>
<span class="p_del">- * This is the fast-path for down_read/up_read. If it succeeds we rely</span>
<span class="p_del">- * on the barriers provided by rcu_sync_enter/exit; see the comments in</span>
<span class="p_del">- * percpu_down_write() and percpu_up_write().</span>
<span class="p_del">- *</span>
<span class="p_del">- * If this helper fails the callers rely on the normal rw_semaphore and</span>
<span class="p_del">- * atomic_dec_and_test(), so in this case we have the necessary barriers.</span>
<span class="p_del">- */</span>
<span class="p_del">-static bool update_fast_ctr(struct percpu_rw_semaphore *brw, unsigned int val)</span>
<span class="p_add">+int __percpu_down_read(struct percpu_rw_semaphore *sem, int try)</span>
 {
<span class="p_del">-	bool success;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Due to having preemption disabled the decrement happens on</span>
<span class="p_add">+	 * the same CPU as the increment, avoiding the</span>
<span class="p_add">+	 * increment-on-one-CPU-and-decrement-on-another problem.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * If the reader misses the writer&#39;s assignment of readers_block, then</span>
<span class="p_add">+	 * the writer is guaranteed to see the reader&#39;s increment.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Conversely, any readers that increment their sem-&gt;read_count after</span>
<span class="p_add">+	 * the writer looks are guaranteed to see the readers_block value,</span>
<span class="p_add">+	 * which in turn means that they are guaranteed to immediately</span>
<span class="p_add">+	 * decrement their sem-&gt;read_count, so that it doesn&#39;t matter that the</span>
<span class="p_add">+	 * writer missed them.</span>
<span class="p_add">+	 */</span>
 
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-	success = rcu_sync_is_idle(&amp;brw-&gt;rss);</span>
<span class="p_del">-	if (likely(success))</span>
<span class="p_del">-		__this_cpu_add(*brw-&gt;fast_read_ctr, val);</span>
<span class="p_del">-	preempt_enable();</span>
<span class="p_add">+	smp_mb(); /* A matches D */</span>
 
<span class="p_del">-	return success;</span>
<span class="p_del">-}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If !readers_block the critical section starts here, matched by the</span>
<span class="p_add">+	 * release in percpu_up_write().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (likely(!smp_load_acquire(&amp;sem-&gt;readers_block)))</span>
<span class="p_add">+		return 1;</span>
 
<span class="p_del">-/*</span>
<span class="p_del">- * Like the normal down_read() this is not recursive, the writer can</span>
<span class="p_del">- * come after the first percpu_down_read() and create the deadlock.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Note: returns with lock_is_held(brw-&gt;rw_sem) == T for lockdep,</span>
<span class="p_del">- * percpu_up_read() does rwsem_release(). This pairs with the usage</span>
<span class="p_del">- * of -&gt;rw_sem in percpu_down/up_write().</span>
<span class="p_del">- */</span>
<span class="p_del">-void percpu_down_read(struct percpu_rw_semaphore *brw)</span>
<span class="p_del">-{</span>
<span class="p_del">-	might_sleep();</span>
<span class="p_del">-	rwsem_acquire_read(&amp;brw-&gt;rw_sem.dep_map, 0, 0, _RET_IP_);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Per the above comment; we still have preemption disabled and</span>
<span class="p_add">+	 * will thus decrement on the same CPU as we incremented.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	__percpu_up_read(sem);</span>
 
<span class="p_del">-	if (likely(update_fast_ctr(brw, +1)))</span>
<span class="p_del">-		return;</span>
<span class="p_add">+	if (try)</span>
<span class="p_add">+		return 0;</span>
 
<span class="p_del">-	/* Avoid rwsem_acquire_read() and rwsem_release() */</span>
<span class="p_del">-	__down_read(&amp;brw-&gt;rw_sem);</span>
<span class="p_del">-	atomic_inc(&amp;brw-&gt;slow_read_ctr);</span>
<span class="p_del">-	__up_read(&amp;brw-&gt;rw_sem);</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL_GPL(percpu_down_read);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We either call schedule() in the wait, or we&#39;ll fall through</span>
<span class="p_add">+	 * and reschedule on the preempt_enable() in percpu_down_read().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	preempt_enable_no_resched();</span>
 
<span class="p_del">-int percpu_down_read_trylock(struct percpu_rw_semaphore *brw)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (unlikely(!update_fast_ctr(brw, +1))) {</span>
<span class="p_del">-		if (!__down_read_trylock(&amp;brw-&gt;rw_sem))</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-		atomic_inc(&amp;brw-&gt;slow_read_ctr);</span>
<span class="p_del">-		__up_read(&amp;brw-&gt;rw_sem);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	rwsem_acquire_read(&amp;brw-&gt;rw_sem.dep_map, 0, 1, _RET_IP_);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Avoid lockdep for the down/up_read() we already have them.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	__down_read(&amp;sem-&gt;rw_sem);</span>
<span class="p_add">+	this_cpu_inc(*sem-&gt;read_count);</span>
<span class="p_add">+	__up_read(&amp;sem-&gt;rw_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+	preempt_disable();</span>
 	return 1;
 }
<span class="p_add">+EXPORT_SYMBOL_GPL(__percpu_down_read);</span>
 
<span class="p_del">-void percpu_up_read(struct percpu_rw_semaphore *brw)</span>
<span class="p_add">+void __percpu_up_read(struct percpu_rw_semaphore *sem)</span>
 {
<span class="p_del">-	rwsem_release(&amp;brw-&gt;rw_sem.dep_map, 1, _RET_IP_);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (likely(update_fast_ctr(brw, -1)))</span>
<span class="p_del">-		return;</span>
<span class="p_add">+	smp_mb(); /* B matches C */</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * In other words, if they see our decrement (presumably to aggregate</span>
<span class="p_add">+	 * zero, as that is the only time it matters) they will also see our</span>
<span class="p_add">+	 * critical section.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	__this_cpu_dec(*sem-&gt;read_count);</span>
 
<span class="p_del">-	/* false-positive is possible but harmless */</span>
<span class="p_del">-	if (atomic_dec_and_test(&amp;brw-&gt;slow_read_ctr))</span>
<span class="p_del">-		wake_up_all(&amp;brw-&gt;write_waitq);</span>
<span class="p_add">+	/* Prod writer to recheck readers_active */</span>
<span class="p_add">+	wake_up(&amp;sem-&gt;writer);</span>
 }
<span class="p_del">-EXPORT_SYMBOL_GPL(percpu_up_read);</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(__percpu_up_read);</span>
<span class="p_add">+</span>
<span class="p_add">+#define per_cpu_sum(var)						\</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	typeof(var) __sum = 0;						\</span>
<span class="p_add">+	int cpu;							\</span>
<span class="p_add">+	compiletime_assert_atomic_type(__sum);				\</span>
<span class="p_add">+	for_each_possible_cpu(cpu)					\</span>
<span class="p_add">+		__sum += per_cpu(var, cpu);				\</span>
<span class="p_add">+	__sum;								\</span>
<span class="p_add">+})</span>
 
<span class="p_del">-static int clear_fast_ctr(struct percpu_rw_semaphore *brw)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Return true if the modular sum of the sem-&gt;read_count per-CPU variable is</span>
<span class="p_add">+ * zero.  If this sum is zero, then it is stable due to the fact that if any</span>
<span class="p_add">+ * newly arriving readers increment a given counter, they will immediately</span>
<span class="p_add">+ * decrement that same counter.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static bool readers_active_check(struct percpu_rw_semaphore *sem)</span>
 {
<span class="p_del">-	unsigned int sum = 0;</span>
<span class="p_del">-	int cpu;</span>
<span class="p_add">+	if (per_cpu_sum(*sem-&gt;read_count) != 0)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we observed the decrement; ensure we see the entire critical</span>
<span class="p_add">+	 * section.</span>
<span class="p_add">+	 */</span>
 
<span class="p_del">-	for_each_possible_cpu(cpu) {</span>
<span class="p_del">-		sum += per_cpu(*brw-&gt;fast_read_ctr, cpu);</span>
<span class="p_del">-		per_cpu(*brw-&gt;fast_read_ctr, cpu) = 0;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	smp_mb(); /* C matches B */</span>
 
<span class="p_del">-	return sum;</span>
<span class="p_add">+	return true;</span>
 }
 
<span class="p_del">-void percpu_down_write(struct percpu_rw_semaphore *brw)</span>
<span class="p_add">+void percpu_down_write(struct percpu_rw_semaphore *sem)</span>
 {
<span class="p_add">+	/* Notify readers to take the slow path. */</span>
<span class="p_add">+	rcu_sync_enter(&amp;sem-&gt;rss);</span>
<span class="p_add">+</span>
<span class="p_add">+	down_write(&amp;sem-&gt;rw_sem);</span>
<span class="p_add">+</span>
 	/*
<span class="p_del">-	 * Make rcu_sync_is_idle() == F and thus disable the fast-path in</span>
<span class="p_del">-	 * percpu_down_read() and percpu_up_read(), and wait for gp pass.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * The latter synchronises us with the preceding readers which used</span>
<span class="p_del">-	 * the fast-past, so we can not miss the result of __this_cpu_add()</span>
<span class="p_del">-	 * or anything else inside their criticial sections.</span>
<span class="p_add">+	 * Notify new readers to block; up until now, and thus throughout the</span>
<span class="p_add">+	 * longish rcu_sync_enter() above, new readers could still come in.</span>
 	 */
<span class="p_del">-	rcu_sync_enter(&amp;brw-&gt;rss);</span>
<span class="p_add">+	WRITE_ONCE(sem-&gt;readers_block, 1);</span>
 
<span class="p_del">-	/* exclude other writers, and block the new readers completely */</span>
<span class="p_del">-	down_write(&amp;brw-&gt;rw_sem);</span>
<span class="p_add">+	smp_mb(); /* D matches A */</span>
 
<span class="p_del">-	/* nobody can use fast_read_ctr, move its sum into slow_read_ctr */</span>
<span class="p_del">-	atomic_add(clear_fast_ctr(brw), &amp;brw-&gt;slow_read_ctr);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If they don&#39;t see our writer of readers_block, then we are</span>
<span class="p_add">+	 * guaranteed to see their sem-&gt;read_count increment, and therefore</span>
<span class="p_add">+	 * will wait for them.</span>
<span class="p_add">+	 */</span>
 
<span class="p_del">-	/* wait for all readers to complete their percpu_up_read() */</span>
<span class="p_del">-	wait_event(brw-&gt;write_waitq, !atomic_read(&amp;brw-&gt;slow_read_ctr));</span>
<span class="p_add">+	/* Wait for all now active readers to complete. */</span>
<span class="p_add">+	wait_event(sem-&gt;writer, readers_active_check(sem));</span>
 }
 EXPORT_SYMBOL_GPL(percpu_down_write);
 
<span class="p_del">-void percpu_up_write(struct percpu_rw_semaphore *brw)</span>
<span class="p_add">+void percpu_up_write(struct percpu_rw_semaphore *sem)</span>
 {
<span class="p_del">-	/* release the lock, but the readers can&#39;t use the fast-path */</span>
<span class="p_del">-	up_write(&amp;brw-&gt;rw_sem);</span>
 	/*
<span class="p_del">-	 * Enable the fast-path in percpu_down_read() and percpu_up_read()</span>
<span class="p_del">-	 * but only after another gp pass; this adds the necessary barrier</span>
<span class="p_del">-	 * to ensure the reader can&#39;t miss the changes done by us.</span>
<span class="p_add">+	 * Signal the writer is done, no fast path yet.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * One reason that we cannot just immediately flip to readers_fast is</span>
<span class="p_add">+	 * that new readers might fail to see the results of this writer&#39;s</span>
<span class="p_add">+	 * critical section.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Therefore we force it through the slow path which guarantees an</span>
<span class="p_add">+	 * acquire and thereby guarantees the critical section&#39;s consistency.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	smp_store_release(&amp;sem-&gt;readers_block, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Release the write lock, this will allow readers back in the game.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	up_write(&amp;sem-&gt;rw_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Once this completes (at least one RCU-sched grace period hence) the</span>
<span class="p_add">+	 * reader fast path will be available again. Safe to use outside the</span>
<span class="p_add">+	 * exclusive write lock because its counting.</span>
 	 */
<span class="p_del">-	rcu_sync_exit(&amp;brw-&gt;rss);</span>
<span class="p_add">+	rcu_sync_exit(&amp;sem-&gt;rss);</span>
 }
 EXPORT_SYMBOL_GPL(percpu_up_write);
<span class="p_header">diff --git a/kernel/locking/qspinlock_paravirt.h b/kernel/locking/qspinlock_paravirt.h</span>
<span class="p_header">index 8a99abf58080..e3b5520005db 100644</span>
<span class="p_header">--- a/kernel/locking/qspinlock_paravirt.h</span>
<span class="p_header">+++ b/kernel/locking/qspinlock_paravirt.h</span>
<span class="p_chunk">@@ -70,11 +70,14 @@</span> <span class="p_context"> struct pv_node {</span>
 static inline bool pv_queued_spin_steal_lock(struct qspinlock *lock)
 {
 	struct __qspinlock *l = (void *)lock;
<span class="p_del">-	int ret = !(atomic_read(&amp;lock-&gt;val) &amp; _Q_LOCKED_PENDING_MASK) &amp;&amp;</span>
<span class="p_del">-		   (cmpxchg(&amp;l-&gt;locked, 0, _Q_LOCKED_VAL) == 0);</span>
 
<span class="p_del">-	qstat_inc(qstat_pv_lock_stealing, ret);</span>
<span class="p_del">-	return ret;</span>
<span class="p_add">+	if (!(atomic_read(&amp;lock-&gt;val) &amp; _Q_LOCKED_PENDING_MASK) &amp;&amp;</span>
<span class="p_add">+	    (cmpxchg(&amp;l-&gt;locked, 0, _Q_LOCKED_VAL) == 0)) {</span>
<span class="p_add">+		qstat_inc(qstat_pv_lock_stealing, true);</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return false;</span>
 }
 
 /*
<span class="p_chunk">@@ -257,7 +260,6 @@</span> <span class="p_context"> static struct pv_node *pv_unhash(struct qspinlock *lock)</span>
 static inline bool
 pv_wait_early(struct pv_node *prev, int loop)
 {
<span class="p_del">-</span>
 	if ((loop &amp; PV_PREV_CHECK_MASK) != 0)
 		return false;
 
<span class="p_chunk">@@ -286,12 +288,10 @@</span> <span class="p_context"> static void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)</span>
 {
 	struct pv_node *pn = (struct pv_node *)node;
 	struct pv_node *pp = (struct pv_node *)prev;
<span class="p_del">-	int waitcnt = 0;</span>
 	int loop;
 	bool wait_early;
 
<span class="p_del">-	/* waitcnt processing will be compiled out if !QUEUED_LOCK_STAT */</span>
<span class="p_del">-	for (;; waitcnt++) {</span>
<span class="p_add">+	for (;;) {</span>
 		for (wait_early = false, loop = SPIN_THRESHOLD; loop; loop--) {
 			if (READ_ONCE(node-&gt;locked))
 				return;
<span class="p_chunk">@@ -315,7 +315,6 @@</span> <span class="p_context"> static void pv_wait_node(struct mcs_spinlock *node, struct mcs_spinlock *prev)</span>
 
 		if (!READ_ONCE(node-&gt;locked)) {
 			qstat_inc(qstat_pv_wait_node, true);
<span class="p_del">-			qstat_inc(qstat_pv_wait_again, waitcnt);</span>
 			qstat_inc(qstat_pv_wait_early, wait_early);
 			pv_wait(&amp;pn-&gt;state, vcpu_halted);
 		}
<span class="p_chunk">@@ -456,12 +455,9 @@</span> <span class="p_context"> pv_wait_head_or_lock(struct qspinlock *lock, struct mcs_spinlock *node)</span>
 		pv_wait(&amp;l-&gt;locked, _Q_SLOW_VAL);
 
 		/*
<span class="p_del">-		 * The unlocker should have freed the lock before kicking the</span>
<span class="p_del">-		 * CPU. So if the lock is still not free, it is a spurious</span>
<span class="p_del">-		 * wakeup or another vCPU has stolen the lock. The current</span>
<span class="p_del">-		 * vCPU should spin again.</span>
<span class="p_add">+		 * Because of lock stealing, the queue head vCPU may not be</span>
<span class="p_add">+		 * able to acquire the lock before it has to wait again.</span>
 		 */
<span class="p_del">-		qstat_inc(qstat_pv_spurious_wakeup, READ_ONCE(l-&gt;locked));</span>
 	}
 
 	/*
<span class="p_chunk">@@ -544,7 +540,7 @@</span> <span class="p_context"> __visible void __pv_queued_spin_unlock(struct qspinlock *lock)</span>
 	 * unhash. Otherwise it would be possible to have multiple @lock
 	 * entries, which would be BAD.
 	 */
<span class="p_del">-	locked = cmpxchg(&amp;l-&gt;locked, _Q_LOCKED_VAL, 0);</span>
<span class="p_add">+	locked = cmpxchg_release(&amp;l-&gt;locked, _Q_LOCKED_VAL, 0);</span>
 	if (likely(locked == _Q_LOCKED_VAL))
 		return;
 
<span class="p_header">diff --git a/kernel/locking/qspinlock_stat.h b/kernel/locking/qspinlock_stat.h</span>
<span class="p_header">index b9d031516254..eb0a599fcf58 100644</span>
<span class="p_header">--- a/kernel/locking/qspinlock_stat.h</span>
<span class="p_header">+++ b/kernel/locking/qspinlock_stat.h</span>
<span class="p_chunk">@@ -24,8 +24,8 @@</span> <span class="p_context"></span>
  *   pv_latency_wake	- average latency (ns) from vCPU kick to wakeup
  *   pv_lock_slowpath	- # of locking operations via the slowpath
  *   pv_lock_stealing	- # of lock stealing operations
<span class="p_del">- *   pv_spurious_wakeup	- # of spurious wakeups</span>
<span class="p_del">- *   pv_wait_again	- # of vCPU wait&#39;s that happened after a vCPU kick</span>
<span class="p_add">+ *   pv_spurious_wakeup	- # of spurious wakeups in non-head vCPUs</span>
<span class="p_add">+ *   pv_wait_again	- # of wait&#39;s after a queue head vCPU kick</span>
  *   pv_wait_early	- # of early vCPU wait&#39;s
  *   pv_wait_head	- # of vCPU wait&#39;s at the queue head
  *   pv_wait_node	- # of vCPU wait&#39;s at a non-head queue node
<span class="p_header">diff --git a/kernel/locking/rwsem-xadd.c b/kernel/locking/rwsem-xadd.c</span>
<span class="p_header">index 447e08de1fab..2337b4bb2366 100644</span>
<span class="p_header">--- a/kernel/locking/rwsem-xadd.c</span>
<span class="p_header">+++ b/kernel/locking/rwsem-xadd.c</span>
<span class="p_chunk">@@ -121,16 +121,19 @@</span> <span class="p_context"> enum rwsem_wake_type {</span>
  * - woken process blocks are discarded from the list after having task zeroed
  * - writers are only marked woken if downgrading is false
  */
<span class="p_del">-static struct rw_semaphore *</span>
<span class="p_del">-__rwsem_mark_wake(struct rw_semaphore *sem,</span>
<span class="p_del">-		  enum rwsem_wake_type wake_type, struct wake_q_head *wake_q)</span>
<span class="p_add">+static void __rwsem_mark_wake(struct rw_semaphore *sem,</span>
<span class="p_add">+			      enum rwsem_wake_type wake_type,</span>
<span class="p_add">+			      struct wake_q_head *wake_q)</span>
 {
<span class="p_del">-	struct rwsem_waiter *waiter;</span>
<span class="p_del">-	struct task_struct *tsk;</span>
<span class="p_del">-	struct list_head *next;</span>
<span class="p_del">-	long oldcount, woken, loop, adjustment;</span>
<span class="p_add">+	struct rwsem_waiter *waiter, *tmp;</span>
<span class="p_add">+	long oldcount, woken = 0, adjustment = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Take a peek at the queue head waiter such that we can determine</span>
<span class="p_add">+	 * the wakeup(s) to perform.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	waiter = list_first_entry(&amp;sem-&gt;wait_list, struct rwsem_waiter, list);</span>
 
<span class="p_del">-	waiter = list_entry(sem-&gt;wait_list.next, struct rwsem_waiter, list);</span>
 	if (waiter-&gt;type == RWSEM_WAITING_FOR_WRITE) {
 		if (wake_type == RWSEM_WAKE_ANY) {
 			/*
<span class="p_chunk">@@ -142,19 +145,19 @@</span> <span class="p_context"> __rwsem_mark_wake(struct rw_semaphore *sem,</span>
 			 */
 			wake_q_add(wake_q, waiter-&gt;task);
 		}
<span class="p_del">-		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+		return;</span>
 	}
 
<span class="p_del">-	/* Writers might steal the lock before we grant it to the next reader.</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Writers might steal the lock before we grant it to the next reader.</span>
 	 * We prefer to do the first reader grant before counting readers
 	 * so we can bail out early if a writer stole the lock.
 	 */
<span class="p_del">-	adjustment = 0;</span>
 	if (wake_type != RWSEM_WAKE_READ_OWNED) {
 		adjustment = RWSEM_ACTIVE_READ_BIAS;
  try_reader_grant:
 		oldcount = atomic_long_fetch_add(adjustment, &amp;sem-&gt;count);
<span class="p_del">-</span>
 		if (unlikely(oldcount &lt; RWSEM_WAITING_BIAS)) {
 			/*
 			 * If the count is still less than RWSEM_WAITING_BIAS
<span class="p_chunk">@@ -164,7 +167,8 @@</span> <span class="p_context"> __rwsem_mark_wake(struct rw_semaphore *sem,</span>
 			 */
 			if (atomic_long_add_return(-adjustment, &amp;sem-&gt;count) &lt;
 			    RWSEM_WAITING_BIAS)
<span class="p_del">-				goto out;</span>
<span class="p_add">+				return;</span>
<span class="p_add">+</span>
 			/* Last active locker left. Retry waking readers. */
 			goto try_reader_grant;
 		}
<span class="p_chunk">@@ -176,38 +180,23 @@</span> <span class="p_context"> __rwsem_mark_wake(struct rw_semaphore *sem,</span>
 		rwsem_set_reader_owned(sem);
 	}
 
<span class="p_del">-	/* Grant an infinite number of read locks to the readers at the front</span>
<span class="p_del">-	 * of the queue.  Note we increment the &#39;active part&#39; of the count by</span>
<span class="p_del">-	 * the number of readers before waking any processes up.</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Grant an infinite number of read locks to the readers at the front</span>
<span class="p_add">+	 * of the queue. We know that woken will be at least 1 as we accounted</span>
<span class="p_add">+	 * for above. Note we increment the &#39;active part&#39; of the count by the</span>
<span class="p_add">+	 * number of readers before waking any processes up.</span>
 	 */
<span class="p_del">-	woken = 0;</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		woken++;</span>
<span class="p_add">+	list_for_each_entry_safe(waiter, tmp, &amp;sem-&gt;wait_list, list) {</span>
<span class="p_add">+		struct task_struct *tsk;</span>
 
<span class="p_del">-		if (waiter-&gt;list.next == &amp;sem-&gt;wait_list)</span>
<span class="p_add">+		if (waiter-&gt;type == RWSEM_WAITING_FOR_WRITE)</span>
 			break;
 
<span class="p_del">-		waiter = list_entry(waiter-&gt;list.next,</span>
<span class="p_del">-					struct rwsem_waiter, list);</span>
<span class="p_del">-</span>
<span class="p_del">-	} while (waiter-&gt;type != RWSEM_WAITING_FOR_WRITE);</span>
<span class="p_del">-</span>
<span class="p_del">-	adjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;</span>
<span class="p_del">-	if (waiter-&gt;type != RWSEM_WAITING_FOR_WRITE)</span>
<span class="p_del">-		/* hit end of list above */</span>
<span class="p_del">-		adjustment -= RWSEM_WAITING_BIAS;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (adjustment)</span>
<span class="p_del">-		atomic_long_add(adjustment, &amp;sem-&gt;count);</span>
<span class="p_del">-</span>
<span class="p_del">-	next = sem-&gt;wait_list.next;</span>
<span class="p_del">-	loop = woken;</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		waiter = list_entry(next, struct rwsem_waiter, list);</span>
<span class="p_del">-		next = waiter-&gt;list.next;</span>
<span class="p_add">+		woken++;</span>
 		tsk = waiter-&gt;task;
 
 		wake_q_add(wake_q, tsk);
<span class="p_add">+		list_del(&amp;waiter-&gt;list);</span>
 		/*
 		 * Ensure that the last operation is setting the reader
 		 * waiter to nil such that rwsem_down_read_failed() cannot
<span class="p_chunk">@@ -215,13 +204,16 @@</span> <span class="p_context"> __rwsem_mark_wake(struct rw_semaphore *sem,</span>
 		 * to the task to wakeup.
 		 */
 		smp_store_release(&amp;waiter-&gt;task, NULL);
<span class="p_del">-	} while (--loop);</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	sem-&gt;wait_list.next = next;</span>
<span class="p_del">-	next-&gt;prev = &amp;sem-&gt;wait_list;</span>
<span class="p_add">+	adjustment = woken * RWSEM_ACTIVE_READ_BIAS - adjustment;</span>
<span class="p_add">+	if (list_empty(&amp;sem-&gt;wait_list)) {</span>
<span class="p_add">+		/* hit end of list above */</span>
<span class="p_add">+		adjustment -= RWSEM_WAITING_BIAS;</span>
<span class="p_add">+	}</span>
 
<span class="p_del">- out:</span>
<span class="p_del">-	return sem;</span>
<span class="p_add">+	if (adjustment)</span>
<span class="p_add">+		atomic_long_add(adjustment, &amp;sem-&gt;count);</span>
 }
 
 /*
<span class="p_chunk">@@ -235,7 +227,6 @@</span> <span class="p_context"> struct rw_semaphore __sched *rwsem_down_read_failed(struct rw_semaphore *sem)</span>
 	struct task_struct *tsk = current;
 	WAKE_Q(wake_q);
 
<span class="p_del">-	/* set up my own style of waitqueue */</span>
 	waiter.task = tsk;
 	waiter.type = RWSEM_WAITING_FOR_READ;
 
<span class="p_chunk">@@ -247,7 +238,8 @@</span> <span class="p_context"> struct rw_semaphore __sched *rwsem_down_read_failed(struct rw_semaphore *sem)</span>
 	/* we&#39;re now waiting on the lock, but no longer actively locking */
 	count = atomic_long_add_return(adjustment, &amp;sem-&gt;count);
 
<span class="p_del">-	/* If there are no active locks, wake the front queued process(es).</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If there are no active locks, wake the front queued process(es).</span>
 	 *
 	 * If there are no writers and we are first in the queue,
 	 * wake our own waiter to join the existing active readers !
<span class="p_chunk">@@ -255,7 +247,7 @@</span> <span class="p_context"> struct rw_semaphore __sched *rwsem_down_read_failed(struct rw_semaphore *sem)</span>
 	if (count == RWSEM_WAITING_BIAS ||
 	    (count &gt; RWSEM_WAITING_BIAS &amp;&amp;
 	     adjustment != -RWSEM_ACTIVE_READ_BIAS))
<span class="p_del">-		sem = __rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &amp;wake_q);</span>
<span class="p_add">+		__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &amp;wake_q);</span>
 
 	raw_spin_unlock_irq(&amp;sem-&gt;wait_lock);
 	wake_up_q(&amp;wake_q);
<span class="p_chunk">@@ -505,7 +497,7 @@</span> <span class="p_context"> __rwsem_down_write_failed_common(struct rw_semaphore *sem, int state)</span>
 		if (count &gt; RWSEM_WAITING_BIAS) {
 			WAKE_Q(wake_q);
 
<span class="p_del">-			sem = __rwsem_mark_wake(sem, RWSEM_WAKE_READERS, &amp;wake_q);</span>
<span class="p_add">+			__rwsem_mark_wake(sem, RWSEM_WAKE_READERS, &amp;wake_q);</span>
 			/*
 			 * The wakeup is normally called _after_ the wait_lock
 			 * is released, but given that we are proactively waking
<span class="p_chunk">@@ -614,9 +606,8 @@</span> <span class="p_context"> struct rw_semaphore *rwsem_wake(struct rw_semaphore *sem)</span>
 	raw_spin_lock_irqsave(&amp;sem-&gt;wait_lock, flags);
 locked:
 
<span class="p_del">-	/* do nothing if list empty */</span>
 	if (!list_empty(&amp;sem-&gt;wait_list))
<span class="p_del">-		sem = __rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &amp;wake_q);</span>
<span class="p_add">+		__rwsem_mark_wake(sem, RWSEM_WAKE_ANY, &amp;wake_q);</span>
 
 	raw_spin_unlock_irqrestore(&amp;sem-&gt;wait_lock, flags);
 	wake_up_q(&amp;wake_q);
<span class="p_chunk">@@ -638,9 +629,8 @@</span> <span class="p_context"> struct rw_semaphore *rwsem_downgrade_wake(struct rw_semaphore *sem)</span>
 
 	raw_spin_lock_irqsave(&amp;sem-&gt;wait_lock, flags);
 
<span class="p_del">-	/* do nothing if list empty */</span>
 	if (!list_empty(&amp;sem-&gt;wait_list))
<span class="p_del">-		sem = __rwsem_mark_wake(sem, RWSEM_WAKE_READ_OWNED, &amp;wake_q);</span>
<span class="p_add">+		__rwsem_mark_wake(sem, RWSEM_WAKE_READ_OWNED, &amp;wake_q);</span>
 
 	raw_spin_unlock_irqrestore(&amp;sem-&gt;wait_lock, flags);
 	wake_up_q(&amp;wake_q);
<span class="p_header">diff --git a/kernel/rcu/sync.c b/kernel/rcu/sync.c</span>
<span class="p_header">index be922c9f3d37..50d1861f7759 100644</span>
<span class="p_header">--- a/kernel/rcu/sync.c</span>
<span class="p_header">+++ b/kernel/rcu/sync.c</span>
<span class="p_chunk">@@ -68,6 +68,8 @@</span> <span class="p_context"> void rcu_sync_lockdep_assert(struct rcu_sync *rsp)</span>
 	RCU_LOCKDEP_WARN(!gp_ops[rsp-&gt;gp_type].held(),
 			 &quot;suspicious rcu_sync_is_idle() usage&quot;);
 }
<span class="p_add">+</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(rcu_sync_lockdep_assert);</span>
 #endif
 
 /**
<span class="p_chunk">@@ -83,6 +85,18 @@</span> <span class="p_context"> void rcu_sync_init(struct rcu_sync *rsp, enum rcu_sync_type type)</span>
 }
 
 /**
<span class="p_add">+ * Must be called after rcu_sync_init() and before first use.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Ensures rcu_sync_is_idle() returns false and rcu_sync_{enter,exit}()</span>
<span class="p_add">+ * pairs turn into NO-OPs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void rcu_sync_enter_start(struct rcu_sync *rsp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	rsp-&gt;gp_count++;</span>
<span class="p_add">+	rsp-&gt;gp_state = GP_PASSED;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/**</span>
  * rcu_sync_enter() - Force readers onto slowpath
  * @rsp: Pointer to rcu_sync structure to use for synchronization
  *
<span class="p_header">diff --git a/kernel/stop_machine.c b/kernel/stop_machine.c</span>
<span class="p_header">index 4a1ca5f6da7e..ae6f41fb9cba 100644</span>
<span class="p_header">--- a/kernel/stop_machine.c</span>
<span class="p_header">+++ b/kernel/stop_machine.c</span>
<span class="p_chunk">@@ -20,7 +20,6 @@</span> <span class="p_context"></span>
 #include &lt;linux/kallsyms.h&gt;
 #include &lt;linux/smpboot.h&gt;
 #include &lt;linux/atomic.h&gt;
<span class="p_del">-#include &lt;linux/lglock.h&gt;</span>
 #include &lt;linux/nmi.h&gt;
 
 /*
<span class="p_chunk">@@ -47,13 +46,9 @@</span> <span class="p_context"> struct cpu_stopper {</span>
 static DEFINE_PER_CPU(struct cpu_stopper, cpu_stopper);
 static bool stop_machine_initialized = false;
 
<span class="p_del">-/*</span>
<span class="p_del">- * Avoids a race between stop_two_cpus and global stop_cpus, where</span>
<span class="p_del">- * the stoppers could get queued up in reverse order, leading to</span>
<span class="p_del">- * system deadlock. Using an lglock means stop_two_cpus remains</span>
<span class="p_del">- * relatively cheap.</span>
<span class="p_del">- */</span>
<span class="p_del">-DEFINE_STATIC_LGLOCK(stop_cpus_lock);</span>
<span class="p_add">+/* static data for stop_cpus */</span>
<span class="p_add">+static DEFINE_MUTEX(stop_cpus_mutex);</span>
<span class="p_add">+static bool stop_cpus_in_progress;</span>
 
 static void cpu_stop_init_done(struct cpu_stop_done *done, unsigned int nr_todo)
 {
<span class="p_chunk">@@ -230,14 +225,26 @@</span> <span class="p_context"> static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,</span>
 	struct cpu_stopper *stopper1 = per_cpu_ptr(&amp;cpu_stopper, cpu1);
 	struct cpu_stopper *stopper2 = per_cpu_ptr(&amp;cpu_stopper, cpu2);
 	int err;
<span class="p_del">-</span>
<span class="p_del">-	lg_double_lock(&amp;stop_cpus_lock, cpu1, cpu2);</span>
<span class="p_add">+retry:</span>
 	spin_lock_irq(&amp;stopper1-&gt;lock);
 	spin_lock_nested(&amp;stopper2-&gt;lock, SINGLE_DEPTH_NESTING);
 
 	err = -ENOENT;
 	if (!stopper1-&gt;enabled || !stopper2-&gt;enabled)
 		goto unlock;
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Ensure that if we race with __stop_cpus() the stoppers won&#39;t get</span>
<span class="p_add">+	 * queued up in reverse order leading to system deadlock.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We can&#39;t miss stop_cpus_in_progress if queue_stop_cpus_work() has</span>
<span class="p_add">+	 * queued a work on cpu1 but not on cpu2, we hold both locks.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * It can be falsely true but it is safe to spin until it is cleared,</span>
<span class="p_add">+	 * queue_stop_cpus_work() does everything under preempt_disable().</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	err = -EDEADLK;</span>
<span class="p_add">+	if (unlikely(stop_cpus_in_progress))</span>
<span class="p_add">+			goto unlock;</span>
 
 	err = 0;
 	__cpu_stop_queue_work(stopper1, work1);
<span class="p_chunk">@@ -245,8 +252,12 @@</span> <span class="p_context"> static int cpu_stop_queue_two_works(int cpu1, struct cpu_stop_work *work1,</span>
 unlock:
 	spin_unlock(&amp;stopper2-&gt;lock);
 	spin_unlock_irq(&amp;stopper1-&gt;lock);
<span class="p_del">-	lg_double_unlock(&amp;stop_cpus_lock, cpu1, cpu2);</span>
 
<span class="p_add">+	if (unlikely(err == -EDEADLK)) {</span>
<span class="p_add">+		while (stop_cpus_in_progress)</span>
<span class="p_add">+			cpu_relax();</span>
<span class="p_add">+		goto retry;</span>
<span class="p_add">+	}</span>
 	return err;
 }
 /**
<span class="p_chunk">@@ -316,9 +327,6 @@</span> <span class="p_context"> bool stop_one_cpu_nowait(unsigned int cpu, cpu_stop_fn_t fn, void *arg,</span>
 	return cpu_stop_queue_work(cpu, work_buf);
 }
 
<span class="p_del">-/* static data for stop_cpus */</span>
<span class="p_del">-static DEFINE_MUTEX(stop_cpus_mutex);</span>
<span class="p_del">-</span>
 static bool queue_stop_cpus_work(const struct cpumask *cpumask,
 				 cpu_stop_fn_t fn, void *arg,
 				 struct cpu_stop_done *done)
<span class="p_chunk">@@ -332,7 +340,8 @@</span> <span class="p_context"> static bool queue_stop_cpus_work(const struct cpumask *cpumask,</span>
 	 * preempted by a stopper which might wait for other stoppers
 	 * to enter @fn which can lead to deadlock.
 	 */
<span class="p_del">-	lg_global_lock(&amp;stop_cpus_lock);</span>
<span class="p_add">+	preempt_disable();</span>
<span class="p_add">+	stop_cpus_in_progress = true;</span>
 	for_each_cpu(cpu, cpumask) {
 		work = &amp;per_cpu(cpu_stopper.stop_work, cpu);
 		work-&gt;fn = fn;
<span class="p_chunk">@@ -341,7 +350,8 @@</span> <span class="p_context"> static bool queue_stop_cpus_work(const struct cpumask *cpumask,</span>
 		if (cpu_stop_queue_work(cpu, work))
 			queued = true;
 	}
<span class="p_del">-	lg_global_unlock(&amp;stop_cpus_lock);</span>
<span class="p_add">+	stop_cpus_in_progress = false;</span>
<span class="p_add">+	preempt_enable();</span>
 
 	return queued;
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



