
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/2] mm/mmu_notifier: avoid double notification when it is useless v2 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/2] mm/mmu_notifier: avoid double notification when it is useless v2</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 17, 2017, 3:10 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171017031003.7481-2-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10010611/mbox/"
   >mbox</a>
|
   <a href="/patch/10010611/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10010611/">/patch/10010611/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	97DCA601D5 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 17 Oct 2017 03:10:24 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8833C286CA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 17 Oct 2017 03:10:24 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 7C99D28047; Tue, 17 Oct 2017 03:10:24 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3398528047
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 17 Oct 2017 03:10:23 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1758030AbdJQDKT (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 16 Oct 2017 23:10:19 -0400
Received: from mx1.redhat.com ([209.132.183.28]:37176 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1754825AbdJQDKQ (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 16 Oct 2017 23:10:16 -0400
Received: from smtp.corp.redhat.com
	(int-mx06.intmail.prod.int.phx2.redhat.com [10.5.11.16])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 7B43F356F9;
	Tue, 17 Oct 2017 03:10:15 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com 7B43F356F9
Authentication-Results: ext-mx06.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx06.extmail.prod.ext.phx2.redhat.com;
	spf=fail smtp.mailfrom=jglisse@redhat.com
Received: from localhost.localdomain.com (ovpn-120-5.rdu2.redhat.com
	[10.10.120.5])
	by smtp.corp.redhat.com (Postfix) with ESMTP id 70F455C1A1;
	Tue, 17 Oct 2017 03:10:13 +0000 (UTC)
From: jglisse@redhat.com
To: linux-mm@kvack.org
Cc: linux-kernel@vger.kernel.org,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Nadav Amit &lt;nadav.amit@gmail.com&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Joerg Roedel &lt;jroedel@suse.de&gt;,
	Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;,
	David Woodhouse &lt;dwmw2@infradead.org&gt;,
	Alistair Popple &lt;alistair@popple.id.au&gt;,
	Michael Ellerman &lt;mpe@ellerman.id.au&gt;,
	Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;,
	Stephen Rothwell &lt;sfr@canb.auug.org.au&gt;,
	Andrew Donnellan &lt;andrew.donnellan@au1.ibm.com&gt;,
	iommu@lists.linux-foundation.org, linuxppc-dev@lists.ozlabs.org,
	linux-next@vger.kernel.org
Subject: [PATCH 1/2] mm/mmu_notifier: avoid double notification when it is
	useless v2
Date: Mon, 16 Oct 2017 23:10:02 -0400
Message-Id: &lt;20171017031003.7481-2-jglisse@redhat.com&gt;
In-Reply-To: &lt;20171017031003.7481-1-jglisse@redhat.com&gt;
References: &lt;20171017031003.7481-1-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.16
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.30]);
	Tue, 17 Oct 2017 03:10:15 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Oct. 17, 2017, 3:10 a.m.</div>
<pre class="content">
<span class="from">From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>

This patch only affects users of mmu_notifier-&gt;invalidate_range callback
which are device drivers related to ATS/PASID, CAPI, IOMMUv2, SVM ...
and it is an optimization for those users. Everyone else is unaffected
by it.

When clearing a pte/pmd we are given a choice to notify the event under
the page table lock (notify version of *_clear_flush helpers do call the
mmu_notifier_invalidate_range). But that notification is not necessary in
all cases.

This patches remove almost all cases where it is useless to have a call
to mmu_notifier_invalidate_range before mmu_notifier_invalidate_range_end.
It also adds documentation in all those cases explaining why.

Below is a more in depth analysis of why this is fine to do this:

For secondary TLB (non CPU TLB) like IOMMU TLB or device TLB (when device
use thing like ATS/PASID to get the IOMMU to walk the CPU page table to
access a process virtual address space). There is only 2 cases when you
need to notify those secondary TLB while holding page table lock when
clearing a pte/pmd:

  A) page backing address is free before mmu_notifier_invalidate_range_end
  B) a page table entry is updated to point to a new page (COW, write fault
     on zero page, __replace_page(), ...)

Case A is obvious you do not want to take the risk for the device to write
to a page that might now be used by something completely different.

Case B is more subtle. For correctness it requires the following sequence
to happen:
  - take page table lock
  - clear page table entry and notify (pmd/pte_huge_clear_flush_notify())
  - set page table entry to point to new page

If clearing the page table entry is not followed by a notify before setting
the new pte/pmd value then you can break memory model like C11 or C++11 for
the device.

Consider the following scenario (device use a feature similar to ATS/
PASID):

Two address addrA and addrB such that |addrA - addrB| &gt;= PAGE_SIZE we
assume they are write protected for COW (other case of B apply too).

[Time N] -----------------------------------------------------------------
CPU-thread-0  {try to write to addrA}
CPU-thread-1  {try to write to addrB}
CPU-thread-2  {}
CPU-thread-3  {}
DEV-thread-0  {read addrA and populate device TLB}
DEV-thread-2  {read addrB and populate device TLB}
[Time N+1] ---------------------------------------------------------------
CPU-thread-0  {COW_step0: {mmu_notifier_invalidate_range_start(addrA)}}
CPU-thread-1  {COW_step0: {mmu_notifier_invalidate_range_start(addrB)}}
CPU-thread-2  {}
CPU-thread-3  {}
DEV-thread-0  {}
DEV-thread-2  {}
[Time N+2] ---------------------------------------------------------------
CPU-thread-0  {COW_step1: {update page table point to new page for addrA}}
CPU-thread-1  {COW_step1: {update page table point to new page for addrB}}
CPU-thread-2  {}
CPU-thread-3  {}
DEV-thread-0  {}
DEV-thread-2  {}
[Time N+3] ---------------------------------------------------------------
CPU-thread-0  {preempted}
CPU-thread-1  {preempted}
CPU-thread-2  {write to addrA which is a write to new page}
CPU-thread-3  {}
DEV-thread-0  {}
DEV-thread-2  {}
[Time N+3] ---------------------------------------------------------------
CPU-thread-0  {preempted}
CPU-thread-1  {preempted}
CPU-thread-2  {}
CPU-thread-3  {write to addrB which is a write to new page}
DEV-thread-0  {}
DEV-thread-2  {}
[Time N+4] ---------------------------------------------------------------
CPU-thread-0  {preempted}
CPU-thread-1  {COW_step3: {mmu_notifier_invalidate_range_end(addrB)}}
CPU-thread-2  {}
CPU-thread-3  {}
DEV-thread-0  {}
DEV-thread-2  {}
[Time N+5] ---------------------------------------------------------------
CPU-thread-0  {preempted}
CPU-thread-1  {}
CPU-thread-2  {}
CPU-thread-3  {}
DEV-thread-0  {read addrA from old page}
DEV-thread-2  {read addrB from new page}

So here because at time N+2 the clear page table entry was not pair with a
notification to invalidate the secondary TLB, the device see the new value
for addrB before seing the new value for addrA. This break total memory
ordering for the device.

When changing a pte to write protect or to point to a new write protected
page with same content (KSM) it is ok to delay invalidate_range callback to
mmu_notifier_invalidate_range_end() outside the page table lock. This is
true even if the thread doing page table update is preempted right after
releasing page table lock before calling mmu_notifier_invalidate_range_end

Changed since v1:
  - typos (thanks to Andrea)
  - Avoid unnecessary precaution in try_to_unmap() (Andrea)
  - Be more conservative in try_to_unmap_one()
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
Cc: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
Cc: Nadav Amit &lt;nadav.amit@gmail.com&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Joerg Roedel &lt;jroedel@suse.de&gt;
Cc: Suravee Suthikulpanit &lt;suravee.suthikulpanit@amd.com&gt;
Cc: David Woodhouse &lt;dwmw2@infradead.org&gt;
Cc: Alistair Popple &lt;alistair@popple.id.au&gt;
Cc: Michael Ellerman &lt;mpe@ellerman.id.au&gt;
Cc: Benjamin Herrenschmidt &lt;benh@kernel.crashing.org&gt;
Cc: Stephen Rothwell &lt;sfr@canb.auug.org.au&gt;
Cc: Andrew Donnellan &lt;andrew.donnellan@au1.ibm.com&gt;

Cc: iommu@lists.linux-foundation.org
Cc: linuxppc-dev@lists.ozlabs.org
Cc: linux-next@vger.kernel.org
---
 Documentation/vm/mmu_notifier.txt | 93 +++++++++++++++++++++++++++++++++++++++
 fs/dax.c                          |  9 +++-
 include/linux/mmu_notifier.h      |  3 +-
 mm/huge_memory.c                  | 20 +++++++--
 mm/hugetlb.c                      | 16 +++++--
 mm/ksm.c                          | 15 ++++++-
 mm/rmap.c                         | 59 ++++++++++++++++++++++---
 7 files changed, 198 insertions(+), 17 deletions(-)
 create mode 100644 Documentation/vm/mmu_notifier.txt
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - Oct. 19, 2017, 3:04 a.m.</div>
<pre class="content">
On Mon, 16 Oct 2017 23:10:02 -0400
jglisse@redhat.com wrote:
<span class="quote">
&gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt; +		 * downgrading page table protection not changing it to point</span>
<span class="quote">&gt; +		 * to a new page.</span>
<span class="quote">&gt; +		 *</span>
<span class="quote">&gt; +		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt;  		if (pmdp) {</span>
<span class="quote">&gt;  #ifdef CONFIG_FS_DAX_PMD</span>
<span class="quote">&gt;  			pmd_t pmd;</span>
<span class="quote">&gt; @@ -628,7 +635,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt;  			pmd = pmd_wrprotect(pmd);</span>
<span class="quote">&gt;  			pmd = pmd_mkclean(pmd);</span>
<span class="quote">&gt;  			set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd);</span>
<span class="quote">&gt; -			mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>

Could the secondary TLB still see the mapping as dirty and propagate the dirty bit back?
<span class="quote">
&gt;  unlock_pmd:</span>
<span class="quote">&gt;  			spin_unlock(ptl);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; @@ -643,7 +649,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt;  			pte = pte_wrprotect(pte);</span>
<span class="quote">&gt;  			pte = pte_mkclean(pte);</span>
<span class="quote">&gt;  			set_pte_at(vma-&gt;vm_mm, address, ptep, pte);</span>
<span class="quote">&gt; -			mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>

Ditto
<span class="quote">
&gt;  unlock_pte:</span>
<span class="quote">&gt;  			pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; index 6866e8126982..49c925c96b8a 100644</span>
<span class="quote">&gt; --- a/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; +++ b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; @@ -155,7 +155,8 @@ struct mmu_notifier_ops {</span>
<span class="quote">&gt;  	 * shared page-tables, it not necessary to implement the</span>
<span class="quote">&gt;  	 * invalidate_range_start()/end() notifiers, as</span>
<span class="quote">&gt;  	 * invalidate_range() alread catches the points in time when an</span>
<span class="quote">&gt; -	 * external TLB range needs to be flushed.</span>
<span class="quote">&gt; +	 * external TLB range needs to be flushed. For more in depth</span>
<span class="quote">&gt; +	 * discussion on this see Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;  	 *</span>
<span class="quote">&gt;  	 * The invalidate_range() function is called under the ptl</span>
<span class="quote">&gt;  	 * spin-lock and not allowed to sleep.</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index c037d3d34950..ff5bc647b51d 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1186,8 +1186,15 @@ static int do_huge_pmd_wp_page_fallback(struct vm_fault *vmf, pmd_t orig_pmd,</span>
<span class="quote">&gt;  		goto out_free_pages;</span>
<span class="quote">&gt;  	VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Leave pmd empty until pte is filled note we must notify here as</span>
<span class="quote">&gt; +	 * concurrent CPU thread might write to new page before the call to</span>
<span class="quote">&gt; +	 * mmu_notifier_invalidate_range_end() happens which can lead to a</span>
<span class="quote">&gt; +	 * device seeing memory write in different order than CPU.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt;  	pmdp_huge_clear_flush_notify(vma, haddr, vmf-&gt;pmd);</span>
<span class="quote">&gt; -	/* leave pmd empty until pte is filled */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	pgtable = pgtable_trans_huge_withdraw(vma-&gt;vm_mm, vmf-&gt;pmd);</span>
<span class="quote">&gt;  	pmd_populate(vma-&gt;vm_mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; @@ -2026,8 +2033,15 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	pmd_t _pmd;</span>
<span class="quote">&gt;  	int i;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	/* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; -	pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Leave pmd empty until pte is filled note that it is fine to delay</span>
<span class="quote">&gt; +	 * notification until mmu_notifier_invalidate_range_end() as we are</span>
<span class="quote">&gt; +	 * replacing a zero pmd write protected page with a zero pte write</span>
<span class="quote">&gt; +	 * protected page.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	pmdp_huge_clear_flush(vma, haddr, pmd);</span>

Shouldn&#39;t the secondary TLB know if the page size changed?
<span class="quote">
&gt;  </span>
<span class="quote">&gt;  	pgtable = pgtable_trans_huge_withdraw(mm, pmd);</span>
<span class="quote">&gt;  	pmd_populate(mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index 1768efa4c501..63a63f1b536c 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -3254,9 +3254,14 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
<span class="quote">&gt;  			set_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt;  			if (cow) {</span>
<span class="quote">&gt; +				/*</span>
<span class="quote">&gt; +				 * No need to notify as we are downgrading page</span>
<span class="quote">&gt; +				 * table protection not changing it to point</span>
<span class="quote">&gt; +				 * to a new page.</span>
<span class="quote">&gt; +				 *</span>
<span class="quote">&gt; +				 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; +				 */</span>
<span class="quote">&gt;  				huge_ptep_set_wrprotect(src, addr, src_pte);</span>

OK.. so we could get write faults on write accesses from the device.
<span class="quote">
&gt; -				mmu_notifier_invalidate_range(src, mmun_start,</span>
<span class="quote">&gt; -								   mmun_end);</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  			entry = huge_ptep_get(src_pte);</span>
<span class="quote">&gt;  			ptepage = pte_page(entry);</span>
<span class="quote">&gt; @@ -4288,7 +4293,12 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	 * and that page table be reused and filled with junk.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	flush_hugetlb_tlb_range(vma, start, end);</span>
<span class="quote">&gt; -	mmu_notifier_invalidate_range(mm, start, end);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * No need to call mmu_notifier_invalidate_range() we are downgrading</span>
<span class="quote">&gt; +	 * page table protection not changing it to point to a new page.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt;  	i_mmap_unlock_write(vma-&gt;vm_file-&gt;f_mapping);</span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt; index 6cb60f46cce5..be8f4576f842 100644</span>
<span class="quote">&gt; --- a/mm/ksm.c</span>
<span class="quote">&gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt; @@ -1052,8 +1052,13 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt;  		 * So we clear the pte and flush the tlb before the check</span>
<span class="quote">&gt;  		 * this assure us that no O_DIRECT can happen after the check</span>
<span class="quote">&gt;  		 * or in the middle of the check.</span>
<span class="quote">&gt; +		 *</span>
<span class="quote">&gt; +		 * No need to notify as we are downgrading page table to read</span>
<span class="quote">&gt; +		 * only not changing it to point to a new page.</span>
<span class="quote">&gt; +		 *</span>
<span class="quote">&gt; +		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		entry = ptep_clear_flush_notify(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt; +		entry = ptep_clear_flush(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Check that no O_DIRECT or similar I/O is in progress on the</span>
<span class="quote">&gt;  		 * page</span>
<span class="quote">&gt; @@ -1136,7 +1141,13 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	flush_cache_page(vma, addr, pte_pfn(*ptep));</span>
<span class="quote">&gt; -	ptep_clear_flush_notify(vma, addr, ptep);</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * No need to notify as we are replacing a read only page with another</span>
<span class="quote">&gt; +	 * read only page with the same content.</span>
<span class="quote">&gt; +	 *</span>
<span class="quote">&gt; +	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	ptep_clear_flush(vma, addr, ptep);</span>
<span class="quote">&gt;  	set_pte_at_notify(mm, addr, ptep, newpte);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	page_remove_rmap(page, false);</span>
<span class="quote">&gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; index 061826278520..6b5a0f219ac0 100644</span>
<span class="quote">&gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; @@ -937,10 +937,15 @@ static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -		if (ret) {</span>
<span class="quote">&gt; -			mmu_notifier_invalidate_range(vma-&gt;vm_mm, cstart, cend);</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt; +		 * downgrading page table protection not changing it to point</span>
<span class="quote">&gt; +		 * to a new page.</span>
<span class="quote">&gt; +		 *</span>
<span class="quote">&gt; +		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (ret)</span>
<span class="quote">&gt;  			(*cleaned)++;</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; @@ -1424,6 +1429,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt;  				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt;  			set_pte_at(mm, pvmw.address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt; +			 * against the special swap migration pte.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt;  			goto discard;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1481,6 +1490,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			 * will take care of the rest.</span>
<span class="quote">&gt;  			 */</span>
<span class="quote">&gt;  			dec_mm_counter(mm, mm_counter(page));</span>
<span class="quote">&gt; +			/* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt; +			mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; +						      address + PAGE_SIZE);</span>
<span class="quote">&gt;  		} else if (IS_ENABLED(CONFIG_MIGRATION) &amp;&amp;</span>
<span class="quote">&gt;  				(flags &amp; (TTU_MIGRATION|TTU_SPLIT_FREEZE))) {</span>
<span class="quote">&gt;  			swp_entry_t entry;</span>
<span class="quote">&gt; @@ -1496,6 +1508,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt;  				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt;  			set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt; +			 * against the special swap migration pte.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt;  		} else if (PageAnon(page)) {</span>
<span class="quote">&gt;  			swp_entry_t entry = { .val = page_private(subpage) };</span>
<span class="quote">&gt;  			pte_t swp_pte;</span>
<span class="quote">&gt; @@ -1507,6 +1523,8 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				WARN_ON_ONCE(1);</span>
<span class="quote">&gt;  				ret = false;</span>
<span class="quote">&gt;  				/* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt; +				mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; +							address + PAGE_SIZE);</span>
<span class="quote">&gt;  				page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt;  				break;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; @@ -1514,6 +1532,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			/* MADV_FREE page check */</span>
<span class="quote">&gt;  			if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt;  				if (!PageDirty(page)) {</span>
<span class="quote">&gt; +					/* Invalidate as we cleared the pte */</span>
<span class="quote">&gt; +					mmu_notifier_invalidate_range(mm,</span>
<span class="quote">&gt; +						address, address + PAGE_SIZE);</span>
<span class="quote">&gt;  					dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt;  					goto discard;</span>
<span class="quote">&gt;  				}</span>
<span class="quote">&gt; @@ -1547,13 +1568,39 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt;  				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt;  			set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; -		} else</span>
<span class="quote">&gt; +			/* Invalidate as we cleared the pte */</span>
<span class="quote">&gt; +			mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; +						      address + PAGE_SIZE);</span>
<span class="quote">&gt; +		} else {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * We should not need to notify here as we reach this</span>
<span class="quote">&gt; +			 * case only from freeze_page() itself only call from</span>
<span class="quote">&gt; +			 * split_huge_page_to_list() so everything below must</span>
<span class="quote">&gt; +			 * be true:</span>
<span class="quote">&gt; +			 *   - page is not anonymous</span>
<span class="quote">&gt; +			 *   - page is locked</span>
<span class="quote">&gt; +			 *</span>
<span class="quote">&gt; +			 * So as it is a locked file back page thus it can not</span>
<span class="quote">&gt; +			 * be remove from the page cache and replace by a new</span>
<span class="quote">&gt; +			 * page before mmu_notifier_invalidate_range_end so no</span>
<span class="quote">&gt; +			 * concurrent thread might update its page table to</span>
<span class="quote">&gt; +			 * point at new page while a device still is using this</span>
<span class="quote">&gt; +			 * page.</span>
<span class="quote">&gt; +			 *</span>
<span class="quote">&gt; +			 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt;  			dec_mm_counter(mm, mm_counter_file(page));</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  discard:</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * No need to call mmu_notifier_invalidate_range() it has be</span>
<span class="quote">&gt; +		 * done above for all cases requiring it to happen under page</span>
<span class="quote">&gt; +		 * table lock before mmu_notifier_invalidate_range_end()</span>
<span class="quote">&gt; +		 *</span>
<span class="quote">&gt; +		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt;  		page_remove_rmap(subpage, PageHuge(page));</span>
<span class="quote">&gt;  		put_page(page);</span>
<span class="quote">&gt; -		mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; -					      address + PAGE_SIZE);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>

Looking at the patchset, I understand the efficiency, but I am concerned
with correctness.

Balbir Singh.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Oct. 19, 2017, 3:28 a.m.</div>
<pre class="content">
On Thu, Oct 19, 2017 at 02:04:26PM +1100, Balbir Singh wrote:
<span class="quote">&gt; On Mon, 16 Oct 2017 23:10:02 -0400</span>
<span class="quote">&gt; jglisse@redhat.com wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; +		 * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt; &gt; +		 * downgrading page table protection not changing it to point</span>
<span class="quote">&gt; &gt; +		 * to a new page.</span>
<span class="quote">&gt; &gt; +		 *</span>
<span class="quote">&gt; &gt; +		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; +		 */</span>
<span class="quote">&gt; &gt;  		if (pmdp) {</span>
<span class="quote">&gt; &gt;  #ifdef CONFIG_FS_DAX_PMD</span>
<span class="quote">&gt; &gt;  			pmd_t pmd;</span>
<span class="quote">&gt; &gt; @@ -628,7 +635,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt; &gt;  			pmd = pmd_wrprotect(pmd);</span>
<span class="quote">&gt; &gt;  			pmd = pmd_mkclean(pmd);</span>
<span class="quote">&gt; &gt;  			set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd);</span>
<span class="quote">&gt; &gt; -			mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Could the secondary TLB still see the mapping as dirty and propagate the dirty bit back?</span>

I am assuming hardware does sane thing of setting the dirty bit only
when walking the CPU page table when device does a write fault ie
once the device get a write TLB entry the dirty is set by the IOMMU
when walking the page table before returning the lookup result to the
device and that it won&#39;t be set again latter (ie propagated back
latter).

I should probably have spell that out and maybe some of the ATS/PASID
implementer did not do that.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;  unlock_pmd:</span>
<span class="quote">&gt; &gt;  			spin_unlock(ptl);</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt; @@ -643,7 +649,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt; &gt;  			pte = pte_wrprotect(pte);</span>
<span class="quote">&gt; &gt;  			pte = pte_mkclean(pte);</span>
<span class="quote">&gt; &gt;  			set_pte_at(vma-&gt;vm_mm, address, ptep, pte);</span>
<span class="quote">&gt; &gt; -			mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ditto</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;  unlock_pte:</span>
<span class="quote">&gt; &gt;  			pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt; index 6866e8126982..49c925c96b8a 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt; @@ -155,7 +155,8 @@ struct mmu_notifier_ops {</span>
<span class="quote">&gt; &gt;  	 * shared page-tables, it not necessary to implement the</span>
<span class="quote">&gt; &gt;  	 * invalidate_range_start()/end() notifiers, as</span>
<span class="quote">&gt; &gt;  	 * invalidate_range() alread catches the points in time when an</span>
<span class="quote">&gt; &gt; -	 * external TLB range needs to be flushed.</span>
<span class="quote">&gt; &gt; +	 * external TLB range needs to be flushed. For more in depth</span>
<span class="quote">&gt; &gt; +	 * discussion on this see Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;  	 *</span>
<span class="quote">&gt; &gt;  	 * The invalidate_range() function is called under the ptl</span>
<span class="quote">&gt; &gt;  	 * spin-lock and not allowed to sleep.</span>
<span class="quote">&gt; &gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; index c037d3d34950..ff5bc647b51d 100644</span>
<span class="quote">&gt; &gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; @@ -1186,8 +1186,15 @@ static int do_huge_pmd_wp_page_fallback(struct vm_fault *vmf, pmd_t orig_pmd,</span>
<span class="quote">&gt; &gt;  		goto out_free_pages;</span>
<span class="quote">&gt; &gt;  	VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Leave pmd empty until pte is filled note we must notify here as</span>
<span class="quote">&gt; &gt; +	 * concurrent CPU thread might write to new page before the call to</span>
<span class="quote">&gt; &gt; +	 * mmu_notifier_invalidate_range_end() happens which can lead to a</span>
<span class="quote">&gt; &gt; +	 * device seeing memory write in different order than CPU.</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt;  	pmdp_huge_clear_flush_notify(vma, haddr, vmf-&gt;pmd);</span>
<span class="quote">&gt; &gt; -	/* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	pgtable = pgtable_trans_huge_withdraw(vma-&gt;vm_mm, vmf-&gt;pmd);</span>
<span class="quote">&gt; &gt;  	pmd_populate(vma-&gt;vm_mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; &gt; @@ -2026,8 +2033,15 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  	pmd_t _pmd;</span>
<span class="quote">&gt; &gt;  	int i;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -	/* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; &gt; -	pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Leave pmd empty until pte is filled note that it is fine to delay</span>
<span class="quote">&gt; &gt; +	 * notification until mmu_notifier_invalidate_range_end() as we are</span>
<span class="quote">&gt; &gt; +	 * replacing a zero pmd write protected page with a zero pte write</span>
<span class="quote">&gt; &gt; +	 * protected page.</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	pmdp_huge_clear_flush(vma, haddr, pmd);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Shouldn&#39;t the secondary TLB know if the page size changed?</span>

It should not matter, we are talking virtual to physical on behalf
of a device against a process address space. So the hardware should
not care about the page size.

Moreover if any of the new 512 (assuming 2MB huge and 4K pages) zero
4K pages is replace by something new then a device TLB shootdown will
happen before the new page is set.

Only issue i can think of is if the IOMMU TLB (if there is one) or
the device TLB (you do expect that there is one) does not invalidate
TLB entry if the TLB shootdown is smaller than the TLB entry. That
would be idiotic but yes i know hardware bug.
<span class="quote">

&gt; </span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	pgtable = pgtable_trans_huge_withdraw(mm, pmd);</span>
<span class="quote">&gt; &gt;  	pmd_populate(mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; &gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; index 1768efa4c501..63a63f1b536c 100644</span>
<span class="quote">&gt; &gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; @@ -3254,9 +3254,14 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
<span class="quote">&gt; &gt;  			set_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);</span>
<span class="quote">&gt; &gt;  		} else {</span>
<span class="quote">&gt; &gt;  			if (cow) {</span>
<span class="quote">&gt; &gt; +				/*</span>
<span class="quote">&gt; &gt; +				 * No need to notify as we are downgrading page</span>
<span class="quote">&gt; &gt; +				 * table protection not changing it to point</span>
<span class="quote">&gt; &gt; +				 * to a new page.</span>
<span class="quote">&gt; &gt; +				 *</span>
<span class="quote">&gt; &gt; +				 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; +				 */</span>
<span class="quote">&gt; &gt;  				huge_ptep_set_wrprotect(src, addr, src_pte);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK.. so we could get write faults on write accesses from the device.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; -				mmu_notifier_invalidate_range(src, mmun_start,</span>
<span class="quote">&gt; &gt; -								   mmun_end);</span>
<span class="quote">&gt; &gt;  			}</span>
<span class="quote">&gt; &gt;  			entry = huge_ptep_get(src_pte);</span>
<span class="quote">&gt; &gt;  			ptepage = pte_page(entry);</span>
<span class="quote">&gt; &gt; @@ -4288,7 +4293,12 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  	 * and that page table be reused and filled with junk.</span>
<span class="quote">&gt; &gt;  	 */</span>
<span class="quote">&gt; &gt;  	flush_hugetlb_tlb_range(vma, start, end);</span>
<span class="quote">&gt; &gt; -	mmu_notifier_invalidate_range(mm, start, end);</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * No need to call mmu_notifier_invalidate_range() we are downgrading</span>
<span class="quote">&gt; &gt; +	 * page table protection not changing it to point to a new page.</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt;  	i_mmap_unlock_write(vma-&gt;vm_file-&gt;f_mapping);</span>
<span class="quote">&gt; &gt;  	mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt; &gt; index 6cb60f46cce5..be8f4576f842 100644</span>
<span class="quote">&gt; &gt; --- a/mm/ksm.c</span>
<span class="quote">&gt; &gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt; &gt; @@ -1052,8 +1052,13 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt; &gt;  		 * So we clear the pte and flush the tlb before the check</span>
<span class="quote">&gt; &gt;  		 * this assure us that no O_DIRECT can happen after the check</span>
<span class="quote">&gt; &gt;  		 * or in the middle of the check.</span>
<span class="quote">&gt; &gt; +		 *</span>
<span class="quote">&gt; &gt; +		 * No need to notify as we are downgrading page table to read</span>
<span class="quote">&gt; &gt; +		 * only not changing it to point to a new page.</span>
<span class="quote">&gt; &gt; +		 *</span>
<span class="quote">&gt; &gt; +		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;  		 */</span>
<span class="quote">&gt; &gt; -		entry = ptep_clear_flush_notify(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt; &gt; +		entry = ptep_clear_flush(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt; &gt;  		/*</span>
<span class="quote">&gt; &gt;  		 * Check that no O_DIRECT or similar I/O is in progress on the</span>
<span class="quote">&gt; &gt;  		 * page</span>
<span class="quote">&gt; &gt; @@ -1136,7 +1141,13 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	flush_cache_page(vma, addr, pte_pfn(*ptep));</span>
<span class="quote">&gt; &gt; -	ptep_clear_flush_notify(vma, addr, ptep);</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * No need to notify as we are replacing a read only page with another</span>
<span class="quote">&gt; &gt; +	 * read only page with the same content.</span>
<span class="quote">&gt; &gt; +	 *</span>
<span class="quote">&gt; &gt; +	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	ptep_clear_flush(vma, addr, ptep);</span>
<span class="quote">&gt; &gt;  	set_pte_at_notify(mm, addr, ptep, newpte);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	page_remove_rmap(page, false);</span>
<span class="quote">&gt; &gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; &gt; index 061826278520..6b5a0f219ac0 100644</span>
<span class="quote">&gt; &gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; &gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; &gt; @@ -937,10 +937,15 @@ static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; -		if (ret) {</span>
<span class="quote">&gt; &gt; -			mmu_notifier_invalidate_range(vma-&gt;vm_mm, cstart, cend);</span>
<span class="quote">&gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; +		 * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt; &gt; +		 * downgrading page table protection not changing it to point</span>
<span class="quote">&gt; &gt; +		 * to a new page.</span>
<span class="quote">&gt; &gt; +		 *</span>
<span class="quote">&gt; &gt; +		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; +		 */</span>
<span class="quote">&gt; &gt; +		if (ret)</span>
<span class="quote">&gt; &gt;  			(*cleaned)++;</span>
<span class="quote">&gt; &gt; -		}</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt; @@ -1424,6 +1429,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  			if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt;  				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt;  			set_pte_at(mm, pvmw.address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt; &gt; +			 * against the special swap migration pte.</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt;  			goto discard;</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -1481,6 +1490,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  			 * will take care of the rest.</span>
<span class="quote">&gt; &gt;  			 */</span>
<span class="quote">&gt; &gt;  			dec_mm_counter(mm, mm_counter(page));</span>
<span class="quote">&gt; &gt; +			/* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; +			mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; +						      address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt;  		} else if (IS_ENABLED(CONFIG_MIGRATION) &amp;&amp;</span>
<span class="quote">&gt; &gt;  				(flags &amp; (TTU_MIGRATION|TTU_SPLIT_FREEZE))) {</span>
<span class="quote">&gt; &gt;  			swp_entry_t entry;</span>
<span class="quote">&gt; &gt; @@ -1496,6 +1508,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  			if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt;  				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt;  			set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt; &gt; +			 * against the special swap migration pte.</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt;  		} else if (PageAnon(page)) {</span>
<span class="quote">&gt; &gt;  			swp_entry_t entry = { .val = page_private(subpage) };</span>
<span class="quote">&gt; &gt;  			pte_t swp_pte;</span>
<span class="quote">&gt; &gt; @@ -1507,6 +1523,8 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  				WARN_ON_ONCE(1);</span>
<span class="quote">&gt; &gt;  				ret = false;</span>
<span class="quote">&gt; &gt;  				/* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; +				mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; +							address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt;  				page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt;  				break;</span>
<span class="quote">&gt; &gt;  			}</span>
<span class="quote">&gt; &gt; @@ -1514,6 +1532,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  			/* MADV_FREE page check */</span>
<span class="quote">&gt; &gt;  			if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt; &gt;  				if (!PageDirty(page)) {</span>
<span class="quote">&gt; &gt; +					/* Invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; +					mmu_notifier_invalidate_range(mm,</span>
<span class="quote">&gt; &gt; +						address, address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt;  					dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt;  					goto discard;</span>
<span class="quote">&gt; &gt;  				}</span>
<span class="quote">&gt; &gt; @@ -1547,13 +1568,39 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  			if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt;  				swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt;  			set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt; -		} else</span>
<span class="quote">&gt; &gt; +			/* Invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; +			mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; +						      address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; +		} else {</span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * We should not need to notify here as we reach this</span>
<span class="quote">&gt; &gt; +			 * case only from freeze_page() itself only call from</span>
<span class="quote">&gt; &gt; +			 * split_huge_page_to_list() so everything below must</span>
<span class="quote">&gt; &gt; +			 * be true:</span>
<span class="quote">&gt; &gt; +			 *   - page is not anonymous</span>
<span class="quote">&gt; &gt; +			 *   - page is locked</span>
<span class="quote">&gt; &gt; +			 *</span>
<span class="quote">&gt; &gt; +			 * So as it is a locked file back page thus it can not</span>
<span class="quote">&gt; &gt; +			 * be remove from the page cache and replace by a new</span>
<span class="quote">&gt; &gt; +			 * page before mmu_notifier_invalidate_range_end so no</span>
<span class="quote">&gt; &gt; +			 * concurrent thread might update its page table to</span>
<span class="quote">&gt; &gt; +			 * point at new page while a device still is using this</span>
<span class="quote">&gt; &gt; +			 * page.</span>
<span class="quote">&gt; &gt; +			 *</span>
<span class="quote">&gt; &gt; +			 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; &gt;  			dec_mm_counter(mm, mm_counter_file(page));</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt;  discard:</span>
<span class="quote">&gt; &gt; +		/*</span>
<span class="quote">&gt; &gt; +		 * No need to call mmu_notifier_invalidate_range() it has be</span>
<span class="quote">&gt; &gt; +		 * done above for all cases requiring it to happen under page</span>
<span class="quote">&gt; &gt; +		 * table lock before mmu_notifier_invalidate_range_end()</span>
<span class="quote">&gt; &gt; +		 *</span>
<span class="quote">&gt; &gt; +		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; +		 */</span>
<span class="quote">&gt; &gt;  		page_remove_rmap(subpage, PageHuge(page));</span>
<span class="quote">&gt; &gt;  		put_page(page);</span>
<span class="quote">&gt; &gt; -		mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; -					      address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Looking at the patchset, I understand the efficiency, but I am concerned</span>
<span class="quote">&gt; with correctness.</span>

I am fine in holding this off from reaching Linus but only way to flush this
issues out if any is to have this patch in linux-next or somewhere were they
get a chance of being tested.

Note that the second patch is always safe. I agree that this one might
not be if hardware implementation is idiotic (well that would be my
opinion and any opinion/point of view can be challenge :))
<span class="quote">
&gt; </span>
<span class="quote">&gt; Balbir Singh.</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - Oct. 19, 2017, 10:53 a.m.</div>
<pre class="content">
On Thu, Oct 19, 2017 at 2:28 PM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:
<span class="quote">&gt; On Thu, Oct 19, 2017 at 02:04:26PM +1100, Balbir Singh wrote:</span>
<span class="quote">&gt;&gt; On Mon, 16 Oct 2017 23:10:02 -0400</span>
<span class="quote">&gt;&gt; jglisse@redhat.com wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; +           /*</span>
<span class="quote">&gt;&gt; &gt; +            * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt;&gt; &gt; +            * downgrading page table protection not changing it to point</span>
<span class="quote">&gt;&gt; &gt; +            * to a new page.</span>
<span class="quote">&gt;&gt; &gt; +            *</span>
<span class="quote">&gt;&gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt; +            */</span>
<span class="quote">&gt;&gt; &gt;             if (pmdp) {</span>
<span class="quote">&gt;&gt; &gt;  #ifdef CONFIG_FS_DAX_PMD</span>
<span class="quote">&gt;&gt; &gt;                     pmd_t pmd;</span>
<span class="quote">&gt;&gt; &gt; @@ -628,7 +635,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt;&gt; &gt;                     pmd = pmd_wrprotect(pmd);</span>
<span class="quote">&gt;&gt; &gt;                     pmd = pmd_mkclean(pmd);</span>
<span class="quote">&gt;&gt; &gt;                     set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd);</span>
<span class="quote">&gt;&gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Could the secondary TLB still see the mapping as dirty and propagate the dirty bit back?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I am assuming hardware does sane thing of setting the dirty bit only</span>
<span class="quote">&gt; when walking the CPU page table when device does a write fault ie</span>
<span class="quote">&gt; once the device get a write TLB entry the dirty is set by the IOMMU</span>
<span class="quote">&gt; when walking the page table before returning the lookup result to the</span>
<span class="quote">&gt; device and that it won&#39;t be set again latter (ie propagated back</span>
<span class="quote">&gt; latter).</span>
<span class="quote">&gt;</span>

The other possibility is that the hardware things the page is writable
and already
marked dirty. It allows writes and does not set the dirty bit?
<span class="quote">
&gt; I should probably have spell that out and maybe some of the ATS/PASID</span>
<span class="quote">&gt; implementer did not do that.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;  unlock_pmd:</span>
<span class="quote">&gt;&gt; &gt;                     spin_unlock(ptl);</span>
<span class="quote">&gt;&gt; &gt;  #endif</span>
<span class="quote">&gt;&gt; &gt; @@ -643,7 +649,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt;&gt; &gt;                     pte = pte_wrprotect(pte);</span>
<span class="quote">&gt;&gt; &gt;                     pte = pte_mkclean(pte);</span>
<span class="quote">&gt;&gt; &gt;                     set_pte_at(vma-&gt;vm_mm, address, ptep, pte);</span>
<span class="quote">&gt;&gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Ditto</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;  unlock_pte:</span>
<span class="quote">&gt;&gt; &gt;                     pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt;&gt; &gt;             }</span>
<span class="quote">&gt;&gt; &gt; diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt;&gt; &gt; index 6866e8126982..49c925c96b8a 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/include/linux/mmu_notifier.h</span>
<span class="quote">&gt;&gt; &gt; +++ b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt;&gt; &gt; @@ -155,7 +155,8 @@ struct mmu_notifier_ops {</span>
<span class="quote">&gt;&gt; &gt;      * shared page-tables, it not necessary to implement the</span>
<span class="quote">&gt;&gt; &gt;      * invalidate_range_start()/end() notifiers, as</span>
<span class="quote">&gt;&gt; &gt;      * invalidate_range() alread catches the points in time when an</span>
<span class="quote">&gt;&gt; &gt; -    * external TLB range needs to be flushed.</span>
<span class="quote">&gt;&gt; &gt; +    * external TLB range needs to be flushed. For more in depth</span>
<span class="quote">&gt;&gt; &gt; +    * discussion on this see Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt;      *</span>
<span class="quote">&gt;&gt; &gt;      * The invalidate_range() function is called under the ptl</span>
<span class="quote">&gt;&gt; &gt;      * spin-lock and not allowed to sleep.</span>
<span class="quote">&gt;&gt; &gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; &gt; index c037d3d34950..ff5bc647b51d 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; &gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt;&gt; &gt; @@ -1186,8 +1186,15 @@ static int do_huge_pmd_wp_page_fallback(struct vm_fault *vmf, pmd_t orig_pmd,</span>
<span class="quote">&gt;&gt; &gt;             goto out_free_pages;</span>
<span class="quote">&gt;&gt; &gt;     VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; +   /*</span>
<span class="quote">&gt;&gt; &gt; +    * Leave pmd empty until pte is filled note we must notify here as</span>
<span class="quote">&gt;&gt; &gt; +    * concurrent CPU thread might write to new page before the call to</span>
<span class="quote">&gt;&gt; &gt; +    * mmu_notifier_invalidate_range_end() happens which can lead to a</span>
<span class="quote">&gt;&gt; &gt; +    * device seeing memory write in different order than CPU.</span>
<span class="quote">&gt;&gt; &gt; +    *</span>
<span class="quote">&gt;&gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt; +    */</span>
<span class="quote">&gt;&gt; &gt;     pmdp_huge_clear_flush_notify(vma, haddr, vmf-&gt;pmd);</span>
<span class="quote">&gt;&gt; &gt; -   /* leave pmd empty until pte is filled */</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;     pgtable = pgtable_trans_huge_withdraw(vma-&gt;vm_mm, vmf-&gt;pmd);</span>
<span class="quote">&gt;&gt; &gt;     pmd_populate(vma-&gt;vm_mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt;&gt; &gt; @@ -2026,8 +2033,15 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; &gt;     pmd_t _pmd;</span>
<span class="quote">&gt;&gt; &gt;     int i;</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; -   /* leave pmd empty until pte is filled */</span>
<span class="quote">&gt;&gt; &gt; -   pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="quote">&gt;&gt; &gt; +   /*</span>
<span class="quote">&gt;&gt; &gt; +    * Leave pmd empty until pte is filled note that it is fine to delay</span>
<span class="quote">&gt;&gt; &gt; +    * notification until mmu_notifier_invalidate_range_end() as we are</span>
<span class="quote">&gt;&gt; &gt; +    * replacing a zero pmd write protected page with a zero pte write</span>
<span class="quote">&gt;&gt; &gt; +    * protected page.</span>
<span class="quote">&gt;&gt; &gt; +    *</span>
<span class="quote">&gt;&gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt; +    */</span>
<span class="quote">&gt;&gt; &gt; +   pmdp_huge_clear_flush(vma, haddr, pmd);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Shouldn&#39;t the secondary TLB know if the page size changed?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It should not matter, we are talking virtual to physical on behalf</span>
<span class="quote">&gt; of a device against a process address space. So the hardware should</span>
<span class="quote">&gt; not care about the page size.</span>
<span class="quote">&gt;</span>

Does that not indicate how much the device can access? Could it try
to access more than what is mapped?
<span class="quote">
&gt; Moreover if any of the new 512 (assuming 2MB huge and 4K pages) zero</span>
<span class="quote">&gt; 4K pages is replace by something new then a device TLB shootdown will</span>
<span class="quote">&gt; happen before the new page is set.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Only issue i can think of is if the IOMMU TLB (if there is one) or</span>
<span class="quote">&gt; the device TLB (you do expect that there is one) does not invalidate</span>
<span class="quote">&gt; TLB entry if the TLB shootdown is smaller than the TLB entry. That</span>
<span class="quote">&gt; would be idiotic but yes i know hardware bug.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;     pgtable = pgtable_trans_huge_withdraw(mm, pmd);</span>
<span class="quote">&gt;&gt; &gt;     pmd_populate(mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt;&gt; &gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; &gt; index 1768efa4c501..63a63f1b536c 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; &gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; &gt; @@ -3254,9 +3254,14 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
<span class="quote">&gt;&gt; &gt;                     set_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);</span>
<span class="quote">&gt;&gt; &gt;             } else {</span>
<span class="quote">&gt;&gt; &gt;                     if (cow) {</span>
<span class="quote">&gt;&gt; &gt; +                           /*</span>
<span class="quote">&gt;&gt; &gt; +                            * No need to notify as we are downgrading page</span>
<span class="quote">&gt;&gt; &gt; +                            * table protection not changing it to point</span>
<span class="quote">&gt;&gt; &gt; +                            * to a new page.</span>
<span class="quote">&gt;&gt; &gt; +                            *</span>
<span class="quote">&gt;&gt; &gt; +                            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt; +                            */</span>
<span class="quote">&gt;&gt; &gt;                             huge_ptep_set_wrprotect(src, addr, src_pte);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; OK.. so we could get write faults on write accesses from the device.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &gt; -                           mmu_notifier_invalidate_range(src, mmun_start,</span>
<span class="quote">&gt;&gt; &gt; -                                                              mmun_end);</span>
<span class="quote">&gt;&gt; &gt;                     }</span>
<span class="quote">&gt;&gt; &gt;                     entry = huge_ptep_get(src_pte);</span>
<span class="quote">&gt;&gt; &gt;                     ptepage = pte_page(entry);</span>
<span class="quote">&gt;&gt; &gt; @@ -4288,7 +4293,12 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; &gt;      * and that page table be reused and filled with junk.</span>
<span class="quote">&gt;&gt; &gt;      */</span>
<span class="quote">&gt;&gt; &gt;     flush_hugetlb_tlb_range(vma, start, end);</span>
<span class="quote">&gt;&gt; &gt; -   mmu_notifier_invalidate_range(mm, start, end);</span>
<span class="quote">&gt;&gt; &gt; +   /*</span>
<span class="quote">&gt;&gt; &gt; +    * No need to call mmu_notifier_invalidate_range() we are downgrading</span>
<span class="quote">&gt;&gt; &gt; +    * page table protection not changing it to point to a new page.</span>
<span class="quote">&gt;&gt; &gt; +    *</span>
<span class="quote">&gt;&gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt; +    */</span>
<span class="quote">&gt;&gt; &gt;     i_mmap_unlock_write(vma-&gt;vm_file-&gt;f_mapping);</span>
<span class="quote">&gt;&gt; &gt;     mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt;&gt; &gt; index 6cb60f46cce5..be8f4576f842 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/mm/ksm.c</span>
<span class="quote">&gt;&gt; &gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt;&gt; &gt; @@ -1052,8 +1052,13 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt;&gt; &gt;              * So we clear the pte and flush the tlb before the check</span>
<span class="quote">&gt;&gt; &gt;              * this assure us that no O_DIRECT can happen after the check</span>
<span class="quote">&gt;&gt; &gt;              * or in the middle of the check.</span>
<span class="quote">&gt;&gt; &gt; +            *</span>
<span class="quote">&gt;&gt; &gt; +            * No need to notify as we are downgrading page table to read</span>
<span class="quote">&gt;&gt; &gt; +            * only not changing it to point to a new page.</span>
<span class="quote">&gt;&gt; &gt; +            *</span>
<span class="quote">&gt;&gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt;              */</span>
<span class="quote">&gt;&gt; &gt; -           entry = ptep_clear_flush_notify(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt;&gt; &gt; +           entry = ptep_clear_flush(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt;&gt; &gt;             /*</span>
<span class="quote">&gt;&gt; &gt;              * Check that no O_DIRECT or similar I/O is in progress on the</span>
<span class="quote">&gt;&gt; &gt;              * page</span>
<span class="quote">&gt;&gt; &gt; @@ -1136,7 +1141,13 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt;&gt; &gt;     }</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;     flush_cache_page(vma, addr, pte_pfn(*ptep));</span>
<span class="quote">&gt;&gt; &gt; -   ptep_clear_flush_notify(vma, addr, ptep);</span>
<span class="quote">&gt;&gt; &gt; +   /*</span>
<span class="quote">&gt;&gt; &gt; +    * No need to notify as we are replacing a read only page with another</span>
<span class="quote">&gt;&gt; &gt; +    * read only page with the same content.</span>
<span class="quote">&gt;&gt; &gt; +    *</span>
<span class="quote">&gt;&gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt; +    */</span>
<span class="quote">&gt;&gt; &gt; +   ptep_clear_flush(vma, addr, ptep);</span>
<span class="quote">&gt;&gt; &gt;     set_pte_at_notify(mm, addr, ptep, newpte);</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;     page_remove_rmap(page, false);</span>
<span class="quote">&gt;&gt; &gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt;&gt; &gt; index 061826278520..6b5a0f219ac0 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/mm/rmap.c</span>
<span class="quote">&gt;&gt; &gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt;&gt; &gt; @@ -937,10 +937,15 @@ static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; &gt;  #endif</span>
<span class="quote">&gt;&gt; &gt;             }</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; -           if (ret) {</span>
<span class="quote">&gt;&gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, cstart, cend);</span>
<span class="quote">&gt;&gt; &gt; +           /*</span>
<span class="quote">&gt;&gt; &gt; +            * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt;&gt; &gt; +            * downgrading page table protection not changing it to point</span>
<span class="quote">&gt;&gt; &gt; +            * to a new page.</span>
<span class="quote">&gt;&gt; &gt; +            *</span>
<span class="quote">&gt;&gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt; +            */</span>
<span class="quote">&gt;&gt; &gt; +           if (ret)</span>
<span class="quote">&gt;&gt; &gt;                     (*cleaned)++;</span>
<span class="quote">&gt;&gt; &gt; -           }</span>
<span class="quote">&gt;&gt; &gt;     }</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;     mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt;&gt; &gt; @@ -1424,6 +1429,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt;&gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt;&gt; &gt;                     set_pte_at(mm, pvmw.address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt;&gt; &gt; +                   /*</span>
<span class="quote">&gt;&gt; &gt; +                    * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt;&gt; &gt; +                    * against the special swap migration pte.</span>
<span class="quote">&gt;&gt; &gt; +                    */</span>
<span class="quote">&gt;&gt; &gt;                     goto discard;</span>
<span class="quote">&gt;&gt; &gt;             }</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; @@ -1481,6 +1490,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; &gt;                      * will take care of the rest.</span>
<span class="quote">&gt;&gt; &gt;                      */</span>
<span class="quote">&gt;&gt; &gt;                     dec_mm_counter(mm, mm_counter(page));</span>
<span class="quote">&gt;&gt; &gt; +                   /* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt;&gt; &gt; +                   mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt;&gt; &gt; +                                                 address + PAGE_SIZE);</span>
<span class="quote">&gt;&gt; &gt;             } else if (IS_ENABLED(CONFIG_MIGRATION) &amp;&amp;</span>
<span class="quote">&gt;&gt; &gt;                             (flags &amp; (TTU_MIGRATION|TTU_SPLIT_FREEZE))) {</span>
<span class="quote">&gt;&gt; &gt;                     swp_entry_t entry;</span>
<span class="quote">&gt;&gt; &gt; @@ -1496,6 +1508,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt;&gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt;&gt; &gt;                     set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt;&gt; &gt; +                   /*</span>
<span class="quote">&gt;&gt; &gt; +                    * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt;&gt; &gt; +                    * against the special swap migration pte.</span>
<span class="quote">&gt;&gt; &gt; +                    */</span>
<span class="quote">&gt;&gt; &gt;             } else if (PageAnon(page)) {</span>
<span class="quote">&gt;&gt; &gt;                     swp_entry_t entry = { .val = page_private(subpage) };</span>
<span class="quote">&gt;&gt; &gt;                     pte_t swp_pte;</span>
<span class="quote">&gt;&gt; &gt; @@ -1507,6 +1523,8 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; &gt;                             WARN_ON_ONCE(1);</span>
<span class="quote">&gt;&gt; &gt;                             ret = false;</span>
<span class="quote">&gt;&gt; &gt;                             /* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt;&gt; &gt; +                           mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt;&gt; &gt; +                                                   address + PAGE_SIZE);</span>
<span class="quote">&gt;&gt; &gt;                             page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt;&gt; &gt;                             break;</span>
<span class="quote">&gt;&gt; &gt;                     }</span>
<span class="quote">&gt;&gt; &gt; @@ -1514,6 +1532,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; &gt;                     /* MADV_FREE page check */</span>
<span class="quote">&gt;&gt; &gt;                     if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt;&gt; &gt;                             if (!PageDirty(page)) {</span>
<span class="quote">&gt;&gt; &gt; +                                   /* Invalidate as we cleared the pte */</span>
<span class="quote">&gt;&gt; &gt; +                                   mmu_notifier_invalidate_range(mm,</span>
<span class="quote">&gt;&gt; &gt; +                                           address, address + PAGE_SIZE);</span>
<span class="quote">&gt;&gt; &gt;                                     dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt;&gt; &gt;                                     goto discard;</span>
<span class="quote">&gt;&gt; &gt;                             }</span>
<span class="quote">&gt;&gt; &gt; @@ -1547,13 +1568,39 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt;&gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt;&gt; &gt;                     set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt;&gt; &gt; -           } else</span>
<span class="quote">&gt;&gt; &gt; +                   /* Invalidate as we cleared the pte */</span>
<span class="quote">&gt;&gt; &gt; +                   mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt;&gt; &gt; +                                                 address + PAGE_SIZE);</span>
<span class="quote">&gt;&gt; &gt; +           } else {</span>
<span class="quote">&gt;&gt; &gt; +                   /*</span>
<span class="quote">&gt;&gt; &gt; +                    * We should not need to notify here as we reach this</span>
<span class="quote">&gt;&gt; &gt; +                    * case only from freeze_page() itself only call from</span>
<span class="quote">&gt;&gt; &gt; +                    * split_huge_page_to_list() so everything below must</span>
<span class="quote">&gt;&gt; &gt; +                    * be true:</span>
<span class="quote">&gt;&gt; &gt; +                    *   - page is not anonymous</span>
<span class="quote">&gt;&gt; &gt; +                    *   - page is locked</span>
<span class="quote">&gt;&gt; &gt; +                    *</span>
<span class="quote">&gt;&gt; &gt; +                    * So as it is a locked file back page thus it can not</span>
<span class="quote">&gt;&gt; &gt; +                    * be remove from the page cache and replace by a new</span>
<span class="quote">&gt;&gt; &gt; +                    * page before mmu_notifier_invalidate_range_end so no</span>
<span class="quote">&gt;&gt; &gt; +                    * concurrent thread might update its page table to</span>
<span class="quote">&gt;&gt; &gt; +                    * point at new page while a device still is using this</span>
<span class="quote">&gt;&gt; &gt; +                    * page.</span>
<span class="quote">&gt;&gt; &gt; +                    *</span>
<span class="quote">&gt;&gt; &gt; +                    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt; +                    */</span>
<span class="quote">&gt;&gt; &gt;                     dec_mm_counter(mm, mm_counter_file(page));</span>
<span class="quote">&gt;&gt; &gt; +           }</span>
<span class="quote">&gt;&gt; &gt;  discard:</span>
<span class="quote">&gt;&gt; &gt; +           /*</span>
<span class="quote">&gt;&gt; &gt; +            * No need to call mmu_notifier_invalidate_range() it has be</span>
<span class="quote">&gt;&gt; &gt; +            * done above for all cases requiring it to happen under page</span>
<span class="quote">&gt;&gt; &gt; +            * table lock before mmu_notifier_invalidate_range_end()</span>
<span class="quote">&gt;&gt; &gt; +            *</span>
<span class="quote">&gt;&gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt;&gt; &gt; +            */</span>
<span class="quote">&gt;&gt; &gt;             page_remove_rmap(subpage, PageHuge(page));</span>
<span class="quote">&gt;&gt; &gt;             put_page(page);</span>
<span class="quote">&gt;&gt; &gt; -           mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt;&gt; &gt; -                                         address + PAGE_SIZE);</span>
<span class="quote">&gt;&gt; &gt;     }</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;     mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Looking at the patchset, I understand the efficiency, but I am concerned</span>
<span class="quote">&gt;&gt; with correctness.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I am fine in holding this off from reaching Linus but only way to flush this</span>
<span class="quote">&gt; issues out if any is to have this patch in linux-next or somewhere were they</span>
<span class="quote">&gt; get a chance of being tested.</span>
<span class="quote">&gt;</span>

Yep, I would like to see some additional testing around npu and get Alistair
Popple to comment as well
<span class="quote">
&gt; Note that the second patch is always safe. I agree that this one might</span>
<span class="quote">&gt; not be if hardware implementation is idiotic (well that would be my</span>
<span class="quote">&gt; opinion and any opinion/point of view can be challenge :))</span>


You mean the only_end variant that avoids shootdown after pmd/pte changes
that avoid the _start/_end and have just the only_end variant? That seemed
reasonable to me, but I&#39;ve not tested it or evaluated it in depth

Balbir Singh.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Oct. 19, 2017, 4:58 p.m.</div>
<pre class="content">
On Thu, Oct 19, 2017 at 09:53:11PM +1100, Balbir Singh wrote:
<span class="quote">&gt; On Thu, Oct 19, 2017 at 2:28 PM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt; &gt; On Thu, Oct 19, 2017 at 02:04:26PM +1100, Balbir Singh wrote:</span>
<span class="quote">&gt; &gt;&gt; On Mon, 16 Oct 2017 23:10:02 -0400</span>
<span class="quote">&gt; &gt;&gt; jglisse@redhat.com wrote:</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; +           /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * downgrading page table protection not changing it to point</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * to a new page.</span>
<span class="quote">&gt; &gt;&gt; &gt; +            *</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt; +            */</span>
<span class="quote">&gt; &gt;&gt; &gt;             if (pmdp) {</span>
<span class="quote">&gt; &gt;&gt; &gt;  #ifdef CONFIG_FS_DAX_PMD</span>
<span class="quote">&gt; &gt;&gt; &gt;                     pmd_t pmd;</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -628,7 +635,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt; &gt;&gt; &gt;                     pmd = pmd_wrprotect(pmd);</span>
<span class="quote">&gt; &gt;&gt; &gt;                     pmd = pmd_mkclean(pmd);</span>
<span class="quote">&gt; &gt;&gt; &gt;                     set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd);</span>
<span class="quote">&gt; &gt;&gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Could the secondary TLB still see the mapping as dirty and propagate the dirty bit back?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I am assuming hardware does sane thing of setting the dirty bit only</span>
<span class="quote">&gt; &gt; when walking the CPU page table when device does a write fault ie</span>
<span class="quote">&gt; &gt; once the device get a write TLB entry the dirty is set by the IOMMU</span>
<span class="quote">&gt; &gt; when walking the page table before returning the lookup result to the</span>
<span class="quote">&gt; &gt; device and that it won&#39;t be set again latter (ie propagated back</span>
<span class="quote">&gt; &gt; latter).</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The other possibility is that the hardware things the page is writable</span>
<span class="quote">&gt; and already</span>
<span class="quote">&gt; marked dirty. It allows writes and does not set the dirty bit?</span>

I thought about this some more and the patch can not regress anything
that is not broken today. So if we assume that device can propagate
dirty bit because it can cache the write protection than all current
code is broken for two reasons:

First one is current code clear pte entry, build a new pte value with
write protection and update pte entry with new pte value. So any PASID/
ATS platform that allows device to cache the write bit and set dirty
bit anytime after that can race during that window and you would loose
the dirty bit of the device. That is not that bad as you are gonna
propagate the dirty bit to the struct page.

Second one is if the dirty bit is propagated back to the new write
protected pte. Quick look at code it seems that when we zap pte or
or mkclean we don&#39;t check that the pte has write permission but only
care about the dirty bit. So it should not have any bad consequence.

After this patch only the second window is bigger and thus more likely
to happen. But nothing sinister should happen from that.
<span class="quote">

&gt; </span>
<span class="quote">&gt; &gt; I should probably have spell that out and maybe some of the ATS/PASID</span>
<span class="quote">&gt; &gt; implementer did not do that.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;  unlock_pmd:</span>
<span class="quote">&gt; &gt;&gt; &gt;                     spin_unlock(ptl);</span>
<span class="quote">&gt; &gt;&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -643,7 +649,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt; &gt;&gt; &gt;                     pte = pte_wrprotect(pte);</span>
<span class="quote">&gt; &gt;&gt; &gt;                     pte = pte_mkclean(pte);</span>
<span class="quote">&gt; &gt;&gt; &gt;                     set_pte_at(vma-&gt;vm_mm, address, ptep, pte);</span>
<span class="quote">&gt; &gt;&gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Ditto</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;  unlock_pte:</span>
<span class="quote">&gt; &gt;&gt; &gt;                     pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt; &gt;&gt; &gt;             }</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt;&gt; &gt; index 6866e8126982..49c925c96b8a 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -155,7 +155,8 @@ struct mmu_notifier_ops {</span>
<span class="quote">&gt; &gt;&gt; &gt;      * shared page-tables, it not necessary to implement the</span>
<span class="quote">&gt; &gt;&gt; &gt;      * invalidate_range_start()/end() notifiers, as</span>
<span class="quote">&gt; &gt;&gt; &gt;      * invalidate_range() alread catches the points in time when an</span>
<span class="quote">&gt; &gt;&gt; &gt; -    * external TLB range needs to be flushed.</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * external TLB range needs to be flushed. For more in depth</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * discussion on this see Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt;      *</span>
<span class="quote">&gt; &gt;&gt; &gt;      * The invalidate_range() function is called under the ptl</span>
<span class="quote">&gt; &gt;&gt; &gt;      * spin-lock and not allowed to sleep.</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt;&gt; &gt; index c037d3d34950..ff5bc647b51d 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -1186,8 +1186,15 @@ static int do_huge_pmd_wp_page_fallback(struct vm_fault *vmf, pmd_t orig_pmd,</span>
<span class="quote">&gt; &gt;&gt; &gt;             goto out_free_pages;</span>
<span class="quote">&gt; &gt;&gt; &gt;     VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; +   /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * Leave pmd empty until pte is filled note we must notify here as</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * concurrent CPU thread might write to new page before the call to</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * mmu_notifier_invalidate_range_end() happens which can lead to a</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * device seeing memory write in different order than CPU.</span>
<span class="quote">&gt; &gt;&gt; &gt; +    *</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt; +    */</span>
<span class="quote">&gt; &gt;&gt; &gt;     pmdp_huge_clear_flush_notify(vma, haddr, vmf-&gt;pmd);</span>
<span class="quote">&gt; &gt;&gt; &gt; -   /* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;     pgtable = pgtable_trans_huge_withdraw(vma-&gt;vm_mm, vmf-&gt;pmd);</span>
<span class="quote">&gt; &gt;&gt; &gt;     pmd_populate(vma-&gt;vm_mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -2026,8 +2033,15 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt; &gt;     pmd_t _pmd;</span>
<span class="quote">&gt; &gt;&gt; &gt;     int i;</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; -   /* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; &gt;&gt; &gt; -   pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="quote">&gt; &gt;&gt; &gt; +   /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * Leave pmd empty until pte is filled note that it is fine to delay</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * notification until mmu_notifier_invalidate_range_end() as we are</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * replacing a zero pmd write protected page with a zero pte write</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * protected page.</span>
<span class="quote">&gt; &gt;&gt; &gt; +    *</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt; +    */</span>
<span class="quote">&gt; &gt;&gt; &gt; +   pmdp_huge_clear_flush(vma, haddr, pmd);</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Shouldn&#39;t the secondary TLB know if the page size changed?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; It should not matter, we are talking virtual to physical on behalf</span>
<span class="quote">&gt; &gt; of a device against a process address space. So the hardware should</span>
<span class="quote">&gt; &gt; not care about the page size.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Does that not indicate how much the device can access? Could it try</span>
<span class="quote">&gt; to access more than what is mapped?</span>

Assuming device has huge TLB and 2MB huge page with 4K small page.
You are going from one 1 TLB covering a 2MB zero page to 512 TLB
each covering 4K. Both case is read only and both case are pointing
to same data (ie zero).

It is fine to delay the TLB invalidate on the device to the call of
mmu_notifier_invalidate_range_end(). The device will keep using the
huge TLB for a little longer but both CPU and device are looking at
same data.

Now if there is a racing thread that replace one of the 512 zeor page
after the split but before mmu_notifier_invalidate_range_end() that
code path would call mmu_notifier_invalidate_range() before changing
the pte to point to something else. Which should shoot down the device
TLB (it would be a serious device bug if this did not work).
<span class="quote">

&gt; </span>
<span class="quote">&gt; &gt; Moreover if any of the new 512 (assuming 2MB huge and 4K pages) zero</span>
<span class="quote">&gt; &gt; 4K pages is replace by something new then a device TLB shootdown will</span>
<span class="quote">&gt; &gt; happen before the new page is set.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Only issue i can think of is if the IOMMU TLB (if there is one) or</span>
<span class="quote">&gt; &gt; the device TLB (you do expect that there is one) does not invalidate</span>
<span class="quote">&gt; &gt; TLB entry if the TLB shootdown is smaller than the TLB entry. That</span>
<span class="quote">&gt; &gt; would be idiotic but yes i know hardware bug.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;     pgtable = pgtable_trans_huge_withdraw(mm, pmd);</span>
<span class="quote">&gt; &gt;&gt; &gt;     pmd_populate(mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; &gt;&gt; &gt; index 1768efa4c501..63a63f1b536c 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -3254,9 +3254,14 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
<span class="quote">&gt; &gt;&gt; &gt;                     set_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);</span>
<span class="quote">&gt; &gt;&gt; &gt;             } else {</span>
<span class="quote">&gt; &gt;&gt; &gt;                     if (cow) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +                           /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +                            * No need to notify as we are downgrading page</span>
<span class="quote">&gt; &gt;&gt; &gt; +                            * table protection not changing it to point</span>
<span class="quote">&gt; &gt;&gt; &gt; +                            * to a new page.</span>
<span class="quote">&gt; &gt;&gt; &gt; +                            *</span>
<span class="quote">&gt; &gt;&gt; &gt; +                            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt; +                            */</span>
<span class="quote">&gt; &gt;&gt; &gt;                             huge_ptep_set_wrprotect(src, addr, src_pte);</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; OK.. so we could get write faults on write accesses from the device.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; -                           mmu_notifier_invalidate_range(src, mmun_start,</span>
<span class="quote">&gt; &gt;&gt; &gt; -                                                              mmun_end);</span>
<span class="quote">&gt; &gt;&gt; &gt;                     }</span>
<span class="quote">&gt; &gt;&gt; &gt;                     entry = huge_ptep_get(src_pte);</span>
<span class="quote">&gt; &gt;&gt; &gt;                     ptepage = pte_page(entry);</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -4288,7 +4293,12 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt; &gt;      * and that page table be reused and filled with junk.</span>
<span class="quote">&gt; &gt;&gt; &gt;      */</span>
<span class="quote">&gt; &gt;&gt; &gt;     flush_hugetlb_tlb_range(vma, start, end);</span>
<span class="quote">&gt; &gt;&gt; &gt; -   mmu_notifier_invalidate_range(mm, start, end);</span>
<span class="quote">&gt; &gt;&gt; &gt; +   /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * No need to call mmu_notifier_invalidate_range() we are downgrading</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * page table protection not changing it to point to a new page.</span>
<span class="quote">&gt; &gt;&gt; &gt; +    *</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt; +    */</span>
<span class="quote">&gt; &gt;&gt; &gt;     i_mmap_unlock_write(vma-&gt;vm_file-&gt;f_mapping);</span>
<span class="quote">&gt; &gt;&gt; &gt;     mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt; &gt;&gt; &gt; index 6cb60f46cce5..be8f4576f842 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/mm/ksm.c</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -1052,8 +1052,13 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt; &gt;&gt; &gt;              * So we clear the pte and flush the tlb before the check</span>
<span class="quote">&gt; &gt;&gt; &gt;              * this assure us that no O_DIRECT can happen after the check</span>
<span class="quote">&gt; &gt;&gt; &gt;              * or in the middle of the check.</span>
<span class="quote">&gt; &gt;&gt; &gt; +            *</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * No need to notify as we are downgrading page table to read</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * only not changing it to point to a new page.</span>
<span class="quote">&gt; &gt;&gt; &gt; +            *</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt;              */</span>
<span class="quote">&gt; &gt;&gt; &gt; -           entry = ptep_clear_flush_notify(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt; &gt;&gt; &gt; +           entry = ptep_clear_flush(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt; &gt;&gt; &gt;             /*</span>
<span class="quote">&gt; &gt;&gt; &gt;              * Check that no O_DIRECT or similar I/O is in progress on the</span>
<span class="quote">&gt; &gt;&gt; &gt;              * page</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -1136,7 +1141,13 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt; &gt;&gt; &gt;     }</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;     flush_cache_page(vma, addr, pte_pfn(*ptep));</span>
<span class="quote">&gt; &gt;&gt; &gt; -   ptep_clear_flush_notify(vma, addr, ptep);</span>
<span class="quote">&gt; &gt;&gt; &gt; +   /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * No need to notify as we are replacing a read only page with another</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * read only page with the same content.</span>
<span class="quote">&gt; &gt;&gt; &gt; +    *</span>
<span class="quote">&gt; &gt;&gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt; +    */</span>
<span class="quote">&gt; &gt;&gt; &gt; +   ptep_clear_flush(vma, addr, ptep);</span>
<span class="quote">&gt; &gt;&gt; &gt;     set_pte_at_notify(mm, addr, ptep, newpte);</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;     page_remove_rmap(page, false);</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; &gt;&gt; &gt; index 061826278520..6b5a0f219ac0 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -937,10 +937,15 @@ static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt;&gt; &gt;             }</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; -           if (ret) {</span>
<span class="quote">&gt; &gt;&gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, cstart, cend);</span>
<span class="quote">&gt; &gt;&gt; &gt; +           /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * downgrading page table protection not changing it to point</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * to a new page.</span>
<span class="quote">&gt; &gt;&gt; &gt; +            *</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt; +            */</span>
<span class="quote">&gt; &gt;&gt; &gt; +           if (ret)</span>
<span class="quote">&gt; &gt;&gt; &gt;                     (*cleaned)++;</span>
<span class="quote">&gt; &gt;&gt; &gt; -           }</span>
<span class="quote">&gt; &gt;&gt; &gt;     }</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;     mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -1424,6 +1429,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt;&gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt;&gt; &gt;                     set_pte_at(mm, pvmw.address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt;&gt; &gt; +                   /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * against the special swap migration pte.</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    */</span>
<span class="quote">&gt; &gt;&gt; &gt;                     goto discard;</span>
<span class="quote">&gt; &gt;&gt; &gt;             }</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -1481,6 +1490,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt; &gt;                      * will take care of the rest.</span>
<span class="quote">&gt; &gt;&gt; &gt;                      */</span>
<span class="quote">&gt; &gt;&gt; &gt;                     dec_mm_counter(mm, mm_counter(page));</span>
<span class="quote">&gt; &gt;&gt; &gt; +                   /* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt;&gt; &gt; +                   mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                                 address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt;&gt; &gt;             } else if (IS_ENABLED(CONFIG_MIGRATION) &amp;&amp;</span>
<span class="quote">&gt; &gt;&gt; &gt;                             (flags &amp; (TTU_MIGRATION|TTU_SPLIT_FREEZE))) {</span>
<span class="quote">&gt; &gt;&gt; &gt;                     swp_entry_t entry;</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -1496,6 +1508,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt;&gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt;&gt; &gt;                     set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt;&gt; &gt; +                   /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * against the special swap migration pte.</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    */</span>
<span class="quote">&gt; &gt;&gt; &gt;             } else if (PageAnon(page)) {</span>
<span class="quote">&gt; &gt;&gt; &gt;                     swp_entry_t entry = { .val = page_private(subpage) };</span>
<span class="quote">&gt; &gt;&gt; &gt;                     pte_t swp_pte;</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -1507,6 +1523,8 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt; &gt;                             WARN_ON_ONCE(1);</span>
<span class="quote">&gt; &gt;&gt; &gt;                             ret = false;</span>
<span class="quote">&gt; &gt;&gt; &gt;                             /* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt;&gt; &gt; +                           mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                                   address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt;&gt; &gt;                             page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt;&gt; &gt;                             break;</span>
<span class="quote">&gt; &gt;&gt; &gt;                     }</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -1514,6 +1532,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt; &gt;                     /* MADV_FREE page check */</span>
<span class="quote">&gt; &gt;&gt; &gt;                     if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt; &gt;&gt; &gt;                             if (!PageDirty(page)) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                   /* Invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                   mmu_notifier_invalidate_range(mm,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                           address, address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt;&gt; &gt;                                     dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt;&gt; &gt;                                     goto discard;</span>
<span class="quote">&gt; &gt;&gt; &gt;                             }</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -1547,13 +1568,39 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt;&gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt;&gt; &gt;                     set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt;&gt; &gt; -           } else</span>
<span class="quote">&gt; &gt;&gt; &gt; +                   /* Invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt;&gt; &gt; +                   mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                                 address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt;&gt; &gt; +           } else {</span>
<span class="quote">&gt; &gt;&gt; &gt; +                   /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * We should not need to notify here as we reach this</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * case only from freeze_page() itself only call from</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * split_huge_page_to_list() so everything below must</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * be true:</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    *   - page is not anonymous</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    *   - page is locked</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    *</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * So as it is a locked file back page thus it can not</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * be remove from the page cache and replace by a new</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * page before mmu_notifier_invalidate_range_end so no</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * concurrent thread might update its page table to</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * point at new page while a device still is using this</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * page.</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    *</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt; +                    */</span>
<span class="quote">&gt; &gt;&gt; &gt;                     dec_mm_counter(mm, mm_counter_file(page));</span>
<span class="quote">&gt; &gt;&gt; &gt; +           }</span>
<span class="quote">&gt; &gt;&gt; &gt;  discard:</span>
<span class="quote">&gt; &gt;&gt; &gt; +           /*</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * No need to call mmu_notifier_invalidate_range() it has be</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * done above for all cases requiring it to happen under page</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * table lock before mmu_notifier_invalidate_range_end()</span>
<span class="quote">&gt; &gt;&gt; &gt; +            *</span>
<span class="quote">&gt; &gt;&gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt;&gt; &gt; +            */</span>
<span class="quote">&gt; &gt;&gt; &gt;             page_remove_rmap(subpage, PageHuge(page));</span>
<span class="quote">&gt; &gt;&gt; &gt;             put_page(page);</span>
<span class="quote">&gt; &gt;&gt; &gt; -           mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt;&gt; &gt; -                                         address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt;&gt; &gt;     }</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;     mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Looking at the patchset, I understand the efficiency, but I am concerned</span>
<span class="quote">&gt; &gt;&gt; with correctness.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I am fine in holding this off from reaching Linus but only way to flush this</span>
<span class="quote">&gt; &gt; issues out if any is to have this patch in linux-next or somewhere were they</span>
<span class="quote">&gt; &gt; get a chance of being tested.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yep, I would like to see some additional testing around npu and get Alistair</span>
<span class="quote">&gt; Popple to comment as well</span>

I think this patch is fine. The only one race window that it might make
bigger should have no bad consequences.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; Note that the second patch is always safe. I agree that this one might</span>
<span class="quote">&gt; &gt; not be if hardware implementation is idiotic (well that would be my</span>
<span class="quote">&gt; &gt; opinion and any opinion/point of view can be challenge :))</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; You mean the only_end variant that avoids shootdown after pmd/pte changes</span>
<span class="quote">&gt; that avoid the _start/_end and have just the only_end variant? That seemed</span>
<span class="quote">&gt; reasonable to me, but I&#39;ve not tested it or evaluated it in depth</span>

Yes, patch 2/2 in this serie is definitly fine. It invalidate the device
TLB right after clearing pte entry and avoid latter unecessary invalidation
of same TLB.

Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - Oct. 21, 2017, 5:54 a.m.</div>
<pre class="content">
On Thu, 2017-10-19 at 12:58 -0400, Jerome Glisse wrote:
<span class="quote">&gt; On Thu, Oct 19, 2017 at 09:53:11PM +1100, Balbir Singh wrote:</span>
<span class="quote">&gt; &gt; On Thu, Oct 19, 2017 at 2:28 PM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu, Oct 19, 2017 at 02:04:26PM +1100, Balbir Singh wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Mon, 16 Oct 2017 23:10:02 -0400</span>
<span class="quote">&gt; &gt; &gt; &gt; jglisse@redhat.com wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +           /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * downgrading page table protection not changing it to point</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * to a new page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             if (pmdp) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  #ifdef CONFIG_FS_DAX_PMD</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     pmd_t pmd;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -628,7 +635,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     pmd = pmd_wrprotect(pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     pmd = pmd_mkclean(pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Could the secondary TLB still see the mapping as dirty and propagate the dirty bit back?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I am assuming hardware does sane thing of setting the dirty bit only</span>
<span class="quote">&gt; &gt; &gt; when walking the CPU page table when device does a write fault ie</span>
<span class="quote">&gt; &gt; &gt; once the device get a write TLB entry the dirty is set by the IOMMU</span>
<span class="quote">&gt; &gt; &gt; when walking the page table before returning the lookup result to the</span>
<span class="quote">&gt; &gt; &gt; device and that it won&#39;t be set again latter (ie propagated back</span>
<span class="quote">&gt; &gt; &gt; latter).</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The other possibility is that the hardware things the page is writable</span>
<span class="quote">&gt; &gt; and already</span>
<span class="quote">&gt; &gt; marked dirty. It allows writes and does not set the dirty bit?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I thought about this some more and the patch can not regress anything</span>
<span class="quote">&gt; that is not broken today. So if we assume that device can propagate</span>
<span class="quote">&gt; dirty bit because it can cache the write protection than all current</span>
<span class="quote">&gt; code is broken for two reasons:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; First one is current code clear pte entry, build a new pte value with</span>
<span class="quote">&gt; write protection and update pte entry with new pte value. So any PASID/</span>
<span class="quote">&gt; ATS platform that allows device to cache the write bit and set dirty</span>
<span class="quote">&gt; bit anytime after that can race during that window and you would loose</span>
<span class="quote">&gt; the dirty bit of the device. That is not that bad as you are gonna</span>
<span class="quote">&gt; propagate the dirty bit to the struct page.</span>

But they stay consistent with the notifiers, so from the OS perspective
it notifies of any PTE changes as they happen. When the ATS platform sees
invalidation, it invalidates it&#39;s PTE&#39;s as well.

I was speaking of the case where the ATS platform could assume it has
write access and has not seen any invalidation, the OS could return
back to user space or the caller with write bit clear, but the ATS
platform could still do a write since it&#39;s not seen the invalidation.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Second one is if the dirty bit is propagated back to the new write</span>
<span class="quote">&gt; protected pte. Quick look at code it seems that when we zap pte or</span>
<span class="quote">&gt; or mkclean we don&#39;t check that the pte has write permission but only</span>
<span class="quote">&gt; care about the dirty bit. So it should not have any bad consequence.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; After this patch only the second window is bigger and thus more likely</span>
<span class="quote">&gt; to happen. But nothing sinister should happen from that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I should probably have spell that out and maybe some of the ATS/PASID</span>
<span class="quote">&gt; &gt; &gt; implementer did not do that.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  unlock_pmd:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     spin_unlock(ptl);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  #endif</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -643,7 +649,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     pte = pte_wrprotect(pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     pte = pte_mkclean(pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     set_pte_at(vma-&gt;vm_mm, address, ptep, pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Ditto</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  unlock_pte:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; index 6866e8126982..49c925c96b8a 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; --- a/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +++ b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -155,7 +155,8 @@ struct mmu_notifier_ops {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;      * shared page-tables, it not necessary to implement the</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;      * invalidate_range_start()/end() notifiers, as</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;      * invalidate_range() alread catches the points in time when an</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -    * external TLB range needs to be flushed.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * external TLB range needs to be flushed. For more in depth</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * discussion on this see Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;      *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;      * The invalidate_range() function is called under the ptl</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;      * spin-lock and not allowed to sleep.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; index c037d3d34950..ff5bc647b51d 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -1186,8 +1186,15 @@ static int do_huge_pmd_wp_page_fallback(struct vm_fault *vmf, pmd_t orig_pmd,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             goto out_free_pages;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * Leave pmd empty until pte is filled note we must notify here as</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * concurrent CPU thread might write to new page before the call to</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * mmu_notifier_invalidate_range_end() happens which can lead to a</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * device seeing memory write in different order than CPU.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     pmdp_huge_clear_flush_notify(vma, haddr, vmf-&gt;pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -   /* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     pgtable = pgtable_trans_huge_withdraw(vma-&gt;vm_mm, vmf-&gt;pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     pmd_populate(vma-&gt;vm_mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -2026,8 +2033,15 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     pmd_t _pmd;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     int i;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -   /* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -   pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * Leave pmd empty until pte is filled note that it is fine to delay</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * notification until mmu_notifier_invalidate_range_end() as we are</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * replacing a zero pmd write protected page with a zero pte write</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * protected page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +   pmdp_huge_clear_flush(vma, haddr, pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Shouldn&#39;t the secondary TLB know if the page size changed?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; It should not matter, we are talking virtual to physical on behalf</span>
<span class="quote">&gt; &gt; &gt; of a device against a process address space. So the hardware should</span>
<span class="quote">&gt; &gt; &gt; not care about the page size.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Does that not indicate how much the device can access? Could it try</span>
<span class="quote">&gt; &gt; to access more than what is mapped?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Assuming device has huge TLB and 2MB huge page with 4K small page.</span>
<span class="quote">&gt; You are going from one 1 TLB covering a 2MB zero page to 512 TLB</span>
<span class="quote">&gt; each covering 4K. Both case is read only and both case are pointing</span>
<span class="quote">&gt; to same data (ie zero).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is fine to delay the TLB invalidate on the device to the call of</span>
<span class="quote">&gt; mmu_notifier_invalidate_range_end(). The device will keep using the</span>
<span class="quote">&gt; huge TLB for a little longer but both CPU and device are looking at</span>
<span class="quote">&gt; same data.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Now if there is a racing thread that replace one of the 512 zeor page</span>
<span class="quote">&gt; after the split but before mmu_notifier_invalidate_range_end() that</span>
<span class="quote">&gt; code path would call mmu_notifier_invalidate_range() before changing</span>
<span class="quote">&gt; the pte to point to something else. Which should shoot down the device</span>
<span class="quote">&gt; TLB (it would be a serious device bug if this did not work).</span>

OK.. This seems reasonable, but I&#39;d really like to see if it can be
tested
<span class="quote">
&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Moreover if any of the new 512 (assuming 2MB huge and 4K pages) zero</span>
<span class="quote">&gt; &gt; &gt; 4K pages is replace by something new then a device TLB shootdown will</span>
<span class="quote">&gt; &gt; &gt; happen before the new page is set.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Only issue i can think of is if the IOMMU TLB (if there is one) or</span>
<span class="quote">&gt; &gt; &gt; the device TLB (you do expect that there is one) does not invalidate</span>
<span class="quote">&gt; &gt; &gt; TLB entry if the TLB shootdown is smaller than the TLB entry. That</span>
<span class="quote">&gt; &gt; &gt; would be idiotic but yes i know hardware bug.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     pgtable = pgtable_trans_huge_withdraw(mm, pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     pmd_populate(mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; index 1768efa4c501..63a63f1b536c 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -3254,9 +3254,14 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     set_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             } else {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     if (cow) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                           /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                            * No need to notify as we are downgrading page</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                            * table protection not changing it to point</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                            * to a new page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                            */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             huge_ptep_set_wrprotect(src, addr, src_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; OK.. so we could get write faults on write accesses from the device.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -                           mmu_notifier_invalidate_range(src, mmun_start,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -                                                              mmun_end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     entry = huge_ptep_get(src_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     ptepage = pte_page(entry);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -4288,7 +4293,12 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;      * and that page table be reused and filled with junk.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;      */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     flush_hugetlb_tlb_range(vma, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -   mmu_notifier_invalidate_range(mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * No need to call mmu_notifier_invalidate_range() we are downgrading</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * page table protection not changing it to point to a new page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     i_mmap_unlock_write(vma-&gt;vm_file-&gt;f_mapping);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; index 6cb60f46cce5..be8f4576f842 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; --- a/mm/ksm.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -1052,8 +1052,13 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;              * So we clear the pte and flush the tlb before the check</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;              * this assure us that no O_DIRECT can happen after the check</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;              * or in the middle of the check.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * No need to notify as we are downgrading page table to read</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * only not changing it to point to a new page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;              */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -           entry = ptep_clear_flush_notify(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +           entry = ptep_clear_flush(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;              * Check that no O_DIRECT or similar I/O is in progress on the</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;              * page</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -1136,7 +1141,13 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     flush_cache_page(vma, addr, pte_pfn(*ptep));</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -   ptep_clear_flush_notify(vma, addr, ptep);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * No need to notify as we are replacing a read only page with another</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * read only page with the same content.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +   ptep_clear_flush(vma, addr, ptep);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     set_pte_at_notify(mm, addr, ptep, newpte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     page_remove_rmap(page, false);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; index 061826278520..6b5a0f219ac0 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -937,10 +937,15 @@ static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  #endif</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -           if (ret) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, cstart, cend);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +           /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * downgrading page table protection not changing it to point</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * to a new page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +           if (ret)</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     (*cleaned)++;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -           }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -1424,6 +1429,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     set_pte_at(mm, pvmw.address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * against the special swap migration pte.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     goto discard;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -1481,6 +1490,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                      * will take care of the rest.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                      */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     dec_mm_counter(mm, mm_counter(page));</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                   /* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                   mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                                                 address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             } else if (IS_ENABLED(CONFIG_MIGRATION) &amp;&amp;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             (flags &amp; (TTU_MIGRATION|TTU_SPLIT_FREEZE))) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     swp_entry_t entry;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -1496,6 +1508,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * against the special swap migration pte.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             } else if (PageAnon(page)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     swp_entry_t entry = { .val = page_private(subpage) };</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     pte_t swp_pte;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -1507,6 +1523,8 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             WARN_ON_ONCE(1);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             ret = false;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             /* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                           mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                                                   address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             break;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -1514,6 +1532,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     /* MADV_FREE page check */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             if (!PageDirty(page)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                                   /* Invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                                   mmu_notifier_invalidate_range(mm,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                                           address, address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                                     dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                                     goto discard;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; @@ -1547,13 +1568,39 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -           } else</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                   /* Invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                   mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                                                 address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +           } else {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * We should not need to notify here as we reach this</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * case only from freeze_page() itself only call from</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * split_huge_page_to_list() so everything below must</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * be true:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    *   - page is not anonymous</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    *   - page is locked</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * So as it is a locked file back page thus it can not</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * be remove from the page cache and replace by a new</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * page before mmu_notifier_invalidate_range_end so no</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * concurrent thread might update its page table to</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * point at new page while a device still is using this</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +                    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;                     dec_mm_counter(mm, mm_counter_file(page));</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +           }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;  discard:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +           /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * No need to call mmu_notifier_invalidate_range() it has be</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * done above for all cases requiring it to happen under page</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * table lock before mmu_notifier_invalidate_range_end()</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; +            */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             page_remove_rmap(subpage, PageHuge(page));</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;             put_page(page);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -           mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; -                                         address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt;     mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Looking at the patchset, I understand the efficiency, but I am concerned</span>
<span class="quote">&gt; &gt; &gt; &gt; with correctness.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I am fine in holding this off from reaching Linus but only way to flush this</span>
<span class="quote">&gt; &gt; &gt; issues out if any is to have this patch in linux-next or somewhere were they</span>
<span class="quote">&gt; &gt; &gt; get a chance of being tested.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yep, I would like to see some additional testing around npu and get Alistair</span>
<span class="quote">&gt; &gt; Popple to comment as well</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think this patch is fine. The only one race window that it might make</span>
<span class="quote">&gt; bigger should have no bad consequences.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Note that the second patch is always safe. I agree that this one might</span>
<span class="quote">&gt; &gt; &gt; not be if hardware implementation is idiotic (well that would be my</span>
<span class="quote">&gt; &gt; &gt; opinion and any opinion/point of view can be challenge :))</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; You mean the only_end variant that avoids shootdown after pmd/pte changes</span>
<span class="quote">&gt; &gt; that avoid the _start/_end and have just the only_end variant? That seemed</span>
<span class="quote">&gt; &gt; reasonable to me, but I&#39;ve not tested it or evaluated it in depth</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, patch 2/2 in this serie is definitly fine. It invalidate the device</span>
<span class="quote">&gt; TLB right after clearing pte entry and avoid latter unecessary invalidation</span>
<span class="quote">&gt; of same TLB.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Jérôme</span>

Balbir Singh.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Oct. 21, 2017, 3:47 p.m.</div>
<pre class="content">
On Sat, Oct 21, 2017 at 04:54:40PM +1100, Balbir Singh wrote:
<span class="quote">&gt; On Thu, 2017-10-19 at 12:58 -0400, Jerome Glisse wrote:</span>
<span class="quote">&gt; &gt; On Thu, Oct 19, 2017 at 09:53:11PM +1100, Balbir Singh wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu, Oct 19, 2017 at 2:28 PM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Thu, Oct 19, 2017 at 02:04:26PM +1100, Balbir Singh wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; On Mon, 16 Oct 2017 23:10:02 -0400</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; jglisse@redhat.com wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +           /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * downgrading page table protection not changing it to point</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * to a new page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             if (pmdp) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;  #ifdef CONFIG_FS_DAX_PMD</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     pmd_t pmd;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -628,7 +635,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     pmd = pmd_wrprotect(pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     pmd = pmd_mkclean(pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Could the secondary TLB still see the mapping as dirty and propagate the dirty bit back?</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; I am assuming hardware does sane thing of setting the dirty bit only</span>
<span class="quote">&gt; &gt; &gt; &gt; when walking the CPU page table when device does a write fault ie</span>
<span class="quote">&gt; &gt; &gt; &gt; once the device get a write TLB entry the dirty is set by the IOMMU</span>
<span class="quote">&gt; &gt; &gt; &gt; when walking the page table before returning the lookup result to the</span>
<span class="quote">&gt; &gt; &gt; &gt; device and that it won&#39;t be set again latter (ie propagated back</span>
<span class="quote">&gt; &gt; &gt; &gt; latter).</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; The other possibility is that the hardware things the page is writable</span>
<span class="quote">&gt; &gt; &gt; and already</span>
<span class="quote">&gt; &gt; &gt; marked dirty. It allows writes and does not set the dirty bit?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I thought about this some more and the patch can not regress anything</span>
<span class="quote">&gt; &gt; that is not broken today. So if we assume that device can propagate</span>
<span class="quote">&gt; &gt; dirty bit because it can cache the write protection than all current</span>
<span class="quote">&gt; &gt; code is broken for two reasons:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; First one is current code clear pte entry, build a new pte value with</span>
<span class="quote">&gt; &gt; write protection and update pte entry with new pte value. So any PASID/</span>
<span class="quote">&gt; &gt; ATS platform that allows device to cache the write bit and set dirty</span>
<span class="quote">&gt; &gt; bit anytime after that can race during that window and you would loose</span>
<span class="quote">&gt; &gt; the dirty bit of the device. That is not that bad as you are gonna</span>
<span class="quote">&gt; &gt; propagate the dirty bit to the struct page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But they stay consistent with the notifiers, so from the OS perspective</span>
<span class="quote">&gt; it notifies of any PTE changes as they happen. When the ATS platform sees</span>
<span class="quote">&gt; invalidation, it invalidates it&#39;s PTE&#39;s as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I was speaking of the case where the ATS platform could assume it has</span>
<span class="quote">&gt; write access and has not seen any invalidation, the OS could return</span>
<span class="quote">&gt; back to user space or the caller with write bit clear, but the ATS</span>
<span class="quote">&gt; platform could still do a write since it&#39;s not seen the invalidation.</span>

I understood what you said and what is above apply. I am removing only
one of the invalidation not both. So with that patch the invalidation
is delayed after the page table lock drop but before dax/page_mkclean
returns. Hence any further activity will be read only on any device too
once we exit those functions.

The only difference is the window during which device can report dirty
pte. Before that patch the 2 &quot;~bogus~&quot; window were small:
  First window between pmd/pte_get_clear_flush and set_pte/pmd
  Second window between set_pte/pmd and mmu_notifier_invalidate_range

The first window stay the same, the second window is bigger, potentialy
lot bigger if thread is prempted before mmu_notifier_invalidate_range_end

But that is fine as in that case the page is reported as dirty and thus
we are not missing anything and the kernel code does not care about
seeing read only pte mark as dirty.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Second one is if the dirty bit is propagated back to the new write</span>
<span class="quote">&gt; &gt; protected pte. Quick look at code it seems that when we zap pte or</span>
<span class="quote">&gt; &gt; or mkclean we don&#39;t check that the pte has write permission but only</span>
<span class="quote">&gt; &gt; care about the dirty bit. So it should not have any bad consequence.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; After this patch only the second window is bigger and thus more likely</span>
<span class="quote">&gt; &gt; to happen. But nothing sinister should happen from that.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; I should probably have spell that out and maybe some of the ATS/PASID</span>
<span class="quote">&gt; &gt; &gt; &gt; implementer did not do that.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;  unlock_pmd:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     spin_unlock(ptl);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;  #endif</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -643,7 +649,6 @@ static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     pte = pte_wrprotect(pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     pte = pte_mkclean(pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     set_pte_at(vma-&gt;vm_mm, address, ptep, pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Ditto</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;  unlock_pte:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     pte_unmap_unlock(ptep, ptl);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; index 6866e8126982..49c925c96b8a 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; --- a/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +++ b/include/linux/mmu_notifier.h</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -155,7 +155,8 @@ struct mmu_notifier_ops {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;      * shared page-tables, it not necessary to implement the</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;      * invalidate_range_start()/end() notifiers, as</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;      * invalidate_range() alread catches the points in time when an</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -    * external TLB range needs to be flushed.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * external TLB range needs to be flushed. For more in depth</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * discussion on this see Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;      *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;      * The invalidate_range() function is called under the ptl</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;      * spin-lock and not allowed to sleep.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; index c037d3d34950..ff5bc647b51d 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -1186,8 +1186,15 @@ static int do_huge_pmd_wp_page_fallback(struct vm_fault *vmf, pmd_t orig_pmd,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             goto out_free_pages;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * Leave pmd empty until pte is filled note we must notify here as</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * concurrent CPU thread might write to new page before the call to</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * mmu_notifier_invalidate_range_end() happens which can lead to a</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * device seeing memory write in different order than CPU.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     pmdp_huge_clear_flush_notify(vma, haddr, vmf-&gt;pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -   /* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     pgtable = pgtable_trans_huge_withdraw(vma-&gt;vm_mm, vmf-&gt;pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     pmd_populate(vma-&gt;vm_mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -2026,8 +2033,15 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     pmd_t _pmd;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     int i;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -   /* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -   pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * Leave pmd empty until pte is filled note that it is fine to delay</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * notification until mmu_notifier_invalidate_range_end() as we are</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * replacing a zero pmd write protected page with a zero pte write</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * protected page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +   pmdp_huge_clear_flush(vma, haddr, pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Shouldn&#39;t the secondary TLB know if the page size changed?</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; It should not matter, we are talking virtual to physical on behalf</span>
<span class="quote">&gt; &gt; &gt; &gt; of a device against a process address space. So the hardware should</span>
<span class="quote">&gt; &gt; &gt; &gt; not care about the page size.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Does that not indicate how much the device can access? Could it try</span>
<span class="quote">&gt; &gt; &gt; to access more than what is mapped?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Assuming device has huge TLB and 2MB huge page with 4K small page.</span>
<span class="quote">&gt; &gt; You are going from one 1 TLB covering a 2MB zero page to 512 TLB</span>
<span class="quote">&gt; &gt; each covering 4K. Both case is read only and both case are pointing</span>
<span class="quote">&gt; &gt; to same data (ie zero).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It is fine to delay the TLB invalidate on the device to the call of</span>
<span class="quote">&gt; &gt; mmu_notifier_invalidate_range_end(). The device will keep using the</span>
<span class="quote">&gt; &gt; huge TLB for a little longer but both CPU and device are looking at</span>
<span class="quote">&gt; &gt; same data.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Now if there is a racing thread that replace one of the 512 zeor page</span>
<span class="quote">&gt; &gt; after the split but before mmu_notifier_invalidate_range_end() that</span>
<span class="quote">&gt; &gt; code path would call mmu_notifier_invalidate_range() before changing</span>
<span class="quote">&gt; &gt; the pte to point to something else. Which should shoot down the device</span>
<span class="quote">&gt; &gt; TLB (it would be a serious device bug if this did not work).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; OK.. This seems reasonable, but I&#39;d really like to see if it can be</span>
<span class="quote">&gt; tested</span>

Well hard to test, many factors first each device might react differently.
Device that only store TLB at 4k granularity are fine. Clever device that
can store TLB for 4k, 2M, ... can ignore an invalidation that is smaller
than their TLB entry ie getting a 4K invalidation would not invalidate a
2MB TLB entry in the device. I consider this as buggy. I will go look at
the PCIE ATS specification one more time and see if there is any wording
related that. I might bring up a question to the PCIE standard body if not.

Second factor is that it is a race between split zero and a write fault.
I can probably do a crappy patch that msleep if split happens against a
given mm to increase the race window. But i would be testing against one
device (right now i can only access AMD IOMMUv2 devices with discret ATS
GPU)
<span class="quote">

&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Moreover if any of the new 512 (assuming 2MB huge and 4K pages) zero</span>
<span class="quote">&gt; &gt; &gt; &gt; 4K pages is replace by something new then a device TLB shootdown will</span>
<span class="quote">&gt; &gt; &gt; &gt; happen before the new page is set.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Only issue i can think of is if the IOMMU TLB (if there is one) or</span>
<span class="quote">&gt; &gt; &gt; &gt; the device TLB (you do expect that there is one) does not invalidate</span>
<span class="quote">&gt; &gt; &gt; &gt; TLB entry if the TLB shootdown is smaller than the TLB entry. That</span>
<span class="quote">&gt; &gt; &gt; &gt; would be idiotic but yes i know hardware bug.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     pgtable = pgtable_trans_huge_withdraw(mm, pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     pmd_populate(mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; index 1768efa4c501..63a63f1b536c 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -3254,9 +3254,14 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     set_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             } else {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     if (cow) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                           /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                            * No need to notify as we are downgrading page</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                            * table protection not changing it to point</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                            * to a new page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                            */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             huge_ptep_set_wrprotect(src, addr, src_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; OK.. so we could get write faults on write accesses from the device.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -                           mmu_notifier_invalidate_range(src, mmun_start,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -                                                              mmun_end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     entry = huge_ptep_get(src_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     ptepage = pte_page(entry);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -4288,7 +4293,12 @@ unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;      * and that page table be reused and filled with junk.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;      */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     flush_hugetlb_tlb_range(vma, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -   mmu_notifier_invalidate_range(mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * No need to call mmu_notifier_invalidate_range() we are downgrading</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * page table protection not changing it to point to a new page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     i_mmap_unlock_write(vma-&gt;vm_file-&gt;f_mapping);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     mmu_notifier_invalidate_range_end(mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; index 6cb60f46cce5..be8f4576f842 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; --- a/mm/ksm.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +++ b/mm/ksm.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -1052,8 +1052,13 @@ static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;              * So we clear the pte and flush the tlb before the check</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;              * this assure us that no O_DIRECT can happen after the check</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;              * or in the middle of the check.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * No need to notify as we are downgrading page table to read</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * only not changing it to point to a new page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;              */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -           entry = ptep_clear_flush_notify(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +           entry = ptep_clear_flush(vma, pvmw.address, pvmw.pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;              * Check that no O_DIRECT or similar I/O is in progress on the</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;              * page</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -1136,7 +1141,13 @@ static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     flush_cache_page(vma, addr, pte_pfn(*ptep));</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -   ptep_clear_flush_notify(vma, addr, ptep);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * No need to notify as we are replacing a read only page with another</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * read only page with the same content.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +   ptep_clear_flush(vma, addr, ptep);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     set_pte_at_notify(mm, addr, ptep, newpte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     page_remove_rmap(page, false);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; index 061826278520..6b5a0f219ac0 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -937,10 +937,15 @@ static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;  #endif</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -           if (ret) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -                   mmu_notifier_invalidate_range(vma-&gt;vm_mm, cstart, cend);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +           /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * downgrading page table protection not changing it to point</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * to a new page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +           if (ret)</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     (*cleaned)++;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -           }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -1424,6 +1429,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     set_pte_at(mm, pvmw.address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * against the special swap migration pte.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     goto discard;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -1481,6 +1490,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                      * will take care of the rest.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                      */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     dec_mm_counter(mm, mm_counter(page));</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                   /* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                   mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                                                 address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             } else if (IS_ENABLED(CONFIG_MIGRATION) &amp;&amp;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             (flags &amp; (TTU_MIGRATION|TTU_SPLIT_FREEZE))) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     swp_entry_t entry;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -1496,6 +1508,10 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * No need to invalidate here it will synchronize on</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * against the special swap migration pte.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             } else if (PageAnon(page)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     swp_entry_t entry = { .val = page_private(subpage) };</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     pte_t swp_pte;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -1507,6 +1523,8 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             WARN_ON_ONCE(1);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             ret = false;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             /* We have to invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                           mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                                                   address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             break;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -1514,6 +1532,9 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     /* MADV_FREE page check */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             if (!PageDirty(page)) {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                                   /* Invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                                   mmu_notifier_invalidate_range(mm,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                                           address, address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                                     dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                                     goto discard;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; @@ -1547,13 +1568,39 @@ static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     if (pte_soft_dirty(pteval))</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                             swp_pte = pte_swp_mksoft_dirty(swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     set_pte_at(mm, address, pvmw.pte, swp_pte);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -           } else</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                   /* Invalidate as we cleared the pte */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                   mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                                                 address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +           } else {</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * We should not need to notify here as we reach this</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * case only from freeze_page() itself only call from</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * split_huge_page_to_list() so everything below must</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * be true:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    *   - page is not anonymous</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    *   - page is locked</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * So as it is a locked file back page thus it can not</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * be remove from the page cache and replace by a new</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * page before mmu_notifier_invalidate_range_end so no</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * concurrent thread might update its page table to</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * point at new page while a device still is using this</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +                    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;                     dec_mm_counter(mm, mm_counter_file(page));</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +           }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;  discard:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +           /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * No need to call mmu_notifier_invalidate_range() it has be</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * done above for all cases requiring it to happen under page</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * table lock before mmu_notifier_invalidate_range_end()</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; +            */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             page_remove_rmap(subpage, PageHuge(page));</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;             put_page(page);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -           mmu_notifier_invalidate_range(mm, address,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; -                                         address + PAGE_SIZE);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     }</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt;     mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Looking at the patchset, I understand the efficiency, but I am concerned</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; with correctness.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; I am fine in holding this off from reaching Linus but only way to flush this</span>
<span class="quote">&gt; &gt; &gt; &gt; issues out if any is to have this patch in linux-next or somewhere were they</span>
<span class="quote">&gt; &gt; &gt; &gt; get a chance of being tested.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Yep, I would like to see some additional testing around npu and get Alistair</span>
<span class="quote">&gt; &gt; &gt; Popple to comment as well</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think this patch is fine. The only one race window that it might make</span>
<span class="quote">&gt; &gt; bigger should have no bad consequences.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Note that the second patch is always safe. I agree that this one might</span>
<span class="quote">&gt; &gt; &gt; &gt; not be if hardware implementation is idiotic (well that would be my</span>
<span class="quote">&gt; &gt; &gt; &gt; opinion and any opinion/point of view can be challenge :))</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; You mean the only_end variant that avoids shootdown after pmd/pte changes</span>
<span class="quote">&gt; &gt; &gt; that avoid the _start/_end and have just the only_end variant? That seemed</span>
<span class="quote">&gt; &gt; &gt; reasonable to me, but I&#39;ve not tested it or evaluated it in depth</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yes, patch 2/2 in this serie is definitly fine. It invalidate the device</span>
<span class="quote">&gt; &gt; TLB right after clearing pte entry and avoid latter unecessary invalidation</span>
<span class="quote">&gt; &gt; of same TLB.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Jérôme</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Balbir Singh.</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Oct. 23, 2017, 8:35 p.m.</div>
<pre class="content">
On Sat, Oct 21, 2017 at 11:47:03AM -0400, Jerome Glisse wrote:
<span class="quote">&gt; On Sat, Oct 21, 2017 at 04:54:40PM +1100, Balbir Singh wrote:</span>
<span class="quote">&gt; &gt; On Thu, 2017-10-19 at 12:58 -0400, Jerome Glisse wrote:</span>
<span class="quote">&gt; &gt; &gt; On Thu, Oct 19, 2017 at 09:53:11PM +1100, Balbir Singh wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Thu, Oct 19, 2017 at 2:28 PM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; On Thu, Oct 19, 2017 at 02:04:26PM +1100, Balbir Singh wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; On Mon, 16 Oct 2017 23:10:02 -0400</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; jglisse@redhat.com wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; From: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>

[...]
<span class="quote">
&gt; &gt; &gt; &gt; &gt; &gt; &gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; index c037d3d34950..ff5bc647b51d 100644</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; @@ -1186,8 +1186,15 @@ static int do_huge_pmd_wp_page_fallback(struct vm_fault *vmf, pmd_t orig_pmd,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt;             goto out_free_pages;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt;     VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    * Leave pmd empty until pte is filled note we must notify here as</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    * concurrent CPU thread might write to new page before the call to</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    * mmu_notifier_invalidate_range_end() happens which can lead to a</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    * device seeing memory write in different order than CPU.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt;     pmdp_huge_clear_flush_notify(vma, haddr, vmf-&gt;pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; -   /* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt;     pgtable = pgtable_trans_huge_withdraw(vma-&gt;vm_mm, vmf-&gt;pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt;     pmd_populate(vma-&gt;vm_mm, &amp;_pmd, pgtable);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; @@ -2026,8 +2033,15 @@ static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt;     pmd_t _pmd;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt;     int i;</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; -   /* leave pmd empty until pte is filled */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; -   pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +   /*</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    * Leave pmd empty until pte is filled note that it is fine to delay</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    * notification until mmu_notifier_invalidate_range_end() as we are</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    * replacing a zero pmd write protected page with a zero pte write</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    * protected page.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    *</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    * See Documentation/vm/mmu_notifier.txt</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +    */</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; +   pmdp_huge_clear_flush(vma, haddr, pmd);</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; Shouldn&#39;t the secondary TLB know if the page size changed?</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; It should not matter, we are talking virtual to physical on behalf</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; of a device against a process address space. So the hardware should</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; not care about the page size.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Does that not indicate how much the device can access? Could it try</span>
<span class="quote">&gt; &gt; &gt; &gt; to access more than what is mapped?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Assuming device has huge TLB and 2MB huge page with 4K small page.</span>
<span class="quote">&gt; &gt; &gt; You are going from one 1 TLB covering a 2MB zero page to 512 TLB</span>
<span class="quote">&gt; &gt; &gt; each covering 4K. Both case is read only and both case are pointing</span>
<span class="quote">&gt; &gt; &gt; to same data (ie zero).</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; It is fine to delay the TLB invalidate on the device to the call of</span>
<span class="quote">&gt; &gt; &gt; mmu_notifier_invalidate_range_end(). The device will keep using the</span>
<span class="quote">&gt; &gt; &gt; huge TLB for a little longer but both CPU and device are looking at</span>
<span class="quote">&gt; &gt; &gt; same data.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Now if there is a racing thread that replace one of the 512 zeor page</span>
<span class="quote">&gt; &gt; &gt; after the split but before mmu_notifier_invalidate_range_end() that</span>
<span class="quote">&gt; &gt; &gt; code path would call mmu_notifier_invalidate_range() before changing</span>
<span class="quote">&gt; &gt; &gt; the pte to point to something else. Which should shoot down the device</span>
<span class="quote">&gt; &gt; &gt; TLB (it would be a serious device bug if this did not work).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; OK.. This seems reasonable, but I&#39;d really like to see if it can be</span>
<span class="quote">&gt; &gt; tested</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well hard to test, many factors first each device might react differently.</span>
<span class="quote">&gt; Device that only store TLB at 4k granularity are fine. Clever device that</span>
<span class="quote">&gt; can store TLB for 4k, 2M, ... can ignore an invalidation that is smaller</span>
<span class="quote">&gt; than their TLB entry ie getting a 4K invalidation would not invalidate a</span>
<span class="quote">&gt; 2MB TLB entry in the device. I consider this as buggy. I will go look at</span>
<span class="quote">&gt; the PCIE ATS specification one more time and see if there is any wording</span>
<span class="quote">&gt; related that. I might bring up a question to the PCIE standard body if not.</span>

So inside PCIE ATS there is the definition of &quot;minimum translation or
invalidate size&quot; which says 4096 bytes. So my understanding is that
hardware must support 4K invalidation in all the case and thus we shoud
be safe from possible hazard above.

But none the less i will repost without the optimization for huge page
to be more concervative as anyway we want to be correct before we care
about last bit of optimization.

Cheers,
Jérôme
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/vm/mmu_notifier.txt b/Documentation/vm/mmu_notifier.txt</span>
new file mode 100644
<span class="p_header">index 000000000000..23b462566bb7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/Documentation/vm/mmu_notifier.txt</span>
<span class="p_chunk">@@ -0,0 +1,93 @@</span> <span class="p_context"></span>
<span class="p_add">+When do you need to notify inside page table lock ?</span>
<span class="p_add">+</span>
<span class="p_add">+When clearing a pte/pmd we are given a choice to notify the event through</span>
<span class="p_add">+(notify version of *_clear_flush call mmu_notifier_invalidate_range) under</span>
<span class="p_add">+the page table lock. But that notification is not necessary in all cases.</span>
<span class="p_add">+</span>
<span class="p_add">+For secondary TLB (non CPU TLB) like IOMMU TLB or device TLB (when device use</span>
<span class="p_add">+thing like ATS/PASID to get the IOMMU to walk the CPU page table to access a</span>
<span class="p_add">+process virtual address space). There is only 2 cases when you need to notify</span>
<span class="p_add">+those secondary TLB while holding page table lock when clearing a pte/pmd:</span>
<span class="p_add">+</span>
<span class="p_add">+  A) page backing address is free before mmu_notifier_invalidate_range_end()</span>
<span class="p_add">+  B) a page table entry is updated to point to a new page (COW, write fault</span>
<span class="p_add">+     on zero page, __replace_page(), ...)</span>
<span class="p_add">+</span>
<span class="p_add">+Case A is obvious you do not want to take the risk for the device to write to</span>
<span class="p_add">+a page that might now be used by some completely different task.</span>
<span class="p_add">+</span>
<span class="p_add">+Case B is more subtle. For correctness it requires the following sequence to</span>
<span class="p_add">+happen:</span>
<span class="p_add">+  - take page table lock</span>
<span class="p_add">+  - clear page table entry and notify ([pmd/pte]p_huge_clear_flush_notify())</span>
<span class="p_add">+  - set page table entry to point to new page</span>
<span class="p_add">+</span>
<span class="p_add">+If clearing the page table entry is not followed by a notify before setting</span>
<span class="p_add">+the new pte/pmd value then you can break memory model like C11 or C++11 for</span>
<span class="p_add">+the device.</span>
<span class="p_add">+</span>
<span class="p_add">+Consider the following scenario (device use a feature similar to ATS/PASID):</span>
<span class="p_add">+</span>
<span class="p_add">+Two address addrA and addrB such that |addrA - addrB| &gt;= PAGE_SIZE we assume</span>
<span class="p_add">+they are write protected for COW (other case of B apply too).</span>
<span class="p_add">+</span>
<span class="p_add">+[Time N] --------------------------------------------------------------------</span>
<span class="p_add">+CPU-thread-0  {try to write to addrA}</span>
<span class="p_add">+CPU-thread-1  {try to write to addrB}</span>
<span class="p_add">+CPU-thread-2  {}</span>
<span class="p_add">+CPU-thread-3  {}</span>
<span class="p_add">+DEV-thread-0  {read addrA and populate device TLB}</span>
<span class="p_add">+DEV-thread-2  {read addrB and populate device TLB}</span>
<span class="p_add">+[Time N+1] ------------------------------------------------------------------</span>
<span class="p_add">+CPU-thread-0  {COW_step0: {mmu_notifier_invalidate_range_start(addrA)}}</span>
<span class="p_add">+CPU-thread-1  {COW_step0: {mmu_notifier_invalidate_range_start(addrB)}}</span>
<span class="p_add">+CPU-thread-2  {}</span>
<span class="p_add">+CPU-thread-3  {}</span>
<span class="p_add">+DEV-thread-0  {}</span>
<span class="p_add">+DEV-thread-2  {}</span>
<span class="p_add">+[Time N+2] ------------------------------------------------------------------</span>
<span class="p_add">+CPU-thread-0  {COW_step1: {update page table to point to new page for addrA}}</span>
<span class="p_add">+CPU-thread-1  {COW_step1: {update page table to point to new page for addrB}}</span>
<span class="p_add">+CPU-thread-2  {}</span>
<span class="p_add">+CPU-thread-3  {}</span>
<span class="p_add">+DEV-thread-0  {}</span>
<span class="p_add">+DEV-thread-2  {}</span>
<span class="p_add">+[Time N+3] ------------------------------------------------------------------</span>
<span class="p_add">+CPU-thread-0  {preempted}</span>
<span class="p_add">+CPU-thread-1  {preempted}</span>
<span class="p_add">+CPU-thread-2  {write to addrA which is a write to new page}</span>
<span class="p_add">+CPU-thread-3  {}</span>
<span class="p_add">+DEV-thread-0  {}</span>
<span class="p_add">+DEV-thread-2  {}</span>
<span class="p_add">+[Time N+3] ------------------------------------------------------------------</span>
<span class="p_add">+CPU-thread-0  {preempted}</span>
<span class="p_add">+CPU-thread-1  {preempted}</span>
<span class="p_add">+CPU-thread-2  {}</span>
<span class="p_add">+CPU-thread-3  {write to addrB which is a write to new page}</span>
<span class="p_add">+DEV-thread-0  {}</span>
<span class="p_add">+DEV-thread-2  {}</span>
<span class="p_add">+[Time N+4] ------------------------------------------------------------------</span>
<span class="p_add">+CPU-thread-0  {preempted}</span>
<span class="p_add">+CPU-thread-1  {COW_step3: {mmu_notifier_invalidate_range_end(addrB)}}</span>
<span class="p_add">+CPU-thread-2  {}</span>
<span class="p_add">+CPU-thread-3  {}</span>
<span class="p_add">+DEV-thread-0  {}</span>
<span class="p_add">+DEV-thread-2  {}</span>
<span class="p_add">+[Time N+5] ------------------------------------------------------------------</span>
<span class="p_add">+CPU-thread-0  {preempted}</span>
<span class="p_add">+CPU-thread-1  {}</span>
<span class="p_add">+CPU-thread-2  {}</span>
<span class="p_add">+CPU-thread-3  {}</span>
<span class="p_add">+DEV-thread-0  {read addrA from old page}</span>
<span class="p_add">+DEV-thread-2  {read addrB from new page}</span>
<span class="p_add">+</span>
<span class="p_add">+So here because at time N+2 the clear page table entry was not pair with a</span>
<span class="p_add">+notification to invalidate the secondary TLB, the device see the new value for</span>
<span class="p_add">+addrB before seing the new value for addrA. This break total memory ordering</span>
<span class="p_add">+for the device.</span>
<span class="p_add">+</span>
<span class="p_add">+When changing a pte to write protect or to point to a new write protected page</span>
<span class="p_add">+with same content (KSM) it is fine to delay the mmu_notifier_invalidate_range</span>
<span class="p_add">+call to mmu_notifier_invalidate_range_end() outside the page table lock. This</span>
<span class="p_add">+is true even if the thread doing the page table update is preempted right after</span>
<span class="p_add">+releasing page table lock but before call mmu_notifier_invalidate_range_end().</span>
<span class="p_header">diff --git a/fs/dax.c b/fs/dax.c</span>
<span class="p_header">index f3a44a7c14b3..9ec797424e4f 100644</span>
<span class="p_header">--- a/fs/dax.c</span>
<span class="p_header">+++ b/fs/dax.c</span>
<span class="p_chunk">@@ -614,6 +614,13 @@</span> <span class="p_context"> static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
 		if (follow_pte_pmd(vma-&gt;vm_mm, address, &amp;start, &amp;end, &amp;ptep, &amp;pmdp, &amp;ptl))
 			continue;
 
<span class="p_add">+		/*</span>
<span class="p_add">+		 * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="p_add">+		 * downgrading page table protection not changing it to point</span>
<span class="p_add">+		 * to a new page.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="p_add">+		 */</span>
 		if (pmdp) {
 #ifdef CONFIG_FS_DAX_PMD
 			pmd_t pmd;
<span class="p_chunk">@@ -628,7 +635,6 @@</span> <span class="p_context"> static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
 			pmd = pmd_wrprotect(pmd);
 			pmd = pmd_mkclean(pmd);
 			set_pmd_at(vma-&gt;vm_mm, address, pmdp, pmd);
<span class="p_del">-			mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
 unlock_pmd:
 			spin_unlock(ptl);
 #endif
<span class="p_chunk">@@ -643,7 +649,6 @@</span> <span class="p_context"> static void dax_mapping_entry_mkclean(struct address_space *mapping,</span>
 			pte = pte_wrprotect(pte);
 			pte = pte_mkclean(pte);
 			set_pte_at(vma-&gt;vm_mm, address, ptep, pte);
<span class="p_del">-			mmu_notifier_invalidate_range(vma-&gt;vm_mm, start, end);</span>
 unlock_pte:
 			pte_unmap_unlock(ptep, ptl);
 		}
<span class="p_header">diff --git a/include/linux/mmu_notifier.h b/include/linux/mmu_notifier.h</span>
<span class="p_header">index 6866e8126982..49c925c96b8a 100644</span>
<span class="p_header">--- a/include/linux/mmu_notifier.h</span>
<span class="p_header">+++ b/include/linux/mmu_notifier.h</span>
<span class="p_chunk">@@ -155,7 +155,8 @@</span> <span class="p_context"> struct mmu_notifier_ops {</span>
 	 * shared page-tables, it not necessary to implement the
 	 * invalidate_range_start()/end() notifiers, as
 	 * invalidate_range() alread catches the points in time when an
<span class="p_del">-	 * external TLB range needs to be flushed.</span>
<span class="p_add">+	 * external TLB range needs to be flushed. For more in depth</span>
<span class="p_add">+	 * discussion on this see Documentation/vm/mmu_notifier.txt</span>
 	 *
 	 * The invalidate_range() function is called under the ptl
 	 * spin-lock and not allowed to sleep.
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index c037d3d34950..ff5bc647b51d 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1186,8 +1186,15 @@</span> <span class="p_context"> static int do_huge_pmd_wp_page_fallback(struct vm_fault *vmf, pmd_t orig_pmd,</span>
 		goto out_free_pages;
 	VM_BUG_ON_PAGE(!PageHead(page), page);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Leave pmd empty until pte is filled note we must notify here as</span>
<span class="p_add">+	 * concurrent CPU thread might write to new page before the call to</span>
<span class="p_add">+	 * mmu_notifier_invalidate_range_end() happens which can lead to a</span>
<span class="p_add">+	 * device seeing memory write in different order than CPU.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="p_add">+	 */</span>
 	pmdp_huge_clear_flush_notify(vma, haddr, vmf-&gt;pmd);
<span class="p_del">-	/* leave pmd empty until pte is filled */</span>
 
 	pgtable = pgtable_trans_huge_withdraw(vma-&gt;vm_mm, vmf-&gt;pmd);
 	pmd_populate(vma-&gt;vm_mm, &amp;_pmd, pgtable);
<span class="p_chunk">@@ -2026,8 +2033,15 @@</span> <span class="p_context"> static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,</span>
 	pmd_t _pmd;
 	int i;
 
<span class="p_del">-	/* leave pmd empty until pte is filled */</span>
<span class="p_del">-	pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Leave pmd empty until pte is filled note that it is fine to delay</span>
<span class="p_add">+	 * notification until mmu_notifier_invalidate_range_end() as we are</span>
<span class="p_add">+	 * replacing a zero pmd write protected page with a zero pte write</span>
<span class="p_add">+	 * protected page.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pmdp_huge_clear_flush(vma, haddr, pmd);</span>
 
 	pgtable = pgtable_trans_huge_withdraw(mm, pmd);
 	pmd_populate(mm, &amp;_pmd, pgtable);
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 1768efa4c501..63a63f1b536c 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -3254,9 +3254,14 @@</span> <span class="p_context"> int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,</span>
 			set_huge_swap_pte_at(dst, addr, dst_pte, entry, sz);
 		} else {
 			if (cow) {
<span class="p_add">+				/*</span>
<span class="p_add">+				 * No need to notify as we are downgrading page</span>
<span class="p_add">+				 * table protection not changing it to point</span>
<span class="p_add">+				 * to a new page.</span>
<span class="p_add">+				 *</span>
<span class="p_add">+				 * See Documentation/vm/mmu_notifier.txt</span>
<span class="p_add">+				 */</span>
 				huge_ptep_set_wrprotect(src, addr, src_pte);
<span class="p_del">-				mmu_notifier_invalidate_range(src, mmun_start,</span>
<span class="p_del">-								   mmun_end);</span>
 			}
 			entry = huge_ptep_get(src_pte);
 			ptepage = pte_page(entry);
<span class="p_chunk">@@ -4288,7 +4293,12 @@</span> <span class="p_context"> unsigned long hugetlb_change_protection(struct vm_area_struct *vma,</span>
 	 * and that page table be reused and filled with junk.
 	 */
 	flush_hugetlb_tlb_range(vma, start, end);
<span class="p_del">-	mmu_notifier_invalidate_range(mm, start, end);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * No need to call mmu_notifier_invalidate_range() we are downgrading</span>
<span class="p_add">+	 * page table protection not changing it to point to a new page.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="p_add">+	 */</span>
 	i_mmap_unlock_write(vma-&gt;vm_file-&gt;f_mapping);
 	mmu_notifier_invalidate_range_end(mm, start, end);
 
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index 6cb60f46cce5..be8f4576f842 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -1052,8 +1052,13 @@</span> <span class="p_context"> static int write_protect_page(struct vm_area_struct *vma, struct page *page,</span>
 		 * So we clear the pte and flush the tlb before the check
 		 * this assure us that no O_DIRECT can happen after the check
 		 * or in the middle of the check.
<span class="p_add">+		 *</span>
<span class="p_add">+		 * No need to notify as we are downgrading page table to read</span>
<span class="p_add">+		 * only not changing it to point to a new page.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * See Documentation/vm/mmu_notifier.txt</span>
 		 */
<span class="p_del">-		entry = ptep_clear_flush_notify(vma, pvmw.address, pvmw.pte);</span>
<span class="p_add">+		entry = ptep_clear_flush(vma, pvmw.address, pvmw.pte);</span>
 		/*
 		 * Check that no O_DIRECT or similar I/O is in progress on the
 		 * page
<span class="p_chunk">@@ -1136,7 +1141,13 @@</span> <span class="p_context"> static int replace_page(struct vm_area_struct *vma, struct page *page,</span>
 	}
 
 	flush_cache_page(vma, addr, pte_pfn(*ptep));
<span class="p_del">-	ptep_clear_flush_notify(vma, addr, ptep);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * No need to notify as we are replacing a read only page with another</span>
<span class="p_add">+	 * read only page with the same content.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * See Documentation/vm/mmu_notifier.txt</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ptep_clear_flush(vma, addr, ptep);</span>
 	set_pte_at_notify(mm, addr, ptep, newpte);
 
 	page_remove_rmap(page, false);
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 061826278520..6b5a0f219ac0 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -937,10 +937,15 @@</span> <span class="p_context"> static bool page_mkclean_one(struct page *page, struct vm_area_struct *vma,</span>
 #endif
 		}
 
<span class="p_del">-		if (ret) {</span>
<span class="p_del">-			mmu_notifier_invalidate_range(vma-&gt;vm_mm, cstart, cend);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * No need to call mmu_notifier_invalidate_range() as we are</span>
<span class="p_add">+		 * downgrading page table protection not changing it to point</span>
<span class="p_add">+		 * to a new page.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (ret)</span>
 			(*cleaned)++;
<span class="p_del">-		}</span>
 	}
 
 	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);
<span class="p_chunk">@@ -1424,6 +1429,10 @@</span> <span class="p_context"> static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 			if (pte_soft_dirty(pteval))
 				swp_pte = pte_swp_mksoft_dirty(swp_pte);
 			set_pte_at(mm, pvmw.address, pvmw.pte, swp_pte);
<span class="p_add">+			/*</span>
<span class="p_add">+			 * No need to invalidate here it will synchronize on</span>
<span class="p_add">+			 * against the special swap migration pte.</span>
<span class="p_add">+			 */</span>
 			goto discard;
 		}
 
<span class="p_chunk">@@ -1481,6 +1490,9 @@</span> <span class="p_context"> static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 			 * will take care of the rest.
 			 */
 			dec_mm_counter(mm, mm_counter(page));
<span class="p_add">+			/* We have to invalidate as we cleared the pte */</span>
<span class="p_add">+			mmu_notifier_invalidate_range(mm, address,</span>
<span class="p_add">+						      address + PAGE_SIZE);</span>
 		} else if (IS_ENABLED(CONFIG_MIGRATION) &amp;&amp;
 				(flags &amp; (TTU_MIGRATION|TTU_SPLIT_FREEZE))) {
 			swp_entry_t entry;
<span class="p_chunk">@@ -1496,6 +1508,10 @@</span> <span class="p_context"> static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 			if (pte_soft_dirty(pteval))
 				swp_pte = pte_swp_mksoft_dirty(swp_pte);
 			set_pte_at(mm, address, pvmw.pte, swp_pte);
<span class="p_add">+			/*</span>
<span class="p_add">+			 * No need to invalidate here it will synchronize on</span>
<span class="p_add">+			 * against the special swap migration pte.</span>
<span class="p_add">+			 */</span>
 		} else if (PageAnon(page)) {
 			swp_entry_t entry = { .val = page_private(subpage) };
 			pte_t swp_pte;
<span class="p_chunk">@@ -1507,6 +1523,8 @@</span> <span class="p_context"> static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 				WARN_ON_ONCE(1);
 				ret = false;
 				/* We have to invalidate as we cleared the pte */
<span class="p_add">+				mmu_notifier_invalidate_range(mm, address,</span>
<span class="p_add">+							address + PAGE_SIZE);</span>
 				page_vma_mapped_walk_done(&amp;pvmw);
 				break;
 			}
<span class="p_chunk">@@ -1514,6 +1532,9 @@</span> <span class="p_context"> static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 			/* MADV_FREE page check */
 			if (!PageSwapBacked(page)) {
 				if (!PageDirty(page)) {
<span class="p_add">+					/* Invalidate as we cleared the pte */</span>
<span class="p_add">+					mmu_notifier_invalidate_range(mm,</span>
<span class="p_add">+						address, address + PAGE_SIZE);</span>
 					dec_mm_counter(mm, MM_ANONPAGES);
 					goto discard;
 				}
<span class="p_chunk">@@ -1547,13 +1568,39 @@</span> <span class="p_context"> static bool try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 			if (pte_soft_dirty(pteval))
 				swp_pte = pte_swp_mksoft_dirty(swp_pte);
 			set_pte_at(mm, address, pvmw.pte, swp_pte);
<span class="p_del">-		} else</span>
<span class="p_add">+			/* Invalidate as we cleared the pte */</span>
<span class="p_add">+			mmu_notifier_invalidate_range(mm, address,</span>
<span class="p_add">+						      address + PAGE_SIZE);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * We should not need to notify here as we reach this</span>
<span class="p_add">+			 * case only from freeze_page() itself only call from</span>
<span class="p_add">+			 * split_huge_page_to_list() so everything below must</span>
<span class="p_add">+			 * be true:</span>
<span class="p_add">+			 *   - page is not anonymous</span>
<span class="p_add">+			 *   - page is locked</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * So as it is a locked file back page thus it can not</span>
<span class="p_add">+			 * be remove from the page cache and replace by a new</span>
<span class="p_add">+			 * page before mmu_notifier_invalidate_range_end so no</span>
<span class="p_add">+			 * concurrent thread might update its page table to</span>
<span class="p_add">+			 * point at new page while a device still is using this</span>
<span class="p_add">+			 * page.</span>
<span class="p_add">+			 *</span>
<span class="p_add">+			 * See Documentation/vm/mmu_notifier.txt</span>
<span class="p_add">+			 */</span>
 			dec_mm_counter(mm, mm_counter_file(page));
<span class="p_add">+		}</span>
 discard:
<span class="p_add">+		/*</span>
<span class="p_add">+		 * No need to call mmu_notifier_invalidate_range() it has be</span>
<span class="p_add">+		 * done above for all cases requiring it to happen under page</span>
<span class="p_add">+		 * table lock before mmu_notifier_invalidate_range_end()</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * See Documentation/vm/mmu_notifier.txt</span>
<span class="p_add">+		 */</span>
 		page_remove_rmap(subpage, PageHuge(page));
 		put_page(page);
<span class="p_del">-		mmu_notifier_invalidate_range(mm, address,</span>
<span class="p_del">-					      address + PAGE_SIZE);</span>
 	}
 
 	mmu_notifier_invalidate_range_end(vma-&gt;vm_mm, start, end);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



