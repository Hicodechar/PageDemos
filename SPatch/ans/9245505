
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86/mm changes for v4.8 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86/mm changes for v4.8</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 25, 2016, 9:50 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160725095033.GA28605@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9245505/mbox/"
   >mbox</a>
|
   <a href="/patch/9245505/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9245505/">/patch/9245505/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	269FF607D3 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 25 Jul 2016 09:51:09 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 07F4925250
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 25 Jul 2016 09:51:09 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id EF8702711E; Mon, 25 Jul 2016 09:51:08 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7032B25250
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 25 Jul 2016 09:51:01 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752726AbcGYJuy (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 25 Jul 2016 05:50:54 -0400
Received: from mail-wm0-f66.google.com ([74.125.82.66]:35497 &quot;EHLO
	mail-wm0-f66.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752481AbcGYJul (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 25 Jul 2016 05:50:41 -0400
Received: by mail-wm0-f66.google.com with SMTP id i5so15946329wmg.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 25 Jul 2016 02:50:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:user-agent;
	bh=4ogOfwlVEE3zGkRqAm1v+Zsr6+xYi8KSZ+V2JGmLpFs=;
	b=h0CFEp65enIYSlSzjRA7u6HNhn77Qm+35+ajJ+UBPFZLkS+LAznf+Thr52HMaYULvK
	5Gni1SuLdtGe1T4pSJbHAWEmiCkpxcRXOY3BC7QcZiEHTgiue0zuZJEaKUJ60pB5LiD5
	RyK1bYqMQ6NgDFhvusgE/aTiE5q21mY4pLvPfr/Upm2Ew9jAxlctZ8N53JWDkvDSBU2b
	HkChICcquJGl82bSHLq+36jhHwInuPW1UCVwKHaGHtnbC2SvqqnKGOMTJj9UIb4tL6OI
	iB8oITd79p+xn04iFKYTpBW2hn18REpgoDrmDhwgRWgLXAei7xlfH8i2oK+Rk+iswhvV
	pVpw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:user-agent;
	bh=4ogOfwlVEE3zGkRqAm1v+Zsr6+xYi8KSZ+V2JGmLpFs=;
	b=KR0BZln1B0tbRpo8zGORa4nUVLeWJuq8gONG3JYDstsH0FdD1Mk8JKd2C6nlTPI9pE
	9Tkse4NbWOXYGDcXC5pDYbCsNOdZFnrBE3YefowfOxoA/UyvLiUtzj5VczjbAe4S99GD
	+yj8ZF1zUv015loAsCHm/PWpaRugZk2dtkcMC7QHAc5MDzUpQA3QvEsIonCbPc6JYDwX
	wXr8/g0I6pkNMfczxSVju9JdX4Zdn/CJxjH7CdNq4NJZMaCbJ/eL+WgiVFP1cSpVJWWp
	rV0PC6xRnR9UY5TVwP8u0XSbuy/WwR+Rj4js8S0yPWLdZF8t36M0at9O0vOSU5s2pB8P
	P1jw==
X-Gm-Message-State: AEkoouufc3fxUGW5Atgt5bOQkFwJA0nwTEv3nRVw17yQV0OL+SZoT+BZHAaurfVbGyIaCQ==
X-Received: by 10.194.235.69 with SMTP id uk5mr16207973wjc.53.1469440237193; 
	Mon, 25 Jul 2016 02:50:37 -0700 (PDT)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	v189sm26461405wmv.12.2016.07.25.02.50.34
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 25 Jul 2016 02:50:35 -0700 (PDT)
Date: Mon, 25 Jul 2016 11:50:33 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Borislav Petkov &lt;bp@alien8.de&gt;, Andy Lutomirski &lt;luto@kernel.org&gt;
Subject: [GIT PULL] x86/mm changes for v4.8
Message-ID: &lt;20160725095033.GA28605@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.24 (2015-08-30)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - July 25, 2016, 9:50 a.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-mm-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-mm-for-linus

   # HEAD: 55920d31f1e3fea06702c74271dd56c4fc9b70ca x86/mm/cpa: Add missing comment in populate_pdg()

Various x86 low level modifications:

 - Preparatory work to support virtually mapped kernel stacks (Andy Lutomirski)

 - Support for 64-bit __get_user() on 32-bit kernels (Benjamin LaHaise)

 - (Involved) workaround for Knights Landing CPU erratum (Dave Hansen)

 - MPX enhancements (Dave Hansen)

 - mremap() extension to allow remapping of the special VDSO vma, for purposes of 
   user level context save/restore. (Dmitry Safonov)

 - hweight and entry code cleanups (Borislav Petkov)

 - bitops code generation optimizations and cleanups with modern GCC (H. Peter Anvin)

 - syscall entry code optimizations (Paolo Bonzini)


  out-of-topic modifications in x86-mm-for-linus:
  -------------------------------------------------
  drivers/pnp/isapnp/proc.c          # 13d4ea097d18: x86/uaccess: Move thread_inf
  fs/read_write.c                    # 3ebfd81f7fb3: x86/syscalls: Add compat_sys
  include/linux/context_tracking.h   # 2e9d1e150abf: x86/entry: Avoid interrupt f
  include/linux/mm_types.h           # b059a453b1cf: x86/vdso: Add mremap hook to
  include/linux/random.h             # 117780eef774: x86, asm: use bool for bitop
  lib/Makefile                       # f5967101e9de: x86/hweight: Get rid of the 
  lib/bitmap.c                       # 13d4ea097d18: x86/uaccess: Move thread_inf
  lib/hweight.c                      # f5967101e9de: x86/hweight: Get rid of the 
  mm/mmap.c                          # b059a453b1cf: x86/vdso: Add mremap hook to
  tools/testing/selftests/x86/Makefile# f80fd3a5fff8: selftests/x86: Add vDSO mrem
                                   # e754aedc26ef: x86/mpx, selftests: Add MPX 
  tools/testing/selftests/x86/mpx-debug.h# e754aedc26ef: x86/mpx, selftests: Add MPX 
  tools/testing/selftests/x86/mpx-dig.c# e754aedc26ef: x86/mpx, selftests: Add MPX 
  tools/testing/selftests/x86/mpx-hw.h# e754aedc26ef: x86/mpx, selftests: Add MPX 
  tools/testing/selftests/x86/mpx-mini-test.c# e754aedc26ef: x86/mpx, selftests: Add MPX 
  tools/testing/selftests/x86/mpx-mm.h# e754aedc26ef: x86/mpx, selftests: Add MPX 
  tools/testing/selftests/x86/test_mremap_vdso.c# f80fd3a5fff8: selftests/x86: Add vDSO mrem

 Thanks,

	Ingo

------------------&gt;
Andy Lutomirski (14):
      x86/xen: Simplify set_aliased_prot()
      x86/vdso/32: Assemble sigreturn.S separately
      x86/mm/cpa: In populate_pgd(), don&#39;t set the PGD entry until it&#39;s populated
      x86/mm: Remove kernel_unmap_pages_in_pgd() and efi_cleanup_page_tables()
      x86/dumpstack: Try harder to get a call trace on stack overflow
      x86/dumpstack/64: Handle faults when printing the &quot;Stack: &quot; part of an OOPS
      x86/mm/64: In vmalloc_fault(), use CR3 instead of current-&gt;active_mm
      x86/dumpstack: When OOPSing, rewind the stack before do_exit()
      x86/uaccess: Move thread_info::uaccess_err and thread_info::sig_on_uaccess_err to thread_struct
      x86/uaccess: Move thread_info::addr_limit to thread_struct
      x86/smp: Remove stack_smp_processor_id()
      x86/smp: Remove unnecessary initialization of thread_info::cpu
      x86/mm/cpa: Fix populate_pgd(): Stop trying to deallocate failed PUDs
      x86/mm/cpa: Add missing comment in populate_pdg()

Benjamin LaHaise (1):
      x86/mm/32: Add support for 64-bit __get_user() on 32-bit kernels

Borislav Petkov (2):
      x86/hweight: Get rid of the special calling convention
      x86/asm/entry: Make thunk&#39;s restore a local label

Dave Hansen (7):
      x86/signals: Add missing signal_compat code for x86 features
      x86/signals: Add build-time checks to the siginfo compat code
      x86/mpx, selftests: Add MPX self test
      x86/mm: Move swap offset/type up in PTE to work around erratum
      x86/mm: Ignore A/D bits in pte/pmd/pud_none()
      x86/mm: Disallow running with 32-bit PTEs to work around erratum
      x86/mm: Use pte_none() to test for empty PTE

Dmitry Safonov (2):
      x86/vdso: Add mremap hook to vm_special_mapping
      selftests/x86: Add vDSO mremap() test

H. Peter Anvin (10):
      x86, bitops: remove use of &quot;sbb&quot; to return CF
      x86, asm: use bool for bitops and other assembly outputs
      x86, asm: change the GEN_*_RMWcc() macros to not quote the condition
      x86, asm: define CC_SET() and CC_OUT() macros
      x86, asm: change GEN_*_RMWcc() to use CC_SET()/CC_OUT()
      x86, asm: Use CC_SET()/CC_OUT() in &lt;asm/bitops.h&gt;
      x86, asm: Use CC_SET()/CC_OUT() in &lt;asm/percpu.h&gt;
      x86, asm: Use CC_SET()/CC_OUT() in &lt;asm/rwsem.h&gt;
      x86, asm, boot: Use CC_SET()/CC_OUT() in arch/x86/boot/boot.h
      x86, asm: Use CC_SET()/CC_OUT() and static_cpu_has() in archrandom.h

H.J. Lu (1):
      x86/syscalls: Add compat_sys_preadv64v2/compat_sys_pwritev64v2

Ingo Molnar (2):
      x86/mm/hotplug: Don&#39;t remove PGD entries in remove_pagetable()
      x86/dumpstack: Rename thread_struct::sig_on_uaccess_error to sig_on_uaccess_err

Jiri Kosina (1):
      x86/mm/pat, /dev/mem: Remove superfluous error message

Mathias Krause (1):
      x86/extable: Ensure entries are swapped completely when sorting

Paolo Bonzini (2):
      x86/entry: Avoid interrupt flag save and restore
      x86/entry: Inline enter_from_user_mode()


 arch/x86/Kconfig                               |    5 -
 arch/x86/boot/bitops.h                         |    8 +-
 arch/x86/boot/boot.h                           |   18 +-
 arch/x86/boot/cpu.c                            |    2 +
 arch/x86/boot/cpucheck.c                       |   33 +
 arch/x86/boot/cpuflags.c                       |    1 +
 arch/x86/boot/cpuflags.h                       |    1 +
 arch/x86/boot/string.c                         |    2 +-
 arch/x86/entry/common.c                        |    6 +-
 arch/x86/entry/entry_32.S                      |   11 +
 arch/x86/entry/entry_64.S                      |   11 +
 arch/x86/entry/syscalls/syscall_64.tbl         |    4 +-
 arch/x86/entry/thunk_64.S                      |    6 +-
 arch/x86/entry/vdso/Makefile                   |    5 +-
 arch/x86/entry/vdso/vdso32/sigreturn.S         |    8 -
 arch/x86/entry/vdso/vdso32/system_call.S       |    7 +-
 arch/x86/entry/vdso/vma.c                      |   47 +-
 arch/x86/entry/vsyscall/vsyscall_64.c          |   10 +-
 arch/x86/include/asm/apm.h                     |    6 +-
 arch/x86/include/asm/arch_hweight.h            |   24 +-
 arch/x86/include/asm/archrandom.h              |  132 +-
 arch/x86/include/asm/asm.h                     |   12 +
 arch/x86/include/asm/atomic.h                  |   16 +-
 arch/x86/include/asm/atomic64_64.h             |   18 +-
 arch/x86/include/asm/bitops.h                  |   50 +-
 arch/x86/include/asm/checksum_32.h             |    3 +-
 arch/x86/include/asm/compat.h                  |   11 +
 arch/x86/include/asm/cpu.h                     |    1 -
 arch/x86/include/asm/efi.h                     |    1 -
 arch/x86/include/asm/local.h                   |   16 +-
 arch/x86/include/asm/percpu.h                  |   17 +-
 arch/x86/include/asm/pgtable.h                 |   13 +-
 arch/x86/include/asm/pgtable_64.h              |   26 +-
 arch/x86/include/asm/pgtable_types.h           |    8 +-
 arch/x86/include/asm/preempt.h                 |    2 +-
 arch/x86/include/asm/processor.h               |   20 +-
 arch/x86/include/asm/rmwcc.h                   |   20 +-
 arch/x86/include/asm/rwsem.h                   |   17 +-
 arch/x86/include/asm/signal.h                  |    6 +-
 arch/x86/include/asm/smp.h                     |    6 -
 arch/x86/include/asm/sync_bitops.h             |   18 +-
 arch/x86/include/asm/thread_info.h             |    9 -
 arch/x86/include/asm/uaccess.h                 |   33 +-
 arch/x86/include/asm/unistd.h                  |    2 +
 arch/x86/kernel/asm-offsets.c                  |    4 +-
 arch/x86/kernel/cpu/common.c                   |    2 +-
 arch/x86/kernel/cpu/rdrand.c                   |    4 +-
 arch/x86/kernel/dumpstack.c                    |   20 +-
 arch/x86/kernel/dumpstack_64.c                 |   12 +-
 arch/x86/kernel/i386_ksyms_32.c                |    2 +
 arch/x86/kernel/signal_compat.c                |  108 ++
 arch/x86/kernel/smpboot.c                      |    1 -
 arch/x86/kernel/vm86_32.c                      |    5 +-
 arch/x86/kernel/x8664_ksyms_64.c               |    3 +
 arch/x86/lib/Makefile                          |    2 +-
 arch/x86/lib/copy_user_64.S                    |    8 +-
 arch/x86/lib/csum-wrappers_64.c                |    1 +
 arch/x86/lib/getuser.S                         |   20 +-
 arch/x86/lib/hweight.S                         |   77 ++
 arch/x86/lib/putuser.S                         |   10 +-
 arch/x86/lib/usercopy_64.c                     |    2 +-
 arch/x86/mm/extable.c                          |    2 +-
 arch/x86/mm/fault.c                            |    4 +-
 arch/x86/mm/init_64.c                          |   37 +-
 arch/x86/mm/pageattr.c                         |   37 +-
 arch/x86/mm/pat.c                              |    5 +-
 arch/x86/mm/pgtable_32.c                       |    2 +-
 arch/x86/platform/efi/efi.c                    |    2 -
 arch/x86/platform/efi/efi_32.c                 |    3 -
 arch/x86/platform/efi/efi_64.c                 |    5 -
 arch/x86/xen/enlighten.c                       |    4 +-
 drivers/char/mem.c                             |    6 +-
 drivers/pnp/isapnp/proc.c                      |    2 +-
 fs/read_write.c                                |   18 +
 include/linux/context_tracking.h               |   15 +
 include/linux/mm_types.h                       |    3 +
 include/linux/random.h                         |   12 +-
 lib/Makefile                                   |    5 -
 lib/bitmap.c                                   |    2 +-
 lib/hweight.c                                  |    4 +
 mm/mmap.c                                      |   10 +
 tools/testing/selftests/x86/Makefile           |    4 +-
 tools/testing/selftests/x86/mpx-debug.h        |   14 +
 tools/testing/selftests/x86/mpx-dig.c          |  498 ++++++++
 tools/testing/selftests/x86/mpx-hw.h           |  123 ++
 tools/testing/selftests/x86/mpx-mini-test.c    | 1585 ++++++++++++++++++++++++
 tools/testing/selftests/x86/mpx-mm.h           |    9 +
 tools/testing/selftests/x86/test_mremap_vdso.c |  111 ++
 88 files changed, 3069 insertions(+), 406 deletions(-)
 create mode 100644 arch/x86/lib/hweight.S
 create mode 100644 tools/testing/selftests/x86/mpx-debug.h
 create mode 100644 tools/testing/selftests/x86/mpx-dig.c
 create mode 100644 tools/testing/selftests/x86/mpx-hw.h
 create mode 100644 tools/testing/selftests/x86/mpx-mini-test.c
 create mode 100644 tools/testing/selftests/x86/mpx-mm.h
 create mode 100644 tools/testing/selftests/x86/test_mremap_vdso.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index d9a94da0c29f..df884a522c39 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -294,11 +294,6 @@</span> <span class="p_context"> config X86_32_LAZY_GS</span>
 	def_bool y
 	depends on X86_32 &amp;&amp; !CC_STACKPROTECTOR
 
<span class="p_del">-config ARCH_HWEIGHT_CFLAGS</span>
<span class="p_del">-	string</span>
<span class="p_del">-	default &quot;-fcall-saved-ecx -fcall-saved-edx&quot; if X86_32</span>
<span class="p_del">-	default &quot;-fcall-saved-rdi -fcall-saved-rsi -fcall-saved-rdx -fcall-saved-rcx -fcall-saved-r8 -fcall-saved-r9 -fcall-saved-r10 -fcall-saved-r11&quot; if X86_64</span>
<span class="p_del">-</span>
 config ARCH_SUPPORTS_UPROBES
 	def_bool y
 
<span class="p_header">diff --git a/arch/x86/boot/bitops.h b/arch/x86/boot/bitops.h</span>
<span class="p_header">index 878e4b9940d9..0d41d68131cc 100644</span>
<span class="p_header">--- a/arch/x86/boot/bitops.h</span>
<span class="p_header">+++ b/arch/x86/boot/bitops.h</span>
<span class="p_chunk">@@ -16,14 +16,16 @@</span> <span class="p_context"></span>
 #define BOOT_BITOPS_H
 #define _LINUX_BITOPS_H		/* Inhibit inclusion of &lt;linux/bitops.h&gt; */
 
<span class="p_del">-static inline int constant_test_bit(int nr, const void *addr)</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool constant_test_bit(int nr, const void *addr)</span>
 {
 	const u32 *p = (const u32 *)addr;
 	return ((1UL &lt;&lt; (nr &amp; 31)) &amp; (p[nr &gt;&gt; 5])) != 0;
 }
<span class="p_del">-static inline int variable_test_bit(int nr, const void *addr)</span>
<span class="p_add">+static inline bool variable_test_bit(int nr, const void *addr)</span>
 {
<span class="p_del">-	u8 v;</span>
<span class="p_add">+	bool v;</span>
 	const u32 *p = (const u32 *)addr;
 
 	asm(&quot;btl %2,%1; setc %0&quot; : &quot;=qm&quot; (v) : &quot;m&quot; (*p), &quot;Ir&quot; (nr));
<span class="p_header">diff --git a/arch/x86/boot/boot.h b/arch/x86/boot/boot.h</span>
<span class="p_header">index 9011a88353de..e5612f3e3b57 100644</span>
<span class="p_header">--- a/arch/x86/boot/boot.h</span>
<span class="p_header">+++ b/arch/x86/boot/boot.h</span>
<span class="p_chunk">@@ -24,6 +24,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/types.h&gt;
 #include &lt;linux/edd.h&gt;
 #include &lt;asm/setup.h&gt;
<span class="p_add">+#include &lt;asm/asm.h&gt;</span>
 #include &quot;bitops.h&quot;
 #include &quot;ctype.h&quot;
 #include &quot;cpuflags.h&quot;
<span class="p_chunk">@@ -176,18 +177,18 @@</span> <span class="p_context"> static inline void wrgs32(u32 v, addr_t addr)</span>
 }
 
 /* Note: these only return true/false, not a signed return value! */
<span class="p_del">-static inline int memcmp_fs(const void *s1, addr_t s2, size_t len)</span>
<span class="p_add">+static inline bool memcmp_fs(const void *s1, addr_t s2, size_t len)</span>
 {
<span class="p_del">-	u8 diff;</span>
<span class="p_del">-	asm volatile(&quot;fs; repe; cmpsb; setnz %0&quot;</span>
<span class="p_del">-		     : &quot;=qm&quot; (diff), &quot;+D&quot; (s1), &quot;+S&quot; (s2), &quot;+c&quot; (len));</span>
<span class="p_add">+	bool diff;</span>
<span class="p_add">+	asm volatile(&quot;fs; repe; cmpsb&quot; CC_SET(nz)</span>
<span class="p_add">+		     : CC_OUT(nz) (diff), &quot;+D&quot; (s1), &quot;+S&quot; (s2), &quot;+c&quot; (len));</span>
 	return diff;
 }
<span class="p_del">-static inline int memcmp_gs(const void *s1, addr_t s2, size_t len)</span>
<span class="p_add">+static inline bool memcmp_gs(const void *s1, addr_t s2, size_t len)</span>
 {
<span class="p_del">-	u8 diff;</span>
<span class="p_del">-	asm volatile(&quot;gs; repe; cmpsb; setnz %0&quot;</span>
<span class="p_del">-		     : &quot;=qm&quot; (diff), &quot;+D&quot; (s1), &quot;+S&quot; (s2), &quot;+c&quot; (len));</span>
<span class="p_add">+	bool diff;</span>
<span class="p_add">+	asm volatile(&quot;gs; repe; cmpsb&quot; CC_SET(nz)</span>
<span class="p_add">+		     : CC_OUT(nz) (diff), &quot;+D&quot; (s1), &quot;+S&quot; (s2), &quot;+c&quot; (len));</span>
 	return diff;
 }
 
<span class="p_chunk">@@ -294,6 +295,7 @@</span> <span class="p_context"> static inline int cmdline_find_option_bool(const char *option)</span>
 
 /* cpu.c, cpucheck.c */
 int check_cpu(int *cpu_level_ptr, int *req_level_ptr, u32 **err_flags_ptr);
<span class="p_add">+int check_knl_erratum(void);</span>
 int validate_cpu(void);
 
 /* early_serial_console.c */
<span class="p_header">diff --git a/arch/x86/boot/cpu.c b/arch/x86/boot/cpu.c</span>
<span class="p_header">index 29207f69ae8c..26240dde081e 100644</span>
<span class="p_header">--- a/arch/x86/boot/cpu.c</span>
<span class="p_header">+++ b/arch/x86/boot/cpu.c</span>
<span class="p_chunk">@@ -93,6 +93,8 @@</span> <span class="p_context"> int validate_cpu(void)</span>
 		show_cap_strs(err_flags);
 		putchar(&#39;\n&#39;);
 		return -1;
<span class="p_add">+	} else if (check_knl_erratum()) {</span>
<span class="p_add">+		return -1;</span>
 	} else {
 		return 0;
 	}
<span class="p_header">diff --git a/arch/x86/boot/cpucheck.c b/arch/x86/boot/cpucheck.c</span>
<span class="p_header">index 1fd7d575092e..4ad7d70e8739 100644</span>
<span class="p_header">--- a/arch/x86/boot/cpucheck.c</span>
<span class="p_header">+++ b/arch/x86/boot/cpucheck.c</span>
<span class="p_chunk">@@ -24,6 +24,7 @@</span> <span class="p_context"></span>
 # include &quot;boot.h&quot;
 #endif
 #include &lt;linux/types.h&gt;
<span class="p_add">+#include &lt;asm/intel-family.h&gt;</span>
 #include &lt;asm/processor-flags.h&gt;
 #include &lt;asm/required-features.h&gt;
 #include &lt;asm/msr-index.h&gt;
<span class="p_chunk">@@ -175,6 +176,8 @@</span> <span class="p_context"> int check_cpu(int *cpu_level_ptr, int *req_level_ptr, u32 **err_flags_ptr)</span>
 			puts(&quot;WARNING: PAE disabled. Use parameter &#39;forcepae&#39; to enable at your own risk!\n&quot;);
 		}
 	}
<span class="p_add">+	if (!err)</span>
<span class="p_add">+		err = check_knl_erratum();</span>
 
 	if (err_flags_ptr)
 		*err_flags_ptr = err ? err_flags : NULL;
<span class="p_chunk">@@ -185,3 +188,33 @@</span> <span class="p_context"> int check_cpu(int *cpu_level_ptr, int *req_level_ptr, u32 **err_flags_ptr)</span>
 
 	return (cpu.level &lt; req_level || err) ? -1 : 0;
 }
<span class="p_add">+</span>
<span class="p_add">+int check_knl_erratum(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * First check for the affected model/family:</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!is_intel() ||</span>
<span class="p_add">+	    cpu.family != 6 ||</span>
<span class="p_add">+	    cpu.model != INTEL_FAM6_XEON_PHI_KNL)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This erratum affects the Accessed/Dirty bits, and can</span>
<span class="p_add">+	 * cause stray bits to be set in !Present PTEs.  We have</span>
<span class="p_add">+	 * enough bits in our 64-bit PTEs (which we have on real</span>
<span class="p_add">+	 * 64-bit mode or PAE) to avoid using these troublesome</span>
<span class="p_add">+	 * bits.  But, we do not have enough space in our 32-bit</span>
<span class="p_add">+	 * PTEs.  So, refuse to run on 32-bit non-PAE kernels.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_X86_64) || IS_ENABLED(CONFIG_X86_PAE))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	puts(&quot;This 32-bit kernel can not run on this Xeon Phi x200\n&quot;</span>
<span class="p_add">+	     &quot;processor due to a processor erratum.  Use a 64-bit\n&quot;</span>
<span class="p_add">+	     &quot;kernel, or enable PAE in this 32-bit kernel.\n\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return -1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_header">diff --git a/arch/x86/boot/cpuflags.c b/arch/x86/boot/cpuflags.c</span>
<span class="p_header">index 431fa5f84537..6687ab953257 100644</span>
<span class="p_header">--- a/arch/x86/boot/cpuflags.c</span>
<span class="p_header">+++ b/arch/x86/boot/cpuflags.c</span>
<span class="p_chunk">@@ -102,6 +102,7 @@</span> <span class="p_context"> void get_cpuflags(void)</span>
 			cpuid(0x1, &amp;tfms, &amp;ignored, &amp;cpu.flags[4],
 			      &amp;cpu.flags[0]);
 			cpu.level = (tfms &gt;&gt; 8) &amp; 15;
<span class="p_add">+			cpu.family = cpu.level;</span>
 			cpu.model = (tfms &gt;&gt; 4) &amp; 15;
 			if (cpu.level &gt;= 6)
 				cpu.model += ((tfms &gt;&gt; 16) &amp; 0xf) &lt;&lt; 4;
<span class="p_header">diff --git a/arch/x86/boot/cpuflags.h b/arch/x86/boot/cpuflags.h</span>
<span class="p_header">index 4cb404fd45ce..15ad56a3f905 100644</span>
<span class="p_header">--- a/arch/x86/boot/cpuflags.h</span>
<span class="p_header">+++ b/arch/x86/boot/cpuflags.h</span>
<span class="p_chunk">@@ -6,6 +6,7 @@</span> <span class="p_context"></span>
 
 struct cpu_features {
 	int level;		/* Family, or 64 for x86-64 */
<span class="p_add">+	int family;		/* Family, always */</span>
 	int model;
 	u32 flags[NCAPINTS];
 };
<span class="p_header">diff --git a/arch/x86/boot/string.c b/arch/x86/boot/string.c</span>
<span class="p_header">index 318b8465d302..cc3bd583dce1 100644</span>
<span class="p_header">--- a/arch/x86/boot/string.c</span>
<span class="p_header">+++ b/arch/x86/boot/string.c</span>
<span class="p_chunk">@@ -17,7 +17,7 @@</span> <span class="p_context"></span>
 
 int memcmp(const void *s1, const void *s2, size_t len)
 {
<span class="p_del">-	u8 diff;</span>
<span class="p_add">+	bool diff;</span>
 	asm(&quot;repe; cmpsb; setnz %0&quot;
 	    : &quot;=qm&quot; (diff), &quot;+D&quot; (s1), &quot;+S&quot; (s2), &quot;+c&quot; (len));
 	return diff;
<span class="p_header">diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c</span>
<span class="p_header">index ec138e538c44..9e1e27d31c6d 100644</span>
<span class="p_header">--- a/arch/x86/entry/common.c</span>
<span class="p_header">+++ b/arch/x86/entry/common.c</span>
<span class="p_chunk">@@ -40,10 +40,10 @@</span> <span class="p_context"> static struct thread_info *pt_regs_to_thread_info(struct pt_regs *regs)</span>
 
 #ifdef CONFIG_CONTEXT_TRACKING
 /* Called on entry from user mode with IRQs off. */
<span class="p_del">-__visible void enter_from_user_mode(void)</span>
<span class="p_add">+__visible inline void enter_from_user_mode(void)</span>
 {
 	CT_WARN_ON(ct_state() != CONTEXT_USER);
<span class="p_del">-	user_exit();</span>
<span class="p_add">+	user_exit_irqoff();</span>
 }
 #else
 static inline void enter_from_user_mode(void) {}
<span class="p_chunk">@@ -274,7 +274,7 @@</span> <span class="p_context"> __visible inline void prepare_exit_to_usermode(struct pt_regs *regs)</span>
 	ti-&gt;status &amp;= ~TS_COMPAT;
 #endif
 
<span class="p_del">-	user_enter();</span>
<span class="p_add">+	user_enter_irqoff();</span>
 }
 
 #define SYSCALL_EXIT_WORK_FLAGS				\
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index 983e5d3a0d27..0b56666e6039 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -1153,3 +1153,14 @@</span> <span class="p_context"> ENTRY(async_page_fault)</span>
 	jmp	error_code
 END(async_page_fault)
 #endif
<span class="p_add">+</span>
<span class="p_add">+ENTRY(rewind_stack_do_exit)</span>
<span class="p_add">+	/* Prevent any naive code from trying to unwind to our caller. */</span>
<span class="p_add">+	xorl	%ebp, %ebp</span>
<span class="p_add">+</span>
<span class="p_add">+	movl	PER_CPU_VAR(cpu_current_top_of_stack), %esi</span>
<span class="p_add">+	leal	-TOP_OF_KERNEL_STACK_PADDING-PTREGS_SIZE(%esi), %esp</span>
<span class="p_add">+</span>
<span class="p_add">+	call	do_exit</span>
<span class="p_add">+1:	jmp 1b</span>
<span class="p_add">+END(rewind_stack_do_exit)</span>
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index 9ee0da1807ed..b846875aeea6 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -1423,3 +1423,14 @@</span> <span class="p_context"> ENTRY(ignore_sysret)</span>
 	mov	$-ENOSYS, %eax
 	sysret
 END(ignore_sysret)
<span class="p_add">+</span>
<span class="p_add">+ENTRY(rewind_stack_do_exit)</span>
<span class="p_add">+	/* Prevent any naive code from trying to unwind to our caller. */</span>
<span class="p_add">+	xorl	%ebp, %ebp</span>
<span class="p_add">+</span>
<span class="p_add">+	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rax</span>
<span class="p_add">+	leaq	-TOP_OF_KERNEL_STACK_PADDING-PTREGS_SIZE(%rax), %rsp</span>
<span class="p_add">+</span>
<span class="p_add">+	call	do_exit</span>
<span class="p_add">+1:	jmp 1b</span>
<span class="p_add">+END(rewind_stack_do_exit)</span>
<span class="p_header">diff --git a/arch/x86/entry/syscalls/syscall_64.tbl b/arch/x86/entry/syscalls/syscall_64.tbl</span>
<span class="p_header">index 555263e385c9..e9ce9c7c39b4 100644</span>
<span class="p_header">--- a/arch/x86/entry/syscalls/syscall_64.tbl</span>
<span class="p_header">+++ b/arch/x86/entry/syscalls/syscall_64.tbl</span>
<span class="p_chunk">@@ -374,5 +374,5 @@</span> <span class="p_context"></span>
 543	x32	io_setup		compat_sys_io_setup
 544	x32	io_submit		compat_sys_io_submit
 545	x32	execveat		compat_sys_execveat/ptregs
<span class="p_del">-534	x32	preadv2			compat_sys_preadv2</span>
<span class="p_del">-535	x32	pwritev2		compat_sys_pwritev2</span>
<span class="p_add">+546	x32	preadv2			compat_sys_preadv64v2</span>
<span class="p_add">+547	x32	pwritev2		compat_sys_pwritev64v2</span>
<span class="p_header">diff --git a/arch/x86/entry/thunk_64.S b/arch/x86/entry/thunk_64.S</span>
<span class="p_header">index 027aec4a74df..627ecbcb2e62 100644</span>
<span class="p_header">--- a/arch/x86/entry/thunk_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/thunk_64.S</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"></span>
 	.endif
 
 	call \func
<span class="p_del">-	jmp  restore</span>
<span class="p_add">+	jmp  .L_restore</span>
 	_ASM_NOKPROBE(\name)
 	.endm
 
<span class="p_chunk">@@ -54,7 +54,7 @@</span> <span class="p_context"></span>
 #if defined(CONFIG_TRACE_IRQFLAGS) \
  || defined(CONFIG_DEBUG_LOCK_ALLOC) \
  || defined(CONFIG_PREEMPT)
<span class="p_del">-restore:</span>
<span class="p_add">+.L_restore:</span>
 	popq %r11
 	popq %r10
 	popq %r9
<span class="p_chunk">@@ -66,5 +66,5 @@</span> <span class="p_context"></span>
 	popq %rdi
 	popq %rbp
 	ret
<span class="p_del">-	_ASM_NOKPROBE(restore)</span>
<span class="p_add">+	_ASM_NOKPROBE(.L_restore)</span>
 #endif
<span class="p_header">diff --git a/arch/x86/entry/vdso/Makefile b/arch/x86/entry/vdso/Makefile</span>
<span class="p_header">index 253b72eaade6..68b63fddc209 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/Makefile</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/Makefile</span>
<span class="p_chunk">@@ -134,7 +134,7 @@</span> <span class="p_context"> VDSO_LDFLAGS_vdso32.lds = -m32 -Wl,-m,elf_i386 -Wl,-soname=linux-gate.so.1</span>
 override obj-dirs = $(dir $(obj)) $(obj)/vdso32/
 
 targets += vdso32/vdso32.lds
<span class="p_del">-targets += vdso32/note.o vdso32/vclock_gettime.o vdso32/system_call.o</span>
<span class="p_add">+targets += vdso32/note.o vdso32/system_call.o vdso32/sigreturn.o</span>
 targets += vdso32/vclock_gettime.o
 
 KBUILD_AFLAGS_32 := $(filter-out -m64,$(KBUILD_AFLAGS)) -DBUILD_VDSO
<span class="p_chunk">@@ -156,7 +156,8 @@</span> <span class="p_context"> $(obj)/vdso32.so.dbg: FORCE \</span>
 		      $(obj)/vdso32/vdso32.lds \
 		      $(obj)/vdso32/vclock_gettime.o \
 		      $(obj)/vdso32/note.o \
<span class="p_del">-		      $(obj)/vdso32/system_call.o</span>
<span class="p_add">+		      $(obj)/vdso32/system_call.o \</span>
<span class="p_add">+		      $(obj)/vdso32/sigreturn.o</span>
 	$(call if_changed,vdso)
 
 #
<span class="p_header">diff --git a/arch/x86/entry/vdso/vdso32/sigreturn.S b/arch/x86/entry/vdso/vdso32/sigreturn.S</span>
<span class="p_header">index d7ec4e251c0a..20633e026e82 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vdso32/sigreturn.S</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vdso32/sigreturn.S</span>
<span class="p_chunk">@@ -1,11 +1,3 @@</span> <span class="p_context"></span>
<span class="p_del">-/*</span>
<span class="p_del">- * Common code for the sigreturn entry points in vDSO images.</span>
<span class="p_del">- * So far this code is the same for both int80 and sysenter versions.</span>
<span class="p_del">- * This file is #include&#39;d by int80.S et al to define them first thing.</span>
<span class="p_del">- * The kernel assumes that the addresses of these routines are constant</span>
<span class="p_del">- * for all vDSO implementations.</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
 #include &lt;linux/linkage.h&gt;
 #include &lt;asm/unistd_32.h&gt;
 #include &lt;asm/asm-offsets.h&gt;
<span class="p_header">diff --git a/arch/x86/entry/vdso/vdso32/system_call.S b/arch/x86/entry/vdso/vdso32/system_call.S</span>
<span class="p_header">index 0109ac6cb79c..ed4bc9731cbb 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vdso32/system_call.S</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vdso32/system_call.S</span>
<span class="p_chunk">@@ -2,16 +2,11 @@</span> <span class="p_context"></span>
  * AT_SYSINFO entry point
 */
 
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
 #include &lt;asm/dwarf2.h&gt;
 #include &lt;asm/cpufeatures.h&gt;
 #include &lt;asm/alternative-asm.h&gt;
 
<span class="p_del">-/*</span>
<span class="p_del">- * First get the common code for the sigreturn entry points.</span>
<span class="p_del">- * This must come first.</span>
<span class="p_del">- */</span>
<span class="p_del">-#include &quot;sigreturn.S&quot;</span>
<span class="p_del">-</span>
 	.text
 	.globl __kernel_vsyscall
 	.type __kernel_vsyscall,@function
<span class="p_header">diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">index ab220ac9b3b9..3329844e3c43 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vma.c</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/random.h&gt;
 #include &lt;linux/elf.h&gt;
 #include &lt;linux/cpu.h&gt;
<span class="p_add">+#include &lt;linux/ptrace.h&gt;</span>
 #include &lt;asm/pvclock.h&gt;
 #include &lt;asm/vgtod.h&gt;
 #include &lt;asm/proto.h&gt;
<span class="p_chunk">@@ -97,10 +98,40 @@</span> <span class="p_context"> static int vdso_fault(const struct vm_special_mapping *sm,</span>
 	return 0;
 }
 
<span class="p_del">-static const struct vm_special_mapping text_mapping = {</span>
<span class="p_del">-	.name = &quot;[vdso]&quot;,</span>
<span class="p_del">-	.fault = vdso_fault,</span>
<span class="p_del">-};</span>
<span class="p_add">+static void vdso_fix_landing(const struct vdso_image *image,</span>
<span class="p_add">+		struct vm_area_struct *new_vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+#if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION</span>
<span class="p_add">+	if (in_ia32_syscall() &amp;&amp; image == &amp;vdso_image_32) {</span>
<span class="p_add">+		struct pt_regs *regs = current_pt_regs();</span>
<span class="p_add">+		unsigned long vdso_land = image-&gt;sym_int80_landing_pad;</span>
<span class="p_add">+		unsigned long old_land_addr = vdso_land +</span>
<span class="p_add">+			(unsigned long)current-&gt;mm-&gt;context.vdso;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Fixing userspace landing - look at do_fast_syscall_32 */</span>
<span class="p_add">+		if (regs-&gt;ip == old_land_addr)</span>
<span class="p_add">+			regs-&gt;ip = new_vma-&gt;vm_start + vdso_land;</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int vdso_mremap(const struct vm_special_mapping *sm,</span>
<span class="p_add">+		struct vm_area_struct *new_vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long new_size = new_vma-&gt;vm_end - new_vma-&gt;vm_start;</span>
<span class="p_add">+	const struct vdso_image *image = current-&gt;mm-&gt;context.vdso_image;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (image-&gt;size != new_size)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (WARN_ON_ONCE(current-&gt;mm != new_vma-&gt;vm_mm))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+</span>
<span class="p_add">+	vdso_fix_landing(image, new_vma);</span>
<span class="p_add">+	current-&gt;mm-&gt;context.vdso = (void __user *)new_vma-&gt;vm_start;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
 
 static int vvar_fault(const struct vm_special_mapping *sm,
 		      struct vm_area_struct *vma, struct vm_fault *vmf)
<span class="p_chunk">@@ -151,6 +182,12 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, bool calculate_addr)</span>
 	struct vm_area_struct *vma;
 	unsigned long addr, text_start;
 	int ret = 0;
<span class="p_add">+</span>
<span class="p_add">+	static const struct vm_special_mapping vdso_mapping = {</span>
<span class="p_add">+		.name = &quot;[vdso]&quot;,</span>
<span class="p_add">+		.fault = vdso_fault,</span>
<span class="p_add">+		.mremap = vdso_mremap,</span>
<span class="p_add">+	};</span>
 	static const struct vm_special_mapping vvar_mapping = {
 		.name = &quot;[vvar]&quot;,
 		.fault = vvar_fault,
<span class="p_chunk">@@ -185,7 +222,7 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, bool calculate_addr)</span>
 				       image-&gt;size,
 				       VM_READ|VM_EXEC|
 				       VM_MAYREAD|VM_MAYWRITE|VM_MAYEXEC,
<span class="p_del">-				       &amp;text_mapping);</span>
<span class="p_add">+				       &amp;vdso_mapping);</span>
 
 	if (IS_ERR(vma)) {
 		ret = PTR_ERR(vma);
<span class="p_header">diff --git a/arch/x86/entry/vsyscall/vsyscall_64.c b/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_header">index 174c2549939d..75fc719b7f31 100644</span>
<span class="p_header">--- a/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_header">+++ b/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_chunk">@@ -96,7 +96,7 @@</span> <span class="p_context"> static bool write_ok_or_segv(unsigned long ptr, size_t size)</span>
 {
 	/*
 	 * XXX: if access_ok, get_user, and put_user handled
<span class="p_del">-	 * sig_on_uaccess_error, this could go away.</span>
<span class="p_add">+	 * sig_on_uaccess_err, this could go away.</span>
 	 */
 
 	if (!access_ok(VERIFY_WRITE, (void __user *)ptr, size)) {
<span class="p_chunk">@@ -125,7 +125,7 @@</span> <span class="p_context"> bool emulate_vsyscall(struct pt_regs *regs, unsigned long address)</span>
 	struct task_struct *tsk;
 	unsigned long caller;
 	int vsyscall_nr, syscall_nr, tmp;
<span class="p_del">-	int prev_sig_on_uaccess_error;</span>
<span class="p_add">+	int prev_sig_on_uaccess_err;</span>
 	long ret;
 
 	/*
<span class="p_chunk">@@ -221,8 +221,8 @@</span> <span class="p_context"> bool emulate_vsyscall(struct pt_regs *regs, unsigned long address)</span>
 	 * With a real vsyscall, page faults cause SIGSEGV.  We want to
 	 * preserve that behavior to make writing exploits harder.
 	 */
<span class="p_del">-	prev_sig_on_uaccess_error = current_thread_info()-&gt;sig_on_uaccess_error;</span>
<span class="p_del">-	current_thread_info()-&gt;sig_on_uaccess_error = 1;</span>
<span class="p_add">+	prev_sig_on_uaccess_err = current-&gt;thread.sig_on_uaccess_err;</span>
<span class="p_add">+	current-&gt;thread.sig_on_uaccess_err = 1;</span>
 
 	ret = -EFAULT;
 	switch (vsyscall_nr) {
<span class="p_chunk">@@ -243,7 +243,7 @@</span> <span class="p_context"> bool emulate_vsyscall(struct pt_regs *regs, unsigned long address)</span>
 		break;
 	}
 
<span class="p_del">-	current_thread_info()-&gt;sig_on_uaccess_error = prev_sig_on_uaccess_error;</span>
<span class="p_add">+	current-&gt;thread.sig_on_uaccess_err = prev_sig_on_uaccess_err;</span>
 
 check_fault:
 	if (ret == -EFAULT) {
<span class="p_header">diff --git a/arch/x86/include/asm/apm.h b/arch/x86/include/asm/apm.h</span>
<span class="p_header">index 20370c6db74b..93eebc636c76 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/apm.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/apm.h</span>
<span class="p_chunk">@@ -45,11 +45,11 @@</span> <span class="p_context"> static inline void apm_bios_call_asm(u32 func, u32 ebx_in, u32 ecx_in,</span>
 		: &quot;memory&quot;, &quot;cc&quot;);
 }
 
<span class="p_del">-static inline u8 apm_bios_call_simple_asm(u32 func, u32 ebx_in,</span>
<span class="p_del">-						u32 ecx_in, u32 *eax)</span>
<span class="p_add">+static inline bool apm_bios_call_simple_asm(u32 func, u32 ebx_in,</span>
<span class="p_add">+					    u32 ecx_in, u32 *eax)</span>
 {
 	int	cx, dx, si;
<span class="p_del">-	u8	error;</span>
<span class="p_add">+	bool	error;</span>
 
 	/*
 	 * N.B. We do NOT need a cld after the BIOS call
<span class="p_header">diff --git a/arch/x86/include/asm/arch_hweight.h b/arch/x86/include/asm/arch_hweight.h</span>
<span class="p_header">index 02e799fa43d1..e7cd63175de4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/arch_hweight.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/arch_hweight.h</span>
<span class="p_chunk">@@ -4,8 +4,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/cpufeatures.h&gt;
 
 #ifdef CONFIG_64BIT
<span class="p_del">-/* popcnt %edi, %eax -- redundant REX prefix for alignment */</span>
<span class="p_del">-#define POPCNT32 &quot;.byte 0xf3,0x40,0x0f,0xb8,0xc7&quot;</span>
<span class="p_add">+/* popcnt %edi, %eax */</span>
<span class="p_add">+#define POPCNT32 &quot;.byte 0xf3,0x0f,0xb8,0xc7&quot;</span>
 /* popcnt %rdi, %rax */
 #define POPCNT64 &quot;.byte 0xf3,0x48,0x0f,0xb8,0xc7&quot;
 #define REG_IN &quot;D&quot;
<span class="p_chunk">@@ -17,19 +17,15 @@</span> <span class="p_context"></span>
 #define REG_OUT &quot;a&quot;
 #endif
 
<span class="p_del">-/*</span>
<span class="p_del">- * __sw_hweightXX are called from within the alternatives below</span>
<span class="p_del">- * and callee-clobbered registers need to be taken care of. See</span>
<span class="p_del">- * ARCH_HWEIGHT_CFLAGS in &lt;arch/x86/Kconfig&gt; for the respective</span>
<span class="p_del">- * compiler switches.</span>
<span class="p_del">- */</span>
<span class="p_add">+#define __HAVE_ARCH_SW_HWEIGHT</span>
<span class="p_add">+</span>
 static __always_inline unsigned int __arch_hweight32(unsigned int w)
 {
<span class="p_del">-	unsigned int res = 0;</span>
<span class="p_add">+	unsigned int res;</span>
 
 	asm (ALTERNATIVE(&quot;call __sw_hweight32&quot;, POPCNT32, X86_FEATURE_POPCNT)
<span class="p_del">-		     : &quot;=&quot;REG_OUT (res)</span>
<span class="p_del">-		     : REG_IN (w));</span>
<span class="p_add">+			 : &quot;=&quot;REG_OUT (res)</span>
<span class="p_add">+			 : REG_IN (w));</span>
 
 	return res;
 }
<span class="p_chunk">@@ -53,11 +49,11 @@</span> <span class="p_context"> static inline unsigned long __arch_hweight64(__u64 w)</span>
 #else
 static __always_inline unsigned long __arch_hweight64(__u64 w)
 {
<span class="p_del">-	unsigned long res = 0;</span>
<span class="p_add">+	unsigned long res;</span>
 
 	asm (ALTERNATIVE(&quot;call __sw_hweight64&quot;, POPCNT64, X86_FEATURE_POPCNT)
<span class="p_del">-		     : &quot;=&quot;REG_OUT (res)</span>
<span class="p_del">-		     : REG_IN (w));</span>
<span class="p_add">+			 : &quot;=&quot;REG_OUT (res)</span>
<span class="p_add">+			 : REG_IN (w));</span>
 
 	return res;
 }
<span class="p_header">diff --git a/arch/x86/include/asm/archrandom.h b/arch/x86/include/asm/archrandom.h</span>
<span class="p_header">index 69f1366f1aa3..5b0579abb398 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/archrandom.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/archrandom.h</span>
<span class="p_chunk">@@ -25,8 +25,6 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/processor.h&gt;
 #include &lt;asm/cpufeature.h&gt;
<span class="p_del">-#include &lt;asm/alternative.h&gt;</span>
<span class="p_del">-#include &lt;asm/nops.h&gt;</span>
 
 #define RDRAND_RETRY_LOOPS	10
 
<span class="p_chunk">@@ -40,97 +38,91 @@</span> <span class="p_context"></span>
 # define RDSEED_LONG	RDSEED_INT
 #endif
 
<span class="p_del">-#ifdef CONFIG_ARCH_RANDOM</span>
<span class="p_add">+/* Unconditional execution of RDRAND and RDSEED */</span>
 
<span class="p_del">-/* Instead of arch_get_random_long() when alternatives haven&#39;t run. */</span>
<span class="p_del">-static inline int rdrand_long(unsigned long *v)</span>
<span class="p_add">+static inline bool rdrand_long(unsigned long *v)</span>
 {
<span class="p_del">-	int ok;</span>
<span class="p_del">-	asm volatile(&quot;1: &quot; RDRAND_LONG &quot;\n\t&quot;</span>
<span class="p_del">-		     &quot;jc 2f\n\t&quot;</span>
<span class="p_del">-		     &quot;decl %0\n\t&quot;</span>
<span class="p_del">-		     &quot;jnz 1b\n\t&quot;</span>
<span class="p_del">-		     &quot;2:&quot;</span>
<span class="p_del">-		     : &quot;=r&quot; (ok), &quot;=a&quot; (*v)</span>
<span class="p_del">-		     : &quot;0&quot; (RDRAND_RETRY_LOOPS));</span>
<span class="p_del">-	return ok;</span>
<span class="p_add">+	bool ok;</span>
<span class="p_add">+	unsigned int retry = RDRAND_RETRY_LOOPS;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		asm volatile(RDRAND_LONG &quot;\n\t&quot;</span>
<span class="p_add">+			     CC_SET(c)</span>
<span class="p_add">+			     : CC_OUT(c) (ok), &quot;=a&quot; (*v));</span>
<span class="p_add">+		if (ok)</span>
<span class="p_add">+			return true;</span>
<span class="p_add">+	} while (--retry);</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool rdrand_int(unsigned int *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool ok;</span>
<span class="p_add">+	unsigned int retry = RDRAND_RETRY_LOOPS;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		asm volatile(RDRAND_INT &quot;\n\t&quot;</span>
<span class="p_add">+			     CC_SET(c)</span>
<span class="p_add">+			     : CC_OUT(c) (ok), &quot;=a&quot; (*v));</span>
<span class="p_add">+		if (ok)</span>
<span class="p_add">+			return true;</span>
<span class="p_add">+	} while (--retry);</span>
<span class="p_add">+	return false;</span>
 }
 
<span class="p_del">-/* A single attempt at RDSEED */</span>
 static inline bool rdseed_long(unsigned long *v)
 {
<span class="p_del">-	unsigned char ok;</span>
<span class="p_add">+	bool ok;</span>
 	asm volatile(RDSEED_LONG &quot;\n\t&quot;
<span class="p_del">-		     &quot;setc %0&quot;</span>
<span class="p_del">-		     : &quot;=qm&quot; (ok), &quot;=a&quot; (*v));</span>
<span class="p_add">+		     CC_SET(c)</span>
<span class="p_add">+		     : CC_OUT(c) (ok), &quot;=a&quot; (*v));</span>
 	return ok;
 }
 
<span class="p_del">-#define GET_RANDOM(name, type, rdrand, nop)			\</span>
<span class="p_del">-static inline int name(type *v)					\</span>
<span class="p_del">-{								\</span>
<span class="p_del">-	int ok;							\</span>
<span class="p_del">-	alternative_io(&quot;movl $0, %0\n\t&quot;			\</span>
<span class="p_del">-		       nop,					\</span>
<span class="p_del">-		       &quot;\n1: &quot; rdrand &quot;\n\t&quot;			\</span>
<span class="p_del">-		       &quot;jc 2f\n\t&quot;				\</span>
<span class="p_del">-		       &quot;decl %0\n\t&quot;                            \</span>
<span class="p_del">-		       &quot;jnz 1b\n\t&quot;                             \</span>
<span class="p_del">-		       &quot;2:&quot;,                                    \</span>
<span class="p_del">-		       X86_FEATURE_RDRAND,                      \</span>
<span class="p_del">-		       ASM_OUTPUT2(&quot;=r&quot; (ok), &quot;=a&quot; (*v)),       \</span>
<span class="p_del">-		       &quot;0&quot; (RDRAND_RETRY_LOOPS));		\</span>
<span class="p_del">-	return ok;						\</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#define GET_SEED(name, type, rdseed, nop)			\</span>
<span class="p_del">-static inline int name(type *v)					\</span>
<span class="p_del">-{								\</span>
<span class="p_del">-	unsigned char ok;					\</span>
<span class="p_del">-	alternative_io(&quot;movb $0, %0\n\t&quot;			\</span>
<span class="p_del">-		       nop,					\</span>
<span class="p_del">-		       rdseed &quot;\n\t&quot;				\</span>
<span class="p_del">-		       &quot;setc %0&quot;,				\</span>
<span class="p_del">-		       X86_FEATURE_RDSEED,                      \</span>
<span class="p_del">-		       ASM_OUTPUT2(&quot;=q&quot; (ok), &quot;=a&quot; (*v)));	\</span>
<span class="p_del">-	return ok;						\</span>
<span class="p_add">+static inline bool rdseed_int(unsigned int *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool ok;</span>
<span class="p_add">+	asm volatile(RDSEED_INT &quot;\n\t&quot;</span>
<span class="p_add">+		     CC_SET(c)</span>
<span class="p_add">+		     : CC_OUT(c) (ok), &quot;=a&quot; (*v));</span>
<span class="p_add">+	return ok;</span>
 }
 
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-</span>
<span class="p_del">-GET_RANDOM(arch_get_random_long, unsigned long, RDRAND_LONG, ASM_NOP5);</span>
<span class="p_del">-GET_RANDOM(arch_get_random_int, unsigned int, RDRAND_INT, ASM_NOP4);</span>
<span class="p_del">-</span>
<span class="p_del">-GET_SEED(arch_get_random_seed_long, unsigned long, RDSEED_LONG, ASM_NOP5);</span>
<span class="p_del">-GET_SEED(arch_get_random_seed_int, unsigned int, RDSEED_INT, ASM_NOP4);</span>
<span class="p_del">-</span>
<span class="p_del">-#else</span>
<span class="p_del">-</span>
<span class="p_del">-GET_RANDOM(arch_get_random_long, unsigned long, RDRAND_LONG, ASM_NOP3);</span>
<span class="p_del">-GET_RANDOM(arch_get_random_int, unsigned int, RDRAND_INT, ASM_NOP3);</span>
<span class="p_del">-</span>
<span class="p_del">-GET_SEED(arch_get_random_seed_long, unsigned long, RDSEED_LONG, ASM_NOP4);</span>
<span class="p_del">-GET_SEED(arch_get_random_seed_int, unsigned int, RDSEED_INT, ASM_NOP4);</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* CONFIG_X86_64 */</span>
<span class="p_del">-</span>
<span class="p_add">+/* Conditional execution based on CPU type */</span>
 #define arch_has_random()	static_cpu_has(X86_FEATURE_RDRAND)
 #define arch_has_random_seed()	static_cpu_has(X86_FEATURE_RDSEED)
 
<span class="p_del">-#else</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * These are the generic interfaces; they must not be declared if the</span>
<span class="p_add">+ * stubs in &lt;linux/random.h&gt; are to be invoked,</span>
<span class="p_add">+ * i.e. CONFIG_ARCH_RANDOM is not defined.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_ARCH_RANDOM</span>
 
<span class="p_del">-static inline int rdrand_long(unsigned long *v)</span>
<span class="p_add">+static inline bool arch_get_random_long(unsigned long *v)</span>
 {
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return arch_has_random() ? rdrand_long(v) : false;</span>
 }
 
<span class="p_del">-static inline bool rdseed_long(unsigned long *v)</span>
<span class="p_add">+static inline bool arch_get_random_int(unsigned int *v)</span>
 {
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return arch_has_random() ? rdrand_int(v) : false;</span>
 }
 
<span class="p_del">-#endif  /* CONFIG_ARCH_RANDOM */</span>
<span class="p_add">+static inline bool arch_get_random_seed_long(unsigned long *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return arch_has_random_seed() ? rdseed_long(v) : false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool arch_get_random_seed_int(unsigned int *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return arch_has_random_seed() ? rdseed_int(v) : false;</span>
<span class="p_add">+}</span>
 
 extern void x86_init_rdrand(struct cpuinfo_x86 *c);
 
<span class="p_add">+#else  /* !CONFIG_ARCH_RANDOM */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void x86_init_rdrand(struct cpuinfo_x86 *c) { }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif  /* !CONFIG_ARCH_RANDOM */</span>
<span class="p_add">+</span>
 #endif /* ASM_X86_ARCHRANDOM_H */
<span class="p_header">diff --git a/arch/x86/include/asm/asm.h b/arch/x86/include/asm/asm.h</span>
<span class="p_header">index f5063b6659eb..7acb51c49fec 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/asm.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/asm.h</span>
<span class="p_chunk">@@ -42,6 +42,18 @@</span> <span class="p_context"></span>
 #define _ASM_SI		__ASM_REG(si)
 #define _ASM_DI		__ASM_REG(di)
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Macros to generate condition code outputs from inline assembly,</span>
<span class="p_add">+ * The output operand must be type &quot;bool&quot;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef __GCC_ASM_FLAG_OUTPUTS__</span>
<span class="p_add">+# define CC_SET(c) &quot;\n\t/* output condition code &quot; #c &quot;*/\n&quot;</span>
<span class="p_add">+# define CC_OUT(c) &quot;=@cc&quot; #c</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define CC_SET(c) &quot;\n\tset&quot; #c &quot; %[_cc_&quot; #c &quot;]\n&quot;</span>
<span class="p_add">+# define CC_OUT(c) [_cc_ ## c] &quot;=qm&quot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* Exception table entry */
 #ifdef __ASSEMBLY__
 # define _ASM_EXTABLE_HANDLE(from, to, handler)			\
<span class="p_header">diff --git a/arch/x86/include/asm/atomic.h b/arch/x86/include/asm/atomic.h</span>
<span class="p_header">index 3e8674288198..7322c1566f63 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/atomic.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/atomic.h</span>
<span class="p_chunk">@@ -75,9 +75,9 @@</span> <span class="p_context"> static __always_inline void atomic_sub(int i, atomic_t *v)</span>
  * true if the result is zero, or false for all
  * other cases.
  */
<span class="p_del">-static __always_inline int atomic_sub_and_test(int i, atomic_t *v)</span>
<span class="p_add">+static __always_inline bool atomic_sub_and_test(int i, atomic_t *v)</span>
 {
<span class="p_del">-	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;subl&quot;, v-&gt;counter, &quot;er&quot;, i, &quot;%0&quot;, &quot;e&quot;);</span>
<span class="p_add">+	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;subl&quot;, v-&gt;counter, &quot;er&quot;, i, &quot;%0&quot;, e);</span>
 }
 
 /**
<span class="p_chunk">@@ -112,9 +112,9 @@</span> <span class="p_context"> static __always_inline void atomic_dec(atomic_t *v)</span>
  * returns true if the result is 0, or false for all other
  * cases.
  */
<span class="p_del">-static __always_inline int atomic_dec_and_test(atomic_t *v)</span>
<span class="p_add">+static __always_inline bool atomic_dec_and_test(atomic_t *v)</span>
 {
<span class="p_del">-	GEN_UNARY_RMWcc(LOCK_PREFIX &quot;decl&quot;, v-&gt;counter, &quot;%0&quot;, &quot;e&quot;);</span>
<span class="p_add">+	GEN_UNARY_RMWcc(LOCK_PREFIX &quot;decl&quot;, v-&gt;counter, &quot;%0&quot;, e);</span>
 }
 
 /**
<span class="p_chunk">@@ -125,9 +125,9 @@</span> <span class="p_context"> static __always_inline int atomic_dec_and_test(atomic_t *v)</span>
  * and returns true if the result is zero, or false for all
  * other cases.
  */
<span class="p_del">-static __always_inline int atomic_inc_and_test(atomic_t *v)</span>
<span class="p_add">+static __always_inline bool atomic_inc_and_test(atomic_t *v)</span>
 {
<span class="p_del">-	GEN_UNARY_RMWcc(LOCK_PREFIX &quot;incl&quot;, v-&gt;counter, &quot;%0&quot;, &quot;e&quot;);</span>
<span class="p_add">+	GEN_UNARY_RMWcc(LOCK_PREFIX &quot;incl&quot;, v-&gt;counter, &quot;%0&quot;, e);</span>
 }
 
 /**
<span class="p_chunk">@@ -139,9 +139,9 @@</span> <span class="p_context"> static __always_inline int atomic_inc_and_test(atomic_t *v)</span>
  * if the result is negative, or false when
  * result is greater than or equal to zero.
  */
<span class="p_del">-static __always_inline int atomic_add_negative(int i, atomic_t *v)</span>
<span class="p_add">+static __always_inline bool atomic_add_negative(int i, atomic_t *v)</span>
 {
<span class="p_del">-	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;addl&quot;, v-&gt;counter, &quot;er&quot;, i, &quot;%0&quot;, &quot;s&quot;);</span>
<span class="p_add">+	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;addl&quot;, v-&gt;counter, &quot;er&quot;, i, &quot;%0&quot;, s);</span>
 }
 
 /**
<span class="p_header">diff --git a/arch/x86/include/asm/atomic64_64.h b/arch/x86/include/asm/atomic64_64.h</span>
<span class="p_header">index 037351022f54..57bf925710d9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/atomic64_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/atomic64_64.h</span>
<span class="p_chunk">@@ -70,9 +70,9 @@</span> <span class="p_context"> static inline void atomic64_sub(long i, atomic64_t *v)</span>
  * true if the result is zero, or false for all
  * other cases.
  */
<span class="p_del">-static inline int atomic64_sub_and_test(long i, atomic64_t *v)</span>
<span class="p_add">+static inline bool atomic64_sub_and_test(long i, atomic64_t *v)</span>
 {
<span class="p_del">-	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;subq&quot;, v-&gt;counter, &quot;er&quot;, i, &quot;%0&quot;, &quot;e&quot;);</span>
<span class="p_add">+	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;subq&quot;, v-&gt;counter, &quot;er&quot;, i, &quot;%0&quot;, e);</span>
 }
 
 /**
<span class="p_chunk">@@ -109,9 +109,9 @@</span> <span class="p_context"> static __always_inline void atomic64_dec(atomic64_t *v)</span>
  * returns true if the result is 0, or false for all other
  * cases.
  */
<span class="p_del">-static inline int atomic64_dec_and_test(atomic64_t *v)</span>
<span class="p_add">+static inline bool atomic64_dec_and_test(atomic64_t *v)</span>
 {
<span class="p_del">-	GEN_UNARY_RMWcc(LOCK_PREFIX &quot;decq&quot;, v-&gt;counter, &quot;%0&quot;, &quot;e&quot;);</span>
<span class="p_add">+	GEN_UNARY_RMWcc(LOCK_PREFIX &quot;decq&quot;, v-&gt;counter, &quot;%0&quot;, e);</span>
 }
 
 /**
<span class="p_chunk">@@ -122,9 +122,9 @@</span> <span class="p_context"> static inline int atomic64_dec_and_test(atomic64_t *v)</span>
  * and returns true if the result is zero, or false for all
  * other cases.
  */
<span class="p_del">-static inline int atomic64_inc_and_test(atomic64_t *v)</span>
<span class="p_add">+static inline bool atomic64_inc_and_test(atomic64_t *v)</span>
 {
<span class="p_del">-	GEN_UNARY_RMWcc(LOCK_PREFIX &quot;incq&quot;, v-&gt;counter, &quot;%0&quot;, &quot;e&quot;);</span>
<span class="p_add">+	GEN_UNARY_RMWcc(LOCK_PREFIX &quot;incq&quot;, v-&gt;counter, &quot;%0&quot;, e);</span>
 }
 
 /**
<span class="p_chunk">@@ -136,9 +136,9 @@</span> <span class="p_context"> static inline int atomic64_inc_and_test(atomic64_t *v)</span>
  * if the result is negative, or false when
  * result is greater than or equal to zero.
  */
<span class="p_del">-static inline int atomic64_add_negative(long i, atomic64_t *v)</span>
<span class="p_add">+static inline bool atomic64_add_negative(long i, atomic64_t *v)</span>
 {
<span class="p_del">-	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;addq&quot;, v-&gt;counter, &quot;er&quot;, i, &quot;%0&quot;, &quot;s&quot;);</span>
<span class="p_add">+	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;addq&quot;, v-&gt;counter, &quot;er&quot;, i, &quot;%0&quot;, s);</span>
 }
 
 /**
<span class="p_chunk">@@ -180,7 +180,7 @@</span> <span class="p_context"> static inline long atomic64_xchg(atomic64_t *v, long new)</span>
  * Atomically adds @a to @v, so long as it was not @u.
  * Returns the old value of @v.
  */
<span class="p_del">-static inline int atomic64_add_unless(atomic64_t *v, long a, long u)</span>
<span class="p_add">+static inline bool atomic64_add_unless(atomic64_t *v, long a, long u)</span>
 {
 	long c, old;
 	c = atomic64_read(v);
<span class="p_header">diff --git a/arch/x86/include/asm/bitops.h b/arch/x86/include/asm/bitops.h</span>
<span class="p_header">index 7766d1cf096e..68557f52b961 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/bitops.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/bitops.h</span>
<span class="p_chunk">@@ -201,9 +201,9 @@</span> <span class="p_context"> static __always_inline void change_bit(long nr, volatile unsigned long *addr)</span>
  * This operation is atomic and cannot be reordered.
  * It also implies a memory barrier.
  */
<span class="p_del">-static __always_inline int test_and_set_bit(long nr, volatile unsigned long *addr)</span>
<span class="p_add">+static __always_inline bool test_and_set_bit(long nr, volatile unsigned long *addr)</span>
 {
<span class="p_del">-	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;bts&quot;, *addr, &quot;Ir&quot;, nr, &quot;%0&quot;, &quot;c&quot;);</span>
<span class="p_add">+	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;bts&quot;, *addr, &quot;Ir&quot;, nr, &quot;%0&quot;, c);</span>
 }
 
 /**
<span class="p_chunk">@@ -213,7 +213,7 @@</span> <span class="p_context"> static __always_inline int test_and_set_bit(long nr, volatile unsigned long *add</span>
  *
  * This is the same as test_and_set_bit on x86.
  */
<span class="p_del">-static __always_inline int</span>
<span class="p_add">+static __always_inline bool</span>
 test_and_set_bit_lock(long nr, volatile unsigned long *addr)
 {
 	return test_and_set_bit(nr, addr);
<span class="p_chunk">@@ -228,13 +228,13 @@</span> <span class="p_context"> test_and_set_bit_lock(long nr, volatile unsigned long *addr)</span>
  * If two examples of this operation race, one can appear to succeed
  * but actually fail.  You must protect multiple accesses with a lock.
  */
<span class="p_del">-static __always_inline int __test_and_set_bit(long nr, volatile unsigned long *addr)</span>
<span class="p_add">+static __always_inline bool __test_and_set_bit(long nr, volatile unsigned long *addr)</span>
 {
<span class="p_del">-	int oldbit;</span>
<span class="p_add">+	bool oldbit;</span>
 
 	asm(&quot;bts %2,%1\n\t&quot;
<span class="p_del">-	    &quot;sbb %0,%0&quot;</span>
<span class="p_del">-	    : &quot;=r&quot; (oldbit), ADDR</span>
<span class="p_add">+	    CC_SET(c)</span>
<span class="p_add">+	    : CC_OUT(c) (oldbit), ADDR</span>
 	    : &quot;Ir&quot; (nr));
 	return oldbit;
 }
<span class="p_chunk">@@ -247,9 +247,9 @@</span> <span class="p_context"> static __always_inline int __test_and_set_bit(long nr, volatile unsigned long *a</span>
  * This operation is atomic and cannot be reordered.
  * It also implies a memory barrier.
  */
<span class="p_del">-static __always_inline int test_and_clear_bit(long nr, volatile unsigned long *addr)</span>
<span class="p_add">+static __always_inline bool test_and_clear_bit(long nr, volatile unsigned long *addr)</span>
 {
<span class="p_del">-	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;btr&quot;, *addr, &quot;Ir&quot;, nr, &quot;%0&quot;, &quot;c&quot;);</span>
<span class="p_add">+	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;btr&quot;, *addr, &quot;Ir&quot;, nr, &quot;%0&quot;, c);</span>
 }
 
 /**
<span class="p_chunk">@@ -268,25 +268,25 @@</span> <span class="p_context"> static __always_inline int test_and_clear_bit(long nr, volatile unsigned long *a</span>
  * accessed from a hypervisor on the same CPU if running in a VM: don&#39;t change
  * this without also updating arch/x86/kernel/kvm.c
  */
<span class="p_del">-static __always_inline int __test_and_clear_bit(long nr, volatile unsigned long *addr)</span>
<span class="p_add">+static __always_inline bool __test_and_clear_bit(long nr, volatile unsigned long *addr)</span>
 {
<span class="p_del">-	int oldbit;</span>
<span class="p_add">+	bool oldbit;</span>
 
 	asm volatile(&quot;btr %2,%1\n\t&quot;
<span class="p_del">-		     &quot;sbb %0,%0&quot;</span>
<span class="p_del">-		     : &quot;=r&quot; (oldbit), ADDR</span>
<span class="p_add">+		     CC_SET(c)</span>
<span class="p_add">+		     : CC_OUT(c) (oldbit), ADDR</span>
 		     : &quot;Ir&quot; (nr));
 	return oldbit;
 }
 
 /* WARNING: non atomic and it can be reordered! */
<span class="p_del">-static __always_inline int __test_and_change_bit(long nr, volatile unsigned long *addr)</span>
<span class="p_add">+static __always_inline bool __test_and_change_bit(long nr, volatile unsigned long *addr)</span>
 {
<span class="p_del">-	int oldbit;</span>
<span class="p_add">+	bool oldbit;</span>
 
 	asm volatile(&quot;btc %2,%1\n\t&quot;
<span class="p_del">-		     &quot;sbb %0,%0&quot;</span>
<span class="p_del">-		     : &quot;=r&quot; (oldbit), ADDR</span>
<span class="p_add">+		     CC_SET(c)</span>
<span class="p_add">+		     : CC_OUT(c) (oldbit), ADDR</span>
 		     : &quot;Ir&quot; (nr) : &quot;memory&quot;);
 
 	return oldbit;
<span class="p_chunk">@@ -300,24 +300,24 @@</span> <span class="p_context"> static __always_inline int __test_and_change_bit(long nr, volatile unsigned long</span>
  * This operation is atomic and cannot be reordered.
  * It also implies a memory barrier.
  */
<span class="p_del">-static __always_inline int test_and_change_bit(long nr, volatile unsigned long *addr)</span>
<span class="p_add">+static __always_inline bool test_and_change_bit(long nr, volatile unsigned long *addr)</span>
 {
<span class="p_del">-	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;btc&quot;, *addr, &quot;Ir&quot;, nr, &quot;%0&quot;, &quot;c&quot;);</span>
<span class="p_add">+	GEN_BINARY_RMWcc(LOCK_PREFIX &quot;btc&quot;, *addr, &quot;Ir&quot;, nr, &quot;%0&quot;, c);</span>
 }
 
<span class="p_del">-static __always_inline int constant_test_bit(long nr, const volatile unsigned long *addr)</span>
<span class="p_add">+static __always_inline bool constant_test_bit(long nr, const volatile unsigned long *addr)</span>
 {
 	return ((1UL &lt;&lt; (nr &amp; (BITS_PER_LONG-1))) &amp;
 		(addr[nr &gt;&gt; _BITOPS_LONG_SHIFT])) != 0;
 }
 
<span class="p_del">-static __always_inline int variable_test_bit(long nr, volatile const unsigned long *addr)</span>
<span class="p_add">+static __always_inline bool variable_test_bit(long nr, volatile const unsigned long *addr)</span>
 {
<span class="p_del">-	int oldbit;</span>
<span class="p_add">+	bool oldbit;</span>
 
 	asm volatile(&quot;bt %2,%1\n\t&quot;
<span class="p_del">-		     &quot;sbb %0,%0&quot;</span>
<span class="p_del">-		     : &quot;=r&quot; (oldbit)</span>
<span class="p_add">+		     CC_SET(c)</span>
<span class="p_add">+		     : CC_OUT(c) (oldbit)</span>
 		     : &quot;m&quot; (*(unsigned long *)addr), &quot;Ir&quot; (nr));
 
 	return oldbit;
<span class="p_chunk">@@ -329,7 +329,7 @@</span> <span class="p_context"> static __always_inline int variable_test_bit(long nr, volatile const unsigned lo</span>
  * @nr: bit number to test
  * @addr: Address to start counting from
  */
<span class="p_del">-static int test_bit(int nr, const volatile unsigned long *addr);</span>
<span class="p_add">+static bool test_bit(int nr, const volatile unsigned long *addr);</span>
 #endif
 
 #define test_bit(nr, addr)			\
<span class="p_header">diff --git a/arch/x86/include/asm/checksum_32.h b/arch/x86/include/asm/checksum_32.h</span>
<span class="p_header">index 532f85e6651f..7b53743ed267 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/checksum_32.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/checksum_32.h</span>
<span class="p_chunk">@@ -2,8 +2,7 @@</span> <span class="p_context"></span>
 #define _ASM_X86_CHECKSUM_32_H
 
 #include &lt;linux/in6.h&gt;
<span class="p_del">-</span>
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 /*
  * computes the checksum of a memory block at buff, length len,
<span class="p_header">diff --git a/arch/x86/include/asm/compat.h b/arch/x86/include/asm/compat.h</span>
<span class="p_header">index 5a3b2c119ed0..a18806165fe4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/compat.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/compat.h</span>
<span class="p_chunk">@@ -40,6 +40,7 @@</span> <span class="p_context"> typedef s32		compat_long_t;</span>
 typedef s64 __attribute__((aligned(4))) compat_s64;
 typedef u32		compat_uint_t;
 typedef u32		compat_ulong_t;
<span class="p_add">+typedef u32		compat_u32;</span>
 typedef u64 __attribute__((aligned(4))) compat_u64;
 typedef u32		compat_uptr_t;
 
<span class="p_chunk">@@ -181,6 +182,16 @@</span> <span class="p_context"> typedef struct compat_siginfo {</span>
 		/* SIGILL, SIGFPE, SIGSEGV, SIGBUS */
 		struct {
 			unsigned int _addr;	/* faulting insn/memory ref. */
<span class="p_add">+			short int _addr_lsb;	/* Valid LSB of the reported address. */</span>
<span class="p_add">+			union {</span>
<span class="p_add">+				/* used when si_code=SEGV_BNDERR */</span>
<span class="p_add">+				struct {</span>
<span class="p_add">+					compat_uptr_t _lower;</span>
<span class="p_add">+					compat_uptr_t _upper;</span>
<span class="p_add">+				} _addr_bnd;</span>
<span class="p_add">+				/* used when si_code=SEGV_PKUERR */</span>
<span class="p_add">+				compat_u32 _pkey;</span>
<span class="p_add">+			};</span>
 		} _sigfault;
 
 		/* SIGPOLL */
<span class="p_header">diff --git a/arch/x86/include/asm/cpu.h b/arch/x86/include/asm/cpu.h</span>
<span class="p_header">index 678637ad7476..59d34c521d96 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpu.h</span>
<span class="p_chunk">@@ -17,7 +17,6 @@</span> <span class="p_context"> static inline void prefill_possible_map(void) {}</span>
 
 #define cpu_physical_id(cpu)			boot_cpu_physical_apicid
 #define safe_smp_processor_id()			0
<span class="p_del">-#define stack_smp_processor_id()		0</span>
 
 #endif /* CONFIG_SMP */
 
<span class="p_header">diff --git a/arch/x86/include/asm/efi.h b/arch/x86/include/asm/efi.h</span>
<span class="p_header">index 78d1e7467eae..45ea38df86d4 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/efi.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/efi.h</span>
<span class="p_chunk">@@ -125,7 +125,6 @@</span> <span class="p_context"> extern void __init efi_map_region_fixed(efi_memory_desc_t *md);</span>
 extern void efi_sync_low_kernel_mappings(void);
 extern int __init efi_alloc_page_tables(void);
 extern int __init efi_setup_page_tables(unsigned long pa_memmap, unsigned num_pages);
<span class="p_del">-extern void __init efi_cleanup_page_tables(unsigned long pa_memmap, unsigned num_pages);</span>
 extern void __init old_map_region(efi_memory_desc_t *md);
 extern void __init runtime_code_page_mkexec(void);
 extern void __init efi_runtime_update_mappings(void);
<span class="p_header">diff --git a/arch/x86/include/asm/local.h b/arch/x86/include/asm/local.h</span>
<span class="p_header">index 4ad6560847b1..7511978093eb 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/local.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/local.h</span>
<span class="p_chunk">@@ -50,9 +50,9 @@</span> <span class="p_context"> static inline void local_sub(long i, local_t *l)</span>
  * true if the result is zero, or false for all
  * other cases.
  */
<span class="p_del">-static inline int local_sub_and_test(long i, local_t *l)</span>
<span class="p_add">+static inline bool local_sub_and_test(long i, local_t *l)</span>
 {
<span class="p_del">-	GEN_BINARY_RMWcc(_ASM_SUB, l-&gt;a.counter, &quot;er&quot;, i, &quot;%0&quot;, &quot;e&quot;);</span>
<span class="p_add">+	GEN_BINARY_RMWcc(_ASM_SUB, l-&gt;a.counter, &quot;er&quot;, i, &quot;%0&quot;, e);</span>
 }
 
 /**
<span class="p_chunk">@@ -63,9 +63,9 @@</span> <span class="p_context"> static inline int local_sub_and_test(long i, local_t *l)</span>
  * returns true if the result is 0, or false for all other
  * cases.
  */
<span class="p_del">-static inline int local_dec_and_test(local_t *l)</span>
<span class="p_add">+static inline bool local_dec_and_test(local_t *l)</span>
 {
<span class="p_del">-	GEN_UNARY_RMWcc(_ASM_DEC, l-&gt;a.counter, &quot;%0&quot;, &quot;e&quot;);</span>
<span class="p_add">+	GEN_UNARY_RMWcc(_ASM_DEC, l-&gt;a.counter, &quot;%0&quot;, e);</span>
 }
 
 /**
<span class="p_chunk">@@ -76,9 +76,9 @@</span> <span class="p_context"> static inline int local_dec_and_test(local_t *l)</span>
  * and returns true if the result is zero, or false for all
  * other cases.
  */
<span class="p_del">-static inline int local_inc_and_test(local_t *l)</span>
<span class="p_add">+static inline bool local_inc_and_test(local_t *l)</span>
 {
<span class="p_del">-	GEN_UNARY_RMWcc(_ASM_INC, l-&gt;a.counter, &quot;%0&quot;, &quot;e&quot;);</span>
<span class="p_add">+	GEN_UNARY_RMWcc(_ASM_INC, l-&gt;a.counter, &quot;%0&quot;, e);</span>
 }
 
 /**
<span class="p_chunk">@@ -90,9 +90,9 @@</span> <span class="p_context"> static inline int local_inc_and_test(local_t *l)</span>
  * if the result is negative, or false when
  * result is greater than or equal to zero.
  */
<span class="p_del">-static inline int local_add_negative(long i, local_t *l)</span>
<span class="p_add">+static inline bool local_add_negative(long i, local_t *l)</span>
 {
<span class="p_del">-	GEN_BINARY_RMWcc(_ASM_ADD, l-&gt;a.counter, &quot;er&quot;, i, &quot;%0&quot;, &quot;s&quot;);</span>
<span class="p_add">+	GEN_BINARY_RMWcc(_ASM_ADD, l-&gt;a.counter, &quot;er&quot;, i, &quot;%0&quot;, s);</span>
 }
 
 /**
<span class="p_header">diff --git a/arch/x86/include/asm/percpu.h b/arch/x86/include/asm/percpu.h</span>
<span class="p_header">index e0ba66ca68c6..e02e3f80d363 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/percpu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/percpu.h</span>
<span class="p_chunk">@@ -510,14 +510,15 @@</span> <span class="p_context"> do {									\</span>
 /* This is not atomic against other CPUs -- CPU preemption needs to be off */
 #define x86_test_and_clear_bit_percpu(bit, var)				\
 ({									\
<span class="p_del">-	int old__;							\</span>
<span class="p_del">-	asm volatile(&quot;btr %2,&quot;__percpu_arg(1)&quot;\n\tsbbl %0,%0&quot;		\</span>
<span class="p_del">-		     : &quot;=r&quot; (old__), &quot;+m&quot; (var)				\</span>
<span class="p_add">+	bool old__;							\</span>
<span class="p_add">+	asm volatile(&quot;btr %2,&quot;__percpu_arg(1)&quot;\n\t&quot;			\</span>
<span class="p_add">+		     CC_SET(c)						\</span>
<span class="p_add">+		     : CC_OUT(c) (old__), &quot;+m&quot; (var)			\</span>
 		     : &quot;dIr&quot; (bit));					\
 	old__;								\
 })
 
<span class="p_del">-static __always_inline int x86_this_cpu_constant_test_bit(unsigned int nr,</span>
<span class="p_add">+static __always_inline bool x86_this_cpu_constant_test_bit(unsigned int nr,</span>
                         const unsigned long __percpu *addr)
 {
 	unsigned long __percpu *a = (unsigned long *)addr + nr / BITS_PER_LONG;
<span class="p_chunk">@@ -529,14 +530,14 @@</span> <span class="p_context"> static __always_inline int x86_this_cpu_constant_test_bit(unsigned int nr,</span>
 #endif
 }
 
<span class="p_del">-static inline int x86_this_cpu_variable_test_bit(int nr,</span>
<span class="p_add">+static inline bool x86_this_cpu_variable_test_bit(int nr,</span>
                         const unsigned long __percpu *addr)
 {
<span class="p_del">-	int oldbit;</span>
<span class="p_add">+	bool oldbit;</span>
 
 	asm volatile(&quot;bt &quot;__percpu_arg(2)&quot;,%1\n\t&quot;
<span class="p_del">-			&quot;sbb %0,%0&quot;</span>
<span class="p_del">-			: &quot;=r&quot; (oldbit)</span>
<span class="p_add">+			CC_SET(c)</span>
<span class="p_add">+			: CC_OUT(c) (oldbit)</span>
 			: &quot;m&quot; (*(unsigned long *)addr), &quot;Ir&quot; (nr));
 
 	return oldbit;
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index 1a27396b6ea0..2815d268af8b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -480,7 +480,7 @@</span> <span class="p_context"> pte_t *populate_extra_pte(unsigned long vaddr);</span>
 
 static inline int pte_none(pte_t pte)
 {
<span class="p_del">-	return !pte.pte;</span>
<span class="p_add">+	return !(pte.pte &amp; ~(_PAGE_KNL_ERRATUM_MASK));</span>
 }
 
 #define __HAVE_ARCH_PTE_SAME
<span class="p_chunk">@@ -552,7 +552,8 @@</span> <span class="p_context"> static inline int pmd_none(pmd_t pmd)</span>
 {
 	/* Only check low word on 32-bit platforms, since it might be
 	   out of sync with upper half. */
<span class="p_del">-	return (unsigned long)native_pmd_val(pmd) == 0;</span>
<span class="p_add">+	unsigned long val = native_pmd_val(pmd);</span>
<span class="p_add">+	return (val &amp; ~_PAGE_KNL_ERRATUM_MASK) == 0;</span>
 }
 
 static inline unsigned long pmd_page_vaddr(pmd_t pmd)
<span class="p_chunk">@@ -616,7 +617,7 @@</span> <span class="p_context"> static inline unsigned long pages_to_mb(unsigned long npg)</span>
 #if CONFIG_PGTABLE_LEVELS &gt; 2
 static inline int pud_none(pud_t pud)
 {
<span class="p_del">-	return native_pud_val(pud) == 0;</span>
<span class="p_add">+	return (native_pud_val(pud) &amp; ~(_PAGE_KNL_ERRATUM_MASK)) == 0;</span>
 }
 
 static inline int pud_present(pud_t pud)
<span class="p_chunk">@@ -694,6 +695,12 @@</span> <span class="p_context"> static inline int pgd_bad(pgd_t pgd)</span>
 
 static inline int pgd_none(pgd_t pgd)
 {
<span class="p_add">+	/*</span>
<span class="p_add">+	 * There is no need to do a workaround for the KNL stray</span>
<span class="p_add">+	 * A/D bit erratum here.  PGDs only point to page tables</span>
<span class="p_add">+	 * except on 32-bit non-PAE which is not supported on</span>
<span class="p_add">+	 * KNL.</span>
<span class="p_add">+	 */</span>
 	return !native_pgd_val(pgd);
 }
 #endif	/* CONFIG_PGTABLE_LEVELS &gt; 3 */
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 2ee781114d34..7e8ec7ae10fa 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -140,18 +140,32 @@</span> <span class="p_context"> static inline int pgd_large(pgd_t pgd) { return 0; }</span>
 #define pte_offset_map(dir, address) pte_offset_kernel((dir), (address))
 #define pte_unmap(pte) ((void)(pte))/* NOP */
 
<span class="p_del">-/* Encode and de-code a swap entry */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Encode and de-code a swap entry</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * |     ...            | 11| 10|  9|8|7|6|5| 4| 3|2|1|0| &lt;- bit number</span>
<span class="p_add">+ * |     ...            |SW3|SW2|SW1|G|L|D|A|CD|WT|U|W|P| &lt;- bit names</span>
<span class="p_add">+ * | OFFSET (14-&gt;63) | TYPE (10-13) |0|X|X|X| X| X|X|X|0| &lt;- swp entry</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * G (8) is aliased and used as a PROT_NONE indicator for</span>
<span class="p_add">+ * !present ptes.  We need to start storing swap entries above</span>
<span class="p_add">+ * there.  We also need to avoid using A and D because of an</span>
<span class="p_add">+ * erratum where they can be incorrectly set by hardware on</span>
<span class="p_add">+ * non-present PTEs.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define SWP_TYPE_FIRST_BIT (_PAGE_BIT_PROTNONE + 1)</span>
 #define SWP_TYPE_BITS 5
<span class="p_del">-#define SWP_OFFSET_SHIFT (_PAGE_BIT_PROTNONE + 1)</span>
<span class="p_add">+/* Place the offset above the type: */</span>
<span class="p_add">+#define SWP_OFFSET_FIRST_BIT (SWP_TYPE_FIRST_BIT + SWP_TYPE_BITS + 1)</span>
 
 #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT &gt; SWP_TYPE_BITS)
 
<span class="p_del">-#define __swp_type(x)			(((x).val &gt;&gt; (_PAGE_BIT_PRESENT + 1)) \</span>
<span class="p_add">+#define __swp_type(x)			(((x).val &gt;&gt; (SWP_TYPE_FIRST_BIT)) \</span>
 					 &amp; ((1U &lt;&lt; SWP_TYPE_BITS) - 1))
<span class="p_del">-#define __swp_offset(x)			((x).val &gt;&gt; SWP_OFFSET_SHIFT)</span>
<span class="p_add">+#define __swp_offset(x)			((x).val &gt;&gt; SWP_OFFSET_FIRST_BIT)</span>
 #define __swp_entry(type, offset)	((swp_entry_t) { \
<span class="p_del">-					 ((type) &lt;&lt; (_PAGE_BIT_PRESENT + 1)) \</span>
<span class="p_del">-					 | ((offset) &lt;&lt; SWP_OFFSET_SHIFT) })</span>
<span class="p_add">+					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \</span>
<span class="p_add">+					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })</span>
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })
 #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })
 
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_types.h b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">index 7b5efe264eff..f1218f512f62 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_types.h</span>
<span class="p_chunk">@@ -70,6 +70,12 @@</span> <span class="p_context"></span>
 			 _PAGE_PKEY_BIT2 | \
 			 _PAGE_PKEY_BIT3)
 
<span class="p_add">+#if defined(CONFIG_X86_64) || defined(CONFIG_X86_PAE)</span>
<span class="p_add">+#define _PAGE_KNL_ERRATUM_MASK (_PAGE_DIRTY | _PAGE_ACCESSED)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define _PAGE_KNL_ERRATUM_MASK 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_KMEMCHECK
 #define _PAGE_HIDDEN	(_AT(pteval_t, 1) &lt;&lt; _PAGE_BIT_HIDDEN)
 #else
<span class="p_chunk">@@ -475,8 +481,6 @@</span> <span class="p_context"> extern pmd_t *lookup_pmd_address(unsigned long address);</span>
 extern phys_addr_t slow_virt_to_phys(void *__address);
 extern int kernel_map_pages_in_pgd(pgd_t *pgd, u64 pfn, unsigned long address,
 				   unsigned numpages, unsigned long page_flags);
<span class="p_del">-void kernel_unmap_pages_in_pgd(pgd_t *root, unsigned long address,</span>
<span class="p_del">-			       unsigned numpages);</span>
 #endif	/* !__ASSEMBLY__ */
 
 #endif /* _ASM_X86_PGTABLE_DEFS_H */
<span class="p_header">diff --git a/arch/x86/include/asm/preempt.h b/arch/x86/include/asm/preempt.h</span>
<span class="p_header">index d397deb58146..17f218645701 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/preempt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/preempt.h</span>
<span class="p_chunk">@@ -81,7 +81,7 @@</span> <span class="p_context"> static __always_inline void __preempt_count_sub(int val)</span>
  */
 static __always_inline bool __preempt_count_dec_and_test(void)
 {
<span class="p_del">-	GEN_UNARY_RMWcc(&quot;decl&quot;, __preempt_count, __percpu_arg(0), &quot;e&quot;);</span>
<span class="p_add">+	GEN_UNARY_RMWcc(&quot;decl&quot;, __preempt_count, __percpu_arg(0), e);</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 62c6cc3cc5d3..89314ed74fee 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -371,6 +371,10 @@</span> <span class="p_context"> extern unsigned int xstate_size;</span>
 
 struct perf_event;
 
<span class="p_add">+typedef struct {</span>
<span class="p_add">+	unsigned long		seg;</span>
<span class="p_add">+} mm_segment_t;</span>
<span class="p_add">+</span>
 struct thread_struct {
 	/* Cached TLS descriptors: */
 	struct desc_struct	tls_array[GDT_ENTRY_TLS_ENTRIES];
<span class="p_chunk">@@ -419,6 +423,11 @@</span> <span class="p_context"> struct thread_struct {</span>
 	/* Max allowed port in the bitmap, in bytes: */
 	unsigned		io_bitmap_max;
 
<span class="p_add">+	mm_segment_t		addr_limit;</span>
<span class="p_add">+</span>
<span class="p_add">+	unsigned int		sig_on_uaccess_err:1;</span>
<span class="p_add">+	unsigned int		uaccess_err:1;	/* uaccess failed */</span>
<span class="p_add">+</span>
 	/* Floating point and extended processor state */
 	struct fpu		fpu;
 	/*
<span class="p_chunk">@@ -490,11 +499,6 @@</span> <span class="p_context"> static inline void load_sp0(struct tss_struct *tss,</span>
 #define set_iopl_mask native_set_iopl_mask
 #endif /* CONFIG_PARAVIRT */
 
<span class="p_del">-typedef struct {</span>
<span class="p_del">-	unsigned long		seg;</span>
<span class="p_del">-} mm_segment_t;</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
 /* Free all resources held by a thread. */
 extern void release_thread(struct task_struct *);
 
<span class="p_chunk">@@ -716,6 +720,7 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 	.sp0			= TOP_OF_INIT_STACK,			  \
 	.sysenter_cs		= __KERNEL_CS,				  \
 	.io_bitmap_ptr		= NULL,					  \
<span class="p_add">+	.addr_limit		= KERNEL_DS,				  \</span>
 }
 
 extern unsigned long thread_saved_pc(struct task_struct *tsk);
<span class="p_chunk">@@ -765,8 +770,9 @@</span> <span class="p_context"> extern unsigned long thread_saved_pc(struct task_struct *tsk);</span>
 #define STACK_TOP		TASK_SIZE
 #define STACK_TOP_MAX		TASK_SIZE_MAX
 
<span class="p_del">-#define INIT_THREAD  { \</span>
<span class="p_del">-	.sp0 = TOP_OF_INIT_STACK \</span>
<span class="p_add">+#define INIT_THREAD  {						\</span>
<span class="p_add">+	.sp0			= TOP_OF_INIT_STACK,		\</span>
<span class="p_add">+	.addr_limit		= KERNEL_DS,			\</span>
 }
 
 /*
<span class="p_header">diff --git a/arch/x86/include/asm/rmwcc.h b/arch/x86/include/asm/rmwcc.h</span>
<span class="p_header">index 8f7866a5b9a4..661dd305694a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/rmwcc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/rmwcc.h</span>
<span class="p_chunk">@@ -1,11 +1,13 @@</span> <span class="p_context"></span>
 #ifndef _ASM_X86_RMWcc
 #define _ASM_X86_RMWcc
 
<span class="p_del">-#ifdef CC_HAVE_ASM_GOTO</span>
<span class="p_add">+#if !defined(__GCC_ASM_FLAG_OUTPUTS__) &amp;&amp; defined(CC_HAVE_ASM_GOTO)</span>
<span class="p_add">+</span>
<span class="p_add">+/* Use asm goto */</span>
 
 #define __GEN_RMWcc(fullop, var, cc, ...)				\
 do {									\
<span class="p_del">-	asm_volatile_goto (fullop &quot;; j&quot; cc &quot; %l[cc_label]&quot;		\</span>
<span class="p_add">+	asm_volatile_goto (fullop &quot;; j&quot; #cc &quot; %l[cc_label]&quot;		\</span>
 			: : &quot;m&quot; (var), ## __VA_ARGS__ 			\
 			: &quot;memory&quot; : cc_label);				\
 	return 0;							\
<span class="p_chunk">@@ -19,15 +21,17 @@</span> <span class="p_context"> cc_label:								\</span>
 #define GEN_BINARY_RMWcc(op, var, vcon, val, arg0, cc)			\
 	__GEN_RMWcc(op &quot; %1, &quot; arg0, var, cc, vcon (val))
 
<span class="p_del">-#else /* !CC_HAVE_ASM_GOTO */</span>
<span class="p_add">+#else /* defined(__GCC_ASM_FLAG_OUTPUTS__) || !defined(CC_HAVE_ASM_GOTO) */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Use flags output or a set instruction */</span>
 
 #define __GEN_RMWcc(fullop, var, cc, ...)				\
 do {									\
<span class="p_del">-	char c;								\</span>
<span class="p_del">-	asm volatile (fullop &quot;; set&quot; cc &quot; %1&quot;				\</span>
<span class="p_del">-			: &quot;+m&quot; (var), &quot;=qm&quot; (c)				\</span>
<span class="p_add">+	bool c;								\</span>
<span class="p_add">+	asm volatile (fullop &quot;;&quot; CC_SET(cc)				\</span>
<span class="p_add">+			: &quot;+m&quot; (var), CC_OUT(cc) (c)			\</span>
 			: __VA_ARGS__ : &quot;memory&quot;);			\
<span class="p_del">-	return c != 0;							\</span>
<span class="p_add">+	return c;							\</span>
 } while (0)
 
 #define GEN_UNARY_RMWcc(op, var, arg0, cc)				\
<span class="p_chunk">@@ -36,6 +40,6 @@</span> <span class="p_context"> do {									\</span>
 #define GEN_BINARY_RMWcc(op, var, vcon, val, arg0, cc)			\
 	__GEN_RMWcc(op &quot; %2, &quot; arg0, var, cc, vcon (val))
 
<span class="p_del">-#endif /* CC_HAVE_ASM_GOTO */</span>
<span class="p_add">+#endif /* defined(__GCC_ASM_FLAG_OUTPUTS__) || !defined(CC_HAVE_ASM_GOTO) */</span>
 
 #endif /* _ASM_X86_RMWcc */
<span class="p_header">diff --git a/arch/x86/include/asm/rwsem.h b/arch/x86/include/asm/rwsem.h</span>
<span class="p_header">index 453744c1d347..1e8be263065e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/rwsem.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/rwsem.h</span>
<span class="p_chunk">@@ -77,7 +77,7 @@</span> <span class="p_context"> static inline void __down_read(struct rw_semaphore *sem)</span>
 /*
  * trylock for reading -- returns 1 if successful, 0 if contention
  */
<span class="p_del">-static inline int __down_read_trylock(struct rw_semaphore *sem)</span>
<span class="p_add">+static inline bool __down_read_trylock(struct rw_semaphore *sem)</span>
 {
 	long result, tmp;
 	asm volatile(&quot;# beginning __down_read_trylock\n\t&quot;
<span class="p_chunk">@@ -93,7 +93,7 @@</span> <span class="p_context"> static inline int __down_read_trylock(struct rw_semaphore *sem)</span>
 		     : &quot;+m&quot; (sem-&gt;count), &quot;=&amp;a&quot; (result), &quot;=&amp;r&quot; (tmp)
 		     : &quot;i&quot; (RWSEM_ACTIVE_READ_BIAS)
 		     : &quot;memory&quot;, &quot;cc&quot;);
<span class="p_del">-	return result &gt;= 0 ? 1 : 0;</span>
<span class="p_add">+	return result &gt;= 0;</span>
 }
 
 /*
<span class="p_chunk">@@ -134,9 +134,10 @@</span> <span class="p_context"> static inline int __down_write_killable(struct rw_semaphore *sem)</span>
 /*
  * trylock for writing -- returns 1 if successful, 0 if contention
  */
<span class="p_del">-static inline int __down_write_trylock(struct rw_semaphore *sem)</span>
<span class="p_add">+static inline bool __down_write_trylock(struct rw_semaphore *sem)</span>
 {
<span class="p_del">-	long result, tmp;</span>
<span class="p_add">+	bool result;</span>
<span class="p_add">+	long tmp0, tmp1;</span>
 	asm volatile(&quot;# beginning __down_write_trylock\n\t&quot;
 		     &quot;  mov          %0,%1\n\t&quot;
 		     &quot;1:\n\t&quot;
<span class="p_chunk">@@ -144,14 +145,14 @@</span> <span class="p_context"> static inline int __down_write_trylock(struct rw_semaphore *sem)</span>
 		     /* was the active mask 0 before? */
 		     &quot;  jnz          2f\n\t&quot;
 		     &quot;  mov          %1,%2\n\t&quot;
<span class="p_del">-		     &quot;  add          %3,%2\n\t&quot;</span>
<span class="p_add">+		     &quot;  add          %4,%2\n\t&quot;</span>
 		     LOCK_PREFIX &quot;  cmpxchg  %2,%0\n\t&quot;
 		     &quot;  jnz	     1b\n\t&quot;
 		     &quot;2:\n\t&quot;
<span class="p_del">-		     &quot;  sete         %b1\n\t&quot;</span>
<span class="p_del">-		     &quot;  movzbl       %b1, %k1\n\t&quot;</span>
<span class="p_add">+		     CC_SET(e)</span>
 		     &quot;# ending __down_write_trylock\n\t&quot;
<span class="p_del">-		     : &quot;+m&quot; (sem-&gt;count), &quot;=&amp;a&quot; (result), &quot;=&amp;r&quot; (tmp)</span>
<span class="p_add">+		     : &quot;+m&quot; (sem-&gt;count), &quot;=&amp;a&quot; (tmp0), &quot;=&amp;r&quot; (tmp1),</span>
<span class="p_add">+		       CC_OUT(e) (result)</span>
 		     : &quot;er&quot; (RWSEM_ACTIVE_WRITE_BIAS)
 		     : &quot;memory&quot;, &quot;cc&quot;);
 	return result;
<span class="p_header">diff --git a/arch/x86/include/asm/signal.h b/arch/x86/include/asm/signal.h</span>
<span class="p_header">index 2138c9ae19ee..dd1e7d6387ab 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/signal.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/signal.h</span>
<span class="p_chunk">@@ -81,9 +81,9 @@</span> <span class="p_context"> static inline int __const_sigismember(sigset_t *set, int _sig)</span>
 
 static inline int __gen_sigismember(sigset_t *set, int _sig)
 {
<span class="p_del">-	int ret;</span>
<span class="p_del">-	asm(&quot;btl %2,%1\n\tsbbl %0,%0&quot;</span>
<span class="p_del">-	    : &quot;=r&quot;(ret) : &quot;m&quot;(*set), &quot;Ir&quot;(_sig-1) : &quot;cc&quot;);</span>
<span class="p_add">+	unsigned char ret;</span>
<span class="p_add">+	asm(&quot;btl %2,%1\n\tsetc %0&quot;</span>
<span class="p_add">+	    : &quot;=qm&quot;(ret) : &quot;m&quot;(*set), &quot;Ir&quot;(_sig-1) : &quot;cc&quot;);</span>
 	return ret;
 }
 
<span class="p_header">diff --git a/arch/x86/include/asm/smp.h b/arch/x86/include/asm/smp.h</span>
<span class="p_header">index 66b057306f40..0576b6157f3a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/smp.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/smp.h</span>
<span class="p_chunk">@@ -172,12 +172,6 @@</span> <span class="p_context"> extern int safe_smp_processor_id(void);</span>
 #elif defined(CONFIG_X86_64_SMP)
 #define raw_smp_processor_id() (this_cpu_read(cpu_number))
 
<span class="p_del">-#define stack_smp_processor_id()					\</span>
<span class="p_del">-({								\</span>
<span class="p_del">-	struct thread_info *ti;						\</span>
<span class="p_del">-	__asm__(&quot;andq %%rsp,%0; &quot;:&quot;=r&quot; (ti) : &quot;0&quot; (CURRENT_MASK));	\</span>
<span class="p_del">-	ti-&gt;cpu;							\</span>
<span class="p_del">-})</span>
 #define safe_smp_processor_id()		smp_processor_id()
 
 #endif
<span class="p_header">diff --git a/arch/x86/include/asm/sync_bitops.h b/arch/x86/include/asm/sync_bitops.h</span>
<span class="p_header">index f28a24b51dc7..cbf8847d02a0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/sync_bitops.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/sync_bitops.h</span>
<span class="p_chunk">@@ -79,10 +79,10 @@</span> <span class="p_context"> static inline void sync_change_bit(long nr, volatile unsigned long *addr)</span>
  */
 static inline int sync_test_and_set_bit(long nr, volatile unsigned long *addr)
 {
<span class="p_del">-	int oldbit;</span>
<span class="p_add">+	unsigned char oldbit;</span>
 
<span class="p_del">-	asm volatile(&quot;lock; bts %2,%1\n\tsbbl %0,%0&quot;</span>
<span class="p_del">-		     : &quot;=r&quot; (oldbit), &quot;+m&quot; (ADDR)</span>
<span class="p_add">+	asm volatile(&quot;lock; bts %2,%1\n\tsetc %0&quot;</span>
<span class="p_add">+		     : &quot;=qm&quot; (oldbit), &quot;+m&quot; (ADDR)</span>
 		     : &quot;Ir&quot; (nr) : &quot;memory&quot;);
 	return oldbit;
 }
<span class="p_chunk">@@ -97,10 +97,10 @@</span> <span class="p_context"> static inline int sync_test_and_set_bit(long nr, volatile unsigned long *addr)</span>
  */
 static inline int sync_test_and_clear_bit(long nr, volatile unsigned long *addr)
 {
<span class="p_del">-	int oldbit;</span>
<span class="p_add">+	unsigned char oldbit;</span>
 
<span class="p_del">-	asm volatile(&quot;lock; btr %2,%1\n\tsbbl %0,%0&quot;</span>
<span class="p_del">-		     : &quot;=r&quot; (oldbit), &quot;+m&quot; (ADDR)</span>
<span class="p_add">+	asm volatile(&quot;lock; btr %2,%1\n\tsetc %0&quot;</span>
<span class="p_add">+		     : &quot;=qm&quot; (oldbit), &quot;+m&quot; (ADDR)</span>
 		     : &quot;Ir&quot; (nr) : &quot;memory&quot;);
 	return oldbit;
 }
<span class="p_chunk">@@ -115,10 +115,10 @@</span> <span class="p_context"> static inline int sync_test_and_clear_bit(long nr, volatile unsigned long *addr)</span>
  */
 static inline int sync_test_and_change_bit(long nr, volatile unsigned long *addr)
 {
<span class="p_del">-	int oldbit;</span>
<span class="p_add">+	unsigned char oldbit;</span>
 
<span class="p_del">-	asm volatile(&quot;lock; btc %2,%1\n\tsbbl %0,%0&quot;</span>
<span class="p_del">-		     : &quot;=r&quot; (oldbit), &quot;+m&quot; (ADDR)</span>
<span class="p_add">+	asm volatile(&quot;lock; btc %2,%1\n\tsetc %0&quot;</span>
<span class="p_add">+		     : &quot;=qm&quot; (oldbit), &quot;+m&quot; (ADDR)</span>
 		     : &quot;Ir&quot; (nr) : &quot;memory&quot;);
 	return oldbit;
 }
<span class="p_header">diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h</span>
<span class="p_header">index 30c133ac05cd..89bff044a6f5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/thread_info.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/thread_info.h</span>
<span class="p_chunk">@@ -57,9 +57,6 @@</span> <span class="p_context"> struct thread_info {</span>
 	__u32			flags;		/* low level flags */
 	__u32			status;		/* thread synchronous flags */
 	__u32			cpu;		/* current CPU */
<span class="p_del">-	mm_segment_t		addr_limit;</span>
<span class="p_del">-	unsigned int		sig_on_uaccess_error:1;</span>
<span class="p_del">-	unsigned int		uaccess_err:1;	/* uaccess failed */</span>
 };
 
 #define INIT_THREAD_INFO(tsk)			\
<span class="p_chunk">@@ -67,7 +64,6 @@</span> <span class="p_context"> struct thread_info {</span>
 	.task		= &amp;tsk,			\
 	.flags		= 0,			\
 	.cpu		= 0,			\
<span class="p_del">-	.addr_limit	= KERNEL_DS,		\</span>
 }
 
 #define init_thread_info	(init_thread_union.thread_info)
<span class="p_chunk">@@ -186,11 +182,6 @@</span> <span class="p_context"> static inline unsigned long current_stack_pointer(void)</span>
 # define cpu_current_top_of_stack (cpu_tss + TSS_sp0)
 #endif
 
<span class="p_del">-/* Load thread_info address into &quot;reg&quot; */</span>
<span class="p_del">-#define GET_THREAD_INFO(reg) \</span>
<span class="p_del">-	_ASM_MOV PER_CPU_VAR(cpu_current_top_of_stack),reg ; \</span>
<span class="p_del">-	_ASM_SUB $(THREAD_SIZE),reg ;</span>
<span class="p_del">-</span>
 /*
  * ASM operand which evaluates to a &#39;thread_info&#39; address of
  * the current task, if it is known that &quot;reg&quot; is exactly &quot;off&quot;
<span class="p_header">diff --git a/arch/x86/include/asm/uaccess.h b/arch/x86/include/asm/uaccess.h</span>
<span class="p_header">index 2982387ba817..c03bfb68c503 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/uaccess.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/uaccess.h</span>
<span class="p_chunk">@@ -29,12 +29,12 @@</span> <span class="p_context"></span>
 #define USER_DS 	MAKE_MM_SEG(TASK_SIZE_MAX)
 
 #define get_ds()	(KERNEL_DS)
<span class="p_del">-#define get_fs()	(current_thread_info()-&gt;addr_limit)</span>
<span class="p_del">-#define set_fs(x)	(current_thread_info()-&gt;addr_limit = (x))</span>
<span class="p_add">+#define get_fs()	(current-&gt;thread.addr_limit)</span>
<span class="p_add">+#define set_fs(x)	(current-&gt;thread.addr_limit = (x))</span>
 
 #define segment_eq(a, b)	((a).seg == (b).seg)
 
<span class="p_del">-#define user_addr_max() (current_thread_info()-&gt;addr_limit.seg)</span>
<span class="p_add">+#define user_addr_max() (current-&gt;thread.addr_limit.seg)</span>
 #define __addr_ok(addr) 	\
 	((unsigned long __force)(addr) &lt; user_addr_max())
 
<span class="p_chunk">@@ -342,7 +342,26 @@</span> <span class="p_context"> do {									\</span>
 } while (0)
 
 #ifdef CONFIG_X86_32
<span class="p_del">-#define __get_user_asm_u64(x, ptr, retval, errret)	(x) = __get_user_bad()</span>
<span class="p_add">+#define __get_user_asm_u64(x, ptr, retval, errret)			\</span>
<span class="p_add">+({									\</span>
<span class="p_add">+	__typeof__(ptr) __ptr = (ptr);					\</span>
<span class="p_add">+	asm volatile(ASM_STAC &quot;\n&quot;					\</span>
<span class="p_add">+		     &quot;1:	movl %2,%%eax\n&quot;			\</span>
<span class="p_add">+		     &quot;2:	movl %3,%%edx\n&quot;			\</span>
<span class="p_add">+		     &quot;3: &quot; ASM_CLAC &quot;\n&quot;				\</span>
<span class="p_add">+		     &quot;.section .fixup,\&quot;ax\&quot;\n&quot;				\</span>
<span class="p_add">+		     &quot;4:	mov %4,%0\n&quot;				\</span>
<span class="p_add">+		     &quot;	xorl %%eax,%%eax\n&quot;				\</span>
<span class="p_add">+		     &quot;	xorl %%edx,%%edx\n&quot;				\</span>
<span class="p_add">+		     &quot;	jmp 3b\n&quot;					\</span>
<span class="p_add">+		     &quot;.previous\n&quot;					\</span>
<span class="p_add">+		     _ASM_EXTABLE(1b, 4b)				\</span>
<span class="p_add">+		     _ASM_EXTABLE(2b, 4b)				\</span>
<span class="p_add">+		     : &quot;=r&quot; (retval), &quot;=A&quot;(x)				\</span>
<span class="p_add">+		     : &quot;m&quot; (__m(__ptr)), &quot;m&quot; __m(((u32 *)(__ptr)) + 1),	\</span>
<span class="p_add">+		       &quot;i&quot; (errret), &quot;0&quot; (retval));			\</span>
<span class="p_add">+})</span>
<span class="p_add">+</span>
 #define __get_user_asm_ex_u64(x, ptr)			(x) = __get_user_bad()
 #else
 #define __get_user_asm_u64(x, ptr, retval, errret) \
<span class="p_chunk">@@ -429,7 +448,7 @@</span> <span class="p_context"> do {									\</span>
 #define __get_user_nocheck(x, ptr, size)				\
 ({									\
 	int __gu_err;							\
<span class="p_del">-	unsigned long __gu_val;						\</span>
<span class="p_add">+	__inttype(*(ptr)) __gu_val;					\</span>
 	__uaccess_begin();						\
 	__get_user_size(__gu_val, (ptr), (size), __gu_err, -EFAULT);	\
 	__uaccess_end();						\
<span class="p_chunk">@@ -468,13 +487,13 @@</span> <span class="p_context"> struct __large_struct { unsigned long buf[100]; };</span>
  * uaccess_try and catch
  */
 #define uaccess_try	do {						\
<span class="p_del">-	current_thread_info()-&gt;uaccess_err = 0;				\</span>
<span class="p_add">+	current-&gt;thread.uaccess_err = 0;				\</span>
 	__uaccess_begin();						\
 	barrier();
 
 #define uaccess_catch(err)						\
 	__uaccess_end();						\
<span class="p_del">-	(err) |= (current_thread_info()-&gt;uaccess_err ? -EFAULT : 0);	\</span>
<span class="p_add">+	(err) |= (current-&gt;thread.uaccess_err ? -EFAULT : 0);		\</span>
 } while (0)
 
 /**
<span class="p_header">diff --git a/arch/x86/include/asm/unistd.h b/arch/x86/include/asm/unistd.h</span>
<span class="p_header">index 2b19caa4081c..32712a925f26 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/unistd.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/unistd.h</span>
<span class="p_chunk">@@ -26,6 +26,8 @@</span> <span class="p_context"></span>
 #  define __ARCH_WANT_COMPAT_SYS_GETDENTS64
 #  define __ARCH_WANT_COMPAT_SYS_PREADV64
 #  define __ARCH_WANT_COMPAT_SYS_PWRITEV64
<span class="p_add">+#  define __ARCH_WANT_COMPAT_SYS_PREADV64V2</span>
<span class="p_add">+#  define __ARCH_WANT_COMPAT_SYS_PWRITEV64V2</span>
 
 # endif
 
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">index 674134e9f5e5..2bd5c6ff7ee7 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_chunk">@@ -31,7 +31,9 @@</span> <span class="p_context"> void common(void) {</span>
 	BLANK();
 	OFFSET(TI_flags, thread_info, flags);
 	OFFSET(TI_status, thread_info, status);
<span class="p_del">-	OFFSET(TI_addr_limit, thread_info, addr_limit);</span>
<span class="p_add">+</span>
<span class="p_add">+	BLANK();</span>
<span class="p_add">+	OFFSET(TASK_addr_limit, task_struct, thread.addr_limit);</span>
 
 	BLANK();
 	OFFSET(crypto_tfm_ctx_offset, crypto_tfm, __crt_ctx);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 0fe6953f421c..d22a7b9c4f0e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -1452,7 +1452,7 @@</span> <span class="p_context"> void cpu_init(void)</span>
 	struct task_struct *me;
 	struct tss_struct *t;
 	unsigned long v;
<span class="p_del">-	int cpu = stack_smp_processor_id();</span>
<span class="p_add">+	int cpu = raw_smp_processor_id();</span>
 	int i;
 
 	wait_for_master_cpu(cpu);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/rdrand.c b/arch/x86/kernel/cpu/rdrand.c</span>
<span class="p_header">index f6f50c4ceaec..cfa97ff67bda 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/rdrand.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/rdrand.c</span>
<span class="p_chunk">@@ -39,9 +39,9 @@</span> <span class="p_context"> __setup(&quot;nordrand&quot;, x86_rdrand_setup);</span>
  */
 #define SANITY_CHECK_LOOPS 8
 
<span class="p_add">+#ifdef CONFIG_ARCH_RANDOM</span>
 void x86_init_rdrand(struct cpuinfo_x86 *c)
 {
<span class="p_del">-#ifdef CONFIG_ARCH_RANDOM</span>
 	unsigned long tmp;
 	int i;
 
<span class="p_chunk">@@ -55,5 +55,5 @@</span> <span class="p_context"> void x86_init_rdrand(struct cpuinfo_x86 *c)</span>
 			return;
 		}
 	}
<span class="p_del">-#endif</span>
 }
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack.c b/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">index ef8017ca5ba9..de8242d8bb61 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack.c</span>
<span class="p_chunk">@@ -87,7 +87,7 @@</span> <span class="p_context"> static inline int valid_stack_ptr(struct task_struct *task,</span>
 		else
 			return 0;
 	}
<span class="p_del">-	return p &gt; t &amp;&amp; p &lt; t + THREAD_SIZE - size;</span>
<span class="p_add">+	return p &gt;= t &amp;&amp; p &lt; t + THREAD_SIZE - size;</span>
 }
 
 unsigned long
<span class="p_chunk">@@ -98,6 +98,14 @@</span> <span class="p_context"> print_context_stack(struct task_struct *task,</span>
 {
 	struct stack_frame *frame = (struct stack_frame *)bp;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we overflowed the stack into a guard page, jump back to the</span>
<span class="p_add">+	 * bottom of the usable stack.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((unsigned long)task_stack_page(task) - (unsigned long)stack &lt;</span>
<span class="p_add">+	    PAGE_SIZE)</span>
<span class="p_add">+		stack = (unsigned long *)task_stack_page(task);</span>
<span class="p_add">+</span>
 	while (valid_stack_ptr(task, stack, sizeof(*stack), end)) {
 		unsigned long addr;
 
<span class="p_chunk">@@ -226,6 +234,8 @@</span> <span class="p_context"> unsigned long oops_begin(void)</span>
 EXPORT_SYMBOL_GPL(oops_begin);
 NOKPROBE_SYMBOL(oops_begin);
 
<span class="p_add">+void __noreturn rewind_stack_do_exit(int signr);</span>
<span class="p_add">+</span>
 void oops_end(unsigned long flags, struct pt_regs *regs, int signr)
 {
 	if (regs &amp;&amp; kexec_should_crash(current))
<span class="p_chunk">@@ -247,7 +257,13 @@</span> <span class="p_context"> void oops_end(unsigned long flags, struct pt_regs *regs, int signr)</span>
 		panic(&quot;Fatal exception in interrupt&quot;);
 	if (panic_on_oops)
 		panic(&quot;Fatal exception&quot;);
<span class="p_del">-	do_exit(signr);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We&#39;re not going to return, but we might be on an IST stack or</span>
<span class="p_add">+	 * have very little stack space left.  Rewind the stack and kill</span>
<span class="p_add">+	 * the task.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	rewind_stack_do_exit(signr);</span>
 }
 NOKPROBE_SYMBOL(oops_end);
 
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack_64.c b/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_header">index d558a8a49016..2552a1eadfed 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_chunk">@@ -272,6 +272,8 @@</span> <span class="p_context"> show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
 
 	stack = sp;
 	for (i = 0; i &lt; kstack_depth_to_print; i++) {
<span class="p_add">+		unsigned long word;</span>
<span class="p_add">+</span>
 		if (stack &gt;= irq_stack &amp;&amp; stack &lt;= irq_stack_end) {
 			if (stack == irq_stack_end) {
 				stack = (unsigned long *) (irq_stack_end[-1]);
<span class="p_chunk">@@ -281,12 +283,18 @@</span> <span class="p_context"> show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
 		if (kstack_end(stack))
 			break;
 		}
<span class="p_add">+</span>
<span class="p_add">+		if (probe_kernel_address(stack, word))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
 		if ((i % STACKSLOTS_PER_LINE) == 0) {
 			if (i != 0)
 				pr_cont(&quot;\n&quot;);
<span class="p_del">-			printk(&quot;%s %016lx&quot;, log_lvl, *stack++);</span>
<span class="p_add">+			printk(&quot;%s %016lx&quot;, log_lvl, word);</span>
 		} else
<span class="p_del">-			pr_cont(&quot; %016lx&quot;, *stack++);</span>
<span class="p_add">+			pr_cont(&quot; %016lx&quot;, word);</span>
<span class="p_add">+</span>
<span class="p_add">+		stack++;</span>
 		touch_nmi_watchdog();
 	}
 	preempt_enable();
<span class="p_header">diff --git a/arch/x86/kernel/i386_ksyms_32.c b/arch/x86/kernel/i386_ksyms_32.c</span>
<span class="p_header">index 64341aa485ae..d40ee8a38fed 100644</span>
<span class="p_header">--- a/arch/x86/kernel/i386_ksyms_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/i386_ksyms_32.c</span>
<span class="p_chunk">@@ -42,3 +42,5 @@</span> <span class="p_context"> EXPORT_SYMBOL(empty_zero_page);</span>
 EXPORT_SYMBOL(___preempt_schedule);
 EXPORT_SYMBOL(___preempt_schedule_notrace);
 #endif
<span class="p_add">+</span>
<span class="p_add">+EXPORT_SYMBOL(__sw_hweight32);</span>
<span class="p_header">diff --git a/arch/x86/kernel/signal_compat.c b/arch/x86/kernel/signal_compat.c</span>
<span class="p_header">index dc3c0b1c816f..b44564bf86a8 100644</span>
<span class="p_header">--- a/arch/x86/kernel/signal_compat.c</span>
<span class="p_header">+++ b/arch/x86/kernel/signal_compat.c</span>
<span class="p_chunk">@@ -1,11 +1,104 @@</span> <span class="p_context"></span>
 #include &lt;linux/compat.h&gt;
 #include &lt;linux/uaccess.h&gt;
 
<span class="p_add">+/*</span>
<span class="p_add">+ * The compat_siginfo_t structure and handing code is very easy</span>
<span class="p_add">+ * to break in several ways.  It must always be updated when new</span>
<span class="p_add">+ * updates are made to the main siginfo_t, and</span>
<span class="p_add">+ * copy_siginfo_to_user32() must be updated when the</span>
<span class="p_add">+ * (arch-independent) copy_siginfo_to_user() is updated.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * It is also easy to put a new member in the compat_siginfo_t</span>
<span class="p_add">+ * which has implicit alignment which can move internal structure</span>
<span class="p_add">+ * alignment around breaking the ABI.  This can happen if you,</span>
<span class="p_add">+ * for instance, put a plain 64-bit value in there.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void signal_compat_build_tests(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int _sifields_offset = offsetof(compat_siginfo_t, _sifields);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If adding a new si_code, there is probably new data in</span>
<span class="p_add">+	 * the siginfo.  Make sure folks bumping the si_code</span>
<span class="p_add">+	 * limits also have to look at this code.  Make sure any</span>
<span class="p_add">+	 * new fields are handled in copy_siginfo_to_user32()!</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUILD_BUG_ON(NSIGILL  != 8);</span>
<span class="p_add">+	BUILD_BUG_ON(NSIGFPE  != 8);</span>
<span class="p_add">+	BUILD_BUG_ON(NSIGSEGV != 4);</span>
<span class="p_add">+	BUILD_BUG_ON(NSIGBUS  != 5);</span>
<span class="p_add">+	BUILD_BUG_ON(NSIGTRAP != 4);</span>
<span class="p_add">+	BUILD_BUG_ON(NSIGCHLD != 6);</span>
<span class="p_add">+	BUILD_BUG_ON(NSIGSYS  != 1);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* This is part of the ABI and can never change in size: */</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(compat_siginfo_t) != 128);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The offsets of all the (unioned) si_fields are fixed</span>
<span class="p_add">+	 * in the ABI, of course.  Make sure none of them ever</span>
<span class="p_add">+	 * move and are always at the beginning:</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUILD_BUG_ON(offsetof(compat_siginfo_t, _sifields) != 3 * sizeof(int));</span>
<span class="p_add">+#define CHECK_CSI_OFFSET(name)	  BUILD_BUG_ON(_sifields_offset != offsetof(compat_siginfo_t, _sifields.name))</span>
<span class="p_add">+</span>
<span class="p_add">+	 /*</span>
<span class="p_add">+	 * Ensure that the size of each si_field never changes.</span>
<span class="p_add">+	 * If it does, it is a sign that the</span>
<span class="p_add">+	 * copy_siginfo_to_user32() code below needs to updated</span>
<span class="p_add">+	 * along with the size in the CHECK_SI_SIZE().</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We repeat this check for both the generic and compat</span>
<span class="p_add">+	 * siginfos.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Note: it is OK for these to grow as long as the whole</span>
<span class="p_add">+	 * structure stays within the padding size (checked</span>
<span class="p_add">+	 * above).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+#define CHECK_CSI_SIZE(name, size) BUILD_BUG_ON(size != sizeof(((compat_siginfo_t *)0)-&gt;_sifields.name))</span>
<span class="p_add">+#define CHECK_SI_SIZE(name, size) BUILD_BUG_ON(size != sizeof(((siginfo_t *)0)-&gt;_sifields.name))</span>
<span class="p_add">+</span>
<span class="p_add">+	CHECK_CSI_OFFSET(_kill);</span>
<span class="p_add">+	CHECK_CSI_SIZE  (_kill, 2*sizeof(int));</span>
<span class="p_add">+	CHECK_SI_SIZE   (_kill, 2*sizeof(int));</span>
<span class="p_add">+</span>
<span class="p_add">+	CHECK_CSI_OFFSET(_timer);</span>
<span class="p_add">+	CHECK_CSI_SIZE  (_timer, 5*sizeof(int));</span>
<span class="p_add">+	CHECK_SI_SIZE   (_timer, 6*sizeof(int));</span>
<span class="p_add">+</span>
<span class="p_add">+	CHECK_CSI_OFFSET(_rt);</span>
<span class="p_add">+	CHECK_CSI_SIZE  (_rt, 3*sizeof(int));</span>
<span class="p_add">+	CHECK_SI_SIZE   (_rt, 4*sizeof(int));</span>
<span class="p_add">+</span>
<span class="p_add">+	CHECK_CSI_OFFSET(_sigchld);</span>
<span class="p_add">+	CHECK_CSI_SIZE  (_sigchld, 5*sizeof(int));</span>
<span class="p_add">+	CHECK_SI_SIZE   (_sigchld, 8*sizeof(int));</span>
<span class="p_add">+</span>
<span class="p_add">+	CHECK_CSI_OFFSET(_sigchld_x32);</span>
<span class="p_add">+	CHECK_CSI_SIZE  (_sigchld_x32, 7*sizeof(int));</span>
<span class="p_add">+	/* no _sigchld_x32 in the generic siginfo_t */</span>
<span class="p_add">+</span>
<span class="p_add">+	CHECK_CSI_OFFSET(_sigfault);</span>
<span class="p_add">+	CHECK_CSI_SIZE  (_sigfault, 4*sizeof(int));</span>
<span class="p_add">+	CHECK_SI_SIZE   (_sigfault, 8*sizeof(int));</span>
<span class="p_add">+</span>
<span class="p_add">+	CHECK_CSI_OFFSET(_sigpoll);</span>
<span class="p_add">+	CHECK_CSI_SIZE  (_sigpoll, 2*sizeof(int));</span>
<span class="p_add">+	CHECK_SI_SIZE   (_sigpoll, 4*sizeof(int));</span>
<span class="p_add">+</span>
<span class="p_add">+	CHECK_CSI_OFFSET(_sigsys);</span>
<span class="p_add">+	CHECK_CSI_SIZE  (_sigsys, 3*sizeof(int));</span>
<span class="p_add">+	CHECK_SI_SIZE   (_sigsys, 4*sizeof(int));</span>
<span class="p_add">+</span>
<span class="p_add">+	/* any new si_fields should be added here */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int copy_siginfo_to_user32(compat_siginfo_t __user *to, const siginfo_t *from)
 {
 	int err = 0;
 	bool ia32 = test_thread_flag(TIF_IA32);
 
<span class="p_add">+	signal_compat_build_tests();</span>
<span class="p_add">+</span>
 	if (!access_ok(VERIFY_WRITE, to, sizeof(compat_siginfo_t)))
 		return -EFAULT;
 
<span class="p_chunk">@@ -32,6 +125,21 @@</span> <span class="p_context"> int copy_siginfo_to_user32(compat_siginfo_t __user *to, const siginfo_t *from)</span>
 					  &amp;to-&gt;_sifields._pad[0]);
 			switch (from-&gt;si_code &gt;&gt; 16) {
 			case __SI_FAULT &gt;&gt; 16:
<span class="p_add">+				if (from-&gt;si_signo == SIGBUS &amp;&amp;</span>
<span class="p_add">+				    (from-&gt;si_code == BUS_MCEERR_AR ||</span>
<span class="p_add">+				     from-&gt;si_code == BUS_MCEERR_AO))</span>
<span class="p_add">+					put_user_ex(from-&gt;si_addr_lsb, &amp;to-&gt;si_addr_lsb);</span>
<span class="p_add">+</span>
<span class="p_add">+				if (from-&gt;si_signo == SIGSEGV) {</span>
<span class="p_add">+					if (from-&gt;si_code == SEGV_BNDERR) {</span>
<span class="p_add">+						compat_uptr_t lower = (unsigned long)&amp;to-&gt;si_lower;</span>
<span class="p_add">+						compat_uptr_t upper = (unsigned long)&amp;to-&gt;si_upper;</span>
<span class="p_add">+						put_user_ex(lower, &amp;to-&gt;si_lower);</span>
<span class="p_add">+						put_user_ex(upper, &amp;to-&gt;si_upper);</span>
<span class="p_add">+					}</span>
<span class="p_add">+					if (from-&gt;si_code == SEGV_PKUERR)</span>
<span class="p_add">+						put_user_ex(from-&gt;si_pkey, &amp;to-&gt;si_pkey);</span>
<span class="p_add">+				}</span>
 				break;
 			case __SI_SYS &gt;&gt; 16:
 				put_user_ex(from-&gt;si_syscall, &amp;to-&gt;si_syscall);
<span class="p_header">diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c</span>
<span class="p_header">index fafe8b923cac..0e91dbeca2fd 100644</span>
<span class="p_header">--- a/arch/x86/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/smpboot.c</span>
<span class="p_chunk">@@ -1285,7 +1285,6 @@</span> <span class="p_context"> void __init native_smp_prepare_cpus(unsigned int max_cpus)</span>
 	cpumask_copy(cpu_callin_mask, cpumask_of(0));
 	mb();
 
<span class="p_del">-	current_thread_info()-&gt;cpu = 0;  /* needed? */</span>
 	for_each_possible_cpu(i) {
 		zalloc_cpumask_var(&amp;per_cpu(cpu_sibling_map, i), GFP_KERNEL);
 		zalloc_cpumask_var(&amp;per_cpu(cpu_core_map, i), GFP_KERNEL);
<span class="p_header">diff --git a/arch/x86/kernel/vm86_32.c b/arch/x86/kernel/vm86_32.c</span>
<span class="p_header">index 3dce1ca0a653..01f30e56f99e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/vm86_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/vm86_32.c</span>
<span class="p_chunk">@@ -440,10 +440,7 @@</span> <span class="p_context"> static inline unsigned long get_vflags(struct kernel_vm86_regs *regs)</span>
 
 static inline int is_revectored(int nr, struct revectored_struct *bitmap)
 {
<span class="p_del">-	__asm__ __volatile__(&quot;btl %2,%1\n\tsbbl %0,%0&quot;</span>
<span class="p_del">-		:&quot;=r&quot; (nr)</span>
<span class="p_del">-		:&quot;m&quot; (*bitmap), &quot;r&quot; (nr));</span>
<span class="p_del">-	return nr;</span>
<span class="p_add">+	return test_bit(nr, bitmap-&gt;__map);</span>
 }
 
 #define val_byte(val, n) (((__u8 *)&amp;val)[n])
<span class="p_header">diff --git a/arch/x86/kernel/x8664_ksyms_64.c b/arch/x86/kernel/x8664_ksyms_64.c</span>
<span class="p_header">index cd05942bc918..f1aebfb49c36 100644</span>
<span class="p_header">--- a/arch/x86/kernel/x8664_ksyms_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/x8664_ksyms_64.c</span>
<span class="p_chunk">@@ -44,6 +44,9 @@</span> <span class="p_context"> EXPORT_SYMBOL(clear_page);</span>
 
 EXPORT_SYMBOL(csum_partial);
 
<span class="p_add">+EXPORT_SYMBOL(__sw_hweight32);</span>
<span class="p_add">+EXPORT_SYMBOL(__sw_hweight64);</span>
<span class="p_add">+</span>
 /*
  * Export string functions. We normally rely on gcc builtin for most of these,
  * but gcc sometimes decides not to inline them.
<span class="p_header">diff --git a/arch/x86/lib/Makefile b/arch/x86/lib/Makefile</span>
<span class="p_header">index 72a576752a7e..ec969cc3eb20 100644</span>
<span class="p_header">--- a/arch/x86/lib/Makefile</span>
<span class="p_header">+++ b/arch/x86/lib/Makefile</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"> lib-y += memcpy_$(BITS).o</span>
 lib-$(CONFIG_RWSEM_XCHGADD_ALGORITHM) += rwsem.o
 lib-$(CONFIG_INSTRUCTION_DECODER) += insn.o inat.o
 
<span class="p_del">-obj-y += msr.o msr-reg.o msr-reg-export.o</span>
<span class="p_add">+obj-y += msr.o msr-reg.o msr-reg-export.o hweight.o</span>
 
 ifeq ($(CONFIG_X86_32),y)
         obj-y += atomic64_32.o
<span class="p_header">diff --git a/arch/x86/lib/copy_user_64.S b/arch/x86/lib/copy_user_64.S</span>
<span class="p_header">index 2b0ef26da0bd..bf603ebbfd8e 100644</span>
<span class="p_header">--- a/arch/x86/lib/copy_user_64.S</span>
<span class="p_header">+++ b/arch/x86/lib/copy_user_64.S</span>
<span class="p_chunk">@@ -17,11 +17,11 @@</span> <span class="p_context"></span>
 
 /* Standard copy_to_user with segment limit checking */
 ENTRY(_copy_to_user)
<span class="p_del">-	GET_THREAD_INFO(%rax)</span>
<span class="p_add">+	mov PER_CPU_VAR(current_task), %rax</span>
 	movq %rdi,%rcx
 	addq %rdx,%rcx
 	jc bad_to_user
<span class="p_del">-	cmpq TI_addr_limit(%rax),%rcx</span>
<span class="p_add">+	cmpq TASK_addr_limit(%rax),%rcx</span>
 	ja bad_to_user
 	ALTERNATIVE_2 &quot;jmp copy_user_generic_unrolled&quot;,		\
 		      &quot;jmp copy_user_generic_string&quot;,		\
<span class="p_chunk">@@ -32,11 +32,11 @@</span> <span class="p_context"> ENDPROC(_copy_to_user)</span>
 
 /* Standard copy_from_user with segment limit checking */
 ENTRY(_copy_from_user)
<span class="p_del">-	GET_THREAD_INFO(%rax)</span>
<span class="p_add">+	mov PER_CPU_VAR(current_task), %rax</span>
 	movq %rsi,%rcx
 	addq %rdx,%rcx
 	jc bad_from_user
<span class="p_del">-	cmpq TI_addr_limit(%rax),%rcx</span>
<span class="p_add">+	cmpq TASK_addr_limit(%rax),%rcx</span>
 	ja bad_from_user
 	ALTERNATIVE_2 &quot;jmp copy_user_generic_unrolled&quot;,		\
 		      &quot;jmp copy_user_generic_string&quot;,		\
<span class="p_header">diff --git a/arch/x86/lib/csum-wrappers_64.c b/arch/x86/lib/csum-wrappers_64.c</span>
<span class="p_header">index 28a6654f0d08..b6fcb9a9ddbc 100644</span>
<span class="p_header">--- a/arch/x86/lib/csum-wrappers_64.c</span>
<span class="p_header">+++ b/arch/x86/lib/csum-wrappers_64.c</span>
<span class="p_chunk">@@ -6,6 +6,7 @@</span> <span class="p_context"></span>
  */
 #include &lt;asm/checksum.h&gt;
 #include &lt;linux/module.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 #include &lt;asm/smap.h&gt;
 
 /**
<span class="p_header">diff --git a/arch/x86/lib/getuser.S b/arch/x86/lib/getuser.S</span>
<span class="p_header">index 46668cda4ffd..0ef5128c2de8 100644</span>
<span class="p_header">--- a/arch/x86/lib/getuser.S</span>
<span class="p_header">+++ b/arch/x86/lib/getuser.S</span>
<span class="p_chunk">@@ -35,8 +35,8 @@</span> <span class="p_context"></span>
 
 	.text
 ENTRY(__get_user_1)
<span class="p_del">-	GET_THREAD_INFO(%_ASM_DX)</span>
<span class="p_del">-	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX</span>
<span class="p_add">+	mov PER_CPU_VAR(current_task), %_ASM_DX</span>
<span class="p_add">+	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX</span>
 	jae bad_get_user
 	ASM_STAC
 1:	movzbl (%_ASM_AX),%edx
<span class="p_chunk">@@ -48,8 +48,8 @@</span> <span class="p_context"> ENDPROC(__get_user_1)</span>
 ENTRY(__get_user_2)
 	add $1,%_ASM_AX
 	jc bad_get_user
<span class="p_del">-	GET_THREAD_INFO(%_ASM_DX)</span>
<span class="p_del">-	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX</span>
<span class="p_add">+	mov PER_CPU_VAR(current_task), %_ASM_DX</span>
<span class="p_add">+	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX</span>
 	jae bad_get_user
 	ASM_STAC
 2:	movzwl -1(%_ASM_AX),%edx
<span class="p_chunk">@@ -61,8 +61,8 @@</span> <span class="p_context"> ENDPROC(__get_user_2)</span>
 ENTRY(__get_user_4)
 	add $3,%_ASM_AX
 	jc bad_get_user
<span class="p_del">-	GET_THREAD_INFO(%_ASM_DX)</span>
<span class="p_del">-	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX</span>
<span class="p_add">+	mov PER_CPU_VAR(current_task), %_ASM_DX</span>
<span class="p_add">+	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX</span>
 	jae bad_get_user
 	ASM_STAC
 3:	movl -3(%_ASM_AX),%edx
<span class="p_chunk">@@ -75,8 +75,8 @@</span> <span class="p_context"> ENTRY(__get_user_8)</span>
 #ifdef CONFIG_X86_64
 	add $7,%_ASM_AX
 	jc bad_get_user
<span class="p_del">-	GET_THREAD_INFO(%_ASM_DX)</span>
<span class="p_del">-	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX</span>
<span class="p_add">+	mov PER_CPU_VAR(current_task), %_ASM_DX</span>
<span class="p_add">+	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX</span>
 	jae bad_get_user
 	ASM_STAC
 4:	movq -7(%_ASM_AX),%rdx
<span class="p_chunk">@@ -86,8 +86,8 @@</span> <span class="p_context"> ENTRY(__get_user_8)</span>
 #else
 	add $7,%_ASM_AX
 	jc bad_get_user_8
<span class="p_del">-	GET_THREAD_INFO(%_ASM_DX)</span>
<span class="p_del">-	cmp TI_addr_limit(%_ASM_DX),%_ASM_AX</span>
<span class="p_add">+	mov PER_CPU_VAR(current_task), %_ASM_DX</span>
<span class="p_add">+	cmp TASK_addr_limit(%_ASM_DX),%_ASM_AX</span>
 	jae bad_get_user_8
 	ASM_STAC
 4:	movl -7(%_ASM_AX),%edx
<span class="p_header">diff --git a/arch/x86/lib/hweight.S b/arch/x86/lib/hweight.S</span>
new file mode 100644
<span class="p_header">index 000000000000..02de3d74d2c5</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/lib/hweight.S</span>
<span class="p_chunk">@@ -0,0 +1,77 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/asm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * unsigned int __sw_hweight32(unsigned int w)</span>
<span class="p_add">+ * %rdi: w</span>
<span class="p_add">+ */</span>
<span class="p_add">+ENTRY(__sw_hweight32)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	movl %edi, %eax				# w</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	__ASM_SIZE(push,) %__ASM_REG(dx)</span>
<span class="p_add">+	movl %eax, %edx				# w -&gt; t</span>
<span class="p_add">+	shrl %edx				# t &gt;&gt;= 1</span>
<span class="p_add">+	andl $0x55555555, %edx			# t &amp;= 0x55555555</span>
<span class="p_add">+	subl %edx, %eax				# w -= t</span>
<span class="p_add">+</span>
<span class="p_add">+	movl %eax, %edx				# w -&gt; t</span>
<span class="p_add">+	shrl $2, %eax				# w_tmp &gt;&gt;= 2</span>
<span class="p_add">+	andl $0x33333333, %edx			# t	&amp;= 0x33333333</span>
<span class="p_add">+	andl $0x33333333, %eax			# w_tmp &amp;= 0x33333333</span>
<span class="p_add">+	addl %edx, %eax				# w = w_tmp + t</span>
<span class="p_add">+</span>
<span class="p_add">+	movl %eax, %edx				# w -&gt; t</span>
<span class="p_add">+	shrl $4, %edx				# t &gt;&gt;= 4</span>
<span class="p_add">+	addl %edx, %eax				# w_tmp += t</span>
<span class="p_add">+	andl  $0x0f0f0f0f, %eax			# w_tmp &amp;= 0x0f0f0f0f</span>
<span class="p_add">+	imull $0x01010101, %eax, %eax		# w_tmp *= 0x01010101</span>
<span class="p_add">+	shrl $24, %eax				# w = w_tmp &gt;&gt; 24</span>
<span class="p_add">+	__ASM_SIZE(pop,) %__ASM_REG(dx)</span>
<span class="p_add">+	ret</span>
<span class="p_add">+ENDPROC(__sw_hweight32)</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(__sw_hweight64)</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	pushq   %rdx</span>
<span class="p_add">+</span>
<span class="p_add">+	movq    %rdi, %rdx                      # w -&gt; t</span>
<span class="p_add">+	movabsq $0x5555555555555555, %rax</span>
<span class="p_add">+	shrq    %rdx                            # t &gt;&gt;= 1</span>
<span class="p_add">+	andq    %rdx, %rax                      # t &amp;= 0x5555555555555555</span>
<span class="p_add">+	movabsq $0x3333333333333333, %rdx</span>
<span class="p_add">+	subq    %rax, %rdi                      # w -= t</span>
<span class="p_add">+</span>
<span class="p_add">+	movq    %rdi, %rax                      # w -&gt; t</span>
<span class="p_add">+	shrq    $2, %rdi                        # w_tmp &gt;&gt;= 2</span>
<span class="p_add">+	andq    %rdx, %rax                      # t     &amp;= 0x3333333333333333</span>
<span class="p_add">+	andq    %rdi, %rdx                      # w_tmp &amp;= 0x3333333333333333</span>
<span class="p_add">+	addq    %rdx, %rax                      # w = w_tmp + t</span>
<span class="p_add">+</span>
<span class="p_add">+	movq    %rax, %rdx                      # w -&gt; t</span>
<span class="p_add">+	shrq    $4, %rdx                        # t &gt;&gt;= 4</span>
<span class="p_add">+	addq    %rdx, %rax                      # w_tmp += t</span>
<span class="p_add">+	movabsq $0x0f0f0f0f0f0f0f0f, %rdx</span>
<span class="p_add">+	andq    %rdx, %rax                      # w_tmp &amp;= 0x0f0f0f0f0f0f0f0f</span>
<span class="p_add">+	movabsq $0x0101010101010101, %rdx</span>
<span class="p_add">+	imulq   %rdx, %rax                      # w_tmp *= 0x0101010101010101</span>
<span class="p_add">+	shrq    $56, %rax                       # w = w_tmp &gt;&gt; 56</span>
<span class="p_add">+</span>
<span class="p_add">+	popq    %rdx</span>
<span class="p_add">+	ret</span>
<span class="p_add">+#else /* CONFIG_X86_32 */</span>
<span class="p_add">+	/* We&#39;re getting an u64 arg in (%eax,%edx): unsigned long hweight64(__u64 w) */</span>
<span class="p_add">+	pushl   %ecx</span>
<span class="p_add">+</span>
<span class="p_add">+	call    __sw_hweight32</span>
<span class="p_add">+	movl    %eax, %ecx                      # stash away result</span>
<span class="p_add">+	movl    %edx, %eax                      # second part of input</span>
<span class="p_add">+	call    __sw_hweight32</span>
<span class="p_add">+	addl    %ecx, %eax                      # result</span>
<span class="p_add">+</span>
<span class="p_add">+	popl    %ecx</span>
<span class="p_add">+	ret</span>
<span class="p_add">+#endif</span>
<span class="p_add">+ENDPROC(__sw_hweight64)</span>
<span class="p_header">diff --git a/arch/x86/lib/putuser.S b/arch/x86/lib/putuser.S</span>
<span class="p_header">index e0817a12d323..c891ece81e5b 100644</span>
<span class="p_header">--- a/arch/x86/lib/putuser.S</span>
<span class="p_header">+++ b/arch/x86/lib/putuser.S</span>
<span class="p_chunk">@@ -29,14 +29,14 @@</span> <span class="p_context"></span>
  * as they get called from within inline assembly.
  */
 
<span class="p_del">-#define ENTER	GET_THREAD_INFO(%_ASM_BX)</span>
<span class="p_add">+#define ENTER	mov PER_CPU_VAR(current_task), %_ASM_BX</span>
 #define EXIT	ASM_CLAC ;	\
 		ret
 
 .text
 ENTRY(__put_user_1)
 	ENTER
<span class="p_del">-	cmp TI_addr_limit(%_ASM_BX),%_ASM_CX</span>
<span class="p_add">+	cmp TASK_addr_limit(%_ASM_BX),%_ASM_CX</span>
 	jae bad_put_user
 	ASM_STAC
 1:	movb %al,(%_ASM_CX)
<span class="p_chunk">@@ -46,7 +46,7 @@</span> <span class="p_context"> ENDPROC(__put_user_1)</span>
 
 ENTRY(__put_user_2)
 	ENTER
<span class="p_del">-	mov TI_addr_limit(%_ASM_BX),%_ASM_BX</span>
<span class="p_add">+	mov TASK_addr_limit(%_ASM_BX),%_ASM_BX</span>
 	sub $1,%_ASM_BX
 	cmp %_ASM_BX,%_ASM_CX
 	jae bad_put_user
<span class="p_chunk">@@ -58,7 +58,7 @@</span> <span class="p_context"> ENDPROC(__put_user_2)</span>
 
 ENTRY(__put_user_4)
 	ENTER
<span class="p_del">-	mov TI_addr_limit(%_ASM_BX),%_ASM_BX</span>
<span class="p_add">+	mov TASK_addr_limit(%_ASM_BX),%_ASM_BX</span>
 	sub $3,%_ASM_BX
 	cmp %_ASM_BX,%_ASM_CX
 	jae bad_put_user
<span class="p_chunk">@@ -70,7 +70,7 @@</span> <span class="p_context"> ENDPROC(__put_user_4)</span>
 
 ENTRY(__put_user_8)
 	ENTER
<span class="p_del">-	mov TI_addr_limit(%_ASM_BX),%_ASM_BX</span>
<span class="p_add">+	mov TASK_addr_limit(%_ASM_BX),%_ASM_BX</span>
 	sub $7,%_ASM_BX
 	cmp %_ASM_BX,%_ASM_CX
 	jae bad_put_user
<span class="p_header">diff --git a/arch/x86/lib/usercopy_64.c b/arch/x86/lib/usercopy_64.c</span>
<span class="p_header">index 0a42327a59d7..9f760cdcaf40 100644</span>
<span class="p_header">--- a/arch/x86/lib/usercopy_64.c</span>
<span class="p_header">+++ b/arch/x86/lib/usercopy_64.c</span>
<span class="p_chunk">@@ -6,7 +6,7 @@</span> <span class="p_context"></span>
  * Copyright 2002 Andi Kleen &lt;ak@suse.de&gt;
  */
 #include &lt;linux/module.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 /*
  * Zero Userspace
<span class="p_header">diff --git a/arch/x86/mm/extable.c b/arch/x86/mm/extable.c</span>
<span class="p_header">index 4bb53b89f3c5..0f90cc218d04 100644</span>
<span class="p_header">--- a/arch/x86/mm/extable.c</span>
<span class="p_header">+++ b/arch/x86/mm/extable.c</span>
<span class="p_chunk">@@ -37,7 +37,7 @@</span> <span class="p_context"> bool ex_handler_ext(const struct exception_table_entry *fixup,</span>
 		   struct pt_regs *regs, int trapnr)
 {
 	/* Special hack for uaccess_err */
<span class="p_del">-	current_thread_info()-&gt;uaccess_err = 1;</span>
<span class="p_add">+	current-&gt;thread.uaccess_err = 1;</span>
 	regs-&gt;ip = ex_fixup_addr(fixup);
 	return true;
 }
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index 7d1fa7cd2374..d22161ab941d 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -439,7 +439,7 @@</span> <span class="p_context"> static noinline int vmalloc_fault(unsigned long address)</span>
 	 * happen within a race in page table update. In the later
 	 * case just flush:
 	 */
<span class="p_del">-	pgd = pgd_offset(current-&gt;active_mm, address);</span>
<span class="p_add">+	pgd = (pgd_t *)__va(read_cr3()) + pgd_index(address);</span>
 	pgd_ref = pgd_offset_k(address);
 	if (pgd_none(*pgd_ref))
 		return -1;
<span class="p_chunk">@@ -737,7 +737,7 @@</span> <span class="p_context"> no_context(struct pt_regs *regs, unsigned long error_code,</span>
 		 * In this case we need to make sure we&#39;re not recursively
 		 * faulting through the emulate_vsyscall() logic.
 		 */
<span class="p_del">-		if (current_thread_info()-&gt;sig_on_uaccess_error &amp;&amp; signal) {</span>
<span class="p_add">+		if (current-&gt;thread.sig_on_uaccess_err &amp;&amp; signal) {</span>
 			tsk-&gt;thread.trap_nr = X86_TRAP_PF;
 			tsk-&gt;thread.error_code = error_code | PF_USER;
 			tsk-&gt;thread.cr2 = address;
<span class="p_header">diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c</span>
<span class="p_header">index bce2e5d9edd4..e14f87057c3f 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_64.c</span>
<span class="p_chunk">@@ -354,7 +354,7 @@</span> <span class="p_context"> phys_pte_init(pte_t *pte_page, unsigned long addr, unsigned long end,</span>
 		 * pagetable pages as RO. So assume someone who pre-setup
 		 * these mappings are more intelligent.
 		 */
<span class="p_del">-		if (pte_val(*pte)) {</span>
<span class="p_add">+		if (!pte_none(*pte)) {</span>
 			if (!after_bootmem)
 				pages++;
 			continue;
<span class="p_chunk">@@ -396,7 +396,7 @@</span> <span class="p_context"> phys_pmd_init(pmd_t *pmd_page, unsigned long address, unsigned long end,</span>
 			continue;
 		}
 
<span class="p_del">-		if (pmd_val(*pmd)) {</span>
<span class="p_add">+		if (!pmd_none(*pmd)) {</span>
 			if (!pmd_large(*pmd)) {
 				spin_lock(&amp;init_mm.page_table_lock);
 				pte = (pte_t *)pmd_page_vaddr(*pmd);
<span class="p_chunk">@@ -470,7 +470,7 @@</span> <span class="p_context"> phys_pud_init(pud_t *pud_page, unsigned long addr, unsigned long end,</span>
 			continue;
 		}
 
<span class="p_del">-		if (pud_val(*pud)) {</span>
<span class="p_add">+		if (!pud_none(*pud)) {</span>
 			if (!pud_large(*pud)) {
 				pmd = pmd_offset(pud, 0);
 				last_map_addr = phys_pmd_init(pmd, addr, end,
<span class="p_chunk">@@ -673,7 +673,7 @@</span> <span class="p_context"> static void __meminit free_pte_table(pte_t *pte_start, pmd_t *pmd)</span>
 
 	for (i = 0; i &lt; PTRS_PER_PTE; i++) {
 		pte = pte_start + i;
<span class="p_del">-		if (pte_val(*pte))</span>
<span class="p_add">+		if (!pte_none(*pte))</span>
 			return;
 	}
 
<span class="p_chunk">@@ -691,7 +691,7 @@</span> <span class="p_context"> static void __meminit free_pmd_table(pmd_t *pmd_start, pud_t *pud)</span>
 
 	for (i = 0; i &lt; PTRS_PER_PMD; i++) {
 		pmd = pmd_start + i;
<span class="p_del">-		if (pmd_val(*pmd))</span>
<span class="p_add">+		if (!pmd_none(*pmd))</span>
 			return;
 	}
 
<span class="p_chunk">@@ -702,27 +702,6 @@</span> <span class="p_context"> static void __meminit free_pmd_table(pmd_t *pmd_start, pud_t *pud)</span>
 	spin_unlock(&amp;init_mm.page_table_lock);
 }
 
<span class="p_del">-/* Return true if pgd is changed, otherwise return false. */</span>
<span class="p_del">-static bool __meminit free_pud_table(pud_t *pud_start, pgd_t *pgd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pud_t *pud;</span>
<span class="p_del">-	int i;</span>
<span class="p_del">-</span>
<span class="p_del">-	for (i = 0; i &lt; PTRS_PER_PUD; i++) {</span>
<span class="p_del">-		pud = pud_start + i;</span>
<span class="p_del">-		if (pud_val(*pud))</span>
<span class="p_del">-			return false;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* free a pud table */</span>
<span class="p_del">-	free_pagetable(pgd_page(*pgd), 0);</span>
<span class="p_del">-	spin_lock(&amp;init_mm.page_table_lock);</span>
<span class="p_del">-	pgd_clear(pgd);</span>
<span class="p_del">-	spin_unlock(&amp;init_mm.page_table_lock);</span>
<span class="p_del">-</span>
<span class="p_del">-	return true;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void __meminit
 remove_pte_table(pte_t *pte_start, unsigned long addr, unsigned long end,
 		 bool direct)
<span class="p_chunk">@@ -913,7 +892,6 @@</span> <span class="p_context"> remove_pagetable(unsigned long start, unsigned long end, bool direct)</span>
 	unsigned long addr;
 	pgd_t *pgd;
 	pud_t *pud;
<span class="p_del">-	bool pgd_changed = false;</span>
 
 	for (addr = start; addr &lt; end; addr = next) {
 		next = pgd_addr_end(addr, end);
<span class="p_chunk">@@ -924,13 +902,8 @@</span> <span class="p_context"> remove_pagetable(unsigned long start, unsigned long end, bool direct)</span>
 
 		pud = (pud_t *)pgd_page_vaddr(*pgd);
 		remove_pud_table(pud, addr, next, direct);
<span class="p_del">-		if (free_pud_table(pud, pgd))</span>
<span class="p_del">-			pgd_changed = true;</span>
 	}
 
<span class="p_del">-	if (pgd_changed)</span>
<span class="p_del">-		sync_global_pgds(start, end - 1, 1);</span>
<span class="p_del">-</span>
 	flush_tlb_all();
 }
 
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index 7a1f7bbf4105..47870a534877 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -746,18 +746,6 @@</span> <span class="p_context"> static bool try_to_free_pmd_page(pmd_t *pmd)</span>
 	return true;
 }
 
<span class="p_del">-static bool try_to_free_pud_page(pud_t *pud)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int i;</span>
<span class="p_del">-</span>
<span class="p_del">-	for (i = 0; i &lt; PTRS_PER_PUD; i++)</span>
<span class="p_del">-		if (!pud_none(pud[i]))</span>
<span class="p_del">-			return false;</span>
<span class="p_del">-</span>
<span class="p_del">-	free_page((unsigned long)pud);</span>
<span class="p_del">-	return true;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static bool unmap_pte_range(pmd_t *pmd, unsigned long start, unsigned long end)
 {
 	pte_t *pte = pte_offset_kernel(pmd, start);
<span class="p_chunk">@@ -871,16 +859,6 @@</span> <span class="p_context"> static void unmap_pud_range(pgd_t *pgd, unsigned long start, unsigned long end)</span>
 	 */
 }
 
<span class="p_del">-static void unmap_pgd_range(pgd_t *root, unsigned long addr, unsigned long end)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pgd_t *pgd_entry = root + pgd_index(addr);</span>
<span class="p_del">-</span>
<span class="p_del">-	unmap_pud_range(pgd_entry, addr, end);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (try_to_free_pud_page((pud_t *)pgd_page_vaddr(*pgd_entry)))</span>
<span class="p_del">-		pgd_clear(pgd_entry);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static int alloc_pte_page(pmd_t *pmd)
 {
 	pte_t *pte = (pte_t *)get_zeroed_page(GFP_KERNEL | __GFP_NOTRACK);
<span class="p_chunk">@@ -1113,7 +1091,12 @@</span> <span class="p_context"> static int populate_pgd(struct cpa_data *cpa, unsigned long addr)</span>
 
 	ret = populate_pud(cpa, addr, pgd_entry, pgprot);
 	if (ret &lt; 0) {
<span class="p_del">-		unmap_pgd_range(cpa-&gt;pgd, addr,</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Leave the PUD page in place in case some other CPU or thread</span>
<span class="p_add">+		 * already found it, but remove any useless entries we just</span>
<span class="p_add">+		 * added to it.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		unmap_pud_range(pgd_entry, addr,</span>
 				addr + (cpa-&gt;numpages &lt;&lt; PAGE_SHIFT));
 		return ret;
 	}
<span class="p_chunk">@@ -1185,7 +1168,7 @@</span> <span class="p_context"> static int __change_page_attr(struct cpa_data *cpa, int primary)</span>
 		return __cpa_process_fault(cpa, address, primary);
 
 	old_pte = *kpte;
<span class="p_del">-	if (!pte_val(old_pte))</span>
<span class="p_add">+	if (pte_none(old_pte))</span>
 		return __cpa_process_fault(cpa, address, primary);
 
 	if (level == PG_LEVEL_4K) {
<span class="p_chunk">@@ -1991,12 +1974,6 @@</span> <span class="p_context"> int kernel_map_pages_in_pgd(pgd_t *pgd, u64 pfn, unsigned long address,</span>
 	return retval;
 }
 
<span class="p_del">-void kernel_unmap_pages_in_pgd(pgd_t *root, unsigned long address,</span>
<span class="p_del">-			       unsigned numpages)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unmap_pgd_range(root, address, address + (numpages &lt;&lt; PAGE_SHIFT));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 /*
  * The testcases use internal knowledge of the implementation that shouldn&#39;t
  * be exposed to the rest of the kernel. Include these directly here.
<span class="p_header">diff --git a/arch/x86/mm/pat.c b/arch/x86/mm/pat.c</span>
<span class="p_header">index fb0604f11eec..db00e3e2f3dc 100644</span>
<span class="p_header">--- a/arch/x86/mm/pat.c</span>
<span class="p_header">+++ b/arch/x86/mm/pat.c</span>
<span class="p_chunk">@@ -755,11 +755,8 @@</span> <span class="p_context"> static inline int range_is_allowed(unsigned long pfn, unsigned long size)</span>
 		return 1;
 
 	while (cursor &lt; to) {
<span class="p_del">-		if (!devmem_is_allowed(pfn)) {</span>
<span class="p_del">-			pr_info(&quot;x86/PAT: Program %s tried to access /dev/mem between [mem %#010Lx-%#010Lx], PAT prevents it\n&quot;,</span>
<span class="p_del">-				current-&gt;comm, from, to - 1);</span>
<span class="p_add">+		if (!devmem_is_allowed(pfn))</span>
 			return 0;
<span class="p_del">-		}</span>
 		cursor += PAGE_SIZE;
 		pfn++;
 	}
<span class="p_header">diff --git a/arch/x86/mm/pgtable_32.c b/arch/x86/mm/pgtable_32.c</span>
<span class="p_header">index 75cc0978d45d..e67ae0e6c59d 100644</span>
<span class="p_header">--- a/arch/x86/mm/pgtable_32.c</span>
<span class="p_header">+++ b/arch/x86/mm/pgtable_32.c</span>
<span class="p_chunk">@@ -47,7 +47,7 @@</span> <span class="p_context"> void set_pte_vaddr(unsigned long vaddr, pte_t pteval)</span>
 		return;
 	}
 	pte = pte_offset_kernel(pmd, vaddr);
<span class="p_del">-	if (pte_val(pteval))</span>
<span class="p_add">+	if (!pte_none(pteval))</span>
 		set_pte_at(&amp;init_mm, vaddr, pte, pteval);
 	else
 		pte_clear(&amp;init_mm, vaddr, pte);
<span class="p_header">diff --git a/arch/x86/platform/efi/efi.c b/arch/x86/platform/efi/efi.c</span>
<span class="p_header">index f93545e7dc54..62986e5fbdba 100644</span>
<span class="p_header">--- a/arch/x86/platform/efi/efi.c</span>
<span class="p_header">+++ b/arch/x86/platform/efi/efi.c</span>
<span class="p_chunk">@@ -978,8 +978,6 @@</span> <span class="p_context"> static void __init __efi_enter_virtual_mode(void)</span>
 	 * EFI mixed mode we need all of memory to be accessible when
 	 * we pass parameters to the EFI runtime services in the
 	 * thunking code.
<span class="p_del">-	 *</span>
<span class="p_del">-	 * efi_cleanup_page_tables(__pa(new_memmap), 1 &lt;&lt; pg_shift);</span>
 	 */
 	free_pages((unsigned long)new_memmap, pg_shift);
 
<span class="p_header">diff --git a/arch/x86/platform/efi/efi_32.c b/arch/x86/platform/efi/efi_32.c</span>
<span class="p_header">index 338402b91d2e..cef39b097649 100644</span>
<span class="p_header">--- a/arch/x86/platform/efi/efi_32.c</span>
<span class="p_header">+++ b/arch/x86/platform/efi/efi_32.c</span>
<span class="p_chunk">@@ -49,9 +49,6 @@</span> <span class="p_context"> int __init efi_setup_page_tables(unsigned long pa_memmap, unsigned num_pages)</span>
 {
 	return 0;
 }
<span class="p_del">-void __init efi_cleanup_page_tables(unsigned long pa_memmap, unsigned num_pages)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
 
 void __init efi_map_region(efi_memory_desc_t *md)
 {
<span class="p_header">diff --git a/arch/x86/platform/efi/efi_64.c b/arch/x86/platform/efi/efi_64.c</span>
<span class="p_header">index b226b3f497f1..d288dcea1ffe 100644</span>
<span class="p_header">--- a/arch/x86/platform/efi/efi_64.c</span>
<span class="p_header">+++ b/arch/x86/platform/efi/efi_64.c</span>
<span class="p_chunk">@@ -285,11 +285,6 @@</span> <span class="p_context"> int __init efi_setup_page_tables(unsigned long pa_memmap, unsigned num_pages)</span>
 	return 0;
 }
 
<span class="p_del">-void __init efi_cleanup_page_tables(unsigned long pa_memmap, unsigned num_pages)</span>
<span class="p_del">-{</span>
<span class="p_del">-	kernel_unmap_pages_in_pgd(efi_pgd, pa_memmap, num_pages);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void __init __map_region(efi_memory_desc_t *md, u64 va)
 {
 	unsigned long flags = _PAGE_RW;
<span class="p_header">diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c</span>
<span class="p_header">index 760789ae8562..0f87db2cc6a8 100644</span>
<span class="p_header">--- a/arch/x86/xen/enlighten.c</span>
<span class="p_header">+++ b/arch/x86/xen/enlighten.c</span>
<span class="p_chunk">@@ -521,9 +521,7 @@</span> <span class="p_context"> static void set_aliased_prot(void *v, pgprot_t prot)</span>
 
 	preempt_disable();
 
<span class="p_del">-	pagefault_disable();	/* Avoid warnings due to being atomic. */</span>
<span class="p_del">-	__get_user(dummy, (unsigned char __user __force *)v);</span>
<span class="p_del">-	pagefault_enable();</span>
<span class="p_add">+	probe_kernel_read(&amp;dummy, v, 1);</span>
 
 	if (HYPERVISOR_update_va_mapping((unsigned long)v, pte, 0))
 		BUG();
<span class="p_header">diff --git a/drivers/char/mem.c b/drivers/char/mem.c</span>
<span class="p_header">index 71025c2f6bbb..d633974e7f8b 100644</span>
<span class="p_header">--- a/drivers/char/mem.c</span>
<span class="p_header">+++ b/drivers/char/mem.c</span>
<span class="p_chunk">@@ -66,12 +66,8 @@</span> <span class="p_context"> static inline int range_is_allowed(unsigned long pfn, unsigned long size)</span>
 	u64 cursor = from;
 
 	while (cursor &lt; to) {
<span class="p_del">-		if (!devmem_is_allowed(pfn)) {</span>
<span class="p_del">-			printk(KERN_INFO</span>
<span class="p_del">-		&quot;Program %s tried to access /dev/mem between %Lx-&gt;%Lx.\n&quot;,</span>
<span class="p_del">-				current-&gt;comm, from, to);</span>
<span class="p_add">+		if (!devmem_is_allowed(pfn))</span>
 			return 0;
<span class="p_del">-		}</span>
 		cursor += PAGE_SIZE;
 		pfn++;
 	}
<span class="p_header">diff --git a/drivers/pnp/isapnp/proc.c b/drivers/pnp/isapnp/proc.c</span>
<span class="p_header">index 5edee645d890..262285e48a09 100644</span>
<span class="p_header">--- a/drivers/pnp/isapnp/proc.c</span>
<span class="p_header">+++ b/drivers/pnp/isapnp/proc.c</span>
<span class="p_chunk">@@ -21,7 +21,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/isapnp.h&gt;
 #include &lt;linux/proc_fs.h&gt;
 #include &lt;linux/init.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 extern struct pnp_protocol isapnp_protocol;
 
<span class="p_header">diff --git a/fs/read_write.c b/fs/read_write.c</span>
<span class="p_header">index 933b53a375b4..66215a7b17cf 100644</span>
<span class="p_header">--- a/fs/read_write.c</span>
<span class="p_header">+++ b/fs/read_write.c</span>
<span class="p_chunk">@@ -1168,6 +1168,15 @@</span> <span class="p_context"> COMPAT_SYSCALL_DEFINE5(preadv, compat_ulong_t, fd,</span>
 	return do_compat_preadv64(fd, vec, vlen, pos, 0);
 }
 
<span class="p_add">+#ifdef __ARCH_WANT_COMPAT_SYS_PREADV64V2</span>
<span class="p_add">+COMPAT_SYSCALL_DEFINE5(preadv64v2, unsigned long, fd,</span>
<span class="p_add">+		const struct compat_iovec __user *,vec,</span>
<span class="p_add">+		unsigned long, vlen, loff_t, pos, int, flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return do_compat_preadv64(fd, vec, vlen, pos, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 COMPAT_SYSCALL_DEFINE6(preadv2, compat_ulong_t, fd,
 		const struct compat_iovec __user *,vec,
 		compat_ulong_t, vlen, u32, pos_low, u32, pos_high,
<span class="p_chunk">@@ -1265,6 +1274,15 @@</span> <span class="p_context"> COMPAT_SYSCALL_DEFINE5(pwritev, compat_ulong_t, fd,</span>
 	return do_compat_pwritev64(fd, vec, vlen, pos, 0);
 }
 
<span class="p_add">+#ifdef __ARCH_WANT_COMPAT_SYS_PWRITEV64V2</span>
<span class="p_add">+COMPAT_SYSCALL_DEFINE5(pwritev64v2, unsigned long, fd,</span>
<span class="p_add">+		const struct compat_iovec __user *,vec,</span>
<span class="p_add">+		unsigned long, vlen, loff_t, pos, int, flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return do_compat_pwritev64(fd, vec, vlen, pos, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 COMPAT_SYSCALL_DEFINE6(pwritev2, compat_ulong_t, fd,
 		const struct compat_iovec __user *,vec,
 		compat_ulong_t, vlen, u32, pos_low, u32, pos_high, int, flags)
<span class="p_header">diff --git a/include/linux/context_tracking.h b/include/linux/context_tracking.h</span>
<span class="p_header">index d259274238db..d9aef2a0ec8e 100644</span>
<span class="p_header">--- a/include/linux/context_tracking.h</span>
<span class="p_header">+++ b/include/linux/context_tracking.h</span>
<span class="p_chunk">@@ -31,6 +31,19 @@</span> <span class="p_context"> static inline void user_exit(void)</span>
 		context_tracking_exit(CONTEXT_USER);
 }
 
<span class="p_add">+/* Called with interrupts disabled.  */</span>
<span class="p_add">+static inline void user_enter_irqoff(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (context_tracking_is_enabled())</span>
<span class="p_add">+		__context_tracking_enter(CONTEXT_USER);</span>
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void user_exit_irqoff(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (context_tracking_is_enabled())</span>
<span class="p_add">+		__context_tracking_exit(CONTEXT_USER);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline enum ctx_state exception_enter(void)
 {
 	enum ctx_state prev_ctx;
<span class="p_chunk">@@ -69,6 +82,8 @@</span> <span class="p_context"> static inline enum ctx_state ct_state(void)</span>
 #else
 static inline void user_enter(void) { }
 static inline void user_exit(void) { }
<span class="p_add">+static inline void user_enter_irqoff(void) { }</span>
<span class="p_add">+static inline void user_exit_irqoff(void) { }</span>
 static inline enum ctx_state exception_enter(void) { return 0; }
 static inline void exception_exit(enum ctx_state prev_ctx) { }
 static inline enum ctx_state ct_state(void) { return CONTEXT_DISABLED; }
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index ca3e517980a0..917f2b6a0cde 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -594,6 +594,9 @@</span> <span class="p_context"> struct vm_special_mapping {</span>
 	int (*fault)(const struct vm_special_mapping *sm,
 		     struct vm_area_struct *vma,
 		     struct vm_fault *vmf);
<span class="p_add">+</span>
<span class="p_add">+	int (*mremap)(const struct vm_special_mapping *sm,</span>
<span class="p_add">+		     struct vm_area_struct *new_vma);</span>
 };
 
 enum tlb_flush_reason {
<span class="p_header">diff --git a/include/linux/random.h b/include/linux/random.h</span>
<span class="p_header">index e47e533742b5..3d6e9815cd85 100644</span>
<span class="p_header">--- a/include/linux/random.h</span>
<span class="p_header">+++ b/include/linux/random.h</span>
<span class="p_chunk">@@ -95,27 +95,27 @@</span> <span class="p_context"> static inline void prandom_seed_state(struct rnd_state *state, u64 seed)</span>
 #ifdef CONFIG_ARCH_RANDOM
 # include &lt;asm/archrandom.h&gt;
 #else
<span class="p_del">-static inline int arch_get_random_long(unsigned long *v)</span>
<span class="p_add">+static inline bool arch_get_random_long(unsigned long *v)</span>
 {
 	return 0;
 }
<span class="p_del">-static inline int arch_get_random_int(unsigned int *v)</span>
<span class="p_add">+static inline bool arch_get_random_int(unsigned int *v)</span>
 {
 	return 0;
 }
<span class="p_del">-static inline int arch_has_random(void)</span>
<span class="p_add">+static inline bool arch_has_random(void)</span>
 {
 	return 0;
 }
<span class="p_del">-static inline int arch_get_random_seed_long(unsigned long *v)</span>
<span class="p_add">+static inline bool arch_get_random_seed_long(unsigned long *v)</span>
 {
 	return 0;
 }
<span class="p_del">-static inline int arch_get_random_seed_int(unsigned int *v)</span>
<span class="p_add">+static inline bool arch_get_random_seed_int(unsigned int *v)</span>
 {
 	return 0;
 }
<span class="p_del">-static inline int arch_has_random_seed(void)</span>
<span class="p_add">+static inline bool arch_has_random_seed(void)</span>
 {
 	return 0;
 }
<span class="p_header">diff --git a/lib/Makefile b/lib/Makefile</span>
<span class="p_header">index ff6a7a6c6395..07d06a8b9788 100644</span>
<span class="p_header">--- a/lib/Makefile</span>
<span class="p_header">+++ b/lib/Makefile</span>
<span class="p_chunk">@@ -15,9 +15,6 @@</span> <span class="p_context"> KCOV_INSTRUMENT_rbtree.o := n</span>
 KCOV_INSTRUMENT_list_debug.o := n
 KCOV_INSTRUMENT_debugobjects.o := n
 KCOV_INSTRUMENT_dynamic_debug.o := n
<span class="p_del">-# Kernel does not boot if we instrument this file as it uses custom calling</span>
<span class="p_del">-# convention (see CONFIG_ARCH_HWEIGHT_CFLAGS).</span>
<span class="p_del">-KCOV_INSTRUMENT_hweight.o := n</span>
 
 lib-y := ctype.o string.o vsprintf.o cmdline.o \
 	 rbtree.o radix-tree.o dump_stack.o timerqueue.o\
<span class="p_chunk">@@ -74,8 +71,6 @@</span> <span class="p_context"> obj-$(CONFIG_HAS_IOMEM) += iomap_copy.o devres.o</span>
 obj-$(CONFIG_CHECK_SIGNATURE) += check_signature.o
 obj-$(CONFIG_DEBUG_LOCKING_API_SELFTESTS) += locking-selftest.o
 
<span class="p_del">-GCOV_PROFILE_hweight.o := n</span>
<span class="p_del">-CFLAGS_hweight.o = $(subst $(quote),,$(CONFIG_ARCH_HWEIGHT_CFLAGS))</span>
 obj-$(CONFIG_GENERIC_HWEIGHT) += hweight.o
 
 obj-$(CONFIG_BTREE) += btree.o
<span class="p_header">diff --git a/lib/bitmap.c b/lib/bitmap.c</span>
<span class="p_header">index c66da508cbf7..eca88087fa8a 100644</span>
<span class="p_header">--- a/lib/bitmap.c</span>
<span class="p_header">+++ b/lib/bitmap.c</span>
<span class="p_chunk">@@ -14,9 +14,9 @@</span> <span class="p_context"></span>
 #include &lt;linux/bug.h&gt;
 #include &lt;linux/kernel.h&gt;
 #include &lt;linux/string.h&gt;
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
 
 #include &lt;asm/page.h&gt;
<span class="p_del">-#include &lt;asm/uaccess.h&gt;</span>
 
 /*
  * bitmaps provide an array of bits, implemented using an an
<span class="p_header">diff --git a/lib/hweight.c b/lib/hweight.c</span>
<span class="p_header">index 9a5c1f221558..43273a7d83cf 100644</span>
<span class="p_header">--- a/lib/hweight.c</span>
<span class="p_header">+++ b/lib/hweight.c</span>
<span class="p_chunk">@@ -9,6 +9,7 @@</span> <span class="p_context"></span>
  * The Hamming Weight of a number is the total number of bits set in it.
  */
 
<span class="p_add">+#ifndef __HAVE_ARCH_SW_HWEIGHT</span>
 unsigned int __sw_hweight32(unsigned int w)
 {
 #ifdef CONFIG_ARCH_HAS_FAST_MULTIPLIER
<span class="p_chunk">@@ -25,6 +26,7 @@</span> <span class="p_context"> unsigned int __sw_hweight32(unsigned int w)</span>
 #endif
 }
 EXPORT_SYMBOL(__sw_hweight32);
<span class="p_add">+#endif</span>
 
 unsigned int __sw_hweight16(unsigned int w)
 {
<span class="p_chunk">@@ -43,6 +45,7 @@</span> <span class="p_context"> unsigned int __sw_hweight8(unsigned int w)</span>
 }
 EXPORT_SYMBOL(__sw_hweight8);
 
<span class="p_add">+#ifndef __HAVE_ARCH_SW_HWEIGHT</span>
 unsigned long __sw_hweight64(__u64 w)
 {
 #if BITS_PER_LONG == 32
<span class="p_chunk">@@ -65,3 +68,4 @@</span> <span class="p_context"> unsigned long __sw_hweight64(__u64 w)</span>
 #endif
 }
 EXPORT_SYMBOL(__sw_hweight64);
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index de2c1769cc68..234edffec1d0 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -2943,9 +2943,19 @@</span> <span class="p_context"> static const char *special_mapping_name(struct vm_area_struct *vma)</span>
 	return ((struct vm_special_mapping *)vma-&gt;vm_private_data)-&gt;name;
 }
 
<span class="p_add">+static int special_mapping_mremap(struct vm_area_struct *new_vma)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_special_mapping *sm = new_vma-&gt;vm_private_data;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (sm-&gt;mremap)</span>
<span class="p_add">+		return sm-&gt;mremap(sm, new_vma);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static const struct vm_operations_struct special_mapping_vmops = {
 	.close = special_mapping_close,
 	.fault = special_mapping_fault,
<span class="p_add">+	.mremap = special_mapping_mremap,</span>
 	.name = special_mapping_name,
 };
 
<span class="p_header">diff --git a/tools/testing/selftests/x86/Makefile b/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">index c73425de3cfe..4f747ee07f10 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/Makefile</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/Makefile</span>
<span class="p_chunk">@@ -4,8 +4,8 @@</span> <span class="p_context"> include ../lib.mk</span>
 
 .PHONY: all all_32 all_64 warn_32bit_failure clean
 
<span class="p_del">-TARGETS_C_BOTHBITS := single_step_syscall sysret_ss_attrs syscall_nt ptrace_syscall \</span>
<span class="p_del">-			check_initial_reg_state sigreturn ldt_gdt iopl</span>
<span class="p_add">+TARGETS_C_BOTHBITS := single_step_syscall sysret_ss_attrs syscall_nt ptrace_syscall test_mremap_vdso \</span>
<span class="p_add">+			check_initial_reg_state sigreturn ldt_gdt iopl mpx-mini-test</span>
 TARGETS_C_32BIT_ONLY := entry_from_vm86 syscall_arg_fault test_syscall_vdso unwind_vdso \
 			test_FCMOV test_FCOMI test_FISTTP \
 			vdso_restorer
<span class="p_header">diff --git a/tools/testing/selftests/x86/mpx-debug.h b/tools/testing/selftests/x86/mpx-debug.h</span>
new file mode 100644
<span class="p_header">index 000000000000..9230981f2e12</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/mpx-debug.h</span>
<span class="p_chunk">@@ -0,0 +1,14 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _MPX_DEBUG_H</span>
<span class="p_add">+#define _MPX_DEBUG_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef DEBUG_LEVEL</span>
<span class="p_add">+#define DEBUG_LEVEL 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#define dprintf_level(level, args...) do { if(level &lt;= DEBUG_LEVEL) printf(args); } while(0)</span>
<span class="p_add">+#define dprintf1(args...) dprintf_level(1, args)</span>
<span class="p_add">+#define dprintf2(args...) dprintf_level(2, args)</span>
<span class="p_add">+#define dprintf3(args...) dprintf_level(3, args)</span>
<span class="p_add">+#define dprintf4(args...) dprintf_level(4, args)</span>
<span class="p_add">+#define dprintf5(args...) dprintf_level(5, args)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _MPX_DEBUG_H */</span>
<span class="p_header">diff --git a/tools/testing/selftests/x86/mpx-dig.c b/tools/testing/selftests/x86/mpx-dig.c</span>
new file mode 100644
<span class="p_header">index 000000000000..ce85356d7e2e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/mpx-dig.c</span>
<span class="p_chunk">@@ -0,0 +1,498 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Written by Dave Hansen &lt;dave.hansen@intel.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;stdlib.h&gt;</span>
<span class="p_add">+#include &lt;sys/types.h&gt;</span>
<span class="p_add">+#include &lt;unistd.h&gt;</span>
<span class="p_add">+#include &lt;stdio.h&gt;</span>
<span class="p_add">+#include &lt;errno.h&gt;</span>
<span class="p_add">+#include &lt;sys/types.h&gt;</span>
<span class="p_add">+#include &lt;sys/stat.h&gt;</span>
<span class="p_add">+#include &lt;unistd.h&gt;</span>
<span class="p_add">+#include &lt;sys/mman.h&gt;</span>
<span class="p_add">+#include &lt;string.h&gt;</span>
<span class="p_add">+#include &lt;fcntl.h&gt;</span>
<span class="p_add">+#include &quot;mpx-debug.h&quot;</span>
<span class="p_add">+#include &quot;mpx-mm.h&quot;</span>
<span class="p_add">+#include &quot;mpx-hw.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned long bounds_dir_global;</span>
<span class="p_add">+</span>
<span class="p_add">+#define mpx_dig_abort()	__mpx_dig_abort(__FILE__, __func__, __LINE__)</span>
<span class="p_add">+static void inline __mpx_dig_abort(const char *file, const char *func, int line)</span>
<span class="p_add">+{</span>
<span class="p_add">+	fprintf(stderr, &quot;MPX dig abort @ %s::%d in %s()\n&quot;, file, line, func);</span>
<span class="p_add">+	printf(&quot;MPX dig abort @ %s::%d in %s()\n&quot;, file, line, func);</span>
<span class="p_add">+	abort();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * run like this (BDIR finds the probably bounds directory):</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	BDIR=&quot;$(cat /proc/$pid/smaps | grep -B1 2097152 \</span>
<span class="p_add">+ *		| head -1 | awk -F- &#39;{print $1}&#39;)&quot;;</span>
<span class="p_add">+ *	./mpx-dig $pid 0x$BDIR</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * NOTE:</span>
<span class="p_add">+ *	assumes that the only 2097152-kb VMA is the bounds dir</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+long nr_incore(void *ptr, unsigned long size_bytes)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	long ret = 0;</span>
<span class="p_add">+	long vec_len = size_bytes / PAGE_SIZE;</span>
<span class="p_add">+	unsigned char *vec = malloc(vec_len);</span>
<span class="p_add">+	int incore_ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!vec)</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+</span>
<span class="p_add">+	incore_ret = mincore(ptr, size_bytes, vec);</span>
<span class="p_add">+	if (incore_ret) {</span>
<span class="p_add">+		printf(&quot;mincore ret: %d\n&quot;, incore_ret);</span>
<span class="p_add">+		perror(&quot;mincore&quot;);</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+	}</span>
<span class="p_add">+	for (i = 0; i &lt; vec_len; i++)</span>
<span class="p_add">+		ret += vec[i];</span>
<span class="p_add">+	free(vec);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int open_proc(int pid, char *file)</span>
<span class="p_add">+{</span>
<span class="p_add">+	static char buf[100];</span>
<span class="p_add">+	int fd;</span>
<span class="p_add">+</span>
<span class="p_add">+	snprintf(&amp;buf[0], sizeof(buf), &quot;/proc/%d/%s&quot;, pid, file);</span>
<span class="p_add">+	fd = open(&amp;buf[0], O_RDONLY);</span>
<span class="p_add">+	if (fd &lt; 0)</span>
<span class="p_add">+		perror(buf);</span>
<span class="p_add">+</span>
<span class="p_add">+	return fd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct vaddr_range {</span>
<span class="p_add">+	unsigned long start;</span>
<span class="p_add">+	unsigned long end;</span>
<span class="p_add">+};</span>
<span class="p_add">+struct vaddr_range *ranges;</span>
<span class="p_add">+int nr_ranges_allocated;</span>
<span class="p_add">+int nr_ranges_populated;</span>
<span class="p_add">+int last_range = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+int __pid_load_vaddrs(int pid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+	int proc_maps_fd = open_proc(pid, &quot;maps&quot;);</span>
<span class="p_add">+	char linebuf[10000];</span>
<span class="p_add">+	unsigned long start;</span>
<span class="p_add">+	unsigned long end;</span>
<span class="p_add">+	char rest[1000];</span>
<span class="p_add">+	FILE *f = fdopen(proc_maps_fd, &quot;r&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!f)</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+	nr_ranges_populated = 0;</span>
<span class="p_add">+	while (!feof(f)) {</span>
<span class="p_add">+		char *readret = fgets(linebuf, sizeof(linebuf), f);</span>
<span class="p_add">+		int parsed;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (readret == NULL) {</span>
<span class="p_add">+			if (feof(f))</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			mpx_dig_abort();</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		parsed = sscanf(linebuf, &quot;%lx-%lx%s&quot;, &amp;start, &amp;end, rest);</span>
<span class="p_add">+		if (parsed != 3)</span>
<span class="p_add">+			mpx_dig_abort();</span>
<span class="p_add">+</span>
<span class="p_add">+		dprintf4(&quot;result[%d]: %lx-%lx&lt;-&gt;%s\n&quot;, parsed, start, end, rest);</span>
<span class="p_add">+		if (nr_ranges_populated &gt;= nr_ranges_allocated) {</span>
<span class="p_add">+			ret = -E2BIG;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		ranges[nr_ranges_populated].start = start;</span>
<span class="p_add">+		ranges[nr_ranges_populated].end = end;</span>
<span class="p_add">+		nr_ranges_populated++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	last_range = -1;</span>
<span class="p_add">+	fclose(f);</span>
<span class="p_add">+	close(proc_maps_fd);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int pid_load_vaddrs(int pid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf2(&quot;%s(%d)\n&quot;, __func__, pid);</span>
<span class="p_add">+	if (!ranges) {</span>
<span class="p_add">+		nr_ranges_allocated = 4;</span>
<span class="p_add">+		ranges = malloc(nr_ranges_allocated * sizeof(ranges[0]));</span>
<span class="p_add">+		dprintf2(&quot;%s(%d) allocated %d ranges @ %p\n&quot;, __func__, pid,</span>
<span class="p_add">+			 nr_ranges_allocated, ranges);</span>
<span class="p_add">+		assert(ranges != NULL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		ret = __pid_load_vaddrs(pid);</span>
<span class="p_add">+		if (!ret)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		if (ret == -E2BIG) {</span>
<span class="p_add">+			dprintf2(&quot;%s(%d) need to realloc\n&quot;, __func__, pid);</span>
<span class="p_add">+			nr_ranges_allocated *= 2;</span>
<span class="p_add">+			ranges = realloc(ranges,</span>
<span class="p_add">+					nr_ranges_allocated * sizeof(ranges[0]));</span>
<span class="p_add">+			dprintf2(&quot;%s(%d) allocated %d ranges @ %p\n&quot;, __func__,</span>
<span class="p_add">+					pid, nr_ranges_allocated, ranges);</span>
<span class="p_add">+			assert(ranges != NULL);</span>
<span class="p_add">+			dprintf1(&quot;reallocating to hold %d ranges\n&quot;, nr_ranges_allocated);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} while (1);</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf2(&quot;%s(%d) done\n&quot;, __func__, pid);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int vaddr_in_range(unsigned long vaddr, struct vaddr_range *r)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (vaddr &lt; r-&gt;start)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	if (vaddr &gt;= r-&gt;end)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	return 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int vaddr_mapped_by_range(unsigned long vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (last_range &gt; 0 &amp;&amp; vaddr_in_range(vaddr, &amp;ranges[last_range]))</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nr_ranges_populated; i++) {</span>
<span class="p_add">+		struct vaddr_range *r = &amp;ranges[i];</span>
<span class="p_add">+</span>
<span class="p_add">+		if (vaddr_in_range(vaddr, r))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		last_range = i;</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+const int bt_entry_size_bytes = sizeof(unsigned long) * 4;</span>
<span class="p_add">+</span>
<span class="p_add">+void *read_bounds_table_into_buf(unsigned long table_vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef MPX_DIG_STANDALONE</span>
<span class="p_add">+	static char bt_buf[MPX_BOUNDS_TABLE_SIZE_BYTES];</span>
<span class="p_add">+	off_t seek_ret = lseek(fd, table_vaddr, SEEK_SET);</span>
<span class="p_add">+	if (seek_ret != table_vaddr)</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+</span>
<span class="p_add">+	int read_ret = read(fd, &amp;bt_buf, sizeof(bt_buf));</span>
<span class="p_add">+	if (read_ret != sizeof(bt_buf))</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+	return &amp;bt_buf;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return (void *)table_vaddr;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int dump_table(unsigned long table_vaddr, unsigned long base_controlled_vaddr,</span>
<span class="p_add">+		unsigned long bde_vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long offset_inside_bt;</span>
<span class="p_add">+	int nr_entries = 0;</span>
<span class="p_add">+	int do_abort = 0;</span>
<span class="p_add">+	char *bt_buf;</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf3(&quot;%s() base_controlled_vaddr: 0x%012lx bde_vaddr: 0x%012lx\n&quot;,</span>
<span class="p_add">+			__func__, base_controlled_vaddr, bde_vaddr);</span>
<span class="p_add">+</span>
<span class="p_add">+	bt_buf = read_bounds_table_into_buf(table_vaddr);</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf4(&quot;%s() read done\n&quot;, __func__);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (offset_inside_bt = 0;</span>
<span class="p_add">+	     offset_inside_bt &lt; MPX_BOUNDS_TABLE_SIZE_BYTES;</span>
<span class="p_add">+	     offset_inside_bt += bt_entry_size_bytes) {</span>
<span class="p_add">+		unsigned long bt_entry_index;</span>
<span class="p_add">+		unsigned long bt_entry_controls;</span>
<span class="p_add">+		unsigned long this_bt_entry_for_vaddr;</span>
<span class="p_add">+		unsigned long *bt_entry_buf;</span>
<span class="p_add">+		int i;</span>
<span class="p_add">+</span>
<span class="p_add">+		dprintf4(&quot;%s() offset_inside_bt: 0x%lx of 0x%llx\n&quot;, __func__,</span>
<span class="p_add">+			offset_inside_bt, MPX_BOUNDS_TABLE_SIZE_BYTES);</span>
<span class="p_add">+		bt_entry_buf = (void *)&amp;bt_buf[offset_inside_bt];</span>
<span class="p_add">+		if (!bt_buf) {</span>
<span class="p_add">+			printf(&quot;null bt_buf\n&quot;);</span>
<span class="p_add">+			mpx_dig_abort();</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (!bt_entry_buf) {</span>
<span class="p_add">+			printf(&quot;null bt_entry_buf\n&quot;);</span>
<span class="p_add">+			mpx_dig_abort();</span>
<span class="p_add">+		}</span>
<span class="p_add">+		dprintf4(&quot;%s() reading *bt_entry_buf @ %p\n&quot;, __func__,</span>
<span class="p_add">+				bt_entry_buf);</span>
<span class="p_add">+		if (!bt_entry_buf[0] &amp;&amp;</span>
<span class="p_add">+		    !bt_entry_buf[1] &amp;&amp;</span>
<span class="p_add">+		    !bt_entry_buf[2] &amp;&amp;</span>
<span class="p_add">+		    !bt_entry_buf[3])</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		nr_entries++;</span>
<span class="p_add">+</span>
<span class="p_add">+		bt_entry_index = offset_inside_bt/bt_entry_size_bytes;</span>
<span class="p_add">+		bt_entry_controls = sizeof(void *);</span>
<span class="p_add">+		this_bt_entry_for_vaddr =</span>
<span class="p_add">+			base_controlled_vaddr + bt_entry_index*bt_entry_controls;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We sign extend vaddr bits 48-&gt;63 which effectively</span>
<span class="p_add">+		 * creates a hole in the virtual address space.</span>
<span class="p_add">+		 * This calculation corrects for the hole.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (this_bt_entry_for_vaddr &gt; 0x00007fffffffffffUL)</span>
<span class="p_add">+			this_bt_entry_for_vaddr |= 0xffff800000000000;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!vaddr_mapped_by_range(this_bt_entry_for_vaddr)) {</span>
<span class="p_add">+			printf(&quot;bt_entry_buf: %p\n&quot;, bt_entry_buf);</span>
<span class="p_add">+			printf(&quot;there is a bte for %lx but no mapping\n&quot;,</span>
<span class="p_add">+					this_bt_entry_for_vaddr);</span>
<span class="p_add">+			printf(&quot;	  bde   vaddr: %016lx\n&quot;, bde_vaddr);</span>
<span class="p_add">+			printf(&quot;base_controlled_vaddr: %016lx\n&quot;, base_controlled_vaddr);</span>
<span class="p_add">+			printf(&quot;	  table_vaddr: %016lx\n&quot;, table_vaddr);</span>
<span class="p_add">+			printf(&quot;	  entry vaddr: %016lx @ offset %lx\n&quot;,</span>
<span class="p_add">+				table_vaddr + offset_inside_bt, offset_inside_bt);</span>
<span class="p_add">+			do_abort = 1;</span>
<span class="p_add">+			mpx_dig_abort();</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (DEBUG_LEVEL &lt; 4)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		printf(&quot;table entry[%lx]: &quot;, offset_inside_bt);</span>
<span class="p_add">+		for (i = 0; i &lt; bt_entry_size_bytes; i += sizeof(unsigned long))</span>
<span class="p_add">+			printf(&quot;0x%016lx &quot;, bt_entry_buf[i]);</span>
<span class="p_add">+		printf(&quot;\n&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (do_abort)</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+	dprintf4(&quot;%s() done\n&quot;,  __func__);</span>
<span class="p_add">+	return nr_entries;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int search_bd_buf(char *buf, int len_bytes, unsigned long bd_offset_bytes,</span>
<span class="p_add">+		int *nr_populated_bdes)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long i;</span>
<span class="p_add">+	int total_entries = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf3(&quot;%s(%p, %x, %lx, ...) buf end: %p\n&quot;, __func__, buf,</span>
<span class="p_add">+			len_bytes, bd_offset_bytes, buf + len_bytes);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; len_bytes; i += sizeof(unsigned long)) {</span>
<span class="p_add">+		unsigned long bd_index = (bd_offset_bytes + i) / sizeof(unsigned long);</span>
<span class="p_add">+		unsigned long *bounds_dir_entry_ptr = (unsigned long *)&amp;buf[i];</span>
<span class="p_add">+		unsigned long bounds_dir_entry;</span>
<span class="p_add">+		unsigned long bd_for_vaddr;</span>
<span class="p_add">+		unsigned long bt_start;</span>
<span class="p_add">+		unsigned long bt_tail;</span>
<span class="p_add">+		int nr_entries;</span>
<span class="p_add">+</span>
<span class="p_add">+		dprintf4(&quot;%s() loop i: %ld bounds_dir_entry_ptr: %p\n&quot;, __func__, i,</span>
<span class="p_add">+				bounds_dir_entry_ptr);</span>
<span class="p_add">+</span>
<span class="p_add">+		bounds_dir_entry = *bounds_dir_entry_ptr;</span>
<span class="p_add">+		if (!bounds_dir_entry) {</span>
<span class="p_add">+			dprintf4(&quot;no bounds dir at index 0x%lx / 0x%lx &quot;</span>
<span class="p_add">+				 &quot;start at offset:%lx %lx\n&quot;, bd_index, bd_index,</span>
<span class="p_add">+					bd_offset_bytes, i);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		dprintf3(&quot;found bounds_dir_entry: 0x%lx @ &quot;</span>
<span class="p_add">+			 &quot;index 0x%lx buf ptr: %p\n&quot;, bounds_dir_entry, i,</span>
<span class="p_add">+					&amp;buf[i]);</span>
<span class="p_add">+		/* mask off the enable bit: */</span>
<span class="p_add">+		bounds_dir_entry &amp;= ~0x1;</span>
<span class="p_add">+		(*nr_populated_bdes)++;</span>
<span class="p_add">+		dprintf4(&quot;nr_populated_bdes: %p\n&quot;, nr_populated_bdes);</span>
<span class="p_add">+		dprintf4(&quot;*nr_populated_bdes: %d\n&quot;, *nr_populated_bdes);</span>
<span class="p_add">+</span>
<span class="p_add">+		bt_start = bounds_dir_entry;</span>
<span class="p_add">+		bt_tail = bounds_dir_entry + MPX_BOUNDS_TABLE_SIZE_BYTES - 1;</span>
<span class="p_add">+		if (!vaddr_mapped_by_range(bt_start)) {</span>
<span class="p_add">+			printf(&quot;bounds directory 0x%lx points to nowhere\n&quot;,</span>
<span class="p_add">+					bounds_dir_entry);</span>
<span class="p_add">+			mpx_dig_abort();</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (!vaddr_mapped_by_range(bt_tail)) {</span>
<span class="p_add">+			printf(&quot;bounds directory end 0x%lx points to nowhere\n&quot;,</span>
<span class="p_add">+					bt_tail);</span>
<span class="p_add">+			mpx_dig_abort();</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Each bounds directory entry controls 1MB of virtual address</span>
<span class="p_add">+		 * space.  This variable is the virtual address in the process</span>
<span class="p_add">+		 * of the beginning of the area controlled by this bounds_dir.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		bd_for_vaddr = bd_index * (1UL&lt;&lt;20);</span>
<span class="p_add">+</span>
<span class="p_add">+		nr_entries = dump_table(bounds_dir_entry, bd_for_vaddr,</span>
<span class="p_add">+				bounds_dir_global+bd_offset_bytes+i);</span>
<span class="p_add">+		total_entries += nr_entries;</span>
<span class="p_add">+		dprintf5(&quot;dir entry[%4ld @ %p]: 0x%lx %6d entries &quot;</span>
<span class="p_add">+			 &quot;total this buf: %7d bd_for_vaddrs: 0x%lx -&gt; 0x%lx\n&quot;,</span>
<span class="p_add">+				bd_index, buf+i,</span>
<span class="p_add">+				bounds_dir_entry, nr_entries, total_entries,</span>
<span class="p_add">+				bd_for_vaddr, bd_for_vaddr + (1UL&lt;&lt;20));</span>
<span class="p_add">+	}</span>
<span class="p_add">+	dprintf3(&quot;%s(%p, %x, %lx, ...) done\n&quot;, __func__, buf, len_bytes,</span>
<span class="p_add">+			bd_offset_bytes);</span>
<span class="p_add">+	return total_entries;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int proc_pid_mem_fd = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+void *fill_bounds_dir_buf_other(long byte_offset_inside_bounds_dir,</span>
<span class="p_add">+			   long buffer_size_bytes, void *buffer)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long seekto = bounds_dir_global + byte_offset_inside_bounds_dir;</span>
<span class="p_add">+	int read_ret;</span>
<span class="p_add">+	off_t seek_ret = lseek(proc_pid_mem_fd, seekto, SEEK_SET);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (seek_ret != seekto)</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+</span>
<span class="p_add">+	read_ret = read(proc_pid_mem_fd, buffer, buffer_size_bytes);</span>
<span class="p_add">+	/* there shouldn&#39;t practically be short reads of /proc/$pid/mem */</span>
<span class="p_add">+	if (read_ret != buffer_size_bytes)</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+</span>
<span class="p_add">+	return buffer;</span>
<span class="p_add">+}</span>
<span class="p_add">+void *fill_bounds_dir_buf_self(long byte_offset_inside_bounds_dir,</span>
<span class="p_add">+			   long buffer_size_bytes, void *buffer)</span>
<span class="p_add">+</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned char vec[buffer_size_bytes / PAGE_SIZE];</span>
<span class="p_add">+	char *dig_bounds_dir_ptr =</span>
<span class="p_add">+		(void *)(bounds_dir_global + byte_offset_inside_bounds_dir);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * use mincore() to quickly find the areas of the bounds directory</span>
<span class="p_add">+	 * that have memory and thus will be worth scanning.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int incore_ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	int incore = 0;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf4(&quot;%s() dig_bounds_dir_ptr: %p\n&quot;, __func__, dig_bounds_dir_ptr);</span>
<span class="p_add">+</span>
<span class="p_add">+	incore_ret = mincore(dig_bounds_dir_ptr, buffer_size_bytes, &amp;vec[0]);</span>
<span class="p_add">+	if (incore_ret) {</span>
<span class="p_add">+		printf(&quot;mincore ret: %d\n&quot;, incore_ret);</span>
<span class="p_add">+		perror(&quot;mincore&quot;);</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+	}</span>
<span class="p_add">+	for (i = 0; i &lt; sizeof(vec); i++)</span>
<span class="p_add">+		incore += vec[i];</span>
<span class="p_add">+	dprintf4(&quot;%s() total incore: %d\n&quot;, __func__, incore);</span>
<span class="p_add">+	if (!incore)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	dprintf3(&quot;%s() total incore: %d\n&quot;, __func__, incore);</span>
<span class="p_add">+	return dig_bounds_dir_ptr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int inspect_pid(int pid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	static int dig_nr;</span>
<span class="p_add">+	long offset_inside_bounds_dir;</span>
<span class="p_add">+	char bounds_dir_buf[sizeof(unsigned long) * (1UL &lt;&lt; 15)];</span>
<span class="p_add">+	char *dig_bounds_dir_ptr;</span>
<span class="p_add">+	int total_entries = 0;</span>
<span class="p_add">+	int nr_populated_bdes = 0;</span>
<span class="p_add">+	int inspect_self;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (getpid() == pid) {</span>
<span class="p_add">+		dprintf4(&quot;inspecting self\n&quot;);</span>
<span class="p_add">+		inspect_self = 1;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		dprintf4(&quot;inspecting pid %d\n&quot;, pid);</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	for (offset_inside_bounds_dir = 0;</span>
<span class="p_add">+	     offset_inside_bounds_dir &lt; MPX_BOUNDS_TABLE_SIZE_BYTES;</span>
<span class="p_add">+	     offset_inside_bounds_dir += sizeof(bounds_dir_buf)) {</span>
<span class="p_add">+		static int bufs_skipped;</span>
<span class="p_add">+		int this_entries;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (inspect_self) {</span>
<span class="p_add">+			dig_bounds_dir_ptr =</span>
<span class="p_add">+				fill_bounds_dir_buf_self(offset_inside_bounds_dir,</span>
<span class="p_add">+							 sizeof(bounds_dir_buf),</span>
<span class="p_add">+							 &amp;bounds_dir_buf[0]);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			dig_bounds_dir_ptr =</span>
<span class="p_add">+				fill_bounds_dir_buf_other(offset_inside_bounds_dir,</span>
<span class="p_add">+							  sizeof(bounds_dir_buf),</span>
<span class="p_add">+							  &amp;bounds_dir_buf[0]);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (!dig_bounds_dir_ptr) {</span>
<span class="p_add">+			bufs_skipped++;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		this_entries = search_bd_buf(dig_bounds_dir_ptr,</span>
<span class="p_add">+					sizeof(bounds_dir_buf),</span>
<span class="p_add">+					offset_inside_bounds_dir,</span>
<span class="p_add">+					&amp;nr_populated_bdes);</span>
<span class="p_add">+		total_entries += this_entries;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	printf(&quot;mpx dig (%3d) complete, SUCCESS (%8d / %4d)\n&quot;, ++dig_nr,</span>
<span class="p_add">+			total_entries, nr_populated_bdes);</span>
<span class="p_add">+	return total_entries + nr_populated_bdes;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef MPX_DIG_REMOTE</span>
<span class="p_add">+int main(int argc, char **argv)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+	char *c;</span>
<span class="p_add">+	unsigned long bounds_dir_entry;</span>
<span class="p_add">+	int pid;</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;mpx-dig starting...\n&quot;);</span>
<span class="p_add">+	err = sscanf(argv[1], &quot;%d&quot;, &amp;pid);</span>
<span class="p_add">+	printf(&quot;parsing: &#39;%s&#39;, err: %d\n&quot;, argv[1], err);</span>
<span class="p_add">+	if (err != 1)</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+</span>
<span class="p_add">+	err = sscanf(argv[2], &quot;%lx&quot;, &amp;bounds_dir_global);</span>
<span class="p_add">+	printf(&quot;parsing: &#39;%s&#39;: %d\n&quot;, argv[2], err);</span>
<span class="p_add">+	if (err != 1)</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+</span>
<span class="p_add">+	proc_pid_mem_fd = open_proc(pid, &quot;mem&quot;);</span>
<span class="p_add">+	if (proc_pid_mem_fd &lt; 0)</span>
<span class="p_add">+		mpx_dig_abort();</span>
<span class="p_add">+</span>
<span class="p_add">+	inspect_pid(pid);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+long inspect_me(struct mpx_bounds_dir *bounds_dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int pid = getpid();</span>
<span class="p_add">+</span>
<span class="p_add">+	pid_load_vaddrs(pid);</span>
<span class="p_add">+	bounds_dir_global = (unsigned long)bounds_dir;</span>
<span class="p_add">+	dprintf4(&quot;enter %s() bounds dir: %p\n&quot;, __func__, bounds_dir);</span>
<span class="p_add">+	return inspect_pid(pid);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/tools/testing/selftests/x86/mpx-hw.h b/tools/testing/selftests/x86/mpx-hw.h</span>
new file mode 100644
<span class="p_header">index 000000000000..093c190178a9</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/mpx-hw.h</span>
<span class="p_chunk">@@ -0,0 +1,123 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _MPX_HW_H</span>
<span class="p_add">+#define _MPX_HW_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;assert.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Describe the MPX Hardware Layout in here */</span>
<span class="p_add">+</span>
<span class="p_add">+#define NR_MPX_BOUNDS_REGISTERS 4</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_ENTRY_SIZE_BYTES	16 /* 4 * 32-bits */</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_SIZE_BYTES		(1ULL &lt;&lt; 14) /* 16k */</span>
<span class="p_add">+#define MPX_BOUNDS_DIR_ENTRY_SIZE_BYTES		4</span>
<span class="p_add">+#define MPX_BOUNDS_DIR_SIZE_BYTES		(1ULL &lt;&lt; 22) /* 4MB */</span>
<span class="p_add">+</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_BOTTOM_BIT		2</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_TOP_BIT		11</span>
<span class="p_add">+#define MPX_BOUNDS_DIR_BOTTOM_BIT		12</span>
<span class="p_add">+#define MPX_BOUNDS_DIR_TOP_BIT			31</span>
<span class="p_add">+</span>
<span class="p_add">+#else</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Linear Address of &quot;pointer&quot; (LAp)</span>
<span class="p_add">+ *   0 -&gt;  2: ignored</span>
<span class="p_add">+ *   3 -&gt; 19: index in to bounds table</span>
<span class="p_add">+ *  20 -&gt; 47: index in to bounds directory</span>
<span class="p_add">+ *  48 -&gt; 63: ignored</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_ENTRY_SIZE_BYTES	32</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_SIZE_BYTES		(1ULL &lt;&lt; 22) /* 4MB */</span>
<span class="p_add">+#define MPX_BOUNDS_DIR_ENTRY_SIZE_BYTES		8</span>
<span class="p_add">+#define MPX_BOUNDS_DIR_SIZE_BYTES		(1ULL &lt;&lt; 31) /* 2GB */</span>
<span class="p_add">+</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_BOTTOM_BIT		3</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_TOP_BIT		19</span>
<span class="p_add">+#define MPX_BOUNDS_DIR_BOTTOM_BIT		20</span>
<span class="p_add">+#define MPX_BOUNDS_DIR_TOP_BIT			47</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define MPX_BOUNDS_DIR_NR_ENTRIES	\</span>
<span class="p_add">+	(MPX_BOUNDS_DIR_SIZE_BYTES/MPX_BOUNDS_DIR_ENTRY_SIZE_BYTES)</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_NR_ENTRIES	\</span>
<span class="p_add">+	(MPX_BOUNDS_TABLE_SIZE_BYTES/MPX_BOUNDS_TABLE_ENTRY_SIZE_BYTES)</span>
<span class="p_add">+</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_ENTRY_VALID_BIT	0x1</span>
<span class="p_add">+</span>
<span class="p_add">+struct mpx_bd_entry {</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		char x[MPX_BOUNDS_DIR_ENTRY_SIZE_BYTES];</span>
<span class="p_add">+		void *contents[1];</span>
<span class="p_add">+	};</span>
<span class="p_add">+} __attribute__((packed));</span>
<span class="p_add">+</span>
<span class="p_add">+struct mpx_bt_entry {</span>
<span class="p_add">+	union {</span>
<span class="p_add">+		char x[MPX_BOUNDS_TABLE_ENTRY_SIZE_BYTES];</span>
<span class="p_add">+		unsigned long contents[1];</span>
<span class="p_add">+	};</span>
<span class="p_add">+} __attribute__((packed));</span>
<span class="p_add">+</span>
<span class="p_add">+struct mpx_bounds_dir {</span>
<span class="p_add">+	struct mpx_bd_entry entries[MPX_BOUNDS_DIR_NR_ENTRIES];</span>
<span class="p_add">+} __attribute__((packed));</span>
<span class="p_add">+</span>
<span class="p_add">+struct mpx_bounds_table {</span>
<span class="p_add">+	struct mpx_bt_entry entries[MPX_BOUNDS_TABLE_NR_ENTRIES];</span>
<span class="p_add">+} __attribute__((packed));</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long GET_BITS(unsigned long val, int bottombit, int topbit)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int total_nr_bits = topbit - bottombit;</span>
<span class="p_add">+	unsigned long mask = (1UL &lt;&lt; total_nr_bits)-1;</span>
<span class="p_add">+	return (val &gt;&gt; bottombit) &amp; mask;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long __vaddr_bounds_table_index(void *vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return GET_BITS((unsigned long)vaddr, MPX_BOUNDS_TABLE_BOTTOM_BIT,</span>
<span class="p_add">+					      MPX_BOUNDS_TABLE_TOP_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long __vaddr_bounds_directory_index(void *vaddr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return GET_BITS((unsigned long)vaddr, MPX_BOUNDS_DIR_BOTTOM_BIT,</span>
<span class="p_add">+					      MPX_BOUNDS_DIR_TOP_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct mpx_bd_entry *mpx_vaddr_to_bd_entry(void *vaddr,</span>
<span class="p_add">+		struct mpx_bounds_dir *bounds_dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long index = __vaddr_bounds_directory_index(vaddr);</span>
<span class="p_add">+	return &amp;bounds_dir-&gt;entries[index];</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int bd_entry_valid(struct mpx_bd_entry *bounds_dir_entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long __bd_entry = (unsigned long)bounds_dir_entry-&gt;contents;</span>
<span class="p_add">+	return (__bd_entry &amp; MPX_BOUNDS_TABLE_ENTRY_VALID_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct mpx_bounds_table *</span>
<span class="p_add">+__bd_entry_to_bounds_table(struct mpx_bd_entry *bounds_dir_entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long __bd_entry = (unsigned long)bounds_dir_entry-&gt;contents;</span>
<span class="p_add">+	assert(__bd_entry &amp; MPX_BOUNDS_TABLE_ENTRY_VALID_BIT);</span>
<span class="p_add">+	__bd_entry &amp;= ~MPX_BOUNDS_TABLE_ENTRY_VALID_BIT;</span>
<span class="p_add">+	return (struct mpx_bounds_table *)__bd_entry;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct mpx_bt_entry *</span>
<span class="p_add">+mpx_vaddr_to_bt_entry(void *vaddr, struct mpx_bounds_dir *bounds_dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mpx_bd_entry *bde = mpx_vaddr_to_bd_entry(vaddr, bounds_dir);</span>
<span class="p_add">+	struct mpx_bounds_table *bt = __bd_entry_to_bounds_table(bde);</span>
<span class="p_add">+	unsigned long index = __vaddr_bounds_table_index(vaddr);</span>
<span class="p_add">+	return &amp;bt-&gt;entries[index];</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _MPX_HW_H */</span>
<span class="p_header">diff --git a/tools/testing/selftests/x86/mpx-mini-test.c b/tools/testing/selftests/x86/mpx-mini-test.c</span>
new file mode 100644
<span class="p_header">index 000000000000..616ee9673339</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/mpx-mini-test.c</span>
<span class="p_chunk">@@ -0,0 +1,1585 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * mpx-mini-test.c: routines to test Intel MPX (Memory Protection eXtentions)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Written by:</span>
<span class="p_add">+ * &quot;Ren, Qiaowei&quot; &lt;qiaowei.ren@intel.com&gt;</span>
<span class="p_add">+ * &quot;Wei, Gang&quot; &lt;gang.wei@intel.com&gt;</span>
<span class="p_add">+ * &quot;Hansen, Dave&quot; &lt;dave.hansen@intel.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms and conditions of the GNU General Public License,</span>
<span class="p_add">+ * version 2.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * 2014-12-05: Dave Hansen: fixed all of the compiler warnings, and made sure</span>
<span class="p_add">+ *	       it works on 32-bit.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+int inspect_every_this_many_mallocs = 100;</span>
<span class="p_add">+int zap_all_every_this_many_mallocs = 1000;</span>
<span class="p_add">+</span>
<span class="p_add">+#define _GNU_SOURCE</span>
<span class="p_add">+#define _LARGEFILE64_SOURCE</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;string.h&gt;</span>
<span class="p_add">+#include &lt;stdio.h&gt;</span>
<span class="p_add">+#include &lt;stdint.h&gt;</span>
<span class="p_add">+#include &lt;stdbool.h&gt;</span>
<span class="p_add">+#include &lt;signal.h&gt;</span>
<span class="p_add">+#include &lt;assert.h&gt;</span>
<span class="p_add">+#include &lt;stdlib.h&gt;</span>
<span class="p_add">+#include &lt;ucontext.h&gt;</span>
<span class="p_add">+#include &lt;sys/mman.h&gt;</span>
<span class="p_add">+#include &lt;sys/types.h&gt;</span>
<span class="p_add">+#include &lt;sys/stat.h&gt;</span>
<span class="p_add">+#include &lt;fcntl.h&gt;</span>
<span class="p_add">+#include &lt;unistd.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;mpx-hw.h&quot;</span>
<span class="p_add">+#include &quot;mpx-debug.h&quot;</span>
<span class="p_add">+#include &quot;mpx-mm.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __always_inline</span>
<span class="p_add">+#define __always_inline inline __attribute__((always_inline)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef TEST_DURATION_SECS</span>
<span class="p_add">+#define TEST_DURATION_SECS 3</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+void write_int_to(char *prefix, char *file, int int_to_write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char buf[100];</span>
<span class="p_add">+	int fd = open(file, O_RDWR);</span>
<span class="p_add">+	int len;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	assert(fd &gt;= 0);</span>
<span class="p_add">+	len = snprintf(buf, sizeof(buf), &quot;%s%d&quot;, prefix, int_to_write);</span>
<span class="p_add">+	assert(len &gt;= 0);</span>
<span class="p_add">+	assert(len &lt; sizeof(buf));</span>
<span class="p_add">+	ret = write(fd, buf, len);</span>
<span class="p_add">+	assert(ret == len);</span>
<span class="p_add">+	ret = close(fd);</span>
<span class="p_add">+	assert(!ret);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void write_pid_to(char *prefix, char *file)</span>
<span class="p_add">+{</span>
<span class="p_add">+	write_int_to(prefix, file, getpid());</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void trace_me(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+/* tracing events dir */</span>
<span class="p_add">+#define TED &quot;/sys/kernel/debug/tracing/events/&quot;</span>
<span class="p_add">+/*</span>
<span class="p_add">+	write_pid_to(&quot;common_pid=&quot;, TED &quot;signal/filter&quot;);</span>
<span class="p_add">+	write_pid_to(&quot;common_pid=&quot;, TED &quot;exceptions/filter&quot;);</span>
<span class="p_add">+	write_int_to(&quot;&quot;, TED &quot;signal/enable&quot;, 1);</span>
<span class="p_add">+	write_int_to(&quot;&quot;, TED &quot;exceptions/enable&quot;, 1);</span>
<span class="p_add">+*/</span>
<span class="p_add">+	write_pid_to(&quot;&quot;, &quot;/sys/kernel/debug/tracing/set_ftrace_pid&quot;);</span>
<span class="p_add">+	write_int_to(&quot;&quot;, &quot;/sys/kernel/debug/tracing/trace&quot;, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define test_failed() __test_failed(__FILE__, __LINE__)</span>
<span class="p_add">+static void __test_failed(char *f, int l)</span>
<span class="p_add">+{</span>
<span class="p_add">+	fprintf(stderr, &quot;abort @ %s::%d\n&quot;, f, l);</span>
<span class="p_add">+	abort();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Error Printf */</span>
<span class="p_add">+#define eprintf(args...)	fprintf(stderr, args)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+</span>
<span class="p_add">+/* i386 directory size is 4MB */</span>
<span class="p_add">+#define REG_IP_IDX	REG_EIP</span>
<span class="p_add">+#define REX_PREFIX</span>
<span class="p_add">+</span>
<span class="p_add">+#define XSAVE_OFFSET_IN_FPMEM	sizeof(struct _libc_fpstate)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * __cpuid() is from the Linux Kernel:</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void __cpuid(unsigned int *eax, unsigned int *ebx,</span>
<span class="p_add">+		unsigned int *ecx, unsigned int *edx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* ecx is often an input as well as an output. */</span>
<span class="p_add">+	asm volatile(</span>
<span class="p_add">+		&quot;push %%ebx;&quot;</span>
<span class="p_add">+		&quot;cpuid;&quot;</span>
<span class="p_add">+		&quot;mov %%ebx, %1;&quot;</span>
<span class="p_add">+		&quot;pop %%ebx&quot;</span>
<span class="p_add">+		: &quot;=a&quot; (*eax),</span>
<span class="p_add">+		  &quot;=g&quot; (*ebx),</span>
<span class="p_add">+		  &quot;=c&quot; (*ecx),</span>
<span class="p_add">+		  &quot;=d&quot; (*edx)</span>
<span class="p_add">+		: &quot;0&quot; (*eax), &quot;2&quot; (*ecx));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* __i386__ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define REG_IP_IDX	REG_RIP</span>
<span class="p_add">+#define REX_PREFIX &quot;0x48, &quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#define XSAVE_OFFSET_IN_FPMEM	0</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * __cpuid() is from the Linux Kernel:</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void __cpuid(unsigned int *eax, unsigned int *ebx,</span>
<span class="p_add">+		unsigned int *ecx, unsigned int *edx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* ecx is often an input as well as an output. */</span>
<span class="p_add">+	asm volatile(</span>
<span class="p_add">+		&quot;cpuid;&quot;</span>
<span class="p_add">+		: &quot;=a&quot; (*eax),</span>
<span class="p_add">+		  &quot;=b&quot; (*ebx),</span>
<span class="p_add">+		  &quot;=c&quot; (*ecx),</span>
<span class="p_add">+		  &quot;=d&quot; (*edx)</span>
<span class="p_add">+		: &quot;0&quot; (*eax), &quot;2&quot; (*ecx));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* !__i386__ */</span>
<span class="p_add">+</span>
<span class="p_add">+struct xsave_hdr_struct {</span>
<span class="p_add">+	uint64_t xstate_bv;</span>
<span class="p_add">+	uint64_t reserved1[2];</span>
<span class="p_add">+	uint64_t reserved2[5];</span>
<span class="p_add">+} __attribute__((packed));</span>
<span class="p_add">+</span>
<span class="p_add">+struct bndregs_struct {</span>
<span class="p_add">+	uint64_t bndregs[8];</span>
<span class="p_add">+} __attribute__((packed));</span>
<span class="p_add">+</span>
<span class="p_add">+struct bndcsr_struct {</span>
<span class="p_add">+	uint64_t cfg_reg_u;</span>
<span class="p_add">+	uint64_t status_reg;</span>
<span class="p_add">+} __attribute__((packed));</span>
<span class="p_add">+</span>
<span class="p_add">+struct xsave_struct {</span>
<span class="p_add">+	uint8_t fpu_sse[512];</span>
<span class="p_add">+	struct xsave_hdr_struct xsave_hdr;</span>
<span class="p_add">+	uint8_t ymm[256];</span>
<span class="p_add">+	uint8_t lwp[128];</span>
<span class="p_add">+	struct bndregs_struct bndregs;</span>
<span class="p_add">+	struct bndcsr_struct bndcsr;</span>
<span class="p_add">+} __attribute__((packed));</span>
<span class="p_add">+</span>
<span class="p_add">+uint8_t __attribute__((__aligned__(64))) buffer[4096];</span>
<span class="p_add">+struct xsave_struct *xsave_buf = (struct xsave_struct *)buffer;</span>
<span class="p_add">+</span>
<span class="p_add">+uint8_t __attribute__((__aligned__(64))) test_buffer[4096];</span>
<span class="p_add">+struct xsave_struct *xsave_test_buf = (struct xsave_struct *)test_buffer;</span>
<span class="p_add">+</span>
<span class="p_add">+uint64_t num_bnd_chk;</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void xrstor_state(struct xsave_struct *fx, uint64_t mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint32_t lmask = mask;</span>
<span class="p_add">+	uint32_t hmask = mask &gt;&gt; 32;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;.byte &quot; REX_PREFIX &quot;0x0f,0xae,0x2f\n\t&quot;</span>
<span class="p_add">+		     : : &quot;D&quot; (fx), &quot;m&quot; (*fx), &quot;a&quot; (lmask), &quot;d&quot; (hmask)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void xsave_state_1(void *_fx, uint64_t mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint32_t lmask = mask;</span>
<span class="p_add">+	uint32_t hmask = mask &gt;&gt; 32;</span>
<span class="p_add">+	unsigned char *fx = _fx;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;.byte &quot; REX_PREFIX &quot;0x0f,0xae,0x27\n\t&quot;</span>
<span class="p_add">+		     : : &quot;D&quot; (fx), &quot;m&quot; (*fx), &quot;a&quot; (lmask), &quot;d&quot; (hmask)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline uint64_t xgetbv(uint32_t index)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint32_t eax, edx;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;.byte 0x0f,0x01,0xd0&quot; /* xgetbv */</span>
<span class="p_add">+		     : &quot;=a&quot; (eax), &quot;=d&quot; (edx)</span>
<span class="p_add">+		     : &quot;c&quot; (index));</span>
<span class="p_add">+	return eax + ((uint64_t)edx &lt;&lt; 32);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static uint64_t read_mpx_status_sig(ucontext_t *uctxt)</span>
<span class="p_add">+{</span>
<span class="p_add">+	memset(buffer, 0, sizeof(buffer));</span>
<span class="p_add">+	memcpy(buffer,</span>
<span class="p_add">+		(uint8_t *)uctxt-&gt;uc_mcontext.fpregs + XSAVE_OFFSET_IN_FPMEM,</span>
<span class="p_add">+		sizeof(struct xsave_struct));</span>
<span class="p_add">+</span>
<span class="p_add">+	return xsave_buf-&gt;bndcsr.status_reg;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;pthread.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static uint8_t *get_next_inst_ip(uint8_t *addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint8_t *ip = addr;</span>
<span class="p_add">+	uint8_t sib;</span>
<span class="p_add">+	uint8_t rm;</span>
<span class="p_add">+	uint8_t mod;</span>
<span class="p_add">+	uint8_t base;</span>
<span class="p_add">+	uint8_t modrm;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* determine the prefix. */</span>
<span class="p_add">+	switch(*ip) {</span>
<span class="p_add">+	case 0xf2:</span>
<span class="p_add">+	case 0xf3:</span>
<span class="p_add">+	case 0x66:</span>
<span class="p_add">+		ip++;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* look for rex prefix */</span>
<span class="p_add">+	if ((*ip &amp; 0x40) == 0x40)</span>
<span class="p_add">+		ip++;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Make sure we have a MPX instruction. */</span>
<span class="p_add">+	if (*ip++ != 0x0f)</span>
<span class="p_add">+		return addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Skip the op code byte. */</span>
<span class="p_add">+	ip++;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Get the modrm byte. */</span>
<span class="p_add">+	modrm = *ip++;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Break it down into parts. */</span>
<span class="p_add">+	rm = modrm &amp; 7;</span>
<span class="p_add">+	mod = (modrm &gt;&gt; 6);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Init the parts of the address mode. */</span>
<span class="p_add">+	base = 8;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Is it a mem mode? */</span>
<span class="p_add">+	if (mod != 3) {</span>
<span class="p_add">+		/* look for scaled indexed addressing */</span>
<span class="p_add">+		if (rm == 4) {</span>
<span class="p_add">+			/* SIB addressing */</span>
<span class="p_add">+			sib = *ip++;</span>
<span class="p_add">+			base = sib &amp; 7;</span>
<span class="p_add">+			switch (mod) {</span>
<span class="p_add">+			case 0:</span>
<span class="p_add">+				if (base == 5)</span>
<span class="p_add">+					ip += 4;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+</span>
<span class="p_add">+			case 1:</span>
<span class="p_add">+				ip++;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+</span>
<span class="p_add">+			case 2:</span>
<span class="p_add">+				ip += 4;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* MODRM addressing */</span>
<span class="p_add">+			switch (mod) {</span>
<span class="p_add">+			case 0:</span>
<span class="p_add">+				/* DISP32 addressing, no base */</span>
<span class="p_add">+				if (rm == 5)</span>
<span class="p_add">+					ip += 4;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+</span>
<span class="p_add">+			case 1:</span>
<span class="p_add">+				ip++;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+</span>
<span class="p_add">+			case 2:</span>
<span class="p_add">+				ip += 4;</span>
<span class="p_add">+				break;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ip;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef si_lower</span>
<span class="p_add">+static inline void *__si_bounds_lower(siginfo_t *si)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return si-&gt;si_lower;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void *__si_bounds_upper(siginfo_t *si)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return si-&gt;si_upper;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void **__si_bounds_hack(siginfo_t *si)</span>
<span class="p_add">+{</span>
<span class="p_add">+	void *sigfault = &amp;si-&gt;_sifields._sigfault;</span>
<span class="p_add">+	void *end_sigfault = sigfault + sizeof(si-&gt;_sifields._sigfault);</span>
<span class="p_add">+	void **__si_lower = end_sigfault;</span>
<span class="p_add">+</span>
<span class="p_add">+	return __si_lower;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void *__si_bounds_lower(siginfo_t *si)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return *__si_bounds_hack(si);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void *__si_bounds_upper(siginfo_t *si)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (*__si_bounds_hack(si)) + sizeof(void *);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static int br_count;</span>
<span class="p_add">+static int expected_bnd_index = -1;</span>
<span class="p_add">+uint64_t shadow_plb[NR_MPX_BOUNDS_REGISTERS][2]; /* shadow MPX bound registers */</span>
<span class="p_add">+unsigned long shadow_map[NR_MPX_BOUNDS_REGISTERS];</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The kernel is supposed to provide some information about the bounds</span>
<span class="p_add">+ * exception in the siginfo.  It should match what we have in the bounds</span>
<span class="p_add">+ * registers that we are checking against.  Just check against the shadow copy</span>
<span class="p_add">+ * since it is easily available, and we also check that *it* matches the real</span>
<span class="p_add">+ * registers.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void check_siginfo_vs_shadow(siginfo_t* si)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int siginfo_ok = 1;</span>
<span class="p_add">+	void *shadow_lower = (void *)(unsigned long)shadow_plb[expected_bnd_index][0];</span>
<span class="p_add">+	void *shadow_upper = (void *)(unsigned long)shadow_plb[expected_bnd_index][1];</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((expected_bnd_index &lt; 0) ||</span>
<span class="p_add">+	    (expected_bnd_index &gt;= NR_MPX_BOUNDS_REGISTERS)) {</span>
<span class="p_add">+		fprintf(stderr, &quot;ERROR: invalid expected_bnd_index: %d\n&quot;,</span>
<span class="p_add">+			expected_bnd_index);</span>
<span class="p_add">+		exit(6);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (__si_bounds_lower(si) != shadow_lower)</span>
<span class="p_add">+		siginfo_ok = 0;</span>
<span class="p_add">+	if (__si_bounds_upper(si) != shadow_upper)</span>
<span class="p_add">+		siginfo_ok = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!siginfo_ok) {</span>
<span class="p_add">+		fprintf(stderr, &quot;ERROR: siginfo bounds do not match &quot;</span>
<span class="p_add">+			&quot;shadow bounds for register %d\n&quot;, expected_bnd_index);</span>
<span class="p_add">+		exit(7);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void handler(int signum, siginfo_t *si, void *vucontext)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	ucontext_t *uctxt = vucontext;</span>
<span class="p_add">+	int trapno;</span>
<span class="p_add">+	unsigned long ip;</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf1(&quot;entered signal handler\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	trapno = uctxt-&gt;uc_mcontext.gregs[REG_TRAPNO];</span>
<span class="p_add">+	ip = uctxt-&gt;uc_mcontext.gregs[REG_IP_IDX];</span>
<span class="p_add">+</span>
<span class="p_add">+	if (trapno == 5) {</span>
<span class="p_add">+		typeof(si-&gt;si_addr) *si_addr_ptr = &amp;si-&gt;si_addr;</span>
<span class="p_add">+		uint64_t status = read_mpx_status_sig(uctxt);</span>
<span class="p_add">+		uint64_t br_reason =  status &amp; 0x3;</span>
<span class="p_add">+</span>
<span class="p_add">+		br_count++;</span>
<span class="p_add">+		dprintf1(&quot;#BR 0x%jx (total seen: %d)\n&quot;, status, br_count);</span>
<span class="p_add">+</span>
<span class="p_add">+#define __SI_FAULT      (3 &lt;&lt; 16)</span>
<span class="p_add">+#define SEGV_BNDERR     (__SI_FAULT|3)  /* failed address bound checks */</span>
<span class="p_add">+</span>
<span class="p_add">+		dprintf2(&quot;Saw a #BR! status 0x%jx at %016lx br_reason: %jx\n&quot;,</span>
<span class="p_add">+				status, ip, br_reason);</span>
<span class="p_add">+		dprintf2(&quot;si_signo: %d\n&quot;, si-&gt;si_signo);</span>
<span class="p_add">+		dprintf2(&quot;  signum: %d\n&quot;, signum);</span>
<span class="p_add">+		dprintf2(&quot;info-&gt;si_code == SEGV_BNDERR: %d\n&quot;,</span>
<span class="p_add">+				(si-&gt;si_code == SEGV_BNDERR));</span>
<span class="p_add">+		dprintf2(&quot;info-&gt;si_code: %d\n&quot;, si-&gt;si_code);</span>
<span class="p_add">+		dprintf2(&quot;info-&gt;si_lower: %p\n&quot;, __si_bounds_lower(si));</span>
<span class="p_add">+		dprintf2(&quot;info-&gt;si_upper: %p\n&quot;, __si_bounds_upper(si));</span>
<span class="p_add">+</span>
<span class="p_add">+		check_siginfo_vs_shadow(si);</span>
<span class="p_add">+</span>
<span class="p_add">+		for (i = 0; i &lt; 8; i++)</span>
<span class="p_add">+			dprintf3(&quot;[%d]: %p\n&quot;, i, si_addr_ptr[i]);</span>
<span class="p_add">+		switch (br_reason) {</span>
<span class="p_add">+		case 0: /* traditional BR */</span>
<span class="p_add">+			fprintf(stderr,</span>
<span class="p_add">+				&quot;Undefined status with bound exception:%jx\n&quot;,</span>
<span class="p_add">+				 status);</span>
<span class="p_add">+			exit(5);</span>
<span class="p_add">+		case 1: /* #BR MPX bounds exception */</span>
<span class="p_add">+			/* these are normal and we expect to see them */</span>
<span class="p_add">+			dprintf1(&quot;bounds exception (normal): status 0x%jx at %p si_addr: %p\n&quot;,</span>
<span class="p_add">+				status, (void *)ip, si-&gt;si_addr);</span>
<span class="p_add">+			num_bnd_chk++;</span>
<span class="p_add">+			uctxt-&gt;uc_mcontext.gregs[REG_IP_IDX] =</span>
<span class="p_add">+				(greg_t)get_next_inst_ip((uint8_t *)ip);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		case 2:</span>
<span class="p_add">+			fprintf(stderr, &quot;#BR status == 2, missing bounds table,&quot;</span>
<span class="p_add">+					&quot;kernel should have handled!!\n&quot;);</span>
<span class="p_add">+			exit(4);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			fprintf(stderr, &quot;bound check error: status 0x%jx at %p\n&quot;,</span>
<span class="p_add">+				status, (void *)ip);</span>
<span class="p_add">+			num_bnd_chk++;</span>
<span class="p_add">+			uctxt-&gt;uc_mcontext.gregs[REG_IP_IDX] =</span>
<span class="p_add">+				(greg_t)get_next_inst_ip((uint8_t *)ip);</span>
<span class="p_add">+			fprintf(stderr, &quot;bound check error: si_addr %p\n&quot;, si-&gt;si_addr);</span>
<span class="p_add">+			exit(3);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else if (trapno == 14) {</span>
<span class="p_add">+		eprintf(&quot;ERROR: In signal handler, page fault, trapno = %d, ip = %016lx\n&quot;,</span>
<span class="p_add">+			trapno, ip);</span>
<span class="p_add">+		eprintf(&quot;si_addr %p\n&quot;, si-&gt;si_addr);</span>
<span class="p_add">+		eprintf(&quot;REG_ERR: %lx\n&quot;, (unsigned long)uctxt-&gt;uc_mcontext.gregs[REG_ERR]);</span>
<span class="p_add">+		test_failed();</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		eprintf(&quot;unexpected trap %d! at 0x%lx\n&quot;, trapno, ip);</span>
<span class="p_add">+		eprintf(&quot;si_addr %p\n&quot;, si-&gt;si_addr);</span>
<span class="p_add">+		eprintf(&quot;REG_ERR: %lx\n&quot;, (unsigned long)uctxt-&gt;uc_mcontext.gregs[REG_ERR]);</span>
<span class="p_add">+		test_failed();</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void cpuid_count(unsigned int op, int count,</span>
<span class="p_add">+			       unsigned int *eax, unsigned int *ebx,</span>
<span class="p_add">+			       unsigned int *ecx, unsigned int *edx)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*eax = op;</span>
<span class="p_add">+	*ecx = count;</span>
<span class="p_add">+	__cpuid(eax, ebx, ecx, edx);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define XSTATE_CPUID	    0x0000000d</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * List of XSAVE features Linux knows about:</span>
<span class="p_add">+ */</span>
<span class="p_add">+enum xfeature_bit {</span>
<span class="p_add">+	XSTATE_BIT_FP,</span>
<span class="p_add">+	XSTATE_BIT_SSE,</span>
<span class="p_add">+	XSTATE_BIT_YMM,</span>
<span class="p_add">+	XSTATE_BIT_BNDREGS,</span>
<span class="p_add">+	XSTATE_BIT_BNDCSR,</span>
<span class="p_add">+	XSTATE_BIT_OPMASK,</span>
<span class="p_add">+	XSTATE_BIT_ZMM_Hi256,</span>
<span class="p_add">+	XSTATE_BIT_Hi16_ZMM,</span>
<span class="p_add">+</span>
<span class="p_add">+	XFEATURES_NR_MAX,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define XSTATE_FP	       (1 &lt;&lt; XSTATE_BIT_FP)</span>
<span class="p_add">+#define XSTATE_SSE	      (1 &lt;&lt; XSTATE_BIT_SSE)</span>
<span class="p_add">+#define XSTATE_YMM	      (1 &lt;&lt; XSTATE_BIT_YMM)</span>
<span class="p_add">+#define XSTATE_BNDREGS	  (1 &lt;&lt; XSTATE_BIT_BNDREGS)</span>
<span class="p_add">+#define XSTATE_BNDCSR	   (1 &lt;&lt; XSTATE_BIT_BNDCSR)</span>
<span class="p_add">+#define XSTATE_OPMASK	   (1 &lt;&lt; XSTATE_BIT_OPMASK)</span>
<span class="p_add">+#define XSTATE_ZMM_Hi256	(1 &lt;&lt; XSTATE_BIT_ZMM_Hi256)</span>
<span class="p_add">+#define XSTATE_Hi16_ZMM	 (1 &lt;&lt; XSTATE_BIT_Hi16_ZMM)</span>
<span class="p_add">+</span>
<span class="p_add">+#define MPX_XSTATES		(XSTATE_BNDREGS | XSTATE_BNDCSR) /* 0x18 */</span>
<span class="p_add">+</span>
<span class="p_add">+bool one_bit(unsigned int x, int bit)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !!(x &amp; (1&lt;&lt;bit));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void print_state_component(int state_bit_nr, char *name)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int eax, ebx, ecx, edx;</span>
<span class="p_add">+	unsigned int state_component_size;</span>
<span class="p_add">+	unsigned int state_component_supervisor;</span>
<span class="p_add">+	unsigned int state_component_user;</span>
<span class="p_add">+	unsigned int state_component_aligned;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* See SDM Section 13.2 */</span>
<span class="p_add">+	cpuid_count(XSTATE_CPUID, state_bit_nr, &amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_add">+	assert(eax || ebx || ecx);</span>
<span class="p_add">+	state_component_size = eax;</span>
<span class="p_add">+	state_component_supervisor = ((!ebx) &amp;&amp; one_bit(ecx, 0));</span>
<span class="p_add">+	state_component_user = !one_bit(ecx, 0);</span>
<span class="p_add">+	state_component_aligned = one_bit(ecx, 1);</span>
<span class="p_add">+	printf(&quot;%8s: size: %d user: %d supervisor: %d aligned: %d\n&quot;,</span>
<span class="p_add">+		name,</span>
<span class="p_add">+		state_component_size,	    state_component_user,</span>
<span class="p_add">+		state_component_supervisor, state_component_aligned);</span>
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* Intel-defined CPU features, CPUID level 0x00000001 (ecx) */</span>
<span class="p_add">+#define XSAVE_FEATURE_BIT       (26)  /* XSAVE/XRSTOR/XSETBV/XGETBV */</span>
<span class="p_add">+#define OSXSAVE_FEATURE_BIT     (27) /* XSAVE enabled in the OS */</span>
<span class="p_add">+</span>
<span class="p_add">+bool check_mpx_support(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int eax, ebx, ecx, edx;</span>
<span class="p_add">+</span>
<span class="p_add">+	cpuid_count(1, 0, &amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We can&#39;t do much without XSAVE, so just make these assert()&#39;s */</span>
<span class="p_add">+	if (!one_bit(ecx, XSAVE_FEATURE_BIT)) {</span>
<span class="p_add">+		fprintf(stderr, &quot;processor lacks XSAVE, can not run MPX tests\n&quot;);</span>
<span class="p_add">+		exit(0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!one_bit(ecx, OSXSAVE_FEATURE_BIT)) {</span>
<span class="p_add">+		fprintf(stderr, &quot;processor lacks OSXSAVE, can not run MPX tests\n&quot;);</span>
<span class="p_add">+		exit(0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* CPUs not supporting the XSTATE CPUID leaf do not support MPX */</span>
<span class="p_add">+	/* Is this redundant with the feature bit checks? */</span>
<span class="p_add">+	cpuid_count(0, 0, &amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_add">+	if (eax &lt; XSTATE_CPUID) {</span>
<span class="p_add">+		fprintf(stderr, &quot;processor lacks XSTATE CPUID leaf,&quot;</span>
<span class="p_add">+				&quot; can not run MPX tests\n&quot;);</span>
<span class="p_add">+		exit(0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;XSAVE is supported by HW &amp; OS\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	cpuid_count(XSTATE_CPUID, 0, &amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;XSAVE processor supported state mask: 0x%x\n&quot;, eax);</span>
<span class="p_add">+	printf(&quot;XSAVE OS supported state mask: 0x%jx\n&quot;, xgetbv(0));</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Make sure that the MPX states are enabled in in XCR0 */</span>
<span class="p_add">+	if ((eax &amp; MPX_XSTATES) != MPX_XSTATES) {</span>
<span class="p_add">+		fprintf(stderr, &quot;processor lacks MPX XSTATE(s), can not run MPX tests\n&quot;);</span>
<span class="p_add">+		exit(0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Make sure the MPX states are supported by XSAVE* */</span>
<span class="p_add">+	if ((xgetbv(0) &amp; MPX_XSTATES) != MPX_XSTATES) {</span>
<span class="p_add">+		fprintf(stderr, &quot;MPX XSTATE(s) no enabled in XCR0, &quot;</span>
<span class="p_add">+				&quot;can not run MPX tests\n&quot;);</span>
<span class="p_add">+		exit(0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	print_state_component(XSTATE_BIT_BNDREGS, &quot;BNDREGS&quot;);</span>
<span class="p_add">+	print_state_component(XSTATE_BIT_BNDCSR,  &quot;BNDCSR&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void enable_mpx(void *l1base)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* enable point lookup */</span>
<span class="p_add">+	memset(buffer, 0, sizeof(buffer));</span>
<span class="p_add">+	xrstor_state(xsave_buf, 0x18);</span>
<span class="p_add">+</span>
<span class="p_add">+	xsave_buf-&gt;xsave_hdr.xstate_bv = 0x10;</span>
<span class="p_add">+	xsave_buf-&gt;bndcsr.cfg_reg_u = (unsigned long)l1base | 1;</span>
<span class="p_add">+	xsave_buf-&gt;bndcsr.status_reg = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf2(&quot;bf xrstor\n&quot;);</span>
<span class="p_add">+	dprintf2(&quot;xsave cndcsr: status %jx, configu %jx\n&quot;,</span>
<span class="p_add">+	       xsave_buf-&gt;bndcsr.status_reg, xsave_buf-&gt;bndcsr.cfg_reg_u);</span>
<span class="p_add">+	xrstor_state(xsave_buf, 0x18);</span>
<span class="p_add">+	dprintf2(&quot;after xrstor\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	xsave_state_1(xsave_buf, 0x18);</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf1(&quot;xsave bndcsr: status %jx, configu %jx\n&quot;,</span>
<span class="p_add">+	       xsave_buf-&gt;bndcsr.status_reg, xsave_buf-&gt;bndcsr.cfg_reg_u);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;sys/prctl.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct mpx_bounds_dir *bounds_dir_ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned long __bd_incore(const char *func, int line)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long ret = nr_incore(bounds_dir_ptr, MPX_BOUNDS_DIR_SIZE_BYTES);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+#define bd_incore() __bd_incore(__func__, __LINE__)</span>
<span class="p_add">+</span>
<span class="p_add">+void check_clear(void *ptr, unsigned long sz)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long *i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = ptr; (void *)i &lt; ptr + sz; i++) {</span>
<span class="p_add">+		if (*i) {</span>
<span class="p_add">+			dprintf1(&quot;%p is NOT clear at %p\n&quot;, ptr, i);</span>
<span class="p_add">+			assert(0);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	dprintf1(&quot;%p is clear for %lx\n&quot;, ptr, sz);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void check_clear_bd(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	check_clear(bounds_dir_ptr, 2UL &lt;&lt; 30);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define USE_MALLOC_FOR_BOUNDS_DIR 1</span>
<span class="p_add">+bool process_specific_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long size;</span>
<span class="p_add">+	unsigned long *dir;</span>
<span class="p_add">+	/* Guarantee we have the space to align it, add padding: */</span>
<span class="p_add">+	unsigned long pad = getpagesize();</span>
<span class="p_add">+</span>
<span class="p_add">+	size = 2UL &lt;&lt; 30; /* 2GB */</span>
<span class="p_add">+	if (sizeof(unsigned long) == 4)</span>
<span class="p_add">+		size = 4UL &lt;&lt; 20; /* 4MB */</span>
<span class="p_add">+	dprintf1(&quot;trying to allocate %ld MB bounds directory\n&quot;, (size &gt;&gt; 20));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (USE_MALLOC_FOR_BOUNDS_DIR) {</span>
<span class="p_add">+		unsigned long _dir;</span>
<span class="p_add">+</span>
<span class="p_add">+		dir = malloc(size + pad);</span>
<span class="p_add">+		assert(dir);</span>
<span class="p_add">+		_dir = (unsigned long)dir;</span>
<span class="p_add">+		_dir += 0xfffUL;</span>
<span class="p_add">+		_dir &amp;= ~0xfffUL;</span>
<span class="p_add">+		dir = (void *)_dir;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This makes debugging easier because the address</span>
<span class="p_add">+		 * calculations are simpler:</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		dir = mmap((void *)0x200000000000, size + pad,</span>
<span class="p_add">+				PROT_READ|PROT_WRITE,</span>
<span class="p_add">+				MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);</span>
<span class="p_add">+		if (dir == (void *)-1) {</span>
<span class="p_add">+			perror(&quot;unable to allocate bounds directory&quot;);</span>
<span class="p_add">+			abort();</span>
<span class="p_add">+		}</span>
<span class="p_add">+		check_clear(dir, size);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	bounds_dir_ptr = (void *)dir;</span>
<span class="p_add">+	madvise(bounds_dir_ptr, size, MADV_NOHUGEPAGE);</span>
<span class="p_add">+	bd_incore();</span>
<span class="p_add">+	dprintf1(&quot;bounds directory: 0x%p -&gt; 0x%p\n&quot;, bounds_dir_ptr,</span>
<span class="p_add">+			(char *)bounds_dir_ptr + size);</span>
<span class="p_add">+	check_clear(dir, size);</span>
<span class="p_add">+	enable_mpx(dir);</span>
<span class="p_add">+	check_clear(dir, size);</span>
<span class="p_add">+	if (prctl(43, 0, 0, 0, 0)) {</span>
<span class="p_add">+		printf(&quot;no MPX support\n&quot;);</span>
<span class="p_add">+		abort();</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+bool process_specific_finish(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (prctl(44)) {</span>
<span class="p_add">+		printf(&quot;no MPX support\n&quot;);</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void setup_handler()</span>
<span class="p_add">+{</span>
<span class="p_add">+	int r, rs;</span>
<span class="p_add">+	struct sigaction newact;</span>
<span class="p_add">+	struct sigaction oldact;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* #BR is mapped to sigsegv */</span>
<span class="p_add">+	int signum  = SIGSEGV;</span>
<span class="p_add">+</span>
<span class="p_add">+	newact.sa_handler = 0;   /* void(*)(int)*/</span>
<span class="p_add">+	newact.sa_sigaction = handler; /* void (*)(int, siginfo_t*, void *) */</span>
<span class="p_add">+</span>
<span class="p_add">+	/*sigset_t - signals to block while in the handler */</span>
<span class="p_add">+	/* get the old signal mask. */</span>
<span class="p_add">+	rs = sigprocmask(SIG_SETMASK, 0, &amp;newact.sa_mask);</span>
<span class="p_add">+	assert(rs == 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* call sa_sigaction, not sa_handler*/</span>
<span class="p_add">+	newact.sa_flags = SA_SIGINFO;</span>
<span class="p_add">+</span>
<span class="p_add">+	newact.sa_restorer = 0;  /* void(*)(), obsolete */</span>
<span class="p_add">+	r = sigaction(signum, &amp;newact, &amp;oldact);</span>
<span class="p_add">+	assert(r == 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void mpx_prepare(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	dprintf2(&quot;%s()\n&quot;, __func__);</span>
<span class="p_add">+	setup_handler();</span>
<span class="p_add">+	process_specific_init();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void mpx_cleanup(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	printf(&quot;%s(): %jd BRs. bye...\n&quot;, __func__, num_bnd_chk);</span>
<span class="p_add">+	process_specific_finish();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*-------------- the following is test case ---------------*/</span>
<span class="p_add">+#include &lt;stdint.h&gt;</span>
<span class="p_add">+#include &lt;stdbool.h&gt;</span>
<span class="p_add">+#include &lt;stdlib.h&gt;</span>
<span class="p_add">+#include &lt;stdio.h&gt;</span>
<span class="p_add">+#include &lt;time.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+uint64_t num_lower_brs;</span>
<span class="p_add">+uint64_t num_upper_brs;</span>
<span class="p_add">+</span>
<span class="p_add">+#define MPX_CONFIG_OFFSET 1024</span>
<span class="p_add">+#define MPX_BOUNDS_OFFSET 960</span>
<span class="p_add">+#define MPX_HEADER_OFFSET 512</span>
<span class="p_add">+#define MAX_ADDR_TESTED (1&lt;&lt;28)</span>
<span class="p_add">+#define TEST_ROUNDS 100</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+      0F 1A /r BNDLDX-Load</span>
<span class="p_add">+      0F 1B /r BNDSTX-Store Extended Bounds Using Address Translation</span>
<span class="p_add">+   66 0F 1A /r BNDMOV bnd1, bnd2/m128</span>
<span class="p_add">+   66 0F 1B /r BNDMOV bnd1/m128, bnd2</span>
<span class="p_add">+   F2 0F 1A /r BNDCU bnd, r/m64</span>
<span class="p_add">+   F2 0F 1B /r BNDCN bnd, r/m64</span>
<span class="p_add">+   F3 0F 1A /r BNDCL bnd, r/m64</span>
<span class="p_add">+   F3 0F 1B /r BNDMK bnd, m64</span>
<span class="p_add">+*/</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void xsave_state(void *_fx, uint64_t mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint32_t lmask = mask;</span>
<span class="p_add">+	uint32_t hmask = mask &gt;&gt; 32;</span>
<span class="p_add">+	unsigned char *fx = _fx;</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;.byte &quot; REX_PREFIX &quot;0x0f,0xae,0x27\n\t&quot;</span>
<span class="p_add">+		     : : &quot;D&quot; (fx), &quot;m&quot; (*fx), &quot;a&quot; (lmask), &quot;d&quot; (hmask)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_clear_bnd0(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long size = 0;</span>
<span class="p_add">+	void *ptr = NULL;</span>
<span class="p_add">+	/* F3 0F 1B /r BNDMK bnd, m64			*/</span>
<span class="p_add">+	/* f3 0f 1b 04 11    bndmk  (%rcx,%rdx,1),%bnd0	*/</span>
<span class="p_add">+	asm volatile(&quot;.byte 0xf3,0x0f,0x1b,0x04,0x11\n\t&quot;</span>
<span class="p_add">+		     : : &quot;c&quot; (ptr), &quot;d&quot; (size-1)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_make_bound_helper(unsigned long ptr,</span>
<span class="p_add">+		unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* F3 0F 1B /r		BNDMK bnd, m64			*/</span>
<span class="p_add">+	/* f3 0f 1b 04 11       bndmk  (%rcx,%rdx,1),%bnd0	*/</span>
<span class="p_add">+	asm volatile(&quot;.byte 0xf3,0x0f,0x1b,0x04,0x11\n\t&quot;</span>
<span class="p_add">+		     : : &quot;c&quot; (ptr), &quot;d&quot; (size-1)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_check_lowerbound_helper(unsigned long ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* F3 0F 1A /r	NDCL bnd, r/m64			*/</span>
<span class="p_add">+	/* f3 0f 1a 01	bndcl  (%rcx),%bnd0		*/</span>
<span class="p_add">+	asm volatile(&quot;.byte 0xf3,0x0f,0x1a,0x01\n\t&quot;</span>
<span class="p_add">+		     : : &quot;c&quot; (ptr)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_check_upperbound_helper(unsigned long ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* F2 0F 1A /r	BNDCU bnd, r/m64	*/</span>
<span class="p_add">+	/* f2 0f 1a 01	bndcu  (%rcx),%bnd0	*/</span>
<span class="p_add">+	asm volatile(&quot;.byte 0xf2,0x0f,0x1a,0x01\n\t&quot;</span>
<span class="p_add">+		     : : &quot;c&quot; (ptr)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_movbndreg_helper()</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* 66 0F 1B /r	BNDMOV bnd1/m128, bnd2	*/</span>
<span class="p_add">+	/* 66 0f 1b c2	bndmov %bnd0,%bnd2	*/</span>
<span class="p_add">+</span>
<span class="p_add">+	asm volatile(&quot;.byte 0x66,0x0f,0x1b,0xc2\n\t&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_movbnd2mem_helper(uint8_t *mem)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* 66 0F 1B /r	BNDMOV bnd1/m128, bnd2	*/</span>
<span class="p_add">+	/* 66 0f 1b 01	bndmov %bnd0,(%rcx)	*/</span>
<span class="p_add">+	asm volatile(&quot;.byte 0x66,0x0f,0x1b,0x01\n\t&quot;</span>
<span class="p_add">+		     : : &quot;c&quot; (mem)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_movbnd_from_mem_helper(uint8_t *mem)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* 66 0F 1A /r	BNDMOV bnd1, bnd2/m128	*/</span>
<span class="p_add">+	/* 66 0f 1a 01	bndmov (%rcx),%bnd0	*/</span>
<span class="p_add">+	asm volatile(&quot;.byte 0x66,0x0f,0x1a,0x01\n\t&quot;</span>
<span class="p_add">+		     : : &quot;c&quot; (mem)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_store_dsc_helper(unsigned long ptr_addr,</span>
<span class="p_add">+		unsigned long ptr_val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* 0F 1B /r	BNDSTX-Store Extended Bounds Using Address Translation	*/</span>
<span class="p_add">+	/* 0f 1b 04 11	bndstx %bnd0,(%rcx,%rdx,1)				*/</span>
<span class="p_add">+	asm volatile(&quot;.byte 0x0f,0x1b,0x04,0x11\n\t&quot;</span>
<span class="p_add">+		     : : &quot;c&quot; (ptr_addr), &quot;d&quot; (ptr_val)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_load_dsc_helper(unsigned long ptr_addr,</span>
<span class="p_add">+		unsigned long ptr_val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* 0F 1A /r	BNDLDX-Load			*/</span>
<span class="p_add">+	/*/ 0f 1a 04 11	bndldx (%rcx,%rdx,1),%bnd0	*/</span>
<span class="p_add">+	asm volatile(&quot;.byte 0x0f,0x1a,0x04,0x11\n\t&quot;</span>
<span class="p_add">+		     : : &quot;c&quot; (ptr_addr), &quot;d&quot; (ptr_val)</span>
<span class="p_add">+		     :   &quot;memory&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __print_context(void *__print_xsave_buffer, int line)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint64_t *bounds = (uint64_t *)(__print_xsave_buffer + MPX_BOUNDS_OFFSET);</span>
<span class="p_add">+	uint64_t *cfg    = (uint64_t *)(__print_xsave_buffer + MPX_CONFIG_OFFSET);</span>
<span class="p_add">+</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	eprintf(&quot;%s()::%d\n&quot;, &quot;print_context&quot;, line);</span>
<span class="p_add">+	for (i = 0; i &lt; 4; i++) {</span>
<span class="p_add">+		eprintf(&quot;bound[%d]: 0x%016lx 0x%016lx(0x%016lx)\n&quot;, i,</span>
<span class="p_add">+		       (unsigned long)bounds[i*2],</span>
<span class="p_add">+		       ~(unsigned long)bounds[i*2+1],</span>
<span class="p_add">+			(unsigned long)bounds[i*2+1]);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	eprintf(&quot;cpcfg: %jx  cpstatus: %jx\n&quot;, cfg[0], cfg[1]);</span>
<span class="p_add">+}</span>
<span class="p_add">+#define print_context(x) __print_context(x, __LINE__)</span>
<span class="p_add">+#ifdef DEBUG</span>
<span class="p_add">+#define dprint_context(x) print_context(x)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define dprint_context(x) do{}while(0)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+void init()</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	srand((unsigned int)time(NULL));</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; 4; i++) {</span>
<span class="p_add">+		shadow_plb[i][0] = 0;</span>
<span class="p_add">+		shadow_plb[i][1] = ~(unsigned long)0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+long int __mpx_random(int line)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef NOT_SO_RANDOM</span>
<span class="p_add">+	static long fake = 722122311;</span>
<span class="p_add">+	fake += 563792075;</span>
<span class="p_add">+	return fakse;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return random();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+#define mpx_random() __mpx_random(__LINE__)</span>
<span class="p_add">+</span>
<span class="p_add">+uint8_t *get_random_addr()</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint8_t*addr = (uint8_t *)(unsigned long)(rand() % MAX_ADDR_TESTED);</span>
<span class="p_add">+	return (addr - (unsigned long)addr % sizeof(uint8_t *));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool compare_context(void *__xsave_buffer)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint64_t *bounds = (uint64_t *)(__xsave_buffer + MPX_BOUNDS_OFFSET);</span>
<span class="p_add">+</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	for (i = 0; i &lt; 4; i++) {</span>
<span class="p_add">+		dprintf3(&quot;shadow[%d]{%016lx/%016lx}\nbounds[%d]{%016lx/%016lx}\n&quot;,</span>
<span class="p_add">+		       i, (unsigned long)shadow_plb[i][0], (unsigned long)shadow_plb[i][1],</span>
<span class="p_add">+		       i, (unsigned long)bounds[i*2],     ~(unsigned long)bounds[i*2+1]);</span>
<span class="p_add">+		if ((shadow_plb[i][0] != bounds[i*2]) ||</span>
<span class="p_add">+		    (shadow_plb[i][1] != ~(unsigned long)bounds[i*2+1])) {</span>
<span class="p_add">+			eprintf(&quot;ERROR comparing shadow to real bound register %d\n&quot;, i);</span>
<span class="p_add">+			eprintf(&quot;shadow{0x%016lx/0x%016lx}\nbounds{0x%016lx/0x%016lx}\n&quot;,</span>
<span class="p_add">+			       (unsigned long)shadow_plb[i][0], (unsigned long)shadow_plb[i][1],</span>
<span class="p_add">+			       (unsigned long)bounds[i*2], (unsigned long)bounds[i*2+1]);</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void mkbnd_shadow(uint8_t *ptr, int index, long offset)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint64_t *lower = (uint64_t *)&amp;(shadow_plb[index][0]);</span>
<span class="p_add">+	uint64_t *upper = (uint64_t *)&amp;(shadow_plb[index][1]);</span>
<span class="p_add">+	*lower = (unsigned long)ptr;</span>
<span class="p_add">+	*upper = (unsigned long)ptr + offset - 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void check_lowerbound_shadow(uint8_t *ptr, int index)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint64_t *lower = (uint64_t *)&amp;(shadow_plb[index][0]);</span>
<span class="p_add">+	if (*lower &gt; (uint64_t)(unsigned long)ptr)</span>
<span class="p_add">+		num_lower_brs++;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		dprintf1(&quot;LowerBoundChk passed:%p\n&quot;, ptr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void check_upperbound_shadow(uint8_t *ptr, int index)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint64_t upper = *(uint64_t *)&amp;(shadow_plb[index][1]);</span>
<span class="p_add">+	if (upper &lt; (uint64_t)(unsigned long)ptr)</span>
<span class="p_add">+		num_upper_brs++;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		dprintf1(&quot;UpperBoundChk passed:%p\n&quot;, ptr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+__always_inline void movbndreg_shadow(int src, int dest)</span>
<span class="p_add">+{</span>
<span class="p_add">+	shadow_plb[dest][0] = shadow_plb[src][0];</span>
<span class="p_add">+	shadow_plb[dest][1] = shadow_plb[src][1];</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+__always_inline void movbnd2mem_shadow(int src, unsigned long *dest)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long *lower = (unsigned long *)&amp;(shadow_plb[src][0]);</span>
<span class="p_add">+	unsigned long *upper = (unsigned long *)&amp;(shadow_plb[src][1]);</span>
<span class="p_add">+	*dest = *lower;</span>
<span class="p_add">+	*(dest+1) = *upper;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+__always_inline void movbnd_from_mem_shadow(unsigned long *src, int dest)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long *lower = (unsigned long *)&amp;(shadow_plb[dest][0]);</span>
<span class="p_add">+	unsigned long *upper = (unsigned long *)&amp;(shadow_plb[dest][1]);</span>
<span class="p_add">+	*lower = *src;</span>
<span class="p_add">+	*upper = *(src+1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+__always_inline void stdsc_shadow(int index, uint8_t *ptr, uint8_t *ptr_val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	shadow_map[0] = (unsigned long)shadow_plb[index][0];</span>
<span class="p_add">+	shadow_map[1] = (unsigned long)shadow_plb[index][1];</span>
<span class="p_add">+	shadow_map[2] = (unsigned long)ptr_val;</span>
<span class="p_add">+	dprintf3(&quot;%s(%d, %p, %p) set shadow map[2]: %p\n&quot;, __func__,</span>
<span class="p_add">+			index, ptr, ptr_val, ptr_val);</span>
<span class="p_add">+	/*ptr ignored */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void lddsc_shadow(int index, uint8_t *ptr, uint8_t *ptr_val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint64_t lower = shadow_map[0];</span>
<span class="p_add">+	uint64_t upper = shadow_map[1];</span>
<span class="p_add">+	uint8_t *value = (uint8_t *)shadow_map[2];</span>
<span class="p_add">+</span>
<span class="p_add">+	if (value != ptr_val) {</span>
<span class="p_add">+		dprintf2(&quot;%s(%d, %p, %p) init shadow bounds[%d] &quot;</span>
<span class="p_add">+			 &quot;because %p != %p\n&quot;, __func__, index, ptr,</span>
<span class="p_add">+			 ptr_val, index, value, ptr_val);</span>
<span class="p_add">+		shadow_plb[index][0] = 0;</span>
<span class="p_add">+		shadow_plb[index][1] = ~(unsigned long)0;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		shadow_plb[index][0] = lower;</span>
<span class="p_add">+		shadow_plb[index][1] = upper;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/* ptr ignored */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper0(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mpx_make_bound_helper((unsigned long)ptr, 0x1800);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper0_shadow(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mkbnd_shadow(ptr, 0, 0x1800);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper1(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* these are hard-coded to check bnd0 */</span>
<span class="p_add">+	expected_bnd_index = 0;</span>
<span class="p_add">+	mpx_check_lowerbound_helper((unsigned long)(ptr-1));</span>
<span class="p_add">+	mpx_check_upperbound_helper((unsigned long)(ptr+0x1800));</span>
<span class="p_add">+	/* reset this since we do not expect any more bounds exceptions */</span>
<span class="p_add">+	expected_bnd_index = -1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper1_shadow(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	check_lowerbound_shadow(ptr-1, 0);</span>
<span class="p_add">+	check_upperbound_shadow(ptr+0x1800, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper2(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mpx_make_bound_helper((unsigned long)ptr, 0x1800);</span>
<span class="p_add">+	mpx_movbndreg_helper();</span>
<span class="p_add">+	mpx_movbnd2mem_helper(buf);</span>
<span class="p_add">+	mpx_make_bound_helper((unsigned long)(ptr+0x12), 0x1800);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper2_shadow(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mkbnd_shadow(ptr, 0, 0x1800);</span>
<span class="p_add">+	movbndreg_shadow(0, 2);</span>
<span class="p_add">+	movbnd2mem_shadow(0, (unsigned long *)buf);</span>
<span class="p_add">+	mkbnd_shadow(ptr+0x12, 0, 0x1800);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper3(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mpx_movbnd_from_mem_helper(buf);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper3_shadow(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	movbnd_from_mem_shadow((unsigned long *)buf, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper4(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mpx_store_dsc_helper((unsigned long)buf, (unsigned long)ptr);</span>
<span class="p_add">+	mpx_make_bound_helper((unsigned long)(ptr+0x12), 0x1800);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper4_shadow(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	stdsc_shadow(0, buf, ptr);</span>
<span class="p_add">+	mkbnd_shadow(ptr+0x12, 0, 0x1800);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper5(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mpx_load_dsc_helper((unsigned long)buf, (unsigned long)ptr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static __always_inline void mpx_test_helper5_shadow(uint8_t *buf, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	lddsc_shadow(0, buf, ptr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define NR_MPX_TEST_FUNCTIONS 6</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * For compatibility reasons, MPX will clear the bounds registers</span>
<span class="p_add">+ * when you make function calls (among other things).  We have to</span>
<span class="p_add">+ * preserve the registers in between calls to the &quot;helpers&quot; since</span>
<span class="p_add">+ * they build on each other.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Be very careful not to make any function calls inside the</span>
<span class="p_add">+ * helpers, or anywhere else beween the xrstor and xsave.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define run_helper(helper_nr, buf, buf_shadow, ptr)	do {	\</span>
<span class="p_add">+	xrstor_state(xsave_test_buf, flags);			\</span>
<span class="p_add">+	mpx_test_helper##helper_nr(buf, ptr);			\</span>
<span class="p_add">+	xsave_state(xsave_test_buf, flags);			\</span>
<span class="p_add">+	mpx_test_helper##helper_nr##_shadow(buf_shadow, ptr);	\</span>
<span class="p_add">+} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+static void run_helpers(int nr, uint8_t *buf, uint8_t *buf_shadow, uint8_t *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	uint64_t flags = 0x18;</span>
<span class="p_add">+</span>
<span class="p_add">+	dprint_context(xsave_test_buf);</span>
<span class="p_add">+	switch (nr) {</span>
<span class="p_add">+	case 0:</span>
<span class="p_add">+		run_helper(0, buf, buf_shadow, ptr);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 1:</span>
<span class="p_add">+		run_helper(1, buf, buf_shadow, ptr);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 2:</span>
<span class="p_add">+		run_helper(2, buf, buf_shadow, ptr);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 3:</span>
<span class="p_add">+		run_helper(3, buf, buf_shadow, ptr);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 4:</span>
<span class="p_add">+		run_helper(4, buf, buf_shadow, ptr);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case 5:</span>
<span class="p_add">+		run_helper(5, buf, buf_shadow, ptr);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		test_failed();</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	dprint_context(xsave_test_buf);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned long buf_shadow[1024]; /* used to check load / store descriptors */</span>
<span class="p_add">+extern long inspect_me(struct mpx_bounds_dir *bounds_dir);</span>
<span class="p_add">+</span>
<span class="p_add">+long cover_buf_with_bt_entries(void *buf, long buf_len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	long nr_to_fill;</span>
<span class="p_add">+	int ratio = 1000;</span>
<span class="p_add">+	unsigned long buf_len_in_ptrs;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Fill about 1/100 of the space with bt entries */</span>
<span class="p_add">+	nr_to_fill = buf_len / (sizeof(unsigned long) * ratio);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!nr_to_fill)</span>
<span class="p_add">+		dprintf3(&quot;%s() nr_to_fill: %ld\n&quot;, __func__, nr_to_fill);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Align the buffer to pointer size */</span>
<span class="p_add">+	while (((unsigned long)buf) % sizeof(void *)) {</span>
<span class="p_add">+		buf++;</span>
<span class="p_add">+		buf_len--;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/* We are storing pointers, so make */</span>
<span class="p_add">+	buf_len_in_ptrs = buf_len / sizeof(void *);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nr_to_fill; i++) {</span>
<span class="p_add">+		long index = (mpx_random() % buf_len_in_ptrs);</span>
<span class="p_add">+		void *ptr = buf + index * sizeof(unsigned long);</span>
<span class="p_add">+		unsigned long ptr_addr = (unsigned long)ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* ptr and size can be anything */</span>
<span class="p_add">+		mpx_make_bound_helper((unsigned long)ptr, 8);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * take bnd0 and put it in to bounds tables &quot;buf + index&quot; is an</span>
<span class="p_add">+		 * address inside the buffer where we are pretending that we</span>
<span class="p_add">+		 * are going to put a pointer We do not, though because we will</span>
<span class="p_add">+		 * never load entries from the table, so it doesn&#39;t matter.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		mpx_store_dsc_helper(ptr_addr, (unsigned long)ptr);</span>
<span class="p_add">+		dprintf4(&quot;storing bound table entry for %lx (buf start @ %p)\n&quot;,</span>
<span class="p_add">+				ptr_addr, buf);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return nr_to_fill;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned long align_down(unsigned long alignme, unsigned long align_to)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return alignme &amp; ~(align_to-1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned long align_up(unsigned long alignme, unsigned long align_to)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (alignme + align_to - 1) &amp; ~(align_to-1);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Using 1MB alignment guarantees that each no allocation</span>
<span class="p_add">+ * will overlap with another&#39;s bounds tables.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We have to cook our own allocator here.  malloc() can</span>
<span class="p_add">+ * mix other allocation with ours which means that even</span>
<span class="p_add">+ * if we free all of our allocations, there might still</span>
<span class="p_add">+ * be bounds tables for the *areas* since there is other</span>
<span class="p_add">+ * valid memory there.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We also can&#39;t use malloc() because a free() of an area</span>
<span class="p_add">+ * might not free it back to the kernel.  We want it</span>
<span class="p_add">+ * completely unmapped an malloc() does not guarantee</span>
<span class="p_add">+ * that.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+long alignment = 4096;</span>
<span class="p_add">+long sz_alignment = 4096;</span>
<span class="p_add">+#else</span>
<span class="p_add">+long alignment = 1 * MB;</span>
<span class="p_add">+long sz_alignment = 1 * MB;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+void *mpx_mini_alloc(unsigned long sz)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long long tries = 0;</span>
<span class="p_add">+	static void *last;</span>
<span class="p_add">+	void *ptr;</span>
<span class="p_add">+	void *try_at;</span>
<span class="p_add">+</span>
<span class="p_add">+	sz = align_up(sz, sz_alignment);</span>
<span class="p_add">+</span>
<span class="p_add">+	try_at = last + alignment;</span>
<span class="p_add">+	while (1) {</span>
<span class="p_add">+		ptr = mmap(try_at, sz, PROT_READ|PROT_WRITE,</span>
<span class="p_add">+				MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);</span>
<span class="p_add">+		if (ptr == (void *)-1)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+		if (ptr == try_at)</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		munmap(ptr, sz);</span>
<span class="p_add">+		try_at += alignment;</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This isn&#39;t quite correct for 32-bit binaries</span>
<span class="p_add">+		 * on 64-bit kernels since they can use the</span>
<span class="p_add">+		 * entire 32-bit address space, but it&#39;s close</span>
<span class="p_add">+		 * enough.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (try_at &gt; (void *)0xC0000000)</span>
<span class="p_add">+#else</span>
<span class="p_add">+		if (try_at &gt; (void *)0x0000800000000000)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+			try_at = (void *)0x0;</span>
<span class="p_add">+		if (!(++tries % 10000))</span>
<span class="p_add">+			dprintf1(&quot;stuck in %s(), tries: %lld\n&quot;, __func__, tries);</span>
<span class="p_add">+		continue;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	last = ptr;</span>
<span class="p_add">+	dprintf3(&quot;mpx_mini_alloc(0x%lx) returning: %p\n&quot;, sz, ptr);</span>
<span class="p_add">+	return ptr;</span>
<span class="p_add">+}</span>
<span class="p_add">+void mpx_mini_free(void *ptr, long sz)</span>
<span class="p_add">+{</span>
<span class="p_add">+	dprintf2(&quot;%s() ptr: %p\n&quot;, __func__, ptr);</span>
<span class="p_add">+	if ((unsigned long)ptr &gt; 0x100000000000) {</span>
<span class="p_add">+		dprintf1(&quot;uh oh !!!!!!!!!!!!!!! pointer too high: %p\n&quot;, ptr);</span>
<span class="p_add">+		test_failed();</span>
<span class="p_add">+	}</span>
<span class="p_add">+	sz = align_up(sz, sz_alignment);</span>
<span class="p_add">+	dprintf3(&quot;%s() ptr: %p before munmap\n&quot;, __func__, ptr);</span>
<span class="p_add">+	munmap(ptr, sz);</span>
<span class="p_add">+	dprintf3(&quot;%s() ptr: %p DONE\n&quot;, __func__, ptr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define NR_MALLOCS 100</span>
<span class="p_add">+struct one_malloc {</span>
<span class="p_add">+	char *ptr;</span>
<span class="p_add">+	int nr_filled_btes;</span>
<span class="p_add">+	unsigned long size;</span>
<span class="p_add">+};</span>
<span class="p_add">+struct one_malloc mallocs[NR_MALLOCS];</span>
<span class="p_add">+</span>
<span class="p_add">+void free_one_malloc(int index)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long free_ptr;</span>
<span class="p_add">+	unsigned long mask;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!mallocs[index].ptr)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	mpx_mini_free(mallocs[index].ptr, mallocs[index].size);</span>
<span class="p_add">+	dprintf4(&quot;freed[%d]:  %p\n&quot;, index, mallocs[index].ptr);</span>
<span class="p_add">+</span>
<span class="p_add">+	free_ptr = (unsigned long)mallocs[index].ptr;</span>
<span class="p_add">+	mask = alignment-1;</span>
<span class="p_add">+	dprintf4(&quot;lowerbits: %lx / %lx mask: %lx\n&quot;, free_ptr,</span>
<span class="p_add">+			(free_ptr &amp; mask), mask);</span>
<span class="p_add">+	assert((free_ptr &amp; mask) == 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	mallocs[index].ptr = NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_COVERS 4096</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define MPX_BOUNDS_TABLE_COVERS (1 * MB)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+void zap_everything(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	long after_zap;</span>
<span class="p_add">+	long before_zap;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	before_zap = inspect_me(bounds_dir_ptr);</span>
<span class="p_add">+	dprintf1(&quot;zapping everything start: %ld\n&quot;, before_zap);</span>
<span class="p_add">+	for (i = 0; i &lt; NR_MALLOCS; i++)</span>
<span class="p_add">+		free_one_malloc(i);</span>
<span class="p_add">+</span>
<span class="p_add">+	after_zap = inspect_me(bounds_dir_ptr);</span>
<span class="p_add">+	dprintf1(&quot;zapping everything done: %ld\n&quot;, after_zap);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We only guarantee to empty the thing out if our allocations are</span>
<span class="p_add">+	 * exactly aligned on the boundaries of a boudns table.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((alignment &gt;= MPX_BOUNDS_TABLE_COVERS) &amp;&amp;</span>
<span class="p_add">+	    (sz_alignment &gt;= MPX_BOUNDS_TABLE_COVERS)) {</span>
<span class="p_add">+		if (after_zap != 0)</span>
<span class="p_add">+			test_failed();</span>
<span class="p_add">+</span>
<span class="p_add">+		assert(after_zap == 0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void do_one_malloc(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	static int malloc_counter;</span>
<span class="p_add">+	long sz;</span>
<span class="p_add">+	int rand_index = (mpx_random() % NR_MALLOCS);</span>
<span class="p_add">+	void *ptr = mallocs[rand_index].ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf3(&quot;%s() enter\n&quot;, __func__);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ptr) {</span>
<span class="p_add">+		dprintf3(&quot;freeing one malloc at index: %d\n&quot;, rand_index);</span>
<span class="p_add">+		free_one_malloc(rand_index);</span>
<span class="p_add">+		if (mpx_random() % (NR_MALLOCS*3) == 3) {</span>
<span class="p_add">+			int i;</span>
<span class="p_add">+			dprintf3(&quot;zapping some more\n&quot;);</span>
<span class="p_add">+			for (i = rand_index; i &lt; NR_MALLOCS; i++)</span>
<span class="p_add">+				free_one_malloc(i);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if ((mpx_random() % zap_all_every_this_many_mallocs) == 4)</span>
<span class="p_add">+			zap_everything();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* 1-&gt;~1M */</span>
<span class="p_add">+	sz = (1 + mpx_random() % 1000) * 1000;</span>
<span class="p_add">+	ptr = mpx_mini_alloc(sz);</span>
<span class="p_add">+	if (!ptr) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If we are failing allocations, just assume we</span>
<span class="p_add">+		 * are out of memory and zap everything.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		dprintf3(&quot;zapping everything because out of memory\n&quot;);</span>
<span class="p_add">+		zap_everything();</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf3(&quot;malloc: %p size: 0x%lx\n&quot;, ptr, sz);</span>
<span class="p_add">+	mallocs[rand_index].nr_filled_btes = cover_buf_with_bt_entries(ptr, sz);</span>
<span class="p_add">+	mallocs[rand_index].ptr = ptr;</span>
<span class="p_add">+	mallocs[rand_index].size = sz;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	if ((++malloc_counter) % inspect_every_this_many_mallocs == 0)</span>
<span class="p_add">+		inspect_me(bounds_dir_ptr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void run_timed_test(void (*test_func)(void))</span>
<span class="p_add">+{</span>
<span class="p_add">+	int done = 0;</span>
<span class="p_add">+	long iteration = 0;</span>
<span class="p_add">+	static time_t last_print;</span>
<span class="p_add">+	time_t now;</span>
<span class="p_add">+	time_t start;</span>
<span class="p_add">+</span>
<span class="p_add">+	time(&amp;start);</span>
<span class="p_add">+	while (!done) {</span>
<span class="p_add">+		time(&amp;now);</span>
<span class="p_add">+		if ((now - start) &gt; TEST_DURATION_SECS)</span>
<span class="p_add">+			done = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+		test_func();</span>
<span class="p_add">+		iteration++;</span>
<span class="p_add">+</span>
<span class="p_add">+		if ((now - last_print &gt; 1) || done) {</span>
<span class="p_add">+			printf(&quot;iteration %ld complete, OK so far\n&quot;, iteration);</span>
<span class="p_add">+			last_print = now;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void check_bounds_table_frees(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	printf(&quot;executing unmaptest\n&quot;);</span>
<span class="p_add">+	inspect_me(bounds_dir_ptr);</span>
<span class="p_add">+	run_timed_test(&amp;do_one_malloc);</span>
<span class="p_add">+	printf(&quot;done with malloc() fun\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void insn_test_failed(int test_nr, int test_round, void *buf,</span>
<span class="p_add">+		void *buf_shadow, void *ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	print_context(xsave_test_buf);</span>
<span class="p_add">+	eprintf(&quot;ERROR: test %d round %d failed\n&quot;, test_nr, test_round);</span>
<span class="p_add">+	while (test_nr == 5) {</span>
<span class="p_add">+		struct mpx_bt_entry *bte;</span>
<span class="p_add">+		struct mpx_bounds_dir *bd = (void *)bounds_dir_ptr;</span>
<span class="p_add">+		struct mpx_bd_entry *bde = mpx_vaddr_to_bd_entry(buf, bd);</span>
<span class="p_add">+</span>
<span class="p_add">+		printf(&quot;  bd: %p\n&quot;, bd);</span>
<span class="p_add">+		printf(&quot;&amp;bde: %p\n&quot;, bde);</span>
<span class="p_add">+		printf(&quot;*bde: %lx\n&quot;, *(unsigned long *)bde);</span>
<span class="p_add">+		if (!bd_entry_valid(bde))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+</span>
<span class="p_add">+		bte = mpx_vaddr_to_bt_entry(buf, bd);</span>
<span class="p_add">+		printf(&quot; te: %p\n&quot;, bte);</span>
<span class="p_add">+		printf(&quot;bte[0]: %lx\n&quot;, bte-&gt;contents[0]);</span>
<span class="p_add">+		printf(&quot;bte[1]: %lx\n&quot;, bte-&gt;contents[1]);</span>
<span class="p_add">+		printf(&quot;bte[2]: %lx\n&quot;, bte-&gt;contents[2]);</span>
<span class="p_add">+		printf(&quot;bte[3]: %lx\n&quot;, bte-&gt;contents[3]);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	test_failed();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void check_mpx_insns_and_tables(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int successes = 0;</span>
<span class="p_add">+	int failures  = 0;</span>
<span class="p_add">+	int buf_size = (1024*1024);</span>
<span class="p_add">+	unsigned long *buf = malloc(buf_size);</span>
<span class="p_add">+	const int total_nr_tests = NR_MPX_TEST_FUNCTIONS * TEST_ROUNDS;</span>
<span class="p_add">+	int i, j;</span>
<span class="p_add">+</span>
<span class="p_add">+	memset(buf, 0, buf_size);</span>
<span class="p_add">+	memset(buf_shadow, 0, sizeof(buf_shadow));</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; TEST_ROUNDS; i++) {</span>
<span class="p_add">+		uint8_t *ptr = get_random_addr() + 8;</span>
<span class="p_add">+</span>
<span class="p_add">+		for (j = 0; j &lt; NR_MPX_TEST_FUNCTIONS; j++) {</span>
<span class="p_add">+			if (0 &amp;&amp; j != 5) {</span>
<span class="p_add">+				successes++;</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			dprintf2(&quot;starting test %d round %d\n&quot;, j, i);</span>
<span class="p_add">+			dprint_context(xsave_test_buf);</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * test5 loads an address from the bounds tables.</span>
<span class="p_add">+			 * The load will only complete if &#39;ptr&#39; matches</span>
<span class="p_add">+			 * the load and the store, so with random addrs,</span>
<span class="p_add">+			 * the odds of this are very small.  Make it</span>
<span class="p_add">+			 * higher by only moving &#39;ptr&#39; 1/10 times.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (random() % 10 &lt;= 0)</span>
<span class="p_add">+				ptr = get_random_addr() + 8;</span>
<span class="p_add">+			dprintf3(&quot;random ptr{%p}\n&quot;, ptr);</span>
<span class="p_add">+			dprint_context(xsave_test_buf);</span>
<span class="p_add">+			run_helpers(j, (void *)buf, (void *)buf_shadow, ptr);</span>
<span class="p_add">+			dprint_context(xsave_test_buf);</span>
<span class="p_add">+			if (!compare_context(xsave_test_buf)) {</span>
<span class="p_add">+				insn_test_failed(j, i, buf, buf_shadow, ptr);</span>
<span class="p_add">+				failures++;</span>
<span class="p_add">+				goto exit;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			successes++;</span>
<span class="p_add">+			dprint_context(xsave_test_buf);</span>
<span class="p_add">+			dprintf2(&quot;finished test %d round %d\n&quot;, j, i);</span>
<span class="p_add">+			dprintf3(&quot;\n&quot;);</span>
<span class="p_add">+			dprint_context(xsave_test_buf);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+exit:</span>
<span class="p_add">+	dprintf2(&quot;\nabout to free:\n&quot;);</span>
<span class="p_add">+	free(buf);</span>
<span class="p_add">+	dprintf1(&quot;successes: %d\n&quot;, successes);</span>
<span class="p_add">+	dprintf1(&quot; failures: %d\n&quot;, failures);</span>
<span class="p_add">+	dprintf1(&quot;    tests: %d\n&quot;, total_nr_tests);</span>
<span class="p_add">+	dprintf1(&quot; expected: %jd #BRs\n&quot;, num_upper_brs + num_lower_brs);</span>
<span class="p_add">+	dprintf1(&quot;      saw: %d #BRs\n&quot;, br_count);</span>
<span class="p_add">+	if (failures) {</span>
<span class="p_add">+		eprintf(&quot;ERROR: non-zero number of failures\n&quot;);</span>
<span class="p_add">+		exit(20);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (successes != total_nr_tests) {</span>
<span class="p_add">+		eprintf(&quot;ERROR: succeded fewer than number of tries (%d != %d)\n&quot;,</span>
<span class="p_add">+				successes, total_nr_tests);</span>
<span class="p_add">+		exit(21);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (num_upper_brs + num_lower_brs != br_count) {</span>
<span class="p_add">+		eprintf(&quot;ERROR: unexpected number of #BRs: %jd %jd %d\n&quot;,</span>
<span class="p_add">+				num_upper_brs, num_lower_brs, br_count);</span>
<span class="p_add">+		eprintf(&quot;successes: %d\n&quot;, successes);</span>
<span class="p_add">+		eprintf(&quot; failures: %d\n&quot;, failures);</span>
<span class="p_add">+		eprintf(&quot;    tests: %d\n&quot;, total_nr_tests);</span>
<span class="p_add">+		eprintf(&quot; expected: %jd #BRs\n&quot;, num_upper_brs + num_lower_brs);</span>
<span class="p_add">+		eprintf(&quot;      saw: %d #BRs\n&quot;, br_count);</span>
<span class="p_add">+		exit(22);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is supposed to SIGSEGV nicely once the kernel</span>
<span class="p_add">+ * can no longer allocate vaddr space.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void exhaust_vaddr_space(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long ptr;</span>
<span class="p_add">+	/* Try to make sure there is no room for a bounds table anywhere */</span>
<span class="p_add">+	unsigned long skip = MPX_BOUNDS_TABLE_SIZE_BYTES - PAGE_SIZE;</span>
<span class="p_add">+#ifdef __i386__</span>
<span class="p_add">+	unsigned long max_vaddr = 0xf7788000UL;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	unsigned long max_vaddr = 0x800000000000UL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	dprintf1(&quot;%s() start\n&quot;, __func__);</span>
<span class="p_add">+	/* do not start at 0, we aren&#39;t allowed to map there */</span>
<span class="p_add">+	for (ptr = PAGE_SIZE; ptr &lt; max_vaddr; ptr += skip) {</span>
<span class="p_add">+		void *ptr_ret;</span>
<span class="p_add">+		int ret = madvise((void *)ptr, PAGE_SIZE, MADV_NORMAL);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!ret) {</span>
<span class="p_add">+			dprintf1(&quot;madvise() %lx ret: %d\n&quot;, ptr, ret);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		ptr_ret = mmap((void *)ptr, PAGE_SIZE, PROT_READ|PROT_WRITE,</span>
<span class="p_add">+				MAP_ANONYMOUS|MAP_PRIVATE, -1, 0);</span>
<span class="p_add">+		if (ptr_ret != (void *)ptr) {</span>
<span class="p_add">+			perror(&quot;mmap&quot;);</span>
<span class="p_add">+			dprintf1(&quot;mmap(%lx) ret: %p\n&quot;, ptr, ptr_ret);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (!(ptr &amp; 0xffffff))</span>
<span class="p_add">+			dprintf1(&quot;mmap(%lx) ret: %p\n&quot;, ptr, ptr_ret);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	for (ptr = PAGE_SIZE; ptr &lt; max_vaddr; ptr += skip) {</span>
<span class="p_add">+		dprintf2(&quot;covering 0x%lx with bounds table entries\n&quot;, ptr);</span>
<span class="p_add">+		cover_buf_with_bt_entries((void *)ptr, PAGE_SIZE);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	dprintf1(&quot;%s() end\n&quot;, __func__);</span>
<span class="p_add">+	printf(&quot;done with vaddr space fun\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void mpx_table_test(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	printf(&quot;starting mpx bounds table test\n&quot;);</span>
<span class="p_add">+	run_timed_test(check_mpx_insns_and_tables);</span>
<span class="p_add">+	printf(&quot;done with mpx bounds table test\n&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int main(int argc, char **argv)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int unmaptest = 0;</span>
<span class="p_add">+	int vaddrexhaust = 0;</span>
<span class="p_add">+	int tabletest = 0;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	check_mpx_support();</span>
<span class="p_add">+	mpx_prepare();</span>
<span class="p_add">+	srandom(11179);</span>
<span class="p_add">+</span>
<span class="p_add">+	bd_incore();</span>
<span class="p_add">+	init();</span>
<span class="p_add">+	bd_incore();</span>
<span class="p_add">+</span>
<span class="p_add">+	trace_me();</span>
<span class="p_add">+</span>
<span class="p_add">+	xsave_state((void *)xsave_test_buf, 0x1f);</span>
<span class="p_add">+	if (!compare_context(xsave_test_buf))</span>
<span class="p_add">+		printf(&quot;Init failed\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 1; i &lt; argc; i++) {</span>
<span class="p_add">+		if (!strcmp(argv[i], &quot;unmaptest&quot;))</span>
<span class="p_add">+			unmaptest = 1;</span>
<span class="p_add">+		if (!strcmp(argv[i], &quot;vaddrexhaust&quot;))</span>
<span class="p_add">+			vaddrexhaust = 1;</span>
<span class="p_add">+		if (!strcmp(argv[i], &quot;tabletest&quot;))</span>
<span class="p_add">+			tabletest = 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (!(unmaptest || vaddrexhaust || tabletest)) {</span>
<span class="p_add">+		unmaptest = 1;</span>
<span class="p_add">+		/* vaddrexhaust = 1; */</span>
<span class="p_add">+		tabletest = 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (unmaptest)</span>
<span class="p_add">+		check_bounds_table_frees();</span>
<span class="p_add">+	if (tabletest)</span>
<span class="p_add">+		mpx_table_test();</span>
<span class="p_add">+	if (vaddrexhaust)</span>
<span class="p_add">+		exhaust_vaddr_space();</span>
<span class="p_add">+	printf(&quot;%s completed successfully\n&quot;, argv[0]);</span>
<span class="p_add">+	exit(0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;mpx-dig.c&quot;</span>
<span class="p_header">diff --git a/tools/testing/selftests/x86/mpx-mm.h b/tools/testing/selftests/x86/mpx-mm.h</span>
new file mode 100644
<span class="p_header">index 000000000000..af706a5398f7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/mpx-mm.h</span>
<span class="p_chunk">@@ -0,0 +1,9 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _MPX_MM_H</span>
<span class="p_add">+#define _MPX_MM_H</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_SIZE 4096</span>
<span class="p_add">+#define MB (1UL&lt;&lt;20)</span>
<span class="p_add">+</span>
<span class="p_add">+extern long nr_incore(void *ptr, unsigned long size_bytes);</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _MPX_MM_H */</span>
<span class="p_header">diff --git a/tools/testing/selftests/x86/test_mremap_vdso.c b/tools/testing/selftests/x86/test_mremap_vdso.c</span>
new file mode 100644
<span class="p_header">index 000000000000..bf0d687c7db7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/test_mremap_vdso.c</span>
<span class="p_chunk">@@ -0,0 +1,111 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * 32-bit test to check vDSO mremap.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (c) 2016 Dmitry Safonov</span>
<span class="p_add">+ * Suggested-by: Andrew Lutomirski</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms and conditions of the GNU General Public License,</span>
<span class="p_add">+ * version 2, as published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope it will be useful, but</span>
<span class="p_add">+ * WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU</span>
<span class="p_add">+ * General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Can be built statically:</span>
<span class="p_add">+ * gcc -Os -Wall -static -m32 test_mremap_vdso.c</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define _GNU_SOURCE</span>
<span class="p_add">+#include &lt;stdio.h&gt;</span>
<span class="p_add">+#include &lt;errno.h&gt;</span>
<span class="p_add">+#include &lt;unistd.h&gt;</span>
<span class="p_add">+#include &lt;string.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;sys/mman.h&gt;</span>
<span class="p_add">+#include &lt;sys/auxv.h&gt;</span>
<span class="p_add">+#include &lt;sys/syscall.h&gt;</span>
<span class="p_add">+#include &lt;sys/wait.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define PAGE_SIZE	4096</span>
<span class="p_add">+</span>
<span class="p_add">+static int try_to_remap(void *vdso_addr, unsigned long size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	void *dest_addr, *new_addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Searching for memory location where to remap */</span>
<span class="p_add">+	dest_addr = mmap(0, size, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);</span>
<span class="p_add">+	if (dest_addr == MAP_FAILED) {</span>
<span class="p_add">+		printf(&quot;[WARN]\tmmap failed (%d): %m\n&quot;, errno);</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	printf(&quot;[NOTE]\tMoving vDSO: [%p, %#lx] -&gt; [%p, %#lx]\n&quot;,</span>
<span class="p_add">+		vdso_addr, (unsigned long)vdso_addr + size,</span>
<span class="p_add">+		dest_addr, (unsigned long)dest_addr + size);</span>
<span class="p_add">+	fflush(stdout);</span>
<span class="p_add">+</span>
<span class="p_add">+	new_addr = mremap(vdso_addr, size, size,</span>
<span class="p_add">+			MREMAP_FIXED|MREMAP_MAYMOVE, dest_addr);</span>
<span class="p_add">+	if ((unsigned long)new_addr == (unsigned long)-1) {</span>
<span class="p_add">+		munmap(dest_addr, size);</span>
<span class="p_add">+		if (errno == EINVAL) {</span>
<span class="p_add">+			printf(&quot;[NOTE]\tvDSO partial move failed, will try with bigger size\n&quot;);</span>
<span class="p_add">+			return -1; /* Retry with larger */</span>
<span class="p_add">+		}</span>
<span class="p_add">+		printf(&quot;[FAIL]\tmremap failed (%d): %m\n&quot;, errno);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int main(int argc, char **argv, char **envp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pid_t child;</span>
<span class="p_add">+</span>
<span class="p_add">+	child = fork();</span>
<span class="p_add">+	if (child == -1) {</span>
<span class="p_add">+		printf(&quot;[WARN]\tfailed to fork (%d): %m\n&quot;, errno);</span>
<span class="p_add">+		return 1;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (child == 0) {</span>
<span class="p_add">+		unsigned long vdso_size = PAGE_SIZE;</span>
<span class="p_add">+		unsigned long auxval;</span>
<span class="p_add">+		int ret = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+		auxval = getauxval(AT_SYSINFO_EHDR);</span>
<span class="p_add">+		printf(&quot;\tAT_SYSINFO_EHDR is %#lx\n&quot;, auxval);</span>
<span class="p_add">+		if (!auxval || auxval == -ENOENT) {</span>
<span class="p_add">+			printf(&quot;[WARN]\tgetauxval failed\n&quot;);</span>
<span class="p_add">+			return 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Simpler than parsing ELF header */</span>
<span class="p_add">+		while (ret &lt; 0) {</span>
<span class="p_add">+			ret = try_to_remap((void *)auxval, vdso_size);</span>
<span class="p_add">+			vdso_size += PAGE_SIZE;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Glibc is likely to explode now - exit with raw syscall */</span>
<span class="p_add">+		asm volatile (&quot;int $0x80&quot; : : &quot;a&quot; (__NR_exit), &quot;b&quot; (!!ret));</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		int status;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (waitpid(child, &amp;status, 0) != child ||</span>
<span class="p_add">+			!WIFEXITED(status)) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tmremap() of the vDSO does not work on this kernel!\n&quot;);</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+		} else if (WEXITSTATUS(status) != 0) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tChild failed with %d\n&quot;,</span>
<span class="p_add">+					WEXITSTATUS(status));</span>
<span class="p_add">+			return 1;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		printf(&quot;[OK]\n&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



