
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V4,4/6] mm: reclaim MADV_FREE pages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V4,4/6] mm: reclaim MADV_FREE pages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 22, 2017, 6:50 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;94eccf0fcf927f31377a60d7a9f900b7e743fb06.1487788131.git.shli@fb.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9587373/mbox/"
   >mbox</a>
|
   <a href="/patch/9587373/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9587373/">/patch/9587373/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	49D6D6020B for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 22 Feb 2017 18:53:11 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3B13528648
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 22 Feb 2017 18:53:11 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 2FDC42864F; Wed, 22 Feb 2017 18:53:11 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 8C3BF28648
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 22 Feb 2017 18:53:10 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933395AbdBVSw1 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 22 Feb 2017 13:52:27 -0500
Received: from mx0b-00082601.pphosted.com ([67.231.153.30]:40087 &quot;EHLO
	mx0b-00082601.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1754909AbdBVSvX (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 22 Feb 2017 13:51:23 -0500
Received: from pps.filterd (m0109331.ppops.net [127.0.0.1])
	by mx0a-00082601.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v1MImI9X021075
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 22 Feb 2017 10:50:47 -0800
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=fb.com;
	h=from : to : cc : subject
	: date : message-id : in-reply-to : references : mime-version :
	content-type; s=facebook;
	bh=FXDaRamDekNKHQc3imlgGz3IL5It96Qd/ojvE8vOMOw=; 
	b=pnCbhV8YlOaSewA+0wclI71DBMlgLKX4Y+uaJwNbAysUSdy6/0B8p6yiG9AFL9xb2Iw8
	XajdvZl1aEbBsEB++KI/948nKCabsKRF1WP3b5SFpm6Ij41R8xa8qHLKOchSrFe/XVij
	r8asycgb9x2SFSupO3903Ib7rbJst6WtHRk= 
Received: from mail.thefacebook.com ([199.201.64.23])
	by mx0a-00082601.pphosted.com with ESMTP id 28sbwh0yhm-5
	(version=TLSv1 cipher=ECDHE-RSA-AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Wed, 22 Feb 2017 10:50:47 -0800
Received: from mx-out.facebook.com (192.168.52.123) by
	PRN-CHUB14.TheFacebook.com (192.168.16.24) with Microsoft SMTP Server
	(TLS) id 14.3.294.0; Wed, 22 Feb 2017 10:50:45 -0800
Received: from facebook.com (2401:db00:21:603d:face:0:19:0)     by
	mx-out.facebook.com (10.223.100.97) with ESMTP id
	d1570f34f92f11e6938a24be0593f280-8d5fca00 for
	&lt;linux-kernel@vger.kernel.org&gt;; Wed, 22 Feb 2017 10:50:45 -0800
Received: by devbig638.prn2.facebook.com (Postfix, from userid 11222)   id
	4E22D42A8020; Wed, 22 Feb 2017 10:50:44 -0800 (PST)
Smtp-Origin-Hostprefix: devbig
From: Shaohua Li &lt;shli@fb.com&gt;
Smtp-Origin-Hostname: devbig638.prn2.facebook.com
To: &lt;linux-mm@kvack.org&gt;, &lt;linux-kernel@vger.kernel.org&gt;
CC: &lt;Kernel-team@fb.com&gt;, &lt;mhocko@suse.com&gt;, &lt;minchan@kernel.org&gt;,
	&lt;hughd@google.com&gt;, &lt;hannes@cmpxchg.org&gt;, &lt;riel@redhat.com&gt;,
	&lt;mgorman@techsingularity.net&gt;, &lt;akpm@linux-foundation.org&gt;
Smtp-Origin-Cluster: prn2c22
Subject: [PATCH V4 4/6] mm: reclaim MADV_FREE pages
Date: Wed, 22 Feb 2017 10:50:42 -0800
Message-ID: &lt;94eccf0fcf927f31377a60d7a9f900b7e743fb06.1487788131.git.shli@fb.com&gt;
X-Mailer: git-send-email 2.9.3
In-Reply-To: &lt;cover.1487788131.git.shli@fb.com&gt;
References: &lt;cover.1487788131.git.shli@fb.com&gt;
X-FB-Internal: Safe
MIME-Version: 1.0
Content-Type: text/plain
X-Proofpoint-Spam-Reason: safe
X-FB-Internal: Safe
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-02-22_12:, , signatures=0
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 22, 2017, 6:50 p.m.</div>
<pre class="content">
When memory pressure is high, we free MADV_FREE pages. If the pages are
not dirty in pte, the pages could be freed immediately. Otherwise we
can&#39;t reclaim them. We put the pages back to anonumous LRU list (by
setting SwapBacked flag) and the pages will be reclaimed in normal
swapout way.

We use normal page reclaim policy. Since MADV_FREE pages are put into
inactive file list, such pages and inactive file pages are reclaimed
according to their age. This is expected, because we don&#39;t want to
reclaim too many MADV_FREE pages before used once pages.

Based on Minchan&#39;s original patch

Cc: Michal Hocko &lt;mhocko@suse.com&gt;
Cc: Minchan Kim &lt;minchan@kernel.org&gt;
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;
Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
<span class="signed-off-by">Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
---
 include/linux/rmap.h |  2 +-
 mm/huge_memory.c     |  2 ++
 mm/madvise.c         |  1 +
 mm/rmap.c            | 10 ++++++++--
 mm/vmscan.c          | 34 ++++++++++++++++++++++------------
 5 files changed, 34 insertions(+), 15 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Feb. 23, 2017, 4:13 p.m.</div>
<pre class="content">
On Wed, Feb 22, 2017 at 10:50:42AM -0800, Shaohua Li wrote:
<span class="quote">&gt; @@ -1424,6 +1424,12 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt;  				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt;  				goto discard;</span>
<span class="quote">&gt; +			} else if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt; +				/* dirty MADV_FREE page */</span>
<span class="quote">&gt; +				set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; +				ret = SWAP_DIRTY;</span>
<span class="quote">&gt; +				page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; +				break;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			if (swap_duplicate(entry) &lt; 0) {</span>
<span class="quote">&gt; @@ -1525,8 +1531,8 @@ int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page)) {</span>
<span class="quote">&gt;  		ret = SWAP_SUCCESS;</span>
<span class="quote">&gt; -		if (rp.lazyfreed &amp;&amp; !PageDirty(page))</span>
<span class="quote">&gt; -			ret = SWAP_LZFREE;</span>
<span class="quote">&gt; +		if (rp.lazyfreed &amp;&amp; PageDirty(page))</span>
<span class="quote">&gt; +			ret = SWAP_DIRTY;</span>

Can this actually happen? If the page is dirty, ret should already be
SWAP_DIRTY, right? How would a dirty page get fully unmapped?

It seems to me rp.lazyfreed can be removed entirely now that we don&#39;t
have to identify the lazyfree case anymore. The failure case is much
easier to identify - all it takes is a single pte to be dirty.
<span class="quote">
&gt; @@ -1118,8 +1120,10 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Anonymous process memory has backing store?</span>
<span class="quote">&gt;  		 * Try to allocate it some swap space here.</span>
<span class="quote">&gt; +		 * Lazyfree page could be freed directly</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="quote">&gt; +		if (PageAnon(page) &amp;&amp; !PageSwapCache(page) &amp;&amp;</span>
<span class="quote">&gt; +		    PageSwapBacked(page)) {</span>

Nit: I&#39;d do PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp; !PageSwapCache()
since anon &amp;&amp; swapbacked together describe the page type and swapcache
the state. Plus, anon &amp;&amp; swapbacked go together everywhere else.

Otherwise, looks very straight-forward!

Thanks
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 23, 2017, 5:19 p.m.</div>
<pre class="content">
On Thu, Feb 23, 2017 at 11:13:42AM -0500, Johannes Weiner wrote:
<span class="quote">&gt; On Wed, Feb 22, 2017 at 10:50:42AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; @@ -1424,6 +1424,12 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt;  				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt;  				goto discard;</span>
<span class="quote">&gt; &gt; +			} else if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt; &gt; +				/* dirty MADV_FREE page */</span>
<span class="quote">&gt; &gt; +				set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; &gt; +				ret = SWAP_DIRTY;</span>
<span class="quote">&gt; &gt; +				page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt; +				break;</span>
<span class="quote">&gt; &gt;  			}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  			if (swap_duplicate(entry) &lt; 0) {</span>
<span class="quote">&gt; &gt; @@ -1525,8 +1531,8 @@ int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page)) {</span>
<span class="quote">&gt; &gt;  		ret = SWAP_SUCCESS;</span>
<span class="quote">&gt; &gt; -		if (rp.lazyfreed &amp;&amp; !PageDirty(page))</span>
<span class="quote">&gt; &gt; -			ret = SWAP_LZFREE;</span>
<span class="quote">&gt; &gt; +		if (rp.lazyfreed &amp;&amp; PageDirty(page))</span>
<span class="quote">&gt; &gt; +			ret = SWAP_DIRTY;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can this actually happen? If the page is dirty, ret should already be</span>
<span class="quote">&gt; SWAP_DIRTY, right? How would a dirty page get fully unmapped?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It seems to me rp.lazyfreed can be removed entirely now that we don&#39;t</span>
<span class="quote">&gt; have to identify the lazyfree case anymore. The failure case is much</span>
<span class="quote">&gt; easier to identify - all it takes is a single pte to be dirty.</span>

ok, I get mixed up. Yes, this couldn&#39;t happen any more since we changed the
behavior of try_to_unmap_one. Will delete this in next post.

Thanks,
Shaohua
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Feb. 24, 2017, 2:12 a.m.</div>
<pre class="content">
On Wed, Feb 22, 2017 at 10:50:42AM -0800, Shaohua Li wrote:
<span class="quote">&gt; When memory pressure is high, we free MADV_FREE pages. If the pages are</span>
<span class="quote">&gt; not dirty in pte, the pages could be freed immediately. Otherwise we</span>
<span class="quote">&gt; can&#39;t reclaim them. We put the pages back to anonumous LRU list (by</span>
<span class="quote">&gt; setting SwapBacked flag) and the pages will be reclaimed in normal</span>
<span class="quote">&gt; swapout way.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We use normal page reclaim policy. Since MADV_FREE pages are put into</span>
<span class="quote">&gt; inactive file list, such pages and inactive file pages are reclaimed</span>
<span class="quote">&gt; according to their age. This is expected, because we don&#39;t want to</span>
<span class="quote">&gt; reclaim too many MADV_FREE pages before used once pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Based on Minchan&#39;s original patch</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/rmap.h |  2 +-</span>
<span class="quote">&gt;  mm/huge_memory.c     |  2 ++</span>
<span class="quote">&gt;  mm/madvise.c         |  1 +</span>
<span class="quote">&gt;  mm/rmap.c            | 10 ++++++++--</span>
<span class="quote">&gt;  mm/vmscan.c          | 34 ++++++++++++++++++++++------------</span>
<span class="quote">&gt;  5 files changed, 34 insertions(+), 15 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="quote">&gt; index e2cd8f9..2bfd8c6 100644</span>
<span class="quote">&gt; --- a/include/linux/rmap.h</span>
<span class="quote">&gt; +++ b/include/linux/rmap.h</span>
<span class="quote">&gt; @@ -300,6 +300,6 @@ static inline int page_mkclean(struct page *page)</span>
<span class="quote">&gt;  #define SWAP_AGAIN	1</span>
<span class="quote">&gt;  #define SWAP_FAIL	2</span>
<span class="quote">&gt;  #define SWAP_MLOCK	3</span>
<span class="quote">&gt; -#define SWAP_LZFREE	4</span>
<span class="quote">&gt; +#define SWAP_DIRTY	4</span>

Could you write down about SWAP_DIRTY in try_to_unmap&#39;s description?

&lt; snip &gt;
<span class="quote">
&gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; index c621088..083f32e 100644</span>
<span class="quote">&gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; @@ -1424,6 +1424,12 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt;  				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt;  				goto discard;</span>
<span class="quote">&gt; +			} else if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt; +				/* dirty MADV_FREE page */</span>
<span class="quote">&gt; +				set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; +				ret = SWAP_DIRTY;</span>
<span class="quote">&gt; +				page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; +				break;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			if (swap_duplicate(entry) &lt; 0) {</span>
<span class="quote">&gt; @@ -1525,8 +1531,8 @@ int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page)) {</span>
<span class="quote">&gt;  		ret = SWAP_SUCCESS;</span>
<span class="quote">&gt; -		if (rp.lazyfreed &amp;&amp; !PageDirty(page))</span>
<span class="quote">&gt; -			ret = SWAP_LZFREE;</span>
<span class="quote">&gt; +		if (rp.lazyfreed &amp;&amp; PageDirty(page))</span>
<span class="quote">&gt; +			ret = SWAP_DIRTY;</span>

Hmm, I don&#39;t understand why we need to introduce new return value.
Can&#39;t we set SetPageSwapBacked and return SWAP_FAIL in try_to_unmap_one?
<span class="quote">
&gt;  	}</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt; index 68ea50d..830981a 100644</span>
<span class="quote">&gt; --- a/mm/vmscan.c</span>
<span class="quote">&gt; +++ b/mm/vmscan.c</span>

&lt; snip &gt;
<span class="quote">&gt;  			goto keep_locked;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* Double the slab pressure for mapped and swapcache pages */</span>
<span class="quote">&gt; -		if (page_mapped(page) || PageSwapCache(page))</span>
<span class="quote">&gt; +		if ((page_mapped(page) || PageSwapCache(page)) &amp;&amp;</span>
<span class="quote">&gt; +		    !(PageAnon(page) &amp;&amp; !PageSwapBacked(page)))</span>
<span class="quote">&gt;  			sc-&gt;nr_scanned++;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		may_enter_fs = (sc-&gt;gfp_mask &amp; __GFP_FS) ||</span>
<span class="quote">&gt; @@ -1118,8 +1120,10 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Anonymous process memory has backing store?</span>
<span class="quote">&gt;  		 * Try to allocate it some swap space here.</span>
<span class="quote">&gt; +		 * Lazyfree page could be freed directly</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="quote">&gt; +		if (PageAnon(page) &amp;&amp; !PageSwapCache(page) &amp;&amp;</span>
<span class="quote">&gt; +		    PageSwapBacked(page)) {</span>
<span class="quote">&gt;  			if (!(sc-&gt;gfp_mask &amp; __GFP_IO))</span>
<span class="quote">&gt;  				goto keep_locked;</span>
<span class="quote">&gt;  			if (!add_to_swap(page, page_list))</span>
<span class="quote">&gt; @@ -1140,9 +1144,12 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		 * The page is mapped into the page tables of one or more</span>
<span class="quote">&gt;  		 * processes. Try to unmap it here.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="quote">&gt; +		if (page_mapped(page)) {</span>
<span class="quote">&gt;  			switch (ret = try_to_unmap(page,</span>
<span class="quote">&gt;  				ttu_flags | TTU_BATCH_FLUSH)) {</span>
<span class="quote">&gt; +			case SWAP_DIRTY:</span>
<span class="quote">&gt; +				SetPageSwapBacked(page);</span>
<span class="quote">&gt; +				/* fall through */</span>
<span class="quote">&gt;  			case SWAP_FAIL:</span>
<span class="quote">&gt;  				nr_unmap_fail++;</span>
<span class="quote">&gt;  				goto activate_locked;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 24, 2017, 6:14 a.m.</div>
<pre class="content">
On Fri, Feb 24, 2017 at 11:12:18AM +0900, Minchan Kim wrote:
<span class="quote">&gt; On Wed, Feb 22, 2017 at 10:50:42AM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; When memory pressure is high, we free MADV_FREE pages. If the pages are</span>
<span class="quote">&gt; &gt; not dirty in pte, the pages could be freed immediately. Otherwise we</span>
<span class="quote">&gt; &gt; can&#39;t reclaim them. We put the pages back to anonumous LRU list (by</span>
<span class="quote">&gt; &gt; setting SwapBacked flag) and the pages will be reclaimed in normal</span>
<span class="quote">&gt; &gt; swapout way.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We use normal page reclaim policy. Since MADV_FREE pages are put into</span>
<span class="quote">&gt; &gt; inactive file list, such pages and inactive file pages are reclaimed</span>
<span class="quote">&gt; &gt; according to their age. This is expected, because we don&#39;t want to</span>
<span class="quote">&gt; &gt; reclaim too many MADV_FREE pages before used once pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Based on Minchan&#39;s original patch</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; &gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; &gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; &gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  include/linux/rmap.h |  2 +-</span>
<span class="quote">&gt; &gt;  mm/huge_memory.c     |  2 ++</span>
<span class="quote">&gt; &gt;  mm/madvise.c         |  1 +</span>
<span class="quote">&gt; &gt;  mm/rmap.c            | 10 ++++++++--</span>
<span class="quote">&gt; &gt;  mm/vmscan.c          | 34 ++++++++++++++++++++++------------</span>
<span class="quote">&gt; &gt;  5 files changed, 34 insertions(+), 15 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; index e2cd8f9..2bfd8c6 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; @@ -300,6 +300,6 @@ static inline int page_mkclean(struct page *page)</span>
<span class="quote">&gt; &gt;  #define SWAP_AGAIN	1</span>
<span class="quote">&gt; &gt;  #define SWAP_FAIL	2</span>
<span class="quote">&gt; &gt;  #define SWAP_MLOCK	3</span>
<span class="quote">&gt; &gt; -#define SWAP_LZFREE	4</span>
<span class="quote">&gt; &gt; +#define SWAP_DIRTY	4</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Could you write down about SWAP_DIRTY in try_to_unmap&#39;s description?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &lt; snip &gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; &gt; index c621088..083f32e 100644</span>
<span class="quote">&gt; &gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; &gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; &gt; @@ -1424,6 +1424,12 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;  				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; &gt;  				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt; &gt;  				goto discard;</span>
<span class="quote">&gt; &gt; +			} else if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt; &gt; +				/* dirty MADV_FREE page */</span>
<span class="quote">&gt; &gt; +				set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; &gt; +				ret = SWAP_DIRTY;</span>
<span class="quote">&gt; &gt; +				page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; &gt; +				break;</span>
<span class="quote">&gt; &gt;  			}</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  			if (swap_duplicate(entry) &lt; 0) {</span>
<span class="quote">&gt; &gt; @@ -1525,8 +1531,8 @@ int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page)) {</span>
<span class="quote">&gt; &gt;  		ret = SWAP_SUCCESS;</span>
<span class="quote">&gt; &gt; -		if (rp.lazyfreed &amp;&amp; !PageDirty(page))</span>
<span class="quote">&gt; &gt; -			ret = SWAP_LZFREE;</span>
<span class="quote">&gt; &gt; +		if (rp.lazyfreed &amp;&amp; PageDirty(page))</span>
<span class="quote">&gt; &gt; +			ret = SWAP_DIRTY;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm, I don&#39;t understand why we need to introduce new return value.</span>
<span class="quote">&gt; Can&#39;t we set SetPageSwapBacked and return SWAP_FAIL in try_to_unmap_one?</span>

Original idea in my mind is to activate page in SWAP_DIRTY but not activate
page in SWAP_FAIL for other failures. But later we choose to ignore all corner
cases and always activate pages for all failures. So you are right, we don&#39;t
need the new return value right now.

Thanks,
Shaohua
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Feb. 24, 2017, 3:36 p.m.</div>
<pre class="content">
On Fri, Feb 24, 2017 at 11:12:18AM +0900, Minchan Kim wrote:
<span class="quote">&gt; &gt; @@ -1525,8 +1531,8 @@ int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page)) {</span>
<span class="quote">&gt; &gt;  		ret = SWAP_SUCCESS;</span>
<span class="quote">&gt; &gt; -		if (rp.lazyfreed &amp;&amp; !PageDirty(page))</span>
<span class="quote">&gt; &gt; -			ret = SWAP_LZFREE;</span>
<span class="quote">&gt; &gt; +		if (rp.lazyfreed &amp;&amp; PageDirty(page))</span>
<span class="quote">&gt; &gt; +			ret = SWAP_DIRTY;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm, I don&#39;t understand why we need to introduce new return value.</span>
<span class="quote">&gt; Can&#39;t we set SetPageSwapBacked and return SWAP_FAIL in try_to_unmap_one?</span>

I think that&#39;s a bad idea. A function called &quot;try_to_unmap&quot; shouldn&#39;t
have as a side effect that it changes the page&#39;s LRU type in an error
case. Let try_to_unmap be about unmapping the page. If it fails, make
it report why and let the caller deal with the fallout. Any LRU fixups
are much better placed in vmscan.c.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Feb. 24, 2017, 11:26 p.m.</div>
<pre class="content">
Hi Johannes,

On Fri, Feb 24, 2017 at 10:36:55AM -0500, Johannes Weiner wrote:
<span class="quote">&gt; On Fri, Feb 24, 2017 at 11:12:18AM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; &gt; @@ -1525,8 +1531,8 @@ int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
<span class="quote">&gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt;  	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page)) {</span>
<span class="quote">&gt; &gt; &gt;  		ret = SWAP_SUCCESS;</span>
<span class="quote">&gt; &gt; &gt; -		if (rp.lazyfreed &amp;&amp; !PageDirty(page))</span>
<span class="quote">&gt; &gt; &gt; -			ret = SWAP_LZFREE;</span>
<span class="quote">&gt; &gt; &gt; +		if (rp.lazyfreed &amp;&amp; PageDirty(page))</span>
<span class="quote">&gt; &gt; &gt; +			ret = SWAP_DIRTY;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Hmm, I don&#39;t understand why we need to introduce new return value.</span>
<span class="quote">&gt; &gt; Can&#39;t we set SetPageSwapBacked and return SWAP_FAIL in try_to_unmap_one?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think that&#39;s a bad idea. A function called &quot;try_to_unmap&quot; shouldn&#39;t</span>
<span class="quote">&gt; have as a side effect that it changes the page&#39;s LRU type in an error</span>
<span class="quote">&gt; case. Let try_to_unmap be about unmapping the page. If it fails, make</span>
<span class="quote">&gt; it report why and let the caller deal with the fallout. Any LRU fixups</span>
<span class="quote">&gt; are much better placed in vmscan.c.</span>

I don&#39;t think it&#39;s page&#39;s LRU type change. SetPageSwapBacked is just
indication that page is swappable or not.
Like mlock_vma_page in try_to_unmap_one, we can set SetPageSwapBacked
if we found the lazyfree page dirty. If we don&#39;t need to move dirty
lazyfree page to another LRU list, it would be better to not introduce
new return value in try_to_unmap.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index e2cd8f9..2bfd8c6 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -300,6 +300,6 @@</span> <span class="p_context"> static inline int page_mkclean(struct page *page)</span>
 #define SWAP_AGAIN	1
 #define SWAP_FAIL	2
 #define SWAP_MLOCK	3
<span class="p_del">-#define SWAP_LZFREE	4</span>
<span class="p_add">+#define SWAP_DIRTY	4</span>
 
 #endif	/* _LINUX_RMAP_H */
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 3b7ee0c..4c7454b 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1571,6 +1571,8 @@</span> <span class="p_context"> bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		set_pmd_at(mm, addr, pmd, orig_pmd);
 		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);
 	}
<span class="p_add">+</span>
<span class="p_add">+	mark_page_lazyfree(page);</span>
 	ret = true;
 out:
 	spin_unlock(ptl);
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 61e10b1..225af7d 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -413,6 +413,7 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 			set_pte_at(mm, addr, pte, ptent);
 			tlb_remove_tlb_entry(tlb, pte, addr);
 		}
<span class="p_add">+		mark_page_lazyfree(page);</span>
 	}
 out:
 	if (nr_swap) {
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index c621088..083f32e 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1424,6 +1424,12 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 				dec_mm_counter(mm, MM_ANONPAGES);
 				rp-&gt;lazyfreed++;
 				goto discard;
<span class="p_add">+			} else if (!PageSwapBacked(page)) {</span>
<span class="p_add">+				/* dirty MADV_FREE page */</span>
<span class="p_add">+				set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="p_add">+				ret = SWAP_DIRTY;</span>
<span class="p_add">+				page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="p_add">+				break;</span>
 			}
 
 			if (swap_duplicate(entry) &lt; 0) {
<span class="p_chunk">@@ -1525,8 +1531,8 @@</span> <span class="p_context"> int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
 
 	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page)) {
 		ret = SWAP_SUCCESS;
<span class="p_del">-		if (rp.lazyfreed &amp;&amp; !PageDirty(page))</span>
<span class="p_del">-			ret = SWAP_LZFREE;</span>
<span class="p_add">+		if (rp.lazyfreed &amp;&amp; PageDirty(page))</span>
<span class="p_add">+			ret = SWAP_DIRTY;</span>
 	}
 	return ret;
 }
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 68ea50d..830981a 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -911,7 +911,8 @@</span> <span class="p_context"> static void page_check_dirty_writeback(struct page *page,</span>
 	 * Anonymous pages are not handled by flushers and must be written
 	 * from reclaim context. Do not stall reclaim based on them
 	 */
<span class="p_del">-	if (!page_is_file_cache(page)) {</span>
<span class="p_add">+	if (!page_is_file_cache(page) ||</span>
<span class="p_add">+	    (PageAnon(page) &amp;&amp; !PageSwapBacked(page))) {</span>
 		*dirty = false;
 		*writeback = false;
 		return;
<span class="p_chunk">@@ -992,7 +993,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 			goto keep_locked;
 
 		/* Double the slab pressure for mapped and swapcache pages */
<span class="p_del">-		if (page_mapped(page) || PageSwapCache(page))</span>
<span class="p_add">+		if ((page_mapped(page) || PageSwapCache(page)) &amp;&amp;</span>
<span class="p_add">+		    !(PageAnon(page) &amp;&amp; !PageSwapBacked(page)))</span>
 			sc-&gt;nr_scanned++;
 
 		may_enter_fs = (sc-&gt;gfp_mask &amp; __GFP_FS) ||
<span class="p_chunk">@@ -1118,8 +1120,10 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		/*
 		 * Anonymous process memory has backing store?
 		 * Try to allocate it some swap space here.
<span class="p_add">+		 * Lazyfree page could be freed directly</span>
 		 */
<span class="p_del">-		if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="p_add">+		if (PageAnon(page) &amp;&amp; !PageSwapCache(page) &amp;&amp;</span>
<span class="p_add">+		    PageSwapBacked(page)) {</span>
 			if (!(sc-&gt;gfp_mask &amp; __GFP_IO))
 				goto keep_locked;
 			if (!add_to_swap(page, page_list))
<span class="p_chunk">@@ -1140,9 +1144,12 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 * The page is mapped into the page tables of one or more
 		 * processes. Try to unmap it here.
 		 */
<span class="p_del">-		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="p_add">+		if (page_mapped(page)) {</span>
 			switch (ret = try_to_unmap(page,
 				ttu_flags | TTU_BATCH_FLUSH)) {
<span class="p_add">+			case SWAP_DIRTY:</span>
<span class="p_add">+				SetPageSwapBacked(page);</span>
<span class="p_add">+				/* fall through */</span>
 			case SWAP_FAIL:
 				nr_unmap_fail++;
 				goto activate_locked;
<span class="p_chunk">@@ -1150,8 +1157,6 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 				goto keep_locked;
 			case SWAP_MLOCK:
 				goto cull_mlocked;
<span class="p_del">-			case SWAP_LZFREE:</span>
<span class="p_del">-				goto lazyfree;</span>
 			case SWAP_SUCCESS:
 				; /* try to free the page below */
 			}
<span class="p_chunk">@@ -1263,10 +1268,18 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 			}
 		}
 
<span class="p_del">-lazyfree:</span>
<span class="p_del">-		if (!mapping || !__remove_mapping(mapping, page, true))</span>
<span class="p_del">-			goto keep_locked;</span>
<span class="p_add">+		if (PageAnon(page) &amp;&amp; !PageSwapBacked(page)) {</span>
<span class="p_add">+			/* follow __remove_mapping for reference */</span>
<span class="p_add">+			if (!page_ref_freeze(page, 1))</span>
<span class="p_add">+				goto keep_locked;</span>
<span class="p_add">+			if (PageDirty(page)) {</span>
<span class="p_add">+				page_ref_unfreeze(page, 1);</span>
<span class="p_add">+				goto keep_locked;</span>
<span class="p_add">+			}</span>
 
<span class="p_add">+			count_vm_event(PGLAZYFREED);</span>
<span class="p_add">+		} else if (!mapping || !__remove_mapping(mapping, page, true))</span>
<span class="p_add">+			goto keep_locked;</span>
 		/*
 		 * At this point, we have no other references and there is
 		 * no way to pick any more up (removed from LRU, removed
<span class="p_chunk">@@ -1276,9 +1289,6 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 */
 		__ClearPageLocked(page);
 free_it:
<span class="p_del">-		if (ret == SWAP_LZFREE)</span>
<span class="p_del">-			count_vm_event(PGLAZYFREED);</span>
<span class="p_del">-</span>
 		nr_reclaimed++;
 
 		/*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



