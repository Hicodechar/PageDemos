
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[36/66] perf bench: Copy kernel files needed to build mem{cpy,set} x86_64 benchmarks - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [36/66] perf bench: Copy kernel files needed to build mem{cpy,set} x86_64 benchmarks</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=91481">Arnaldo Carvalho de Melo</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 12, 2016, 10:40 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1468363241-14555-37-git-send-email-acme@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9226539/mbox/"
   >mbox</a>
|
   <a href="/patch/9226539/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9226539/">/patch/9226539/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	0339160572 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 Jul 2016 22:55:03 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E67BD27DCD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 Jul 2016 22:55:02 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id DAFC127F9C; Tue, 12 Jul 2016 22:55:02 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 2B6A327DCD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 12 Jul 2016 22:55:00 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753453AbcGLWvI (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 12 Jul 2016 18:51:08 -0400
Received: from merlin.infradead.org ([205.233.59.134]:42277 &quot;EHLO
	merlin.infradead.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752697AbcGLWlX (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 12 Jul 2016 18:41:23 -0400
Received: from [179.235.155.208] (helo=jouet.infradead.org)
	by merlin.infradead.org with esmtpsa (Exim 4.85_2 #1 (Red Hat Linux))
	id 1bN6ML-0005m9-Kj; Tue, 12 Jul 2016 22:40:58 +0000
Received: by jouet.infradead.org (Postfix, from userid 1000)
	id C47B3143C5E; Tue, 12 Jul 2016 19:40:46 -0300 (BRT)
From: Arnaldo Carvalho de Melo &lt;acme@kernel.org&gt;
To: Ingo Molnar &lt;mingo@kernel.org&gt;
Cc: linux-kernel@vger.kernel.org, Arnaldo Carvalho de Melo &lt;acme@redhat.com&gt;,
	Adrian Hunter &lt;adrian.hunter@intel.com&gt;,
	David Ahern &lt;dsahern@gmail.com&gt;, Jiri Olsa &lt;jolsa@kernel.org&gt;,
	Namhyung Kim &lt;namhyung@kernel.org&gt;, Wang Nan &lt;wangnan0@huawei.com&gt;
Subject: [PATCH 36/66] perf bench: Copy kernel files needed to build mem{cpy,
	set} x86_64 benchmarks
Date: Tue, 12 Jul 2016 19:40:11 -0300
Message-Id: &lt;1468363241-14555-37-git-send-email-acme@kernel.org&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1468363241-14555-1-git-send-email-acme@kernel.org&gt;
References: &lt;1468363241-14555-1-git-send-email-acme@kernel.org&gt;
X-SRS-Rewrite: SMTP reverse-path rewritten from &lt;acme@infradead.org&gt; by
	merlin.infradead.org. See http://www.infradead.org/rpr.html
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=91481">Arnaldo Carvalho de Melo</a> - July 12, 2016, 10:40 p.m.</div>
<pre class="content">
<span class="from">From: Arnaldo Carvalho de Melo &lt;acme@redhat.com&gt;</span>

We can&#39;t access kernel files directly from tools/, so copy the required
bits, and make sure that we detect when the original files, in the
kernel, gets modified.

Cc: Adrian Hunter &lt;adrian.hunter@intel.com&gt;
Cc: David Ahern &lt;dsahern@gmail.com&gt;
Cc: Jiri Olsa &lt;jolsa@kernel.org&gt;
Cc: Namhyung Kim &lt;namhyung@kernel.org&gt;
Cc: Wang Nan &lt;wangnan0@huawei.com&gt;
Link: http://lkml.kernel.org/n/tip-z7e76274ch5j4nugv048qacb@git.kernel.org
<span class="signed-off-by">Signed-off-by: Arnaldo Carvalho de Melo &lt;acme@redhat.com&gt;</span>
---
 tools/arch/x86/include/asm/cpufeatures.h       | 316 +++++++++++++++++++++++++
 tools/arch/x86/include/asm/disabled-features.h |  60 +++++
 tools/arch/x86/include/asm/required-features.h | 103 ++++++++
 tools/arch/x86/lib/memcpy_64.S                 | 297 +++++++++++++++++++++++
 tools/arch/x86/lib/memset_64.S                 | 138 +++++++++++
 tools/include/asm/alternative-asm.h            |   9 +
 tools/perf/MANIFEST                            |   9 +-
 tools/perf/Makefile.perf                       |  15 ++
 tools/perf/bench/mem-memcpy-x86-64-asm.S       |   2 +-
 tools/perf/bench/mem-memset-x86-64-asm.S       |   2 +-
 tools/perf/util/include/asm/alternative-asm.h  |   9 -
 11 files changed, 946 insertions(+), 14 deletions(-)
 create mode 100644 tools/arch/x86/include/asm/cpufeatures.h
 create mode 100644 tools/arch/x86/include/asm/disabled-features.h
 create mode 100644 tools/arch/x86/include/asm/required-features.h
 create mode 100644 tools/arch/x86/lib/memcpy_64.S
 create mode 100644 tools/arch/x86/lib/memset_64.S
 create mode 100644 tools/include/asm/alternative-asm.h
 delete mode 100644 tools/perf/util/include/asm/alternative-asm.h
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/tools/arch/x86/include/asm/cpufeatures.h b/tools/arch/x86/include/asm/cpufeatures.h</span>
new file mode 100644
<span class="p_header">index 000000000000..4a413485f9eb</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -0,0 +1,316 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_X86_CPUFEATURES_H</span>
<span class="p_add">+#define _ASM_X86_CPUFEATURES_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_X86_REQUIRED_FEATURES_H</span>
<span class="p_add">+#include &lt;asm/required-features.h&gt;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _ASM_X86_DISABLED_FEATURES_H</span>
<span class="p_add">+#include &lt;asm/disabled-features.h&gt;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Defines x86 CPU feature bits</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define NCAPINTS	18	/* N 32-bit words worth of info */</span>
<span class="p_add">+#define NBUGINTS	1	/* N 32-bit bug flags */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Note: If the comment begins with a quoted string, that string is used</span>
<span class="p_add">+ * in /proc/cpuinfo instead of the macro name.  If the string is &quot;&quot;,</span>
<span class="p_add">+ * this feature bit is not displayed in /proc/cpuinfo at all.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Intel-defined CPU features, CPUID level 0x00000001 (edx), word 0 */</span>
<span class="p_add">+#define X86_FEATURE_FPU		( 0*32+ 0) /* Onboard FPU */</span>
<span class="p_add">+#define X86_FEATURE_VME		( 0*32+ 1) /* Virtual Mode Extensions */</span>
<span class="p_add">+#define X86_FEATURE_DE		( 0*32+ 2) /* Debugging Extensions */</span>
<span class="p_add">+#define X86_FEATURE_PSE		( 0*32+ 3) /* Page Size Extensions */</span>
<span class="p_add">+#define X86_FEATURE_TSC		( 0*32+ 4) /* Time Stamp Counter */</span>
<span class="p_add">+#define X86_FEATURE_MSR		( 0*32+ 5) /* Model-Specific Registers */</span>
<span class="p_add">+#define X86_FEATURE_PAE		( 0*32+ 6) /* Physical Address Extensions */</span>
<span class="p_add">+#define X86_FEATURE_MCE		( 0*32+ 7) /* Machine Check Exception */</span>
<span class="p_add">+#define X86_FEATURE_CX8		( 0*32+ 8) /* CMPXCHG8 instruction */</span>
<span class="p_add">+#define X86_FEATURE_APIC	( 0*32+ 9) /* Onboard APIC */</span>
<span class="p_add">+#define X86_FEATURE_SEP		( 0*32+11) /* SYSENTER/SYSEXIT */</span>
<span class="p_add">+#define X86_FEATURE_MTRR	( 0*32+12) /* Memory Type Range Registers */</span>
<span class="p_add">+#define X86_FEATURE_PGE		( 0*32+13) /* Page Global Enable */</span>
<span class="p_add">+#define X86_FEATURE_MCA		( 0*32+14) /* Machine Check Architecture */</span>
<span class="p_add">+#define X86_FEATURE_CMOV	( 0*32+15) /* CMOV instructions */</span>
<span class="p_add">+					  /* (plus FCMOVcc, FCOMI with FPU) */</span>
<span class="p_add">+#define X86_FEATURE_PAT		( 0*32+16) /* Page Attribute Table */</span>
<span class="p_add">+#define X86_FEATURE_PSE36	( 0*32+17) /* 36-bit PSEs */</span>
<span class="p_add">+#define X86_FEATURE_PN		( 0*32+18) /* Processor serial number */</span>
<span class="p_add">+#define X86_FEATURE_CLFLUSH	( 0*32+19) /* CLFLUSH instruction */</span>
<span class="p_add">+#define X86_FEATURE_DS		( 0*32+21) /* &quot;dts&quot; Debug Store */</span>
<span class="p_add">+#define X86_FEATURE_ACPI	( 0*32+22) /* ACPI via MSR */</span>
<span class="p_add">+#define X86_FEATURE_MMX		( 0*32+23) /* Multimedia Extensions */</span>
<span class="p_add">+#define X86_FEATURE_FXSR	( 0*32+24) /* FXSAVE/FXRSTOR, CR4.OSFXSR */</span>
<span class="p_add">+#define X86_FEATURE_XMM		( 0*32+25) /* &quot;sse&quot; */</span>
<span class="p_add">+#define X86_FEATURE_XMM2	( 0*32+26) /* &quot;sse2&quot; */</span>
<span class="p_add">+#define X86_FEATURE_SELFSNOOP	( 0*32+27) /* &quot;ss&quot; CPU self snoop */</span>
<span class="p_add">+#define X86_FEATURE_HT		( 0*32+28) /* Hyper-Threading */</span>
<span class="p_add">+#define X86_FEATURE_ACC		( 0*32+29) /* &quot;tm&quot; Automatic clock control */</span>
<span class="p_add">+#define X86_FEATURE_IA64	( 0*32+30) /* IA-64 processor */</span>
<span class="p_add">+#define X86_FEATURE_PBE		( 0*32+31) /* Pending Break Enable */</span>
<span class="p_add">+</span>
<span class="p_add">+/* AMD-defined CPU features, CPUID level 0x80000001, word 1 */</span>
<span class="p_add">+/* Don&#39;t duplicate feature flags which are redundant with Intel! */</span>
<span class="p_add">+#define X86_FEATURE_SYSCALL	( 1*32+11) /* SYSCALL/SYSRET */</span>
<span class="p_add">+#define X86_FEATURE_MP		( 1*32+19) /* MP Capable. */</span>
<span class="p_add">+#define X86_FEATURE_NX		( 1*32+20) /* Execute Disable */</span>
<span class="p_add">+#define X86_FEATURE_MMXEXT	( 1*32+22) /* AMD MMX extensions */</span>
<span class="p_add">+#define X86_FEATURE_FXSR_OPT	( 1*32+25) /* FXSAVE/FXRSTOR optimizations */</span>
<span class="p_add">+#define X86_FEATURE_GBPAGES	( 1*32+26) /* &quot;pdpe1gb&quot; GB pages */</span>
<span class="p_add">+#define X86_FEATURE_RDTSCP	( 1*32+27) /* RDTSCP */</span>
<span class="p_add">+#define X86_FEATURE_LM		( 1*32+29) /* Long Mode (x86-64) */</span>
<span class="p_add">+#define X86_FEATURE_3DNOWEXT	( 1*32+30) /* AMD 3DNow! extensions */</span>
<span class="p_add">+#define X86_FEATURE_3DNOW	( 1*32+31) /* 3DNow! */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Transmeta-defined CPU features, CPUID level 0x80860001, word 2 */</span>
<span class="p_add">+#define X86_FEATURE_RECOVERY	( 2*32+ 0) /* CPU in recovery mode */</span>
<span class="p_add">+#define X86_FEATURE_LONGRUN	( 2*32+ 1) /* Longrun power control */</span>
<span class="p_add">+#define X86_FEATURE_LRTI	( 2*32+ 3) /* LongRun table interface */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Other features, Linux-defined mapping, word 3 */</span>
<span class="p_add">+/* This range is used for feature bits which conflict or are synthesized */</span>
<span class="p_add">+#define X86_FEATURE_CXMMX	( 3*32+ 0) /* Cyrix MMX extensions */</span>
<span class="p_add">+#define X86_FEATURE_K6_MTRR	( 3*32+ 1) /* AMD K6 nonstandard MTRRs */</span>
<span class="p_add">+#define X86_FEATURE_CYRIX_ARR	( 3*32+ 2) /* Cyrix ARRs (= MTRRs) */</span>
<span class="p_add">+#define X86_FEATURE_CENTAUR_MCR	( 3*32+ 3) /* Centaur MCRs (= MTRRs) */</span>
<span class="p_add">+/* cpu types for specific tunings: */</span>
<span class="p_add">+#define X86_FEATURE_K8		( 3*32+ 4) /* &quot;&quot; Opteron, Athlon64 */</span>
<span class="p_add">+#define X86_FEATURE_K7		( 3*32+ 5) /* &quot;&quot; Athlon */</span>
<span class="p_add">+#define X86_FEATURE_P3		( 3*32+ 6) /* &quot;&quot; P3 */</span>
<span class="p_add">+#define X86_FEATURE_P4		( 3*32+ 7) /* &quot;&quot; P4 */</span>
<span class="p_add">+#define X86_FEATURE_CONSTANT_TSC ( 3*32+ 8) /* TSC ticks at a constant rate */</span>
<span class="p_add">+#define X86_FEATURE_UP		( 3*32+ 9) /* smp kernel running on up */</span>
<span class="p_add">+#define X86_FEATURE_ART		( 3*32+10) /* Platform has always running timer (ART) */</span>
<span class="p_add">+#define X86_FEATURE_ARCH_PERFMON ( 3*32+11) /* Intel Architectural PerfMon */</span>
<span class="p_add">+#define X86_FEATURE_PEBS	( 3*32+12) /* Precise-Event Based Sampling */</span>
<span class="p_add">+#define X86_FEATURE_BTS		( 3*32+13) /* Branch Trace Store */</span>
<span class="p_add">+#define X86_FEATURE_SYSCALL32	( 3*32+14) /* &quot;&quot; syscall in ia32 userspace */</span>
<span class="p_add">+#define X86_FEATURE_SYSENTER32	( 3*32+15) /* &quot;&quot; sysenter in ia32 userspace */</span>
<span class="p_add">+#define X86_FEATURE_REP_GOOD	( 3*32+16) /* rep microcode works well */</span>
<span class="p_add">+#define X86_FEATURE_MFENCE_RDTSC ( 3*32+17) /* &quot;&quot; Mfence synchronizes RDTSC */</span>
<span class="p_add">+#define X86_FEATURE_LFENCE_RDTSC ( 3*32+18) /* &quot;&quot; Lfence synchronizes RDTSC */</span>
<span class="p_add">+#define X86_FEATURE_ACC_POWER	( 3*32+19) /* AMD Accumulated Power Mechanism */</span>
<span class="p_add">+#define X86_FEATURE_NOPL	( 3*32+20) /* The NOPL (0F 1F) instructions */</span>
<span class="p_add">+#define X86_FEATURE_ALWAYS	( 3*32+21) /* &quot;&quot; Always-present feature */</span>
<span class="p_add">+#define X86_FEATURE_XTOPOLOGY	( 3*32+22) /* cpu topology enum extensions */</span>
<span class="p_add">+#define X86_FEATURE_TSC_RELIABLE ( 3*32+23) /* TSC is known to be reliable */</span>
<span class="p_add">+#define X86_FEATURE_NONSTOP_TSC	( 3*32+24) /* TSC does not stop in C states */</span>
<span class="p_add">+/* free, was #define X86_FEATURE_CLFLUSH_MONITOR ( 3*32+25) * &quot;&quot; clflush reqd with monitor */</span>
<span class="p_add">+#define X86_FEATURE_EXTD_APICID	( 3*32+26) /* has extended APICID (8 bits) */</span>
<span class="p_add">+#define X86_FEATURE_AMD_DCM     ( 3*32+27) /* multi-node processor */</span>
<span class="p_add">+#define X86_FEATURE_APERFMPERF	( 3*32+28) /* APERFMPERF */</span>
<span class="p_add">+#define X86_FEATURE_EAGER_FPU	( 3*32+29) /* &quot;eagerfpu&quot; Non lazy FPU restore */</span>
<span class="p_add">+#define X86_FEATURE_NONSTOP_TSC_S3 ( 3*32+30) /* TSC doesn&#39;t stop in S3 state */</span>
<span class="p_add">+#define X86_FEATURE_MCE_RECOVERY ( 3*32+31) /* cpu has recoverable machine checks */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Intel-defined CPU features, CPUID level 0x00000001 (ecx), word 4 */</span>
<span class="p_add">+#define X86_FEATURE_XMM3	( 4*32+ 0) /* &quot;pni&quot; SSE-3 */</span>
<span class="p_add">+#define X86_FEATURE_PCLMULQDQ	( 4*32+ 1) /* PCLMULQDQ instruction */</span>
<span class="p_add">+#define X86_FEATURE_DTES64	( 4*32+ 2) /* 64-bit Debug Store */</span>
<span class="p_add">+#define X86_FEATURE_MWAIT	( 4*32+ 3) /* &quot;monitor&quot; Monitor/Mwait support */</span>
<span class="p_add">+#define X86_FEATURE_DSCPL	( 4*32+ 4) /* &quot;ds_cpl&quot; CPL Qual. Debug Store */</span>
<span class="p_add">+#define X86_FEATURE_VMX		( 4*32+ 5) /* Hardware virtualization */</span>
<span class="p_add">+#define X86_FEATURE_SMX		( 4*32+ 6) /* Safer mode */</span>
<span class="p_add">+#define X86_FEATURE_EST		( 4*32+ 7) /* Enhanced SpeedStep */</span>
<span class="p_add">+#define X86_FEATURE_TM2		( 4*32+ 8) /* Thermal Monitor 2 */</span>
<span class="p_add">+#define X86_FEATURE_SSSE3	( 4*32+ 9) /* Supplemental SSE-3 */</span>
<span class="p_add">+#define X86_FEATURE_CID		( 4*32+10) /* Context ID */</span>
<span class="p_add">+#define X86_FEATURE_SDBG	( 4*32+11) /* Silicon Debug */</span>
<span class="p_add">+#define X86_FEATURE_FMA		( 4*32+12) /* Fused multiply-add */</span>
<span class="p_add">+#define X86_FEATURE_CX16	( 4*32+13) /* CMPXCHG16B */</span>
<span class="p_add">+#define X86_FEATURE_XTPR	( 4*32+14) /* Send Task Priority Messages */</span>
<span class="p_add">+#define X86_FEATURE_PDCM	( 4*32+15) /* Performance Capabilities */</span>
<span class="p_add">+#define X86_FEATURE_PCID	( 4*32+17) /* Process Context Identifiers */</span>
<span class="p_add">+#define X86_FEATURE_DCA		( 4*32+18) /* Direct Cache Access */</span>
<span class="p_add">+#define X86_FEATURE_XMM4_1	( 4*32+19) /* &quot;sse4_1&quot; SSE-4.1 */</span>
<span class="p_add">+#define X86_FEATURE_XMM4_2	( 4*32+20) /* &quot;sse4_2&quot; SSE-4.2 */</span>
<span class="p_add">+#define X86_FEATURE_X2APIC	( 4*32+21) /* x2APIC */</span>
<span class="p_add">+#define X86_FEATURE_MOVBE	( 4*32+22) /* MOVBE instruction */</span>
<span class="p_add">+#define X86_FEATURE_POPCNT      ( 4*32+23) /* POPCNT instruction */</span>
<span class="p_add">+#define X86_FEATURE_TSC_DEADLINE_TIMER	( 4*32+24) /* Tsc deadline timer */</span>
<span class="p_add">+#define X86_FEATURE_AES		( 4*32+25) /* AES instructions */</span>
<span class="p_add">+#define X86_FEATURE_XSAVE	( 4*32+26) /* XSAVE/XRSTOR/XSETBV/XGETBV */</span>
<span class="p_add">+#define X86_FEATURE_OSXSAVE	( 4*32+27) /* &quot;&quot; XSAVE enabled in the OS */</span>
<span class="p_add">+#define X86_FEATURE_AVX		( 4*32+28) /* Advanced Vector Extensions */</span>
<span class="p_add">+#define X86_FEATURE_F16C	( 4*32+29) /* 16-bit fp conversions */</span>
<span class="p_add">+#define X86_FEATURE_RDRAND	( 4*32+30) /* The RDRAND instruction */</span>
<span class="p_add">+#define X86_FEATURE_HYPERVISOR	( 4*32+31) /* Running on a hypervisor */</span>
<span class="p_add">+</span>
<span class="p_add">+/* VIA/Cyrix/Centaur-defined CPU features, CPUID level 0xC0000001, word 5 */</span>
<span class="p_add">+#define X86_FEATURE_XSTORE	( 5*32+ 2) /* &quot;rng&quot; RNG present (xstore) */</span>
<span class="p_add">+#define X86_FEATURE_XSTORE_EN	( 5*32+ 3) /* &quot;rng_en&quot; RNG enabled */</span>
<span class="p_add">+#define X86_FEATURE_XCRYPT	( 5*32+ 6) /* &quot;ace&quot; on-CPU crypto (xcrypt) */</span>
<span class="p_add">+#define X86_FEATURE_XCRYPT_EN	( 5*32+ 7) /* &quot;ace_en&quot; on-CPU crypto enabled */</span>
<span class="p_add">+#define X86_FEATURE_ACE2	( 5*32+ 8) /* Advanced Cryptography Engine v2 */</span>
<span class="p_add">+#define X86_FEATURE_ACE2_EN	( 5*32+ 9) /* ACE v2 enabled */</span>
<span class="p_add">+#define X86_FEATURE_PHE		( 5*32+10) /* PadLock Hash Engine */</span>
<span class="p_add">+#define X86_FEATURE_PHE_EN	( 5*32+11) /* PHE enabled */</span>
<span class="p_add">+#define X86_FEATURE_PMM		( 5*32+12) /* PadLock Montgomery Multiplier */</span>
<span class="p_add">+#define X86_FEATURE_PMM_EN	( 5*32+13) /* PMM enabled */</span>
<span class="p_add">+</span>
<span class="p_add">+/* More extended AMD flags: CPUID level 0x80000001, ecx, word 6 */</span>
<span class="p_add">+#define X86_FEATURE_LAHF_LM	( 6*32+ 0) /* LAHF/SAHF in long mode */</span>
<span class="p_add">+#define X86_FEATURE_CMP_LEGACY	( 6*32+ 1) /* If yes HyperThreading not valid */</span>
<span class="p_add">+#define X86_FEATURE_SVM		( 6*32+ 2) /* Secure virtual machine */</span>
<span class="p_add">+#define X86_FEATURE_EXTAPIC	( 6*32+ 3) /* Extended APIC space */</span>
<span class="p_add">+#define X86_FEATURE_CR8_LEGACY	( 6*32+ 4) /* CR8 in 32-bit mode */</span>
<span class="p_add">+#define X86_FEATURE_ABM		( 6*32+ 5) /* Advanced bit manipulation */</span>
<span class="p_add">+#define X86_FEATURE_SSE4A	( 6*32+ 6) /* SSE-4A */</span>
<span class="p_add">+#define X86_FEATURE_MISALIGNSSE ( 6*32+ 7) /* Misaligned SSE mode */</span>
<span class="p_add">+#define X86_FEATURE_3DNOWPREFETCH ( 6*32+ 8) /* 3DNow prefetch instructions */</span>
<span class="p_add">+#define X86_FEATURE_OSVW	( 6*32+ 9) /* OS Visible Workaround */</span>
<span class="p_add">+#define X86_FEATURE_IBS		( 6*32+10) /* Instruction Based Sampling */</span>
<span class="p_add">+#define X86_FEATURE_XOP		( 6*32+11) /* extended AVX instructions */</span>
<span class="p_add">+#define X86_FEATURE_SKINIT	( 6*32+12) /* SKINIT/STGI instructions */</span>
<span class="p_add">+#define X86_FEATURE_WDT		( 6*32+13) /* Watchdog timer */</span>
<span class="p_add">+#define X86_FEATURE_LWP		( 6*32+15) /* Light Weight Profiling */</span>
<span class="p_add">+#define X86_FEATURE_FMA4	( 6*32+16) /* 4 operands MAC instructions */</span>
<span class="p_add">+#define X86_FEATURE_TCE		( 6*32+17) /* translation cache extension */</span>
<span class="p_add">+#define X86_FEATURE_NODEID_MSR	( 6*32+19) /* NodeId MSR */</span>
<span class="p_add">+#define X86_FEATURE_TBM		( 6*32+21) /* trailing bit manipulations */</span>
<span class="p_add">+#define X86_FEATURE_TOPOEXT	( 6*32+22) /* topology extensions CPUID leafs */</span>
<span class="p_add">+#define X86_FEATURE_PERFCTR_CORE ( 6*32+23) /* core performance counter extensions */</span>
<span class="p_add">+#define X86_FEATURE_PERFCTR_NB  ( 6*32+24) /* NB performance counter extensions */</span>
<span class="p_add">+#define X86_FEATURE_BPEXT	(6*32+26) /* data breakpoint extension */</span>
<span class="p_add">+#define X86_FEATURE_PTSC	( 6*32+27) /* performance time-stamp counter */</span>
<span class="p_add">+#define X86_FEATURE_PERFCTR_L2	( 6*32+28) /* L2 performance counter extensions */</span>
<span class="p_add">+#define X86_FEATURE_MWAITX	( 6*32+29) /* MWAIT extension (MONITORX/MWAITX) */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Auxiliary flags: Linux defined - For features scattered in various</span>
<span class="p_add">+ * CPUID levels like 0x6, 0xA etc, word 7.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Reuse free bits when adding new feature flags!</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_FEATURE_CPB		( 7*32+ 2) /* AMD Core Performance Boost */</span>
<span class="p_add">+#define X86_FEATURE_EPB		( 7*32+ 3) /* IA32_ENERGY_PERF_BIAS support */</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_FEATURE_HW_PSTATE	( 7*32+ 8) /* AMD HW-PState */</span>
<span class="p_add">+#define X86_FEATURE_PROC_FEEDBACK ( 7*32+ 9) /* AMD ProcFeedbackInterface */</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_FEATURE_INTEL_PT	( 7*32+15) /* Intel Processor Trace */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Virtualization flags: Linux defined, word 8 */</span>
<span class="p_add">+#define X86_FEATURE_TPR_SHADOW  ( 8*32+ 0) /* Intel TPR Shadow */</span>
<span class="p_add">+#define X86_FEATURE_VNMI        ( 8*32+ 1) /* Intel Virtual NMI */</span>
<span class="p_add">+#define X86_FEATURE_FLEXPRIORITY ( 8*32+ 2) /* Intel FlexPriority */</span>
<span class="p_add">+#define X86_FEATURE_EPT         ( 8*32+ 3) /* Intel Extended Page Table */</span>
<span class="p_add">+#define X86_FEATURE_VPID        ( 8*32+ 4) /* Intel Virtual Processor ID */</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_FEATURE_VMMCALL     ( 8*32+15) /* Prefer vmmcall to vmcall */</span>
<span class="p_add">+#define X86_FEATURE_XENPV       ( 8*32+16) /* &quot;&quot; Xen paravirtual guest */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */</span>
<span class="p_add">+#define X86_FEATURE_FSGSBASE	( 9*32+ 0) /* {RD/WR}{FS/GS}BASE instructions*/</span>
<span class="p_add">+#define X86_FEATURE_TSC_ADJUST	( 9*32+ 1) /* TSC adjustment MSR 0x3b */</span>
<span class="p_add">+#define X86_FEATURE_BMI1	( 9*32+ 3) /* 1st group bit manipulation extensions */</span>
<span class="p_add">+#define X86_FEATURE_HLE		( 9*32+ 4) /* Hardware Lock Elision */</span>
<span class="p_add">+#define X86_FEATURE_AVX2	( 9*32+ 5) /* AVX2 instructions */</span>
<span class="p_add">+#define X86_FEATURE_SMEP	( 9*32+ 7) /* Supervisor Mode Execution Protection */</span>
<span class="p_add">+#define X86_FEATURE_BMI2	( 9*32+ 8) /* 2nd group bit manipulation extensions */</span>
<span class="p_add">+#define X86_FEATURE_ERMS	( 9*32+ 9) /* Enhanced REP MOVSB/STOSB */</span>
<span class="p_add">+#define X86_FEATURE_INVPCID	( 9*32+10) /* Invalidate Processor Context ID */</span>
<span class="p_add">+#define X86_FEATURE_RTM		( 9*32+11) /* Restricted Transactional Memory */</span>
<span class="p_add">+#define X86_FEATURE_CQM		( 9*32+12) /* Cache QoS Monitoring */</span>
<span class="p_add">+#define X86_FEATURE_MPX		( 9*32+14) /* Memory Protection Extension */</span>
<span class="p_add">+#define X86_FEATURE_AVX512F	( 9*32+16) /* AVX-512 Foundation */</span>
<span class="p_add">+#define X86_FEATURE_AVX512DQ	( 9*32+17) /* AVX-512 DQ (Double/Quad granular) Instructions */</span>
<span class="p_add">+#define X86_FEATURE_RDSEED	( 9*32+18) /* The RDSEED instruction */</span>
<span class="p_add">+#define X86_FEATURE_ADX		( 9*32+19) /* The ADCX and ADOX instructions */</span>
<span class="p_add">+#define X86_FEATURE_SMAP	( 9*32+20) /* Supervisor Mode Access Prevention */</span>
<span class="p_add">+#define X86_FEATURE_PCOMMIT	( 9*32+22) /* PCOMMIT instruction */</span>
<span class="p_add">+#define X86_FEATURE_CLFLUSHOPT	( 9*32+23) /* CLFLUSHOPT instruction */</span>
<span class="p_add">+#define X86_FEATURE_CLWB	( 9*32+24) /* CLWB instruction */</span>
<span class="p_add">+#define X86_FEATURE_AVX512PF	( 9*32+26) /* AVX-512 Prefetch */</span>
<span class="p_add">+#define X86_FEATURE_AVX512ER	( 9*32+27) /* AVX-512 Exponential and Reciprocal */</span>
<span class="p_add">+#define X86_FEATURE_AVX512CD	( 9*32+28) /* AVX-512 Conflict Detection */</span>
<span class="p_add">+#define X86_FEATURE_SHA_NI	( 9*32+29) /* SHA1/SHA256 Instruction Extensions */</span>
<span class="p_add">+#define X86_FEATURE_AVX512BW	( 9*32+30) /* AVX-512 BW (Byte/Word granular) Instructions */</span>
<span class="p_add">+#define X86_FEATURE_AVX512VL	( 9*32+31) /* AVX-512 VL (128/256 Vector Length) Extensions */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Extended state features, CPUID level 0x0000000d:1 (eax), word 10 */</span>
<span class="p_add">+#define X86_FEATURE_XSAVEOPT	(10*32+ 0) /* XSAVEOPT */</span>
<span class="p_add">+#define X86_FEATURE_XSAVEC	(10*32+ 1) /* XSAVEC */</span>
<span class="p_add">+#define X86_FEATURE_XGETBV1	(10*32+ 2) /* XGETBV with ECX = 1 */</span>
<span class="p_add">+#define X86_FEATURE_XSAVES	(10*32+ 3) /* XSAVES/XRSTORS */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:0 (edx), word 11 */</span>
<span class="p_add">+#define X86_FEATURE_CQM_LLC	(11*32+ 1) /* LLC QoS if 1 */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Intel-defined CPU QoS Sub-leaf, CPUID level 0x0000000F:1 (edx), word 12 */</span>
<span class="p_add">+#define X86_FEATURE_CQM_OCCUP_LLC (12*32+ 0) /* LLC occupancy monitoring if 1 */</span>
<span class="p_add">+#define X86_FEATURE_CQM_MBM_TOTAL (12*32+ 1) /* LLC Total MBM monitoring */</span>
<span class="p_add">+#define X86_FEATURE_CQM_MBM_LOCAL (12*32+ 2) /* LLC Local MBM monitoring */</span>
<span class="p_add">+</span>
<span class="p_add">+/* AMD-defined CPU features, CPUID level 0x80000008 (ebx), word 13 */</span>
<span class="p_add">+#define X86_FEATURE_CLZERO	(13*32+0) /* CLZERO instruction */</span>
<span class="p_add">+#define X86_FEATURE_IRPERF	(13*32+1) /* Instructions Retired Count */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Thermal and Power Management Leaf, CPUID level 0x00000006 (eax), word 14 */</span>
<span class="p_add">+#define X86_FEATURE_DTHERM	(14*32+ 0) /* Digital Thermal Sensor */</span>
<span class="p_add">+#define X86_FEATURE_IDA		(14*32+ 1) /* Intel Dynamic Acceleration */</span>
<span class="p_add">+#define X86_FEATURE_ARAT	(14*32+ 2) /* Always Running APIC Timer */</span>
<span class="p_add">+#define X86_FEATURE_PLN		(14*32+ 4) /* Intel Power Limit Notification */</span>
<span class="p_add">+#define X86_FEATURE_PTS		(14*32+ 6) /* Intel Package Thermal Status */</span>
<span class="p_add">+#define X86_FEATURE_HWP		(14*32+ 7) /* Intel Hardware P-states */</span>
<span class="p_add">+#define X86_FEATURE_HWP_NOTIFY	(14*32+ 8) /* HWP Notification */</span>
<span class="p_add">+#define X86_FEATURE_HWP_ACT_WINDOW (14*32+ 9) /* HWP Activity Window */</span>
<span class="p_add">+#define X86_FEATURE_HWP_EPP	(14*32+10) /* HWP Energy Perf. Preference */</span>
<span class="p_add">+#define X86_FEATURE_HWP_PKG_REQ (14*32+11) /* HWP Package Level Request */</span>
<span class="p_add">+</span>
<span class="p_add">+/* AMD SVM Feature Identification, CPUID level 0x8000000a (edx), word 15 */</span>
<span class="p_add">+#define X86_FEATURE_NPT		(15*32+ 0) /* Nested Page Table support */</span>
<span class="p_add">+#define X86_FEATURE_LBRV	(15*32+ 1) /* LBR Virtualization support */</span>
<span class="p_add">+#define X86_FEATURE_SVML	(15*32+ 2) /* &quot;svm_lock&quot; SVM locking MSR */</span>
<span class="p_add">+#define X86_FEATURE_NRIPS	(15*32+ 3) /* &quot;nrip_save&quot; SVM next_rip save */</span>
<span class="p_add">+#define X86_FEATURE_TSCRATEMSR  (15*32+ 4) /* &quot;tsc_scale&quot; TSC scaling support */</span>
<span class="p_add">+#define X86_FEATURE_VMCBCLEAN   (15*32+ 5) /* &quot;vmcb_clean&quot; VMCB clean bits support */</span>
<span class="p_add">+#define X86_FEATURE_FLUSHBYASID (15*32+ 6) /* flush-by-ASID support */</span>
<span class="p_add">+#define X86_FEATURE_DECODEASSISTS (15*32+ 7) /* Decode Assists support */</span>
<span class="p_add">+#define X86_FEATURE_PAUSEFILTER (15*32+10) /* filtered pause intercept */</span>
<span class="p_add">+#define X86_FEATURE_PFTHRESHOLD (15*32+12) /* pause filter threshold */</span>
<span class="p_add">+#define X86_FEATURE_AVIC	(15*32+13) /* Virtual Interrupt Controller */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Intel-defined CPU features, CPUID level 0x00000007:0 (ecx), word 16 */</span>
<span class="p_add">+#define X86_FEATURE_PKU		(16*32+ 3) /* Protection Keys for Userspace */</span>
<span class="p_add">+#define X86_FEATURE_OSPKE	(16*32+ 4) /* OS Protection Keys Enable */</span>
<span class="p_add">+</span>
<span class="p_add">+/* AMD-defined CPU features, CPUID level 0x80000007 (ebx), word 17 */</span>
<span class="p_add">+#define X86_FEATURE_OVERFLOW_RECOV (17*32+0) /* MCA overflow recovery support */</span>
<span class="p_add">+#define X86_FEATURE_SUCCOR	(17*32+1) /* Uncorrectable error containment and recovery */</span>
<span class="p_add">+#define X86_FEATURE_SMCA	(17*32+3) /* Scalable MCA */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * BUG word(s)</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define X86_BUG(x)		(NCAPINTS*32 + (x))</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_BUG_F00F		X86_BUG(0) /* Intel F00F */</span>
<span class="p_add">+#define X86_BUG_FDIV		X86_BUG(1) /* FPU FDIV */</span>
<span class="p_add">+#define X86_BUG_COMA		X86_BUG(2) /* Cyrix 6x86 coma */</span>
<span class="p_add">+#define X86_BUG_AMD_TLB_MMATCH	X86_BUG(3) /* &quot;tlb_mmatch&quot; AMD Erratum 383 */</span>
<span class="p_add">+#define X86_BUG_AMD_APIC_C1E	X86_BUG(4) /* &quot;apic_c1e&quot; AMD Erratum 400 */</span>
<span class="p_add">+#define X86_BUG_11AP		X86_BUG(5) /* Bad local APIC aka 11AP */</span>
<span class="p_add">+#define X86_BUG_FXSAVE_LEAK	X86_BUG(6) /* FXSAVE leaks FOP/FIP/FOP */</span>
<span class="p_add">+#define X86_BUG_CLFLUSH_MONITOR	X86_BUG(7) /* AAI65, CLFLUSH required before MONITOR */</span>
<span class="p_add">+#define X86_BUG_SYSRET_SS_ATTRS	X86_BUG(8) /* SYSRET doesn&#39;t fix up SS attrs */</span>
<span class="p_add">+#define X86_BUG_NULL_SEG	X86_BUG(9) /* Nulling a selector preserves the base */</span>
<span class="p_add">+#define X86_BUG_SWAPGS_FENCE	X86_BUG(10) /* SWAPGS without input dep on GS */</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_32</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * 64-bit kernels don&#39;t use X86_BUG_ESPFIX.  Make the define conditional</span>
<span class="p_add">+ * to avoid confusion.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define X86_BUG_ESPFIX		X86_BUG(9) /* &quot;&quot; IRET to 16-bit SS corrupts ESP/RSP high bits */</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_CPUFEATURES_H */</span>
<span class="p_header">diff --git a/tools/arch/x86/include/asm/disabled-features.h b/tools/arch/x86/include/asm/disabled-features.h</span>
new file mode 100644
<span class="p_header">index 000000000000..911e9358ceb1</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/arch/x86/include/asm/disabled-features.h</span>
<span class="p_chunk">@@ -0,0 +1,60 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_X86_DISABLED_FEATURES_H</span>
<span class="p_add">+#define _ASM_X86_DISABLED_FEATURES_H</span>
<span class="p_add">+</span>
<span class="p_add">+/* These features, although they might be available in a CPU</span>
<span class="p_add">+ * will not be used because the compile options to support</span>
<span class="p_add">+ * them are not present.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This code allows them to be checked and disabled at</span>
<span class="p_add">+ * compile time without an explicit #ifdef.  Use</span>
<span class="p_add">+ * cpu_feature_enabled().</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_INTEL_MPX</span>
<span class="p_add">+# define DISABLE_MPX	0</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define DISABLE_MPX	(1&lt;&lt;(X86_FEATURE_MPX &amp; 31))</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+# define DISABLE_VME		(1&lt;&lt;(X86_FEATURE_VME &amp; 31))</span>
<span class="p_add">+# define DISABLE_K6_MTRR	(1&lt;&lt;(X86_FEATURE_K6_MTRR &amp; 31))</span>
<span class="p_add">+# define DISABLE_CYRIX_ARR	(1&lt;&lt;(X86_FEATURE_CYRIX_ARR &amp; 31))</span>
<span class="p_add">+# define DISABLE_CENTAUR_MCR	(1&lt;&lt;(X86_FEATURE_CENTAUR_MCR &amp; 31))</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define DISABLE_VME		0</span>
<span class="p_add">+# define DISABLE_K6_MTRR	0</span>
<span class="p_add">+# define DISABLE_CYRIX_ARR	0</span>
<span class="p_add">+# define DISABLE_CENTAUR_MCR	0</span>
<span class="p_add">+#endif /* CONFIG_X86_64 */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS</span>
<span class="p_add">+# define DISABLE_PKU		0</span>
<span class="p_add">+# define DISABLE_OSPKE		0</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define DISABLE_PKU		(1&lt;&lt;(X86_FEATURE_PKU &amp; 31))</span>
<span class="p_add">+# define DISABLE_OSPKE		(1&lt;&lt;(X86_FEATURE_OSPKE &amp; 31))</span>
<span class="p_add">+#endif /* CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Make sure to add features to the correct mask</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define DISABLED_MASK0	(DISABLE_VME)</span>
<span class="p_add">+#define DISABLED_MASK1	0</span>
<span class="p_add">+#define DISABLED_MASK2	0</span>
<span class="p_add">+#define DISABLED_MASK3	(DISABLE_CYRIX_ARR|DISABLE_CENTAUR_MCR|DISABLE_K6_MTRR)</span>
<span class="p_add">+#define DISABLED_MASK4	0</span>
<span class="p_add">+#define DISABLED_MASK5	0</span>
<span class="p_add">+#define DISABLED_MASK6	0</span>
<span class="p_add">+#define DISABLED_MASK7	0</span>
<span class="p_add">+#define DISABLED_MASK8	0</span>
<span class="p_add">+#define DISABLED_MASK9	(DISABLE_MPX)</span>
<span class="p_add">+#define DISABLED_MASK10	0</span>
<span class="p_add">+#define DISABLED_MASK11	0</span>
<span class="p_add">+#define DISABLED_MASK12	0</span>
<span class="p_add">+#define DISABLED_MASK13	0</span>
<span class="p_add">+#define DISABLED_MASK14	0</span>
<span class="p_add">+#define DISABLED_MASK15	0</span>
<span class="p_add">+#define DISABLED_MASK16	(DISABLE_PKU|DISABLE_OSPKE)</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_DISABLED_FEATURES_H */</span>
<span class="p_header">diff --git a/tools/arch/x86/include/asm/required-features.h b/tools/arch/x86/include/asm/required-features.h</span>
new file mode 100644
<span class="p_header">index 000000000000..4916144e3c42</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/arch/x86/include/asm/required-features.h</span>
<span class="p_chunk">@@ -0,0 +1,103 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_X86_REQUIRED_FEATURES_H</span>
<span class="p_add">+#define _ASM_X86_REQUIRED_FEATURES_H</span>
<span class="p_add">+</span>
<span class="p_add">+/* Define minimum CPUID feature set for kernel These bits are checked</span>
<span class="p_add">+   really early to actually display a visible error message before the</span>
<span class="p_add">+   kernel dies.  Make sure to assign features to the proper mask!</span>
<span class="p_add">+</span>
<span class="p_add">+   Some requirements that are not in CPUID yet are also in the</span>
<span class="p_add">+   CONFIG_X86_MINIMUM_CPU_FAMILY which is checked too.</span>
<span class="p_add">+</span>
<span class="p_add">+   The real information is in arch/x86/Kconfig.cpu, this just converts</span>
<span class="p_add">+   the CONFIGs into a bitmask */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_MATH_EMULATION</span>
<span class="p_add">+# define NEED_FPU	(1&lt;&lt;(X86_FEATURE_FPU &amp; 31))</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define NEED_FPU	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_X86_PAE) || defined(CONFIG_X86_64)</span>
<span class="p_add">+# define NEED_PAE	(1&lt;&lt;(X86_FEATURE_PAE &amp; 31))</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define NEED_PAE	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_CMPXCHG64</span>
<span class="p_add">+# define NEED_CX8	(1&lt;&lt;(X86_FEATURE_CX8 &amp; 31))</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define NEED_CX8	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_X86_CMOV) || defined(CONFIG_X86_64)</span>
<span class="p_add">+# define NEED_CMOV	(1&lt;&lt;(X86_FEATURE_CMOV &amp; 31))</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define NEED_CMOV	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_USE_3DNOW</span>
<span class="p_add">+# define NEED_3DNOW	(1&lt;&lt;(X86_FEATURE_3DNOW &amp; 31))</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define NEED_3DNOW	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_X86_P6_NOP) || defined(CONFIG_X86_64)</span>
<span class="p_add">+# define NEED_NOPL	(1&lt;&lt;(X86_FEATURE_NOPL &amp; 31))</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define NEED_NOPL	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_MATOM</span>
<span class="p_add">+# define NEED_MOVBE	(1&lt;&lt;(X86_FEATURE_MOVBE &amp; 31))</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define NEED_MOVBE	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+#ifdef CONFIG_PARAVIRT</span>
<span class="p_add">+/* Paravirtualized systems may not have PSE or PGE available */</span>
<span class="p_add">+#define NEED_PSE	0</span>
<span class="p_add">+#define NEED_PGE	0</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define NEED_PSE	(1&lt;&lt;(X86_FEATURE_PSE) &amp; 31)</span>
<span class="p_add">+#define NEED_PGE	(1&lt;&lt;(X86_FEATURE_PGE) &amp; 31)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#define NEED_MSR	(1&lt;&lt;(X86_FEATURE_MSR &amp; 31))</span>
<span class="p_add">+#define NEED_FXSR	(1&lt;&lt;(X86_FEATURE_FXSR &amp; 31))</span>
<span class="p_add">+#define NEED_XMM	(1&lt;&lt;(X86_FEATURE_XMM &amp; 31))</span>
<span class="p_add">+#define NEED_XMM2	(1&lt;&lt;(X86_FEATURE_XMM2 &amp; 31))</span>
<span class="p_add">+#define NEED_LM		(1&lt;&lt;(X86_FEATURE_LM &amp; 31))</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define NEED_PSE	0</span>
<span class="p_add">+#define NEED_MSR	0</span>
<span class="p_add">+#define NEED_PGE	0</span>
<span class="p_add">+#define NEED_FXSR	0</span>
<span class="p_add">+#define NEED_XMM	0</span>
<span class="p_add">+#define NEED_XMM2	0</span>
<span class="p_add">+#define NEED_LM		0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define REQUIRED_MASK0	(NEED_FPU|NEED_PSE|NEED_MSR|NEED_PAE|\</span>
<span class="p_add">+			 NEED_CX8|NEED_PGE|NEED_FXSR|NEED_CMOV|\</span>
<span class="p_add">+			 NEED_XMM|NEED_XMM2)</span>
<span class="p_add">+#define SSE_MASK	(NEED_XMM|NEED_XMM2)</span>
<span class="p_add">+</span>
<span class="p_add">+#define REQUIRED_MASK1	(NEED_LM|NEED_3DNOW)</span>
<span class="p_add">+</span>
<span class="p_add">+#define REQUIRED_MASK2	0</span>
<span class="p_add">+#define REQUIRED_MASK3	(NEED_NOPL)</span>
<span class="p_add">+#define REQUIRED_MASK4	(NEED_MOVBE)</span>
<span class="p_add">+#define REQUIRED_MASK5	0</span>
<span class="p_add">+#define REQUIRED_MASK6	0</span>
<span class="p_add">+#define REQUIRED_MASK7	0</span>
<span class="p_add">+#define REQUIRED_MASK8	0</span>
<span class="p_add">+#define REQUIRED_MASK9	0</span>
<span class="p_add">+#define REQUIRED_MASK10	0</span>
<span class="p_add">+#define REQUIRED_MASK11	0</span>
<span class="p_add">+#define REQUIRED_MASK12	0</span>
<span class="p_add">+#define REQUIRED_MASK13	0</span>
<span class="p_add">+#define REQUIRED_MASK14	0</span>
<span class="p_add">+#define REQUIRED_MASK15	0</span>
<span class="p_add">+#define REQUIRED_MASK16	0</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_REQUIRED_FEATURES_H */</span>
<span class="p_header">diff --git a/tools/arch/x86/lib/memcpy_64.S b/tools/arch/x86/lib/memcpy_64.S</span>
new file mode 100644
<span class="p_header">index 000000000000..2ec0b0abbfaa</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/arch/x86/lib/memcpy_64.S</span>
<span class="p_chunk">@@ -0,0 +1,297 @@</span> <span class="p_context"></span>
<span class="p_add">+/* Copyright 2002 Andi Kleen */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
<span class="p_add">+#include &lt;asm/errno.h&gt;</span>
<span class="p_add">+#include &lt;asm/cpufeatures.h&gt;</span>
<span class="p_add">+#include &lt;asm/alternative-asm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * We build a jump to memcpy_orig by default which gets NOPped out on</span>
<span class="p_add">+ * the majority of x86 CPUs which set REP_GOOD. In addition, CPUs which</span>
<span class="p_add">+ * have the enhanced REP MOVSB/STOSB feature (ERMS), change those NOPs</span>
<span class="p_add">+ * to a jmp to memcpy_erms which does the REP; MOVSB mem copy.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+.weak memcpy</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * memcpy - Copy a memory block.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Input:</span>
<span class="p_add">+ *  rdi destination</span>
<span class="p_add">+ *  rsi source</span>
<span class="p_add">+ *  rdx count</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Output:</span>
<span class="p_add">+ * rax original destination</span>
<span class="p_add">+ */</span>
<span class="p_add">+ENTRY(__memcpy)</span>
<span class="p_add">+ENTRY(memcpy)</span>
<span class="p_add">+	ALTERNATIVE_2 &quot;jmp memcpy_orig&quot;, &quot;&quot;, X86_FEATURE_REP_GOOD, \</span>
<span class="p_add">+		      &quot;jmp memcpy_erms&quot;, X86_FEATURE_ERMS</span>
<span class="p_add">+</span>
<span class="p_add">+	movq %rdi, %rax</span>
<span class="p_add">+	movq %rdx, %rcx</span>
<span class="p_add">+	shrq $3, %rcx</span>
<span class="p_add">+	andl $7, %edx</span>
<span class="p_add">+	rep movsq</span>
<span class="p_add">+	movl %edx, %ecx</span>
<span class="p_add">+	rep movsb</span>
<span class="p_add">+	ret</span>
<span class="p_add">+ENDPROC(memcpy)</span>
<span class="p_add">+ENDPROC(__memcpy)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * memcpy_erms() - enhanced fast string memcpy. This is faster and</span>
<span class="p_add">+ * simpler than memcpy. Use memcpy_erms when possible.</span>
<span class="p_add">+ */</span>
<span class="p_add">+ENTRY(memcpy_erms)</span>
<span class="p_add">+	movq %rdi, %rax</span>
<span class="p_add">+	movq %rdx, %rcx</span>
<span class="p_add">+	rep movsb</span>
<span class="p_add">+	ret</span>
<span class="p_add">+ENDPROC(memcpy_erms)</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(memcpy_orig)</span>
<span class="p_add">+	movq %rdi, %rax</span>
<span class="p_add">+</span>
<span class="p_add">+	cmpq $0x20, %rdx</span>
<span class="p_add">+	jb .Lhandle_tail</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We check whether memory false dependence could occur,</span>
<span class="p_add">+	 * then jump to corresponding copy mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	cmp  %dil, %sil</span>
<span class="p_add">+	jl .Lcopy_backward</span>
<span class="p_add">+	subq $0x20, %rdx</span>
<span class="p_add">+.Lcopy_forward_loop:</span>
<span class="p_add">+	subq $0x20,	%rdx</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Move in blocks of 4x8 bytes:</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq 0*8(%rsi),	%r8</span>
<span class="p_add">+	movq 1*8(%rsi),	%r9</span>
<span class="p_add">+	movq 2*8(%rsi),	%r10</span>
<span class="p_add">+	movq 3*8(%rsi),	%r11</span>
<span class="p_add">+	leaq 4*8(%rsi),	%rsi</span>
<span class="p_add">+</span>
<span class="p_add">+	movq %r8,	0*8(%rdi)</span>
<span class="p_add">+	movq %r9,	1*8(%rdi)</span>
<span class="p_add">+	movq %r10,	2*8(%rdi)</span>
<span class="p_add">+	movq %r11,	3*8(%rdi)</span>
<span class="p_add">+	leaq 4*8(%rdi),	%rdi</span>
<span class="p_add">+	jae  .Lcopy_forward_loop</span>
<span class="p_add">+	addl $0x20,	%edx</span>
<span class="p_add">+	jmp  .Lhandle_tail</span>
<span class="p_add">+</span>
<span class="p_add">+.Lcopy_backward:</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Calculate copy position to tail.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	addq %rdx,	%rsi</span>
<span class="p_add">+	addq %rdx,	%rdi</span>
<span class="p_add">+	subq $0x20,	%rdx</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At most 3 ALU operations in one cycle,</span>
<span class="p_add">+	 * so append NOPS in the same 16 bytes trunk.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	.p2align 4</span>
<span class="p_add">+.Lcopy_backward_loop:</span>
<span class="p_add">+	subq $0x20,	%rdx</span>
<span class="p_add">+	movq -1*8(%rsi),	%r8</span>
<span class="p_add">+	movq -2*8(%rsi),	%r9</span>
<span class="p_add">+	movq -3*8(%rsi),	%r10</span>
<span class="p_add">+	movq -4*8(%rsi),	%r11</span>
<span class="p_add">+	leaq -4*8(%rsi),	%rsi</span>
<span class="p_add">+	movq %r8,		-1*8(%rdi)</span>
<span class="p_add">+	movq %r9,		-2*8(%rdi)</span>
<span class="p_add">+	movq %r10,		-3*8(%rdi)</span>
<span class="p_add">+	movq %r11,		-4*8(%rdi)</span>
<span class="p_add">+	leaq -4*8(%rdi),	%rdi</span>
<span class="p_add">+	jae  .Lcopy_backward_loop</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Calculate copy position to head.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	addl $0x20,	%edx</span>
<span class="p_add">+	subq %rdx,	%rsi</span>
<span class="p_add">+	subq %rdx,	%rdi</span>
<span class="p_add">+.Lhandle_tail:</span>
<span class="p_add">+	cmpl $16,	%edx</span>
<span class="p_add">+	jb   .Lless_16bytes</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Move data from 16 bytes to 31 bytes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq 0*8(%rsi), %r8</span>
<span class="p_add">+	movq 1*8(%rsi),	%r9</span>
<span class="p_add">+	movq -2*8(%rsi, %rdx),	%r10</span>
<span class="p_add">+	movq -1*8(%rsi, %rdx),	%r11</span>
<span class="p_add">+	movq %r8,	0*8(%rdi)</span>
<span class="p_add">+	movq %r9,	1*8(%rdi)</span>
<span class="p_add">+	movq %r10,	-2*8(%rdi, %rdx)</span>
<span class="p_add">+	movq %r11,	-1*8(%rdi, %rdx)</span>
<span class="p_add">+	retq</span>
<span class="p_add">+	.p2align 4</span>
<span class="p_add">+.Lless_16bytes:</span>
<span class="p_add">+	cmpl $8,	%edx</span>
<span class="p_add">+	jb   .Lless_8bytes</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Move data from 8 bytes to 15 bytes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq 0*8(%rsi),	%r8</span>
<span class="p_add">+	movq -1*8(%rsi, %rdx),	%r9</span>
<span class="p_add">+	movq %r8,	0*8(%rdi)</span>
<span class="p_add">+	movq %r9,	-1*8(%rdi, %rdx)</span>
<span class="p_add">+	retq</span>
<span class="p_add">+	.p2align 4</span>
<span class="p_add">+.Lless_8bytes:</span>
<span class="p_add">+	cmpl $4,	%edx</span>
<span class="p_add">+	jb   .Lless_3bytes</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Move data from 4 bytes to 7 bytes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movl (%rsi), %ecx</span>
<span class="p_add">+	movl -4(%rsi, %rdx), %r8d</span>
<span class="p_add">+	movl %ecx, (%rdi)</span>
<span class="p_add">+	movl %r8d, -4(%rdi, %rdx)</span>
<span class="p_add">+	retq</span>
<span class="p_add">+	.p2align 4</span>
<span class="p_add">+.Lless_3bytes:</span>
<span class="p_add">+	subl $1, %edx</span>
<span class="p_add">+	jb .Lend</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Move data from 1 bytes to 3 bytes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movzbl (%rsi), %ecx</span>
<span class="p_add">+	jz .Lstore_1byte</span>
<span class="p_add">+	movzbq 1(%rsi), %r8</span>
<span class="p_add">+	movzbq (%rsi, %rdx), %r9</span>
<span class="p_add">+	movb %r8b, 1(%rdi)</span>
<span class="p_add">+	movb %r9b, (%rdi, %rdx)</span>
<span class="p_add">+.Lstore_1byte:</span>
<span class="p_add">+	movb %cl, (%rdi)</span>
<span class="p_add">+</span>
<span class="p_add">+.Lend:</span>
<span class="p_add">+	retq</span>
<span class="p_add">+ENDPROC(memcpy_orig)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef CONFIG_UML</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * memcpy_mcsafe - memory copy with machine check exception handling</span>
<span class="p_add">+ * Note that we only catch machine checks when reading the source addresses.</span>
<span class="p_add">+ * Writes to target are posted and don&#39;t generate machine checks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+ENTRY(memcpy_mcsafe)</span>
<span class="p_add">+	cmpl $8, %edx</span>
<span class="p_add">+	/* Less than 8 bytes? Go to byte copy loop */</span>
<span class="p_add">+	jb .L_no_whole_words</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check for bad alignment of source */</span>
<span class="p_add">+	testl $7, %esi</span>
<span class="p_add">+	/* Already aligned */</span>
<span class="p_add">+	jz .L_8byte_aligned</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Copy one byte at a time until source is 8-byte aligned */</span>
<span class="p_add">+	movl %esi, %ecx</span>
<span class="p_add">+	andl $7, %ecx</span>
<span class="p_add">+	subl $8, %ecx</span>
<span class="p_add">+	negl %ecx</span>
<span class="p_add">+	subl %ecx, %edx</span>
<span class="p_add">+.L_copy_leading_bytes:</span>
<span class="p_add">+	movb (%rsi), %al</span>
<span class="p_add">+	movb %al, (%rdi)</span>
<span class="p_add">+	incq %rsi</span>
<span class="p_add">+	incq %rdi</span>
<span class="p_add">+	decl %ecx</span>
<span class="p_add">+	jnz .L_copy_leading_bytes</span>
<span class="p_add">+</span>
<span class="p_add">+.L_8byte_aligned:</span>
<span class="p_add">+	/* Figure out how many whole cache lines (64-bytes) to copy */</span>
<span class="p_add">+	movl %edx, %ecx</span>
<span class="p_add">+	andl $63, %edx</span>
<span class="p_add">+	shrl $6, %ecx</span>
<span class="p_add">+	jz .L_no_whole_cache_lines</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Loop copying whole cache lines */</span>
<span class="p_add">+.L_cache_w0: movq (%rsi), %r8</span>
<span class="p_add">+.L_cache_w1: movq 1*8(%rsi), %r9</span>
<span class="p_add">+.L_cache_w2: movq 2*8(%rsi), %r10</span>
<span class="p_add">+.L_cache_w3: movq 3*8(%rsi), %r11</span>
<span class="p_add">+	movq %r8, (%rdi)</span>
<span class="p_add">+	movq %r9, 1*8(%rdi)</span>
<span class="p_add">+	movq %r10, 2*8(%rdi)</span>
<span class="p_add">+	movq %r11, 3*8(%rdi)</span>
<span class="p_add">+.L_cache_w4: movq 4*8(%rsi), %r8</span>
<span class="p_add">+.L_cache_w5: movq 5*8(%rsi), %r9</span>
<span class="p_add">+.L_cache_w6: movq 6*8(%rsi), %r10</span>
<span class="p_add">+.L_cache_w7: movq 7*8(%rsi), %r11</span>
<span class="p_add">+	movq %r8, 4*8(%rdi)</span>
<span class="p_add">+	movq %r9, 5*8(%rdi)</span>
<span class="p_add">+	movq %r10, 6*8(%rdi)</span>
<span class="p_add">+	movq %r11, 7*8(%rdi)</span>
<span class="p_add">+	leaq 64(%rsi), %rsi</span>
<span class="p_add">+	leaq 64(%rdi), %rdi</span>
<span class="p_add">+	decl %ecx</span>
<span class="p_add">+	jnz .L_cache_w0</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Are there any trailing 8-byte words? */</span>
<span class="p_add">+.L_no_whole_cache_lines:</span>
<span class="p_add">+	movl %edx, %ecx</span>
<span class="p_add">+	andl $7, %edx</span>
<span class="p_add">+	shrl $3, %ecx</span>
<span class="p_add">+	jz .L_no_whole_words</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Copy trailing words */</span>
<span class="p_add">+.L_copy_trailing_words:</span>
<span class="p_add">+	movq (%rsi), %r8</span>
<span class="p_add">+	mov %r8, (%rdi)</span>
<span class="p_add">+	leaq 8(%rsi), %rsi</span>
<span class="p_add">+	leaq 8(%rdi), %rdi</span>
<span class="p_add">+	decl %ecx</span>
<span class="p_add">+	jnz .L_copy_trailing_words</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Any trailing bytes? */</span>
<span class="p_add">+.L_no_whole_words:</span>
<span class="p_add">+	andl %edx, %edx</span>
<span class="p_add">+	jz .L_done_memcpy_trap</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Copy trailing bytes */</span>
<span class="p_add">+	movl %edx, %ecx</span>
<span class="p_add">+.L_copy_trailing_bytes:</span>
<span class="p_add">+	movb (%rsi), %al</span>
<span class="p_add">+	movb %al, (%rdi)</span>
<span class="p_add">+	incq %rsi</span>
<span class="p_add">+	incq %rdi</span>
<span class="p_add">+	decl %ecx</span>
<span class="p_add">+	jnz .L_copy_trailing_bytes</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Copy successful. Return zero */</span>
<span class="p_add">+.L_done_memcpy_trap:</span>
<span class="p_add">+	xorq %rax, %rax</span>
<span class="p_add">+	ret</span>
<span class="p_add">+ENDPROC(memcpy_mcsafe)</span>
<span class="p_add">+</span>
<span class="p_add">+	.section .fixup, &quot;ax&quot;</span>
<span class="p_add">+	/* Return -EFAULT for any failure */</span>
<span class="p_add">+.L_memcpy_mcsafe_fail:</span>
<span class="p_add">+	mov	$-EFAULT, %rax</span>
<span class="p_add">+	ret</span>
<span class="p_add">+</span>
<span class="p_add">+	.previous</span>
<span class="p_add">+</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_copy_leading_bytes, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_cache_w0, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_cache_w1, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_cache_w3, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_cache_w3, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_cache_w4, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_cache_w5, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_cache_w6, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_cache_w7, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_copy_trailing_words, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+	_ASM_EXTABLE_FAULT(.L_copy_trailing_bytes, .L_memcpy_mcsafe_fail)</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/tools/arch/x86/lib/memset_64.S b/tools/arch/x86/lib/memset_64.S</span>
new file mode 100644
<span class="p_header">index 000000000000..e1229ecd2a82</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/arch/x86/lib/memset_64.S</span>
<span class="p_chunk">@@ -0,0 +1,138 @@</span> <span class="p_context"></span>
<span class="p_add">+/* Copyright 2002 Andi Kleen, SuSE Labs */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/linkage.h&gt;</span>
<span class="p_add">+#include &lt;asm/cpufeatures.h&gt;</span>
<span class="p_add">+#include &lt;asm/alternative-asm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+.weak memset</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * ISO C memset - set a memory block to a byte value. This function uses fast</span>
<span class="p_add">+ * string to get better performance than the original function. The code is</span>
<span class="p_add">+ * simpler and shorter than the original function as well.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * rdi   destination</span>
<span class="p_add">+ * rsi   value (char)</span>
<span class="p_add">+ * rdx   count (bytes)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * rax   original destination</span>
<span class="p_add">+ */</span>
<span class="p_add">+ENTRY(memset)</span>
<span class="p_add">+ENTRY(__memset)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Some CPUs support enhanced REP MOVSB/STOSB feature. It is recommended</span>
<span class="p_add">+	 * to use it when possible. If not available, use fast string instructions.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Otherwise, use original memset function.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ALTERNATIVE_2 &quot;jmp memset_orig&quot;, &quot;&quot;, X86_FEATURE_REP_GOOD, \</span>
<span class="p_add">+		      &quot;jmp memset_erms&quot;, X86_FEATURE_ERMS</span>
<span class="p_add">+</span>
<span class="p_add">+	movq %rdi,%r9</span>
<span class="p_add">+	movq %rdx,%rcx</span>
<span class="p_add">+	andl $7,%edx</span>
<span class="p_add">+	shrq $3,%rcx</span>
<span class="p_add">+	/* expand byte value  */</span>
<span class="p_add">+	movzbl %sil,%esi</span>
<span class="p_add">+	movabs $0x0101010101010101,%rax</span>
<span class="p_add">+	imulq %rsi,%rax</span>
<span class="p_add">+	rep stosq</span>
<span class="p_add">+	movl %edx,%ecx</span>
<span class="p_add">+	rep stosb</span>
<span class="p_add">+	movq %r9,%rax</span>
<span class="p_add">+	ret</span>
<span class="p_add">+ENDPROC(memset)</span>
<span class="p_add">+ENDPROC(__memset)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * ISO C memset - set a memory block to a byte value. This function uses</span>
<span class="p_add">+ * enhanced rep stosb to override the fast string function.</span>
<span class="p_add">+ * The code is simpler and shorter than the fast string function as well.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * rdi   destination</span>
<span class="p_add">+ * rsi   value (char)</span>
<span class="p_add">+ * rdx   count (bytes)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * rax   original destination</span>
<span class="p_add">+ */</span>
<span class="p_add">+ENTRY(memset_erms)</span>
<span class="p_add">+	movq %rdi,%r9</span>
<span class="p_add">+	movb %sil,%al</span>
<span class="p_add">+	movq %rdx,%rcx</span>
<span class="p_add">+	rep stosb</span>
<span class="p_add">+	movq %r9,%rax</span>
<span class="p_add">+	ret</span>
<span class="p_add">+ENDPROC(memset_erms)</span>
<span class="p_add">+</span>
<span class="p_add">+ENTRY(memset_orig)</span>
<span class="p_add">+	movq %rdi,%r10</span>
<span class="p_add">+</span>
<span class="p_add">+	/* expand byte value  */</span>
<span class="p_add">+	movzbl %sil,%ecx</span>
<span class="p_add">+	movabs $0x0101010101010101,%rax</span>
<span class="p_add">+	imulq  %rcx,%rax</span>
<span class="p_add">+</span>
<span class="p_add">+	/* align dst */</span>
<span class="p_add">+	movl  %edi,%r9d</span>
<span class="p_add">+	andl  $7,%r9d</span>
<span class="p_add">+	jnz  .Lbad_alignment</span>
<span class="p_add">+.Lafter_bad_alignment:</span>
<span class="p_add">+</span>
<span class="p_add">+	movq  %rdx,%rcx</span>
<span class="p_add">+	shrq  $6,%rcx</span>
<span class="p_add">+	jz	 .Lhandle_tail</span>
<span class="p_add">+</span>
<span class="p_add">+	.p2align 4</span>
<span class="p_add">+.Lloop_64:</span>
<span class="p_add">+	decq  %rcx</span>
<span class="p_add">+	movq  %rax,(%rdi)</span>
<span class="p_add">+	movq  %rax,8(%rdi)</span>
<span class="p_add">+	movq  %rax,16(%rdi)</span>
<span class="p_add">+	movq  %rax,24(%rdi)</span>
<span class="p_add">+	movq  %rax,32(%rdi)</span>
<span class="p_add">+	movq  %rax,40(%rdi)</span>
<span class="p_add">+	movq  %rax,48(%rdi)</span>
<span class="p_add">+	movq  %rax,56(%rdi)</span>
<span class="p_add">+	leaq  64(%rdi),%rdi</span>
<span class="p_add">+	jnz    .Lloop_64</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Handle tail in loops. The loops should be faster than hard</span>
<span class="p_add">+	   to predict jump tables. */</span>
<span class="p_add">+	.p2align 4</span>
<span class="p_add">+.Lhandle_tail:</span>
<span class="p_add">+	movl	%edx,%ecx</span>
<span class="p_add">+	andl    $63&amp;(~7),%ecx</span>
<span class="p_add">+	jz 		.Lhandle_7</span>
<span class="p_add">+	shrl	$3,%ecx</span>
<span class="p_add">+	.p2align 4</span>
<span class="p_add">+.Lloop_8:</span>
<span class="p_add">+	decl   %ecx</span>
<span class="p_add">+	movq  %rax,(%rdi)</span>
<span class="p_add">+	leaq  8(%rdi),%rdi</span>
<span class="p_add">+	jnz    .Lloop_8</span>
<span class="p_add">+</span>
<span class="p_add">+.Lhandle_7:</span>
<span class="p_add">+	andl	$7,%edx</span>
<span class="p_add">+	jz      .Lende</span>
<span class="p_add">+	.p2align 4</span>
<span class="p_add">+.Lloop_1:</span>
<span class="p_add">+	decl    %edx</span>
<span class="p_add">+	movb 	%al,(%rdi)</span>
<span class="p_add">+	leaq	1(%rdi),%rdi</span>
<span class="p_add">+	jnz     .Lloop_1</span>
<span class="p_add">+</span>
<span class="p_add">+.Lende:</span>
<span class="p_add">+	movq	%r10,%rax</span>
<span class="p_add">+	ret</span>
<span class="p_add">+</span>
<span class="p_add">+.Lbad_alignment:</span>
<span class="p_add">+	cmpq $7,%rdx</span>
<span class="p_add">+	jbe	.Lhandle_7</span>
<span class="p_add">+	movq %rax,(%rdi)	/* unaligned store */</span>
<span class="p_add">+	movq $8,%r8</span>
<span class="p_add">+	subq %r9,%r8</span>
<span class="p_add">+	addq %r8,%rdi</span>
<span class="p_add">+	subq %r8,%rdx</span>
<span class="p_add">+	jmp .Lafter_bad_alignment</span>
<span class="p_add">+.Lfinal:</span>
<span class="p_add">+ENDPROC(memset_orig)</span>
<span class="p_header">diff --git a/tools/include/asm/alternative-asm.h b/tools/include/asm/alternative-asm.h</span>
new file mode 100644
<span class="p_header">index 000000000000..2a4d1bfa2988</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/tools/include/asm/alternative-asm.h</span>
<span class="p_chunk">@@ -0,0 +1,9 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _TOOLS_ASM_ALTERNATIVE_ASM_H</span>
<span class="p_add">+#define _TOOLS_ASM_ALTERNATIVE_ASM_H</span>
<span class="p_add">+</span>
<span class="p_add">+/* Just disable it so we can build arch/x86/lib/memcpy_64.S for perf bench: */</span>
<span class="p_add">+</span>
<span class="p_add">+#define altinstruction_entry #</span>
<span class="p_add">+#define ALTERNATIVE_2 #</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/tools/perf/MANIFEST b/tools/perf/MANIFEST</span>
<span class="p_header">index 0b1ebf3c08f6..cf85d1cd1c91 100644</span>
<span class="p_header">--- a/tools/perf/MANIFEST</span>
<span class="p_header">+++ b/tools/perf/MANIFEST</span>
<span class="p_chunk">@@ -12,6 +12,11 @@</span> <span class="p_context"> tools/arch/sparc/include/asm/barrier_32.h</span>
 tools/arch/sparc/include/asm/barrier_64.h
 tools/arch/tile/include/asm/barrier.h
 tools/arch/x86/include/asm/barrier.h
<span class="p_add">+tools/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_add">+tools/arch/x86/include/asm/disabled-features.h</span>
<span class="p_add">+tools/arch/x86/include/asm/required-features.h</span>
<span class="p_add">+tools/arch/x86/lib/memcpy_64.S</span>
<span class="p_add">+tools/arch/x86/lib/memset_64.S</span>
 tools/arch/xtensa/include/asm/barrier.h
 tools/scripts
 tools/build
<span class="p_chunk">@@ -31,6 +36,7 @@</span> <span class="p_context"> tools/lib/find_bit.c</span>
 tools/lib/bitmap.c
 tools/lib/str_error_r.c
 tools/lib/vsprintf.c
<span class="p_add">+tools/include/asm/alternative-asm.h</span>
 tools/include/asm/atomic.h
 tools/include/asm/barrier.h
 tools/include/asm/bug.h
<span class="p_chunk">@@ -74,9 +80,6 @@</span> <span class="p_context"> include/linux/swab.h</span>
 arch/*/include/asm/unistd*.h
 arch/*/include/uapi/asm/unistd*.h
 arch/*/include/uapi/asm/perf_regs.h
<span class="p_del">-arch/*/lib/memcpy*.S</span>
<span class="p_del">-arch/*/lib/memset*.S</span>
<span class="p_del">-arch/*/include/asm/*features.h</span>
 include/linux/poison.h
 include/linux/hw_breakpoint.h
 include/uapi/linux/bpf.h
<span class="p_header">diff --git a/tools/perf/Makefile.perf b/tools/perf/Makefile.perf</span>
<span class="p_header">index 5e5f8cb1dd83..809735c6cb26 100644</span>
<span class="p_header">--- a/tools/perf/Makefile.perf</span>
<span class="p_header">+++ b/tools/perf/Makefile.perf</span>
<span class="p_chunk">@@ -348,6 +348,21 @@</span> <span class="p_context"> $(PERF_IN): prepare FORCE</span>
 	@(test -f ../../include/uapi/linux/perf_event.h &amp;&amp; ( \
         (diff -B ../include/uapi/linux/perf_event.h ../../include/uapi/linux/perf_event.h &gt;/dev/null) \
         || echo &quot;Warning: tools/include/uapi/linux/perf_event.h differs from kernel&quot; &gt;&amp;2 )) || true
<span class="p_add">+	@(test -f ../../arch/x86/include/asm/disabled-features.h &amp;&amp; ( \</span>
<span class="p_add">+        (diff -B ../arch/x86/include/asm/disabled-features.h ../../arch/x86/include/asm/disabled-features.h &gt;/dev/null) \</span>
<span class="p_add">+        || echo &quot;Warning: tools/arch/x86/include/asm/disabled-features.h differs from kernel&quot; &gt;&amp;2 )) || true</span>
<span class="p_add">+	@(test -f ../../arch/x86/include/asm/required-features.h &amp;&amp; ( \</span>
<span class="p_add">+        (diff -B ../arch/x86/include/asm/required-features.h ../../arch/x86/include/asm/required-features.h &gt;/dev/null) \</span>
<span class="p_add">+        || echo &quot;Warning: tools/arch/x86/include/asm/required-features.h differs from kernel&quot; &gt;&amp;2 )) || true</span>
<span class="p_add">+	@(test -f ../../arch/x86/include/asm/cpufeatures.h &amp;&amp; ( \</span>
<span class="p_add">+        (diff -B ../arch/x86/include/asm/cpufeatures.h ../../arch/x86/include/asm/cpufeatures.h &gt;/dev/null) \</span>
<span class="p_add">+        || echo &quot;Warning: tools/arch/x86/include/asm/cpufeatures.h differs from kernel&quot; &gt;&amp;2 )) || true</span>
<span class="p_add">+	@(test -f ../../arch/x86/lib/memcpy_64.S &amp;&amp; ( \</span>
<span class="p_add">+        (diff -B ../arch/x86/lib/memcpy_64.S ../../arch/x86/lib/memcpy_64.S &gt;/dev/null) \</span>
<span class="p_add">+        || echo &quot;Warning: tools/arch/x86/lib/memcpy_64.S differs from kernel&quot; &gt;&amp;2 )) || true</span>
<span class="p_add">+	@(test -f ../../arch/x86/lib/memset_64.S &amp;&amp; ( \</span>
<span class="p_add">+        (diff -B ../arch/x86/lib/memset_64.S ../../arch/x86/lib/memset_64.S &gt;/dev/null) \</span>
<span class="p_add">+        || echo &quot;Warning: tools/arch/x86/lib/memset_64.S differs from kernel&quot; &gt;&amp;2 )) || true</span>
 	$(Q)$(MAKE) $(build)=perf
 
 $(OUTPUT)perf: $(PERFLIBS) $(PERF_IN) $(LIBTRACEEVENT_DYNAMIC_LIST)
<span class="p_header">diff --git a/tools/perf/bench/mem-memcpy-x86-64-asm.S b/tools/perf/bench/mem-memcpy-x86-64-asm.S</span>
<span class="p_header">index 5c3cce082cb8..f700369bb0f6 100644</span>
<span class="p_header">--- a/tools/perf/bench/mem-memcpy-x86-64-asm.S</span>
<span class="p_header">+++ b/tools/perf/bench/mem-memcpy-x86-64-asm.S</span>
<span class="p_chunk">@@ -6,7 +6,7 @@</span> <span class="p_context"></span>
 #define globl p2align 4; .globl
 #define _ASM_EXTABLE_FAULT(x, y)
 
<span class="p_del">-#include &quot;../../../arch/x86/lib/memcpy_64.S&quot;</span>
<span class="p_add">+#include &quot;../../arch/x86/lib/memcpy_64.S&quot;</span>
 /*
  * We need to provide note.GNU-stack section, saying that we want
  * NOT executable stack. Otherwise the final linking will assume that
<span class="p_header">diff --git a/tools/perf/bench/mem-memset-x86-64-asm.S b/tools/perf/bench/mem-memset-x86-64-asm.S</span>
<span class="p_header">index de278784c866..58407aa24c1b 100644</span>
<span class="p_header">--- a/tools/perf/bench/mem-memset-x86-64-asm.S</span>
<span class="p_header">+++ b/tools/perf/bench/mem-memset-x86-64-asm.S</span>
<span class="p_chunk">@@ -1,7 +1,7 @@</span> <span class="p_context"></span>
 #define memset MEMSET /* don&#39;t hide glibc&#39;s memset() */
 #define altinstr_replacement text
 #define globl p2align 4; .globl
<span class="p_del">-#include &quot;../../../arch/x86/lib/memset_64.S&quot;</span>
<span class="p_add">+#include &quot;../../arch/x86/lib/memset_64.S&quot;</span>
 
 /*
  * We need to provide note.GNU-stack section, saying that we want
<span class="p_header">diff --git a/tools/perf/util/include/asm/alternative-asm.h b/tools/perf/util/include/asm/alternative-asm.h</span>
deleted file mode 100644
<span class="p_header">index 3a3a0f16456a..000000000000</span>
<span class="p_header">--- a/tools/perf/util/include/asm/alternative-asm.h</span>
<span class="p_header">+++ /dev/null</span>
<span class="p_chunk">@@ -1,9 +0,0 @@</span> <span class="p_context"></span>
<span class="p_del">-#ifndef _PERF_ASM_ALTERNATIVE_ASM_H</span>
<span class="p_del">-#define _PERF_ASM_ALTERNATIVE_ASM_H</span>
<span class="p_del">-</span>
<span class="p_del">-/* Just disable it so we can build arch/x86/lib/memcpy_64.S for perf bench: */</span>
<span class="p_del">-</span>
<span class="p_del">-#define altinstruction_entry #</span>
<span class="p_del">-#define ALTERNATIVE_2 #</span>
<span class="p_del">-</span>
<span class="p_del">-#endif</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



