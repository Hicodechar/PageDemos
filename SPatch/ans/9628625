
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,09/16] mm/hmm: heterogeneous memory management (HMM for short) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,09/16] mm/hmm: heterogeneous memory management (HMM for short)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 16, 2017, 4:05 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1489680335-6594-10-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9628625/mbox/"
   >mbox</a>
|
   <a href="/patch/9628625/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9628625/">/patch/9628625/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	4F27E6048C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 15:14:16 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4058D26E4F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 15:14:16 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 34EA4203C0; Thu, 16 Mar 2017 15:14:16 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 42EE2203C0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 15:14:15 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1754223AbdCPPOH (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 16 Mar 2017 11:14:07 -0400
Received: from mx1.redhat.com ([209.132.183.28]:46754 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1753067AbdCPPMP (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 16 Mar 2017 11:12:15 -0400
Received: from smtp.corp.redhat.com
	(int-mx02.intmail.prod.int.phx2.redhat.com [10.5.11.12])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id CC9DD7AE90;
	Thu, 16 Mar 2017 15:04:00 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com CC9DD7AE90
Authentication-Results: ext-mx01.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx01.extmail.prod.ext.phx2.redhat.com;
	spf=pass smtp.mailfrom=jglisse@redhat.com
DKIM-Filter: OpenDKIM Filter v2.11.0 mx1.redhat.com CC9DD7AE90
Received: from xgl-cortex.ml2.eng.bos.redhat.com
	(xgl-cortex.ml2.eng.bos.redhat.com [10.19.160.80])
	by smtp.corp.redhat.com (Postfix) with ESMTP id B3CC7183A3;
	Thu, 16 Mar 2017 15:03:59 +0000 (UTC)
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	David Nellans &lt;dnellans@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Sherry Cheung &lt;SCheung@nvidia.com&gt;, Subhash Gutti &lt;sgutti@nvidia.com&gt;
Subject: [HMM 09/16] mm/hmm: heterogeneous memory management (HMM for short)
Date: Thu, 16 Mar 2017 12:05:28 -0400
Message-Id: &lt;1489680335-6594-10-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1489680335-6594-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1489680335-6594-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.12
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.25]);
	Thu, 16 Mar 2017 15:04:01 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - March 16, 2017, 4:05 p.m.</div>
<pre class="content">
HMM provides 3 separate types of functionality:
    - Mirroring: synchronize CPU page table and device page table
    - Device memory: allocating struct page for device memory
    - Migration: migrating regular memory to device memory

This patch introduces some common helpers and definitions to all of
those 3 functionality.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
---
 MAINTAINERS              |   7 +++
 include/linux/hmm.h      | 153 +++++++++++++++++++++++++++++++++++++++++++++++
 include/linux/mm_types.h |   5 ++
 kernel/fork.c            |   2 +
 mm/Kconfig               |   4 ++
 mm/Makefile              |   1 +
 mm/hmm.c                 |  81 +++++++++++++++++++++++++
 7 files changed, 253 insertions(+)
 create mode 100644 include/linux/hmm.h
 create mode 100644 mm/hmm.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - March 19, 2017, 8:09 p.m.</div>
<pre class="content">
On Thu, Mar 16, 2017 at 12:05:28PM -0400, J?r?me Glisse wrote:
<span class="quote">&gt; HMM provides 3 separate types of functionality:</span>
<span class="quote">&gt;     - Mirroring: synchronize CPU page table and device page table</span>
<span class="quote">&gt;     - Device memory: allocating struct page for device memory</span>
<span class="quote">&gt;     - Migration: migrating regular memory to device memory</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch introduces some common helpers and definitions to all of</span>
<span class="quote">&gt; those 3 functionality.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Evgeny Baskakov &lt;ebaskakov@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt; &lt;SNIP&gt;</span>
<span class="quote">&gt; + * Mirroring:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * HMM provides helpers to mirror a process address space on a device. For this,</span>
<span class="quote">&gt; + * it provides several helpers to order device page table updates with respect</span>
<span class="quote">&gt; + * to CPU page table updates. The requirement is that for any given virtual</span>
<span class="quote">&gt; + * address the CPU and device page table cannot point to different physical</span>
<span class="quote">&gt; + * pages. It uses the mmu_notifier API behind the scenes.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Device memory:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * HMM provides helpers to help leverage device memory. Device memory is, at any</span>
<span class="quote">&gt; + * given time, either CPU-addressable like regular memory, or completely</span>
<span class="quote">&gt; + * unaddressable. In both cases the device memory is associated with dedicated</span>
<span class="quote">&gt; + * struct pages (which are allocated as if for hotplugged memory). Device memory</span>
<span class="quote">&gt; + * management is under the responsibility of the device driver. HMM only</span>
<span class="quote">&gt; + * allocates and initializes the struct pages associated with the device memory,</span>
<span class="quote">&gt; + * by hotplugging a ZONE_DEVICE memory range.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Allocating struct pages for device memory allows us to use device memory</span>
<span class="quote">&gt; + * almost like regular CPU memory. Unlike regular memory, however, it cannot be</span>
<span class="quote">&gt; + * added to the lru, nor can any memory allocation can use device memory</span>
<span class="quote">&gt; + * directly. Device memory will only end up in use by a process if the device</span>
<span class="quote">&gt; + * driver migrates some of the process memory from regular memory to device</span>
<span class="quote">&gt; + * memory.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Migration:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * The existing memory migration mechanism (mm/migrate.c) does not allow using</span>
<span class="quote">&gt; + * anything other than the CPU to copy from source to destination memory.</span>
<span class="quote">&gt; + * Moreover, existing code does not provide a way to migrate based on a virtual</span>
<span class="quote">&gt; + * address range. Existing code only supports struct-page-based migration. Also,</span>
<span class="quote">&gt; + * the migration flow does not allow for graceful failure at intermediate stages</span>
<span class="quote">&gt; + * of the migration process.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * HMM solves all of the above, by providing a simple API:</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + *      hmm_vma_migrate(ops, vma, src_pfns, dst_pfns, start, end, private);</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * finalize_and_map(). The first,  alloc_and_copy(), allocates the destination</span>

Somethinig is missing from that sentence. It doesn&#39;t parse as it is. The
previous helper was migrate_vma from two patches ago so something has
gone side-ways. I think you meant to explain the ops parameter here but
it got munged.

If so, it would best to put the explanation of the API and ops with
their declarations and reference them from here. Someone looking to
understand migrate_vma() will not necessarily be inspired to check hmm.h
for the information.
<span class="quote">
&gt; &lt;SNIP&gt;</span>

Various helpers looked ok
<span class="quote">
&gt; +/* Below are for HMM internal use only! Not to be used by device driver! */</span>
<span class="quote">&gt; +void hmm_mm_destroy(struct mm_struct *mm);</span>
<span class="quote">&gt; +</span>

This will be ignored at least once :) . Not that it matters as assuming
the driver is a module, it&#39;ll not resolve the symbol,
<span class="quote">
&gt; @@ -495,6 +496,10 @@ struct mm_struct {</span>
<span class="quote">&gt;  	atomic_long_t hugetlb_usage;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  	struct work_struct async_put_work;</span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_HMM)</span>
<span class="quote">&gt; +	/* HMM need to track few things per mm */</span>
<span class="quote">&gt; +	struct hmm *hmm;</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>

Inevitable really but not too bad in comparison to putting pfn_to_page
unnecessarily in the fault path or updating every migration user.
<span class="quote">
&gt; @@ -289,6 +289,10 @@ config MIGRATION</span>
<span class="quote">&gt;  config ARCH_ENABLE_HUGEPAGE_MIGRATION</span>
<span class="quote">&gt;  	bool</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config HMM</span>
<span class="quote">&gt; +	bool</span>
<span class="quote">&gt; +	depends on MMU</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config PHYS_ADDR_T_64BIT</span>
<span class="quote">&gt;  	def_bool 64BIT || ARCH_PHYS_ADDR_T_64BIT</span>
<span class="quote">&gt;  </span>

That&#39;s a bit sparse in terms of documentation and information
distribution maintainers if they want to enable this.
<span class="quote">
&gt; &lt;SNIP&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +void hmm_mm_destroy(struct mm_struct *mm)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct hmm *hmm;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We should not need to lock here as no one should be able to register</span>
<span class="quote">&gt; +	 * a new HMM while an mm is being destroy. But just to be safe ...</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt; +	hmm = mm-&gt;hmm;</span>
<span class="quote">&gt; +	mm-&gt;hmm = NULL;</span>
<span class="quote">&gt; +	spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="quote">&gt; +	kfree(hmm);</span>
<span class="quote">&gt; +}</span>

Eh? 

I actually reacted very badly to the locking before I read the comment
to the extent I searched the patch again looking for the locking
documentation that said this was needed.

Ditch that lock, it&#39;s called from mmdrop context and the register
handler doesn&#39;t have the same locking. It&#39;s also expanding the scope of
what that lock is for into an area it does not belong.

Everything else looked fine.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="p_header">index c776906..af37f7c 100644</span>
<span class="p_header">--- a/MAINTAINERS</span>
<span class="p_header">+++ b/MAINTAINERS</span>
<span class="p_chunk">@@ -5964,6 +5964,13 @@</span> <span class="p_context"> S:	Supported</span>
 F:	drivers/scsi/hisi_sas/
 F:	Documentation/devicetree/bindings/scsi/hisilicon-sas.txt
 
<span class="p_add">+HMM - Heterogeneous Memory Management</span>
<span class="p_add">+M:	Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+L:	linux-mm@kvack.org</span>
<span class="p_add">+S:	Maintained</span>
<span class="p_add">+F:	mm/hmm*</span>
<span class="p_add">+F:	include/linux/hmm*</span>
<span class="p_add">+</span>
 HOST AP DRIVER
 M:	Jouni Malinen &lt;j@w1.fi&gt;
 L:	linux-wireless@vger.kernel.org
<span class="p_header">diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
new file mode 100644
<span class="p_header">index 0000000..9fb6767</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/hmm.h</span>
<span class="p_chunk">@@ -0,0 +1,153 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright 2013 Red Hat Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * HMM provides 3 separate types of functionality:</span>
<span class="p_add">+ *   - Mirroring: synchronize CPU page table and device page table</span>
<span class="p_add">+ *   - Device memory: allocating struct pages for device memory</span>
<span class="p_add">+ *   - Migration: migrating regular memory to device memory</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each can be used independently from the others.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Mirroring:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM provides helpers to mirror a process address space on a device. For this,</span>
<span class="p_add">+ * it provides several helpers to order device page table updates with respect</span>
<span class="p_add">+ * to CPU page table updates. The requirement is that for any given virtual</span>
<span class="p_add">+ * address the CPU and device page table cannot point to different physical</span>
<span class="p_add">+ * pages. It uses the mmu_notifier API behind the scenes.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Device memory:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM provides helpers to help leverage device memory. Device memory is, at any</span>
<span class="p_add">+ * given time, either CPU-addressable like regular memory, or completely</span>
<span class="p_add">+ * unaddressable. In both cases the device memory is associated with dedicated</span>
<span class="p_add">+ * struct pages (which are allocated as if for hotplugged memory). Device memory</span>
<span class="p_add">+ * management is under the responsibility of the device driver. HMM only</span>
<span class="p_add">+ * allocates and initializes the struct pages associated with the device memory,</span>
<span class="p_add">+ * by hotplugging a ZONE_DEVICE memory range.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Allocating struct pages for device memory allows us to use device memory</span>
<span class="p_add">+ * almost like regular CPU memory. Unlike regular memory, however, it cannot be</span>
<span class="p_add">+ * added to the lru, nor can any memory allocation can use device memory</span>
<span class="p_add">+ * directly. Device memory will only end up in use by a process if the device</span>
<span class="p_add">+ * driver migrates some of the process memory from regular memory to device</span>
<span class="p_add">+ * memory.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Migration:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The existing memory migration mechanism (mm/migrate.c) does not allow using</span>
<span class="p_add">+ * anything other than the CPU to copy from source to destination memory.</span>
<span class="p_add">+ * Moreover, existing code does not provide a way to migrate based on a virtual</span>
<span class="p_add">+ * address range. Existing code only supports struct-page-based migration. Also,</span>
<span class="p_add">+ * the migration flow does not allow for graceful failure at intermediate stages</span>
<span class="p_add">+ * of the migration process.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM solves all of the above, by providing a simple API:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *      hmm_vma_migrate(ops, vma, src_pfns, dst_pfns, start, end, private);</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * finalize_and_map(). The first,  alloc_and_copy(), allocates the destination</span>
<span class="p_add">+ * memory and initializes it using source memory. Migration can fail at this</span>
<span class="p_add">+ * point, and the device driver then has a place to abort the migration. The</span>
<span class="p_add">+ * finalize_and_map() callback allows the device driver to know which pages</span>
<span class="p_add">+ * were successfully migrated and which were not.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This can easily be used outside of the original HMM use case.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This header file contain all the APIs related to hmm_vma_migrate. Additional</span>
<span class="p_add">+ * detailed documentation may be found below.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef LINUX_HMM_H</span>
<span class="p_add">+#define LINUX_HMM_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/kconfig.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#if IS_ENABLED(CONFIG_HMM)</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_t - HMM use its own pfn type to keep several flags per page</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Flags:</span>
<span class="p_add">+ * HMM_PFN_VALID: pfn is valid</span>
<span class="p_add">+ * HMM_PFN_WRITE: CPU page table have the write permission set</span>
<span class="p_add">+ */</span>
<span class="p_add">+typedef unsigned long hmm_pfn_t;</span>
<span class="p_add">+</span>
<span class="p_add">+#define HMM_PFN_VALID (1 &lt;&lt; 0)</span>
<span class="p_add">+#define HMM_PFN_WRITE (1 &lt;&lt; 1)</span>
<span class="p_add">+#define HMM_PFN_SHIFT 2</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_to_page() - return struct page pointed to by a valid hmm_pfn_t</span>
<span class="p_add">+ * @pfn: hmm_pfn_t to convert to struct page</span>
<span class="p_add">+ * Returns: struct page pointer if pfn is a valid hmm_pfn_t, NULL otherwise</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If the hmm_pfn_t is valid (ie valid flag set) then return the struct page</span>
<span class="p_add">+ * matching the pfn value store in the hmm_pfn_t. Otherwise return NULL.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline struct page *hmm_pfn_to_page(hmm_pfn_t pfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	return pfn_to_page(pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_to_pfn() - return pfn value store in a hmm_pfn_t</span>
<span class="p_add">+ * @pfn: hmm_pfn_t to extract pfn from</span>
<span class="p_add">+ * Returns: pfn value if hmm_pfn_t is valid, -1UL otherwise</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline unsigned long hmm_pfn_to_pfn(hmm_pfn_t pfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!(pfn &amp; HMM_PFN_VALID))</span>
<span class="p_add">+		return -1UL;</span>
<span class="p_add">+	return (pfn &gt;&gt; HMM_PFN_SHIFT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_from_page() - create a valid hmm_pfn_t value from struct page</span>
<span class="p_add">+ * @page: struct page pointer for which to create the hmm_pfn_t</span>
<span class="p_add">+ * Returns: valid hmm_pfn_t for the page</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline hmm_pfn_t hmm_pfn_from_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (page_to_pfn(page) &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_pfn_from_pfn() - create a valid hmm_pfn_t value from pfn</span>
<span class="p_add">+ * @pfn: pfn value for which to create the hmm_pfn_t</span>
<span class="p_add">+ * Returns: valid hmm_pfn_t for the pfn</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline hmm_pfn_t hmm_pfn_from_pfn(unsigned long pfn)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (pfn &lt;&lt; HMM_PFN_SHIFT) | HMM_PFN_VALID;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* Below are for HMM internal use only! Not to be used by device driver! */</span>
<span class="p_add">+void hmm_mm_destroy(struct mm_struct *mm);</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="p_add">+</span>
<span class="p_add">+/* Below are for HMM internal use only! Not to be used by device driver! */</span>
<span class="p_add">+static inline void hmm_mm_destroy(struct mm_struct *mm) {}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* IS_ENABLED(CONFIG_HMM) */</span>
<span class="p_add">+#endif /* LINUX_HMM_H */</span>
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index f60f45f..81068ce 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -23,6 +23,7 @@</span> <span class="p_context"></span>
 
 struct address_space;
 struct mem_cgroup;
<span class="p_add">+struct hmm;</span>
 
 /*
  * Each physical page in the system has a struct page associated with
<span class="p_chunk">@@ -495,6 +496,10 @@</span> <span class="p_context"> struct mm_struct {</span>
 	atomic_long_t hugetlb_usage;
 #endif
 	struct work_struct async_put_work;
<span class="p_add">+#if IS_ENABLED(CONFIG_HMM)</span>
<span class="p_add">+	/* HMM need to track few things per mm */</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+#endif</span>
 };
 
 extern struct mm_struct init_mm;
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 6c463c80..1f8d612 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -37,6 +37,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/binfmts.h&gt;
 #include &lt;linux/mman.h&gt;
 #include &lt;linux/mmu_notifier.h&gt;
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
 #include &lt;linux/fs.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/vmacache.h&gt;
<span class="p_chunk">@@ -863,6 +864,7 @@</span> <span class="p_context"> void __mmdrop(struct mm_struct *mm)</span>
 	BUG_ON(mm == &amp;init_mm);
 	mm_free_pgd(mm);
 	destroy_context(mm);
<span class="p_add">+	hmm_mm_destroy(mm);</span>
 	mmu_notifier_mm_destroy(mm);
 	check_mm(mm);
 	put_user_ns(mm-&gt;user_ns);
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index 9502315..fe8ad24 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -289,6 +289,10 @@</span> <span class="p_context"> config MIGRATION</span>
 config ARCH_ENABLE_HUGEPAGE_MIGRATION
 	bool
 
<span class="p_add">+config HMM</span>
<span class="p_add">+	bool</span>
<span class="p_add">+	depends on MMU</span>
<span class="p_add">+</span>
 config PHYS_ADDR_T_64BIT
 	def_bool 64BIT || ARCH_PHYS_ADDR_T_64BIT
 
<span class="p_header">diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="p_header">index 026f6a8..9eb4121 100644</span>
<span class="p_header">--- a/mm/Makefile</span>
<span class="p_header">+++ b/mm/Makefile</span>
<span class="p_chunk">@@ -75,6 +75,7 @@</span> <span class="p_context"> obj-$(CONFIG_FAILSLAB) += failslab.o</span>
 obj-$(CONFIG_MEMORY_HOTPLUG) += memory_hotplug.o
 obj-$(CONFIG_MEMTEST)		+= memtest.o
 obj-$(CONFIG_MIGRATION) += migrate.o
<span class="p_add">+obj-$(CONFIG_HMM) += hmm.o</span>
 obj-$(CONFIG_QUICKLIST) += quicklist.o
 obj-$(CONFIG_TRANSPARENT_HUGEPAGE) += huge_memory.o khugepaged.o
 obj-$(CONFIG_PAGE_COUNTER) += page_counter.o
<span class="p_header">diff --git a/mm/hmm.c b/mm/hmm.c</span>
new file mode 100644
<span class="p_header">index 0000000..ed3a847</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/hmm.c</span>
<span class="p_chunk">@@ -0,0 +1,81 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright 2013 Red Hat Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Refer to include/linux/hmm.h for information about heterogeneous memory</span>
<span class="p_add">+ * management or HMM for short.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * struct hmm - HMM per mm struct</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mm: mm struct this HMM struct is bound to</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm {</span>
<span class="p_add">+	struct mm_struct	*mm;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * hmm_register - register HMM against an mm (HMM internal)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mm: mm struct to attach to</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is not intended to be used directly by device drivers. It allocates an</span>
<span class="p_add">+ * HMM struct if mm does not have one, and initializes it.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct hmm *hmm_register(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!mm-&gt;hmm) {</span>
<span class="p_add">+		struct hmm *hmm = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		hmm = kmalloc(sizeof(*hmm), GFP_KERNEL);</span>
<span class="p_add">+		if (!hmm)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+		hmm-&gt;mm = mm;</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+		if (!mm-&gt;hmm)</span>
<span class="p_add">+			mm-&gt;hmm = hmm;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			kfree(hmm);</span>
<span class="p_add">+		spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The hmm struct can only be freed once the mm_struct goes away,</span>
<span class="p_add">+	 * hence we should always have pre-allocated an new hmm struct</span>
<span class="p_add">+	 * above.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return mm-&gt;hmm;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void hmm_mm_destroy(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We should not need to lock here as no one should be able to register</span>
<span class="p_add">+	 * a new HMM while an mm is being destroy. But just to be safe ...</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	spin_lock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+	hmm = mm-&gt;hmm;</span>
<span class="p_add">+	mm-&gt;hmm = NULL;</span>
<span class="p_add">+	spin_unlock(&amp;mm-&gt;page_table_lock);</span>
<span class="p_add">+	kfree(hmm);</span>
<span class="p_add">+}</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



