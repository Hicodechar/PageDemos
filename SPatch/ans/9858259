
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>oom_reaper: close race without using oom_lock - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    oom_reaper: close race without using oom_lock</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 23, 2017, 12:41 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;201707230941.BFG30203.OFHSJtFFVQLOMO@I-love.SAKURA.ne.jp&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9858259/mbox/"
   >mbox</a>
|
   <a href="/patch/9858259/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9858259/">/patch/9858259/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	8661260380 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 23 Jul 2017 00:42:22 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6E766284FE
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 23 Jul 2017 00:42:22 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 635CD2850E; Sun, 23 Jul 2017 00:42:22 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 5329B284FE
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Sun, 23 Jul 2017 00:42:20 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755108AbdGWAmM (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Sat, 22 Jul 2017 20:42:12 -0400
Received: from www262.sakura.ne.jp ([202.181.97.72]:23675 &quot;EHLO
	www262.sakura.ne.jp&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751957AbdGWAmK (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Sat, 22 Jul 2017 20:42:10 -0400
Received: from fsav109.sakura.ne.jp (fsav109.sakura.ne.jp [27.133.134.236])
	by www262.sakura.ne.jp (8.14.5/8.14.5) with ESMTP id
	v6N0foGn083812; Sun, 23 Jul 2017 09:41:50 +0900 (JST)
	(envelope-from penguin-kernel@I-love.SAKURA.ne.jp)
Received: from www262.sakura.ne.jp (202.181.97.72) by fsav109.sakura.ne.jp
	(F-Secure/fsigk_smtp/530/fsav109.sakura.ne.jp); 
	Sun, 23 Jul 2017 09:41:50 +0900 (JST)
X-Virus-Status: clean(F-Secure/fsigk_smtp/530/fsav109.sakura.ne.jp)
Received: from AQUA (softbank126227147111.bbtec.net [126.227.147.111])
	(authenticated bits=0)
	by www262.sakura.ne.jp (8.14.5/8.14.5) with ESMTP id v6N0foCR083808; 
	Sun, 23 Jul 2017 09:41:50 +0900 (JST)
	(envelope-from penguin-kernel@I-love.SAKURA.ne.jp)
To: mhocko@kernel.org
Cc: linux-mm@kvack.org, hannes@cmpxchg.org, rientjes@google.com,
	linux-kernel@vger.kernel.org
Subject: Re: [PATCH] oom_reaper: close race without using oom_lock
From: Tetsuo Handa &lt;penguin-kernel@I-love.SAKURA.ne.jp&gt;
References: &lt;20170720141138.GJ9058@dhcp22.suse.cz&gt;
	&lt;201707210647.BDH57894.MQOtFFOJHLSOFV@I-love.SAKURA.ne.jp&gt;
	&lt;20170721150002.GF5944@dhcp22.suse.cz&gt;
	&lt;201707220018.DAE21384.JQFLVMFHSFtOOO@I-love.SAKURA.ne.jp&gt;
	&lt;20170721153353.GG5944@dhcp22.suse.cz&gt;
In-Reply-To: &lt;20170721153353.GG5944@dhcp22.suse.cz&gt;
Message-Id: &lt;201707230941.BFG30203.OFHSJtFFVQLOMO@I-love.SAKURA.ne.jp&gt;
X-Mailer: Winbiff [Version 2.51 PL2]
X-Accept-Language: ja,en,zh
Date: Sun, 23 Jul 2017 09:41:50 +0900
Mime-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a> - July 23, 2017, 12:41 a.m.</div>
<pre class="content">
Michal Hocko wrote:
<span class="quote">&gt; On Sat 22-07-17 00:18:48, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; OK, so let&#39;s say you have another task just about to jump into</span>
<span class="quote">&gt; &gt; &gt; out_of_memory and ... end up in the same situation.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Right.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt;                                                     This race is just</span>
<span class="quote">&gt; &gt; &gt; unavoidable.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; There is no perfect way (always timing dependent). But</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I would rather not add a code which _pretends_ it solves something. If</span>
<span class="quote">&gt; we see the above race a real problem in out there then we should think</span>
<span class="quote">&gt; about how to fix it. I definitely do not want to add more hack into an</span>
<span class="quote">&gt; already complicated code base.</span>

So, how can we verify the above race a real problem? I consider that
it is impossible. The &quot; free:%lukB&quot; field by show_free_areas() is too
random/inaccurate/racy/outdated for evaluating this race window.

Only actually calling alloc_page_from_freelist() immediately after
MMF_OOM_SKIP test (like Patch1 shown below) can evaluate this race window,
but I know that you won&#39;t allow me to add such code to the OOM killer layer.

Your &quot;[RFC PATCH] mm, oom: allow oom reaper to race with exit_mmap&quot; patch
is shown below as Patch2.

My &quot;ignore MMF_OOM_SKIP once&quot; patch is shown below as Patch3.

My &quot;wait for oom_lock&quot; patch is shown below as Patch4.

Patch1:
----------------------------------------
 include/linux/oom.h |  4 ++++
 mm/internal.h       |  4 ++++
 mm/oom_kill.c       | 28 +++++++++++++++++++++++++++-
 mm/page_alloc.c     | 10 +++++++---
 4 files changed, 42 insertions(+), 4 deletions(-)

----------------------------------------

Memory stressor is shown below.
----------------------------------------
#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#include &lt;unistd.h&gt;
#include &lt;sys/types.h&gt;
#include &lt;sys/stat.h&gt;
#include &lt;fcntl.h&gt;
#include &lt;poll.h&gt;

int main(int argc, char *argv[])
{
        static char buffer[4096] = { };
        char *buf = NULL;
        unsigned long size;
        unsigned long i;
        for (i = 0; i &lt; 1024; i++) {
                if (fork() == 0) {
                        int fd = open(&quot;/proc/self/oom_score_adj&quot;, O_WRONLY);
                        write(fd, &quot;1000&quot;, 4);
                        close(fd);
                        sleep(1);
                        if (!i)
                                pause();
                        snprintf(buffer, sizeof(buffer), &quot;/tmp/file.%u&quot;, getpid());
                        fd = open(buffer, O_WRONLY | O_CREAT | O_APPEND, 0600);
                        while (write(fd, buffer, sizeof(buffer)) == sizeof(buffer)) {
                                poll(NULL, 0, 10);
                                fsync(fd);
                        }
                        _exit(0);
                }
        }
        for (size = 1048576; size &lt; 512UL * (1 &lt;&lt; 30); size &lt;&lt;= 1) {
                char *cp = realloc(buf, size);
                if (!cp) {
                        size &gt;&gt;= 1;
                        break;
                }
                buf = cp;
        }
        sleep(2);
        /* Will cause OOM due to overcommit */
        for (i = 0; i &lt; size; i += 4096)
                buf[i] = 0;
        pause();
        return 0;
}
----------------------------------------

Log is at http://I-love.SAKURA.ne.jp/tmp/serial-20170722.txt.xz .

# grep MMF_OOM_SKIP serial-20170722.txt | sed -e &#39;s/=/ /g&#39; | awk &#39; { if ($5 + $7) printf(&quot;%10u %10u %10f\n&quot;, $5, $7, ($5*100/($5+$7))); else printf(&quot;-----\n&quot;); }&#39;
----------------------------------------
----- # Patch1
         0         10   0.000000
         0         25   0.000000
        16        178   8.247423
        16        591   2.635914
        51       1476   3.339882
        51       1517   3.252551
        51       1559   3.167702
        51       1602   3.085299
        51       1646   3.005303
        51       1832   2.708444
        51       1931   2.573158
        51       2141   2.326642
       172       2950   5.509289
       172       4890   3.397866
       471       7916   5.615834
       471       8255   5.397662
       471       8717   5.126252
       471       8954   4.997347
       471       9435   4.754694
       471      10060   4.472510
       471      10840   4.164088
       471      10973   4.115694
       471      12475   3.638189
       471      14318   3.184800
       471      14762   3.091971
       471      16122   2.838546
       471      16433   2.786323
       471      16748   2.735350
       471      17067   2.685597
       471      18507   2.481821
       471      19173   2.397679
       471      22002   2.095848
       471      22173   2.080021
       471      22867   2.018168
       655      26574   2.405524
       655      30397   2.109365
       655      31030   2.067224
       655      32971   1.947897
       655      33414   1.922569
       655      33637   1.910066
       682      34285   1.950410
       682      34740   1.925357
       936      34740   2.623613
       936      34740   2.623613
       936      34777   2.620894
       936      34846   2.615840
       936      35104   2.597114
       968      35377   2.663365
      1046      36776   2.765586
      1099      38417   2.781152
      1176      41715   2.741834
      1176      42957   2.664673
      1286      55200   2.276670
      1640      67105   2.385628
      2138     186214   1.135109
      2138     188287   1.122752
      2138     188288   1.122746
      2164     188724   1.133649
      2164     189131   1.131237
      2164     189432   1.129460
      2164     190152   1.125231
      2164     190323   1.124232
      2164     190890   1.120930
      2164     193030   1.108641
      2164     197603   1.083262
      2283     199866   1.129365
      2283     202543   1.114605
      2283     203293   1.110538
      2437     204552   1.177357
----- # Patch1 + Patch2
         2        151   1.307190
         2        188   1.052632
         2        208   0.952381
         2        208   0.952381
         2        223   0.888889
         8        355   2.203857
        62        640   8.831909
        96       1681   5.402364
        96       3381   2.761001
       190       5403   3.397104
       344      14944   2.250131
       589      31461   1.837754
       589      65517   0.890993
       589      99284   0.589749
       750     204676   0.365095
      1157     283736   0.406117
      1157     286966   0.401565
      1647     368642   0.444788
      4870     494913   0.974423
      8615     646051   1.315938
      9266     743860   1.230339
----- # Patch1 + Patch2 + Patch3
         0         39   0.000000
         0        109   0.000000
         0        189   0.000000
         0        922   0.000000
        31       1101   2.738516
        31       1130   2.670112
        31       1175   2.570481
        31       1214   2.489960
        31       1230   2.458366
      2204      16429  11.828476
      9855      78544  11.148316
     17286     165828   9.440021
     29345     276217   9.603616
     41258     413082   9.080865
     63125     597249   9.558977
     73859     799400   8.457857
    100960     965601   9.465938
    100960     965806   9.464119
    100960     967986   9.444818
    101025     969145   9.440089
    101040     976753   9.374713
    101040     982309   9.326634
    101040     982469   9.325257
    101100     983224   9.323781
    101227     990001   9.276430
    101715    1045386   8.867136
    101968    1063231   8.751123
    103042    1090044   8.636595
    104288    1154220   8.286638
    105186    1230825   7.873139
----- # Patch1 + Patch2 + Patch3 + Patch4
      5400        297  94.786730
      5941       1843  76.323227
      7750       4445  63.550636
      9443       8928  51.401666
     11596      29502  28.215485
     11596     417423   2.702911
     11596     525783   2.157881
     14241     529736   2.617942
     21111     550020   3.696350
     45408     610006   6.928140
     82501     654515  11.193923
     98495     676552  12.708262
    111349     709904  13.558428
    133540     742574  15.242309
    203589     854338  19.244144
    249020    1049335  19.179654
----------------------------------------

The result shows that this race is highly timing dependent, but it
at least shows that it is not rare case that get_page_from_freelist()
can succeed after we checked that victim&#39;s mm already has MMF_OOM_SKIP.

So, how can we check the above race a real problem? I consider that
it is impossible.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 24, 2017, 6:38 a.m.</div>
<pre class="content">
On Sun 23-07-17 09:41:50, Tetsuo Handa wrote:
<span class="quote">&gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Sat 22-07-17 00:18:48, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; OK, so let&#39;s say you have another task just about to jump into</span>
<span class="quote">&gt; &gt; &gt; &gt; out_of_memory and ... end up in the same situation.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Right.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt;                                                     This race is just</span>
<span class="quote">&gt; &gt; &gt; &gt; unavoidable.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; There is no perfect way (always timing dependent). But</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I would rather not add a code which _pretends_ it solves something. If</span>
<span class="quote">&gt; &gt; we see the above race a real problem in out there then we should think</span>
<span class="quote">&gt; &gt; about how to fix it. I definitely do not want to add more hack into an</span>
<span class="quote">&gt; &gt; already complicated code base.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So, how can we verify the above race a real problem?</span>

Try to simulate a _real_ workload and see whether we kill more tasks
than necessary. 
<span class="quote">
&gt; I consider that</span>
<span class="quote">&gt; it is impossible. The &quot; free:%lukB&quot; field by show_free_areas() is too</span>
<span class="quote">&gt; random/inaccurate/racy/outdated for evaluating this race window.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Only actually calling alloc_page_from_freelist() immediately after</span>
<span class="quote">&gt; MMF_OOM_SKIP test (like Patch1 shown below) can evaluate this race window,</span>
<span class="quote">&gt; but I know that you won&#39;t allow me to add such code to the OOM killer layer.</span>

Sigh. It is not about _me_ allowing you something or not. It is about
what makes sense and under which circumstances and usual cost benefit
evaluation. In other words, any patch has to be _justified_. I am really
tired of repeating this simple thing over and over again.

Anyway, the change you are proposing is wrong for two reasons. First,
you are in non-preemptible context in oom_evaluate_task so you cannot
call into get_page_from_freelist (node_reclaim) and secondly it is a
very specific hack while there is a whole category of possible races
where someone frees memory (e.g. and exiting task which smells like what
you see in your testing) while we are selecting an oom victim which
can be quite an expensive operation. Such races are unfortunate but
unavoidable unless we synchronize oom kill with any memory freeing which
smells like a no-go to me. We can try a last allocation attempt right
before we go and kill something (which still wouldn&#39;t be race free) but
that might cause other issues - e.g. prolonged trashing without ever
killing something - but I haven&#39;t evaluated those to be honest.

[...]
<span class="quote">
&gt; The result shows that this race is highly timing dependent, but it</span>
<span class="quote">&gt; at least shows that it is not rare case that get_page_from_freelist()</span>
<span class="quote">&gt; can succeed after we checked that victim&#39;s mm already has MMF_OOM_SKIP.</span>

It might be not rare for the extreme test case you are using. Do not
forget you spawn many tasks and them exiting might race with the oom
selection. I am really skeptical this reflects a real usecase.
<span class="quote">
&gt; So, how can we check the above race a real problem? I consider that</span>
<span class="quote">&gt; it is impossible.</span>

And so I would be rather reluctant to add more hacks^Wheuristics...
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a> - July 26, 2017, 11:33 a.m.</div>
<pre class="content">
Michal Hocko wrote:
<span class="quote">&gt; On Sun 23-07-17 09:41:50, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; So, how can we verify the above race a real problem?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Try to simulate a _real_ workload and see whether we kill more tasks</span>
<span class="quote">&gt; than necessary. </span>

Whether it is a _real_ workload or not cannot become an answer.

If somebody is trying to allocate hundreds/thousands of pages after memory of
an OOM victim was reaped, avoiding this race window makes no sense; next OOM
victim will be selected anyway. But if somebody is trying to allocate only one
page and then is planning to release a lot of memory, avoiding this race window
can save somebody from being OOM-killed needlessly. This race window depends on
what the threads are about to do, not whether the workload is natural or
artificial.

My question is, how can users know it if somebody was OOM-killed needlessly
by allowing MMF_OOM_SKIP to race.
<span class="quote">
&gt; Anyway, the change you are proposing is wrong for two reasons. First,</span>
<span class="quote">&gt; you are in non-preemptible context in oom_evaluate_task so you cannot</span>
<span class="quote">&gt; call into get_page_from_freelist (node_reclaim) and secondly it is a</span>
<span class="quote">&gt; very specific hack while there is a whole category of possible races</span>
<span class="quote">&gt; where someone frees memory (e.g. and exiting task which smells like what</span>
<span class="quote">&gt; you see in your testing) while we are selecting an oom victim which</span>
<span class="quote">&gt; can be quite an expensive operation.</span>

Oh, I didn&#39;t know that get_page_from_freelist() might sleep.
I was assuming that get_page_from_freelist() never sleeps because it is
called from !can_direct_reclaim context. But looking into that function,
it is gfpflags_allow_blocking() from node_reclaim() from
get_page_from_freelist() that prevents !can_direct_reclaim context from
sleeping.

OK. I have to either mask __GFP_DIRECT_RECLAIM or postpone till
oom_kill_process(). Well, I came to worry about get_page_from_freelist()
at __alloc_pages_may_oom() which is called after oom_lock is taken.

Is it guaranteed that __node_reclaim() never (even indirectly) waits for
__GFP_DIRECT_RECLAIM &amp;&amp; !__GFP_NORETRY memory allocation? If it is not
guaranteed, calling __alloc_pages_may_oom(__GFP_DIRECT_RECLAIM) with oom_lock
taken can prevent __GFP_DIRECT_RECLAIM &amp;&amp; !__GFP_NORETRY memory allocation from
completing (because did_some_progress will be forever set to 1 due to oom_lock
already taken). A possible location of OOM lockup unless it is guaranteed.
<span class="quote">
&gt;                                      Such races are unfortunate but</span>
<span class="quote">&gt; unavoidable unless we synchronize oom kill with any memory freeing which</span>
<span class="quote">&gt; smells like a no-go to me. We can try a last allocation attempt right</span>
<span class="quote">&gt; before we go and kill something (which still wouldn&#39;t be race free) but</span>
<span class="quote">&gt; that might cause other issues - e.g. prolonged trashing without ever</span>
<span class="quote">&gt; killing something - but I haven&#39;t evaluated those to be honest.</span>

Yes, postpone last get_page_from_freelist() attempt till oom_kill_process()
will be what we would afford at best.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - July 26, 2017, 11:46 a.m.</div>
<pre class="content">
On Wed 26-07-17 20:33:21, Tetsuo Handa wrote:
<span class="quote">&gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Sun 23-07-17 09:41:50, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; So, how can we verify the above race a real problem?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Try to simulate a _real_ workload and see whether we kill more tasks</span>
<span class="quote">&gt; &gt; than necessary. </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Whether it is a _real_ workload or not cannot become an answer.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If somebody is trying to allocate hundreds/thousands of pages after memory of</span>
<span class="quote">&gt; an OOM victim was reaped, avoiding this race window makes no sense; next OOM</span>
<span class="quote">&gt; victim will be selected anyway. But if somebody is trying to allocate only one</span>
<span class="quote">&gt; page and then is planning to release a lot of memory, avoiding this race window</span>
<span class="quote">&gt; can save somebody from being OOM-killed needlessly. This race window depends on</span>
<span class="quote">&gt; what the threads are about to do, not whether the workload is natural or</span>
<span class="quote">&gt; artificial.</span>

And with a desparate lack of crystal ball we cannot do much about that
really.
<span class="quote">
&gt; My question is, how can users know it if somebody was OOM-killed needlessly</span>
<span class="quote">&gt; by allowing MMF_OOM_SKIP to race.</span>

Is it really important to know that the race is due to MMF_OOM_SKIP?
Isn&#39;t it sufficient to see that we kill too many tasks and then debug it
further once something hits that?

[...]
<span class="quote">&gt; Is it guaranteed that __node_reclaim() never (even indirectly) waits for</span>
<span class="quote">&gt; __GFP_DIRECT_RECLAIM &amp;&amp; !__GFP_NORETRY memory allocation?</span>

this is a direct reclaim which can go down to slab shrinkers with all
the usual fun...
<span class="quote">
&gt; &gt;                                      Such races are unfortunate but</span>
<span class="quote">&gt; &gt; unavoidable unless we synchronize oom kill with any memory freeing which</span>
<span class="quote">&gt; &gt; smells like a no-go to me. We can try a last allocation attempt right</span>
<span class="quote">&gt; &gt; before we go and kill something (which still wouldn&#39;t be race free) but</span>
<span class="quote">&gt; &gt; that might cause other issues - e.g. prolonged trashing without ever</span>
<span class="quote">&gt; &gt; killing something - but I haven&#39;t evaluated those to be honest.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, postpone last get_page_from_freelist() attempt till oom_kill_process()</span>
<span class="quote">&gt; will be what we would afford at best.</span>

as I&#39;ve said this would have to be evaluated very carefully and a strong
usecase would have to be shown.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a> - Aug. 5, 2017, 1:02 a.m.</div>
<pre class="content">
Michal Hocko wrote:
<span class="quote">&gt; On Wed 26-07-17 20:33:21, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; On Sun 23-07-17 09:41:50, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; So, how can we verify the above race a real problem?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Try to simulate a _real_ workload and see whether we kill more tasks</span>
<span class="quote">&gt; &gt; &gt; than necessary. </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Whether it is a _real_ workload or not cannot become an answer.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If somebody is trying to allocate hundreds/thousands of pages after memory of</span>
<span class="quote">&gt; &gt; an OOM victim was reaped, avoiding this race window makes no sense; next OOM</span>
<span class="quote">&gt; &gt; victim will be selected anyway. But if somebody is trying to allocate only one</span>
<span class="quote">&gt; &gt; page and then is planning to release a lot of memory, avoiding this race window</span>
<span class="quote">&gt; &gt; can save somebody from being OOM-killed needlessly. This race window depends on</span>
<span class="quote">&gt; &gt; what the threads are about to do, not whether the workload is natural or</span>
<span class="quote">&gt; &gt; artificial.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And with a desparate lack of crystal ball we cannot do much about that</span>
<span class="quote">&gt; really.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; My question is, how can users know it if somebody was OOM-killed needlessly</span>
<span class="quote">&gt; &gt; by allowing MMF_OOM_SKIP to race.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Is it really important to know that the race is due to MMF_OOM_SKIP?</span>

Yes, it is really important. Needlessly selecting even one OOM victim is
a pain which is difficult to explain to and persuade some of customers.
<span class="quote">
&gt; Isn&#39;t it sufficient to see that we kill too many tasks and then debug it</span>
<span class="quote">&gt; further once something hits that?</span>

It is not sufficient.
<span class="quote">
&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; Is it guaranteed that __node_reclaim() never (even indirectly) waits for</span>
<span class="quote">&gt; &gt; __GFP_DIRECT_RECLAIM &amp;&amp; !__GFP_NORETRY memory allocation?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; this is a direct reclaim which can go down to slab shrinkers with all</span>
<span class="quote">&gt; the usual fun...</span>

Excuse me, but does that mean &quot;Yes, it is&quot; ?

As far as I checked, most shrinkers use non-scheduling operations other than
cond_resched(). But some shrinkers use lock_page()/down_write() etc. I worry
that such shrinkers might wait for __GFP_DIRECT_RECLAIM &amp;&amp; !__GFP_NORETRY
memory allocation (i.e. &quot;No, it isn&#39;t&quot;).
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 7, 2017, 6:02 a.m.</div>
<pre class="content">
On Sat 05-08-17 10:02:55, Tetsuo Handa wrote:
<span class="quote">&gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Wed 26-07-17 20:33:21, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Sun 23-07-17 09:41:50, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; So, how can we verify the above race a real problem?</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Try to simulate a _real_ workload and see whether we kill more tasks</span>
<span class="quote">&gt; &gt; &gt; &gt; than necessary. </span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Whether it is a _real_ workload or not cannot become an answer.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; If somebody is trying to allocate hundreds/thousands of pages after memory of</span>
<span class="quote">&gt; &gt; &gt; an OOM victim was reaped, avoiding this race window makes no sense; next OOM</span>
<span class="quote">&gt; &gt; &gt; victim will be selected anyway. But if somebody is trying to allocate only one</span>
<span class="quote">&gt; &gt; &gt; page and then is planning to release a lot of memory, avoiding this race window</span>
<span class="quote">&gt; &gt; &gt; can save somebody from being OOM-killed needlessly. This race window depends on</span>
<span class="quote">&gt; &gt; &gt; what the threads are about to do, not whether the workload is natural or</span>
<span class="quote">&gt; &gt; &gt; artificial.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; And with a desparate lack of crystal ball we cannot do much about that</span>
<span class="quote">&gt; &gt; really.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; My question is, how can users know it if somebody was OOM-killed needlessly</span>
<span class="quote">&gt; &gt; &gt; by allowing MMF_OOM_SKIP to race.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Is it really important to know that the race is due to MMF_OOM_SKIP?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, it is really important. Needlessly selecting even one OOM victim is</span>
<span class="quote">&gt; a pain which is difficult to explain to and persuade some of customers.</span>

How is this any different from a race with a task exiting an releasing
some memory after we have crossed the point of no return and will kill
something?
<span class="quote">
&gt; &gt; Isn&#39;t it sufficient to see that we kill too many tasks and then debug it</span>
<span class="quote">&gt; &gt; further once something hits that?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is not sufficient.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; Is it guaranteed that __node_reclaim() never (even indirectly) waits for</span>
<span class="quote">&gt; &gt; &gt; __GFP_DIRECT_RECLAIM &amp;&amp; !__GFP_NORETRY memory allocation?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; this is a direct reclaim which can go down to slab shrinkers with all</span>
<span class="quote">&gt; &gt; the usual fun...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Excuse me, but does that mean &quot;Yes, it is&quot; ?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As far as I checked, most shrinkers use non-scheduling operations other than</span>
<span class="quote">&gt; cond_resched(). But some shrinkers use lock_page()/down_write() etc. I worry</span>
<span class="quote">&gt; that such shrinkers might wait for __GFP_DIRECT_RECLAIM &amp;&amp; !__GFP_NORETRY</span>
<span class="quote">&gt; memory allocation (i.e. &quot;No, it isn&#39;t&quot;).</span>

Yes that is possible. Once you are in the shrinker land then you have to
count with everything. And if you want to imply that
get_page_from_freelist inside __alloc_pages_may_oom may lockup while
holding the oom_lock then you might be right but I haven&#39;t checked that
too deeply. It might be very well possible that the node reclaim bails
out early when we are under OOM.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 10, 2017, 11:34 a.m.</div>
<pre class="content">
On Tue 08-08-17 11:14:50, Tetsuo Handa wrote:
<span class="quote">&gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Sat 05-08-17 10:02:55, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; On Wed 26-07-17 20:33:21, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; On Sun 23-07-17 09:41:50, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; &gt; So, how can we verify the above race a real problem?</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; Try to simulate a _real_ workload and see whether we kill more tasks</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; &gt; than necessary. </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; Whether it is a _real_ workload or not cannot become an answer.</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; If somebody is trying to allocate hundreds/thousands of pages after memory of</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; an OOM victim was reaped, avoiding this race window makes no sense; next OOM</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; victim will be selected anyway. But if somebody is trying to allocate only one</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; page and then is planning to release a lot of memory, avoiding this race window</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; can save somebody from being OOM-killed needlessly. This race window depends on</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; what the threads are about to do, not whether the workload is natural or</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; artificial.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; And with a desparate lack of crystal ball we cannot do much about that</span>
<span class="quote">&gt; &gt; &gt; &gt; really.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; My question is, how can users know it if somebody was OOM-killed needlessly</span>
<span class="quote">&gt; &gt; &gt; &gt; &gt; by allowing MMF_OOM_SKIP to race.</span>
<span class="quote">&gt; &gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; &gt; Is it really important to know that the race is due to MMF_OOM_SKIP?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Yes, it is really important. Needlessly selecting even one OOM victim is</span>
<span class="quote">&gt; &gt; &gt; a pain which is difficult to explain to and persuade some of customers.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; How is this any different from a race with a task exiting an releasing</span>
<span class="quote">&gt; &gt; some memory after we have crossed the point of no return and will kill</span>
<span class="quote">&gt; &gt; something?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m not complaining about an exiting task releasing some memory after we have</span>
<span class="quote">&gt; crossed the point of no return.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What I&#39;m saying is that we can postpone &quot;the point of no return&quot; if we ignore</span>
<span class="quote">&gt; MMF_OOM_SKIP for once (both this &quot;oom_reaper: close race without using oom_lock&quot;</span>
<span class="quote">&gt; thread and &quot;mm, oom: task_will_free_mem(current) should ignore MMF_OOM_SKIP for</span>
<span class="quote">&gt; once.&quot; thread). These are race conditions we can avoid without crystal ball.</span>

If those races are really that common than we can handle them even
without &quot;try once more&quot; tricks. Really this is just an ugly hack. If you
really care then make sure that we always try to allocate from memory
reserves before going down the oom path. In other words, try to find a
robust solution rather than tweaks around a problem.

[...]
<span class="quote">&gt; &gt; Yes that is possible. Once you are in the shrinker land then you have to</span>
<span class="quote">&gt; &gt; count with everything. And if you want to imply that</span>
<span class="quote">&gt; &gt; get_page_from_freelist inside __alloc_pages_may_oom may lockup while</span>
<span class="quote">&gt; &gt; holding the oom_lock then you might be right but I haven&#39;t checked that</span>
<span class="quote">&gt; &gt; too deeply. It might be very well possible that the node reclaim bails</span>
<span class="quote">&gt; &gt; out early when we are under OOM.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, I worry that get_page_from_freelist() with oom_lock held might lockup.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If we are about to invoke the OOM killer for the first time, it is likely that</span>
<span class="quote">&gt; __node_reclaim() finds nothing to reclaim and will bail out immediately. But if</span>
<span class="quote">&gt; we are about to invoke the OOM killer again, it is possible that small amount of</span>
<span class="quote">&gt; memory was reclaimed by the OOM killer/reaper, and all reclaimed memory was assigned</span>
<span class="quote">&gt; to things which __node_reclaim() will find and try to reclaim, and any thread which</span>
<span class="quote">&gt; took oom_lock will call __node_reclaim() and __node_reclaim() find something</span>
<span class="quote">&gt; reclaimable if __GFP_DIRECT_RECLAIM &amp;&amp; !__GFP_NORETRY memory allocation is involved.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We should consider such situation volatile (i.e. should not make assumption that</span>
<span class="quote">&gt; get_page_from_freelist() with oom_lock held shall bail out immediately) if shrinkers</span>
<span class="quote">&gt; which (directly or indirectly) involve __GFP_DIRECT_RECLAIM &amp;&amp; !__GFP_NORETRY memory</span>
<span class="quote">&gt; allocation are permitted.</span>

Well, I think you are so focused on details that you most probably miss
a large picture here. Just think about the purpose of the node reclaim.
It is there to _prefer_ local allocations than go to a distant NUMA
node. So rather than speculating about details maybe it makes sense to
consider whether it actually makes any sense to even try to node reclaim
when we are OOM. In other words why to do an additional reclaim when we
just found out that all reclaim attempts have failed...
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/oom.h b/include/linux/oom.h</span>
<span class="p_header">index 8a266e2..1b0bbb6 100644</span>
<span class="p_header">--- a/include/linux/oom.h</span>
<span class="p_header">+++ b/include/linux/oom.h</span>
<span class="p_chunk">@@ -11,6 +11,7 @@</span> <span class="p_context"></span>
 struct notifier_block;
 struct mem_cgroup;
 struct task_struct;
<span class="p_add">+struct alloc_context;</span>
 
 /*
  * Details of the page allocation that triggered the oom killer that are used to
<span class="p_chunk">@@ -39,6 +40,9 @@</span> <span class="p_context"> struct oom_control {</span>
 	unsigned long totalpages;
 	struct task_struct *chosen;
 	unsigned long chosen_points;
<span class="p_add">+</span>
<span class="p_add">+	const struct alloc_context *alloc_context;</span>
<span class="p_add">+	unsigned int alloc_flags;</span>
 };
 
 extern struct mutex oom_lock;
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 24d88f0..95a08b5 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -522,4 +522,8 @@</span> <span class="p_context"> static inline bool is_migrate_highatomic_page(struct page *page)</span>
 	return get_pageblock_migratetype(page) == MIGRATE_HIGHATOMIC;
 }
 
<span class="p_add">+struct page *get_page_from_freelist(gfp_t gfp_mask, unsigned int order,</span>
<span class="p_add">+				    int alloc_flags,</span>
<span class="p_add">+				    const struct alloc_context *ac);</span>
<span class="p_add">+</span>
 #endif	/* __MM_INTERNAL_H */
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 9e8b4f0..fb7b2c8 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -288,6 +288,9 @@</span> <span class="p_context"> static enum oom_constraint constrained_alloc(struct oom_control *oc)</span>
 	return CONSTRAINT_NONE;
 }
 
<span class="p_add">+static unsigned int mmf_oom_skip_raced;</span>
<span class="p_add">+static unsigned int mmf_oom_skip_not_raced;</span>
<span class="p_add">+</span>
 static int oom_evaluate_task(struct task_struct *task, void *arg)
 {
 	struct oom_control *oc = arg;
<span class="p_chunk">@@ -303,8 +306,21 @@</span> <span class="p_context"> static int oom_evaluate_task(struct task_struct *task, void *arg)</span>
 	 * any memory is quite low.
 	 */
 	if (!is_sysrq_oom(oc) &amp;&amp; tsk_is_oom_victim(task)) {
<span class="p_del">-		if (test_bit(MMF_OOM_SKIP, &amp;task-&gt;signal-&gt;oom_mm-&gt;flags))</span>
<span class="p_add">+		if (test_bit(MMF_OOM_SKIP, &amp;task-&gt;signal-&gt;oom_mm-&gt;flags)) {</span>
<span class="p_add">+			const struct alloc_context *ac = oc-&gt;alloc_context;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (ac) {</span>
<span class="p_add">+				struct page *page = get_page_from_freelist</span>
<span class="p_add">+					(oc-&gt;gfp_mask, oc-&gt;order,</span>
<span class="p_add">+					 oc-&gt;alloc_flags, ac);</span>
<span class="p_add">+				if (page) {</span>
<span class="p_add">+					__free_pages(page, oc-&gt;order);</span>
<span class="p_add">+					mmf_oom_skip_raced++;</span>
<span class="p_add">+				} else</span>
<span class="p_add">+					mmf_oom_skip_not_raced++;</span>
<span class="p_add">+			}</span>
 			goto next;
<span class="p_add">+		}</span>
 		goto abort;
 	}
 
<span class="p_chunk">@@ -1059,6 +1075,16 @@</span> <span class="p_context"> bool out_of_memory(struct oom_control *oc)</span>
 		 */
 		schedule_timeout_killable(1);
 	}
<span class="p_add">+	{</span>
<span class="p_add">+		static unsigned long last;</span>
<span class="p_add">+		unsigned long now = jiffies;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!last || time_after(now, last + 5 * HZ)) {</span>
<span class="p_add">+			last = now;</span>
<span class="p_add">+			pr_info(&quot;MMF_OOM_SKIP: raced=%u not_raced=%u\n&quot;,</span>
<span class="p_add">+				mmf_oom_skip_raced, mmf_oom_skip_not_raced);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
 	return !!oc-&gt;chosen;
 }
 
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 80e4adb..4cf2861 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -3054,7 +3054,7 @@</span> <span class="p_context"> static bool zone_allows_reclaim(struct zone *local_zone, struct zone *zone)</span>
  * get_page_from_freelist goes through the zonelist trying to allocate
  * a page.
  */
<span class="p_del">-static struct page *</span>
<span class="p_add">+struct page *</span>
 get_page_from_freelist(gfp_t gfp_mask, unsigned int order, int alloc_flags,
 						const struct alloc_context *ac)
 {
<span class="p_chunk">@@ -3245,7 +3245,8 @@</span> <span class="p_context"> void warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...)</span>
 
 static inline struct page *
 __alloc_pages_may_oom(gfp_t gfp_mask, unsigned int order,
<span class="p_del">-	const struct alloc_context *ac, unsigned long *did_some_progress)</span>
<span class="p_add">+		      unsigned int alloc_flags, const struct alloc_context *ac,</span>
<span class="p_add">+		      unsigned long *did_some_progress)</span>
 {
 	struct oom_control oc = {
 		.zonelist = ac-&gt;zonelist,
<span class="p_chunk">@@ -3253,6 +3254,8 @@</span> <span class="p_context"> void warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...)</span>
 		.memcg = NULL,
 		.gfp_mask = gfp_mask,
 		.order = order,
<span class="p_add">+		.alloc_context = ac,</span>
<span class="p_add">+		.alloc_flags = alloc_flags,</span>
 	};
 	struct page *page;
 
<span class="p_chunk">@@ -3955,7 +3958,8 @@</span> <span class="p_context"> bool gfp_pfmemalloc_allowed(gfp_t gfp_mask)</span>
 		goto retry_cpuset;
 
 	/* Reclaim has failed us, start killing things */
<span class="p_del">-	page = __alloc_pages_may_oom(gfp_mask, order, ac, &amp;did_some_progress);</span>
<span class="p_add">+	page = __alloc_pages_may_oom(gfp_mask, order, alloc_flags, ac,</span>
<span class="p_add">+				     &amp;did_some_progress);</span>
 	if (page)
 		goto got_pg;
 
<span class="p_del">----------------------------------------</span>

Patch2:
<span class="p_del">----------------------------------------</span>
 mm/mmap.c     |  7 +++++++
 mm/oom_kill.c | 35 +++++------------------------------
 2 files changed, 12 insertions(+), 30 deletions(-)

<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index f19efcf..669f07d 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -2993,6 +2993,11 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 	/* Use -1 here to ensure all VMAs in the mm are unmapped */
 	unmap_vmas(&amp;tlb, vma, 0, -1);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * oom reaper might race with exit_mmap so make sure we won&#39;t free</span>
<span class="p_add">+	 * page tables or unmap VMAs under its feet</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	down_write(&amp;mm-&gt;mmap_sem);</span>
 	free_pgtables(&amp;tlb, vma, FIRST_USER_ADDRESS, USER_PGTABLES_CEILING);
 	tlb_finish_mmu(&amp;tlb, 0, -1);
 
<span class="p_chunk">@@ -3005,7 +3010,9 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 			nr_accounted += vma_pages(vma);
 		vma = remove_vma(vma);
 	}
<span class="p_add">+	mm-&gt;mmap = NULL;</span>
 	vm_unacct_memory(nr_accounted);
<span class="p_add">+	up_write(&amp;mm-&gt;mmap_sem);</span>
 }
 
 /* Insert vm structure into process list sorted by address
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index fb7b2c8..3ef14f0 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -486,39 +486,16 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 {
 	struct mmu_gather tlb;
 	struct vm_area_struct *vma;
<span class="p_del">-	bool ret = true;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We have to make sure to not race with the victim exit path</span>
<span class="p_del">-	 * and cause premature new oom victim selection:</span>
<span class="p_del">-	 * __oom_reap_task_mm		exit_mm</span>
<span class="p_del">-	 *   mmget_not_zero</span>
<span class="p_del">-	 *				  mmput</span>
<span class="p_del">-	 *				    atomic_dec_and_test</span>
<span class="p_del">-	 *				  exit_oom_victim</span>
<span class="p_del">-	 *				[...]</span>
<span class="p_del">-	 *				out_of_memory</span>
<span class="p_del">-	 *				  select_bad_process</span>
<span class="p_del">-	 *				    # no TIF_MEMDIE task selects new victim</span>
<span class="p_del">-	 *  unmap_page_range # frees some memory</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	mutex_lock(&amp;oom_lock);</span>
 
 	if (!down_read_trylock(&amp;mm-&gt;mmap_sem)) {
<span class="p_del">-		ret = false;</span>
 		trace_skip_task_reaping(tsk-&gt;pid);
<span class="p_del">-		goto unlock_oom;</span>
<span class="p_add">+		return false;</span>
 	}
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * increase mm_users only after we know we will reap something so</span>
<span class="p_del">-	 * that the mmput_async is called only when we have reaped something</span>
<span class="p_del">-	 * and delayed __mmput doesn&#39;t matter that much</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!mmget_not_zero(mm)) {</span>
<span class="p_add">+	/* There is nothing to reap so bail out without signs in the log */</span>
<span class="p_add">+	if (!mm-&gt;mmap) {</span>
 		up_read(&amp;mm-&gt;mmap_sem);
<span class="p_del">-		trace_skip_task_reaping(tsk-&gt;pid);</span>
<span class="p_del">-		goto unlock_oom;</span>
<span class="p_add">+		return true;</span>
 	}
 
 	trace_start_task_reaping(tsk-&gt;pid);
<span class="p_chunk">@@ -565,9 +542,7 @@</span> <span class="p_context"> static bool __oom_reap_task_mm(struct task_struct *tsk, struct mm_struct *mm)</span>
 	 */
 	mmput_async(mm);
 	trace_finish_task_reaping(tsk-&gt;pid);
<span class="p_del">-unlock_oom:</span>
<span class="p_del">-	mutex_unlock(&amp;oom_lock);</span>
<span class="p_del">-	return ret;</span>
<span class="p_add">+	return true;</span>
 }
 
 #define MAX_OOM_REAP_RETRIES 10
<span class="p_del">----------------------------------------</span>

Patch3:
<span class="p_del">----------------------------------------</span>
 mm/oom_kill.c | 8 ++++++--
 1 file changed, 6 insertions(+), 2 deletions(-)

<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 3ef14f0..9cc6634 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -306,7 +306,7 @@</span> <span class="p_context"> static int oom_evaluate_task(struct task_struct *task, void *arg)</span>
 	 * any memory is quite low.
 	 */
 	if (!is_sysrq_oom(oc) &amp;&amp; tsk_is_oom_victim(task)) {
<span class="p_del">-		if (test_bit(MMF_OOM_SKIP, &amp;task-&gt;signal-&gt;oom_mm-&gt;flags)) {</span>
<span class="p_add">+		if (task-&gt;signal-&gt;oom_mm-&gt;async_put_work.func) {</span>
 			const struct alloc_context *ac = oc-&gt;alloc_context;
 
 			if (ac) {
<span class="p_chunk">@@ -321,6 +321,8 @@</span> <span class="p_context"> static int oom_evaluate_task(struct task_struct *task, void *arg)</span>
 			}
 			goto next;
 		}
<span class="p_add">+		if (test_bit(MMF_OOM_SKIP, &amp;task-&gt;signal-&gt;oom_mm-&gt;flags))</span>
<span class="p_add">+			task-&gt;signal-&gt;oom_mm-&gt;async_put_work.func = (void *) 1;</span>
 		goto abort;
 	}
 
<span class="p_chunk">@@ -652,8 +654,10 @@</span> <span class="p_context"> static void mark_oom_victim(struct task_struct *tsk)</span>
 		return;
 
 	/* oom_mm is bound to the signal struct life time. */
<span class="p_del">-	if (!cmpxchg(&amp;tsk-&gt;signal-&gt;oom_mm, NULL, mm))</span>
<span class="p_add">+	if (!cmpxchg(&amp;tsk-&gt;signal-&gt;oom_mm, NULL, mm)) {</span>
 		mmgrab(tsk-&gt;signal-&gt;oom_mm);
<span class="p_add">+		tsk-&gt;signal-&gt;oom_mm-&gt;async_put_work.func = NULL;</span>
<span class="p_add">+	}</span>
 
 	/*
 	 * Make sure that the task is woken up from uninterruptible sleep
<span class="p_del">----------------------------------------</span>

Patch4:
<span class="p_del">----------------------------------------</span>
 mm/page_alloc.c | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-)

<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 4cf2861..3e0e7da 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -3265,7 +3265,7 @@</span> <span class="p_context"> void warn_alloc(gfp_t gfp_mask, nodemask_t *nodemask, const char *fmt, ...)</span>
 	 * Acquire the oom lock.  If that fails, somebody else is
 	 * making progress for us.
 	 */
<span class="p_del">-	if (!mutex_trylock(&amp;oom_lock)) {</span>
<span class="p_add">+	if (mutex_lock_killable(&amp;oom_lock)) {</span>
 		*did_some_progress = 1;
 		schedule_timeout_uninterruptible(1);
 		return NULL;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



