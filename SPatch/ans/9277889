
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86 fixes - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86 fixes</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 12, 2016, 7:46 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160812194646.GA30997@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9277889/mbox/"
   >mbox</a>
|
   <a href="/patch/9277889/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9277889/">/patch/9277889/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	7F4D7600CB for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 12 Aug 2016 19:47:00 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 6957228AE5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 12 Aug 2016 19:47:00 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 5E34A28AE8; Fri, 12 Aug 2016 19:47:00 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	FSL_HELO_FAKE, RCVD_IN_DNSWL_HI,
	T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 91C8A28AE5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 12 Aug 2016 19:46:58 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752554AbcHLTqy (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 12 Aug 2016 15:46:54 -0400
Received: from mail-wm0-f65.google.com ([74.125.82.65]:33038 &quot;EHLO
	mail-wm0-f65.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751031AbcHLTqw (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 12 Aug 2016 15:46:52 -0400
Received: by mail-wm0-f65.google.com with SMTP id o80so4747468wme.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Fri, 12 Aug 2016 12:46:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:user-agent;
	bh=AEzX7y5PMHBOGlNi7xIjf4sCaOBbjBfEx2RWmhxnjjk=;
	b=fG1PvmisxAE+x9e0C8ojbFRdHe6op5FziuL7+VCuvIkQIm1baMcXLFLpZ9cf4KIV1c
	NE8OYz7BgEiJjKgt6C4uSs58nem7MzqnBkNX/DovwGxaAi4Mtt3v+tBs5cIkTlrsj09b
	8WU8HsgopwQeZyPVkjcNcK5gnNowlLQtM1+zzPXgCkr7g/uDZfcKW2tfMg/zc8uPtdsF
	RS/6mEVLP2I0yfOkBuiGK4HbwPB8sOfATWaVBlMZWZi9bgH+6eNO07HwV6p1jthp/fV6
	bPFt2dF80WExtTJiGLSvC0zcW32tgYpstzKumclwPl+l/tfhRJkzR8UEt6oqIQDGe2oW
	lPRg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:user-agent;
	bh=AEzX7y5PMHBOGlNi7xIjf4sCaOBbjBfEx2RWmhxnjjk=;
	b=Unxn1DdxpWq+pMuTY36Yr+NygTnt/ffHxQhv6Ai2yoDkC57ZireTyFQ0ZbUPqVWdoE
	unQlwMHLKAbCRJM2eaLMXY+eMZVrl7XxDLhN8CVCEIe8fwN14yCoPEdTZ44dDIZzjmQV
	rQ9RTG1qX7ppuZ6zgA/iI75StXge8Mfk+gdHoP9UMmarW/cm5OHekptTddpoCzNJx/DE
	YxIISRrnBo0XZYIkVbsiaEubOvF+Vh8kM+oen4m+af2WZFWBVGs51JawykWsoDhsnAuN
	eDszYKAdhu3AbJ0mqFX8jRwui1fHFfJTF2Kc1f/hL7qUod3QMre09Lag5vpbXEIv/cmU
	AYcA==
X-Gm-Message-State: AEkoouui1e5PANEbVn1HiUGmixMYu34vKFbP2NmKgofsMPaP9LkdDuEzrOY6873zMkVXKg==
X-Received: by 10.28.88.144 with SMTP id m138mr506759wmb.79.1471031210309;
	Fri, 12 Aug 2016 12:46:50 -0700 (PDT)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	d80sm3901484wmd.14.2016.08.12.12.46.47
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Fri, 12 Aug 2016 12:46:48 -0700 (PDT)
Date: Fri, 12 Aug 2016 21:46:46 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;
Subject: [GIT PULL] x86 fixes
Message-ID: &lt;20160812194646.GA30997@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.24 (2015-08-30)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Aug. 12, 2016, 7:46 p.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-urgent-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-urgent-for-linus

   # HEAD: d52c0569bab4edc888832df44dc7ac28517134f6 x86/apic/x2apic, smp/hotplug: Don&#39;t use before alloc in x2apic_cluster_probe()

This is bigger than usual - the reason is partly a pent-up stream of fixes after 
the merge window and partly accidental. The fixes are:

 - 5 patches to fix a boot failure on Andy Lutomirsky&#39;s laptop.
 - a KASAN fix
 - 4 SGI UV platform fixes
 - a warnings fix
 - a documentation update
 - a swap entry definition fix
 - a pkeys fix
 - an irq stats fix

 Thanks,

	Ingo

------------------&gt;
Aaron Lu (1):
      x86/irq: Do not substract irq_tlb_count from irq_call_count

Alexander Potapenko (1):
      x86, kasan, ftrace: Put APIC interrupt handlers into .irqentry.text

Andy Lutomirski (5):
      x86/boot: Run reserve_bios_regions() after we initialize the memory map
      x86/boot: Synchronize trampoline_cr4_features and mmu_cr4_features directly
      x86/boot: Defer setup_real_mode() to early_initcall time
      x86/boot: Rework reserve_real_mode() to allow multiple tries
      x86/efi: Allocate a trampoline if needed in efi_free_boot_services()

Borislav Petkov (1):
      x86/entry: Clarify the RF saving/restoring situation with SYSCALL/SYSRET

Dave Hansen (2):
      x86/mm/pkeys: Fix compact mode by removing protection keys&#39; XSAVE buffer manipulation
      x86/mm: Fix swap entry comment and macro

Mike Travis (4):
      x86/platform/UV: Fix problem with UV4 Socket IDs not being contiguous
      x86/platform/UV: Fix bug with iounmap() of the UV4 EFI System Table causing a crash
      x86/platform/UV: Fix problem with UV4 BIOS providing incorrect PXM values
      x86/platform/UV: Fix kernel panic running RHEL kdump kernel on UV systems

Nicolas Iooss (1):
      x86/mm/kaslr: Fix -Wformat-security warning

Sebastian Andrzej Siewior (2):
      x86/mm: Disable preemption during CR3 read+write
      x86/apic/x2apic, smp/hotplug: Don&#39;t use before alloc in x2apic_cluster_probe()

Thomas Garnier (2):
      x86/mm/KASLR: Fix physical memory calculation on KASLR memory randomization
      x86/mm/KASLR: Increase BRK pages for KASLR memory randomization

Valdis Kletnieks (1):
      x86/build: Reduce the W=1 warnings noise when compiling x86 syscall tables


 arch/x86/entry/Makefile               |   2 +
 arch/x86/entry/entry_64.S             |  25 ++++--
 arch/x86/include/asm/hardirq.h        |   4 -
 arch/x86/include/asm/pgtable_64.h     |   4 +-
 arch/x86/include/asm/realmode.h       |  10 ++-
 arch/x86/include/asm/tlbflush.h       |   7 ++
 arch/x86/include/asm/uv/bios.h        |   5 +-
 arch/x86/kernel/apic/x2apic_cluster.c |  13 +++-
 arch/x86/kernel/apic/x2apic_uv_x.c    |  42 +++++------
 arch/x86/kernel/fpu/xstate.c          | 138 +++++-----------------------------
 arch/x86/kernel/head32.c              |   2 -
 arch/x86/kernel/head64.c              |   1 -
 arch/x86/kernel/irq.c                 |   3 +-
 arch/x86/kernel/setup.c               |  27 ++++---
 arch/x86/lib/kaslr.c                  |   2 +-
 arch/x86/mm/init.c                    |  14 +++-
 arch/x86/mm/kaslr.c                   |   2 +-
 arch/x86/platform/efi/quirks.c        |  21 ++++++
 arch/x86/platform/uv/bios_uv.c        |   8 +-
 arch/x86/realmode/init.c              |  47 +++++++++---
 20 files changed, 182 insertions(+), 195 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/entry/Makefile b/arch/x86/entry/Makefile</span>
<span class="p_header">index fe91c25092da..77f28ce9c646 100644</span>
<span class="p_header">--- a/arch/x86/entry/Makefile</span>
<span class="p_header">+++ b/arch/x86/entry/Makefile</span>
<span class="p_chunk">@@ -5,6 +5,8 @@</span> <span class="p_context"></span>
 OBJECT_FILES_NON_STANDARD_entry_$(BITS).o   := y
 OBJECT_FILES_NON_STANDARD_entry_64_compat.o := y
 
<span class="p_add">+CFLAGS_syscall_64.o		+= -Wno-override-init</span>
<span class="p_add">+CFLAGS_syscall_32.o		+= -Wno-override-init</span>
 obj-y				:= entry_$(BITS).o thunk_$(BITS).o syscall_$(BITS).o
 obj-y				+= common.o
 
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index b846875aeea6..d172c619c449 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -288,11 +288,15 @@</span> <span class="p_context"> GLOBAL(entry_SYSCALL_64_after_swapgs)</span>
 	jne	opportunistic_sysret_failed
 
 	/*
<span class="p_del">-	 * SYSRET can&#39;t restore RF.  SYSRET can restore TF, but unlike IRET,</span>
<span class="p_del">-	 * restoring TF results in a trap from userspace immediately after</span>
<span class="p_del">-	 * SYSRET.  This would cause an infinite loop whenever #DB happens</span>
<span class="p_del">-	 * with register state that satisfies the opportunistic SYSRET</span>
<span class="p_del">-	 * conditions.  For example, single-stepping this user code:</span>
<span class="p_add">+	 * SYSCALL clears RF when it saves RFLAGS in R11 and SYSRET cannot</span>
<span class="p_add">+	 * restore RF properly. If the slowpath sets it for whatever reason, we</span>
<span class="p_add">+	 * need to restore it correctly.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * SYSRET can restore TF, but unlike IRET, restoring TF results in a</span>
<span class="p_add">+	 * trap from userspace immediately after SYSRET.  This would cause an</span>
<span class="p_add">+	 * infinite loop whenever #DB happens with register state that satisfies</span>
<span class="p_add">+	 * the opportunistic SYSRET conditions.  For example, single-stepping</span>
<span class="p_add">+	 * this user code:</span>
 	 *
 	 *           movq	$stuck_here, %rcx
 	 *           pushfq
<span class="p_chunk">@@ -601,9 +605,20 @@</span> <span class="p_context"> apicinterrupt3 \num trace(\sym) smp_trace(\sym)</span>
 .endm
 #endif
 
<span class="p_add">+/* Make sure APIC interrupt handlers end up in the irqentry section: */</span>
<span class="p_add">+#if defined(CONFIG_FUNCTION_GRAPH_TRACER) || defined(CONFIG_KASAN)</span>
<span class="p_add">+# define PUSH_SECTION_IRQENTRY	.pushsection .irqentry.text, &quot;ax&quot;</span>
<span class="p_add">+# define POP_SECTION_IRQENTRY	.popsection</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define PUSH_SECTION_IRQENTRY</span>
<span class="p_add">+# define POP_SECTION_IRQENTRY</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 .macro apicinterrupt num sym do_sym
<span class="p_add">+PUSH_SECTION_IRQENTRY</span>
 apicinterrupt3 \num \sym \do_sym
 trace_apicinterrupt \num \sym
<span class="p_add">+POP_SECTION_IRQENTRY</span>
 .endm
 
 #ifdef CONFIG_SMP
<span class="p_header">diff --git a/arch/x86/include/asm/hardirq.h b/arch/x86/include/asm/hardirq.h</span>
<span class="p_header">index 7178043b0e1d..59405a248fc2 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/hardirq.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/hardirq.h</span>
<span class="p_chunk">@@ -22,10 +22,6 @@</span> <span class="p_context"> typedef struct {</span>
 #ifdef CONFIG_SMP
 	unsigned int irq_resched_count;
 	unsigned int irq_call_count;
<span class="p_del">-	/*</span>
<span class="p_del">-	 * irq_tlb_count is double-counted in irq_call_count, so it must be</span>
<span class="p_del">-	 * subtracted from irq_call_count when displaying irq_call_count</span>
<span class="p_del">-	 */</span>
 	unsigned int irq_tlb_count;
 #endif
 #ifdef CONFIG_X86_THERMAL_VECTOR
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index 7e8ec7ae10fa..1cc82ece9ac1 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -145,7 +145,7 @@</span> <span class="p_context"> static inline int pgd_large(pgd_t pgd) { return 0; }</span>
  *
  * |     ...            | 11| 10|  9|8|7|6|5| 4| 3|2|1|0| &lt;- bit number
  * |     ...            |SW3|SW2|SW1|G|L|D|A|CD|WT|U|W|P| &lt;- bit names
<span class="p_del">- * | OFFSET (14-&gt;63) | TYPE (10-13) |0|X|X|X| X| X|X|X|0| &lt;- swp entry</span>
<span class="p_add">+ * | OFFSET (14-&gt;63) | TYPE (9-13)  |0|X|X|X| X| X|X|X|0| &lt;- swp entry</span>
  *
  * G (8) is aliased and used as a PROT_NONE indicator for
  * !present ptes.  We need to start storing swap entries above
<span class="p_chunk">@@ -156,7 +156,7 @@</span> <span class="p_context"> static inline int pgd_large(pgd_t pgd) { return 0; }</span>
 #define SWP_TYPE_FIRST_BIT (_PAGE_BIT_PROTNONE + 1)
 #define SWP_TYPE_BITS 5
 /* Place the offset above the type: */
<span class="p_del">-#define SWP_OFFSET_FIRST_BIT (SWP_TYPE_FIRST_BIT + SWP_TYPE_BITS + 1)</span>
<span class="p_add">+#define SWP_OFFSET_FIRST_BIT (SWP_TYPE_FIRST_BIT + SWP_TYPE_BITS)</span>
 
 #define MAX_SWAPFILES_CHECK() BUILD_BUG_ON(MAX_SWAPFILES_SHIFT &gt; SWP_TYPE_BITS)
 
<span class="p_header">diff --git a/arch/x86/include/asm/realmode.h b/arch/x86/include/asm/realmode.h</span>
<span class="p_header">index 9c6b890d5e7a..b2988c0ed829 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/realmode.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/realmode.h</span>
<span class="p_chunk">@@ -58,7 +58,15 @@</span> <span class="p_context"> extern unsigned char boot_gdt[];</span>
 extern unsigned char secondary_startup_64[];
 #endif
 
<span class="p_add">+static inline size_t real_mode_size_needed(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (real_mode_header)</span>
<span class="p_add">+		return 0;	/* already allocated. */</span>
<span class="p_add">+</span>
<span class="p_add">+	return ALIGN(real_mode_blob_end - real_mode_blob, PAGE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void set_real_mode_mem(phys_addr_t mem, size_t size);</span>
 void reserve_real_mode(void);
<span class="p_del">-void setup_real_mode(void);</span>
 
 #endif /* _ARCH_X86_REALMODE_H */
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 4e5be94e079a..6fa85944af83 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -135,7 +135,14 @@</span> <span class="p_context"> static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
 
 static inline void __native_flush_tlb(void)
 {
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If current-&gt;mm == NULL then we borrow a mm which may change during a</span>
<span class="p_add">+	 * task switch and therefore we must not be preempted while we write CR3</span>
<span class="p_add">+	 * back:</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	preempt_disable();</span>
 	native_write_cr3(native_read_cr3());
<span class="p_add">+	preempt_enable();</span>
 }
 
 static inline void __native_flush_tlb_global_irq_disabled(void)
<span class="p_header">diff --git a/arch/x86/include/asm/uv/bios.h b/arch/x86/include/asm/uv/bios.h</span>
<span class="p_header">index c852590254d5..e652a7cc6186 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/uv/bios.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/uv/bios.h</span>
<span class="p_chunk">@@ -79,7 +79,7 @@</span> <span class="p_context"> struct uv_gam_range_entry {</span>
 	u16	nasid;		/* HNasid */
 	u16	sockid;		/* Socket ID, high bits of APIC ID */
 	u16	pnode;		/* Index to MMR and GRU spaces */
<span class="p_del">-	u32	pxm;		/* ACPI proximity domain number */</span>
<span class="p_add">+	u32	unused2;</span>
 	u32	limit;		/* PA bits 56:26 (UV_GAM_RANGE_SHFT) */
 };
 
<span class="p_chunk">@@ -88,7 +88,8 @@</span> <span class="p_context"> struct uv_gam_range_entry {</span>
 #define	UV_SYSTAB_VERSION_UV4		0x400	/* UV4 BIOS base version */
 #define	UV_SYSTAB_VERSION_UV4_1		0x401	/* + gpa_shift */
 #define	UV_SYSTAB_VERSION_UV4_2		0x402	/* + TYPE_NVRAM/WINDOW/MBOX */
<span class="p_del">-#define	UV_SYSTAB_VERSION_UV4_LATEST	UV_SYSTAB_VERSION_UV4_2</span>
<span class="p_add">+#define	UV_SYSTAB_VERSION_UV4_3		0x403	/* - GAM Range PXM Value */</span>
<span class="p_add">+#define	UV_SYSTAB_VERSION_UV4_LATEST	UV_SYSTAB_VERSION_UV4_3</span>
 
 #define	UV_SYSTAB_TYPE_UNUSED		0	/* End of table (offset == 0) */
 #define	UV_SYSTAB_TYPE_GAM_PARAMS	1	/* GAM PARAM conversions */
<span class="p_header">diff --git a/arch/x86/kernel/apic/x2apic_cluster.c b/arch/x86/kernel/apic/x2apic_cluster.c</span>
<span class="p_header">index 6368fa69d2af..54f35d988025 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/x2apic_cluster.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/x2apic_cluster.c</span>
<span class="p_chunk">@@ -155,7 +155,7 @@</span> <span class="p_context"> static void init_x2apic_ldr(void)</span>
 /*
  * At CPU state changes, update the x2apic cluster sibling info.
  */
<span class="p_del">-int x2apic_prepare_cpu(unsigned int cpu)</span>
<span class="p_add">+static int x2apic_prepare_cpu(unsigned int cpu)</span>
 {
 	if (!zalloc_cpumask_var(&amp;per_cpu(cpus_in_cluster, cpu), GFP_KERNEL))
 		return -ENOMEM;
<span class="p_chunk">@@ -168,7 +168,7 @@</span> <span class="p_context"> int x2apic_prepare_cpu(unsigned int cpu)</span>
 	return 0;
 }
 
<span class="p_del">-int x2apic_dead_cpu(unsigned int this_cpu)</span>
<span class="p_add">+static int x2apic_dead_cpu(unsigned int this_cpu)</span>
 {
 	int cpu;
 
<span class="p_chunk">@@ -186,13 +186,18 @@</span> <span class="p_context"> int x2apic_dead_cpu(unsigned int this_cpu)</span>
 static int x2apic_cluster_probe(void)
 {
 	int cpu = smp_processor_id();
<span class="p_add">+	int ret;</span>
 
 	if (!x2apic_mode)
 		return 0;
 
<span class="p_add">+	ret = cpuhp_setup_state(CPUHP_X2APIC_PREPARE, &quot;X2APIC_PREPARE&quot;,</span>
<span class="p_add">+				x2apic_prepare_cpu, x2apic_dead_cpu);</span>
<span class="p_add">+	if (ret &lt; 0) {</span>
<span class="p_add">+		pr_err(&quot;Failed to register X2APIC_PREPARE\n&quot;);</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
 	cpumask_set_cpu(cpu, per_cpu(cpus_in_cluster, cpu));
<span class="p_del">-	cpuhp_setup_state(CPUHP_X2APIC_PREPARE, &quot;X2APIC_PREPARE&quot;,</span>
<span class="p_del">-			  x2apic_prepare_cpu, x2apic_dead_cpu);</span>
 	return 1;
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/apic/x2apic_uv_x.c b/arch/x86/kernel/apic/x2apic_uv_x.c</span>
<span class="p_header">index 09b59adaea3f..cb0673c1e940 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/x2apic_uv_x.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/x2apic_uv_x.c</span>
<span class="p_chunk">@@ -223,6 +223,11 @@</span> <span class="p_context"> static int __init uv_acpi_madt_oem_check(char *oem_id, char *oem_table_id)</span>
 	if (strncmp(oem_id, &quot;SGI&quot;, 3) != 0)
 		return 0;
 
<span class="p_add">+	if (numa_off) {</span>
<span class="p_add">+		pr_err(&quot;UV: NUMA is off, disabling UV support\n&quot;);</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* Setup early hub type field in uv_hub_info for Node 0 */
 	uv_cpu_info-&gt;p_uv_hub_info = &amp;uv_hub_info_node0;
 
<span class="p_chunk">@@ -325,7 +330,7 @@</span> <span class="p_context"> static __init void build_uv_gr_table(void)</span>
 	struct uv_gam_range_entry *gre = uv_gre_table;
 	struct uv_gam_range_s *grt;
 	unsigned long last_limit = 0, ram_limit = 0;
<span class="p_del">-	int bytes, i, sid, lsid = -1;</span>
<span class="p_add">+	int bytes, i, sid, lsid = -1, indx = 0, lindx = -1;</span>
 
 	if (!gre)
 		return;
<span class="p_chunk">@@ -356,11 +361,12 @@</span> <span class="p_context"> static __init void build_uv_gr_table(void)</span>
 		}
 		sid = gre-&gt;sockid - _min_socket;
 		if (lsid &lt; sid) {		/* new range */
<span class="p_del">-			grt = &amp;_gr_table[sid];</span>
<span class="p_del">-			grt-&gt;base = lsid;</span>
<span class="p_add">+			grt = &amp;_gr_table[indx];</span>
<span class="p_add">+			grt-&gt;base = lindx;</span>
 			grt-&gt;nasid = gre-&gt;nasid;
 			grt-&gt;limit = last_limit = gre-&gt;limit;
 			lsid = sid;
<span class="p_add">+			lindx = indx++;</span>
 			continue;
 		}
 		if (lsid == sid &amp;&amp; !ram_limit) {	/* update range */
<span class="p_chunk">@@ -371,7 +377,7 @@</span> <span class="p_context"> static __init void build_uv_gr_table(void)</span>
 		}
 		if (!ram_limit) {		/* non-contiguous ram range */
 			grt++;
<span class="p_del">-			grt-&gt;base = sid - 1;</span>
<span class="p_add">+			grt-&gt;base = lindx;</span>
 			grt-&gt;nasid = gre-&gt;nasid;
 			grt-&gt;limit = last_limit = gre-&gt;limit;
 			continue;
<span class="p_chunk">@@ -1155,19 +1161,18 @@</span> <span class="p_context"> static void __init decode_gam_rng_tbl(unsigned long ptr)</span>
 	for (; gre-&gt;type != UV_GAM_RANGE_TYPE_UNUSED; gre++) {
 		if (!index) {
 			pr_info(&quot;UV: GAM Range Table...\n&quot;);
<span class="p_del">-			pr_info(&quot;UV:  # %20s %14s %5s %4s %5s %3s %2s %3s\n&quot;,</span>
<span class="p_add">+			pr_info(&quot;UV:  # %20s %14s %5s %4s %5s %3s %2s\n&quot;,</span>
 				&quot;Range&quot;, &quot;&quot;, &quot;Size&quot;, &quot;Type&quot;, &quot;NASID&quot;,
<span class="p_del">-				&quot;SID&quot;, &quot;PN&quot;, &quot;PXM&quot;);</span>
<span class="p_add">+				&quot;SID&quot;, &quot;PN&quot;);</span>
 		}
 		pr_info(
<span class="p_del">-		&quot;UV: %2d: 0x%014lx-0x%014lx %5luG %3d   %04x  %02x %02x %3d\n&quot;,</span>
<span class="p_add">+		&quot;UV: %2d: 0x%014lx-0x%014lx %5luG %3d   %04x  %02x %02x\n&quot;,</span>
 			index++,
 			(unsigned long)lgre &lt;&lt; UV_GAM_RANGE_SHFT,
 			(unsigned long)gre-&gt;limit &lt;&lt; UV_GAM_RANGE_SHFT,
 			((unsigned long)(gre-&gt;limit - lgre)) &gt;&gt;
 				(30 - UV_GAM_RANGE_SHFT), /* 64M -&gt; 1G */
<span class="p_del">-			gre-&gt;type, gre-&gt;nasid, gre-&gt;sockid,</span>
<span class="p_del">-			gre-&gt;pnode, gre-&gt;pxm);</span>
<span class="p_add">+			gre-&gt;type, gre-&gt;nasid, gre-&gt;sockid, gre-&gt;pnode);</span>
 
 		lgre = gre-&gt;limit;
 		if (sock_min &gt; gre-&gt;sockid)
<span class="p_chunk">@@ -1286,7 +1291,7 @@</span> <span class="p_context"> static void __init build_socket_tables(void)</span>
 		_pnode_to_socket[i] = SOCK_EMPTY;
 
 	/* fill in pnode/node/addr conversion list values */
<span class="p_del">-	pr_info(&quot;UV: GAM Building socket/pnode/pxm conversion tables\n&quot;);</span>
<span class="p_add">+	pr_info(&quot;UV: GAM Building socket/pnode conversion tables\n&quot;);</span>
 	for (; gre-&gt;type != UV_GAM_RANGE_TYPE_UNUSED; gre++) {
 		if (gre-&gt;type == UV_GAM_RANGE_TYPE_HOLE)
 			continue;
<span class="p_chunk">@@ -1294,20 +1299,18 @@</span> <span class="p_context"> static void __init build_socket_tables(void)</span>
 		if (_socket_to_pnode[i] != SOCK_EMPTY)
 			continue;	/* duplicate */
 		_socket_to_pnode[i] = gre-&gt;pnode;
<span class="p_del">-		_socket_to_node[i] = gre-&gt;pxm;</span>
 
 		i = gre-&gt;pnode - minpnode;
 		_pnode_to_socket[i] = gre-&gt;sockid;
 
 		pr_info(
<span class="p_del">-		&quot;UV: sid:%02x type:%d nasid:%04x pn:%02x pxm:%2d pn2s:%2x\n&quot;,</span>
<span class="p_add">+		&quot;UV: sid:%02x type:%d nasid:%04x pn:%02x pn2s:%2x\n&quot;,</span>
 			gre-&gt;sockid, gre-&gt;type, gre-&gt;nasid,
 			_socket_to_pnode[gre-&gt;sockid - minsock],
<span class="p_del">-			_socket_to_node[gre-&gt;sockid - minsock],</span>
 			_pnode_to_socket[gre-&gt;pnode - minpnode]);
 	}
 
<span class="p_del">-	/* check socket -&gt; node values */</span>
<span class="p_add">+	/* Set socket -&gt; node values */</span>
 	lnid = -1;
 	for_each_present_cpu(cpu) {
 		int nid = cpu_to_node(cpu);
<span class="p_chunk">@@ -1318,14 +1321,9 @@</span> <span class="p_context"> static void __init build_socket_tables(void)</span>
 		lnid = nid;
 		apicid = per_cpu(x86_cpu_to_apicid, cpu);
 		sockid = apicid &gt;&gt; uv_cpuid.socketid_shift;
<span class="p_del">-		i = sockid - minsock;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (nid != _socket_to_node[i]) {</span>
<span class="p_del">-			pr_warn(</span>
<span class="p_del">-			&quot;UV: %02x: type:%d socket:%02x PXM:%02x != node:%2d\n&quot;,</span>
<span class="p_del">-				i, sockid, gre-&gt;type, _socket_to_node[i], nid);</span>
<span class="p_del">-			_socket_to_node[i] = nid;</span>
<span class="p_del">-		}</span>
<span class="p_add">+		_socket_to_node[sockid - minsock] = nid;</span>
<span class="p_add">+		pr_info(&quot;UV: sid:%02x: apicid:%04x node:%2d\n&quot;,</span>
<span class="p_add">+			sockid, apicid, nid);</span>
 	}
 
 	/* Setup physical blade to pnode translation from GAM Range Table */
<span class="p_header">diff --git a/arch/x86/kernel/fpu/xstate.c b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">index 680049aa4593..01567aa87503 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/xstate.c</span>
<span class="p_chunk">@@ -866,105 +866,17 @@</span> <span class="p_context"> const void *get_xsave_field_ptr(int xsave_state)</span>
 	return get_xsave_addr(&amp;fpu-&gt;state.xsave, xsave_state);
 }
 
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Set xfeatures (aka XSTATE_BV) bit for a feature that we want</span>
<span class="p_del">- * to take out of its &quot;init state&quot;.  This will ensure that an</span>
<span class="p_del">- * XRSTOR actually restores the state.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void fpu__xfeature_set_non_init(struct xregs_state *xsave,</span>
<span class="p_del">-		int xstate_feature_mask)</span>
<span class="p_del">-{</span>
<span class="p_del">-	xsave-&gt;header.xfeatures |= xstate_feature_mask;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * This function is safe to call whether the FPU is in use or not.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Note that this only works on the current task.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Inputs:</span>
<span class="p_del">- *	@xsave_state: state which is defined in xsave.h (e.g. XFEATURE_MASK_FP,</span>
<span class="p_del">- *	XFEATURE_MASK_SSE, etc...)</span>
<span class="p_del">- *	@xsave_state_ptr: a pointer to a copy of the state that you would</span>
<span class="p_del">- *	like written in to the current task&#39;s FPU xsave state.  This pointer</span>
<span class="p_del">- *	must not be located in the current tasks&#39;s xsave area.</span>
<span class="p_del">- * Output:</span>
<span class="p_del">- *	address of the state in the xsave area or NULL if the state</span>
<span class="p_del">- *	is not present or is in its &#39;init state&#39;.</span>
<span class="p_del">- */</span>
<span class="p_del">-static void fpu__xfeature_set_state(int xstate_feature_mask,</span>
<span class="p_del">-		void *xstate_feature_src, size_t len)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct xregs_state *xsave = &amp;current-&gt;thread.fpu.state.xsave;</span>
<span class="p_del">-	struct fpu *fpu = &amp;current-&gt;thread.fpu;</span>
<span class="p_del">-	void *dst;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!boot_cpu_has(X86_FEATURE_XSAVE)) {</span>
<span class="p_del">-		WARN_ONCE(1, &quot;%s() attempted with no xsave support&quot;, __func__);</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Tell the FPU code that we need the FPU state to be in</span>
<span class="p_del">-	 * &#39;fpu&#39; (not in the registers), and that we need it to</span>
<span class="p_del">-	 * be stable while we write to it.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	fpu__current_fpstate_write_begin();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This method *WILL* *NOT* work for compact-format</span>
<span class="p_del">-	 * buffers.  If the &#39;xstate_feature_mask&#39; is unset in</span>
<span class="p_del">-	 * xcomp_bv then we may need to move other feature state</span>
<span class="p_del">-	 * &quot;up&quot; in the buffer.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (xsave-&gt;header.xcomp_bv &amp; xstate_feature_mask) {</span>
<span class="p_del">-		WARN_ON_ONCE(1);</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* find the location in the xsave buffer of the desired state */</span>
<span class="p_del">-	dst = __raw_xsave_addr(&amp;fpu-&gt;state.xsave, xstate_feature_mask);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Make sure that the pointer being passed in did not</span>
<span class="p_del">-	 * come from the xsave buffer itself.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	WARN_ONCE(xstate_feature_src == dst, &quot;set from xsave buffer itself&quot;);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* put the caller-provided data in the location */</span>
<span class="p_del">-	memcpy(dst, xstate_feature_src, len);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Mark the xfeature so that the CPU knows there is state</span>
<span class="p_del">-	 * in the buffer now.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	fpu__xfeature_set_non_init(xsave, xstate_feature_mask);</span>
<span class="p_del">-out:</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We are done writing to the &#39;fpu&#39;.  Reenable preeption</span>
<span class="p_del">-	 * and (possibly) move the fpstate back in to the fpregs.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	fpu__current_fpstate_write_end();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #define NR_VALID_PKRU_BITS (CONFIG_NR_PROTECTION_KEYS * 2)
 #define PKRU_VALID_MASK (NR_VALID_PKRU_BITS - 1)
 
 /*
<span class="p_del">- * This will go out and modify the XSAVE buffer so that PKRU is</span>
<span class="p_del">- * set to a particular state for access to &#39;pkey&#39;.</span>
<span class="p_del">- *</span>
<span class="p_del">- * PKRU state does affect kernel access to user memory.  We do</span>
<span class="p_del">- * not modfiy PKRU *itself* here, only the XSAVE state that will</span>
<span class="p_del">- * be restored in to PKRU when we return back to userspace.</span>
<span class="p_add">+ * This will go out and modify PKRU register to set the access</span>
<span class="p_add">+ * rights for @pkey to @init_val.</span>
  */
 int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,
 		unsigned long init_val)
 {
<span class="p_del">-	struct xregs_state *xsave = &amp;tsk-&gt;thread.fpu.state.xsave;</span>
<span class="p_del">-	struct pkru_state *old_pkru_state;</span>
<span class="p_del">-	struct pkru_state new_pkru_state;</span>
<span class="p_add">+	u32 old_pkru;</span>
 	int pkey_shift = (pkey * PKRU_BITS_PER_PKEY);
 	u32 new_pkru_bits = 0;
 
<span class="p_chunk">@@ -974,6 +886,15 @@</span> <span class="p_context"> int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,</span>
 	 */
 	if (!boot_cpu_has(X86_FEATURE_OSPKE))
 		return -EINVAL;
<span class="p_add">+	/*</span>
<span class="p_add">+	 * For most XSAVE components, this would be an arduous task:</span>
<span class="p_add">+	 * brining fpstate up to date with fpregs, updating fpstate,</span>
<span class="p_add">+	 * then re-populating fpregs.  But, for components that are</span>
<span class="p_add">+	 * never lazily managed, we can just access the fpregs</span>
<span class="p_add">+	 * directly.  PKRU is never managed lazily, so we can just</span>
<span class="p_add">+	 * manipulate it directly.  Make sure it stays that way.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WARN_ON_ONCE(!use_eager_fpu());</span>
 
 	/* Set the bits we need in PKRU:  */
 	if (init_val &amp; PKEY_DISABLE_ACCESS)
<span class="p_chunk">@@ -984,37 +905,12 @@</span> <span class="p_context"> int arch_set_user_pkey_access(struct task_struct *tsk, int pkey,</span>
 	/* Shift the bits in to the correct place in PKRU for pkey: */
 	new_pkru_bits &lt;&lt;= pkey_shift;
 
<span class="p_del">-	/* Locate old copy of the state in the xsave buffer: */</span>
<span class="p_del">-	old_pkru_state = get_xsave_addr(xsave, XFEATURE_MASK_PKRU);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * When state is not in the buffer, it is in the init</span>
<span class="p_del">-	 * state, set it manually.  Otherwise, copy out the old</span>
<span class="p_del">-	 * state.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!old_pkru_state)</span>
<span class="p_del">-		new_pkru_state.pkru = 0;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		new_pkru_state.pkru = old_pkru_state-&gt;pkru;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Mask off any old bits in place: */</span>
<span class="p_del">-	new_pkru_state.pkru &amp;= ~((PKRU_AD_BIT|PKRU_WD_BIT) &lt;&lt; pkey_shift);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Set the newly-requested bits: */</span>
<span class="p_del">-	new_pkru_state.pkru |= new_pkru_bits;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We could theoretically live without zeroing pkru.pad.</span>
<span class="p_del">-	 * The current XSAVE feature state definition says that</span>
<span class="p_del">-	 * only bytes 0-&gt;3 are used.  But we do not want to</span>
<span class="p_del">-	 * chance leaking kernel stack out to userspace in case a</span>
<span class="p_del">-	 * memcpy() of the whole xsave buffer was done.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * They&#39;re in the same cacheline anyway.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	new_pkru_state.pad = 0;</span>
<span class="p_add">+	/* Get old PKRU and mask off any old bits in place: */</span>
<span class="p_add">+	old_pkru = read_pkru();</span>
<span class="p_add">+	old_pkru &amp;= ~((PKRU_AD_BIT|PKRU_WD_BIT) &lt;&lt; pkey_shift);</span>
 
<span class="p_del">-	fpu__xfeature_set_state(XFEATURE_MASK_PKRU, &amp;new_pkru_state, sizeof(new_pkru_state));</span>
<span class="p_add">+	/* Write old part along with new part: */</span>
<span class="p_add">+	write_pkru(old_pkru | new_pkru_bits);</span>
 
 	return 0;
 }
<span class="p_header">diff --git a/arch/x86/kernel/head32.c b/arch/x86/kernel/head32.c</span>
<span class="p_header">index 2dda0bc4576e..f16c55bfc090 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/head32.c</span>
<span class="p_chunk">@@ -25,8 +25,6 @@</span> <span class="p_context"> static void __init i386_default_early_setup(void)</span>
 	/* Initialize 32bit specific setup functions */
 	x86_init.resources.reserve_resources = i386_reserve_resources;
 	x86_init.mpparse.setup_ioapic_ids = setup_ioapic_ids_from_mpc;
<span class="p_del">-</span>
<span class="p_del">-	reserve_bios_regions();</span>
 }
 
 asmlinkage __visible void __init i386_start_kernel(void)
<span class="p_header">diff --git a/arch/x86/kernel/head64.c b/arch/x86/kernel/head64.c</span>
<span class="p_header">index 99d48e7d2974..54a2372f5dbb 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/head64.c</span>
<span class="p_chunk">@@ -183,7 +183,6 @@</span> <span class="p_context"> void __init x86_64_start_reservations(char *real_mode_data)</span>
 		copy_bootdata(__va(real_mode_data));
 
 	x86_early_init_platform_quirks();
<span class="p_del">-	reserve_bios_regions();</span>
 
 	switch (boot_params.hdr.hardware_subarch) {
 	case X86_SUBARCH_INTEL_MID:
<span class="p_header">diff --git a/arch/x86/kernel/irq.c b/arch/x86/kernel/irq.c</span>
<span class="p_header">index 61521dc19c10..9f669fdd2010 100644</span>
<span class="p_header">--- a/arch/x86/kernel/irq.c</span>
<span class="p_header">+++ b/arch/x86/kernel/irq.c</span>
<span class="p_chunk">@@ -102,8 +102,7 @@</span> <span class="p_context"> int arch_show_interrupts(struct seq_file *p, int prec)</span>
 	seq_puts(p, &quot;  Rescheduling interrupts\n&quot;);
 	seq_printf(p, &quot;%*s: &quot;, prec, &quot;CAL&quot;);
 	for_each_online_cpu(j)
<span class="p_del">-		seq_printf(p, &quot;%10u &quot;, irq_stats(j)-&gt;irq_call_count -</span>
<span class="p_del">-					irq_stats(j)-&gt;irq_tlb_count);</span>
<span class="p_add">+		seq_printf(p, &quot;%10u &quot;, irq_stats(j)-&gt;irq_call_count);</span>
 	seq_puts(p, &quot;  Function call interrupts\n&quot;);
 	seq_printf(p, &quot;%*s: &quot;, prec, &quot;TLB&quot;);
 	for_each_online_cpu(j)
<span class="p_header">diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c</span>
<span class="p_header">index 991b77986d57..0fa60f5f5a16 100644</span>
<span class="p_header">--- a/arch/x86/kernel/setup.c</span>
<span class="p_header">+++ b/arch/x86/kernel/setup.c</span>
<span class="p_chunk">@@ -936,8 +936,6 @@</span> <span class="p_context"> void __init setup_arch(char **cmdline_p)</span>
 
 	x86_init.oem.arch_setup();
 
<span class="p_del">-	kernel_randomize_memory();</span>
<span class="p_del">-</span>
 	iomem_resource.end = (1ULL &lt;&lt; boot_cpu_data.x86_phys_bits) - 1;
 	setup_memory_map();
 	parse_setup_data();
<span class="p_chunk">@@ -1055,6 +1053,12 @@</span> <span class="p_context"> void __init setup_arch(char **cmdline_p)</span>
 
 	max_possible_pfn = max_pfn;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Define random base addresses for memory sections after max_pfn is</span>
<span class="p_add">+	 * defined and before each memory section base is used.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kernel_randomize_memory();</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_32
 	/* max_low_pfn get updated here */
 	find_low_pfn_range();
<span class="p_chunk">@@ -1097,6 +1101,8 @@</span> <span class="p_context"> void __init setup_arch(char **cmdline_p)</span>
 		efi_find_mirror();
 	}
 
<span class="p_add">+	reserve_bios_regions();</span>
<span class="p_add">+</span>
 	/*
 	 * The EFI specification says that boot service code won&#39;t be called
 	 * after ExitBootServices(). This is, in fact, a lie.
<span class="p_chunk">@@ -1125,7 +1131,15 @@</span> <span class="p_context"> void __init setup_arch(char **cmdline_p)</span>
 
 	early_trap_pf_init();
 
<span class="p_del">-	setup_real_mode();</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Update mmu_cr4_features (and, indirectly, trampoline_cr4_features)</span>
<span class="p_add">+	 * with the current CR4 value.  This may not be necessary, but</span>
<span class="p_add">+	 * auditing all the early-boot CR4 manipulation would be needed to</span>
<span class="p_add">+	 * rule it out.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (boot_cpu_data.cpuid_level &gt;= 0)</span>
<span class="p_add">+		/* A CPU has %cr4 if and only if it has CPUID. */</span>
<span class="p_add">+		mmu_cr4_features = __read_cr4();</span>
 
 	memblock_set_current_limit(get_max_mapped());
 
<span class="p_chunk">@@ -1174,13 +1188,6 @@</span> <span class="p_context"> void __init setup_arch(char **cmdline_p)</span>
 
 	kasan_init();
 
<span class="p_del">-	if (boot_cpu_data.cpuid_level &gt;= 0) {</span>
<span class="p_del">-		/* A CPU has %cr4 if and only if it has CPUID */</span>
<span class="p_del">-		mmu_cr4_features = __read_cr4();</span>
<span class="p_del">-		if (trampoline_cr4_features)</span>
<span class="p_del">-			*trampoline_cr4_features = mmu_cr4_features;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 #ifdef CONFIG_X86_32
 	/* sync back kernel address range */
 	clone_pgd_range(initial_page_table + KERNEL_PGD_BOUNDARY,
<span class="p_header">diff --git a/arch/x86/lib/kaslr.c b/arch/x86/lib/kaslr.c</span>
<span class="p_header">index f7dfeda83e5c..121f59c6ee54 100644</span>
<span class="p_header">--- a/arch/x86/lib/kaslr.c</span>
<span class="p_header">+++ b/arch/x86/lib/kaslr.c</span>
<span class="p_chunk">@@ -19,7 +19,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/cpufeature.h&gt;
 #include &lt;asm/setup.h&gt;
 
<span class="p_del">-#define debug_putstr(v) early_printk(v)</span>
<span class="p_add">+#define debug_putstr(v) early_printk(&quot;%s&quot;, v)</span>
 #define has_cpuflag(f) boot_cpu_has(f)
 #define get_boot_seed() kaslr_offset()
 #endif
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index 620928903be3..d28a2d741f9e 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -122,8 +122,18 @@</span> <span class="p_context"> __ref void *alloc_low_pages(unsigned int num)</span>
 	return __va(pfn &lt;&lt; PAGE_SHIFT);
 }
 
<span class="p_del">-/* need 3 4k for initial PMD_SIZE,  3 4k for 0-ISA_END_ADDRESS */</span>
<span class="p_del">-#define INIT_PGT_BUF_SIZE	(6 * PAGE_SIZE)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * By default need 3 4k for initial PMD_SIZE,  3 4k for 0-ISA_END_ADDRESS.</span>
<span class="p_add">+ * With KASLR memory randomization, depending on the machine e820 memory</span>
<span class="p_add">+ * and the PUD alignment. We may need twice more pages when KASLR memory</span>
<span class="p_add">+ * randomization is enabled.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef CONFIG_RANDOMIZE_MEMORY</span>
<span class="p_add">+#define INIT_PGD_PAGE_COUNT      6</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define INIT_PGD_PAGE_COUNT      12</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#define INIT_PGT_BUF_SIZE	(INIT_PGD_PAGE_COUNT * PAGE_SIZE)</span>
 RESERVE_BRK(early_pgt_alloc, INIT_PGT_BUF_SIZE);
 void  __init early_alloc_pgt_buf(void)
 {
<span class="p_header">diff --git a/arch/x86/mm/kaslr.c b/arch/x86/mm/kaslr.c</span>
<span class="p_header">index 26dccd6c0df1..ec8654f117d8 100644</span>
<span class="p_header">--- a/arch/x86/mm/kaslr.c</span>
<span class="p_header">+++ b/arch/x86/mm/kaslr.c</span>
<span class="p_chunk">@@ -97,7 +97,7 @@</span> <span class="p_context"> void __init kernel_randomize_memory(void)</span>
 	 * add padding if needed (especially for memory hotplug support).
 	 */
 	BUG_ON(kaslr_regions[0].base != &amp;page_offset_base);
<span class="p_del">-	memory_tb = ((max_pfn &lt;&lt; PAGE_SHIFT) &gt;&gt; TB_SHIFT) +</span>
<span class="p_add">+	memory_tb = DIV_ROUND_UP(max_pfn &lt;&lt; PAGE_SHIFT, 1UL &lt;&lt; TB_SHIFT) +</span>
 		CONFIG_RANDOMIZE_MEMORY_PHYSICAL_PADDING;
 
 	/* Adapt phyiscal memory region size based on available memory */
<span class="p_header">diff --git a/arch/x86/platform/efi/quirks.c b/arch/x86/platform/efi/quirks.c</span>
<span class="p_header">index 4480c06cade7..89d1146f5a6f 100644</span>
<span class="p_header">--- a/arch/x86/platform/efi/quirks.c</span>
<span class="p_header">+++ b/arch/x86/platform/efi/quirks.c</span>
<span class="p_chunk">@@ -254,6 +254,7 @@</span> <span class="p_context"> void __init efi_free_boot_services(void)</span>
 	for_each_efi_memory_desc(md) {
 		unsigned long long start = md-&gt;phys_addr;
 		unsigned long long size = md-&gt;num_pages &lt;&lt; EFI_PAGE_SHIFT;
<span class="p_add">+		size_t rm_size;</span>
 
 		if (md-&gt;type != EFI_BOOT_SERVICES_CODE &amp;&amp;
 		    md-&gt;type != EFI_BOOT_SERVICES_DATA)
<span class="p_chunk">@@ -263,6 +264,26 @@</span> <span class="p_context"> void __init efi_free_boot_services(void)</span>
 		if (md-&gt;attribute &amp; EFI_MEMORY_RUNTIME)
 			continue;
 
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Nasty quirk: if all sub-1MB memory is used for boot</span>
<span class="p_add">+		 * services, we can get here without having allocated the</span>
<span class="p_add">+		 * real mode trampoline.  It&#39;s too late to hand boot services</span>
<span class="p_add">+		 * memory back to the memblock allocator, so instead</span>
<span class="p_add">+		 * try to manually allocate the trampoline if needed.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * I&#39;ve seen this on a Dell XPS 13 9350 with firmware</span>
<span class="p_add">+		 * 1.4.4 with SGX enabled booting Linux via Fedora 24&#39;s</span>
<span class="p_add">+		 * grub2-efi on a hard disk.  (And no, I don&#39;t know why</span>
<span class="p_add">+		 * this happened, but Linux should still try to boot rather</span>
<span class="p_add">+		 * panicing early.)</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		rm_size = real_mode_size_needed();</span>
<span class="p_add">+		if (rm_size &amp;&amp; (start + rm_size) &lt; (1&lt;&lt;20) &amp;&amp; size &gt;= rm_size) {</span>
<span class="p_add">+			set_real_mode_mem(start, rm_size);</span>
<span class="p_add">+			start += rm_size;</span>
<span class="p_add">+			size -= rm_size;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		free_bootmem_late(start, size);
 	}
 
<span class="p_header">diff --git a/arch/x86/platform/uv/bios_uv.c b/arch/x86/platform/uv/bios_uv.c</span>
<span class="p_header">index 66b2166ea4a1..4e9fd1378aec 100644</span>
<span class="p_header">--- a/arch/x86/platform/uv/bios_uv.c</span>
<span class="p_header">+++ b/arch/x86/platform/uv/bios_uv.c</span>
<span class="p_chunk">@@ -199,12 +199,14 @@</span> <span class="p_context"> void uv_bios_init(void)</span>
 		return;
 	}
 
<span class="p_add">+	/* Starting with UV4 the UV systab size is variable */</span>
 	if (uv_systab-&gt;revision &gt;= UV_SYSTAB_VERSION_UV4) {
<span class="p_add">+		int size = uv_systab-&gt;size;</span>
<span class="p_add">+</span>
 		iounmap(uv_systab);
<span class="p_del">-		uv_systab = ioremap(efi.uv_systab, uv_systab-&gt;size);</span>
<span class="p_add">+		uv_systab = ioremap(efi.uv_systab, size);</span>
 		if (!uv_systab) {
<span class="p_del">-			pr_err(&quot;UV: UVsystab: ioremap(%d) failed!\n&quot;,</span>
<span class="p_del">-				uv_systab-&gt;size);</span>
<span class="p_add">+			pr_err(&quot;UV: UVsystab: ioremap(%d) failed!\n&quot;, size);</span>
 			return;
 		}
 	}
<span class="p_header">diff --git a/arch/x86/realmode/init.c b/arch/x86/realmode/init.c</span>
<span class="p_header">index 705e3fffb4a1..5db706f14111 100644</span>
<span class="p_header">--- a/arch/x86/realmode/init.c</span>
<span class="p_header">+++ b/arch/x86/realmode/init.c</span>
<span class="p_chunk">@@ -1,9 +1,11 @@</span> <span class="p_context"></span>
 #include &lt;linux/io.h&gt;
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
 #include &lt;linux/memblock.h&gt;
 
 #include &lt;asm/cacheflush.h&gt;
 #include &lt;asm/pgtable.h&gt;
 #include &lt;asm/realmode.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 
 struct real_mode_header *real_mode_header;
 u32 *trampoline_cr4_features;
<span class="p_chunk">@@ -11,25 +13,37 @@</span> <span class="p_context"> u32 *trampoline_cr4_features;</span>
 /* Hold the pgd entry used on booting additional CPUs */
 pgd_t trampoline_pgd_entry;
 
<span class="p_add">+void __init set_real_mode_mem(phys_addr_t mem, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	void *base = __va(mem);</span>
<span class="p_add">+</span>
<span class="p_add">+	real_mode_header = (struct real_mode_header *) base;</span>
<span class="p_add">+	printk(KERN_DEBUG &quot;Base memory trampoline at [%p] %llx size %zu\n&quot;,</span>
<span class="p_add">+	       base, (unsigned long long)mem, size);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void __init reserve_real_mode(void)
 {
 	phys_addr_t mem;
<span class="p_del">-	unsigned char *base;</span>
<span class="p_del">-	size_t size = PAGE_ALIGN(real_mode_blob_end - real_mode_blob);</span>
<span class="p_add">+	size_t size = real_mode_size_needed();</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!size)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	WARN_ON(slab_is_available());</span>
 
 	/* Has to be under 1M so we can execute real-mode AP code. */
 	mem = memblock_find_in_range(0, 1&lt;&lt;20, size, PAGE_SIZE);
<span class="p_del">-	if (!mem)</span>
<span class="p_del">-		panic(&quot;Cannot allocate trampoline\n&quot;);</span>
<span class="p_add">+	if (!mem) {</span>
<span class="p_add">+		pr_info(&quot;No sub-1M memory is available for the trampoline\n&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	base = __va(mem);</span>
 	memblock_reserve(mem, size);
<span class="p_del">-	real_mode_header = (struct real_mode_header *) base;</span>
<span class="p_del">-	printk(KERN_DEBUG &quot;Base memory trampoline at [%p] %llx size %zu\n&quot;,</span>
<span class="p_del">-	       base, (unsigned long long)mem, size);</span>
<span class="p_add">+	set_real_mode_mem(mem, size);</span>
 }
 
<span class="p_del">-void __init setup_real_mode(void)</span>
<span class="p_add">+static void __init setup_real_mode(void)</span>
 {
 	u16 real_mode_seg;
 	const u32 *rel;
<span class="p_chunk">@@ -84,7 +98,7 @@</span> <span class="p_context"> void __init setup_real_mode(void)</span>
 
 	trampoline_header-&gt;start = (u64) secondary_startup_64;
 	trampoline_cr4_features = &amp;trampoline_header-&gt;cr4;
<span class="p_del">-	*trampoline_cr4_features = __read_cr4();</span>
<span class="p_add">+	*trampoline_cr4_features = mmu_cr4_features;</span>
 
 	trampoline_pgd = (u64 *) __va(real_mode_header-&gt;trampoline_pgd);
 	trampoline_pgd[0] = trampoline_pgd_entry.pgd;
<span class="p_chunk">@@ -100,7 +114,7 @@</span> <span class="p_context"> void __init setup_real_mode(void)</span>
  * need to mark it executable at do_pre_smp_initcalls() at least,
  * thus run it as a early_initcall().
  */
<span class="p_del">-static int __init set_real_mode_permissions(void)</span>
<span class="p_add">+static void __init set_real_mode_permissions(void)</span>
 {
 	unsigned char *base = (unsigned char *) real_mode_header;
 	size_t size = PAGE_ALIGN(real_mode_blob_end - real_mode_blob);
<span class="p_chunk">@@ -119,7 +133,16 @@</span> <span class="p_context"> static int __init set_real_mode_permissions(void)</span>
 	set_memory_nx((unsigned long) base, size &gt;&gt; PAGE_SHIFT);
 	set_memory_ro((unsigned long) base, ro_size &gt;&gt; PAGE_SHIFT);
 	set_memory_x((unsigned long) text_start, text_size &gt;&gt; PAGE_SHIFT);
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init init_real_mode(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!real_mode_header)</span>
<span class="p_add">+		panic(&quot;Real mode trampoline was not allocated&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	setup_real_mode();</span>
<span class="p_add">+	set_real_mode_permissions();</span>
 
 	return 0;
 }
<span class="p_del">-early_initcall(set_real_mode_permissions);</span>
<span class="p_add">+early_initcall(init_real_mode);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



