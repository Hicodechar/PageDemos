
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>x86,locking: Remove ticket (spin)lock implementation - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    x86,locking: Remove ticket (spin)lock implementation</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 18, 2016, 6:43 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160518184302.GO3193@twins.programming.kicks-ass.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9121791/mbox/"
   >mbox</a>
|
   <a href="/patch/9121791/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9121791/">/patch/9121791/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 8CFDBBF29F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 18 May 2016 18:43:23 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id BF4B02035E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 18 May 2016 18:43:21 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id AEED120328
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 18 May 2016 18:43:19 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753713AbcERSnQ (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 18 May 2016 14:43:16 -0400
Received: from bombadil.infradead.org ([198.137.202.9]:47234 &quot;EHLO
	bombadil.infradead.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751967AbcERSnO (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 18 May 2016 14:43:14 -0400
Received: from j217100.upc-j.chello.nl ([24.132.217.100] helo=twins)
	by bombadil.infradead.org with esmtpsa (Exim 4.80.1 #2 (Red Hat
	Linux)) id 1b36Qy-0005Q8-Tn; Wed, 18 May 2016 18:43:05 +0000
Received: by twins (Postfix, from userid 1000)
	id 7E21A119A6E19; Wed, 18 May 2016 20:43:02 +0200 (CEST)
Date: Wed, 18 May 2016 20:43:02 +0200
From: Peter Zijlstra &lt;peterz@infradead.org&gt;
To: mingo@kernel.org
Cc: linux-kernel@vger.kernel.org, x86@kernel.org, kvm@vger.kernel.org,
	xen-devel@lists.xenproject.org, dhowells@redhat.com,
	Waiman.Long@hpe.com, pbonzini@redhat.com, david.vrabel@citrix.com
Subject: [PATCH] x86,locking: Remove ticket (spin)lock implementation
Message-ID: &lt;20160518184302.GO3193@twins.programming.kicks-ass.net&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.21 (2012-12-30)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.3 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - May 18, 2016, 6:43 p.m.</div>
<pre class="content">
We&#39;ve unconditionally used the queued spinlock for many releases now.

Its time to remove the old ticket lock code.

Cc: Waiman Long &lt;waiman.long@hpe.com&gt;
<span class="signed-off-by">Signed-off-by: Peter Zijlstra (Intel) &lt;peterz@infradead.org&gt;</span>
---
 arch/x86/Kconfig                      |   3 +-
 arch/x86/include/asm/paravirt.h       |  18 ---
 arch/x86/include/asm/paravirt_types.h |   7 -
 arch/x86/include/asm/spinlock.h       | 174 -----------------------
 arch/x86/include/asm/spinlock_types.h |  13 --
 arch/x86/kernel/kvm.c                 | 245 ---------------------------------
 arch/x86/kernel/paravirt-spinlocks.c  |   7 -
 arch/x86/kernel/paravirt_patch_32.c   |   4 +-
 arch/x86/kernel/paravirt_patch_64.c   |   4 +-
 arch/x86/xen/spinlock.c               | 250 +---------------------------------
 10 files changed, 6 insertions(+), 719 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=3407">Konrad Rzeszutek Wilk</a> - May 18, 2016, 7:13 p.m.</div>
<pre class="content">
On Wed, May 18, 2016 at 08:43:02PM +0200, Peter Zijlstra wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; We&#39;ve unconditionally used the queued spinlock for many releases now.</span>

Like since 4.2? I don&#39;t know of any enterprise distro that is shipping anything
more modern than 4.1? Perhaps it would be good to wait until they
at least ship and then give them some time to see if they have found
any issues?
<span class="quote">
&gt; </span>
<span class="quote">&gt; Its time to remove the old ticket lock code.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Waiman Long &lt;waiman.long@hpe.com&gt;</span>
<span class="quote">&gt; Signed-off-by: Peter Zijlstra (Intel) &lt;peterz@infradead.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/x86/Kconfig                      |   3 +-</span>
<span class="quote">&gt;  arch/x86/include/asm/paravirt.h       |  18 ---</span>
<span class="quote">&gt;  arch/x86/include/asm/paravirt_types.h |   7 -</span>
<span class="quote">&gt;  arch/x86/include/asm/spinlock.h       | 174 -----------------------</span>
<span class="quote">&gt;  arch/x86/include/asm/spinlock_types.h |  13 --</span>
<span class="quote">&gt;  arch/x86/kernel/kvm.c                 | 245 ---------------------------------</span>
<span class="quote">&gt;  arch/x86/kernel/paravirt-spinlocks.c  |   7 -</span>
<span class="quote">&gt;  arch/x86/kernel/paravirt_patch_32.c   |   4 +-</span>
<span class="quote">&gt;  arch/x86/kernel/paravirt_patch_64.c   |   4 +-</span>
<span class="quote">&gt;  arch/x86/xen/spinlock.c               | 250 +---------------------------------</span>
<span class="quote">&gt;  10 files changed, 6 insertions(+), 719 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="quote">&gt; index 7bb15747fea2..a5543914f6dd 100644</span>
<span class="quote">&gt; --- a/arch/x86/Kconfig</span>
<span class="quote">&gt; +++ b/arch/x86/Kconfig</span>
<span class="quote">&gt; @@ -705,7 +705,6 @@ config PARAVIRT_DEBUG</span>
<span class="quote">&gt;  config PARAVIRT_SPINLOCKS</span>
<span class="quote">&gt;  	bool &quot;Paravirtualization layer for spinlocks&quot;</span>
<span class="quote">&gt;  	depends on PARAVIRT &amp;&amp; SMP</span>
<span class="quote">&gt; -	select UNINLINE_SPIN_UNLOCK if !QUEUED_SPINLOCKS</span>
<span class="quote">&gt;  	---help---</span>
<span class="quote">&gt;  	  Paravirtualized spinlocks allow a pvops backend to replace the</span>
<span class="quote">&gt;  	  spinlock implementation with something virtualization-friendly</span>
<span class="quote">&gt; @@ -718,7 +717,7 @@ config PARAVIRT_SPINLOCKS</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  config QUEUED_LOCK_STAT</span>
<span class="quote">&gt;  	bool &quot;Paravirt queued spinlock statistics&quot;</span>
<span class="quote">&gt; -	depends on PARAVIRT_SPINLOCKS &amp;&amp; DEBUG_FS &amp;&amp; QUEUED_SPINLOCKS</span>
<span class="quote">&gt; +	depends on PARAVIRT_SPINLOCKS &amp;&amp; DEBUG_FS</span>
<span class="quote">&gt;  	---help---</span>
<span class="quote">&gt;  	  Enable the collection of statistical data on the slowpath</span>
<span class="quote">&gt;  	  behavior of paravirtualized queued spinlocks and report</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="quote">&gt; index 2970d22d7766..4cd8db05301f 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/paravirt.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/paravirt.h</span>
<span class="quote">&gt; @@ -661,8 +661,6 @@ static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,</span>
<span class="quote">&gt;  							u32 val)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -684,22 +682,6 @@ static __always_inline void pv_kick(int cpu)</span>
<span class="quote">&gt;  	PVOP_VCALL1(pv_lock_ops.kick, cpu);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static __always_inline void __ticket_lock_spinning(struct arch_spinlock *lock,</span>
<span class="quote">&gt; -							__ticket_t ticket)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	PVOP_VCALLEE2(pv_lock_ops.lock_spinning, lock, ticket);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static __always_inline void __ticket_unlock_kick(struct arch_spinlock *lock,</span>
<span class="quote">&gt; -							__ticket_t ticket)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	PVOP_VCALL2(pv_lock_ops.unlock_kick, lock, ticket);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  #endif /* SMP &amp;&amp; PARAVIRT_SPINLOCKS */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_X86_32</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="quote">&gt; index 7fa9e7740ba3..60aac60ba25f 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="quote">&gt; @@ -301,23 +301,16 @@ struct pv_mmu_ops {</span>
<span class="quote">&gt;  struct arch_spinlock;</span>
<span class="quote">&gt;  #ifdef CONFIG_SMP</span>
<span class="quote">&gt;  #include &lt;asm/spinlock_types.h&gt;</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -typedef u16 __ticket_t;</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct qspinlock;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct pv_lock_ops {</span>
<span class="quote">&gt; -#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="quote">&gt;  	void (*queued_spin_lock_slowpath)(struct qspinlock *lock, u32 val);</span>
<span class="quote">&gt;  	struct paravirt_callee_save queued_spin_unlock;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	void (*wait)(u8 *ptr, u8 val);</span>
<span class="quote">&gt;  	void (*kick)(int cpu);</span>
<span class="quote">&gt; -#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt; -	struct paravirt_callee_save lock_spinning;</span>
<span class="quote">&gt; -	void (*unlock_kick)(struct arch_spinlock *lock, __ticket_t ticket);</span>
<span class="quote">&gt; -#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /* This contains all the paravirt structures: we get a convenient</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/spinlock.h b/arch/x86/include/asm/spinlock.h</span>
<span class="quote">&gt; index be0a05913b91..921bea7a2708 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/spinlock.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/spinlock.h</span>
<span class="quote">&gt; @@ -20,187 +20,13 @@</span>
<span class="quote">&gt;   * (the type definitions are in asm/spinlock_types.h)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifdef CONFIG_X86_32</span>
<span class="quote">&gt; -# define LOCK_PTR_REG &quot;a&quot;</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -# define LOCK_PTR_REG &quot;D&quot;</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#if defined(CONFIG_X86_32) &amp;&amp; (defined(CONFIG_X86_PPRO_FENCE))</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * On PPro SMP, we use a locked operation to unlock</span>
<span class="quote">&gt; - * (PPro errata 66, 92)</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -# define UNLOCK_LOCK_PREFIX LOCK_PREFIX</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -# define UNLOCK_LOCK_PREFIX</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  /* How long a lock should spin before we consider blocking */</span>
<span class="quote">&gt;  #define SPIN_THRESHOLD	(1 &lt;&lt; 15)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern struct static_key paravirt_ticketlocks_enabled;</span>
<span class="quote">&gt;  static __always_inline bool static_key_false(struct static_key *key);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="quote">&gt;  #include &lt;asm/qspinlock.h&gt;</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#ifdef CONFIG_PARAVIRT_SPINLOCKS</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void __ticket_enter_slowpath(arch_spinlock_t *lock)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	set_bit(0, (volatile unsigned long *)&amp;lock-&gt;tickets.head);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#else  /* !CONFIG_PARAVIRT_SPINLOCKS */</span>
<span class="quote">&gt; -static __always_inline void __ticket_lock_spinning(arch_spinlock_t *lock,</span>
<span class="quote">&gt; -							__ticket_t ticket)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -static inline void __ticket_unlock_kick(arch_spinlock_t *lock,</span>
<span class="quote">&gt; -							__ticket_t ticket)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#endif /* CONFIG_PARAVIRT_SPINLOCKS */</span>
<span class="quote">&gt; -static inline int  __tickets_equal(__ticket_t one, __ticket_t two)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return !((one ^ two) &amp; ~TICKET_SLOWPATH_FLAG);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void __ticket_check_and_clear_slowpath(arch_spinlock_t *lock,</span>
<span class="quote">&gt; -							__ticket_t head)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	if (head &amp; TICKET_SLOWPATH_FLAG) {</span>
<span class="quote">&gt; -		arch_spinlock_t old, new;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		old.tickets.head = head;</span>
<span class="quote">&gt; -		new.tickets.head = head &amp; ~TICKET_SLOWPATH_FLAG;</span>
<span class="quote">&gt; -		old.tickets.tail = new.tickets.head + TICKET_LOCK_INC;</span>
<span class="quote">&gt; -		new.tickets.tail = old.tickets.tail;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		/* try to clear slowpath flag when there are no contenders */</span>
<span class="quote">&gt; -		cmpxchg(&amp;lock-&gt;head_tail, old.head_tail, new.head_tail);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static __always_inline int arch_spin_value_unlocked(arch_spinlock_t lock)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return __tickets_equal(lock.tickets.head, lock.tickets.tail);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -/*</span>
<span class="quote">&gt; - * Ticket locks are conceptually two parts, one indicating the current head of</span>
<span class="quote">&gt; - * the queue, and the other indicating the current tail. The lock is acquired</span>
<span class="quote">&gt; - * by atomically noting the tail and incrementing it by one (thus adding</span>
<span class="quote">&gt; - * ourself to the queue and noting our position), then waiting until the head</span>
<span class="quote">&gt; - * becomes equal to the the initial value of the tail.</span>
<span class="quote">&gt; - *</span>
<span class="quote">&gt; - * We use an xadd covering *both* parts of the lock, to increment the tail and</span>
<span class="quote">&gt; - * also load the position of the head, which takes care of memory ordering</span>
<span class="quote">&gt; - * issues and should be optimal for the uncontended case. Note the tail must be</span>
<span class="quote">&gt; - * in the high part, because a wide xadd increment of the low part would carry</span>
<span class="quote">&gt; - * up and contaminate the high part.</span>
<span class="quote">&gt; - */</span>
<span class="quote">&gt; -static __always_inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	register struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	inc = xadd(&amp;lock-&gt;tickets, inc);</span>
<span class="quote">&gt; -	if (likely(inc.head == inc.tail))</span>
<span class="quote">&gt; -		goto out;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	for (;;) {</span>
<span class="quote">&gt; -		unsigned count = SPIN_THRESHOLD;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		do {</span>
<span class="quote">&gt; -			inc.head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="quote">&gt; -			if (__tickets_equal(inc.head, inc.tail))</span>
<span class="quote">&gt; -				goto clear_slowpath;</span>
<span class="quote">&gt; -			cpu_relax();</span>
<span class="quote">&gt; -		} while (--count);</span>
<span class="quote">&gt; -		__ticket_lock_spinning(lock, inc.tail);</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -clear_slowpath:</span>
<span class="quote">&gt; -	__ticket_check_and_clear_slowpath(lock, inc.head);</span>
<span class="quote">&gt; -out:</span>
<span class="quote">&gt; -	barrier();	/* make sure nothing creeps before the lock is taken */</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static __always_inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	arch_spinlock_t old, new;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	old.tickets = READ_ONCE(lock-&gt;tickets);</span>
<span class="quote">&gt; -	if (!__tickets_equal(old.tickets.head, old.tickets.tail))</span>
<span class="quote">&gt; -		return 0;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	new.head_tail = old.head_tail + (TICKET_LOCK_INC &lt;&lt; TICKET_SHIFT);</span>
<span class="quote">&gt; -	new.head_tail &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* cmpxchg is a full barrier, so nothing can move before it */</span>
<span class="quote">&gt; -	return cmpxchg(&amp;lock-&gt;head_tail, old.head_tail, new.head_tail) == old.head_tail;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static __always_inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	if (TICKET_SLOWPATH_FLAG &amp;&amp;</span>
<span class="quote">&gt; -		static_key_false(&amp;paravirt_ticketlocks_enabled)) {</span>
<span class="quote">&gt; -		__ticket_t head;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		BUILD_BUG_ON(((__ticket_t)NR_CPUS) != NR_CPUS);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		head = xadd(&amp;lock-&gt;tickets.head, TICKET_LOCK_INC);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		if (unlikely(head &amp; TICKET_SLOWPATH_FLAG)) {</span>
<span class="quote">&gt; -			head &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="quote">&gt; -			__ticket_unlock_kick(lock, (head + TICKET_LOCK_INC));</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; -	} else</span>
<span class="quote">&gt; -		__add(&amp;lock-&gt;tickets.head, TICKET_LOCK_INC, UNLOCK_LOCK_PREFIX);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline int arch_spin_is_locked(arch_spinlock_t *lock)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return !__tickets_equal(tmp.tail, tmp.head);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline int arch_spin_is_contended(arch_spinlock_t *lock)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	tmp.head &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="quote">&gt; -	return (__ticket_t)(tmp.tail - tmp.head) &gt; TICKET_LOCK_INC;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -#define arch_spin_is_contended	arch_spin_is_contended</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static __always_inline void arch_spin_lock_flags(arch_spinlock_t *lock,</span>
<span class="quote">&gt; -						  unsigned long flags)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	arch_spin_lock(lock);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void arch_spin_unlock_wait(arch_spinlock_t *lock)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	__ticket_t head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	for (;;) {</span>
<span class="quote">&gt; -		struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="quote">&gt; -		/*</span>
<span class="quote">&gt; -		 * We need to check &quot;unlocked&quot; in a loop, tmp.head == head</span>
<span class="quote">&gt; -		 * can be false positive because of overflow.</span>
<span class="quote">&gt; -		 */</span>
<span class="quote">&gt; -		if (__tickets_equal(tmp.head, tmp.tail) ||</span>
<span class="quote">&gt; -				!__tickets_equal(tmp.head, head))</span>
<span class="quote">&gt; -			break;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		cpu_relax();</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Read-write spinlocks, allowing multiple readers</span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/spinlock_types.h b/arch/x86/include/asm/spinlock_types.h</span>
<span class="quote">&gt; index 65c3e37f879a..25311ebb446c 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/spinlock_types.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/spinlock_types.h</span>
<span class="quote">&gt; @@ -23,20 +23,7 @@ typedef u32 __ticketpair_t;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define TICKET_SHIFT	(sizeof(__ticket_t) * 8)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="quote">&gt;  #include &lt;asm-generic/qspinlock_types.h&gt;</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -typedef struct arch_spinlock {</span>
<span class="quote">&gt; -	union {</span>
<span class="quote">&gt; -		__ticketpair_t head_tail;</span>
<span class="quote">&gt; -		struct __raw_tickets {</span>
<span class="quote">&gt; -			__ticket_t head, tail;</span>
<span class="quote">&gt; -		} tickets;</span>
<span class="quote">&gt; -	};</span>
<span class="quote">&gt; -} arch_spinlock_t;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#define __ARCH_SPIN_LOCK_UNLOCKED	{ { 0 } }</span>
<span class="quote">&gt; -#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm-generic/qrwlock_types.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c</span>
<span class="quote">&gt; index eea2a6f72b31..3c4ab6156a89 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/kvm.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/kvm.c</span>
<span class="quote">&gt; @@ -577,9 +577,6 @@ static void kvm_kick_cpu(int cpu)</span>
<span class="quote">&gt;  	kvm_hypercall2(KVM_HC_KICK_CPU, flags, apicid);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  #include &lt;asm/qspinlock.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void kvm_wait(u8 *ptr, u8 val)</span>
<span class="quote">&gt; @@ -608,243 +605,6 @@ static void kvm_wait(u8 *ptr, u8 val)</span>
<span class="quote">&gt;  	local_irq_restore(flags);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -enum kvm_contention_stat {</span>
<span class="quote">&gt; -	TAKEN_SLOW,</span>
<span class="quote">&gt; -	TAKEN_SLOW_PICKUP,</span>
<span class="quote">&gt; -	RELEASED_SLOW,</span>
<span class="quote">&gt; -	RELEASED_SLOW_KICKED,</span>
<span class="quote">&gt; -	NR_CONTENTION_STATS</span>
<span class="quote">&gt; -};</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#ifdef CONFIG_KVM_DEBUG_FS</span>
<span class="quote">&gt; -#define HISTO_BUCKETS	30</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static struct kvm_spinlock_stats</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	u32 contention_stats[NR_CONTENTION_STATS];</span>
<span class="quote">&gt; -	u32 histo_spin_blocked[HISTO_BUCKETS+1];</span>
<span class="quote">&gt; -	u64 time_blocked;</span>
<span class="quote">&gt; -} spinlock_stats;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static u8 zero_stats;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void check_zero(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	u8 ret;</span>
<span class="quote">&gt; -	u8 old;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	old = READ_ONCE(zero_stats);</span>
<span class="quote">&gt; -	if (unlikely(old)) {</span>
<span class="quote">&gt; -		ret = cmpxchg(&amp;zero_stats, old, 0);</span>
<span class="quote">&gt; -		/* This ensures only one fellow resets the stat */</span>
<span class="quote">&gt; -		if (ret == old)</span>
<span class="quote">&gt; -			memset(&amp;spinlock_stats, 0, sizeof(spinlock_stats));</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void add_stats(enum kvm_contention_stat var, u32 val)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	check_zero();</span>
<span class="quote">&gt; -	spinlock_stats.contention_stats[var] += val;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline u64 spin_time_start(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return sched_clock();</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static void __spin_time_accum(u64 delta, u32 *array)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	unsigned index;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	index = ilog2(delta);</span>
<span class="quote">&gt; -	check_zero();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (index &lt; HISTO_BUCKETS)</span>
<span class="quote">&gt; -		array[index]++;</span>
<span class="quote">&gt; -	else</span>
<span class="quote">&gt; -		array[HISTO_BUCKETS]++;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void spin_time_accum_blocked(u64 start)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	u32 delta;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	delta = sched_clock() - start;</span>
<span class="quote">&gt; -	__spin_time_accum(delta, spinlock_stats.histo_spin_blocked);</span>
<span class="quote">&gt; -	spinlock_stats.time_blocked += delta;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static struct dentry *d_spin_debug;</span>
<span class="quote">&gt; -static struct dentry *d_kvm_debug;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static struct dentry *kvm_init_debugfs(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	d_kvm_debug = debugfs_create_dir(&quot;kvm-guest&quot;, NULL);</span>
<span class="quote">&gt; -	if (!d_kvm_debug)</span>
<span class="quote">&gt; -		printk(KERN_WARNING &quot;Could not create &#39;kvm&#39; debugfs directory\n&quot;);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return d_kvm_debug;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static int __init kvm_spinlock_debugfs(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	struct dentry *d_kvm;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	d_kvm = kvm_init_debugfs();</span>
<span class="quote">&gt; -	if (d_kvm == NULL)</span>
<span class="quote">&gt; -		return -ENOMEM;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	d_spin_debug = debugfs_create_dir(&quot;spinlocks&quot;, d_kvm);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	debugfs_create_u8(&quot;zero_stats&quot;, 0644, d_spin_debug, &amp;zero_stats);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	debugfs_create_u32(&quot;taken_slow&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -		   &amp;spinlock_stats.contention_stats[TAKEN_SLOW]);</span>
<span class="quote">&gt; -	debugfs_create_u32(&quot;taken_slow_pickup&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -		   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_PICKUP]);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	debugfs_create_u32(&quot;released_slow&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -		   &amp;spinlock_stats.contention_stats[RELEASED_SLOW]);</span>
<span class="quote">&gt; -	debugfs_create_u32(&quot;released_slow_kicked&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -		   &amp;spinlock_stats.contention_stats[RELEASED_SLOW_KICKED]);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	debugfs_create_u64(&quot;time_blocked&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -			   &amp;spinlock_stats.time_blocked);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	debugfs_create_u32_array(&quot;histo_blocked&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -		     spinlock_stats.histo_spin_blocked, HISTO_BUCKETS + 1);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return 0;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -fs_initcall(kvm_spinlock_debugfs);</span>
<span class="quote">&gt; -#else  /* !CONFIG_KVM_DEBUG_FS */</span>
<span class="quote">&gt; -static inline void add_stats(enum kvm_contention_stat var, u32 val)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline u64 spin_time_start(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return 0;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void spin_time_accum_blocked(u64 start)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -#endif  /* CONFIG_KVM_DEBUG_FS */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -struct kvm_lock_waiting {</span>
<span class="quote">&gt; -	struct arch_spinlock *lock;</span>
<span class="quote">&gt; -	__ticket_t want;</span>
<span class="quote">&gt; -};</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -/* cpus &#39;waiting&#39; on a spinlock to become available */</span>
<span class="quote">&gt; -static cpumask_t waiting_cpus;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -/* Track spinlock on which a cpu is waiting */</span>
<span class="quote">&gt; -static DEFINE_PER_CPU(struct kvm_lock_waiting, klock_waiting);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -__visible void kvm_lock_spinning(struct arch_spinlock *lock, __ticket_t want)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	struct kvm_lock_waiting *w;</span>
<span class="quote">&gt; -	int cpu;</span>
<span class="quote">&gt; -	u64 start;</span>
<span class="quote">&gt; -	unsigned long flags;</span>
<span class="quote">&gt; -	__ticket_t head;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (in_nmi())</span>
<span class="quote">&gt; -		return;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	w = this_cpu_ptr(&amp;klock_waiting);</span>
<span class="quote">&gt; -	cpu = smp_processor_id();</span>
<span class="quote">&gt; -	start = spin_time_start();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * Make sure an interrupt handler can&#39;t upset things in a</span>
<span class="quote">&gt; -	 * partially setup state.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	local_irq_save(flags);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * The ordering protocol on this is that the &quot;lock&quot; pointer</span>
<span class="quote">&gt; -	 * may only be set non-NULL if the &quot;want&quot; ticket is correct.</span>
<span class="quote">&gt; -	 * If we&#39;re updating &quot;want&quot;, we must first clear &quot;lock&quot;.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	w-&gt;lock = NULL;</span>
<span class="quote">&gt; -	smp_wmb();</span>
<span class="quote">&gt; -	w-&gt;want = want;</span>
<span class="quote">&gt; -	smp_wmb();</span>
<span class="quote">&gt; -	w-&gt;lock = lock;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	add_stats(TAKEN_SLOW, 1);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * This uses set_bit, which is atomic but we should not rely on its</span>
<span class="quote">&gt; -	 * reordering gurantees. So barrier is needed after this call.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	cpumask_set_cpu(cpu, &amp;waiting_cpus);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	barrier();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * Mark entry to slowpath before doing the pickup test to make</span>
<span class="quote">&gt; -	 * sure we don&#39;t deadlock with an unlocker.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	__ticket_enter_slowpath(lock);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* make sure enter_slowpath, which is atomic does not cross the read */</span>
<span class="quote">&gt; -	smp_mb__after_atomic();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * check again make sure it didn&#39;t become free while</span>
<span class="quote">&gt; -	 * we weren&#39;t looking.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="quote">&gt; -	if (__tickets_equal(head, want)) {</span>
<span class="quote">&gt; -		add_stats(TAKEN_SLOW_PICKUP, 1);</span>
<span class="quote">&gt; -		goto out;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * halt until it&#39;s our turn and kicked. Note that we do safe halt</span>
<span class="quote">&gt; -	 * for irq enabled case to avoid hang when lock info is overwritten</span>
<span class="quote">&gt; -	 * in irq spinlock slowpath and no spurious interrupt occur to save us.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	if (arch_irqs_disabled_flags(flags))</span>
<span class="quote">&gt; -		halt();</span>
<span class="quote">&gt; -	else</span>
<span class="quote">&gt; -		safe_halt();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -out:</span>
<span class="quote">&gt; -	cpumask_clear_cpu(cpu, &amp;waiting_cpus);</span>
<span class="quote">&gt; -	w-&gt;lock = NULL;</span>
<span class="quote">&gt; -	local_irq_restore(flags);</span>
<span class="quote">&gt; -	spin_time_accum_blocked(start);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -PV_CALLEE_SAVE_REGS_THUNK(kvm_lock_spinning);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -/* Kick vcpu waiting on @lock-&gt;head to reach value @ticket */</span>
<span class="quote">&gt; -static void kvm_unlock_kick(struct arch_spinlock *lock, __ticket_t ticket)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	int cpu;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	add_stats(RELEASED_SLOW, 1);</span>
<span class="quote">&gt; -	for_each_cpu(cpu, &amp;waiting_cpus) {</span>
<span class="quote">&gt; -		const struct kvm_lock_waiting *w = &amp;per_cpu(klock_waiting, cpu);</span>
<span class="quote">&gt; -		if (READ_ONCE(w-&gt;lock) == lock &amp;&amp;</span>
<span class="quote">&gt; -		    READ_ONCE(w-&gt;want) == ticket) {</span>
<span class="quote">&gt; -			add_stats(RELEASED_SLOW_KICKED, 1);</span>
<span class="quote">&gt; -			kvm_kick_cpu(cpu);</span>
<span class="quote">&gt; -			break;</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Setup pv_lock_ops to exploit KVM_FEATURE_PV_UNHALT if present.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; @@ -856,16 +616,11 @@ void __init kvm_spinlock_init(void)</span>
<span class="quote">&gt;  	if (!kvm_para_has_feature(KVM_FEATURE_PV_UNHALT))</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="quote">&gt;  	__pv_init_lock_hash();</span>
<span class="quote">&gt;  	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;</span>
<span class="quote">&gt;  	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);</span>
<span class="quote">&gt;  	pv_lock_ops.wait = kvm_wait;</span>
<span class="quote">&gt;  	pv_lock_ops.kick = kvm_kick_cpu;</span>
<span class="quote">&gt; -#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt; -	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(kvm_lock_spinning);</span>
<span class="quote">&gt; -	pv_lock_ops.unlock_kick = kvm_unlock_kick;</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static __init int kvm_spinlock_init_jump(void)</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/paravirt-spinlocks.c b/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="quote">&gt; index 33ee3e0efd65..fd672d8b33dc 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="quote">&gt; @@ -8,7 +8,6 @@</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #include &lt;asm/paravirt.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="quote">&gt;  __visible void __native_queued_spin_unlock(struct qspinlock *lock)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	native_queued_spin_unlock(lock);</span>
<span class="quote">&gt; @@ -21,19 +20,13 @@ bool pv_is_native_spin_unlock(void)</span>
<span class="quote">&gt;  	return pv_lock_ops.queued_spin_unlock.func ==</span>
<span class="quote">&gt;  		__raw_callee_save___native_queued_spin_unlock;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  struct pv_lock_ops pv_lock_ops = {</span>
<span class="quote">&gt;  #ifdef CONFIG_SMP</span>
<span class="quote">&gt; -#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="quote">&gt;  	.queued_spin_lock_slowpath = native_queued_spin_lock_slowpath,</span>
<span class="quote">&gt;  	.queued_spin_unlock = PV_CALLEE_SAVE(__native_queued_spin_unlock),</span>
<span class="quote">&gt;  	.wait = paravirt_nop,</span>
<span class="quote">&gt;  	.kick = paravirt_nop,</span>
<span class="quote">&gt; -#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt; -	.lock_spinning = __PV_IS_CALLEE_SAVE(paravirt_nop),</span>
<span class="quote">&gt; -	.unlock_kick = paravirt_nop,</span>
<span class="quote">&gt; -#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt;  #endif /* SMP */</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  EXPORT_SYMBOL(pv_lock_ops);</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/paravirt_patch_32.c b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="quote">&gt; index 158dc0650d5d..920c6ae08592 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="quote">&gt; @@ -10,7 +10,7 @@ DEF_NATIVE(pv_mmu_ops, write_cr3, &quot;mov %eax, %cr3&quot;);</span>
<span class="quote">&gt;  DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;mov %cr3, %eax&quot;);</span>
<span class="quote">&gt;  DEF_NATIVE(pv_cpu_ops, clts, &quot;clts&quot;);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="quote">&gt; +#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
<span class="quote">&gt;  DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%eax)&quot;);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -49,7 +49,7 @@ unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
<span class="quote">&gt;  		PATCH_SITE(pv_mmu_ops, read_cr3);</span>
<span class="quote">&gt;  		PATCH_SITE(pv_mmu_ops, write_cr3);</span>
<span class="quote">&gt;  		PATCH_SITE(pv_cpu_ops, clts);</span>
<span class="quote">&gt; -#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="quote">&gt; +#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
<span class="quote">&gt;  		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):</span>
<span class="quote">&gt;  			if (pv_is_native_spin_unlock()) {</span>
<span class="quote">&gt;  				start = start_pv_lock_ops_queued_spin_unlock;</span>
<span class="quote">&gt; diff --git a/arch/x86/kernel/paravirt_patch_64.c b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="quote">&gt; index e70087a04cc8..bb3840cedb4f 100644</span>
<span class="quote">&gt; --- a/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="quote">&gt; +++ b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="quote">&gt; @@ -19,7 +19,7 @@ DEF_NATIVE(pv_cpu_ops, swapgs, &quot;swapgs&quot;);</span>
<span class="quote">&gt;  DEF_NATIVE(, mov32, &quot;mov %edi, %eax&quot;);</span>
<span class="quote">&gt;  DEF_NATIVE(, mov64, &quot;mov %rdi, %rax&quot;);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="quote">&gt; +#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
<span class="quote">&gt;  DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%rdi)&quot;);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -61,7 +61,7 @@ unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
<span class="quote">&gt;  		PATCH_SITE(pv_cpu_ops, clts);</span>
<span class="quote">&gt;  		PATCH_SITE(pv_mmu_ops, flush_tlb_single);</span>
<span class="quote">&gt;  		PATCH_SITE(pv_cpu_ops, wbinvd);</span>
<span class="quote">&gt; -#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="quote">&gt; +#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
<span class="quote">&gt;  		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):</span>
<span class="quote">&gt;  			if (pv_is_native_spin_unlock()) {</span>
<span class="quote">&gt;  				start = start_pv_lock_ops_queued_spin_unlock;</span>
<span class="quote">&gt; diff --git a/arch/x86/xen/spinlock.c b/arch/x86/xen/spinlock.c</span>
<span class="quote">&gt; index f42e78de1e10..3d6e0064cbfc 100644</span>
<span class="quote">&gt; --- a/arch/x86/xen/spinlock.c</span>
<span class="quote">&gt; +++ b/arch/x86/xen/spinlock.c</span>
<span class="quote">&gt; @@ -21,8 +21,6 @@ static DEFINE_PER_CPU(int, lock_kicker_irq) = -1;</span>
<span class="quote">&gt;  static DEFINE_PER_CPU(char *, irq_name);</span>
<span class="quote">&gt;  static bool xen_pvspin = true;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  #include &lt;asm/qspinlock.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static void xen_qlock_kick(int cpu)</span>
<span class="quote">&gt; @@ -71,207 +69,6 @@ static void xen_qlock_wait(u8 *byte, u8 val)</span>
<span class="quote">&gt;  	xen_poll_irq(irq);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#else /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -enum xen_contention_stat {</span>
<span class="quote">&gt; -	TAKEN_SLOW,</span>
<span class="quote">&gt; -	TAKEN_SLOW_PICKUP,</span>
<span class="quote">&gt; -	TAKEN_SLOW_SPURIOUS,</span>
<span class="quote">&gt; -	RELEASED_SLOW,</span>
<span class="quote">&gt; -	RELEASED_SLOW_KICKED,</span>
<span class="quote">&gt; -	NR_CONTENTION_STATS</span>
<span class="quote">&gt; -};</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#ifdef CONFIG_XEN_DEBUG_FS</span>
<span class="quote">&gt; -#define HISTO_BUCKETS	30</span>
<span class="quote">&gt; -static struct xen_spinlock_stats</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	u32 contention_stats[NR_CONTENTION_STATS];</span>
<span class="quote">&gt; -	u32 histo_spin_blocked[HISTO_BUCKETS+1];</span>
<span class="quote">&gt; -	u64 time_blocked;</span>
<span class="quote">&gt; -} spinlock_stats;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static u8 zero_stats;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void check_zero(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	u8 ret;</span>
<span class="quote">&gt; -	u8 old = READ_ONCE(zero_stats);</span>
<span class="quote">&gt; -	if (unlikely(old)) {</span>
<span class="quote">&gt; -		ret = cmpxchg(&amp;zero_stats, old, 0);</span>
<span class="quote">&gt; -		/* This ensures only one fellow resets the stat */</span>
<span class="quote">&gt; -		if (ret == old)</span>
<span class="quote">&gt; -			memset(&amp;spinlock_stats, 0, sizeof(spinlock_stats));</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void add_stats(enum xen_contention_stat var, u32 val)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	check_zero();</span>
<span class="quote">&gt; -	spinlock_stats.contention_stats[var] += val;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline u64 spin_time_start(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return xen_clocksource_read();</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static void __spin_time_accum(u64 delta, u32 *array)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	unsigned index = ilog2(delta);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	check_zero();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (index &lt; HISTO_BUCKETS)</span>
<span class="quote">&gt; -		array[index]++;</span>
<span class="quote">&gt; -	else</span>
<span class="quote">&gt; -		array[HISTO_BUCKETS]++;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void spin_time_accum_blocked(u64 start)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	u32 delta = xen_clocksource_read() - start;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	__spin_time_accum(delta, spinlock_stats.histo_spin_blocked);</span>
<span class="quote">&gt; -	spinlock_stats.time_blocked += delta;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -#else  /* !CONFIG_XEN_DEBUG_FS */</span>
<span class="quote">&gt; -static inline void add_stats(enum xen_contention_stat var, u32 val)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline u64 spin_time_start(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	return 0;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static inline void spin_time_accum_blocked(u64 start)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -#endif  /* CONFIG_XEN_DEBUG_FS */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -struct xen_lock_waiting {</span>
<span class="quote">&gt; -	struct arch_spinlock *lock;</span>
<span class="quote">&gt; -	__ticket_t want;</span>
<span class="quote">&gt; -};</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static DEFINE_PER_CPU(struct xen_lock_waiting, lock_waiting);</span>
<span class="quote">&gt; -static cpumask_t waiting_cpus;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -__visible void xen_lock_spinning(struct arch_spinlock *lock, __ticket_t want)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	int irq = __this_cpu_read(lock_kicker_irq);</span>
<span class="quote">&gt; -	struct xen_lock_waiting *w = this_cpu_ptr(&amp;lock_waiting);</span>
<span class="quote">&gt; -	int cpu = smp_processor_id();</span>
<span class="quote">&gt; -	u64 start;</span>
<span class="quote">&gt; -	__ticket_t head;</span>
<span class="quote">&gt; -	unsigned long flags;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* If kicker interrupts not initialized yet, just spin */</span>
<span class="quote">&gt; -	if (irq == -1)</span>
<span class="quote">&gt; -		return;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	start = spin_time_start();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * Make sure an interrupt handler can&#39;t upset things in a</span>
<span class="quote">&gt; -	 * partially setup state.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	local_irq_save(flags);</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * We don&#39;t really care if we&#39;re overwriting some other</span>
<span class="quote">&gt; -	 * (lock,want) pair, as that would mean that we&#39;re currently</span>
<span class="quote">&gt; -	 * in an interrupt context, and the outer context had</span>
<span class="quote">&gt; -	 * interrupts enabled.  That has already kicked the VCPU out</span>
<span class="quote">&gt; -	 * of xen_poll_irq(), so it will just return spuriously and</span>
<span class="quote">&gt; -	 * retry with newly setup (lock,want).</span>
<span class="quote">&gt; -	 *</span>
<span class="quote">&gt; -	 * The ordering protocol on this is that the &quot;lock&quot; pointer</span>
<span class="quote">&gt; -	 * may only be set non-NULL if the &quot;want&quot; ticket is correct.</span>
<span class="quote">&gt; -	 * If we&#39;re updating &quot;want&quot;, we must first clear &quot;lock&quot;.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	w-&gt;lock = NULL;</span>
<span class="quote">&gt; -	smp_wmb();</span>
<span class="quote">&gt; -	w-&gt;want = want;</span>
<span class="quote">&gt; -	smp_wmb();</span>
<span class="quote">&gt; -	w-&gt;lock = lock;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* This uses set_bit, which atomic and therefore a barrier */</span>
<span class="quote">&gt; -	cpumask_set_cpu(cpu, &amp;waiting_cpus);</span>
<span class="quote">&gt; -	add_stats(TAKEN_SLOW, 1);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* clear pending */</span>
<span class="quote">&gt; -	xen_clear_irq_pending(irq);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* Only check lock once pending cleared */</span>
<span class="quote">&gt; -	barrier();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * Mark entry to slowpath before doing the pickup test to make</span>
<span class="quote">&gt; -	 * sure we don&#39;t deadlock with an unlocker.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	__ticket_enter_slowpath(lock);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* make sure enter_slowpath, which is atomic does not cross the read */</span>
<span class="quote">&gt; -	smp_mb__after_atomic();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * check again make sure it didn&#39;t become free while</span>
<span class="quote">&gt; -	 * we weren&#39;t looking</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -	head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="quote">&gt; -	if (__tickets_equal(head, want)) {</span>
<span class="quote">&gt; -		add_stats(TAKEN_SLOW_PICKUP, 1);</span>
<span class="quote">&gt; -		goto out;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* Allow interrupts while blocked */</span>
<span class="quote">&gt; -	local_irq_restore(flags);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/*</span>
<span class="quote">&gt; -	 * If an interrupt happens here, it will leave the wakeup irq</span>
<span class="quote">&gt; -	 * pending, which will cause xen_poll_irq() to return</span>
<span class="quote">&gt; -	 * immediately.</span>
<span class="quote">&gt; -	 */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	/* Block until irq becomes pending (or perhaps a spurious wakeup) */</span>
<span class="quote">&gt; -	xen_poll_irq(irq);</span>
<span class="quote">&gt; -	add_stats(TAKEN_SLOW_SPURIOUS, !xen_test_irq_pending(irq));</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	local_irq_save(flags);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	kstat_incr_irq_this_cpu(irq);</span>
<span class="quote">&gt; -out:</span>
<span class="quote">&gt; -	cpumask_clear_cpu(cpu, &amp;waiting_cpus);</span>
<span class="quote">&gt; -	w-&gt;lock = NULL;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	local_irq_restore(flags);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	spin_time_accum_blocked(start);</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -PV_CALLEE_SAVE_REGS_THUNK(xen_lock_spinning);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static void xen_unlock_kick(struct arch_spinlock *lock, __ticket_t next)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	int cpu;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	add_stats(RELEASED_SLOW, 1);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	for_each_cpu(cpu, &amp;waiting_cpus) {</span>
<span class="quote">&gt; -		const struct xen_lock_waiting *w = &amp;per_cpu(lock_waiting, cpu);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -		/* Make sure we read lock before want */</span>
<span class="quote">&gt; -		if (READ_ONCE(w-&gt;lock) == lock &amp;&amp;</span>
<span class="quote">&gt; -		    READ_ONCE(w-&gt;want) == next) {</span>
<span class="quote">&gt; -			add_stats(RELEASED_SLOW_KICKED, 1);</span>
<span class="quote">&gt; -			xen_send_IPI_one(cpu, XEN_SPIN_UNLOCK_VECTOR);</span>
<span class="quote">&gt; -			break;</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  static irqreturn_t dummy_handler(int irq, void *dev_id)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	BUG();</span>
<span class="quote">&gt; @@ -334,16 +131,12 @@ void __init xen_init_spinlocks(void)</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	printk(KERN_DEBUG &quot;xen: PV spinlocks enabled\n&quot;);</span>
<span class="quote">&gt; -#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	__pv_init_lock_hash();</span>
<span class="quote">&gt;  	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;</span>
<span class="quote">&gt;  	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);</span>
<span class="quote">&gt;  	pv_lock_ops.wait = xen_qlock_wait;</span>
<span class="quote">&gt;  	pv_lock_ops.kick = xen_qlock_kick;</span>
<span class="quote">&gt; -#else</span>
<span class="quote">&gt; -	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(xen_lock_spinning);</span>
<span class="quote">&gt; -	pv_lock_ops.unlock_kick = xen_unlock_kick;</span>
<span class="quote">&gt; -#endif</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; @@ -372,44 +165,3 @@ static __init int xen_parse_nopvspin(char *arg)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  early_param(&quot;xen_nopvspin&quot;, xen_parse_nopvspin);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#if defined(CONFIG_XEN_DEBUG_FS) &amp;&amp; !defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static struct dentry *d_spin_debug;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -static int __init xen_spinlock_debugfs(void)</span>
<span class="quote">&gt; -{</span>
<span class="quote">&gt; -	struct dentry *d_xen = xen_init_debugfs();</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (d_xen == NULL)</span>
<span class="quote">&gt; -		return -ENOMEM;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	if (!xen_pvspin)</span>
<span class="quote">&gt; -		return 0;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	d_spin_debug = debugfs_create_dir(&quot;spinlocks&quot;, d_xen);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	debugfs_create_u8(&quot;zero_stats&quot;, 0644, d_spin_debug, &amp;zero_stats);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	debugfs_create_u32(&quot;taken_slow&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW]);</span>
<span class="quote">&gt; -	debugfs_create_u32(&quot;taken_slow_pickup&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_PICKUP]);</span>
<span class="quote">&gt; -	debugfs_create_u32(&quot;taken_slow_spurious&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_SPURIOUS]);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	debugfs_create_u32(&quot;released_slow&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -			   &amp;spinlock_stats.contention_stats[RELEASED_SLOW]);</span>
<span class="quote">&gt; -	debugfs_create_u32(&quot;released_slow_kicked&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -			   &amp;spinlock_stats.contention_stats[RELEASED_SLOW_KICKED]);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	debugfs_create_u64(&quot;time_blocked&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -			   &amp;spinlock_stats.time_blocked);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	debugfs_create_u32_array(&quot;histo_blocked&quot;, 0444, d_spin_debug,</span>
<span class="quote">&gt; -				spinlock_stats.histo_spin_blocked, HISTO_BUCKETS + 1);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	return 0;</span>
<span class="quote">&gt; -}</span>
<span class="quote">&gt; -fs_initcall(xen_spinlock_debugfs);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#endif	/* CONFIG_XEN_DEBUG_FS */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; _______________________________________________</span>
<span class="quote">&gt; Xen-devel mailing list</span>
<span class="quote">&gt; Xen-devel@lists.xen.org</span>
<span class="quote">&gt; http://lists.xen.org/xen-devel</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137">Peter Zijlstra</a> - May 18, 2016, 7:34 p.m.</div>
<pre class="content">
On Wed, May 18, 2016 at 03:13:44PM -0400, Konrad Rzeszutek Wilk wrote:
<span class="quote">&gt; On Wed, May 18, 2016 at 08:43:02PM +0200, Peter Zijlstra wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We&#39;ve unconditionally used the queued spinlock for many releases now.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Like since 4.2?</span>

Yeah, that seems to be the right number.
<span class="quote">
&gt; I don&#39;t know of any enterprise distro that is shipping anything</span>
<span class="quote">&gt; more modern than 4.1?</span>

RHEL 7			-- v3.10
SLES 12			-- v3.12
Debian Jessie		-- v3.16
Ubuntu 16.04 LTS	-- v4.4

But waiting for the major enterprise distros (RHEL/SLES) would mean
another decade or so before people start using it. We don&#39;t usually wait
this long for anything.
<span class="quote">
&gt; Perhaps it would be good to wait until they</span>
<span class="quote">&gt; at least ship and then give them some time to see if they have found</span>
<span class="quote">&gt; any issues?</span>

My motivation was that people keep trying to send patches against the
ticket lock code... David did just today, and he&#39;s not the first.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - May 19, 2016, 7:16 a.m.</div>
<pre class="content">
* Peter Zijlstra &lt;peterz@infradead.org&gt; wrote:
<span class="quote">
&gt; My motivation was that people keep trying to send patches against the ticket </span>
<span class="quote">&gt; lock code... David did just today, and he&#39;s not the first.</span>

Yeah, let&#39;s just remove dead code ASAP. It&#39;s not like it&#39;s hard to add (back) new 
code, should the need arise.

Thanks,

	Ingo
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=38901">David Vrabel</a> - May 19, 2016, 9:07 a.m.</div>
<pre class="content">
On 18/05/16 19:43, Peter Zijlstra wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; We&#39;ve unconditionally used the queued spinlock for many releases now.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Its time to remove the old ticket lock code.</span>

Xen parts:
<span class="acked-by">
Acked-by: David Vrabel &lt;david.vrabel@citrix.com&gt;</span>

David
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2536">Paolo Bonzini</a> - May 19, 2016, 10:52 a.m.</div>
<pre class="content">
On 18/05/2016 21:34, Peter Zijlstra wrote:
<span class="quote">&gt;&gt; I don&#39;t know of any enterprise distro that is shipping anything</span>
<span class="quote">&gt;&gt; &gt; more modern than 4.1?</span>
<span class="quote">&gt; RHEL 7			-- v3.10</span>
<span class="quote">&gt; SLES 12			-- v3.12</span>
<span class="quote">&gt; Debian Jessie		-- v3.16</span>
<span class="quote">&gt; Ubuntu 16.04 LTS	-- v4.4</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But waiting for the major enterprise distros (RHEL/SLES) would mean</span>
<span class="quote">&gt; another decade or so before people start using it. We don&#39;t usually wait</span>
<span class="quote">&gt; this long for anything.</span>

We&#39;re looking at converting a few specific spinlocks to qspinlock in
RHEL, though we cannot convert all of them due to the spin_lock_t ABI.
It won&#39;t get into customer&#39;s hands for a while of course.

Thanks,

Paolo
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 7bb15747fea2..a5543914f6dd 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -705,7 +705,6 @@</span> <span class="p_context"> config PARAVIRT_DEBUG</span>
 config PARAVIRT_SPINLOCKS
 	bool &quot;Paravirtualization layer for spinlocks&quot;
 	depends on PARAVIRT &amp;&amp; SMP
<span class="p_del">-	select UNINLINE_SPIN_UNLOCK if !QUEUED_SPINLOCKS</span>
 	---help---
 	  Paravirtualized spinlocks allow a pvops backend to replace the
 	  spinlock implementation with something virtualization-friendly
<span class="p_chunk">@@ -718,7 +717,7 @@</span> <span class="p_context"> config PARAVIRT_SPINLOCKS</span>
 
 config QUEUED_LOCK_STAT
 	bool &quot;Paravirt queued spinlock statistics&quot;
<span class="p_del">-	depends on PARAVIRT_SPINLOCKS &amp;&amp; DEBUG_FS &amp;&amp; QUEUED_SPINLOCKS</span>
<span class="p_add">+	depends on PARAVIRT_SPINLOCKS &amp;&amp; DEBUG_FS</span>
 	---help---
 	  Enable the collection of statistical data on the slowpath
 	  behavior of paravirtualized queued spinlocks and report
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">index 2970d22d7766..4cd8db05301f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt.h</span>
<span class="p_chunk">@@ -661,8 +661,6 @@</span> <span class="p_context"> static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,</span>
 
 #if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_PARAVIRT_SPINLOCKS)
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_del">-</span>
 static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
 							u32 val)
 {
<span class="p_chunk">@@ -684,22 +682,6 @@</span> <span class="p_context"> static __always_inline void pv_kick(int cpu)</span>
 	PVOP_VCALL1(pv_lock_ops.kick, cpu);
 }
 
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void __ticket_lock_spinning(struct arch_spinlock *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-	PVOP_VCALLEE2(pv_lock_ops.lock_spinning, lock, ticket);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void __ticket_unlock_kick(struct arch_spinlock *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-	PVOP_VCALL2(pv_lock_ops.unlock_kick, lock, ticket);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
 #endif /* SMP &amp;&amp; PARAVIRT_SPINLOCKS */
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">index 7fa9e7740ba3..60aac60ba25f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_chunk">@@ -301,23 +301,16 @@</span> <span class="p_context"> struct pv_mmu_ops {</span>
 struct arch_spinlock;
 #ifdef CONFIG_SMP
 #include &lt;asm/spinlock_types.h&gt;
<span class="p_del">-#else</span>
<span class="p_del">-typedef u16 __ticket_t;</span>
 #endif
 
 struct qspinlock;
 
 struct pv_lock_ops {
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 	void (*queued_spin_lock_slowpath)(struct qspinlock *lock, u32 val);
 	struct paravirt_callee_save queued_spin_unlock;
 
 	void (*wait)(u8 *ptr, u8 val);
 	void (*kick)(int cpu);
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-	struct paravirt_callee_save lock_spinning;</span>
<span class="p_del">-	void (*unlock_kick)(struct arch_spinlock *lock, __ticket_t ticket);</span>
<span class="p_del">-#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
 };
 
 /* This contains all the paravirt structures: we get a convenient
<span class="p_header">diff --git a/arch/x86/include/asm/spinlock.h b/arch/x86/include/asm/spinlock.h</span>
<span class="p_header">index be0a05913b91..921bea7a2708 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/spinlock.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/spinlock.h</span>
<span class="p_chunk">@@ -20,187 +20,13 @@</span> <span class="p_context"></span>
  * (the type definitions are in asm/spinlock_types.h)
  */
 
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-# define LOCK_PTR_REG &quot;a&quot;</span>
<span class="p_del">-#else</span>
<span class="p_del">-# define LOCK_PTR_REG &quot;D&quot;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#if defined(CONFIG_X86_32) &amp;&amp; (defined(CONFIG_X86_PPRO_FENCE))</span>
<span class="p_del">-/*</span>
<span class="p_del">- * On PPro SMP, we use a locked operation to unlock</span>
<span class="p_del">- * (PPro errata 66, 92)</span>
<span class="p_del">- */</span>
<span class="p_del">-# define UNLOCK_LOCK_PREFIX LOCK_PREFIX</span>
<span class="p_del">-#else</span>
<span class="p_del">-# define UNLOCK_LOCK_PREFIX</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 /* How long a lock should spin before we consider blocking */
 #define SPIN_THRESHOLD	(1 &lt;&lt; 15)
 
 extern struct static_key paravirt_ticketlocks_enabled;
 static __always_inline bool static_key_false(struct static_key *key);
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 #include &lt;asm/qspinlock.h&gt;
<span class="p_del">-#else</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_PARAVIRT_SPINLOCKS</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void __ticket_enter_slowpath(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	set_bit(0, (volatile unsigned long *)&amp;lock-&gt;tickets.head);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#else  /* !CONFIG_PARAVIRT_SPINLOCKS */</span>
<span class="p_del">-static __always_inline void __ticket_lock_spinning(arch_spinlock_t *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-static inline void __ticket_unlock_kick(arch_spinlock_t *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* CONFIG_PARAVIRT_SPINLOCKS */</span>
<span class="p_del">-static inline int  __tickets_equal(__ticket_t one, __ticket_t two)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return !((one ^ two) &amp; ~TICKET_SLOWPATH_FLAG);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void __ticket_check_and_clear_slowpath(arch_spinlock_t *lock,</span>
<span class="p_del">-							__ticket_t head)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (head &amp; TICKET_SLOWPATH_FLAG) {</span>
<span class="p_del">-		arch_spinlock_t old, new;</span>
<span class="p_del">-</span>
<span class="p_del">-		old.tickets.head = head;</span>
<span class="p_del">-		new.tickets.head = head &amp; ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-		old.tickets.tail = new.tickets.head + TICKET_LOCK_INC;</span>
<span class="p_del">-		new.tickets.tail = old.tickets.tail;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* try to clear slowpath flag when there are no contenders */</span>
<span class="p_del">-		cmpxchg(&amp;lock-&gt;head_tail, old.head_tail, new.head_tail);</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline int arch_spin_value_unlocked(arch_spinlock_t lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return __tickets_equal(lock.tickets.head, lock.tickets.tail);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Ticket locks are conceptually two parts, one indicating the current head of</span>
<span class="p_del">- * the queue, and the other indicating the current tail. The lock is acquired</span>
<span class="p_del">- * by atomically noting the tail and incrementing it by one (thus adding</span>
<span class="p_del">- * ourself to the queue and noting our position), then waiting until the head</span>
<span class="p_del">- * becomes equal to the the initial value of the tail.</span>
<span class="p_del">- *</span>
<span class="p_del">- * We use an xadd covering *both* parts of the lock, to increment the tail and</span>
<span class="p_del">- * also load the position of the head, which takes care of memory ordering</span>
<span class="p_del">- * issues and should be optimal for the uncontended case. Note the tail must be</span>
<span class="p_del">- * in the high part, because a wide xadd increment of the low part would carry</span>
<span class="p_del">- * up and contaminate the high part.</span>
<span class="p_del">- */</span>
<span class="p_del">-static __always_inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	register struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };</span>
<span class="p_del">-</span>
<span class="p_del">-	inc = xadd(&amp;lock-&gt;tickets, inc);</span>
<span class="p_del">-	if (likely(inc.head == inc.tail))</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-</span>
<span class="p_del">-	for (;;) {</span>
<span class="p_del">-		unsigned count = SPIN_THRESHOLD;</span>
<span class="p_del">-</span>
<span class="p_del">-		do {</span>
<span class="p_del">-			inc.head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-			if (__tickets_equal(inc.head, inc.tail))</span>
<span class="p_del">-				goto clear_slowpath;</span>
<span class="p_del">-			cpu_relax();</span>
<span class="p_del">-		} while (--count);</span>
<span class="p_del">-		__ticket_lock_spinning(lock, inc.tail);</span>
<span class="p_del">-	}</span>
<span class="p_del">-clear_slowpath:</span>
<span class="p_del">-	__ticket_check_and_clear_slowpath(lock, inc.head);</span>
<span class="p_del">-out:</span>
<span class="p_del">-	barrier();	/* make sure nothing creeps before the lock is taken */</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	arch_spinlock_t old, new;</span>
<span class="p_del">-</span>
<span class="p_del">-	old.tickets = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-	if (!__tickets_equal(old.tickets.head, old.tickets.tail))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	new.head_tail = old.head_tail + (TICKET_LOCK_INC &lt;&lt; TICKET_SHIFT);</span>
<span class="p_del">-	new.head_tail &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* cmpxchg is a full barrier, so nothing can move before it */</span>
<span class="p_del">-	return cmpxchg(&amp;lock-&gt;head_tail, old.head_tail, new.head_tail) == old.head_tail;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (TICKET_SLOWPATH_FLAG &amp;&amp;</span>
<span class="p_del">-		static_key_false(&amp;paravirt_ticketlocks_enabled)) {</span>
<span class="p_del">-		__ticket_t head;</span>
<span class="p_del">-</span>
<span class="p_del">-		BUILD_BUG_ON(((__ticket_t)NR_CPUS) != NR_CPUS);</span>
<span class="p_del">-</span>
<span class="p_del">-		head = xadd(&amp;lock-&gt;tickets.head, TICKET_LOCK_INC);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (unlikely(head &amp; TICKET_SLOWPATH_FLAG)) {</span>
<span class="p_del">-			head &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-			__ticket_unlock_kick(lock, (head + TICKET_LOCK_INC));</span>
<span class="p_del">-		}</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		__add(&amp;lock-&gt;tickets.head, TICKET_LOCK_INC, UNLOCK_LOCK_PREFIX);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int arch_spin_is_locked(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-</span>
<span class="p_del">-	return !__tickets_equal(tmp.tail, tmp.head);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int arch_spin_is_contended(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-</span>
<span class="p_del">-	tmp.head &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-	return (__ticket_t)(tmp.tail - tmp.head) &gt; TICKET_LOCK_INC;</span>
<span class="p_del">-}</span>
<span class="p_del">-#define arch_spin_is_contended	arch_spin_is_contended</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void arch_spin_lock_flags(arch_spinlock_t *lock,</span>
<span class="p_del">-						  unsigned long flags)</span>
<span class="p_del">-{</span>
<span class="p_del">-	arch_spin_lock(lock);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void arch_spin_unlock_wait(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__ticket_t head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-</span>
<span class="p_del">-	for (;;) {</span>
<span class="p_del">-		struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * We need to check &quot;unlocked&quot; in a loop, tmp.head == head</span>
<span class="p_del">-		 * can be false positive because of overflow.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (__tickets_equal(tmp.head, tmp.tail) ||</span>
<span class="p_del">-				!__tickets_equal(tmp.head, head))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-</span>
<span class="p_del">-		cpu_relax();</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
 
 /*
  * Read-write spinlocks, allowing multiple readers
<span class="p_header">diff --git a/arch/x86/include/asm/spinlock_types.h b/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_header">index 65c3e37f879a..25311ebb446c 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_chunk">@@ -23,20 +23,7 @@</span> <span class="p_context"> typedef u32 __ticketpair_t;</span>
 
 #define TICKET_SHIFT	(sizeof(__ticket_t) * 8)
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 #include &lt;asm-generic/qspinlock_types.h&gt;
<span class="p_del">-#else</span>
<span class="p_del">-typedef struct arch_spinlock {</span>
<span class="p_del">-	union {</span>
<span class="p_del">-		__ticketpair_t head_tail;</span>
<span class="p_del">-		struct __raw_tickets {</span>
<span class="p_del">-			__ticket_t head, tail;</span>
<span class="p_del">-		} tickets;</span>
<span class="p_del">-	};</span>
<span class="p_del">-} arch_spinlock_t;</span>
<span class="p_del">-</span>
<span class="p_del">-#define __ARCH_SPIN_LOCK_UNLOCKED	{ { 0 } }</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
 
 #include &lt;asm-generic/qrwlock_types.h&gt;
 
<span class="p_header">diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c</span>
<span class="p_header">index eea2a6f72b31..3c4ab6156a89 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvm.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvm.c</span>
<span class="p_chunk">@@ -577,9 +577,6 @@</span> <span class="p_context"> static void kvm_kick_cpu(int cpu)</span>
 	kvm_hypercall2(KVM_HC_KICK_CPU, flags, apicid);
 }
 
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_del">-</span>
 #include &lt;asm/qspinlock.h&gt;
 
 static void kvm_wait(u8 *ptr, u8 val)
<span class="p_chunk">@@ -608,243 +605,6 @@</span> <span class="p_context"> static void kvm_wait(u8 *ptr, u8 val)</span>
 	local_irq_restore(flags);
 }
 
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
<span class="p_del">-enum kvm_contention_stat {</span>
<span class="p_del">-	TAKEN_SLOW,</span>
<span class="p_del">-	TAKEN_SLOW_PICKUP,</span>
<span class="p_del">-	RELEASED_SLOW,</span>
<span class="p_del">-	RELEASED_SLOW_KICKED,</span>
<span class="p_del">-	NR_CONTENTION_STATS</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_KVM_DEBUG_FS</span>
<span class="p_del">-#define HISTO_BUCKETS	30</span>
<span class="p_del">-</span>
<span class="p_del">-static struct kvm_spinlock_stats</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 contention_stats[NR_CONTENTION_STATS];</span>
<span class="p_del">-	u32 histo_spin_blocked[HISTO_BUCKETS+1];</span>
<span class="p_del">-	u64 time_blocked;</span>
<span class="p_del">-} spinlock_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static u8 zero_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void check_zero(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u8 ret;</span>
<span class="p_del">-	u8 old;</span>
<span class="p_del">-</span>
<span class="p_del">-	old = READ_ONCE(zero_stats);</span>
<span class="p_del">-	if (unlikely(old)) {</span>
<span class="p_del">-		ret = cmpxchg(&amp;zero_stats, old, 0);</span>
<span class="p_del">-		/* This ensures only one fellow resets the stat */</span>
<span class="p_del">-		if (ret == old)</span>
<span class="p_del">-			memset(&amp;spinlock_stats, 0, sizeof(spinlock_stats));</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void add_stats(enum kvm_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-	spinlock_stats.contention_stats[var] += val;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return sched_clock();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __spin_time_accum(u64 delta, u32 *array)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned index;</span>
<span class="p_del">-</span>
<span class="p_del">-	index = ilog2(delta);</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (index &lt; HISTO_BUCKETS)</span>
<span class="p_del">-		array[index]++;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		array[HISTO_BUCKETS]++;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 delta;</span>
<span class="p_del">-</span>
<span class="p_del">-	delta = sched_clock() - start;</span>
<span class="p_del">-	__spin_time_accum(delta, spinlock_stats.histo_spin_blocked);</span>
<span class="p_del">-	spinlock_stats.time_blocked += delta;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static struct dentry *d_spin_debug;</span>
<span class="p_del">-static struct dentry *d_kvm_debug;</span>
<span class="p_del">-</span>
<span class="p_del">-static struct dentry *kvm_init_debugfs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	d_kvm_debug = debugfs_create_dir(&quot;kvm-guest&quot;, NULL);</span>
<span class="p_del">-	if (!d_kvm_debug)</span>
<span class="p_del">-		printk(KERN_WARNING &quot;Could not create &#39;kvm&#39; debugfs directory\n&quot;);</span>
<span class="p_del">-</span>
<span class="p_del">-	return d_kvm_debug;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init kvm_spinlock_debugfs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct dentry *d_kvm;</span>
<span class="p_del">-</span>
<span class="p_del">-	d_kvm = kvm_init_debugfs();</span>
<span class="p_del">-	if (d_kvm == NULL)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-	d_spin_debug = debugfs_create_dir(&quot;spinlocks&quot;, d_kvm);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u8(&quot;zero_stats&quot;, 0644, d_spin_debug, &amp;zero_stats);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[TAKEN_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow_pickup&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_PICKUP]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[RELEASED_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow_kicked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[RELEASED_SLOW_KICKED]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u64(&quot;time_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.time_blocked);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32_array(&quot;histo_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		     spinlock_stats.histo_spin_blocked, HISTO_BUCKETS + 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-fs_initcall(kvm_spinlock_debugfs);</span>
<span class="p_del">-#else  /* !CONFIG_KVM_DEBUG_FS */</span>
<span class="p_del">-static inline void add_stats(enum kvm_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif  /* CONFIG_KVM_DEBUG_FS */</span>
<span class="p_del">-</span>
<span class="p_del">-struct kvm_lock_waiting {</span>
<span class="p_del">-	struct arch_spinlock *lock;</span>
<span class="p_del">-	__ticket_t want;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-/* cpus &#39;waiting&#39; on a spinlock to become available */</span>
<span class="p_del">-static cpumask_t waiting_cpus;</span>
<span class="p_del">-</span>
<span class="p_del">-/* Track spinlock on which a cpu is waiting */</span>
<span class="p_del">-static DEFINE_PER_CPU(struct kvm_lock_waiting, klock_waiting);</span>
<span class="p_del">-</span>
<span class="p_del">-__visible void kvm_lock_spinning(struct arch_spinlock *lock, __ticket_t want)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct kvm_lock_waiting *w;</span>
<span class="p_del">-	int cpu;</span>
<span class="p_del">-	u64 start;</span>
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-	__ticket_t head;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (in_nmi())</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	w = this_cpu_ptr(&amp;klock_waiting);</span>
<span class="p_del">-	cpu = smp_processor_id();</span>
<span class="p_del">-	start = spin_time_start();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Make sure an interrupt handler can&#39;t upset things in a</span>
<span class="p_del">-	 * partially setup state.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The ordering protocol on this is that the &quot;lock&quot; pointer</span>
<span class="p_del">-	 * may only be set non-NULL if the &quot;want&quot; ticket is correct.</span>
<span class="p_del">-	 * If we&#39;re updating &quot;want&quot;, we must first clear &quot;lock&quot;.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;want = want;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;lock = lock;</span>
<span class="p_del">-</span>
<span class="p_del">-	add_stats(TAKEN_SLOW, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This uses set_bit, which is atomic but we should not rely on its</span>
<span class="p_del">-	 * reordering gurantees. So barrier is needed after this call.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	cpumask_set_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-</span>
<span class="p_del">-	barrier();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Mark entry to slowpath before doing the pickup test to make</span>
<span class="p_del">-	 * sure we don&#39;t deadlock with an unlocker.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	__ticket_enter_slowpath(lock);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* make sure enter_slowpath, which is atomic does not cross the read */</span>
<span class="p_del">-	smp_mb__after_atomic();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * check again make sure it didn&#39;t become free while</span>
<span class="p_del">-	 * we weren&#39;t looking.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-	if (__tickets_equal(head, want)) {</span>
<span class="p_del">-		add_stats(TAKEN_SLOW_PICKUP, 1);</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * halt until it&#39;s our turn and kicked. Note that we do safe halt</span>
<span class="p_del">-	 * for irq enabled case to avoid hang when lock info is overwritten</span>
<span class="p_del">-	 * in irq spinlock slowpath and no spurious interrupt occur to save us.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (arch_irqs_disabled_flags(flags))</span>
<span class="p_del">-		halt();</span>
<span class="p_del">-	else</span>
<span class="p_del">-		safe_halt();</span>
<span class="p_del">-</span>
<span class="p_del">-out:</span>
<span class="p_del">-	cpumask_clear_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-	spin_time_accum_blocked(start);</span>
<span class="p_del">-}</span>
<span class="p_del">-PV_CALLEE_SAVE_REGS_THUNK(kvm_lock_spinning);</span>
<span class="p_del">-</span>
<span class="p_del">-/* Kick vcpu waiting on @lock-&gt;head to reach value @ticket */</span>
<span class="p_del">-static void kvm_unlock_kick(struct arch_spinlock *lock, __ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	add_stats(RELEASED_SLOW, 1);</span>
<span class="p_del">-	for_each_cpu(cpu, &amp;waiting_cpus) {</span>
<span class="p_del">-		const struct kvm_lock_waiting *w = &amp;per_cpu(klock_waiting, cpu);</span>
<span class="p_del">-		if (READ_ONCE(w-&gt;lock) == lock &amp;&amp;</span>
<span class="p_del">-		    READ_ONCE(w-&gt;want) == ticket) {</span>
<span class="p_del">-			add_stats(RELEASED_SLOW_KICKED, 1);</span>
<span class="p_del">-			kvm_kick_cpu(cpu);</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
 /*
  * Setup pv_lock_ops to exploit KVM_FEATURE_PV_UNHALT if present.
  */
<span class="p_chunk">@@ -856,16 +616,11 @@</span> <span class="p_context"> void __init kvm_spinlock_init(void)</span>
 	if (!kvm_para_has_feature(KVM_FEATURE_PV_UNHALT))
 		return;
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 	__pv_init_lock_hash();
 	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
 	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
 	pv_lock_ops.wait = kvm_wait;
 	pv_lock_ops.kick = kvm_kick_cpu;
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(kvm_lock_spinning);</span>
<span class="p_del">-	pv_lock_ops.unlock_kick = kvm_unlock_kick;</span>
<span class="p_del">-#endif</span>
 }
 
 static __init int kvm_spinlock_init_jump(void)
<span class="p_header">diff --git a/arch/x86/kernel/paravirt-spinlocks.c b/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_header">index 33ee3e0efd65..fd672d8b33dc 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_chunk">@@ -8,7 +8,6 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/paravirt.h&gt;
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 __visible void __native_queued_spin_unlock(struct qspinlock *lock)
 {
 	native_queued_spin_unlock(lock);
<span class="p_chunk">@@ -21,19 +20,13 @@</span> <span class="p_context"> bool pv_is_native_spin_unlock(void)</span>
 	return pv_lock_ops.queued_spin_unlock.func ==
 		__raw_callee_save___native_queued_spin_unlock;
 }
<span class="p_del">-#endif</span>
 
 struct pv_lock_ops pv_lock_ops = {
 #ifdef CONFIG_SMP
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 	.queued_spin_lock_slowpath = native_queued_spin_lock_slowpath,
 	.queued_spin_unlock = PV_CALLEE_SAVE(__native_queued_spin_unlock),
 	.wait = paravirt_nop,
 	.kick = paravirt_nop,
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-	.lock_spinning = __PV_IS_CALLEE_SAVE(paravirt_nop),</span>
<span class="p_del">-	.unlock_kick = paravirt_nop,</span>
<span class="p_del">-#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
 #endif /* SMP */
 };
 EXPORT_SYMBOL(pv_lock_ops);
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_32.c b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">index 158dc0650d5d..920c6ae08592 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_chunk">@@ -10,7 +10,7 @@</span> <span class="p_context"> DEF_NATIVE(pv_mmu_ops, write_cr3, &quot;mov %eax, %cr3&quot;);</span>
 DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;mov %cr3, %eax&quot;);
 DEF_NATIVE(pv_cpu_ops, clts, &quot;clts&quot;);
 
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%eax)&quot;);
 #endif
 
<span class="p_chunk">@@ -49,7 +49,7 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_mmu_ops, read_cr3);
 		PATCH_SITE(pv_mmu_ops, write_cr3);
 		PATCH_SITE(pv_cpu_ops, clts);
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):
 			if (pv_is_native_spin_unlock()) {
 				start = start_pv_lock_ops_queued_spin_unlock;
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_64.c b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">index e70087a04cc8..bb3840cedb4f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_chunk">@@ -19,7 +19,7 @@</span> <span class="p_context"> DEF_NATIVE(pv_cpu_ops, swapgs, &quot;swapgs&quot;);</span>
 DEF_NATIVE(, mov32, &quot;mov %edi, %eax&quot;);
 DEF_NATIVE(, mov64, &quot;mov %rdi, %rax&quot;);
 
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%rdi)&quot;);
 #endif
 
<span class="p_chunk">@@ -61,7 +61,7 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_cpu_ops, clts);
 		PATCH_SITE(pv_mmu_ops, flush_tlb_single);
 		PATCH_SITE(pv_cpu_ops, wbinvd);
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):
 			if (pv_is_native_spin_unlock()) {
 				start = start_pv_lock_ops_queued_spin_unlock;
<span class="p_header">diff --git a/arch/x86/xen/spinlock.c b/arch/x86/xen/spinlock.c</span>
<span class="p_header">index f42e78de1e10..3d6e0064cbfc 100644</span>
<span class="p_header">--- a/arch/x86/xen/spinlock.c</span>
<span class="p_header">+++ b/arch/x86/xen/spinlock.c</span>
<span class="p_chunk">@@ -21,8 +21,6 @@</span> <span class="p_context"> static DEFINE_PER_CPU(int, lock_kicker_irq) = -1;</span>
 static DEFINE_PER_CPU(char *, irq_name);
 static bool xen_pvspin = true;
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_del">-</span>
 #include &lt;asm/qspinlock.h&gt;
 
 static void xen_qlock_kick(int cpu)
<span class="p_chunk">@@ -71,207 +69,6 @@</span> <span class="p_context"> static void xen_qlock_wait(u8 *byte, u8 val)</span>
 	xen_poll_irq(irq);
 }
 
<span class="p_del">-#else /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
<span class="p_del">-enum xen_contention_stat {</span>
<span class="p_del">-	TAKEN_SLOW,</span>
<span class="p_del">-	TAKEN_SLOW_PICKUP,</span>
<span class="p_del">-	TAKEN_SLOW_SPURIOUS,</span>
<span class="p_del">-	RELEASED_SLOW,</span>
<span class="p_del">-	RELEASED_SLOW_KICKED,</span>
<span class="p_del">-	NR_CONTENTION_STATS</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_XEN_DEBUG_FS</span>
<span class="p_del">-#define HISTO_BUCKETS	30</span>
<span class="p_del">-static struct xen_spinlock_stats</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 contention_stats[NR_CONTENTION_STATS];</span>
<span class="p_del">-	u32 histo_spin_blocked[HISTO_BUCKETS+1];</span>
<span class="p_del">-	u64 time_blocked;</span>
<span class="p_del">-} spinlock_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static u8 zero_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void check_zero(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u8 ret;</span>
<span class="p_del">-	u8 old = READ_ONCE(zero_stats);</span>
<span class="p_del">-	if (unlikely(old)) {</span>
<span class="p_del">-		ret = cmpxchg(&amp;zero_stats, old, 0);</span>
<span class="p_del">-		/* This ensures only one fellow resets the stat */</span>
<span class="p_del">-		if (ret == old)</span>
<span class="p_del">-			memset(&amp;spinlock_stats, 0, sizeof(spinlock_stats));</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void add_stats(enum xen_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-	spinlock_stats.contention_stats[var] += val;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return xen_clocksource_read();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __spin_time_accum(u64 delta, u32 *array)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned index = ilog2(delta);</span>
<span class="p_del">-</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (index &lt; HISTO_BUCKETS)</span>
<span class="p_del">-		array[index]++;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		array[HISTO_BUCKETS]++;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 delta = xen_clocksource_read() - start;</span>
<span class="p_del">-</span>
<span class="p_del">-	__spin_time_accum(delta, spinlock_stats.histo_spin_blocked);</span>
<span class="p_del">-	spinlock_stats.time_blocked += delta;</span>
<span class="p_del">-}</span>
<span class="p_del">-#else  /* !CONFIG_XEN_DEBUG_FS */</span>
<span class="p_del">-static inline void add_stats(enum xen_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif  /* CONFIG_XEN_DEBUG_FS */</span>
<span class="p_del">-</span>
<span class="p_del">-struct xen_lock_waiting {</span>
<span class="p_del">-	struct arch_spinlock *lock;</span>
<span class="p_del">-	__ticket_t want;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-static DEFINE_PER_CPU(struct xen_lock_waiting, lock_waiting);</span>
<span class="p_del">-static cpumask_t waiting_cpus;</span>
<span class="p_del">-</span>
<span class="p_del">-__visible void xen_lock_spinning(struct arch_spinlock *lock, __ticket_t want)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int irq = __this_cpu_read(lock_kicker_irq);</span>
<span class="p_del">-	struct xen_lock_waiting *w = this_cpu_ptr(&amp;lock_waiting);</span>
<span class="p_del">-	int cpu = smp_processor_id();</span>
<span class="p_del">-	u64 start;</span>
<span class="p_del">-	__ticket_t head;</span>
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* If kicker interrupts not initialized yet, just spin */</span>
<span class="p_del">-	if (irq == -1)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	start = spin_time_start();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Make sure an interrupt handler can&#39;t upset things in a</span>
<span class="p_del">-	 * partially setup state.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We don&#39;t really care if we&#39;re overwriting some other</span>
<span class="p_del">-	 * (lock,want) pair, as that would mean that we&#39;re currently</span>
<span class="p_del">-	 * in an interrupt context, and the outer context had</span>
<span class="p_del">-	 * interrupts enabled.  That has already kicked the VCPU out</span>
<span class="p_del">-	 * of xen_poll_irq(), so it will just return spuriously and</span>
<span class="p_del">-	 * retry with newly setup (lock,want).</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * The ordering protocol on this is that the &quot;lock&quot; pointer</span>
<span class="p_del">-	 * may only be set non-NULL if the &quot;want&quot; ticket is correct.</span>
<span class="p_del">-	 * If we&#39;re updating &quot;want&quot;, we must first clear &quot;lock&quot;.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;want = want;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;lock = lock;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* This uses set_bit, which atomic and therefore a barrier */</span>
<span class="p_del">-	cpumask_set_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-	add_stats(TAKEN_SLOW, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* clear pending */</span>
<span class="p_del">-	xen_clear_irq_pending(irq);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Only check lock once pending cleared */</span>
<span class="p_del">-	barrier();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Mark entry to slowpath before doing the pickup test to make</span>
<span class="p_del">-	 * sure we don&#39;t deadlock with an unlocker.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	__ticket_enter_slowpath(lock);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* make sure enter_slowpath, which is atomic does not cross the read */</span>
<span class="p_del">-	smp_mb__after_atomic();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * check again make sure it didn&#39;t become free while</span>
<span class="p_del">-	 * we weren&#39;t looking</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-	if (__tickets_equal(head, want)) {</span>
<span class="p_del">-		add_stats(TAKEN_SLOW_PICKUP, 1);</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Allow interrupts while blocked */</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If an interrupt happens here, it will leave the wakeup irq</span>
<span class="p_del">-	 * pending, which will cause xen_poll_irq() to return</span>
<span class="p_del">-	 * immediately.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Block until irq becomes pending (or perhaps a spurious wakeup) */</span>
<span class="p_del">-	xen_poll_irq(irq);</span>
<span class="p_del">-	add_stats(TAKEN_SLOW_SPURIOUS, !xen_test_irq_pending(irq));</span>
<span class="p_del">-</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	kstat_incr_irq_this_cpu(irq);</span>
<span class="p_del">-out:</span>
<span class="p_del">-	cpumask_clear_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	spin_time_accum_blocked(start);</span>
<span class="p_del">-}</span>
<span class="p_del">-PV_CALLEE_SAVE_REGS_THUNK(xen_lock_spinning);</span>
<span class="p_del">-</span>
<span class="p_del">-static void xen_unlock_kick(struct arch_spinlock *lock, __ticket_t next)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	add_stats(RELEASED_SLOW, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	for_each_cpu(cpu, &amp;waiting_cpus) {</span>
<span class="p_del">-		const struct xen_lock_waiting *w = &amp;per_cpu(lock_waiting, cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Make sure we read lock before want */</span>
<span class="p_del">-		if (READ_ONCE(w-&gt;lock) == lock &amp;&amp;</span>
<span class="p_del">-		    READ_ONCE(w-&gt;want) == next) {</span>
<span class="p_del">-			add_stats(RELEASED_SLOW_KICKED, 1);</span>
<span class="p_del">-			xen_send_IPI_one(cpu, XEN_SPIN_UNLOCK_VECTOR);</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
 static irqreturn_t dummy_handler(int irq, void *dev_id)
 {
 	BUG();
<span class="p_chunk">@@ -334,16 +131,12 @@</span> <span class="p_context"> void __init xen_init_spinlocks(void)</span>
 		return;
 	}
 	printk(KERN_DEBUG &quot;xen: PV spinlocks enabled\n&quot;);
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+</span>
 	__pv_init_lock_hash();
 	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
 	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
 	pv_lock_ops.wait = xen_qlock_wait;
 	pv_lock_ops.kick = xen_qlock_kick;
<span class="p_del">-#else</span>
<span class="p_del">-	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(xen_lock_spinning);</span>
<span class="p_del">-	pv_lock_ops.unlock_kick = xen_unlock_kick;</span>
<span class="p_del">-#endif</span>
 }
 
 /*
<span class="p_chunk">@@ -372,44 +165,3 @@</span> <span class="p_context"> static __init int xen_parse_nopvspin(char *arg)</span>
 }
 early_param(&quot;xen_nopvspin&quot;, xen_parse_nopvspin);
 
<span class="p_del">-#if defined(CONFIG_XEN_DEBUG_FS) &amp;&amp; !defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_del">-</span>
<span class="p_del">-static struct dentry *d_spin_debug;</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init xen_spinlock_debugfs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct dentry *d_xen = xen_init_debugfs();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (d_xen == NULL)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!xen_pvspin)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	d_spin_debug = debugfs_create_dir(&quot;spinlocks&quot;, d_xen);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u8(&quot;zero_stats&quot;, 0644, d_spin_debug, &amp;zero_stats);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow_pickup&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_PICKUP]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow_spurious&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_SPURIOUS]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[RELEASED_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow_kicked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[RELEASED_SLOW_KICKED]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u64(&quot;time_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.time_blocked);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32_array(&quot;histo_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-				spinlock_stats.histo_spin_blocked, HISTO_BUCKETS + 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-fs_initcall(xen_spinlock_debugfs);</span>
<span class="p_del">-</span>
<span class="p_del">-#endif	/* CONFIG_XEN_DEBUG_FS */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



