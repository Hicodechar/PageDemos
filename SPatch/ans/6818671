
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[05/15] HMM: introduce heterogeneous memory management v4. - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [05/15] HMM: introduce heterogeneous memory management v4.</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 17, 2015, 6:52 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1437159145-6548-6-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6818671/mbox/"
   >mbox</a>
|
   <a href="/patch/6818671/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6818671/">/patch/6818671/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 163119F380
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Jul 2015 18:54:20 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 3FAF6207D4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Jul 2015 18:54:18 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 1E937207DC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 17 Jul 2015 18:54:16 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753912AbbGQSxh (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 17 Jul 2015 14:53:37 -0400
Received: from mx1.redhat.com ([209.132.183.28]:35527 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1753818AbbGQSxd (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 17 Jul 2015 14:53:33 -0400
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	by mx1.redhat.com (Postfix) with ESMTPS id D0357BE296;
	Fri, 17 Jul 2015 18:53:32 +0000 (UTC)
Received: from localhost.localdomain.com (vpn-56-84.rdu2.redhat.com
	[10.10.56.84])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id t6HIqscG031583; Fri, 17 Jul 2015 14:53:28 -0400
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;, &lt;joro@8bytes.org&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;,
	Johannes Weiner &lt;jweiner@redhat.com&gt;,
	Larry Woodman &lt;lwoodman@redhat.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Airlie &lt;airlied@redhat.com&gt;, Brendan Conoboy &lt;blc@redhat.com&gt;,
	Joe Donohue &lt;jdonohue@redhat.com&gt;, Christophe Harle &lt;charle@nvidia.com&gt;,
	Duncan Poole &lt;dpoole@nvidia.com&gt;, Sherry Cheung &lt;SCheung@nvidia.com&gt;,
	Subhash Gutti &lt;sgutti@nvidia.com&gt;, John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;,
	Lucien Dunning &lt;ldunning@nvidia.com&gt;,
	Cameron Buschardt &lt;cabuschardt@nvidia.com&gt;,
	Arvind Gopalakrishnan &lt;arvindg@nvidia.com&gt;,
	Haggai Eran &lt;haggaie@mellanox.com&gt;,
	Shachar Raindel &lt;raindel@mellanox.com&gt;, Liran Liss &lt;liranl@mellanox.com&gt;,
	Roland Dreier &lt;roland@purestorage.com&gt;, Ben Sander &lt;ben.sander@amd.com&gt;,
	Greg Stoner &lt;Greg.Stoner@amd.com&gt;, John Bridgman &lt;John.Bridgman@amd.com&gt;,
	Michael Mantor &lt;Michael.Mantor@amd.com&gt;,
	Paul Blinzer &lt;Paul.Blinzer@amd.com&gt;,
	Leonid Shamis &lt;Leonid.Shamis@amd.com&gt;,
	Laurent Morichetti &lt;Laurent.Morichetti@amd.com&gt;,
	Alexander Deucher &lt;Alexander.Deucher@amd.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Jatin Kumar &lt;jakumar@nvidia.com&gt;
Subject: [PATCH 05/15] HMM: introduce heterogeneous memory management v4.
Date: Fri, 17 Jul 2015 14:52:15 -0400
Message-Id: &lt;1437159145-6548-6-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1437159145-6548-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1437159145-6548-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.1 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - July 17, 2015, 6:52 p.m.</div>
<pre class="content">
This patch only introduce core HMM functions for registering a new
mirror and stopping a mirror as well as HMM device registering and
unregistering.

The lifecycle of HMM object is handled differently then the one of
mmu_notifier because unlike mmu_notifier there can be concurrent
call from both mm code to HMM code and/or from device driver code
to HMM code. Moreover lifetime of HMM can be uncorrelated from the
lifetime of the process that is being mirror (GPU might take longer
time to cleanup).

Changed since v1:
  - Updated comment of hmm_device_register().

Changed since v2:
  - Expose struct hmm for easy access to mm struct.
  - Simplify hmm_mirror_register() arguments.
  - Removed the device name.
  - Refcount the mirror struct internaly to HMM allowing to get
    rid of the srcu and making the device driver callback error
    handling simpler.
  - Safe to call several time hmm_mirror_unregister().
  - Rework the mmu_notifier unregistration and release callback.

Changed since v3:
  - Rework hmm_mirror lifetime rules.
  - Synchronize with mmu_notifier srcu before droping mirror last
    reference in hmm_mirror_unregister()
  - Use spinlock for device&#39;s mirror list.
  - Export mirror ref/unref functions.
  - English syntax fixes.
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Sherry Cheung &lt;SCheung@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Subhash Gutti &lt;sgutti@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Mark Hairgrove &lt;mhairgrove@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: John Hubbard &lt;jhubbard@nvidia.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Jatin Kumar &lt;jakumar@nvidia.com&gt;</span>
---
 MAINTAINERS              |   7 +
 include/linux/hmm.h      | 173 +++++++++++++++++++++
 include/linux/mm.h       |  11 ++
 include/linux/mm_types.h |  14 ++
 kernel/fork.c            |   2 +
 mm/Kconfig               |  14 ++
 mm/Makefile              |   1 +
 mm/hmm.c                 | 381 +++++++++++++++++++++++++++++++++++++++++++++++
 8 files changed, 603 insertions(+)
 create mode 100644 include/linux/hmm.h
 create mode 100644 mm/hmm.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=11822">Jerome Glisse</a> - Aug. 3, 2015, 11:56 a.m.</div>
<pre class="content">
On Mon, Aug 03, 2015 at 01:20:13PM +0530, Girish KS wrote:
<span class="quote">&gt; On 18-Jul-2015 12:47 am, &quot;Jérôme Glisse&quot; &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt; &gt;</span>

[...]
<span class="quote">
&gt; &gt; +int hmm_mirror_register(struct hmm_mirror *mirror)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +       struct mm_struct *mm = current-&gt;mm;</span>
<span class="quote">&gt; &gt; +       struct hmm *hmm = NULL;</span>
<span class="quote">&gt; &gt; +       int ret = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       /* Sanity checks. */</span>
<span class="quote">&gt; &gt; +       BUG_ON(!mirror);</span>
<span class="quote">&gt; &gt; +       BUG_ON(!mirror-&gt;device);</span>
<span class="quote">&gt; &gt; +       BUG_ON(!mm);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       /*</span>
<span class="quote">&gt; &gt; +        * Initialize the mirror struct fields, the mlist init and del</span>
<span class="quote">&gt; dance is</span>
<span class="quote">&gt; &gt; +        * necessary to make the error path easier for driver and for hmm.</span>
<span class="quote">&gt; &gt; +        */</span>
<span class="quote">&gt; &gt; +       kref_init(&amp;mirror-&gt;kref);</span>
<span class="quote">&gt; &gt; +       INIT_HLIST_NODE(&amp;mirror-&gt;mlist);</span>
<span class="quote">&gt; &gt; +       INIT_LIST_HEAD(&amp;mirror-&gt;dlist);</span>
<span class="quote">&gt; &gt; +       spin_lock(&amp;mirror-&gt;device-&gt;lock);</span>
<span class="quote">&gt; &gt; +       list_add(&amp;mirror-&gt;dlist, &amp;mirror-&gt;device-&gt;mirrors);</span>
<span class="quote">&gt; &gt; +       spin_unlock(&amp;mirror-&gt;device-&gt;lock);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +       hmm = mm-&gt;hmm ? hmm_ref(hmm) : NULL;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Instead of hmm mm-&gt;hmm would be the right param to be passed.  Here even</span>
<span class="quote">&gt; though mm-&gt;hmm is true hmm_ref returns NULL. Because hmm is not updated</span>
<span class="quote">&gt; after initialization in the beginning.</span>

ENOPARSE ? While this can be simplified to hmm = hmm_ref(mm-&gt;hmm); I do not
see what you mean. The mm struct might already have a valid hmm field set,
and that valid hmm struct might also already be in the process of being
destroy. So hmm_ref() might either return the same hmm pointer if the hmm
object is not about to be release or NULL. But at this point there is no
certainty on the return value of hmm_ref().

Note that because we have the mmap sem in write mode we know it is safe
to dereference mm-&gt;hmm and even to overwrite that field it if it is being
destroy concurently.

Cheers,
Jérôme
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/MAINTAINERS b/MAINTAINERS</span>
<span class="p_header">index 2d3d55c..8ebdc17 100644</span>
<span class="p_header">--- a/MAINTAINERS</span>
<span class="p_header">+++ b/MAINTAINERS</span>
<span class="p_chunk">@@ -4870,6 +4870,13 @@</span> <span class="p_context"> F:	include/uapi/linux/if_hippi.h</span>
 F:	net/802/hippi.c
 F:	drivers/net/hippi/
 
<span class="p_add">+HMM - Heterogeneous Memory Management</span>
<span class="p_add">+M:	Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+L:	linux-mm@kvack.org</span>
<span class="p_add">+S:	Maintained</span>
<span class="p_add">+F:	mm/hmm.c</span>
<span class="p_add">+F:	include/linux/hmm.h</span>
<span class="p_add">+</span>
 HOST AP DRIVER
 M:	Jouni Malinen &lt;j@w1.fi&gt;
 L:	hostap@shmoo.com (subscribers-only)
<span class="p_header">diff --git a/include/linux/hmm.h b/include/linux/hmm.h</span>
new file mode 100644
<span class="p_header">index 0000000..b559c0b</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/hmm.h</span>
<span class="p_chunk">@@ -0,0 +1,173 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright 2013 Red Hat Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+/* This is a heterogeneous memory management (hmm). In a nutshell this provide</span>
<span class="p_add">+ * an API to mirror a process address on a device which has its own mmu using</span>
<span class="p_add">+ * its own page table for the process. It supports everything except special</span>
<span class="p_add">+ * vma.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Mandatory hardware features :</span>
<span class="p_add">+ *   - An mmu with pagetable.</span>
<span class="p_add">+ *   - Read only flag per cpu page.</span>
<span class="p_add">+ *   - Page fault ie hardware must stop and wait for kernel to service fault.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Optional hardware features :</span>
<span class="p_add">+ *   - Dirty bit per cpu page.</span>
<span class="p_add">+ *   - Access bit per cpu page.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The hmm code handle all the interfacing with the core kernel mm code and</span>
<span class="p_add">+ * provide a simple API. It does support migrating system memory to device</span>
<span class="p_add">+ * memory and handle migration back to system memory on cpu page fault.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Migrated memory is considered as swaped from cpu and core mm code point of</span>
<span class="p_add">+ * view.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifndef _HMM_H</span>
<span class="p_add">+#define _HMM_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_HMM</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/list.h&gt;</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;linux/atomic.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm_types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mmu_notifier.h&gt;</span>
<span class="p_add">+#include &lt;linux/workqueue.h&gt;</span>
<span class="p_add">+#include &lt;linux/mman.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+struct hmm_device;</span>
<span class="p_add">+struct hmm_mirror;</span>
<span class="p_add">+struct hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_device - Each device must register one and only one hmm_device.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The hmm_device is the link btw HMM and each device driver.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* struct hmm_device_operations - HMM device operation callback</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm_device_ops {</span>
<span class="p_add">+	/* release() - mirror must stop using the address space.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * @mirror: The mirror that link process address space with the device.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * When this is called, device driver must kill all device thread using</span>
<span class="p_add">+	 * this mirror. It is call either from :</span>
<span class="p_add">+	 *   - mm dying (all process using this mm exiting).</span>
<span class="p_add">+	 *   - hmm_mirror_unregister() (if no other thread holds a reference)</span>
<span class="p_add">+	 *   - outcome of some device error reported by any of the device</span>
<span class="p_add">+	 *     callback against that mirror.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	void (*release)(struct hmm_mirror *mirror);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* free() - mirror can be freed.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * @mirror: The mirror that link process address space with the device.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * When this is called, device driver can free the underlying memory</span>
<span class="p_add">+	 * associated with that mirror. Note this is call from atomic context</span>
<span class="p_add">+	 * so device driver callback can not sleep.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	void (*free)(struct hmm_mirror *mirror);</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* struct hmm - per mm_struct HMM states.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mm: The mm struct this hmm is associated with.</span>
<span class="p_add">+ * @mirrors: List of all mirror for this mm (one per device).</span>
<span class="p_add">+ * @vm_end: Last valid address for this mm (exclusive).</span>
<span class="p_add">+ * @kref: Reference counter.</span>
<span class="p_add">+ * @rwsem: Serialize the mirror list modifications.</span>
<span class="p_add">+ * @mmu_notifier: The mmu_notifier of this mm.</span>
<span class="p_add">+ * @rcu: For delayed cleanup call from mmu_notifier.release() callback.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For each process address space (mm_struct) there is one and only one hmm</span>
<span class="p_add">+ * struct. hmm functions will redispatch to each devices the change made to</span>
<span class="p_add">+ * the process address space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Device driver must not access this structure other than for getting the</span>
<span class="p_add">+ * mm pointer.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm {</span>
<span class="p_add">+	struct mm_struct	*mm;</span>
<span class="p_add">+	struct hlist_head	mirrors;</span>
<span class="p_add">+	unsigned long		vm_end;</span>
<span class="p_add">+	struct kref		kref;</span>
<span class="p_add">+	struct rw_semaphore	rwsem;</span>
<span class="p_add">+	struct mmu_notifier	mmu_notifier;</span>
<span class="p_add">+	struct rcu_head		rcu;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* struct hmm_device - per device HMM structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @dev: Linux device structure pointer.</span>
<span class="p_add">+ * @ops: The hmm operations callback.</span>
<span class="p_add">+ * @mirrors: List of all active mirrors for the device.</span>
<span class="p_add">+ * @lock: Lock protecting mirrors list.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each device that want to mirror an address space must register one of this</span>
<span class="p_add">+ * struct (only once per linux device).</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm_device {</span>
<span class="p_add">+	struct device			*dev;</span>
<span class="p_add">+	const struct hmm_device_ops	*ops;</span>
<span class="p_add">+	struct list_head		mirrors;</span>
<span class="p_add">+	spinlock_t			lock;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+int hmm_device_register(struct hmm_device *device);</span>
<span class="p_add">+int hmm_device_unregister(struct hmm_device *device);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_mirror - device specific mirroring functions.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each device that mirror a process has a uniq hmm_mirror struct associating</span>
<span class="p_add">+ * the process address space with the device. Same process can be mirrored by</span>
<span class="p_add">+ * several different devices at the same time.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* struct hmm_mirror - per device and per mm HMM structure</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @device: The hmm_device struct this hmm_mirror is associated to.</span>
<span class="p_add">+ * @hmm: The hmm struct this hmm_mirror is associated to.</span>
<span class="p_add">+ * @kref: Reference counter (private to HMM do not use).</span>
<span class="p_add">+ * @dlist: List of all hmm_mirror for same device.</span>
<span class="p_add">+ * @mlist: List of all hmm_mirror for same process.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each device that want to mirror an address space must register one of this</span>
<span class="p_add">+ * struct for each of the address space it wants to mirror. Same device can</span>
<span class="p_add">+ * mirror several different address space. As well same address space can be</span>
<span class="p_add">+ * mirror by different devices.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm_mirror {</span>
<span class="p_add">+	struct hmm_device	*device;</span>
<span class="p_add">+	struct hmm		*hmm;</span>
<span class="p_add">+	struct kref		kref;</span>
<span class="p_add">+	struct list_head	dlist;</span>
<span class="p_add">+	struct hlist_node	mlist;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+int hmm_mirror_register(struct hmm_mirror *mirror);</span>
<span class="p_add">+void hmm_mirror_unregister(struct hmm_mirror *mirror);</span>
<span class="p_add">+struct hmm_mirror *hmm_mirror_ref(struct hmm_mirror *mirror);</span>
<span class="p_add">+void hmm_mirror_unref(struct hmm_mirror **mirror);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_HMM */</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 2e872f9..b5bf210 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -2243,5 +2243,16 @@</span> <span class="p_context"> void __init setup_nr_node_ids(void);</span>
 static inline void setup_nr_node_ids(void) {}
 #endif
 
<span class="p_add">+#ifdef CONFIG_HMM</span>
<span class="p_add">+static inline void hmm_mm_init(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	mm-&gt;hmm = NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else /* !CONFIG_HMM */</span>
<span class="p_add">+static inline void hmm_mm_init(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* !CONFIG_HMM */</span>
<span class="p_add">+</span>
 #endif /* __KERNEL__ */
 #endif /* _LINUX_MM_H */
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index 0038ac7..fa05917 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -15,6 +15,10 @@</span> <span class="p_context"></span>
 #include &lt;asm/page.h&gt;
 #include &lt;asm/mmu.h&gt;
 
<span class="p_add">+#ifdef CONFIG_HMM</span>
<span class="p_add">+struct hmm;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifndef AT_VECTOR_SIZE_ARCH
 #define AT_VECTOR_SIZE_ARCH 0
 #endif
<span class="p_chunk">@@ -451,6 +455,16 @@</span> <span class="p_context"> struct mm_struct {</span>
 #ifdef CONFIG_MMU_NOTIFIER
 	struct mmu_notifier_mm *mmu_notifier_mm;
 #endif
<span class="p_add">+#ifdef CONFIG_HMM</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * hmm always register an mmu_notifier we rely on mmu notifier to keep</span>
<span class="p_add">+	 * refcount on mm struct as well as forbiding registering hmm on a</span>
<span class="p_add">+	 * dying mm</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * This field is set with mmap_sem held in write mode.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+#endif</span>
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS
 	pgtable_t pmd_huge_pte; /* protected by page_table_lock */
 #endif
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index 1bfefc6..0d1f446 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -27,6 +27,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/binfmts.h&gt;
 #include &lt;linux/mman.h&gt;
 #include &lt;linux/mmu_notifier.h&gt;
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
 #include &lt;linux/fs.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/vmacache.h&gt;
<span class="p_chunk">@@ -597,6 +598,7 @@</span> <span class="p_context"> static struct mm_struct *mm_init(struct mm_struct *mm, struct task_struct *p)</span>
 	mm_init_aio(mm);
 	mm_init_owner(mm, p);
 	mmu_notifier_mm_init(mm);
<span class="p_add">+	hmm_mm_init(mm);</span>
 	clear_tlb_flush_pending(mm);
 #if defined(CONFIG_TRANSPARENT_HUGEPAGE) &amp;&amp; !USE_SPLIT_PMD_PTLOCKS
 	mm-&gt;pmd_huge_pte = NULL;
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index e79de2b..e1e0a82 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -654,3 +654,17 @@</span> <span class="p_context"> config DEFERRED_STRUCT_PAGE_INIT</span>
 	  when kswapd starts. This has a potential performance impact on
 	  processes running early in the lifetime of the systemm until kswapd
 	  finishes the initialisation.
<span class="p_add">+</span>
<span class="p_add">+if STAGING</span>
<span class="p_add">+config HMM</span>
<span class="p_add">+	bool &quot;Enable heterogeneous memory management (HMM)&quot;</span>
<span class="p_add">+	depends on MMU</span>
<span class="p_add">+	select MMU_NOTIFIER</span>
<span class="p_add">+	default n</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Heterogeneous memory management provide infrastructure for a device</span>
<span class="p_add">+	  to mirror a process address space into an hardware mmu or into any</span>
<span class="p_add">+	  things supporting pagefault like event.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If unsure, say N to disable hmm.</span>
<span class="p_add">+endif # STAGING</span>
<span class="p_header">diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="p_header">index 98c4eae..90ca9c4 100644</span>
<span class="p_header">--- a/mm/Makefile</span>
<span class="p_header">+++ b/mm/Makefile</span>
<span class="p_chunk">@@ -78,3 +78,4 @@</span> <span class="p_context"> obj-$(CONFIG_CMA)	+= cma.o</span>
 obj-$(CONFIG_MEMORY_BALLOON) += balloon_compaction.o
 obj-$(CONFIG_PAGE_EXTENSION) += page_ext.o
 obj-$(CONFIG_CMA_DEBUGFS) += cma_debug.o
<span class="p_add">+obj-$(CONFIG_HMM) += hmm.o</span>
<span class="p_header">diff --git a/mm/hmm.c b/mm/hmm.c</span>
new file mode 100644
<span class="p_header">index 0000000..198fe37</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/hmm.c</span>
<span class="p_chunk">@@ -0,0 +1,381 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright 2013 Red Hat Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License as published by</span>
<span class="p_add">+ * the Free Software Foundation; either version 2 of the License, or</span>
<span class="p_add">+ * (at your option) any later version.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+/* This is the core code for heterogeneous memory management (HMM). HMM intend</span>
<span class="p_add">+ * to provide helper for mirroring a process address space on a device as well</span>
<span class="p_add">+ * as allowing migration of data between system memory and device memory refer</span>
<span class="p_add">+ * as remote memory from here on out.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Refer to include/linux/hmm.h for further information on general design.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/export.h&gt;</span>
<span class="p_add">+#include &lt;linux/bitmap.h&gt;</span>
<span class="p_add">+#include &lt;linux/list.h&gt;</span>
<span class="p_add">+#include &lt;linux/rculist.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/mmu_notifier.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/hugetlb.h&gt;</span>
<span class="p_add">+#include &lt;linux/fs.h&gt;</span>
<span class="p_add">+#include &lt;linux/file.h&gt;</span>
<span class="p_add">+#include &lt;linux/ksm.h&gt;</span>
<span class="p_add">+#include &lt;linux/rmap.h&gt;</span>
<span class="p_add">+#include &lt;linux/swap.h&gt;</span>
<span class="p_add">+#include &lt;linux/swapops.h&gt;</span>
<span class="p_add">+#include &lt;linux/mmu_context.h&gt;</span>
<span class="p_add">+#include &lt;linux/memcontrol.h&gt;</span>
<span class="p_add">+#include &lt;linux/hmm.h&gt;</span>
<span class="p_add">+#include &lt;linux/wait.h&gt;</span>
<span class="p_add">+#include &lt;linux/mman.h&gt;</span>
<span class="p_add">+#include &lt;linux/delay.h&gt;</span>
<span class="p_add">+#include &lt;linux/workqueue.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;internal.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+static struct mmu_notifier_ops hmm_notifier_ops;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm - core HMM functions.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Core HMM functions that deal with all the process mm activities.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+static int hmm_init(struct hmm *hmm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	hmm-&gt;mm = current-&gt;mm;</span>
<span class="p_add">+	hmm-&gt;vm_end = TASK_SIZE;</span>
<span class="p_add">+	kref_init(&amp;hmm-&gt;kref);</span>
<span class="p_add">+	INIT_HLIST_HEAD(&amp;hmm-&gt;mirrors);</span>
<span class="p_add">+	init_rwsem(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* register notifier */</span>
<span class="p_add">+	hmm-&gt;mmu_notifier.ops = &amp;hmm_notifier_ops;</span>
<span class="p_add">+	return __mmu_notifier_register(&amp;hmm-&gt;mmu_notifier, current-&gt;mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int hmm_add_mirror(struct hmm *hmm, struct hmm_mirror *mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_mirror *tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	down_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+	hlist_for_each_entry(tmp, &amp;hmm-&gt;mirrors, mlist)</span>
<span class="p_add">+		if (tmp-&gt;device == mirror-&gt;device) {</span>
<span class="p_add">+			/* Same device can mirror only once. */</span>
<span class="p_add">+			up_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	hlist_add_head(&amp;mirror-&gt;mlist, &amp;hmm-&gt;mirrors);</span>
<span class="p_add">+	hmm_mirror_ref(mirror);</span>
<span class="p_add">+	up_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct hmm *hmm_ref(struct hmm *hmm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!hmm || !kref_get_unless_zero(&amp;hmm-&gt;kref))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	return hmm;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_destroy_delayed(struct rcu_head *rcu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm = container_of(rcu, struct hmm, rcu);</span>
<span class="p_add">+	kfree(hmm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_destroy(struct kref *kref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm = container_of(kref, struct hmm, kref);</span>
<span class="p_add">+	BUG_ON(!hlist_empty(&amp;hmm-&gt;mirrors));</span>
<span class="p_add">+</span>
<span class="p_add">+	down_write(&amp;hmm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	/* A new hmm might have been register before reaching that point. */</span>
<span class="p_add">+	if (hmm-&gt;mm-&gt;hmm == hmm)</span>
<span class="p_add">+		hmm-&gt;mm-&gt;hmm = NULL;</span>
<span class="p_add">+	up_write(&amp;hmm-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_unregister_no_release(&amp;hmm-&gt;mmu_notifier, hmm-&gt;mm);</span>
<span class="p_add">+</span>
<span class="p_add">+	mmu_notifier_call_srcu(&amp;hmm-&gt;rcu, &amp;hmm_destroy_delayed);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct hmm *hmm_unref(struct hmm *hmm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (hmm)</span>
<span class="p_add">+		kref_put(&amp;hmm-&gt;kref, hmm_destroy);</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_notifier - HMM callback for mmu_notifier tracking change to process mm.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * HMM use use mmu notifier to track change made to process address space.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void hmm_notifier_release(struct mmu_notifier *mn, struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm *hmm;</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm = hmm_ref(container_of(mn, struct hmm, mmu_notifier));</span>
<span class="p_add">+	if (!hmm)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	down_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+	while (hmm-&gt;mirrors.first) {</span>
<span class="p_add">+		struct hmm_mirror *mirror;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Here we are holding the mirror reference from the mirror</span>
<span class="p_add">+		 * list. As list removal is synchronized through rwsem, no</span>
<span class="p_add">+		 * other thread can assume it holds that reference.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		mirror = hlist_entry(hmm-&gt;mirrors.first,</span>
<span class="p_add">+				     struct hmm_mirror,</span>
<span class="p_add">+				     mlist);</span>
<span class="p_add">+		hlist_del_init(&amp;mirror-&gt;mlist);</span>
<span class="p_add">+		up_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+		mirror-&gt;device-&gt;ops-&gt;release(mirror);</span>
<span class="p_add">+		hmm_mirror_unref(&amp;mirror);</span>
<span class="p_add">+</span>
<span class="p_add">+		down_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	up_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm_unref(hmm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct mmu_notifier_ops hmm_notifier_ops = {</span>
<span class="p_add">+	.release		= hmm_notifier_release,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_mirror - per device mirroring functions.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Each device that mirror a process has a uniq hmm_mirror struct. A process</span>
<span class="p_add">+ * can be mirror by several devices at the same time.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Below are all the functions and their helpers use by device driver to mirror</span>
<span class="p_add">+ * the process address space. Those functions either deals with updating the</span>
<span class="p_add">+ * device page table (through hmm callback). Or provide helper functions use by</span>
<span class="p_add">+ * the device driver to fault in range of memory in the device page table.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct hmm_mirror *hmm_mirror_ref(struct hmm_mirror *mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!mirror || !kref_get_unless_zero(&amp;mirror-&gt;kref))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	return mirror;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_mirror_ref);</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_mirror_destroy(struct kref *kref)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_device *device;</span>
<span class="p_add">+	struct hmm_mirror *mirror;</span>
<span class="p_add">+</span>
<span class="p_add">+	mirror = container_of(kref, struct hmm_mirror, kref);</span>
<span class="p_add">+	device = mirror-&gt;device;</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm_unref(mirror-&gt;hmm);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock(&amp;device-&gt;lock);</span>
<span class="p_add">+	list_del_init(&amp;mirror-&gt;dlist);</span>
<span class="p_add">+	device-&gt;ops-&gt;free(mirror);</span>
<span class="p_add">+	spin_unlock(&amp;device-&gt;lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void hmm_mirror_unref(struct hmm_mirror **mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_mirror *tmp = mirror ? *mirror : NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tmp) {</span>
<span class="p_add">+		*mirror = NULL;</span>
<span class="p_add">+		kref_put(&amp;tmp-&gt;kref, hmm_mirror_destroy);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_mirror_unref);</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_mirror_register() - register mirror against current process for a device.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mirror: The mirror struct being registered.</span>
<span class="p_add">+ * Returns: 0 on success or -ENOMEM, -EINVAL on error.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Call when device driver want to start mirroring a process address space. The</span>
<span class="p_add">+ * HMM shim will register mmu_notifier and start monitoring process address</span>
<span class="p_add">+ * space changes. Hence callback to device driver might happen even before this</span>
<span class="p_add">+ * function return.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The task device driver want to mirror must be current !</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Only one mirror per mm and hmm_device can be created, it will return NULL if</span>
<span class="p_add">+ * the hmm_device already has an hmm_mirror for the the mm.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int hmm_mirror_register(struct hmm_mirror *mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = current-&gt;mm;</span>
<span class="p_add">+	struct hmm *hmm = NULL;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Sanity checks. */</span>
<span class="p_add">+	BUG_ON(!mirror);</span>
<span class="p_add">+	BUG_ON(!mirror-&gt;device);</span>
<span class="p_add">+	BUG_ON(!mm);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Initialize the mirror struct fields, the mlist init and del dance is</span>
<span class="p_add">+	 * necessary to make the error path easier for driver and for hmm.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kref_init(&amp;mirror-&gt;kref);</span>
<span class="p_add">+	INIT_HLIST_NODE(&amp;mirror-&gt;mlist);</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;mirror-&gt;dlist);</span>
<span class="p_add">+	spin_lock(&amp;mirror-&gt;device-&gt;lock);</span>
<span class="p_add">+	list_add(&amp;mirror-&gt;dlist, &amp;mirror-&gt;device-&gt;mirrors);</span>
<span class="p_add">+	spin_unlock(&amp;mirror-&gt;device-&gt;lock);</span>
<span class="p_add">+</span>
<span class="p_add">+	down_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm = mm-&gt;hmm ? hmm_ref(hmm) : NULL;</span>
<span class="p_add">+	if (hmm == NULL) {</span>
<span class="p_add">+		/* no hmm registered yet so register one */</span>
<span class="p_add">+		hmm = kzalloc(sizeof(*mm-&gt;hmm), GFP_KERNEL);</span>
<span class="p_add">+		if (hmm == NULL) {</span>
<span class="p_add">+			up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			goto error;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = hmm_init(hmm);</span>
<span class="p_add">+		if (ret) {</span>
<span class="p_add">+			up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+			kfree(hmm);</span>
<span class="p_add">+			goto error;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		mm-&gt;hmm = hmm;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	mirror-&gt;hmm = hmm;</span>
<span class="p_add">+	ret = hmm_add_mirror(hmm, mirror);</span>
<span class="p_add">+	up_write(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		mirror-&gt;hmm = NULL;</span>
<span class="p_add">+		hmm_unref(hmm);</span>
<span class="p_add">+		goto error;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+error:</span>
<span class="p_add">+	spin_lock(&amp;mirror-&gt;device-&gt;lock);</span>
<span class="p_add">+	list_del_init(&amp;mirror-&gt;dlist);</span>
<span class="p_add">+	spin_unlock(&amp;mirror-&gt;device-&gt;lock);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_mirror_register);</span>
<span class="p_add">+</span>
<span class="p_add">+static void hmm_mirror_kill(struct hmm_mirror *mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct hmm_device *device = mirror-&gt;device;</span>
<span class="p_add">+	struct hmm *hmm = hmm_ref(mirror-&gt;hmm);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!hmm)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	down_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+	if (!hlist_unhashed(&amp;mirror-&gt;mlist)) {</span>
<span class="p_add">+		hlist_del_init(&amp;mirror-&gt;mlist);</span>
<span class="p_add">+		up_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+		device-&gt;ops-&gt;release(mirror);</span>
<span class="p_add">+		hmm_mirror_unref(&amp;mirror);</span>
<span class="p_add">+	} else</span>
<span class="p_add">+		up_write(&amp;hmm-&gt;rwsem);</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm_unref(hmm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_mirror_unregister() - unregister a mirror.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @mirror: The mirror that link process address space with the device.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Driver can call this function when it wants to stop mirroring a process.</span>
<span class="p_add">+ * This will trigger a call to the -&gt;release() callback if it did not aleady</span>
<span class="p_add">+ * happen.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note that caller must hold a reference on the mirror.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * THIS CAN NOT BE CALL FROM device-&gt;release() CALLBACK OR IT WILL DEADLOCK.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void hmm_mirror_unregister(struct hmm_mirror *mirror)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (mirror == NULL)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	hmm_mirror_kill(mirror);</span>
<span class="p_add">+	mmu_notifier_synchronize();</span>
<span class="p_add">+	hmm_mirror_unref(&amp;mirror);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_mirror_unregister);</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_device - Each device driver must register one and only one hmm_device</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The hmm_device is the link btw HMM and each device driver.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_device_register() - register a device with HMM.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @device: The hmm_device struct.</span>
<span class="p_add">+ * Returns: 0 on success or -EINVAL otherwise.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Call when device driver want to register itself with HMM. Device driver must</span>
<span class="p_add">+ * only register once.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int hmm_device_register(struct hmm_device *device)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* sanity check */</span>
<span class="p_add">+	BUG_ON(!device);</span>
<span class="p_add">+	BUG_ON(!device-&gt;ops);</span>
<span class="p_add">+	BUG_ON(!device-&gt;ops-&gt;release);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_init(&amp;device-&gt;lock);</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;device-&gt;mirrors);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_device_register);</span>
<span class="p_add">+</span>
<span class="p_add">+/* hmm_device_unregister() - unregister a device with HMM.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * @device: The hmm_device struct.</span>
<span class="p_add">+ * Returns: 0 on success or -EBUSY otherwise.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Call when device driver want to unregister itself with HMM. This will check</span>
<span class="p_add">+ * that there is no any active mirror and returns -EBUSY if so.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int hmm_device_unregister(struct hmm_device *device)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spin_lock(&amp;device-&gt;lock);</span>
<span class="p_add">+	if (!list_empty(&amp;device-&gt;mirrors)) {</span>
<span class="p_add">+		spin_unlock(&amp;device-&gt;lock);</span>
<span class="p_add">+		return -EBUSY;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	spin_unlock(&amp;device-&gt;lock);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(hmm_device_unregister);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



