
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,17/17] mm: add knob to tune lazyfreeing - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,17/17] mm: add knob to tune lazyfreeing</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 12, 2015, 4:33 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1447302793-5376-18-git-send-email-minchan@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7599241/mbox/"
   >mbox</a>
|
   <a href="/patch/7599241/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7599241/">/patch/7599241/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 1C70F9F1D3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Nov 2015 04:34:31 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 62413207D9
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Nov 2015 04:34:29 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 68500207DB
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Nov 2015 04:34:27 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753434AbbKLEdr (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 11 Nov 2015 23:33:47 -0500
Received: from LGEAMRELO12.lge.com ([156.147.23.52]:50348 &quot;EHLO
	lgeamrelo12.lge.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753346AbbKLEcr (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 11 Nov 2015 23:32:47 -0500
Received: from unknown (HELO lgeamrelo02.lge.com) (156.147.1.126)
	by 156.147.23.52 with ESMTP; 12 Nov 2015 13:32:45 +0900
X-Original-SENDERIP: 156.147.1.126
X-Original-MAILFROM: minchan@kernel.org
Received: from unknown (HELO localhost.localdomain) (10.177.223.161)
	by 156.147.1.126 with ESMTP; 12 Nov 2015 13:32:45 +0900
X-Original-SENDERIP: 10.177.223.161
X-Original-MAILFROM: minchan@kernel.org
From: Minchan Kim &lt;minchan@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	Michael Kerrisk &lt;mtk.manpages@gmail.com&gt;,
	linux-api@vger.kernel.org, Hugh Dickins &lt;hughd@google.com&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	KOSAKI Motohiro &lt;kosaki.motohiro@jp.fujitsu.com&gt;,
	Jason Evans &lt;je@fb.com&gt;, Daniel Micay &lt;danielmicay@gmail.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	Shaohua Li &lt;shli@kernel.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	yalin.wang2010@gmail.com, Minchan Kim &lt;minchan@kernel.org&gt;
Subject: [PATCH v3 17/17] mm: add knob to tune lazyfreeing
Date: Thu, 12 Nov 2015 13:33:13 +0900
Message-Id: &lt;1447302793-5376-18-git-send-email-minchan@kernel.org&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1447302793-5376-1-git-send-email-minchan@kernel.org&gt;
References: &lt;1447302793-5376-1-git-send-email-minchan@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.2 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 12, 2015, 4:33 a.m.</div>
<pre class="content">
MADV_FREEed page&#39;s hotness is very arguble.
Someone think it&#39;s hot while others are it&#39;s cold.

Quote from Shaohua
&quot;
My main concern is the policy how we should treat the FREE pages. Moving it to
inactive lru is definitionly a good start, I&#39;m wondering if it&#39;s enough. The
MADV_FREE increases memory pressure and cause unnecessary reclaim because of
the lazy memory free. While MADV_FREE is intended to be a better replacement of
MADV_DONTNEED, MADV_DONTNEED doesn&#39;t have the memory pressure issue as it free
memory immediately. So I hope the MADV_FREE doesn&#39;t have impact on memory
pressure too. I&#39;m thinking of adding an extra lru list and wartermark for this
to make sure FREE pages can be freed before system wide page reclaim. As you
said, this is arguable, but I hope we can discuss about this issue more.
&quot;

Quote from me
&quot;
It seems the divergence comes from MADV_FREE is *replacement* of MADV_DONTNEED.
But I don&#39;t think so. If we could discard MADV_FREEed page *anytime*, I agree
but it&#39;s not true because the page would be dirty state when VM want to reclaim.

I&#39;m also against with your&#39;s suggestion which let&#39;s discard FREEed page before
system wide page reclaim because system would have lots of clean cold page
caches or anonymous pages. In such case, reclaiming of them would be better.
Yeb, it&#39;s really workload-dependent so we might need some heuristic which is
normally what we want to avoid.

Having said that, I agree with you we could do better than the deactivation
and frankly speaking, I&#39;m thinking of another LRU list(e.g. tentatively named
&quot;ezreclaim LRU list&quot;). What I have in mind is to age (anon|file|ez)
fairly. IOW, I want to percolate ez-LRU list reclaiming into get_scan_count.
When the MADV_FREE is called, we could move hinted pages from anon-LRU to
ez-LRU and then If VM find to not be able to discard a page in ez-LRU,
it could promote it to acive-anon-LRU which would be very natural aging
concept because it mean someone touches the page recenlty.
With that, I don&#39;t want to bias one side and don&#39;t want to add some knob for
tuning the heuristic but let&#39;s rely on common fair aging scheme of VM.
&quot;

Quote from Johannes
&quot;
thread 1:
Even if we&#39;re wrong about the aging of those MADV_FREE pages, their
contents are invalidated; they can be discarded freely, and restoring
them is a mere GFP_ZERO allocation. All other anonymous pages have to
be written to disk, and potentially be read back.

[ Arguably, MADV_FREE pages should even be reclaimed before inactive
  page cache. It&#39;s the same cost to discard both types of pages, but
  restoring page cache involves IO. ]

It probably makes sense to stop thinking about them as anonymous pages
entirely at this point when it comes to aging. They&#39;re really not. The
LRU lists are split to differentiate access patterns and cost of page
stealing (and restoring). From that angle, MADV_FREE pages really have
nothing in common with in-use anonymous pages, and so they shouldn&#39;t
be on the same LRU list.

thread:2
What about them is hot? They contain garbage, you have to write to
them before you can use them. Granted, you might have to refetch
cachelines if you don&#39;t do cacheline-aligned populating writes, but
you can do a lot of them before it&#39;s more expensive than doing IO.

&quot;

Quote from Daniel
&quot;
thread:1
Keep in mind that this is memory the kernel wouldn&#39;t be getting back at
all if the allocator wasn&#39;t going out of the way to purge it, and they
aren&#39;t going to go out of their way to purge it if it means the kernel
is going to steal the pages when there isn&#39;t actually memory pressure.

An allocator would be using MADV_DONTNEED if it didn&#39;t expect that the
pages were going to be used against shortly. MADV_FREE indicates that it
has time to inform the kernel that they&#39;re unused but they could still
be very hot.

thread:2
It&#39;s hot because applications churn through memory via the allocator.

Drop the pages and the application is now churning through page faults
and zeroing rather than simply reusing memory. It&#39;s not something that
may happen, it *will* happen. A page in the page cache *may* be reused,
but often won&#39;t be, especially when the I/O patterns don&#39;t line up well
with the way it works.

The whole point of the feature is not requiring the allocator to have
elaborate mechanisms for aging pages and throttling purging. That ends
up resulting in lots of memory held by userspace where the kernel can&#39;t
reclaim it under memory pressure. If it&#39;s dropped before page cache, it
isn&#39;t going to be able to replace any of that logic in allocators.

The page cache is speculative. Page caching by allocators is not really
speculative. Using MADV_FREE on the pages at all is speculative. The
memory is probably going to be reused fairly soon (unless the process
exits, and then it doesn&#39;t matter), but purging will end up reducing
memory usage for the portions that aren&#39;t.

It would be a different story for a full unpinning/pinning feature since
that would have other use cases (speculative caches), but this is really
only useful in allocators.
&quot;
You could read all thread from https://lkml.org/lkml/2015/11/4/51

Yeah, with arguble issue and there is no one decision, I think it
means we should provide the knob &quot;lazyfreeness&quot;(I hope someone
give better naming).

It&#39;s similar to swapppiness so higher values will discard MADV_FREE
pages agreessively. If memory pressure happens and system works with
DEF_PRIOIRTY(ex, clean cold caches), VM doesn&#39;t discard any hinted
pages until the scanning priority is increased.

If memory pressure is higher(ie, the priority is not DEF_PRIORITY),
it scans

	nr_to_reclaim * priority * lazyfreensss(def: 20) / 50

If system has low free memory and file cache, it start to discard
MADV_FREEed pages unconditionally even though user set lazyfreeness to 0.
<span class="signed-off-by">
Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
---
 Documentation/sysctl/vm.txt | 13 +++++++++
 drivers/base/node.c         |  4 +--
 fs/proc/meminfo.c           |  4 +--
 include/linux/memcontrol.h  |  1 +
 include/linux/mmzone.h      |  9 +++---
 include/linux/swap.h        | 15 ++++++++++
 kernel/sysctl.c             |  9 ++++++
 mm/memcontrol.c             | 32 +++++++++++++++++++++-
 mm/vmscan.c                 | 67 ++++++++++++++++++++++++++++-----------------
 mm/vmstat.c                 |  2 +-
 10 files changed, 121 insertions(+), 35 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=19951">Shaohua Li</a> - Nov. 12, 2015, 7:44 p.m.</div>
<pre class="content">
On Thu, Nov 12, 2015 at 01:33:13PM +0900, Minchan Kim wrote:
<span class="quote">&gt; MADV_FREEed page&#39;s hotness is very arguble.</span>
<span class="quote">&gt; Someone think it&#39;s hot while others are it&#39;s cold.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Quote from Shaohua</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; My main concern is the policy how we should treat the FREE pages. Moving it to</span>
<span class="quote">&gt; inactive lru is definitionly a good start, I&#39;m wondering if it&#39;s enough. The</span>
<span class="quote">&gt; MADV_FREE increases memory pressure and cause unnecessary reclaim because of</span>
<span class="quote">&gt; the lazy memory free. While MADV_FREE is intended to be a better replacement of</span>
<span class="quote">&gt; MADV_DONTNEED, MADV_DONTNEED doesn&#39;t have the memory pressure issue as it free</span>
<span class="quote">&gt; memory immediately. So I hope the MADV_FREE doesn&#39;t have impact on memory</span>
<span class="quote">&gt; pressure too. I&#39;m thinking of adding an extra lru list and wartermark for this</span>
<span class="quote">&gt; to make sure FREE pages can be freed before system wide page reclaim. As you</span>
<span class="quote">&gt; said, this is arguable, but I hope we can discuss about this issue more.</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Quote from me</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; It seems the divergence comes from MADV_FREE is *replacement* of MADV_DONTNEED.</span>
<span class="quote">&gt; But I don&#39;t think so. If we could discard MADV_FREEed page *anytime*, I agree</span>
<span class="quote">&gt; but it&#39;s not true because the page would be dirty state when VM want to reclaim.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;m also against with your&#39;s suggestion which let&#39;s discard FREEed page before</span>
<span class="quote">&gt; system wide page reclaim because system would have lots of clean cold page</span>
<span class="quote">&gt; caches or anonymous pages. In such case, reclaiming of them would be better.</span>
<span class="quote">&gt; Yeb, it&#39;s really workload-dependent so we might need some heuristic which is</span>
<span class="quote">&gt; normally what we want to avoid.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Having said that, I agree with you we could do better than the deactivation</span>
<span class="quote">&gt; and frankly speaking, I&#39;m thinking of another LRU list(e.g. tentatively named</span>
<span class="quote">&gt; &quot;ezreclaim LRU list&quot;). What I have in mind is to age (anon|file|ez)</span>
<span class="quote">&gt; fairly. IOW, I want to percolate ez-LRU list reclaiming into get_scan_count.</span>
<span class="quote">&gt; When the MADV_FREE is called, we could move hinted pages from anon-LRU to</span>
<span class="quote">&gt; ez-LRU and then If VM find to not be able to discard a page in ez-LRU,</span>
<span class="quote">&gt; it could promote it to acive-anon-LRU which would be very natural aging</span>
<span class="quote">&gt; concept because it mean someone touches the page recenlty.</span>
<span class="quote">&gt; With that, I don&#39;t want to bias one side and don&#39;t want to add some knob for</span>
<span class="quote">&gt; tuning the heuristic but let&#39;s rely on common fair aging scheme of VM.</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Quote from Johannes</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; thread 1:</span>
<span class="quote">&gt; Even if we&#39;re wrong about the aging of those MADV_FREE pages, their</span>
<span class="quote">&gt; contents are invalidated; they can be discarded freely, and restoring</span>
<span class="quote">&gt; them is a mere GFP_ZERO allocation. All other anonymous pages have to</span>
<span class="quote">&gt; be written to disk, and potentially be read back.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [ Arguably, MADV_FREE pages should even be reclaimed before inactive</span>
<span class="quote">&gt;   page cache. It&#39;s the same cost to discard both types of pages, but</span>
<span class="quote">&gt;   restoring page cache involves IO. ]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It probably makes sense to stop thinking about them as anonymous pages</span>
<span class="quote">&gt; entirely at this point when it comes to aging. They&#39;re really not. The</span>
<span class="quote">&gt; LRU lists are split to differentiate access patterns and cost of page</span>
<span class="quote">&gt; stealing (and restoring). From that angle, MADV_FREE pages really have</span>
<span class="quote">&gt; nothing in common with in-use anonymous pages, and so they shouldn&#39;t</span>
<span class="quote">&gt; be on the same LRU list.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; thread:2</span>
<span class="quote">&gt; What about them is hot? They contain garbage, you have to write to</span>
<span class="quote">&gt; them before you can use them. Granted, you might have to refetch</span>
<span class="quote">&gt; cachelines if you don&#39;t do cacheline-aligned populating writes, but</span>
<span class="quote">&gt; you can do a lot of them before it&#39;s more expensive than doing IO.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Quote from Daniel</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; thread:1</span>
<span class="quote">&gt; Keep in mind that this is memory the kernel wouldn&#39;t be getting back at</span>
<span class="quote">&gt; all if the allocator wasn&#39;t going out of the way to purge it, and they</span>
<span class="quote">&gt; aren&#39;t going to go out of their way to purge it if it means the kernel</span>
<span class="quote">&gt; is going to steal the pages when there isn&#39;t actually memory pressure.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; An allocator would be using MADV_DONTNEED if it didn&#39;t expect that the</span>
<span class="quote">&gt; pages were going to be used against shortly. MADV_FREE indicates that it</span>
<span class="quote">&gt; has time to inform the kernel that they&#39;re unused but they could still</span>
<span class="quote">&gt; be very hot.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; thread:2</span>
<span class="quote">&gt; It&#39;s hot because applications churn through memory via the allocator.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Drop the pages and the application is now churning through page faults</span>
<span class="quote">&gt; and zeroing rather than simply reusing memory. It&#39;s not something that</span>
<span class="quote">&gt; may happen, it *will* happen. A page in the page cache *may* be reused,</span>
<span class="quote">&gt; but often won&#39;t be, especially when the I/O patterns don&#39;t line up well</span>
<span class="quote">&gt; with the way it works.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The whole point of the feature is not requiring the allocator to have</span>
<span class="quote">&gt; elaborate mechanisms for aging pages and throttling purging. That ends</span>
<span class="quote">&gt; up resulting in lots of memory held by userspace where the kernel can&#39;t</span>
<span class="quote">&gt; reclaim it under memory pressure. If it&#39;s dropped before page cache, it</span>
<span class="quote">&gt; isn&#39;t going to be able to replace any of that logic in allocators.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The page cache is speculative. Page caching by allocators is not really</span>
<span class="quote">&gt; speculative. Using MADV_FREE on the pages at all is speculative. The</span>
<span class="quote">&gt; memory is probably going to be reused fairly soon (unless the process</span>
<span class="quote">&gt; exits, and then it doesn&#39;t matter), but purging will end up reducing</span>
<span class="quote">&gt; memory usage for the portions that aren&#39;t.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It would be a different story for a full unpinning/pinning feature since</span>
<span class="quote">&gt; that would have other use cases (speculative caches), but this is really</span>
<span class="quote">&gt; only useful in allocators.</span>
<span class="quote">&gt; &quot;</span>
<span class="quote">&gt; You could read all thread from https://lkml.org/lkml/2015/11/4/51</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yeah, with arguble issue and there is no one decision, I think it</span>
<span class="quote">&gt; means we should provide the knob &quot;lazyfreeness&quot;(I hope someone</span>
<span class="quote">&gt; give better naming).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s similar to swapppiness so higher values will discard MADV_FREE</span>
<span class="quote">&gt; pages agreessively. If memory pressure happens and system works with</span>
<span class="quote">&gt; DEF_PRIOIRTY(ex, clean cold caches), VM doesn&#39;t discard any hinted</span>
<span class="quote">&gt; pages until the scanning priority is increased.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If memory pressure is higher(ie, the priority is not DEF_PRIORITY),</span>
<span class="quote">&gt; it scans</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	nr_to_reclaim * priority * lazyfreensss(def: 20) / 50</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If system has low free memory and file cache, it start to discard</span>
<span class="quote">&gt; MADV_FREEed pages unconditionally even though user set lazyfreeness to 0.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  Documentation/sysctl/vm.txt | 13 +++++++++</span>
<span class="quote">&gt;  drivers/base/node.c         |  4 +--</span>
<span class="quote">&gt;  fs/proc/meminfo.c           |  4 +--</span>
<span class="quote">&gt;  include/linux/memcontrol.h  |  1 +</span>
<span class="quote">&gt;  include/linux/mmzone.h      |  9 +++---</span>
<span class="quote">&gt;  include/linux/swap.h        | 15 ++++++++++</span>
<span class="quote">&gt;  kernel/sysctl.c             |  9 ++++++</span>
<span class="quote">&gt;  mm/memcontrol.c             | 32 +++++++++++++++++++++-</span>
<span class="quote">&gt;  mm/vmscan.c                 | 67 ++++++++++++++++++++++++++++-----------------</span>
<span class="quote">&gt;  mm/vmstat.c                 |  2 +-</span>
<span class="quote">&gt;  10 files changed, 121 insertions(+), 35 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/Documentation/sysctl/vm.txt b/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; index a4482fceacec..c1dc63381f2c 100644</span>
<span class="quote">&gt; --- a/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; +++ b/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; @@ -56,6 +56,7 @@ files can be found in mm/swap.c.</span>
<span class="quote">&gt;  - percpu_pagelist_fraction</span>
<span class="quote">&gt;  - stat_interval</span>
<span class="quote">&gt;  - swappiness</span>
<span class="quote">&gt; +- lazyfreeness</span>
<span class="quote">&gt;  - user_reserve_kbytes</span>
<span class="quote">&gt;  - vfs_cache_pressure</span>
<span class="quote">&gt;  - zone_reclaim_mode</span>
<span class="quote">&gt; @@ -737,6 +738,18 @@ The default value is 60.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  ==============================================================</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +lazyfreeness</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +This control is used to define how aggressive the kernel will discard</span>
<span class="quote">&gt; +MADV_FREE hinted pages.  Higher values will increase agressiveness,</span>
<span class="quote">&gt; +lower values decrease the amount of discarding.  A value of 0 instructs</span>
<span class="quote">&gt; +the kernel not to initiate discarding until the amount of free and</span>
<span class="quote">&gt; +file-backed pages is less than the high water mark in a zone.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +The default value is 20.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +==============================================================</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  - user_reserve_kbytes</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  When overcommit_memory is set to 2, &quot;never overcommit&quot; mode, reserve</span>
<span class="quote">&gt; diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="quote">&gt; index f7a1f2107b43..3b0bf1b78b2e 100644</span>
<span class="quote">&gt; --- a/drivers/base/node.c</span>
<span class="quote">&gt; +++ b/drivers/base/node.c</span>
<span class="quote">&gt; @@ -69,8 +69,8 @@ static ssize_t node_read_meminfo(struct device *dev,</span>
<span class="quote">&gt;  		       &quot;Node %d Inactive(anon): %8lu kB\n&quot;</span>
<span class="quote">&gt;  		       &quot;Node %d Active(file):   %8lu kB\n&quot;</span>
<span class="quote">&gt;  		       &quot;Node %d Inactive(file): %8lu kB\n&quot;</span>
<span class="quote">&gt; -		       &quot;Node %d Unevictable:    %8lu kB\n&quot;</span>
<span class="quote">&gt;  		       &quot;Node %d LazyFree:	%8lu kB\n&quot;</span>
<span class="quote">&gt; +		       &quot;Node %d Unevictable:    %8lu kB\n&quot;</span>
<span class="quote">&gt;  		       &quot;Node %d Mlocked:        %8lu kB\n&quot;,</span>
<span class="quote">&gt;  		       nid, K(i.totalram),</span>
<span class="quote">&gt;  		       nid, K(i.freeram),</span>
<span class="quote">&gt; @@ -83,8 +83,8 @@ static ssize_t node_read_meminfo(struct device *dev,</span>
<span class="quote">&gt;  		       nid, K(node_page_state(nid, NR_INACTIVE_ANON)),</span>
<span class="quote">&gt;  		       nid, K(node_page_state(nid, NR_ACTIVE_FILE)),</span>
<span class="quote">&gt;  		       nid, K(node_page_state(nid, NR_INACTIVE_FILE)),</span>
<span class="quote">&gt; -		       nid, K(node_page_state(nid, NR_UNEVICTABLE)),</span>
<span class="quote">&gt;  		       nid, K(node_page_state(nid, NR_LZFREE)),</span>
<span class="quote">&gt; +		       nid, K(node_page_state(nid, NR_UNEVICTABLE)),</span>
<span class="quote">&gt;  		       nid, K(node_page_state(nid, NR_MLOCK)));</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifdef CONFIG_HIGHMEM</span>
<span class="quote">&gt; diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c</span>
<span class="quote">&gt; index 3444f7c4e0b6..f47e6a5aa2e5 100644</span>
<span class="quote">&gt; --- a/fs/proc/meminfo.c</span>
<span class="quote">&gt; +++ b/fs/proc/meminfo.c</span>
<span class="quote">&gt; @@ -101,8 +101,8 @@ static int meminfo_proc_show(struct seq_file *m, void *v)</span>
<span class="quote">&gt;  		&quot;Inactive(anon): %8lu kB\n&quot;</span>
<span class="quote">&gt;  		&quot;Active(file):   %8lu kB\n&quot;</span>
<span class="quote">&gt;  		&quot;Inactive(file): %8lu kB\n&quot;</span>
<span class="quote">&gt; -		&quot;Unevictable:    %8lu kB\n&quot;</span>
<span class="quote">&gt;  		&quot;LazyFree:	 %8lu kB\n&quot;</span>
<span class="quote">&gt; +		&quot;Unevictable:    %8lu kB\n&quot;</span>
<span class="quote">&gt;  		&quot;Mlocked:        %8lu kB\n&quot;</span>
<span class="quote">&gt;  #ifdef CONFIG_HIGHMEM</span>
<span class="quote">&gt;  		&quot;HighTotal:      %8lu kB\n&quot;</span>
<span class="quote">&gt; @@ -159,8 +159,8 @@ static int meminfo_proc_show(struct seq_file *m, void *v)</span>
<span class="quote">&gt;  		K(pages[LRU_INACTIVE_ANON]),</span>
<span class="quote">&gt;  		K(pages[LRU_ACTIVE_FILE]),</span>
<span class="quote">&gt;  		K(pages[LRU_INACTIVE_FILE]),</span>
<span class="quote">&gt; -		K(pages[LRU_UNEVICTABLE]),</span>
<span class="quote">&gt;  		K(pages[LRU_LZFREE]),</span>
<span class="quote">&gt; +		K(pages[LRU_UNEVICTABLE]),</span>
<span class="quote">&gt;  		K(global_page_state(NR_MLOCK)),</span>
<span class="quote">&gt;  #ifdef CONFIG_HIGHMEM</span>
<span class="quote">&gt;  		K(i.totalhigh),</span>
<span class="quote">&gt; diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="quote">&gt; index 3e3318ddfc0e..5522ff733506 100644</span>
<span class="quote">&gt; --- a/include/linux/memcontrol.h</span>
<span class="quote">&gt; +++ b/include/linux/memcontrol.h</span>
<span class="quote">&gt; @@ -210,6 +210,7 @@ struct mem_cgroup {</span>
<span class="quote">&gt;  	int		under_oom;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	int	swappiness;</span>
<span class="quote">&gt; +	int	lzfreeness;</span>
<span class="quote">&gt;  	/* OOM-Killer disable */</span>
<span class="quote">&gt;  	int		oom_kill_disable;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="quote">&gt; index 1aaa436da0d5..cca514a9701d 100644</span>
<span class="quote">&gt; --- a/include/linux/mmzone.h</span>
<span class="quote">&gt; +++ b/include/linux/mmzone.h</span>
<span class="quote">&gt; @@ -120,8 +120,8 @@ enum zone_stat_item {</span>
<span class="quote">&gt;  	NR_ACTIVE_ANON,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt;  	NR_INACTIVE_FILE,	/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt;  	NR_ACTIVE_FILE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt; -	NR_UNEVICTABLE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt;  	NR_LZFREE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt; +	NR_UNEVICTABLE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt;  	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */</span>
<span class="quote">&gt;  	NR_ANON_PAGES,	/* Mapped anonymous pages */</span>
<span class="quote">&gt;  	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.</span>
<span class="quote">&gt; @@ -179,14 +179,15 @@ enum lru_list {</span>
<span class="quote">&gt;  	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,</span>
<span class="quote">&gt;  	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,</span>
<span class="quote">&gt;  	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,</span>
<span class="quote">&gt; -	LRU_UNEVICTABLE,</span>
<span class="quote">&gt;  	LRU_LZFREE,</span>
<span class="quote">&gt; +	LRU_UNEVICTABLE,</span>
<span class="quote">&gt;  	NR_LRU_LISTS</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define for_each_lru(lru) for (lru = 0; lru &lt; NR_LRU_LISTS; lru++)</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -#define for_each_evictable_lru(lru) for (lru = 0; lru &lt;= LRU_ACTIVE_FILE; lru++)</span>
<span class="quote">&gt; +#define for_each_anon_file_lru(lru) \</span>
<span class="quote">&gt; +		for (lru = 0; lru &lt;= LRU_ACTIVE_FILE; lru++)</span>
<span class="quote">&gt; +#define for_each_evictable_lru(lru) for (lru = 0; lru &lt;= LRU_LZFREE; lru++)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline int is_file_lru(enum lru_list lru)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="quote">&gt; index f0310eeab3ec..73bcdc9d0e88 100644</span>
<span class="quote">&gt; --- a/include/linux/swap.h</span>
<span class="quote">&gt; +++ b/include/linux/swap.h</span>
<span class="quote">&gt; @@ -330,6 +330,7 @@ extern unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *mem,</span>
<span class="quote">&gt;  						unsigned long *nr_scanned);</span>
<span class="quote">&gt;  extern unsigned long shrink_all_memory(unsigned long nr_pages);</span>
<span class="quote">&gt;  extern int vm_swappiness;</span>
<span class="quote">&gt; +extern int vm_lazyfreeness;</span>
<span class="quote">&gt;  extern int remove_mapping(struct address_space *mapping, struct page *page);</span>
<span class="quote">&gt;  extern unsigned long vm_total_pages;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -361,11 +362,25 @@ static inline int mem_cgroup_swappiness(struct mem_cgroup *memcg)</span>
<span class="quote">&gt;  	return memcg-&gt;swappiness;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline int mem_cgroup_lzfreeness(struct mem_cgroup *memcg)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	/* root ? */</span>
<span class="quote">&gt; +	if (mem_cgroup_disabled() || !memcg-&gt;css.parent)</span>
<span class="quote">&gt; +		return vm_lazyfreeness;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return memcg-&gt;lzfreeness;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  static inline int mem_cgroup_swappiness(struct mem_cgroup *mem)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	return vm_swappiness;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int mem_cgroup_lzfreeness(struct mem_cgroup *mem)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return vm_lazyfreeness;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  #ifdef CONFIG_MEMCG_SWAP</span>
<span class="quote">&gt;  extern void mem_cgroup_swapout(struct page *page, swp_entry_t entry);</span>
<span class="quote">&gt; diff --git a/kernel/sysctl.c b/kernel/sysctl.c</span>
<span class="quote">&gt; index e69201d8094e..2496b10c08e9 100644</span>
<span class="quote">&gt; --- a/kernel/sysctl.c</span>
<span class="quote">&gt; +++ b/kernel/sysctl.c</span>
<span class="quote">&gt; @@ -1268,6 +1268,15 @@ static struct ctl_table vm_table[] = {</span>
<span class="quote">&gt;  		.extra1		= &amp;zero,</span>
<span class="quote">&gt;  		.extra2		= &amp;one_hundred,</span>
<span class="quote">&gt;  	},</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt; +		.procname	= &quot;lazyfreeness&quot;,</span>
<span class="quote">&gt; +		.data		= &amp;vm_lazyfreeness,</span>
<span class="quote">&gt; +		.maxlen		= sizeof(vm_lazyfreeness),</span>
<span class="quote">&gt; +		.mode		= 0644,</span>
<span class="quote">&gt; +		.proc_handler	= proc_dointvec_minmax,</span>
<span class="quote">&gt; +		.extra1		= &amp;zero,</span>
<span class="quote">&gt; +		.extra2		= &amp;one_hundred,</span>
<span class="quote">&gt; +	},</span>
<span class="quote">&gt;  #ifdef CONFIG_HUGETLB_PAGE</span>
<span class="quote">&gt;  	{</span>
<span class="quote">&gt;  		.procname	= &quot;nr_hugepages&quot;,</span>
<span class="quote">&gt; diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="quote">&gt; index 1dc599ce1bcb..5bdbe2a20dc0 100644</span>
<span class="quote">&gt; --- a/mm/memcontrol.c</span>
<span class="quote">&gt; +++ b/mm/memcontrol.c</span>
<span class="quote">&gt; @@ -108,8 +108,8 @@ static const char * const mem_cgroup_lru_names[] = {</span>
<span class="quote">&gt;  	&quot;active_anon&quot;,</span>
<span class="quote">&gt;  	&quot;inactive_file&quot;,</span>
<span class="quote">&gt;  	&quot;active_file&quot;,</span>
<span class="quote">&gt; -	&quot;unevictable&quot;,</span>
<span class="quote">&gt;  	&quot;lazyfree&quot;,</span>
<span class="quote">&gt; +	&quot;unevictable&quot;,</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define THRESHOLDS_EVENTS_TARGET 128</span>
<span class="quote">&gt; @@ -3288,6 +3288,30 @@ static int mem_cgroup_swappiness_write(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static u64 mem_cgroup_lzfreeness_read(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt; +				      struct cftype *cft)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mem_cgroup *memcg = mem_cgroup_from_css(css);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return mem_cgroup_lzfreeness(memcg);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int mem_cgroup_lzfreeness_write(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt; +				       struct cftype *cft, u64 val)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mem_cgroup *memcg = mem_cgroup_from_css(css);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (val &gt; 100)</span>
<span class="quote">&gt; +		return -EINVAL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (css-&gt;parent)</span>
<span class="quote">&gt; +		memcg-&gt;lzfreeness = val;</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		vm_lazyfreeness = val;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static void __mem_cgroup_threshold(struct mem_cgroup *memcg, bool swap)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct mem_cgroup_threshold_ary *t;</span>
<span class="quote">&gt; @@ -4085,6 +4109,11 @@ static struct cftype mem_cgroup_legacy_files[] = {</span>
<span class="quote">&gt;  		.write_u64 = mem_cgroup_swappiness_write,</span>
<span class="quote">&gt;  	},</span>
<span class="quote">&gt;  	{</span>
<span class="quote">&gt; +		.name = &quot;lazyfreeness&quot;,</span>
<span class="quote">&gt; +		.read_u64 = mem_cgroup_lzfreeness_read,</span>
<span class="quote">&gt; +		.write_u64 = mem_cgroup_lzfreeness_write,</span>
<span class="quote">&gt; +	},</span>
<span class="quote">&gt; +	{</span>
<span class="quote">&gt;  		.name = &quot;move_charge_at_immigrate&quot;,</span>
<span class="quote">&gt;  		.read_u64 = mem_cgroup_move_charge_read,</span>
<span class="quote">&gt;  		.write_u64 = mem_cgroup_move_charge_write,</span>
<span class="quote">&gt; @@ -4305,6 +4334,7 @@ mem_cgroup_css_online(struct cgroup_subsys_state *css)</span>
<span class="quote">&gt;  	memcg-&gt;use_hierarchy = parent-&gt;use_hierarchy;</span>
<span class="quote">&gt;  	memcg-&gt;oom_kill_disable = parent-&gt;oom_kill_disable;</span>
<span class="quote">&gt;  	memcg-&gt;swappiness = mem_cgroup_swappiness(parent);</span>
<span class="quote">&gt; +	memcg-&gt;lzfreeness = mem_cgroup_lzfreeness(parent);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (parent-&gt;use_hierarchy) {</span>
<span class="quote">&gt;  		page_counter_init(&amp;memcg-&gt;memory, &amp;parent-&gt;memory);</span>
<span class="quote">&gt; diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt; index cd65db9d3004..f1abc8a6ca31 100644</span>
<span class="quote">&gt; --- a/mm/vmscan.c</span>
<span class="quote">&gt; +++ b/mm/vmscan.c</span>
<span class="quote">&gt; @@ -141,6 +141,10 @@ struct scan_control {</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int vm_swappiness = 60;</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * From 0 .. 100.  Higher means more lazy freeing.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +int vm_lazyfreeness = 20;</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;   * The total number of pages which are beyond the high watermark within all</span>
<span class="quote">&gt;   * zones.</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; @@ -2012,10 +2016,11 @@ enum scan_balance {</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * nr[0] = anon inactive pages to scan; nr[1] = anon active pages to scan</span>
<span class="quote">&gt;   * nr[2] = file inactive pages to scan; nr[3] = file active pages to scan</span>
<span class="quote">&gt; + * nr[4] = lazy free pages to scan;</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
<span class="quote">&gt; -			   struct scan_control *sc, unsigned long *nr,</span>
<span class="quote">&gt; -			   unsigned long *lru_pages)</span>
<span class="quote">&gt; +			int lzfreeness, struct scan_control *sc,</span>
<span class="quote">&gt; +			unsigned long *nr, unsigned long *lru_pages)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct zone_reclaim_stat *reclaim_stat = &amp;lruvec-&gt;reclaim_stat;</span>
<span class="quote">&gt;  	u64 fraction[2];</span>
<span class="quote">&gt; @@ -2023,12 +2028,13 @@ static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
<span class="quote">&gt;  	struct zone *zone = lruvec_zone(lruvec);</span>
<span class="quote">&gt;  	unsigned long anon_prio, file_prio;</span>
<span class="quote">&gt;  	enum scan_balance scan_balance;</span>
<span class="quote">&gt; -	unsigned long anon, file;</span>
<span class="quote">&gt; +	unsigned long anon, file, lzfree;</span>
<span class="quote">&gt;  	bool force_scan = false;</span>
<span class="quote">&gt;  	unsigned long ap, fp;</span>
<span class="quote">&gt;  	enum lru_list lru;</span>
<span class="quote">&gt;  	bool some_scanned;</span>
<span class="quote">&gt;  	int pass;</span>
<span class="quote">&gt; +	unsigned long scan_lzfree = 0;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * If the zone or memcg is small, nr[l] can be 0.  This</span>
<span class="quote">&gt; @@ -2166,7 +2172,7 @@ static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
<span class="quote">&gt;  	/* Only use force_scan on second pass. */</span>
<span class="quote">&gt;  	for (pass = 0; !some_scanned &amp;&amp; pass &lt; 2; pass++) {</span>
<span class="quote">&gt;  		*lru_pages = 0;</span>
<span class="quote">&gt; -		for_each_evictable_lru(lru) {</span>
<span class="quote">&gt; +		for_each_anon_file_lru(lru) {</span>
<span class="quote">&gt;  			int file = is_file_lru(lru);</span>
<span class="quote">&gt;  			unsigned long size;</span>
<span class="quote">&gt;  			unsigned long scan;</span>
<span class="quote">&gt; @@ -2212,6 +2218,28 @@ static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
<span class="quote">&gt;  			some_scanned |= !!scan;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	lzfree = get_lru_size(lruvec, LRU_LZFREE);</span>
<span class="quote">&gt; +	if (lzfree) {</span>
<span class="quote">&gt; +		scan_lzfree = sc-&gt;nr_to_reclaim *</span>
<span class="quote">&gt; +				(DEF_PRIORITY - sc-&gt;priority);</span>

scan_lzfree == 0 if sc-&gt;priority == DEF_PRIORITY, is this intended?
<span class="quote">&gt; +		scan_lzfree = div64_u64(scan_lzfree *</span>
<span class="quote">&gt; +					lzfreeness, 50);</span>
<span class="quote">&gt; +		if (!scan_lzfree) {</span>
<span class="quote">&gt; +			unsigned long zonefile, zonefree;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			zonefree = zone_page_state(zone, NR_FREE_PAGES);</span>
<span class="quote">&gt; +			zonefile = zone_page_state(zone, NR_ACTIVE_FILE) +</span>
<span class="quote">&gt; +				zone_page_state(zone, NR_INACTIVE_FILE);</span>
<span class="quote">&gt; +			if (unlikely(zonefile + zonefree &lt;=</span>
<span class="quote">&gt; +					high_wmark_pages(zone))) {</span>
<span class="quote">&gt; +				scan_lzfree = get_lru_size(lruvec,</span>
<span class="quote">&gt; +						LRU_LZFREE) &gt;&gt; sc-&gt;priority;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	nr[LRU_LZFREE] = min(scan_lzfree, lzfree);</span>
<span class="quote">&gt;  }</span>

Looks there is no setting to only reclaim lazyfree pages. Could we have an
option for this? It&#39;s legit we don&#39;t want to trash page cache because of
lazyfree memory.

Thanks,
Shaohua
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Nov. 13, 2015, 6:20 a.m.</div>
<pre class="content">
On Thu, Nov 12, 2015 at 11:44:53AM -0800, Shaohua Li wrote:
<span class="quote">&gt; On Thu, Nov 12, 2015 at 01:33:13PM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; MADV_FREEed page&#39;s hotness is very arguble.</span>
<span class="quote">&gt; &gt; Someone think it&#39;s hot while others are it&#39;s cold.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Quote from Shaohua</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; My main concern is the policy how we should treat the FREE pages. Moving it to</span>
<span class="quote">&gt; &gt; inactive lru is definitionly a good start, I&#39;m wondering if it&#39;s enough. The</span>
<span class="quote">&gt; &gt; MADV_FREE increases memory pressure and cause unnecessary reclaim because of</span>
<span class="quote">&gt; &gt; the lazy memory free. While MADV_FREE is intended to be a better replacement of</span>
<span class="quote">&gt; &gt; MADV_DONTNEED, MADV_DONTNEED doesn&#39;t have the memory pressure issue as it free</span>
<span class="quote">&gt; &gt; memory immediately. So I hope the MADV_FREE doesn&#39;t have impact on memory</span>
<span class="quote">&gt; &gt; pressure too. I&#39;m thinking of adding an extra lru list and wartermark for this</span>
<span class="quote">&gt; &gt; to make sure FREE pages can be freed before system wide page reclaim. As you</span>
<span class="quote">&gt; &gt; said, this is arguable, but I hope we can discuss about this issue more.</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Quote from me</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; It seems the divergence comes from MADV_FREE is *replacement* of MADV_DONTNEED.</span>
<span class="quote">&gt; &gt; But I don&#39;t think so. If we could discard MADV_FREEed page *anytime*, I agree</span>
<span class="quote">&gt; &gt; but it&#39;s not true because the page would be dirty state when VM want to reclaim.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;m also against with your&#39;s suggestion which let&#39;s discard FREEed page before</span>
<span class="quote">&gt; &gt; system wide page reclaim because system would have lots of clean cold page</span>
<span class="quote">&gt; &gt; caches or anonymous pages. In such case, reclaiming of them would be better.</span>
<span class="quote">&gt; &gt; Yeb, it&#39;s really workload-dependent so we might need some heuristic which is</span>
<span class="quote">&gt; &gt; normally what we want to avoid.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Having said that, I agree with you we could do better than the deactivation</span>
<span class="quote">&gt; &gt; and frankly speaking, I&#39;m thinking of another LRU list(e.g. tentatively named</span>
<span class="quote">&gt; &gt; &quot;ezreclaim LRU list&quot;). What I have in mind is to age (anon|file|ez)</span>
<span class="quote">&gt; &gt; fairly. IOW, I want to percolate ez-LRU list reclaiming into get_scan_count.</span>
<span class="quote">&gt; &gt; When the MADV_FREE is called, we could move hinted pages from anon-LRU to</span>
<span class="quote">&gt; &gt; ez-LRU and then If VM find to not be able to discard a page in ez-LRU,</span>
<span class="quote">&gt; &gt; it could promote it to acive-anon-LRU which would be very natural aging</span>
<span class="quote">&gt; &gt; concept because it mean someone touches the page recenlty.</span>
<span class="quote">&gt; &gt; With that, I don&#39;t want to bias one side and don&#39;t want to add some knob for</span>
<span class="quote">&gt; &gt; tuning the heuristic but let&#39;s rely on common fair aging scheme of VM.</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Quote from Johannes</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; thread 1:</span>
<span class="quote">&gt; &gt; Even if we&#39;re wrong about the aging of those MADV_FREE pages, their</span>
<span class="quote">&gt; &gt; contents are invalidated; they can be discarded freely, and restoring</span>
<span class="quote">&gt; &gt; them is a mere GFP_ZERO allocation. All other anonymous pages have to</span>
<span class="quote">&gt; &gt; be written to disk, and potentially be read back.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; [ Arguably, MADV_FREE pages should even be reclaimed before inactive</span>
<span class="quote">&gt; &gt;   page cache. It&#39;s the same cost to discard both types of pages, but</span>
<span class="quote">&gt; &gt;   restoring page cache involves IO. ]</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It probably makes sense to stop thinking about them as anonymous pages</span>
<span class="quote">&gt; &gt; entirely at this point when it comes to aging. They&#39;re really not. The</span>
<span class="quote">&gt; &gt; LRU lists are split to differentiate access patterns and cost of page</span>
<span class="quote">&gt; &gt; stealing (and restoring). From that angle, MADV_FREE pages really have</span>
<span class="quote">&gt; &gt; nothing in common with in-use anonymous pages, and so they shouldn&#39;t</span>
<span class="quote">&gt; &gt; be on the same LRU list.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; thread:2</span>
<span class="quote">&gt; &gt; What about them is hot? They contain garbage, you have to write to</span>
<span class="quote">&gt; &gt; them before you can use them. Granted, you might have to refetch</span>
<span class="quote">&gt; &gt; cachelines if you don&#39;t do cacheline-aligned populating writes, but</span>
<span class="quote">&gt; &gt; you can do a lot of them before it&#39;s more expensive than doing IO.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Quote from Daniel</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; thread:1</span>
<span class="quote">&gt; &gt; Keep in mind that this is memory the kernel wouldn&#39;t be getting back at</span>
<span class="quote">&gt; &gt; all if the allocator wasn&#39;t going out of the way to purge it, and they</span>
<span class="quote">&gt; &gt; aren&#39;t going to go out of their way to purge it if it means the kernel</span>
<span class="quote">&gt; &gt; is going to steal the pages when there isn&#39;t actually memory pressure.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; An allocator would be using MADV_DONTNEED if it didn&#39;t expect that the</span>
<span class="quote">&gt; &gt; pages were going to be used against shortly. MADV_FREE indicates that it</span>
<span class="quote">&gt; &gt; has time to inform the kernel that they&#39;re unused but they could still</span>
<span class="quote">&gt; &gt; be very hot.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; thread:2</span>
<span class="quote">&gt; &gt; It&#39;s hot because applications churn through memory via the allocator.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Drop the pages and the application is now churning through page faults</span>
<span class="quote">&gt; &gt; and zeroing rather than simply reusing memory. It&#39;s not something that</span>
<span class="quote">&gt; &gt; may happen, it *will* happen. A page in the page cache *may* be reused,</span>
<span class="quote">&gt; &gt; but often won&#39;t be, especially when the I/O patterns don&#39;t line up well</span>
<span class="quote">&gt; &gt; with the way it works.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The whole point of the feature is not requiring the allocator to have</span>
<span class="quote">&gt; &gt; elaborate mechanisms for aging pages and throttling purging. That ends</span>
<span class="quote">&gt; &gt; up resulting in lots of memory held by userspace where the kernel can&#39;t</span>
<span class="quote">&gt; &gt; reclaim it under memory pressure. If it&#39;s dropped before page cache, it</span>
<span class="quote">&gt; &gt; isn&#39;t going to be able to replace any of that logic in allocators.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The page cache is speculative. Page caching by allocators is not really</span>
<span class="quote">&gt; &gt; speculative. Using MADV_FREE on the pages at all is speculative. The</span>
<span class="quote">&gt; &gt; memory is probably going to be reused fairly soon (unless the process</span>
<span class="quote">&gt; &gt; exits, and then it doesn&#39;t matter), but purging will end up reducing</span>
<span class="quote">&gt; &gt; memory usage for the portions that aren&#39;t.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It would be a different story for a full unpinning/pinning feature since</span>
<span class="quote">&gt; &gt; that would have other use cases (speculative caches), but this is really</span>
<span class="quote">&gt; &gt; only useful in allocators.</span>
<span class="quote">&gt; &gt; &quot;</span>
<span class="quote">&gt; &gt; You could read all thread from https://lkml.org/lkml/2015/11/4/51</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Yeah, with arguble issue and there is no one decision, I think it</span>
<span class="quote">&gt; &gt; means we should provide the knob &quot;lazyfreeness&quot;(I hope someone</span>
<span class="quote">&gt; &gt; give better naming).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; It&#39;s similar to swapppiness so higher values will discard MADV_FREE</span>
<span class="quote">&gt; &gt; pages agreessively. If memory pressure happens and system works with</span>
<span class="quote">&gt; &gt; DEF_PRIOIRTY(ex, clean cold caches), VM doesn&#39;t discard any hinted</span>
<span class="quote">&gt; &gt; pages until the scanning priority is increased.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If memory pressure is higher(ie, the priority is not DEF_PRIORITY),</span>
<span class="quote">&gt; &gt; it scans</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	nr_to_reclaim * priority * lazyfreensss(def: 20) / 50</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; If system has low free memory and file cache, it start to discard</span>
<span class="quote">&gt; &gt; MADV_FREEed pages unconditionally even though user set lazyfreeness to 0.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  Documentation/sysctl/vm.txt | 13 +++++++++</span>
<span class="quote">&gt; &gt;  drivers/base/node.c         |  4 +--</span>
<span class="quote">&gt; &gt;  fs/proc/meminfo.c           |  4 +--</span>
<span class="quote">&gt; &gt;  include/linux/memcontrol.h  |  1 +</span>
<span class="quote">&gt; &gt;  include/linux/mmzone.h      |  9 +++---</span>
<span class="quote">&gt; &gt;  include/linux/swap.h        | 15 ++++++++++</span>
<span class="quote">&gt; &gt;  kernel/sysctl.c             |  9 ++++++</span>
<span class="quote">&gt; &gt;  mm/memcontrol.c             | 32 +++++++++++++++++++++-</span>
<span class="quote">&gt; &gt;  mm/vmscan.c                 | 67 ++++++++++++++++++++++++++++-----------------</span>
<span class="quote">&gt; &gt;  mm/vmstat.c                 |  2 +-</span>
<span class="quote">&gt; &gt;  10 files changed, 121 insertions(+), 35 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/Documentation/sysctl/vm.txt b/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; &gt; index a4482fceacec..c1dc63381f2c 100644</span>
<span class="quote">&gt; &gt; --- a/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; &gt; +++ b/Documentation/sysctl/vm.txt</span>
<span class="quote">&gt; &gt; @@ -56,6 +56,7 @@ files can be found in mm/swap.c.</span>
<span class="quote">&gt; &gt;  - percpu_pagelist_fraction</span>
<span class="quote">&gt; &gt;  - stat_interval</span>
<span class="quote">&gt; &gt;  - swappiness</span>
<span class="quote">&gt; &gt; +- lazyfreeness</span>
<span class="quote">&gt; &gt;  - user_reserve_kbytes</span>
<span class="quote">&gt; &gt;  - vfs_cache_pressure</span>
<span class="quote">&gt; &gt;  - zone_reclaim_mode</span>
<span class="quote">&gt; &gt; @@ -737,6 +738,18 @@ The default value is 60.</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  ==============================================================</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +lazyfreeness</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +This control is used to define how aggressive the kernel will discard</span>
<span class="quote">&gt; &gt; +MADV_FREE hinted pages.  Higher values will increase agressiveness,</span>
<span class="quote">&gt; &gt; +lower values decrease the amount of discarding.  A value of 0 instructs</span>
<span class="quote">&gt; &gt; +the kernel not to initiate discarding until the amount of free and</span>
<span class="quote">&gt; &gt; +file-backed pages is less than the high water mark in a zone.</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +The default value is 20.</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +==============================================================</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  - user_reserve_kbytes</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  When overcommit_memory is set to 2, &quot;never overcommit&quot; mode, reserve</span>
<span class="quote">&gt; &gt; diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="quote">&gt; &gt; index f7a1f2107b43..3b0bf1b78b2e 100644</span>
<span class="quote">&gt; &gt; --- a/drivers/base/node.c</span>
<span class="quote">&gt; &gt; +++ b/drivers/base/node.c</span>
<span class="quote">&gt; &gt; @@ -69,8 +69,8 @@ static ssize_t node_read_meminfo(struct device *dev,</span>
<span class="quote">&gt; &gt;  		       &quot;Node %d Inactive(anon): %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt;  		       &quot;Node %d Active(file):   %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt;  		       &quot;Node %d Inactive(file): %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt; -		       &quot;Node %d Unevictable:    %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt;  		       &quot;Node %d LazyFree:	%8lu kB\n&quot;</span>
<span class="quote">&gt; &gt; +		       &quot;Node %d Unevictable:    %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt;  		       &quot;Node %d Mlocked:        %8lu kB\n&quot;,</span>
<span class="quote">&gt; &gt;  		       nid, K(i.totalram),</span>
<span class="quote">&gt; &gt;  		       nid, K(i.freeram),</span>
<span class="quote">&gt; &gt; @@ -83,8 +83,8 @@ static ssize_t node_read_meminfo(struct device *dev,</span>
<span class="quote">&gt; &gt;  		       nid, K(node_page_state(nid, NR_INACTIVE_ANON)),</span>
<span class="quote">&gt; &gt;  		       nid, K(node_page_state(nid, NR_ACTIVE_FILE)),</span>
<span class="quote">&gt; &gt;  		       nid, K(node_page_state(nid, NR_INACTIVE_FILE)),</span>
<span class="quote">&gt; &gt; -		       nid, K(node_page_state(nid, NR_UNEVICTABLE)),</span>
<span class="quote">&gt; &gt;  		       nid, K(node_page_state(nid, NR_LZFREE)),</span>
<span class="quote">&gt; &gt; +		       nid, K(node_page_state(nid, NR_UNEVICTABLE)),</span>
<span class="quote">&gt; &gt;  		       nid, K(node_page_state(nid, NR_MLOCK)));</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  #ifdef CONFIG_HIGHMEM</span>
<span class="quote">&gt; &gt; diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c</span>
<span class="quote">&gt; &gt; index 3444f7c4e0b6..f47e6a5aa2e5 100644</span>
<span class="quote">&gt; &gt; --- a/fs/proc/meminfo.c</span>
<span class="quote">&gt; &gt; +++ b/fs/proc/meminfo.c</span>
<span class="quote">&gt; &gt; @@ -101,8 +101,8 @@ static int meminfo_proc_show(struct seq_file *m, void *v)</span>
<span class="quote">&gt; &gt;  		&quot;Inactive(anon): %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt;  		&quot;Active(file):   %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt;  		&quot;Inactive(file): %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt; -		&quot;Unevictable:    %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt;  		&quot;LazyFree:	 %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt; +		&quot;Unevictable:    %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt;  		&quot;Mlocked:        %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt;  #ifdef CONFIG_HIGHMEM</span>
<span class="quote">&gt; &gt;  		&quot;HighTotal:      %8lu kB\n&quot;</span>
<span class="quote">&gt; &gt; @@ -159,8 +159,8 @@ static int meminfo_proc_show(struct seq_file *m, void *v)</span>
<span class="quote">&gt; &gt;  		K(pages[LRU_INACTIVE_ANON]),</span>
<span class="quote">&gt; &gt;  		K(pages[LRU_ACTIVE_FILE]),</span>
<span class="quote">&gt; &gt;  		K(pages[LRU_INACTIVE_FILE]),</span>
<span class="quote">&gt; &gt; -		K(pages[LRU_UNEVICTABLE]),</span>
<span class="quote">&gt; &gt;  		K(pages[LRU_LZFREE]),</span>
<span class="quote">&gt; &gt; +		K(pages[LRU_UNEVICTABLE]),</span>
<span class="quote">&gt; &gt;  		K(global_page_state(NR_MLOCK)),</span>
<span class="quote">&gt; &gt;  #ifdef CONFIG_HIGHMEM</span>
<span class="quote">&gt; &gt;  		K(i.totalhigh),</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="quote">&gt; &gt; index 3e3318ddfc0e..5522ff733506 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/memcontrol.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/memcontrol.h</span>
<span class="quote">&gt; &gt; @@ -210,6 +210,7 @@ struct mem_cgroup {</span>
<span class="quote">&gt; &gt;  	int		under_oom;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	int	swappiness;</span>
<span class="quote">&gt; &gt; +	int	lzfreeness;</span>
<span class="quote">&gt; &gt;  	/* OOM-Killer disable */</span>
<span class="quote">&gt; &gt;  	int		oom_kill_disable;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="quote">&gt; &gt; index 1aaa436da0d5..cca514a9701d 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/mmzone.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/mmzone.h</span>
<span class="quote">&gt; &gt; @@ -120,8 +120,8 @@ enum zone_stat_item {</span>
<span class="quote">&gt; &gt;  	NR_ACTIVE_ANON,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt; &gt;  	NR_INACTIVE_FILE,	/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt; &gt;  	NR_ACTIVE_FILE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt; &gt; -	NR_UNEVICTABLE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt; &gt;  	NR_LZFREE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt; &gt; +	NR_UNEVICTABLE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
<span class="quote">&gt; &gt;  	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */</span>
<span class="quote">&gt; &gt;  	NR_ANON_PAGES,	/* Mapped anonymous pages */</span>
<span class="quote">&gt; &gt;  	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.</span>
<span class="quote">&gt; &gt; @@ -179,14 +179,15 @@ enum lru_list {</span>
<span class="quote">&gt; &gt;  	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,</span>
<span class="quote">&gt; &gt;  	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,</span>
<span class="quote">&gt; &gt;  	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,</span>
<span class="quote">&gt; &gt; -	LRU_UNEVICTABLE,</span>
<span class="quote">&gt; &gt;  	LRU_LZFREE,</span>
<span class="quote">&gt; &gt; +	LRU_UNEVICTABLE,</span>
<span class="quote">&gt; &gt;  	NR_LRU_LISTS</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  #define for_each_lru(lru) for (lru = 0; lru &lt; NR_LRU_LISTS; lru++)</span>
<span class="quote">&gt; &gt; -</span>
<span class="quote">&gt; &gt; -#define for_each_evictable_lru(lru) for (lru = 0; lru &lt;= LRU_ACTIVE_FILE; lru++)</span>
<span class="quote">&gt; &gt; +#define for_each_anon_file_lru(lru) \</span>
<span class="quote">&gt; &gt; +		for (lru = 0; lru &lt;= LRU_ACTIVE_FILE; lru++)</span>
<span class="quote">&gt; &gt; +#define for_each_evictable_lru(lru) for (lru = 0; lru &lt;= LRU_LZFREE; lru++)</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static inline int is_file_lru(enum lru_list lru)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="quote">&gt; &gt; index f0310eeab3ec..73bcdc9d0e88 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/swap.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/swap.h</span>
<span class="quote">&gt; &gt; @@ -330,6 +330,7 @@ extern unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *mem,</span>
<span class="quote">&gt; &gt;  						unsigned long *nr_scanned);</span>
<span class="quote">&gt; &gt;  extern unsigned long shrink_all_memory(unsigned long nr_pages);</span>
<span class="quote">&gt; &gt;  extern int vm_swappiness;</span>
<span class="quote">&gt; &gt; +extern int vm_lazyfreeness;</span>
<span class="quote">&gt; &gt;  extern int remove_mapping(struct address_space *mapping, struct page *page);</span>
<span class="quote">&gt; &gt;  extern unsigned long vm_total_pages;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; @@ -361,11 +362,25 @@ static inline int mem_cgroup_swappiness(struct mem_cgroup *memcg)</span>
<span class="quote">&gt; &gt;  	return memcg-&gt;swappiness;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +static inline int mem_cgroup_lzfreeness(struct mem_cgroup *memcg)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	/* root ? */</span>
<span class="quote">&gt; &gt; +	if (mem_cgroup_disabled() || !memcg-&gt;css.parent)</span>
<span class="quote">&gt; &gt; +		return vm_lazyfreeness;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return memcg-&gt;lzfreeness;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  #else</span>
<span class="quote">&gt; &gt;  static inline int mem_cgroup_swappiness(struct mem_cgroup *mem)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;  	return vm_swappiness;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static inline int mem_cgroup_lzfreeness(struct mem_cgroup *mem)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	return vm_lazyfreeness;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt;  #ifdef CONFIG_MEMCG_SWAP</span>
<span class="quote">&gt; &gt;  extern void mem_cgroup_swapout(struct page *page, swp_entry_t entry);</span>
<span class="quote">&gt; &gt; diff --git a/kernel/sysctl.c b/kernel/sysctl.c</span>
<span class="quote">&gt; &gt; index e69201d8094e..2496b10c08e9 100644</span>
<span class="quote">&gt; &gt; --- a/kernel/sysctl.c</span>
<span class="quote">&gt; &gt; +++ b/kernel/sysctl.c</span>
<span class="quote">&gt; &gt; @@ -1268,6 +1268,15 @@ static struct ctl_table vm_table[] = {</span>
<span class="quote">&gt; &gt;  		.extra1		= &amp;zero,</span>
<span class="quote">&gt; &gt;  		.extra2		= &amp;one_hundred,</span>
<span class="quote">&gt; &gt;  	},</span>
<span class="quote">&gt; &gt; +	{</span>
<span class="quote">&gt; &gt; +		.procname	= &quot;lazyfreeness&quot;,</span>
<span class="quote">&gt; &gt; +		.data		= &amp;vm_lazyfreeness,</span>
<span class="quote">&gt; &gt; +		.maxlen		= sizeof(vm_lazyfreeness),</span>
<span class="quote">&gt; &gt; +		.mode		= 0644,</span>
<span class="quote">&gt; &gt; +		.proc_handler	= proc_dointvec_minmax,</span>
<span class="quote">&gt; &gt; +		.extra1		= &amp;zero,</span>
<span class="quote">&gt; &gt; +		.extra2		= &amp;one_hundred,</span>
<span class="quote">&gt; &gt; +	},</span>
<span class="quote">&gt; &gt;  #ifdef CONFIG_HUGETLB_PAGE</span>
<span class="quote">&gt; &gt;  	{</span>
<span class="quote">&gt; &gt;  		.procname	= &quot;nr_hugepages&quot;,</span>
<span class="quote">&gt; &gt; diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="quote">&gt; &gt; index 1dc599ce1bcb..5bdbe2a20dc0 100644</span>
<span class="quote">&gt; &gt; --- a/mm/memcontrol.c</span>
<span class="quote">&gt; &gt; +++ b/mm/memcontrol.c</span>
<span class="quote">&gt; &gt; @@ -108,8 +108,8 @@ static const char * const mem_cgroup_lru_names[] = {</span>
<span class="quote">&gt; &gt;  	&quot;active_anon&quot;,</span>
<span class="quote">&gt; &gt;  	&quot;inactive_file&quot;,</span>
<span class="quote">&gt; &gt;  	&quot;active_file&quot;,</span>
<span class="quote">&gt; &gt; -	&quot;unevictable&quot;,</span>
<span class="quote">&gt; &gt;  	&quot;lazyfree&quot;,</span>
<span class="quote">&gt; &gt; +	&quot;unevictable&quot;,</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  #define THRESHOLDS_EVENTS_TARGET 128</span>
<span class="quote">&gt; &gt; @@ -3288,6 +3288,30 @@ static int mem_cgroup_swappiness_write(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt; &gt;  	return 0;</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +static u64 mem_cgroup_lzfreeness_read(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt; &gt; +				      struct cftype *cft)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct mem_cgroup *memcg = mem_cgroup_from_css(css);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return mem_cgroup_lzfreeness(memcg);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int mem_cgroup_lzfreeness_write(struct cgroup_subsys_state *css,</span>
<span class="quote">&gt; &gt; +				       struct cftype *cft, u64 val)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct mem_cgroup *memcg = mem_cgroup_from_css(css);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (val &gt; 100)</span>
<span class="quote">&gt; &gt; +		return -EINVAL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (css-&gt;parent)</span>
<span class="quote">&gt; &gt; +		memcg-&gt;lzfreeness = val;</span>
<span class="quote">&gt; &gt; +	else</span>
<span class="quote">&gt; &gt; +		vm_lazyfreeness = val;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt;  static void __mem_cgroup_threshold(struct mem_cgroup *memcg, bool swap)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;  	struct mem_cgroup_threshold_ary *t;</span>
<span class="quote">&gt; &gt; @@ -4085,6 +4109,11 @@ static struct cftype mem_cgroup_legacy_files[] = {</span>
<span class="quote">&gt; &gt;  		.write_u64 = mem_cgroup_swappiness_write,</span>
<span class="quote">&gt; &gt;  	},</span>
<span class="quote">&gt; &gt;  	{</span>
<span class="quote">&gt; &gt; +		.name = &quot;lazyfreeness&quot;,</span>
<span class="quote">&gt; &gt; +		.read_u64 = mem_cgroup_lzfreeness_read,</span>
<span class="quote">&gt; &gt; +		.write_u64 = mem_cgroup_lzfreeness_write,</span>
<span class="quote">&gt; &gt; +	},</span>
<span class="quote">&gt; &gt; +	{</span>
<span class="quote">&gt; &gt;  		.name = &quot;move_charge_at_immigrate&quot;,</span>
<span class="quote">&gt; &gt;  		.read_u64 = mem_cgroup_move_charge_read,</span>
<span class="quote">&gt; &gt;  		.write_u64 = mem_cgroup_move_charge_write,</span>
<span class="quote">&gt; &gt; @@ -4305,6 +4334,7 @@ mem_cgroup_css_online(struct cgroup_subsys_state *css)</span>
<span class="quote">&gt; &gt;  	memcg-&gt;use_hierarchy = parent-&gt;use_hierarchy;</span>
<span class="quote">&gt; &gt;  	memcg-&gt;oom_kill_disable = parent-&gt;oom_kill_disable;</span>
<span class="quote">&gt; &gt;  	memcg-&gt;swappiness = mem_cgroup_swappiness(parent);</span>
<span class="quote">&gt; &gt; +	memcg-&gt;lzfreeness = mem_cgroup_lzfreeness(parent);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	if (parent-&gt;use_hierarchy) {</span>
<span class="quote">&gt; &gt;  		page_counter_init(&amp;memcg-&gt;memory, &amp;parent-&gt;memory);</span>
<span class="quote">&gt; &gt; diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt; &gt; index cd65db9d3004..f1abc8a6ca31 100644</span>
<span class="quote">&gt; &gt; --- a/mm/vmscan.c</span>
<span class="quote">&gt; &gt; +++ b/mm/vmscan.c</span>
<span class="quote">&gt; &gt; @@ -141,6 +141,10 @@ struct scan_control {</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt;  int vm_swappiness = 60;</span>
<span class="quote">&gt; &gt;  /*</span>
<span class="quote">&gt; &gt; + * From 0 .. 100.  Higher means more lazy freeing.</span>
<span class="quote">&gt; &gt; + */</span>
<span class="quote">&gt; &gt; +int vm_lazyfreeness = 20;</span>
<span class="quote">&gt; &gt; +/*</span>
<span class="quote">&gt; &gt;   * The total number of pages which are beyond the high watermark within all</span>
<span class="quote">&gt; &gt;   * zones.</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt; @@ -2012,10 +2016,11 @@ enum scan_balance {</span>
<span class="quote">&gt; &gt;   *</span>
<span class="quote">&gt; &gt;   * nr[0] = anon inactive pages to scan; nr[1] = anon active pages to scan</span>
<span class="quote">&gt; &gt;   * nr[2] = file inactive pages to scan; nr[3] = file active pages to scan</span>
<span class="quote">&gt; &gt; + * nr[4] = lazy free pages to scan;</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt;  static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
<span class="quote">&gt; &gt; -			   struct scan_control *sc, unsigned long *nr,</span>
<span class="quote">&gt; &gt; -			   unsigned long *lru_pages)</span>
<span class="quote">&gt; &gt; +			int lzfreeness, struct scan_control *sc,</span>
<span class="quote">&gt; &gt; +			unsigned long *nr, unsigned long *lru_pages)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt;  	struct zone_reclaim_stat *reclaim_stat = &amp;lruvec-&gt;reclaim_stat;</span>
<span class="quote">&gt; &gt;  	u64 fraction[2];</span>
<span class="quote">&gt; &gt; @@ -2023,12 +2028,13 @@ static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
<span class="quote">&gt; &gt;  	struct zone *zone = lruvec_zone(lruvec);</span>
<span class="quote">&gt; &gt;  	unsigned long anon_prio, file_prio;</span>
<span class="quote">&gt; &gt;  	enum scan_balance scan_balance;</span>
<span class="quote">&gt; &gt; -	unsigned long anon, file;</span>
<span class="quote">&gt; &gt; +	unsigned long anon, file, lzfree;</span>
<span class="quote">&gt; &gt;  	bool force_scan = false;</span>
<span class="quote">&gt; &gt;  	unsigned long ap, fp;</span>
<span class="quote">&gt; &gt;  	enum lru_list lru;</span>
<span class="quote">&gt; &gt;  	bool some_scanned;</span>
<span class="quote">&gt; &gt;  	int pass;</span>
<span class="quote">&gt; &gt; +	unsigned long scan_lzfree = 0;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  	/*</span>
<span class="quote">&gt; &gt;  	 * If the zone or memcg is small, nr[l] can be 0.  This</span>
<span class="quote">&gt; &gt; @@ -2166,7 +2172,7 @@ static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
<span class="quote">&gt; &gt;  	/* Only use force_scan on second pass. */</span>
<span class="quote">&gt; &gt;  	for (pass = 0; !some_scanned &amp;&amp; pass &lt; 2; pass++) {</span>
<span class="quote">&gt; &gt;  		*lru_pages = 0;</span>
<span class="quote">&gt; &gt; -		for_each_evictable_lru(lru) {</span>
<span class="quote">&gt; &gt; +		for_each_anon_file_lru(lru) {</span>
<span class="quote">&gt; &gt;  			int file = is_file_lru(lru);</span>
<span class="quote">&gt; &gt;  			unsigned long size;</span>
<span class="quote">&gt; &gt;  			unsigned long scan;</span>
<span class="quote">&gt; &gt; @@ -2212,6 +2218,28 @@ static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
<span class="quote">&gt; &gt;  			some_scanned |= !!scan;</span>
<span class="quote">&gt; &gt;  		}</span>
<span class="quote">&gt; &gt;  	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	lzfree = get_lru_size(lruvec, LRU_LZFREE);</span>
<span class="quote">&gt; &gt; +	if (lzfree) {</span>
<span class="quote">&gt; &gt; +		scan_lzfree = sc-&gt;nr_to_reclaim *</span>
<span class="quote">&gt; &gt; +				(DEF_PRIORITY - sc-&gt;priority);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; scan_lzfree == 0 if sc-&gt;priority == DEF_PRIORITY, is this intended?</span>
<span class="quote">&gt; &gt; +		scan_lzfree = div64_u64(scan_lzfree *</span>
<span class="quote">&gt; &gt; +					lzfreeness, 50);</span>
<span class="quote">&gt; &gt; +		if (!scan_lzfree) {</span>
<span class="quote">&gt; &gt; +			unsigned long zonefile, zonefree;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +			zonefree = zone_page_state(zone, NR_FREE_PAGES);</span>
<span class="quote">&gt; &gt; +			zonefile = zone_page_state(zone, NR_ACTIVE_FILE) +</span>
<span class="quote">&gt; &gt; +				zone_page_state(zone, NR_INACTIVE_FILE);</span>
<span class="quote">&gt; &gt; +			if (unlikely(zonefile + zonefree &lt;=</span>
<span class="quote">&gt; &gt; +					high_wmark_pages(zone))) {</span>
<span class="quote">&gt; &gt; +				scan_lzfree = get_lru_size(lruvec,</span>
<span class="quote">&gt; &gt; +						LRU_LZFREE) &gt;&gt; sc-&gt;priority;</span>
<span class="quote">&gt; &gt; +			}</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	nr[LRU_LZFREE] = min(scan_lzfree, lzfree);</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Looks there is no setting to only reclaim lazyfree pages. Could we have an</span>
<span class="quote">&gt; option for this? It&#39;s legit we don&#39;t want to trash page cache because of</span>
<span class="quote">&gt; lazyfree memory.</span>

Once we introduc the knob, it could be doable.
I will do it in next spin.

Thanks for the review!
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/sysctl/vm.txt b/Documentation/sysctl/vm.txt</span>
<span class="p_header">index a4482fceacec..c1dc63381f2c 100644</span>
<span class="p_header">--- a/Documentation/sysctl/vm.txt</span>
<span class="p_header">+++ b/Documentation/sysctl/vm.txt</span>
<span class="p_chunk">@@ -56,6 +56,7 @@</span> <span class="p_context"> files can be found in mm/swap.c.</span>
 - percpu_pagelist_fraction
 - stat_interval
 - swappiness
<span class="p_add">+- lazyfreeness</span>
 - user_reserve_kbytes
 - vfs_cache_pressure
 - zone_reclaim_mode
<span class="p_chunk">@@ -737,6 +738,18 @@</span> <span class="p_context"> The default value is 60.</span>
 
 ==============================================================
 
<span class="p_add">+lazyfreeness</span>
<span class="p_add">+</span>
<span class="p_add">+This control is used to define how aggressive the kernel will discard</span>
<span class="p_add">+MADV_FREE hinted pages.  Higher values will increase agressiveness,</span>
<span class="p_add">+lower values decrease the amount of discarding.  A value of 0 instructs</span>
<span class="p_add">+the kernel not to initiate discarding until the amount of free and</span>
<span class="p_add">+file-backed pages is less than the high water mark in a zone.</span>
<span class="p_add">+</span>
<span class="p_add">+The default value is 20.</span>
<span class="p_add">+</span>
<span class="p_add">+==============================================================</span>
<span class="p_add">+</span>
 - user_reserve_kbytes
 
 When overcommit_memory is set to 2, &quot;never overcommit&quot; mode, reserve
<span class="p_header">diff --git a/drivers/base/node.c b/drivers/base/node.c</span>
<span class="p_header">index f7a1f2107b43..3b0bf1b78b2e 100644</span>
<span class="p_header">--- a/drivers/base/node.c</span>
<span class="p_header">+++ b/drivers/base/node.c</span>
<span class="p_chunk">@@ -69,8 +69,8 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       &quot;Node %d Inactive(anon): %8lu kB\n&quot;
 		       &quot;Node %d Active(file):   %8lu kB\n&quot;
 		       &quot;Node %d Inactive(file): %8lu kB\n&quot;
<span class="p_del">-		       &quot;Node %d Unevictable:    %8lu kB\n&quot;</span>
 		       &quot;Node %d LazyFree:	%8lu kB\n&quot;
<span class="p_add">+		       &quot;Node %d Unevictable:    %8lu kB\n&quot;</span>
 		       &quot;Node %d Mlocked:        %8lu kB\n&quot;,
 		       nid, K(i.totalram),
 		       nid, K(i.freeram),
<span class="p_chunk">@@ -83,8 +83,8 @@</span> <span class="p_context"> static ssize_t node_read_meminfo(struct device *dev,</span>
 		       nid, K(node_page_state(nid, NR_INACTIVE_ANON)),
 		       nid, K(node_page_state(nid, NR_ACTIVE_FILE)),
 		       nid, K(node_page_state(nid, NR_INACTIVE_FILE)),
<span class="p_del">-		       nid, K(node_page_state(nid, NR_UNEVICTABLE)),</span>
 		       nid, K(node_page_state(nid, NR_LZFREE)),
<span class="p_add">+		       nid, K(node_page_state(nid, NR_UNEVICTABLE)),</span>
 		       nid, K(node_page_state(nid, NR_MLOCK)));
 
 #ifdef CONFIG_HIGHMEM
<span class="p_header">diff --git a/fs/proc/meminfo.c b/fs/proc/meminfo.c</span>
<span class="p_header">index 3444f7c4e0b6..f47e6a5aa2e5 100644</span>
<span class="p_header">--- a/fs/proc/meminfo.c</span>
<span class="p_header">+++ b/fs/proc/meminfo.c</span>
<span class="p_chunk">@@ -101,8 +101,8 @@</span> <span class="p_context"> static int meminfo_proc_show(struct seq_file *m, void *v)</span>
 		&quot;Inactive(anon): %8lu kB\n&quot;
 		&quot;Active(file):   %8lu kB\n&quot;
 		&quot;Inactive(file): %8lu kB\n&quot;
<span class="p_del">-		&quot;Unevictable:    %8lu kB\n&quot;</span>
 		&quot;LazyFree:	 %8lu kB\n&quot;
<span class="p_add">+		&quot;Unevictable:    %8lu kB\n&quot;</span>
 		&quot;Mlocked:        %8lu kB\n&quot;
 #ifdef CONFIG_HIGHMEM
 		&quot;HighTotal:      %8lu kB\n&quot;
<span class="p_chunk">@@ -159,8 +159,8 @@</span> <span class="p_context"> static int meminfo_proc_show(struct seq_file *m, void *v)</span>
 		K(pages[LRU_INACTIVE_ANON]),
 		K(pages[LRU_ACTIVE_FILE]),
 		K(pages[LRU_INACTIVE_FILE]),
<span class="p_del">-		K(pages[LRU_UNEVICTABLE]),</span>
 		K(pages[LRU_LZFREE]),
<span class="p_add">+		K(pages[LRU_UNEVICTABLE]),</span>
 		K(global_page_state(NR_MLOCK)),
 #ifdef CONFIG_HIGHMEM
 		K(i.totalhigh),
<span class="p_header">diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="p_header">index 3e3318ddfc0e..5522ff733506 100644</span>
<span class="p_header">--- a/include/linux/memcontrol.h</span>
<span class="p_header">+++ b/include/linux/memcontrol.h</span>
<span class="p_chunk">@@ -210,6 +210,7 @@</span> <span class="p_context"> struct mem_cgroup {</span>
 	int		under_oom;
 
 	int	swappiness;
<span class="p_add">+	int	lzfreeness;</span>
 	/* OOM-Killer disable */
 	int		oom_kill_disable;
 
<span class="p_header">diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="p_header">index 1aaa436da0d5..cca514a9701d 100644</span>
<span class="p_header">--- a/include/linux/mmzone.h</span>
<span class="p_header">+++ b/include/linux/mmzone.h</span>
<span class="p_chunk">@@ -120,8 +120,8 @@</span> <span class="p_context"> enum zone_stat_item {</span>
 	NR_ACTIVE_ANON,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
 	NR_INACTIVE_FILE,	/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
 	NR_ACTIVE_FILE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
<span class="p_del">-	NR_UNEVICTABLE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
 	NR_LZFREE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */
<span class="p_add">+	NR_UNEVICTABLE,		/*  &quot;     &quot;     &quot;   &quot;       &quot;         */</span>
 	NR_MLOCK,		/* mlock()ed pages found and moved off LRU */
 	NR_ANON_PAGES,	/* Mapped anonymous pages */
 	NR_FILE_MAPPED,	/* pagecache pages mapped into pagetables.
<span class="p_chunk">@@ -179,14 +179,15 @@</span> <span class="p_context"> enum lru_list {</span>
 	LRU_ACTIVE_ANON = LRU_BASE + LRU_ACTIVE,
 	LRU_INACTIVE_FILE = LRU_BASE + LRU_FILE,
 	LRU_ACTIVE_FILE = LRU_BASE + LRU_FILE + LRU_ACTIVE,
<span class="p_del">-	LRU_UNEVICTABLE,</span>
 	LRU_LZFREE,
<span class="p_add">+	LRU_UNEVICTABLE,</span>
 	NR_LRU_LISTS
 };
 
 #define for_each_lru(lru) for (lru = 0; lru &lt; NR_LRU_LISTS; lru++)
<span class="p_del">-</span>
<span class="p_del">-#define for_each_evictable_lru(lru) for (lru = 0; lru &lt;= LRU_ACTIVE_FILE; lru++)</span>
<span class="p_add">+#define for_each_anon_file_lru(lru) \</span>
<span class="p_add">+		for (lru = 0; lru &lt;= LRU_ACTIVE_FILE; lru++)</span>
<span class="p_add">+#define for_each_evictable_lru(lru) for (lru = 0; lru &lt;= LRU_LZFREE; lru++)</span>
 
 static inline int is_file_lru(enum lru_list lru)
 {
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index f0310eeab3ec..73bcdc9d0e88 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -330,6 +330,7 @@</span> <span class="p_context"> extern unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *mem,</span>
 						unsigned long *nr_scanned);
 extern unsigned long shrink_all_memory(unsigned long nr_pages);
 extern int vm_swappiness;
<span class="p_add">+extern int vm_lazyfreeness;</span>
 extern int remove_mapping(struct address_space *mapping, struct page *page);
 extern unsigned long vm_total_pages;
 
<span class="p_chunk">@@ -361,11 +362,25 @@</span> <span class="p_context"> static inline int mem_cgroup_swappiness(struct mem_cgroup *memcg)</span>
 	return memcg-&gt;swappiness;
 }
 
<span class="p_add">+static inline int mem_cgroup_lzfreeness(struct mem_cgroup *memcg)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* root ? */</span>
<span class="p_add">+	if (mem_cgroup_disabled() || !memcg-&gt;css.parent)</span>
<span class="p_add">+		return vm_lazyfreeness;</span>
<span class="p_add">+</span>
<span class="p_add">+	return memcg-&gt;lzfreeness;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #else
 static inline int mem_cgroup_swappiness(struct mem_cgroup *mem)
 {
 	return vm_swappiness;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline int mem_cgroup_lzfreeness(struct mem_cgroup *mem)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return vm_lazyfreeness;</span>
<span class="p_add">+}</span>
 #endif
 #ifdef CONFIG_MEMCG_SWAP
 extern void mem_cgroup_swapout(struct page *page, swp_entry_t entry);
<span class="p_header">diff --git a/kernel/sysctl.c b/kernel/sysctl.c</span>
<span class="p_header">index e69201d8094e..2496b10c08e9 100644</span>
<span class="p_header">--- a/kernel/sysctl.c</span>
<span class="p_header">+++ b/kernel/sysctl.c</span>
<span class="p_chunk">@@ -1268,6 +1268,15 @@</span> <span class="p_context"> static struct ctl_table vm_table[] = {</span>
 		.extra1		= &amp;zero,
 		.extra2		= &amp;one_hundred,
 	},
<span class="p_add">+	{</span>
<span class="p_add">+		.procname	= &quot;lazyfreeness&quot;,</span>
<span class="p_add">+		.data		= &amp;vm_lazyfreeness,</span>
<span class="p_add">+		.maxlen		= sizeof(vm_lazyfreeness),</span>
<span class="p_add">+		.mode		= 0644,</span>
<span class="p_add">+		.proc_handler	= proc_dointvec_minmax,</span>
<span class="p_add">+		.extra1		= &amp;zero,</span>
<span class="p_add">+		.extra2		= &amp;one_hundred,</span>
<span class="p_add">+	},</span>
 #ifdef CONFIG_HUGETLB_PAGE
 	{
 		.procname	= &quot;nr_hugepages&quot;,
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 1dc599ce1bcb..5bdbe2a20dc0 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -108,8 +108,8 @@</span> <span class="p_context"> static const char * const mem_cgroup_lru_names[] = {</span>
 	&quot;active_anon&quot;,
 	&quot;inactive_file&quot;,
 	&quot;active_file&quot;,
<span class="p_del">-	&quot;unevictable&quot;,</span>
 	&quot;lazyfree&quot;,
<span class="p_add">+	&quot;unevictable&quot;,</span>
 };
 
 #define THRESHOLDS_EVENTS_TARGET 128
<span class="p_chunk">@@ -3288,6 +3288,30 @@</span> <span class="p_context"> static int mem_cgroup_swappiness_write(struct cgroup_subsys_state *css,</span>
 	return 0;
 }
 
<span class="p_add">+static u64 mem_cgroup_lzfreeness_read(struct cgroup_subsys_state *css,</span>
<span class="p_add">+				      struct cftype *cft)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mem_cgroup *memcg = mem_cgroup_from_css(css);</span>
<span class="p_add">+</span>
<span class="p_add">+	return mem_cgroup_lzfreeness(memcg);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int mem_cgroup_lzfreeness_write(struct cgroup_subsys_state *css,</span>
<span class="p_add">+				       struct cftype *cft, u64 val)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mem_cgroup *memcg = mem_cgroup_from_css(css);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (val &gt; 100)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (css-&gt;parent)</span>
<span class="p_add">+		memcg-&gt;lzfreeness = val;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		vm_lazyfreeness = val;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void __mem_cgroup_threshold(struct mem_cgroup *memcg, bool swap)
 {
 	struct mem_cgroup_threshold_ary *t;
<span class="p_chunk">@@ -4085,6 +4109,11 @@</span> <span class="p_context"> static struct cftype mem_cgroup_legacy_files[] = {</span>
 		.write_u64 = mem_cgroup_swappiness_write,
 	},
 	{
<span class="p_add">+		.name = &quot;lazyfreeness&quot;,</span>
<span class="p_add">+		.read_u64 = mem_cgroup_lzfreeness_read,</span>
<span class="p_add">+		.write_u64 = mem_cgroup_lzfreeness_write,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	{</span>
 		.name = &quot;move_charge_at_immigrate&quot;,
 		.read_u64 = mem_cgroup_move_charge_read,
 		.write_u64 = mem_cgroup_move_charge_write,
<span class="p_chunk">@@ -4305,6 +4334,7 @@</span> <span class="p_context"> mem_cgroup_css_online(struct cgroup_subsys_state *css)</span>
 	memcg-&gt;use_hierarchy = parent-&gt;use_hierarchy;
 	memcg-&gt;oom_kill_disable = parent-&gt;oom_kill_disable;
 	memcg-&gt;swappiness = mem_cgroup_swappiness(parent);
<span class="p_add">+	memcg-&gt;lzfreeness = mem_cgroup_lzfreeness(parent);</span>
 
 	if (parent-&gt;use_hierarchy) {
 		page_counter_init(&amp;memcg-&gt;memory, &amp;parent-&gt;memory);
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index cd65db9d3004..f1abc8a6ca31 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -141,6 +141,10 @@</span> <span class="p_context"> struct scan_control {</span>
  */
 int vm_swappiness = 60;
 /*
<span class="p_add">+ * From 0 .. 100.  Higher means more lazy freeing.</span>
<span class="p_add">+ */</span>
<span class="p_add">+int vm_lazyfreeness = 20;</span>
<span class="p_add">+/*</span>
  * The total number of pages which are beyond the high watermark within all
  * zones.
  */
<span class="p_chunk">@@ -2012,10 +2016,11 @@</span> <span class="p_context"> enum scan_balance {</span>
  *
  * nr[0] = anon inactive pages to scan; nr[1] = anon active pages to scan
  * nr[2] = file inactive pages to scan; nr[3] = file active pages to scan
<span class="p_add">+ * nr[4] = lazy free pages to scan;</span>
  */
 static void get_scan_count(struct lruvec *lruvec, int swappiness,
<span class="p_del">-			   struct scan_control *sc, unsigned long *nr,</span>
<span class="p_del">-			   unsigned long *lru_pages)</span>
<span class="p_add">+			int lzfreeness, struct scan_control *sc,</span>
<span class="p_add">+			unsigned long *nr, unsigned long *lru_pages)</span>
 {
 	struct zone_reclaim_stat *reclaim_stat = &amp;lruvec-&gt;reclaim_stat;
 	u64 fraction[2];
<span class="p_chunk">@@ -2023,12 +2028,13 @@</span> <span class="p_context"> static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
 	struct zone *zone = lruvec_zone(lruvec);
 	unsigned long anon_prio, file_prio;
 	enum scan_balance scan_balance;
<span class="p_del">-	unsigned long anon, file;</span>
<span class="p_add">+	unsigned long anon, file, lzfree;</span>
 	bool force_scan = false;
 	unsigned long ap, fp;
 	enum lru_list lru;
 	bool some_scanned;
 	int pass;
<span class="p_add">+	unsigned long scan_lzfree = 0;</span>
 
 	/*
 	 * If the zone or memcg is small, nr[l] can be 0.  This
<span class="p_chunk">@@ -2166,7 +2172,7 @@</span> <span class="p_context"> static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
 	/* Only use force_scan on second pass. */
 	for (pass = 0; !some_scanned &amp;&amp; pass &lt; 2; pass++) {
 		*lru_pages = 0;
<span class="p_del">-		for_each_evictable_lru(lru) {</span>
<span class="p_add">+		for_each_anon_file_lru(lru) {</span>
 			int file = is_file_lru(lru);
 			unsigned long size;
 			unsigned long scan;
<span class="p_chunk">@@ -2212,6 +2218,28 @@</span> <span class="p_context"> static void get_scan_count(struct lruvec *lruvec, int swappiness,</span>
 			some_scanned |= !!scan;
 		}
 	}
<span class="p_add">+</span>
<span class="p_add">+	lzfree = get_lru_size(lruvec, LRU_LZFREE);</span>
<span class="p_add">+	if (lzfree) {</span>
<span class="p_add">+		scan_lzfree = sc-&gt;nr_to_reclaim *</span>
<span class="p_add">+				(DEF_PRIORITY - sc-&gt;priority);</span>
<span class="p_add">+		scan_lzfree = div64_u64(scan_lzfree *</span>
<span class="p_add">+					lzfreeness, 50);</span>
<span class="p_add">+		if (!scan_lzfree) {</span>
<span class="p_add">+			unsigned long zonefile, zonefree;</span>
<span class="p_add">+</span>
<span class="p_add">+			zonefree = zone_page_state(zone, NR_FREE_PAGES);</span>
<span class="p_add">+			zonefile = zone_page_state(zone, NR_ACTIVE_FILE) +</span>
<span class="p_add">+				zone_page_state(zone, NR_INACTIVE_FILE);</span>
<span class="p_add">+			if (unlikely(zonefile + zonefree &lt;=</span>
<span class="p_add">+					high_wmark_pages(zone))) {</span>
<span class="p_add">+				scan_lzfree = get_lru_size(lruvec,</span>
<span class="p_add">+						LRU_LZFREE) &gt;&gt; sc-&gt;priority;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	nr[LRU_LZFREE] = min(scan_lzfree, lzfree);</span>
 }
 
 #ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
<span class="p_chunk">@@ -2235,23 +2263,22 @@</span> <span class="p_context"> static inline void init_tlb_ubc(void)</span>
  * This is a basic per-zone page freer.  Used by both kswapd and direct reclaim.
  */
 static void shrink_lruvec(struct lruvec *lruvec, int swappiness,
<span class="p_del">-			  struct scan_control *sc, unsigned long *lru_pages)</span>
<span class="p_add">+			int lzfreeness, struct scan_control *sc,</span>
<span class="p_add">+			unsigned long *lru_pages)</span>
 {
 	unsigned long nr[NR_LRU_LISTS];
 	unsigned long targets[NR_LRU_LISTS];
 	unsigned long nr_to_scan;
<span class="p_del">-	unsigned long nr_to_scan_lzfree;</span>
 	enum lru_list lru;
 	unsigned long nr_reclaimed = 0;
 	unsigned long nr_to_reclaim = sc-&gt;nr_to_reclaim;
 	struct blk_plug plug;
 	bool scan_adjusted;
 
<span class="p_del">-	get_scan_count(lruvec, swappiness, sc, nr, lru_pages);</span>
<span class="p_add">+	get_scan_count(lruvec, swappiness, lzfreeness, sc, nr, lru_pages);</span>
 
 	/* Record the original scan target for proportional adjustments later */
 	memcpy(targets, nr, sizeof(nr));
<span class="p_del">-	nr_to_scan_lzfree = get_lru_size(lruvec, LRU_LZFREE);</span>
 
 	/*
 	 * Global reclaiming within direct reclaim at DEF_PRIORITY is a normal
<span class="p_chunk">@@ -2269,22 +2296,9 @@</span> <span class="p_context"> static void shrink_lruvec(struct lruvec *lruvec, int swappiness,</span>
 
 	init_tlb_ubc();
 
<span class="p_del">-	while (nr_to_scan_lzfree) {</span>
<span class="p_del">-		nr_to_scan = min(nr_to_scan_lzfree, SWAP_CLUSTER_MAX);</span>
<span class="p_del">-		nr_to_scan_lzfree -= nr_to_scan;</span>
<span class="p_del">-</span>
<span class="p_del">-		nr_reclaimed += shrink_inactive_list(nr_to_scan, lruvec,</span>
<span class="p_del">-						sc, LRU_LZFREE);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	if (nr_reclaimed &gt;= nr_to_reclaim) {</span>
<span class="p_del">-		sc-&gt;nr_reclaimed += nr_reclaimed;</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	blk_start_plug(&amp;plug);
 	while (nr[LRU_INACTIVE_ANON] || nr[LRU_ACTIVE_FILE] ||
<span class="p_del">-					nr[LRU_INACTIVE_FILE]) {</span>
<span class="p_add">+		nr[LRU_INACTIVE_FILE] || nr[LRU_LZFREE]) {</span>
 		unsigned long nr_anon, nr_file, percentage;
 		unsigned long nr_scanned;
 
<span class="p_chunk">@@ -2466,7 +2480,7 @@</span> <span class="p_context"> static bool shrink_zone(struct zone *zone, struct scan_control *sc,</span>
 			unsigned long lru_pages;
 			unsigned long scanned;
 			struct lruvec *lruvec;
<span class="p_del">-			int swappiness;</span>
<span class="p_add">+			int swappiness, lzfreeness;</span>
 
 			if (mem_cgroup_low(root, memcg)) {
 				if (!sc-&gt;may_thrash)
<span class="p_chunk">@@ -2476,9 +2490,11 @@</span> <span class="p_context"> static bool shrink_zone(struct zone *zone, struct scan_control *sc,</span>
 
 			lruvec = mem_cgroup_zone_lruvec(zone, memcg);
 			swappiness = mem_cgroup_swappiness(memcg);
<span class="p_add">+			lzfreeness = mem_cgroup_lzfreeness(memcg);</span>
 			scanned = sc-&gt;nr_scanned;
 
<span class="p_del">-			shrink_lruvec(lruvec, swappiness, sc, &amp;lru_pages);</span>
<span class="p_add">+			shrink_lruvec(lruvec, swappiness, lzfreeness,</span>
<span class="p_add">+					sc, &amp;lru_pages);</span>
 			zone_lru_pages += lru_pages;
 
 			if (memcg &amp;&amp; is_classzone)
<span class="p_chunk">@@ -2944,6 +2960,7 @@</span> <span class="p_context"> unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *memcg,</span>
 	};
 	struct lruvec *lruvec = mem_cgroup_zone_lruvec(zone, memcg);
 	int swappiness = mem_cgroup_swappiness(memcg);
<span class="p_add">+	int lzfreeness = mem_cgroup_lzfreeness(memcg);</span>
 	unsigned long lru_pages;
 
 	sc.gfp_mask = (gfp_mask &amp; GFP_RECLAIM_MASK) |
<span class="p_chunk">@@ -2960,7 +2977,7 @@</span> <span class="p_context"> unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *memcg,</span>
 	 * will pick up pages from other mem cgroup&#39;s as well. We hack
 	 * the priority and make it zero.
 	 */
<span class="p_del">-	shrink_lruvec(lruvec, swappiness, &amp;sc, &amp;lru_pages);</span>
<span class="p_add">+	shrink_lruvec(lruvec, swappiness, lzfreeness, &amp;sc, &amp;lru_pages);</span>
 
 	trace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);
 
<span class="p_header">diff --git a/mm/vmstat.c b/mm/vmstat.c</span>
<span class="p_header">index df95d9473bba..43effd0374d9 100644</span>
<span class="p_header">--- a/mm/vmstat.c</span>
<span class="p_header">+++ b/mm/vmstat.c</span>
<span class="p_chunk">@@ -703,8 +703,8 @@</span> <span class="p_context"> const char * const vmstat_text[] = {</span>
 	&quot;nr_active_anon&quot;,
 	&quot;nr_inactive_file&quot;,
 	&quot;nr_active_file&quot;,
<span class="p_del">-	&quot;nr_unevictable&quot;,</span>
 	&quot;nr_lazyfree&quot;,
<span class="p_add">+	&quot;nr_unevictable&quot;,</span>
 	&quot;nr_mlock&quot;,
 	&quot;nr_anon_pages&quot;,
 	&quot;nr_mapped&quot;,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



