
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,11/11] x86/mm: Try to preserve old TLB entries using PCID - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,11/11] x86/mm: Try to preserve old TLB entries using PCID</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 21, 2017, 1:38 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;alpine.DEB.2.20.1706211159430.2328@nanos&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9801881/mbox/"
   >mbox</a>
|
   <a href="/patch/9801881/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9801881/">/patch/9801881/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	AB31660329 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Jun 2017 13:38:28 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9AEE72842B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Jun 2017 13:38:28 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 8F4B7285DA; Wed, 21 Jun 2017 13:38:28 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 802642842B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 21 Jun 2017 13:38:27 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751744AbdFUNiU (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 21 Jun 2017 09:38:20 -0400
Received: from Galois.linutronix.de ([146.0.238.70]:50038 &quot;EHLO
	Galois.linutronix.de&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751138AbdFUNiS (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 21 Jun 2017 09:38:18 -0400
Received: from localhost ([127.0.0.1]) by Galois.linutronix.de with esmtps
	(TLS1.2:DHE_RSA_AES_256_CBC_SHA256:256) (Exim 4.80)
	(envelope-from &lt;tglx@linutronix.de&gt;)
	id 1dNfoq-0008G1-FC; Wed, 21 Jun 2017 15:37:16 +0200
Date: Wed, 21 Jun 2017 15:38:06 +0200 (CEST)
From: Thomas Gleixner &lt;tglx@linutronix.de&gt;
To: Andy Lutomirski &lt;luto@kernel.org&gt;
cc: x86@kernel.org, linux-kernel@vger.kernel.org,
	Borislav Petkov &lt;bp@alien8.de&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	&quot;linux-mm@kvack.org&quot; &lt;linux-mm@kvack.org&gt;,
	Nadav Amit &lt;nadav.amit@gmail.com&gt;, Rik van Riel &lt;riel@redhat.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Arjan van de Ven &lt;arjan@linux.intel.com&gt;,
	Peter Zijlstra &lt;peterz@infradead.org&gt;
Subject: Re: [PATCH v3 11/11] x86/mm: Try to preserve old TLB entries using
	PCID
In-Reply-To: &lt;a8cdfbbb17785aed10980d24692745f68615a584.1498022414.git.luto@kernel.org&gt;
Message-ID: &lt;alpine.DEB.2.20.1706211159430.2328@nanos&gt;
References: &lt;cover.1498022414.git.luto@kernel.org&gt;
	&lt;a8cdfbbb17785aed10980d24692745f68615a584.1498022414.git.luto@kernel.org&gt;
User-Agent: Alpine 2.20 (DEB 67 2015-01-07)
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - June 21, 2017, 1:38 p.m.</div>
<pre class="content">
On Tue, 20 Jun 2017, Andy Lutomirski wrote:
<span class="quote">&gt; This patch uses PCID differently.  We use a PCID to identify a</span>
<span class="quote">&gt; recently-used mm on a per-cpu basis.  An mm has no fixed PCID</span>
<span class="quote">&gt; binding at all; instead, we give it a fresh PCID each time it&#39;s</span>
<span class="quote">&gt; loaded except in cases where we want to preserve the TLB, in which</span>
<span class="quote">&gt; case we reuse a recent value.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This seems to save about 100ns on context switches between mms.</span>

Depending on the work load I assume. For a CPU switching between a large
number of processes consecutively it won&#39;t make a change. In fact it will
be slower due to the extra few cycles required for rotating the asid, but I
doubt that this can be measured.
<span class="quote"> 
&gt; +/*</span>
<span class="quote">&gt; + * 6 because 6 should be plenty and struct tlb_state will fit in</span>
<span class="quote">&gt; + * two cache lines.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#define NR_DYNAMIC_ASIDS 6</span>

That requires a conditional branch 

	if (asid &gt;= NR_DYNAMIC_ASIDS) {
		asid = 0;
		....
	}

The question is whether 4 IDs would be sufficient which trades the branch
for a mask operation. Or you go for 8 and spend another cache line.
<span class="quote">
&gt;  atomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,</span>
<span class="quote">&gt; +			    u16 *new_asid, bool *need_flush)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	u16 asid;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_cpu_has(X86_FEATURE_PCID)) {</span>
<span class="quote">&gt; +		*new_asid = 0;</span>
<span class="quote">&gt; +		*need_flush = true;</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	for (asid = 0; asid &lt; NR_DYNAMIC_ASIDS; asid++) {</span>
<span class="quote">&gt; +		if (this_cpu_read(cpu_tlbstate.ctxs[asid].ctx_id) !=</span>
<span class="quote">&gt; +		    next-&gt;context.ctx_id)</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		*new_asid = asid;</span>
<span class="quote">&gt; +		*need_flush = (this_cpu_read(cpu_tlbstate.ctxs[asid].tlb_gen) &lt;</span>
<span class="quote">&gt; +			       next_tlb_gen);</span>
<span class="quote">&gt; +		return;</span>
<span class="quote">&gt; +	}</span>

Hmm. So this loop needs to be taken unconditionally even if the task stays
on the same CPU. And of course the number of dynamic IDs has to be short in
order to makes this loop suck performance wise.

Something like the completely disfunctional below might be worthwhile to
explore. At least arch/x86/mm/ compiles :)

It gets rid of the loop search and lifts the limit of dynamic ids by
trading it with a percpu variable in mm_context_t.

Thanks,

	tglx

8&lt;--------------------
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - June 21, 2017, 1:40 p.m.</div>
<pre class="content">
On Wed, 21 Jun 2017, Thomas Gleixner wrote:
<span class="quote">&gt; &gt; +	for (asid = 0; asid &lt; NR_DYNAMIC_ASIDS; asid++) {</span>
<span class="quote">&gt; &gt; +		if (this_cpu_read(cpu_tlbstate.ctxs[asid].ctx_id) !=</span>
<span class="quote">&gt; &gt; +		    next-&gt;context.ctx_id)</span>
<span class="quote">&gt; &gt; +			continue;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		*new_asid = asid;</span>
<span class="quote">&gt; &gt; +		*need_flush = (this_cpu_read(cpu_tlbstate.ctxs[asid].tlb_gen) &lt;</span>
<span class="quote">&gt; &gt; +			       next_tlb_gen);</span>
<span class="quote">&gt; &gt; +		return;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm. So this loop needs to be taken unconditionally even if the task stays</span>
<span class="quote">&gt; on the same CPU. And of course the number of dynamic IDs has to be short in</span>
<span class="quote">&gt; order to makes this loop suck performance wise.</span>

 ...  not suck ...
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - June 22, 2017, 2:57 a.m.</div>
<pre class="content">
On Wed, Jun 21, 2017 at 6:38 AM, Thomas Gleixner &lt;tglx@linutronix.de&gt; wrote:
<span class="quote">&gt; On Tue, 20 Jun 2017, Andy Lutomirski wrote:</span>
<span class="quote">&gt;&gt; This patch uses PCID differently.  We use a PCID to identify a</span>
<span class="quote">&gt;&gt; recently-used mm on a per-cpu basis.  An mm has no fixed PCID</span>
<span class="quote">&gt;&gt; binding at all; instead, we give it a fresh PCID each time it&#39;s</span>
<span class="quote">&gt;&gt; loaded except in cases where we want to preserve the TLB, in which</span>
<span class="quote">&gt;&gt; case we reuse a recent value.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This seems to save about 100ns on context switches between mms.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Depending on the work load I assume. For a CPU switching between a large</span>
<span class="quote">&gt; number of processes consecutively it won&#39;t make a change. In fact it will</span>
<span class="quote">&gt; be slower due to the extra few cycles required for rotating the asid, but I</span>
<span class="quote">&gt; doubt that this can be measured.</span>

True.  I suspect this can be improved -- see below.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt; +/*</span>
<span class="quote">&gt;&gt; + * 6 because 6 should be plenty and struct tlb_state will fit in</span>
<span class="quote">&gt;&gt; + * two cache lines.</span>
<span class="quote">&gt;&gt; + */</span>
<span class="quote">&gt;&gt; +#define NR_DYNAMIC_ASIDS 6</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That requires a conditional branch</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (asid &gt;= NR_DYNAMIC_ASIDS) {</span>
<span class="quote">&gt;                 asid = 0;</span>
<span class="quote">&gt;                 ....</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The question is whether 4 IDs would be sufficient which trades the branch</span>
<span class="quote">&gt; for a mask operation. Or you go for 8 and spend another cache line.</span>

Interesting.  I&#39;m inclined to either leave it at 6 or reduce it to 4
for now and to optimize later.
<span class="quote">
&gt;</span>
<span class="quote">&gt;&gt;  atomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; +static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,</span>
<span class="quote">&gt;&gt; +                         u16 *new_asid, bool *need_flush)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +     u16 asid;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     if (!static_cpu_has(X86_FEATURE_PCID)) {</span>
<span class="quote">&gt;&gt; +             *new_asid = 0;</span>
<span class="quote">&gt;&gt; +             *need_flush = true;</span>
<span class="quote">&gt;&gt; +             return;</span>
<span class="quote">&gt;&gt; +     }</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +     for (asid = 0; asid &lt; NR_DYNAMIC_ASIDS; asid++) {</span>
<span class="quote">&gt;&gt; +             if (this_cpu_read(cpu_tlbstate.ctxs[asid].ctx_id) !=</span>
<span class="quote">&gt;&gt; +                 next-&gt;context.ctx_id)</span>
<span class="quote">&gt;&gt; +                     continue;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +             *new_asid = asid;</span>
<span class="quote">&gt;&gt; +             *need_flush = (this_cpu_read(cpu_tlbstate.ctxs[asid].tlb_gen) &lt;</span>
<span class="quote">&gt;&gt; +                            next_tlb_gen);</span>
<span class="quote">&gt;&gt; +             return;</span>
<span class="quote">&gt;&gt; +     }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Hmm. So this loop needs to be taken unconditionally even if the task stays</span>
<span class="quote">&gt; on the same CPU. And of course the number of dynamic IDs has to be short in</span>
<span class="quote">&gt; order to makes this loop suck performance wise.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Something like the completely disfunctional below might be worthwhile to</span>
<span class="quote">&gt; explore. At least arch/x86/mm/ compiles :)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It gets rid of the loop search and lifts the limit of dynamic ids by</span>
<span class="quote">&gt; trading it with a percpu variable in mm_context_t.</span>

That would work, but it would take a lot more memory on large systems
with lots of processes, and I&#39;d also be concerned that we might run
out of dynamic percpu space.

How about a different idea: make the percpu data structure look like a
4-way set associative cache.  The ctxs array could be, say, 1024
entries long without using crazy amounts of memory.  We&#39;d divide it
into 256 buckets, so you&#39;d index it like ctxs[4*bucket + slot].  For
each mm, we choose a random bucket (from 0 through 256), and then we&#39;d
just loop over the four slots in the bucket in choose_asid().  This
would require very slightly more arithmetic (I&#39;d guess only one or two
cycles, though) but, critically, wouldn&#39;t touch any more cachelines.

The downside of both of these approaches over the one in this patch is
that the change that the percpu cacheline we need is not in the cache
is quite a bit higher since it&#39;s potentially a different cacheline for
each mm.  It would probably still be a win because avoiding the flush
is really quite valuable.

What do you think?  The added code would be tiny.

(P.S. Why doesn&#39;t random_p32() try arch_random_int()?)

--Andy
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">--- a/arch/x86/include/asm/mmu.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu.h</span>
<span class="p_chunk">@@ -25,6 +25,8 @@</span> <span class="p_context"> typedef struct {</span>
 	 */
 	atomic64_t tlb_gen;
 
<span class="p_add">+	u32 __percpu	*asids;</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MODIFY_LDT_SYSCALL
 	struct ldt_struct *ldt;
 #endif
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -156,11 +156,23 @@</span> <span class="p_context"> static inline void destroy_context(struc</span>
 	destroy_context_ldt(mm);
 }
 
<span class="p_del">-extern void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_del">-		      struct task_struct *tsk);</span>
<span class="p_add">+extern void __switch_mm(struct mm_struct *prev, struct mm_struct *next);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_add">+			     struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__switch_mm(prev, next);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern void __switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void switch_mm_irqs_off(struct mm_struct *prev,</span>
<span class="p_add">+				      struct mm_struct *next,</span>
<span class="p_add">+				      struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__switch_mm_irqs_off(prev, next);</span>
<span class="p_add">+}</span>
 
<span class="p_del">-extern void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_del">-			       struct task_struct *tsk);</span>
 #define switch_mm_irqs_off switch_mm_irqs_off
 
 #define activate_mm(prev, next)			\
<span class="p_chunk">@@ -299,6 +311,9 @@</span> <span class="p_context"> static inline unsigned long __get_curren</span>
 {
 	unsigned long cr3 = __pa(this_cpu_read(cpu_tlbstate.loaded_mm)-&gt;pgd);
 
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		cr3 |= this_cpu_read(cpu_tlbstate.loaded_mm_asid);</span>
<span class="p_add">+</span>
 	/* For now, be very restrictive about when this can be called. */
 	VM_WARN_ON(in_nmi() || !in_atomic());
 
<span class="p_header">--- a/arch/x86/include/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor-flags.h</span>
<span class="p_chunk">@@ -35,6 +35,7 @@</span> <span class="p_context"></span>
 /* Mask off the address space ID bits. */
 #define CR3_ADDR_MASK 0x7FFFFFFFFFFFF000ull
 #define CR3_PCID_MASK 0xFFFull
<span class="p_add">+#define CR3_NOFLUSH (1UL &lt;&lt; 63)</span>
 #else
 /*
  * CR3_ADDR_MASK needs at least bits 31:5 set on PAE systems, and we save
<span class="p_chunk">@@ -42,6 +43,7 @@</span> <span class="p_context"></span>
  */
 #define CR3_ADDR_MASK 0xFFFFFFFFull
 #define CR3_PCID_MASK 0ull
<span class="p_add">+#define CR3_NOFLUSH 0</span>
 #endif
 
 #endif /* _ASM_X86_PROCESSOR_FLAGS_H */
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -82,6 +82,15 @@</span> <span class="p_context"> static inline u64 bump_mm_tlb_gen(struct</span>
 #define __flush_tlb_single(addr) __native_flush_tlb_single(addr)
 #endif
 
<span class="p_add">+/*</span>
<span class="p_add">+ * NR_DYNAMIC_ASIDS must be a power of 2. 4 makes tlb_state fit into two</span>
<span class="p_add">+ * cache lines.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define NR_DYNAMIC_ASIDS_BITS	2</span>
<span class="p_add">+#define NR_DYNAMIC_ASIDS	(1U &lt;&lt; NR_DYNAMIC_ASIDS_BITS)</span>
<span class="p_add">+#define DYNAMIC_ASIDS_MASK	(NR_DYNAMIC_ASIDS - 1)</span>
<span class="p_add">+#define ASID_NEEDS_FLUSH	(1U &lt;&lt; 16)</span>
<span class="p_add">+</span>
 struct tlb_context {
 	u64 ctx_id;
 	u64 tlb_gen;
<span class="p_chunk">@@ -95,6 +104,8 @@</span> <span class="p_context"> struct tlb_state {</span>
 	 * mode even if we&#39;ve already switched back to swapper_pg_dir.
 	 */
 	struct mm_struct *loaded_mm;
<span class="p_add">+	u16 loaded_mm_asid;</span>
<span class="p_add">+	u16 next_asid;</span>
 
 	/*
 	 * Access to this CR4 shadow and to H/W CR4 is protected by
<span class="p_chunk">@@ -104,7 +115,8 @@</span> <span class="p_context"> struct tlb_state {</span>
 
 	/*
 	 * This is a list of all contexts that might exist in the TLB.
<span class="p_del">-	 * Since we don&#39;t yet use PCID, there is only one context.</span>
<span class="p_add">+	 * There is one per ASID that we use, and the ASID (what the</span>
<span class="p_add">+	 * CPU calls PCID) is the index into ctxts.</span>
 	 *
 	 * For each context, ctx_id indicates which mm the TLB&#39;s user
 	 * entries came from.  As an invariant, the TLB will never
<span class="p_chunk">@@ -114,8 +126,13 @@</span> <span class="p_context"> struct tlb_state {</span>
 	 * To be clear, this means that it&#39;s legal for the TLB code to
 	 * flush the TLB without updating tlb_gen.  This can happen
 	 * (for now, at least) due to paravirt remote flushes.
<span class="p_add">+	 *</span>
<span class="p_add">+	 * NB: context 0 is a bit special, since it&#39;s also used by</span>
<span class="p_add">+	 * various bits of init code.  This is fine -- code that</span>
<span class="p_add">+	 * isn&#39;t aware of PCID will end up harmlessly flushing</span>
<span class="p_add">+	 * context 0.</span>
 	 */
<span class="p_del">-	struct tlb_context ctxs[1];</span>
<span class="p_add">+	struct tlb_context ctxs[NR_DYNAMIC_ASIDS];</span>
 };
 DECLARE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate);
 
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -812,6 +812,7 @@</span> <span class="p_context"> void __init zone_sizes_init(void)</span>
 
 DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {
 	.loaded_mm = &amp;init_mm,
<span class="p_add">+	.next_asid = 1,</span>
 	.cr4 = ~0UL,	/* fail hard if we screw up cr4 shadow initialization */
 };
 EXPORT_SYMBOL_GPL(cpu_tlbstate);
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -30,6 +30,30 @@</span> <span class="p_context"></span>
 
 atomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);
 
<span class="p_add">+static u32 choose_new_asid(mm_context_t *mmctx, u64 next_tlb_gen)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u64 next_ctx_id = mmctx-&gt;ctx_id;</span>
<span class="p_add">+	struct tlb_context *ctx;</span>
<span class="p_add">+	u32 asid;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		return ASID_NEEDS_FLUSH;</span>
<span class="p_add">+</span>
<span class="p_add">+	asid = this_cpu_read(*mmctx-&gt;asids);</span>
<span class="p_add">+	ctx = this_cpu_ptr(cpu_tlbstate.ctxs + asid);</span>
<span class="p_add">+	if (likely(ctx-&gt;ctx_id == next_ctx_id)) {</span>
<span class="p_add">+		return ctx-&gt;tlb_gen == next_tlb_gen ?</span>
<span class="p_add">+			asid : asid | ASID_NEEDS_FLUSH ;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Slot got reused. Get a new one */</span>
<span class="p_add">+	asid = this_cpu_inc_return(cpu_tlbstate.next_asid) - 1;</span>
<span class="p_add">+	asid &amp;= DYNAMIC_ASIDS_MASK;</span>
<span class="p_add">+	this_cpu_write(*mmctx-&gt;asids, asid);</span>
<span class="p_add">+	ctx = this_cpu_ptr(cpu_tlbstate.ctxs + asid);</span>
<span class="p_add">+	return asid | ASID_NEEDS_FLUSH;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void leave_mm(int cpu)
 {
 	struct mm_struct *loaded_mm = this_cpu_read(cpu_tlbstate.loaded_mm);
<span class="p_chunk">@@ -48,24 +72,23 @@</span> <span class="p_context"> void leave_mm(int cpu)</span>
 	/* Warn if we&#39;re not lazy. */
 	WARN_ON(cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm)));
 
<span class="p_del">-	switch_mm(NULL, &amp;init_mm, NULL);</span>
<span class="p_add">+	__switch_mm(NULL, &amp;init_mm);</span>
 }
 
<span class="p_del">-void switch_mm(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_del">-	       struct task_struct *tsk)</span>
<span class="p_add">+void __switch_mm(struct mm_struct *prev, struct mm_struct *next)</span>
 {
 	unsigned long flags;
 
 	local_irq_save(flags);
<span class="p_del">-	switch_mm_irqs_off(prev, next, tsk);</span>
<span class="p_add">+	__switch_mm_irqs_off(prev, next);</span>
 	local_irq_restore(flags);
 }
 
<span class="p_del">-void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
<span class="p_del">-			struct task_struct *tsk)</span>
<span class="p_add">+void __switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next)</span>
 {
 	unsigned cpu = smp_processor_id();
 	struct mm_struct *real_prev = this_cpu_read(cpu_tlbstate.loaded_mm);
<span class="p_add">+	u16 prev_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);</span>
 	u64 next_tlb_gen;
 
 	/*
<span class="p_chunk">@@ -81,7 +104,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct</span>
 	if (IS_ENABLED(CONFIG_PROVE_LOCKING))
 		WARN_ON_ONCE(!irqs_disabled());
 
<span class="p_del">-	VM_BUG_ON(read_cr3_pa() != __pa(real_prev-&gt;pgd));</span>
<span class="p_add">+	VM_BUG_ON(__read_cr3() != (__pa(real_prev-&gt;pgd) | prev_asid));</span>
 
 	if (real_prev == next) {
 		if (cpumask_test_cpu(cpu, mm_cpumask(next))) {
<span class="p_chunk">@@ -98,10 +121,10 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct</span>
 		cpumask_set_cpu(cpu, mm_cpumask(next));
 		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);
 
<span class="p_del">-		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=</span>
<span class="p_add">+		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[prev_asid].ctx_id) !=</span>
 			  next-&gt;context.ctx_id);
 
<span class="p_del">-		if (this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen) &lt;</span>
<span class="p_add">+		if (this_cpu_read(cpu_tlbstate.ctxs[prev_asid].tlb_gen) &lt;</span>
 		    next_tlb_gen) {
 			/*
 			 * Ideally, we&#39;d have a flush_tlb() variant that
<span class="p_chunk">@@ -109,9 +132,9 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct</span>
 			 * be faster on Xen PV and on hypothetical CPUs
 			 * on which INVPCID is fast.
 			 */
<span class="p_del">-			this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="p_add">+			this_cpu_write(cpu_tlbstate.ctxs[prev_asid].tlb_gen,</span>
 				       next_tlb_gen);
<span class="p_del">-			write_cr3(__pa(next-&gt;pgd));</span>
<span class="p_add">+			write_cr3(__pa(next-&gt;pgd) | prev_asid);</span>
 			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH,
 					TLB_FLUSH_ALL);
 		}
<span class="p_chunk">@@ -122,6 +145,8 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct</span>
 		 * are not reflected in tlb_gen.)
 		 */
 	} else {
<span class="p_add">+		u32 new_asid;</span>
<span class="p_add">+</span>
 		if (IS_ENABLED(CONFIG_VMAP_STACK)) {
 			/*
 			 * If our current stack is in vmalloc space and isn&#39;t
<span class="p_chunk">@@ -141,7 +166,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct</span>
 		if (cpumask_test_cpu(cpu, mm_cpumask(real_prev)))
 			cpumask_clear_cpu(cpu, mm_cpumask(real_prev));
 
<span class="p_del">-		WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
<span class="p_add">+		VM_WARN_ON_ONCE(cpumask_test_cpu(cpu, mm_cpumask(next)));</span>
 
 		/*
 		 * Start remote flushes and then read tlb_gen.
<span class="p_chunk">@@ -149,17 +174,25 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct</span>
 		cpumask_set_cpu(cpu, mm_cpumask(next));
 		next_tlb_gen = atomic64_read(&amp;next-&gt;context.tlb_gen);
 
<span class="p_del">-		VM_BUG_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) ==</span>
<span class="p_del">-			  next-&gt;context.ctx_id);</span>
<span class="p_add">+		new_asid = choose_new_asid(&amp;next-&gt;context, next_tlb_gen);</span>
 
<span class="p_del">-		this_cpu_write(cpu_tlbstate.ctxs[0].ctx_id,</span>
<span class="p_del">-			       next-&gt;context.ctx_id);</span>
<span class="p_del">-		this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen,</span>
<span class="p_del">-			       next_tlb_gen);</span>
<span class="p_del">-		this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="p_del">-		write_cr3(__pa(next-&gt;pgd));</span>
<span class="p_add">+		if (new_asid &amp; ASID_NEEDS_FLUSH) {</span>
<span class="p_add">+			new_asid &amp;= DYNAMIC_ASIDS_MASK;</span>
<span class="p_add">+			this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id,</span>
<span class="p_add">+				       next-&gt;context.ctx_id);</span>
<span class="p_add">+			this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen,</span>
<span class="p_add">+				       next_tlb_gen);</span>
<span class="p_add">+			write_cr3(__pa(next-&gt;pgd) | new_asid);</span>
<span class="p_add">+			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH,</span>
<span class="p_add">+					TLB_FLUSH_ALL);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* The new ASID is already up to date. */</span>
<span class="p_add">+			write_cr3(__pa(next-&gt;pgd) | new_asid | CR3_NOFLUSH);</span>
<span class="p_add">+			trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, 0);</span>
<span class="p_add">+		}</span>
 
<span class="p_del">-		trace_tlb_flush(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.loaded_mm, next);</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.loaded_mm_asid, new_asid);</span>
 	}
 
 	load_mm_cr4(next);
<span class="p_chunk">@@ -170,6 +203,7 @@</span> <span class="p_context"> static void flush_tlb_func_common(const</span>
 				  bool local, enum tlb_flush_reason reason)
 {
 	struct mm_struct *loaded_mm = this_cpu_read(cpu_tlbstate.loaded_mm);
<span class="p_add">+	u32 loaded_mm_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);</span>
 
 	/*
 	 * Our memory ordering requirement is that any TLB fills that
<span class="p_chunk">@@ -179,12 +213,12 @@</span> <span class="p_context"> static void flush_tlb_func_common(const</span>
 	 * atomic64_read operation won&#39;t be reordered by the compiler.
 	 */
 	u64 mm_tlb_gen = atomic64_read(&amp;loaded_mm-&gt;context.tlb_gen);
<span class="p_del">-	u64 local_tlb_gen = this_cpu_read(cpu_tlbstate.ctxs[0].tlb_gen);</span>
<span class="p_add">+	u64 local_tlb_gen = this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen);</span>
 
 	/* This code cannot presently handle being reentered. */
 	VM_WARN_ON(!irqs_disabled());
 
<span class="p_del">-	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[0].ctx_id) !=</span>
<span class="p_add">+	VM_WARN_ON(this_cpu_read(cpu_tlbstate.ctxs[loaded_mm_asid].ctx_id) !=</span>
 		   loaded_mm-&gt;context.ctx_id);
 
 	if (!cpumask_test_cpu(smp_processor_id(), mm_cpumask(loaded_mm))) {
<span class="p_chunk">@@ -256,7 +290,7 @@</span> <span class="p_context"> static void flush_tlb_func_common(const</span>
 	}
 
 	/* Both paths above update our state to mm_tlb_gen. */
<span class="p_del">-	this_cpu_write(cpu_tlbstate.ctxs[0].tlb_gen, mm_tlb_gen);</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.ctxs[loaded_mm_asid].tlb_gen, mm_tlb_gen);</span>
 }
 
 static void flush_tlb_func_local(void *info, enum tlb_flush_reason reason)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



