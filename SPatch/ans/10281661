
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v1,2/4] ioremap: Invalidate TLB after huge mappings - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v1,2/4] ioremap: Invalidate TLB after huge mappings</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=103411">Chintan Pandya</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 14, 2018, 8:48 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1521017305-28518-3-git-send-email-cpandya@codeaurora.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10281661/mbox/"
   >mbox</a>
|
   <a href="/patch/10281661/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10281661/">/patch/10281661/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	AF25F60211 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 14 Mar 2018 08:49:56 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 9E17828791
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 14 Mar 2018 08:49:56 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 9152028794; Wed, 14 Mar 2018 08:49:56 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DC2B528791
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 14 Mar 2018 08:49:55 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932356AbeCNItv (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 14 Mar 2018 04:49:51 -0400
Received: from smtp.codeaurora.org ([198.145.29.96]:58634 &quot;EHLO
	smtp.codeaurora.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1753394AbeCNIsx (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 14 Mar 2018 04:48:53 -0400
Received: by smtp.codeaurora.org (Postfix, from userid 1000)
	id 7C925608CB; Wed, 14 Mar 2018 08:48:52 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=codeaurora.org;
	s=default; t=1521017332;
	bh=0cbaYAReSctfUKsE9ry3OLwsRwLHi76ohNnH7VdlyXA=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=b+288mJzf3AksEepNAiC+59gdA93SWkqUsKkQ9TpPcoqAiIEtyIf0BcziDwB/ovT7
	V4PEgmNStcB0mNsGxXfQIM5urFxB2xtAX4lZssvdvK/2VC46Sw7QXaZaFK26m+sxJR
	PUlcsuy5O8WcXCIffaSuE6ymX0n8al7CR2Ulx1eA=
Received: from cpandya-linux.qualcomm.com
	(blr-c-bdr-fw-01_globalnat_allzones-outside.qualcomm.com
	[103.229.19.19])
	(using TLSv1.2 with cipher ECDHE-RSA-AES128-SHA256 (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: cpandya@smtp.codeaurora.org)
	by smtp.codeaurora.org (Postfix) with ESMTPSA id 877C3608CB;
	Wed, 14 Mar 2018 08:48:46 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=codeaurora.org;
	s=default; t=1521017331;
	bh=0cbaYAReSctfUKsE9ry3OLwsRwLHi76ohNnH7VdlyXA=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=YczjPr3lQcLJmtqrTfjqq+nx4hRDOxkl9mhaTITykULOfH4lo14nTyaPFT6XxfyN+
	XtN/ZbGRUF/i5OxEn2Ly7qrFJZ0hHtuS2NzN5n97RgZD7ZkXXHAz7pr2QRPWUsWM7n
	va1MpsLAcdZtlHQTsoGDJkppqf5QHHEXWTj3MSv8=
DMARC-Filter: OpenDMARC Filter v1.3.2 smtp.codeaurora.org 877C3608CB
Authentication-Results: pdx-caf-mail.web.codeaurora.org;
	dmarc=none (p=none dis=none)
	header.from=codeaurora.org
Authentication-Results: pdx-caf-mail.web.codeaurora.org;
	spf=none smtp.mailfrom=cpandya@codeaurora.org
From: Chintan Pandya &lt;cpandya@codeaurora.org&gt;
To: catalin.marinas@arm.com, will.deacon@arm.com, arnd@arndb.de
Cc: mark.rutland@arm.com, ard.biesheuvel@linaro.org,
	marc.zyngier@arm.com, james.morse@arm.com,
	kristina.martsenko@arm.com, takahiro.akashi@linaro.org,
	gregkh@linuxfoundation.org, tglx@linutronix.de,
	linux-arm-kernel@lists.infradead.org, linux-kernel@vger.kernel.org,
	linux-arch@vger.kernel.org, akpm@linux-foundation.org,
	toshi.kani@hpe.com, Chintan Pandya &lt;cpandya@codeaurora.org&gt;
Subject: [PATCH v1 2/4] ioremap: Invalidate TLB after huge mappings
Date: Wed, 14 Mar 2018 14:18:23 +0530
Message-Id: &lt;1521017305-28518-3-git-send-email-cpandya@codeaurora.org&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1521017305-28518-1-git-send-email-cpandya@codeaurora.org&gt;
References: &lt;1521017305-28518-1-git-send-email-cpandya@codeaurora.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=103411">Chintan Pandya</a> - March 14, 2018, 8:48 a.m.</div>
<pre class="content">
If huge mappings are enabled, they can override
valid intermediate previous mappings. Some MMU
can speculatively pre-fetch these intermediate
entries even after unmap. That&#39;s because unmap
will clear only last level entries in page table
keeping intermediate (pud/pmd) entries still valid.

This can potentially lead to stale TLB entries
which needs invalidation after map.

Some more info: https://lkml.org/lkml/2017/12/23/3

There is one noted case for ARM64 where such stale
TLB entries causes 3rd level translation fault even
after correct (huge) mapping is available.

See the case below (reproduced locally with tests),

[17505.330123] Unable to handle kernel paging request at virtual address ffffff801ae00000
[17505.338100] pgd = ffffff800a761000
[17505.341566] [ffffff801ae00000] *pgd=000000017e1be003, *pud=000000017e1be003, *pmd=00e8000098000f05
[17505.350704] ------------[ cut here ]------------
[17505.355362] Kernel BUG at ffffff8008238c30 [verbose debug info unavailable]
[17505.362375] Internal error: Oops: 96000007 [#1] PREEMPT SMP
[17505.367996] Modules linked in:
[17505.371114] CPU: 6 PID: 488 Comm: chintan-ioremap Not tainted 4.9.81+ #160
[17505.378039] Hardware name: Qualcomm Technologies, Inc. SDM845 v1 MTP (DT)
[17505.384885] task: ffffffc0e3e61180 task.stack: ffffffc0e3e70000
[17505.390868] PC is at io_remap_test+0x2e0/0x444
[17505.395352] LR is at io_remap_test+0x2d0/0x444
[17505.399835] pc : [&lt;ffffff8008238c30&gt;] lr : [&lt;ffffff8008238c20&gt;] pstate: 60c00005
[17505.407282] sp : ffffffc0e3e73d70
[17505.410624] x29: ffffffc0e3e73d70 x28: ffffff801ae00008
[17505.416031] x27: ffffff801ae00010 x26: ffffff801ae00018
[17505.421436] x25: ffffff801ae00020 x24: ffffff801adfffe0
[17505.426840] x23: ffffff801adfffe8 x22: ffffff801adffff0
[17505.432244] x21: ffffff801adffff8 x20: ffffff801ae00000
[17505.437648] x19: 0000000000000005 x18: 0000000000000000
[17505.443052] x17: 00000000b3409452 x16: 00000000923da470
[17505.448456] x15: 0000000071c9763c x14: 00000000a15658fa
[17505.453860] x13: 000000005cae96bf x12: 00000000e6d5c44a
[17505.459264] x11: 0140000000000000 x10: ffffff80099a1000
[17505.464668] x9 : 0000000000000000 x8 : ffffffc0e3e73d68
[17505.470072] x7 : ffffff80099d3220 x6 : 0000000000000015
[17505.475476] x5 : 00000c00004ad32a x4 : 000000000000000a
[17505.480880] x3 : 000000000682aaab x2 : 0000001345c2ad2e
[17505.486284] x1 : 7d78d61de56639ba x0 : 0000000000000001

Hence, invalidate once we override pmd/pud with huge
mappings.
<span class="signed-off-by">
Signed-off-by: Chintan Pandya &lt;cpandya@codeaurora.org&gt;</span>
---
 lib/ioremap.c | 9 +++++++--
 1 file changed, 7 insertions(+), 2 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - March 14, 2018, 10:48 a.m.</div>
<pre class="content">
On Wed, Mar 14, 2018 at 02:18:23PM +0530, Chintan Pandya wrote:
<span class="quote">&gt; If huge mappings are enabled, they can override</span>
<span class="quote">&gt; valid intermediate previous mappings. Some MMU</span>
<span class="quote">&gt; can speculatively pre-fetch these intermediate</span>
<span class="quote">&gt; entries even after unmap. That&#39;s because unmap</span>
<span class="quote">&gt; will clear only last level entries in page table</span>
<span class="quote">&gt; keeping intermediate (pud/pmd) entries still valid.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This can potentially lead to stale TLB entries</span>
<span class="quote">&gt; which needs invalidation after map.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Some more info: https://lkml.org/lkml/2017/12/23/3</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; There is one noted case for ARM64 where such stale</span>
<span class="quote">&gt; TLB entries causes 3rd level translation fault even</span>
<span class="quote">&gt; after correct (huge) mapping is available.</span>
<span class="quote">
&gt; Hence, invalidate once we override pmd/pud with huge</span>
<span class="quote">&gt; mappings.</span>
<span class="quote">
&gt;  static int __read_mostly ioremap_p4d_capable;</span>
<span class="quote">&gt; @@ -92,8 +93,10 @@ static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,</span>
<span class="quote">&gt;  		if (ioremap_pmd_enabled() &amp;&amp;</span>
<span class="quote">&gt;  		    ((next - addr) == PMD_SIZE) &amp;&amp;</span>
<span class="quote">&gt;  		    IS_ALIGNED(phys_addr + addr, PMD_SIZE)) {</span>
<span class="quote">&gt; -			if (pmd_set_huge(pmd, phys_addr + addr, prot))</span>
<span class="quote">&gt; +			if (pmd_set_huge(pmd, phys_addr + addr, prot)) {</span>
<span class="quote">&gt; +				flush_tlb_pgtable(&amp;init_mm, addr);</span>
<span class="quote">&gt;  				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (ioremap_pte_range(pmd, addr, next, phys_addr + addr, prot))</span>
<span class="quote">&gt; @@ -118,8 +121,10 @@ static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,</span>
<span class="quote">&gt;  		if (ioremap_pud_enabled() &amp;&amp;</span>
<span class="quote">&gt;  		    ((next - addr) == PUD_SIZE) &amp;&amp;</span>
<span class="quote">&gt;  		    IS_ALIGNED(phys_addr + addr, PUD_SIZE)) {</span>
<span class="quote">&gt; -			if (pud_set_huge(pud, phys_addr + addr, prot))</span>
<span class="quote">&gt; +			if (pud_set_huge(pud, phys_addr + addr, prot)) {</span>
<span class="quote">&gt; +				flush_tlb_pgtable(&amp;init_mm, addr);</span>
<span class="quote">&gt;  				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		}</span>

As has been noted in previous threads, the ARM architecture requires a
Break-Before-Make sequence when changing an entry from a table to a
block, as is the case here.

The means the necessary sequence is:

	1. Make the entry invalid
	2. Invalidate relevant TLB entries
	3. Write the new entry

Whereas above, the sequence is

	1. Write the new entry
	2. invalidate relevant TLB entries

Which is insufficient, and will lead to a number of problems.

Therefore, NAK to this patch.

Please read up on the Break-Before-Make requirements in the ARM ARM.

Thanks,
Mark.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=103411">Chintan Pandya</a> - March 14, 2018, 11:20 a.m.</div>
<pre class="content">
On 3/14/2018 4:18 PM, Mark Rutland wrote:
<span class="quote">&gt; On Wed, Mar 14, 2018 at 02:18:23PM +0530, Chintan Pandya wrote:</span>
<span class="quote">&gt;&gt; If huge mappings are enabled, they can override</span>
<span class="quote">&gt;&gt; valid intermediate previous mappings. Some MMU</span>
<span class="quote">&gt;&gt; can speculatively pre-fetch these intermediate</span>
<span class="quote">&gt;&gt; entries even after unmap. That&#39;s because unmap</span>
<span class="quote">&gt;&gt; will clear only last level entries in page table</span>
<span class="quote">&gt;&gt; keeping intermediate (pud/pmd) entries still valid.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This can potentially lead to stale TLB entries</span>
<span class="quote">&gt;&gt; which needs invalidation after map.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Some more info: https://lkml.org/lkml/2017/12/23/3</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; There is one noted case for ARM64 where such stale</span>
<span class="quote">&gt;&gt; TLB entries causes 3rd level translation fault even</span>
<span class="quote">&gt;&gt; after correct (huge) mapping is available.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Hence, invalidate once we override pmd/pud with huge</span>
<span class="quote">&gt;&gt; mappings.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;   static int __read_mostly ioremap_p4d_capable;</span>
<span class="quote">&gt;&gt; @@ -92,8 +93,10 @@ static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,</span>
<span class="quote">&gt;&gt;   		if (ioremap_pmd_enabled() &amp;&amp;</span>
<span class="quote">&gt;&gt;   		    ((next - addr) == PMD_SIZE) &amp;&amp;</span>
<span class="quote">&gt;&gt;   		    IS_ALIGNED(phys_addr + addr, PMD_SIZE)) {</span>
<span class="quote">&gt;&gt; -			if (pmd_set_huge(pmd, phys_addr + addr, prot))</span>
<span class="quote">&gt;&gt; +			if (pmd_set_huge(pmd, phys_addr + addr, prot)) {</span>
<span class="quote">&gt;&gt; +				flush_tlb_pgtable(&amp;init_mm, addr);</span>
<span class="quote">&gt;&gt;   				continue;</span>
<span class="quote">&gt;&gt; +			}</span>
<span class="quote">&gt;&gt;   		}</span>
<span class="quote">&gt;&gt;   </span>
<span class="quote">&gt;&gt;   		if (ioremap_pte_range(pmd, addr, next, phys_addr + addr, prot))</span>
<span class="quote">&gt;&gt; @@ -118,8 +121,10 @@ static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,</span>
<span class="quote">&gt;&gt;   		if (ioremap_pud_enabled() &amp;&amp;</span>
<span class="quote">&gt;&gt;   		    ((next - addr) == PUD_SIZE) &amp;&amp;</span>
<span class="quote">&gt;&gt;   		    IS_ALIGNED(phys_addr + addr, PUD_SIZE)) {</span>
<span class="quote">&gt;&gt; -			if (pud_set_huge(pud, phys_addr + addr, prot))</span>
<span class="quote">&gt;&gt; +			if (pud_set_huge(pud, phys_addr + addr, prot)) {</span>
<span class="quote">&gt;&gt; +				flush_tlb_pgtable(&amp;init_mm, addr);</span>
<span class="quote">&gt;&gt;   				continue;</span>
<span class="quote">&gt;&gt; +			}</span>
<span class="quote">&gt;&gt;   		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As has been noted in previous threads, the ARM architecture requires a</span>
<span class="quote">&gt; Break-Before-Make sequence when changing an entry from a table to a</span>
<span class="quote">&gt; block, as is the case here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The means the necessary sequence is:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	1. Make the entry invalid</span>
<span class="quote">&gt; 	2. Invalidate relevant TLB entries</span>
<span class="quote">&gt; 	3. Write the new entry</span>
<span class="quote">&gt; </span>
We do this for PTEs. I don&#39;t see this applicable to PMDs. Because,

1) To mark any PMD invalid, we need to be sure that next level page
    table (I mean all the 512 PTEs) should be zero. That requires us
    to scan entire last level page. A big perf hit !
2) We need to perform step 1 for every unmap as we never know which
    unmap will make last level page table empty.

Moreover, problem comes only when 4K mapping was followed by 2M
mapping. In all other cases, retaining valid PMD has obvious perf
gain. That&#39;s what walk-cache is supposed to be introduced for.

So, I think to touch only problematic case and fix it with TLB
invalidate.
<span class="quote">
&gt; Whereas above, the sequence is</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	1. Write the new entry</span>
<span class="quote">&gt; 	2. invalidate relevant TLB entries</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Which is insufficient, and will lead to a number of problems.</span>
I couldn&#39;t think of new problems with this approach. Could you share
any problematic scenarios ?

Also, my test-case runs fine with these patches for 10+ hours.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Therefore, NAK to this patch.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Please read up on the Break-Before-Make requirements in the ARM ARM.</span>
Sure, will get more from here.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Thanks,</span>
<span class="quote">&gt; Mark.</span>
<span class="quote">&gt; </span>
Thanks for the review Mark.

Chintan
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=30282">Mark Rutland</a> - March 14, 2018, 11:48 a.m.</div>
<pre class="content">
On Wed, Mar 14, 2018 at 04:50:35PM +0530, Chintan Pandya wrote:
<span class="quote">&gt; On 3/14/2018 4:18 PM, Mark Rutland wrote:</span>
<span class="quote">&gt; &gt; On Wed, Mar 14, 2018 at 02:18:23PM +0530, Chintan Pandya wrote:</span>
<span class="quote">&gt; &gt; As has been noted in previous threads, the ARM architecture requires a</span>
<span class="quote">&gt; &gt; Break-Before-Make sequence when changing an entry from a table to a</span>
<span class="quote">&gt; &gt; block, as is the case here.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; The means the necessary sequence is:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	1. Make the entry invalid</span>
<span class="quote">&gt; &gt; 	2. Invalidate relevant TLB entries</span>
<span class="quote">&gt; &gt; 	3. Write the new entry</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; We do this for PTEs. I don&#39;t see this applicable to PMDs.</span>

The architecture requires this for *all* levels of page table, when
certain changes are made. Switching an entry from a table to block (or
vice versa) is one of those changes, and this definitely applies to
PMDs.
<span class="quote">
&gt; Because,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 1) To mark any PMD invalid, we need to be sure that next level page</span>
<span class="quote">&gt;    table (I mean all the 512 PTEs) should be zero. That requires us</span>
<span class="quote">&gt;    to scan entire last level page. A big perf hit !</span>

This is in ioremap code. Under what workload does this constitute a perf
hit?

Regardless, so long as we mark the pmd entry invalid before the TLB
invalidation, we don&#39;t need to touch the next level table at all. We
just require a sequence like:

	pmd_clear(*pmdp);
	flush_tlb_kernel_range(pmd_start_addr, pmd_end_addr);
	pmd_set_huge(*pmdp, phys, prot);
<span class="quote">

&gt; 2) We need to perform step 1 for every unmap as we never know which</span>
<span class="quote">&gt;    unmap will make last level page table empty.</span>

Sorry, I don&#39;t follow. Could you elaborate on the problem?
<span class="quote">
&gt; Moreover, problem comes only when 4K mapping was followed by 2M</span>
<span class="quote">&gt; mapping. In all other cases, retaining valid PMD has obvious perf</span>
<span class="quote">&gt; gain. That&#39;s what walk-cache is supposed to be introduced for.</span>

Retaining a valid PMD in the TLB that *differs* from a valid PMD in the
page tables is a big problem.

The architecture requires BBM, as this permits CPUs to allocate PMDs
into TLBs at *any* time, even if there&#39;s already PMD in the TLB for a
given address.

Thus, CPUs can allocate *both* valid PMDs into the TLBs. When this
happens, a TLB lookup can:

1) return either of the PMDs.

2) raise a TLB conflict abort.

3) return an amalgamation of the two entries (e.g. provide an erroneous
   address).

Note that (3) is particularly scary:

* The CPU could raise an SError if the amalgamated entry is junk.

* If a memory access hits an amalgamated entry, it may use the wrong
  physical address, attributes, or permissions, resulting in a number of
  potential problems.

* If the amalgamated entry looks like a partial walk, the TLB might try
  to perform a walk starting at the physical address in the amalgamated
  entry. This would cause page table walks to access bogus addresses,
  allocating junk into TLBs, and may result in SErrors or other aborts.
<span class="quote">
&gt; &gt; Whereas above, the sequence is</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; 	1. Write the new entry</span>
<span class="quote">&gt; &gt; 	2. invalidate relevant TLB entries</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Which is insufficient, and will lead to a number of problems.</span>
<span class="quote">&gt; I couldn&#39;t think of new problems with this approach. Could you share</span>
<span class="quote">&gt; any problematic scenarios ?</span>

Please see above.
<span class="quote">
&gt; Also, my test-case runs fine with these patches for 10+ hours.</span>

While this may happen to work on particular platforms, it is not
guaranteed per the architecture, and will fail on others.

Thanks,
Mark.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/lib/ioremap.c b/lib/ioremap.c</span>
<span class="p_header">index b808a39..c1e1341 100644</span>
<span class="p_header">--- a/lib/ioremap.c</span>
<span class="p_header">+++ b/lib/ioremap.c</span>
<span class="p_chunk">@@ -13,6 +13,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/export.h&gt;
 #include &lt;asm/cacheflush.h&gt;
 #include &lt;asm/pgtable.h&gt;
<span class="p_add">+#include &lt;asm-generic/tlb.h&gt;</span>
 
 #ifdef CONFIG_HAVE_ARCH_HUGE_VMAP
 static int __read_mostly ioremap_p4d_capable;
<span class="p_chunk">@@ -92,8 +93,10 @@</span> <span class="p_context"> static inline int ioremap_pmd_range(pud_t *pud, unsigned long addr,</span>
 		if (ioremap_pmd_enabled() &amp;&amp;
 		    ((next - addr) == PMD_SIZE) &amp;&amp;
 		    IS_ALIGNED(phys_addr + addr, PMD_SIZE)) {
<span class="p_del">-			if (pmd_set_huge(pmd, phys_addr + addr, prot))</span>
<span class="p_add">+			if (pmd_set_huge(pmd, phys_addr + addr, prot)) {</span>
<span class="p_add">+				flush_tlb_pgtable(&amp;init_mm, addr);</span>
 				continue;
<span class="p_add">+			}</span>
 		}
 
 		if (ioremap_pte_range(pmd, addr, next, phys_addr + addr, prot))
<span class="p_chunk">@@ -118,8 +121,10 @@</span> <span class="p_context"> static inline int ioremap_pud_range(p4d_t *p4d, unsigned long addr,</span>
 		if (ioremap_pud_enabled() &amp;&amp;
 		    ((next - addr) == PUD_SIZE) &amp;&amp;
 		    IS_ALIGNED(phys_addr + addr, PUD_SIZE)) {
<span class="p_del">-			if (pud_set_huge(pud, phys_addr + addr, prot))</span>
<span class="p_add">+			if (pud_set_huge(pud, phys_addr + addr, prot)) {</span>
<span class="p_add">+				flush_tlb_pgtable(&amp;init_mm, addr);</span>
 				continue;
<span class="p_add">+			}</span>
 		}
 
 		if (ioremap_pmd_range(pud, addr, next, phys_addr + addr, prot))

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



