
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,PTI,3/3] x86/pti: Put the LDT in its own PGD if PTI is on - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,PTI,3/3] x86/pti: Put the LDT in its own PGD if PTI is on</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 8, 2017, 7:59 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;04e1697bd6ba902ea19c91f43b40dcd057e2745b.1512763129.git.luto@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10103233/mbox/"
   >mbox</a>
|
   <a href="/patch/10103233/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10103233/">/patch/10103233/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	EBAD060325 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  8 Dec 2017 19:59:33 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id DB51628F24
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  8 Dec 2017 19:59:33 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id CE82528F26; Fri,  8 Dec 2017 19:59:33 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EE5E528F24
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri,  8 Dec 2017 19:59:32 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753080AbdLHT7a (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 8 Dec 2017 14:59:30 -0500
Received: from mail.kernel.org ([198.145.29.99]:41198 &quot;EHLO mail.kernel.org&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1752415AbdLHT7R (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 8 Dec 2017 14:59:17 -0500
Received: from localhost (c-71-202-137-17.hsd1.ca.comcast.net
	[71.202.137.17])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mail.kernel.org (Postfix) with ESMTPSA id 4BD8D2197D;
	Fri,  8 Dec 2017 19:59:16 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mail.kernel.org 4BD8D2197D
Authentication-Results: mail.kernel.org;
	dmarc=none (p=none dis=none) header.from=kernel.org
Authentication-Results: mail.kernel.org;
	spf=none smtp.mailfrom=luto@kernel.org
From: Andy Lutomirski &lt;luto@kernel.org&gt;
To: x86@kernel.org
Cc: linux-kernel@vger.kernel.org, Borislav Petkov &lt;bp@alien8.de&gt;,
	Brian Gerst &lt;brgerst@gmail.com&gt;, David Laight &lt;David.Laight@aculab.com&gt;,
	Kees Cook &lt;keescook@chromium.org&gt;, Peter Zijlstra &lt;peterz@infradead.org&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;
Subject: [RFC PTI 3/3] x86/pti: Put the LDT in its own PGD if PTI is on
Date: Fri,  8 Dec 2017 11:59:11 -0800
Message-Id: &lt;04e1697bd6ba902ea19c91f43b40dcd057e2745b.1512763129.git.luto@kernel.org&gt;
X-Mailer: git-send-email 2.13.6
In-Reply-To: &lt;cover.1512763129.git.luto@kernel.org&gt;
References: &lt;cover.1512763129.git.luto@kernel.org&gt;
In-Reply-To: &lt;cover.1512763129.git.luto@kernel.org&gt;
References: &lt;cover.1512763129.git.luto@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=125831">Andrew Lutomirski</a> - Dec. 8, 2017, 7:59 p.m.</div>
<pre class="content">
With PTI on, we need the LDT to be in the usermode tables somewhere,
and the LDT is per-mm.

tglx had a hack to have a per-cpu LDT and context switch it, but it
was probably insanely slow due to the required TLB flushes.

Instead, take advantage of the fact that we have an address space
hole that gives us a completely unused pgd and make that pgd be
per-mm.  We can put the LDT in it.

This has a down side: the LDT isn&#39;t (currently) randomized, and an
attack that can write the LDT is instant root due to call gates
(thanks, AMD, for leaving call gates in AMD64 but designing them
wrong so they&#39;re only useful for exploits).  We could mitigate this
by making the LDT read-only or randomizing it, either of which is
strightforward on top of this patch.

XXX: The 5-level case needs testing and docs updates
<span class="signed-off-by">
Signed-off-by: Andy Lutomirski &lt;luto@kernel.org&gt;</span>
---
 Documentation/x86/x86_64/mm.txt         |  11 ++-
 arch/x86/include/asm/mmu_context.h      |  33 +++++++-
 arch/x86/include/asm/pgtable_64_types.h |   2 +
 arch/x86/include/asm/processor.h        |  23 ++++--
 arch/x86/kernel/ldt.c                   | 139 +++++++++++++++++++++++++++++---
 5 files changed, 185 insertions(+), 23 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=107">Thomas Gleixner</a> - Dec. 8, 2017, 9:48 p.m.</div>
<pre class="content">
On Fri, 8 Dec 2017, Andy Lutomirski wrote:
<span class="quote">
&gt; With PTI on, we need the LDT to be in the usermode tables somewhere,</span>
<span class="quote">&gt; and the LDT is per-mm.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; tglx had a hack to have a per-cpu LDT and context switch it, but it</span>
<span class="quote">&gt; was probably insanely slow due to the required TLB flushes.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Instead, take advantage of the fact that we have an address space</span>
<span class="quote">&gt; hole that gives us a completely unused pgd and make that pgd be</span>
<span class="quote">&gt; per-mm.  We can put the LDT in it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This has a down side: the LDT isn&#39;t (currently) randomized, and an</span>
<span class="quote">&gt; attack that can write the LDT is instant root due to call gates</span>
<span class="quote">&gt; (thanks, AMD, for leaving call gates in AMD64 but designing them</span>
<span class="quote">&gt; wrong so they&#39;re only useful for exploits).  We could mitigate this</span>
<span class="quote">&gt; by making the LDT read-only or randomizing it, either of which is</span>
<span class="quote">&gt; strightforward on top of this patch.</span>

Randomizing yes. RO not so much. I spent quite some time with Peter Zijstra
to make that work.

The problem is that the CPU tries to write the accessed bit in the entry
which is referenced by a selector register.

In most cases this is a non-issue because the access happens not when the
selector is loaded, it happens when the selector is referenced, e.g. mov
fs:.... This results in a very nice to identify page fault and writing the
accessed bit from a special fault handler works nicely.

Now whats special is the stack segment. If the LDT has been modified and
the IRET or whatever brings the task back to user space, it loads SS from
stack and this immediately results in a #GP, which does not tell what
failed and a fixup of the address bit does not work in that case. So the
#GP just comes back.

Forcing he accessed bit to 1 when installing the descriptors does not help
either.

So yes, thanks a lot to the folks who came up with that wonderful exploit
target.
<span class="quote">
&gt; +static int map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +#ifdef CONFIG_X86_64</span>
<span class="quote">&gt; +	pgd_t *pgd;</span>
<span class="quote">&gt; +	p4d_t *p4d;</span>
<span class="quote">&gt; +	pud_t *pud;</span>
<span class="quote">&gt; +	pmd_t *pmd;</span>
<span class="quote">&gt; +	pte_t *pte;</span>
<span class="quote">&gt; +	bool is_vmalloc;</span>
<span class="quote">&gt; +	bool had_top_level_entry;</span>
<span class="quote">&gt; +	int i;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="quote">&gt; +		return 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	WARN_ON(ldt-&gt;slot != -1);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Both LDT slots are contained in a single PMD, so we can pass</span>
<span class="quote">&gt; +	 * LDT_BASE_ADDR instead of the real address to all the _offset</span>
<span class="quote">&gt; +	 * helpers.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Did we already have the top level entry allocated?  We can&#39;t</span>
<span class="quote">&gt; +	 * use pgd_none() for this because it doens&#39;t do anything on</span>
<span class="quote">&gt; +	 * 4-level page table kernels.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	had_top_level_entry = (pgd-&gt;pgd != 0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	p4d = p4d_alloc(mm, pgd, LDT_BASE_ADDR);</span>
<span class="quote">&gt; +	if (!p4d)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	pud = pud_alloc(mm, p4d, LDT_BASE_ADDR);</span>
<span class="quote">&gt; +	if (!pud)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; +	pmd = pmd_alloc(mm, pud, LDT_BASE_ADDR);</span>
<span class="quote">&gt; +	if (!pmd)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; +	if (pte_alloc(mm, pmd, LDT_BASE_ADDR))</span>
<span class="quote">&gt; +		return -ENOMEM;</span>

I really find it disgusting that we have a gazillion copies of exactly the
same code which just populates stuff up the place where PTEs can be
installed.

Can we pretty please have _ONE_ general out of line helper function for
this?
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	if (mm-&gt;context.ldt) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * We already had an LDT.  The top-level entry should already</span>
<span class="quote">&gt; +		 * have been allocated and synchronized with the usermode</span>
<span class="quote">&gt; +		 * tables.</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		WARN_ON(!had_top_level_entry);</span>
<span class="quote">&gt; +		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="quote">&gt; +			WARN_ON(!kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		WARN_ON(had_top_level_entry);</span>
<span class="quote">&gt; +		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="quote">&gt; +			WARN_ON(kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="quote">&gt; +			set_pgd(kernel_to_user_pgdp(pgd), *pgd);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>

Just get rid of that single page allocation thing and use vmalloc
unconditionally.

Thanks,

	tglx
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/x86/x86_64/mm.txt b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">index 2d7d6590ade8..bfa44e1cb293 100644</span>
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -12,13 +12,15 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_add">+fffffe8000000000 - fffffeffffffffff (=39 bits) LDT range</span>
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
 ... unused hole ...
 ffffffff80000000 - ffffffff9fffffff (=512 MB)  kernel text mapping, from phys 0
<span class="p_del">-ffffffffa0000000 - ffffffffff5fffff (=1526 MB) module mapping space (variable)</span>
<span class="p_del">-ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls</span>
<span class="p_add">+ffffffffa0000000 - [fixmap start]   (~1526 MB) module mapping space (variable)</span>
<span class="p_add">+[fixmap start]   - ffffffffff5fffff kernel-internal fixmap range</span>
<span class="p_add">+ffffffffff600000 - ffffffffff600fff (=4 kB) legacy vsyscall ABI</span>
 ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole
 
 Virtual memory map with 5 level page tables:
<span class="p_chunk">@@ -39,8 +41,9 @@</span> <span class="p_context"> ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks</span>
 ffffffef00000000 - fffffffeffffffff (=64 GB) EFI region mapping space
 ... unused hole ...
 ffffffff80000000 - ffffffff9fffffff (=512 MB)  kernel text mapping, from phys 0
<span class="p_del">-ffffffffa0000000 - ffffffffff5fffff (=1526 MB) module mapping space</span>
<span class="p_del">-ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls</span>
<span class="p_add">+ffffffffa0000000 - [fixmap start]   (~1526 MB) module mapping space</span>
<span class="p_add">+[fixmap start]   - ffffffffff5fffff kernel-internal fixmap range</span>
<span class="p_add">+ffffffffff600000 - ffffffffff600fff (=4 kB) legacy vsyscall ABI</span>
 ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole
 
 Architecture defines a 64-bit virtual address. Implementations can support
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 5e1a1ecb65c6..eb87bbeddacc 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -52,13 +52,29 @@</span> <span class="p_context"> struct ldt_struct {</span>
 	 */
 	struct desc_struct *entries;
 	unsigned int nr_entries;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PTI is in use, then the entries array is not mapped while we&#39;re</span>
<span class="p_add">+	 * in user mode.  The whole array will be aliased at the addressed</span>
<span class="p_add">+	 * given by ldt_slot_va(slot).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int slot;</span>
 };
 
<span class="p_add">+/* This is a multiple of PAGE_SIZE. */</span>
<span class="p_add">+#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+static void *ldt_slot_va(int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (void *)(LDT_BASE_ADDR + LDT_SLOT_STRIDE * slot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Used for LDT copy/destruction.
  */
 int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm);
 void destroy_context_ldt(struct mm_struct *mm);
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm);</span>
 #else	/* CONFIG_MODIFY_LDT_SYSCALL */
 static inline int init_new_context_ldt(struct task_struct *tsk,
 				       struct mm_struct *mm)
<span class="p_chunk">@@ -90,10 +106,20 @@</span> <span class="p_context"> static inline void load_mm_ldt(struct mm_struct *mm)</span>
 	 * that we can see.
 	 */
 
<span class="p_del">-	if (unlikely(ldt))</span>
<span class="p_del">-		set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (unlikely(ldt)) {</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="p_add">+			if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {</span>
<span class="p_add">+				clear_LDT();</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			set_ldt(ldt_slot_va(ldt-&gt;slot), ldt-&gt;nr_entries);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
 		clear_LDT();
<span class="p_add">+	}</span>
 #else
 	clear_LDT();
 #endif
<span class="p_chunk">@@ -185,6 +211,7 @@</span> <span class="p_context"> static inline void arch_dup_mmap(struct mm_struct *oldmm,</span>
 static inline void arch_exit_mmap(struct mm_struct *mm)
 {
 	paravirt_arch_exit_mmap(mm);
<span class="p_add">+	ldt_arch_exit_mmap(mm);</span>
 }
 
 #ifdef CONFIG_X86_64
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64_types.h b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">index 6d5f45dcd4a1..130f575f8d1e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -100,6 +100,8 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define MODULES_LEN   (MODULES_END - MODULES_VADDR)
 #define ESPFIX_PGD_ENTRY _AC(-2, UL)
 #define ESPFIX_BASE_ADDR (ESPFIX_PGD_ENTRY &lt;&lt; P4D_SHIFT)
<span class="p_add">+#define LDT_PGD_ENTRY _AC(-3, UL)</span>
<span class="p_add">+#define LDT_BASE_ADDR (LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #define EFI_VA_START	 ( -4 * (_AC(1, UL) &lt;&lt; 30))
 #define EFI_VA_END	 (-68 * (_AC(1, UL) &lt;&lt; 30))
 
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 9e482d8b0b97..9c18da64daa9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -851,13 +851,22 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 
 #else
 /*
<span class="p_del">- * User space process size. 47bits minus one guard page.  The guard</span>
<span class="p_del">- * page is necessary on Intel CPUs: if a SYSCALL instruction is at</span>
<span class="p_del">- * the highest possible canonical userspace address, then that</span>
<span class="p_del">- * syscall will enter the kernel with a non-canonical return</span>
<span class="p_del">- * address, and SYSRET will explode dangerously.  We avoid this</span>
<span class="p_del">- * particular problem by preventing anything from being mapped</span>
<span class="p_del">- * at the maximum canonical address.</span>
<span class="p_add">+ * User space process size.  This is the first address outside the user range.</span>
<span class="p_add">+ * There are a few constraints that determine this:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On Intel CPUs, if a SYSCALL instruction is at the highest canonical</span>
<span class="p_add">+ * address, then that syscall will enter the kernel with a</span>
<span class="p_add">+ * non-canonical return address, and SYSRET will explode dangerously.</span>
<span class="p_add">+ * We avoid this particular problem by preventing anything executable</span>
<span class="p_add">+ * from being mapped at the maximum canonical address.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On AMD CPUs in the Ryzen family, there&#39;s a nasty bug in which the</span>
<span class="p_add">+ * CPUs malfunction if they execute code from the highest canonical page.</span>
<span class="p_add">+ * They&#39;ll speculate right off the end of the canonical space, and</span>
<span class="p_add">+ * bad things happen.  This is worked around in the same way as the</span>
<span class="p_add">+ * Intel problem.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With page table isolation enabled, we map the LDT in ... [stay tuned]</span>
  */
 #define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
 
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index ae5615b03def..8170b57fc498 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -19,6 +19,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/uaccess.h&gt;
 
 #include &lt;asm/ldt.h&gt;
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_chunk">@@ -46,13 +47,12 @@</span> <span class="p_context"> static void refresh_ldt_segments(void)</span>
 static void flush_ldt(void *__mm)
 {
 	struct mm_struct *mm = __mm;
<span class="p_del">-	mm_context_t *pc;</span>
 
 	if (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)
 		return;
 
<span class="p_del">-	pc = &amp;mm-&gt;context;</span>
<span class="p_del">-	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;nr_entries);</span>
<span class="p_add">+	__flush_tlb_all();</span>
<span class="p_add">+	load_mm_ldt(mm);</span>
 
 	refresh_ldt_segments();
 }
<span class="p_chunk">@@ -90,9 +90,113 @@</span> <span class="p_context"> static struct ldt_struct *alloc_ldt_struct(unsigned int num_entries)</span>
 	}
 
 	new_ldt-&gt;nr_entries = num_entries;
<span class="p_add">+	new_ldt-&gt;slot = -1;</span>
 	return new_ldt;
 }
 
<span class="p_add">+static int map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	p4d_t *p4d;</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	bool is_vmalloc;</span>
<span class="p_add">+	bool had_top_level_entry;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	WARN_ON(ldt-&gt;slot != -1);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Both LDT slots are contained in a single PMD, so we can pass</span>
<span class="p_add">+	 * LDT_BASE_ADDR instead of the real address to all the _offset</span>
<span class="p_add">+	 * helpers.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Did we already have the top level entry allocated?  We can&#39;t</span>
<span class="p_add">+	 * use pgd_none() for this because it doens&#39;t do anything on</span>
<span class="p_add">+	 * 4-level page table kernels.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	had_top_level_entry = (pgd-&gt;pgd != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	p4d = p4d_alloc(mm, pgd, LDT_BASE_ADDR);</span>
<span class="p_add">+	if (!p4d)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_alloc(mm, p4d, LDT_BASE_ADDR);</span>
<span class="p_add">+	if (!pud)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	pmd = pmd_alloc(mm, pud, LDT_BASE_ADDR);</span>
<span class="p_add">+	if (!pmd)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	if (pte_alloc(mm, pmd, LDT_BASE_ADDR))</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mm-&gt;context.ldt) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We already had an LDT.  The top-level entry should already</span>
<span class="p_add">+		 * have been allocated and synchronized with the usermode</span>
<span class="p_add">+		 * tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(!had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+			WARN_ON(!kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		WARN_ON(had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI)) {</span>
<span class="p_add">+			WARN_ON(kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+			set_pgd(kernel_to_user_pgdp(pgd), *pgd);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i * PAGE_SIZE &lt; ldt-&gt;nr_entries * LDT_ENTRY_SIZE; i++) {</span>
<span class="p_add">+		unsigned long offset = i &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		unsigned long va = (unsigned long)ldt_slot_va(slot) + offset;</span>
<span class="p_add">+		const void *src = (char *)ldt-&gt;entries + offset;</span>
<span class="p_add">+		unsigned long pfn = is_vmalloc ? vmalloc_to_pfn(src) :</span>
<span class="p_add">+			page_to_pfn(virt_to_page(src));</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = pte_offset_kernel(pmd, va);</span>
<span class="p_add">+		set_pte(pte, pfn_pte(pfn, __pgprot(__PAGE_KERNEL &amp; ~_PAGE_GLOBAL)));</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_mm_range(mm,</span>
<span class="p_add">+			   (unsigned long)ldt_slot_va(slot),</span>
<span class="p_add">+			   (unsigned long)ldt_slot_va(slot) + LDT_SLOT_STRIDE,</span>
<span class="p_add">+			   0);</span>
<span class="p_add">+</span>
<span class="p_add">+	ldt-&gt;slot = slot;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	return -EINVAL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_ldt_pgtables(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	struct mmu_gather tlb;</span>
<span class="p_add">+	unsigned long start = LDT_BASE_ADDR;</span>
<span class="p_add">+	unsigned long end = start + (1UL &lt;&lt; PGDIR_SHIFT);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has_bug(X86_BUG_CPU_SECURE_MODE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="p_add">+	free_pgd_range(&amp;tlb, start, end, start, end);</span>
<span class="p_add">+	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* After calling this, the LDT is immutable. */
 static void finalize_ldt_struct(struct ldt_struct *ldt)
 {
<span class="p_chunk">@@ -134,17 +238,15 @@</span> <span class="p_context"> int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
 	int retval = 0;
 
 	mutex_init(&amp;mm-&gt;context.lock);
<span class="p_add">+	mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+</span>
 	old_mm = current-&gt;mm;
<span class="p_del">-	if (!old_mm) {</span>
<span class="p_del">-		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+	if (!old_mm)</span>
 		return 0;
<span class="p_del">-	}</span>
 
 	mutex_lock(&amp;old_mm-&gt;context.lock);
<span class="p_del">-	if (!old_mm-&gt;context.ldt) {</span>
<span class="p_del">-		mm-&gt;context.ldt = NULL;</span>
<span class="p_add">+	if (!old_mm-&gt;context.ldt)</span>
 		goto out_unlock;
<span class="p_del">-	}</span>
 
 	new_ldt = alloc_ldt_struct(old_mm-&gt;context.ldt-&gt;nr_entries);
 	if (!new_ldt) {
<span class="p_chunk">@@ -155,8 +257,17 @@</span> <span class="p_context"> int init_new_context_ldt(struct task_struct *tsk, struct mm_struct *mm)</span>
 	memcpy(new_ldt-&gt;entries, old_mm-&gt;context.ldt-&gt;entries,
 	       new_ldt-&gt;nr_entries * LDT_ENTRY_SIZE);
 	finalize_ldt_struct(new_ldt);
<span class="p_add">+	retval = map_ldt_struct(mm, new_ldt, 0);</span>
<span class="p_add">+	if (retval)</span>
<span class="p_add">+		goto out_free;</span>
 
 	mm-&gt;context.ldt = new_ldt;
<span class="p_add">+	goto out_unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+out_free:</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+	free_ldt_struct(new_ldt);</span>
<span class="p_add">+	return retval;</span>
 
 out_unlock:
 	mutex_unlock(&amp;old_mm-&gt;context.lock);
<span class="p_chunk">@@ -174,6 +285,11 @@</span> <span class="p_context"> void destroy_context_ldt(struct mm_struct *mm)</span>
 	mm-&gt;context.ldt = NULL;
 }
 
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int read_ldt(void __user *ptr, unsigned long bytecount)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_chunk">@@ -285,6 +401,11 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 
 	new_ldt-&gt;entries[ldt_info.entry_number] = ldt;
 	finalize_ldt_struct(new_ldt);
<span class="p_add">+	error = map_ldt_struct(mm, new_ldt, old_ldt ? !old_ldt-&gt;slot : 0);</span>
<span class="p_add">+	if (error) {</span>
<span class="p_add">+		free_ldt_struct(old_ldt);</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
 
 	install_ldt(mm, new_ldt);
 	free_ldt_struct(old_ldt);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



