
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[13/31] nds32: DMA mapping API - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [13/31] nds32: DMA mapping API</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 8, 2017, 5:55 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;763e6a252de1ad8ccd28344fd0676ae2f92f796d.1510118606.git.green.hu@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10047809/mbox/"
   >mbox</a>
|
   <a href="/patch/10047809/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10047809/">/patch/10047809/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C59D260381 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  8 Nov 2017 06:27:11 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B3C4D2A4CF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  8 Nov 2017 06:27:11 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A87A12A4D1; Wed,  8 Nov 2017 06:27:11 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM, RCVD_IN_DNSWL_HI,
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 715E52A4CF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  8 Nov 2017 06:27:10 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755027AbdKHG0l (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 8 Nov 2017 01:26:41 -0500
Received: from mail-pl0-f68.google.com ([209.85.160.68]:44526 &quot;EHLO
	mail-pl0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1754069AbdKHGUa (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 8 Nov 2017 01:20:30 -0500
Received: by mail-pl0-f68.google.com with SMTP id g16so668710plj.1;
	Tue, 07 Nov 2017 22:20:30 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20161025;
	h=from:to:cc:subject:date:message-id:in-reply-to:references
	:in-reply-to:references;
	bh=eoM9XptiViYne5XeHQbbi5n90v6YpYZ1R++LiO3mi/g=;
	b=p/zRvFk9iyOV5EiJHyIrpyAgCkZKTLSErByz1ksKV/+pQvv0d4A7gjdleoc84WNkVX
	teUHlC0fox4rvMTafsjtpHcuw7vmmbWl6GslOikLDKObPbA8ED5r0gfglwGGoLggi/a4
	sIg4w+Vjyq9a/MiVmJxs8Vwg1TwEPPPw0d3PRN1kmCdlddXbMcn+aaAMXxmPCSiKbz3+
	0XVZHcXRWUBAHE+n60YpCivk1d8mIhKJMAld7sKzW3qBudgpNaICDQKKCREfnLNdPMgM
	bcwYN88mlpVPVGnbulxlauHm9RkvQU7fU223BobZrw4now4leh29gIG4qo8plftIZSfO
	2lxw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references:in-reply-to:references;
	bh=eoM9XptiViYne5XeHQbbi5n90v6YpYZ1R++LiO3mi/g=;
	b=G9UzwE0P3nnDqYjq+XEqoF/sYwFEkHGw6/haWJMoPG4Gplgbk26JMu2hUc8Wpte4Ui
	QoJEt7c54YLT3KWklIB6VQvBMDILrZH9/5KJGiD3WICfxfaQiuJXiNcQjE4i3r4fDdZX
	5D2Rpvsw1VNtc6QbPbO6ppjKS+CQQR0IdG4LFvh20y3DEjWAPkw5cDAm7J18SlFA7yMJ
	IkIUE42byEWZYkSANk3MC1E22CkTAJiqfSLJ/oySPUbjpr1AFmln7zrWFj4DFlO858EG
	FT8iUWJIXO+NfPHkKgvz0M2tU/PLnfYvnZR0YMV4DtWkgIGONjTgpIBnR5baBW673cOE
	kVSQ==
X-Gm-Message-State: AJaThX7KtSaJZ3/Fk/mcfBw81DqSCDWuI3jDMBy763O9i5GLyxCArY6Q
	5peBpEK7OKeACduj1Ogwfj4=
X-Google-Smtp-Source: ABhQp+QDitfin1pTRIY4mMqbAxRgpuFWgaO/S0FhiVEpiWai671NvrIh/FyoqwHuxuc8Aaj5UODk0A==
X-Received: by 10.84.246.197 with SMTP id j5mr1227397plt.7.1510122029361;
	Tue, 07 Nov 2017 22:20:29 -0800 (PST)
Received: from app09.andestech.com ([118.163.51.199])
	by smtp.gmail.com with ESMTPSA id
	a4sm6581339pfj.72.2017.11.07.22.20.26
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 07 Nov 2017 22:20:28 -0800 (PST)
From: Greentime Hu &lt;green.hu@gmail.com&gt;
To: greentime@andestech.com, linux-kernel@vger.kernel.org,
	arnd@arndb.de, linux-arch@vger.kernel.org, tglx@linutronix.de,
	jason@lakedaemon.net, marc.zyngier@arm.com, robh+dt@kernel.org,
	netdev@vger.kernel.org
Cc: green.hu@gmail.com, Vincent Chen &lt;vincentc@andestech.com&gt;
Subject: [PATCH 13/31] nds32: DMA mapping API
Date: Wed,  8 Nov 2017 13:55:01 +0800
Message-Id: &lt;763e6a252de1ad8ccd28344fd0676ae2f92f796d.1510118606.git.green.hu@gmail.com&gt;
X-Mailer: git-send-email 1.7.9.5
In-Reply-To: &lt;cover.1510118606.git.green.hu@gmail.com&gt;
References: &lt;cover.1510118606.git.green.hu@gmail.com&gt;
In-Reply-To: &lt;cover.1510118606.git.green.hu@gmail.com&gt;
References: &lt;cover.1510118606.git.green.hu@gmail.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Nov. 8, 2017, 5:55 a.m.</div>
<pre class="content">
<span class="from">From: Greentime Hu &lt;greentime@andestech.com&gt;</span>
<span class="signed-off-by">
Signed-off-by: Vincent Chen &lt;vincentc@andestech.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Greentime Hu &lt;greentime@andestech.com&gt;</span>
---
 arch/nds32/include/asm/dma-mapping.h |   27 ++
 arch/nds32/kernel/dma.c              |  478 ++++++++++++++++++++++++++++++++++
 2 files changed, 505 insertions(+)
 create mode 100644 arch/nds32/include/asm/dma-mapping.h
 create mode 100644 arch/nds32/kernel/dma.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Nov. 8, 2017, 9:09 a.m.</div>
<pre class="content">
On Wed, Nov 8, 2017 at 6:55 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:
<span class="quote">
&gt; +static void consistent_sync(void *vaddr, size_t size, int direction)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       unsigned long start = (unsigned long)vaddr;</span>
<span class="quote">&gt; +       unsigned long end = start + size;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +       switch (direction) {</span>
<span class="quote">&gt; +       case DMA_FROM_DEVICE:   /* invalidate only */</span>
<span class="quote">&gt; +               cpu_dma_inval_range(start, end);</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case DMA_TO_DEVICE:     /* writeback only */</span>
<span class="quote">&gt; +               cpu_dma_wb_range(start, end);</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       case DMA_BIDIRECTIONAL: /* writeback and invalidate */</span>
<span class="quote">&gt; +               cpu_dma_wbinval_range(start, end);</span>
<span class="quote">&gt; +               break;</span>
<span class="quote">&gt; +       default:</span>
<span class="quote">&gt; +               BUG();</span>
<span class="quote">&gt; +       }</span>
<span class="quote">&gt; +}</span>
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +static void</span>
<span class="quote">&gt; +nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,</span>
<span class="quote">&gt; +                             size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       consistent_sync((void *)dma_to_virt(dev, handle), size, dir);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void</span>
<span class="quote">&gt; +nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,</span>
<span class="quote">&gt; +                                size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +       consistent_sync((void *)dma_to_virt(dev, handle), size, dir);</span>
<span class="quote">&gt; +}</span>

You do the same cache operations for _to_cpu and _to_device, which
usually works,
but is more expensive than you need. It&#39;s better to take the ownership into
account and only do what you need.

     Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Nov. 9, 2017, 7:12 a.m.</div>
<pre class="content">
2017-11-08 17:09 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:
<span class="quote">&gt; On Wed, Nov 8, 2017 at 6:55 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +static void consistent_sync(void *vaddr, size_t size, int direction)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       unsigned long start = (unsigned long)vaddr;</span>
<span class="quote">&gt;&gt; +       unsigned long end = start + size;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +       switch (direction) {</span>
<span class="quote">&gt;&gt; +       case DMA_FROM_DEVICE:   /* invalidate only */</span>
<span class="quote">&gt;&gt; +               cpu_dma_inval_range(start, end);</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case DMA_TO_DEVICE:     /* writeback only */</span>
<span class="quote">&gt;&gt; +               cpu_dma_wb_range(start, end);</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       case DMA_BIDIRECTIONAL: /* writeback and invalidate */</span>
<span class="quote">&gt;&gt; +               cpu_dma_wbinval_range(start, end);</span>
<span class="quote">&gt;&gt; +               break;</span>
<span class="quote">&gt;&gt; +       default:</span>
<span class="quote">&gt;&gt; +               BUG();</span>
<span class="quote">&gt;&gt; +       }</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void</span>
<span class="quote">&gt;&gt; +nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,</span>
<span class="quote">&gt;&gt; +                             size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       consistent_sync((void *)dma_to_virt(dev, handle), size, dir);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void</span>
<span class="quote">&gt;&gt; +nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,</span>
<span class="quote">&gt;&gt; +                                size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +       consistent_sync((void *)dma_to_virt(dev, handle), size, dir);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; You do the same cache operations for _to_cpu and _to_device, which</span>
<span class="quote">&gt; usually works,</span>
<span class="quote">&gt; but is more expensive than you need. It&#39;s better to take the ownership into</span>
<span class="quote">&gt; account and only do what you need.</span>
<span class="quote">&gt;</span>

Thanks.

Like this?

static void
nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,
                              size_t size, enum dma_data_direction dir)
{
        consistent_sync((void *)dma_to_virt(dev, handle), size,
DMA_FROM_DEVICE);
}

static void
nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,
                                 size_t size, enum dma_data_direction dir)
{
        consistent_sync((void *)dma_to_virt(dev, handle), size,
DMA_TO_DEVICE);
}
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Nov. 9, 2017, 10:14 a.m.</div>
<pre class="content">
On Thu, Nov 9, 2017 at 8:12 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:
<span class="quote">&gt; 2017-11-08 17:09 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:</span>
<span class="quote">&gt;&gt; On Wed, Nov 8, 2017 at 6:55 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">
&gt;&gt; You do the same cache operations for _to_cpu and _to_device, which</span>
<span class="quote">&gt;&gt; usually works,</span>
<span class="quote">&gt;&gt; but is more expensive than you need. It&#39;s better to take the ownership into</span>
<span class="quote">&gt;&gt; account and only do what you need.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt; Like this?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; static void</span>
<span class="quote">&gt; nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,</span>
<span class="quote">&gt;                               size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         consistent_sync((void *)dma_to_virt(dev, handle), size,</span>
<span class="quote">&gt; DMA_FROM_DEVICE);</span>
<span class="quote">&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; static void</span>
<span class="quote">&gt; nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,</span>
<span class="quote">&gt;                                  size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt; {</span>
<span class="quote">&gt;         consistent_sync((void *)dma_to_virt(dev, handle), size,</span>
<span class="quote">&gt; DMA_TO_DEVICE);</span>
<span class="quote">&gt; }</span>

No, it&#39;s more complicated than that. You need to pass both the direction of the
DMA transaction and the ownership to consistent_sync(), and then do the
correct cache maintenance operation for each of the six combinations.

Which operation that is depends on the microarchitecture to some degree,
e.g. on machines that can load arbitrary cache lines during speculative
execution, you have to invalidate the caches during both
_for_device/FROM_DEVICE _for_cpu/FROM_DEVICE, while machines
without speculative execution can skip the second invalidation, they
only need to get rid of dirty cache lines before the DMA from device.

Usually you don&#39;t have to do a writeback during _for_cpu, since there
are no dirty cache lines after the _for_device operation.

It&#39;s not entirely clear what the correct behavior is for buffers that
are not cache line aligned, some architectures use wbinval instead
of inval for the _for_device/_FROM_DEVICE operation, on
any partial cache line, but you wouldn&#39;t want to do that on the
_for_cpu/_FROM_DEVICE operation.

       Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=171217">Greentime Hu</a> - Nov. 10, 2017, 8:13 a.m.</div>
<pre class="content">
2017-11-09 18:14 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:
<span class="quote">&gt; On Thu, Nov 9, 2017 at 8:12 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt; 2017-11-08 17:09 GMT+08:00 Arnd Bergmann &lt;arnd@arndb.de&gt;:</span>
<span class="quote">&gt;&gt;&gt; On Wed, Nov 8, 2017 at 6:55 AM, Greentime Hu &lt;green.hu@gmail.com&gt; wrote:</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;&gt; You do the same cache operations for _to_cpu and _to_device, which</span>
<span class="quote">&gt;&gt;&gt; usually works,</span>
<span class="quote">&gt;&gt;&gt; but is more expensive than you need. It&#39;s better to take the ownership into</span>
<span class="quote">&gt;&gt;&gt; account and only do what you need.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt; Like this?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; static void</span>
<span class="quote">&gt;&gt; nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,</span>
<span class="quote">&gt;&gt;                               size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt;&gt; {</span>
<span class="quote">&gt;&gt;         consistent_sync((void *)dma_to_virt(dev, handle), size,</span>
<span class="quote">&gt;&gt; DMA_FROM_DEVICE);</span>
<span class="quote">&gt;&gt; }</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; static void</span>
<span class="quote">&gt;&gt; nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,</span>
<span class="quote">&gt;&gt;                                  size_t size, enum dma_data_direction dir)</span>
<span class="quote">&gt;&gt; {</span>
<span class="quote">&gt;&gt;         consistent_sync((void *)dma_to_virt(dev, handle), size,</span>
<span class="quote">&gt;&gt; DMA_TO_DEVICE);</span>
<span class="quote">&gt;&gt; }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; No, it&#39;s more complicated than that. You need to pass both the direction of the</span>
<span class="quote">&gt; DMA transaction and the ownership to consistent_sync(), and then do the</span>
<span class="quote">&gt; correct cache maintenance operation for each of the six combinations.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Which operation that is depends on the microarchitecture to some degree,</span>
<span class="quote">&gt; e.g. on machines that can load arbitrary cache lines during speculative</span>
<span class="quote">&gt; execution, you have to invalidate the caches during both</span>
<span class="quote">&gt; _for_device/FROM_DEVICE _for_cpu/FROM_DEVICE, while machines</span>
<span class="quote">&gt; without speculative execution can skip the second invalidation, they</span>
<span class="quote">&gt; only need to get rid of dirty cache lines before the DMA from device.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Usually you don&#39;t have to do a writeback during _for_cpu, since there</span>
<span class="quote">&gt; are no dirty cache lines after the _for_device operation.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It&#39;s not entirely clear what the correct behavior is for buffers that</span>
<span class="quote">&gt; are not cache line aligned, some architectures use wbinval instead</span>
<span class="quote">&gt; of inval for the _for_device/_FROM_DEVICE operation, on</span>
<span class="quote">&gt; any partial cache line, but you wouldn&#39;t want to do that on the</span>
<span class="quote">&gt; _for_cpu/_FROM_DEVICE operation.</span>


I get your point. I prefer to keep it that way because it will be a
little bit complex.
I will still study the code to see what I can improve in the next version patch.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/nds32/include/asm/dma-mapping.h b/arch/nds32/include/asm/dma-mapping.h</span>
new file mode 100644
<span class="p_header">index 0000000..63dbeb7</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/include/asm/dma-mapping.h</span>
<span class="p_chunk">@@ -0,0 +1,27 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef ASMNDS32_DMA_MAPPING_H</span>
<span class="p_add">+#define ASMNDS32_DMA_MAPPING_H</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct dma_map_ops nds32_dma_ops;</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct dma_map_ops *get_arch_dma_ops(struct bus_type *bus)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;nds32_dma_ops;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/nds32/kernel/dma.c b/arch/nds32/kernel/dma.c</span>
new file mode 100644
<span class="p_header">index 0000000..939702e</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/nds32/kernel/dma.c</span>
<span class="p_chunk">@@ -0,0 +1,478 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2005-2017 Andes Technology Corporation</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/export.h&gt;</span>
<span class="p_add">+#include &lt;linux/string.h&gt;</span>
<span class="p_add">+#include &lt;linux/scatterlist.h&gt;</span>
<span class="p_add">+#include &lt;linux/dma-mapping.h&gt;</span>
<span class="p_add">+#include &lt;linux/io.h&gt;</span>
<span class="p_add">+#include &lt;linux/cache.h&gt;</span>
<span class="p_add">+#include &lt;linux/highmem.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;asm/cacheflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/dma-mapping.h&gt;</span>
<span class="p_add">+#include &lt;asm/proc-fns.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This is the page table (2MB) covering uncached, DMA consistent allocations</span>
<span class="p_add">+ */</span>
<span class="p_add">+static pte_t *consistent_pte;</span>
<span class="p_add">+static DEFINE_RAW_SPINLOCK(consistent_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * VM region handling support.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This should become something generic, handling VM region allocations for</span>
<span class="p_add">+ * vmalloc and similar (ioremap, module space, etc).</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * I envisage vmalloc()&#39;s supporting vm_struct becoming:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  struct vm_struct {</span>
<span class="p_add">+ *    struct vm_region	region;</span>
<span class="p_add">+ *    unsigned long	flags;</span>
<span class="p_add">+ *    struct page	**pages;</span>
<span class="p_add">+ *    unsigned int	nr_pages;</span>
<span class="p_add">+ *    unsigned long	phys_addr;</span>
<span class="p_add">+ *  };</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * get_vm_area() would then call vm_region_alloc with an appropriate</span>
<span class="p_add">+ * struct vm_region head (eg):</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *  struct vm_region vmalloc_head = {</span>
<span class="p_add">+ *	.vm_list	= LIST_HEAD_INIT(vmalloc_head.vm_list),</span>
<span class="p_add">+ *	.vm_start	= VMALLOC_START,</span>
<span class="p_add">+ *	.vm_end		= VMALLOC_END,</span>
<span class="p_add">+ *  };</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * However, vmalloc_head.vm_start is variable (typically, it is dependent on</span>
<span class="p_add">+ * the amount of RAM found at boot time.)  I would imagine that get_vm_area()</span>
<span class="p_add">+ * would have to initialise this each time prior to calling vm_region_alloc().</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct arch_vm_region {</span>
<span class="p_add">+	struct list_head vm_list;</span>
<span class="p_add">+	unsigned long vm_start;</span>
<span class="p_add">+	unsigned long vm_end;</span>
<span class="p_add">+	struct page *vm_pages;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct arch_vm_region consistent_head = {</span>
<span class="p_add">+	.vm_list = LIST_HEAD_INIT(consistent_head.vm_list),</span>
<span class="p_add">+	.vm_start = CONSISTENT_BASE,</span>
<span class="p_add">+	.vm_end = CONSISTENT_END,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct arch_vm_region *vm_region_alloc(struct arch_vm_region *head,</span>
<span class="p_add">+					      size_t size, int gfp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = head-&gt;vm_start, end = head-&gt;vm_end - size;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	struct arch_vm_region *c, *new;</span>
<span class="p_add">+</span>
<span class="p_add">+	new = kmalloc(sizeof(struct arch_vm_region), gfp);</span>
<span class="p_add">+	if (!new)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_lock_irqsave(&amp;consistent_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry(c, &amp;head-&gt;vm_list, vm_list) {</span>
<span class="p_add">+		if ((addr + size) &lt; addr)</span>
<span class="p_add">+			goto nospc;</span>
<span class="p_add">+		if ((addr + size) &lt;= c-&gt;vm_start)</span>
<span class="p_add">+			goto found;</span>
<span class="p_add">+		addr = c-&gt;vm_end;</span>
<span class="p_add">+		if (addr &gt; end)</span>
<span class="p_add">+			goto nospc;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+found:</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Insert this entry _before_ the one we found.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	list_add_tail(&amp;new-&gt;vm_list, &amp;c-&gt;vm_list);</span>
<span class="p_add">+	new-&gt;vm_start = addr;</span>
<span class="p_add">+	new-&gt;vm_end = addr + size;</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+	return new;</span>
<span class="p_add">+</span>
<span class="p_add">+nospc:</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+	kfree(new);</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct arch_vm_region *vm_region_find(struct arch_vm_region *head,</span>
<span class="p_add">+					     unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arch_vm_region *c;</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry(c, &amp;head-&gt;vm_list, vm_list) {</span>
<span class="p_add">+		if (c-&gt;vm_start == addr)</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	c = NULL;</span>
<span class="p_add">+out:</span>
<span class="p_add">+	return c;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/* FIXME: attrs is not used. */</span>
<span class="p_add">+static void *nds32_dma_alloc_coherent(struct device *dev, size_t size,</span>
<span class="p_add">+				      dma_addr_t * handle, gfp_t gfp,</span>
<span class="p_add">+				      unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	struct arch_vm_region *c;</span>
<span class="p_add">+	unsigned long order;</span>
<span class="p_add">+	u64 mask = ISA_DMA_THRESHOLD, limit;</span>
<span class="p_add">+	pgprot_t prot = pgprot_noncached(PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!consistent_pte) {</span>
<span class="p_add">+		pr_err(&quot;%s: not initialized\n&quot;, __func__);</span>
<span class="p_add">+		dump_stack();</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (dev) {</span>
<span class="p_add">+		mask = dev-&gt;coherent_dma_mask;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Sanity check the DMA mask - it must be non-zero, and</span>
<span class="p_add">+		 * must be able to be satisfied by a DMA allocation.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (mask == 0) {</span>
<span class="p_add">+			dev_warn(dev, &quot;coherent DMA mask is unset\n&quot;);</span>
<span class="p_add">+			goto no_page;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		if ((~mask) &amp; ISA_DMA_THRESHOLD) {</span>
<span class="p_add">+			dev_warn(dev, &quot;coherent DMA mask %#llx is smaller &quot;</span>
<span class="p_add">+				 &quot;than system GFP_DMA mask %#llx\n&quot;,</span>
<span class="p_add">+				 mask, (unsigned long long)ISA_DMA_THRESHOLD);</span>
<span class="p_add">+			goto no_page;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Sanity check the allocation size.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	size = PAGE_ALIGN(size);</span>
<span class="p_add">+	limit = (mask + 1) &amp; ~mask;</span>
<span class="p_add">+	if ((limit &amp;&amp; size &gt;= limit) ||</span>
<span class="p_add">+	    size &gt;= (CONSISTENT_END - CONSISTENT_BASE)) {</span>
<span class="p_add">+		pr_warn(&quot;coherent allocation too big &quot;</span>
<span class="p_add">+			&quot;(requested %#x mask %#llx)\n&quot;, size, mask);</span>
<span class="p_add">+		goto no_page;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	order = get_order(size);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mask != 0xffffffff)</span>
<span class="p_add">+		gfp |= GFP_DMA;</span>
<span class="p_add">+</span>
<span class="p_add">+	page = alloc_pages(gfp, order);</span>
<span class="p_add">+	if (!page)</span>
<span class="p_add">+		goto no_page;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Invalidate any data that might be lurking in the</span>
<span class="p_add">+	 * kernel direct-mapped region for device DMA.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	{</span>
<span class="p_add">+		unsigned long kaddr = (unsigned long)page_address(page);</span>
<span class="p_add">+		memset(page_address(page), 0, size);</span>
<span class="p_add">+		cpu_dma_wbinval_range(kaddr, kaddr + size);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Allocate a virtual address in the consistent mapping region.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	c = vm_region_alloc(&amp;consistent_head, size,</span>
<span class="p_add">+			    gfp &amp; ~(__GFP_DMA | __GFP_HIGHMEM));</span>
<span class="p_add">+	if (c) {</span>
<span class="p_add">+		pte_t *pte = consistent_pte + CONSISTENT_OFFSET(c-&gt;vm_start);</span>
<span class="p_add">+		struct page *end = page + (1 &lt;&lt; order);</span>
<span class="p_add">+</span>
<span class="p_add">+		c-&gt;vm_pages = page;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Set the &quot;dma handle&quot;</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		*handle = page_to_dma(dev, page);</span>
<span class="p_add">+</span>
<span class="p_add">+		do {</span>
<span class="p_add">+			BUG_ON(!pte_none(*pte));</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * x86 does not mark the pages reserved...</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			SetPageReserved(page);</span>
<span class="p_add">+			set_pte(pte, mk_pte(page, prot));</span>
<span class="p_add">+			page++;</span>
<span class="p_add">+			pte++;</span>
<span class="p_add">+		} while (size -= PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Free the otherwise unused pages.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		while (page &lt; end) {</span>
<span class="p_add">+			__free_page(page);</span>
<span class="p_add">+			page++;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		return (void *)c-&gt;vm_start;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (page)</span>
<span class="p_add">+		__free_pages(page, order);</span>
<span class="p_add">+no_page:</span>
<span class="p_add">+	*handle = ~0;</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void nds32_dma_free(struct device *dev, size_t size, void *cpu_addr,</span>
<span class="p_add">+			   dma_addr_t handle, unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct arch_vm_region *c;</span>
<span class="p_add">+	unsigned long flags, addr;</span>
<span class="p_add">+	pte_t *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+	size = PAGE_ALIGN(size);</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_lock_irqsave(&amp;consistent_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	c = vm_region_find(&amp;consistent_head, (unsigned long)cpu_addr);</span>
<span class="p_add">+	if (!c)</span>
<span class="p_add">+		goto no_area;</span>
<span class="p_add">+</span>
<span class="p_add">+	if ((c-&gt;vm_end - c-&gt;vm_start) != size) {</span>
<span class="p_add">+		pr_err(&quot;%s: freeing wrong coherent size (%ld != %d)\n&quot;,</span>
<span class="p_add">+		       __func__, c-&gt;vm_end - c-&gt;vm_start, size);</span>
<span class="p_add">+		dump_stack();</span>
<span class="p_add">+		size = c-&gt;vm_end - c-&gt;vm_start;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ptep = consistent_pte + CONSISTENT_OFFSET(c-&gt;vm_start);</span>
<span class="p_add">+	addr = c-&gt;vm_start;</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pte_t pte = ptep_get_and_clear(&amp;init_mm, addr, ptep);</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+</span>
<span class="p_add">+		ptep++;</span>
<span class="p_add">+		addr += PAGE_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_none(pte) &amp;&amp; pte_present(pte)) {</span>
<span class="p_add">+			pfn = pte_pfn(pte);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (pfn_valid(pfn)) {</span>
<span class="p_add">+				struct page *page = pfn_to_page(pfn);</span>
<span class="p_add">+</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * x86 does not mark the pages reserved...</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				ClearPageReserved(page);</span>
<span class="p_add">+</span>
<span class="p_add">+				__free_page(page);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		pr_crit(&quot;%s: bad page in kernel page table\n&quot;, __func__);</span>
<span class="p_add">+	} while (size -= PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(c-&gt;vm_start, c-&gt;vm_end);</span>
<span class="p_add">+</span>
<span class="p_add">+	list_del(&amp;c-&gt;vm_list);</span>
<span class="p_add">+</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	kfree(c);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+</span>
<span class="p_add">+no_area:</span>
<span class="p_add">+	raw_spin_unlock_irqrestore(&amp;consistent_lock, flags);</span>
<span class="p_add">+	pr_err(&quot;%s: trying to free invalid coherent area: %p\n&quot;,</span>
<span class="p_add">+	       __func__, cpu_addr);</span>
<span class="p_add">+	dump_stack();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Initialise the consistent memory allocation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int __init consistent_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	pmd_t *pmd;</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+	int ret = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		pgd = pgd_offset(&amp;init_mm, CONSISTENT_BASE);</span>
<span class="p_add">+		pmd = pmd_alloc(&amp;init_mm, pgd, CONSISTENT_BASE);</span>
<span class="p_add">+		if (!pmd) {</span>
<span class="p_add">+			pr_err(&quot;%s: no pmd tables\n&quot;, __func__);</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		/* The first level mapping may be created in somewhere.</span>
<span class="p_add">+		 * It&#39;s not necessary to warn here. */</span>
<span class="p_add">+		/* WARN_ON(!pmd_none(*pmd)); */</span>
<span class="p_add">+</span>
<span class="p_add">+		pte = pte_alloc_kernel(pmd, CONSISTENT_BASE);</span>
<span class="p_add">+		if (!pte) {</span>
<span class="p_add">+			ret = -ENOMEM;</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		consistent_pte = pte;</span>
<span class="p_add">+	} while (0);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+core_initcall(consistent_init);</span>
<span class="p_add">+static void consistent_sync(void *vaddr, size_t size, int direction);</span>
<span class="p_add">+static dma_addr_t nds32_dma_map_page(struct device *dev, struct page *page,</span>
<span class="p_add">+				     unsigned long offset, size_t size,</span>
<span class="p_add">+				     enum dma_data_direction dir,</span>
<span class="p_add">+				     unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync((void *)(page_address(page) + offset), size, dir);</span>
<span class="p_add">+	return page_to_phys(page) + offset;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void nds32_dma_unmap_page(struct device *dev, dma_addr_t handle,</span>
<span class="p_add">+				 size_t size, enum dma_data_direction dir,</span>
<span class="p_add">+				 unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync(phys_to_virt(handle), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Make an area consistent for devices.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void consistent_sync(void *vaddr, size_t size, int direction)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long start = (unsigned long)vaddr;</span>
<span class="p_add">+	unsigned long end = start + size;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (direction) {</span>
<span class="p_add">+	case DMA_FROM_DEVICE:	/* invalidate only */</span>
<span class="p_add">+		cpu_dma_inval_range(start, end);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case DMA_TO_DEVICE:	/* writeback only */</span>
<span class="p_add">+		cpu_dma_wb_range(start, end);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case DMA_BIDIRECTIONAL:	/* writeback and invalidate */</span>
<span class="p_add">+		cpu_dma_wbinval_range(start, end);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int nds32_dma_map_sg(struct device *dev, struct scatterlist *sg,</span>
<span class="p_add">+			    int nents, enum dma_data_direction dir,</span>
<span class="p_add">+			    unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nents; i++, sg++) {</span>
<span class="p_add">+		void *virt;</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+		struct page *page = sg_page(sg);</span>
<span class="p_add">+</span>
<span class="p_add">+		sg-&gt;dma_address = page_to_dma(dev, page) + sg-&gt;offset;</span>
<span class="p_add">+		pfn = page_to_pfn(page) + sg-&gt;offset / PAGE_SIZE;</span>
<span class="p_add">+		page = pfn_to_page(pfn);</span>
<span class="p_add">+		if (PageHighMem(page)) {</span>
<span class="p_add">+			virt = kmap_atomic(page);</span>
<span class="p_add">+			consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+			kunmap_atomic(virt);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			if (sg-&gt;offset &gt; PAGE_SIZE)</span>
<span class="p_add">+				panic(&quot;sg-&gt;offset:%08x &gt; PAGE_SIZE\n&quot;,</span>
<span class="p_add">+				      sg-&gt;offset);</span>
<span class="p_add">+			virt = page_address(page) + sg-&gt;offset;</span>
<span class="p_add">+			consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return nents;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void nds32_dma_unmap_sg(struct device *dev, struct scatterlist *sg,</span>
<span class="p_add">+			       int nhwentries, enum dma_data_direction dir,</span>
<span class="p_add">+			       unsigned long attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_single_for_cpu(struct device *dev, dma_addr_t handle,</span>
<span class="p_add">+			      size_t size, enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync((void *)dma_to_virt(dev, handle), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_single_for_device(struct device *dev, dma_addr_t handle,</span>
<span class="p_add">+				 size_t size, enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	consistent_sync((void *)dma_to_virt(dev, handle), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_sg_for_cpu(struct device *dev, struct scatterlist *sg, int nents,</span>
<span class="p_add">+			  enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nents; i++, sg++) {</span>
<span class="p_add">+		char *virt =</span>
<span class="p_add">+		    page_address((struct page *)sg-&gt;page_link) + sg-&gt;offset;</span>
<span class="p_add">+		consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void</span>
<span class="p_add">+nds32_dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,</span>
<span class="p_add">+			     int nents, enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; nents; i++, sg++) {</span>
<span class="p_add">+		char *virt =</span>
<span class="p_add">+		    page_address((struct page *)sg-&gt;page_link) + sg-&gt;offset;</span>
<span class="p_add">+		consistent_sync(virt, sg-&gt;length, dir);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct dma_map_ops nds32_dma_ops = {</span>
<span class="p_add">+	.alloc = nds32_dma_alloc_coherent,</span>
<span class="p_add">+	.free = nds32_dma_free,</span>
<span class="p_add">+	.map_page = nds32_dma_map_page,</span>
<span class="p_add">+	.unmap_page = nds32_dma_unmap_page,</span>
<span class="p_add">+	.map_sg = nds32_dma_map_sg,</span>
<span class="p_add">+	.unmap_sg = nds32_dma_unmap_sg,</span>
<span class="p_add">+	.sync_single_for_device = nds32_dma_sync_single_for_device,</span>
<span class="p_add">+	.sync_single_for_cpu = nds32_dma_sync_single_for_cpu,</span>
<span class="p_add">+	.sync_sg_for_cpu = nds32_dma_sync_sg_for_cpu,</span>
<span class="p_add">+	.sync_sg_for_device = nds32_dma_sync_sg_for_device,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+EXPORT_SYMBOL(nds32_dma_ops);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



