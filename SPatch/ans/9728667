
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v2,6/9] mm/follow_page_mask: Add support for hugepage directory entry - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v2,6/9] mm/follow_page_mask: Add support for hugepage directory entry</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 16, 2017, 9:23 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1494926612-23928-7-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9728667/mbox/"
   >mbox</a>
|
   <a href="/patch/9728667/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9728667/">/patch/9728667/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	0042B6028A for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 May 2017 09:24:20 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E47DF28A13
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 May 2017 09:24:19 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id D6D7F28A16; Tue, 16 May 2017 09:24:19 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1489F28A0D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 16 May 2017 09:24:19 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752434AbdEPJYN (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 16 May 2017 05:24:13 -0400
Received: from mx0a-001b2d01.pphosted.com ([148.163.156.1]:38495 &quot;EHLO
	mx0a-001b2d01.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1752389AbdEPJYI (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 16 May 2017 05:24:08 -0400
Received: from pps.filterd (m0098399.ppops.net [127.0.0.1])
	by mx0a-001b2d01.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v4G9NuSS023262
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 16 May 2017 05:24:07 -0400
Received: from e31.co.us.ibm.com (e31.co.us.ibm.com [32.97.110.149])
	by mx0a-001b2d01.pphosted.com with ESMTP id 2afxk1rfry-1
	(version=TLSv1.2 cipher=AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Tue, 16 May 2017 05:24:07 -0400
Received: from localhost
	by e31.co.us.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use
	Only! Violators will be prosecuted
	for &lt;linux-kernel@vger.kernel.org&gt; from
	&lt;aneesh.kumar@linux.vnet.ibm.com&gt;; Tue, 16 May 2017 03:24:01 -0600
Received: from b03cxnp08025.gho.boulder.ibm.com (9.17.130.17)
	by e31.co.us.ibm.com (192.168.1.131) with IBM ESMTP SMTP Gateway:
	Authorized Use Only! Violators will be prosecuted; 
	Tue, 16 May 2017 03:23:58 -0600
Received: from b03ledav004.gho.boulder.ibm.com
	(b03ledav004.gho.boulder.ibm.com [9.17.130.235])
	by b03cxnp08025.gho.boulder.ibm.com (8.14.9/8.14.9/NCO v10.0) with
	ESMTP id v4G9NvF910944938; Tue, 16 May 2017 02:23:57 -0700
Received: from b03ledav004.gho.boulder.ibm.com (unknown [127.0.0.1])
	by IMSVA (Postfix) with ESMTP id C23DE78037;
	Tue, 16 May 2017 03:23:57 -0600 (MDT)
Received: from skywalker.in.ibm.com (unknown [9.124.35.179])
	by b03ledav004.gho.boulder.ibm.com (Postfix) with ESMTP id DDDAC7803F;
	Tue, 16 May 2017 03:23:55 -0600 (MDT)
From: &quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
To: akpm@linux-foundation.org, mpe@ellerman.id.au
Cc: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	linuxppc-dev@lists.ozlabs.org,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;
Subject: [PATCH v2 6/9] mm/follow_page_mask: Add support for hugepage
	directory entry
Date: Tue, 16 May 2017 14:53:29 +0530
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1494926612-23928-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
References: &lt;1494926612-23928-1-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-TM-AS-GCONF: 00
x-cbid: 17051609-8235-0000-0000-00000B811EA9
X-IBM-SpamModules-Scores: 
X-IBM-SpamModules-Versions: BY=3.00007071; HX=3.00000241; KW=3.00000007;
	PH=3.00000004; SC=3.00000212; SDB=6.00861192; UDB=6.00427112;
	IPR=6.00640855; 
	BA=6.00005351; NDR=6.00000001; ZLA=6.00000005; ZF=6.00000009;
	ZB=6.00000000; 
	ZP=6.00000000; ZH=6.00000000; ZU=6.00000002; MB=3.00015478;
	XFM=3.00000015; UTC=2017-05-16 09:24:00
X-IBM-AV-DETECTION: SAVI=unused REMOTE=unused XFE=unused
x-cbparentid: 17051609-8236-0000-0000-00003BD3A428
Message-Id: &lt;1494926612-23928-7-git-send-email-aneesh.kumar@linux.vnet.ibm.com&gt;
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-05-16_03:, , signatures=0
X-Proofpoint-Spam-Details: rule=outbound_notspam policy=outbound score=0
	spamscore=0 suspectscore=0
	malwarescore=0 phishscore=0 adultscore=0 bulkscore=0 classifier=spam
	adjust=0 reason=mlx scancount=1 engine=8.0.1-1703280000
	definitions=main-1705160083
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1141">Aneesh Kumar K.V</a> - May 16, 2017, 9:23 a.m.</div>
<pre class="content">
Architectures like ppc64 supports hugepage size that is not mapped to any of
of the page table levels. Instead they add an alternate page table entry format
called hugepage directory (hugepd). hugepd indicates that the page table entry maps
to a set of hugetlb pages. Add support for this in generic follow_page_mask
code. We already support this format in the generic gup code.

The defaul implementation prints warning and returns NULL. We will add ppc64
support in later patches
<span class="signed-off-by">
Signed-off-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
---
 include/linux/hugetlb.h |  4 ++++
 mm/gup.c                | 33 +++++++++++++++++++++++++++++++++
 mm/hugetlb.c            |  8 ++++++++
 3 files changed, 45 insertions(+)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index f66c1d4e0d1f..caee7c4664c8 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -141,6 +141,9 @@</span> <span class="p_context"> pte_t *huge_pte_offset(struct mm_struct *mm, unsigned long addr);</span>
 int huge_pmd_unshare(struct mm_struct *mm, unsigned long *addr, pte_t *ptep);
 struct page *follow_huge_addr(struct mm_struct *mm, unsigned long address,
 			      int write);
<span class="p_add">+struct page *follow_huge_pd(struct vm_area_struct *vma,</span>
<span class="p_add">+			    unsigned long address, hugepd_t hpd,</span>
<span class="p_add">+			    int flags, int pdshift);</span>
 struct page *follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 				pmd_t *pmd, int flags);
 struct page *follow_huge_pud(struct mm_struct *mm, unsigned long address,
<span class="p_chunk">@@ -175,6 +178,7 @@</span> <span class="p_context"> static inline void hugetlb_report_meminfo(struct seq_file *m)</span>
 static inline void hugetlb_show_meminfo(void)
 {
 }
<span class="p_add">+#define follow_huge_pd(vma, addr, hpd, flags, pdshift) NULL</span>
 #define follow_huge_pmd(mm, addr, pmd, flags)	NULL
 #define follow_huge_pud(mm, addr, pud, flags)	NULL
 #define follow_huge_pgd(mm, addr, pgd, flags)	NULL
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 65255389620a..a7f5b82e15f3 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -226,6 +226,14 @@</span> <span class="p_context"> static struct page *follow_pmd_mask(struct vm_area_struct *vma,</span>
 			return page;
 		return no_page_table(vma, flags);
 	}
<span class="p_add">+	if (is_hugepd(__hugepd(pmd_val(*pmd)))) {</span>
<span class="p_add">+		page = follow_huge_pd(vma, address,</span>
<span class="p_add">+				      __hugepd(pmd_val(*pmd)), flags,</span>
<span class="p_add">+				      PMD_SHIFT);</span>
<span class="p_add">+		if (page)</span>
<span class="p_add">+			return page;</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	}</span>
 	if (pmd_devmap(*pmd)) {
 		ptl = pmd_lock(mm, pmd);
 		page = follow_devmap_pmd(vma, address, pmd, flags);
<span class="p_chunk">@@ -292,6 +300,14 @@</span> <span class="p_context"> static struct page *follow_pud_mask(struct vm_area_struct *vma,</span>
 			return page;
 		return no_page_table(vma, flags);
 	}
<span class="p_add">+	if (is_hugepd(__hugepd(pud_val(*pud)))) {</span>
<span class="p_add">+		page = follow_huge_pd(vma, address,</span>
<span class="p_add">+				      __hugepd(pud_val(*pud)), flags,</span>
<span class="p_add">+				      PUD_SHIFT);</span>
<span class="p_add">+		if (page)</span>
<span class="p_add">+			return page;</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	}</span>
 	if (pud_devmap(*pud)) {
 		ptl = pud_lock(mm, pud);
 		page = follow_devmap_pud(vma, address, pud, flags);
<span class="p_chunk">@@ -311,6 +327,7 @@</span> <span class="p_context"> static struct page *follow_p4d_mask(struct vm_area_struct *vma,</span>
 				    unsigned int flags, unsigned int *page_mask)
 {
 	p4d_t *p4d;
<span class="p_add">+	struct page *page;</span>
 
 	p4d = p4d_offset(pgdp, address);
 	if (p4d_none(*p4d))
<span class="p_chunk">@@ -319,6 +336,14 @@</span> <span class="p_context"> static struct page *follow_p4d_mask(struct vm_area_struct *vma,</span>
 	if (unlikely(p4d_bad(*p4d)))
 		return no_page_table(vma, flags);
 
<span class="p_add">+	if (is_hugepd(__hugepd(p4d_val(*p4d)))) {</span>
<span class="p_add">+		page = follow_huge_pd(vma, address,</span>
<span class="p_add">+				      __hugepd(p4d_val(*p4d)), flags,</span>
<span class="p_add">+				      P4D_SHIFT);</span>
<span class="p_add">+		if (page)</span>
<span class="p_add">+			return page;</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	}</span>
 	return follow_pud_mask(vma, address, p4d, flags, page_mask);
 }
 
<span class="p_chunk">@@ -363,6 +388,14 @@</span> <span class="p_context"> struct page *follow_page_mask(struct vm_area_struct *vma,</span>
 			return page;
 		return no_page_table(vma, flags);
 	}
<span class="p_add">+	if (is_hugepd(__hugepd(pgd_val(*pgd)))) {</span>
<span class="p_add">+		page = follow_huge_pd(vma, address,</span>
<span class="p_add">+				      __hugepd(pgd_val(*pgd)), flags,</span>
<span class="p_add">+				      PGDIR_SHIFT);</span>
<span class="p_add">+		if (page)</span>
<span class="p_add">+			return page;</span>
<span class="p_add">+		return no_page_table(vma, flags);</span>
<span class="p_add">+	}</span>
 
 	return follow_p4d_mask(vma, address, pgd, flags, page_mask);
 }
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index a12d3cab04fe..58307d62ac37 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -4643,6 +4643,14 @@</span> <span class="p_context"> follow_huge_addr(struct mm_struct *mm, unsigned long address,</span>
 }
 
 struct page * __weak
<span class="p_add">+follow_huge_pd(struct vm_area_struct *vma,</span>
<span class="p_add">+	       unsigned long address, hugepd_t hpd, int flags, int pdshift)</span>
<span class="p_add">+{</span>
<span class="p_add">+	WARN(1, &quot;hugepd follow called with no support for hugepage directory format\n&quot;);</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct page * __weak</span>
 follow_huge_pmd(struct mm_struct *mm, unsigned long address,
 		pmd_t *pmd, int flags)
 {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



