
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>mm, hugetlb: use memory policy when available - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    mm, hugetlb: use memory policy when available</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2302">Dave Hansen</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 20, 2015, 7:53 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20151020195317.ADA052D8@viggo.jf.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/7450081/mbox/"
   >mbox</a>
|
   <a href="/patch/7450081/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/7450081/">/patch/7450081/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 59AEA9F302
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 20 Oct 2015 19:53:29 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 4999620870
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 20 Oct 2015 19:53:28 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 241C32086D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 20 Oct 2015 19:53:26 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752449AbbJTTxW (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 20 Oct 2015 15:53:22 -0400
Received: from mga14.intel.com ([192.55.52.115]:47296 &quot;EHLO mga14.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751621AbbJTTxR (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 20 Oct 2015 15:53:17 -0400
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
	by fmsmga103.fm.intel.com with ESMTP; 20 Oct 2015 12:53:17 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.17,708,1437462000&quot;; d=&quot;scan&#39;208&quot;;a=&quot;815536843&quot;
Received: from viggo.jf.intel.com (HELO localhost.localdomain)
	([10.54.39.19])
	by fmsmga001.fm.intel.com with ESMTP; 20 Oct 2015 12:53:17 -0700
Subject: [PATCH] mm, hugetlb: use memory policy when available
To: dave@sr71.net
Cc: akpm@linux-foundation.org, n-horiguchi@ah.jp.nec.com,
	mike.kravetz@oracle.com, hillf.zj@alibaba-inc.com,
	rientjes@google.com, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org, dave.hansen@linux.intel.com
From: Dave Hansen &lt;dave@sr71.net&gt;
Date: Tue, 20 Oct 2015 12:53:17 -0700
Message-Id: &lt;20151020195317.ADA052D8@viggo.jf.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2302">Dave Hansen</a> - Oct. 20, 2015, 7:53 p.m.</div>
<pre class="content">
<span class="from">From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>

I have a hugetlbfs user which is never explicitly allocating huge pages
with &#39;nr_hugepages&#39;.  They only set &#39;nr_overcommit_hugepages&#39; and then let
the pages be allocated from the buddy allocator at fault time.

This works, but they noticed that mbind() was not doing them any good and
the pages were being allocated without respect for the policy they
specified.

The code in question is this:
<span class="quote">
&gt; struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
...
<span class="quote">&gt;         page = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);</span>
<span class="quote">&gt;         if (!page) {</span>
<span class="quote">&gt;                 page = alloc_buddy_huge_page(h, NUMA_NO_NODE);</span>

dequeue_huge_page_vma() is smart and will respect the VMA&#39;s memory policy.
But, it only grabs _existing_ huge pages from the huge page pool.  If the
pool is empty, we fall back to alloc_buddy_huge_page() which obviously
can&#39;t do anything with the VMA&#39;s policy because it isn&#39;t even passed the
VMA.

Almost everybody preallocates huge pages.  That&#39;s probably why nobody has
ever noticed this.  Looking back at the git history, I don&#39;t think this
_ever_ worked from when alloc_buddy_huge_page() was introduced in 7893d1d5,
8 years ago.

The fix is to pass vma/addr down in to the places where we actually call in
to the buddy allocator.  It&#39;s fairly straightforward plumbing.  This has
been lightly tested.

Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;
Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Cc: Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;
Cc: David Rientjes &lt;rientjes@google.com&gt;
Cc: linux-mm@kvack.org
Cc: linux-kernel@vger.kernel.org
<span class="signed-off-by">Signed-off-by: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>
---

 b/mm/hugetlb.c |  111 ++++++++++++++++++++++++++++++++++++++++++++++++++-------
 1 file changed, 99 insertions(+), 12 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=41">Andrew Morton</a> - Oct. 20, 2015, 10:19 p.m.</div>
<pre class="content">
On Tue, 20 Oct 2015 12:53:17 -0700 Dave Hansen &lt;dave@sr71.net&gt; wrote:
<span class="quote">
&gt; </span>
<span class="quote">&gt; From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have a hugetlbfs user which is never explicitly allocating huge pages</span>
<span class="quote">&gt; with &#39;nr_hugepages&#39;.  They only set &#39;nr_overcommit_hugepages&#39; and then let</span>
<span class="quote">&gt; the pages be allocated from the buddy allocator at fault time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This works, but they noticed that mbind() was not doing them any good and</span>
<span class="quote">&gt; the pages were being allocated without respect for the policy they</span>
<span class="quote">&gt; specified.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The code in question is this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt; &gt;         page = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);</span>
<span class="quote">&gt; &gt;         if (!page) {</span>
<span class="quote">&gt; &gt;                 page = alloc_buddy_huge_page(h, NUMA_NO_NODE);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; dequeue_huge_page_vma() is smart and will respect the VMA&#39;s memory policy.</span>
<span class="quote">&gt; But, it only grabs _existing_ huge pages from the huge page pool.  If the</span>
<span class="quote">&gt; pool is empty, we fall back to alloc_buddy_huge_page() which obviously</span>
<span class="quote">&gt; can&#39;t do anything with the VMA&#39;s policy because it isn&#39;t even passed the</span>
<span class="quote">&gt; VMA.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Almost everybody preallocates huge pages.  That&#39;s probably why nobody has</span>
<span class="quote">&gt; ever noticed this.  Looking back at the git history, I don&#39;t think this</span>
<span class="quote">&gt; _ever_ worked from when alloc_buddy_huge_page() was introduced in 7893d1d5,</span>
<span class="quote">&gt; 8 years ago.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The fix is to pass vma/addr down in to the places where we actually call in</span>
<span class="quote">&gt; to the buddy allocator.  It&#39;s fairly straightforward plumbing.  This has</span>
<span class="quote">&gt; been lightly tested.</span>

huh.  Fair enough.
<span class="quote">
&gt;  b/mm/hugetlb.c |  111 ++++++++++++++++++++++++++++++++++++++++++++++++++-------</span>

Is it worth deporking this for the CONFIG_NUMA=n case?


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Oct. 21, 2015, 3:12 p.m.</div>
<pre class="content">
On Tue, Oct 20, 2015 at 12:53:17PM -0700, Dave Hansen wrote:
<span class="quote">&gt; @@ -1445,6 +1514,10 @@ static struct page *alloc_buddy_huge_pag</span>
<span class="quote">&gt;  	if (hstate_is_gigantic(h))</span>
<span class="quote">&gt;  		return NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (vma || addr) {</span>
<span class="quote">&gt; +		WARN_ON_ONCE(!addr || addr == -1);</span>

Trinity triggered the WARN for me:

[  118.647212] WARNING: CPU: 10 PID: 9621 at /home/kas/linux/mm/mm/hugetlb.c:1514 __alloc_buddy_huge_page+0x2c8/0x300()
[  118.648698] Modules linked in:
[  118.649105] CPU: 10 PID: 9621 Comm: trinity-c147 Not tainted 4.2.0-dirty #651
[  118.649909] Hardware name: QEMU Standard PC (Q35 + ICH9, 2009), BIOS Debian-1.8.2-1 04/01/2014
[  118.650929]  ffffffff81ca6ad8 ffff88081f7f3c68 ffffffff818a9977 0000000080000001
[  118.651889]  0000000000000000 ffff88081f7f3ca8 ffffffff810574d6 ffff88081f7f3c98
[  118.652965]  0000000000000000 ffffffff830a87e0 00000000ffffffff ffffffffffffffff
[  118.653988] Call Trace:
[  118.654315]  [&lt;ffffffff818a9977&gt;] dump_stack+0x4f/0x7b
[  118.654936]  [&lt;ffffffff810574d6&gt;] warn_slowpath_common+0x86/0xc0
[  118.655630]  [&lt;ffffffff810575ca&gt;] warn_slowpath_null+0x1a/0x20
[  118.656427]  [&lt;ffffffff811ac5e8&gt;] __alloc_buddy_huge_page+0x2c8/0x300
[  118.657185]  [&lt;ffffffff811ad081&gt;] hugetlb_acct_memory+0xa1/0x3d0
[  118.657897]  [&lt;ffffffff811ab241&gt;] ? region_chg+0x1f1/0x200
[  118.658559]  [&lt;ffffffff811ae932&gt;] hugetlb_reserve_pages+0x92/0x250
[  118.659289]  [&lt;ffffffff812d517c&gt;] hugetlb_file_setup+0x14c/0x320
[  118.659994]  [&lt;ffffffff813d2fd5&gt;] newseg+0x135/0x370
[  118.660713]  [&lt;ffffffff813cc134&gt;] ? ipcget+0x44/0x2d0
[  118.661306]  [&lt;ffffffff813cc360&gt;] ipcget+0x270/0x2d0
[  118.661911]  [&lt;ffffffff813d3525&gt;] SyS_shmget+0x45/0x50
[  118.663409]  [&lt;ffffffff818b2c7c&gt;] tracesys_phase2+0x84/0x89
[  118.664199] ---[ end trace d2829191292b44ef ]---
<span class="quote">

&gt; +		WARN_ON_ONCE(nid != NUMA_NO_NODE);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt;  	/*</span>
<span class="quote">&gt;  	 * Assume we will successfully allocate the surplus page to</span>
<span class="quote">&gt;  	 * prevent racing processes from causing the surplus to exceed</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=49271">Sasha Levin</a> - Oct. 22, 2015, 9:39 p.m.</div>
<pre class="content">
On 10/20/2015 03:53 PM, Dave Hansen wrote:
<span class="quote">&gt; From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have a hugetlbfs user which is never explicitly allocating huge pages</span>
<span class="quote">&gt; with &#39;nr_hugepages&#39;.  They only set &#39;nr_overcommit_hugepages&#39; and then let</span>
<span class="quote">&gt; the pages be allocated from the buddy allocator at fault time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This works, but they noticed that mbind() was not doing them any good and</span>
<span class="quote">&gt; the pages were being allocated without respect for the policy they</span>
<span class="quote">&gt; specified.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The code in question is this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; &gt; struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;&gt; &gt;         page = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);</span>
<span class="quote">&gt;&gt; &gt;         if (!page) {</span>
<span class="quote">&gt;&gt; &gt;                 page = alloc_buddy_huge_page(h, NUMA_NO_NODE);</span>
<span class="quote">&gt; dequeue_huge_page_vma() is smart and will respect the VMA&#39;s memory policy.</span>
<span class="quote">&gt; But, it only grabs _existing_ huge pages from the huge page pool.  If the</span>
<span class="quote">&gt; pool is empty, we fall back to alloc_buddy_huge_page() which obviously</span>
<span class="quote">&gt; can&#39;t do anything with the VMA&#39;s policy because it isn&#39;t even passed the</span>
<span class="quote">&gt; VMA.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Almost everybody preallocates huge pages.  That&#39;s probably why nobody has</span>
<span class="quote">&gt; ever noticed this.  Looking back at the git history, I don&#39;t think this</span>
<span class="quote">&gt; _ever_ worked from when alloc_buddy_huge_page() was introduced in 7893d1d5,</span>
<span class="quote">&gt; 8 years ago.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The fix is to pass vma/addr down in to the places where we actually call in</span>
<span class="quote">&gt; to the buddy allocator.  It&#39;s fairly straightforward plumbing.  This has</span>
<span class="quote">&gt; been lightly tested.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt; Cc: Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;</span>
<span class="quote">&gt; Cc: David Rientjes &lt;rientjes@google.com&gt;</span>
<span class="quote">&gt; Cc: linux-mm@kvack.org</span>
<span class="quote">&gt; Cc: linux-kernel@vger.kernel.org</span>
<span class="quote">&gt; Signed-off-by: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>

Hey Dave,

Trinity seems to be able to hit the newly added warnings pretty easily:

[  339.282065] WARNING: CPU: 4 PID: 10181 at mm/hugetlb.c:1520 __alloc_buddy_huge_page+0xff/0xa80()
[  339.360228] Modules linked in:
[  339.360838] CPU: 4 PID: 10181 Comm: trinity-c291 Not tainted 4.3.0-rc6-next-20151022-sasha-00040-g5ecc711-dirty #2608
[  339.362629]  ffff88015e59c000 00000000e6475701 ffff88015e61f9a0 ffffffff9dd3ef48
[  339.363896]  0000000000000000 ffff88015e61f9e0 ffffffff9c32d1ca ffffffff9c7175bf
[  339.365167]  ffffffffabddc0c8 ffff88015e61faf0 0000000000000000 ffffffffffffffff
[  339.366387] Call Trace:
[  339.366831]  [&lt;ffffffff9dd3ef48&gt;] dump_stack+0x4e/0x86
[  339.367648]  [&lt;ffffffff9c32d1ca&gt;] warn_slowpath_common+0xfa/0x120
[  339.368635]  [&lt;ffffffff9c7175bf&gt;] ? __alloc_buddy_huge_page+0xff/0xa80
[  339.369631]  [&lt;ffffffff9c32d3ca&gt;] warn_slowpath_null+0x1a/0x20
[  339.370574]  [&lt;ffffffff9c7175bf&gt;] __alloc_buddy_huge_page+0xff/0xa80
[  339.371551]  [&lt;ffffffff9c7174c0&gt;] ? return_unused_surplus_pages+0x120/0x120
[  339.372698]  [&lt;ffffffff9dda0327&gt;] ? debug_smp_processor_id+0x17/0x20
[  339.373683]  [&lt;ffffffff9c41574b&gt;] ? get_lock_stats+0x1b/0x80
[  339.374551]  [&lt;ffffffff9c42e901&gt;] ? __raw_callee_save___pv_queued_spin_unlock+0x11/0x20
[  339.375744]  [&lt;ffffffff9c433870&gt;] ? do_raw_spin_unlock+0x1d0/0x1e0
[  339.376728]  [&lt;ffffffff9c718333&gt;] hugetlb_acct_memory+0x193/0x990
[  339.377663]  [&lt;ffffffff9c7181a0&gt;] ? dequeue_huge_page_node+0x260/0x260
[  339.378658]  [&lt;ffffffff9c41c970&gt;] ? trace_hardirqs_on_caller+0x540/0x5e0
[  339.379671]  [&lt;ffffffff9c71e469&gt;] hugetlb_reserve_pages+0x229/0x330
[  339.380738]  [&lt;ffffffff9cba273b&gt;] hugetlb_file_setup+0x54b/0x810
[  339.381689]  [&lt;ffffffff9cba21f0&gt;] ? hugetlbfs_fallocate+0x9e0/0x9e0
[  339.382653]  [&lt;ffffffff9dd669f0&gt;] ? scnprintf+0x100/0x100
[  339.383526]  [&lt;ffffffff9da638af&gt;] newseg+0x49f/0xa70
[  339.384371]  [&lt;ffffffff9dda0327&gt;] ? debug_smp_processor_id+0x17/0x20
[  339.385345]  [&lt;ffffffff9da63410&gt;] ? shm_try_destroy_orphaned+0x190/0x190
[  339.386365]  [&lt;ffffffff9da52cf0&gt;] ? ipcget+0x60/0x510
[  339.387139]  [&lt;ffffffff9da52d1f&gt;] ipcget+0x8f/0x510
[  339.387902]  [&lt;ffffffff9c0046f0&gt;] ? do_audit_syscall_entry+0x2b0/0x2b0
[  339.388931]  [&lt;ffffffff9da64e1a&gt;] SyS_shmget+0x11a/0x160
[  339.389737]  [&lt;ffffffff9da64d00&gt;] ? is_file_shm_hugepages+0x40/0x40
[  339.393268]  [&lt;ffffffff9c006ac2&gt;] ? syscall_trace_enter_phase2+0x462/0x5f0
[  339.395643]  [&lt;ffffffffa55ce0f8&gt;] tracesys_phase2+0x88/0x8d


Thanks,
Sasha
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2302">Dave Hansen</a> - Oct. 22, 2015, 9:42 p.m.</div>
<pre class="content">
On 10/22/2015 02:39 PM, Sasha Levin wrote:
<span class="quote">&gt; Trinity seems to be able to hit the newly added warnings pretty easily:</span>

Kirill reported the same thing.  Is it fixed with this applied?
<span class="quote">
&gt; http://ozlabs.org/~akpm/mmots/broken-out/mm-hugetlb-use-memory-policy-when-available-fix.patch</span>


--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=49271">Sasha Levin</a> - Nov. 3, 2015, 7:12 p.m.</div>
<pre class="content">
On 10/22/2015 05:42 PM, Dave Hansen wrote:
<span class="quote">&gt; On 10/22/2015 02:39 PM, Sasha Levin wrote:</span>
<span class="quote">&gt;&gt; &gt; Trinity seems to be able to hit the newly added warnings pretty easily:</span>
<span class="quote">&gt; Kirill reported the same thing.  Is it fixed with this applied?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; &gt; http://ozlabs.org/~akpm/mmots/broken-out/mm-hugetlb-use-memory-policy-when-available-fix.patch</span>

Yup, that works for me.


Thanks,
Sasha
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - Nov. 5, 2015, 1:47 p.m.</div>
<pre class="content">
On 10/20/2015 09:53 PM, Dave Hansen wrote:
<span class="quote">&gt; From: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I have a hugetlbfs user which is never explicitly allocating huge pages</span>
<span class="quote">&gt; with &#39;nr_hugepages&#39;.  They only set &#39;nr_overcommit_hugepages&#39; and then let</span>
<span class="quote">&gt; the pages be allocated from the buddy allocator at fault time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This works, but they noticed that mbind() was not doing them any good and</span>
<span class="quote">&gt; the pages were being allocated without respect for the policy they</span>
<span class="quote">&gt; specified.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The code in question is this:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt;&gt;         page = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);</span>
<span class="quote">&gt;&gt;         if (!page) {</span>
<span class="quote">&gt;&gt;                 page = alloc_buddy_huge_page(h, NUMA_NO_NODE);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; dequeue_huge_page_vma() is smart and will respect the VMA&#39;s memory policy.</span>
<span class="quote">&gt; But, it only grabs _existing_ huge pages from the huge page pool.  If the</span>
<span class="quote">&gt; pool is empty, we fall back to alloc_buddy_huge_page() which obviously</span>
<span class="quote">&gt; can&#39;t do anything with the VMA&#39;s policy because it isn&#39;t even passed the</span>
<span class="quote">&gt; VMA.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Almost everybody preallocates huge pages.  That&#39;s probably why nobody has</span>
<span class="quote">&gt; ever noticed this.  Looking back at the git history, I don&#39;t think this</span>
<span class="quote">&gt; _ever_ worked from when alloc_buddy_huge_page() was introduced in 7893d1d5,</span>
<span class="quote">&gt; 8 years ago.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The fix is to pass vma/addr down in to the places where we actually call in</span>
<span class="quote">&gt; to the buddy allocator.  It&#39;s fairly straightforward plumbing.  This has</span>
<span class="quote">&gt; been lightly tested.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt; Cc: Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;</span>
<span class="quote">&gt; Cc: David Rientjes &lt;rientjes@google.com&gt;</span>
<span class="quote">&gt; Cc: linux-mm@kvack.org</span>
<span class="quote">&gt; Cc: linux-kernel@vger.kernel.org</span>
<span class="quote">&gt; Signed-off-by: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;</span>

Together with the fix and NUMA=n cleanup

Acked=by: Vlastimil Babka &lt;vbabka@suse.cz&gt;

--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff -puN mm/hugetlb.c~hugetlbfs-fix-mbind-when-demand-allocating mm/hugetlb.c</span>
<span class="p_header">--- a/mm/hugetlb.c~hugetlbfs-fix-mbind-when-demand-allocating	2015-10-20 12:50:55.598628613 -0700</span>
<span class="p_header">+++ b/mm/hugetlb.c	2015-10-20 12:50:55.605628929 -0700</span>
<span class="p_chunk">@@ -1437,7 +1437,76 @@</span> <span class="p_context"> void dissolve_free_huge_pages(unsigned l</span>
 		dissolve_free_huge_page(pfn_to_page(pfn));
 }
 
<span class="p_del">-static struct page *alloc_buddy_huge_page(struct hstate *h, int nid)</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * There are 3 ways this can get called:</span>
<span class="p_add">+ * 1. With vma+addr: we use the VMA&#39;s memory policy</span>
<span class="p_add">+ * 2. With !vma, but nid=NUMA_NO_NODE:  We try to allocate a huge</span>
<span class="p_add">+ *    page from any node, and let the buddy allocator itself figure</span>
<span class="p_add">+ *    it out.</span>
<span class="p_add">+ * 3. With !vma, but nid!=NUMA_NO_NODE.  We allocate a huge page</span>
<span class="p_add">+ *    strictly from &#39;nid&#39;</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct page *__hugetlb_alloc_buddy_huge_page(struct hstate *h,</span>
<span class="p_add">+		struct vm_area_struct *vma, unsigned long addr, int nid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int order = huge_page_order(h);</span>
<span class="p_add">+	gfp_t gfp = htlb_alloc_mask(h)|__GFP_COMP|__GFP_REPEAT|__GFP_NOWARN;</span>
<span class="p_add">+	unsigned int cpuset_mems_cookie;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We need a VMA to get a memory policy.  If we do not</span>
<span class="p_add">+	 * have one, we use the &#39;nid&#39; argument</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!vma) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If a specific node is requested, make sure to</span>
<span class="p_add">+		 * get memory from there, but only when a node</span>
<span class="p_add">+		 * is explicitly specified.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (nid != NUMA_NO_NODE)</span>
<span class="p_add">+			gfp |= __GFP_THISNODE;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Make sure to call something that can handle</span>
<span class="p_add">+		 * nid=NUMA_NO_NODE</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		return alloc_pages_node(nid, gfp, order);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * OK, so we have a VMA.  Fetch the mempolicy and try to</span>
<span class="p_add">+	 * allocate a huge page with it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		struct mempolicy *mpol;</span>
<span class="p_add">+		struct zonelist *zl;</span>
<span class="p_add">+		nodemask_t *nodemask;</span>
<span class="p_add">+</span>
<span class="p_add">+		cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="p_add">+		zl = huge_zonelist(vma, addr, gfp, &amp;mpol, &amp;nodemask);</span>
<span class="p_add">+		mpol_cond_put(mpol);</span>
<span class="p_add">+		page = __alloc_pages_nodemask(gfp, order, zl, nodemask);</span>
<span class="p_add">+		if (page)</span>
<span class="p_add">+			return page;</span>
<span class="p_add">+	} while (read_mems_allowed_retry(cpuset_mems_cookie));</span>
<span class="p_add">+</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * There are two ways to allocate a huge page:</span>
<span class="p_add">+ * 1. When you have a VMA and an address (like a fault)</span>
<span class="p_add">+ * 2. When you have no VMA (like when setting /proc/.../nr_hugepages)</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * &#39;vma&#39; and &#39;addr&#39; are only for (1).  &#39;nid&#39; is always NUMA_NO_NODE in</span>
<span class="p_add">+ * this case which signifies that the allocation should be done with</span>
<span class="p_add">+ * respect for the VMA&#39;s memory policy.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * For (2), we ignore &#39;vma&#39; and &#39;addr&#39; and use &#39;nid&#39; exclusively. This</span>
<span class="p_add">+ * implies that memory policies will not be taken in to account.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static struct page *__alloc_buddy_huge_page(struct hstate *h,</span>
<span class="p_add">+		struct vm_area_struct *vma, unsigned long addr, int nid)</span>
 {
 	struct page *page;
 	unsigned int r_nid;
<span class="p_chunk">@@ -1445,6 +1514,10 @@</span> <span class="p_context"> static struct page *alloc_buddy_huge_pag</span>
 	if (hstate_is_gigantic(h))
 		return NULL;
 
<span class="p_add">+	if (vma || addr) {</span>
<span class="p_add">+		WARN_ON_ONCE(!addr || addr == -1);</span>
<span class="p_add">+		WARN_ON_ONCE(nid != NUMA_NO_NODE);</span>
<span class="p_add">+	}</span>
 	/*
 	 * Assume we will successfully allocate the surplus page to
 	 * prevent racing processes from causing the surplus to exceed
<span class="p_chunk">@@ -1478,14 +1551,7 @@</span> <span class="p_context"> static struct page *alloc_buddy_huge_pag</span>
 	}
 	spin_unlock(&amp;hugetlb_lock);
 
<span class="p_del">-	if (nid == NUMA_NO_NODE)</span>
<span class="p_del">-		page = alloc_pages(htlb_alloc_mask(h)|__GFP_COMP|</span>
<span class="p_del">-				   __GFP_REPEAT|__GFP_NOWARN,</span>
<span class="p_del">-				   huge_page_order(h));</span>
<span class="p_del">-	else</span>
<span class="p_del">-		page = __alloc_pages_node(nid,</span>
<span class="p_del">-			htlb_alloc_mask(h)|__GFP_COMP|__GFP_THISNODE|</span>
<span class="p_del">-			__GFP_REPEAT|__GFP_NOWARN, huge_page_order(h));</span>
<span class="p_add">+	page = __hugetlb_alloc_buddy_huge_page(h, vma, addr, nid);</span>
 
 	spin_lock(&amp;hugetlb_lock);
 	if (page) {
<span class="p_chunk">@@ -1510,6 +1576,27 @@</span> <span class="p_context"> static struct page *alloc_buddy_huge_pag</span>
 }
 
 /*
<span class="p_add">+ * Allocate a huge page from &#39;nid&#39;.  Note, &#39;nid&#39; may be</span>
<span class="p_add">+ * NUMA_NO_NODE, which means that it may be allocated</span>
<span class="p_add">+ * anywhere.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct page *__alloc_buddy_huge_page_no_mpol(struct hstate *h, int nid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	return __alloc_buddy_huge_page(h, NULL, addr, nid);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Use the VMA&#39;s mpolicy to allocate a huge page from the buddy.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct page *__alloc_buddy_huge_page_with_mpol(struct hstate *h,</span>
<span class="p_add">+		struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return __alloc_buddy_huge_page(h, vma, addr, NUMA_NO_NODE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * This allocation function is useful in the context where vma is irrelevant.
  * E.g. soft-offlining uses this function because it only cares physical
  * address of error page.
<span class="p_chunk">@@ -1524,7 +1611,7 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct</span>
 	spin_unlock(&amp;hugetlb_lock);
 
 	if (!page)
<span class="p_del">-		page = alloc_buddy_huge_page(h, nid);</span>
<span class="p_add">+		page = __alloc_buddy_huge_page_no_mpol(h, nid);</span>
 
 	return page;
 }
<span class="p_chunk">@@ -1554,7 +1641,7 @@</span> <span class="p_context"> static int gather_surplus_pages(struct h</span>
 retry:
 	spin_unlock(&amp;hugetlb_lock);
 	for (i = 0; i &lt; needed; i++) {
<span class="p_del">-		page = alloc_buddy_huge_page(h, NUMA_NO_NODE);</span>
<span class="p_add">+		page = __alloc_buddy_huge_page_no_mpol(h, NUMA_NO_NODE);</span>
 		if (!page) {
 			alloc_ok = false;
 			break;
<span class="p_chunk">@@ -1787,7 +1874,7 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_a</span>
 	page = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, gbl_chg);
 	if (!page) {
 		spin_unlock(&amp;hugetlb_lock);
<span class="p_del">-		page = alloc_buddy_huge_page(h, NUMA_NO_NODE);</span>
<span class="p_add">+		page = __alloc_buddy_huge_page_with_mpol(h, vma, addr);</span>
 		if (!page)
 			goto out_uncharge_cgroup;
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



