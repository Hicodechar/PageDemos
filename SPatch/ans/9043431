
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4/5] iommu/mediatek: add support for mtk iommu generation one HW - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4/5] iommu/mediatek: add support for mtk iommu generation one HW</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=166241">honghui.zhang@mediatek.com</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 9, 2016, 8 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1462780816-5288-5-git-send-email-honghui.zhang@mediatek.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9043431/mbox/"
   >mbox</a>
|
   <a href="/patch/9043431/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9043431/">/patch/9043431/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 2FF07BF29F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 May 2016 08:01:38 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 880E5200FE
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 May 2016 08:01:36 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id AB5D2200E0
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  9 May 2016 08:01:34 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752640AbcEIIBJ (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 9 May 2016 04:01:09 -0400
Received: from mailgw02.mediatek.com ([210.61.82.184]:12092 &quot;EHLO
	mailgw02.mediatek.com&quot; rhost-flags-OK-FAIL-OK-FAIL) by
	vger.kernel.org with ESMTP id S1752576AbcEIIBF (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 9 May 2016 04:01:05 -0400
Received: from mtkhts07.mediatek.inc [(172.21.101.69)] by
	mailgw02.mediatek.com (envelope-from &lt;honghui.zhang@mediatek.com&gt;)
	(mhqrelay.mediatek.com ESMTP with TLS)
	with ESMTP id 2064866740; Mon, 09 May 2016 16:00:56 +0800
Received: from mhfsdcap03.localdomain (10.17.3.153) by mtkhts07.mediatek.inc
	(172.21.101.73) with Microsoft SMTP Server id 14.3.266.1;
	Mon, 9 May 2016 16:00:54 +0800
From: &lt;honghui.zhang@mediatek.com&gt;
To: &lt;joro@8bytes.org&gt;, &lt;treding@nvidia.com&gt;, &lt;mark.rutland@arm.com&gt;,
	&lt;matthias.bgg@gmail.com&gt;
CC: &lt;p.zabel@pengutronix.de&gt;, &lt;devicetree@vger.kernel.org&gt;,
	&lt;pebolle@tiscali.nl&gt;, &lt;kendrick.hsu@mediatek.com&gt;, &lt;arnd@arndb.de&gt;,
	&lt;srv_heupstream@mediatek.com&gt;, &lt;catalin.marinas@arm.com&gt;,
	&lt;will.deacon@arm.com&gt;, &lt;linux-kernel@vger.kernel.org&gt;,
	&lt;tfiga@google.com&gt;, &lt;iommu@lists.linux-foundation.org&gt;,
	&lt;robh+dt@kernel.org&gt;, &lt;djkurtz@google.com&gt;,
	&lt;kernel@pengutronix.de&gt;, &lt;linux-mediatek@lists.infradead.org&gt;,
	&lt;linux-arm-kernel@lists.infradead.org&gt;, &lt;l.stach@pengutronix.de&gt;,
	&lt;yingjoe.chen@mediatek.com&gt;, &lt;eddie.huang@mediatek.com&gt;,
	&lt;youlin.pei@mediatek.com&gt;, &lt;erin.lo@mediatek.com&gt;,
	Honghui Zhang &lt;honghui.zhang@mediatek.com&gt;
Subject: [PATCH 4/5] iommu/mediatek: add support for mtk iommu generation
	one HW
Date: Mon, 9 May 2016 16:00:15 +0800
Message-ID: &lt;1462780816-5288-5-git-send-email-honghui.zhang@mediatek.com&gt;
X-Mailer: git-send-email 1.8.1.1.dirty
In-Reply-To: &lt;1462780816-5288-1-git-send-email-honghui.zhang@mediatek.com&gt;
References: &lt;1462780816-5288-1-git-send-email-honghui.zhang@mediatek.com&gt;
MIME-Version: 1.0
Content-Type: text/plain
X-MTK: N
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-9.0 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=166241">honghui.zhang@mediatek.com</a> - May 9, 2016, 8 a.m.</div>
<pre class="content">
<span class="from">From: Honghui Zhang &lt;honghui.zhang@mediatek.com&gt;</span>

Mediatek SoC&#39;s M4U have two generations of HW architcture. Generation one
use flat, one layer pagetable, and was shipped with ARM architecture, it
only support 4K size page mapping. MT2701 SoC use this generation one
m4u HW. Generation two uses the ARM short-descriptor translation table
format for address translation, and was shipped with ARM64 architecture,
MT8173 use this generation two m4u HW. All the two generation iommu HW
only have one iommu domain, and all it&#39;s iommu clients share the same
iova address.

These two generation m4u HW have slit different register groups and
register offset, but most register names are the same. This patch add iommu
support for mediatek SoC mt2701.
<span class="signed-off-by">
Signed-off-by: Honghui Zhang &lt;honghui.zhang@mediatek.com&gt;</span>
---
 drivers/iommu/Kconfig        |  19 ++
 drivers/iommu/Makefile       |   1 +
 drivers/iommu/mtk_iommu.h    |   4 +
 drivers/iommu/mtk_iommu_v1.c | 767 +++++++++++++++++++++++++++++++++++++++++++
 4 files changed, 791 insertions(+)
 create mode 100644 drivers/iommu/mtk_iommu_v1.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=77581">Robin Murphy</a> - May 10, 2016, 10:28 a.m.</div>
<pre class="content">
On 09/05/16 09:00, honghui.zhang@mediatek.com wrote:
[...]
<span class="quote">&gt; +static void *mtk_iommu_alloc_pgt(struct device *dev, size_t size, gfp_t gfp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	dma_addr_t dma;</span>
<span class="quote">&gt; +	void *pages = alloc_pages_exact(size, gfp | __GFP_ZERO);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (!pages)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	dma = dma_map_single(dev, pages, size, DMA_TO_DEVICE);</span>
<span class="quote">&gt; +	if (dma_mapping_error(dev, dma))</span>
<span class="quote">&gt; +		goto out_free;</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * We depend on the IOMMU being able to work with any physical</span>
<span class="quote">&gt; +	 * address directly, so if the DMA layer suggests otherwise by</span>
<span class="quote">&gt; +	 * translating or truncating them, that bodes very badly...</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	if (dma != virt_to_phys(pages))</span>
<span class="quote">&gt; +		goto out_unmap;</span>

Given that you&#39;ve only got a single table to allocate, and at 4MB it has 
a fair chance of failing beyond early boot time, just use 
dma_alloc_coherent() - you don&#39;t need to care about the dma &lt;-&gt; phys 
relationship because you don&#39;t have multi-level tables to walk. That 
way, you can get rid of all the awkward streaming DMA stuff, and also 
benefit from CMA to avoid allocation failures.
<span class="quote">
&gt; +	kmemleak_ignore(pages);</span>
<span class="quote">&gt; +	return pages;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +out_unmap:</span>
<span class="quote">&gt; +	dev_err(dev, &quot;Cannot accommodate DMA translation for IOMMU page tables\n&quot;);</span>
<span class="quote">&gt; +	dma_unmap_single(dev, dma, size, DMA_TO_DEVICE);</span>
<span class="quote">&gt; +out_free:</span>
<span class="quote">&gt; +	free_pages_exact(pages, size);</span>
<span class="quote">&gt; +	return NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void mtk_iommu_free_pgt(struct device *dev, void *pages, size_t size)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	dma_unmap_single(dev, (dma_addr_t)virt_to_phys(pages),</span>
<span class="quote">&gt; +			 size, DMA_TO_DEVICE);</span>
<span class="quote">&gt; +	free_pages_exact(pages, size);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int mtk_iommu_domain_finalise(struct mtk_iommu_data *data)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mtk_iommu_domain *dom = data-&gt;m4u_dom;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_init(&amp;dom-&gt;pgtlock);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	dom-&gt;pgt_va = mtk_iommu_alloc_pgt(data-&gt;dev,</span>
<span class="quote">&gt; +				dom-&gt;pgt_size, GFP_KERNEL);</span>
<span class="quote">&gt; +	if (!dom-&gt;pgt_va)</span>
<span class="quote">&gt; +		return -ENOMEM;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	dom-&gt;pgt_pa = virt_to_phys(dom-&gt;pgt_va);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	writel(dom-&gt;pgt_pa, data-&gt;base + REG_MMU_PT_BASE_ADDR);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	dom-&gt;cookie = (void *)data;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static struct iommu_domain *mtk_iommu_domain_alloc(unsigned type)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mtk_iommu_domain *dom;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (type != IOMMU_DOMAIN_UNMANAGED)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	dom = kzalloc(sizeof(*dom), GFP_KERNEL);</span>
<span class="quote">&gt; +	if (!dom)</span>
<span class="quote">&gt; +		return NULL;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * MTK m4u support 4GB iova address space, and oly support 4K page</span>
<span class="quote">&gt; +	 * mapping. So the pagetable size should be exactly as 4M.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	dom-&gt;pgt_size = SZ_4M;</span>

If the table size is fixed, then why bother having a variable at all?
<span class="quote">
&gt; +	return &amp;dom-&gt;domain;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void mtk_iommu_domain_free(struct iommu_domain *domain)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	kfree(to_mtk_domain(domain));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>

[...]
<span class="quote">
&gt; +static int mtk_iommu_map(struct iommu_domain *domain, unsigned long iova,</span>
<span class="quote">&gt; +			 phys_addr_t paddr, size_t size, int prot)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct mtk_iommu_domain *dom = to_mtk_domain(domain);</span>
<span class="quote">&gt; +	struct mtk_iommu_data *data = dom-&gt;cookie;</span>
<span class="quote">&gt; +	unsigned int page_num = size &gt;&gt; MTK_IOMMU_PAGE_SHIFT;</span>

Since you only advertise a single page size, this will always be 1, so 
you could either get rid of the loop here...
<span class="quote">
&gt; +	unsigned long flags;</span>
<span class="quote">&gt; +	unsigned int i;</span>
<span class="quote">&gt; +	u32 *pgt_base_iova;</span>
<span class="quote">&gt; +	u32 pabase = (u32)paddr;</span>
<span class="quote">&gt; +	int map_size = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	spin_lock_irqsave(&amp;dom-&gt;pgtlock, flags);</span>
<span class="quote">&gt; +	pgt_base_iova = dom-&gt;pgt_va + (iova  &gt;&gt; MTK_IOMMU_PAGE_SHIFT);</span>
<span class="quote">&gt; +	for (i = 0; i &lt; page_num; i++) {</span>
<span class="quote">&gt; +		pgt_base_iova[i] = pabase | F_DESC_VALID | F_DESC_NONSEC;</span>
<span class="quote">&gt; +		pabase += MTK_IOMMU_PAGE_SIZE;</span>
<span class="quote">&gt; +		map_size += MTK_IOMMU_PAGE_SIZE;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	dma_sync_single_for_device(data-&gt;dev,</span>
<span class="quote">&gt; +			dom-&gt;pgt_pa + (iova &gt;&gt; MTK_IOMMU_PAGE_SHIFT),</span>
<span class="quote">&gt; +			(size &gt;&gt; MTK_IOMMU_PAGE_SHIFT) * sizeof(u32),</span>
<span class="quote">&gt; +			DMA_TO_DEVICE);</span>
<span class="quote">&gt; +	spin_unlock_irqrestore(&amp;dom-&gt;pgtlock, flags);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mtk_iommu_tlb_flush_range(data, iova, size);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return map_size;</span>
<span class="quote">&gt; +}</span>

[...]
<span class="quote">
&gt; +static struct iommu_ops mtk_iommu_ops = {</span>
<span class="quote">&gt; +	.domain_alloc	= mtk_iommu_domain_alloc,</span>
<span class="quote">&gt; +	.domain_free	= mtk_iommu_domain_free,</span>
<span class="quote">&gt; +	.attach_dev	= mtk_iommu_attach_device,</span>
<span class="quote">&gt; +	.detach_dev	= mtk_iommu_detach_device,</span>
<span class="quote">&gt; +	.map		= mtk_iommu_map,</span>
<span class="quote">&gt; +	.unmap		= mtk_iommu_unmap,</span>
<span class="quote">&gt; +	.map_sg		= default_iommu_map_sg,</span>
<span class="quote">&gt; +	.iova_to_phys	= mtk_iommu_iova_to_phys,</span>
<span class="quote">&gt; +	.add_device	= mtk_iommu_add_device,</span>
<span class="quote">&gt; +	.remove_device	= mtk_iommu_remove_device,</span>
<span class="quote">&gt; +	.device_group	= mtk_iommu_device_group,</span>
<span class="quote">&gt; +	.pgsize_bitmap	= MTK_IOMMU_PAGE_SIZE,</span>
<span class="quote">&gt; +};</span>

...or perhaps advertise .pgsize_bitmap = ~0UL &lt;&lt; MTK_IOMMU_PAGE_SHIFT 
here, so you actually can handle multiple entries at once for larger 
mappings - given how simple the page table format is that doesn&#39;t seem 
too unreasonable, especially since it should give you a big efficiency 
win in terms of TLB maintenance.

Robin.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=166241">honghui.zhang@mediatek.com</a> - May 12, 2016, 11:26 a.m.</div>
<pre class="content">
hi, Robin, thanks very much for your comments.

On 5/10/2016 6:28 PM, Robin Murphy wrote:
<span class="quote">&gt; On 09/05/16 09:00, honghui.zhang@mediatek.com wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt;&gt; +static void *mtk_iommu_alloc_pgt(struct device *dev, size_t size, gfp_t gfp)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    dma_addr_t dma;</span>
<span class="quote">&gt;&gt; +    void *pages = alloc_pages_exact(size, gfp | __GFP_ZERO);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    if (!pages)</span>
<span class="quote">&gt;&gt; +        return NULL;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    dma = dma_map_single(dev, pages, size, DMA_TO_DEVICE);</span>
<span class="quote">&gt;&gt; +    if (dma_mapping_error(dev, dma))</span>
<span class="quote">&gt;&gt; +        goto out_free;</span>
<span class="quote">&gt;&gt; +    /*</span>
<span class="quote">&gt;&gt; +     * We depend on the IOMMU being able to work with any physical</span>
<span class="quote">&gt;&gt; +     * address directly, so if the DMA layer suggests otherwise by</span>
<span class="quote">&gt;&gt; +     * translating or truncating them, that bodes very badly...</span>
<span class="quote">&gt;&gt; +     */</span>
<span class="quote">&gt;&gt; +    if (dma != virt_to_phys(pages))</span>
<span class="quote">&gt;&gt; +        goto out_unmap;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Given that you&#39;ve only got a single table to allocate, and at 4MB it has a fair chance of failing beyond early boot time, just use dma_alloc_coherent() - you don&#39;t need to care about the dma &lt;-&gt; phys relationship because you don&#39;t have multi-level tables to walk. That way, you can get rid of all the awkward streaming DMA stuff, and also benefit from CMA to avoid allocation failures.</span>
<span class="quote">&gt; </span>

thanks, use dma_alloc_coherent is good for me.
<span class="quote">
&gt;&gt; +    kmemleak_ignore(pages);</span>
<span class="quote">&gt;&gt; +    return pages;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +out_unmap:</span>
<span class="quote">&gt;&gt; +    dev_err(dev, &quot;Cannot accommodate DMA translation for IOMMU page tables\n&quot;);</span>
<span class="quote">&gt;&gt; +    dma_unmap_single(dev, dma, size, DMA_TO_DEVICE);</span>
<span class="quote">&gt;&gt; +out_free:</span>
<span class="quote">&gt;&gt; +    free_pages_exact(pages, size);</span>
<span class="quote">&gt;&gt; +    return NULL;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void mtk_iommu_free_pgt(struct device *dev, void *pages, size_t size)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    dma_unmap_single(dev, (dma_addr_t)virt_to_phys(pages),</span>
<span class="quote">&gt;&gt; +             size, DMA_TO_DEVICE);</span>
<span class="quote">&gt;&gt; +    free_pages_exact(pages, size);</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static int mtk_iommu_domain_finalise(struct mtk_iommu_data *data)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    struct mtk_iommu_domain *dom = data-&gt;m4u_dom;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    spin_lock_init(&amp;dom-&gt;pgtlock);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    dom-&gt;pgt_va = mtk_iommu_alloc_pgt(data-&gt;dev,</span>
<span class="quote">&gt;&gt; +                dom-&gt;pgt_size, GFP_KERNEL);</span>
<span class="quote">&gt;&gt; +    if (!dom-&gt;pgt_va)</span>
<span class="quote">&gt;&gt; +        return -ENOMEM;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    dom-&gt;pgt_pa = virt_to_phys(dom-&gt;pgt_va);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    writel(dom-&gt;pgt_pa, data-&gt;base + REG_MMU_PT_BASE_ADDR);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    dom-&gt;cookie = (void *)data;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    return 0;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static struct iommu_domain *mtk_iommu_domain_alloc(unsigned type)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    struct mtk_iommu_domain *dom;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    if (type != IOMMU_DOMAIN_UNMANAGED)</span>
<span class="quote">&gt;&gt; +        return NULL;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    dom = kzalloc(sizeof(*dom), GFP_KERNEL);</span>
<span class="quote">&gt;&gt; +    if (!dom)</span>
<span class="quote">&gt;&gt; +        return NULL;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    /*</span>
<span class="quote">&gt;&gt; +     * MTK m4u support 4GB iova address space, and oly support 4K page</span>
<span class="quote">&gt;&gt; +     * mapping. So the pagetable size should be exactly as 4M.</span>
<span class="quote">&gt;&gt; +     */</span>
<span class="quote">&gt;&gt; +    dom-&gt;pgt_size = SZ_4M;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If the table size is fixed, then why bother having a variable at all?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +    return &amp;dom-&gt;domain;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +static void mtk_iommu_domain_free(struct iommu_domain *domain)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    kfree(to_mtk_domain(domain));</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +static int mtk_iommu_map(struct iommu_domain *domain, unsigned long iova,</span>
<span class="quote">&gt;&gt; +             phys_addr_t paddr, size_t size, int prot)</span>
<span class="quote">&gt;&gt; +{</span>
<span class="quote">&gt;&gt; +    struct mtk_iommu_domain *dom = to_mtk_domain(domain);</span>
<span class="quote">&gt;&gt; +    struct mtk_iommu_data *data = dom-&gt;cookie;</span>
<span class="quote">&gt;&gt; +    unsigned int page_num = size &gt;&gt; MTK_IOMMU_PAGE_SHIFT;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Since you only advertise a single page size, this will always be 1, so you could either get rid of the loop here...</span>
<span class="quote">&gt; </span>

I would prefer your following advise to change the pgsize_bitmap. 
I would follow your advise in the next version.
Thanks very much.
<span class="quote">
&gt;&gt; +    unsigned long flags;</span>
<span class="quote">&gt;&gt; +    unsigned int i;</span>
<span class="quote">&gt;&gt; +    u32 *pgt_base_iova;</span>
<span class="quote">&gt;&gt; +    u32 pabase = (u32)paddr;</span>
<span class="quote">&gt;&gt; +    int map_size = 0;</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    spin_lock_irqsave(&amp;dom-&gt;pgtlock, flags);</span>
<span class="quote">&gt;&gt; +    pgt_base_iova = dom-&gt;pgt_va + (iova  &gt;&gt; MTK_IOMMU_PAGE_SHIFT);</span>
<span class="quote">&gt;&gt; +    for (i = 0; i &lt; page_num; i++) {</span>
<span class="quote">&gt;&gt; +        pgt_base_iova[i] = pabase | F_DESC_VALID | F_DESC_NONSEC;</span>
<span class="quote">&gt;&gt; +        pabase += MTK_IOMMU_PAGE_SIZE;</span>
<span class="quote">&gt;&gt; +        map_size += MTK_IOMMU_PAGE_SIZE;</span>
<span class="quote">&gt;&gt; +    }</span>
<span class="quote">&gt;&gt; +    dma_sync_single_for_device(data-&gt;dev,</span>
<span class="quote">&gt;&gt; +            dom-&gt;pgt_pa + (iova &gt;&gt; MTK_IOMMU_PAGE_SHIFT),</span>
<span class="quote">&gt;&gt; +            (size &gt;&gt; MTK_IOMMU_PAGE_SHIFT) * sizeof(u32),</span>
<span class="quote">&gt;&gt; +            DMA_TO_DEVICE);</span>
<span class="quote">&gt;&gt; +    spin_unlock_irqrestore(&amp;dom-&gt;pgtlock, flags);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    mtk_iommu_tlb_flush_range(data, iova, size);</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt; +    return map_size;</span>
<span class="quote">&gt;&gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; +static struct iommu_ops mtk_iommu_ops = {</span>
<span class="quote">&gt;&gt; +    .domain_alloc    = mtk_iommu_domain_alloc,</span>
<span class="quote">&gt;&gt; +    .domain_free    = mtk_iommu_domain_free,</span>
<span class="quote">&gt;&gt; +    .attach_dev    = mtk_iommu_attach_device,</span>
<span class="quote">&gt;&gt; +    .detach_dev    = mtk_iommu_detach_device,</span>
<span class="quote">&gt;&gt; +    .map        = mtk_iommu_map,</span>
<span class="quote">&gt;&gt; +    .unmap        = mtk_iommu_unmap,</span>
<span class="quote">&gt;&gt; +    .map_sg        = default_iommu_map_sg,</span>
<span class="quote">&gt;&gt; +    .iova_to_phys    = mtk_iommu_iova_to_phys,</span>
<span class="quote">&gt;&gt; +    .add_device    = mtk_iommu_add_device,</span>
<span class="quote">&gt;&gt; +    .remove_device    = mtk_iommu_remove_device,</span>
<span class="quote">&gt;&gt; +    .device_group    = mtk_iommu_device_group,</span>
<span class="quote">&gt;&gt; +    .pgsize_bitmap    = MTK_IOMMU_PAGE_SIZE,</span>
<span class="quote">&gt;&gt; +};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...or perhaps advertise .pgsize_bitmap = ~0UL &lt;&lt; MTK_IOMMU_PAGE_SHIFT here, so you actually can handle multiple entries at once for larger mappings - given how simple the page table format is that doesn&#39;t seem too unreasonable, especially since it should give you a big efficiency win in terms of TLB maintenance.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Robin.</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=166241">honghui.zhang@mediatek.com</a> - May 12, 2016, 12:41 p.m.</div>
<pre class="content">
Hi, Robin, Thanks very much for your comment, and sorry for the last
reply format.

On Tue, 2016-05-10 at 11:28 +0100, Robin Murphy wrote:
<span class="quote">&gt; On 09/05/16 09:00, honghui.zhang@mediatek.com wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; +static void *mtk_iommu_alloc_pgt(struct device *dev, size_t size, gfp_t gfp)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	dma_addr_t dma;</span>
<span class="quote">&gt; &gt; +	void *pages = alloc_pages_exact(size, gfp | __GFP_ZERO);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (!pages)</span>
<span class="quote">&gt; &gt; +		return NULL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	dma = dma_map_single(dev, pages, size, DMA_TO_DEVICE);</span>
<span class="quote">&gt; &gt; +	if (dma_mapping_error(dev, dma))</span>
<span class="quote">&gt; &gt; +		goto out_free;</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * We depend on the IOMMU being able to work with any physical</span>
<span class="quote">&gt; &gt; +	 * address directly, so if the DMA layer suggests otherwise by</span>
<span class="quote">&gt; &gt; +	 * translating or truncating them, that bodes very badly...</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	if (dma != virt_to_phys(pages))</span>
<span class="quote">&gt; &gt; +		goto out_unmap;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Given that you&#39;ve only got a single table to allocate, and at 4MB it has </span>
<span class="quote">&gt; a fair chance of failing beyond early boot time, just use </span>
<span class="quote">&gt; dma_alloc_coherent() - you don&#39;t need to care about the dma &lt;-&gt; phys </span>
<span class="quote">&gt; relationship because you don&#39;t have multi-level tables to walk. That </span>
<span class="quote">&gt; way, you can get rid of all the awkward streaming DMA stuff, and also </span>
<span class="quote">&gt; benefit from CMA to avoid allocation failures.</span>
<span class="quote">&gt; </span>

The dma_alloc_coheret interface is good enough for me, thanks.
<span class="quote">
&gt; &gt; +	kmemleak_ignore(pages);</span>
<span class="quote">&gt; &gt; +	return pages;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +out_unmap:</span>
<span class="quote">&gt; &gt; +	dev_err(dev, &quot;Cannot accommodate DMA translation for IOMMU page tables\n&quot;);</span>
<span class="quote">&gt; &gt; +	dma_unmap_single(dev, dma, size, DMA_TO_DEVICE);</span>
<span class="quote">&gt; &gt; +out_free:</span>
<span class="quote">&gt; &gt; +	free_pages_exact(pages, size);</span>
<span class="quote">&gt; &gt; +	return NULL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void mtk_iommu_free_pgt(struct device *dev, void *pages, size_t size)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	dma_unmap_single(dev, (dma_addr_t)virt_to_phys(pages),</span>
<span class="quote">&gt; &gt; +			 size, DMA_TO_DEVICE);</span>
<span class="quote">&gt; &gt; +	free_pages_exact(pages, size);</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static int mtk_iommu_domain_finalise(struct mtk_iommu_data *data)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct mtk_iommu_domain *dom = data-&gt;m4u_dom;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	spin_lock_init(&amp;dom-&gt;pgtlock);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	dom-&gt;pgt_va = mtk_iommu_alloc_pgt(data-&gt;dev,</span>
<span class="quote">&gt; &gt; +				dom-&gt;pgt_size, GFP_KERNEL);</span>
<span class="quote">&gt; &gt; +	if (!dom-&gt;pgt_va)</span>
<span class="quote">&gt; &gt; +		return -ENOMEM;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	dom-&gt;pgt_pa = virt_to_phys(dom-&gt;pgt_va);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	writel(dom-&gt;pgt_pa, data-&gt;base + REG_MMU_PT_BASE_ADDR);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	dom-&gt;cookie = (void *)data;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return 0;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static struct iommu_domain *mtk_iommu_domain_alloc(unsigned type)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct mtk_iommu_domain *dom;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	if (type != IOMMU_DOMAIN_UNMANAGED)</span>
<span class="quote">&gt; &gt; +		return NULL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	dom = kzalloc(sizeof(*dom), GFP_KERNEL);</span>
<span class="quote">&gt; &gt; +	if (!dom)</span>
<span class="quote">&gt; &gt; +		return NULL;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * MTK m4u support 4GB iova address space, and oly support 4K page</span>
<span class="quote">&gt; &gt; +	 * mapping. So the pagetable size should be exactly as 4M.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	dom-&gt;pgt_size = SZ_4M;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; If the table size is fixed, then why bother having a variable at all?</span>

I will follow your advise for next version.
thanks.
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +	return &amp;dom-&gt;domain;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +static void mtk_iommu_domain_free(struct iommu_domain *domain)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	kfree(to_mtk_domain(domain));</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +static int mtk_iommu_map(struct iommu_domain *domain, unsigned long iova,</span>
<span class="quote">&gt; &gt; +			 phys_addr_t paddr, size_t size, int prot)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	struct mtk_iommu_domain *dom = to_mtk_domain(domain);</span>
<span class="quote">&gt; &gt; +	struct mtk_iommu_data *data = dom-&gt;cookie;</span>
<span class="quote">&gt; &gt; +	unsigned int page_num = size &gt;&gt; MTK_IOMMU_PAGE_SHIFT;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Since you only advertise a single page size, this will always be 1, so </span>
<span class="quote">&gt; you could either get rid of the loop here...</span>

I would prefer your following advise to modify the pgsize_bitmap, thanks
very much.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; +	unsigned long flags;</span>
<span class="quote">&gt; &gt; +	unsigned int i;</span>
<span class="quote">&gt; &gt; +	u32 *pgt_base_iova;</span>
<span class="quote">&gt; &gt; +	u32 pabase = (u32)paddr;</span>
<span class="quote">&gt; &gt; +	int map_size = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	spin_lock_irqsave(&amp;dom-&gt;pgtlock, flags);</span>
<span class="quote">&gt; &gt; +	pgt_base_iova = dom-&gt;pgt_va + (iova  &gt;&gt; MTK_IOMMU_PAGE_SHIFT);</span>
<span class="quote">&gt; &gt; +	for (i = 0; i &lt; page_num; i++) {</span>
<span class="quote">&gt; &gt; +		pgt_base_iova[i] = pabase | F_DESC_VALID | F_DESC_NONSEC;</span>
<span class="quote">&gt; &gt; +		pabase += MTK_IOMMU_PAGE_SIZE;</span>
<span class="quote">&gt; &gt; +		map_size += MTK_IOMMU_PAGE_SIZE;</span>
<span class="quote">&gt; &gt; +	}</span>
<span class="quote">&gt; &gt; +	dma_sync_single_for_device(data-&gt;dev,</span>
<span class="quote">&gt; &gt; +			dom-&gt;pgt_pa + (iova &gt;&gt; MTK_IOMMU_PAGE_SHIFT),</span>
<span class="quote">&gt; &gt; +			(size &gt;&gt; MTK_IOMMU_PAGE_SHIFT) * sizeof(u32),</span>
<span class="quote">&gt; &gt; +			DMA_TO_DEVICE);</span>
<span class="quote">&gt; &gt; +	spin_unlock_irqrestore(&amp;dom-&gt;pgtlock, flags);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	mtk_iommu_tlb_flush_range(data, iova, size);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	return map_size;</span>
<span class="quote">&gt; &gt; +}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +static struct iommu_ops mtk_iommu_ops = {</span>
<span class="quote">&gt; &gt; +	.domain_alloc	= mtk_iommu_domain_alloc,</span>
<span class="quote">&gt; &gt; +	.domain_free	= mtk_iommu_domain_free,</span>
<span class="quote">&gt; &gt; +	.attach_dev	= mtk_iommu_attach_device,</span>
<span class="quote">&gt; &gt; +	.detach_dev	= mtk_iommu_detach_device,</span>
<span class="quote">&gt; &gt; +	.map		= mtk_iommu_map,</span>
<span class="quote">&gt; &gt; +	.unmap		= mtk_iommu_unmap,</span>
<span class="quote">&gt; &gt; +	.map_sg		= default_iommu_map_sg,</span>
<span class="quote">&gt; &gt; +	.iova_to_phys	= mtk_iommu_iova_to_phys,</span>
<span class="quote">&gt; &gt; +	.add_device	= mtk_iommu_add_device,</span>
<span class="quote">&gt; &gt; +	.remove_device	= mtk_iommu_remove_device,</span>
<span class="quote">&gt; &gt; +	.device_group	= mtk_iommu_device_group,</span>
<span class="quote">&gt; &gt; +	.pgsize_bitmap	= MTK_IOMMU_PAGE_SIZE,</span>
<span class="quote">&gt; &gt; +};</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...or perhaps advertise .pgsize_bitmap = ~0UL &lt;&lt; MTK_IOMMU_PAGE_SHIFT </span>
<span class="quote">&gt; here, so you actually can handle multiple entries at once for larger </span>
<span class="quote">&gt; mappings - given how simple the page table format is that doesn&#39;t seem </span>
<span class="quote">&gt; too unreasonable, especially since it should give you a big efficiency </span>
<span class="quote">&gt; win in terms of TLB maintenance.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Robin.</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/iommu/Kconfig b/drivers/iommu/Kconfig</span>
<span class="p_header">index dd1dc39..2e17d70 100644</span>
<span class="p_header">--- a/drivers/iommu/Kconfig</span>
<span class="p_header">+++ b/drivers/iommu/Kconfig</span>
<span class="p_chunk">@@ -354,4 +354,23 @@</span> <span class="p_context"> config MTK_IOMMU</span>
 
 	  If unsure, say N here.
 
<span class="p_add">+config MTK_IOMMU_V1</span>
<span class="p_add">+	bool &quot;MTK IOMMU Version 1 (M4U gen1) Support&quot;</span>
<span class="p_add">+	depends on ARM || ARM64</span>
<span class="p_add">+	depends on ARCH_MEDIATEK || COMPILE_TEST</span>
<span class="p_add">+	select ARM_DMA_USE_IOMMU</span>
<span class="p_add">+	select IOMMU_API</span>
<span class="p_add">+	select IOMMU_DMA</span>
<span class="p_add">+	select MEMORY</span>
<span class="p_add">+	select MTK_SMI</span>
<span class="p_add">+	select COMMON_CLK_MT2701_MMSYS</span>
<span class="p_add">+	select COMMON_CLK_MT2701_IMGSYS</span>
<span class="p_add">+	select COMMON_CLK_MT2701_VDECSYS</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Support for the M4U on certain Mediatek SoCs. M4U generation 1 HW is</span>
<span class="p_add">+	  Multimedia Memory Managememt Unit. This option enables remapping of</span>
<span class="p_add">+	  DMA memory accesses for the multimedia subsystem.</span>
<span class="p_add">+</span>
<span class="p_add">+	  if unsure, say N here.</span>
<span class="p_add">+</span>
 endif # IOMMU_SUPPORT
<span class="p_header">diff --git a/drivers/iommu/Makefile b/drivers/iommu/Makefile</span>
<span class="p_header">index c6edb31..778baf5 100644</span>
<span class="p_header">--- a/drivers/iommu/Makefile</span>
<span class="p_header">+++ b/drivers/iommu/Makefile</span>
<span class="p_chunk">@@ -18,6 +18,7 @@</span> <span class="p_context"> obj-$(CONFIG_INTEL_IOMMU_SVM) += intel-svm.o</span>
 obj-$(CONFIG_IPMMU_VMSA) += ipmmu-vmsa.o
 obj-$(CONFIG_IRQ_REMAP) += intel_irq_remapping.o irq_remapping.o
 obj-$(CONFIG_MTK_IOMMU) += mtk_iommu.o
<span class="p_add">+obj-$(CONFIG_MTK_IOMMU_V1) += mtk_iommu_v1.o</span>
 obj-$(CONFIG_OMAP_IOMMU) += omap-iommu.o
 obj-$(CONFIG_OMAP_IOMMU_DEBUG) += omap-iommu-debug.o
 obj-$(CONFIG_ROCKCHIP_IOMMU) += rockchip-iommu.o
<span class="p_header">diff --git a/drivers/iommu/mtk_iommu.h b/drivers/iommu/mtk_iommu.h</span>
<span class="p_header">index 5656355..c894784 100644</span>
<span class="p_header">--- a/drivers/iommu/mtk_iommu.h</span>
<span class="p_header">+++ b/drivers/iommu/mtk_iommu.h</span>
<span class="p_chunk">@@ -48,6 +48,10 @@</span> <span class="p_context"> struct mtk_iommu_domain {</span>
 	struct io_pgtable_ops		*iop;
 
 	struct iommu_domain		domain;
<span class="p_add">+	size_t				pgt_size;</span>
<span class="p_add">+	void				*pgt_va;</span>
<span class="p_add">+	dma_addr_t			pgt_pa;</span>
<span class="p_add">+	void				*cookie;</span>
 };
 
 struct mtk_iommu_data {
<span class="p_header">diff --git a/drivers/iommu/mtk_iommu_v1.c b/drivers/iommu/mtk_iommu_v1.c</span>
new file mode 100644
<span class="p_header">index 0000000..1fece92</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/drivers/iommu/mtk_iommu_v1.c</span>
<span class="p_chunk">@@ -0,0 +1,767 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (c) 2015-2016 MediaTek Inc.</span>
<span class="p_add">+ * Author: Yong Wu &lt;yong.wu@mediatek.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/bootmem.h&gt;</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+#include &lt;linux/clk.h&gt;</span>
<span class="p_add">+#include &lt;linux/component.h&gt;</span>
<span class="p_add">+#include &lt;linux/device.h&gt;</span>
<span class="p_add">+#include &lt;linux/dma-iommu.h&gt;</span>
<span class="p_add">+#include &lt;linux/err.h&gt;</span>
<span class="p_add">+#include &lt;linux/interrupt.h&gt;</span>
<span class="p_add">+#include &lt;linux/io.h&gt;</span>
<span class="p_add">+#include &lt;linux/iommu.h&gt;</span>
<span class="p_add">+#include &lt;linux/iopoll.h&gt;</span>
<span class="p_add">+#include &lt;linux/kmemleak.h&gt;</span>
<span class="p_add">+#include &lt;linux/list.h&gt;</span>
<span class="p_add">+#include &lt;linux/of_address.h&gt;</span>
<span class="p_add">+#include &lt;linux/of_iommu.h&gt;</span>
<span class="p_add">+#include &lt;linux/of_irq.h&gt;</span>
<span class="p_add">+#include &lt;linux/of_platform.h&gt;</span>
<span class="p_add">+#include &lt;linux/platform_device.h&gt;</span>
<span class="p_add">+#include &lt;linux/slab.h&gt;</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;asm/barrier.h&gt;</span>
<span class="p_add">+#include &lt;asm/dma-iommu.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+#include &lt;dt-bindings/memory/mt2701-larb-port.h&gt;</span>
<span class="p_add">+#include &lt;soc/mediatek/smi.h&gt;</span>
<span class="p_add">+#include &quot;mtk_iommu.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#define REG_MMU_PT_BASE_ADDR			0x000</span>
<span class="p_add">+</span>
<span class="p_add">+#define F_ALL_INVLD				0x2</span>
<span class="p_add">+#define F_MMU_INV_RANGE				0x1</span>
<span class="p_add">+#define F_INVLD_EN0				BIT(0)</span>
<span class="p_add">+#define F_INVLD_EN1				BIT(1)</span>
<span class="p_add">+</span>
<span class="p_add">+#define F_MMU_FAULT_VA_MSK			0xfffff000</span>
<span class="p_add">+#define MTK_PROTECT_PA_ALIGN			128</span>
<span class="p_add">+</span>
<span class="p_add">+#define REG_MMU_CTRL_REG			0x210</span>
<span class="p_add">+#define F_MMU_CTRL_COHERENT_EN			BIT(8)</span>
<span class="p_add">+#define REG_MMU_IVRP_PADDR			0x214</span>
<span class="p_add">+#define REG_MMU_INT_CONTROL			0x220</span>
<span class="p_add">+#define F_INT_TRANSLATION_FAULT			BIT(0)</span>
<span class="p_add">+#define F_INT_MAIN_MULTI_HIT_FAULT		BIT(1)</span>
<span class="p_add">+#define F_INT_INVALID_PA_FAULT			BIT(2)</span>
<span class="p_add">+#define F_INT_ENTRY_REPLACEMENT_FAULT		BIT(3)</span>
<span class="p_add">+#define F_INT_TABLE_WALK_FAULT			BIT(4)</span>
<span class="p_add">+#define F_INT_TLB_MISS_FAULT			BIT(5)</span>
<span class="p_add">+#define F_INT_PFH_DMA_FIFO_OVERFLOW		BIT(6)</span>
<span class="p_add">+#define F_INT_MISS_DMA_FIFO_OVERFLOW		BIT(7)</span>
<span class="p_add">+</span>
<span class="p_add">+#define F_MMU_TF_PROTECT_SEL(prot)		(((prot) &amp; 0x3) &lt;&lt; 5)</span>
<span class="p_add">+#define F_INT_CLR_BIT				BIT(12)</span>
<span class="p_add">+</span>
<span class="p_add">+#define REG_MMU_FAULT_ST			0x224</span>
<span class="p_add">+#define REG_MMU_FAULT_VA			0x228</span>
<span class="p_add">+#define REG_MMU_INVLD_PA			0x22C</span>
<span class="p_add">+#define REG_MMU_INT_ID				0x388</span>
<span class="p_add">+#define REG_MMU_INVALIDATE			0x5c0</span>
<span class="p_add">+#define REG_MMU_INVLD_START_A			0x5c4</span>
<span class="p_add">+#define REG_MMU_INVLD_END_A			0x5c8</span>
<span class="p_add">+</span>
<span class="p_add">+#define REG_MMU_INV_SEL				0x5d8</span>
<span class="p_add">+#define REG_MMU_STANDARD_AXI_MODE		0x5e8</span>
<span class="p_add">+</span>
<span class="p_add">+#define REG_MMU_DCM				0x5f0</span>
<span class="p_add">+#define F_MMU_DCM_ON				BIT(1)</span>
<span class="p_add">+#define REG_MMU_CPE_DONE			0x60c</span>
<span class="p_add">+#define F_DESC_VALID				0x2</span>
<span class="p_add">+#define F_DESC_NONSEC				BIT(3)</span>
<span class="p_add">+#define MT2701_M4U_TF_LARB(TF)			(6 - (((TF) &gt;&gt; 13) &amp; 0x7))</span>
<span class="p_add">+#define MT2701_M4U_TF_PORT(TF)			(((TF) &gt;&gt; 8) &amp; 0xF)</span>
<span class="p_add">+/* MTK generation one iommu HW only support 4K size mapping */</span>
<span class="p_add">+#define MTK_IOMMU_PAGE_SHIFT			12</span>
<span class="p_add">+#define MTK_IOMMU_PAGE_SIZE			(1UL &lt;&lt; MTK_IOMMU_PAGE_SHIFT)</span>
<span class="p_add">+</span>
<span class="p_add">+static const int mt2701_m4u_in_larb[] = {</span>
<span class="p_add">+	LARB0_PORT_OFFSET, LARB1_PORT_OFFSET,</span>
<span class="p_add">+	LARB2_PORT_OFFSET, LARB3_PORT_OFFSET</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int mt2701_m4u_to_larb(int id)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = ARRAY_SIZE(mt2701_m4u_in_larb); i &gt;= 0; i--)</span>
<span class="p_add">+		if ((id) &gt;= mt2701_m4u_in_larb[i])</span>
<span class="p_add">+			return i;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int mt2701_m4u_to_port(int id)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int larb = mt2701_m4u_to_larb(id);</span>
<span class="p_add">+</span>
<span class="p_add">+	return id - mt2701_m4u_in_larb[larb];</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mtk_iommu_tlb_flush_all(void *cookie)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_data *data = cookie;</span>
<span class="p_add">+</span>
<span class="p_add">+	writel_relaxed(F_INVLD_EN1 | F_INVLD_EN0,</span>
<span class="p_add">+			data-&gt;base + REG_MMU_INV_SEL);</span>
<span class="p_add">+	writel_relaxed(F_ALL_INVLD, data-&gt;base + REG_MMU_INVALIDATE);</span>
<span class="p_add">+	wmb(); /* Make sure the tlb flush all done */</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mtk_iommu_tlb_flush_range(void *cookie,</span>
<span class="p_add">+				unsigned long iova, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_data *data = cookie;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+	u32 tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	writel_relaxed(F_INVLD_EN1 | F_INVLD_EN0,</span>
<span class="p_add">+		data-&gt;base + REG_MMU_INV_SEL);</span>
<span class="p_add">+	writel_relaxed(iova &amp; F_MMU_FAULT_VA_MSK,</span>
<span class="p_add">+		data-&gt;base + REG_MMU_INVLD_START_A);</span>
<span class="p_add">+	writel_relaxed((iova + size - 1) &amp; F_MMU_FAULT_VA_MSK,</span>
<span class="p_add">+		data-&gt;base + REG_MMU_INVLD_END_A);</span>
<span class="p_add">+	writel_relaxed(F_MMU_INV_RANGE, data-&gt;base + REG_MMU_INVALIDATE);</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = readl_poll_timeout_atomic(data-&gt;base + REG_MMU_CPE_DONE,</span>
<span class="p_add">+				tmp, tmp != 0, 10, 100000);</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		dev_warn(data-&gt;dev,</span>
<span class="p_add">+			 &quot;Partial TLB flush timed out, falling back to full flush\n&quot;);</span>
<span class="p_add">+		mtk_iommu_tlb_flush_all(cookie);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	/* Clear the CPE status */</span>
<span class="p_add">+	writel_relaxed(0, data-&gt;base + REG_MMU_CPE_DONE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static irqreturn_t mtk_iommu_isr(int irq, void *dev_id)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_data *data = dev_id;</span>
<span class="p_add">+	struct mtk_iommu_domain *dom = data-&gt;m4u_dom;</span>
<span class="p_add">+	u32 int_state, regval, fault_iova, fault_pa;</span>
<span class="p_add">+	unsigned int fault_larb, fault_port;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Read error information from registers */</span>
<span class="p_add">+	int_state = readl_relaxed(data-&gt;base + REG_MMU_FAULT_ST);</span>
<span class="p_add">+	fault_iova = readl_relaxed(data-&gt;base + REG_MMU_FAULT_VA);</span>
<span class="p_add">+</span>
<span class="p_add">+	fault_iova &amp;= F_MMU_FAULT_VA_MSK;</span>
<span class="p_add">+	fault_pa = readl_relaxed(data-&gt;base + REG_MMU_INVLD_PA);</span>
<span class="p_add">+	regval = readl_relaxed(data-&gt;base + REG_MMU_INT_ID);</span>
<span class="p_add">+	fault_larb = MT2701_M4U_TF_LARB(regval);</span>
<span class="p_add">+	fault_port = MT2701_M4U_TF_PORT(regval);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * MTK v1 iommu HW could not determin whether the fault is read or</span>
<span class="p_add">+	 * write fault, report as read fault.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (report_iommu_fault(&amp;dom-&gt;domain, data-&gt;dev, fault_iova,</span>
<span class="p_add">+			IOMMU_FAULT_READ))</span>
<span class="p_add">+		dev_err_ratelimited(data-&gt;dev,</span>
<span class="p_add">+			&quot;fault type=0x%x iova=0x%x pa=0x%x larb=%d port=%d\n&quot;,</span>
<span class="p_add">+			int_state, fault_iova, fault_pa,</span>
<span class="p_add">+			fault_larb, fault_port);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Interrupt clear */</span>
<span class="p_add">+	regval = readl_relaxed(data-&gt;base + REG_MMU_INT_CONTROL);</span>
<span class="p_add">+	regval |= F_INT_CLR_BIT;</span>
<span class="p_add">+	writel_relaxed(regval, data-&gt;base + REG_MMU_INT_CONTROL);</span>
<span class="p_add">+</span>
<span class="p_add">+	mtk_iommu_tlb_flush_all(data);</span>
<span class="p_add">+</span>
<span class="p_add">+	return IRQ_HANDLED;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mtk_iommu_config(struct mtk_iommu_data *data,</span>
<span class="p_add">+			     struct device *dev, bool enable)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_client_priv *head, *cur, *next;</span>
<span class="p_add">+	struct mtk_smi_larb_iommu    *larb_mmu;</span>
<span class="p_add">+	unsigned int                 larbid, portid;</span>
<span class="p_add">+</span>
<span class="p_add">+	head = dev-&gt;archdata.iommu;</span>
<span class="p_add">+	list_for_each_entry_safe(cur, next, &amp;head-&gt;client, client) {</span>
<span class="p_add">+		larbid = mt2701_m4u_to_larb(cur-&gt;mtk_m4u_id);</span>
<span class="p_add">+		portid = mt2701_m4u_to_port(cur-&gt;mtk_m4u_id);</span>
<span class="p_add">+		larb_mmu = &amp;data-&gt;smi_imu.larb_imu[larbid];</span>
<span class="p_add">+</span>
<span class="p_add">+		dev_dbg(dev, &quot;%s iommu port: %d\n&quot;,</span>
<span class="p_add">+			enable ? &quot;enable&quot; : &quot;disable&quot;, portid);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (enable)</span>
<span class="p_add">+			larb_mmu-&gt;mmu |= MTK_SMI_MMU_EN(portid);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			larb_mmu-&gt;mmu &amp;= ~MTK_SMI_MMU_EN(portid);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void *mtk_iommu_alloc_pgt(struct device *dev, size_t size, gfp_t gfp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	dma_addr_t dma;</span>
<span class="p_add">+	void *pages = alloc_pages_exact(size, gfp | __GFP_ZERO);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!pages)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	dma = dma_map_single(dev, pages, size, DMA_TO_DEVICE);</span>
<span class="p_add">+	if (dma_mapping_error(dev, dma))</span>
<span class="p_add">+		goto out_free;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We depend on the IOMMU being able to work with any physical</span>
<span class="p_add">+	 * address directly, so if the DMA layer suggests otherwise by</span>
<span class="p_add">+	 * translating or truncating them, that bodes very badly...</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (dma != virt_to_phys(pages))</span>
<span class="p_add">+		goto out_unmap;</span>
<span class="p_add">+</span>
<span class="p_add">+	kmemleak_ignore(pages);</span>
<span class="p_add">+	return pages;</span>
<span class="p_add">+</span>
<span class="p_add">+out_unmap:</span>
<span class="p_add">+	dev_err(dev, &quot;Cannot accommodate DMA translation for IOMMU page tables\n&quot;);</span>
<span class="p_add">+	dma_unmap_single(dev, dma, size, DMA_TO_DEVICE);</span>
<span class="p_add">+out_free:</span>
<span class="p_add">+	free_pages_exact(pages, size);</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mtk_iommu_free_pgt(struct device *dev, void *pages, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	dma_unmap_single(dev, (dma_addr_t)virt_to_phys(pages),</span>
<span class="p_add">+			 size, DMA_TO_DEVICE);</span>
<span class="p_add">+	free_pages_exact(pages, size);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int mtk_iommu_domain_finalise(struct mtk_iommu_data *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_domain *dom = data-&gt;m4u_dom;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_init(&amp;dom-&gt;pgtlock);</span>
<span class="p_add">+</span>
<span class="p_add">+	dom-&gt;pgt_va = mtk_iommu_alloc_pgt(data-&gt;dev,</span>
<span class="p_add">+				dom-&gt;pgt_size, GFP_KERNEL);</span>
<span class="p_add">+	if (!dom-&gt;pgt_va)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	dom-&gt;pgt_pa = virt_to_phys(dom-&gt;pgt_va);</span>
<span class="p_add">+</span>
<span class="p_add">+	writel(dom-&gt;pgt_pa, data-&gt;base + REG_MMU_PT_BASE_ADDR);</span>
<span class="p_add">+</span>
<span class="p_add">+	dom-&gt;cookie = (void *)data;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct iommu_domain *mtk_iommu_domain_alloc(unsigned type)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_domain *dom;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (type != IOMMU_DOMAIN_UNMANAGED)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	dom = kzalloc(sizeof(*dom), GFP_KERNEL);</span>
<span class="p_add">+	if (!dom)</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * MTK m4u support 4GB iova address space, and oly support 4K page</span>
<span class="p_add">+	 * mapping. So the pagetable size should be exactly as 4M.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	dom-&gt;pgt_size = SZ_4M;</span>
<span class="p_add">+</span>
<span class="p_add">+	return &amp;dom-&gt;domain;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mtk_iommu_domain_free(struct iommu_domain *domain)</span>
<span class="p_add">+{</span>
<span class="p_add">+	kfree(to_mtk_domain(domain));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int mtk_iommu_attach_device(struct iommu_domain *domain,</span>
<span class="p_add">+				   struct device *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_domain *dom = to_mtk_domain(domain);</span>
<span class="p_add">+	struct mtk_iommu_client_priv *priv = dev-&gt;archdata.iommu;</span>
<span class="p_add">+	struct mtk_iommu_data *data;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!priv)</span>
<span class="p_add">+		return -ENODEV;</span>
<span class="p_add">+</span>
<span class="p_add">+	data = dev_get_drvdata(priv-&gt;m4udev);</span>
<span class="p_add">+	if (!data-&gt;m4u_dom) {</span>
<span class="p_add">+		data-&gt;m4u_dom = dom;</span>
<span class="p_add">+		ret = mtk_iommu_domain_finalise(data);</span>
<span class="p_add">+		if (ret) {</span>
<span class="p_add">+			data-&gt;m4u_dom = NULL;</span>
<span class="p_add">+			return ret;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else if (data-&gt;m4u_dom != dom) {</span>
<span class="p_add">+		/* All the client devices should be in the same m4u domain */</span>
<span class="p_add">+		dev_err(dev, &quot;try to attach into the error iommu domain\n&quot;);</span>
<span class="p_add">+		return -EPERM;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	mtk_iommu_config(data, dev, true);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mtk_iommu_detach_device(struct iommu_domain *domain,</span>
<span class="p_add">+				    struct device *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_client_priv *priv = dev-&gt;archdata.iommu;</span>
<span class="p_add">+	struct mtk_iommu_data *data;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!priv)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	data = dev_get_drvdata(priv-&gt;m4udev);</span>
<span class="p_add">+	mtk_iommu_config(data, dev, false);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int mtk_iommu_map(struct iommu_domain *domain, unsigned long iova,</span>
<span class="p_add">+			 phys_addr_t paddr, size_t size, int prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_domain *dom = to_mtk_domain(domain);</span>
<span class="p_add">+	struct mtk_iommu_data *data = dom-&gt;cookie;</span>
<span class="p_add">+	unsigned int page_num = size &gt;&gt; MTK_IOMMU_PAGE_SHIFT;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	unsigned int i;</span>
<span class="p_add">+	u32 *pgt_base_iova;</span>
<span class="p_add">+	u32 pabase = (u32)paddr;</span>
<span class="p_add">+	int map_size = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;dom-&gt;pgtlock, flags);</span>
<span class="p_add">+	pgt_base_iova = dom-&gt;pgt_va + (iova  &gt;&gt; MTK_IOMMU_PAGE_SHIFT);</span>
<span class="p_add">+	for (i = 0; i &lt; page_num; i++) {</span>
<span class="p_add">+		pgt_base_iova[i] = pabase | F_DESC_VALID | F_DESC_NONSEC;</span>
<span class="p_add">+		pabase += MTK_IOMMU_PAGE_SIZE;</span>
<span class="p_add">+		map_size += MTK_IOMMU_PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	dma_sync_single_for_device(data-&gt;dev,</span>
<span class="p_add">+			dom-&gt;pgt_pa + (iova &gt;&gt; MTK_IOMMU_PAGE_SHIFT),</span>
<span class="p_add">+			(size &gt;&gt; MTK_IOMMU_PAGE_SHIFT) * sizeof(u32),</span>
<span class="p_add">+			DMA_TO_DEVICE);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;dom-&gt;pgtlock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	mtk_iommu_tlb_flush_range(data, iova, size);</span>
<span class="p_add">+</span>
<span class="p_add">+	return map_size;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static size_t mtk_iommu_unmap(struct iommu_domain *domain,</span>
<span class="p_add">+			      unsigned long iova, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_domain *dom = to_mtk_domain(domain);</span>
<span class="p_add">+	struct mtk_iommu_data *data = dom-&gt;cookie;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	unsigned int *pgt_base_iova;</span>
<span class="p_add">+	unsigned int page_num = size &gt;&gt; MTK_IOMMU_PAGE_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;dom-&gt;pgtlock, flags);</span>
<span class="p_add">+	pgt_base_iova = dom-&gt;pgt_va + (iova  &gt;&gt; MTK_IOMMU_PAGE_SHIFT);</span>
<span class="p_add">+	memset(pgt_base_iova, 0, page_num * sizeof(u32));</span>
<span class="p_add">+	dma_sync_single_for_device(data-&gt;dev,</span>
<span class="p_add">+			dom-&gt;pgt_pa + (iova &gt;&gt; MTK_IOMMU_PAGE_SHIFT),</span>
<span class="p_add">+			(size &gt;&gt; MTK_IOMMU_PAGE_SHIFT) * sizeof(u32),</span>
<span class="p_add">+			DMA_TO_DEVICE);</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;dom-&gt;pgtlock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	mtk_iommu_tlb_flush_range(data, iova, size);</span>
<span class="p_add">+</span>
<span class="p_add">+	return size;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static phys_addr_t mtk_iommu_iova_to_phys(struct iommu_domain *domain,</span>
<span class="p_add">+					  dma_addr_t iova)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_domain *dom = to_mtk_domain(domain);</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	phys_addr_t pa;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;dom-&gt;pgtlock, flags);</span>
<span class="p_add">+	pa = *((u32 *)((u32 *)dom-&gt;pgt_va + (iova &gt;&gt; MTK_IOMMU_PAGE_SHIFT)));</span>
<span class="p_add">+	pa = pa &amp; (~(MTK_IOMMU_PAGE_SIZE - 1));</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;dom-&gt;pgtlock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	return pa;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * MTK generaion one iommu HW only support one iommu domain, and all the client</span>
<span class="p_add">+ * sharing the same iova address space.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int mtk_iommu_create_mapping(struct device *dev,</span>
<span class="p_add">+				    struct of_phandle_args *args)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_client_priv *head, *priv, *next;</span>
<span class="p_add">+	struct platform_device *m4updev;</span>
<span class="p_add">+	struct dma_iommu_mapping *mtk_mapping;</span>
<span class="p_add">+	struct device *m4udev;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (args-&gt;args_count != 1) {</span>
<span class="p_add">+		dev_err(dev, &quot;invalid #iommu-cells(%d) property for IOMMU\n&quot;,</span>
<span class="p_add">+			args-&gt;args_count);</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!dev-&gt;archdata.iommu) {</span>
<span class="p_add">+		/* Get the m4u device */</span>
<span class="p_add">+		m4updev = of_find_device_by_node(args-&gt;np);</span>
<span class="p_add">+		of_node_put(args-&gt;np);</span>
<span class="p_add">+		if (WARN_ON(!m4updev))</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+		head = kzalloc(sizeof(*head), GFP_KERNEL);</span>
<span class="p_add">+		if (!head)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+		dev-&gt;archdata.iommu = head;</span>
<span class="p_add">+		INIT_LIST_HEAD(&amp;head-&gt;client);</span>
<span class="p_add">+		head-&gt;m4udev = &amp;m4updev-&gt;dev;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		head = dev-&gt;archdata.iommu;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);</span>
<span class="p_add">+	if (!priv) {</span>
<span class="p_add">+		ret = -ENOMEM;</span>
<span class="p_add">+		goto err_free_mem;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	priv-&gt;mtk_m4u_id = args-&gt;args[0];</span>
<span class="p_add">+	list_add_tail(&amp;priv-&gt;client, &amp;head-&gt;client);</span>
<span class="p_add">+</span>
<span class="p_add">+	m4udev = head-&gt;m4udev;</span>
<span class="p_add">+	mtk_mapping = m4udev-&gt;archdata.iommu;</span>
<span class="p_add">+	if (!mtk_mapping) {</span>
<span class="p_add">+		/* MTK iommu support 4GB iova address space. */</span>
<span class="p_add">+		mtk_mapping = arm_iommu_create_mapping(&amp;platform_bus_type,</span>
<span class="p_add">+						0, 1ULL &lt;&lt; 32);</span>
<span class="p_add">+		if (IS_ERR(mtk_mapping)) {</span>
<span class="p_add">+			ret = PTR_ERR(mtk_mapping);</span>
<span class="p_add">+			goto err_free_mem;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		m4udev-&gt;archdata.iommu = mtk_mapping;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = arm_iommu_attach_device(dev, mtk_mapping);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		goto err_release_mapping;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+err_release_mapping:</span>
<span class="p_add">+	arm_iommu_release_mapping(mtk_mapping);</span>
<span class="p_add">+	m4udev-&gt;archdata.iommu = NULL;</span>
<span class="p_add">+err_free_mem:</span>
<span class="p_add">+	list_for_each_entry_safe(priv, next, &amp;head-&gt;client, client)</span>
<span class="p_add">+		kfree(priv);</span>
<span class="p_add">+	kfree(head);</span>
<span class="p_add">+	dev-&gt;archdata.iommu = NULL;</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int mtk_iommu_add_device(struct device *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct iommu_group *group;</span>
<span class="p_add">+	struct device_node *np;</span>
<span class="p_add">+	struct of_phandle_args iommu_spec;</span>
<span class="p_add">+	int idx = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	while (!of_parse_phandle_with_args(dev-&gt;of_node, &quot;iommus&quot;,</span>
<span class="p_add">+				   &quot;#iommu-cells&quot;, idx,</span>
<span class="p_add">+				   &amp;iommu_spec)) {</span>
<span class="p_add">+		np = iommu_spec.np;</span>
<span class="p_add">+		mtk_iommu_create_mapping(dev, &amp;iommu_spec);</span>
<span class="p_add">+</span>
<span class="p_add">+		of_node_put(np);</span>
<span class="p_add">+		idx++;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!dev-&gt;archdata.iommu) /* Not a iommu client device */</span>
<span class="p_add">+		return -ENODEV;</span>
<span class="p_add">+</span>
<span class="p_add">+	group = iommu_group_get_for_dev(dev);</span>
<span class="p_add">+	if (IS_ERR(group))</span>
<span class="p_add">+		return PTR_ERR(group);</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu_group_put(group);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mtk_iommu_remove_device(struct device *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_client_priv *head, *cur, *next;</span>
<span class="p_add">+</span>
<span class="p_add">+	head = dev-&gt;archdata.iommu;</span>
<span class="p_add">+	if (!head)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry_safe(cur, next, &amp;head-&gt;client, client) {</span>
<span class="p_add">+		list_del(&amp;cur-&gt;client);</span>
<span class="p_add">+		kfree(cur);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	kfree(head);</span>
<span class="p_add">+	dev-&gt;archdata.iommu = NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu_group_remove_device(dev);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct iommu_group *mtk_iommu_device_group(struct device *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_data *data;</span>
<span class="p_add">+	struct mtk_iommu_client_priv *priv;</span>
<span class="p_add">+</span>
<span class="p_add">+	priv = dev-&gt;archdata.iommu;</span>
<span class="p_add">+	if (!priv)</span>
<span class="p_add">+		return ERR_PTR(-ENODEV);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* All the client devices are in the same m4u iommu-group */</span>
<span class="p_add">+	data = dev_get_drvdata(priv-&gt;m4udev);</span>
<span class="p_add">+	if (!data-&gt;m4u_group) {</span>
<span class="p_add">+		data-&gt;m4u_group = iommu_group_alloc();</span>
<span class="p_add">+		if (IS_ERR(data-&gt;m4u_group))</span>
<span class="p_add">+			dev_err(dev, &quot;Failed to allocate M4U IOMMU group\n&quot;);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return data-&gt;m4u_group;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int mtk_iommu_hw_init(const struct mtk_iommu_data *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u32 regval;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = clk_prepare_enable(data-&gt;bclk);</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		dev_err(data-&gt;dev, &quot;Failed to enable iommu bclk(%d)\n&quot;, ret);</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	regval = F_MMU_CTRL_COHERENT_EN | F_MMU_TF_PROTECT_SEL(2);</span>
<span class="p_add">+	writel_relaxed(regval, data-&gt;base + REG_MMU_CTRL_REG);</span>
<span class="p_add">+</span>
<span class="p_add">+	regval = F_INT_TRANSLATION_FAULT |</span>
<span class="p_add">+		F_INT_MAIN_MULTI_HIT_FAULT |</span>
<span class="p_add">+		F_INT_INVALID_PA_FAULT |</span>
<span class="p_add">+		F_INT_ENTRY_REPLACEMENT_FAULT |</span>
<span class="p_add">+		F_INT_TABLE_WALK_FAULT |</span>
<span class="p_add">+		F_INT_TLB_MISS_FAULT |</span>
<span class="p_add">+		F_INT_PFH_DMA_FIFO_OVERFLOW |</span>
<span class="p_add">+		F_INT_MISS_DMA_FIFO_OVERFLOW;</span>
<span class="p_add">+	writel_relaxed(regval, data-&gt;base + REG_MMU_INT_CONTROL);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* protect memory,hw will write here while translation fault */</span>
<span class="p_add">+	writel_relaxed(data-&gt;protect_base,</span>
<span class="p_add">+			data-&gt;base + REG_MMU_IVRP_PADDR);</span>
<span class="p_add">+</span>
<span class="p_add">+	writel_relaxed(F_MMU_DCM_ON, data-&gt;base + REG_MMU_DCM);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (devm_request_irq(data-&gt;dev, data-&gt;irq, mtk_iommu_isr, 0,</span>
<span class="p_add">+			     dev_name(data-&gt;dev), (void *)data)) {</span>
<span class="p_add">+		writel_relaxed(0, data-&gt;base + REG_MMU_PT_BASE_ADDR);</span>
<span class="p_add">+		clk_disable_unprepare(data-&gt;bclk);</span>
<span class="p_add">+		dev_err(data-&gt;dev, &quot;Failed @ IRQ-%d Request\n&quot;, data-&gt;irq);</span>
<span class="p_add">+		return -ENODEV;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct iommu_ops mtk_iommu_ops = {</span>
<span class="p_add">+	.domain_alloc	= mtk_iommu_domain_alloc,</span>
<span class="p_add">+	.domain_free	= mtk_iommu_domain_free,</span>
<span class="p_add">+	.attach_dev	= mtk_iommu_attach_device,</span>
<span class="p_add">+	.detach_dev	= mtk_iommu_detach_device,</span>
<span class="p_add">+	.map		= mtk_iommu_map,</span>
<span class="p_add">+	.unmap		= mtk_iommu_unmap,</span>
<span class="p_add">+	.map_sg		= default_iommu_map_sg,</span>
<span class="p_add">+	.iova_to_phys	= mtk_iommu_iova_to_phys,</span>
<span class="p_add">+	.add_device	= mtk_iommu_add_device,</span>
<span class="p_add">+	.remove_device	= mtk_iommu_remove_device,</span>
<span class="p_add">+	.device_group	= mtk_iommu_device_group,</span>
<span class="p_add">+	.pgsize_bitmap	= MTK_IOMMU_PAGE_SIZE,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct of_device_id mtk_iommu_of_ids[] = {</span>
<span class="p_add">+	{ .compatible = &quot;mediatek,mt2701-m4u&quot;, },</span>
<span class="p_add">+	{}</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct component_master_ops mtk_iommu_com_ops = {</span>
<span class="p_add">+	.bind		= mtk_iommu_bind,</span>
<span class="p_add">+	.unbind		= mtk_iommu_unbind,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static int mtk_iommu_probe(struct platform_device *pdev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_data		*data;</span>
<span class="p_add">+	struct device			*dev = &amp;pdev-&gt;dev;</span>
<span class="p_add">+	struct resource			*res;</span>
<span class="p_add">+	struct component_match		*match = NULL;</span>
<span class="p_add">+	void				*protect;</span>
<span class="p_add">+	int				i, larb_nr, ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	data = devm_kzalloc(dev, sizeof(*data), GFP_KERNEL);</span>
<span class="p_add">+	if (!data)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+	data-&gt;dev = dev;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Protect memory. HW will access here while translation fault.*/</span>
<span class="p_add">+	protect = devm_kzalloc(dev, MTK_PROTECT_PA_ALIGN * 2, GFP_KERNEL);</span>
<span class="p_add">+	if (!protect)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	data-&gt;protect_base = ALIGN(virt_to_phys(protect), MTK_PROTECT_PA_ALIGN);</span>
<span class="p_add">+</span>
<span class="p_add">+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);</span>
<span class="p_add">+	data-&gt;base = devm_ioremap_resource(dev, res);</span>
<span class="p_add">+	if (IS_ERR(data-&gt;base))</span>
<span class="p_add">+		return PTR_ERR(data-&gt;base);</span>
<span class="p_add">+</span>
<span class="p_add">+	data-&gt;irq = platform_get_irq(pdev, 0);</span>
<span class="p_add">+	if (data-&gt;irq &lt; 0)</span>
<span class="p_add">+		return data-&gt;irq;</span>
<span class="p_add">+</span>
<span class="p_add">+	data-&gt;bclk = devm_clk_get(dev, &quot;bclk&quot;);</span>
<span class="p_add">+	if (IS_ERR(data-&gt;bclk))</span>
<span class="p_add">+		return PTR_ERR(data-&gt;bclk);</span>
<span class="p_add">+</span>
<span class="p_add">+	larb_nr = of_count_phandle_with_args(dev-&gt;of_node,</span>
<span class="p_add">+					&quot;mediatek,larbs&quot;, NULL);</span>
<span class="p_add">+	if (larb_nr &lt; 0)</span>
<span class="p_add">+		return larb_nr;</span>
<span class="p_add">+	data-&gt;smi_imu.larb_nr = larb_nr;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; larb_nr; i++) {</span>
<span class="p_add">+		struct device_node *larbnode;</span>
<span class="p_add">+		struct platform_device *plarbdev;</span>
<span class="p_add">+</span>
<span class="p_add">+		larbnode = of_parse_phandle(dev-&gt;of_node, &quot;mediatek,larbs&quot;, i);</span>
<span class="p_add">+		if (!larbnode)</span>
<span class="p_add">+			return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!of_device_is_available(larbnode))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		plarbdev = of_find_device_by_node(larbnode);</span>
<span class="p_add">+		of_node_put(larbnode);</span>
<span class="p_add">+		if (!plarbdev) {</span>
<span class="p_add">+			plarbdev = of_platform_device_create(</span>
<span class="p_add">+						larbnode, NULL,</span>
<span class="p_add">+						platform_bus_type.dev_root);</span>
<span class="p_add">+			if (!plarbdev)</span>
<span class="p_add">+				return -EPROBE_DEFER;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		data-&gt;smi_imu.larb_imu[i].dev = &amp;plarbdev-&gt;dev;</span>
<span class="p_add">+</span>
<span class="p_add">+		component_match_add(dev, &amp;match, compare_of, larbnode);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	platform_set_drvdata(pdev, data);</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = mtk_iommu_hw_init(data);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!iommu_present(&amp;platform_bus_type))</span>
<span class="p_add">+		bus_set_iommu(&amp;platform_bus_type,  &amp;mtk_iommu_ops);</span>
<span class="p_add">+</span>
<span class="p_add">+	return component_master_add_with_match(dev, &amp;mtk_iommu_com_ops, match);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int mtk_iommu_remove(struct platform_device *pdev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_data *data = platform_get_drvdata(pdev);</span>
<span class="p_add">+	struct mtk_iommu_domain *dom = data-&gt;m4u_dom;</span>
<span class="p_add">+	struct device *dev = &amp;pdev-&gt;dev;</span>
<span class="p_add">+</span>
<span class="p_add">+	mtk_iommu_free_pgt(dev, dom-&gt;pgt_va, dom-&gt;pgt_size);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (iommu_present(&amp;platform_bus_type))</span>
<span class="p_add">+		bus_set_iommu(&amp;platform_bus_type, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	clk_disable_unprepare(data-&gt;bclk);</span>
<span class="p_add">+	devm_free_irq(&amp;pdev-&gt;dev, data-&gt;irq, data);</span>
<span class="p_add">+	component_master_del(&amp;pdev-&gt;dev, &amp;mtk_iommu_com_ops);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __maybe_unused mtk_iommu_suspend(struct device *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_data *data = dev_get_drvdata(dev);</span>
<span class="p_add">+	struct mtk_iommu_suspend_reg *reg = &amp;data-&gt;reg;</span>
<span class="p_add">+	void __iomem *base = data-&gt;base;</span>
<span class="p_add">+</span>
<span class="p_add">+	reg-&gt;standard_axi_mode = readl_relaxed(base +</span>
<span class="p_add">+					       REG_MMU_STANDARD_AXI_MODE);</span>
<span class="p_add">+	reg-&gt;dcm_dis = readl_relaxed(base + REG_MMU_DCM);</span>
<span class="p_add">+	reg-&gt;ctrl_reg = readl_relaxed(base + REG_MMU_CTRL_REG);</span>
<span class="p_add">+	reg-&gt;int_control0 = readl_relaxed(base + REG_MMU_INT_CONTROL);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __maybe_unused mtk_iommu_resume(struct device *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mtk_iommu_data *data = dev_get_drvdata(dev);</span>
<span class="p_add">+	struct mtk_iommu_suspend_reg *reg = &amp;data-&gt;reg;</span>
<span class="p_add">+	void __iomem *base = data-&gt;base;</span>
<span class="p_add">+</span>
<span class="p_add">+	writel_relaxed(data-&gt;m4u_dom-&gt;pgt_pa, base + REG_MMU_PT_BASE_ADDR);</span>
<span class="p_add">+	writel_relaxed(reg-&gt;standard_axi_mode,</span>
<span class="p_add">+		       base + REG_MMU_STANDARD_AXI_MODE);</span>
<span class="p_add">+	writel_relaxed(reg-&gt;dcm_dis, base + REG_MMU_DCM);</span>
<span class="p_add">+	writel_relaxed(reg-&gt;ctrl_reg, base + REG_MMU_CTRL_REG);</span>
<span class="p_add">+	writel_relaxed(reg-&gt;int_control0, base + REG_MMU_INT_CONTROL);</span>
<span class="p_add">+	writel_relaxed(data-&gt;protect_base, base + REG_MMU_IVRP_PADDR);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+const struct dev_pm_ops mtk_iommu_pm_ops = {</span>
<span class="p_add">+	SET_SYSTEM_SLEEP_PM_OPS(mtk_iommu_suspend, mtk_iommu_resume)</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static struct platform_driver mtk_iommu_driver = {</span>
<span class="p_add">+	.probe	= mtk_iommu_probe,</span>
<span class="p_add">+	.remove	= mtk_iommu_remove,</span>
<span class="p_add">+	.driver	= {</span>
<span class="p_add">+		.name = &quot;mtk-iommu&quot;,</span>
<span class="p_add">+		.of_match_table = mtk_iommu_of_ids,</span>
<span class="p_add">+		.pm = &amp;mtk_iommu_pm_ops,</span>
<span class="p_add">+	}</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init m4u_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = platform_driver_register(&amp;mtk_iommu_driver);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		bus_set_iommu(&amp;platform_bus_type, NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __exit m4u_exit(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return platform_driver_unregister(&amp;mtk_iommu_driver);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+subsys_initcall(m4u_init);</span>
<span class="p_add">+module_exit(m4u_exit);</span>
<span class="p_add">+</span>
<span class="p_add">+MODULE_DESCRIPTION(&quot;IOMMU API for MTK architected m4u v1 implementations&quot;);</span>
<span class="p_add">+MODULE_AUTHOR(&quot;Honghui Zhang &lt;honghui.zhang@mediatek.com&gt;&quot;);</span>
<span class="p_add">+MODULE_LICENSE(&quot;GPL v2&quot;);</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



