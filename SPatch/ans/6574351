
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[3/4] mm: Defer flush of writable TLB entries - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [3/4] mm: Defer flush of writable TLB entries</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 9, 2015, 5:31 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1433871118-15207-4-git-send-email-mgorman@suse.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6574351/mbox/"
   >mbox</a>
|
   <a href="/patch/6574351/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6574351/">/patch/6574351/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 0804BC0020
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 17:33:04 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 4B5952034B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 17:33:02 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 3A02020397
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 17:33:01 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933169AbbFIRcx (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 9 Jun 2015 13:32:53 -0400
Received: from cantor2.suse.de ([195.135.220.15]:50288 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932466AbbFIRcK (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 9 Jun 2015 13:32:10 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay1.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 07E03ADD4;
	Tue,  9 Jun 2015 17:32:09 +0000 (UTC)
From: Mel Gorman &lt;mgorman@suse.de&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Minchan Kim &lt;minchan@kernel.org&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andi Kleen &lt;andi@firstfloor.org&gt;, H Peter Anvin &lt;hpa@zytor.com&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;, Linux-MM &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;
Subject: [PATCH 3/4] mm: Defer flush of writable TLB entries
Date: Tue,  9 Jun 2015 18:31:57 +0100
Message-Id: &lt;1433871118-15207-4-git-send-email-mgorman@suse.de&gt;
X-Mailer: git-send-email 2.3.5
In-Reply-To: &lt;1433871118-15207-1-git-send-email-mgorman@suse.de&gt;
References: &lt;1433871118-15207-1-git-send-email-mgorman@suse.de&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 9, 2015, 5:31 p.m.</div>
<pre class="content">
If a PTE is unmapped and it&#39;s dirty then it was writable recently. Due
to deferred TLB flushing, it&#39;s best to assume a writable TLB cache entry
exists. With that assumption, the TLB must be flushed before any IO can
start or the page is freed to avoid lost writes or data corruption. This
patch defers flushing of potentially writable TLBs as long as possible.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@suse.de&gt;</span>
---
 include/linux/sched.h |  7 +++++++
 mm/internal.h         |  4 ++++
 mm/rmap.c             | 28 +++++++++++++++++++++-------
 mm/vmscan.c           |  7 ++++++-
 4 files changed, 38 insertions(+), 8 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=181">Rik van Riel</a> - June 9, 2015, 8:02 p.m.</div>
<pre class="content">
On 06/09/2015 01:31 PM, Mel Gorman wrote:
<span class="quote">&gt; If a PTE is unmapped and it&#39;s dirty then it was writable recently. Due</span>
<span class="quote">&gt; to deferred TLB flushing, it&#39;s best to assume a writable TLB cache entry</span>
<span class="quote">&gt; exists. With that assumption, the TLB must be flushed before any IO can</span>
<span class="quote">&gt; start or the page is freed to avoid lost writes or data corruption. This</span>
<span class="quote">&gt; patch defers flushing of potentially writable TLBs as long as possible.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Mel Gorman &lt;mgorman@suse.de&gt;</span>
<span class="acked-by">
Acked-by: Rik van Riel &lt;riel@redhat.com&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - June 10, 2015, 7:50 a.m.</div>
<pre class="content">
* Mel Gorman &lt;mgorman@suse.de&gt; wrote:
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * If the PTE was dirty then it&#39;s best to assume it&#39;s writable. The</span>
<span class="quote">&gt; +	 * caller must use try_to_unmap_flush_dirty() or try_to_unmap_flush()</span>
<span class="quote">&gt; +	 * before the page any IO is initiated.</span>
<span class="quote">&gt; +	 */</span>

Speling nit: &quot;before the page any IO is initiated&quot; does not parse for me.
<span class="quote">
&gt; +			/*</span>
<span class="quote">&gt; +			 * Page is dirty. Flush the TLB if a writable entry</span>
<span class="quote">&gt; +			 * potentially exists to avoid CPU writes after IO</span>
<span class="quote">&gt; +			 * starts and then write it out here</span>
<span class="quote">&gt; +			 */</span>

s/here/here.

or:

s/here/here:

Thanks,

	Ingo
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 10, 2015, 8:17 a.m.</div>
<pre class="content">
On Wed, Jun 10, 2015 at 09:50:34AM +0200, Ingo Molnar wrote:
<span class="quote">&gt; </span>
<span class="quote">&gt; * Mel Gorman &lt;mgorman@suse.de&gt; wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * If the PTE was dirty then it&#39;s best to assume it&#39;s writable. The</span>
<span class="quote">&gt; &gt; +	 * caller must use try_to_unmap_flush_dirty() or try_to_unmap_flush()</span>
<span class="quote">&gt; &gt; +	 * before the page any IO is initiated.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Speling nit: &quot;before the page any IO is initiated&quot; does not parse for me.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; +			/*</span>
<span class="quote">&gt; &gt; +			 * Page is dirty. Flush the TLB if a writable entry</span>
<span class="quote">&gt; &gt; +			 * potentially exists to avoid CPU writes after IO</span>
<span class="quote">&gt; &gt; +			 * starts and then write it out here</span>
<span class="quote">&gt; &gt; +			 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; s/here/here.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; or:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; s/here/here:</span>
<span class="quote">&gt; </span>

Both fixed, thanks.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index d891e01f0445..6b787a7f6c38 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -1299,6 +1299,13 @@</span> <span class="p_context"> struct tlbflush_unmap_batch {</span>
 
 	/* True if any bit in cpumask is set */
 	bool flush_required;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If true then the PTE was dirty when unmapped. The entry must be</span>
<span class="p_add">+	 * flushed before IO is initiated or a stale TLB entry potentially</span>
<span class="p_add">+	 * allows an update without redirtying the page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	bool writable;</span>
 };
 
 struct task_struct {
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 465e621b86b1..4bbe774497b2 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -438,10 +438,14 @@</span> <span class="p_context"> struct tlbflush_unmap_batch;</span>
 
 #ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
 void try_to_unmap_flush(void);
<span class="p_add">+void try_to_unmap_flush_dirty(void);</span>
 #else
 static inline void try_to_unmap_flush(void)
 {
 }
<span class="p_add">+static inline void try_to_unmap_flush_dirty(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
 
 #endif /* CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH */
 #endif	/* __MM_INTERNAL_H */
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 4cadb60df74a..1e36b2fb3e95 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -623,16 +623,34 @@</span> <span class="p_context"> void try_to_unmap_flush(void)</span>
 	}
 	cpumask_clear(&amp;tlb_ubc-&gt;cpumask);
 	tlb_ubc-&gt;flush_required = false;
<span class="p_add">+	tlb_ubc-&gt;writable = false;</span>
 	put_cpu();
 }
 
<span class="p_add">+/* Flush iff there are potentially writable TLB entries that can race with IO */</span>
<span class="p_add">+void try_to_unmap_flush_dirty(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (tlb_ubc &amp;&amp; tlb_ubc-&gt;writable)</span>
<span class="p_add">+		try_to_unmap_flush();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void set_tlb_ubc_flush_pending(struct mm_struct *mm,
<span class="p_del">-		struct page *page)</span>
<span class="p_add">+		struct page *page, bool writable)</span>
 {
 	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;
 
 	cpumask_or(&amp;tlb_ubc-&gt;cpumask, &amp;tlb_ubc-&gt;cpumask, mm_cpumask(mm));
 	tlb_ubc-&gt;flush_required = true;
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If the PTE was dirty then it&#39;s best to assume it&#39;s writable. The</span>
<span class="p_add">+	 * caller must use try_to_unmap_flush_dirty() or try_to_unmap_flush()</span>
<span class="p_add">+	 * before the page any IO is initiated.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (writable)</span>
<span class="p_add">+		tlb_ubc-&gt;writable = true;</span>
 }
 
 /*
<span class="p_chunk">@@ -655,7 +673,7 @@</span> <span class="p_context"> static bool should_defer_flush(struct mm_struct *mm, enum ttu_flags flags)</span>
 }
 #else
 static void set_tlb_ubc_flush_pending(struct mm_struct *mm,
<span class="p_del">-		struct page *page)</span>
<span class="p_add">+		struct page *page, bool writable)</span>
 {
 }
 
<span class="p_chunk">@@ -1307,11 +1325,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 		 */
 		pteval = ptep_get_and_clear(mm, address, pte);
 
<span class="p_del">-		/* Potentially writable TLBs must be flushed before IO */</span>
<span class="p_del">-		if (pte_dirty(pteval))</span>
<span class="p_del">-			flush_tlb_page(vma, address);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			set_tlb_ubc_flush_pending(mm, page);</span>
<span class="p_add">+		set_tlb_ubc_flush_pending(mm, page, pte_dirty(pteval));</span>
 	} else {
 		pteval = ptep_clear_flush(vma, address, pte);
 	}
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index f16e07aaef59..c329eca98edf 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -1065,7 +1065,12 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 			if (!sc-&gt;may_writepage)
 				goto keep_locked;
 
<span class="p_del">-			/* Page is dirty, try to write it out here */</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Page is dirty. Flush the TLB if a writable entry</span>
<span class="p_add">+			 * potentially exists to avoid CPU writes after IO</span>
<span class="p_add">+			 * starts and then write it out here</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			try_to_unmap_flush_dirty();</span>
 			switch (pageout(page, mapping, sc)) {
 			case PAGE_KEEP:
 				goto keep_locked;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



