
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4/5] drm/i915: Use __sg_alloc_table_from_pages for allocating object backing store - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4/5] drm/i915: Use __sg_alloc_table_from_pages for allocating object backing store</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=170415">Tvrtko Ursulin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 21, 2016, 2:11 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1477059083-3500-5-git-send-email-tvrtko.ursulin@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9389317/mbox/"
   >mbox</a>
|
   <a href="/patch/9389317/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9389317/">/patch/9389317/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	2E1A7607F0 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 21 Oct 2016 14:12:58 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1E32F2A236
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 21 Oct 2016 14:12:58 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 1202B2A23A; Fri, 21 Oct 2016 14:12:58 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, RCVD_IN_DNSWL_HI, RCVD_IN_SORBS_SPAM autolearn=unavailable
	version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 718A72A236
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 21 Oct 2016 14:12:57 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S934439AbcJUOMr (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 21 Oct 2016 10:12:47 -0400
Received: from mail-qk0-f193.google.com ([209.85.220.193]:34197 &quot;EHLO
	mail-qk0-f193.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S933757AbcJUOLn (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 21 Oct 2016 10:11:43 -0400
Received: by mail-qk0-f193.google.com with SMTP id n189so7280149qke.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Fri, 21 Oct 2016 07:11:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=ursulin-net.20150623.gappssmtp.com; s=20150623;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=XpsPDAvGqDfr6J0swt00GgKbyBO0TQs+c8HFgmMs4hY=;
	b=Tq4HjDHDQw/7+bnX7Ae951PbIEPtPJDmtu+7P4aFYLU2qV1Tpxg2P4IuhROc5M8frk
	PUdYJ3/KYcxn3DUBUfO39QDB0UPfWdbgKGYQMoH+63bA3chwqS3WlBkQ92KF+kVRB8OY
	9iEU3wPekCTUrRUUcCzGGomgFPeXfdXjfX3vnLETC5tRwaNaRtEBPzX4ngIRlctx9+lA
	YLaQODDifi0abwDDIOFElhRLs/rCxQ54RCX3wIbtF4R7Je3GN/s9goZ+3RlfmoNWnZMG
	WV+iPaaWsdXW/G9+FiVuLhCMh+j2GHj9r4VEgKwTwZ/MtCM0t139oYAntyEeK6LGm4Ko
	GKDg==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=XpsPDAvGqDfr6J0swt00GgKbyBO0TQs+c8HFgmMs4hY=;
	b=azj2L9jH4hXY8VKqg4/0MyfViLmaSa7nGwnJkfP1n/w0dTn3lXGuWjxi7BSTNfg3KR
	SqFUIRRBHFnNtQeNq9TVFAKNXH45YOp264ZO8FRFb/Z066txzumLcsJYXCn7XE7ZZbfR
	5GViECDRNzGag+pOG5BT7P4O6zka0j27JPLea+6DPU92cQJHPe/+ehtpM5n1ShUb1Kkh
	3XJxApsLD5/gMrRwfJF0ij/FWEgeAO+kAXGTlzEnMu12wV0eY4CziyvMHKH7DXFpMnOB
	Exc0pjhv1Q/IQiXBwjqa19pijFDclCGpFuQ6U4TPNCUMhLnAAvOg0Sw7ZZ//ivtlwp2I
	OHuQ==
X-Gm-Message-State: ABUngvdp9ppG+9Z91FJbeUYOvjmPILok4eHb0L37bZuTCog3shnWcGHu7k8vXC8YnkJsJA==
X-Received: by 10.194.170.163 with SMTP id an3mr947106wjc.73.1477059101732; 
	Fri, 21 Oct 2016 07:11:41 -0700 (PDT)
Received: from e31.Home ([2a02:c7d:9b6d:e300:916a:6cab:ac67:71c2])
	by smtp.gmail.com with ESMTPSA id
	ya1sm3114013wjb.23.2016.10.21.07.11.40
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Fri, 21 Oct 2016 07:11:41 -0700 (PDT)
From: Tvrtko Ursulin &lt;tursulin@ursulin.net&gt;
X-Google-Original-From: Tvrtko Ursulin &lt;tvrtko.ursulin@linux.intel.com&gt;
To: Intel-gfx@lists.freedesktop.org
Cc: linux-kernel@vger.kernel.org, linux-media@vger.kernel.org,
	Chris Wilson &lt;chris@chris-wilson.co.uk&gt;,
	Tvrtko Ursulin &lt;tvrtko.ursulin@intel.com&gt;
Subject: [PATCH 4/5] drm/i915: Use __sg_alloc_table_from_pages for
	allocating object backing store
Date: Fri, 21 Oct 2016 15:11:22 +0100
Message-Id: &lt;1477059083-3500-5-git-send-email-tvrtko.ursulin@linux.intel.com&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1477059083-3500-1-git-send-email-tvrtko.ursulin@linux.intel.com&gt;
References: &lt;1477059083-3500-1-git-send-email-tvrtko.ursulin@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170415">Tvrtko Ursulin</a> - Oct. 21, 2016, 2:11 p.m.</div>
<pre class="content">
<span class="from">From: Tvrtko Ursulin &lt;tvrtko.ursulin@intel.com&gt;</span>

With the current way of allocating backing store which over-estimates
the number of sg entries required we typically waste around 1-6 MiB of
memory on unused sg entries at runtime.

We can instead have the intermediate step of storing our pages in an
array and use __sg_alloc_table_from_pages which will create us the
most compact list possible.
<span class="signed-off-by">
Signed-off-by: Tvrtko Ursulin &lt;tvrtko.ursulin@intel.com&gt;</span>
---
 drivers/gpu/drm/i915/i915_gem.c | 72 ++++++++++++++++++++---------------------
 1 file changed, 35 insertions(+), 37 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1064">Chris Wilson</a> - Oct. 21, 2016, 2:27 p.m.</div>
<pre class="content">
On Fri, Oct 21, 2016 at 03:11:22PM +0100, Tvrtko Ursulin wrote:
<span class="quote">&gt; @@ -2236,18 +2233,16 @@ i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
<span class="quote">&gt;  	BUG_ON(obj-&gt;base.read_domains &amp; I915_GEM_GPU_DOMAINS);</span>
<span class="quote">&gt;  	BUG_ON(obj-&gt;base.write_domain &amp; I915_GEM_GPU_DOMAINS);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	max_segment = swiotlb_max_size();</span>
<span class="quote">&gt; -	if (!max_segment)</span>
<span class="quote">&gt; -		max_segment = rounddown(UINT_MAX, PAGE_SIZE);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt; -	st = kmalloc(sizeof(*st), GFP_KERNEL);</span>
<span class="quote">&gt; -	if (st == NULL)</span>
<span class="quote">&gt; -		return -ENOMEM;</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  	page_count = obj-&gt;base.size / PAGE_SIZE;</span>
<span class="quote">&gt; -	if (sg_alloc_table(st, page_count, GFP_KERNEL)) {</span>
<span class="quote">&gt; -		kfree(st);</span>
<span class="quote">&gt; +	pages = drm_malloc_gfp(page_count, sizeof(struct page *),</span>
<span class="quote">&gt; +			       GFP_TEMPORARY | __GFP_ZERO);</span>
<span class="quote">&gt; +	if (!pages)</span>
<span class="quote">&gt;  		return -ENOMEM;</span>

Full circle! The whole reason this exists was to avoid that vmalloc. I
don&#39;t really want it back...
-Chris
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=82731">tvrtko.ursulin@linux.intel.com</a> - Oct. 21, 2016, 2:55 p.m.</div>
<pre class="content">
On 21/10/2016 15:27, Chris Wilson wrote:
<span class="quote">&gt; On Fri, Oct 21, 2016 at 03:11:22PM +0100, Tvrtko Ursulin wrote:</span>
<span class="quote">&gt;&gt; @@ -2236,18 +2233,16 @@ i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
<span class="quote">&gt;&gt;  	BUG_ON(obj-&gt;base.read_domains &amp; I915_GEM_GPU_DOMAINS);</span>
<span class="quote">&gt;&gt;  	BUG_ON(obj-&gt;base.write_domain &amp; I915_GEM_GPU_DOMAINS);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; -	max_segment = swiotlb_max_size();</span>
<span class="quote">&gt;&gt; -	if (!max_segment)</span>
<span class="quote">&gt;&gt; -		max_segment = rounddown(UINT_MAX, PAGE_SIZE);</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt; -	st = kmalloc(sizeof(*st), GFP_KERNEL);</span>
<span class="quote">&gt;&gt; -	if (st == NULL)</span>
<span class="quote">&gt;&gt; -		return -ENOMEM;</span>
<span class="quote">&gt;&gt; -</span>
<span class="quote">&gt;&gt;  	page_count = obj-&gt;base.size / PAGE_SIZE;</span>
<span class="quote">&gt;&gt; -	if (sg_alloc_table(st, page_count, GFP_KERNEL)) {</span>
<span class="quote">&gt;&gt; -		kfree(st);</span>
<span class="quote">&gt;&gt; +	pages = drm_malloc_gfp(page_count, sizeof(struct page *),</span>
<span class="quote">&gt;&gt; +			       GFP_TEMPORARY | __GFP_ZERO);</span>
<span class="quote">&gt;&gt; +	if (!pages)</span>
<span class="quote">&gt;&gt;  		return -ENOMEM;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Full circle! The whole reason this exists was to avoid that vmalloc. I</span>
<span class="quote">&gt; don&#39;t really want it back...</span>

Yes, it is not ideal.

However all objects under 4 MiB should fall under the kmalloc fast path 
(8 KiB of struct page pointers, which should always be available), and 
possibly bigger ones as well if there is room.

It only fallbacks to vmalloc for objects larger than 4 MiB, when it also 
fails to get the page pointer array from the SLAB (GFP_TEMPORARY).

So perhaps SLAB would most of the time have some nice chunks for us to 
pretty much limit vmalloc apart for the huge objects? And then, is 
creation time for those so performance critical?

I came up with this because I started to dislike my previous 
sg_trim_table approach as too ugly. It had an advantage of simplicity 
after fixing the theoretical chunk overflow in sg_alloc_table_from_pages.

If we choose none of the two, only third option I can think of is to 
allocate the sg table as we add entries to it. I don&#39;t think it would be 
hard to do that.

Regards,

Tvrtko
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">index 8ed8e24025ac..4bf675568a37 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/i915/i915_gem.c</span>
<span class="p_chunk">@@ -2208,9 +2208,9 @@</span> <span class="p_context"> i915_gem_object_put_pages(struct drm_i915_gem_object *obj)</span>
 static unsigned int swiotlb_max_size(void)
 {
 #if IS_ENABLED(CONFIG_SWIOTLB)
<span class="p_del">-	return rounddown(swiotlb_nr_tbl() &lt;&lt; IO_TLB_SHIFT, PAGE_SIZE);</span>
<span class="p_add">+	return swiotlb_nr_tbl() &lt;&lt; IO_TLB_SHIFT;</span>
 #else
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return UINT_MAX;</span>
 #endif
 }
 
<span class="p_chunk">@@ -2221,11 +2221,8 @@</span> <span class="p_context"> i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
 	int page_count, i;
 	struct address_space *mapping;
 	struct sg_table *st;
<span class="p_del">-	struct scatterlist *sg;</span>
<span class="p_del">-	struct sgt_iter sgt_iter;</span>
<span class="p_del">-	struct page *page;</span>
<span class="p_del">-	unsigned long last_pfn = 0;	/* suppress gcc warning */</span>
<span class="p_del">-	unsigned int max_segment;</span>
<span class="p_add">+	struct page *page, **pages;</span>
<span class="p_add">+	unsigned int max_segment = swiotlb_max_size();</span>
 	int ret;
 	gfp_t gfp;
 
<span class="p_chunk">@@ -2236,18 +2233,16 @@</span> <span class="p_context"> i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
 	BUG_ON(obj-&gt;base.read_domains &amp; I915_GEM_GPU_DOMAINS);
 	BUG_ON(obj-&gt;base.write_domain &amp; I915_GEM_GPU_DOMAINS);
 
<span class="p_del">-	max_segment = swiotlb_max_size();</span>
<span class="p_del">-	if (!max_segment)</span>
<span class="p_del">-		max_segment = rounddown(UINT_MAX, PAGE_SIZE);</span>
<span class="p_del">-</span>
<span class="p_del">-	st = kmalloc(sizeof(*st), GFP_KERNEL);</span>
<span class="p_del">-	if (st == NULL)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
 	page_count = obj-&gt;base.size / PAGE_SIZE;
<span class="p_del">-	if (sg_alloc_table(st, page_count, GFP_KERNEL)) {</span>
<span class="p_del">-		kfree(st);</span>
<span class="p_add">+	pages = drm_malloc_gfp(page_count, sizeof(struct page *),</span>
<span class="p_add">+			       GFP_TEMPORARY | __GFP_ZERO);</span>
<span class="p_add">+	if (!pages)</span>
 		return -ENOMEM;
<span class="p_add">+</span>
<span class="p_add">+	st = kmalloc(sizeof(*st), GFP_KERNEL);</span>
<span class="p_add">+	if (st == NULL) {</span>
<span class="p_add">+		ret = -ENOMEM;</span>
<span class="p_add">+		goto err_st;</span>
 	}
 
 	/* Get the list of pages out of our struct file.  They&#39;ll be pinned
<span class="p_chunk">@@ -2258,8 +2253,6 @@</span> <span class="p_context"> i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
 	mapping = obj-&gt;base.filp-&gt;f_mapping;
 	gfp = mapping_gfp_constraint(mapping, ~(__GFP_IO | __GFP_RECLAIM));
 	gfp |= __GFP_NORETRY | __GFP_NOWARN;
<span class="p_del">-	sg = st-&gt;sgl;</span>
<span class="p_del">-	st-&gt;nents = 0;</span>
 	for (i = 0; i &lt; page_count; i++) {
 		page = shmem_read_mapping_page_gfp(mapping, i, gfp);
 		if (IS_ERR(page)) {
<span class="p_chunk">@@ -2281,29 +2274,28 @@</span> <span class="p_context"> i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
 				goto err_pages;
 			}
 		}
<span class="p_del">-		if (!i ||</span>
<span class="p_del">-		    sg-&gt;length &gt;= max_segment ||</span>
<span class="p_del">-		    page_to_pfn(page) != last_pfn + 1) {</span>
<span class="p_del">-			if (i)</span>
<span class="p_del">-				sg = sg_next(sg);</span>
<span class="p_del">-			st-&gt;nents++;</span>
<span class="p_del">-			sg_set_page(sg, page, PAGE_SIZE, 0);</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			sg-&gt;length += PAGE_SIZE;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		last_pfn = page_to_pfn(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		pages[i] = page;</span>
 
 		/* Check that the i965g/gm workaround works. */
<span class="p_del">-		WARN_ON((gfp &amp; __GFP_DMA32) &amp;&amp; (last_pfn &gt;= 0x00100000UL));</span>
<span class="p_add">+		WARN_ON((gfp &amp; __GFP_DMA32) &amp;&amp;</span>
<span class="p_add">+			(page_to_pfn(page) &gt;= 0x00100000UL));</span>
 	}
<span class="p_del">-	if (sg) /* loop terminated early; short sg table */</span>
<span class="p_del">-		sg_mark_end(sg);</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = __sg_alloc_table_from_pages(st, pages, page_count, 0,</span>
<span class="p_add">+					  obj-&gt;base.size, GFP_KERNEL,</span>
<span class="p_add">+					  max_segment);</span>
<span class="p_add">+	if (ret)</span>
<span class="p_add">+		goto err_pages;</span>
<span class="p_add">+</span>
 	obj-&gt;pages = st;
 
 	ret = i915_gem_gtt_prepare_object(obj);
 	if (ret)
 		goto err_pages;
 
<span class="p_add">+	drm_free_large(pages);</span>
<span class="p_add">+</span>
 	if (i915_gem_object_needs_bit17_swizzle(obj))
 		i915_gem_object_do_bit_17_swizzle(obj);
 
<span class="p_chunk">@@ -2314,10 +2306,13 @@</span> <span class="p_context"> i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
 	return 0;
 
 err_pages:
<span class="p_del">-	sg_mark_end(sg);</span>
<span class="p_del">-	for_each_sgt_page(page, sgt_iter, st)</span>
<span class="p_del">-		put_page(page);</span>
<span class="p_del">-	sg_free_table(st);</span>
<span class="p_add">+	for (i = 0; i &lt; page_count; i++) {</span>
<span class="p_add">+		if (pages[i])</span>
<span class="p_add">+			put_page(pages[i]);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	kfree(st);
 
 	/* shmemfs first checks if there is enough memory to allocate the page
<span class="p_chunk">@@ -2331,6 +2326,9 @@</span> <span class="p_context"> i915_gem_object_get_pages_gtt(struct drm_i915_gem_object *obj)</span>
 	if (ret == -ENOSPC)
 		ret = -ENOMEM;
 
<span class="p_add">+err_st:</span>
<span class="p_add">+	drm_free_large(pages);</span>
<span class="p_add">+</span>
 	return ret;
 }
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



