
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86/asm changes for v4.9 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86/asm changes for v4.9</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Oct. 3, 2016, 9:29 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20161003092940.GA691@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9360345/mbox/"
   >mbox</a>
|
   <a href="/patch/9360345/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9360345/">/patch/9360345/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	5F835601C0 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Oct 2016 09:30:18 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 43FA828882
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Oct 2016 09:30:18 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 2F62228890; Mon,  3 Oct 2016 09:30:18 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 003FB28921
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon,  3 Oct 2016 09:30:09 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752831AbcJCJaB (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 3 Oct 2016 05:30:01 -0400
Received: from mail-wm0-f68.google.com ([74.125.82.68]:35081 &quot;EHLO
	mail-wm0-f68.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752077AbcJCJ3u (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 3 Oct 2016 05:29:50 -0400
Received: by mail-wm0-f68.google.com with SMTP id f193so9117240wmg.2
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 03 Oct 2016 02:29:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version
	:content-disposition:user-agent;
	bh=kngLNJ0kCoLV7bF9Jt8pvD3H1KCMzlhJ9OXvZCrgkJA=;
	b=Lyw0k6WBltyjKpoYsDBN+7/Li77O610QfVz3qgfHk1tOZEhEk/Hf1TQazBzc7UaUXo
	Qyw+kG8HabGZLvhf7zbj3u95M6ih8DSMRr6LwJJrNTCFAJDNmFwGSi9qIwz80dahSjjZ
	7q53GmyuIjAR2iAwYyXBj71M75brJZRomOo06ne3PUnNIPrY1zcswy+ahH1fc+Z4gLQS
	TYFMzQxqGfXz+RHzwChoWIkQTqqmN4IIstGDxosyS6f09vhWTlq6zlbALZ+DX1nqGIXV
	py45AIgrqD0xDNhqvS0JGsok/DVxtw52/idyZmgivkQL6O9PvMRpfOrEW+0FFtgV841o
	Srnw==
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:sender:date:from:to:cc:subject:message-id
	:mime-version:content-disposition:user-agent;
	bh=kngLNJ0kCoLV7bF9Jt8pvD3H1KCMzlhJ9OXvZCrgkJA=;
	b=kRAWXPn/QHfVNuCVgfvyCJVuSW+YeeF7N5FwwPJG6Ig1BAuKsZyxhr38VMKk0mnr6g
	/M5Zt9gMfKm8vStrgubJsG3ZssagU9HTj1d5rADT8iH29INWxLJeS0kf1mSvhmVmR0DK
	tEroh2g2LkBt1GyHuBB2TlQ9V6UJ8zSx0g9LeoSa7WawX2EtW+Y/1t4Wmo//ZZLj0+bh
	cdcnu0EqX0fa1sjly14tqx+crFPRm/hCl97uQc0B91YX4IYO8892xQDrf+vvRS7mq4Id
	mvfn33wWdEjArFA2Ioou9znPWCBas8AsFu0Hqpu0hDkBPS2icLGonOAMyQ3LLHZaGRS+
	Nfqw==
X-Gm-Message-State: AA6/9RlhU3zMC+Lt/1680SsM3fP54pARsB23hfrMjSE+WwgOiwhRryXZdQDc406IYEDh0A==
X-Received: by 10.194.56.69 with SMTP id y5mr4479973wjp.4.1475486986791;
	Mon, 03 Oct 2016 02:29:46 -0700 (PDT)
Received: from gmail.com (2E8B0CD5.catv.pool.telekom.hu. [46.139.12.213])
	by smtp.gmail.com with ESMTPSA id
	u64sm17888312wmd.20.2016.10.03.02.29.43
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 03 Oct 2016 02:29:44 -0700 (PDT)
Date: Mon, 3 Oct 2016 11:29:41 +0200
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;, Borislav Petkov &lt;bp@alien8.de&gt;,
	Denys Vlasenko &lt;dvlasenk@redhat.com&gt;,
	Josh Poimboeuf &lt;jpoimboe@redhat.com&gt;
Subject: [GIT PULL] x86/asm changes for v4.9
Message-ID: &lt;20161003092940.GA691@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.24 (2015-08-30)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Oct. 3, 2016, 9:29 a.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-asm-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-asm-for-linus

   # HEAD: 1ef55be16ed69538f89e0a6508be5e62fdc9851c x86/asm: Get rid of __read_cr4_safe()

In this cycle this topic tree has become one of those &#39;super topics&#39; that 
accumulated a lot of changes:

 - Add CONFIG_VMAP_STACK=y support to the core kernel and enable it on x86 - 
   preceded by an array of changes. v4.8 saw preparatory changes in this area 
   already - this is the rest of the work. Includes the thread stack caching 
   performance optimization. (Andy Lutomirski)

 - switch_to() cleanups and all around enhancements. (Brian Gerst)

 - A large number of dumpstack infrastructure enhancements and an unwinder 
   abstraction. The secret long term plan is safe(r) live patching plus maybe 
   another attempt at debuginfo based unwinding - but all these current bits
   are standalone enhancements in a frame pointer based debug environment as well.
   (Josh Poimboeuf)

 - More __ro_after_init and const annotations. (Kees Cook)

 - Enable KASLR for the vmemmap memory region. (Thomas Garnier)

 Thanks,

	Ingo

------------------&gt;
Andy Lutomirski (18):
      fork: Add generic vmalloced stack support
      dma-api: Teach the &quot;DMA-from-stack&quot; check about vmapped stacks
      x86/mm/64: Enable vmapped stacks (CONFIG_HAVE_ARCH_VMAP_STACK=y)
      x86/mm: Improve stack-overflow #PF handling
      virtio_console: Stop doing DMA on the stack
      selftests/x86/sigreturn: Use CX, not AX, as the scratch register
      x86/entry/64: Clean up and document espfix64 stack setup
      x86/asm: Move the thread_info::status field to thread_struct
      sched/core: Allow putting thread_info into task_struct
      x86: Move thread_info into task_struct
      x86/entry/64: Fix a minor comment rebase error
      sched/core: Add try_get_task_stack() and put_task_stack()
      x86/dumpstack: Pin the target stack when dumping it
      x86/process: Pin the target stack in get_wchan()
      lib/syscall: Pin the task stack in collect_syscall()
      sched/core: Free the stack early if CONFIG_THREAD_INFO_IN_TASK
      fork: Optimize task creation by caching two thread stacks per CPU if CONFIG_VMAP_STACK=y
      x86/asm: Get rid of __read_cr4_safe()

Borislav Petkov (2):
      x86/entry: Remove duplicated comment
      x86/entry: Remove outdated comment about SYSCALL targets

Brian Gerst (7):
      sched/x86/32, kgdb: Don&#39;t use thread.ip in sleeping_thread_to_gdb_regs()
      sched/x86/64, kgdb: Clear GDB_PS on 64-bit
      sched/x86: Add &#39;struct inactive_task_frame&#39; to better document the sleeping task stack frame
      sched/x86: Rewrite the switch_to() code
      sched/x86: Pass kernel thread parameters in &#39;struct fork_frame&#39;
      sched/x86: Fix thread_saved_pc()
      sched: Remove __schedule() non-standard frame annotation

Colin Ian King (1):
      selftests/x86: Fix spelling mistake &quot;preseve&quot; -&gt; &quot;preserve&quot;

Joerg Roedel (1):
      iommu/amd: Don&#39;t put completion-wait semaphore on stack

Josh Poimboeuf (35):
      x86/dumpstack: Remove show_trace()
      x86/asm/head: Remove unused init_rsp variable extern
      x86/asm/head: Rename &#39;stack_start&#39; -&gt; &#39;initial_stack&#39;
      x86/dumpstack: Remove extra brackets around &quot;&lt;EOE&gt;&quot;
      x86/head: Remove useless zeroed word
      x86/dumpstack: Fix x86_32 kernel_stack_pointer() previous stack access
      proc: Fix return address printk conversion specifer in /proc/&lt;pid&gt;/stack
      x86/dumpstack: Remove 64-byte gap at end of irq stack
      ftrace: Remove CONFIG_HAVE_FUNCTION_GRAPH_FP_TEST from config
      ftrace: Only allocate the ret_stack &#39;fp&#39; field when needed
      ftrace: Add return address pointer to ftrace_ret_stack
      ftrace: Add ftrace_graph_ret_addr() stack unwinding helpers
      x86/dumpstack/ftrace: Convert dump_trace() callbacks to use ftrace_graph_ret_addr()
      ftrace/x86: Implement HAVE_FUNCTION_GRAPH_RET_ADDR_PTR
      x86/dumpstack/ftrace: Mark function graph handler function as unreliable
      x86/dumpstack/ftrace: Don&#39;t print unreliable addresses in print_context_stack_bp()
      perf/x86: Check perf_callchain_store() error
      oprofile/x86: Add regs-&gt;ip to oprofile trace
      x86/dumpstack: Make printk_stack_address() more generally useful
      x86/dumpstack: Add get_stack_pointer() and get_frame_pointer()
      x86/dumpstack: Remove unnecessary stack pointer arguments
      x86/dumpstack: Allow preemption in show_stack_log_lvl() and dump_trace()
      x86/dumpstack: Simplify in_exception_stack()
      x86/dumpstack: Add get_stack_info() interface
      x86/dumpstack: Add support for unwinding empty IRQ stacks
      x86/dumpstack: Add recursion checking for all stacks
      x86/dumpstack: Remove NULL task pointer convention
      x86/unwind: Add new unwind interface and implementations
      perf/x86: Convert perf_callchain_kernel() to use the new unwinder
      x86/stacktrace: Convert save_stack_trace_*() to use the new unwinder
      oprofile/x86: Convert x86_backtrace() to use the new unwinder
      x86/dumpstack: Convert show_trace_log_lvl() to use the new unwinder
      x86/dumpstack: Remove dump_trace() and related callbacks
      x86/dumpstack: Fix show_stack() task pointer regression
      x86/alternatives: Add stack frame dependency to alternative_call_2()

Kees Cook (1):
      x86: Apply more __ro_after_init and const

Linus Torvalds (2):
      x86/entry: Get rid of pt_regs_to_thread_info()
      um/Stop conflating task_struct::stack with thread_info

Mark Rutland (1):
      thread_info: Use unsigned long for flags

Oleg Nesterov (1):
      kthread: Pin the stack via try_get_task_stack()/put_task_stack() in to_live_kthread() function

Thomas Garnier (1):
      x86/mm/64: Enable KASLR for vmemmap memory region


 Documentation/trace/ftrace-design.txt        |  11 +
 arch/Kconfig                                 |  34 +++
 arch/arm/kernel/ftrace.c                     |   2 +-
 arch/arm64/kernel/entry-ftrace.S             |   2 +-
 arch/arm64/kernel/ftrace.c                   |   2 +-
 arch/blackfin/kernel/ftrace-entry.S          |   4 +-
 arch/blackfin/kernel/ftrace.c                |   2 +-
 arch/ia64/include/asm/thread_info.h          |   2 +-
 arch/microblaze/kernel/ftrace.c              |   2 +-
 arch/mips/kernel/ftrace.c                    |   4 +-
 arch/parisc/kernel/ftrace.c                  |   2 +-
 arch/powerpc/kernel/ftrace.c                 |   3 +-
 arch/s390/kernel/ftrace.c                    |   3 +-
 arch/sh/kernel/ftrace.c                      |   2 +-
 arch/sparc/Kconfig                           |   1 -
 arch/sparc/include/asm/ftrace.h              |   4 +
 arch/sparc/kernel/ftrace.c                   |   2 +-
 arch/tile/kernel/ftrace.c                    |   2 +-
 arch/x86/Kconfig                             |   3 +-
 arch/x86/entry/common.c                      |  24 +-
 arch/x86/entry/entry_32.S                    |  68 ++++--
 arch/x86/entry/entry_64.S                    | 151 +++++++++----
 arch/x86/events/core.c                       |  36 +--
 arch/x86/include/asm/alternative.h           |   8 +-
 arch/x86/include/asm/desc.h                  |   2 +-
 arch/x86/include/asm/fpu/xstate.h            |   3 +-
 arch/x86/include/asm/ftrace.h                |   3 +
 arch/x86/include/asm/kaslr.h                 |   1 +
 arch/x86/include/asm/kdebug.h                |   2 -
 arch/x86/include/asm/paravirt.h              |   4 -
 arch/x86/include/asm/paravirt_types.h        |   1 -
 arch/x86/include/asm/pgtable_64_types.h      |   4 +-
 arch/x86/include/asm/processor.h             |  25 ++-
 arch/x86/include/asm/realmode.h              |   2 +-
 arch/x86/include/asm/smp.h                   |   3 -
 arch/x86/include/asm/special_insns.h         |  22 +-
 arch/x86/include/asm/stacktrace.h            | 120 +++++-----
 arch/x86/include/asm/switch_to.h             | 164 +++++---------
 arch/x86/include/asm/syscall.h               |  20 +-
 arch/x86/include/asm/thread_info.h           |  71 +-----
 arch/x86/include/asm/tlbflush.h              |   2 +-
 arch/x86/include/asm/traps.h                 |   6 +
 arch/x86/include/asm/unwind.h                |  73 ++++++
 arch/x86/kernel/Makefile                     |   6 +
 arch/x86/kernel/acpi/sleep.c                 |   2 +-
 arch/x86/kernel/apic/apic_flat_64.c          |   6 +-
 arch/x86/kernel/apic/apic_noop.c             |   2 +-
 arch/x86/kernel/apic/bigsmp_32.c             |   2 +-
 arch/x86/kernel/apic/msi.c                   |   2 +-
 arch/x86/kernel/apic/probe_32.c              |   4 +-
 arch/x86/kernel/apic/x2apic_cluster.c        |   2 +-
 arch/x86/kernel/apic/x2apic_phys.c           |   2 +-
 arch/x86/kernel/apic/x2apic_uv_x.c           |   2 +-
 arch/x86/kernel/asm-offsets.c                |   7 +-
 arch/x86/kernel/asm-offsets_32.c             |   5 +
 arch/x86/kernel/asm-offsets_64.c             |   5 +
 arch/x86/kernel/cpu/common.c                 |  18 +-
 arch/x86/kernel/cpu/mtrr/main.c              |   4 +-
 arch/x86/kernel/cpu/mtrr/mtrr.h              |   2 +-
 arch/x86/kernel/dumpstack.c                  | 258 +++++++++-------------
 arch/x86/kernel/dumpstack_32.c               | 154 +++++++------
 arch/x86/kernel/dumpstack_64.c               | 318 +++++++++------------------
 arch/x86/kernel/fpu/init.c                   |   1 -
 arch/x86/kernel/ftrace.c                     |   2 +-
 arch/x86/kernel/head_32.S                    |   8 +-
 arch/x86/kernel/head_64.S                    |  12 +-
 arch/x86/kernel/irq_64.c                     |   3 +-
 arch/x86/kernel/kgdb.c                       |   8 +-
 arch/x86/kernel/ksysfs.c                     |   2 +-
 arch/x86/kernel/kvmclock.c                   |   2 +-
 arch/x86/kernel/paravirt.c                   |   3 +-
 arch/x86/kernel/process.c                    |  42 +++-
 arch/x86/kernel/process_32.c                 |  33 +--
 arch/x86/kernel/process_64.c                 |  25 +--
 arch/x86/kernel/ptrace.c                     |  12 +-
 arch/x86/kernel/reboot.c                     |   2 +-
 arch/x86/kernel/setup.c                      |   6 +-
 arch/x86/kernel/setup_percpu.c               |   4 +-
 arch/x86/kernel/signal.c                     |   2 +-
 arch/x86/kernel/smpboot.c                    |   3 +-
 arch/x86/kernel/stacktrace.c                 |  79 +++----
 arch/x86/kernel/traps.c                      |  61 +++++
 arch/x86/kernel/unwind_frame.c               |  93 ++++++++
 arch/x86/kernel/unwind_guess.c               |  43 ++++
 arch/x86/kernel/x86_init.c                   |   6 +-
 arch/x86/kvm/svm.c                           |   2 +-
 arch/x86/kvm/vmx.c                           |   2 +-
 arch/x86/mm/fault.c                          |  32 +++
 arch/x86/mm/kaslr.c                          |  26 ++-
 arch/x86/mm/tlb.c                            |  15 ++
 arch/x86/oprofile/backtrace.c                |  49 ++---
 arch/x86/pci/pcbios.c                        |   7 +-
 arch/x86/power/cpu.c                         |   2 +-
 arch/x86/um/ptrace_32.c                      |   8 +-
 arch/x86/xen/enlighten.c                     |   1 -
 drivers/iommu/amd_iommu.c                    |  51 +++--
 drivers/iommu/amd_iommu_types.h              |   2 +
 fs/proc/base.c                               |   2 +-
 include/linux/ftrace.h                       |  17 +-
 include/linux/init_task.h                    |  11 +
 include/linux/sched.h                        |  81 ++++++-
 include/linux/thread_info.h                  |  15 ++
 init/Kconfig                                 |  10 +
 init/init_task.c                             |   7 +-
 kernel/fork.c                                | 175 +++++++++++++--
 kernel/kthread.c                             |   8 +-
 kernel/sched/core.c                          |   5 +-
 kernel/sched/sched.h                         |   4 +
 kernel/trace/Kconfig                         |   5 -
 kernel/trace/trace_functions_graph.c         |  67 +++++-
 lib/dma-debug.c                              |  36 ++-
 lib/syscall.c                                |  15 +-
 tools/testing/selftests/x86/ptrace_syscall.c |   4 +-
 tools/testing/selftests/x86/sigreturn.c      |  16 +-
 114 files changed, 1722 insertions(+), 1108 deletions(-)
 create mode 100644 arch/x86/include/asm/unwind.h
 create mode 100644 arch/x86/kernel/unwind_frame.c
 create mode 100644 arch/x86/kernel/unwind_guess.c
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/trace/ftrace-design.txt b/Documentation/trace/ftrace-design.txt</span>
<span class="p_header">index dd5f916b351d..a273dd0bbaaa 100644</span>
<span class="p_header">--- a/Documentation/trace/ftrace-design.txt</span>
<span class="p_header">+++ b/Documentation/trace/ftrace-design.txt</span>
<span class="p_chunk">@@ -203,6 +203,17 @@</span> <span class="p_context"> along to ftrace_push_return_trace() instead of a stub value of 0.</span>
 
 Similarly, when you call ftrace_return_to_handler(), pass it the frame pointer.
 
<span class="p_add">+HAVE_FUNCTION_GRAPH_RET_ADDR_PTR</span>
<span class="p_add">+--------------------------------</span>
<span class="p_add">+</span>
<span class="p_add">+An arch may pass in a pointer to the return address on the stack.  This</span>
<span class="p_add">+prevents potential stack unwinding issues where the unwinder gets out of</span>
<span class="p_add">+sync with ret_stack and the wrong addresses are reported by</span>
<span class="p_add">+ftrace_graph_ret_addr().</span>
<span class="p_add">+</span>
<span class="p_add">+Adding support for it is easy: just define the macro in asm/ftrace.h and</span>
<span class="p_add">+pass the return address pointer as the &#39;retp&#39; argument to</span>
<span class="p_add">+ftrace_push_return_trace().</span>
 
 HAVE_FTRACE_NMI_ENTER
 ---------------------
<span class="p_header">diff --git a/arch/Kconfig b/arch/Kconfig</span>
<span class="p_header">index fd6e9712af81..180ea33164dc 100644</span>
<span class="p_header">--- a/arch/Kconfig</span>
<span class="p_header">+++ b/arch/Kconfig</span>
<span class="p_chunk">@@ -696,4 +696,38 @@</span> <span class="p_context"> config ARCH_NO_COHERENT_DMA_MMAP</span>
 config CPU_NO_EFFICIENT_FFS
 	def_bool n
 
<span class="p_add">+config HAVE_ARCH_VMAP_STACK</span>
<span class="p_add">+	def_bool n</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  An arch should select this symbol if it can support kernel stacks</span>
<span class="p_add">+	  in vmalloc space.  This means:</span>
<span class="p_add">+</span>
<span class="p_add">+	  - vmalloc space must be large enough to hold many kernel stacks.</span>
<span class="p_add">+	    This may rule out many 32-bit architectures.</span>
<span class="p_add">+</span>
<span class="p_add">+	  - Stacks in vmalloc space need to work reliably.  For example, if</span>
<span class="p_add">+	    vmap page tables are created on demand, either this mechanism</span>
<span class="p_add">+	    needs to work while the stack points to a virtual address with</span>
<span class="p_add">+	    unpopulated page tables or arch code (switch_to() and switch_mm(),</span>
<span class="p_add">+	    most likely) needs to ensure that the stack&#39;s page table entries</span>
<span class="p_add">+	    are populated before running on a possibly unpopulated stack.</span>
<span class="p_add">+</span>
<span class="p_add">+	  - If the stack overflows into a guard page, something reasonable</span>
<span class="p_add">+	    should happen.  The definition of &quot;reasonable&quot; is flexible, but</span>
<span class="p_add">+	    instantly rebooting without logging anything would be unfriendly.</span>
<span class="p_add">+</span>
<span class="p_add">+config VMAP_STACK</span>
<span class="p_add">+	default y</span>
<span class="p_add">+	bool &quot;Use a virtually-mapped stack&quot;</span>
<span class="p_add">+	depends on HAVE_ARCH_VMAP_STACK &amp;&amp; !KASAN</span>
<span class="p_add">+	---help---</span>
<span class="p_add">+	  Enable this if you want the use virtually-mapped kernel stacks</span>
<span class="p_add">+	  with guard pages.  This causes kernel stack overflows to be</span>
<span class="p_add">+	  caught immediately rather than causing difficult-to-diagnose</span>
<span class="p_add">+	  corruption.</span>
<span class="p_add">+</span>
<span class="p_add">+	  This is presently incompatible with KASAN because KASAN expects</span>
<span class="p_add">+	  the stack to map directly to the KASAN shadow map using a formula</span>
<span class="p_add">+	  that is incorrect if the stack is in vmalloc space.</span>
<span class="p_add">+</span>
 source &quot;kernel/gcov/Kconfig&quot;
<span class="p_header">diff --git a/arch/arm/kernel/ftrace.c b/arch/arm/kernel/ftrace.c</span>
<span class="p_header">index 709ee1d6d4df..3f1759411d51 100644</span>
<span class="p_header">--- a/arch/arm/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/arm/kernel/ftrace.c</span>
<span class="p_chunk">@@ -218,7 +218,7 @@</span> <span class="p_context"> void prepare_ftrace_return(unsigned long *parent, unsigned long self_addr,</span>
 	}
 
 	err = ftrace_push_return_trace(old, self_addr, &amp;trace.depth,
<span class="p_del">-				       frame_pointer);</span>
<span class="p_add">+				       frame_pointer, NULL);</span>
 	if (err == -EBUSY) {
 		*parent = old;
 		return;
<span class="p_header">diff --git a/arch/arm64/kernel/entry-ftrace.S b/arch/arm64/kernel/entry-ftrace.S</span>
<span class="p_header">index 0f03a8fe2314..aef02d2af3b5 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/entry-ftrace.S</span>
<span class="p_header">+++ b/arch/arm64/kernel/entry-ftrace.S</span>
<span class="p_chunk">@@ -219,7 +219,7 @@</span> <span class="p_context"> ENDPROC(ftrace_graph_caller)</span>
  *
  * Run ftrace_return_to_handler() before going back to parent.
  * @fp is checked against the value passed by ftrace_graph_caller()
<span class="p_del">- * only when CONFIG_HAVE_FUNCTION_GRAPH_FP_TEST is enabled.</span>
<span class="p_add">+ * only when HAVE_FUNCTION_GRAPH_FP_TEST is enabled.</span>
  */
 ENTRY(return_to_handler)
 	save_return_regs
<span class="p_header">diff --git a/arch/arm64/kernel/ftrace.c b/arch/arm64/kernel/ftrace.c</span>
<span class="p_header">index ebecf9aa33d1..40ad08ac569a 100644</span>
<span class="p_header">--- a/arch/arm64/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/arm64/kernel/ftrace.c</span>
<span class="p_chunk">@@ -138,7 +138,7 @@</span> <span class="p_context"> void prepare_ftrace_return(unsigned long *parent, unsigned long self_addr,</span>
 		return;
 
 	err = ftrace_push_return_trace(old, self_addr, &amp;trace.depth,
<span class="p_del">-				       frame_pointer);</span>
<span class="p_add">+				       frame_pointer, NULL);</span>
 	if (err == -EBUSY)
 		return;
 	else
<span class="p_header">diff --git a/arch/blackfin/kernel/ftrace-entry.S b/arch/blackfin/kernel/ftrace-entry.S</span>
<span class="p_header">index 28d059540424..3b8bdcbb7da3 100644</span>
<span class="p_header">--- a/arch/blackfin/kernel/ftrace-entry.S</span>
<span class="p_header">+++ b/arch/blackfin/kernel/ftrace-entry.S</span>
<span class="p_chunk">@@ -169,7 +169,7 @@</span> <span class="p_context"> ENTRY(_ftrace_graph_caller)</span>
 	r0 = sp;	/* unsigned long *parent */
 	r1 = [sp];	/* unsigned long self_addr */
 # endif
<span class="p_del">-# ifdef CONFIG_HAVE_FUNCTION_GRAPH_FP_TEST</span>
<span class="p_add">+# ifdef HAVE_FUNCTION_GRAPH_FP_TEST</span>
 	r2 = fp;	/* unsigned long frame_pointer */
 # endif
 	r0 += 16;	/* skip the 4 local regs on stack */
<span class="p_chunk">@@ -190,7 +190,7 @@</span> <span class="p_context"> ENTRY(_return_to_handler)</span>
 	[--sp] = r1;
 
 	/* get original return address */
<span class="p_del">-# ifdef CONFIG_HAVE_FUNCTION_GRAPH_FP_TEST</span>
<span class="p_add">+# ifdef HAVE_FUNCTION_GRAPH_FP_TEST</span>
 	r0 = fp;	/* Blackfin is sane, so omit this */
 # endif
 	call _ftrace_return_to_handler;
<span class="p_header">diff --git a/arch/blackfin/kernel/ftrace.c b/arch/blackfin/kernel/ftrace.c</span>
<span class="p_header">index 095de0fa044d..8dad7589b843 100644</span>
<span class="p_header">--- a/arch/blackfin/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/blackfin/kernel/ftrace.c</span>
<span class="p_chunk">@@ -107,7 +107,7 @@</span> <span class="p_context"> void prepare_ftrace_return(unsigned long *parent, unsigned long self_addr,</span>
 		return;
 
 	if (ftrace_push_return_trace(*parent, self_addr, &amp;trace.depth,
<span class="p_del">-	                             frame_pointer) == -EBUSY)</span>
<span class="p_add">+				     frame_pointer, NULL) == -EBUSY)</span>
 		return;
 
 	trace.func = self_addr;
<span class="p_header">diff --git a/arch/ia64/include/asm/thread_info.h b/arch/ia64/include/asm/thread_info.h</span>
<span class="p_header">index 29bd59790d6c..c7026429816b 100644</span>
<span class="p_header">--- a/arch/ia64/include/asm/thread_info.h</span>
<span class="p_header">+++ b/arch/ia64/include/asm/thread_info.h</span>
<span class="p_chunk">@@ -56,7 +56,7 @@</span> <span class="p_context"> struct thread_info {</span>
 #define alloc_thread_stack_node(tsk, node)	((unsigned long *) 0)
 #define task_thread_info(tsk)	((struct thread_info *) 0)
 #endif
<span class="p_del">-#define free_thread_stack(ti)	/* nothing */</span>
<span class="p_add">+#define free_thread_stack(tsk)	/* nothing */</span>
 #define task_stack_page(tsk)	((void *)(tsk))
 
 #define __HAVE_THREAD_FUNCTIONS
<span class="p_header">diff --git a/arch/microblaze/kernel/ftrace.c b/arch/microblaze/kernel/ftrace.c</span>
<span class="p_header">index fc7b48a52cd5..d57563c58a26 100644</span>
<span class="p_header">--- a/arch/microblaze/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/microblaze/kernel/ftrace.c</span>
<span class="p_chunk">@@ -63,7 +63,7 @@</span> <span class="p_context"> void prepare_ftrace_return(unsigned long *parent, unsigned long self_addr)</span>
 		return;
 	}
 
<span class="p_del">-	err = ftrace_push_return_trace(old, self_addr, &amp;trace.depth, 0);</span>
<span class="p_add">+	err = ftrace_push_return_trace(old, self_addr, &amp;trace.depth, 0, NULL);</span>
 	if (err == -EBUSY) {
 		*parent = old;
 		return;
<span class="p_header">diff --git a/arch/mips/kernel/ftrace.c b/arch/mips/kernel/ftrace.c</span>
<span class="p_header">index 937c54bc8ccc..30a3b75e88eb 100644</span>
<span class="p_header">--- a/arch/mips/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/mips/kernel/ftrace.c</span>
<span class="p_chunk">@@ -382,8 +382,8 @@</span> <span class="p_context"> void prepare_ftrace_return(unsigned long *parent_ra_addr, unsigned long self_ra,</span>
 	if (unlikely(faulted))
 		goto out;
 
<span class="p_del">-	if (ftrace_push_return_trace(old_parent_ra, self_ra, &amp;trace.depth, fp)</span>
<span class="p_del">-	    == -EBUSY) {</span>
<span class="p_add">+	if (ftrace_push_return_trace(old_parent_ra, self_ra, &amp;trace.depth, fp,</span>
<span class="p_add">+				     NULL) == -EBUSY) {</span>
 		*parent_ra_addr = old_parent_ra;
 		return;
 	}
<span class="p_header">diff --git a/arch/parisc/kernel/ftrace.c b/arch/parisc/kernel/ftrace.c</span>
<span class="p_header">index a828a0adf52c..5a5506a35395 100644</span>
<span class="p_header">--- a/arch/parisc/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/parisc/kernel/ftrace.c</span>
<span class="p_chunk">@@ -48,7 +48,7 @@</span> <span class="p_context"> static void __hot prepare_ftrace_return(unsigned long *parent,</span>
 		return;
 
         if (ftrace_push_return_trace(old, self_addr, &amp;trace.depth,
<span class="p_del">-			0 ) == -EBUSY)</span>
<span class="p_add">+				     0, NULL) == -EBUSY)</span>
                 return;
 
 	/* activate parisc_return_to_handler() as return point */
<span class="p_header">diff --git a/arch/powerpc/kernel/ftrace.c b/arch/powerpc/kernel/ftrace.c</span>
<span class="p_header">index cc52d9795f88..a95639b8d4ac 100644</span>
<span class="p_header">--- a/arch/powerpc/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/powerpc/kernel/ftrace.c</span>
<span class="p_chunk">@@ -593,7 +593,8 @@</span> <span class="p_context"> unsigned long prepare_ftrace_return(unsigned long parent, unsigned long ip)</span>
 	if (!ftrace_graph_entry(&amp;trace))
 		goto out;
 
<span class="p_del">-	if (ftrace_push_return_trace(parent, ip, &amp;trace.depth, 0) == -EBUSY)</span>
<span class="p_add">+	if (ftrace_push_return_trace(parent, ip, &amp;trace.depth, 0,</span>
<span class="p_add">+				     NULL) == -EBUSY)</span>
 		goto out;
 
 	parent = return_hooker;
<span class="p_header">diff --git a/arch/s390/kernel/ftrace.c b/arch/s390/kernel/ftrace.c</span>
<span class="p_header">index 0f7bfeba6da6..60a8a4e207ed 100644</span>
<span class="p_header">--- a/arch/s390/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/s390/kernel/ftrace.c</span>
<span class="p_chunk">@@ -209,7 +209,8 @@</span> <span class="p_context"> unsigned long prepare_ftrace_return(unsigned long parent, unsigned long ip)</span>
 	/* Only trace if the calling function expects to. */
 	if (!ftrace_graph_entry(&amp;trace))
 		goto out;
<span class="p_del">-	if (ftrace_push_return_trace(parent, ip, &amp;trace.depth, 0) == -EBUSY)</span>
<span class="p_add">+	if (ftrace_push_return_trace(parent, ip, &amp;trace.depth, 0,</span>
<span class="p_add">+				     NULL) == -EBUSY)</span>
 		goto out;
 	parent = (unsigned long) return_to_handler;
 out:
<span class="p_header">diff --git a/arch/sh/kernel/ftrace.c b/arch/sh/kernel/ftrace.c</span>
<span class="p_header">index 38993e09ef03..95eccd49672f 100644</span>
<span class="p_header">--- a/arch/sh/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/sh/kernel/ftrace.c</span>
<span class="p_chunk">@@ -382,7 +382,7 @@</span> <span class="p_context"> void prepare_ftrace_return(unsigned long *parent, unsigned long self_addr)</span>
 		return;
 	}
 
<span class="p_del">-	err = ftrace_push_return_trace(old, self_addr, &amp;trace.depth, 0);</span>
<span class="p_add">+	err = ftrace_push_return_trace(old, self_addr, &amp;trace.depth, 0, NULL);</span>
 	if (err == -EBUSY) {
 		__raw_writel(old, parent);
 		return;
<span class="p_header">diff --git a/arch/sparc/Kconfig b/arch/sparc/Kconfig</span>
<span class="p_header">index 59b09600dd32..f5d60f14a0bc 100644</span>
<span class="p_header">--- a/arch/sparc/Kconfig</span>
<span class="p_header">+++ b/arch/sparc/Kconfig</span>
<span class="p_chunk">@@ -56,7 +56,6 @@</span> <span class="p_context"> config SPARC64</span>
 	def_bool 64BIT
 	select HAVE_FUNCTION_TRACER
 	select HAVE_FUNCTION_GRAPH_TRACER
<span class="p_del">-	select HAVE_FUNCTION_GRAPH_FP_TEST</span>
 	select HAVE_KRETPROBES
 	select HAVE_KPROBES
 	select HAVE_RCU_TABLE_FREE if SMP
<span class="p_header">diff --git a/arch/sparc/include/asm/ftrace.h b/arch/sparc/include/asm/ftrace.h</span>
<span class="p_header">index 3192a8e42fd6..62755a339a59 100644</span>
<span class="p_header">--- a/arch/sparc/include/asm/ftrace.h</span>
<span class="p_header">+++ b/arch/sparc/include/asm/ftrace.h</span>
<span class="p_chunk">@@ -9,6 +9,10 @@</span> <span class="p_context"></span>
 void _mcount(void);
 #endif
 
<span class="p_add">+#endif /* CONFIG_MCOUNT */</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_SPARC64) &amp;&amp; !defined(CC_USE_FENTRY)</span>
<span class="p_add">+#define HAVE_FUNCTION_GRAPH_FP_TEST</span>
 #endif
 
 #ifdef CONFIG_DYNAMIC_FTRACE
<span class="p_header">diff --git a/arch/sparc/kernel/ftrace.c b/arch/sparc/kernel/ftrace.c</span>
<span class="p_header">index 0a2d2ddff543..6bcff698069b 100644</span>
<span class="p_header">--- a/arch/sparc/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/sparc/kernel/ftrace.c</span>
<span class="p_chunk">@@ -131,7 +131,7 @@</span> <span class="p_context"> unsigned long prepare_ftrace_return(unsigned long parent,</span>
 		return parent + 8UL;
 
 	if (ftrace_push_return_trace(parent, self_addr, &amp;trace.depth,
<span class="p_del">-				     frame_pointer) == -EBUSY)</span>
<span class="p_add">+				     frame_pointer, NULL) == -EBUSY)</span>
 		return parent + 8UL;
 
 	trace.func = self_addr;
<span class="p_header">diff --git a/arch/tile/kernel/ftrace.c b/arch/tile/kernel/ftrace.c</span>
<span class="p_header">index 4a572088b270..b827a418b155 100644</span>
<span class="p_header">--- a/arch/tile/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/tile/kernel/ftrace.c</span>
<span class="p_chunk">@@ -184,7 +184,7 @@</span> <span class="p_context"> void prepare_ftrace_return(unsigned long *parent, unsigned long self_addr,</span>
 	*parent = return_hooker;
 
 	err = ftrace_push_return_trace(old, self_addr, &amp;trace.depth,
<span class="p_del">-				       frame_pointer);</span>
<span class="p_add">+				       frame_pointer, NULL);</span>
 	if (err == -EBUSY) {
 		*parent = old;
 		return;
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 2a1f0ce7c59a..2a83bc8b24c6 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -93,6 +93,7 @@</span> <span class="p_context"> config X86</span>
 	select HAVE_ARCH_TRANSPARENT_HUGEPAGE
 	select HAVE_ARCH_WITHIN_STACK_FRAMES
 	select HAVE_EBPF_JIT			if X86_64
<span class="p_add">+	select HAVE_ARCH_VMAP_STACK		if X86_64</span>
 	select HAVE_CC_STACKPROTECTOR
 	select HAVE_CMPXCHG_DOUBLE
 	select HAVE_CMPXCHG_LOCAL
<span class="p_chunk">@@ -109,7 +110,6 @@</span> <span class="p_context"> config X86</span>
 	select HAVE_EXIT_THREAD
 	select HAVE_FENTRY			if X86_64
 	select HAVE_FTRACE_MCOUNT_RECORD
<span class="p_del">-	select HAVE_FUNCTION_GRAPH_FP_TEST</span>
 	select HAVE_FUNCTION_GRAPH_TRACER
 	select HAVE_FUNCTION_TRACER
 	select HAVE_GCC_PLUGINS
<span class="p_chunk">@@ -157,6 +157,7 @@</span> <span class="p_context"> config X86</span>
 	select SPARSE_IRQ
 	select SRCU
 	select SYSCTL_EXCEPTION_TRACE
<span class="p_add">+	select THREAD_INFO_IN_TASK</span>
 	select USER_STACKTRACE_SUPPORT
 	select VIRT_TO_BUS
 	select X86_DEV_DMA_OPS			if X86_64
<span class="p_header">diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c</span>
<span class="p_header">index 1433f6b4607d..bdd9cc59d20f 100644</span>
<span class="p_header">--- a/arch/x86/entry/common.c</span>
<span class="p_header">+++ b/arch/x86/entry/common.c</span>
<span class="p_chunk">@@ -31,13 +31,6 @@</span> <span class="p_context"></span>
 #define CREATE_TRACE_POINTS
 #include &lt;trace/events/syscalls.h&gt;
 
<span class="p_del">-static struct thread_info *pt_regs_to_thread_info(struct pt_regs *regs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long top_of_stack =</span>
<span class="p_del">-		(unsigned long)(regs + 1) + TOP_OF_KERNEL_STACK_PADDING;</span>
<span class="p_del">-	return (struct thread_info *)(top_of_stack - THREAD_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #ifdef CONFIG_CONTEXT_TRACKING
 /* Called on entry from user mode with IRQs off. */
 __visible inline void enter_from_user_mode(void)
<span class="p_chunk">@@ -71,7 +64,7 @@</span> <span class="p_context"> static long syscall_trace_enter(struct pt_regs *regs)</span>
 {
 	u32 arch = in_ia32_syscall() ? AUDIT_ARCH_I386 : AUDIT_ARCH_X86_64;
 
<span class="p_del">-	struct thread_info *ti = pt_regs_to_thread_info(regs);</span>
<span class="p_add">+	struct thread_info *ti = current_thread_info();</span>
 	unsigned long ret = 0;
 	bool emulated = false;
 	u32 work;
<span class="p_chunk">@@ -173,18 +166,17 @@</span> <span class="p_context"> static void exit_to_usermode_loop(struct pt_regs *regs, u32 cached_flags)</span>
 		/* Disable IRQs and retry */
 		local_irq_disable();
 
<span class="p_del">-		cached_flags = READ_ONCE(pt_regs_to_thread_info(regs)-&gt;flags);</span>
<span class="p_add">+		cached_flags = READ_ONCE(current_thread_info()-&gt;flags);</span>
 
 		if (!(cached_flags &amp; EXIT_TO_USERMODE_LOOP_FLAGS))
 			break;
<span class="p_del">-</span>
 	}
 }
 
 /* Called with IRQs disabled. */
 __visible inline void prepare_exit_to_usermode(struct pt_regs *regs)
 {
<span class="p_del">-	struct thread_info *ti = pt_regs_to_thread_info(regs);</span>
<span class="p_add">+	struct thread_info *ti = current_thread_info();</span>
 	u32 cached_flags;
 
 	if (IS_ENABLED(CONFIG_PROVE_LOCKING) &amp;&amp; WARN_ON(!irqs_disabled()))
<span class="p_chunk">@@ -209,7 +201,7 @@</span> <span class="p_context"> __visible inline void prepare_exit_to_usermode(struct pt_regs *regs)</span>
 	 * special case only applies after poking regs and before the
 	 * very next return to user mode.
 	 */
<span class="p_del">-	ti-&gt;status &amp;= ~(TS_COMPAT|TS_I386_REGS_POKED);</span>
<span class="p_add">+	current-&gt;thread.status &amp;= ~(TS_COMPAT|TS_I386_REGS_POKED);</span>
 #endif
 
 	user_enter_irqoff();
<span class="p_chunk">@@ -247,7 +239,7 @@</span> <span class="p_context"> static void syscall_slow_exit_work(struct pt_regs *regs, u32 cached_flags)</span>
  */
 __visible inline void syscall_return_slowpath(struct pt_regs *regs)
 {
<span class="p_del">-	struct thread_info *ti = pt_regs_to_thread_info(regs);</span>
<span class="p_add">+	struct thread_info *ti = current_thread_info();</span>
 	u32 cached_flags = READ_ONCE(ti-&gt;flags);
 
 	CT_WARN_ON(ct_state() != CONTEXT_KERNEL);
<span class="p_chunk">@@ -270,7 +262,7 @@</span> <span class="p_context"> __visible inline void syscall_return_slowpath(struct pt_regs *regs)</span>
 #ifdef CONFIG_X86_64
 __visible void do_syscall_64(struct pt_regs *regs)
 {
<span class="p_del">-	struct thread_info *ti = pt_regs_to_thread_info(regs);</span>
<span class="p_add">+	struct thread_info *ti = current_thread_info();</span>
 	unsigned long nr = regs-&gt;orig_ax;
 
 	enter_from_user_mode();
<span class="p_chunk">@@ -303,11 +295,11 @@</span> <span class="p_context"> __visible void do_syscall_64(struct pt_regs *regs)</span>
  */
 static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)
 {
<span class="p_del">-	struct thread_info *ti = pt_regs_to_thread_info(regs);</span>
<span class="p_add">+	struct thread_info *ti = current_thread_info();</span>
 	unsigned int nr = (unsigned int)regs-&gt;orig_ax;
 
 #ifdef CONFIG_IA32_EMULATION
<span class="p_del">-	ti-&gt;status |= TS_COMPAT;</span>
<span class="p_add">+	current-&gt;thread.status |= TS_COMPAT;</span>
 #endif
 
 	if (READ_ONCE(ti-&gt;flags) &amp; _TIF_WORK_SYSCALL_ENTRY) {
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index 0b56666e6039..b75a8bcd2d23 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -204,34 +204,70 @@</span> <span class="p_context"></span>
 	POP_GS_EX
 .endm
 
<span class="p_add">+/*</span>
<span class="p_add">+ * %eax: prev task</span>
<span class="p_add">+ * %edx: next task</span>
<span class="p_add">+ */</span>
<span class="p_add">+ENTRY(__switch_to_asm)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Save callee-saved registers</span>
<span class="p_add">+	 * This must match the order in struct inactive_task_frame</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pushl	%ebp</span>
<span class="p_add">+	pushl	%ebx</span>
<span class="p_add">+	pushl	%edi</span>
<span class="p_add">+	pushl	%esi</span>
<span class="p_add">+</span>
<span class="p_add">+	/* switch stack */</span>
<span class="p_add">+	movl	%esp, TASK_threadsp(%eax)</span>
<span class="p_add">+	movl	TASK_threadsp(%edx), %esp</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_CC_STACKPROTECTOR</span>
<span class="p_add">+	movl	TASK_stack_canary(%edx), %ebx</span>
<span class="p_add">+	movl	%ebx, PER_CPU_VAR(stack_canary)+stack_canary_offset</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/* restore callee-saved registers */</span>
<span class="p_add">+	popl	%esi</span>
<span class="p_add">+	popl	%edi</span>
<span class="p_add">+	popl	%ebx</span>
<span class="p_add">+	popl	%ebp</span>
<span class="p_add">+</span>
<span class="p_add">+	jmp	__switch_to</span>
<span class="p_add">+END(__switch_to_asm)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * A newly forked process directly context switches into this address.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * eax: prev task we switched from</span>
<span class="p_add">+ * ebx: kernel thread func (NULL for user thread)</span>
<span class="p_add">+ * edi: kernel thread arg</span>
<span class="p_add">+ */</span>
 ENTRY(ret_from_fork)
 	pushl	%eax
 	call	schedule_tail
 	popl	%eax
 
<span class="p_add">+	testl	%ebx, %ebx</span>
<span class="p_add">+	jnz	1f		/* kernel threads are uncommon */</span>
<span class="p_add">+</span>
<span class="p_add">+2:</span>
 	/* When we fork, we trace the syscall return in the child, too. */
 	movl    %esp, %eax
 	call    syscall_return_slowpath
 	jmp     restore_all
<span class="p_del">-END(ret_from_fork)</span>
<span class="p_del">-</span>
<span class="p_del">-ENTRY(ret_from_kernel_thread)</span>
<span class="p_del">-	pushl	%eax</span>
<span class="p_del">-	call	schedule_tail</span>
<span class="p_del">-	popl	%eax</span>
<span class="p_del">-	movl	PT_EBP(%esp), %eax</span>
<span class="p_del">-	call	*PT_EBX(%esp)</span>
<span class="p_del">-	movl	$0, PT_EAX(%esp)</span>
 
<span class="p_add">+	/* kernel thread */</span>
<span class="p_add">+1:	movl	%edi, %eax</span>
<span class="p_add">+	call	*%ebx</span>
 	/*
<span class="p_del">-	 * Kernel threads return to userspace as if returning from a syscall.</span>
<span class="p_del">-	 * We should check whether anything actually uses this path and, if so,</span>
<span class="p_del">-	 * consider switching it over to ret_from_fork.</span>
<span class="p_add">+	 * A kernel thread is allowed to return here after successfully</span>
<span class="p_add">+	 * calling do_execve().  Exit to userspace to complete the execve()</span>
<span class="p_add">+	 * syscall.</span>
 	 */
<span class="p_del">-	movl    %esp, %eax</span>
<span class="p_del">-	call    syscall_return_slowpath</span>
<span class="p_del">-	jmp     restore_all</span>
<span class="p_del">-ENDPROC(ret_from_kernel_thread)</span>
<span class="p_add">+	movl	$0, PT_EAX(%esp)</span>
<span class="p_add">+	jmp	2b</span>
<span class="p_add">+END(ret_from_fork)</span>
 
 /*
  * Return to user mode is not as complex as all this looks,
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index d172c619c449..80ab68a42621 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -179,7 +179,8 @@</span> <span class="p_context"> GLOBAL(entry_SYSCALL_64_after_swapgs)</span>
 	 * If we need to do entry work or if we guess we&#39;ll need to do
 	 * exit work, go straight to the slow path.
 	 */
<span class="p_del">-	testl	$_TIF_WORK_SYSCALL_ENTRY|_TIF_ALLWORK_MASK, ASM_THREAD_INFO(TI_flags, %rsp, SIZEOF_PTREGS)</span>
<span class="p_add">+	movq	PER_CPU_VAR(current_task), %r11</span>
<span class="p_add">+	testl	$_TIF_WORK_SYSCALL_ENTRY|_TIF_ALLWORK_MASK, TASK_TI_flags(%r11)</span>
 	jnz	entry_SYSCALL64_slow_path
 
 entry_SYSCALL_64_fastpath:
<span class="p_chunk">@@ -217,7 +218,8 @@</span> <span class="p_context"> GLOBAL(entry_SYSCALL_64_after_swapgs)</span>
 	 */
 	DISABLE_INTERRUPTS(CLBR_NONE)
 	TRACE_IRQS_OFF
<span class="p_del">-	testl	$_TIF_ALLWORK_MASK, ASM_THREAD_INFO(TI_flags, %rsp, SIZEOF_PTREGS)</span>
<span class="p_add">+	movq	PER_CPU_VAR(current_task), %r11</span>
<span class="p_add">+	testl	$_TIF_ALLWORK_MASK, TASK_TI_flags(%r11)</span>
 	jnz	1f
 
 	LOCKDEP_SYS_EXIT
<span class="p_chunk">@@ -351,8 +353,7 @@</span> <span class="p_context"> ENTRY(stub_ptregs_64)</span>
 	jmp	entry_SYSCALL64_slow_path
 
 1:
<span class="p_del">-	/* Called from C */</span>
<span class="p_del">-	jmp	*%rax				/* called from C */</span>
<span class="p_add">+	jmp	*%rax				/* Called from C */</span>
 END(stub_ptregs_64)
 
 .macro ptregs_stub func
<span class="p_chunk">@@ -369,41 +370,73 @@</span> <span class="p_context"> END(ptregs_\func)</span>
 #include &lt;asm/syscalls_64.h&gt;
 
 /*
<span class="p_add">+ * %rdi: prev task</span>
<span class="p_add">+ * %rsi: next task</span>
<span class="p_add">+ */</span>
<span class="p_add">+ENTRY(__switch_to_asm)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Save callee-saved registers</span>
<span class="p_add">+	 * This must match the order in inactive_task_frame</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pushq	%rbp</span>
<span class="p_add">+	pushq	%rbx</span>
<span class="p_add">+	pushq	%r12</span>
<span class="p_add">+	pushq	%r13</span>
<span class="p_add">+	pushq	%r14</span>
<span class="p_add">+	pushq	%r15</span>
<span class="p_add">+</span>
<span class="p_add">+	/* switch stack */</span>
<span class="p_add">+	movq	%rsp, TASK_threadsp(%rdi)</span>
<span class="p_add">+	movq	TASK_threadsp(%rsi), %rsp</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_CC_STACKPROTECTOR</span>
<span class="p_add">+	movq	TASK_stack_canary(%rsi), %rbx</span>
<span class="p_add">+	movq	%rbx, PER_CPU_VAR(irq_stack_union)+stack_canary_offset</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/* restore callee-saved registers */</span>
<span class="p_add">+	popq	%r15</span>
<span class="p_add">+	popq	%r14</span>
<span class="p_add">+	popq	%r13</span>
<span class="p_add">+	popq	%r12</span>
<span class="p_add">+	popq	%rbx</span>
<span class="p_add">+	popq	%rbp</span>
<span class="p_add">+</span>
<span class="p_add">+	jmp	__switch_to</span>
<span class="p_add">+END(__switch_to_asm)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * A newly forked process directly context switches into this address.
  *
<span class="p_del">- * rdi: prev task we switched from</span>
<span class="p_add">+ * rax: prev task we switched from</span>
<span class="p_add">+ * rbx: kernel thread func (NULL for user thread)</span>
<span class="p_add">+ * r12: kernel thread arg</span>
  */
 ENTRY(ret_from_fork)
<span class="p_del">-	LOCK ; btr $TIF_FORK, TI_flags(%r8)</span>
<span class="p_del">-</span>
<span class="p_add">+	movq	%rax, %rdi</span>
 	call	schedule_tail			/* rdi: &#39;prev&#39; task parameter */
 
<span class="p_del">-	testb	$3, CS(%rsp)			/* from kernel_thread? */</span>
<span class="p_del">-	jnz	1f</span>
<span class="p_add">+	testq	%rbx, %rbx			/* from kernel_thread? */</span>
<span class="p_add">+	jnz	1f				/* kernel threads are uncommon */</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We came from kernel_thread.  This code path is quite twisted, and</span>
<span class="p_del">-	 * someone should clean it up.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * copy_thread_tls stashes the function pointer in RBX and the</span>
<span class="p_del">-	 * parameter to be passed in RBP.  The called function is permitted</span>
<span class="p_del">-	 * to call do_execve and thereby jump to user mode.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	movq	RBP(%rsp), %rdi</span>
<span class="p_del">-	call	*RBX(%rsp)</span>
<span class="p_del">-	movl	$0, RAX(%rsp)</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Fall through as though we&#39;re exiting a syscall.  This makes a</span>
<span class="p_del">-	 * twisted sort of sense if we just called do_execve.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-</span>
<span class="p_del">-1:</span>
<span class="p_add">+2:</span>
 	movq	%rsp, %rdi
 	call	syscall_return_slowpath	/* returns with IRQs disabled */
 	TRACE_IRQS_ON			/* user mode is traced as IRQS on */
 	SWAPGS
 	jmp	restore_regs_and_iret
<span class="p_add">+</span>
<span class="p_add">+1:</span>
<span class="p_add">+	/* kernel thread */</span>
<span class="p_add">+	movq	%r12, %rdi</span>
<span class="p_add">+	call	*%rbx</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * A kernel thread is allowed to return here after successfully</span>
<span class="p_add">+	 * calling do_execve().  Exit to userspace to complete the execve()</span>
<span class="p_add">+	 * syscall.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq	$0, RAX(%rsp)</span>
<span class="p_add">+	jmp	2b</span>
 END(ret_from_fork)
 
 /*
<span class="p_chunk">@@ -555,27 +588,69 @@</span> <span class="p_context"> ENTRY(native_iret)</span>
 
 #ifdef CONFIG_X86_ESPFIX64
 native_irq_return_ldt:
<span class="p_del">-	pushq	%rax</span>
<span class="p_del">-	pushq	%rdi</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We are running with user GSBASE.  All GPRs contain their user</span>
<span class="p_add">+	 * values.  We have a percpu ESPFIX stack that is eight slots</span>
<span class="p_add">+	 * long (see ESPFIX_STACK_SIZE).  espfix_waddr points to the bottom</span>
<span class="p_add">+	 * of the ESPFIX stack.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We clobber RAX and RDI in this code.  We stash RDI on the</span>
<span class="p_add">+	 * normal stack and RAX on the ESPFIX stack.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The ESPFIX stack layout we set up looks like this:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * --- top of ESPFIX stack ---</span>
<span class="p_add">+	 * SS</span>
<span class="p_add">+	 * RSP</span>
<span class="p_add">+	 * RFLAGS</span>
<span class="p_add">+	 * CS</span>
<span class="p_add">+	 * RIP  &lt;-- RSP points here when we&#39;re done</span>
<span class="p_add">+	 * RAX  &lt;-- espfix_waddr points here</span>
<span class="p_add">+	 * --- bottom of ESPFIX stack ---</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	pushq	%rdi				/* Stash user RDI */</span>
 	SWAPGS
 	movq	PER_CPU_VAR(espfix_waddr), %rdi
<span class="p_del">-	movq	%rax, (0*8)(%rdi)		/* RAX */</span>
<span class="p_del">-	movq	(2*8)(%rsp), %rax		/* RIP */</span>
<span class="p_add">+	movq	%rax, (0*8)(%rdi)		/* user RAX */</span>
<span class="p_add">+	movq	(1*8)(%rsp), %rax		/* user RIP */</span>
 	movq	%rax, (1*8)(%rdi)
<span class="p_del">-	movq	(3*8)(%rsp), %rax		/* CS */</span>
<span class="p_add">+	movq	(2*8)(%rsp), %rax		/* user CS */</span>
 	movq	%rax, (2*8)(%rdi)
<span class="p_del">-	movq	(4*8)(%rsp), %rax		/* RFLAGS */</span>
<span class="p_add">+	movq	(3*8)(%rsp), %rax		/* user RFLAGS */</span>
 	movq	%rax, (3*8)(%rdi)
<span class="p_del">-	movq	(6*8)(%rsp), %rax		/* SS */</span>
<span class="p_add">+	movq	(5*8)(%rsp), %rax		/* user SS */</span>
 	movq	%rax, (5*8)(%rdi)
<span class="p_del">-	movq	(5*8)(%rsp), %rax		/* RSP */</span>
<span class="p_add">+	movq	(4*8)(%rsp), %rax		/* user RSP */</span>
 	movq	%rax, (4*8)(%rdi)
<span class="p_del">-	andl	$0xffff0000, %eax</span>
<span class="p_del">-	popq	%rdi</span>
<span class="p_add">+	/* Now RAX == RSP. */</span>
<span class="p_add">+</span>
<span class="p_add">+	andl	$0xffff0000, %eax		/* RAX = (RSP &amp; 0xffff0000) */</span>
<span class="p_add">+	popq	%rdi				/* Restore user RDI */</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * espfix_stack[31:16] == 0.  The page tables are set up such that</span>
<span class="p_add">+	 * (espfix_stack | (X &amp; 0xffff0000)) points to a read-only alias of</span>
<span class="p_add">+	 * espfix_waddr for any X.  That is, there are 65536 RO aliases of</span>
<span class="p_add">+	 * the same page.  Set up RSP so that RSP[31:16] contains the</span>
<span class="p_add">+	 * respective 16 bits of the /userspace/ RSP and RSP nonetheless</span>
<span class="p_add">+	 * still points to an RO alias of the ESPFIX stack.</span>
<span class="p_add">+	 */</span>
 	orq	PER_CPU_VAR(espfix_stack), %rax
 	SWAPGS
 	movq	%rax, %rsp
<span class="p_del">-	popq	%rax</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At this point, we cannot write to the stack any more, but we can</span>
<span class="p_add">+	 * still read.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	popq	%rax				/* Restore user RAX */</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * RSP now points to an ordinary IRET frame, except that the page</span>
<span class="p_add">+	 * is read-only and RSP[31:16] are preloaded with the userspace</span>
<span class="p_add">+	 * values.  We can now IRET back to userspace.</span>
<span class="p_add">+	 */</span>
 	jmp	native_irq_return_iret
 #endif
 END(common_interrupt)
<span class="p_header">diff --git a/arch/x86/events/core.c b/arch/x86/events/core.c</span>
<span class="p_header">index d0efb5cb1b00..0a8bd7fcdbed 100644</span>
<span class="p_header">--- a/arch/x86/events/core.c</span>
<span class="p_header">+++ b/arch/x86/events/core.c</span>
<span class="p_chunk">@@ -37,6 +37,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/timer.h&gt;
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/ldt.h&gt;
<span class="p_add">+#include &lt;asm/unwind.h&gt;</span>
 
 #include &quot;perf_event.h&quot;
 
<span class="p_chunk">@@ -2247,39 +2248,26 @@</span> <span class="p_context"> void arch_perf_update_userpage(struct perf_event *event,</span>
 	cyc2ns_read_end(data);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * callchain support</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-static int backtrace_stack(void *data, char *name)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int backtrace_address(void *data, unsigned long addr, int reliable)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct perf_callchain_entry_ctx *entry = data;</span>
<span class="p_del">-</span>
<span class="p_del">-	return perf_callchain_store(entry, addr);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static const struct stacktrace_ops backtrace_ops = {</span>
<span class="p_del">-	.stack			= backtrace_stack,</span>
<span class="p_del">-	.address		= backtrace_address,</span>
<span class="p_del">-	.walk_stack		= print_context_stack_bp,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 void
 perf_callchain_kernel(struct perf_callchain_entry_ctx *entry, struct pt_regs *regs)
 {
<span class="p_add">+	struct unwind_state state;</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+</span>
 	if (perf_guest_cbs &amp;&amp; perf_guest_cbs-&gt;is_in_guest()) {
 		/* TODO: We don&#39;t support guest os callchain now */
 		return;
 	}
 
<span class="p_del">-	perf_callchain_store(entry, regs-&gt;ip);</span>
<span class="p_add">+	if (perf_callchain_store(entry, regs-&gt;ip))</span>
<span class="p_add">+		return;</span>
 
<span class="p_del">-	dump_trace(NULL, regs, NULL, 0, &amp;backtrace_ops, entry);</span>
<span class="p_add">+	for (unwind_start(&amp;state, current, regs, NULL); !unwind_done(&amp;state);</span>
<span class="p_add">+	     unwind_next_frame(&amp;state)) {</span>
<span class="p_add">+		addr = unwind_get_return_address(&amp;state);</span>
<span class="p_add">+		if (!addr || perf_callchain_store(entry, addr))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	}</span>
 }
 
 static inline int
<span class="p_header">diff --git a/arch/x86/include/asm/alternative.h b/arch/x86/include/asm/alternative.h</span>
<span class="p_header">index e77a6443104f..1b020381ab38 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/alternative.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/alternative.h</span>
<span class="p_chunk">@@ -217,10 +217,14 @@</span> <span class="p_context"> static inline int alternatives_text_reserved(void *start, void *end)</span>
  */
 #define alternative_call_2(oldfunc, newfunc1, feature1, newfunc2, feature2,   \
 			   output, input...)				      \
<span class="p_add">+{									      \</span>
<span class="p_add">+	register void *__sp asm(_ASM_SP);				      \</span>
 	asm volatile (ALTERNATIVE_2(&quot;call %P[old]&quot;, &quot;call %P[new1]&quot;, feature1,\
 		&quot;call %P[new2]&quot;, feature2)				      \
<span class="p_del">-		: output : [old] &quot;i&quot; (oldfunc), [new1] &quot;i&quot; (newfunc1),	      \</span>
<span class="p_del">-		[new2] &quot;i&quot; (newfunc2), ## input)</span>
<span class="p_add">+		: output, &quot;+r&quot; (__sp)					      \</span>
<span class="p_add">+		: [old] &quot;i&quot; (oldfunc), [new1] &quot;i&quot; (newfunc1),		      \</span>
<span class="p_add">+		  [new2] &quot;i&quot; (newfunc2), ## input);			      \</span>
<span class="p_add">+}</span>
 
 /*
  * use this macro(s) if you need more than one output parameter
<span class="p_header">diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="p_header">index 4e10d73cf018..12080d87da3b 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -36,7 +36,7 @@</span> <span class="p_context"> static inline void fill_ldt(struct desc_struct *desc, const struct user_desc *in</span>
 
 extern struct desc_ptr idt_descr;
 extern gate_desc idt_table[];
<span class="p_del">-extern struct desc_ptr debug_idt_descr;</span>
<span class="p_add">+extern const struct desc_ptr debug_idt_descr;</span>
 extern gate_desc debug_idt_table[];
 
 struct gdt_page {
<span class="p_header">diff --git a/arch/x86/include/asm/fpu/xstate.h b/arch/x86/include/asm/fpu/xstate.h</span>
<span class="p_header">index ae55a43e09c0..d4957ac72b48 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fpu/xstate.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fpu/xstate.h</span>
<span class="p_chunk">@@ -45,7 +45,8 @@</span> <span class="p_context"></span>
 extern u64 xfeatures_mask;
 extern u64 xstate_fx_sw_bytes[USER_XSTATE_FX_SW_WORDS];
 
<span class="p_del">-extern void update_regset_xstate_info(unsigned int size, u64 xstate_mask);</span>
<span class="p_add">+extern void __init update_regset_xstate_info(unsigned int size,</span>
<span class="p_add">+					     u64 xstate_mask);</span>
 
 void fpu__xstate_clear_all_cpu_caps(void);
 void *get_xsave_addr(struct xregs_state *xsave, int xstate);
<span class="p_header">diff --git a/arch/x86/include/asm/ftrace.h b/arch/x86/include/asm/ftrace.h</span>
<span class="p_header">index a4820d4df617..eccd0ac6bc38 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/ftrace.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/ftrace.h</span>
<span class="p_chunk">@@ -6,6 +6,7 @@</span> <span class="p_context"></span>
 # define MCOUNT_ADDR		((unsigned long)(__fentry__))
 #else
 # define MCOUNT_ADDR		((unsigned long)(mcount))
<span class="p_add">+# define HAVE_FUNCTION_GRAPH_FP_TEST</span>
 #endif
 #define MCOUNT_INSN_SIZE	5 /* sizeof mcount call */
 
<span class="p_chunk">@@ -13,6 +14,8 @@</span> <span class="p_context"></span>
 #define ARCH_SUPPORTS_FTRACE_OPS 1
 #endif
 
<span class="p_add">+#define HAVE_FUNCTION_GRAPH_RET_ADDR_PTR</span>
<span class="p_add">+</span>
 #ifndef __ASSEMBLY__
 extern void mcount(void);
 extern atomic_t modifying_ftrace_code;
<span class="p_header">diff --git a/arch/x86/include/asm/kaslr.h b/arch/x86/include/asm/kaslr.h</span>
<span class="p_header">index 2674ee3de748..1052a797d71d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/kaslr.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/kaslr.h</span>
<span class="p_chunk">@@ -6,6 +6,7 @@</span> <span class="p_context"> unsigned long kaslr_get_random_long(const char *purpose);</span>
 #ifdef CONFIG_RANDOMIZE_MEMORY
 extern unsigned long page_offset_base;
 extern unsigned long vmalloc_base;
<span class="p_add">+extern unsigned long vmemmap_base;</span>
 
 void kernel_randomize_memory(void);
 #else
<span class="p_header">diff --git a/arch/x86/include/asm/kdebug.h b/arch/x86/include/asm/kdebug.h</span>
<span class="p_header">index 1ef9d581b5d9..d31881188431 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/kdebug.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/kdebug.h</span>
<span class="p_chunk">@@ -24,8 +24,6 @@</span> <span class="p_context"> enum die_val {</span>
 extern void printk_address(unsigned long address);
 extern void die(const char *, struct pt_regs *,long);
 extern int __must_check __die(const char *, struct pt_regs *, long);
<span class="p_del">-extern void show_trace(struct task_struct *t, struct pt_regs *regs,</span>
<span class="p_del">-		       unsigned long *sp, unsigned long bp);</span>
 extern void show_stack_regs(struct pt_regs *regs);
 extern void __show_regs(struct pt_regs *regs, int all);
 extern unsigned long oops_begin(void);
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">index 2970d22d7766..91b6f4eed3fd 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt.h</span>
<span class="p_chunk">@@ -80,10 +80,6 @@</span> <span class="p_context"> static inline unsigned long __read_cr4(void)</span>
 {
 	return PVOP_CALL0(unsigned long, pv_cpu_ops.read_cr4);
 }
<span class="p_del">-static inline unsigned long __read_cr4_safe(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return PVOP_CALL0(unsigned long, pv_cpu_ops.read_cr4_safe);</span>
<span class="p_del">-}</span>
 
 static inline void __write_cr4(unsigned long x)
 {
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">index 7fa9e7740ba3..fcf243f077ac 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_chunk">@@ -108,7 +108,6 @@</span> <span class="p_context"> struct pv_cpu_ops {</span>
 	unsigned long (*read_cr0)(void);
 	void (*write_cr0)(unsigned long);
 
<span class="p_del">-	unsigned long (*read_cr4_safe)(void);</span>
 	unsigned long (*read_cr4)(void);
 	void (*write_cr4)(unsigned long);
 
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64_types.h b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">index 6fdef9eef2d5..3a264200c62f 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -57,11 +57,13 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define MAXMEM		_AC(__AC(1, UL) &lt;&lt; MAX_PHYSMEM_BITS, UL)
 #define VMALLOC_SIZE_TB	_AC(32, UL)
 #define __VMALLOC_BASE	_AC(0xffffc90000000000, UL)
<span class="p_del">-#define VMEMMAP_START	_AC(0xffffea0000000000, UL)</span>
<span class="p_add">+#define __VMEMMAP_BASE	_AC(0xffffea0000000000, UL)</span>
 #ifdef CONFIG_RANDOMIZE_MEMORY
 #define VMALLOC_START	vmalloc_base
<span class="p_add">+#define VMEMMAP_START	vmemmap_base</span>
 #else
 #define VMALLOC_START	__VMALLOC_BASE
<span class="p_add">+#define VMEMMAP_START	__VMEMMAP_BASE</span>
 #endif /* CONFIG_RANDOMIZE_MEMORY */
 #define VMALLOC_END	(VMALLOC_START + _AC((VMALLOC_SIZE_TB &lt;&lt; 40) - 1, UL))
 #define MODULES_VADDR    (__START_KERNEL_map + KERNEL_IMAGE_SIZE)
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 63def9537a2d..984a7bf17f6a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -389,9 +389,9 @@</span> <span class="p_context"> struct thread_struct {</span>
 	unsigned short		fsindex;
 	unsigned short		gsindex;
 #endif
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-	unsigned long		ip;</span>
<span class="p_del">-#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	u32			status;		/* thread synchronous flags */</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_64
 	unsigned long		fsbase;
 	unsigned long		gsbase;
<span class="p_chunk">@@ -438,6 +438,15 @@</span> <span class="p_context"> struct thread_struct {</span>
 };
 
 /*
<span class="p_add">+ * Thread-synchronous status.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This is different from the flags in that nobody else</span>
<span class="p_add">+ * ever touches our thread-synchronous status, so we don&#39;t</span>
<span class="p_add">+ * have to worry about atomic accesses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Set IOPL bits in EFLAGS from given mask
  */
 static inline void native_set_iopl_mask(unsigned mask)
<span class="p_chunk">@@ -724,8 +733,6 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 	.addr_limit		= KERNEL_DS,				  \
 }
 
<span class="p_del">-extern unsigned long thread_saved_pc(struct task_struct *tsk);</span>
<span class="p_del">-</span>
 /*
  * TOP_OF_KERNEL_STACK_PADDING reserves 8 bytes on top of the ring0 stack.
  * This is necessary to guarantee that the entire &quot;struct pt_regs&quot;
<span class="p_chunk">@@ -776,17 +783,13 @@</span> <span class="p_context"> extern unsigned long thread_saved_pc(struct task_struct *tsk);</span>
 	.addr_limit		= KERNEL_DS,			\
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * Return saved PC of a blocked thread.</span>
<span class="p_del">- * What is this good for? it will be always the scheduler or ret_from_fork.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define thread_saved_pc(t)	READ_ONCE_NOCHECK(*(unsigned long *)((t)-&gt;thread.sp - 8))</span>
<span class="p_del">-</span>
 #define task_pt_regs(tsk)	((struct pt_regs *)(tsk)-&gt;thread.sp0 - 1)
 extern unsigned long KSTK_ESP(struct task_struct *task);
 
 #endif /* CONFIG_X86_64 */
 
<span class="p_add">+extern unsigned long thread_saved_pc(struct task_struct *tsk);</span>
<span class="p_add">+</span>
 extern void start_thread(struct pt_regs *regs, unsigned long new_ip,
 					       unsigned long new_sp);
 
<span class="p_header">diff --git a/arch/x86/include/asm/realmode.h b/arch/x86/include/asm/realmode.h</span>
<span class="p_header">index b2988c0ed829..230e1903acf0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/realmode.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/realmode.h</span>
<span class="p_chunk">@@ -44,9 +44,9 @@</span> <span class="p_context"> struct trampoline_header {</span>
 extern struct real_mode_header *real_mode_header;
 extern unsigned char real_mode_blob_end[];
 
<span class="p_del">-extern unsigned long init_rsp;</span>
 extern unsigned long initial_code;
 extern unsigned long initial_gs;
<span class="p_add">+extern unsigned long initial_stack;</span>
 
 extern unsigned char real_mode_blob[];
 extern unsigned char real_mode_relocs[];
<span class="p_header">diff --git a/arch/x86/include/asm/smp.h b/arch/x86/include/asm/smp.h</span>
<span class="p_header">index ebd0c164cd4e..19980b36f394 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/smp.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/smp.h</span>
<span class="p_chunk">@@ -39,9 +39,6 @@</span> <span class="p_context"> DECLARE_EARLY_PER_CPU_READ_MOSTLY(u16, x86_bios_cpu_apicid);</span>
 DECLARE_EARLY_PER_CPU_READ_MOSTLY(int, x86_cpu_to_logical_apicid);
 #endif
 
<span class="p_del">-/* Static state in head.S used to set up a CPU */</span>
<span class="p_del">-extern unsigned long stack_start; /* Initial stack pointer address */</span>
<span class="p_del">-</span>
 struct task_struct;
 
 struct smp_ops {
<span class="p_header">diff --git a/arch/x86/include/asm/special_insns.h b/arch/x86/include/asm/special_insns.h</span>
<span class="p_header">index 587d7914ea4b..19a2224f9e16 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/special_insns.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/special_insns.h</span>
<span class="p_chunk">@@ -59,22 +59,19 @@</span> <span class="p_context"> static inline void native_write_cr3(unsigned long val)</span>
 static inline unsigned long native_read_cr4(void)
 {
 	unsigned long val;
<span class="p_del">-	asm volatile(&quot;mov %%cr4,%0\n\t&quot; : &quot;=r&quot; (val), &quot;=m&quot; (__force_order));</span>
<span class="p_del">-	return val;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline unsigned long native_read_cr4_safe(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long val;</span>
<span class="p_del">-	/* This could fault if %cr4 does not exist. In x86_64, a cr4 always</span>
<span class="p_del">-	 * exists, so it will never fail. */</span>
 #ifdef CONFIG_X86_32
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This could fault if CR4 does not exist.  Non-existent CR4</span>
<span class="p_add">+	 * is functionally equivalent to CR4 == 0.  Keep it simple and pretend</span>
<span class="p_add">+	 * that CR4 == 0 on CPUs that don&#39;t have CR4.</span>
<span class="p_add">+	 */</span>
 	asm volatile(&quot;1: mov %%cr4, %0\n&quot;
 		     &quot;2:\n&quot;
 		     _ASM_EXTABLE(1b, 2b)
 		     : &quot;=r&quot; (val), &quot;=m&quot; (__force_order) : &quot;0&quot; (0));
 #else
<span class="p_del">-	val = native_read_cr4();</span>
<span class="p_add">+	/* CR4 always exists on x86_64. */</span>
<span class="p_add">+	asm volatile(&quot;mov %%cr4,%0\n\t&quot; : &quot;=r&quot; (val), &quot;=m&quot; (__force_order));</span>
 #endif
 	return val;
 }
<span class="p_chunk">@@ -182,11 +179,6 @@</span> <span class="p_context"> static inline unsigned long __read_cr4(void)</span>
 	return native_read_cr4();
 }
 
<span class="p_del">-static inline unsigned long __read_cr4_safe(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return native_read_cr4_safe();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static inline void __write_cr4(unsigned long x)
 {
 	native_write_cr4(x);
<span class="p_header">diff --git a/arch/x86/include/asm/stacktrace.h b/arch/x86/include/asm/stacktrace.h</span>
<span class="p_header">index 0944218af9e2..37f2e0b377ad 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/stacktrace.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/stacktrace.h</span>
<span class="p_chunk">@@ -8,86 +8,86 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/ptrace.h&gt;
<span class="p_add">+#include &lt;asm/switch_to.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+enum stack_type {</span>
<span class="p_add">+	STACK_TYPE_UNKNOWN,</span>
<span class="p_add">+	STACK_TYPE_TASK,</span>
<span class="p_add">+	STACK_TYPE_IRQ,</span>
<span class="p_add">+	STACK_TYPE_SOFTIRQ,</span>
<span class="p_add">+	STACK_TYPE_EXCEPTION,</span>
<span class="p_add">+	STACK_TYPE_EXCEPTION_LAST = STACK_TYPE_EXCEPTION + N_EXCEPTION_STACKS-1,</span>
<span class="p_add">+};</span>
 
<span class="p_del">-extern int kstack_depth_to_print;</span>
<span class="p_del">-</span>
<span class="p_del">-struct thread_info;</span>
<span class="p_del">-struct stacktrace_ops;</span>
<span class="p_del">-</span>
<span class="p_del">-typedef unsigned long (*walk_stack_t)(struct task_struct *task,</span>
<span class="p_del">-				      unsigned long *stack,</span>
<span class="p_del">-				      unsigned long bp,</span>
<span class="p_del">-				      const struct stacktrace_ops *ops,</span>
<span class="p_del">-				      void *data,</span>
<span class="p_del">-				      unsigned long *end,</span>
<span class="p_del">-				      int *graph);</span>
<span class="p_del">-</span>
<span class="p_del">-extern unsigned long</span>
<span class="p_del">-print_context_stack(struct task_struct *task,</span>
<span class="p_del">-		    unsigned long *stack, unsigned long bp,</span>
<span class="p_del">-		    const struct stacktrace_ops *ops, void *data,</span>
<span class="p_del">-		    unsigned long *end, int *graph);</span>
<span class="p_del">-</span>
<span class="p_del">-extern unsigned long</span>
<span class="p_del">-print_context_stack_bp(struct task_struct *task,</span>
<span class="p_del">-		       unsigned long *stack, unsigned long bp,</span>
<span class="p_del">-		       const struct stacktrace_ops *ops, void *data,</span>
<span class="p_del">-		       unsigned long *end, int *graph);</span>
<span class="p_del">-</span>
<span class="p_del">-/* Generic stack tracer with callbacks */</span>
<span class="p_del">-</span>
<span class="p_del">-struct stacktrace_ops {</span>
<span class="p_del">-	int (*address)(void *data, unsigned long address, int reliable);</span>
<span class="p_del">-	/* On negative return stop dumping */</span>
<span class="p_del">-	int (*stack)(void *data, char *name);</span>
<span class="p_del">-	walk_stack_t	walk_stack;</span>
<span class="p_add">+struct stack_info {</span>
<span class="p_add">+	enum stack_type type;</span>
<span class="p_add">+	unsigned long *begin, *end, *next_sp;</span>
 };
 
<span class="p_del">-void dump_trace(struct task_struct *tsk, struct pt_regs *regs,</span>
<span class="p_del">-		unsigned long *stack, unsigned long bp,</span>
<span class="p_del">-		const struct stacktrace_ops *ops, void *data);</span>
<span class="p_add">+bool in_task_stack(unsigned long *stack, struct task_struct *task,</span>
<span class="p_add">+		   struct stack_info *info);</span>
<span class="p_add">+</span>
<span class="p_add">+int get_stack_info(unsigned long *stack, struct task_struct *task,</span>
<span class="p_add">+		   struct stack_info *info, unsigned long *visit_mask);</span>
<span class="p_add">+</span>
<span class="p_add">+void stack_type_str(enum stack_type type, const char **begin,</span>
<span class="p_add">+		    const char **end);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool on_stack(struct stack_info *info, void *addr, size_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	void *begin = info-&gt;begin;</span>
<span class="p_add">+	void *end   = info-&gt;end;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (info-&gt;type != STACK_TYPE_UNKNOWN &amp;&amp;</span>
<span class="p_add">+		addr &gt;= begin &amp;&amp; addr &lt; end &amp;&amp;</span>
<span class="p_add">+		addr + len &gt; begin &amp;&amp; addr + len &lt;= end);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern int kstack_depth_to_print;</span>
 
 #ifdef CONFIG_X86_32
 #define STACKSLOTS_PER_LINE 8
<span class="p_del">-#define get_bp(bp) asm(&quot;movl %%ebp, %0&quot; : &quot;=r&quot; (bp) :)</span>
 #else
 #define STACKSLOTS_PER_LINE 4
<span class="p_del">-#define get_bp(bp) asm(&quot;movq %%rbp, %0&quot; : &quot;=r&quot; (bp) :)</span>
 #endif
 
 #ifdef CONFIG_FRAME_POINTER
<span class="p_del">-static inline unsigned long</span>
<span class="p_del">-stack_frame(struct task_struct *task, struct pt_regs *regs)</span>
<span class="p_add">+static inline unsigned long *</span>
<span class="p_add">+get_frame_pointer(struct task_struct *task, struct pt_regs *regs)</span>
 {
<span class="p_del">-	unsigned long bp;</span>
<span class="p_del">-</span>
 	if (regs)
<span class="p_del">-		return regs-&gt;bp;</span>
<span class="p_add">+		return (unsigned long *)regs-&gt;bp;</span>
 
<span class="p_del">-	if (task == current) {</span>
<span class="p_del">-		/* Grab bp right from our regs */</span>
<span class="p_del">-		get_bp(bp);</span>
<span class="p_del">-		return bp;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (task == current)</span>
<span class="p_add">+		return __builtin_frame_address(0);</span>
 
<span class="p_del">-	/* bp is the last reg pushed by switch_to */</span>
<span class="p_del">-	return *(unsigned long *)task-&gt;thread.sp;</span>
<span class="p_add">+	return (unsigned long *)((struct inactive_task_frame *)task-&gt;thread.sp)-&gt;bp;</span>
 }
 #else
<span class="p_del">-static inline unsigned long</span>
<span class="p_del">-stack_frame(struct task_struct *task, struct pt_regs *regs)</span>
<span class="p_add">+static inline unsigned long *</span>
<span class="p_add">+get_frame_pointer(struct task_struct *task, struct pt_regs *regs)</span>
 {
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_FRAME_POINTER */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long *</span>
<span class="p_add">+get_stack_pointer(struct task_struct *task, struct pt_regs *regs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (regs)</span>
<span class="p_add">+		return (unsigned long *)kernel_stack_pointer(regs);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (task == current)</span>
<span class="p_add">+		return __builtin_frame_address(0);</span>
<span class="p_add">+</span>
<span class="p_add">+	return (unsigned long *)task-&gt;thread.sp;</span>
 }
<span class="p_del">-#endif</span>
 
<span class="p_del">-extern void</span>
<span class="p_del">-show_trace_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_del">-		   unsigned long *stack, unsigned long bp, char *log_lvl);</span>
<span class="p_add">+void show_trace_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_add">+			unsigned long *stack, char *log_lvl);</span>
 
<span class="p_del">-extern void</span>
<span class="p_del">-show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_del">-		   unsigned long *sp, unsigned long bp, char *log_lvl);</span>
<span class="p_add">+void show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_add">+			unsigned long *sp, char *log_lvl);</span>
 
 extern unsigned int code_bytes;
 
<span class="p_chunk">@@ -106,7 +106,7 @@</span> <span class="p_context"> static inline unsigned long caller_frame_pointer(void)</span>
 {
 	struct stack_frame *frame;
 
<span class="p_del">-	get_bp(frame);</span>
<span class="p_add">+	frame = __builtin_frame_address(0);</span>
 
 #ifdef CONFIG_FRAME_POINTER
 	frame = frame-&gt;next_frame;
<span class="p_header">diff --git a/arch/x86/include/asm/switch_to.h b/arch/x86/include/asm/switch_to.h</span>
<span class="p_header">index 8f321a1b03a1..5cb436acd463 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/switch_to.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/switch_to.h</span>
<span class="p_chunk">@@ -2,130 +2,66 @@</span> <span class="p_context"></span>
 #define _ASM_X86_SWITCH_TO_H
 
 struct task_struct; /* one of the stranger aspects of C forward declarations */
<span class="p_add">+</span>
<span class="p_add">+struct task_struct *__switch_to_asm(struct task_struct *prev,</span>
<span class="p_add">+				    struct task_struct *next);</span>
<span class="p_add">+</span>
 __visible struct task_struct *__switch_to(struct task_struct *prev,
<span class="p_del">-					   struct task_struct *next);</span>
<span class="p_add">+					  struct task_struct *next);</span>
 struct tss_struct;
 void __switch_to_xtra(struct task_struct *prev_p, struct task_struct *next_p,
 		      struct tss_struct *tss);
 
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_add">+/* This runs runs on the previous thread&#39;s stack. */</span>
<span class="p_add">+static inline void prepare_switch_to(struct task_struct *prev,</span>
<span class="p_add">+				     struct task_struct *next)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we switch to a stack that has a top-level paging entry</span>
<span class="p_add">+	 * that is not present in the current mm, the resulting #PF will</span>
<span class="p_add">+	 * will be promoted to a double-fault and we&#39;ll panic.  Probe</span>
<span class="p_add">+	 * the new stack now so that vmalloc_fault can fix up the page</span>
<span class="p_add">+	 * tables if needed.  This can only happen if we use a stack</span>
<span class="p_add">+	 * in vmap space.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We assume that the stack is aligned so that it never spans</span>
<span class="p_add">+	 * more than one top-level paging entry.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * To minimize cache pollution, just follow the stack pointer.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	READ_ONCE(*(unsigned char *)next-&gt;thread.sp);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+asmlinkage void ret_from_fork(void);</span>
<span class="p_add">+</span>
<span class="p_add">+/* data that is pointed to by thread.sp */</span>
<span class="p_add">+struct inactive_task_frame {</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	unsigned long r15;</span>
<span class="p_add">+	unsigned long r14;</span>
<span class="p_add">+	unsigned long r13;</span>
<span class="p_add">+	unsigned long r12;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	unsigned long si;</span>
<span class="p_add">+	unsigned long di;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	unsigned long bx;</span>
<span class="p_add">+	unsigned long bp;</span>
<span class="p_add">+	unsigned long ret_addr;</span>
<span class="p_add">+};</span>
 
<span class="p_del">-#ifdef CONFIG_CC_STACKPROTECTOR</span>
<span class="p_del">-#define __switch_canary							\</span>
<span class="p_del">-	&quot;movl %P[task_canary](%[next]), %%ebx\n\t&quot;			\</span>
<span class="p_del">-	&quot;movl %%ebx, &quot;__percpu_arg([stack_canary])&quot;\n\t&quot;</span>
<span class="p_del">-#define __switch_canary_oparam						\</span>
<span class="p_del">-	, [stack_canary] &quot;=m&quot; (stack_canary.canary)</span>
<span class="p_del">-#define __switch_canary_iparam						\</span>
<span class="p_del">-	, [task_canary] &quot;i&quot; (offsetof(struct task_struct, stack_canary))</span>
<span class="p_del">-#else	/* CC_STACKPROTECTOR */</span>
<span class="p_del">-#define __switch_canary</span>
<span class="p_del">-#define __switch_canary_oparam</span>
<span class="p_del">-#define __switch_canary_iparam</span>
<span class="p_del">-#endif	/* CC_STACKPROTECTOR */</span>
<span class="p_add">+struct fork_frame {</span>
<span class="p_add">+	struct inactive_task_frame frame;</span>
<span class="p_add">+	struct pt_regs regs;</span>
<span class="p_add">+};</span>
 
<span class="p_del">-/*</span>
<span class="p_del">- * Saving eflags is important. It switches not only IOPL between tasks,</span>
<span class="p_del">- * it also protects other tasks from NT leaking through sysenter etc.</span>
<span class="p_del">- */</span>
 #define switch_to(prev, next, last)					\
 do {									\
<span class="p_del">-	/*								\</span>
<span class="p_del">-	 * Context-switching clobbers all registers, so we clobber	\</span>
<span class="p_del">-	 * them explicitly, via unused output variables.		\</span>
<span class="p_del">-	 * (EAX and EBP is not listed because EBP is saved/restored	\</span>
<span class="p_del">-	 * explicitly for wchan access and EAX is the return value of	\</span>
<span class="p_del">-	 * __switch_to())						\</span>
<span class="p_del">-	 */								\</span>
<span class="p_del">-	unsigned long ebx, ecx, edx, esi, edi;				\</span>
<span class="p_del">-									\</span>
<span class="p_del">-	asm volatile(&quot;pushl %%ebp\n\t&quot;		/* save    EBP   */	\</span>
<span class="p_del">-		     &quot;movl %%esp,%[prev_sp]\n\t&quot;	/* save    ESP   */ \</span>
<span class="p_del">-		     &quot;movl %[next_sp],%%esp\n\t&quot;	/* restore ESP   */ \</span>
<span class="p_del">-		     &quot;movl $1f,%[prev_ip]\n\t&quot;	/* save    EIP   */	\</span>
<span class="p_del">-		     &quot;pushl %[next_ip]\n\t&quot;	/* restore EIP   */	\</span>
<span class="p_del">-		     __switch_canary					\</span>
<span class="p_del">-		     &quot;jmp __switch_to\n&quot;	/* regparm call  */	\</span>
<span class="p_del">-		     &quot;1:\t&quot;						\</span>
<span class="p_del">-		     &quot;popl %%ebp\n\t&quot;		/* restore EBP   */	\</span>
<span class="p_del">-									\</span>
<span class="p_del">-		     /* output parameters */				\</span>
<span class="p_del">-		     : [prev_sp] &quot;=m&quot; (prev-&gt;thread.sp),		\</span>
<span class="p_del">-		       [prev_ip] &quot;=m&quot; (prev-&gt;thread.ip),		\</span>
<span class="p_del">-		       &quot;=a&quot; (last),					\</span>
<span class="p_del">-									\</span>
<span class="p_del">-		       /* clobbered output registers: */		\</span>
<span class="p_del">-		       &quot;=b&quot; (ebx), &quot;=c&quot; (ecx), &quot;=d&quot; (edx),		\</span>
<span class="p_del">-		       &quot;=S&quot; (esi), &quot;=D&quot; (edi)				\</span>
<span class="p_del">-		       							\</span>
<span class="p_del">-		       __switch_canary_oparam				\</span>
<span class="p_del">-									\</span>
<span class="p_del">-		       /* input parameters: */				\</span>
<span class="p_del">-		     : [next_sp]  &quot;m&quot; (next-&gt;thread.sp),		\</span>
<span class="p_del">-		       [next_ip]  &quot;m&quot; (next-&gt;thread.ip),		\</span>
<span class="p_del">-		       							\</span>
<span class="p_del">-		       /* regparm parameters for __switch_to(): */	\</span>
<span class="p_del">-		       [prev]     &quot;a&quot; (prev),				\</span>
<span class="p_del">-		       [next]     &quot;d&quot; (next)				\</span>
<span class="p_add">+	prepare_switch_to(prev, next);					\</span>
 									\
<span class="p_del">-		       __switch_canary_iparam				\</span>
<span class="p_del">-									\</span>
<span class="p_del">-		     : /* reloaded segment registers */			\</span>
<span class="p_del">-			&quot;memory&quot;);					\</span>
<span class="p_add">+	((last) = __switch_to_asm((prev), (next)));			\</span>
 } while (0)
 
<span class="p_del">-#else /* CONFIG_X86_32 */</span>
<span class="p_del">-</span>
<span class="p_del">-/* frame pointer must be last for get_wchan */</span>
<span class="p_del">-#define SAVE_CONTEXT    &quot;pushq %%rbp ; movq %%rsi,%%rbp\n\t&quot;</span>
<span class="p_del">-#define RESTORE_CONTEXT &quot;movq %%rbp,%%rsi ; popq %%rbp\t&quot;</span>
<span class="p_del">-</span>
<span class="p_del">-#define __EXTRA_CLOBBER  \</span>
<span class="p_del">-	, &quot;rcx&quot;, &quot;rbx&quot;, &quot;rdx&quot;, &quot;r8&quot;, &quot;r9&quot;, &quot;r10&quot;, &quot;r11&quot;, \</span>
<span class="p_del">-	  &quot;r12&quot;, &quot;r13&quot;, &quot;r14&quot;, &quot;r15&quot;, &quot;flags&quot;</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_CC_STACKPROTECTOR</span>
<span class="p_del">-#define __switch_canary							  \</span>
<span class="p_del">-	&quot;movq %P[task_canary](%%rsi),%%r8\n\t&quot;				  \</span>
<span class="p_del">-	&quot;movq %%r8,&quot;__percpu_arg([gs_canary])&quot;\n\t&quot;</span>
<span class="p_del">-#define __switch_canary_oparam						  \</span>
<span class="p_del">-	, [gs_canary] &quot;=m&quot; (irq_stack_union.stack_canary)</span>
<span class="p_del">-#define __switch_canary_iparam						  \</span>
<span class="p_del">-	, [task_canary] &quot;i&quot; (offsetof(struct task_struct, stack_canary))</span>
<span class="p_del">-#else	/* CC_STACKPROTECTOR */</span>
<span class="p_del">-#define __switch_canary</span>
<span class="p_del">-#define __switch_canary_oparam</span>
<span class="p_del">-#define __switch_canary_iparam</span>
<span class="p_del">-#endif	/* CC_STACKPROTECTOR */</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * There is no need to save or restore flags, because flags are always</span>
<span class="p_del">- * clean in kernel mode, with the possible exception of IOPL.  Kernel IOPL</span>
<span class="p_del">- * has no effect.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define switch_to(prev, next, last) \</span>
<span class="p_del">-	asm volatile(SAVE_CONTEXT					  \</span>
<span class="p_del">-	     &quot;movq %%rsp,%P[threadrsp](%[prev])\n\t&quot; /* save RSP */	  \</span>
<span class="p_del">-	     &quot;movq %P[threadrsp](%[next]),%%rsp\n\t&quot; /* restore RSP */	  \</span>
<span class="p_del">-	     &quot;call __switch_to\n\t&quot;					  \</span>
<span class="p_del">-	     &quot;movq &quot;__percpu_arg([current_task])&quot;,%%rsi\n\t&quot;		  \</span>
<span class="p_del">-	     __switch_canary						  \</span>
<span class="p_del">-	     &quot;movq %P[thread_info](%%rsi),%%r8\n\t&quot;			  \</span>
<span class="p_del">-	     &quot;movq %%rax,%%rdi\n\t&quot; 					  \</span>
<span class="p_del">-	     &quot;testl  %[_tif_fork],%P[ti_flags](%%r8)\n\t&quot;		  \</span>
<span class="p_del">-	     &quot;jnz   ret_from_fork\n\t&quot;					  \</span>
<span class="p_del">-	     RESTORE_CONTEXT						  \</span>
<span class="p_del">-	     : &quot;=a&quot; (last)					  	  \</span>
<span class="p_del">-	       __switch_canary_oparam					  \</span>
<span class="p_del">-	     : [next] &quot;S&quot; (next), [prev] &quot;D&quot; (prev),			  \</span>
<span class="p_del">-	       [threadrsp] &quot;i&quot; (offsetof(struct task_struct, thread.sp)), \</span>
<span class="p_del">-	       [ti_flags] &quot;i&quot; (offsetof(struct thread_info, flags)),	  \</span>
<span class="p_del">-	       [_tif_fork] &quot;i&quot; (_TIF_FORK),			  	  \</span>
<span class="p_del">-	       [thread_info] &quot;i&quot; (offsetof(struct task_struct, stack)),   \</span>
<span class="p_del">-	       [current_task] &quot;m&quot; (current_task)			  \</span>
<span class="p_del">-	       __switch_canary_iparam					  \</span>
<span class="p_del">-	     : &quot;memory&quot;, &quot;cc&quot; __EXTRA_CLOBBER)</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* CONFIG_X86_32 */</span>
<span class="p_del">-</span>
 #endif /* _ASM_X86_SWITCH_TO_H */
<span class="p_header">diff --git a/arch/x86/include/asm/syscall.h b/arch/x86/include/asm/syscall.h</span>
<span class="p_header">index 4e23dd15c661..e3c95e8e61c5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/syscall.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/syscall.h</span>
<span class="p_chunk">@@ -60,7 +60,7 @@</span> <span class="p_context"> static inline long syscall_get_error(struct task_struct *task,</span>
 	 * TS_COMPAT is set for 32-bit syscall entries and then
 	 * remains set until we return to user mode.
 	 */
<span class="p_del">-	if (task_thread_info(task)-&gt;status &amp; (TS_COMPAT|TS_I386_REGS_POKED))</span>
<span class="p_add">+	if (task-&gt;thread.status &amp; (TS_COMPAT|TS_I386_REGS_POKED))</span>
 		/*
 		 * Sign-extend the value so (int)-EFOO becomes (long)-EFOO
 		 * and will match correctly in comparisons.
<span class="p_chunk">@@ -116,7 +116,7 @@</span> <span class="p_context"> static inline void syscall_get_arguments(struct task_struct *task,</span>
 					 unsigned long *args)
 {
 # ifdef CONFIG_IA32_EMULATION
<span class="p_del">-	if (task_thread_info(task)-&gt;status &amp; TS_COMPAT)</span>
<span class="p_add">+	if (task-&gt;thread.status &amp; TS_COMPAT)</span>
 		switch (i) {
 		case 0:
 			if (!n--) break;
<span class="p_chunk">@@ -177,7 +177,7 @@</span> <span class="p_context"> static inline void syscall_set_arguments(struct task_struct *task,</span>
 					 const unsigned long *args)
 {
 # ifdef CONFIG_IA32_EMULATION
<span class="p_del">-	if (task_thread_info(task)-&gt;status &amp; TS_COMPAT)</span>
<span class="p_add">+	if (task-&gt;thread.status &amp; TS_COMPAT)</span>
 		switch (i) {
 		case 0:
 			if (!n--) break;
<span class="p_chunk">@@ -234,18 +234,8 @@</span> <span class="p_context"> static inline void syscall_set_arguments(struct task_struct *task,</span>
 
 static inline int syscall_get_arch(void)
 {
<span class="p_del">-#ifdef CONFIG_IA32_EMULATION</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * TS_COMPAT is set for 32-bit syscall entry and then</span>
<span class="p_del">-	 * remains set until we return to user mode.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * x32 tasks should be considered AUDIT_ARCH_X86_64.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (task_thread_info(current)-&gt;status &amp; TS_COMPAT)</span>
<span class="p_del">-		return AUDIT_ARCH_I386;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-	/* Both x32 and x86_64 are considered &quot;64-bit&quot;. */</span>
<span class="p_del">-	return AUDIT_ARCH_X86_64;</span>
<span class="p_add">+	/* x32 tasks should be considered AUDIT_ARCH_X86_64. */</span>
<span class="p_add">+	return in_ia32_syscall() ? AUDIT_ARCH_I386 : AUDIT_ARCH_X86_64;</span>
 }
 #endif	/* CONFIG_X86_32 */
 
<span class="p_header">diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h</span>
<span class="p_header">index 8b7c8d8e0852..2aaca53c0974 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/thread_info.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/thread_info.h</span>
<span class="p_chunk">@@ -52,21 +52,6 @@</span> <span class="p_context"> struct task_struct;</span>
 #include &lt;asm/cpufeature.h&gt;
 #include &lt;linux/atomic.h&gt;
 
<span class="p_del">-struct thread_info {</span>
<span class="p_del">-	struct task_struct	*task;		/* main task structure */</span>
<span class="p_del">-	__u32			flags;		/* low level flags */</span>
<span class="p_del">-	__u32			status;		/* thread synchronous flags */</span>
<span class="p_del">-	__u32			cpu;		/* current CPU */</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-#define INIT_THREAD_INFO(tsk)			\</span>
<span class="p_del">-{						\</span>
<span class="p_del">-	.task		= &amp;tsk,			\</span>
<span class="p_del">-	.flags		= 0,			\</span>
<span class="p_del">-	.cpu		= 0,			\</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#define init_thread_info	(init_thread_union.thread_info)</span>
 #define init_stack		(init_thread_union.stack)
 
 #else /* !__ASSEMBLY__ */
<span class="p_chunk">@@ -95,7 +80,6 @@</span> <span class="p_context"> struct thread_info {</span>
 #define TIF_UPROBE		12	/* breakpointed or singlestepping */
 #define TIF_NOTSC		16	/* TSC is not accessible in userland */
 #define TIF_IA32		17	/* IA32 compatibility process */
<span class="p_del">-#define TIF_FORK		18	/* ret_from_fork */</span>
 #define TIF_NOHZ		19	/* in adaptive nohz mode */
 #define TIF_MEMDIE		20	/* is terminating due to OOM killer */
 #define TIF_POLLING_NRFLAG	21	/* idle is polling for TIF_NEED_RESCHED */
<span class="p_chunk">@@ -119,7 +103,6 @@</span> <span class="p_context"> struct thread_info {</span>
 #define _TIF_UPROBE		(1 &lt;&lt; TIF_UPROBE)
 #define _TIF_NOTSC		(1 &lt;&lt; TIF_NOTSC)
 #define _TIF_IA32		(1 &lt;&lt; TIF_IA32)
<span class="p_del">-#define _TIF_FORK		(1 &lt;&lt; TIF_FORK)</span>
 #define _TIF_NOHZ		(1 &lt;&lt; TIF_NOHZ)
 #define _TIF_POLLING_NRFLAG	(1 &lt;&lt; TIF_POLLING_NRFLAG)
 #define _TIF_IO_BITMAP		(1 &lt;&lt; TIF_IO_BITMAP)
<span class="p_chunk">@@ -160,11 +143,6 @@</span> <span class="p_context"> struct thread_info {</span>
  */
 #ifndef __ASSEMBLY__
 
<span class="p_del">-static inline struct thread_info *current_thread_info(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return (struct thread_info *)(current_top_of_stack() - THREAD_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static inline unsigned long current_stack_pointer(void)
 {
 	unsigned long sp;
<span class="p_chunk">@@ -226,60 +204,19 @@</span> <span class="p_context"> static inline int arch_within_stack_frames(const void * const stack,</span>
 # define cpu_current_top_of_stack (cpu_tss + TSS_sp0)
 #endif
 
<span class="p_del">-/*</span>
<span class="p_del">- * ASM operand which evaluates to a &#39;thread_info&#39; address of</span>
<span class="p_del">- * the current task, if it is known that &quot;reg&quot; is exactly &quot;off&quot;</span>
<span class="p_del">- * bytes below the top of the stack currently.</span>
<span class="p_del">- *</span>
<span class="p_del">- * ( The kernel stack&#39;s size is known at build time, it is usually</span>
<span class="p_del">- *   2 or 4 pages, and the bottom  of the kernel stack contains</span>
<span class="p_del">- *   the thread_info structure. So to access the thread_info very</span>
<span class="p_del">- *   quickly from assembly code we can calculate down from the</span>
<span class="p_del">- *   top of the kernel stack to the bottom, using constant,</span>
<span class="p_del">- *   build-time calculations only. )</span>
<span class="p_del">- *</span>
<span class="p_del">- * For example, to fetch the current thread_info-&gt;flags value into %eax</span>
<span class="p_del">- * on x86-64 defconfig kernels, in syscall entry code where RSP is</span>
<span class="p_del">- * currently at exactly SIZEOF_PTREGS bytes away from the top of the</span>
<span class="p_del">- * stack:</span>
<span class="p_del">- *</span>
<span class="p_del">- *      mov ASM_THREAD_INFO(TI_flags, %rsp, SIZEOF_PTREGS), %eax</span>
<span class="p_del">- *</span>
<span class="p_del">- * will translate to:</span>
<span class="p_del">- *</span>
<span class="p_del">- *      8b 84 24 b8 c0 ff ff      mov    -0x3f48(%rsp), %eax</span>
<span class="p_del">- *</span>
<span class="p_del">- * which is below the current RSP by almost 16K.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define ASM_THREAD_INFO(field, reg, off) ((field)+(off)-THREAD_SIZE)(reg)</span>
<span class="p_del">-</span>
 #endif
 
<span class="p_del">-/*</span>
<span class="p_del">- * Thread-synchronous status.</span>
<span class="p_del">- *</span>
<span class="p_del">- * This is different from the flags in that nobody else</span>
<span class="p_del">- * ever touches our thread-synchronous status, so we don&#39;t</span>
<span class="p_del">- * have to worry about atomic accesses.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define TS_COMPAT		0x0002	/* 32bit syscall active (64BIT)*/</span>
 #ifdef CONFIG_COMPAT
 #define TS_I386_REGS_POKED	0x0004	/* regs poked by 32-bit ptracer */
 #endif
<span class="p_del">-</span>
 #ifndef __ASSEMBLY__
 
<span class="p_del">-static inline bool in_ia32_syscall(void)</span>
<span class="p_del">-{</span>
 #ifdef CONFIG_X86_32
<span class="p_del">-	return true;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-#ifdef CONFIG_IA32_EMULATION</span>
<span class="p_del">-	if (current_thread_info()-&gt;status &amp; TS_COMPAT)</span>
<span class="p_del">-		return true;</span>
<span class="p_add">+#define in_ia32_syscall() true</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define in_ia32_syscall() (IS_ENABLED(CONFIG_IA32_EMULATION) &amp;&amp; \</span>
<span class="p_add">+			   current-&gt;thread.status &amp; TS_COMPAT)</span>
 #endif
<span class="p_del">-	return false;</span>
<span class="p_del">-}</span>
 
 /*
  * Force syscall return via IRET by making it look as if there was
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index dee8a70382ba..6fa85944af83 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -81,7 +81,7 @@</span> <span class="p_context"> DECLARE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate);</span>
 /* Initialize cr4 shadow for this CPU. */
 static inline void cr4_init_shadow(void)
 {
<span class="p_del">-	this_cpu_write(cpu_tlbstate.cr4, __read_cr4_safe());</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.cr4, __read_cr4());</span>
 }
 
 /* Set in this cpu&#39;s CR4. */
<span class="p_header">diff --git a/arch/x86/include/asm/traps.h b/arch/x86/include/asm/traps.h</span>
<span class="p_header">index c3496619740a..01fd0a7f48cd 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/traps.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/traps.h</span>
<span class="p_chunk">@@ -117,6 +117,12 @@</span> <span class="p_context"> extern void ist_exit(struct pt_regs *regs);</span>
 extern void ist_begin_non_atomic(struct pt_regs *regs);
 extern void ist_end_non_atomic(void);
 
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+void __noreturn handle_stack_overflow(const char *message,</span>
<span class="p_add">+				      struct pt_regs *regs,</span>
<span class="p_add">+				      unsigned long fault_address);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* Interrupts/Exceptions */
 enum {
 	X86_TRAP_DE = 0,	/*  0, Divide-by-zero */
<span class="p_header">diff --git a/arch/x86/include/asm/unwind.h b/arch/x86/include/asm/unwind.h</span>
new file mode 100644
<span class="p_header">index 000000000000..c4b6d1cafa46</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/unwind.h</span>
<span class="p_chunk">@@ -0,0 +1,73 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_X86_UNWIND_H</span>
<span class="p_add">+#define _ASM_X86_UNWIND_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/ftrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/stacktrace.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+struct unwind_state {</span>
<span class="p_add">+	struct stack_info stack_info;</span>
<span class="p_add">+	unsigned long stack_mask;</span>
<span class="p_add">+	struct task_struct *task;</span>
<span class="p_add">+	int graph_idx;</span>
<span class="p_add">+#ifdef CONFIG_FRAME_POINTER</span>
<span class="p_add">+	unsigned long *bp;</span>
<span class="p_add">+#else</span>
<span class="p_add">+	unsigned long *sp;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+void __unwind_start(struct unwind_state *state, struct task_struct *task,</span>
<span class="p_add">+		    struct pt_regs *regs, unsigned long *first_frame);</span>
<span class="p_add">+</span>
<span class="p_add">+bool unwind_next_frame(struct unwind_state *state);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool unwind_done(struct unwind_state *state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return state-&gt;stack_info.type == STACK_TYPE_UNKNOWN;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline</span>
<span class="p_add">+void unwind_start(struct unwind_state *state, struct task_struct *task,</span>
<span class="p_add">+		  struct pt_regs *regs, unsigned long *first_frame)</span>
<span class="p_add">+{</span>
<span class="p_add">+	first_frame = first_frame ? : get_stack_pointer(task, regs);</span>
<span class="p_add">+</span>
<span class="p_add">+	__unwind_start(state, task, regs, first_frame);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_FRAME_POINTER</span>
<span class="p_add">+</span>
<span class="p_add">+static inline</span>
<span class="p_add">+unsigned long *unwind_get_return_address_ptr(struct unwind_state *state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (unwind_done(state))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	return state-&gt;bp + 1;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned long unwind_get_return_address(struct unwind_state *state);</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_FRAME_POINTER */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline</span>
<span class="p_add">+unsigned long *unwind_get_return_address_ptr(struct unwind_state *state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline</span>
<span class="p_add">+unsigned long unwind_get_return_address(struct unwind_state *state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (unwind_done(state))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	return ftrace_graph_ret_addr(state-&gt;task, &amp;state-&gt;graph_idx,</span>
<span class="p_add">+				     *state-&gt;sp, state-&gt;sp);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_FRAME_POINTER */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _ASM_X86_UNWIND_H */</span>
<span class="p_header">diff --git a/arch/x86/kernel/Makefile b/arch/x86/kernel/Makefile</span>
<span class="p_header">index 0503f5bfb18d..45257cf84370 100644</span>
<span class="p_header">--- a/arch/x86/kernel/Makefile</span>
<span class="p_header">+++ b/arch/x86/kernel/Makefile</span>
<span class="p_chunk">@@ -125,6 +125,12 @@</span> <span class="p_context"> obj-$(CONFIG_EFI)			+= sysfb_efi.o</span>
 obj-$(CONFIG_PERF_EVENTS)		+= perf_regs.o
 obj-$(CONFIG_TRACING)			+= tracepoint.o
 
<span class="p_add">+ifdef CONFIG_FRAME_POINTER</span>
<span class="p_add">+obj-y					+= unwind_frame.o</span>
<span class="p_add">+else</span>
<span class="p_add">+obj-y					+= unwind_guess.o</span>
<span class="p_add">+endif</span>
<span class="p_add">+</span>
 ###
 # 64 bit specific files
 ifeq ($(CONFIG_X86_64),y)
<span class="p_header">diff --git a/arch/x86/kernel/acpi/sleep.c b/arch/x86/kernel/acpi/sleep.c</span>
<span class="p_header">index adb3eaf8fe2a..48587335ede8 100644</span>
<span class="p_header">--- a/arch/x86/kernel/acpi/sleep.c</span>
<span class="p_header">+++ b/arch/x86/kernel/acpi/sleep.c</span>
<span class="p_chunk">@@ -99,7 +99,7 @@</span> <span class="p_context"> int x86_acpi_suspend_lowlevel(void)</span>
 	saved_magic = 0x12345678;
 #else /* CONFIG_64BIT */
 #ifdef CONFIG_SMP
<span class="p_del">-	stack_start = (unsigned long)temp_stack + sizeof(temp_stack);</span>
<span class="p_add">+	initial_stack = (unsigned long)temp_stack + sizeof(temp_stack);</span>
 	early_gdt_descr.address =
 			(unsigned long)get_cpu_gdt_table(smp_processor_id());
 	initial_gs = per_cpu_offset(smp_processor_id());
<span class="p_header">diff --git a/arch/x86/kernel/apic/apic_flat_64.c b/arch/x86/kernel/apic/apic_flat_64.c</span>
<span class="p_header">index 5b2ae106bd4a..8862da76ef6f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/apic_flat_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/apic_flat_64.c</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"></span>
 static struct apic apic_physflat;
 static struct apic apic_flat;
 
<span class="p_del">-struct apic __read_mostly *apic = &amp;apic_flat;</span>
<span class="p_add">+struct apic *apic __ro_after_init = &amp;apic_flat;</span>
 EXPORT_SYMBOL_GPL(apic);
 
 static int flat_acpi_madt_oem_check(char *oem_id, char *oem_table_id)
<span class="p_chunk">@@ -154,7 +154,7 @@</span> <span class="p_context"> static int flat_probe(void)</span>
 	return 1;
 }
 
<span class="p_del">-static struct apic apic_flat =  {</span>
<span class="p_add">+static struct apic apic_flat __ro_after_init = {</span>
 	.name				= &quot;flat&quot;,
 	.probe				= flat_probe,
 	.acpi_madt_oem_check		= flat_acpi_madt_oem_check,
<span class="p_chunk">@@ -248,7 +248,7 @@</span> <span class="p_context"> static int physflat_probe(void)</span>
 	return 0;
 }
 
<span class="p_del">-static struct apic apic_physflat =  {</span>
<span class="p_add">+static struct apic apic_physflat __ro_after_init = {</span>
 
 	.name				= &quot;physical flat&quot;,
 	.probe				= physflat_probe,
<span class="p_header">diff --git a/arch/x86/kernel/apic/apic_noop.c b/arch/x86/kernel/apic/apic_noop.c</span>
<span class="p_header">index c05688b2deff..b109e4389c92 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/apic_noop.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/apic_noop.c</span>
<span class="p_chunk">@@ -108,7 +108,7 @@</span> <span class="p_context"> static void noop_apic_write(u32 reg, u32 v)</span>
 	WARN_ON_ONCE(boot_cpu_has(X86_FEATURE_APIC) &amp;&amp; !disable_apic);
 }
 
<span class="p_del">-struct apic apic_noop = {</span>
<span class="p_add">+struct apic apic_noop __ro_after_init = {</span>
 	.name				= &quot;noop&quot;,
 	.probe				= noop_probe,
 	.acpi_madt_oem_check		= NULL,
<span class="p_header">diff --git a/arch/x86/kernel/apic/bigsmp_32.c b/arch/x86/kernel/apic/bigsmp_32.c</span>
<span class="p_header">index 06dbaa458bfe..56012010332c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/bigsmp_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/bigsmp_32.c</span>
<span class="p_chunk">@@ -142,7 +142,7 @@</span> <span class="p_context"> static int probe_bigsmp(void)</span>
 	return dmi_bigsmp;
 }
 
<span class="p_del">-static struct apic apic_bigsmp = {</span>
<span class="p_add">+static struct apic apic_bigsmp __ro_after_init = {</span>
 
 	.name				= &quot;bigsmp&quot;,
 	.probe				= probe_bigsmp,
<span class="p_header">diff --git a/arch/x86/kernel/apic/msi.c b/arch/x86/kernel/apic/msi.c</span>
<span class="p_header">index ade25320df96..015bbf30e3e3 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/msi.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/msi.c</span>
<span class="p_chunk">@@ -269,7 +269,7 @@</span> <span class="p_context"> static void hpet_msi_write_msg(struct irq_data *data, struct msi_msg *msg)</span>
 	hpet_msi_write(irq_data_get_irq_handler_data(data), msg);
 }
 
<span class="p_del">-static struct irq_chip hpet_msi_controller = {</span>
<span class="p_add">+static struct irq_chip hpet_msi_controller __ro_after_init = {</span>
 	.name = &quot;HPET-MSI&quot;,
 	.irq_unmask = hpet_msi_unmask,
 	.irq_mask = hpet_msi_mask,
<span class="p_header">diff --git a/arch/x86/kernel/apic/probe_32.c b/arch/x86/kernel/apic/probe_32.c</span>
<span class="p_header">index 7c43e716c158..e5fb2f086460 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/probe_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/probe_32.c</span>
<span class="p_chunk">@@ -72,7 +72,7 @@</span> <span class="p_context"> static int probe_default(void)</span>
 	return 1;
 }
 
<span class="p_del">-static struct apic apic_default = {</span>
<span class="p_add">+static struct apic apic_default __ro_after_init = {</span>
 
 	.name				= &quot;default&quot;,
 	.probe				= probe_default,
<span class="p_chunk">@@ -126,7 +126,7 @@</span> <span class="p_context"> static struct apic apic_default = {</span>
 
 apic_driver(apic_default);
 
<span class="p_del">-struct apic *apic = &amp;apic_default;</span>
<span class="p_add">+struct apic *apic __ro_after_init = &amp;apic_default;</span>
 EXPORT_SYMBOL_GPL(apic);
 
 static int cmdline_apic __initdata;
<span class="p_header">diff --git a/arch/x86/kernel/apic/x2apic_cluster.c b/arch/x86/kernel/apic/x2apic_cluster.c</span>
<span class="p_header">index 54f35d988025..200af5ae9662 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/x2apic_cluster.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/x2apic_cluster.c</span>
<span class="p_chunk">@@ -227,7 +227,7 @@</span> <span class="p_context"> static void cluster_vector_allocation_domain(int cpu, struct cpumask *retmask,</span>
 		cpumask_and(retmask, mask, per_cpu(cpus_in_cluster, cpu));
 }
 
<span class="p_del">-static struct apic apic_x2apic_cluster = {</span>
<span class="p_add">+static struct apic apic_x2apic_cluster __ro_after_init = {</span>
 
 	.name				= &quot;cluster x2apic&quot;,
 	.probe				= x2apic_cluster_probe,
<span class="p_header">diff --git a/arch/x86/kernel/apic/x2apic_phys.c b/arch/x86/kernel/apic/x2apic_phys.c</span>
<span class="p_header">index 4f13f54f1b1f..ff111f05a314 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/x2apic_phys.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/x2apic_phys.c</span>
<span class="p_chunk">@@ -98,7 +98,7 @@</span> <span class="p_context"> static int x2apic_phys_probe(void)</span>
 	return apic == &amp;apic_x2apic_phys;
 }
 
<span class="p_del">-static struct apic apic_x2apic_phys = {</span>
<span class="p_add">+static struct apic apic_x2apic_phys __ro_after_init = {</span>
 
 	.name				= &quot;physical x2apic&quot;,
 	.probe				= x2apic_phys_probe,
<span class="p_header">diff --git a/arch/x86/kernel/apic/x2apic_uv_x.c b/arch/x86/kernel/apic/x2apic_uv_x.c</span>
<span class="p_header">index cb0673c1e940..b9f6157d4271 100644</span>
<span class="p_header">--- a/arch/x86/kernel/apic/x2apic_uv_x.c</span>
<span class="p_header">+++ b/arch/x86/kernel/apic/x2apic_uv_x.c</span>
<span class="p_chunk">@@ -560,7 +560,7 @@</span> <span class="p_context"> static int uv_probe(void)</span>
 	return apic == &amp;apic_x2apic_uv_x;
 }
 
<span class="p_del">-static struct apic __refdata apic_x2apic_uv_x = {</span>
<span class="p_add">+static struct apic apic_x2apic_uv_x __ro_after_init = {</span>
 
 	.name				= &quot;UV large system&quot;,
 	.probe				= uv_probe,
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">index 2bd5c6ff7ee7..c62e015b126c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_chunk">@@ -29,10 +29,13 @@</span> <span class="p_context"></span>
 
 void common(void) {
 	BLANK();
<span class="p_del">-	OFFSET(TI_flags, thread_info, flags);</span>
<span class="p_del">-	OFFSET(TI_status, thread_info, status);</span>
<span class="p_add">+	OFFSET(TASK_threadsp, task_struct, thread.sp);</span>
<span class="p_add">+#ifdef CONFIG_CC_STACKPROTECTOR</span>
<span class="p_add">+	OFFSET(TASK_stack_canary, task_struct, stack_canary);</span>
<span class="p_add">+#endif</span>
 
 	BLANK();
<span class="p_add">+	OFFSET(TASK_TI_flags, task_struct, thread_info.flags);</span>
 	OFFSET(TASK_addr_limit, task_struct, thread.addr_limit);
 
 	BLANK();
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets_32.c b/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_header">index ecdc1d217dc0..880aa093268d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets_32.c</span>
<span class="p_chunk">@@ -57,6 +57,11 @@</span> <span class="p_context"> void foo(void)</span>
 	/* Size of SYSENTER_stack */
 	DEFINE(SIZEOF_SYSENTER_stack, sizeof(((struct tss_struct *)0)-&gt;SYSENTER_stack));
 
<span class="p_add">+#ifdef CONFIG_CC_STACKPROTECTOR</span>
<span class="p_add">+	BLANK();</span>
<span class="p_add">+	OFFSET(stack_canary_offset, stack_canary, canary);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #if defined(CONFIG_LGUEST) || defined(CONFIG_LGUEST_GUEST) || defined(CONFIG_LGUEST_MODULE)
 	BLANK();
 	OFFSET(LGUEST_DATA_irq_enabled, lguest_data, irq_enabled);
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets_64.c b/arch/x86/kernel/asm-offsets_64.c</span>
<span class="p_header">index d875f97d4e0b..210927ee2e74 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets_64.c</span>
<span class="p_chunk">@@ -56,6 +56,11 @@</span> <span class="p_context"> int main(void)</span>
 	OFFSET(TSS_sp0, tss_struct, x86_tss.sp0);
 	BLANK();
 
<span class="p_add">+#ifdef CONFIG_CC_STACKPROTECTOR</span>
<span class="p_add">+	DEFINE(stack_canary_offset, offsetof(union irq_stack_union, stack_canary));</span>
<span class="p_add">+	BLANK();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	DEFINE(__NR_syscall_max, sizeof(syscalls_64) - 1);
 	DEFINE(NR_syscalls, sizeof(syscalls_64));
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 809eda03c527..06919427d451 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -1265,9 +1265,14 @@</span> <span class="p_context"> static __init int setup_disablecpuid(char *arg)</span>
 __setup(&quot;clearcpuid=&quot;, setup_disablecpuid);
 
 #ifdef CONFIG_X86_64
<span class="p_del">-struct desc_ptr idt_descr = { NR_VECTORS * 16 - 1, (unsigned long) idt_table };</span>
<span class="p_del">-struct desc_ptr debug_idt_descr = { NR_VECTORS * 16 - 1,</span>
<span class="p_del">-				    (unsigned long) debug_idt_table };</span>
<span class="p_add">+struct desc_ptr idt_descr __ro_after_init = {</span>
<span class="p_add">+	.size = NR_VECTORS * 16 - 1,</span>
<span class="p_add">+	.address = (unsigned long) idt_table,</span>
<span class="p_add">+};</span>
<span class="p_add">+const struct desc_ptr debug_idt_descr = {</span>
<span class="p_add">+	.size = NR_VECTORS * 16 - 1,</span>
<span class="p_add">+	.address = (unsigned long) debug_idt_table,</span>
<span class="p_add">+};</span>
 
 DEFINE_PER_CPU_FIRST(union irq_stack_union,
 		     irq_stack_union) __aligned(PAGE_SIZE) __visible;
<span class="p_chunk">@@ -1281,7 +1286,7 @@</span> <span class="p_context"> DEFINE_PER_CPU(struct task_struct *, current_task) ____cacheline_aligned =</span>
 EXPORT_PER_CPU_SYMBOL(current_task);
 
 DEFINE_PER_CPU(char *, irq_stack_ptr) =
<span class="p_del">-	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE - 64;</span>
<span class="p_add">+	init_per_cpu_var(irq_stack_union.irq_stack) + IRQ_STACK_SIZE;</span>
 
 DEFINE_PER_CPU(unsigned int, irq_count) __visible = -1;
 
<span class="p_chunk">@@ -1305,11 +1310,6 @@</span> <span class="p_context"> static DEFINE_PER_CPU_PAGE_ALIGNED(char, exception_stacks</span>
 /* May not be marked __init: used by software suspend */
 void syscall_init(void)
 {
<span class="p_del">-	/*</span>
<span class="p_del">-	 * LSTAR and STAR live in a bit strange symbiosis.</span>
<span class="p_del">-	 * They both write to the same internal register. STAR allows to</span>
<span class="p_del">-	 * set CS/DS but only a 32bit target. LSTAR sets the 64bit rip.</span>
<span class="p_del">-	 */</span>
 	wrmsr(MSR_STAR, 0, (__USER32_CS &lt;&lt; 16) | __KERNEL_CS);
 	wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);
 
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mtrr/main.c b/arch/x86/kernel/cpu/mtrr/main.c</span>
<span class="p_header">index 28f1b54b7fad..24e87e74990d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mtrr/main.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mtrr/main.c</span>
<span class="p_chunk">@@ -72,14 +72,14 @@</span> <span class="p_context"> static DEFINE_MUTEX(mtrr_mutex);</span>
 u64 size_or_mask, size_and_mask;
 static bool mtrr_aps_delayed_init;
 
<span class="p_del">-static const struct mtrr_ops *mtrr_ops[X86_VENDOR_NUM];</span>
<span class="p_add">+static const struct mtrr_ops *mtrr_ops[X86_VENDOR_NUM] __ro_after_init;</span>
 
 const struct mtrr_ops *mtrr_if;
 
 static void set_mtrr(unsigned int reg, unsigned long base,
 		     unsigned long size, mtrr_type type);
 
<span class="p_del">-void set_mtrr_ops(const struct mtrr_ops *ops)</span>
<span class="p_add">+void __init set_mtrr_ops(const struct mtrr_ops *ops)</span>
 {
 	if (ops-&gt;vendor &amp;&amp; ops-&gt;vendor &lt; X86_VENDOR_NUM)
 		mtrr_ops[ops-&gt;vendor] = ops;
<span class="p_header">diff --git a/arch/x86/kernel/cpu/mtrr/mtrr.h b/arch/x86/kernel/cpu/mtrr/mtrr.h</span>
<span class="p_header">index 6c7ced07d16d..ad8bd763efa5 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/mtrr/mtrr.h</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/mtrr/mtrr.h</span>
<span class="p_chunk">@@ -54,7 +54,7 @@</span> <span class="p_context"> void fill_mtrr_var_range(unsigned int index,</span>
 bool get_mtrr_state(void);
 void mtrr_bp_pat_init(void);
 
<span class="p_del">-extern void set_mtrr_ops(const struct mtrr_ops *ops);</span>
<span class="p_add">+extern void __init set_mtrr_ops(const struct mtrr_ops *ops);</span>
 
 extern u64 size_or_mask, size_and_mask;
 extern const struct mtrr_ops *mtrr_if;
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack.c b/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">index 92e8f0a7159c..9b7cf5c28f5f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack.c</span>
<span class="p_chunk">@@ -17,7 +17,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/sysfs.h&gt;
 
 #include &lt;asm/stacktrace.h&gt;
<span class="p_del">-</span>
<span class="p_add">+#include &lt;asm/unwind.h&gt;</span>
 
 int panic_on_unrecovered_nmi;
 int panic_on_io_nmi;
<span class="p_chunk">@@ -25,11 +25,29 @@</span> <span class="p_context"> unsigned int code_bytes = 64;</span>
 int kstack_depth_to_print = 3 * STACKSLOTS_PER_LINE;
 static int die_counter;
 
<span class="p_add">+bool in_task_stack(unsigned long *stack, struct task_struct *task,</span>
<span class="p_add">+		   struct stack_info *info)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long *begin = task_stack_page(task);</span>
<span class="p_add">+	unsigned long *end   = task_stack_page(task) + THREAD_SIZE;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (stack &lt; begin || stack &gt;= end)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	info-&gt;type	= STACK_TYPE_TASK;</span>
<span class="p_add">+	info-&gt;begin	= begin;</span>
<span class="p_add">+	info-&gt;end	= end;</span>
<span class="p_add">+	info-&gt;next_sp	= NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void printk_stack_address(unsigned long address, int reliable,
<span class="p_del">-		void *data)</span>
<span class="p_add">+				 char *log_lvl)</span>
 {
<span class="p_add">+	touch_nmi_watchdog();</span>
 	printk(&quot;%s [&lt;%p&gt;] %s%pB\n&quot;,
<span class="p_del">-		(char *)data, (void *)address, reliable ? &quot;&quot; : &quot;? &quot;,</span>
<span class="p_add">+		log_lvl, (void *)address, reliable ? &quot;&quot; : &quot;? &quot;,</span>
 		(void *)address);
 }
 
<span class="p_chunk">@@ -38,176 +56,120 @@</span> <span class="p_context"> void printk_address(unsigned long address)</span>
 	pr_cont(&quot; [&lt;%p&gt;] %pS\n&quot;, (void *)address, (void *)address);
 }
 
<span class="p_del">-#ifdef CONFIG_FUNCTION_GRAPH_TRACER</span>
<span class="p_del">-static void</span>
<span class="p_del">-print_ftrace_graph_addr(unsigned long addr, void *data,</span>
<span class="p_del">-			const struct stacktrace_ops *ops,</span>
<span class="p_del">-			struct task_struct *task, int *graph)</span>
<span class="p_add">+void show_trace_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_add">+			unsigned long *stack, char *log_lvl)</span>
 {
<span class="p_del">-	unsigned long ret_addr;</span>
<span class="p_del">-	int index;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (addr != (unsigned long)return_to_handler)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	index = task-&gt;curr_ret_stack;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!task-&gt;ret_stack || index &lt; *graph)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	index -= *graph;</span>
<span class="p_del">-	ret_addr = task-&gt;ret_stack[index].ret;</span>
<span class="p_del">-</span>
<span class="p_del">-	ops-&gt;address(data, ret_addr, 1);</span>
<span class="p_add">+	struct unwind_state state;</span>
<span class="p_add">+	struct stack_info stack_info = {0};</span>
<span class="p_add">+	unsigned long visit_mask = 0;</span>
<span class="p_add">+	int graph_idx = 0;</span>
 
<span class="p_del">-	(*graph)++;</span>
<span class="p_del">-}</span>
<span class="p_del">-#else</span>
<span class="p_del">-static inline void</span>
<span class="p_del">-print_ftrace_graph_addr(unsigned long addr, void *data,</span>
<span class="p_del">-			const struct stacktrace_ops *ops,</span>
<span class="p_del">-			struct task_struct *task, int *graph)</span>
<span class="p_del">-{ }</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * x86-64 can have up to three kernel stacks:</span>
<span class="p_del">- * process stack</span>
<span class="p_del">- * interrupt stack</span>
<span class="p_del">- * severe exception (double fault, nmi, stack fault, debug, mce) hardware stack</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int valid_stack_ptr(struct task_struct *task,</span>
<span class="p_del">-			void *p, unsigned int size, void *end)</span>
<span class="p_del">-{</span>
<span class="p_del">-	void *t = task_stack_page(task);</span>
<span class="p_del">-	if (end) {</span>
<span class="p_del">-		if (p &lt; end &amp;&amp; p &gt;= (end-THREAD_SIZE))</span>
<span class="p_del">-			return 1;</span>
<span class="p_del">-		else</span>
<span class="p_del">-			return 0;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return p &gt;= t &amp;&amp; p &lt; t + THREAD_SIZE - size;</span>
<span class="p_del">-}</span>
<span class="p_add">+	printk(&quot;%sCall Trace:\n&quot;, log_lvl);</span>
 
<span class="p_del">-unsigned long</span>
<span class="p_del">-print_context_stack(struct task_struct *task,</span>
<span class="p_del">-		unsigned long *stack, unsigned long bp,</span>
<span class="p_del">-		const struct stacktrace_ops *ops, void *data,</span>
<span class="p_del">-		unsigned long *end, int *graph)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct stack_frame *frame = (struct stack_frame *)bp;</span>
<span class="p_add">+	unwind_start(&amp;state, task, regs, stack);</span>
 
 	/*
<span class="p_del">-	 * If we overflowed the stack into a guard page, jump back to the</span>
<span class="p_del">-	 * bottom of the usable stack.</span>
<span class="p_add">+	 * Iterate through the stacks, starting with the current stack pointer.</span>
<span class="p_add">+	 * Each stack has a pointer to the next one.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * x86-64 can have several stacks:</span>
<span class="p_add">+	 * - task stack</span>
<span class="p_add">+	 * - interrupt stack</span>
<span class="p_add">+	 * - HW exception stacks (double fault, nmi, debug, mce)</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * x86-32 can have up to three stacks:</span>
<span class="p_add">+	 * - task stack</span>
<span class="p_add">+	 * - softirq stack</span>
<span class="p_add">+	 * - hardirq stack</span>
 	 */
<span class="p_del">-	if ((unsigned long)task_stack_page(task) - (unsigned long)stack &lt;</span>
<span class="p_del">-	    PAGE_SIZE)</span>
<span class="p_del">-		stack = (unsigned long *)task_stack_page(task);</span>
<span class="p_del">-</span>
<span class="p_del">-	while (valid_stack_ptr(task, stack, sizeof(*stack), end)) {</span>
<span class="p_del">-		unsigned long addr;</span>
<span class="p_del">-</span>
<span class="p_del">-		addr = *stack;</span>
<span class="p_del">-		if (__kernel_text_address(addr)) {</span>
<span class="p_del">-			if ((unsigned long) stack == bp + sizeof(long)) {</span>
<span class="p_del">-				ops-&gt;address(data, addr, 1);</span>
<span class="p_del">-				frame = frame-&gt;next_frame;</span>
<span class="p_del">-				bp = (unsigned long) frame;</span>
<span class="p_del">-			} else {</span>
<span class="p_del">-				ops-&gt;address(data, addr, 0);</span>
<span class="p_del">-			}</span>
<span class="p_del">-			print_ftrace_graph_addr(addr, data, ops, task, graph);</span>
<span class="p_del">-		}</span>
<span class="p_del">-		stack++;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return bp;</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL_GPL(print_context_stack);</span>
<span class="p_del">-</span>
<span class="p_del">-unsigned long</span>
<span class="p_del">-print_context_stack_bp(struct task_struct *task,</span>
<span class="p_del">-		       unsigned long *stack, unsigned long bp,</span>
<span class="p_del">-		       const struct stacktrace_ops *ops, void *data,</span>
<span class="p_del">-		       unsigned long *end, int *graph)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct stack_frame *frame = (struct stack_frame *)bp;</span>
<span class="p_del">-	unsigned long *ret_addr = &amp;frame-&gt;return_address;</span>
<span class="p_add">+	for (; stack; stack = stack_info.next_sp) {</span>
<span class="p_add">+		const char *str_begin, *str_end;</span>
 
<span class="p_del">-	while (valid_stack_ptr(task, ret_addr, sizeof(*ret_addr), end)) {</span>
<span class="p_del">-		unsigned long addr = *ret_addr;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If we overflowed the task stack into a guard page, jump back</span>
<span class="p_add">+		 * to the bottom of the usable stack.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (task_stack_page(task) - (void *)stack &lt; PAGE_SIZE)</span>
<span class="p_add">+			stack = task_stack_page(task);</span>
 
<span class="p_del">-		if (!__kernel_text_address(addr))</span>
<span class="p_add">+		if (get_stack_info(stack, task, &amp;stack_info, &amp;visit_mask))</span>
 			break;
 
<span class="p_del">-		if (ops-&gt;address(data, addr, 1))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		frame = frame-&gt;next_frame;</span>
<span class="p_del">-		ret_addr = &amp;frame-&gt;return_address;</span>
<span class="p_del">-		print_ftrace_graph_addr(addr, data, ops, task, graph);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return (unsigned long)frame;</span>
<span class="p_del">-}</span>
<span class="p_del">-EXPORT_SYMBOL_GPL(print_context_stack_bp);</span>
<span class="p_del">-</span>
<span class="p_del">-static int print_trace_stack(void *data, char *name)</span>
<span class="p_del">-{</span>
<span class="p_del">-	printk(&quot;%s &lt;%s&gt; &quot;, (char *)data, name);</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Print one address/symbol entries per line.</span>
<span class="p_del">- */</span>
<span class="p_del">-static int print_trace_address(void *data, unsigned long addr, int reliable)</span>
<span class="p_del">-{</span>
<span class="p_del">-	touch_nmi_watchdog();</span>
<span class="p_del">-	printk_stack_address(addr, reliable, data);</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static const struct stacktrace_ops print_trace_ops = {</span>
<span class="p_del">-	.stack			= print_trace_stack,</span>
<span class="p_del">-	.address		= print_trace_address,</span>
<span class="p_del">-	.walk_stack		= print_context_stack,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-void</span>
<span class="p_del">-show_trace_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_del">-		unsigned long *stack, unsigned long bp, char *log_lvl)</span>
<span class="p_del">-{</span>
<span class="p_del">-	printk(&quot;%sCall Trace:\n&quot;, log_lvl);</span>
<span class="p_del">-	dump_trace(task, regs, stack, bp, &amp;print_trace_ops, log_lvl);</span>
<span class="p_del">-}</span>
<span class="p_add">+		stack_type_str(stack_info.type, &amp;str_begin, &amp;str_end);</span>
<span class="p_add">+		if (str_begin)</span>
<span class="p_add">+			printk(&quot;%s &lt;%s&gt; &quot;, log_lvl, str_begin);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Scan the stack, printing any text addresses we find.  At the</span>
<span class="p_add">+		 * same time, follow proper stack frames with the unwinder.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Addresses found during the scan which are not reported by</span>
<span class="p_add">+		 * the unwinder are considered to be additional clues which are</span>
<span class="p_add">+		 * sometimes useful for debugging and are prefixed with &#39;?&#39;.</span>
<span class="p_add">+		 * This also serves as a failsafe option in case the unwinder</span>
<span class="p_add">+		 * goes off in the weeds.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		for (; stack &lt; stack_info.end; stack++) {</span>
<span class="p_add">+			unsigned long real_addr;</span>
<span class="p_add">+			int reliable = 0;</span>
<span class="p_add">+			unsigned long addr = *stack;</span>
<span class="p_add">+			unsigned long *ret_addr_p =</span>
<span class="p_add">+				unwind_get_return_address_ptr(&amp;state);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!__kernel_text_address(addr))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (stack == ret_addr_p)</span>
<span class="p_add">+				reliable = 1;</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * When function graph tracing is enabled for a</span>
<span class="p_add">+			 * function, its return address on the stack is</span>
<span class="p_add">+			 * replaced with the address of an ftrace handler</span>
<span class="p_add">+			 * (return_to_handler).  In that case, before printing</span>
<span class="p_add">+			 * the &quot;real&quot; address, we want to print the handler</span>
<span class="p_add">+			 * address as an &quot;unreliable&quot; hint that function graph</span>
<span class="p_add">+			 * tracing was involved.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			real_addr = ftrace_graph_ret_addr(task, &amp;graph_idx,</span>
<span class="p_add">+							  addr, stack);</span>
<span class="p_add">+			if (real_addr != addr)</span>
<span class="p_add">+				printk_stack_address(addr, 0, log_lvl);</span>
<span class="p_add">+			printk_stack_address(real_addr, reliable, log_lvl);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!reliable)</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Get the next frame from the unwinder.  No need to</span>
<span class="p_add">+			 * check for an error: if anything goes wrong, the rest</span>
<span class="p_add">+			 * of the addresses will just be printed as unreliable.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			unwind_next_frame(&amp;state);</span>
<span class="p_add">+		}</span>
 
<span class="p_del">-void show_trace(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_del">-		unsigned long *stack, unsigned long bp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	show_trace_log_lvl(task, regs, stack, bp, &quot;&quot;);</span>
<span class="p_add">+		if (str_end)</span>
<span class="p_add">+			printk(&quot;%s &lt;%s&gt; &quot;, log_lvl, str_end);</span>
<span class="p_add">+	}</span>
 }
 
 void show_stack(struct task_struct *task, unsigned long *sp)
 {
<span class="p_del">-	unsigned long bp = 0;</span>
<span class="p_del">-	unsigned long stack;</span>
<span class="p_add">+	task = task ? : current;</span>
 
 	/*
 	 * Stack frames below this one aren&#39;t interesting.  Don&#39;t show them
 	 * if we&#39;re printing for %current.
 	 */
<span class="p_del">-	if (!sp &amp;&amp; (!task || task == current)) {</span>
<span class="p_del">-		sp = &amp;stack;</span>
<span class="p_del">-		bp = stack_frame(current, NULL);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (!sp &amp;&amp; task == current)</span>
<span class="p_add">+		sp = get_stack_pointer(current, NULL);</span>
 
<span class="p_del">-	show_stack_log_lvl(task, NULL, sp, bp, &quot;&quot;);</span>
<span class="p_add">+	show_stack_log_lvl(task, NULL, sp, &quot;&quot;);</span>
 }
 
 void show_stack_regs(struct pt_regs *regs)
 {
<span class="p_del">-	show_stack_log_lvl(current, regs, (unsigned long *)regs-&gt;sp, regs-&gt;bp, &quot;&quot;);</span>
<span class="p_add">+	show_stack_log_lvl(current, regs, NULL, &quot;&quot;);</span>
 }
 
 static arch_spinlock_t die_lock = __ARCH_SPIN_LOCK_UNLOCKED;
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack_32.c b/arch/x86/kernel/dumpstack_32.c</span>
<span class="p_header">index 09675712eba8..06eb322b5f9f 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack_32.c</span>
<span class="p_chunk">@@ -16,93 +16,121 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/stacktrace.h&gt;
 
<span class="p_del">-static void *is_irq_stack(void *p, void *irq)</span>
<span class="p_add">+void stack_type_str(enum stack_type type, const char **begin, const char **end)</span>
 {
<span class="p_del">-	if (p &lt; irq || p &gt;= (irq + THREAD_SIZE))</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-	return irq + THREAD_SIZE;</span>
<span class="p_add">+	switch (type) {</span>
<span class="p_add">+	case STACK_TYPE_IRQ:</span>
<span class="p_add">+	case STACK_TYPE_SOFTIRQ:</span>
<span class="p_add">+		*begin = &quot;IRQ&quot;;</span>
<span class="p_add">+		*end   = &quot;EOI&quot;;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		*begin = NULL;</span>
<span class="p_add">+		*end   = NULL;</span>
<span class="p_add">+	}</span>
 }
 
<span class="p_del">-</span>
<span class="p_del">-static void *is_hardirq_stack(unsigned long *stack, int cpu)</span>
<span class="p_add">+static bool in_hardirq_stack(unsigned long *stack, struct stack_info *info)</span>
 {
<span class="p_del">-	void *irq = per_cpu(hardirq_stack, cpu);</span>
<span class="p_add">+	unsigned long *begin = (unsigned long *)this_cpu_read(hardirq_stack);</span>
<span class="p_add">+	unsigned long *end   = begin + (THREAD_SIZE / sizeof(long));</span>
 
<span class="p_del">-	return is_irq_stack(stack, irq);</span>
<span class="p_del">-}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This is a software stack, so &#39;end&#39; can be a valid stack pointer.</span>
<span class="p_add">+	 * It just means the stack is empty.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (stack &lt; begin || stack &gt; end)</span>
<span class="p_add">+		return false;</span>
 
<span class="p_del">-static void *is_softirq_stack(unsigned long *stack, int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	void *irq = per_cpu(softirq_stack, cpu);</span>
<span class="p_add">+	info-&gt;type	= STACK_TYPE_IRQ;</span>
<span class="p_add">+	info-&gt;begin	= begin;</span>
<span class="p_add">+	info-&gt;end	= end;</span>
 
<span class="p_del">-	return is_irq_stack(stack, irq);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * See irq_32.c -- the next stack pointer is stored at the beginning of</span>
<span class="p_add">+	 * the stack.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	info-&gt;next_sp	= (unsigned long *)*begin;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
 }
 
<span class="p_del">-void dump_trace(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_del">-		unsigned long *stack, unsigned long bp,</span>
<span class="p_del">-		const struct stacktrace_ops *ops, void *data)</span>
<span class="p_add">+static bool in_softirq_stack(unsigned long *stack, struct stack_info *info)</span>
 {
<span class="p_del">-	const unsigned cpu = get_cpu();</span>
<span class="p_del">-	int graph = 0;</span>
<span class="p_del">-	u32 *prev_esp;</span>
<span class="p_add">+	unsigned long *begin = (unsigned long *)this_cpu_read(softirq_stack);</span>
<span class="p_add">+	unsigned long *end   = begin + (THREAD_SIZE / sizeof(long));</span>
 
<span class="p_del">-	if (!task)</span>
<span class="p_del">-		task = current;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This is a software stack, so &#39;end&#39; can be a valid stack pointer.</span>
<span class="p_add">+	 * It just means the stack is empty.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (stack &lt; begin || stack &gt; end)</span>
<span class="p_add">+		return false;</span>
 
<span class="p_del">-	if (!stack) {</span>
<span class="p_del">-		unsigned long dummy;</span>
<span class="p_add">+	info-&gt;type	= STACK_TYPE_SOFTIRQ;</span>
<span class="p_add">+	info-&gt;begin	= begin;</span>
<span class="p_add">+	info-&gt;end	= end;</span>
 
<span class="p_del">-		stack = &amp;dummy;</span>
<span class="p_del">-		if (task != current)</span>
<span class="p_del">-			stack = (unsigned long *)task-&gt;thread.sp;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The next stack pointer is stored at the beginning of the stack.</span>
<span class="p_add">+	 * See irq_32.c.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	info-&gt;next_sp	= (unsigned long *)*begin;</span>
 
<span class="p_del">-	if (!bp)</span>
<span class="p_del">-		bp = stack_frame(task, regs);</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
 
<span class="p_del">-	for (;;) {</span>
<span class="p_del">-		void *end_stack;</span>
<span class="p_add">+int get_stack_info(unsigned long *stack, struct task_struct *task,</span>
<span class="p_add">+		   struct stack_info *info, unsigned long *visit_mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!stack)</span>
<span class="p_add">+		goto unknown;</span>
 
<span class="p_del">-		end_stack = is_hardirq_stack(stack, cpu);</span>
<span class="p_del">-		if (!end_stack)</span>
<span class="p_del">-			end_stack = is_softirq_stack(stack, cpu);</span>
<span class="p_add">+	task = task ? : current;</span>
 
<span class="p_del">-		bp = ops-&gt;walk_stack(task, stack, bp, ops, data,</span>
<span class="p_del">-				     end_stack, &amp;graph);</span>
<span class="p_add">+	if (in_task_stack(stack, task, info))</span>
<span class="p_add">+		goto recursion_check;</span>
 
<span class="p_del">-		/* Stop if not on irq stack */</span>
<span class="p_del">-		if (!end_stack)</span>
<span class="p_del">-			break;</span>
<span class="p_add">+	if (task != current)</span>
<span class="p_add">+		goto unknown;</span>
 
<span class="p_del">-		/* The previous esp is saved on the bottom of the stack */</span>
<span class="p_del">-		prev_esp = (u32 *)(end_stack - THREAD_SIZE);</span>
<span class="p_del">-		stack = (unsigned long *)*prev_esp;</span>
<span class="p_del">-		if (!stack)</span>
<span class="p_del">-			break;</span>
<span class="p_add">+	if (in_hardirq_stack(stack, info))</span>
<span class="p_add">+		goto recursion_check;</span>
 
<span class="p_del">-		if (ops-&gt;stack(data, &quot;IRQ&quot;) &lt; 0)</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		touch_nmi_watchdog();</span>
<span class="p_add">+	if (in_softirq_stack(stack, info))</span>
<span class="p_add">+		goto recursion_check;</span>
<span class="p_add">+</span>
<span class="p_add">+	goto unknown;</span>
<span class="p_add">+</span>
<span class="p_add">+recursion_check:</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Make sure we don&#39;t iterate through any given stack more than once.</span>
<span class="p_add">+	 * If it comes up a second time then there&#39;s something wrong going on:</span>
<span class="p_add">+	 * just break out and report an unknown stack type.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (visit_mask) {</span>
<span class="p_add">+		if (*visit_mask &amp; (1UL &lt;&lt; info-&gt;type))</span>
<span class="p_add">+			goto unknown;</span>
<span class="p_add">+		*visit_mask |= 1UL &lt;&lt; info-&gt;type;</span>
 	}
<span class="p_del">-	put_cpu();</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+unknown:</span>
<span class="p_add">+	info-&gt;type = STACK_TYPE_UNKNOWN;</span>
<span class="p_add">+	return -EINVAL;</span>
 }
<span class="p_del">-EXPORT_SYMBOL(dump_trace);</span>
 
<span class="p_del">-void</span>
<span class="p_del">-show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_del">-		   unsigned long *sp, unsigned long bp, char *log_lvl)</span>
<span class="p_add">+void show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_add">+			unsigned long *sp, char *log_lvl)</span>
 {
 	unsigned long *stack;
 	int i;
 
<span class="p_del">-	if (sp == NULL) {</span>
<span class="p_del">-		if (regs)</span>
<span class="p_del">-			sp = (unsigned long *)regs-&gt;sp;</span>
<span class="p_del">-		else if (task)</span>
<span class="p_del">-			sp = (unsigned long *)task-&gt;thread.sp;</span>
<span class="p_del">-		else</span>
<span class="p_del">-			sp = (unsigned long *)&amp;sp;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (!try_get_task_stack(task))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	sp = sp ? : get_stack_pointer(task, regs);</span>
 
 	stack = sp;
 	for (i = 0; i &lt; kstack_depth_to_print; i++) {
<span class="p_chunk">@@ -117,7 +145,9 @@</span> <span class="p_context"> show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
 		touch_nmi_watchdog();
 	}
 	pr_cont(&quot;\n&quot;);
<span class="p_del">-	show_trace_log_lvl(task, regs, sp, bp, log_lvl);</span>
<span class="p_add">+	show_trace_log_lvl(task, regs, sp, log_lvl);</span>
<span class="p_add">+</span>
<span class="p_add">+	put_task_stack(task);</span>
 }
 
 
<span class="p_chunk">@@ -139,7 +169,7 @@</span> <span class="p_context"> void show_regs(struct pt_regs *regs)</span>
 		u8 *ip;
 
 		pr_emerg(&quot;Stack:\n&quot;);
<span class="p_del">-		show_stack_log_lvl(NULL, regs, &amp;regs-&gt;sp, 0, KERN_EMERG);</span>
<span class="p_add">+		show_stack_log_lvl(current, regs, NULL, KERN_EMERG);</span>
 
 		pr_emerg(&quot;Code:&quot;);
 
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack_64.c b/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_header">index 9ee4520ce83c..36cf1a498227 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack_64.c</span>
<span class="p_chunk">@@ -16,261 +16,145 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/stacktrace.h&gt;
 
<span class="p_add">+static char *exception_stack_names[N_EXCEPTION_STACKS] = {</span>
<span class="p_add">+		[ DOUBLEFAULT_STACK-1	]	= &quot;#DF&quot;,</span>
<span class="p_add">+		[ NMI_STACK-1		]	= &quot;NMI&quot;,</span>
<span class="p_add">+		[ DEBUG_STACK-1		]	= &quot;#DB&quot;,</span>
<span class="p_add">+		[ MCE_STACK-1		]	= &quot;#MC&quot;,</span>
<span class="p_add">+};</span>
 
<span class="p_del">-#define N_EXCEPTION_STACKS_END \</span>
<span class="p_del">-		(N_EXCEPTION_STACKS + DEBUG_STKSZ/EXCEPTION_STKSZ - 2)</span>
<span class="p_del">-</span>
<span class="p_del">-static char x86_stack_ids[][8] = {</span>
<span class="p_del">-		[ DEBUG_STACK-1			]	= &quot;#DB&quot;,</span>
<span class="p_del">-		[ NMI_STACK-1			]	= &quot;NMI&quot;,</span>
<span class="p_del">-		[ DOUBLEFAULT_STACK-1		]	= &quot;#DF&quot;,</span>
<span class="p_del">-		[ MCE_STACK-1			]	= &quot;#MC&quot;,</span>
<span class="p_del">-#if DEBUG_STKSZ &gt; EXCEPTION_STKSZ</span>
<span class="p_del">-		[ N_EXCEPTION_STACKS ...</span>
<span class="p_del">-		  N_EXCEPTION_STACKS_END	]	= &quot;#DB[?]&quot;</span>
<span class="p_del">-#endif</span>
<span class="p_add">+static unsigned long exception_stack_sizes[N_EXCEPTION_STACKS] = {</span>
<span class="p_add">+	[0 ... N_EXCEPTION_STACKS - 1]		= EXCEPTION_STKSZ,</span>
<span class="p_add">+	[DEBUG_STACK - 1]			= DEBUG_STKSZ</span>
 };
 
<span class="p_del">-static unsigned long *in_exception_stack(unsigned cpu, unsigned long stack,</span>
<span class="p_del">-					 unsigned *usedp, char **idp)</span>
<span class="p_add">+void stack_type_str(enum stack_type type, const char **begin, const char **end)</span>
 {
<span class="p_del">-	unsigned k;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Iterate over all exception stacks, and figure out whether</span>
<span class="p_del">-	 * &#39;stack&#39; is in one of them:</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	for (k = 0; k &lt; N_EXCEPTION_STACKS; k++) {</span>
<span class="p_del">-		unsigned long end = per_cpu(orig_ist, cpu).ist[k];</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Is &#39;stack&#39; above this exception frame&#39;s end?</span>
<span class="p_del">-		 * If yes then skip to the next frame.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (stack &gt;= end)</span>
<span class="p_del">-			continue;</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Is &#39;stack&#39; above this exception frame&#39;s start address?</span>
<span class="p_del">-		 * If yes then we found the right frame.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (stack &gt;= end - EXCEPTION_STKSZ) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * Make sure we only iterate through an exception</span>
<span class="p_del">-			 * stack once. If it comes up for the second time</span>
<span class="p_del">-			 * then there&#39;s something wrong going on - just</span>
<span class="p_del">-			 * break out and return NULL:</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			if (*usedp &amp; (1U &lt;&lt; k))</span>
<span class="p_del">-				break;</span>
<span class="p_del">-			*usedp |= 1U &lt;&lt; k;</span>
<span class="p_del">-			*idp = x86_stack_ids[k];</span>
<span class="p_del">-			return (unsigned long *)end;</span>
<span class="p_del">-		}</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * If this is a debug stack, and if it has a larger size than</span>
<span class="p_del">-		 * the usual exception stacks, then &#39;stack&#39; might still</span>
<span class="p_del">-		 * be within the lower portion of the debug stack:</span>
<span class="p_del">-		 */</span>
<span class="p_del">-#if DEBUG_STKSZ &gt; EXCEPTION_STKSZ</span>
<span class="p_del">-		if (k == DEBUG_STACK - 1 &amp;&amp; stack &gt;= end - DEBUG_STKSZ) {</span>
<span class="p_del">-			unsigned j = N_EXCEPTION_STACKS - 1;</span>
<span class="p_del">-</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * Black magic. A large debug stack is composed of</span>
<span class="p_del">-			 * multiple exception stack entries, which we</span>
<span class="p_del">-			 * iterate through now. Dont look:</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			do {</span>
<span class="p_del">-				++j;</span>
<span class="p_del">-				end -= EXCEPTION_STKSZ;</span>
<span class="p_del">-				x86_stack_ids[j][4] = &#39;1&#39; +</span>
<span class="p_del">-						(j - N_EXCEPTION_STACKS);</span>
<span class="p_del">-			} while (stack &lt; end - EXCEPTION_STKSZ);</span>
<span class="p_del">-			if (*usedp &amp; (1U &lt;&lt; j))</span>
<span class="p_del">-				break;</span>
<span class="p_del">-			*usedp |= 1U &lt;&lt; j;</span>
<span class="p_del">-			*idp = x86_stack_ids[j];</span>
<span class="p_del">-			return (unsigned long *)end;</span>
<span class="p_del">-		}</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	BUILD_BUG_ON(N_EXCEPTION_STACKS != 4);</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (type) {</span>
<span class="p_add">+	case STACK_TYPE_IRQ:</span>
<span class="p_add">+		*begin = &quot;IRQ&quot;;</span>
<span class="p_add">+		*end   = &quot;EOI&quot;;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case STACK_TYPE_EXCEPTION ... STACK_TYPE_EXCEPTION_LAST:</span>
<span class="p_add">+		*begin = exception_stack_names[type - STACK_TYPE_EXCEPTION];</span>
<span class="p_add">+		*end   = &quot;EOE&quot;;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		*begin = NULL;</span>
<span class="p_add">+		*end   = NULL;</span>
 	}
<span class="p_del">-	return NULL;</span>
 }
 
<span class="p_del">-static inline int</span>
<span class="p_del">-in_irq_stack(unsigned long *stack, unsigned long *irq_stack,</span>
<span class="p_del">-	     unsigned long *irq_stack_end)</span>
<span class="p_add">+static bool in_exception_stack(unsigned long *stack, struct stack_info *info)</span>
 {
<span class="p_del">-	return (stack &gt;= irq_stack &amp;&amp; stack &lt; irq_stack_end);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static const unsigned long irq_stack_size =</span>
<span class="p_del">-	(IRQ_STACK_SIZE - 64) / sizeof(unsigned long);</span>
<span class="p_del">-</span>
<span class="p_del">-enum stack_type {</span>
<span class="p_del">-	STACK_IS_UNKNOWN,</span>
<span class="p_del">-	STACK_IS_NORMAL,</span>
<span class="p_del">-	STACK_IS_EXCEPTION,</span>
<span class="p_del">-	STACK_IS_IRQ,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-static enum stack_type</span>
<span class="p_del">-analyze_stack(int cpu, struct task_struct *task, unsigned long *stack,</span>
<span class="p_del">-	      unsigned long **stack_end, unsigned long *irq_stack,</span>
<span class="p_del">-	      unsigned *used, char **id)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long addr;</span>
<span class="p_add">+	unsigned long *begin, *end;</span>
<span class="p_add">+	struct pt_regs *regs;</span>
<span class="p_add">+	unsigned k;</span>
 
<span class="p_del">-	addr = ((unsigned long)stack &amp; (~(THREAD_SIZE - 1)));</span>
<span class="p_del">-	if ((unsigned long)task_stack_page(task) == addr)</span>
<span class="p_del">-		return STACK_IS_NORMAL;</span>
<span class="p_add">+	BUILD_BUG_ON(N_EXCEPTION_STACKS != 4);</span>
 
<span class="p_del">-	*stack_end = in_exception_stack(cpu, (unsigned long)stack,</span>
<span class="p_del">-					used, id);</span>
<span class="p_del">-	if (*stack_end)</span>
<span class="p_del">-		return STACK_IS_EXCEPTION;</span>
<span class="p_add">+	for (k = 0; k &lt; N_EXCEPTION_STACKS; k++) {</span>
<span class="p_add">+		end   = (unsigned long *)raw_cpu_ptr(&amp;orig_ist)-&gt;ist[k];</span>
<span class="p_add">+		begin = end - (exception_stack_sizes[k] / sizeof(long));</span>
<span class="p_add">+		regs  = (struct pt_regs *)end - 1;</span>
 
<span class="p_del">-	if (!irq_stack)</span>
<span class="p_del">-		return STACK_IS_NORMAL;</span>
<span class="p_add">+		if (stack &lt; begin || stack &gt;= end)</span>
<span class="p_add">+			continue;</span>
 
<span class="p_del">-	*stack_end = irq_stack;</span>
<span class="p_del">-	irq_stack = irq_stack - irq_stack_size;</span>
<span class="p_add">+		info-&gt;type	= STACK_TYPE_EXCEPTION + k;</span>
<span class="p_add">+		info-&gt;begin	= begin;</span>
<span class="p_add">+		info-&gt;end	= end;</span>
<span class="p_add">+		info-&gt;next_sp	= (unsigned long *)regs-&gt;sp;</span>
 
<span class="p_del">-	if (in_irq_stack(stack, irq_stack, *stack_end))</span>
<span class="p_del">-		return STACK_IS_IRQ;</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	return STACK_IS_UNKNOWN;</span>
<span class="p_add">+	return false;</span>
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * x86-64 can have up to three kernel stacks:</span>
<span class="p_del">- * process stack</span>
<span class="p_del">- * interrupt stack</span>
<span class="p_del">- * severe exception (double fault, nmi, stack fault, debug, mce) hardware stack</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-void dump_trace(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_del">-		unsigned long *stack, unsigned long bp,</span>
<span class="p_del">-		const struct stacktrace_ops *ops, void *data)</span>
<span class="p_add">+static bool in_irq_stack(unsigned long *stack, struct stack_info *info)</span>
 {
<span class="p_del">-	const unsigned cpu = get_cpu();</span>
<span class="p_del">-	unsigned long *irq_stack = (unsigned long *)per_cpu(irq_stack_ptr, cpu);</span>
<span class="p_del">-	unsigned long dummy;</span>
<span class="p_del">-	unsigned used = 0;</span>
<span class="p_del">-	int graph = 0;</span>
<span class="p_del">-	int done = 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!task)</span>
<span class="p_del">-		task = current;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!stack) {</span>
<span class="p_del">-		if (regs)</span>
<span class="p_del">-			stack = (unsigned long *)regs-&gt;sp;</span>
<span class="p_del">-		else if (task != current)</span>
<span class="p_del">-			stack = (unsigned long *)task-&gt;thread.sp;</span>
<span class="p_del">-		else</span>
<span class="p_del">-			stack = &amp;dummy;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	unsigned long *end   = (unsigned long *)this_cpu_read(irq_stack_ptr);</span>
<span class="p_add">+	unsigned long *begin = end - (IRQ_STACK_SIZE / sizeof(long));</span>
 
<span class="p_del">-	if (!bp)</span>
<span class="p_del">-		bp = stack_frame(task, regs);</span>
 	/*
<span class="p_del">-	 * Print function call entries in all stacks, starting at the</span>
<span class="p_del">-	 * current stack address. If the stacks consist of nested</span>
<span class="p_del">-	 * exceptions</span>
<span class="p_add">+	 * This is a software stack, so &#39;end&#39; can be a valid stack pointer.</span>
<span class="p_add">+	 * It just means the stack is empty.</span>
 	 */
<span class="p_del">-	while (!done) {</span>
<span class="p_del">-		unsigned long *stack_end;</span>
<span class="p_del">-		enum stack_type stype;</span>
<span class="p_del">-		char *id;</span>
<span class="p_add">+	if (stack &lt; begin || stack &gt; end)</span>
<span class="p_add">+		return false;</span>
 
<span class="p_del">-		stype = analyze_stack(cpu, task, stack, &amp;stack_end,</span>
<span class="p_del">-				      irq_stack, &amp;used, &amp;id);</span>
<span class="p_add">+	info-&gt;type	= STACK_TYPE_IRQ;</span>
<span class="p_add">+	info-&gt;begin	= begin;</span>
<span class="p_add">+	info-&gt;end	= end;</span>
 
<span class="p_del">-		/* Default finish unless specified to continue */</span>
<span class="p_del">-		done = 1;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The next stack pointer is the first thing pushed by the entry code</span>
<span class="p_add">+	 * after switching to the irq stack.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	info-&gt;next_sp = (unsigned long *)*(end - 1);</span>
 
<span class="p_del">-		switch (stype) {</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
 
<span class="p_del">-		/* Break out early if we are on the thread stack */</span>
<span class="p_del">-		case STACK_IS_NORMAL:</span>
<span class="p_del">-			break;</span>
<span class="p_add">+int get_stack_info(unsigned long *stack, struct task_struct *task,</span>
<span class="p_add">+		   struct stack_info *info, unsigned long *visit_mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!stack)</span>
<span class="p_add">+		goto unknown;</span>
 
<span class="p_del">-		case STACK_IS_EXCEPTION:</span>
<span class="p_add">+	task = task ? : current;</span>
 
<span class="p_del">-			if (ops-&gt;stack(data, id) &lt; 0)</span>
<span class="p_del">-				break;</span>
<span class="p_add">+	if (in_task_stack(stack, task, info))</span>
<span class="p_add">+		goto recursion_check;</span>
 
<span class="p_del">-			bp = ops-&gt;walk_stack(task, stack, bp, ops,</span>
<span class="p_del">-					     data, stack_end, &amp;graph);</span>
<span class="p_del">-			ops-&gt;stack(data, &quot;&lt;EOE&gt;&quot;);</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * We link to the next stack via the</span>
<span class="p_del">-			 * second-to-last pointer (index -2 to end) in the</span>
<span class="p_del">-			 * exception stack:</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			stack = (unsigned long *) stack_end[-2];</span>
<span class="p_del">-			done = 0;</span>
<span class="p_del">-			break;</span>
<span class="p_add">+	if (task != current)</span>
<span class="p_add">+		goto unknown;</span>
 
<span class="p_del">-		case STACK_IS_IRQ:</span>
<span class="p_add">+	if (in_exception_stack(stack, info))</span>
<span class="p_add">+		goto recursion_check;</span>
 
<span class="p_del">-			if (ops-&gt;stack(data, &quot;IRQ&quot;) &lt; 0)</span>
<span class="p_del">-				break;</span>
<span class="p_del">-			bp = ops-&gt;walk_stack(task, stack, bp,</span>
<span class="p_del">-				     ops, data, stack_end, &amp;graph);</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * We link to the next stack (which would be</span>
<span class="p_del">-			 * the process stack normally) the last</span>
<span class="p_del">-			 * pointer (index -1 to end) in the IRQ stack:</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			stack = (unsigned long *) (stack_end[-1]);</span>
<span class="p_del">-			irq_stack = NULL;</span>
<span class="p_del">-			ops-&gt;stack(data, &quot;EOI&quot;);</span>
<span class="p_del">-			done = 0;</span>
<span class="p_del">-			break;</span>
<span class="p_add">+	if (in_irq_stack(stack, info))</span>
<span class="p_add">+		goto recursion_check;</span>
 
<span class="p_del">-		case STACK_IS_UNKNOWN:</span>
<span class="p_del">-			ops-&gt;stack(data, &quot;UNK&quot;);</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_add">+	goto unknown;</span>
 
<span class="p_add">+recursion_check:</span>
 	/*
<span class="p_del">-	 * This handles the process stack:</span>
<span class="p_add">+	 * Make sure we don&#39;t iterate through any given stack more than once.</span>
<span class="p_add">+	 * If it comes up a second time then there&#39;s something wrong going on:</span>
<span class="p_add">+	 * just break out and report an unknown stack type.</span>
 	 */
<span class="p_del">-	bp = ops-&gt;walk_stack(task, stack, bp, ops, data, NULL, &amp;graph);</span>
<span class="p_del">-	put_cpu();</span>
<span class="p_add">+	if (visit_mask) {</span>
<span class="p_add">+		if (*visit_mask &amp; (1UL &lt;&lt; info-&gt;type))</span>
<span class="p_add">+			goto unknown;</span>
<span class="p_add">+		*visit_mask |= 1UL &lt;&lt; info-&gt;type;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+unknown:</span>
<span class="p_add">+	info-&gt;type = STACK_TYPE_UNKNOWN;</span>
<span class="p_add">+	return -EINVAL;</span>
 }
<span class="p_del">-EXPORT_SYMBOL(dump_trace);</span>
 
<span class="p_del">-void</span>
<span class="p_del">-show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_del">-		   unsigned long *sp, unsigned long bp, char *log_lvl)</span>
<span class="p_add">+void show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_add">+			unsigned long *sp, char *log_lvl)</span>
 {
 	unsigned long *irq_stack_end;
 	unsigned long *irq_stack;
 	unsigned long *stack;
<span class="p_del">-	int cpu;</span>
 	int i;
 
<span class="p_del">-	preempt_disable();</span>
<span class="p_del">-	cpu = smp_processor_id();</span>
<span class="p_add">+	if (!try_get_task_stack(task))</span>
<span class="p_add">+		return;</span>
 
<span class="p_del">-	irq_stack_end	= (unsigned long *)(per_cpu(irq_stack_ptr, cpu));</span>
<span class="p_del">-	irq_stack	= (unsigned long *)(per_cpu(irq_stack_ptr, cpu) - IRQ_STACK_SIZE);</span>
<span class="p_add">+	irq_stack_end = (unsigned long *)this_cpu_read(irq_stack_ptr);</span>
<span class="p_add">+	irq_stack     = irq_stack_end - (IRQ_STACK_SIZE / sizeof(long));</span>
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Debugging aid: &quot;show_stack(NULL, NULL);&quot; prints the</span>
<span class="p_del">-	 * back trace for this cpu:</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (sp == NULL) {</span>
<span class="p_del">-		if (regs)</span>
<span class="p_del">-			sp = (unsigned long *)regs-&gt;sp;</span>
<span class="p_del">-		else if (task)</span>
<span class="p_del">-			sp = (unsigned long *)task-&gt;thread.sp;</span>
<span class="p_del">-		else</span>
<span class="p_del">-			sp = (unsigned long *)&amp;sp;</span>
<span class="p_del">-	}</span>
<span class="p_add">+	sp = sp ? : get_stack_pointer(task, regs);</span>
 
 	stack = sp;
 	for (i = 0; i &lt; kstack_depth_to_print; i++) {
<span class="p_chunk">@@ -299,18 +183,17 @@</span> <span class="p_context"> show_stack_log_lvl(struct task_struct *task, struct pt_regs *regs,</span>
 		stack++;
 		touch_nmi_watchdog();
 	}
<span class="p_del">-	preempt_enable();</span>
 
 	pr_cont(&quot;\n&quot;);
<span class="p_del">-	show_trace_log_lvl(task, regs, sp, bp, log_lvl);</span>
<span class="p_add">+	show_trace_log_lvl(task, regs, sp, log_lvl);</span>
<span class="p_add">+</span>
<span class="p_add">+	put_task_stack(task);</span>
 }
 
 void show_regs(struct pt_regs *regs)
 {
 	int i;
<span class="p_del">-	unsigned long sp;</span>
 
<span class="p_del">-	sp = regs-&gt;sp;</span>
 	show_regs_print_info(KERN_DEFAULT);
 	__show_regs(regs, 1);
 
<span class="p_chunk">@@ -325,8 +208,7 @@</span> <span class="p_context"> void show_regs(struct pt_regs *regs)</span>
 		u8 *ip;
 
 		printk(KERN_DEFAULT &quot;Stack:\n&quot;);
<span class="p_del">-		show_stack_log_lvl(NULL, regs, (unsigned long *)sp,</span>
<span class="p_del">-				   0, KERN_DEFAULT);</span>
<span class="p_add">+		show_stack_log_lvl(current, regs, NULL, KERN_DEFAULT);</span>
 
 		printk(KERN_DEFAULT &quot;Code: &quot;);
 
<span class="p_header">diff --git a/arch/x86/kernel/fpu/init.c b/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">index 93982aebb398..2f2b8c7ccb85 100644</span>
<span class="p_header">--- a/arch/x86/kernel/fpu/init.c</span>
<span class="p_header">+++ b/arch/x86/kernel/fpu/init.c</span>
<span class="p_chunk">@@ -317,7 +317,6 @@</span> <span class="p_context"> static void __init fpu__init_system_ctx_switch(void)</span>
 	on_boot_cpu = 0;
 
 	WARN_ON_FPU(current-&gt;thread.fpu.fpstate_active);
<span class="p_del">-	current_thread_info()-&gt;status = 0;</span>
 
 	if (boot_cpu_has(X86_FEATURE_XSAVEOPT) &amp;&amp; eagerfpu != DISABLE)
 		eagerfpu = ENABLE;
<span class="p_header">diff --git a/arch/x86/kernel/ftrace.c b/arch/x86/kernel/ftrace.c</span>
<span class="p_header">index d036cfb4495d..8639bb2ae058 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ftrace.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ftrace.c</span>
<span class="p_chunk">@@ -1029,7 +1029,7 @@</span> <span class="p_context"> void prepare_ftrace_return(unsigned long self_addr, unsigned long *parent,</span>
 	}
 
 	if (ftrace_push_return_trace(old, self_addr, &amp;trace.depth,
<span class="p_del">-		    frame_pointer) == -EBUSY) {</span>
<span class="p_add">+				     frame_pointer, parent) == -EBUSY) {</span>
 		*parent = old;
 		return;
 	}
<span class="p_header">diff --git a/arch/x86/kernel/head_32.S b/arch/x86/kernel/head_32.S</span>
<span class="p_header">index 6f8902b0d151..5f401262f12d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_32.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_32.S</span>
<span class="p_chunk">@@ -94,7 +94,7 @@</span> <span class="p_context"> RESERVE_BRK(pagetables, INIT_MAP_SIZE)</span>
  */
 __HEAD
 ENTRY(startup_32)
<span class="p_del">-	movl pa(stack_start),%ecx</span>
<span class="p_add">+	movl pa(initial_stack),%ecx</span>
 	
 	/* test KEEP_SEGMENTS flag to see if the bootloader is asking
 		us to not reload segments */
<span class="p_chunk">@@ -286,7 +286,7 @@</span> <span class="p_context"> num_subarch_entries = (. - subarch_entries) / 4</span>
  * start_secondary().
  */
 ENTRY(start_cpu0)
<span class="p_del">-	movl stack_start, %ecx</span>
<span class="p_add">+	movl initial_stack, %ecx</span>
 	movl %ecx, %esp
 	jmp  *(initial_code)
 ENDPROC(start_cpu0)
<span class="p_chunk">@@ -307,7 +307,7 @@</span> <span class="p_context"> ENTRY(startup_32_smp)</span>
 	movl %eax,%es
 	movl %eax,%fs
 	movl %eax,%gs
<span class="p_del">-	movl pa(stack_start),%ecx</span>
<span class="p_add">+	movl pa(initial_stack),%ecx</span>
 	movl %eax,%ss
 	leal -__PAGE_OFFSET(%ecx),%esp
 
<span class="p_chunk">@@ -703,7 +703,7 @@</span> <span class="p_context"> ENTRY(initial_page_table)</span>
 
 .data
 .balign 4
<span class="p_del">-ENTRY(stack_start)</span>
<span class="p_add">+ENTRY(initial_stack)</span>
 	.long init_thread_union+THREAD_SIZE
 
 __INITRODATA
<span class="p_header">diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S</span>
<span class="p_header">index 9f8efc9f0075..c98a559c346e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S</span>
<span class="p_chunk">@@ -66,7 +66,7 @@</span> <span class="p_context"> L3_START_KERNEL = pud_index(__START_KERNEL_map)</span>
 	 */
 
 	/*
<span class="p_del">-	 * Setup stack for verify_cpu(). &quot;-8&quot; because stack_start is defined</span>
<span class="p_add">+	 * Setup stack for verify_cpu(). &quot;-8&quot; because initial_stack is defined</span>
 	 * this way, see below. Our best guess is a NULL ptr for stack
 	 * termination heuristics and we don&#39;t want to break anything which
 	 * might depend on it (kgdb, ...).
<span class="p_chunk">@@ -226,7 +226,7 @@</span> <span class="p_context"> ENTRY(secondary_startup_64)</span>
 	movq	%rax, %cr0
 
 	/* Setup a boot time stack */
<span class="p_del">-	movq stack_start(%rip), %rsp</span>
<span class="p_add">+	movq initial_stack(%rip), %rsp</span>
 
 	/* zero EFLAGS after setting rsp */
 	pushq $0
<span class="p_chunk">@@ -310,7 +310,7 @@</span> <span class="p_context"> ENDPROC(secondary_startup_64)</span>
  * start_secondary().
  */
 ENTRY(start_cpu0)
<span class="p_del">-	movq stack_start(%rip),%rsp</span>
<span class="p_add">+	movq initial_stack(%rip),%rsp</span>
 	movq	initial_code(%rip),%rax
 	pushq	$0		# fake return address to stop unwinder
 	pushq	$__KERNEL_CS	# set correct cs
<span class="p_chunk">@@ -319,17 +319,15 @@</span> <span class="p_context"> ENTRY(start_cpu0)</span>
 ENDPROC(start_cpu0)
 #endif
 
<span class="p_del">-	/* SMP bootup changes these two */</span>
<span class="p_add">+	/* Both SMP bootup and ACPI suspend change these variables */</span>
 	__REFDATA
 	.balign	8
 	GLOBAL(initial_code)
 	.quad	x86_64_start_kernel
 	GLOBAL(initial_gs)
 	.quad	INIT_PER_CPU_VAR(irq_stack_union)
<span class="p_del">-</span>
<span class="p_del">-	GLOBAL(stack_start)</span>
<span class="p_add">+	GLOBAL(initial_stack)</span>
 	.quad  init_thread_union+THREAD_SIZE-8
<span class="p_del">-	.word  0</span>
 	__FINITDATA
 
 bad_address:
<span class="p_header">diff --git a/arch/x86/kernel/irq_64.c b/arch/x86/kernel/irq_64.c</span>
<span class="p_header">index 4a7903714065..9ebd0b0e73d9 100644</span>
<span class="p_header">--- a/arch/x86/kernel/irq_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/irq_64.c</span>
<span class="p_chunk">@@ -40,8 +40,7 @@</span> <span class="p_context"> static inline void stack_overflow_check(struct pt_regs *regs)</span>
 	if (user_mode(regs))
 		return;
 
<span class="p_del">-	if (regs-&gt;sp &gt;= curbase + sizeof(struct thread_info) +</span>
<span class="p_del">-				  sizeof(struct pt_regs) + STACK_TOP_MARGIN &amp;&amp;</span>
<span class="p_add">+	if (regs-&gt;sp &gt;= curbase + sizeof(struct pt_regs) + STACK_TOP_MARGIN &amp;&amp;</span>
 	    regs-&gt;sp &lt;= curbase + THREAD_SIZE)
 		return;
 
<span class="p_header">diff --git a/arch/x86/kernel/kgdb.c b/arch/x86/kernel/kgdb.c</span>
<span class="p_header">index 04cde527d728..8e36f249646e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kgdb.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kgdb.c</span>
<span class="p_chunk">@@ -50,6 +50,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/apicdef.h&gt;
 #include &lt;asm/apic.h&gt;
 #include &lt;asm/nmi.h&gt;
<span class="p_add">+#include &lt;asm/switch_to.h&gt;</span>
 
 struct dbg_reg_def_t dbg_reg_def[DBG_MAX_REG_NUM] =
 {
<span class="p_chunk">@@ -166,21 +167,19 @@</span> <span class="p_context"> void sleeping_thread_to_gdb_regs(unsigned long *gdb_regs, struct task_struct *p)</span>
 	gdb_regs[GDB_DX]	= 0;
 	gdb_regs[GDB_SI]	= 0;
 	gdb_regs[GDB_DI]	= 0;
<span class="p_del">-	gdb_regs[GDB_BP]	= *(unsigned long *)p-&gt;thread.sp;</span>
<span class="p_add">+	gdb_regs[GDB_BP]	= ((struct inactive_task_frame *)p-&gt;thread.sp)-&gt;bp;</span>
 #ifdef CONFIG_X86_32
 	gdb_regs[GDB_DS]	= __KERNEL_DS;
 	gdb_regs[GDB_ES]	= __KERNEL_DS;
 	gdb_regs[GDB_PS]	= 0;
 	gdb_regs[GDB_CS]	= __KERNEL_CS;
<span class="p_del">-	gdb_regs[GDB_PC]	= p-&gt;thread.ip;</span>
 	gdb_regs[GDB_SS]	= __KERNEL_DS;
 	gdb_regs[GDB_FS]	= 0xFFFF;
 	gdb_regs[GDB_GS]	= 0xFFFF;
 #else
<span class="p_del">-	gdb_regs32[GDB_PS]	= *(unsigned long *)(p-&gt;thread.sp + 8);</span>
<span class="p_add">+	gdb_regs32[GDB_PS]	= 0;</span>
 	gdb_regs32[GDB_CS]	= __KERNEL_CS;
 	gdb_regs32[GDB_SS]	= __KERNEL_DS;
<span class="p_del">-	gdb_regs[GDB_PC]	= 0;</span>
 	gdb_regs[GDB_R8]	= 0;
 	gdb_regs[GDB_R9]	= 0;
 	gdb_regs[GDB_R10]	= 0;
<span class="p_chunk">@@ -190,6 +189,7 @@</span> <span class="p_context"> void sleeping_thread_to_gdb_regs(unsigned long *gdb_regs, struct task_struct *p)</span>
 	gdb_regs[GDB_R14]	= 0;
 	gdb_regs[GDB_R15]	= 0;
 #endif
<span class="p_add">+	gdb_regs[GDB_PC]	= 0;</span>
 	gdb_regs[GDB_SP]	= p-&gt;thread.sp;
 }
 
<span class="p_header">diff --git a/arch/x86/kernel/ksysfs.c b/arch/x86/kernel/ksysfs.c</span>
<span class="p_header">index c2bedaea11f7..4afc67f5facc 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ksysfs.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ksysfs.c</span>
<span class="p_chunk">@@ -184,7 +184,7 @@</span> <span class="p_context"> static ssize_t setup_data_data_read(struct file *fp,</span>
 
 static struct kobj_attribute type_attr = __ATTR_RO(type);
 
<span class="p_del">-static struct bin_attribute data_attr = {</span>
<span class="p_add">+static struct bin_attribute data_attr __ro_after_init = {</span>
 	.attr = {
 		.name = &quot;data&quot;,
 		.mode = S_IRUGO,
<span class="p_header">diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c</span>
<span class="p_header">index 3692249a70f1..60b9949f1e65 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvmclock.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvmclock.c</span>
<span class="p_chunk">@@ -29,7 +29,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/x86_init.h&gt;
 #include &lt;asm/reboot.h&gt;
 
<span class="p_del">-static int kvmclock = 1;</span>
<span class="p_add">+static int kvmclock __ro_after_init = 1;</span>
 static int msr_kvm_system_time = MSR_KVM_SYSTEM_TIME;
 static int msr_kvm_wall_clock = MSR_KVM_WALL_CLOCK;
 static cycle_t kvm_sched_clock_offset;
<span class="p_header">diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c</span>
<span class="p_header">index 1acfd76e3e26..bbf3d5933eaa 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt.c</span>
<span class="p_chunk">@@ -332,7 +332,6 @@</span> <span class="p_context"> __visible struct pv_cpu_ops pv_cpu_ops = {</span>
 	.read_cr0 = native_read_cr0,
 	.write_cr0 = native_write_cr0,
 	.read_cr4 = native_read_cr4,
<span class="p_del">-	.read_cr4_safe = native_read_cr4_safe,</span>
 	.write_cr4 = native_write_cr4,
 #ifdef CONFIG_X86_64
 	.read_cr8 = native_read_cr8,
<span class="p_chunk">@@ -389,7 +388,7 @@</span> <span class="p_context"> NOKPROBE_SYMBOL(native_load_idt);</span>
 #define PTE_IDENT	__PV_IS_CALLEE_SAVE(_paravirt_ident_64)
 #endif
 
<span class="p_del">-struct pv_mmu_ops pv_mmu_ops = {</span>
<span class="p_add">+struct pv_mmu_ops pv_mmu_ops __ro_after_init = {</span>
 
 	.read_cr2 = native_read_cr2,
 	.write_cr2 = native_write_cr2,
<span class="p_header">diff --git a/arch/x86/kernel/process.c b/arch/x86/kernel/process.c</span>
<span class="p_header">index 62c0b0ea2ce4..4002b475171c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process.c</span>
<span class="p_chunk">@@ -32,6 +32,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/tlbflush.h&gt;
 #include &lt;asm/mce.h&gt;
 #include &lt;asm/vm86.h&gt;
<span class="p_add">+#include &lt;asm/switch_to.h&gt;</span>
 
 /*
  * per-CPU TSS segments. Threads are completely &#39;soft&#39; on Linux,
<span class="p_chunk">@@ -513,6 +514,17 @@</span> <span class="p_context"> unsigned long arch_randomize_brk(struct mm_struct *mm)</span>
 }
 
 /*
<span class="p_add">+ * Return saved PC of a blocked thread.</span>
<span class="p_add">+ * What is this good for? it will be always the scheduler or ret_from_fork.</span>
<span class="p_add">+ */</span>
<span class="p_add">+unsigned long thread_saved_pc(struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct inactive_task_frame *frame =</span>
<span class="p_add">+		(struct inactive_task_frame *) READ_ONCE(tsk-&gt;thread.sp);</span>
<span class="p_add">+	return READ_ONCE_NOCHECK(frame-&gt;ret_addr);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Called from fs/proc with a reference on @p to find the function
  * which called into schedule(). This needs to be done carefully
  * because the task might wake up and we might look at a stack
<span class="p_chunk">@@ -520,15 +532,18 @@</span> <span class="p_context"> unsigned long arch_randomize_brk(struct mm_struct *mm)</span>
  */
 unsigned long get_wchan(struct task_struct *p)
 {
<span class="p_del">-	unsigned long start, bottom, top, sp, fp, ip;</span>
<span class="p_add">+	unsigned long start, bottom, top, sp, fp, ip, ret = 0;</span>
 	int count = 0;
 
 	if (!p || p == current || p-&gt;state == TASK_RUNNING)
 		return 0;
 
<span class="p_add">+	if (!try_get_task_stack(p))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	start = (unsigned long)task_stack_page(p);
 	if (!start)
<span class="p_del">-		return 0;</span>
<span class="p_add">+		goto out;</span>
 
 	/*
 	 * Layout of the stack page:
<span class="p_chunk">@@ -537,9 +552,7 @@</span> <span class="p_context"> unsigned long get_wchan(struct task_struct *p)</span>
 	 * PADDING
 	 * ----------- top = topmax - TOP_OF_KERNEL_STACK_PADDING
 	 * stack
<span class="p_del">-	 * ----------- bottom = start + sizeof(thread_info)</span>
<span class="p_del">-	 * thread_info</span>
<span class="p_del">-	 * ----------- start</span>
<span class="p_add">+	 * ----------- bottom = start</span>
 	 *
 	 * The tasks stack pointer points at the location where the
 	 * framepointer is stored. The data on the stack is:
<span class="p_chunk">@@ -550,20 +563,25 @@</span> <span class="p_context"> unsigned long get_wchan(struct task_struct *p)</span>
 	 */
 	top = start + THREAD_SIZE - TOP_OF_KERNEL_STACK_PADDING;
 	top -= 2 * sizeof(unsigned long);
<span class="p_del">-	bottom = start + sizeof(struct thread_info);</span>
<span class="p_add">+	bottom = start;</span>
 
 	sp = READ_ONCE(p-&gt;thread.sp);
 	if (sp &lt; bottom || sp &gt; top)
<span class="p_del">-		return 0;</span>
<span class="p_add">+		goto out;</span>
 
<span class="p_del">-	fp = READ_ONCE_NOCHECK(*(unsigned long *)sp);</span>
<span class="p_add">+	fp = READ_ONCE_NOCHECK(((struct inactive_task_frame *)sp)-&gt;bp);</span>
 	do {
 		if (fp &lt; bottom || fp &gt; top)
<span class="p_del">-			return 0;</span>
<span class="p_add">+			goto out;</span>
 		ip = READ_ONCE_NOCHECK(*(unsigned long *)(fp + sizeof(unsigned long)));
<span class="p_del">-		if (!in_sched_functions(ip))</span>
<span class="p_del">-			return ip;</span>
<span class="p_add">+		if (!in_sched_functions(ip)) {</span>
<span class="p_add">+			ret = ip;</span>
<span class="p_add">+			goto out;</span>
<span class="p_add">+		}</span>
 		fp = READ_ONCE_NOCHECK(*(unsigned long *)fp);
 	} while (count++ &lt; 16 &amp;&amp; p-&gt;state != TASK_RUNNING);
<span class="p_del">-	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	put_task_stack(p);</span>
<span class="p_add">+	return ret;</span>
 }
<span class="p_header">diff --git a/arch/x86/kernel/process_32.c b/arch/x86/kernel/process_32.c</span>
<span class="p_header">index d86be29c38c7..bd7be8efdc4c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_32.c</span>
<span class="p_chunk">@@ -55,17 +55,6 @@</span> <span class="p_context"></span>
 #include &lt;asm/switch_to.h&gt;
 #include &lt;asm/vm86.h&gt;
 
<span class="p_del">-asmlinkage void ret_from_fork(void) __asm__(&quot;ret_from_fork&quot;);</span>
<span class="p_del">-asmlinkage void ret_from_kernel_thread(void) __asm__(&quot;ret_from_kernel_thread&quot;);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Return saved PC of a blocked thread.</span>
<span class="p_del">- */</span>
<span class="p_del">-unsigned long thread_saved_pc(struct task_struct *tsk)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return ((unsigned long *)tsk-&gt;thread.sp)[3];</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 void __show_regs(struct pt_regs *regs, int all)
 {
 	unsigned long cr0 = 0L, cr2 = 0L, cr3 = 0L, cr4 = 0L;
<span class="p_chunk">@@ -101,7 +90,7 @@</span> <span class="p_context"> void __show_regs(struct pt_regs *regs, int all)</span>
 	cr0 = read_cr0();
 	cr2 = read_cr2();
 	cr3 = read_cr3();
<span class="p_del">-	cr4 = __read_cr4_safe();</span>
<span class="p_add">+	cr4 = __read_cr4();</span>
 	printk(KERN_DEFAULT &quot;CR0: %08lx CR2: %08lx CR3: %08lx CR4: %08lx\n&quot;,
 			cr0, cr2, cr3, cr4);
 
<span class="p_chunk">@@ -133,35 +122,31 @@</span> <span class="p_context"> int copy_thread_tls(unsigned long clone_flags, unsigned long sp,</span>
 	unsigned long arg, struct task_struct *p, unsigned long tls)
 {
 	struct pt_regs *childregs = task_pt_regs(p);
<span class="p_add">+	struct fork_frame *fork_frame = container_of(childregs, struct fork_frame, regs);</span>
<span class="p_add">+	struct inactive_task_frame *frame = &amp;fork_frame-&gt;frame;</span>
 	struct task_struct *tsk;
 	int err;
 
<span class="p_del">-	p-&gt;thread.sp = (unsigned long) childregs;</span>
<span class="p_add">+	frame-&gt;bp = 0;</span>
<span class="p_add">+	frame-&gt;ret_addr = (unsigned long) ret_from_fork;</span>
<span class="p_add">+	p-&gt;thread.sp = (unsigned long) fork_frame;</span>
 	p-&gt;thread.sp0 = (unsigned long) (childregs+1);
 	memset(p-&gt;thread.ptrace_bps, 0, sizeof(p-&gt;thread.ptrace_bps));
 
 	if (unlikely(p-&gt;flags &amp; PF_KTHREAD)) {
 		/* kernel thread */
 		memset(childregs, 0, sizeof(struct pt_regs));
<span class="p_del">-		p-&gt;thread.ip = (unsigned long) ret_from_kernel_thread;</span>
<span class="p_del">-		task_user_gs(p) = __KERNEL_STACK_CANARY;</span>
<span class="p_del">-		childregs-&gt;ds = __USER_DS;</span>
<span class="p_del">-		childregs-&gt;es = __USER_DS;</span>
<span class="p_del">-		childregs-&gt;fs = __KERNEL_PERCPU;</span>
<span class="p_del">-		childregs-&gt;bx = sp;	/* function */</span>
<span class="p_del">-		childregs-&gt;bp = arg;</span>
<span class="p_del">-		childregs-&gt;orig_ax = -1;</span>
<span class="p_del">-		childregs-&gt;cs = __KERNEL_CS | get_kernel_rpl();</span>
<span class="p_del">-		childregs-&gt;flags = X86_EFLAGS_IF | X86_EFLAGS_FIXED;</span>
<span class="p_add">+		frame-&gt;bx = sp;		/* function */</span>
<span class="p_add">+		frame-&gt;di = arg;</span>
 		p-&gt;thread.io_bitmap_ptr = NULL;
 		return 0;
 	}
<span class="p_add">+	frame-&gt;bx = 0;</span>
 	*childregs = *current_pt_regs();
 	childregs-&gt;ax = 0;
 	if (sp)
 		childregs-&gt;sp = sp;
 
<span class="p_del">-	p-&gt;thread.ip = (unsigned long) ret_from_fork;</span>
 	task_user_gs(p) = get_user_gs(current_pt_regs());
 
 	p-&gt;thread.io_bitmap_ptr = NULL;
<span class="p_header">diff --git a/arch/x86/kernel/process_64.c b/arch/x86/kernel/process_64.c</span>
<span class="p_header">index 63236d8f84bf..de9acaf2d371 100644</span>
<span class="p_header">--- a/arch/x86/kernel/process_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/process_64.c</span>
<span class="p_chunk">@@ -50,8 +50,6 @@</span> <span class="p_context"></span>
 #include &lt;asm/switch_to.h&gt;
 #include &lt;asm/xen/hypervisor.h&gt;
 
<span class="p_del">-asmlinkage extern void ret_from_fork(void);</span>
<span class="p_del">-</span>
 __visible DEFINE_PER_CPU(unsigned long, rsp_scratch);
 
 /* Prints also some state that isn&#39;t saved in the pt_regs */
<span class="p_chunk">@@ -141,12 +139,17 @@</span> <span class="p_context"> int copy_thread_tls(unsigned long clone_flags, unsigned long sp,</span>
 {
 	int err;
 	struct pt_regs *childregs;
<span class="p_add">+	struct fork_frame *fork_frame;</span>
<span class="p_add">+	struct inactive_task_frame *frame;</span>
 	struct task_struct *me = current;
 
 	p-&gt;thread.sp0 = (unsigned long)task_stack_page(p) + THREAD_SIZE;
 	childregs = task_pt_regs(p);
<span class="p_del">-	p-&gt;thread.sp = (unsigned long) childregs;</span>
<span class="p_del">-	set_tsk_thread_flag(p, TIF_FORK);</span>
<span class="p_add">+	fork_frame = container_of(childregs, struct fork_frame, regs);</span>
<span class="p_add">+	frame = &amp;fork_frame-&gt;frame;</span>
<span class="p_add">+	frame-&gt;bp = 0;</span>
<span class="p_add">+	frame-&gt;ret_addr = (unsigned long) ret_from_fork;</span>
<span class="p_add">+	p-&gt;thread.sp = (unsigned long) fork_frame;</span>
 	p-&gt;thread.io_bitmap_ptr = NULL;
 
 	savesegment(gs, p-&gt;thread.gsindex);
<span class="p_chunk">@@ -160,15 +163,11 @@</span> <span class="p_context"> int copy_thread_tls(unsigned long clone_flags, unsigned long sp,</span>
 	if (unlikely(p-&gt;flags &amp; PF_KTHREAD)) {
 		/* kernel thread */
 		memset(childregs, 0, sizeof(struct pt_regs));
<span class="p_del">-		childregs-&gt;sp = (unsigned long)childregs;</span>
<span class="p_del">-		childregs-&gt;ss = __KERNEL_DS;</span>
<span class="p_del">-		childregs-&gt;bx = sp; /* function */</span>
<span class="p_del">-		childregs-&gt;bp = arg;</span>
<span class="p_del">-		childregs-&gt;orig_ax = -1;</span>
<span class="p_del">-		childregs-&gt;cs = __KERNEL_CS | get_kernel_rpl();</span>
<span class="p_del">-		childregs-&gt;flags = X86_EFLAGS_IF | X86_EFLAGS_FIXED;</span>
<span class="p_add">+		frame-&gt;bx = sp;		/* function */</span>
<span class="p_add">+		frame-&gt;r12 = arg;</span>
 		return 0;
 	}
<span class="p_add">+	frame-&gt;bx = 0;</span>
 	*childregs = *current_pt_regs();
 
 	childregs-&gt;ax = 0;
<span class="p_chunk">@@ -511,7 +510,7 @@</span> <span class="p_context"> void set_personality_ia32(bool x32)</span>
 		current-&gt;personality &amp;= ~READ_IMPLIES_EXEC;
 		/* in_compat_syscall() uses the presence of the x32
 		   syscall bit flag to determine compat status */
<span class="p_del">-		current_thread_info()-&gt;status &amp;= ~TS_COMPAT;</span>
<span class="p_add">+		current-&gt;thread.status &amp;= ~TS_COMPAT;</span>
 	} else {
 		set_thread_flag(TIF_IA32);
 		clear_thread_flag(TIF_X32);
<span class="p_chunk">@@ -519,7 +518,7 @@</span> <span class="p_context"> void set_personality_ia32(bool x32)</span>
 			current-&gt;mm-&gt;context.ia32_compat = TIF_IA32;
 		current-&gt;personality |= force_personality32;
 		/* Prepare the first &quot;return&quot; to user space */
<span class="p_del">-		current_thread_info()-&gt;status |= TS_COMPAT;</span>
<span class="p_add">+		current-&gt;thread.status |= TS_COMPAT;</span>
 	}
 }
 EXPORT_SYMBOL_GPL(set_personality_ia32);
<span class="p_header">diff --git a/arch/x86/kernel/ptrace.c b/arch/x86/kernel/ptrace.c</span>
<span class="p_header">index f79576a541ff..ce94c38cf4d6 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ptrace.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ptrace.c</span>
<span class="p_chunk">@@ -173,8 +173,8 @@</span> <span class="p_context"> unsigned long kernel_stack_pointer(struct pt_regs *regs)</span>
 		return sp;
 
 	prev_esp = (u32 *)(context);
<span class="p_del">-	if (prev_esp)</span>
<span class="p_del">-		return (unsigned long)prev_esp;</span>
<span class="p_add">+	if (*prev_esp)</span>
<span class="p_add">+		return (unsigned long)*prev_esp;</span>
 
 	return (unsigned long)regs;
 }
<span class="p_chunk">@@ -934,7 +934,7 @@</span> <span class="p_context"> static int putreg32(struct task_struct *child, unsigned regno, u32 value)</span>
 		 */
 		regs-&gt;orig_ax = value;
 		if (syscall_get_nr(child, regs) &gt;= 0)
<span class="p_del">-			task_thread_info(child)-&gt;status |= TS_I386_REGS_POKED;</span>
<span class="p_add">+			child-&gt;thread.status |= TS_I386_REGS_POKED;</span>
 		break;
 
 	case offsetof(struct user32, regs.eflags):
<span class="p_chunk">@@ -1250,7 +1250,7 @@</span> <span class="p_context"> long compat_arch_ptrace(struct task_struct *child, compat_long_t request,</span>
 
 #ifdef CONFIG_X86_64
 
<span class="p_del">-static struct user_regset x86_64_regsets[] __read_mostly = {</span>
<span class="p_add">+static struct user_regset x86_64_regsets[] __ro_after_init = {</span>
 	[REGSET_GENERAL] = {
 		.core_note_type = NT_PRSTATUS,
 		.n = sizeof(struct user_regs_struct) / sizeof(long),
<span class="p_chunk">@@ -1291,7 +1291,7 @@</span> <span class="p_context"> static const struct user_regset_view user_x86_64_view = {</span>
 #endif	/* CONFIG_X86_64 */
 
 #if defined CONFIG_X86_32 || defined CONFIG_IA32_EMULATION
<span class="p_del">-static struct user_regset x86_32_regsets[] __read_mostly = {</span>
<span class="p_add">+static struct user_regset x86_32_regsets[] __ro_after_init = {</span>
 	[REGSET_GENERAL] = {
 		.core_note_type = NT_PRSTATUS,
 		.n = sizeof(struct user_regs_struct32) / sizeof(u32),
<span class="p_chunk">@@ -1344,7 +1344,7 @@</span> <span class="p_context"> static const struct user_regset_view user_x86_32_view = {</span>
  */
 u64 xstate_fx_sw_bytes[USER_XSTATE_FX_SW_WORDS];
 
<span class="p_del">-void update_regset_xstate_info(unsigned int size, u64 xstate_mask)</span>
<span class="p_add">+void __init update_regset_xstate_info(unsigned int size, u64 xstate_mask)</span>
 {
 #ifdef CONFIG_X86_64
 	x86_64_regsets[REGSET_XSTATE].n = size / sizeof(u64);
<span class="p_header">diff --git a/arch/x86/kernel/reboot.c b/arch/x86/kernel/reboot.c</span>
<span class="p_header">index 63bf27d972b7..e244c19a2451 100644</span>
<span class="p_header">--- a/arch/x86/kernel/reboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/reboot.c</span>
<span class="p_chunk">@@ -705,7 +705,7 @@</span> <span class="p_context"> static void native_machine_power_off(void)</span>
 	tboot_shutdown(TB_SHUTDOWN_HALT);
 }
 
<span class="p_del">-struct machine_ops machine_ops = {</span>
<span class="p_add">+struct machine_ops machine_ops __ro_after_init = {</span>
 	.power_off = native_machine_power_off,
 	.shutdown = native_machine_shutdown,
 	.emergency_restart = native_machine_emergency_restart,
<span class="p_header">diff --git a/arch/x86/kernel/setup.c b/arch/x86/kernel/setup.c</span>
<span class="p_header">index 98c9cd6f3b5d..3aabfdcbcb52 100644</span>
<span class="p_header">--- a/arch/x86/kernel/setup.c</span>
<span class="p_header">+++ b/arch/x86/kernel/setup.c</span>
<span class="p_chunk">@@ -210,9 +210,9 @@</span> <span class="p_context"> EXPORT_SYMBOL(boot_cpu_data);</span>
 
 
 #if !defined(CONFIG_X86_PAE) || defined(CONFIG_X86_64)
<span class="p_del">-__visible unsigned long mmu_cr4_features;</span>
<span class="p_add">+__visible unsigned long mmu_cr4_features __ro_after_init;</span>
 #else
<span class="p_del">-__visible unsigned long mmu_cr4_features = X86_CR4_PAE;</span>
<span class="p_add">+__visible unsigned long mmu_cr4_features __ro_after_init = X86_CR4_PAE;</span>
 #endif
 
 /* Boot loader ID and version as integers, for the benefit of proc_dointvec */
<span class="p_chunk">@@ -1137,7 +1137,7 @@</span> <span class="p_context"> void __init setup_arch(char **cmdline_p)</span>
 	 * auditing all the early-boot CR4 manipulation would be needed to
 	 * rule it out.
 	 */
<span class="p_del">-	mmu_cr4_features = __read_cr4_safe();</span>
<span class="p_add">+	mmu_cr4_features = __read_cr4();</span>
 
 	memblock_set_current_limit(get_max_mapped());
 
<span class="p_header">diff --git a/arch/x86/kernel/setup_percpu.c b/arch/x86/kernel/setup_percpu.c</span>
<span class="p_header">index 7a40e068302d..2bbd27f89802 100644</span>
<span class="p_header">--- a/arch/x86/kernel/setup_percpu.c</span>
<span class="p_header">+++ b/arch/x86/kernel/setup_percpu.c</span>
<span class="p_chunk">@@ -33,7 +33,7 @@</span> <span class="p_context"> EXPORT_PER_CPU_SYMBOL(cpu_number);</span>
 DEFINE_PER_CPU_READ_MOSTLY(unsigned long, this_cpu_off) = BOOT_PERCPU_OFFSET;
 EXPORT_PER_CPU_SYMBOL(this_cpu_off);
 
<span class="p_del">-unsigned long __per_cpu_offset[NR_CPUS] __read_mostly = {</span>
<span class="p_add">+unsigned long __per_cpu_offset[NR_CPUS] __ro_after_init = {</span>
 	[0 ... NR_CPUS-1] = BOOT_PERCPU_OFFSET,
 };
 EXPORT_SYMBOL(__per_cpu_offset);
<span class="p_chunk">@@ -246,7 +246,7 @@</span> <span class="p_context"> void __init setup_per_cpu_areas(void)</span>
 #ifdef CONFIG_X86_64
 		per_cpu(irq_stack_ptr, cpu) =
 			per_cpu(irq_stack_union.irq_stack, cpu) +
<span class="p_del">-			IRQ_STACK_SIZE - 64;</span>
<span class="p_add">+			IRQ_STACK_SIZE;</span>
 #endif
 #ifdef CONFIG_NUMA
 		per_cpu(x86_cpu_to_node_map, cpu) =
<span class="p_header">diff --git a/arch/x86/kernel/signal.c b/arch/x86/kernel/signal.c</span>
<span class="p_header">index 04cb3212db2d..da20ecb5397a 100644</span>
<span class="p_header">--- a/arch/x86/kernel/signal.c</span>
<span class="p_header">+++ b/arch/x86/kernel/signal.c</span>
<span class="p_chunk">@@ -783,7 +783,7 @@</span> <span class="p_context"> static inline unsigned long get_nr_restart_syscall(const struct pt_regs *regs)</span>
 	 * than the tracee.
 	 */
 #ifdef CONFIG_IA32_EMULATION
<span class="p_del">-	if (current_thread_info()-&gt;status &amp; (TS_COMPAT|TS_I386_REGS_POKED))</span>
<span class="p_add">+	if (current-&gt;thread.status &amp; (TS_COMPAT|TS_I386_REGS_POKED))</span>
 		return __NR_ia32_restart_syscall;
 #endif
 #ifdef CONFIG_X86_X32_ABI
<span class="p_header">diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c</span>
<span class="p_header">index 4296beb8fdd3..7e52f83d3a4b 100644</span>
<span class="p_header">--- a/arch/x86/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/smpboot.c</span>
<span class="p_chunk">@@ -942,7 +942,6 @@</span> <span class="p_context"> void common_cpu_up(unsigned int cpu, struct task_struct *idle)</span>
 	per_cpu(cpu_current_top_of_stack, cpu) =
 		(unsigned long)task_stack_page(idle) + THREAD_SIZE;
 #else
<span class="p_del">-	clear_tsk_thread_flag(idle, TIF_FORK);</span>
 	initial_gs = per_cpu_offset(cpu);
 #endif
 }
<span class="p_chunk">@@ -969,7 +968,7 @@</span> <span class="p_context"> static int do_boot_cpu(int apicid, int cpu, struct task_struct *idle)</span>
 
 	early_gdt_descr.address = (unsigned long)get_cpu_gdt_table(cpu);
 	initial_code = (unsigned long)start_secondary;
<span class="p_del">-	stack_start  = idle-&gt;thread.sp;</span>
<span class="p_add">+	initial_stack  = idle-&gt;thread.sp;</span>
 
 	/*
 	 * Enable the espfix hack for this CPU
<span class="p_header">diff --git a/arch/x86/kernel/stacktrace.c b/arch/x86/kernel/stacktrace.c</span>
<span class="p_header">index 4738f5e0f2ab..0653788026e2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/stacktrace.c</span>
<span class="p_header">+++ b/arch/x86/kernel/stacktrace.c</span>
<span class="p_chunk">@@ -8,80 +8,69 @@</span> <span class="p_context"></span>
 #include &lt;linux/export.h&gt;
 #include &lt;linux/uaccess.h&gt;
 #include &lt;asm/stacktrace.h&gt;
<span class="p_add">+#include &lt;asm/unwind.h&gt;</span>
 
<span class="p_del">-static int save_stack_stack(void *data, char *name)</span>
<span class="p_add">+static int save_stack_address(struct stack_trace *trace, unsigned long addr,</span>
<span class="p_add">+			      bool nosched)</span>
 {
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int</span>
<span class="p_del">-__save_stack_address(void *data, unsigned long addr, bool reliable, bool nosched)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct stack_trace *trace = data;</span>
<span class="p_del">-#ifdef CONFIG_FRAME_POINTER</span>
<span class="p_del">-	if (!reliable)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-#endif</span>
 	if (nosched &amp;&amp; in_sched_functions(addr))
 		return 0;
<span class="p_add">+</span>
 	if (trace-&gt;skip &gt; 0) {
 		trace-&gt;skip--;
 		return 0;
 	}
<span class="p_del">-	if (trace-&gt;nr_entries &lt; trace-&gt;max_entries) {</span>
<span class="p_del">-		trace-&gt;entries[trace-&gt;nr_entries++] = addr;</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		return -1; /* no more room, stop walking the stack */</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
 
<span class="p_del">-static int save_stack_address(void *data, unsigned long addr, int reliable)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return __save_stack_address(data, addr, reliable, false);</span>
<span class="p_add">+	if (trace-&gt;nr_entries &gt;= trace-&gt;max_entries)</span>
<span class="p_add">+		return -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	trace-&gt;entries[trace-&gt;nr_entries++] = addr;</span>
<span class="p_add">+	return 0;</span>
 }
 
<span class="p_del">-static int</span>
<span class="p_del">-save_stack_address_nosched(void *data, unsigned long addr, int reliable)</span>
<span class="p_add">+static void __save_stack_trace(struct stack_trace *trace,</span>
<span class="p_add">+			       struct task_struct *task, struct pt_regs *regs,</span>
<span class="p_add">+			       bool nosched)</span>
 {
<span class="p_del">-	return __save_stack_address(data, addr, reliable, true);</span>
<span class="p_del">-}</span>
<span class="p_add">+	struct unwind_state state;</span>
<span class="p_add">+	unsigned long addr;</span>
 
<span class="p_del">-static const struct stacktrace_ops save_stack_ops = {</span>
<span class="p_del">-	.stack		= save_stack_stack,</span>
<span class="p_del">-	.address	= save_stack_address,</span>
<span class="p_del">-	.walk_stack	= print_context_stack,</span>
<span class="p_del">-};</span>
<span class="p_add">+	if (regs)</span>
<span class="p_add">+		save_stack_address(trace, regs-&gt;ip, nosched);</span>
 
<span class="p_del">-static const struct stacktrace_ops save_stack_ops_nosched = {</span>
<span class="p_del">-	.stack		= save_stack_stack,</span>
<span class="p_del">-	.address	= save_stack_address_nosched,</span>
<span class="p_del">-	.walk_stack	= print_context_stack,</span>
<span class="p_del">-};</span>
<span class="p_add">+	for (unwind_start(&amp;state, task, regs, NULL); !unwind_done(&amp;state);</span>
<span class="p_add">+	     unwind_next_frame(&amp;state)) {</span>
<span class="p_add">+		addr = unwind_get_return_address(&amp;state);</span>
<span class="p_add">+		if (!addr || save_stack_address(trace, addr, nosched))</span>
<span class="p_add">+			break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (trace-&gt;nr_entries &lt; trace-&gt;max_entries)</span>
<span class="p_add">+		trace-&gt;entries[trace-&gt;nr_entries++] = ULONG_MAX;</span>
<span class="p_add">+}</span>
 
 /*
  * Save stack-backtrace addresses into a stack_trace buffer.
  */
 void save_stack_trace(struct stack_trace *trace)
 {
<span class="p_del">-	dump_trace(current, NULL, NULL, 0, &amp;save_stack_ops, trace);</span>
<span class="p_del">-	if (trace-&gt;nr_entries &lt; trace-&gt;max_entries)</span>
<span class="p_del">-		trace-&gt;entries[trace-&gt;nr_entries++] = ULONG_MAX;</span>
<span class="p_add">+	__save_stack_trace(trace, current, NULL, false);</span>
 }
 EXPORT_SYMBOL_GPL(save_stack_trace);
 
 void save_stack_trace_regs(struct pt_regs *regs, struct stack_trace *trace)
 {
<span class="p_del">-	dump_trace(current, regs, NULL, 0, &amp;save_stack_ops, trace);</span>
<span class="p_del">-	if (trace-&gt;nr_entries &lt; trace-&gt;max_entries)</span>
<span class="p_del">-		trace-&gt;entries[trace-&gt;nr_entries++] = ULONG_MAX;</span>
<span class="p_add">+	__save_stack_trace(trace, current, regs, false);</span>
 }
 
 void save_stack_trace_tsk(struct task_struct *tsk, struct stack_trace *trace)
 {
<span class="p_del">-	dump_trace(tsk, NULL, NULL, 0, &amp;save_stack_ops_nosched, trace);</span>
<span class="p_del">-	if (trace-&gt;nr_entries &lt; trace-&gt;max_entries)</span>
<span class="p_del">-		trace-&gt;entries[trace-&gt;nr_entries++] = ULONG_MAX;</span>
<span class="p_add">+	if (!try_get_task_stack(tsk))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	__save_stack_trace(trace, tsk, NULL, true);</span>
<span class="p_add">+</span>
<span class="p_add">+	put_task_stack(tsk);</span>
 }
 EXPORT_SYMBOL_GPL(save_stack_trace_tsk);
 
<span class="p_header">diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c</span>
<span class="p_header">index b70ca12dd389..bd4e3d4d3625 100644</span>
<span class="p_header">--- a/arch/x86/kernel/traps.c</span>
<span class="p_header">+++ b/arch/x86/kernel/traps.c</span>
<span class="p_chunk">@@ -292,12 +292,30 @@</span> <span class="p_context"> DO_ERROR(X86_TRAP_NP,     SIGBUS,  &quot;segment not present&quot;,	segment_not_present)</span>
 DO_ERROR(X86_TRAP_SS,     SIGBUS,  &quot;stack segment&quot;,		stack_segment)
 DO_ERROR(X86_TRAP_AC,     SIGBUS,  &quot;alignment check&quot;,		alignment_check)
 
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+__visible void __noreturn handle_stack_overflow(const char *message,</span>
<span class="p_add">+						struct pt_regs *regs,</span>
<span class="p_add">+						unsigned long fault_address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	printk(KERN_EMERG &quot;BUG: stack guard page was hit at %p (stack is %p..%p)\n&quot;,</span>
<span class="p_add">+		 (void *)fault_address, current-&gt;stack,</span>
<span class="p_add">+		 (char *)current-&gt;stack + THREAD_SIZE - 1);</span>
<span class="p_add">+	die(message, regs, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Be absolutely certain we don&#39;t return. */</span>
<span class="p_add">+	panic(message);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_X86_64
 /* Runs on IST stack */
 dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)
 {
 	static const char str[] = &quot;double fault&quot;;
 	struct task_struct *tsk = current;
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+	unsigned long cr2;</span>
<span class="p_add">+#endif</span>
 
 #ifdef CONFIG_X86_ESPFIX64
 	extern unsigned char native_irq_return_iret[];
<span class="p_chunk">@@ -332,6 +350,49 @@</span> <span class="p_context"> dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)</span>
 	tsk-&gt;thread.error_code = error_code;
 	tsk-&gt;thread.trap_nr = X86_TRAP_DF;
 
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we overflow the stack into a guard page, the CPU will fail</span>
<span class="p_add">+	 * to deliver #PF and will send #DF instead.  Similarly, if we</span>
<span class="p_add">+	 * take any non-IST exception while too close to the bottom of</span>
<span class="p_add">+	 * the stack, the processor will get a page fault while</span>
<span class="p_add">+	 * delivering the exception and will generate a double fault.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * According to the SDM (footnote in 6.15 under &quot;Interrupt 14 -</span>
<span class="p_add">+	 * Page-Fault Exception (#PF):</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *   Processors update CR2 whenever a page fault is detected. If a</span>
<span class="p_add">+	 *   second page fault occurs while an earlier page fault is being</span>
<span class="p_add">+	 *   deliv- ered, the faulting linear address of the second fault will</span>
<span class="p_add">+	 *   overwrite the contents of CR2 (replacing the previous</span>
<span class="p_add">+	 *   address). These updates to CR2 occur even if the page fault</span>
<span class="p_add">+	 *   results in a double fault or occurs during the delivery of a</span>
<span class="p_add">+	 *   double fault.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The logic below has a small possibility of incorrectly diagnosing</span>
<span class="p_add">+	 * some errors as stack overflows.  For example, if the IDT or GDT</span>
<span class="p_add">+	 * gets corrupted such that #GP delivery fails due to a bad descriptor</span>
<span class="p_add">+	 * causing #GP and we hit this condition while CR2 coincidentally</span>
<span class="p_add">+	 * points to the stack guard page, we&#39;ll think we overflowed the</span>
<span class="p_add">+	 * stack.  Given that we&#39;re going to panic one way or another</span>
<span class="p_add">+	 * if this happens, this isn&#39;t necessarily worth fixing.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * If necessary, we could improve the test by only diagnosing</span>
<span class="p_add">+	 * a stack overflow if the saved RSP points within 47 bytes of</span>
<span class="p_add">+	 * the bottom of the stack: if RSP == tsk_stack + 48 and we</span>
<span class="p_add">+	 * take an exception, the stack is already aligned and there</span>
<span class="p_add">+	 * will be enough room SS, RSP, RFLAGS, CS, RIP, and a</span>
<span class="p_add">+	 * possible error code, so a stack overflow would *not* double</span>
<span class="p_add">+	 * fault.  With any less space left, exception delivery could</span>
<span class="p_add">+	 * fail, and, as a practical matter, we&#39;ve overflowed the</span>
<span class="p_add">+	 * stack even if the actual trigger for the double fault was</span>
<span class="p_add">+	 * something else.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	cr2 = read_cr2();</span>
<span class="p_add">+	if ((unsigned long)task_stack_page(tsk) - 1 - cr2 &lt; PAGE_SIZE)</span>
<span class="p_add">+		handle_stack_overflow(&quot;kernel stack overflow (double-fault)&quot;, regs, cr2);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_DOUBLEFAULT
 	df_debug(regs, error_code);
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/unwind_frame.c b/arch/x86/kernel/unwind_frame.c</span>
new file mode 100644
<span class="p_header">index 000000000000..a2456d4d286a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/kernel/unwind_frame.c</span>
<span class="p_chunk">@@ -0,0 +1,93 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/bitops.h&gt;</span>
<span class="p_add">+#include &lt;asm/stacktrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/unwind.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define FRAME_HEADER_SIZE (sizeof(long) * 2)</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned long unwind_get_return_address(struct unwind_state *state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+	unsigned long *addr_p = unwind_get_return_address_ptr(state);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unwind_done(state))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	addr = ftrace_graph_ret_addr(state-&gt;task, &amp;state-&gt;graph_idx, *addr_p,</span>
<span class="p_add">+				     addr_p);</span>
<span class="p_add">+</span>
<span class="p_add">+	return __kernel_text_address(addr) ? addr : 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(unwind_get_return_address);</span>
<span class="p_add">+</span>
<span class="p_add">+static bool update_stack_state(struct unwind_state *state, void *addr,</span>
<span class="p_add">+			       size_t len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct stack_info *info = &amp;state-&gt;stack_info;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If addr isn&#39;t on the current stack, switch to the next one.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * We may have to traverse multiple stacks to deal with the possibility</span>
<span class="p_add">+	 * that &#39;info-&gt;next_sp&#39; could point to an empty stack and &#39;addr&#39; could</span>
<span class="p_add">+	 * be on a subsequent stack.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	while (!on_stack(info, addr, len))</span>
<span class="p_add">+		if (get_stack_info(info-&gt;next_sp, state-&gt;task, info,</span>
<span class="p_add">+				   &amp;state-&gt;stack_mask))</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+bool unwind_next_frame(struct unwind_state *state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long *next_bp;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unwind_done(state))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	next_bp = (unsigned long *)*state-&gt;bp;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* make sure the next frame&#39;s data is accessible */</span>
<span class="p_add">+	if (!update_stack_state(state, next_bp, FRAME_HEADER_SIZE))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* move to the next frame */</span>
<span class="p_add">+	state-&gt;bp = next_bp;</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(unwind_next_frame);</span>
<span class="p_add">+</span>
<span class="p_add">+void __unwind_start(struct unwind_state *state, struct task_struct *task,</span>
<span class="p_add">+		    struct pt_regs *regs, unsigned long *first_frame)</span>
<span class="p_add">+{</span>
<span class="p_add">+	memset(state, 0, sizeof(*state));</span>
<span class="p_add">+	state-&gt;task = task;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* don&#39;t even attempt to start from user mode regs */</span>
<span class="p_add">+	if (regs &amp;&amp; user_mode(regs)) {</span>
<span class="p_add">+		state-&gt;stack_info.type = STACK_TYPE_UNKNOWN;</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* set up the starting stack frame */</span>
<span class="p_add">+	state-&gt;bp = get_frame_pointer(task, regs);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* initialize stack info and make sure the frame data is accessible */</span>
<span class="p_add">+	get_stack_info(state-&gt;bp, state-&gt;task, &amp;state-&gt;stack_info,</span>
<span class="p_add">+		       &amp;state-&gt;stack_mask);</span>
<span class="p_add">+	update_stack_state(state, state-&gt;bp, FRAME_HEADER_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The caller can provide the address of the first frame directly</span>
<span class="p_add">+	 * (first_frame) or indirectly (regs-&gt;sp) to indicate which stack frame</span>
<span class="p_add">+	 * to start unwinding at.  Skip ahead until we reach it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	while (!unwind_done(state) &amp;&amp;</span>
<span class="p_add">+	       (!on_stack(&amp;state-&gt;stack_info, first_frame, sizeof(long)) ||</span>
<span class="p_add">+			state-&gt;bp &lt; first_frame))</span>
<span class="p_add">+		unwind_next_frame(state);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(__unwind_start);</span>
<span class="p_header">diff --git a/arch/x86/kernel/unwind_guess.c b/arch/x86/kernel/unwind_guess.c</span>
new file mode 100644
<span class="p_header">index 000000000000..b5a834c93065</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/kernel/unwind_guess.c</span>
<span class="p_chunk">@@ -0,0 +1,43 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;linux/sched.h&gt;</span>
<span class="p_add">+#include &lt;linux/ftrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/ptrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/bitops.h&gt;</span>
<span class="p_add">+#include &lt;asm/stacktrace.h&gt;</span>
<span class="p_add">+#include &lt;asm/unwind.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+bool unwind_next_frame(struct unwind_state *state)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct stack_info *info = &amp;state-&gt;stack_info;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (unwind_done(state))</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		for (state-&gt;sp++; state-&gt;sp &lt; info-&gt;end; state-&gt;sp++)</span>
<span class="p_add">+			if (__kernel_text_address(*state-&gt;sp))</span>
<span class="p_add">+				return true;</span>
<span class="p_add">+</span>
<span class="p_add">+		state-&gt;sp = info-&gt;next_sp;</span>
<span class="p_add">+</span>
<span class="p_add">+	} while (!get_stack_info(state-&gt;sp, state-&gt;task, info,</span>
<span class="p_add">+				 &amp;state-&gt;stack_mask));</span>
<span class="p_add">+</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(unwind_next_frame);</span>
<span class="p_add">+</span>
<span class="p_add">+void __unwind_start(struct unwind_state *state, struct task_struct *task,</span>
<span class="p_add">+		    struct pt_regs *regs, unsigned long *first_frame)</span>
<span class="p_add">+{</span>
<span class="p_add">+	memset(state, 0, sizeof(*state));</span>
<span class="p_add">+</span>
<span class="p_add">+	state-&gt;task = task;</span>
<span class="p_add">+	state-&gt;sp   = first_frame;</span>
<span class="p_add">+</span>
<span class="p_add">+	get_stack_info(first_frame, state-&gt;task, &amp;state-&gt;stack_info,</span>
<span class="p_add">+		       &amp;state-&gt;stack_mask);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!__kernel_text_address(*first_frame))</span>
<span class="p_add">+		unwind_next_frame(state);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(__unwind_start);</span>
<span class="p_header">diff --git a/arch/x86/kernel/x86_init.c b/arch/x86/kernel/x86_init.c</span>
<span class="p_header">index 76c5e52436c4..0bd9f1287f39 100644</span>
<span class="p_header">--- a/arch/x86/kernel/x86_init.c</span>
<span class="p_header">+++ b/arch/x86/kernel/x86_init.c</span>
<span class="p_chunk">@@ -91,7 +91,7 @@</span> <span class="p_context"> struct x86_cpuinit_ops x86_cpuinit = {</span>
 static void default_nmi_init(void) { };
 static int default_i8042_detect(void) { return 1; };
 
<span class="p_del">-struct x86_platform_ops x86_platform = {</span>
<span class="p_add">+struct x86_platform_ops x86_platform __ro_after_init = {</span>
 	.calibrate_cpu			= native_calibrate_cpu,
 	.calibrate_tsc			= native_calibrate_tsc,
 	.get_wallclock			= mach_get_cmos_time,
<span class="p_chunk">@@ -108,7 +108,7 @@</span> <span class="p_context"> struct x86_platform_ops x86_platform = {</span>
 EXPORT_SYMBOL_GPL(x86_platform);
 
 #if defined(CONFIG_PCI_MSI)
<span class="p_del">-struct x86_msi_ops x86_msi = {</span>
<span class="p_add">+struct x86_msi_ops x86_msi __ro_after_init = {</span>
 	.setup_msi_irqs		= native_setup_msi_irqs,
 	.teardown_msi_irq	= native_teardown_msi_irq,
 	.teardown_msi_irqs	= default_teardown_msi_irqs,
<span class="p_chunk">@@ -137,7 +137,7 @@</span> <span class="p_context"> void arch_restore_msi_irqs(struct pci_dev *dev)</span>
 }
 #endif
 
<span class="p_del">-struct x86_io_apic_ops x86_io_apic_ops = {</span>
<span class="p_add">+struct x86_io_apic_ops x86_io_apic_ops __ro_after_init = {</span>
 	.read			= native_io_apic_read,
 	.disable		= native_disable_io_apic,
 };
<span class="p_header">diff --git a/arch/x86/kvm/svm.c b/arch/x86/kvm/svm.c</span>
<span class="p_header">index af523d84d102..1e6b84b96ea6 100644</span>
<span class="p_header">--- a/arch/x86/kvm/svm.c</span>
<span class="p_header">+++ b/arch/x86/kvm/svm.c</span>
<span class="p_chunk">@@ -4961,7 +4961,7 @@</span> <span class="p_context"> static inline void avic_post_state_restore(struct kvm_vcpu *vcpu)</span>
 	avic_handle_ldr_update(vcpu);
 }
 
<span class="p_del">-static struct kvm_x86_ops svm_x86_ops = {</span>
<span class="p_add">+static struct kvm_x86_ops svm_x86_ops __ro_after_init = {</span>
 	.cpu_has_kvm_support = has_svm,
 	.disabled_by_bios = is_disabled,
 	.hardware_setup = svm_hardware_setup,
<span class="p_header">diff --git a/arch/x86/kvm/vmx.c b/arch/x86/kvm/vmx.c</span>
<span class="p_header">index 5cede40e2552..121fdf6e9ed0 100644</span>
<span class="p_header">--- a/arch/x86/kvm/vmx.c</span>
<span class="p_header">+++ b/arch/x86/kvm/vmx.c</span>
<span class="p_chunk">@@ -11177,7 +11177,7 @@</span> <span class="p_context"> static void vmx_setup_mce(struct kvm_vcpu *vcpu)</span>
 			~FEATURE_CONTROL_LMCE;
 }
 
<span class="p_del">-static struct kvm_x86_ops vmx_x86_ops = {</span>
<span class="p_add">+static struct kvm_x86_ops vmx_x86_ops __ro_after_init = {</span>
 	.cpu_has_kvm_support = cpu_has_kvm_support,
 	.disabled_by_bios = vmx_disabled_by_bios,
 	.hardware_setup = hardware_setup,
<span class="p_header">diff --git a/arch/x86/mm/fault.c b/arch/x86/mm/fault.c</span>
<span class="p_header">index dc8023060456..0b92fce3e6c0 100644</span>
<span class="p_header">--- a/arch/x86/mm/fault.c</span>
<span class="p_header">+++ b/arch/x86/mm/fault.c</span>
<span class="p_chunk">@@ -753,6 +753,38 @@</span> <span class="p_context"> no_context(struct pt_regs *regs, unsigned long error_code,</span>
 		return;
 	}
 
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Stack overflow?  During boot, we can fault near the initial</span>
<span class="p_add">+	 * stack in the direct map, but that&#39;s not an overflow -- check</span>
<span class="p_add">+	 * that we&#39;re in vmalloc space to avoid this.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (is_vmalloc_addr((void *)address) &amp;&amp;</span>
<span class="p_add">+	    (((unsigned long)tsk-&gt;stack - 1 - address &lt; PAGE_SIZE) ||</span>
<span class="p_add">+	     address - ((unsigned long)tsk-&gt;stack + THREAD_SIZE) &lt; PAGE_SIZE)) {</span>
<span class="p_add">+		register void *__sp asm(&quot;rsp&quot;);</span>
<span class="p_add">+		unsigned long stack = this_cpu_read(orig_ist.ist[DOUBLEFAULT_STACK]) - sizeof(void *);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We&#39;re likely to be running with very little stack space</span>
<span class="p_add">+		 * left.  It&#39;s plausible that we&#39;d hit this condition but</span>
<span class="p_add">+		 * double-fault even before we get this far, in which case</span>
<span class="p_add">+		 * we&#39;re fine: the double-fault handler will deal with it.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * We don&#39;t want to make it all the way into the oops code</span>
<span class="p_add">+		 * and then double-fault, though, because we&#39;re likely to</span>
<span class="p_add">+		 * break the console driver and lose most of the stack dump.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		asm volatile (&quot;movq %[stack], %%rsp\n\t&quot;</span>
<span class="p_add">+			      &quot;call handle_stack_overflow\n\t&quot;</span>
<span class="p_add">+			      &quot;1: jmp 1b&quot;</span>
<span class="p_add">+			      : &quot;+r&quot; (__sp)</span>
<span class="p_add">+			      : &quot;D&quot; (&quot;kernel stack overflow (page fault)&quot;),</span>
<span class="p_add">+				&quot;S&quot; (regs), &quot;d&quot; (address),</span>
<span class="p_add">+				[stack] &quot;rm&quot; (stack));</span>
<span class="p_add">+		unreachable();</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	/*
 	 * 32-bit:
 	 *
<span class="p_header">diff --git a/arch/x86/mm/kaslr.c b/arch/x86/mm/kaslr.c</span>
<span class="p_header">index bda8d5eef04d..ddd2661c4502 100644</span>
<span class="p_header">--- a/arch/x86/mm/kaslr.c</span>
<span class="p_header">+++ b/arch/x86/mm/kaslr.c</span>
<span class="p_chunk">@@ -40,17 +40,26 @@</span> <span class="p_context"></span>
  * You need to add an if/def entry if you introduce a new memory region
  * compatible with KASLR. Your entry must be in logical order with memory
  * layout. For example, ESPFIX is before EFI because its virtual address is
<span class="p_del">- * before. You also need to add a BUILD_BUG_ON in kernel_randomize_memory to</span>
<span class="p_add">+ * before. You also need to add a BUILD_BUG_ON() in kernel_randomize_memory() to</span>
  * ensure that this order is correct and won&#39;t be changed.
  */
 static const unsigned long vaddr_start = __PAGE_OFFSET_BASE;
<span class="p_del">-static const unsigned long vaddr_end = VMEMMAP_START;</span>
<span class="p_add">+</span>
<span class="p_add">+#if defined(CONFIG_X86_ESPFIX64)</span>
<span class="p_add">+static const unsigned long vaddr_end = ESPFIX_BASE_ADDR;</span>
<span class="p_add">+#elif defined(CONFIG_EFI)</span>
<span class="p_add">+static const unsigned long vaddr_end = EFI_VA_START;</span>
<span class="p_add">+#else</span>
<span class="p_add">+static const unsigned long vaddr_end = __START_KERNEL_map;</span>
<span class="p_add">+#endif</span>
 
 /* Default values */
 unsigned long page_offset_base = __PAGE_OFFSET_BASE;
 EXPORT_SYMBOL(page_offset_base);
 unsigned long vmalloc_base = __VMALLOC_BASE;
 EXPORT_SYMBOL(vmalloc_base);
<span class="p_add">+unsigned long vmemmap_base = __VMEMMAP_BASE;</span>
<span class="p_add">+EXPORT_SYMBOL(vmemmap_base);</span>
 
 /*
  * Memory regions randomized by KASLR (except modules that use a separate logic
<span class="p_chunk">@@ -63,6 +72,7 @@</span> <span class="p_context"> static __initdata struct kaslr_memory_region {</span>
 } kaslr_regions[] = {
 	{ &amp;page_offset_base, 64/* Maximum */ },
 	{ &amp;vmalloc_base, VMALLOC_SIZE_TB },
<span class="p_add">+	{ &amp;vmemmap_base, 1 },</span>
 };
 
 /* Get size in bytes used by the memory region */
<span class="p_chunk">@@ -89,6 +99,18 @@</span> <span class="p_context"> void __init kernel_randomize_memory(void)</span>
 	struct rnd_state rand_state;
 	unsigned long remain_entropy;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * All these BUILD_BUG_ON checks ensures the memory layout is</span>
<span class="p_add">+	 * consistent with the vaddr_start/vaddr_end variables.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUILD_BUG_ON(vaddr_start &gt;= vaddr_end);</span>
<span class="p_add">+	BUILD_BUG_ON(config_enabled(CONFIG_X86_ESPFIX64) &amp;&amp;</span>
<span class="p_add">+		     vaddr_end &gt;= EFI_VA_START);</span>
<span class="p_add">+	BUILD_BUG_ON((config_enabled(CONFIG_X86_ESPFIX64) ||</span>
<span class="p_add">+		      config_enabled(CONFIG_EFI)) &amp;&amp;</span>
<span class="p_add">+		     vaddr_end &gt;= __START_KERNEL_map);</span>
<span class="p_add">+	BUILD_BUG_ON(vaddr_end &gt; __START_KERNEL_map);</span>
<span class="p_add">+</span>
 	if (!kaslr_memory_enabled())
 		return;
 
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 4dbe65622810..a7655f6caf7d 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -77,10 +77,25 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 	unsigned cpu = smp_processor_id();
 
 	if (likely(prev != next)) {
<span class="p_add">+		if (IS_ENABLED(CONFIG_VMAP_STACK)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If our current stack is in vmalloc space and isn&#39;t</span>
<span class="p_add">+			 * mapped in the new pgd, we&#39;ll double-fault.  Forcibly</span>
<span class="p_add">+			 * map it.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			unsigned int stack_pgd_index = pgd_index(current_stack_pointer());</span>
<span class="p_add">+</span>
<span class="p_add">+			pgd_t *pgd = next-&gt;pgd + stack_pgd_index;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (unlikely(pgd_none(*pgd)))</span>
<span class="p_add">+				set_pgd(pgd, init_mm.pgd[stack_pgd_index]);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 #ifdef CONFIG_SMP
 		this_cpu_write(cpu_tlbstate.state, TLBSTATE_OK);
 		this_cpu_write(cpu_tlbstate.active_mm, next);
 #endif
<span class="p_add">+</span>
 		cpumask_set_cpu(cpu, mm_cpumask(next));
 
 		/*
<span class="p_header">diff --git a/arch/x86/oprofile/backtrace.c b/arch/x86/oprofile/backtrace.c</span>
<span class="p_header">index cb31a4440e58..a2488b6e27d6 100644</span>
<span class="p_header">--- a/arch/x86/oprofile/backtrace.c</span>
<span class="p_header">+++ b/arch/x86/oprofile/backtrace.c</span>
<span class="p_chunk">@@ -16,27 +16,7 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/ptrace.h&gt;
 #include &lt;asm/stacktrace.h&gt;
<span class="p_del">-</span>
<span class="p_del">-static int backtrace_stack(void *data, char *name)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/* Yes, we want all stacks */</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int backtrace_address(void *data, unsigned long addr, int reliable)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned int *depth = data;</span>
<span class="p_del">-</span>
<span class="p_del">-	if ((*depth)--)</span>
<span class="p_del">-		oprofile_add_trace(addr);</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static struct stacktrace_ops backtrace_ops = {</span>
<span class="p_del">-	.stack		= backtrace_stack,</span>
<span class="p_del">-	.address	= backtrace_address,</span>
<span class="p_del">-	.walk_stack	= print_context_stack,</span>
<span class="p_del">-};</span>
<span class="p_add">+#include &lt;asm/unwind.h&gt;</span>
 
 #ifdef CONFIG_COMPAT
 static struct stack_frame_ia32 *
<span class="p_chunk">@@ -113,10 +93,29 @@</span> <span class="p_context"> x86_backtrace(struct pt_regs * const regs, unsigned int depth)</span>
 	struct stack_frame *head = (struct stack_frame *)frame_pointer(regs);
 
 	if (!user_mode(regs)) {
<span class="p_del">-		unsigned long stack = kernel_stack_pointer(regs);</span>
<span class="p_del">-		if (depth)</span>
<span class="p_del">-			dump_trace(NULL, regs, (unsigned long *)stack, 0,</span>
<span class="p_del">-				   &amp;backtrace_ops, &amp;depth);</span>
<span class="p_add">+		struct unwind_state state;</span>
<span class="p_add">+		unsigned long addr;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!depth)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+</span>
<span class="p_add">+		oprofile_add_trace(regs-&gt;ip);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!--depth)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+</span>
<span class="p_add">+		for (unwind_start(&amp;state, current, regs, NULL);</span>
<span class="p_add">+		     !unwind_done(&amp;state); unwind_next_frame(&amp;state)) {</span>
<span class="p_add">+			addr = unwind_get_return_address(&amp;state);</span>
<span class="p_add">+			if (!addr)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+</span>
<span class="p_add">+			oprofile_add_trace(addr);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!--depth)</span>
<span class="p_add">+				break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		return;
 	}
 
<span class="p_header">diff --git a/arch/x86/pci/pcbios.c b/arch/x86/pci/pcbios.c</span>
<span class="p_header">index 9770e55e768f..1d97cea3b3a4 100644</span>
<span class="p_header">--- a/arch/x86/pci/pcbios.c</span>
<span class="p_header">+++ b/arch/x86/pci/pcbios.c</span>
<span class="p_chunk">@@ -120,9 +120,12 @@</span> <span class="p_context"> static unsigned long __init bios32_service(unsigned long service)</span>
 static struct {
 	unsigned long address;
 	unsigned short segment;
<span class="p_del">-} pci_indirect = { 0, __KERNEL_CS };</span>
<span class="p_add">+} pci_indirect __ro_after_init = {</span>
<span class="p_add">+	.address = 0,</span>
<span class="p_add">+	.segment = __KERNEL_CS,</span>
<span class="p_add">+};</span>
 
<span class="p_del">-static int pci_bios_present;</span>
<span class="p_add">+static int pci_bios_present __ro_after_init;</span>
 
 static int __init check_pcibios(void)
 {
<span class="p_header">diff --git a/arch/x86/power/cpu.c b/arch/x86/power/cpu.c</span>
<span class="p_header">index b12c26e2e309..53cace2ec0e2 100644</span>
<span class="p_header">--- a/arch/x86/power/cpu.c</span>
<span class="p_header">+++ b/arch/x86/power/cpu.c</span>
<span class="p_chunk">@@ -130,7 +130,7 @@</span> <span class="p_context"> static void __save_processor_state(struct saved_context *ctxt)</span>
 	ctxt-&gt;cr0 = read_cr0();
 	ctxt-&gt;cr2 = read_cr2();
 	ctxt-&gt;cr3 = read_cr3();
<span class="p_del">-	ctxt-&gt;cr4 = __read_cr4_safe();</span>
<span class="p_add">+	ctxt-&gt;cr4 = __read_cr4();</span>
 #ifdef CONFIG_X86_64
 	ctxt-&gt;cr8 = read_cr8();
 #endif
<span class="p_header">diff --git a/arch/x86/um/ptrace_32.c b/arch/x86/um/ptrace_32.c</span>
<span class="p_header">index a7ef7b131e25..5766ead6fdb9 100644</span>
<span class="p_header">--- a/arch/x86/um/ptrace_32.c</span>
<span class="p_header">+++ b/arch/x86/um/ptrace_32.c</span>
<span class="p_chunk">@@ -194,7 +194,7 @@</span> <span class="p_context"> int peek_user(struct task_struct *child, long addr, long data)</span>
 
 static int get_fpregs(struct user_i387_struct __user *buf, struct task_struct *child)
 {
<span class="p_del">-	int err, n, cpu = ((struct thread_info *) child-&gt;stack)-&gt;cpu;</span>
<span class="p_add">+	int err, n, cpu = task_cpu(child);</span>
 	struct user_i387_struct fpregs;
 
 	err = save_i387_registers(userspace_pid[cpu],
<span class="p_chunk">@@ -211,7 +211,7 @@</span> <span class="p_context"> static int get_fpregs(struct user_i387_struct __user *buf, struct task_struct *c</span>
 
 static int set_fpregs(struct user_i387_struct __user *buf, struct task_struct *child)
 {
<span class="p_del">-	int n, cpu = ((struct thread_info *) child-&gt;stack)-&gt;cpu;</span>
<span class="p_add">+	int n, cpu = task_cpu(child);</span>
 	struct user_i387_struct fpregs;
 
 	n = copy_from_user(&amp;fpregs, buf, sizeof(fpregs));
<span class="p_chunk">@@ -224,7 +224,7 @@</span> <span class="p_context"> static int set_fpregs(struct user_i387_struct __user *buf, struct task_struct *c</span>
 
 static int get_fpxregs(struct user_fxsr_struct __user *buf, struct task_struct *child)
 {
<span class="p_del">-	int err, n, cpu = ((struct thread_info *) child-&gt;stack)-&gt;cpu;</span>
<span class="p_add">+	int err, n, cpu = task_cpu(child);</span>
 	struct user_fxsr_struct fpregs;
 
 	err = save_fpx_registers(userspace_pid[cpu], (unsigned long *) &amp;fpregs);
<span class="p_chunk">@@ -240,7 +240,7 @@</span> <span class="p_context"> static int get_fpxregs(struct user_fxsr_struct __user *buf, struct task_struct *</span>
 
 static int set_fpxregs(struct user_fxsr_struct __user *buf, struct task_struct *child)
 {
<span class="p_del">-	int n, cpu = ((struct thread_info *) child-&gt;stack)-&gt;cpu;</span>
<span class="p_add">+	int n, cpu = task_cpu(child);</span>
 	struct user_fxsr_struct fpregs;
 
 	n = copy_from_user(&amp;fpregs, buf, sizeof(fpregs));
<span class="p_header">diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c</span>
<span class="p_header">index b86ebb1a9a7f..e2cf8fcea6bb 100644</span>
<span class="p_header">--- a/arch/x86/xen/enlighten.c</span>
<span class="p_header">+++ b/arch/x86/xen/enlighten.c</span>
<span class="p_chunk">@@ -1237,7 +1237,6 @@</span> <span class="p_context"> static const struct pv_cpu_ops xen_cpu_ops __initconst = {</span>
 	.write_cr0 = xen_write_cr0,
 
 	.read_cr4 = native_read_cr4,
<span class="p_del">-	.read_cr4_safe = native_read_cr4_safe,</span>
 	.write_cr4 = xen_write_cr4,
 
 #ifdef CONFIG_X86_64
<span class="p_header">diff --git a/drivers/iommu/amd_iommu.c b/drivers/iommu/amd_iommu.c</span>
<span class="p_header">index 96de97a46079..4025291ea0ae 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu.c</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu.c</span>
<span class="p_chunk">@@ -940,15 +940,13 @@</span> <span class="p_context"> static void build_inv_irt(struct iommu_cmd *cmd, u16 devid)</span>
  * Writes the command to the IOMMUs command buffer and informs the
  * hardware about the new command.
  */
<span class="p_del">-static int iommu_queue_command_sync(struct amd_iommu *iommu,</span>
<span class="p_del">-				    struct iommu_cmd *cmd,</span>
<span class="p_del">-				    bool sync)</span>
<span class="p_add">+static int __iommu_queue_command_sync(struct amd_iommu *iommu,</span>
<span class="p_add">+				      struct iommu_cmd *cmd,</span>
<span class="p_add">+				      bool sync)</span>
 {
 	u32 left, tail, head, next_tail;
<span class="p_del">-	unsigned long flags;</span>
 
 again:
<span class="p_del">-	spin_lock_irqsave(&amp;iommu-&gt;lock, flags);</span>
 
 	head      = readl(iommu-&gt;mmio_base + MMIO_CMD_HEAD_OFFSET);
 	tail      = readl(iommu-&gt;mmio_base + MMIO_CMD_TAIL_OFFSET);
<span class="p_chunk">@@ -957,15 +955,14 @@</span> <span class="p_context"> static int iommu_queue_command_sync(struct amd_iommu *iommu,</span>
 
 	if (left &lt;= 2) {
 		struct iommu_cmd sync_cmd;
<span class="p_del">-		volatile u64 sem = 0;</span>
 		int ret;
 
<span class="p_del">-		build_completion_wait(&amp;sync_cmd, (u64)&amp;sem);</span>
<span class="p_del">-		copy_cmd_to_buffer(iommu, &amp;sync_cmd, tail);</span>
<span class="p_add">+		iommu-&gt;cmd_sem = 0;</span>
 
<span class="p_del">-		spin_unlock_irqrestore(&amp;iommu-&gt;lock, flags);</span>
<span class="p_add">+		build_completion_wait(&amp;sync_cmd, (u64)&amp;iommu-&gt;cmd_sem);</span>
<span class="p_add">+		copy_cmd_to_buffer(iommu, &amp;sync_cmd, tail);</span>
 
<span class="p_del">-		if ((ret = wait_on_sem(&amp;sem)) != 0)</span>
<span class="p_add">+		if ((ret = wait_on_sem(&amp;iommu-&gt;cmd_sem)) != 0)</span>
 			return ret;
 
 		goto again;
<span class="p_chunk">@@ -976,9 +973,21 @@</span> <span class="p_context"> static int iommu_queue_command_sync(struct amd_iommu *iommu,</span>
 	/* We need to sync now to make sure all commands are processed */
 	iommu-&gt;need_sync = sync;
 
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int iommu_queue_command_sync(struct amd_iommu *iommu,</span>
<span class="p_add">+				    struct iommu_cmd *cmd,</span>
<span class="p_add">+				    bool sync)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;iommu-&gt;lock, flags);</span>
<span class="p_add">+	ret = __iommu_queue_command_sync(iommu, cmd, sync);</span>
 	spin_unlock_irqrestore(&amp;iommu-&gt;lock, flags);
 
<span class="p_del">-	return 0;</span>
<span class="p_add">+	return ret;</span>
 }
 
 static int iommu_queue_command(struct amd_iommu *iommu, struct iommu_cmd *cmd)
<span class="p_chunk">@@ -993,19 +1002,29 @@</span> <span class="p_context"> static int iommu_queue_command(struct amd_iommu *iommu, struct iommu_cmd *cmd)</span>
 static int iommu_completion_wait(struct amd_iommu *iommu)
 {
 	struct iommu_cmd cmd;
<span class="p_del">-	volatile u64 sem = 0;</span>
<span class="p_add">+	unsigned long flags;</span>
 	int ret;
 
 	if (!iommu-&gt;need_sync)
 		return 0;
 
<span class="p_del">-	build_completion_wait(&amp;cmd, (u64)&amp;sem);</span>
 
<span class="p_del">-	ret = iommu_queue_command_sync(iommu, &amp;cmd, false);</span>
<span class="p_add">+	build_completion_wait(&amp;cmd, (u64)&amp;iommu-&gt;cmd_sem);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;iommu-&gt;lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu-&gt;cmd_sem = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = __iommu_queue_command_sync(iommu, &amp;cmd, false);</span>
 	if (ret)
<span class="p_del">-		return ret;</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = wait_on_sem(&amp;iommu-&gt;cmd_sem);</span>
 
<span class="p_del">-	return wait_on_sem(&amp;sem);</span>
<span class="p_add">+out_unlock:</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;iommu-&gt;lock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
 }
 
 static int iommu_flush_dte(struct amd_iommu *iommu, u16 devid)
<span class="p_header">diff --git a/drivers/iommu/amd_iommu_types.h b/drivers/iommu/amd_iommu_types.h</span>
<span class="p_header">index caf5e3822715..9652848e3155 100644</span>
<span class="p_header">--- a/drivers/iommu/amd_iommu_types.h</span>
<span class="p_header">+++ b/drivers/iommu/amd_iommu_types.h</span>
<span class="p_chunk">@@ -524,6 +524,8 @@</span> <span class="p_context"> struct amd_iommu {</span>
 	struct irq_domain *ir_domain;
 	struct irq_domain *msi_domain;
 #endif
<span class="p_add">+</span>
<span class="p_add">+	volatile u64 __aligned(8) cmd_sem;</span>
 };
 
 #define ACPIHID_UID_LEN 256
<span class="p_header">diff --git a/fs/proc/base.c b/fs/proc/base.c</span>
<span class="p_header">index ac0df4dde823..3b792ab3c0dc 100644</span>
<span class="p_header">--- a/fs/proc/base.c</span>
<span class="p_header">+++ b/fs/proc/base.c</span>
<span class="p_chunk">@@ -483,7 +483,7 @@</span> <span class="p_context"> static int proc_pid_stack(struct seq_file *m, struct pid_namespace *ns,</span>
 		save_stack_trace_tsk(task, &amp;trace);
 
 		for (i = 0; i &lt; trace.nr_entries; i++) {
<span class="p_del">-			seq_printf(m, &quot;[&lt;%pK&gt;] %pS\n&quot;,</span>
<span class="p_add">+			seq_printf(m, &quot;[&lt;%pK&gt;] %pB\n&quot;,</span>
 				   (void *)entries[i], (void *)entries[i]);
 		}
 		unlock_trace(task);
<span class="p_header">diff --git a/include/linux/ftrace.h b/include/linux/ftrace.h</span>
<span class="p_header">index 7d565afe35d2..6f93ac46e7f0 100644</span>
<span class="p_header">--- a/include/linux/ftrace.h</span>
<span class="p_header">+++ b/include/linux/ftrace.h</span>
<span class="p_chunk">@@ -795,7 +795,12 @@</span> <span class="p_context"> struct ftrace_ret_stack {</span>
 	unsigned long func;
 	unsigned long long calltime;
 	unsigned long long subtime;
<span class="p_add">+#ifdef HAVE_FUNCTION_GRAPH_FP_TEST</span>
 	unsigned long fp;
<span class="p_add">+#endif</span>
<span class="p_add">+#ifdef HAVE_FUNCTION_GRAPH_RET_ADDR_PTR</span>
<span class="p_add">+	unsigned long *retp;</span>
<span class="p_add">+#endif</span>
 };
 
 /*
<span class="p_chunk">@@ -807,7 +812,10 @@</span> <span class="p_context"> extern void return_to_handler(void);</span>
 
 extern int
 ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,
<span class="p_del">-			 unsigned long frame_pointer);</span>
<span class="p_add">+			 unsigned long frame_pointer, unsigned long *retp);</span>
<span class="p_add">+</span>
<span class="p_add">+unsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,</span>
<span class="p_add">+				    unsigned long ret, unsigned long *retp);</span>
 
 /*
  * Sometimes we don&#39;t want to trace a function with the function
<span class="p_chunk">@@ -870,6 +878,13 @@</span> <span class="p_context"> static inline int task_curr_ret_stack(struct task_struct *tsk)</span>
 	return -1;
 }
 
<span class="p_add">+static inline unsigned long</span>
<span class="p_add">+ftrace_graph_ret_addr(struct task_struct *task, int *idx, unsigned long ret,</span>
<span class="p_add">+		      unsigned long *retp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void pause_graph_tracing(void) { }
 static inline void unpause_graph_tracing(void) { }
 #endif /* CONFIG_FUNCTION_GRAPH_TRACER */
<span class="p_header">diff --git a/include/linux/init_task.h b/include/linux/init_task.h</span>
<span class="p_header">index f8834f820ec2..325f649d77ff 100644</span>
<span class="p_header">--- a/include/linux/init_task.h</span>
<span class="p_header">+++ b/include/linux/init_task.h</span>
<span class="p_chunk">@@ -15,6 +15,8 @@</span> <span class="p_context"></span>
 #include &lt;net/net_namespace.h&gt;
 #include &lt;linux/sched/rt.h&gt;
 
<span class="p_add">+#include &lt;asm/thread_info.h&gt;</span>
<span class="p_add">+</span>
 #ifdef CONFIG_SMP
 # define INIT_PUSHABLE_TASKS(tsk)					\
 	.pushable_tasks = PLIST_NODE_INIT(tsk.pushable_tasks, MAX_PRIO),
<span class="p_chunk">@@ -183,12 +185,21 @@</span> <span class="p_context"> extern struct task_group root_task_group;</span>
 # define INIT_KASAN(tsk)
 #endif
 
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+# define INIT_TASK_TI(tsk)			\</span>
<span class="p_add">+	.thread_info = INIT_THREAD_INFO(tsk),	\</span>
<span class="p_add">+	.stack_refcount = ATOMIC_INIT(1),</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define INIT_TASK_TI(tsk)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  *  INIT_TASK is used to set up the first task table, touch at
  * your own risk!. Base=0, limit=0x1fffff (=2MB)
  */
 #define INIT_TASK(tsk)	\
 {									\
<span class="p_add">+	INIT_TASK_TI(tsk)						\</span>
 	.state		= 0,						\
 	.stack		= init_stack,					\
 	.usage		= ATOMIC_INIT(2),				\
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 62c68e513e39..abb795afc823 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -1458,6 +1458,13 @@</span> <span class="p_context"> struct tlbflush_unmap_batch {</span>
 };
 
 struct task_struct {
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * For reasons of header soup (see current_thread_info()), this</span>
<span class="p_add">+	 * must be the first element of task_struct.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct thread_info thread_info;</span>
<span class="p_add">+#endif</span>
 	volatile long state;	/* -1 unrunnable, 0 runnable, &gt;0 stopped */
 	void *stack;
 	atomic_t usage;
<span class="p_chunk">@@ -1467,6 +1474,9 @@</span> <span class="p_context"> struct task_struct {</span>
 #ifdef CONFIG_SMP
 	struct llist_node wake_entry;
 	int on_cpu;
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+	unsigned int cpu;	/* current CPU */</span>
<span class="p_add">+#endif</span>
 	unsigned int wakee_flips;
 	unsigned long wakee_flip_decay_ts;
 	struct task_struct *last_wakee;
<span class="p_chunk">@@ -1923,6 +1933,13 @@</span> <span class="p_context"> struct task_struct {</span>
 #ifdef CONFIG_MMU
 	struct task_struct *oom_reaper_list;
 #endif
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+	struct vm_struct *stack_vm_area;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+	/* A live task holds one reference. */</span>
<span class="p_add">+	atomic_t stack_refcount;</span>
<span class="p_add">+#endif</span>
 /* CPU-specific state of this task */
 	struct thread_struct thread;
 /*
<span class="p_chunk">@@ -1939,6 +1956,18 @@</span> <span class="p_context"> extern int arch_task_struct_size __read_mostly;</span>
 # define arch_task_struct_size (sizeof(struct task_struct))
 #endif
 
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return t-&gt;stack_vm_area;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline struct vm_struct *task_stack_vm_area(const struct task_struct *t)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* Future-safe accessor for struct task_struct&#39;s cpus_allowed. */
 #define tsk_cpus_allowed(tsk) (&amp;(tsk)-&gt;cpus_allowed)
 
<span class="p_chunk">@@ -2573,7 +2602,9 @@</span> <span class="p_context"> extern void set_curr_task(int cpu, struct task_struct *p);</span>
 void yield(void);
 
 union thread_union {
<span class="p_add">+#ifndef CONFIG_THREAD_INFO_IN_TASK</span>
 	struct thread_info thread_info;
<span class="p_add">+#endif</span>
 	unsigned long stack[THREAD_SIZE/sizeof(long)];
 };
 
<span class="p_chunk">@@ -3061,10 +3092,34 @@</span> <span class="p_context"> static inline void threadgroup_change_end(struct task_struct *tsk)</span>
 	cgroup_threadgroup_change_end(tsk);
 }
 
<span class="p_del">-#ifndef __HAVE_THREAD_FUNCTIONS</span>
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct thread_info *task_thread_info(struct task_struct *task)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return &amp;task-&gt;thread_info;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * When accessing the stack of a non-current task that might exit, use</span>
<span class="p_add">+ * try_get_task_stack() instead.  task_stack_page will return a pointer</span>
<span class="p_add">+ * that could get freed out from under you.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void *task_stack_page(const struct task_struct *task)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return task-&gt;stack;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#define setup_thread_stack(new,old)	do { } while(0)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline unsigned long *end_of_stack(const struct task_struct *task)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return task-&gt;stack;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#elif !defined(__HAVE_THREAD_FUNCTIONS)</span>
 
 #define task_thread_info(task)	((struct thread_info *)(task)-&gt;stack)
<span class="p_del">-#define task_stack_page(task)	((task)-&gt;stack)</span>
<span class="p_add">+#define task_stack_page(task)	((void *)(task)-&gt;stack)</span>
 
 static inline void setup_thread_stack(struct task_struct *p, struct task_struct *org)
 {
<span class="p_chunk">@@ -3091,6 +3146,24 @@</span> <span class="p_context"> static inline unsigned long *end_of_stack(struct task_struct *p)</span>
 }
 
 #endif
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+static inline void *try_get_task_stack(struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return atomic_inc_not_zero(&amp;tsk-&gt;stack_refcount) ?</span>
<span class="p_add">+		task_stack_page(tsk) : NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern void put_task_stack(struct task_struct *tsk);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void *try_get_task_stack(struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return task_stack_page(tsk);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void put_task_stack(struct task_struct *tsk) {}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #define task_stack_end_corrupted(task) \
 		(*(end_of_stack(task)) != STACK_END_MAGIC)
 
<span class="p_chunk">@@ -3364,7 +3437,11 @@</span> <span class="p_context"> static inline void ptrace_signal_wake_up(struct task_struct *t, bool resume)</span>
 
 static inline unsigned int task_cpu(const struct task_struct *p)
 {
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+	return p-&gt;cpu;</span>
<span class="p_add">+#else</span>
 	return task_thread_info(p)-&gt;cpu;
<span class="p_add">+#endif</span>
 }
 
 static inline int task_node(const struct task_struct *p)
<span class="p_header">diff --git a/include/linux/thread_info.h b/include/linux/thread_info.h</span>
<span class="p_header">index 2b5b10eed74f..45f004e9cc59 100644</span>
<span class="p_header">--- a/include/linux/thread_info.h</span>
<span class="p_header">+++ b/include/linux/thread_info.h</span>
<span class="p_chunk">@@ -13,6 +13,21 @@</span> <span class="p_context"></span>
 struct timespec;
 struct compat_timespec;
 
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+struct thread_info {</span>
<span class="p_add">+	unsigned long		flags;		/* low level flags */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#define INIT_THREAD_INFO(tsk)			\</span>
<span class="p_add">+{						\</span>
<span class="p_add">+	.flags		= 0,			\</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+#define current_thread_info() ((struct thread_info *)current)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * System call restart block.
  */
<span class="p_header">diff --git a/init/Kconfig b/init/Kconfig</span>
<span class="p_header">index cac3f096050d..3b9a47fe843b 100644</span>
<span class="p_header">--- a/init/Kconfig</span>
<span class="p_header">+++ b/init/Kconfig</span>
<span class="p_chunk">@@ -26,6 +26,16 @@</span> <span class="p_context"> config IRQ_WORK</span>
 config BUILDTIME_EXTABLE_SORT
 	bool
 
<span class="p_add">+config THREAD_INFO_IN_TASK</span>
<span class="p_add">+	bool</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Select this to move thread_info off the stack into task_struct.  To</span>
<span class="p_add">+	  make this work, an arch will need to remove all thread_info fields</span>
<span class="p_add">+	  except flags and fix any runtime bugs.</span>
<span class="p_add">+</span>
<span class="p_add">+	  One subtle change that will be needed is to use try_get_task_stack()</span>
<span class="p_add">+	  and put_task_stack() in save_thread_stack_tsk() and get_wchan().</span>
<span class="p_add">+</span>
 menu &quot;General setup&quot;
 
 config BROKEN
<span class="p_header">diff --git a/init/init_task.c b/init/init_task.c</span>
<span class="p_header">index ba0a7f362d9e..11f83be1fa79 100644</span>
<span class="p_header">--- a/init/init_task.c</span>
<span class="p_header">+++ b/init/init_task.c</span>
<span class="p_chunk">@@ -22,5 +22,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(init_task);</span>
  * Initial thread structure. Alignment of this is handled by a special
  * linker map entry.
  */
<span class="p_del">-union thread_union init_thread_union __init_task_data =</span>
<span class="p_del">-	{ INIT_THREAD_INFO(init_task) };</span>
<span class="p_add">+union thread_union init_thread_union __init_task_data = {</span>
<span class="p_add">+#ifndef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+	INIT_THREAD_INFO(init_task)</span>
<span class="p_add">+#endif</span>
<span class="p_add">+};</span>
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index beb31725f7e2..c060c7e7c247 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -158,19 +158,83 @@</span> <span class="p_context"> void __weak arch_release_thread_stack(unsigned long *stack)</span>
  * Allocate pages if THREAD_SIZE is &gt;= PAGE_SIZE, otherwise use a
  * kmemcache based allocator.
  */
<span class="p_del">-# if THREAD_SIZE &gt;= PAGE_SIZE</span>
<span class="p_del">-static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,</span>
<span class="p_del">-						  int node)</span>
<span class="p_add">+# if THREAD_SIZE &gt;= PAGE_SIZE || defined(CONFIG_VMAP_STACK)</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * vmalloc() is a bit slow, and calling vfree() enough times will force a TLB</span>
<span class="p_add">+ * flush.  Try to minimize the number of calls by caching stacks.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define NR_CACHED_STACKS 2</span>
<span class="p_add">+static DEFINE_PER_CPU(struct vm_struct *, cached_stacks[NR_CACHED_STACKS]);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long *alloc_thread_stack_node(struct task_struct *tsk, int node)</span>
 {
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+	void *stack;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	local_irq_disable();</span>
<span class="p_add">+	for (i = 0; i &lt; NR_CACHED_STACKS; i++) {</span>
<span class="p_add">+		struct vm_struct *s = this_cpu_read(cached_stacks[i]);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!s)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		this_cpu_write(cached_stacks[i], NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+		tsk-&gt;stack_vm_area = s;</span>
<span class="p_add">+		local_irq_enable();</span>
<span class="p_add">+		return s-&gt;addr;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	local_irq_enable();</span>
<span class="p_add">+</span>
<span class="p_add">+	stack = __vmalloc_node_range(THREAD_SIZE, THREAD_SIZE,</span>
<span class="p_add">+				     VMALLOC_START, VMALLOC_END,</span>
<span class="p_add">+				     THREADINFO_GFP | __GFP_HIGHMEM,</span>
<span class="p_add">+				     PAGE_KERNEL,</span>
<span class="p_add">+				     0, node, __builtin_return_address(0));</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We can&#39;t call find_vm_area() in interrupt context, and</span>
<span class="p_add">+	 * free_thread_stack() can be called in interrupt context,</span>
<span class="p_add">+	 * so cache the vm_struct.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (stack)</span>
<span class="p_add">+		tsk-&gt;stack_vm_area = find_vm_area(stack);</span>
<span class="p_add">+	return stack;</span>
<span class="p_add">+#else</span>
 	struct page *page = alloc_pages_node(node, THREADINFO_GFP,
 					     THREAD_SIZE_ORDER);
 
 	return page ? page_address(page) : NULL;
<span class="p_add">+#endif</span>
 }
 
<span class="p_del">-static inline void free_thread_stack(unsigned long *stack)</span>
<span class="p_add">+static inline void free_thread_stack(struct task_struct *tsk)</span>
 {
<span class="p_del">-	__free_pages(virt_to_page(stack), THREAD_SIZE_ORDER);</span>
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+	if (task_stack_vm_area(tsk)) {</span>
<span class="p_add">+		unsigned long flags;</span>
<span class="p_add">+		int i;</span>
<span class="p_add">+</span>
<span class="p_add">+		local_irq_save(flags);</span>
<span class="p_add">+		for (i = 0; i &lt; NR_CACHED_STACKS; i++) {</span>
<span class="p_add">+			if (this_cpu_read(cached_stacks[i]))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			this_cpu_write(cached_stacks[i], tsk-&gt;stack_vm_area);</span>
<span class="p_add">+			local_irq_restore(flags);</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		local_irq_restore(flags);</span>
<span class="p_add">+</span>
<span class="p_add">+		vfree(tsk-&gt;stack);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	__free_pages(virt_to_page(tsk-&gt;stack), THREAD_SIZE_ORDER);</span>
 }
 # else
 static struct kmem_cache *thread_stack_cache;
<span class="p_chunk">@@ -181,9 +245,9 @@</span> <span class="p_context"> static unsigned long *alloc_thread_stack_node(struct task_struct *tsk,</span>
 	return kmem_cache_alloc_node(thread_stack_cache, THREADINFO_GFP, node);
 }
 
<span class="p_del">-static void free_thread_stack(unsigned long *stack)</span>
<span class="p_add">+static void free_thread_stack(struct task_struct *tsk)</span>
 {
<span class="p_del">-	kmem_cache_free(thread_stack_cache, stack);</span>
<span class="p_add">+	kmem_cache_free(thread_stack_cache, tsk-&gt;stack);</span>
 }
 
 void thread_stack_cache_init(void)
<span class="p_chunk">@@ -213,24 +277,76 @@</span> <span class="p_context"> struct kmem_cache *vm_area_cachep;</span>
 /* SLAB cache for mm_struct structures (tsk-&gt;mm) */
 static struct kmem_cache *mm_cachep;
 
<span class="p_del">-static void account_kernel_stack(unsigned long *stack, int account)</span>
<span class="p_add">+static void account_kernel_stack(struct task_struct *tsk, int account)</span>
 {
<span class="p_del">-	/* All stack pages are in the same zone and belong to the same memcg. */</span>
<span class="p_del">-	struct page *first_page = virt_to_page(stack);</span>
<span class="p_add">+	void *stack = task_stack_page(tsk);</span>
<span class="p_add">+	struct vm_struct *vm = task_stack_vm_area(tsk);</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(IS_ENABLED(CONFIG_VMAP_STACK) &amp;&amp; PAGE_SIZE % 1024 != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (vm) {</span>
<span class="p_add">+		int i;</span>
 
<span class="p_del">-	mod_zone_page_state(page_zone(first_page), NR_KERNEL_STACK_KB,</span>
<span class="p_del">-			    THREAD_SIZE / 1024 * account);</span>
<span class="p_add">+		BUG_ON(vm-&gt;nr_pages != THREAD_SIZE / PAGE_SIZE);</span>
 
<span class="p_del">-	memcg_kmem_update_page_stat(</span>
<span class="p_del">-		first_page, MEMCG_KERNEL_STACK_KB,</span>
<span class="p_del">-		account * (THREAD_SIZE / 1024));</span>
<span class="p_add">+		for (i = 0; i &lt; THREAD_SIZE / PAGE_SIZE; i++) {</span>
<span class="p_add">+			mod_zone_page_state(page_zone(vm-&gt;pages[i]),</span>
<span class="p_add">+					    NR_KERNEL_STACK_KB,</span>
<span class="p_add">+					    PAGE_SIZE / 1024 * account);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* All stack pages belong to the same memcg. */</span>
<span class="p_add">+		memcg_kmem_update_page_stat(vm-&gt;pages[0], MEMCG_KERNEL_STACK_KB,</span>
<span class="p_add">+					    account * (THREAD_SIZE / 1024));</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * All stack pages are in the same zone and belong to the</span>
<span class="p_add">+		 * same memcg.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		struct page *first_page = virt_to_page(stack);</span>
<span class="p_add">+</span>
<span class="p_add">+		mod_zone_page_state(page_zone(first_page), NR_KERNEL_STACK_KB,</span>
<span class="p_add">+				    THREAD_SIZE / 1024 * account);</span>
<span class="p_add">+</span>
<span class="p_add">+		memcg_kmem_update_page_stat(first_page, MEMCG_KERNEL_STACK_KB,</span>
<span class="p_add">+					    account * (THREAD_SIZE / 1024));</span>
<span class="p_add">+	}</span>
 }
 
<span class="p_del">-void free_task(struct task_struct *tsk)</span>
<span class="p_add">+static void release_task_stack(struct task_struct *tsk)</span>
 {
<span class="p_del">-	account_kernel_stack(tsk-&gt;stack, -1);</span>
<span class="p_add">+	account_kernel_stack(tsk, -1);</span>
 	arch_release_thread_stack(tsk-&gt;stack);
<span class="p_del">-	free_thread_stack(tsk-&gt;stack);</span>
<span class="p_add">+	free_thread_stack(tsk);</span>
<span class="p_add">+	tsk-&gt;stack = NULL;</span>
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+	tsk-&gt;stack_vm_area = NULL;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+void put_task_stack(struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (atomic_dec_and_test(&amp;tsk-&gt;stack_refcount))</span>
<span class="p_add">+		release_task_stack(tsk);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+void free_task(struct task_struct *tsk)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifndef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The task is finally done with both the stack and thread_info,</span>
<span class="p_add">+	 * so free both.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	release_task_stack(tsk);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If the task had a separate stack allocation, it should be gone</span>
<span class="p_add">+	 * by now.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WARN_ON_ONCE(atomic_read(&amp;tsk-&gt;stack_refcount) != 0);</span>
<span class="p_add">+#endif</span>
 	rt_mutex_debug_task_free(tsk);
 	ftrace_graph_exit_task(tsk);
 	put_seccomp_filter(tsk);
<span class="p_chunk">@@ -342,6 +458,7 @@</span> <span class="p_context"> static struct task_struct *dup_task_struct(struct task_struct *orig, int node)</span>
 {
 	struct task_struct *tsk;
 	unsigned long *stack;
<span class="p_add">+	struct vm_struct *stack_vm_area;</span>
 	int err;
 
 	if (node == NUMA_NO_NODE)
<span class="p_chunk">@@ -354,11 +471,26 @@</span> <span class="p_context"> static struct task_struct *dup_task_struct(struct task_struct *orig, int node)</span>
 	if (!stack)
 		goto free_tsk;
 
<span class="p_add">+	stack_vm_area = task_stack_vm_area(tsk);</span>
<span class="p_add">+</span>
 	err = arch_dup_task_struct(tsk, orig);
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * arch_dup_task_struct() clobbers the stack-related fields.  Make</span>
<span class="p_add">+	 * sure they&#39;re properly initialized before using any stack-related</span>
<span class="p_add">+	 * functions again.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	tsk-&gt;stack = stack;</span>
<span class="p_add">+#ifdef CONFIG_VMAP_STACK</span>
<span class="p_add">+	tsk-&gt;stack_vm_area = stack_vm_area;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+	atomic_set(&amp;tsk-&gt;stack_refcount, 1);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 	if (err)
 		goto free_stack;
 
<span class="p_del">-	tsk-&gt;stack = stack;</span>
 #ifdef CONFIG_SECCOMP
 	/*
 	 * We must handle setting up seccomp filters once we&#39;re under
<span class="p_chunk">@@ -390,14 +522,14 @@</span> <span class="p_context"> static struct task_struct *dup_task_struct(struct task_struct *orig, int node)</span>
 	tsk-&gt;task_frag.page = NULL;
 	tsk-&gt;wake_q.next = NULL;
 
<span class="p_del">-	account_kernel_stack(stack, 1);</span>
<span class="p_add">+	account_kernel_stack(tsk, 1);</span>
 
 	kcov_task_init(tsk);
 
 	return tsk;
 
 free_stack:
<span class="p_del">-	free_thread_stack(stack);</span>
<span class="p_add">+	free_thread_stack(tsk);</span>
 free_tsk:
 	free_task_struct(tsk);
 	return NULL;
<span class="p_chunk">@@ -1715,6 +1847,7 @@</span> <span class="p_context"> static struct task_struct *copy_process(unsigned long clone_flags,</span>
 	atomic_dec(&amp;p-&gt;cred-&gt;user-&gt;processes);
 	exit_creds(p);
 bad_fork_free:
<span class="p_add">+	put_task_stack(p);</span>
 	free_task(p);
 fork_out:
 	return ERR_PTR(retval);
<span class="p_header">diff --git a/kernel/kthread.c b/kernel/kthread.c</span>
<span class="p_header">index 9ff173dca1ae..4ab4c3766a80 100644</span>
<span class="p_header">--- a/kernel/kthread.c</span>
<span class="p_header">+++ b/kernel/kthread.c</span>
<span class="p_chunk">@@ -64,7 +64,7 @@</span> <span class="p_context"> static inline struct kthread *to_kthread(struct task_struct *k)</span>
 static struct kthread *to_live_kthread(struct task_struct *k)
 {
 	struct completion *vfork = ACCESS_ONCE(k-&gt;vfork_done);
<span class="p_del">-	if (likely(vfork))</span>
<span class="p_add">+	if (likely(vfork) &amp;&amp; try_get_task_stack(k))</span>
 		return __to_kthread(vfork);
 	return NULL;
 }
<span class="p_chunk">@@ -425,8 +425,10 @@</span> <span class="p_context"> void kthread_unpark(struct task_struct *k)</span>
 {
 	struct kthread *kthread = to_live_kthread(k);
 
<span class="p_del">-	if (kthread)</span>
<span class="p_add">+	if (kthread) {</span>
 		__kthread_unpark(k, kthread);
<span class="p_add">+		put_task_stack(k);</span>
<span class="p_add">+	}</span>
 }
 EXPORT_SYMBOL_GPL(kthread_unpark);
 
<span class="p_chunk">@@ -455,6 +457,7 @@</span> <span class="p_context"> int kthread_park(struct task_struct *k)</span>
 				wait_for_completion(&amp;kthread-&gt;parked);
 			}
 		}
<span class="p_add">+		put_task_stack(k);</span>
 		ret = 0;
 	}
 	return ret;
<span class="p_chunk">@@ -490,6 +493,7 @@</span> <span class="p_context"> int kthread_stop(struct task_struct *k)</span>
 		__kthread_unpark(k, kthread);
 		wake_up_process(k);
 		wait_for_completion(&amp;kthread-&gt;exited);
<span class="p_add">+		put_task_stack(k);</span>
 	}
 	ret = k-&gt;exit_code;
 	put_task_struct(k);
<span class="p_header">diff --git a/kernel/sched/core.c b/kernel/sched/core.c</span>
<span class="p_header">index 44817c640e99..23c6037e2d89 100644</span>
<span class="p_header">--- a/kernel/sched/core.c</span>
<span class="p_header">+++ b/kernel/sched/core.c</span>
<span class="p_chunk">@@ -2772,6 +2772,10 @@</span> <span class="p_context"> static struct rq *finish_task_switch(struct task_struct *prev)</span>
 		 * task and put them back on the free list.
 		 */
 		kprobe_flush_task(prev);
<span class="p_add">+</span>
<span class="p_add">+		/* Task is done with its stack. */</span>
<span class="p_add">+		put_task_stack(prev);</span>
<span class="p_add">+</span>
 		put_task_struct(prev);
 	}
 
<span class="p_chunk">@@ -3403,7 +3407,6 @@</span> <span class="p_context"> static void __sched notrace __schedule(bool preempt)</span>
 
 	balance_callback(rq);
 }
<span class="p_del">-STACK_FRAME_NON_STANDARD(__schedule); /* switch_to() */</span>
 
 static inline void sched_submit_work(struct task_struct *tsk)
 {
<span class="p_header">diff --git a/kernel/sched/sched.h b/kernel/sched/sched.h</span>
<span class="p_header">index c64fc5114004..3655c9625e5b 100644</span>
<span class="p_header">--- a/kernel/sched/sched.h</span>
<span class="p_header">+++ b/kernel/sched/sched.h</span>
<span class="p_chunk">@@ -1000,7 +1000,11 @@</span> <span class="p_context"> static inline void __set_task_cpu(struct task_struct *p, unsigned int cpu)</span>
 	 * per-task data have been completed by this moment.
 	 */
 	smp_wmb();
<span class="p_add">+#ifdef CONFIG_THREAD_INFO_IN_TASK</span>
<span class="p_add">+	p-&gt;cpu = cpu;</span>
<span class="p_add">+#else</span>
 	task_thread_info(p)-&gt;cpu = cpu;
<span class="p_add">+#endif</span>
 	p-&gt;wake_cpu = cpu;
 #endif
 }
<span class="p_header">diff --git a/kernel/trace/Kconfig b/kernel/trace/Kconfig</span>
<span class="p_header">index f4b86e8ca1e7..ba3326785ca4 100644</span>
<span class="p_header">--- a/kernel/trace/Kconfig</span>
<span class="p_header">+++ b/kernel/trace/Kconfig</span>
<span class="p_chunk">@@ -24,11 +24,6 @@</span> <span class="p_context"> config HAVE_FUNCTION_GRAPH_TRACER</span>
 	help
 	  See Documentation/trace/ftrace-design.txt
 
<span class="p_del">-config HAVE_FUNCTION_GRAPH_FP_TEST</span>
<span class="p_del">-	bool</span>
<span class="p_del">-	help</span>
<span class="p_del">-	  See Documentation/trace/ftrace-design.txt</span>
<span class="p_del">-</span>
 config HAVE_DYNAMIC_FTRACE
 	bool
 	help
<span class="p_header">diff --git a/kernel/trace/trace_functions_graph.c b/kernel/trace/trace_functions_graph.c</span>
<span class="p_header">index 7363ccf79512..0cbe38a844fa 100644</span>
<span class="p_header">--- a/kernel/trace/trace_functions_graph.c</span>
<span class="p_header">+++ b/kernel/trace/trace_functions_graph.c</span>
<span class="p_chunk">@@ -119,7 +119,7 @@</span> <span class="p_context"> print_graph_duration(struct trace_array *tr, unsigned long long duration,</span>
 /* Add a function return address to the trace stack on thread info.*/
 int
 ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,
<span class="p_del">-			 unsigned long frame_pointer)</span>
<span class="p_add">+			 unsigned long frame_pointer, unsigned long *retp)</span>
 {
 	unsigned long long calltime;
 	int index;
<span class="p_chunk">@@ -171,7 +171,12 @@</span> <span class="p_context"> ftrace_push_return_trace(unsigned long ret, unsigned long func, int *depth,</span>
 	current-&gt;ret_stack[index].func = func;
 	current-&gt;ret_stack[index].calltime = calltime;
 	current-&gt;ret_stack[index].subtime = 0;
<span class="p_add">+#ifdef HAVE_FUNCTION_GRAPH_FP_TEST</span>
 	current-&gt;ret_stack[index].fp = frame_pointer;
<span class="p_add">+#endif</span>
<span class="p_add">+#ifdef HAVE_FUNCTION_GRAPH_RET_ADDR_PTR</span>
<span class="p_add">+	current-&gt;ret_stack[index].retp = retp;</span>
<span class="p_add">+#endif</span>
 	*depth = current-&gt;curr_ret_stack;
 
 	return 0;
<span class="p_chunk">@@ -204,7 +209,7 @@</span> <span class="p_context"> ftrace_pop_return_trace(struct ftrace_graph_ret *trace, unsigned long *ret,</span>
 		return;
 	}
 
<span class="p_del">-#if defined(CONFIG_HAVE_FUNCTION_GRAPH_FP_TEST) &amp;&amp; !defined(CC_USING_FENTRY)</span>
<span class="p_add">+#ifdef HAVE_FUNCTION_GRAPH_FP_TEST</span>
 	/*
 	 * The arch may choose to record the frame pointer used
 	 * and check it here to make sure that it is what we expect it
<span class="p_chunk">@@ -279,6 +284,64 @@</span> <span class="p_context"> unsigned long ftrace_return_to_handler(unsigned long frame_pointer)</span>
 	return ret;
 }
 
<span class="p_add">+/**</span>
<span class="p_add">+ * ftrace_graph_ret_addr - convert a potentially modified stack return address</span>
<span class="p_add">+ *			   to its original value</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This function can be called by stack unwinding code to convert a found stack</span>
<span class="p_add">+ * return address (&#39;ret&#39;) to its original value, in case the function graph</span>
<span class="p_add">+ * tracer has modified it to be &#39;return_to_handler&#39;.  If the address hasn&#39;t</span>
<span class="p_add">+ * been modified, the unchanged value of &#39;ret&#39; is returned.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * &#39;idx&#39; is a state variable which should be initialized by the caller to zero</span>
<span class="p_add">+ * before the first call.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * &#39;retp&#39; is a pointer to the return address on the stack.  It&#39;s ignored if</span>
<span class="p_add">+ * the arch doesn&#39;t have HAVE_FUNCTION_GRAPH_RET_ADDR_PTR defined.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef HAVE_FUNCTION_GRAPH_RET_ADDR_PTR</span>
<span class="p_add">+unsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,</span>
<span class="p_add">+				    unsigned long ret, unsigned long *retp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int index = task-&gt;curr_ret_stack;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ret != (unsigned long)return_to_handler)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (index &lt; -1)</span>
<span class="p_add">+		index += FTRACE_NOTRACE_DEPTH;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (index &lt; 0)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt;= index; i++)</span>
<span class="p_add">+		if (task-&gt;ret_stack[i].retp == retp)</span>
<span class="p_add">+			return task-&gt;ret_stack[i].ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+#else /* !HAVE_FUNCTION_GRAPH_RET_ADDR_PTR */</span>
<span class="p_add">+unsigned long ftrace_graph_ret_addr(struct task_struct *task, int *idx,</span>
<span class="p_add">+				    unsigned long ret, unsigned long *retp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int task_idx;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (ret != (unsigned long)return_to_handler)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	task_idx = task-&gt;curr_ret_stack;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!task-&gt;ret_stack || task_idx &lt; *idx)</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	task_idx -= *idx;</span>
<span class="p_add">+	(*idx)++;</span>
<span class="p_add">+</span>
<span class="p_add">+	return task-&gt;ret_stack[task_idx].ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* HAVE_FUNCTION_GRAPH_RET_ADDR_PTR */</span>
<span class="p_add">+</span>
 int __trace_graph_entry(struct trace_array *tr,
 				struct ftrace_graph_ent *trace,
 				unsigned long flags,
<span class="p_header">diff --git a/lib/dma-debug.c b/lib/dma-debug.c</span>
<span class="p_header">index fcfa1939ac41..06f02f6aecd2 100644</span>
<span class="p_header">--- a/lib/dma-debug.c</span>
<span class="p_header">+++ b/lib/dma-debug.c</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/stacktrace.h&gt;
 #include &lt;linux/dma-debug.h&gt;
 #include &lt;linux/spinlock.h&gt;
<span class="p_add">+#include &lt;linux/vmalloc.h&gt;</span>
 #include &lt;linux/debugfs.h&gt;
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/export.h&gt;
<span class="p_chunk">@@ -1164,11 +1165,32 @@</span> <span class="p_context"> static void check_unmap(struct dma_debug_entry *ref)</span>
 	put_hash_bucket(bucket, &amp;flags);
 }
 
<span class="p_del">-static void check_for_stack(struct device *dev, void *addr)</span>
<span class="p_add">+static void check_for_stack(struct device *dev,</span>
<span class="p_add">+			    struct page *page, size_t offset)</span>
 {
<span class="p_del">-	if (object_is_on_stack(addr))</span>
<span class="p_del">-		err_printk(dev, NULL, &quot;DMA-API: device driver maps memory from &quot;</span>
<span class="p_del">-				&quot;stack [addr=%p]\n&quot;, addr);</span>
<span class="p_add">+	void *addr;</span>
<span class="p_add">+	struct vm_struct *stack_vm_area = task_stack_vm_area(current);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!stack_vm_area) {</span>
<span class="p_add">+		/* Stack is direct-mapped. */</span>
<span class="p_add">+		if (PageHighMem(page))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		addr = page_address(page) + offset;</span>
<span class="p_add">+		if (object_is_on_stack(addr))</span>
<span class="p_add">+			err_printk(dev, NULL, &quot;DMA-API: device driver maps memory from stack [addr=%p]\n&quot;, addr);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* Stack is vmalloced. */</span>
<span class="p_add">+		int i;</span>
<span class="p_add">+</span>
<span class="p_add">+		for (i = 0; i &lt; stack_vm_area-&gt;nr_pages; i++) {</span>
<span class="p_add">+			if (page != stack_vm_area-&gt;pages[i])</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			addr = (u8 *)current-&gt;stack + i * PAGE_SIZE + offset;</span>
<span class="p_add">+			err_printk(dev, NULL, &quot;DMA-API: device driver maps memory from stack [probable addr=%p]\n&quot;, addr);</span>
<span class="p_add">+			break;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
 }
 
 static inline bool overlap(void *addr, unsigned long len, void *start, void *end)
<span class="p_chunk">@@ -1291,10 +1313,11 @@</span> <span class="p_context"> void debug_dma_map_page(struct device *dev, struct page *page, size_t offset,</span>
 	if (map_single)
 		entry-&gt;type = dma_debug_single;
 
<span class="p_add">+	check_for_stack(dev, page, offset);</span>
<span class="p_add">+</span>
 	if (!PageHighMem(page)) {
 		void *addr = page_address(page) + offset;
 
<span class="p_del">-		check_for_stack(dev, addr);</span>
 		check_for_illegal_area(dev, addr, size);
 	}
 
<span class="p_chunk">@@ -1386,8 +1409,9 @@</span> <span class="p_context"> void debug_dma_map_sg(struct device *dev, struct scatterlist *sg,</span>
 		entry-&gt;sg_call_ents   = nents;
 		entry-&gt;sg_mapped_ents = mapped_ents;
 
<span class="p_add">+		check_for_stack(dev, sg_page(s), s-&gt;offset);</span>
<span class="p_add">+</span>
 		if (!PageHighMem(sg_page(s))) {
<span class="p_del">-			check_for_stack(dev, sg_virt(s));</span>
 			check_for_illegal_area(dev, sg_virt(s), sg_dma_len(s));
 		}
 
<span class="p_header">diff --git a/lib/syscall.c b/lib/syscall.c</span>
<span class="p_header">index e30e03932480..63239e097b13 100644</span>
<span class="p_header">--- a/lib/syscall.c</span>
<span class="p_header">+++ b/lib/syscall.c</span>
<span class="p_chunk">@@ -7,9 +7,19 @@</span> <span class="p_context"> static int collect_syscall(struct task_struct *target, long *callno,</span>
 			   unsigned long args[6], unsigned int maxargs,
 			   unsigned long *sp, unsigned long *pc)
 {
<span class="p_del">-	struct pt_regs *regs = task_pt_regs(target);</span>
<span class="p_del">-	if (unlikely(!regs))</span>
<span class="p_add">+	struct pt_regs *regs;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!try_get_task_stack(target)) {</span>
<span class="p_add">+		/* Task has no stack, so the task isn&#39;t in a syscall. */</span>
<span class="p_add">+		*callno = -1;</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	regs = task_pt_regs(target);</span>
<span class="p_add">+	if (unlikely(!regs)) {</span>
<span class="p_add">+		put_task_stack(target);</span>
 		return -EAGAIN;
<span class="p_add">+	}</span>
 
 	*sp = user_stack_pointer(regs);
 	*pc = instruction_pointer(regs);
<span class="p_chunk">@@ -18,6 +28,7 @@</span> <span class="p_context"> static int collect_syscall(struct task_struct *target, long *callno,</span>
 	if (*callno != -1L &amp;&amp; maxargs &gt; 0)
 		syscall_get_arguments(target, regs, 0, maxargs, args);
 
<span class="p_add">+	put_task_stack(target);</span>
 	return 0;
 }
 
<span class="p_header">diff --git a/tools/testing/selftests/x86/ptrace_syscall.c b/tools/testing/selftests/x86/ptrace_syscall.c</span>
<span class="p_header">index 421456784bc6..b037ce9cf116 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/ptrace_syscall.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/ptrace_syscall.c</span>
<span class="p_chunk">@@ -147,7 +147,7 @@</span> <span class="p_context"> static void test_sys32_regs(void (*do_syscall)(struct syscall_args32 *))</span>
 	if (args.nr != getpid() ||
 	    args.arg0 != 10 || args.arg1 != 11 || args.arg2 != 12 ||
 	    args.arg3 != 13 || args.arg4 != 14 || args.arg5 != 15) {
<span class="p_del">-		printf(&quot;[FAIL]\tgetpid() failed to preseve regs\n&quot;);</span>
<span class="p_add">+		printf(&quot;[FAIL]\tgetpid() failed to preserve regs\n&quot;);</span>
 		nerrs++;
 	} else {
 		printf(&quot;[OK]\tgetpid() preserves regs\n&quot;);
<span class="p_chunk">@@ -162,7 +162,7 @@</span> <span class="p_context"> static void test_sys32_regs(void (*do_syscall)(struct syscall_args32 *))</span>
 	if (args.nr != 0 ||
 	    args.arg0 != getpid() || args.arg1 != SIGUSR1 || args.arg2 != 12 ||
 	    args.arg3 != 13 || args.arg4 != 14 || args.arg5 != 15) {
<span class="p_del">-		printf(&quot;[FAIL]\tkill(getpid(), SIGUSR1) failed to preseve regs\n&quot;);</span>
<span class="p_add">+		printf(&quot;[FAIL]\tkill(getpid(), SIGUSR1) failed to preserve regs\n&quot;);</span>
 		nerrs++;
 	} else {
 		printf(&quot;[OK]\tkill(getpid(), SIGUSR1) preserves regs\n&quot;);
<span class="p_header">diff --git a/tools/testing/selftests/x86/sigreturn.c b/tools/testing/selftests/x86/sigreturn.c</span>
<span class="p_header">index 8a577e7070c6..246145b84a12 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/sigreturn.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/sigreturn.c</span>
<span class="p_chunk">@@ -106,7 +106,7 @@</span> <span class="p_context"> asm (&quot;.pushsection .text\n\t&quot;</span>
      &quot;.type int3, @function\n\t&quot;
      &quot;.align 4096\n\t&quot;
      &quot;int3:\n\t&quot;
<span class="p_del">-     &quot;mov %ss,%eax\n\t&quot;</span>
<span class="p_add">+     &quot;mov %ss,%ecx\n\t&quot;</span>
      &quot;int3\n\t&quot;
      &quot;.size int3, . - int3\n\t&quot;
      &quot;.align 4096, 0xcc\n\t&quot;
<span class="p_chunk">@@ -306,7 +306,7 @@</span> <span class="p_context"> static volatile sig_atomic_t sig_corrupt_final_ss;</span>
 #ifdef __x86_64__
 # define REG_IP REG_RIP
 # define REG_SP REG_RSP
<span class="p_del">-# define REG_AX REG_RAX</span>
<span class="p_add">+# define REG_CX REG_RCX</span>
 
 struct selectors {
 	unsigned short cs, gs, fs, ss;
<span class="p_chunk">@@ -326,7 +326,7 @@</span> <span class="p_context"> static unsigned short *csptr(ucontext_t *ctx)</span>
 #else
 # define REG_IP REG_EIP
 # define REG_SP REG_ESP
<span class="p_del">-# define REG_AX REG_EAX</span>
<span class="p_add">+# define REG_CX REG_ECX</span>
 
 static greg_t *ssptr(ucontext_t *ctx)
 {
<span class="p_chunk">@@ -457,10 +457,10 @@</span> <span class="p_context"> static void sigusr1(int sig, siginfo_t *info, void *ctx_void)</span>
 	ctx-&gt;uc_mcontext.gregs[REG_IP] =
 		sig_cs == code16_sel ? 0 : (unsigned long)&amp;int3;
 	ctx-&gt;uc_mcontext.gregs[REG_SP] = (unsigned long)0x8badf00d5aadc0deULL;
<span class="p_del">-	ctx-&gt;uc_mcontext.gregs[REG_AX] = 0;</span>
<span class="p_add">+	ctx-&gt;uc_mcontext.gregs[REG_CX] = 0;</span>
 
 	memcpy(&amp;requested_regs, &amp;ctx-&gt;uc_mcontext.gregs, sizeof(gregset_t));
<span class="p_del">-	requested_regs[REG_AX] = *ssptr(ctx);	/* The asm code does this. */</span>
<span class="p_add">+	requested_regs[REG_CX] = *ssptr(ctx);	/* The asm code does this. */</span>
 
 	return;
 }
<span class="p_chunk">@@ -482,7 +482,7 @@</span> <span class="p_context"> static void sigtrap(int sig, siginfo_t *info, void *ctx_void)</span>
 	unsigned short ss;
 	asm (&quot;mov %%ss,%0&quot; : &quot;=r&quot; (ss));
 
<span class="p_del">-	greg_t asm_ss = ctx-&gt;uc_mcontext.gregs[REG_AX];</span>
<span class="p_add">+	greg_t asm_ss = ctx-&gt;uc_mcontext.gregs[REG_CX];</span>
 	if (asm_ss != sig_ss &amp;&amp; sig == SIGTRAP) {
 		/* Sanity check failure. */
 		printf(&quot;[FAIL]\tSIGTRAP: ss = %hx, frame ss = %hx, ax = %llx\n&quot;,
<span class="p_chunk">@@ -654,8 +654,8 @@</span> <span class="p_context"> static int test_valid_sigreturn(int cs_bits, bool use_16bit_ss, int force_ss)</span>
 #endif
 
 		/* Sanity check on the kernel */
<span class="p_del">-		if (i == REG_AX &amp;&amp; requested_regs[i] != resulting_regs[i]) {</span>
<span class="p_del">-			printf(&quot;[FAIL]\tAX (saved SP) mismatch: requested 0x%llx; got 0x%llx\n&quot;,</span>
<span class="p_add">+		if (i == REG_CX &amp;&amp; requested_regs[i] != resulting_regs[i]) {</span>
<span class="p_add">+			printf(&quot;[FAIL]\tCX (saved SP) mismatch: requested 0x%llx; got 0x%llx\n&quot;,</span>
 			       (unsigned long long)requested_regs[i],
 			       (unsigned long long)resulting_regs[i]);
 			nerrs++;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



