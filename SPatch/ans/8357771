
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,2/3] iommu: dma-iommu: move IOMMU/DMA-mapping code from ARM64 arch to drivers - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,2/3] iommu: dma-iommu: move IOMMU/DMA-mapping code from ARM64 arch to drivers</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2061">Marek Szyprowski</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 19, 2016, 8:22 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1455870164-25337-3-git-send-email-m.szyprowski@samsung.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8357771/mbox/"
   >mbox</a>
|
   <a href="/patch/8357771/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8357771/">/patch/8357771/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 73A249F372
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 19 Feb 2016 08:23:56 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id EBFE52038D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 19 Feb 2016 08:23:53 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 59FB120412
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 19 Feb 2016 08:23:49 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1427250AbcBSIXm (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 19 Feb 2016 03:23:42 -0500
Received: from mailout1.w1.samsung.com ([210.118.77.11]:24823 &quot;EHLO
	mailout1.w1.samsung.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1426938AbcBSIXD (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 19 Feb 2016 03:23:03 -0500
Received: from eucpsbgm2.samsung.com (unknown [203.254.199.245])
	by mailout1.w1.samsung.com
	(Oracle Communications Messaging Server 7.0.5.31.0 64bit (built May 5
	2014)) with ESMTP id &lt;0O2S0069SCMC7L90@mailout1.w1.samsung.com&gt; for
	linux-kernel@vger.kernel.org; Fri, 19 Feb 2016 08:23:00 +0000 (GMT)
X-AuditID: cbfec7f5-f79b16d000005389-8e-56c6d0e34dd3
Received: from eusync3.samsung.com ( [203.254.199.213])
	by eucpsbgm2.samsung.com (EUCPMTA) with SMTP id AA.64.21385.3E0D6C65;
	Fri, 19 Feb 2016 08:22:59 +0000 (GMT)
Received: from amdc1339.digital.local ([106.116.147.30])
	by eusync3.samsung.com (Oracle Communications Messaging Server
	7.0.5.31.0 64bit (built May  5 2014))
	with ESMTPA id &lt;0O2S00GAZCM67900@eusync3.samsung.com&gt;; Fri,
	19 Feb 2016 08:22:59 +0000 (GMT)
From: Marek Szyprowski &lt;m.szyprowski@samsung.com&gt;
To: iommu@lists.linux-foundation.org,
	linux-arm-kernel@lists.infradead.org, linux-kernel@vger.kernel.org
Cc: Marek Szyprowski &lt;m.szyprowski@samsung.com&gt;,
	linaro-mm-sig@lists.linaro.org, dri-devel@lists.freedesktop.org,
	Arnd Bergmann &lt;arnd@arndb.de&gt;, Will Deacon &lt;will.deacon@arm.com&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	Robin Murphy &lt;robin.murphy@arm.com&gt;,
	Russell King - ARM Linux &lt;linux@arm.linux.org.uk&gt;,
	Joerg Roedel &lt;joro@8bytes.org&gt;,
	Laurent Pinchart &lt;laurent.pinchart@ideasonboard.com&gt;,
	Sakari Ailus &lt;sakari.ailus@iki.fi&gt;, Mark Yao &lt;mark.yao@rock-chips.com&gt;,
	Heiko Stuebner &lt;heiko@sntech.de&gt;, Tomasz Figa &lt;tfiga@chromium.org&gt;,
	Inki Dae &lt;inki.dae@samsung.com&gt;,
	Bartlomiej Zolnierkiewicz &lt;b.zolnierkie@samsung.com&gt;,
	Krzysztof Kozlowski &lt;k.kozlowski@samsung.com&gt;
Subject: [RFC 2/3] iommu: dma-iommu: move IOMMU/DMA-mapping code from ARM64
	arch to drivers
Date: Fri, 19 Feb 2016 09:22:43 +0100
Message-id: &lt;1455870164-25337-3-git-send-email-m.szyprowski@samsung.com&gt;
X-Mailer: git-send-email 1.9.2
In-reply-to: &lt;1455870164-25337-1-git-send-email-m.szyprowski@samsung.com&gt;
References: &lt;1455870164-25337-1-git-send-email-m.szyprowski@samsung.com&gt;
X-Brightmail-Tracker: H4sIAAAAAAAAA+NgFmphkeLIzCtJLcpLzFFi42I5/e/4Vd3HF46FGXRfM7T4O+kYu8XGGetZ
	Ld4v62G0uPL1PZvF/0evWS0m3Z/AYrFgv7VF5+wN7BavXxhadE5cwm7x5cpDJotNj6+xWlze
	NYfN4vZlXou1R+6yW+yYcoDJ4uCHJ6wWZ/avZLP43PqPzeLlxxMsDiIeTw7OY/JYM28No0dL
	cw+bx+9fkxg9ZjdcZPGY3TGT1ePw14UsHve7jzN5bF5S73H732Nmj8k3ljN6/J21n8Wjb8sq
	Ro/t1+Yxe3zeJBfAH8Vlk5Kak1mWWqRvl8CV8eDRctaCafOZKtYvnsPewPj0JWMXIweHhICJ
	RPcz3i5GTiBTTOLCvfVsXYxcHEICSxkl7p7cxwrhNDFJXPtxnB2kik3AUKLrbRcbiC0ikCWx
	//5GJpAiZoGtrBKvW9YzgiSEBWIlrl59zgxiswioSqw+vpYFZBuvgIfE1c4UiG1yEv9frmAC
	sTkFPCVOv14GNlMIqOT/3WXMExh5FzAyrGIUTS1NLihOSs810itOzC0uzUvXS87P3cQIiZuv
	OxiXHrM6xCjAwajEw1uhdyxMiDWxrLgy9xCjBAezkgivrj9QiDclsbIqtSg/vqg0J7X4EKM0
	B4uSOO/MXe9DhATSE0tSs1NTC1KLYLJMHJxSDYx+b/x+/XUPk6/wT/uT0a7MPq+P36JPcnqz
	cW4vc6eL1s/tkrOjZS/d9X2yaUqAsFKzd7fAbqPAKInzB87cvj1b6+h/hruNexe98Gfa+a73
	TRdXZPmDpLsmCeKWMwP+qWQ5/L5+0WhnfVpgunbL2tln87dantpuqSBffvXIb6lF5vrXV/x4
	3a3EUpyRaKjFXFScCAAmSIUTlwIAAA==
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2061">Marek Szyprowski</a> - Feb. 19, 2016, 8:22 a.m.</div>
<pre class="content">
This patch moves all the IOMMU-based DMA-mapping code from arch/arm64/mm
to drivers/iommu/dma-iommu-ops.c. This way it can be easily shared with
ARM architecture, which will also use them.
<span class="signed-off-by">
Signed-off-by: Marek Szyprowski &lt;m.szyprowski@samsung.com&gt;</span>
---
 arch/arm64/include/asm/dma-mapping.h |  39 ++-
 arch/arm64/mm/dma-mapping.c          | 491 ++---------------------------------
 drivers/iommu/Makefile               |   2 +-
 drivers/iommu/dma-iommu-ops.c        | 471 +++++++++++++++++++++++++++++++++
 include/linux/dma-iommu.h            |  14 +
 5 files changed, 538 insertions(+), 479 deletions(-)
 create mode 100644 drivers/iommu/dma-iommu-ops.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=101671">yao mark</a> - April 18, 2016, 2:20 a.m.</div>
<pre class="content">
On 2016?02?19? 16:22, Marek Szyprowski wrote:
<span class="quote">&gt; This patch moves all the IOMMU-based DMA-mapping code from arch/arm64/mm</span>
<span class="quote">&gt; to drivers/iommu/dma-iommu-ops.c. This way it can be easily shared with</span>
<span class="quote">&gt; ARM architecture, which will also use them.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Marek Szyprowski&lt;m.szyprowski@samsung.com&gt;</span>
IOMMU works good on drm/rockchip ARM64 platform with this patch. So
<span class="tested-by">
Tested-by: Mark Yao &lt;mark.yao@rock-chips.com&gt;</span>

Thanks.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/include/asm/dma-mapping.h b/arch/arm64/include/asm/dma-mapping.h</span>
<span class="p_header">index ba437f090a74..3a582d820717 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/dma-mapping.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/dma-mapping.h</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/vmalloc.h&gt;
 
 #include &lt;xen/xen.h&gt;
<span class="p_add">+#include &lt;asm/cacheflush.h&gt;</span>
 #include &lt;asm/xen/hypervisor.h&gt;
 
 #define DMA_ERROR_CODE	(~(dma_addr_t)0)
<span class="p_chunk">@@ -47,14 +48,17 @@</span> <span class="p_context"> static inline struct dma_map_ops *get_dma_ops(struct device *dev)</span>
 		return __generic_dma_ops(dev);
 }
 
<span class="p_add">+static inline void arch_set_dma_ops(struct device *dev, struct dma_map_ops *ops)</span>
<span class="p_add">+{</span>
<span class="p_add">+	dev-&gt;archdata.dma_ops = ops;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,
 			struct iommu_ops *iommu, bool coherent);
 #define arch_setup_dma_ops	arch_setup_dma_ops
 
<span class="p_del">-#ifdef CONFIG_IOMMU_DMA</span>
 void arch_teardown_dma_ops(struct device *dev);
 #define arch_teardown_dma_ops	arch_teardown_dma_ops
<span class="p_del">-#endif</span>
 
 /* do not use this function in a driver */
 static inline bool is_device_dma_coherent(struct device *dev)
<span class="p_chunk">@@ -86,5 +90,36 @@</span> <span class="p_context"> static inline void dma_mark_clean(void *addr, size_t size)</span>
 {
 }
 
<span class="p_add">+static inline void arch_flush_page(struct device *dev, const void *virt,</span>
<span class="p_add">+				   phys_addr_t phys)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__dma_flush_range(virt, virt + PAGE_SIZE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_dma_map_area(phys_addr_t phys, size_t size,</span>
<span class="p_add">+				     enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__dma_map_area(phys_to_virt(phys), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void arch_dma_unmap_area(phys_addr_t phys, size_t size,</span>
<span class="p_add">+				       enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__dma_unmap_area(phys_to_virt(phys), size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgprot_t arch_get_dma_pgprot(struct dma_attrs *attrs,</span>
<span class="p_add">+					pgprot_t prot, bool coherent)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!coherent || dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs))</span>
<span class="p_add">+		return pgprot_writecombine(prot);</span>
<span class="p_add">+	return prot;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+extern void *arch_alloc_from_atomic_pool(size_t size, struct page **ret_page,</span>
<span class="p_add">+					 gfp_t flags);</span>
<span class="p_add">+extern bool arch_in_atomic_pool(void *start, size_t size);</span>
<span class="p_add">+extern int arch_free_from_atomic_pool(void *start, size_t size);</span>
<span class="p_add">+</span>
 #endif	/* __KERNEL__ */
 #endif	/* __ASM_DMA_MAPPING_H */
<span class="p_header">diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">index a6e757cbab77..d8cb8552bbff 100644</span>
<span class="p_header">--- a/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -24,19 +24,12 @@</span> <span class="p_context"></span>
 #include &lt;linux/genalloc.h&gt;
 #include &lt;linux/dma-mapping.h&gt;
 #include &lt;linux/dma-contiguous.h&gt;
<span class="p_add">+#include &lt;linux/dma-iommu.h&gt;</span>
 #include &lt;linux/vmalloc.h&gt;
 #include &lt;linux/swiotlb.h&gt;
 
 #include &lt;asm/cacheflush.h&gt;
 
<span class="p_del">-static pgprot_t __get_dma_pgprot(struct dma_attrs *attrs, pgprot_t prot,</span>
<span class="p_del">-				 bool coherent)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!coherent || dma_get_attr(DMA_ATTR_WRITE_COMBINE, attrs))</span>
<span class="p_del">-		return pgprot_writecombine(prot);</span>
<span class="p_del">-	return prot;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static struct gen_pool *atomic_pool;
 
 #define DEFAULT_DMA_COHERENT_POOL_SIZE  SZ_256K
<span class="p_chunk">@@ -49,7 +42,7 @@</span> <span class="p_context"> static int __init early_coherent_pool(char *p)</span>
 }
 early_param(&quot;coherent_pool&quot;, early_coherent_pool);
 
<span class="p_del">-static void *__alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)</span>
<span class="p_add">+void *arch_alloc_from_atomic_pool(size_t size, struct page **ret_page, gfp_t flags)</span>
 {
 	unsigned long val;
 	void *ptr = NULL;
<span class="p_chunk">@@ -71,14 +64,14 @@</span> <span class="p_context"> static void *__alloc_from_pool(size_t size, struct page **ret_page, gfp_t flags)</span>
 	return ptr;
 }
 
<span class="p_del">-static bool __in_atomic_pool(void *start, size_t size)</span>
<span class="p_add">+bool arch_in_atomic_pool(void *start, size_t size)</span>
 {
 	return addr_in_gen_pool(atomic_pool, (unsigned long)start, size);
 }
 
<span class="p_del">-static int __free_from_pool(void *start, size_t size)</span>
<span class="p_add">+int arch_free_from_atomic_pool(void *start, size_t size)</span>
 {
<span class="p_del">-	if (!__in_atomic_pool(start, size))</span>
<span class="p_add">+	if (!arch_in_atomic_pool(start, size))</span>
 		return 0;
 
 	gen_pool_free(atomic_pool, (unsigned long)start, size);
<span class="p_chunk">@@ -142,13 +135,13 @@</span> <span class="p_context"> static void *__dma_alloc(struct device *dev, size_t size,</span>
 	struct page *page;
 	void *ptr, *coherent_ptr;
 	bool coherent = is_device_dma_coherent(dev);
<span class="p_del">-	pgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL, false);</span>
<span class="p_add">+	pgprot_t prot = arch_get_dma_pgprot(attrs, PAGE_KERNEL, false);</span>
 
 	size = PAGE_ALIGN(size);
 
 	if (!coherent &amp;&amp; !gfpflags_allow_blocking(flags)) {
 		struct page *page = NULL;
<span class="p_del">-		void *addr = __alloc_from_pool(size, &amp;page, flags);</span>
<span class="p_add">+		void *addr = arch_alloc_from_atomic_pool(size, &amp;page, flags);</span>
 
 		if (addr)
 			*dma_handle = phys_to_dma(dev, page_to_phys(page));
<span class="p_chunk">@@ -192,7 +185,7 @@</span> <span class="p_context"> static void __dma_free(struct device *dev, size_t size,</span>
 	size = PAGE_ALIGN(size);
 
 	if (!is_device_dma_coherent(dev)) {
<span class="p_del">-		if (__free_from_pool(vaddr, size))</span>
<span class="p_add">+		if (arch_free_from_atomic_pool(vaddr, size))</span>
 			return;
 		vunmap(vaddr);
 	}
<span class="p_chunk">@@ -312,7 +305,7 @@</span> <span class="p_context"> static int __swiotlb_mmap(struct device *dev,</span>
 	unsigned long pfn = dma_to_phys(dev, dma_addr) &gt;&gt; PAGE_SHIFT;
 	unsigned long off = vma-&gt;vm_pgoff;
 
<span class="p_del">-	vma-&gt;vm_page_prot = __get_dma_pgprot(attrs, vma-&gt;vm_page_prot,</span>
<span class="p_add">+	vma-&gt;vm_page_prot = arch_get_dma_pgprot(attrs, vma-&gt;vm_page_prot,</span>
 					     is_device_dma_coherent(dev));
 
 	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))
<span class="p_chunk">@@ -526,470 +519,16 @@</span> <span class="p_context"> static int __init dma_debug_do_init(void)</span>
 }
 fs_initcall(dma_debug_do_init);
 
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_IOMMU_DMA</span>
<span class="p_del">-#include &lt;linux/dma-iommu.h&gt;</span>
<span class="p_del">-#include &lt;linux/platform_device.h&gt;</span>
<span class="p_del">-#include &lt;linux/amba/bus.h&gt;</span>
<span class="p_del">-</span>
<span class="p_del">-/* Thankfully, all cache ops are by VA so we can ignore phys here */</span>
<span class="p_del">-static void flush_page(struct device *dev, const void *virt, phys_addr_t phys)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__dma_flush_range(virt, virt + PAGE_SIZE);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void *__iommu_alloc_attrs(struct device *dev, size_t size,</span>
<span class="p_del">-				 dma_addr_t *handle, gfp_t gfp,</span>
<span class="p_del">-				 struct dma_attrs *attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	bool coherent = is_device_dma_coherent(dev);</span>
<span class="p_del">-	int ioprot = dma_direction_to_prot(DMA_BIDIRECTIONAL, coherent);</span>
<span class="p_del">-	size_t iosize = size;</span>
<span class="p_del">-	void *addr;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (WARN(!dev, &quot;cannot create IOMMU mapping for unknown device\n&quot;))</span>
<span class="p_del">-		return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	size = PAGE_ALIGN(size);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Some drivers rely on this, and we probably don&#39;t want the</span>
<span class="p_del">-	 * possibility of stale kernel data being read by devices anyway.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	gfp |= __GFP_ZERO;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (gfpflags_allow_blocking(gfp)) {</span>
<span class="p_del">-		struct page **pages;</span>
<span class="p_del">-		pgprot_t prot = __get_dma_pgprot(attrs, PAGE_KERNEL, coherent);</span>
<span class="p_del">-</span>
<span class="p_del">-		pages = iommu_dma_alloc(dev, iosize, gfp, ioprot, handle,</span>
<span class="p_del">-					flush_page);</span>
<span class="p_del">-		if (!pages)</span>
<span class="p_del">-			return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-		addr = dma_common_pages_remap(pages, size, VM_USERMAP, prot,</span>
<span class="p_del">-					      __builtin_return_address(0));</span>
<span class="p_del">-		if (!addr)</span>
<span class="p_del">-			iommu_dma_free(dev, pages, iosize, handle);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		struct page *page;</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * In atomic context we can&#39;t remap anything, so we&#39;ll only</span>
<span class="p_del">-		 * get the virtually contiguous buffer we need by way of a</span>
<span class="p_del">-		 * physically contiguous allocation.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (coherent) {</span>
<span class="p_del">-			page = alloc_pages(gfp, get_order(size));</span>
<span class="p_del">-			addr = page ? page_address(page) : NULL;</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			addr = __alloc_from_pool(size, &amp;page, gfp);</span>
<span class="p_del">-		}</span>
<span class="p_del">-		if (!addr)</span>
<span class="p_del">-			return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-		*handle = iommu_dma_map_page(dev, page, 0, iosize, ioprot);</span>
<span class="p_del">-		if (iommu_dma_mapping_error(dev, *handle)) {</span>
<span class="p_del">-			if (coherent)</span>
<span class="p_del">-				__free_pages(page, get_order(size));</span>
<span class="p_del">-			else</span>
<span class="p_del">-				__free_from_pool(addr, size);</span>
<span class="p_del">-			addr = NULL;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return addr;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __iommu_free_attrs(struct device *dev, size_t size, void *cpu_addr,</span>
<span class="p_del">-			       dma_addr_t handle, struct dma_attrs *attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	size_t iosize = size;</span>
<span class="p_del">-</span>
<span class="p_del">-	size = PAGE_ALIGN(size);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * @cpu_addr will be one of 3 things depending on how it was allocated:</span>
<span class="p_del">-	 * - A remapped array of pages from iommu_dma_alloc(), for all</span>
<span class="p_del">-	 *   non-atomic allocations.</span>
<span class="p_del">-	 * - A non-cacheable alias from the atomic pool, for atomic</span>
<span class="p_del">-	 *   allocations by non-coherent devices.</span>
<span class="p_del">-	 * - A normal lowmem address, for atomic allocations by</span>
<span class="p_del">-	 *   coherent devices.</span>
<span class="p_del">-	 * Hence how dodgy the below logic looks...</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (__in_atomic_pool(cpu_addr, size)) {</span>
<span class="p_del">-		iommu_dma_unmap_page(dev, handle, iosize, 0, NULL);</span>
<span class="p_del">-		__free_from_pool(cpu_addr, size);</span>
<span class="p_del">-	} else if (is_vmalloc_addr(cpu_addr)){</span>
<span class="p_del">-		struct vm_struct *area = find_vm_area(cpu_addr);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (WARN_ON(!area || !area-&gt;pages))</span>
<span class="p_del">-			return;</span>
<span class="p_del">-		iommu_dma_free(dev, area-&gt;pages, iosize, &amp;handle);</span>
<span class="p_del">-		dma_common_free_remap(cpu_addr, size, VM_USERMAP);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		iommu_dma_unmap_page(dev, handle, iosize, 0, NULL);</span>
<span class="p_del">-		__free_pages(virt_to_page(cpu_addr), get_order(size));</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __iommu_mmap_attrs(struct device *dev, struct vm_area_struct *vma,</span>
<span class="p_del">-			      void *cpu_addr, dma_addr_t dma_addr, size_t size,</span>
<span class="p_del">-			      struct dma_attrs *attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct vm_struct *area;</span>
<span class="p_del">-	int ret;</span>
<span class="p_del">-</span>
<span class="p_del">-	vma-&gt;vm_page_prot = __get_dma_pgprot(attrs, vma-&gt;vm_page_prot,</span>
<span class="p_del">-					     is_device_dma_coherent(dev));</span>
<span class="p_del">-</span>
<span class="p_del">-	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="p_del">-		return ret;</span>
<span class="p_del">-</span>
<span class="p_del">-	area = find_vm_area(cpu_addr);</span>
<span class="p_del">-	if (WARN_ON(!area || !area-&gt;pages))</span>
<span class="p_del">-		return -ENXIO;</span>
<span class="p_del">-</span>
<span class="p_del">-	return iommu_dma_mmap(area-&gt;pages, size, vma);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __iommu_get_sgtable(struct device *dev, struct sg_table *sgt,</span>
<span class="p_del">-			       void *cpu_addr, dma_addr_t dma_addr,</span>
<span class="p_del">-			       size_t size, struct dma_attrs *attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned int count = PAGE_ALIGN(size) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_del">-	struct vm_struct *area = find_vm_area(cpu_addr);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (WARN_ON(!area || !area-&gt;pages))</span>
<span class="p_del">-		return -ENXIO;</span>
<span class="p_del">-</span>
<span class="p_del">-	return sg_alloc_table_from_pages(sgt, area-&gt;pages, count, 0, size,</span>
<span class="p_del">-					 GFP_KERNEL);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __iommu_sync_single_for_cpu(struct device *dev,</span>
<span class="p_del">-					dma_addr_t dev_addr, size_t size,</span>
<span class="p_del">-					enum dma_data_direction dir)</span>
<span class="p_del">-{</span>
<span class="p_del">-	phys_addr_t phys;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (is_device_dma_coherent(dev))</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	phys = iommu_iova_to_phys(iommu_get_domain_for_dev(dev), dev_addr);</span>
<span class="p_del">-	__dma_unmap_area(phys_to_virt(phys), size, dir);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __iommu_sync_single_for_device(struct device *dev,</span>
<span class="p_del">-					   dma_addr_t dev_addr, size_t size,</span>
<span class="p_del">-					   enum dma_data_direction dir)</span>
<span class="p_del">-{</span>
<span class="p_del">-	phys_addr_t phys;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (is_device_dma_coherent(dev))</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	phys = iommu_iova_to_phys(iommu_get_domain_for_dev(dev), dev_addr);</span>
<span class="p_del">-	__dma_map_area(phys_to_virt(phys), size, dir);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static dma_addr_t __iommu_map_page(struct device *dev, struct page *page,</span>
<span class="p_del">-				   unsigned long offset, size_t size,</span>
<span class="p_del">-				   enum dma_data_direction dir,</span>
<span class="p_del">-				   struct dma_attrs *attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	bool coherent = is_device_dma_coherent(dev);</span>
<span class="p_del">-	int prot = dma_direction_to_prot(dir, coherent);</span>
<span class="p_del">-	dma_addr_t dev_addr = iommu_dma_map_page(dev, page, offset, size, prot);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!iommu_dma_mapping_error(dev, dev_addr) &amp;&amp;</span>
<span class="p_del">-	    !dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))</span>
<span class="p_del">-		__iommu_sync_single_for_device(dev, dev_addr, size, dir);</span>
<span class="p_del">-</span>
<span class="p_del">-	return dev_addr;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __iommu_unmap_page(struct device *dev, dma_addr_t dev_addr,</span>
<span class="p_del">-			       size_t size, enum dma_data_direction dir,</span>
<span class="p_del">-			       struct dma_attrs *attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))</span>
<span class="p_del">-		__iommu_sync_single_for_cpu(dev, dev_addr, size, dir);</span>
<span class="p_del">-</span>
<span class="p_del">-	iommu_dma_unmap_page(dev, dev_addr, size, dir, attrs);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __iommu_sync_sg_for_cpu(struct device *dev,</span>
<span class="p_del">-				    struct scatterlist *sgl, int nelems,</span>
<span class="p_del">-				    enum dma_data_direction dir)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct scatterlist *sg;</span>
<span class="p_del">-	int i;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (is_device_dma_coherent(dev))</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	for_each_sg(sgl, sg, nelems, i)</span>
<span class="p_del">-		__dma_unmap_area(sg_virt(sg), sg-&gt;length, dir);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __iommu_sync_sg_for_device(struct device *dev,</span>
<span class="p_del">-				       struct scatterlist *sgl, int nelems,</span>
<span class="p_del">-				       enum dma_data_direction dir)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct scatterlist *sg;</span>
<span class="p_del">-	int i;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (is_device_dma_coherent(dev))</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	for_each_sg(sgl, sg, nelems, i)</span>
<span class="p_del">-		__dma_map_area(sg_virt(sg), sg-&gt;length, dir);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __iommu_map_sg_attrs(struct device *dev, struct scatterlist *sgl,</span>
<span class="p_del">-				int nelems, enum dma_data_direction dir,</span>
<span class="p_del">-				struct dma_attrs *attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	bool coherent = is_device_dma_coherent(dev);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))</span>
<span class="p_del">-		__iommu_sync_sg_for_device(dev, sgl, nelems, dir);</span>
<span class="p_del">-</span>
<span class="p_del">-	return iommu_dma_map_sg(dev, sgl, nelems,</span>
<span class="p_del">-			dma_direction_to_prot(dir, coherent));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __iommu_unmap_sg_attrs(struct device *dev,</span>
<span class="p_del">-				   struct scatterlist *sgl, int nelems,</span>
<span class="p_del">-				   enum dma_data_direction dir,</span>
<span class="p_del">-				   struct dma_attrs *attrs)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))</span>
<span class="p_del">-		__iommu_sync_sg_for_cpu(dev, sgl, nelems, dir);</span>
<span class="p_del">-</span>
<span class="p_del">-	iommu_dma_unmap_sg(dev, sgl, nelems, dir, attrs);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static struct dma_map_ops iommu_dma_ops = {</span>
<span class="p_del">-	.alloc = __iommu_alloc_attrs,</span>
<span class="p_del">-	.free = __iommu_free_attrs,</span>
<span class="p_del">-	.mmap = __iommu_mmap_attrs,</span>
<span class="p_del">-	.get_sgtable = __iommu_get_sgtable,</span>
<span class="p_del">-	.map_page = __iommu_map_page,</span>
<span class="p_del">-	.unmap_page = __iommu_unmap_page,</span>
<span class="p_del">-	.map_sg = __iommu_map_sg_attrs,</span>
<span class="p_del">-	.unmap_sg = __iommu_unmap_sg_attrs,</span>
<span class="p_del">-	.sync_single_for_cpu = __iommu_sync_single_for_cpu,</span>
<span class="p_del">-	.sync_single_for_device = __iommu_sync_single_for_device,</span>
<span class="p_del">-	.sync_sg_for_cpu = __iommu_sync_sg_for_cpu,</span>
<span class="p_del">-	.sync_sg_for_device = __iommu_sync_sg_for_device,</span>
<span class="p_del">-	.dma_supported = iommu_dma_supported,</span>
<span class="p_del">-	.mapping_error = iommu_dma_mapping_error,</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * TODO: Right now __iommu_setup_dma_ops() gets called too early to do</span>
<span class="p_del">- * everything it needs to - the device is only partially created and the</span>
<span class="p_del">- * IOMMU driver hasn&#39;t seen it yet, so it can&#39;t have a group. Thus we</span>
<span class="p_del">- * need this delayed attachment dance. Once IOMMU probe ordering is sorted</span>
<span class="p_del">- * to move the arch_setup_dma_ops() call later, all the notifier bits below</span>
<span class="p_del">- * become unnecessary, and will go away.</span>
<span class="p_del">- */</span>
<span class="p_del">-struct iommu_dma_notifier_data {</span>
<span class="p_del">-	struct list_head list;</span>
<span class="p_del">-	struct device *dev;</span>
<span class="p_del">-	const struct iommu_ops *ops;</span>
<span class="p_del">-	u64 dma_base;</span>
<span class="p_del">-	u64 size;</span>
<span class="p_del">-};</span>
<span class="p_del">-static LIST_HEAD(iommu_dma_masters);</span>
<span class="p_del">-static DEFINE_MUTEX(iommu_dma_notifier_lock);</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Temporarily &quot;borrow&quot; a domain feature flag to to tell if we had to resort</span>
<span class="p_del">- * to creating our own domain here, in case we need to clean it up again.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define __IOMMU_DOMAIN_FAKE_DEFAULT		(1U &lt;&lt; 31)</span>
<span class="p_del">-</span>
<span class="p_del">-static bool do_iommu_attach(struct device *dev, const struct iommu_ops *ops,</span>
<span class="p_del">-			   u64 dma_base, u64 size)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct iommu_domain *domain = iommu_get_domain_for_dev(dev);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Best case: The device is either part of a group which was</span>
<span class="p_del">-	 * already attached to a domain in a previous call, or it&#39;s</span>
<span class="p_del">-	 * been put in a default DMA domain by the IOMMU core.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (!domain) {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Urgh. The IOMMU core isn&#39;t going to do default domains</span>
<span class="p_del">-		 * for non-PCI devices anyway, until it has some means of</span>
<span class="p_del">-		 * abstracting the entirely implementation-specific</span>
<span class="p_del">-		 * sideband data/SoC topology/unicorn dust that may or</span>
<span class="p_del">-		 * may not differentiate upstream masters.</span>
<span class="p_del">-		 * So until then, HORRIBLE HACKS!</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		domain = ops-&gt;domain_alloc(IOMMU_DOMAIN_DMA);</span>
<span class="p_del">-		if (!domain)</span>
<span class="p_del">-			goto out_no_domain;</span>
<span class="p_del">-</span>
<span class="p_del">-		domain-&gt;ops = ops;</span>
<span class="p_del">-		domain-&gt;type = IOMMU_DOMAIN_DMA | __IOMMU_DOMAIN_FAKE_DEFAULT;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (iommu_attach_device(domain, dev))</span>
<span class="p_del">-			goto out_put_domain;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	if (iommu_dma_init_domain(domain, dma_base, size))</span>
<span class="p_del">-		goto out_detach;</span>
<span class="p_del">-</span>
<span class="p_del">-	dev-&gt;archdata.dma_ops = &amp;iommu_dma_ops;</span>
<span class="p_del">-	return true;</span>
<span class="p_del">-</span>
<span class="p_del">-out_detach:</span>
<span class="p_del">-	iommu_detach_device(domain, dev);</span>
<span class="p_del">-out_put_domain:</span>
<span class="p_del">-	if (domain-&gt;type &amp; __IOMMU_DOMAIN_FAKE_DEFAULT)</span>
<span class="p_del">-		iommu_domain_free(domain);</span>
<span class="p_del">-out_no_domain:</span>
<span class="p_del">-	pr_warn(&quot;Failed to set up IOMMU for device %s; retaining platform DMA ops\n&quot;,</span>
<span class="p_del">-		dev_name(dev));</span>
<span class="p_del">-	return false;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void queue_iommu_attach(struct device *dev, const struct iommu_ops *ops,</span>
<span class="p_del">-			      u64 dma_base, u64 size)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct iommu_dma_notifier_data *iommudata;</span>
<span class="p_del">-</span>
<span class="p_del">-	iommudata = kzalloc(sizeof(*iommudata), GFP_KERNEL);</span>
<span class="p_del">-	if (!iommudata)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	iommudata-&gt;dev = dev;</span>
<span class="p_del">-	iommudata-&gt;ops = ops;</span>
<span class="p_del">-	iommudata-&gt;dma_base = dma_base;</span>
<span class="p_del">-	iommudata-&gt;size = size;</span>
<span class="p_del">-</span>
<span class="p_del">-	mutex_lock(&amp;iommu_dma_notifier_lock);</span>
<span class="p_del">-	list_add(&amp;iommudata-&gt;list, &amp;iommu_dma_masters);</span>
<span class="p_del">-	mutex_unlock(&amp;iommu_dma_notifier_lock);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __iommu_attach_notifier(struct notifier_block *nb,</span>
<span class="p_del">-				   unsigned long action, void *data)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct iommu_dma_notifier_data *master, *tmp;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (action != BUS_NOTIFY_ADD_DEVICE)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	mutex_lock(&amp;iommu_dma_notifier_lock);</span>
<span class="p_del">-	list_for_each_entry_safe(master, tmp, &amp;iommu_dma_masters, list) {</span>
<span class="p_del">-		if (do_iommu_attach(master-&gt;dev, master-&gt;ops,</span>
<span class="p_del">-				master-&gt;dma_base, master-&gt;size)) {</span>
<span class="p_del">-			list_del(&amp;master-&gt;list);</span>
<span class="p_del">-			kfree(master);</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-	mutex_unlock(&amp;iommu_dma_notifier_lock);</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init register_iommu_dma_ops_notifier(struct bus_type *bus)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct notifier_block *nb = kzalloc(sizeof(*nb), GFP_KERNEL);</span>
<span class="p_del">-	int ret;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!nb)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The device must be attached to a domain before the driver probe</span>
<span class="p_del">-	 * routine gets a chance to start allocating DMA buffers. However,</span>
<span class="p_del">-	 * the IOMMU driver also needs a chance to configure the iommu_group</span>
<span class="p_del">-	 * via its add_device callback first, so we need to make the attach</span>
<span class="p_del">-	 * happen between those two points. Since the IOMMU core uses a bus</span>
<span class="p_del">-	 * notifier with default priority for add_device, do the same but</span>
<span class="p_del">-	 * with a lower priority to ensure the appropriate ordering.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	nb-&gt;notifier_call = __iommu_attach_notifier;</span>
<span class="p_del">-	nb-&gt;priority = -100;</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = bus_register_notifier(bus, nb);</span>
<span class="p_del">-	if (ret) {</span>
<span class="p_del">-		pr_warn(&quot;Failed to register DMA domain notifier; IOMMU DMA ops unavailable on bus &#39;%s&#39;\n&quot;,</span>
<span class="p_del">-			bus-&gt;name);</span>
<span class="p_del">-		kfree(nb);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	return ret;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init __iommu_dma_init(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int ret;</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = iommu_dma_init();</span>
<span class="p_del">-	if (!ret)</span>
<span class="p_del">-		ret = register_iommu_dma_ops_notifier(&amp;platform_bus_type);</span>
<span class="p_del">-	if (!ret)</span>
<span class="p_del">-		ret = register_iommu_dma_ops_notifier(&amp;amba_bustype);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* handle devices queued before this arch_initcall */</span>
<span class="p_del">-	if (!ret)</span>
<span class="p_del">-		__iommu_attach_notifier(NULL, BUS_NOTIFY_ADD_DEVICE, NULL);</span>
<span class="p_del">-	return ret;</span>
<span class="p_del">-}</span>
<span class="p_del">-arch_initcall(__iommu_dma_init);</span>
<span class="p_del">-</span>
<span class="p_del">-static void __iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,</span>
<span class="p_del">-				  const struct iommu_ops *ops)</span>
<span class="p_add">+void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,</span>
<span class="p_add">+			struct iommu_ops *iommu, bool coherent)</span>
 {
<span class="p_del">-	struct iommu_group *group;</span>
<span class="p_add">+	dev-&gt;archdata.dma_coherent = coherent;</span>
 
<span class="p_del">-	if (!ops)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * TODO: As a concession to the future, we&#39;re ready to handle being</span>
<span class="p_del">-	 * called both early and late (i.e. after bus_add_device). Once all</span>
<span class="p_del">-	 * the platform bus code is reworked to call us late and the notifier</span>
<span class="p_del">-	 * junk above goes away, move the body of do_iommu_attach here.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	group = iommu_group_get(dev);</span>
<span class="p_del">-	if (group) {</span>
<span class="p_del">-		do_iommu_attach(dev, ops, dma_base, size);</span>
<span class="p_del">-		iommu_group_put(group);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		queue_iommu_attach(dev, ops, dma_base, size);</span>
<span class="p_del">-	}</span>
<span class="p_add">+	if (!common_iommu_setup_dma_ops(dev, dma_base, size, iommu))</span>
<span class="p_add">+		arch_set_dma_ops(dev, &amp;swiotlb_dma_ops);</span>
 }
 
 void arch_teardown_dma_ops(struct device *dev)
 {
<span class="p_del">-	struct iommu_domain *domain = iommu_get_domain_for_dev(dev);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (domain) {</span>
<span class="p_del">-		iommu_detach_device(domain, dev);</span>
<span class="p_del">-		if (domain-&gt;type &amp; __IOMMU_DOMAIN_FAKE_DEFAULT)</span>
<span class="p_del">-			iommu_domain_free(domain);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	dev-&gt;archdata.dma_ops = NULL;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#else</span>
<span class="p_del">-</span>
<span class="p_del">-static void __iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,</span>
<span class="p_del">-				  struct iommu_ops *iommu)</span>
<span class="p_del">-{ }</span>
<span class="p_del">-</span>
<span class="p_del">-#endif  /* CONFIG_IOMMU_DMA */</span>
<span class="p_del">-</span>
<span class="p_del">-void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,</span>
<span class="p_del">-			struct iommu_ops *iommu, bool coherent)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (!dev-&gt;archdata.dma_ops)</span>
<span class="p_del">-		dev-&gt;archdata.dma_ops = &amp;swiotlb_dma_ops;</span>
<span class="p_del">-</span>
<span class="p_del">-	dev-&gt;archdata.dma_coherent = coherent;</span>
<span class="p_del">-	__iommu_setup_dma_ops(dev, dma_base, size, iommu);</span>
<span class="p_add">+	common_iommu_teardown_dma_ops(dev);</span>
 }
<span class="p_header">diff --git a/drivers/iommu/Makefile b/drivers/iommu/Makefile</span>
<span class="p_header">index 42fc0c25cf1a..c0dbf765bf45 100644</span>
<span class="p_header">--- a/drivers/iommu/Makefile</span>
<span class="p_header">+++ b/drivers/iommu/Makefile</span>
<span class="p_chunk">@@ -1,7 +1,7 @@</span> <span class="p_context"></span>
 obj-$(CONFIG_IOMMU_API) += iommu.o
 obj-$(CONFIG_IOMMU_API) += iommu-traces.o
 obj-$(CONFIG_IOMMU_API) += iommu-sysfs.o
<span class="p_del">-obj-$(CONFIG_IOMMU_DMA) += dma-iommu.o</span>
<span class="p_add">+obj-$(CONFIG_IOMMU_DMA) += dma-iommu.o dma-iommu-ops.o</span>
 obj-$(CONFIG_IOMMU_IO_PGTABLE) += io-pgtable.o
 obj-$(CONFIG_IOMMU_IO_PGTABLE_LPAE) += io-pgtable-arm.o
 obj-$(CONFIG_IOMMU_IOVA) += iova.o
<span class="p_header">diff --git a/drivers/iommu/dma-iommu-ops.c b/drivers/iommu/dma-iommu-ops.c</span>
new file mode 100644
<span class="p_header">index 000000000000..047c47e3c0ab</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/drivers/iommu/dma-iommu-ops.c</span>
<span class="p_chunk">@@ -0,0 +1,471 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * A common IOMMU based DMA-API implementation for ARM and ARM64 architecutes.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful,</span>
<span class="p_add">+ * but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="p_add">+ * GNU General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * You should have received a copy of the GNU General Public License</span>
<span class="p_add">+ * along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/device.h&gt;</span>
<span class="p_add">+#include &lt;linux/dma-iommu.h&gt;</span>
<span class="p_add">+#include &lt;linux/gfp.h&gt;</span>
<span class="p_add">+#include &lt;linux/huge_mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/iommu.h&gt;</span>
<span class="p_add">+#include &lt;linux/iova.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/scatterlist.h&gt;</span>
<span class="p_add">+#include &lt;linux/vmalloc.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/platform_device.h&gt;</span>
<span class="p_add">+#include &lt;linux/amba/bus.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/dma-mapping.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+static void *__iommu_alloc_attrs(struct device *dev, size_t size,</span>
<span class="p_add">+				 dma_addr_t *handle, gfp_t gfp,</span>
<span class="p_add">+				 struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool coherent = is_device_dma_coherent(dev);</span>
<span class="p_add">+	int ioprot = dma_direction_to_prot(DMA_BIDIRECTIONAL, coherent);</span>
<span class="p_add">+	size_t iosize = size;</span>
<span class="p_add">+	void *addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (WARN(!dev, &quot;cannot create IOMMU mapping for unknown device\n&quot;))</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+	size = PAGE_ALIGN(size);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Some drivers rely on this, and we probably don&#39;t want the</span>
<span class="p_add">+	 * possibility of stale kernel data being read by devices anyway.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	gfp |= __GFP_ZERO;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (gfpflags_allow_blocking(gfp)) {</span>
<span class="p_add">+		struct page **pages;</span>
<span class="p_add">+		pgprot_t prot = arch_get_dma_pgprot(attrs, PAGE_KERNEL,</span>
<span class="p_add">+						    coherent);</span>
<span class="p_add">+</span>
<span class="p_add">+		pages = iommu_dma_alloc(dev, iosize, gfp, ioprot, handle,</span>
<span class="p_add">+					arch_flush_page);</span>
<span class="p_add">+		if (!pages)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		addr = dma_common_pages_remap(pages, size, VM_USERMAP, prot,</span>
<span class="p_add">+					      __builtin_return_address(0));</span>
<span class="p_add">+		if (!addr)</span>
<span class="p_add">+			iommu_dma_free(dev, pages, iosize, handle);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * In atomic context we can&#39;t remap anything, so we&#39;ll only</span>
<span class="p_add">+		 * get the virtually contiguous buffer we need by way of a</span>
<span class="p_add">+		 * physically contiguous allocation.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (coherent) {</span>
<span class="p_add">+			page = alloc_pages(gfp, get_order(size));</span>
<span class="p_add">+			addr = page ? page_address(page) : NULL;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			addr = arch_alloc_from_atomic_pool(size, &amp;page, gfp);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (!addr)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		*handle = iommu_dma_map_page(dev, page, 0, iosize, ioprot);</span>
<span class="p_add">+		if (iommu_dma_mapping_error(dev, *handle)) {</span>
<span class="p_add">+			if (coherent)</span>
<span class="p_add">+				__free_pages(page, get_order(size));</span>
<span class="p_add">+			else</span>
<span class="p_add">+				arch_free_from_atomic_pool(addr, size);</span>
<span class="p_add">+			addr = NULL;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return addr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __iommu_free_attrs(struct device *dev, size_t size, void *cpu_addr,</span>
<span class="p_add">+			       dma_addr_t handle, struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	size_t iosize = size;</span>
<span class="p_add">+</span>
<span class="p_add">+	size = PAGE_ALIGN(size);</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * @cpu_addr will be one of 3 things depending on how it was allocated:</span>
<span class="p_add">+	 * - A remapped array of pages from iommu_dma_alloc(), for all</span>
<span class="p_add">+	 *   non-atomic allocations.</span>
<span class="p_add">+	 * - A non-cacheable alias from the atomic pool, for atomic</span>
<span class="p_add">+	 *   allocations by non-coherent devices.</span>
<span class="p_add">+	 * - A normal lowmem address, for atomic allocations by</span>
<span class="p_add">+	 *   coherent devices.</span>
<span class="p_add">+	 * Hence how dodgy the below logic looks...</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (arch_in_atomic_pool(cpu_addr, size)) {</span>
<span class="p_add">+		iommu_dma_unmap_page(dev, handle, iosize, 0, NULL);</span>
<span class="p_add">+		arch_free_from_atomic_pool(cpu_addr, size);</span>
<span class="p_add">+	} else if (is_vmalloc_addr(cpu_addr)){</span>
<span class="p_add">+		struct vm_struct *area = find_vm_area(cpu_addr);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (WARN_ON(!area || !area-&gt;pages))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		iommu_dma_free(dev, area-&gt;pages, iosize, &amp;handle);</span>
<span class="p_add">+		dma_common_free_remap(cpu_addr, size, VM_USERMAP);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		iommu_dma_unmap_page(dev, handle, iosize, 0, NULL);</span>
<span class="p_add">+		__free_pages(virt_to_page(cpu_addr), get_order(size));</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __iommu_mmap_attrs(struct device *dev, struct vm_area_struct *vma,</span>
<span class="p_add">+			      void *cpu_addr, dma_addr_t dma_addr, size_t size,</span>
<span class="p_add">+			      struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_struct *area;</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	vma-&gt;vm_page_prot = arch_get_dma_pgprot(attrs, vma-&gt;vm_page_prot,</span>
<span class="p_add">+					        is_device_dma_coherent(dev));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (dma_mmap_from_coherent(dev, vma, cpu_addr, size, &amp;ret))</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	area = find_vm_area(cpu_addr);</span>
<span class="p_add">+	if (WARN_ON(!area || !area-&gt;pages))</span>
<span class="p_add">+		return -ENXIO;</span>
<span class="p_add">+</span>
<span class="p_add">+	return iommu_dma_mmap(area-&gt;pages, size, vma);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __iommu_get_sgtable(struct device *dev, struct sg_table *sgt,</span>
<span class="p_add">+			       void *cpu_addr, dma_addr_t dma_addr,</span>
<span class="p_add">+			       size_t size, struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int count = PAGE_ALIGN(size) &gt;&gt; PAGE_SHIFT;</span>
<span class="p_add">+	struct vm_struct *area = find_vm_area(cpu_addr);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (WARN_ON(!area || !area-&gt;pages))</span>
<span class="p_add">+		return -ENXIO;</span>
<span class="p_add">+</span>
<span class="p_add">+	return sg_alloc_table_from_pages(sgt, area-&gt;pages, count, 0, size,</span>
<span class="p_add">+					 GFP_KERNEL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __iommu_sync_single_for_cpu(struct device *dev,</span>
<span class="p_add">+					dma_addr_t dev_addr, size_t size,</span>
<span class="p_add">+					enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	phys_addr_t phys;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (is_device_dma_coherent(dev))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	phys = iommu_iova_to_phys(iommu_get_domain_for_dev(dev), dev_addr);</span>
<span class="p_add">+	arch_dma_unmap_area(phys, size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __iommu_sync_single_for_device(struct device *dev,</span>
<span class="p_add">+					   dma_addr_t dev_addr, size_t size,</span>
<span class="p_add">+					   enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	phys_addr_t phys;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (is_device_dma_coherent(dev))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	phys = iommu_iova_to_phys(iommu_get_domain_for_dev(dev), dev_addr);</span>
<span class="p_add">+	arch_dma_map_area(phys, size, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static dma_addr_t __iommu_map_page(struct device *dev, struct page *page,</span>
<span class="p_add">+				   unsigned long offset, size_t size,</span>
<span class="p_add">+				   enum dma_data_direction dir,</span>
<span class="p_add">+				   struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool coherent = is_device_dma_coherent(dev);</span>
<span class="p_add">+	int prot = dma_direction_to_prot(dir, coherent);</span>
<span class="p_add">+	dma_addr_t dev_addr = iommu_dma_map_page(dev, page, offset, size, prot);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!iommu_dma_mapping_error(dev, dev_addr) &amp;&amp;</span>
<span class="p_add">+	    !dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))</span>
<span class="p_add">+		__iommu_sync_single_for_device(dev, dev_addr, size, dir);</span>
<span class="p_add">+</span>
<span class="p_add">+	return dev_addr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __iommu_unmap_page(struct device *dev, dma_addr_t dev_addr,</span>
<span class="p_add">+			       size_t size, enum dma_data_direction dir,</span>
<span class="p_add">+			       struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))</span>
<span class="p_add">+		__iommu_sync_single_for_cpu(dev, dev_addr, size, dir);</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu_dma_unmap_page(dev, dev_addr, size, dir, attrs);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __iommu_sync_sg_for_cpu(struct device *dev,</span>
<span class="p_add">+				    struct scatterlist *sgl, int nelems,</span>
<span class="p_add">+				    enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct scatterlist *sg;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (is_device_dma_coherent(dev))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_sg(sgl, sg, nelems, i)</span>
<span class="p_add">+		arch_dma_unmap_area(sg_phys(sg), sg-&gt;length, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __iommu_sync_sg_for_device(struct device *dev,</span>
<span class="p_add">+				       struct scatterlist *sgl, int nelems,</span>
<span class="p_add">+				       enum dma_data_direction dir)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct scatterlist *sg;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (is_device_dma_coherent(dev))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for_each_sg(sgl, sg, nelems, i)</span>
<span class="p_add">+		arch_dma_map_area(sg_phys(sg), sg-&gt;length, dir);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __iommu_map_sg_attrs(struct device *dev, struct scatterlist *sgl,</span>
<span class="p_add">+				int nelems, enum dma_data_direction dir,</span>
<span class="p_add">+				struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	bool coherent = is_device_dma_coherent(dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))</span>
<span class="p_add">+		__iommu_sync_sg_for_device(dev, sgl, nelems, dir);</span>
<span class="p_add">+</span>
<span class="p_add">+	return iommu_dma_map_sg(dev, sgl, nelems,</span>
<span class="p_add">+			dma_direction_to_prot(dir, coherent));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __iommu_unmap_sg_attrs(struct device *dev,</span>
<span class="p_add">+				   struct scatterlist *sgl, int nelems,</span>
<span class="p_add">+				   enum dma_data_direction dir,</span>
<span class="p_add">+				   struct dma_attrs *attrs)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))</span>
<span class="p_add">+		__iommu_sync_sg_for_cpu(dev, sgl, nelems, dir);</span>
<span class="p_add">+</span>
<span class="p_add">+	iommu_dma_unmap_sg(dev, sgl, nelems, dir, attrs);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static struct dma_map_ops iommu_dma_ops = {</span>
<span class="p_add">+	.alloc = __iommu_alloc_attrs,</span>
<span class="p_add">+	.free = __iommu_free_attrs,</span>
<span class="p_add">+	.mmap = __iommu_mmap_attrs,</span>
<span class="p_add">+	.get_sgtable = __iommu_get_sgtable,</span>
<span class="p_add">+	.map_page = __iommu_map_page,</span>
<span class="p_add">+	.unmap_page = __iommu_unmap_page,</span>
<span class="p_add">+	.map_sg = __iommu_map_sg_attrs,</span>
<span class="p_add">+	.unmap_sg = __iommu_unmap_sg_attrs,</span>
<span class="p_add">+	.sync_single_for_cpu = __iommu_sync_single_for_cpu,</span>
<span class="p_add">+	.sync_single_for_device = __iommu_sync_single_for_device,</span>
<span class="p_add">+	.sync_sg_for_cpu = __iommu_sync_sg_for_cpu,</span>
<span class="p_add">+	.sync_sg_for_device = __iommu_sync_sg_for_device,</span>
<span class="p_add">+	.dma_supported = iommu_dma_supported,</span>
<span class="p_add">+	.mapping_error = iommu_dma_mapping_error,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * TODO: Right now __iommu_setup_dma_ops() gets called too early to do</span>
<span class="p_add">+ * everything it needs to - the device is only partially created and the</span>
<span class="p_add">+ * IOMMU driver hasn&#39;t seen it yet, so it can&#39;t have a group. Thus we</span>
<span class="p_add">+ * need this delayed attachment dance. Once IOMMU probe ordering is sorted</span>
<span class="p_add">+ * to move the arch_setup_dma_ops() call later, all the notifier bits below</span>
<span class="p_add">+ * become unnecessary, and will go away.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct iommu_dma_notifier_data {</span>
<span class="p_add">+	struct list_head list;</span>
<span class="p_add">+	struct device *dev;</span>
<span class="p_add">+	const struct iommu_ops *ops;</span>
<span class="p_add">+	u64 dma_base;</span>
<span class="p_add">+	u64 size;</span>
<span class="p_add">+};</span>
<span class="p_add">+static LIST_HEAD(iommu_dma_masters);</span>
<span class="p_add">+static DEFINE_MUTEX(iommu_dma_notifier_lock);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Temporarily &quot;borrow&quot; a domain feature flag to to tell if we had to resort</span>
<span class="p_add">+ * to creating our own domain here, in case we need to clean it up again.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define __IOMMU_DOMAIN_FAKE_DEFAULT		(1U &lt;&lt; 31)</span>
<span class="p_add">+</span>
<span class="p_add">+static bool do_iommu_attach(struct device *dev, const struct iommu_ops *ops,</span>
<span class="p_add">+			   u64 dma_base, u64 size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct iommu_domain *domain = iommu_get_domain_for_dev(dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Best case: The device is either part of a group which was</span>
<span class="p_add">+	 * already attached to a domain in a previous call, or it&#39;s</span>
<span class="p_add">+	 * been put in a default DMA domain by the IOMMU core.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!domain) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Urgh. The IOMMU core isn&#39;t going to do default domains</span>
<span class="p_add">+		 * for non-PCI devices anyway, until it has some means of</span>
<span class="p_add">+		 * abstracting the entirely implementation-specific</span>
<span class="p_add">+		 * sideband data/SoC topology/unicorn dust that may or</span>
<span class="p_add">+		 * may not differentiate upstream masters.</span>
<span class="p_add">+		 * So until then, HORRIBLE HACKS!</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		domain = ops-&gt;domain_alloc(IOMMU_DOMAIN_DMA);</span>
<span class="p_add">+		if (!domain)</span>
<span class="p_add">+			goto out_no_domain;</span>
<span class="p_add">+</span>
<span class="p_add">+		domain-&gt;ops = ops;</span>
<span class="p_add">+		domain-&gt;type = IOMMU_DOMAIN_DMA | __IOMMU_DOMAIN_FAKE_DEFAULT;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (iommu_attach_device(domain, dev))</span>
<span class="p_add">+			goto out_put_domain;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (iommu_dma_init_domain(domain, dma_base, size))</span>
<span class="p_add">+		goto out_detach;</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_set_dma_ops(dev, &amp;iommu_dma_ops);</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+</span>
<span class="p_add">+out_detach:</span>
<span class="p_add">+	iommu_detach_device(domain, dev);</span>
<span class="p_add">+out_put_domain:</span>
<span class="p_add">+	if (domain-&gt;type &amp; __IOMMU_DOMAIN_FAKE_DEFAULT)</span>
<span class="p_add">+		iommu_domain_free(domain);</span>
<span class="p_add">+out_no_domain:</span>
<span class="p_add">+	pr_warn(&quot;Failed to set up IOMMU for device %s; retaining platform DMA ops\n&quot;,</span>
<span class="p_add">+		dev_name(dev));</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void queue_iommu_attach(struct device *dev, const struct iommu_ops *ops,</span>
<span class="p_add">+			      u64 dma_base, u64 size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct iommu_dma_notifier_data *iommudata;</span>
<span class="p_add">+</span>
<span class="p_add">+	iommudata = kzalloc(sizeof(*iommudata), GFP_KERNEL);</span>
<span class="p_add">+	if (!iommudata)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	iommudata-&gt;dev = dev;</span>
<span class="p_add">+	iommudata-&gt;ops = ops;</span>
<span class="p_add">+	iommudata-&gt;dma_base = dma_base;</span>
<span class="p_add">+	iommudata-&gt;size = size;</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;iommu_dma_notifier_lock);</span>
<span class="p_add">+	list_add(&amp;iommudata-&gt;list, &amp;iommu_dma_masters);</span>
<span class="p_add">+	mutex_unlock(&amp;iommu_dma_notifier_lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __iommu_attach_notifier(struct notifier_block *nb,</span>
<span class="p_add">+				   unsigned long action, void *data)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct iommu_dma_notifier_data *master, *tmp;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (action != BUS_NOTIFY_ADD_DEVICE)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	mutex_lock(&amp;iommu_dma_notifier_lock);</span>
<span class="p_add">+	list_for_each_entry_safe(master, tmp, &amp;iommu_dma_masters, list) {</span>
<span class="p_add">+		if (do_iommu_attach(master-&gt;dev, master-&gt;ops,</span>
<span class="p_add">+				master-&gt;dma_base, master-&gt;size)) {</span>
<span class="p_add">+			list_del(&amp;master-&gt;list);</span>
<span class="p_add">+			kfree(master);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+	mutex_unlock(&amp;iommu_dma_notifier_lock);</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init register_iommu_dma_ops_notifier(struct bus_type *bus)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct notifier_block *nb = kzalloc(sizeof(*nb), GFP_KERNEL);</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!nb)</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The device must be attached to a domain before the driver probe</span>
<span class="p_add">+	 * routine gets a chance to start allocating DMA buffers. However,</span>
<span class="p_add">+	 * the IOMMU driver also needs a chance to configure the iommu_group</span>
<span class="p_add">+	 * via its add_device callback first, so we need to make the attach</span>
<span class="p_add">+	 * happen between those two points. Since the IOMMU core uses a bus</span>
<span class="p_add">+	 * notifier with default priority for add_device, do the same but</span>
<span class="p_add">+	 * with a lower priority to ensure the appropriate ordering.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	nb-&gt;notifier_call = __iommu_attach_notifier;</span>
<span class="p_add">+	nb-&gt;priority = -100;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = bus_register_notifier(bus, nb);</span>
<span class="p_add">+	if (ret) {</span>
<span class="p_add">+		pr_warn(&quot;Failed to register DMA domain notifier; IOMMU DMA ops unavailable on bus &#39;%s&#39;\n&quot;,</span>
<span class="p_add">+			bus-&gt;name);</span>
<span class="p_add">+		kfree(nb);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init __iommu_dma_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = iommu_dma_init();</span>
<span class="p_add">+	if (!ret)</span>
<span class="p_add">+		ret = register_iommu_dma_ops_notifier(&amp;platform_bus_type);</span>
<span class="p_add">+	if (!ret)</span>
<span class="p_add">+		ret = register_iommu_dma_ops_notifier(&amp;amba_bustype);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* handle devices queued before this arch_initcall */</span>
<span class="p_add">+	if (!ret)</span>
<span class="p_add">+		__iommu_attach_notifier(NULL, BUS_NOTIFY_ADD_DEVICE, NULL);</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+arch_initcall(__iommu_dma_init);</span>
<span class="p_add">+</span>
<span class="p_add">+bool common_iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,</span>
<span class="p_add">+				  const struct iommu_ops *ops)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct iommu_group *group;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!ops)</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * TODO: As a concession to the future, we&#39;re ready to handle being</span>
<span class="p_add">+	 * called both early and late (i.e. after bus_add_device). Once all</span>
<span class="p_add">+	 * the platform bus code is reworked to call us late and the notifier</span>
<span class="p_add">+	 * junk above goes away, move the body of do_iommu_attach here.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	group = iommu_group_get(dev);</span>
<span class="p_add">+	if (group) {</span>
<span class="p_add">+		do_iommu_attach(dev, ops, dma_base, size);</span>
<span class="p_add">+		iommu_group_put(group);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		queue_iommu_attach(dev, ops, dma_base, size);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void common_iommu_teardown_dma_ops(struct device *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct iommu_domain *domain = iommu_get_domain_for_dev(dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (domain) {</span>
<span class="p_add">+		iommu_detach_device(domain, dev);</span>
<span class="p_add">+		if (domain-&gt;type &amp; __IOMMU_DOMAIN_FAKE_DEFAULT)</span>
<span class="p_add">+			iommu_domain_free(domain);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_set_dma_ops(dev, NULL);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/include/linux/dma-iommu.h b/include/linux/dma-iommu.h</span>
<span class="p_header">index fc481037478a..01a836c43dc3 100644</span>
<span class="p_header">--- a/include/linux/dma-iommu.h</span>
<span class="p_header">+++ b/include/linux/dma-iommu.h</span>
<span class="p_chunk">@@ -62,6 +62,10 @@</span> <span class="p_context"> void iommu_dma_unmap_sg(struct device *dev, struct scatterlist *sg, int nents,</span>
 int iommu_dma_supported(struct device *dev, u64 mask);
 int iommu_dma_mapping_error(struct device *dev, dma_addr_t dma_addr);
 
<span class="p_add">+bool common_iommu_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,</span>
<span class="p_add">+				  const struct iommu_ops *ops);</span>
<span class="p_add">+void common_iommu_teardown_dma_ops(struct device *dev);</span>
<span class="p_add">+</span>
 #else
 
 struct iommu_domain;
<span class="p_chunk">@@ -80,6 +84,16 @@</span> <span class="p_context"> static inline void iommu_put_dma_cookie(struct iommu_domain *domain)</span>
 {
 }
 
<span class="p_add">+static inline bool common_iommu_setup_dma_ops(struct device *dev, u64 dma_base,</span>
<span class="p_add">+					u64 size, const struct iommu_ops *ops)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void common_iommu_teardown_dma_ops(struct device *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 #endif	/* CONFIG_IOMMU_DMA */
 #endif	/* __KERNEL__ */
 #endif	/* __DMA_IOMMU_H */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



