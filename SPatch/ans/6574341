
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[4/4] mm: Send one IPI per CPU to TLB flush pages that were recently unmapped - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [4/4] mm: Send one IPI per CPU to TLB flush pages that were recently unmapped</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 9, 2015, 5:31 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1433871118-15207-5-git-send-email-mgorman@suse.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6574341/mbox/"
   >mbox</a>
|
   <a href="/patch/6574341/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6574341/">/patch/6574341/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id B184CC0020
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 17:32:52 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 5C99920515
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 17:32:51 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 1DE1620501
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  9 Jun 2015 17:32:50 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933096AbbFIRcn (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 9 Jun 2015 13:32:43 -0400
Received: from cantor2.suse.de ([195.135.220.15]:50304 &quot;EHLO mx2.suse.de&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1753892AbbFIRcL (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 9 Jun 2015 13:32:11 -0400
X-Virus-Scanned: by amavisd-new at test-mx.suse.de
Received: from relay1.suse.de (charybdis-ext.suse.de [195.135.220.254])
	by mx2.suse.de (Postfix) with ESMTP id 5C0D7ADD7;
	Tue,  9 Jun 2015 17:32:10 +0000 (UTC)
From: Mel Gorman &lt;mgorman@suse.de&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Minchan Kim &lt;minchan@kernel.org&gt;, Dave Hansen &lt;dave.hansen@intel.com&gt;,
	Andi Kleen &lt;andi@firstfloor.org&gt;, H Peter Anvin &lt;hpa@zytor.com&gt;,
	Ingo Molnar &lt;mingo@kernel.org&gt;,
	Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;, Linux-MM &lt;linux-mm@kvack.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Mel Gorman &lt;mgorman@suse.de&gt;
Subject: [PATCH 4/4] mm: Send one IPI per CPU to TLB flush pages that were
	recently unmapped
Date: Tue,  9 Jun 2015 18:31:58 +0100
Message-Id: &lt;1433871118-15207-5-git-send-email-mgorman@suse.de&gt;
X-Mailer: git-send-email 2.3.5
In-Reply-To: &lt;1433871118-15207-1-git-send-email-mgorman@suse.de&gt;
References: &lt;1433871118-15207-1-git-send-email-mgorman@suse.de&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=24451">Mel Gorman</a> - June 9, 2015, 5:31 p.m.</div>
<pre class="content">
When unmapping pages, an IPI is sent to flush all TLB entries on CPUs that
potentially have a valid TLB entry. There are many circumstances where
this happens but the obvious one is kswapd reclaiming pages belonging to a
running process as kswapd and the task are likely running on separate CPUs.
This forces processes running the affected CPUs to refill their TLB entries.
This is an unpredictable cost as it heavily depends on the workloads,
the timing and the exact CPU used.

This patch uses a structure similar in principle to a pagevec to collect
a list of PFNs and CPUs that require flushing. It then sends one IPI per
CPU that was mapping any of those pages to flush the list of PFNs. A new
TLB flush helper is required for this and one is added for x86. Other
architectures will need to decide if batching like this is both safe and
worth the overhead.

There is a direct cost to tracking the PFNs both in memory and the cost of
the individual PFN flushes.  In the absolute worst case, the kernel flushes
individual PFNs and none of the active TLB entries were being used. Hence,
this results reflect the full cost without any of the benefit of preserving
existing entries.

On a 4-socket machine the results were

                                        4.1.0-rc6          4.1.0-rc6
                                    batchdirty-v6      batchunmap-v6
Ops lru-file-mmap-read-elapsed   121.27 (  0.00%)   118.79 (  2.05%)

           4.1.0-rc6      4.1.0-rc6
        batchdirty-v6 batchunmap-v6
User          620.84         608.48
System       4245.35        4152.89
Elapsed       122.65         120.15

In this case the workload completed faster and there was less CPU overhead
but as it&#39;s a NUMA machine there are a lot of factors at play. It&#39;s easier
to quantify on a single socket machine;

                                        4.1.0-rc6          4.1.0-rc6
                                    batchdirty-v6      batchunmap-v6
Ops lru-file-mmap-read-elapsed    20.35 (  0.00%)    21.52 ( -5.75%)

           4.1.0-rc6   4.1.0-rc6
        batchdirty-v6r5batchunmap-v6r5
User           58.02       60.70
System         77.57       81.92
Elapsed        22.14       23.16

That shows the workload takes 5.75% longer to complete with a similar
increase in the system CPU usage.

It is expected that there is overhead to tracking the PFNs and flushing
individual pages. This can be quantified but we cannot quantify the
indirect savings due to active unrelated TLB entries being preserved.
Whether this matters depends on whether the workload was using those
entries and if they would be used before a context switch but targeting
the TLB flushes is the conservative and safer choice.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@suse.de&gt;</span>
---
 arch/x86/include/asm/tlbflush.h |  2 ++
 include/linux/sched.h           | 12 ++++++++++--
 init/Kconfig                    | 10 ++++------
 mm/rmap.c                       | 25 +++++++++++++------------
 4 files changed, 29 insertions(+), 20 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index cd791948b286..10c197a649f5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -152,6 +152,8 @@</span> <span class="p_context"> static inline void __flush_tlb_one(unsigned long addr)</span>
  * and page-granular flushes are available only on i486 and up.
  */
 
<span class="p_add">+#define flush_local_tlb_addr(addr) __flush_tlb_single(addr)</span>
<span class="p_add">+</span>
 #ifndef CONFIG_SMP
 
 /* &quot;_up&quot; is for UniProcessor.
<span class="p_header">diff --git a/include/linux/sched.h b/include/linux/sched.h</span>
<span class="p_header">index 6b787a7f6c38..4dbffe0a1868 100644</span>
<span class="p_header">--- a/include/linux/sched.h</span>
<span class="p_header">+++ b/include/linux/sched.h</span>
<span class="p_chunk">@@ -1289,6 +1289,9 @@</span> <span class="p_context"> enum perf_event_task_context {</span>
 	perf_nr_task_contexts,
 };
 
<span class="p_add">+/* Matches SWAP_CLUSTER_MAX but refined to limit header dependencies */</span>
<span class="p_add">+#define BATCH_TLBFLUSH_SIZE 32UL</span>
<span class="p_add">+</span>
 /* Track pages that require TLB flushes */
 struct tlbflush_unmap_batch {
 	/*
<span class="p_chunk">@@ -1297,8 +1300,13 @@</span> <span class="p_context"> struct tlbflush_unmap_batch {</span>
 	 */
 	struct cpumask cpumask;
 
<span class="p_del">-	/* True if any bit in cpumask is set */</span>
<span class="p_del">-	bool flush_required;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The number and list of pfns to be flushed. PFNs are tracked instead</span>
<span class="p_add">+	 * of struct pages to avoid multiple page-&gt;pfn lookups by each CPU that</span>
<span class="p_add">+	 * receives an IPI in percpu_flush_tlb_batch_pages.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	unsigned int nr_pages;</span>
<span class="p_add">+	unsigned long pfns[BATCH_TLBFLUSH_SIZE];</span>
 
 	/*
 	 * If true then the PTE was dirty when unmapped. The entry must be
<span class="p_header">diff --git a/init/Kconfig b/init/Kconfig</span>
<span class="p_header">index 6e6fa4842250..095b3d470c3f 100644</span>
<span class="p_header">--- a/init/Kconfig</span>
<span class="p_header">+++ b/init/Kconfig</span>
<span class="p_chunk">@@ -904,12 +904,10 @@</span> <span class="p_context"> config ARCH_SUPPORTS_NUMA_BALANCING</span>
 	bool
 
 #
<span class="p_del">-# For architectures that prefer to flush all TLBs after a number of pages</span>
<span class="p_del">-# are unmapped instead of sending one IPI per page to flush. The architecture</span>
<span class="p_del">-# must provide guarantees on what happens if a clean TLB cache entry is</span>
<span class="p_del">-# written after the unmap. Details are in mm/rmap.c near the check for</span>
<span class="p_del">-# should_defer_flush. The architecture should also consider if the full flush</span>
<span class="p_del">-# and the refill costs are offset by the savings of sending fewer IPIs.</span>
<span class="p_add">+# For architectures that have a local TLB flush for a PFN without knowledge</span>
<span class="p_add">+# of the VMA. The architecture must provide guarantees on what happens if</span>
<span class="p_add">+# a clean TLB cache entry is written after the unmap. Details are in mm/rmap.c</span>
<span class="p_add">+# near the check for should_defer_flush.</span>
 config ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
 	bool
 
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 1e36b2fb3e95..0085b0eb720c 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -586,15 +586,12 @@</span> <span class="p_context"> vma_address(struct page *page, struct vm_area_struct *vma)</span>
 #ifdef CONFIG_ARCH_WANT_BATCHED_UNMAP_TLB_FLUSH
 static void percpu_flush_tlb_batch_pages(void *data)
 {
<span class="p_del">-	/*</span>
<span class="p_del">-	 * All TLB entries are flushed on the assumption that it is</span>
<span class="p_del">-	 * cheaper to flush all TLBs and let them be refilled than</span>
<span class="p_del">-	 * flushing individual PFNs. Note that we do not track mm&#39;s</span>
<span class="p_del">-	 * to flush as that might simply be multiple full TLB flushes</span>
<span class="p_del">-	 * for no gain.</span>
<span class="p_del">-	 */</span>
<span class="p_add">+	struct tlbflush_unmap_batch *tlb_ubc = data;</span>
<span class="p_add">+	unsigned int i;</span>
<span class="p_add">+</span>
 	count_vm_tlb_event(NR_TLB_REMOTE_FLUSH_RECEIVED);
<span class="p_del">-	local_flush_tlb();</span>
<span class="p_add">+	for (i = 0; i &lt; tlb_ubc-&gt;nr_pages; i++)</span>
<span class="p_add">+		flush_local_tlb_addr(tlb_ubc-&gt;pfns[i] &lt;&lt; PAGE_SHIFT);</span>
 }
 
 /*
<span class="p_chunk">@@ -608,10 +605,10 @@</span> <span class="p_context"> void try_to_unmap_flush(void)</span>
 	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;
 	int cpu;
 
<span class="p_del">-	if (!tlb_ubc || !tlb_ubc-&gt;flush_required)</span>
<span class="p_add">+	if (!tlb_ubc || !tlb_ubc-&gt;nr_pages)</span>
 		return;
 
<span class="p_del">-	trace_tlb_flush(TLB_REMOTE_SHOOTDOWN, -1UL);</span>
<span class="p_add">+	trace_tlb_flush(TLB_REMOTE_SHOOTDOWN, tlb_ubc-&gt;nr_pages);</span>
 
 	cpu = get_cpu();
 	if (cpumask_test_cpu(cpu, &amp;tlb_ubc-&gt;cpumask))
<span class="p_chunk">@@ -622,7 +619,7 @@</span> <span class="p_context"> void try_to_unmap_flush(void)</span>
 			percpu_flush_tlb_batch_pages, (void *)tlb_ubc, true);
 	}
 	cpumask_clear(&amp;tlb_ubc-&gt;cpumask);
<span class="p_del">-	tlb_ubc-&gt;flush_required = false;</span>
<span class="p_add">+	tlb_ubc-&gt;nr_pages = 0;</span>
 	tlb_ubc-&gt;writable = false;
 	put_cpu();
 }
<span class="p_chunk">@@ -642,7 +639,8 @@</span> <span class="p_context"> static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
 	struct tlbflush_unmap_batch *tlb_ubc = current-&gt;tlb_ubc;
 
 	cpumask_or(&amp;tlb_ubc-&gt;cpumask, &amp;tlb_ubc-&gt;cpumask, mm_cpumask(mm));
<span class="p_del">-	tlb_ubc-&gt;flush_required = true;</span>
<span class="p_add">+	tlb_ubc-&gt;pfns[tlb_ubc-&gt;nr_pages] = page_to_pfn(page);</span>
<span class="p_add">+	tlb_ubc-&gt;nr_pages++;</span>
 
 	/*
 	 * If the PTE was dirty then it&#39;s best to assume it&#39;s writable. The
<span class="p_chunk">@@ -651,6 +649,9 @@</span> <span class="p_context"> static void set_tlb_ubc_flush_pending(struct mm_struct *mm,</span>
 	 */
 	if (writable)
 		tlb_ubc-&gt;writable = true;
<span class="p_add">+</span>
<span class="p_add">+	if (tlb_ubc-&gt;nr_pages == BATCH_TLBFLUSH_SIZE)</span>
<span class="p_add">+		try_to_unmap_flush();</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



