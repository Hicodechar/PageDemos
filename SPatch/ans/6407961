
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[08/11] mm: debug: kill VM_BUG_ON_PAGE - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [08/11] mm: debug: kill VM_BUG_ON_PAGE</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=49271">Sasha Levin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>May 14, 2015, 5:10 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1431623414-1905-9-git-send-email-sasha.levin@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6407961/mbox/"
   >mbox</a>
|
   <a href="/patch/6407961/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6407961/">/patch/6407961/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 909529F1CC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 14 May 2015 17:12:30 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 3DE3A20444
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 14 May 2015 17:12:27 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 8DF0F20397
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 14 May 2015 17:12:23 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1030350AbbENRLn (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 14 May 2015 13:11:43 -0400
Received: from userp1040.oracle.com ([156.151.31.81]:29374 &quot;EHLO
	userp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1030270AbbENRKr (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 14 May 2015 13:10:47 -0400
Received: from userv0021.oracle.com (userv0021.oracle.com [156.151.31.71])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with
	ESMTP id t4EHAXGk016970
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Thu, 14 May 2015 17:10:33 GMT
Received: from aserv0122.oracle.com (aserv0122.oracle.com [141.146.126.236])
	by userv0021.oracle.com (8.13.8/8.13.8) with ESMTP id
	t4EHAXrF010759
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL); 
	Thu, 14 May 2015 17:10:33 GMT
Received: from abhmp0016.oracle.com (abhmp0016.oracle.com [141.146.116.22])
	by aserv0122.oracle.com (8.13.8/8.13.8) with ESMTP id
	t4EHAXlj023862; Thu, 14 May 2015 17:10:33 GMT
Received: from lappy.hsd1.nh.comcast.net (/10.159.228.231)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Thu, 14 May 2015 10:10:32 -0700
From: Sasha Levin &lt;sasha.levin@oracle.com&gt;
To: linux-mm@kvack.org
Cc: akpm@linux-foundation.org, linux-kernel@vger.kernel.org,
	kirill@shutemov.name, Sasha Levin &lt;sasha.levin@oracle.com&gt;
Subject: [PATCH 08/11] mm: debug: kill VM_BUG_ON_PAGE
Date: Thu, 14 May 2015 13:10:11 -0400
Message-Id: &lt;1431623414-1905-9-git-send-email-sasha.levin@oracle.com&gt;
X-Mailer: git-send-email 2.1.0
In-Reply-To: &lt;1431623414-1905-1-git-send-email-sasha.levin@oracle.com&gt;
References: &lt;1431623414-1905-1-git-send-email-sasha.levin@oracle.com&gt;
X-Source-IP: userv0021.oracle.com [156.151.31.71]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	T_RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=49271">Sasha Levin</a> - May 14, 2015, 5:10 p.m.</div>
<pre class="content">
Just use VM_BUG() instead.
<span class="signed-off-by">
Signed-off-by: Sasha Levin &lt;sasha.levin@oracle.com&gt;</span>
---
 arch/x86/mm/gup.c              |    8 +++----
 include/linux/hugetlb.h        |    2 +-
 include/linux/hugetlb_cgroup.h |    4 ++--
 include/linux/mm.h             |   22 +++++++++---------
 include/linux/mmdebug.h        |    8 -------
 include/linux/page-flags.h     |   26 +++++++++++-----------
 include/linux/pagemap.h        |   11 ++++-----
 mm/cleancache.c                |    6 ++---
 mm/compaction.c                |    2 +-
 mm/filemap.c                   |   18 +++++++--------
 mm/gup.c                       |    6 ++---
 mm/huge_memory.c               |   38 +++++++++++++++----------------
 mm/hugetlb.c                   |   14 ++++++------
 mm/hugetlb_cgroup.c            |    2 +-
 mm/internal.h                  |    8 +++----
 mm/ksm.c                       |   13 ++++++-----
 mm/memcontrol.c                |   48 ++++++++++++++++++++--------------------
 mm/memory.c                    |    8 +++----
 mm/migrate.c                   |    6 ++---
 mm/mlock.c                     |    4 ++--
 mm/page_alloc.c                |   26 +++++++++++-----------
 mm/page_io.c                   |    4 ++--
 mm/rmap.c                      |   14 ++++++------
 mm/shmem.c                     |   10 +++++----
 mm/slub.c                      |    4 ++--
 mm/swap.c                      |   39 ++++++++++++++++----------------
 mm/swap_state.c                |   16 +++++++-------
 mm/swapfile.c                  |    8 +++----
 mm/vmscan.c                    |   24 ++++++++++----------
 29 files changed, 198 insertions(+), 201 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/mm/gup.c b/arch/x86/mm/gup.c</span>
<span class="p_header">index 81bf3d2..b04ea9e 100644</span>
<span class="p_header">--- a/arch/x86/mm/gup.c</span>
<span class="p_header">+++ b/arch/x86/mm/gup.c</span>
<span class="p_chunk">@@ -108,8 +108,8 @@</span> <span class="p_context"> static noinline int gup_pte_range(pmd_t pmd, unsigned long addr,</span>
 
 static inline void get_head_page_multiple(struct page *page, int nr)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(page != compound_head(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(page_count(page) == 0, page);</span>
<span class="p_add">+	VM_BUG(page != compound_head(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(page_count(page) == 0, &quot;%pZp&quot;, page);</span>
 	atomic_add(nr, &amp;page-&gt;_count);
 	SetPageReferenced(page);
 }
<span class="p_chunk">@@ -135,7 +135,7 @@</span> <span class="p_context"> static noinline int gup_huge_pmd(pmd_t pmd, unsigned long addr,</span>
 	head = pte_page(pte);
 	page = head + ((addr &amp; ~PMD_MASK) &gt;&gt; PAGE_SHIFT);
 	do {
<span class="p_del">-		VM_BUG_ON_PAGE(compound_head(page) != head, page);</span>
<span class="p_add">+		VM_BUG(compound_head(page) != head, &quot;%pZp&quot;, page);</span>
 		pages[*nr] = page;
 		if (PageTail(page))
 			get_huge_page_tail(page);
<span class="p_chunk">@@ -212,7 +212,7 @@</span> <span class="p_context"> static noinline int gup_huge_pud(pud_t pud, unsigned long addr,</span>
 	head = pte_page(pte);
 	page = head + ((addr &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);
 	do {
<span class="p_del">-		VM_BUG_ON_PAGE(compound_head(page) != head, page);</span>
<span class="p_add">+		VM_BUG(compound_head(page) != head, &quot;%pZp&quot;, page);</span>
 		pages[*nr] = page;
 		if (PageTail(page))
 			get_huge_page_tail(page);
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 2050261..0da5cc4 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -415,7 +415,7 @@</span> <span class="p_context"> static inline pte_t arch_make_huge_pte(pte_t entry, struct vm_area_struct *vma,</span>
 
 static inline struct hstate *page_hstate(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHuge(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHuge(page), &quot;%pZp&quot;, page);</span>
 	return size_to_hstate(PAGE_SIZE &lt;&lt; compound_order(page));
 }
 
<span class="p_header">diff --git a/include/linux/hugetlb_cgroup.h b/include/linux/hugetlb_cgroup.h</span>
<span class="p_header">index bcc853e..7cca841 100644</span>
<span class="p_header">--- a/include/linux/hugetlb_cgroup.h</span>
<span class="p_header">+++ b/include/linux/hugetlb_cgroup.h</span>
<span class="p_chunk">@@ -28,7 +28,7 @@</span> <span class="p_context"> struct hugetlb_cgroup;</span>
 
 static inline struct hugetlb_cgroup *hugetlb_cgroup_from_page(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHuge(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHuge(page), &quot;%pZp&quot;, page);</span>
 
 	if (compound_order(page) &lt; HUGETLB_CGROUP_MIN_ORDER)
 		return NULL;
<span class="p_chunk">@@ -38,7 +38,7 @@</span> <span class="p_context"> static inline struct hugetlb_cgroup *hugetlb_cgroup_from_page(struct page *page)</span>
 static inline
 int set_hugetlb_cgroup(struct page *page, struct hugetlb_cgroup *h_cg)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHuge(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHuge(page), &quot;%pZp&quot;, page);</span>
 
 	if (compound_order(page) &lt; HUGETLB_CGROUP_MIN_ORDER)
 		return -1;
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index be9247c..3affbc8 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -340,7 +340,7 @@</span> <span class="p_context"> static inline int get_freepage_migratetype(struct page *page)</span>
  */
 static inline int put_page_testzero(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) == 0, page);</span>
<span class="p_add">+	VM_BUG(atomic_read(&amp;page-&gt;_count) == 0, &quot;%pZp&quot;, page);</span>
 	return atomic_dec_and_test(&amp;page-&gt;_count);
 }
 
<span class="p_chunk">@@ -404,7 +404,7 @@</span> <span class="p_context"> extern void kvfree(const void *addr);</span>
 static inline void compound_lock(struct page *page)
 {
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_del">-	VM_BUG_ON_PAGE(PageSlab(page), page);</span>
<span class="p_add">+	VM_BUG(PageSlab(page), &quot;%pZp&quot;, page);</span>
 	bit_spin_lock(PG_compound_lock, &amp;page-&gt;flags);
 #endif
 }
<span class="p_chunk">@@ -412,7 +412,7 @@</span> <span class="p_context"> static inline void compound_lock(struct page *page)</span>
 static inline void compound_unlock(struct page *page)
 {
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_del">-	VM_BUG_ON_PAGE(PageSlab(page), page);</span>
<span class="p_add">+	VM_BUG(PageSlab(page), &quot;%pZp&quot;, page);</span>
 	bit_spin_unlock(PG_compound_lock, &amp;page-&gt;flags);
 #endif
 }
<span class="p_chunk">@@ -448,7 +448,7 @@</span> <span class="p_context"> static inline void page_mapcount_reset(struct page *page)</span>
 
 static inline int page_mapcount(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(PageSlab(page), page);</span>
<span class="p_add">+	VM_BUG(PageSlab(page), &quot;%pZp&quot;, page);</span>
 	return atomic_read(&amp;page-&gt;_mapcount) + 1;
 }
 
<span class="p_chunk">@@ -472,7 +472,7 @@</span> <span class="p_context"> static inline bool __compound_tail_refcounted(struct page *page)</span>
  */
 static inline bool compound_tail_refcounted(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHead(page), &quot;%pZp&quot;, page);</span>
 	return __compound_tail_refcounted(page);
 }
 
<span class="p_chunk">@@ -481,9 +481,9 @@</span> <span class="p_context"> static inline void get_huge_page_tail(struct page *page)</span>
 	/*
 	 * __split_huge_page_refcount() cannot run from under us.
 	 */
<span class="p_del">-	VM_BUG_ON_PAGE(!PageTail(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) != 0, page);</span>
<span class="p_add">+	VM_BUG(!PageTail(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(page_mapcount(page) &lt; 0, &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(atomic_read(&amp;page-&gt;_count) != 0, &quot;%pZp&quot;, page);</span>
 	if (compound_tail_refcounted(page-&gt;first_page))
 		atomic_inc(&amp;page-&gt;_mapcount);
 }
<span class="p_chunk">@@ -499,7 +499,7 @@</span> <span class="p_context"> static inline void get_page(struct page *page)</span>
 	 * Getting a normal page or the head of a compound page
 	 * requires to already have an elevated page-&gt;_count.
 	 */
<span class="p_del">-	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) &lt;= 0, page);</span>
<span class="p_add">+	VM_BUG(atomic_read(&amp;page-&gt;_count) &lt;= 0, &quot;%pZp&quot;, page);</span>
 	atomic_inc(&amp;page-&gt;_count);
 }
 
<span class="p_chunk">@@ -1441,7 +1441,7 @@</span> <span class="p_context"> static inline bool ptlock_init(struct page *page)</span>
 	 * slab code uses page-&gt;slab_cache and page-&gt;first_page (for tail
 	 * pages), which share storage with page-&gt;ptl.
 	 */
<span class="p_del">-	VM_BUG_ON_PAGE(*(unsigned long *)&amp;page-&gt;ptl, page);</span>
<span class="p_add">+	VM_BUG(*(unsigned long *)&amp;page-&gt;ptl, &quot;%pZp&quot;, page);</span>
 	if (!ptlock_alloc(page))
 		return false;
 	spin_lock_init(ptlock_ptr(page));
<span class="p_chunk">@@ -1538,7 +1538,7 @@</span> <span class="p_context"> static inline bool pgtable_pmd_page_ctor(struct page *page)</span>
 static inline void pgtable_pmd_page_dtor(struct page *page)
 {
 #ifdef CONFIG_TRANSPARENT_HUGEPAGE
<span class="p_del">-	VM_BUG_ON_PAGE(page-&gt;pmd_huge_pte, page);</span>
<span class="p_add">+	VM_BUG(page-&gt;pmd_huge_pte, &quot;%pZp&quot;, page);</span>
 #endif
 	ptlock_free(page);
 }
<span class="p_header">diff --git a/include/linux/mmdebug.h b/include/linux/mmdebug.h</span>
<span class="p_header">index 42f41e3..f43f868 100644</span>
<span class="p_header">--- a/include/linux/mmdebug.h</span>
<span class="p_header">+++ b/include/linux/mmdebug.h</span>
<span class="p_chunk">@@ -20,13 +20,6 @@</span> <span class="p_context"> char *format_mm(const struct mm_struct *mm, char *buf, char *end);</span>
 		}							\
 	} while (0)
 #define VM_BUG_ON(cond) VM_BUG(cond, &quot;%s\n&quot;, __stringify(cond))
<span class="p_del">-#define VM_BUG_ON_PAGE(cond, page)					\</span>
<span class="p_del">-	do {								\</span>
<span class="p_del">-		if (unlikely(cond)) {					\</span>
<span class="p_del">-			pr_emerg(&quot;%pZp&quot;, page);				\</span>
<span class="p_del">-			BUG();						\</span>
<span class="p_del">-		}							\</span>
<span class="p_del">-	} while (0)</span>
 #define VM_BUG_ON_VMA(cond, vma)					\
 	do {								\
 		if (unlikely(cond)) {					\
<span class="p_chunk">@@ -55,7 +48,6 @@</span> <span class="p_context"> static char *format_mm(const struct mm_struct *mm, char *buf, char *end)</span>
 }
 #define VM_BUG(cond, fmt...) BUILD_BUG_ON_INVALID(cond)
 #define VM_BUG_ON(cond) BUILD_BUG_ON_INVALID(cond)
<span class="p_del">-#define VM_BUG_ON_PAGE(cond, page) VM_BUG_ON(cond)</span>
 #define VM_BUG_ON_VMA(cond, vma) VM_BUG_ON(cond)
 #define VM_BUG_ON_MM(cond, mm) VM_BUG_ON(cond)
 #define VM_WARN_ON(cond) BUILD_BUG_ON_INVALID(cond)
<span class="p_header">diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h</span>
<span class="p_header">index 91b7f9b..f1a18ad 100644</span>
<span class="p_header">--- a/include/linux/page-flags.h</span>
<span class="p_header">+++ b/include/linux/page-flags.h</span>
<span class="p_chunk">@@ -139,13 +139,13 @@</span> <span class="p_context"> enum pageflags {</span>
 #define PF_HEAD(page, enforce)	compound_head(page)
 #define PF_NO_TAIL(page, enforce) ({					\
 		if (enforce)						\
<span class="p_del">-			VM_BUG_ON_PAGE(PageTail(page), page);		\</span>
<span class="p_add">+			VM_BUG(PageTail(page), &quot;%pZp&quot;, page);		\</span>
 		else							\
 			page = compound_head(page);			\
 		page;})
 #define PF_NO_COMPOUND(page, enforce) ({					\
 		if (enforce)						\
<span class="p_del">-			VM_BUG_ON_PAGE(PageCompound(page), page);	\</span>
<span class="p_add">+			VM_BUG(PageCompound(page), &quot;%pZp&quot;, page);	\</span>
 		page;})
 
 /*
<span class="p_chunk">@@ -429,14 +429,14 @@</span> <span class="p_context"> static inline int PageUptodate(struct page *page)</span>
 
 static inline void __SetPageUptodate(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(PageTail(page), page);</span>
<span class="p_add">+	VM_BUG(PageTail(page), &quot;%pZp&quot;, page);</span>
 	smp_wmb();
 	__set_bit(PG_uptodate, &amp;page-&gt;flags);
 }
 
 static inline void SetPageUptodate(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(PageTail(page), page);</span>
<span class="p_add">+	VM_BUG(PageTail(page), &quot;%pZp&quot;, page);</span>
 	/*
 	 * Memory barrier must be issued before setting the PG_uptodate bit,
 	 * so that all previous stores issued in order to bring the page
<span class="p_chunk">@@ -572,7 +572,7 @@</span> <span class="p_context"> static inline bool page_huge_active(struct page *page)</span>
  */
 static inline int PageTransHuge(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(PageTail(page), page);</span>
<span class="p_add">+	VM_BUG(PageTail(page), &quot;%pZp&quot;, page);</span>
 	return PageHead(page);
 }
 
<span class="p_chunk">@@ -620,13 +620,13 @@</span> <span class="p_context"> static inline int PageBuddy(struct page *page)</span>
 
 static inline void __SetPageBuddy(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_mapcount) != -1, page);</span>
<span class="p_add">+	VM_BUG(atomic_read(&amp;page-&gt;_mapcount) != -1, &quot;%pZp&quot;, page);</span>
 	atomic_set(&amp;page-&gt;_mapcount, PAGE_BUDDY_MAPCOUNT_VALUE);
 }
 
 static inline void __ClearPageBuddy(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageBuddy(page), page);</span>
<span class="p_add">+	VM_BUG(!PageBuddy(page), &quot;%pZp&quot;, page);</span>
 	atomic_set(&amp;page-&gt;_mapcount, -1);
 }
 
<span class="p_chunk">@@ -639,13 +639,13 @@</span> <span class="p_context"> static inline int PageBalloon(struct page *page)</span>
 
 static inline void __SetPageBalloon(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_mapcount) != -1, page);</span>
<span class="p_add">+	VM_BUG(atomic_read(&amp;page-&gt;_mapcount) != -1, &quot;%pZp&quot;, page);</span>
 	atomic_set(&amp;page-&gt;_mapcount, PAGE_BALLOON_MAPCOUNT_VALUE);
 }
 
 static inline void __ClearPageBalloon(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageBalloon(page), page);</span>
<span class="p_add">+	VM_BUG(!PageBalloon(page), &quot;%pZp&quot;, page);</span>
 	atomic_set(&amp;page-&gt;_mapcount, -1);
 }
 
<span class="p_chunk">@@ -655,25 +655,25 @@</span> <span class="p_context"> static inline void __ClearPageBalloon(struct page *page)</span>
  */
 static inline int PageSlabPfmemalloc(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageSlab(page), page);</span>
<span class="p_add">+	VM_BUG(!PageSlab(page), &quot;%pZp&quot;, page);</span>
 	return PageActive(page);
 }
 
 static inline void SetPageSlabPfmemalloc(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageSlab(page), page);</span>
<span class="p_add">+	VM_BUG(!PageSlab(page), &quot;%pZp&quot;, page);</span>
 	SetPageActive(page);
 }
 
 static inline void __ClearPageSlabPfmemalloc(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageSlab(page), page);</span>
<span class="p_add">+	VM_BUG(!PageSlab(page), &quot;%pZp&quot;, page);</span>
 	__ClearPageActive(page);
 }
 
 static inline void ClearPageSlabPfmemalloc(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageSlab(page), page);</span>
<span class="p_add">+	VM_BUG(!PageSlab(page), &quot;%pZp&quot;, page);</span>
 	ClearPageActive(page);
 }
 
<span class="p_header">diff --git a/include/linux/pagemap.h b/include/linux/pagemap.h</span>
<span class="p_header">index 7c37907..fa9ba8b 100644</span>
<span class="p_header">--- a/include/linux/pagemap.h</span>
<span class="p_header">+++ b/include/linux/pagemap.h</span>
<span class="p_chunk">@@ -157,7 +157,7 @@</span> <span class="p_context"> static inline int page_cache_get_speculative(struct page *page)</span>
 	 * disabling preempt, and hence no need for the &quot;speculative get&quot; that
 	 * SMP requires.
 	 */
<span class="p_del">-	VM_BUG_ON_PAGE(page_count(page) == 0, page);</span>
<span class="p_add">+	VM_BUG(page_count(page) == 0, &quot;%pZp&quot;, page);</span>
 	atomic_inc(&amp;page-&gt;_count);
 
 #else
<span class="p_chunk">@@ -170,7 +170,7 @@</span> <span class="p_context"> static inline int page_cache_get_speculative(struct page *page)</span>
 		return 0;
 	}
 #endif
<span class="p_del">-	VM_BUG_ON_PAGE(PageTail(page), page);</span>
<span class="p_add">+	VM_BUG(PageTail(page), &quot;%pZp&quot;, page);</span>
 
 	return 1;
 }
<span class="p_chunk">@@ -186,14 +186,15 @@</span> <span class="p_context"> static inline int page_cache_add_speculative(struct page *page, int count)</span>
 # ifdef CONFIG_PREEMPT_COUNT
 	VM_BUG_ON(!in_atomic());
 # endif
<span class="p_del">-	VM_BUG_ON_PAGE(page_count(page) == 0, page);</span>
<span class="p_add">+	VM_BUG(page_count(page) == 0, &quot;%pZp&quot;, page);</span>
 	atomic_add(count, &amp;page-&gt;_count);
 
 #else
 	if (unlikely(!atomic_add_unless(&amp;page-&gt;_count, count, 0)))
 		return 0;
 #endif
<span class="p_del">-	VM_BUG_ON_PAGE(PageCompound(page) &amp;&amp; page != compound_head(page), page);</span>
<span class="p_add">+	VM_BUG(PageCompound(page) &amp;&amp; page != compound_head(page), &quot;%pZp&quot;,</span>
<span class="p_add">+	       page);</span>
 
 	return 1;
 }
<span class="p_chunk">@@ -205,7 +206,7 @@</span> <span class="p_context"> static inline int page_freeze_refs(struct page *page, int count)</span>
 
 static inline void page_unfreeze_refs(struct page *page, int count)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(page_count(page) != 0, page);</span>
<span class="p_add">+	VM_BUG(page_count(page) != 0, &quot;%pZp&quot;, page);</span>
 	VM_BUG_ON(count == 0);
 
 	atomic_set(&amp;page-&gt;_count, count);
<span class="p_header">diff --git a/mm/cleancache.c b/mm/cleancache.c</span>
<span class="p_header">index 8fc5081..d4d5ce0 100644</span>
<span class="p_header">--- a/mm/cleancache.c</span>
<span class="p_header">+++ b/mm/cleancache.c</span>
<span class="p_chunk">@@ -185,7 +185,7 @@</span> <span class="p_context"> int __cleancache_get_page(struct page *page)</span>
 		goto out;
 	}
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 	pool_id = page-&gt;mapping-&gt;host-&gt;i_sb-&gt;cleancache_poolid;
 	if (pool_id &lt; 0)
 		goto out;
<span class="p_chunk">@@ -223,7 +223,7 @@</span> <span class="p_context"> void __cleancache_put_page(struct page *page)</span>
 		return;
 	}
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 	pool_id = page-&gt;mapping-&gt;host-&gt;i_sb-&gt;cleancache_poolid;
 	if (pool_id &gt;= 0 &amp;&amp;
 		cleancache_get_key(page-&gt;mapping-&gt;host, &amp;key) &gt;= 0) {
<span class="p_chunk">@@ -252,7 +252,7 @@</span> <span class="p_context"> void __cleancache_invalidate_page(struct address_space *mapping,</span>
 		return;
 
 	if (pool_id &gt;= 0) {
<span class="p_del">-		VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+		VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 		if (cleancache_get_key(mapping-&gt;host, &amp;key) &gt;= 0) {
 			cleancache_ops-&gt;invalidate_page(pool_id,
 					key, page-&gt;index);
<span class="p_header">diff --git a/mm/compaction.c b/mm/compaction.c</span>
<span class="p_header">index 6ef2fdf..170bf6c 100644</span>
<span class="p_header">--- a/mm/compaction.c</span>
<span class="p_header">+++ b/mm/compaction.c</span>
<span class="p_chunk">@@ -779,7 +779,7 @@</span> <span class="p_context"> isolate_migratepages_block(struct compact_control *cc, unsigned long low_pfn,</span>
 		if (__isolate_lru_page(page, isolate_mode) != 0)
 			continue;
 
<span class="p_del">-		VM_BUG_ON_PAGE(PageCompound(page), page);</span>
<span class="p_add">+		VM_BUG(PageCompound(page), &quot;%pZp&quot;, page);</span>
 
 		/* Successfully isolated */
 		del_page_from_lru_list(page, lruvec, page_lru(page));
<span class="p_header">diff --git a/mm/filemap.c b/mm/filemap.c</span>
<span class="p_header">index 6ad0a80..ec1ab0aa 100644</span>
<span class="p_header">--- a/mm/filemap.c</span>
<span class="p_header">+++ b/mm/filemap.c</span>
<span class="p_chunk">@@ -462,9 +462,9 @@</span> <span class="p_context"> int replace_page_cache_page(struct page *old, struct page *new, gfp_t gfp_mask)</span>
 {
 	int error;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(old), old);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(new), new);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(new-&gt;mapping, new);</span>
<span class="p_add">+	VM_BUG(!PageLocked(old), &quot;%pZp&quot;, old);</span>
<span class="p_add">+	VM_BUG(!PageLocked(new), &quot;%pZp&quot;, new);</span>
<span class="p_add">+	VM_BUG(new-&gt;mapping, &quot;%pZp&quot;, new);</span>
 
 	error = radix_tree_preload(gfp_mask &amp; ~__GFP_HIGHMEM);
 	if (!error) {
<span class="p_chunk">@@ -549,8 +549,8 @@</span> <span class="p_context"> static int __add_to_page_cache_locked(struct page *page,</span>
 	struct mem_cgroup *memcg;
 	int error;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(PageSwapBacked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(PageSwapBacked(page), &quot;%pZp&quot;, page);</span>
 
 	if (!huge) {
 		error = mem_cgroup_try_charge(page, current-&gt;mm,
<span class="p_chunk">@@ -743,7 +743,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(add_page_wait_queue);</span>
 void unlock_page(struct page *page)
 {
 	page = compound_head(page);
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 	clear_bit_unlock(PG_locked, &amp;page-&gt;flags);
 	smp_mb__after_atomic();
 	wake_up_page(page, PG_locked);
<span class="p_chunk">@@ -1036,7 +1036,7 @@</span> <span class="p_context"> repeat:</span>
 			page_cache_release(page);
 			goto repeat;
 		}
<span class="p_del">-		VM_BUG_ON_PAGE(page-&gt;index != offset, page);</span>
<span class="p_add">+		VM_BUG(page-&gt;index != offset, &quot;%pZp&quot;, page);</span>
 	}
 	return page;
 }
<span class="p_chunk">@@ -1093,7 +1093,7 @@</span> <span class="p_context"> repeat:</span>
 			page_cache_release(page);
 			goto repeat;
 		}
<span class="p_del">-		VM_BUG_ON_PAGE(page-&gt;index != offset, page);</span>
<span class="p_add">+		VM_BUG(page-&gt;index != offset, &quot;%pZp&quot;, page);</span>
 	}
 
 	if (page &amp;&amp; (fgp_flags &amp; FGP_ACCESSED))
<span class="p_chunk">@@ -1914,7 +1914,7 @@</span> <span class="p_context"> retry_find:</span>
 		put_page(page);
 		goto retry_find;
 	}
<span class="p_del">-	VM_BUG_ON_PAGE(page-&gt;index != offset, page);</span>
<span class="p_add">+	VM_BUG(page-&gt;index != offset, &quot;%pZp&quot;, page);</span>
 
 	/*
 	 * We have a locked page in the page cache, now we need to check
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 6297f6b..743648e 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -1084,7 +1084,7 @@</span> <span class="p_context"> static int gup_huge_pmd(pmd_t orig, pmd_t *pmdp, unsigned long addr,</span>
 	page = head + ((addr &amp; ~PMD_MASK) &gt;&gt; PAGE_SHIFT);
 	tail = page;
 	do {
<span class="p_del">-		VM_BUG_ON_PAGE(compound_head(page) != head, page);</span>
<span class="p_add">+		VM_BUG(compound_head(page) != head, &quot;%pZp&quot;, page);</span>
 		pages[*nr] = page;
 		(*nr)++;
 		page++;
<span class="p_chunk">@@ -1131,7 +1131,7 @@</span> <span class="p_context"> static int gup_huge_pud(pud_t orig, pud_t *pudp, unsigned long addr,</span>
 	page = head + ((addr &amp; ~PUD_MASK) &gt;&gt; PAGE_SHIFT);
 	tail = page;
 	do {
<span class="p_del">-		VM_BUG_ON_PAGE(compound_head(page) != head, page);</span>
<span class="p_add">+		VM_BUG(compound_head(page) != head, &quot;%pZp&quot;, page);</span>
 		pages[*nr] = page;
 		(*nr)++;
 		page++;
<span class="p_chunk">@@ -1174,7 +1174,7 @@</span> <span class="p_context"> static int gup_huge_pgd(pgd_t orig, pgd_t *pgdp, unsigned long addr,</span>
 	page = head + ((addr &amp; ~PGDIR_MASK) &gt;&gt; PAGE_SHIFT);
 	tail = page;
 	do {
<span class="p_del">-		VM_BUG_ON_PAGE(compound_head(page) != head, page);</span>
<span class="p_add">+		VM_BUG(compound_head(page) != head, &quot;%pZp&quot;, page);</span>
 		pages[*nr] = page;
 		(*nr)++;
 		page++;
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index e103a9a..82ccd2c 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -723,7 +723,7 @@</span> <span class="p_context"> static int __do_huge_pmd_anonymous_page(struct mm_struct *mm,</span>
 	pgtable_t pgtable;
 	spinlock_t *ptl;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageCompound(page), page);</span>
<span class="p_add">+	VM_BUG(!PageCompound(page), &quot;%pZp&quot;, page);</span>
 
 	if (mem_cgroup_try_charge(page, mm, gfp, &amp;memcg))
 		return VM_FAULT_OOM;
<span class="p_chunk">@@ -897,7 +897,7 @@</span> <span class="p_context"> int copy_huge_pmd(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 		goto out;
 	}
 	src_page = pmd_page(pmd);
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHead(src_page), src_page);</span>
<span class="p_add">+	VM_BUG(!PageHead(src_page), &quot;%pZp&quot;, src_page);</span>
 	get_page(src_page);
 	page_dup_rmap(src_page);
 	add_mm_counter(dst_mm, MM_ANONPAGES, HPAGE_PMD_NR);
<span class="p_chunk">@@ -1029,7 +1029,7 @@</span> <span class="p_context"> static int do_huge_pmd_wp_page_fallback(struct mm_struct *mm,</span>
 	ptl = pmd_lock(mm, pmd);
 	if (unlikely(!pmd_same(*pmd, orig_pmd)))
 		goto out_free_pages;
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHead(page), &quot;%pZp&quot;, page);</span>
 
 	pmdp_clear_flush_notify(vma, haddr, pmd);
 	/* leave pmd empty until pte is filled */
<span class="p_chunk">@@ -1101,7 +1101,7 @@</span> <span class="p_context"> int do_huge_pmd_wp_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		goto out_unlock;
 
 	page = pmd_page(orig_pmd);
<span class="p_del">-	VM_BUG_ON_PAGE(!PageCompound(page) || !PageHead(page), page);</span>
<span class="p_add">+	VM_BUG(!PageCompound(page) || !PageHead(page), &quot;%pZp&quot;, page);</span>
 	if (page_mapcount(page) == 1) {
 		pmd_t entry;
 		entry = pmd_mkyoung(orig_pmd);
<span class="p_chunk">@@ -1184,7 +1184,7 @@</span> <span class="p_context"> alloc:</span>
 			add_mm_counter(mm, MM_ANONPAGES, HPAGE_PMD_NR);
 			put_huge_zero_page();
 		} else {
<span class="p_del">-			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+			VM_BUG(!PageHead(page), &quot;%pZp&quot;, page);</span>
 			page_remove_rmap(page);
 			put_page(page);
 		}
<span class="p_chunk">@@ -1222,7 +1222,7 @@</span> <span class="p_context"> struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
 		goto out;
 
 	page = pmd_page(*pmd);
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHead(page), &quot;%pZp&quot;, page);</span>
 	if (flags &amp; FOLL_TOUCH) {
 		pmd_t _pmd;
 		/*
<span class="p_chunk">@@ -1247,7 +1247,7 @@</span> <span class="p_context"> struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
 		}
 	}
 	page += (addr &amp; ~HPAGE_PMD_MASK) &gt;&gt; PAGE_SHIFT;
<span class="p_del">-	VM_BUG_ON_PAGE(!PageCompound(page), page);</span>
<span class="p_add">+	VM_BUG(!PageCompound(page), &quot;%pZp&quot;, page);</span>
 	if (flags &amp; FOLL_GET)
 		get_page_foll(page);
 
<span class="p_chunk">@@ -1400,7 +1400,7 @@</span> <span class="p_context"> int madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 
 		/* No hugepage in swapcache */
 		page = pmd_page(orig_pmd);
<span class="p_del">-		VM_BUG_ON_PAGE(PageSwapCache(page), page);</span>
<span class="p_add">+		VM_BUG(PageSwapCache(page), &quot;%pZp&quot;, page);</span>
 
 		orig_pmd = pmd_mkold(orig_pmd);
 		orig_pmd = pmd_mkclean(orig_pmd);
<span class="p_chunk">@@ -1441,9 +1441,9 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		} else {
 			page = pmd_page(orig_pmd);
 			page_remove_rmap(page);
<span class="p_del">-			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_add">+			VM_BUG(page_mapcount(page) &lt; 0, &quot;%pZp&quot;, page);</span>
 			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);
<span class="p_del">-			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+			VM_BUG(!PageHead(page), &quot;%pZp&quot;, page);</span>
 			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);
 			spin_unlock(ptl);
 			tlb_remove_page(tlb, page);
<span class="p_chunk">@@ -2189,9 +2189,9 @@</span> <span class="p_context"> static int __collapse_huge_page_isolate(struct vm_area_struct *vma,</span>
 		if (unlikely(!page))
 			goto out;
 
<span class="p_del">-		VM_BUG_ON_PAGE(PageCompound(page), page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!PageAnon(page), page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!PageSwapBacked(page), page);</span>
<span class="p_add">+		VM_BUG(PageCompound(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+		VM_BUG(!PageAnon(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+		VM_BUG(!PageSwapBacked(page), &quot;%pZp&quot;, page);</span>
 
 		/*
 		 * We can do it before isolate_lru_page because the
<span class="p_chunk">@@ -2234,8 +2234,8 @@</span> <span class="p_context"> static int __collapse_huge_page_isolate(struct vm_area_struct *vma,</span>
 		}
 		/* 0 stands for page_is_file_cache(page) == false */
 		inc_zone_page_state(page, NR_ISOLATED_ANON + 0);
<span class="p_del">-		VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_add">+		VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+		VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
 
 		/* If there is no mapped pte young don&#39;t collapse the page */
 		if (pte_young(pteval) || PageReferenced(page) ||
<span class="p_chunk">@@ -2277,7 +2277,7 @@</span> <span class="p_context"> static void __collapse_huge_page_copy(pte_t *pte, struct page *page,</span>
 		} else {
 			src_page = pte_page(pteval);
 			copy_user_highpage(page, src_page, address, vma);
<span class="p_del">-			VM_BUG_ON_PAGE(page_mapcount(src_page) != 1, src_page);</span>
<span class="p_add">+			VM_BUG(page_mapcount(src_page) != 1, &quot;%pZp&quot;, src_page);</span>
 			release_pte_page(src_page);
 			/*
 			 * ptl mostly unnecessary, but preempt has to
<span class="p_chunk">@@ -2380,7 +2380,7 @@</span> <span class="p_context"> khugepaged_alloc_page(struct page **hpage, gfp_t gfp, struct mm_struct *mm,</span>
 		       struct vm_area_struct *vma, unsigned long address,
 		       int node)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(*hpage, *hpage);</span>
<span class="p_add">+	VM_BUG(*hpage, &quot;%pZp&quot;, *hpage);</span>
 
 	/*
 	 * Before allocating the hugepage, release the mmap_sem read lock.
<span class="p_chunk">@@ -2654,7 +2654,7 @@</span> <span class="p_context"> static int khugepaged_scan_pmd(struct mm_struct *mm,</span>
 		if (khugepaged_scan_abort(node))
 			goto out_unmap;
 		khugepaged_node_load[node]++;
<span class="p_del">-		VM_BUG_ON_PAGE(PageCompound(page), page);</span>
<span class="p_add">+		VM_BUG(PageCompound(page), &quot;%pZp&quot;, page);</span>
 		if (!PageLRU(page) || PageLocked(page) || !PageAnon(page))
 			goto out_unmap;
 		/*
<span class="p_chunk">@@ -2952,7 +2952,7 @@</span> <span class="p_context"> again:</span>
 		return;
 	}
 	page = pmd_page(*pmd);
<span class="p_del">-	VM_BUG_ON_PAGE(!page_count(page), page);</span>
<span class="p_add">+	VM_BUG(!page_count(page), &quot;%pZp&quot;, page);</span>
 	get_page(page);
 	spin_unlock(ptl);
 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 716465a..55c75da 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -901,7 +901,7 @@</span> <span class="p_context"> static void update_and_free_page(struct hstate *h, struct page *page)</span>
 				1 &lt;&lt; PG_active | 1 &lt;&lt; PG_private |
 				1 &lt;&lt; PG_writeback);
 	}
<span class="p_del">-	VM_BUG_ON_PAGE(hugetlb_cgroup_from_page(page), page);</span>
<span class="p_add">+	VM_BUG(hugetlb_cgroup_from_page(page), &quot;%pZp&quot;, page);</span>
 	set_compound_page_dtor(page, NULL);
 	set_page_refcounted(page);
 	if (hstate_is_gigantic(h)) {
<span class="p_chunk">@@ -932,20 +932,20 @@</span> <span class="p_context"> struct hstate *size_to_hstate(unsigned long size)</span>
  */
 bool page_huge_active(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHuge(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHuge(page), &quot;%pZp&quot;, page);</span>
 	return PageHead(page) &amp;&amp; PagePrivate(&amp;page[1]);
 }
 
 /* never called for tail page */
 static void set_page_huge_active(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHeadHuge(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHeadHuge(page), &quot;%pZp&quot;, page);</span>
 	SetPagePrivate(&amp;page[1]);
 }
 
 static void clear_page_huge_active(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHeadHuge(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHeadHuge(page), &quot;%pZp&quot;, page);</span>
 	ClearPagePrivate(&amp;page[1]);
 }
 
<span class="p_chunk">@@ -1373,7 +1373,7 @@</span> <span class="p_context"> retry:</span>
 		 * no users -- drop the buddy allocator&#39;s reference.
 		 */
 		put_page_testzero(page);
<span class="p_del">-		VM_BUG_ON_PAGE(page_count(page), page);</span>
<span class="p_add">+		VM_BUG(page_count(page), &quot;%pZp&quot;, page);</span>
 		enqueue_huge_page(h, page);
 	}
 free:
<span class="p_chunk">@@ -3938,7 +3938,7 @@</span> <span class="p_context"> bool isolate_huge_page(struct page *page, struct list_head *list)</span>
 {
 	bool ret = true;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHead(page), &quot;%pZp&quot;, page);</span>
 	spin_lock(&amp;hugetlb_lock);
 	if (!page_huge_active(page) || !get_page_unless_zero(page)) {
 		ret = false;
<span class="p_chunk">@@ -3953,7 +3953,7 @@</span> <span class="p_context"> unlock:</span>
 
 void putback_active_hugepage(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHead(page), &quot;%pZp&quot;, page);</span>
 	spin_lock(&amp;hugetlb_lock);
 	set_page_huge_active(page);
 	list_move_tail(&amp;page-&gt;lru, &amp;(page_hstate(page))-&gt;hugepage_activelist);
<span class="p_header">diff --git a/mm/hugetlb_cgroup.c b/mm/hugetlb_cgroup.c</span>
<span class="p_header">index 6e00574..9df90f5 100644</span>
<span class="p_header">--- a/mm/hugetlb_cgroup.c</span>
<span class="p_header">+++ b/mm/hugetlb_cgroup.c</span>
<span class="p_chunk">@@ -403,7 +403,7 @@</span> <span class="p_context"> void hugetlb_cgroup_migrate(struct page *oldhpage, struct page *newhpage)</span>
 	if (hugetlb_cgroup_disabled())
 		return;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHuge(oldhpage), oldhpage);</span>
<span class="p_add">+	VM_BUG(!PageHuge(oldhpage), &quot;%pZp&quot;, oldhpage);</span>
 	spin_lock(&amp;hugetlb_lock);
 	h_cg = hugetlb_cgroup_from_page(oldhpage);
 	set_hugetlb_cgroup(oldhpage, NULL);
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index a48cbef..b7d9a96 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -42,8 +42,8 @@</span> <span class="p_context"> static inline unsigned long ra_submit(struct file_ra_state *ra,</span>
  */
 static inline void set_page_refcounted(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(PageTail(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count), page);</span>
<span class="p_add">+	VM_BUG(PageTail(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(atomic_read(&amp;page-&gt;_count), &quot;%pZp&quot;, page);</span>
 	set_page_count(page, 1);
 }
 
<span class="p_chunk">@@ -61,7 +61,7 @@</span> <span class="p_context"> static inline void __get_page_tail_foll(struct page *page,</span>
 	 * speculative page access (like in
 	 * page_cache_get_speculative()) on tail pages.
 	 */
<span class="p_del">-	VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;first_page-&gt;_count) &lt;= 0, page);</span>
<span class="p_add">+	VM_BUG(atomic_read(&amp;page-&gt;first_page-&gt;_count) &lt;= 0, &quot;%pZp&quot;, page);</span>
 	if (get_page_head)
 		atomic_inc(&amp;page-&gt;first_page-&gt;_count);
 	get_huge_page_tail(page);
<span class="p_chunk">@@ -86,7 +86,7 @@</span> <span class="p_context"> static inline void get_page_foll(struct page *page)</span>
 		 * Getting a normal page or the head of a compound page
 		 * requires to already have an elevated page-&gt;_count.
 		 */
<span class="p_del">-		VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) &lt;= 0, page);</span>
<span class="p_add">+		VM_BUG(atomic_read(&amp;page-&gt;_count) &lt;= 0, &quot;%pZp&quot;, page);</span>
 		atomic_inc(&amp;page-&gt;_count);
 	}
 }
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index bc7be0e..040185f 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -1897,13 +1897,13 @@</span> <span class="p_context"> int rmap_walk_ksm(struct page *page, struct rmap_walk_control *rwc)</span>
 	int ret = SWAP_AGAIN;
 	int search_new_forks = 0;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageKsm(page), page);</span>
<span class="p_add">+	VM_BUG(!PageKsm(page), &quot;%pZp&quot;, page);</span>
 
 	/*
 	 * Rely on the page lock to protect against concurrent modifications
 	 * to that page&#39;s node of the stable tree.
 	 */
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 
 	stable_node = page_stable_node(page);
 	if (!stable_node)
<span class="p_chunk">@@ -1957,13 +1957,14 @@</span> <span class="p_context"> void ksm_migrate_page(struct page *newpage, struct page *oldpage)</span>
 {
 	struct stable_node *stable_node;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(oldpage), oldpage);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(newpage), newpage);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(newpage-&gt;mapping != oldpage-&gt;mapping, newpage);</span>
<span class="p_add">+	VM_BUG(!PageLocked(oldpage), &quot;%pZp&quot;, oldpage);</span>
<span class="p_add">+	VM_BUG(!PageLocked(newpage), &quot;%pZp&quot;, newpage);</span>
<span class="p_add">+	VM_BUG(newpage-&gt;mapping != oldpage-&gt;mapping, &quot;%pZp&quot;, newpage);</span>
 
 	stable_node = page_stable_node(newpage);
 	if (stable_node) {
<span class="p_del">-		VM_BUG_ON_PAGE(stable_node-&gt;kpfn != page_to_pfn(oldpage), oldpage);</span>
<span class="p_add">+		VM_BUG(stable_node-&gt;kpfn != page_to_pfn(oldpage), &quot;%pZp&quot;,</span>
<span class="p_add">+		       oldpage);</span>
 		stable_node-&gt;kpfn = page_to_pfn(newpage);
 		/*
 		 * newpage-&gt;mapping was set in advance; now we need smp_wmb()
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 14c2f20..6ae7c39 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -2365,7 +2365,7 @@</span> <span class="p_context"> struct mem_cgroup *try_get_mem_cgroup_from_page(struct page *page)</span>
 	unsigned short id;
 	swp_entry_t ent;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 
 	memcg = page-&gt;mem_cgroup;
 	if (memcg) {
<span class="p_chunk">@@ -2407,7 +2407,7 @@</span> <span class="p_context"> static void unlock_page_lru(struct page *page, int isolated)</span>
 		struct lruvec *lruvec;
 
 		lruvec = mem_cgroup_page_lruvec(page, zone);
<span class="p_del">-		VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_add">+		VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
 		SetPageLRU(page);
 		add_page_to_lru_list(page, lruvec, page_lru(page));
 	}
<span class="p_chunk">@@ -2419,7 +2419,7 @@</span> <span class="p_context"> static void commit_charge(struct page *page, struct mem_cgroup *memcg,</span>
 {
 	int isolated;
 
<span class="p_del">-	VM_BUG_ON_PAGE(page-&gt;mem_cgroup, page);</span>
<span class="p_add">+	VM_BUG(page-&gt;mem_cgroup, &quot;%pZp&quot;, page);</span>
 
 	/*
 	 * In some cases, SwapCache and FUSE(splice_buf-&gt;radixtree), the page
<span class="p_chunk">@@ -2726,7 +2726,7 @@</span> <span class="p_context"> void __memcg_kmem_uncharge_pages(struct page *page, int order)</span>
 	if (!memcg)
 		return;
 
<span class="p_del">-	VM_BUG_ON_PAGE(mem_cgroup_is_root(memcg), page);</span>
<span class="p_add">+	VM_BUG(mem_cgroup_is_root(memcg), &quot;%pZp&quot;, page);</span>
 
 	memcg_uncharge_kmem(memcg, 1 &lt;&lt; order);
 	page-&gt;mem_cgroup = NULL;
<span class="p_chunk">@@ -4748,7 +4748,7 @@</span> <span class="p_context"> static int mem_cgroup_move_account(struct page *page,</span>
 	int ret;
 
 	VM_BUG_ON(from == to);
<span class="p_del">-	VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_add">+	VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
 	/*
 	 * The page is isolated from LRU. So, collapse function
 	 * will not handle this page. But page splitting can happen.
<span class="p_chunk">@@ -4864,7 +4864,7 @@</span> <span class="p_context"> static enum mc_target_type get_mctgt_type_thp(struct vm_area_struct *vma,</span>
 	enum mc_target_type ret = MC_TARGET_NONE;
 
 	page = pmd_page(pmd);
<span class="p_del">-	VM_BUG_ON_PAGE(!page || !PageHead(page), page);</span>
<span class="p_add">+	VM_BUG(!page || !PageHead(page), &quot;%pZp&quot;, page);</span>
 	if (!(mc.flags &amp; MOVE_ANON))
 		return ret;
 	if (page-&gt;mem_cgroup == mc.from) {
<span class="p_chunk">@@ -5479,7 +5479,7 @@</span> <span class="p_context"> int mem_cgroup_try_charge(struct page *page, struct mm_struct *mm,</span>
 
 	if (PageTransHuge(page)) {
 		nr_pages &lt;&lt;= compound_order(page);
<span class="p_del">-		VM_BUG_ON_PAGE(!PageTransHuge(page), page);</span>
<span class="p_add">+		VM_BUG(!PageTransHuge(page), &quot;%pZp&quot;, page);</span>
 	}
 
 	if (do_swap_account &amp;&amp; PageSwapCache(page))
<span class="p_chunk">@@ -5521,8 +5521,8 @@</span> <span class="p_context"> void mem_cgroup_commit_charge(struct page *page, struct mem_cgroup *memcg,</span>
 {
 	unsigned int nr_pages = 1;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!page-&gt;mapping, page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(PageLRU(page) &amp;&amp; !lrucare, page);</span>
<span class="p_add">+	VM_BUG(!page-&gt;mapping, &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(PageLRU(page) &amp;&amp; !lrucare, &quot;%pZp&quot;, page);</span>
 
 	if (mem_cgroup_disabled())
 		return;
<span class="p_chunk">@@ -5538,7 +5538,7 @@</span> <span class="p_context"> void mem_cgroup_commit_charge(struct page *page, struct mem_cgroup *memcg,</span>
 
 	if (PageTransHuge(page)) {
 		nr_pages &lt;&lt;= compound_order(page);
<span class="p_del">-		VM_BUG_ON_PAGE(!PageTransHuge(page), page);</span>
<span class="p_add">+		VM_BUG(!PageTransHuge(page), &quot;%pZp&quot;, page);</span>
 	}
 
 	local_irq_disable();
<span class="p_chunk">@@ -5580,7 +5580,7 @@</span> <span class="p_context"> void mem_cgroup_cancel_charge(struct page *page, struct mem_cgroup *memcg)</span>
 
 	if (PageTransHuge(page)) {
 		nr_pages &lt;&lt;= compound_order(page);
<span class="p_del">-		VM_BUG_ON_PAGE(!PageTransHuge(page), page);</span>
<span class="p_add">+		VM_BUG(!PageTransHuge(page), &quot;%pZp&quot;, page);</span>
 	}
 
 	cancel_charge(memcg, nr_pages);
<span class="p_chunk">@@ -5630,8 +5630,8 @@</span> <span class="p_context"> static void uncharge_list(struct list_head *page_list)</span>
 		page = list_entry(next, struct page, lru);
 		next = page-&gt;lru.next;
 
<span class="p_del">-		VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(page_count(page), page);</span>
<span class="p_add">+		VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+		VM_BUG(page_count(page), &quot;%pZp&quot;, page);</span>
 
 		if (!page-&gt;mem_cgroup)
 			continue;
<span class="p_chunk">@@ -5653,7 +5653,7 @@</span> <span class="p_context"> static void uncharge_list(struct list_head *page_list)</span>
 
 		if (PageTransHuge(page)) {
 			nr_pages &lt;&lt;= compound_order(page);
<span class="p_del">-			VM_BUG_ON_PAGE(!PageTransHuge(page), page);</span>
<span class="p_add">+			VM_BUG(!PageTransHuge(page), &quot;%pZp&quot;, page);</span>
 			nr_huge += nr_pages;
 		}
 
<span class="p_chunk">@@ -5724,13 +5724,13 @@</span> <span class="p_context"> void mem_cgroup_migrate(struct page *oldpage, struct page *newpage,</span>
 	struct mem_cgroup *memcg;
 	int isolated;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(oldpage), oldpage);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(newpage), newpage);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!lrucare &amp;&amp; PageLRU(oldpage), oldpage);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!lrucare &amp;&amp; PageLRU(newpage), newpage);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(PageAnon(oldpage) != PageAnon(newpage), newpage);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(PageTransHuge(oldpage) != PageTransHuge(newpage),</span>
<span class="p_del">-		       newpage);</span>
<span class="p_add">+	VM_BUG(!PageLocked(oldpage), &quot;%pZp&quot;, oldpage);</span>
<span class="p_add">+	VM_BUG(!PageLocked(newpage), &quot;%pZp&quot;, newpage);</span>
<span class="p_add">+	VM_BUG(!lrucare &amp;&amp; PageLRU(oldpage), &quot;%pZp&quot;, oldpage);</span>
<span class="p_add">+	VM_BUG(!lrucare &amp;&amp; PageLRU(newpage), &quot;%pZp&quot;, newpage);</span>
<span class="p_add">+	VM_BUG(PageAnon(oldpage) != PageAnon(newpage), &quot;%pZp&quot;, newpage);</span>
<span class="p_add">+	VM_BUG(PageTransHuge(oldpage) != PageTransHuge(newpage), &quot;%pZp&quot;,</span>
<span class="p_add">+	       newpage);</span>
 
 	if (mem_cgroup_disabled())
 		return;
<span class="p_chunk">@@ -5812,8 +5812,8 @@</span> <span class="p_context"> void mem_cgroup_swapout(struct page *page, swp_entry_t entry)</span>
 	struct mem_cgroup *memcg;
 	unsigned short oldid;
 
<span class="p_del">-	VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(page_count(page), page);</span>
<span class="p_add">+	VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(page_count(page), &quot;%pZp&quot;, page);</span>
 
 	if (!do_swap_account)
 		return;
<span class="p_chunk">@@ -5825,7 +5825,7 @@</span> <span class="p_context"> void mem_cgroup_swapout(struct page *page, swp_entry_t entry)</span>
 		return;
 
 	oldid = swap_cgroup_record(entry, mem_cgroup_id(memcg));
<span class="p_del">-	VM_BUG_ON_PAGE(oldid, page);</span>
<span class="p_add">+	VM_BUG(oldid, &quot;%pZp&quot;, page);</span>
 	mem_cgroup_swap_statistics(memcg, true);
 
 	page-&gt;mem_cgroup = NULL;
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 6e5d4bd..dd509d9 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -302,7 +302,7 @@</span> <span class="p_context"> int __tlb_remove_page(struct mmu_gather *tlb, struct page *page)</span>
 			return 0;
 		batch = tlb-&gt;active;
 	}
<span class="p_del">-	VM_BUG_ON_PAGE(batch-&gt;nr &gt; batch-&gt;max, page);</span>
<span class="p_add">+	VM_BUG(batch-&gt;nr &gt; batch-&gt;max, &quot;%pZp&quot;, page);</span>
 
 	return batch-&gt;max - batch-&gt;nr;
 }
<span class="p_chunk">@@ -1977,7 +1977,7 @@</span> <span class="p_context"> static int do_page_mkwrite(struct vm_area_struct *vma, struct page *page,</span>
 		}
 		ret |= VM_FAULT_LOCKED;
 	} else
<span class="p_del">-		VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+		VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -2020,7 +2020,7 @@</span> <span class="p_context"> static inline int wp_page_reuse(struct mm_struct *mm,</span>
 			lock_page(page);
 
 		dirtied = set_page_dirty(page);
<span class="p_del">-		VM_BUG_ON_PAGE(PageAnon(page), page);</span>
<span class="p_add">+		VM_BUG(PageAnon(page), &quot;%pZp&quot;, page);</span>
 		mapping = page-&gt;mapping;
 		unlock_page(page);
 		page_cache_release(page);
<span class="p_chunk">@@ -2763,7 +2763,7 @@</span> <span class="p_context"> static int __do_fault(struct vm_area_struct *vma, unsigned long address,</span>
 	if (unlikely(!(ret &amp; VM_FAULT_LOCKED)))
 		lock_page(vmf.page);
 	else
<span class="p_del">-		VM_BUG_ON_PAGE(!PageLocked(vmf.page), vmf.page);</span>
<span class="p_add">+		VM_BUG(!PageLocked(vmf.page), &quot;%pZp&quot;, vmf.page);</span>
 
  out:
 	*page = vmf.page;
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index 022adc2..2693888 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -500,7 +500,7 @@</span> <span class="p_context"> void migrate_page_copy(struct page *newpage, struct page *page)</span>
 	if (PageUptodate(page))
 		SetPageUptodate(newpage);
 	if (TestClearPageActive(page)) {
<span class="p_del">-		VM_BUG_ON_PAGE(PageUnevictable(page), page);</span>
<span class="p_add">+		VM_BUG(PageUnevictable(page), &quot;%pZp&quot;, page);</span>
 		SetPageActive(newpage);
 	} else if (TestClearPageUnevictable(page))
 		SetPageUnevictable(newpage);
<span class="p_chunk">@@ -869,7 +869,7 @@</span> <span class="p_context"> static int __unmap_and_move(struct page *page, struct page *newpage,</span>
 	 * free the metadata, so the page can be freed.
 	 */
 	if (!page-&gt;mapping) {
<span class="p_del">-		VM_BUG_ON_PAGE(PageAnon(page), page);</span>
<span class="p_add">+		VM_BUG(PageAnon(page), &quot;%pZp&quot;, page);</span>
 		if (page_has_private(page)) {
 			try_to_free_buffers(page);
 			goto out_unlock;
<span class="p_chunk">@@ -1606,7 +1606,7 @@</span> <span class="p_context"> static int numamigrate_isolate_page(pg_data_t *pgdat, struct page *page)</span>
 {
 	int page_lru;
 
<span class="p_del">-	VM_BUG_ON_PAGE(compound_order(page) &amp;&amp; !PageTransHuge(page), page);</span>
<span class="p_add">+	VM_BUG(compound_order(page) &amp;&amp; !PageTransHuge(page), &quot;%pZp&quot;, page);</span>
 
 	/* Avoid migrating to a node that is nearly full */
 	if (!migrate_balanced_pgdat(pgdat, 1UL &lt;&lt; compound_order(page)))
<span class="p_header">diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="p_header">index 6fd2cf1..54269cd 100644</span>
<span class="p_header">--- a/mm/mlock.c</span>
<span class="p_header">+++ b/mm/mlock.c</span>
<span class="p_chunk">@@ -232,8 +232,8 @@</span> <span class="p_context"> static int __mlock_posix_error_return(long retval)</span>
 static bool __putback_lru_fast_prepare(struct page *page, struct pagevec *pvec,
 		int *pgrescued)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 
 	if (page_mapcount(page) &lt;= 1 &amp;&amp; page_evictable(page)) {
 		pagevec_add(pvec, page);
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index 06577ec..4d3668f 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -596,7 +596,7 @@</span> <span class="p_context"> static inline int page_is_buddy(struct page *page, struct page *buddy,</span>
 		if (page_zone_id(page) != page_zone_id(buddy))
 			return 0;
 
<span class="p_del">-		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);</span>
<span class="p_add">+		VM_BUG(page_count(buddy) != 0, &quot;%pZp&quot;, buddy);</span>
 
 		return 1;
 	}
<span class="p_chunk">@@ -610,7 +610,7 @@</span> <span class="p_context"> static inline int page_is_buddy(struct page *page, struct page *buddy,</span>
 		if (page_zone_id(page) != page_zone_id(buddy))
 			return 0;
 
<span class="p_del">-		VM_BUG_ON_PAGE(page_count(buddy) != 0, buddy);</span>
<span class="p_add">+		VM_BUG(page_count(buddy) != 0, &quot;%pZp&quot;, buddy);</span>
 
 		return 1;
 	}
<span class="p_chunk">@@ -654,7 +654,7 @@</span> <span class="p_context"> static inline void __free_one_page(struct page *page,</span>
 	int max_order = MAX_ORDER;
 
 	VM_BUG_ON(!zone_is_initialized(zone));
<span class="p_del">-	VM_BUG_ON_PAGE(page-&gt;flags &amp; PAGE_FLAGS_CHECK_AT_PREP, page);</span>
<span class="p_add">+	VM_BUG(page-&gt;flags &amp; PAGE_FLAGS_CHECK_AT_PREP, &quot;%pZp&quot;, page);</span>
 
 	VM_BUG_ON(migratetype == -1);
 	if (is_migrate_isolate(migratetype)) {
<span class="p_chunk">@@ -671,8 +671,8 @@</span> <span class="p_context"> static inline void __free_one_page(struct page *page,</span>
 
 	page_idx = pfn &amp; ((1 &lt;&lt; max_order) - 1);
 
<span class="p_del">-	VM_BUG_ON_PAGE(page_idx &amp; ((1 &lt;&lt; order) - 1), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(bad_range(zone, page), page);</span>
<span class="p_add">+	VM_BUG(page_idx &amp; ((1 &lt;&lt; order) - 1), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(bad_range(zone, page), &quot;%pZp&quot;, page);</span>
 
 	while (order &lt; max_order - 1) {
 		buddy_idx = __find_buddy_index(page_idx, order);
<span class="p_chunk">@@ -930,8 +930,8 @@</span> <span class="p_context"> static bool free_pages_prepare(struct page *page, unsigned int order)</span>
 	bool compound = PageCompound(page);
 	int i, bad = 0;
 
<span class="p_del">-	VM_BUG_ON_PAGE(PageTail(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(compound &amp;&amp; compound_order(page) != order, page);</span>
<span class="p_add">+	VM_BUG(PageTail(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(compound &amp;&amp; compound_order(page) != order, &quot;%pZp&quot;, page);</span>
 
 	trace_mm_page_free(page, order);
 	kmemcheck_free_shadow(page, order);
<span class="p_chunk">@@ -1246,7 +1246,7 @@</span> <span class="p_context"> static inline void expand(struct zone *zone, struct page *page,</span>
 		area--;
 		high--;
 		size &gt;&gt;= 1;
<span class="p_del">-		VM_BUG_ON_PAGE(bad_range(zone, &amp;page[size]), &amp;page[size]);</span>
<span class="p_add">+		VM_BUG(bad_range(zone, &amp;page[size]), &quot;%pZp&quot;, &amp;page[size]);</span>
 
 		if (IS_ENABLED(CONFIG_DEBUG_PAGEALLOC) &amp;&amp;
 			debug_guardpage_enabled() &amp;&amp;
<span class="p_chunk">@@ -1418,7 +1418,7 @@</span> <span class="p_context"> int move_freepages(struct zone *zone,</span>
 
 	for (page = start_page; page &lt;= end_page;) {
 		/* Make sure we are not inadvertently changing nodes */
<span class="p_del">-		VM_BUG_ON_PAGE(page_to_nid(page) != zone_to_nid(zone), page);</span>
<span class="p_add">+		VM_BUG(page_to_nid(page) != zone_to_nid(zone), &quot;%pZp&quot;, page);</span>
 
 		if (!pfn_valid_within(page_to_pfn(page))) {
 			page++;
<span class="p_chunk">@@ -1943,8 +1943,8 @@</span> <span class="p_context"> void split_page(struct page *page, unsigned int order)</span>
 {
 	int i;
 
<span class="p_del">-	VM_BUG_ON_PAGE(PageCompound(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!page_count(page), page);</span>
<span class="p_add">+	VM_BUG(PageCompound(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(!page_count(page), &quot;%pZp&quot;, page);</span>
 
 #ifdef CONFIG_KMEMCHECK
 	/*
<span class="p_chunk">@@ -2096,7 +2096,7 @@</span> <span class="p_context"> struct page *buffered_rmqueue(struct zone *preferred_zone,</span>
 	zone_statistics(preferred_zone, zone, gfp_flags);
 	local_irq_restore(flags);
 
<span class="p_del">-	VM_BUG_ON_PAGE(bad_range(zone, page), page);</span>
<span class="p_add">+	VM_BUG(bad_range(zone, page), &quot;%pZp&quot;, page);</span>
 	return page;
 
 failed:
<span class="p_chunk">@@ -6514,7 +6514,7 @@</span> <span class="p_context"> void set_pfnblock_flags_mask(struct page *page, unsigned long flags,</span>
 	word_bitidx = bitidx / BITS_PER_LONG;
 	bitidx &amp;= (BITS_PER_LONG-1);
 
<span class="p_del">-	VM_BUG_ON_PAGE(!zone_spans_pfn(zone, pfn), page);</span>
<span class="p_add">+	VM_BUG(!zone_spans_pfn(zone, pfn), &quot;%pZp&quot;, page);</span>
 
 	bitidx += end_bitidx;
 	mask &lt;&lt;= (BITS_PER_LONG - bitidx - 1);
<span class="p_header">diff --git a/mm/page_io.c b/mm/page_io.c</span>
<span class="p_header">index 6424869..deea5be 100644</span>
<span class="p_header">--- a/mm/page_io.c</span>
<span class="p_header">+++ b/mm/page_io.c</span>
<span class="p_chunk">@@ -331,8 +331,8 @@</span> <span class="p_context"> int swap_readpage(struct page *page)</span>
 	int ret = 0;
 	struct swap_info_struct *sis = page_swap_info(page);
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(PageUptodate(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(PageUptodate(page), &quot;%pZp&quot;, page);</span>
 	if (frontswap_load(page) == 0) {
 		SetPageUptodate(page);
 		unlock_page(page);
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index dad23a4..f8a6bca 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -971,9 +971,9 @@</span> <span class="p_context"> void page_move_anon_rmap(struct page *page,</span>
 {
 	struct anon_vma *anon_vma = vma-&gt;anon_vma;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 	VM_BUG_ON_VMA(!anon_vma, vma);
<span class="p_del">-	VM_BUG_ON_PAGE(page-&gt;index != linear_page_index(vma, address), page);</span>
<span class="p_add">+	VM_BUG(page-&gt;index != linear_page_index(vma, address), &quot;%pZp&quot;, page);</span>
 
 	anon_vma = (void *) anon_vma + PAGE_MAPPING_ANON;
 	page-&gt;mapping = (struct address_space *) anon_vma;
<span class="p_chunk">@@ -1078,7 +1078,7 @@</span> <span class="p_context"> void do_page_add_anon_rmap(struct page *page,</span>
 	if (unlikely(PageKsm(page)))
 		return;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 	/* address might be in next vma when migration races vma_adjust */
 	if (first)
 		__page_set_anon_rmap(page, vma, address, exclusive);
<span class="p_chunk">@@ -1274,7 +1274,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 		pte_t swp_pte;
 
 		if (flags &amp; TTU_FREE) {
<span class="p_del">-			VM_BUG_ON_PAGE(PageSwapCache(page), page);</span>
<span class="p_add">+			VM_BUG(PageSwapCache(page), &quot;%pZp&quot;, page);</span>
 			if (!dirty &amp;&amp; !PageDirty(page)) {
 				/* It&#39;s a freeable page by MADV_FREE */
 				dec_mm_counter(mm, MM_ANONPAGES);
<span class="p_chunk">@@ -1407,7 +1407,7 @@</span> <span class="p_context"> int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
 		.anon_lock = page_lock_anon_vma_read,
 	};
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHuge(page) &amp;&amp; PageTransHuge(page), page);</span>
<span class="p_add">+	VM_BUG(!PageHuge(page) &amp;&amp; PageTransHuge(page), &quot;%pZp&quot;, page);</span>
 
 	/*
 	 * During exec, a temporary VMA is setup and later moved.
<span class="p_chunk">@@ -1453,7 +1453,7 @@</span> <span class="p_context"> int try_to_munlock(struct page *page)</span>
 
 	};
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page) || PageLRU(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page) || PageLRU(page), &quot;%pZp&quot;, page);</span>
 
 	ret = rmap_walk(page, &amp;rwc);
 	return ret;
<span class="p_chunk">@@ -1559,7 +1559,7 @@</span> <span class="p_context"> static int rmap_walk_file(struct page *page, struct rmap_walk_control *rwc)</span>
 	 * structure at mapping cannot be freed and reused yet,
 	 * so we can safely take mapping-&gt;i_mmap_rwsem.
 	 */
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 
 	if (!mapping)
 		return ret;
<span class="p_header">diff --git a/mm/shmem.c b/mm/shmem.c</span>
<span class="p_header">index 3f974a1..888dfb0 100644</span>
<span class="p_header">--- a/mm/shmem.c</span>
<span class="p_header">+++ b/mm/shmem.c</span>
<span class="p_chunk">@@ -295,8 +295,8 @@</span> <span class="p_context"> static int shmem_add_to_page_cache(struct page *page,</span>
 {
 	int error;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!PageSwapBacked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(!PageSwapBacked(page), &quot;%pZp&quot;, page);</span>
 
 	page_cache_get(page);
 	page-&gt;mapping = mapping;
<span class="p_chunk">@@ -436,7 +436,8 @@</span> <span class="p_context"> static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,</span>
 				continue;
 			if (!unfalloc || !PageUptodate(page)) {
 				if (page-&gt;mapping == mapping) {
<span class="p_del">-					VM_BUG_ON_PAGE(PageWriteback(page), page);</span>
<span class="p_add">+					VM_BUG(PageWriteback(page), &quot;%pZp&quot;,</span>
<span class="p_add">+					       page);</span>
 					truncate_inode_page(mapping, page);
 				}
 			}
<span class="p_chunk">@@ -513,7 +514,8 @@</span> <span class="p_context"> static void shmem_undo_range(struct inode *inode, loff_t lstart, loff_t lend,</span>
 			lock_page(page);
 			if (!unfalloc || !PageUptodate(page)) {
 				if (page-&gt;mapping == mapping) {
<span class="p_del">-					VM_BUG_ON_PAGE(PageWriteback(page), page);</span>
<span class="p_add">+					VM_BUG(PageWriteback(page), &quot;%pZp&quot;,</span>
<span class="p_add">+					       page);</span>
 					truncate_inode_page(mapping, page);
 				} else {
 					/* Page was replaced by swap: retry */
<span class="p_header">diff --git a/mm/slub.c b/mm/slub.c</span>
<span class="p_header">index f920dc5..f516e0c 100644</span>
<span class="p_header">--- a/mm/slub.c</span>
<span class="p_header">+++ b/mm/slub.c</span>
<span class="p_chunk">@@ -338,13 +338,13 @@</span> <span class="p_context"> static inline int oo_objects(struct kmem_cache_order_objects x)</span>
  */
 static __always_inline void slab_lock(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(PageTail(page), page);</span>
<span class="p_add">+	VM_BUG(PageTail(page), &quot;%pZp&quot;, page);</span>
 	bit_spin_lock(PG_locked, &amp;page-&gt;flags);
 }
 
 static __always_inline void slab_unlock(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(PageTail(page), page);</span>
<span class="p_add">+	VM_BUG(PageTail(page), &quot;%pZp&quot;, page);</span>
 	__bit_spin_unlock(PG_locked, &amp;page-&gt;flags);
 }
 
<span class="p_header">diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="p_header">index 8773de0..47af078 100644</span>
<span class="p_header">--- a/mm/swap.c</span>
<span class="p_header">+++ b/mm/swap.c</span>
<span class="p_chunk">@@ -59,7 +59,7 @@</span> <span class="p_context"> static void __page_cache_release(struct page *page)</span>
 
 		spin_lock_irqsave(&amp;zone-&gt;lru_lock, flags);
 		lruvec = mem_cgroup_page_lruvec(page, zone);
<span class="p_del">-		VM_BUG_ON_PAGE(!PageLRU(page), page);</span>
<span class="p_add">+		VM_BUG(!PageLRU(page), &quot;%pZp&quot;, page);</span>
 		__ClearPageLRU(page);
 		del_page_from_lru_list(page, lruvec, page_off_lru(page));
 		spin_unlock_irqrestore(&amp;zone-&gt;lru_lock, flags);
<span class="p_chunk">@@ -131,8 +131,8 @@</span> <span class="p_context"> void put_unrefcounted_compound_page(struct page *page_head, struct page *page)</span>
 		 * __split_huge_page_refcount cannot race
 		 * here, see the comment above this function.
 		 */
<span class="p_del">-		VM_BUG_ON_PAGE(!PageHead(page_head), page_head);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(page_mapcount(page) != 0, page);</span>
<span class="p_add">+		VM_BUG(!PageHead(page_head), &quot;%pZp&quot;, page_head);</span>
<span class="p_add">+		VM_BUG(page_mapcount(page) != 0, &quot;%pZp&quot;, page);</span>
 		if (put_page_testzero(page_head)) {
 			/*
 			 * If this is the tail of a slab THP page,
<span class="p_chunk">@@ -148,7 +148,7 @@</span> <span class="p_context"> void put_unrefcounted_compound_page(struct page *page_head, struct page *page)</span>
 			 * not go away until the compound page enters
 			 * the buddy allocator.
 			 */
<span class="p_del">-			VM_BUG_ON_PAGE(PageSlab(page_head), page_head);</span>
<span class="p_add">+			VM_BUG(PageSlab(page_head), &quot;%pZp&quot;, page_head);</span>
 			__put_compound_page(page_head);
 		}
 	} else
<span class="p_chunk">@@ -202,7 +202,7 @@</span> <span class="p_context"> out_put_single:</span>
 				__put_single_page(page);
 			return;
 		}
<span class="p_del">-		VM_BUG_ON_PAGE(page_head != page-&gt;first_page, page);</span>
<span class="p_add">+		VM_BUG(page_head != page-&gt;first_page, &quot;%pZp&quot;, page);</span>
 		/*
 		 * We can release the refcount taken by
 		 * get_page_unless_zero() now that
<span class="p_chunk">@@ -210,12 +210,13 @@</span> <span class="p_context"> out_put_single:</span>
 		 * compound_lock.
 		 */
 		if (put_page_testzero(page_head))
<span class="p_del">-			VM_BUG_ON_PAGE(1, page_head);</span>
<span class="p_add">+			VM_BUG(1, &quot;%pZp&quot;, page_head);</span>
 		/* __split_huge_page_refcount will wait now */
<span class="p_del">-		VM_BUG_ON_PAGE(page_mapcount(page) &lt;= 0, page);</span>
<span class="p_add">+		VM_BUG(page_mapcount(page) &lt;= 0, &quot;%pZp&quot;, page);</span>
 		atomic_dec(&amp;page-&gt;_mapcount);
<span class="p_del">-		VM_BUG_ON_PAGE(atomic_read(&amp;page_head-&gt;_count) &lt;= 0, page_head);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(atomic_read(&amp;page-&gt;_count) != 0, page);</span>
<span class="p_add">+		VM_BUG(atomic_read(&amp;page_head-&gt;_count) &lt;= 0, &quot;%pZp&quot;,</span>
<span class="p_add">+		       page_head);</span>
<span class="p_add">+		VM_BUG(atomic_read(&amp;page-&gt;_count) != 0, &quot;%pZp&quot;, page);</span>
 		compound_unlock_irqrestore(page_head, flags);
 
 		if (put_page_testzero(page_head)) {
<span class="p_chunk">@@ -226,7 +227,7 @@</span> <span class="p_context"> out_put_single:</span>
 		}
 	} else {
 		/* @page_head is a dangling pointer */
<span class="p_del">-		VM_BUG_ON_PAGE(PageTail(page), page);</span>
<span class="p_add">+		VM_BUG(PageTail(page), &quot;%pZp&quot;, page);</span>
 		goto out_put_single;
 	}
 }
<span class="p_chunk">@@ -306,7 +307,7 @@</span> <span class="p_context"> bool __get_page_tail(struct page *page)</span>
 			 * page. __split_huge_page_refcount
 			 * cannot race here.
 			 */
<span class="p_del">-			VM_BUG_ON_PAGE(!PageHead(page_head), page_head);</span>
<span class="p_add">+			VM_BUG(!PageHead(page_head), &quot;%pZp&quot;, page_head);</span>
 			__get_page_tail_foll(page, true);
 			return true;
 		} else {
<span class="p_chunk">@@ -668,8 +669,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(lru_cache_add_file);</span>
  */
 void lru_cache_add(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(PageActive(page) &amp;&amp; PageUnevictable(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_add">+	VM_BUG(PageActive(page) &amp;&amp; PageUnevictable(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
 	__lru_cache_add(page);
 }
 
<span class="p_chunk">@@ -710,7 +711,7 @@</span> <span class="p_context"> void add_page_to_unevictable_list(struct page *page)</span>
 void lru_cache_add_active_or_unevictable(struct page *page,
 					 struct vm_area_struct *vma)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_add">+	VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
 
 	if (likely((vma-&gt;vm_flags &amp; (VM_LOCKED | VM_SPECIAL)) != VM_LOCKED)) {
 		SetPageActive(page);
<span class="p_chunk">@@ -995,7 +996,7 @@</span> <span class="p_context"> void release_pages(struct page **pages, int nr, bool cold)</span>
 			}
 
 			lruvec = mem_cgroup_page_lruvec(page, zone);
<span class="p_del">-			VM_BUG_ON_PAGE(!PageLRU(page), page);</span>
<span class="p_add">+			VM_BUG(!PageLRU(page), &quot;%pZp&quot;, page);</span>
 			__ClearPageLRU(page);
 			del_page_from_lru_list(page, lruvec, page_off_lru(page));
 		}
<span class="p_chunk">@@ -1038,9 +1039,9 @@</span> <span class="p_context"> void lru_add_page_tail(struct page *page, struct page *page_tail,</span>
 {
 	const int file = 0;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(PageCompound(page_tail), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(PageLRU(page_tail), page);</span>
<span class="p_add">+	VM_BUG(!PageHead(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(PageCompound(page_tail), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(PageLRU(page_tail), &quot;%pZp&quot;, page);</span>
 	VM_BUG_ON(NR_CPUS != 1 &amp;&amp;
 		  !spin_is_locked(&amp;lruvec_zone(lruvec)-&gt;lru_lock));
 
<span class="p_chunk">@@ -1079,7 +1080,7 @@</span> <span class="p_context"> static void __pagevec_lru_add_fn(struct page *page, struct lruvec *lruvec,</span>
 	int active = PageActive(page);
 	enum lru_list lru = page_lru(page);
 
<span class="p_del">-	VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_add">+	VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
 
 	SetPageLRU(page);
 	add_page_to_lru_list(page, lruvec, lru);
<span class="p_header">diff --git a/mm/swap_state.c b/mm/swap_state.c</span>
<span class="p_header">index a2611ce..0609662 100644</span>
<span class="p_header">--- a/mm/swap_state.c</span>
<span class="p_header">+++ b/mm/swap_state.c</span>
<span class="p_chunk">@@ -81,9 +81,9 @@</span> <span class="p_context"> int __add_to_swap_cache(struct page *page, swp_entry_t entry)</span>
 	int error;
 	struct address_space *address_space;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(PageSwapCache(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!PageSwapBacked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(PageSwapCache(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(!PageSwapBacked(page), &quot;%pZp&quot;, page);</span>
 
 	page_cache_get(page);
 	SetPageSwapCache(page);
<span class="p_chunk">@@ -137,9 +137,9 @@</span> <span class="p_context"> void __delete_from_swap_cache(struct page *page)</span>
 	swp_entry_t entry;
 	struct address_space *address_space;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!PageSwapCache(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(PageWriteback(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(!PageSwapCache(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(PageWriteback(page), &quot;%pZp&quot;, page);</span>
 
 	entry.val = page_private(page);
 	address_space = swap_address_space(entry);
<span class="p_chunk">@@ -163,8 +163,8 @@</span> <span class="p_context"> int add_to_swap(struct page *page, struct list_head *list)</span>
 	swp_entry_t entry;
 	int err;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_del">-	VM_BUG_ON_PAGE(!PageUptodate(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+	VM_BUG(!PageUptodate(page), &quot;%pZp&quot;, page);</span>
 
 	entry = get_swap_page();
 	if (!entry.val)
<span class="p_header">diff --git a/mm/swapfile.c b/mm/swapfile.c</span>
<span class="p_header">index a7e7210..d71dcd6 100644</span>
<span class="p_header">--- a/mm/swapfile.c</span>
<span class="p_header">+++ b/mm/swapfile.c</span>
<span class="p_chunk">@@ -884,7 +884,7 @@</span> <span class="p_context"> int reuse_swap_page(struct page *page)</span>
 {
 	int count;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 	if (unlikely(PageKsm(page)))
 		return 0;
 	count = page_mapcount(page);
<span class="p_chunk">@@ -904,7 +904,7 @@</span> <span class="p_context"> int reuse_swap_page(struct page *page)</span>
  */
 int try_to_free_swap(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 
 	if (!PageSwapCache(page))
 		return 0;
<span class="p_chunk">@@ -2710,7 +2710,7 @@</span> <span class="p_context"> struct swap_info_struct *page_swap_info(struct page *page)</span>
  */
 struct address_space *__page_file_mapping(struct page *page)
 {
<span class="p_del">-	VM_BUG_ON_PAGE(!PageSwapCache(page), page);</span>
<span class="p_add">+	VM_BUG(!PageSwapCache(page), &quot;%pZp&quot;, page);</span>
 	return page_swap_info(page)-&gt;swap_file-&gt;f_mapping;
 }
 EXPORT_SYMBOL_GPL(__page_file_mapping);
<span class="p_chunk">@@ -2718,7 +2718,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(__page_file_mapping);</span>
 pgoff_t __page_file_index(struct page *page)
 {
 	swp_entry_t swap = { .val = page_private(page) };
<span class="p_del">-	VM_BUG_ON_PAGE(!PageSwapCache(page), page);</span>
<span class="p_add">+	VM_BUG(!PageSwapCache(page), &quot;%pZp&quot;, page);</span>
 	return swp_offset(swap);
 }
 EXPORT_SYMBOL_GPL(__page_file_index);
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 7d20d36..d63586f 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -688,7 +688,7 @@</span> <span class="p_context"> void putback_lru_page(struct page *page)</span>
 	bool is_unevictable;
 	int was_unevictable = PageUnevictable(page);
 
<span class="p_del">-	VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_add">+	VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
 
 redo:
 	ClearPageUnevictable(page);
<span class="p_chunk">@@ -761,7 +761,7 @@</span> <span class="p_context"> static enum page_references page_check_references(struct page *page,</span>
 	unsigned long vm_flags;
 	int pte_dirty;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!PageLocked(page), page);</span>
<span class="p_add">+	VM_BUG(!PageLocked(page), &quot;%pZp&quot;, page);</span>
 
 	referenced_ptes = page_referenced(page, 1, sc-&gt;target_mem_cgroup,
 					  &amp;vm_flags, &amp;pte_dirty);
<span class="p_chunk">@@ -887,8 +887,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		if (!trylock_page(page))
 			goto keep;
 
<span class="p_del">-		VM_BUG_ON_PAGE(PageActive(page), page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(page_zone(page) != zone, page);</span>
<span class="p_add">+		VM_BUG(PageActive(page), &quot;%pZp&quot;, page);</span>
<span class="p_add">+		VM_BUG(page_zone(page) != zone, &quot;%pZp&quot;, page);</span>
 
 		sc-&gt;nr_scanned++;
 
<span class="p_chunk">@@ -1059,7 +1059,7 @@</span> <span class="p_context"> unmap:</span>
 				 * due to skipping of swapcache so we free
 				 * page in here rather than __remove_mapping.
 				 */
<span class="p_del">-				VM_BUG_ON_PAGE(PageSwapCache(page), page);</span>
<span class="p_add">+				VM_BUG(PageSwapCache(page), &quot;%pZp&quot;, page);</span>
 				if (!page_freeze_refs(page, 1))
 					goto keep_locked;
 				__ClearPageLocked(page);
<span class="p_chunk">@@ -1196,14 +1196,14 @@</span> <span class="p_context"> activate_locked:</span>
 		/* Not a candidate for swapping, so reclaim swap space. */
 		if (PageSwapCache(page) &amp;&amp; vm_swap_full())
 			try_to_free_swap(page);
<span class="p_del">-		VM_BUG_ON_PAGE(PageActive(page), page);</span>
<span class="p_add">+		VM_BUG(PageActive(page), &quot;%pZp&quot;, page);</span>
 		SetPageActive(page);
 		pgactivate++;
 keep_locked:
 		unlock_page(page);
 keep:
 		list_add(&amp;page-&gt;lru, &amp;ret_pages);
<span class="p_del">-		VM_BUG_ON_PAGE(PageLRU(page) || PageUnevictable(page), page);</span>
<span class="p_add">+		VM_BUG(PageLRU(page) || PageUnevictable(page), &quot;%pZp&quot;, page);</span>
 	}
 
 	mem_cgroup_uncharge_list(&amp;free_pages);
<span class="p_chunk">@@ -1358,7 +1358,7 @@</span> <span class="p_context"> static unsigned long isolate_lru_pages(unsigned long nr_to_scan,</span>
 		page = lru_to_page(src);
 		prefetchw_prev_lru_page(page, src, flags);
 
<span class="p_del">-		VM_BUG_ON_PAGE(!PageLRU(page), page);</span>
<span class="p_add">+		VM_BUG(!PageLRU(page), &quot;%pZp&quot;, page);</span>
 
 		switch (__isolate_lru_page(page, mode)) {
 		case 0:
<span class="p_chunk">@@ -1413,7 +1413,7 @@</span> <span class="p_context"> int isolate_lru_page(struct page *page)</span>
 {
 	int ret = -EBUSY;
 
<span class="p_del">-	VM_BUG_ON_PAGE(!page_count(page), page);</span>
<span class="p_add">+	VM_BUG(!page_count(page), &quot;%pZp&quot;, page);</span>
 
 	if (PageLRU(page)) {
 		struct zone *zone = page_zone(page);
<span class="p_chunk">@@ -1501,7 +1501,7 @@</span> <span class="p_context"> putback_inactive_pages(struct lruvec *lruvec, struct list_head *page_list)</span>
 		struct page *page = lru_to_page(page_list);
 		int lru;
 
<span class="p_del">-		VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_add">+		VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
 		list_del(&amp;page-&gt;lru);
 		if (unlikely(!page_evictable(page))) {
 			spin_unlock_irq(&amp;zone-&gt;lru_lock);
<span class="p_chunk">@@ -1736,7 +1736,7 @@</span> <span class="p_context"> static void move_active_pages_to_lru(struct lruvec *lruvec,</span>
 		page = lru_to_page(list);
 		lruvec = mem_cgroup_page_lruvec(page, zone);
 
<span class="p_del">-		VM_BUG_ON_PAGE(PageLRU(page), page);</span>
<span class="p_add">+		VM_BUG(PageLRU(page), &quot;%pZp&quot;, page);</span>
 		SetPageLRU(page);
 
 		nr_pages = hpage_nr_pages(page);
<span class="p_chunk">@@ -3863,7 +3863,7 @@</span> <span class="p_context"> void check_move_unevictable_pages(struct page **pages, int nr_pages)</span>
 		if (page_evictable(page)) {
 			enum lru_list lru = page_lru_base_type(page);
 
<span class="p_del">-			VM_BUG_ON_PAGE(PageActive(page), page);</span>
<span class="p_add">+			VM_BUG(PageActive(page), &quot;%pZp&quot;, page);</span>
 			ClearPageUnevictable(page);
 			del_page_from_lru_list(page, lruvec, LRU_UNEVICTABLE);
 			add_page_to_lru_list(page, lruvec, lru);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



