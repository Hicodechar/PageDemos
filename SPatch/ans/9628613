
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,04/16] mm/ZONE_DEVICE/unaddressable: add support for un-addressable device memory v3 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,04/16] mm/ZONE_DEVICE/unaddressable: add support for un-addressable device memory v3</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 16, 2017, 4:05 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1489680335-6594-5-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9628613/mbox/"
   >mbox</a>
|
   <a href="/patch/9628613/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9628613/">/patch/9628613/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	A2E226048C for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 15:12:29 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 907D427D45
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 15:12:29 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 8516828537; Thu, 16 Mar 2017 15:12:29 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3D11927D45
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 16 Mar 2017 15:12:28 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753086AbdCPPMV (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 16 Mar 2017 11:12:21 -0400
Received: from mx1.redhat.com ([209.132.183.28]:54200 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1753084AbdCPPMP (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 16 Mar 2017 11:12:15 -0400
Received: from smtp.corp.redhat.com
	(int-mx02.intmail.prod.int.phx2.redhat.com [10.5.11.12])
	(using TLSv1.2 with cipher AECDH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id D55EB42BB4;
	Thu, 16 Mar 2017 15:03:54 +0000 (UTC)
DMARC-Filter: OpenDMARC Filter v1.3.2 mx1.redhat.com D55EB42BB4
Authentication-Results: ext-mx06.extmail.prod.ext.phx2.redhat.com;
	dmarc=none (p=none dis=none) header.from=redhat.com
Authentication-Results: ext-mx06.extmail.prod.ext.phx2.redhat.com;
	spf=pass smtp.mailfrom=jglisse@redhat.com
DKIM-Filter: OpenDKIM Filter v2.11.0 mx1.redhat.com D55EB42BB4
Received: from xgl-cortex.ml2.eng.bos.redhat.com
	(xgl-cortex.ml2.eng.bos.redhat.com [10.19.160.80])
	by smtp.corp.redhat.com (Postfix) with ESMTP id DC55060F83;
	Thu, 16 Mar 2017 15:03:52 +0000 (UTC)
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	David Nellans &lt;dnellans@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Dan Williams &lt;dan.j.williams@intel.com&gt;,
	Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;
Subject: [HMM 04/16] mm/ZONE_DEVICE/unaddressable: add support for
	un-addressable device memory v3
Date: Thu, 16 Mar 2017 12:05:23 -0400
Message-Id: &lt;1489680335-6594-5-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1489680335-6594-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1489680335-6594-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.79 on 10.5.11.12
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.30]);
	Thu, 16 Mar 2017 15:03:55 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - March 16, 2017, 4:05 p.m.</div>
<pre class="content">
This add support for un-addressable device memory. Such memory is hotpluged
only so we can have struct page but we should never map them as such memory
can not be accessed by CPU. For that reason it uses a special swap entry for
CPU page table entry.

This patch implement all the logic from special swap type to handling CPU
page fault through a callback specified in the ZONE_DEVICE pgmap struct.

Architecture that wish to support un-addressable device memory should make
sure to never populate the kernel linar mapping for the physical range.

This feature potentially breaks memory hotplug unless every driver using it
magically predicts the future addresses of where memory will be hotplugged.

Changes since v2:
  -  Do not change devm_memremap_pages()
Changes since v1:
  - Add unaddressable memory resource descriptor enum
  - Explain why memory hotplug can fail because of un-addressable memory
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
Cc: Dan Williams &lt;dan.j.williams@intel.com&gt;
Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;
---
 fs/proc/task_mmu.c             |  7 +++++
 include/linux/ioport.h         |  1 +
 include/linux/memory_hotplug.h |  7 +++++
 include/linux/memremap.h       | 18 ++++++++++++
 include/linux/swap.h           | 18 ++++++++++--
 include/linux/swapops.h        | 67 ++++++++++++++++++++++++++++++++++++++++++
 kernel/memremap.c              | 22 ++++++++++++--
 mm/Kconfig                     | 12 ++++++++
 mm/memory.c                    | 66 ++++++++++++++++++++++++++++++++++++++++-
 mm/memory_hotplug.c            | 10 +++++--
 mm/mprotect.c                  | 12 ++++++++
 11 files changed, 232 insertions(+), 8 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - March 19, 2017, 8:09 p.m.</div>
<pre class="content">
Nits mainly

On Thu, Mar 16, 2017 at 12:05:23PM -0400, J?r?me Glisse wrote:
<span class="quote">&gt; This add support for un-addressable device memory. Such memory is hotpluged</span>

hotplugged
<span class="quote">
&gt; only so we can have struct page but we should never map them as such memory</span>
<span class="quote">&gt; can not be accessed by CPU. For that reason it uses a special swap entry for</span>
<span class="quote">&gt; CPU page table entry.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch implement all the logic from special swap type to handling CPU</span>
<span class="quote">&gt; page fault through a callback specified in the ZONE_DEVICE pgmap struct.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Architecture that wish to support un-addressable device memory should make</span>
<span class="quote">&gt; sure to never populate the kernel linar mapping for the physical range.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This feature potentially breaks memory hotplug unless every driver using it</span>
<span class="quote">&gt; magically predicts the future addresses of where memory will be hotplugged.</span>
<span class="quote">&gt; </span>


Note in the changelog that enabling this option reduces the maximum
number of swapfiles that can be activated.

Also, you have to read quite a lot of the patch before you learn that
the struct pages are required for migration. It&#39;s be nice to lead with
why struct pages are required for memory that can never be
CPU-accessible.
<span class="quote">
&gt; &lt;SNIP&gt;</span>
<span class="quote">&gt; diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="quote">&gt; index 45e91dd..ba564bc 100644</span>
<span class="quote">&gt; --- a/include/linux/swap.h</span>
<span class="quote">&gt; +++ b/include/linux/swap.h</span>
<span class="quote">&gt; @@ -51,6 +51,17 @@ static inline int current_is_kswapd(void)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * Un-addressable device memory support</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +#ifdef CONFIG_DEVICE_UNADDRESSABLE</span>
<span class="quote">&gt; +#define SWP_DEVICE_NUM 2</span>
<span class="quote">&gt; +#define SWP_DEVICE_WRITE (MAX_SWAPFILES + SWP_HWPOISON_NUM + SWP_MIGRATION_NUM)</span>
<span class="quote">&gt; +#define SWP_DEVICE (MAX_SWAPFILES + SWP_HWPOISON_NUM + SWP_MIGRATION_NUM + 1)</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +#define SWP_DEVICE_NUM 0</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;   * NUMA node memory migration support</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt; @@ -72,7 +83,8 @@ static inline int current_is_kswapd(void)</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #define MAX_SWAPFILES \</span>
<span class="quote">&gt; -	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
<span class="quote">&gt; +	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_DEVICE_NUM - \</span>
<span class="quote">&gt; +	SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Magic header for a swap area. The first part of the union is</span>

Max swap count reduced here and it looks fine other than the limitation
should be clear.
<span class="quote">
&gt; @@ -435,8 +447,8 @@ static inline void show_swap_cache_info(void)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -#define free_swap_and_cache(swp)	is_migration_entry(swp)</span>
<span class="quote">&gt; -#define swapcache_prepare(swp)		is_migration_entry(swp)</span>
<span class="quote">&gt; +#define free_swap_and_cache(e) (is_migration_entry(e) || is_device_entry(e))</span>
<span class="quote">&gt; +#define swapcache_prepare(e) (is_migration_entry(e) || is_device_entry(e))</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  static inline int add_swap_count_continuation(swp_entry_t swp, gfp_t gfp_mask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt; index 5c3a5f3..0e339f0 100644</span>
<span class="quote">&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt; @@ -100,6 +100,73 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
<span class="quote">&gt;  	return (void *)(value | RADIX_TREE_EXCEPTIONAL_ENTRY);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_DEVICE_UNADDRESSABLE)</span>
<span class="quote">&gt; +static inline swp_entry_t make_device_entry(struct page *page, bool write)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return swp_entry(write?SWP_DEVICE_WRITE:SWP_DEVICE, page_to_pfn(page));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>

Minor naming nit. Migration has READ and WRITE migration types but this
has SWP_DEVICE and SWP_DEVICE_WRITE. This was the first time it was
clear there are READ/WRITE types but with different naming.
<span class="quote">
&gt; +static inline bool is_device_entry(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	int type = swp_type(entry);</span>
<span class="quote">&gt; +	return type == SWP_DEVICE || type == SWP_DEVICE_WRITE;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void make_device_entry_read(swp_entry_t *entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	*entry = swp_entry(SWP_DEVICE, swp_offset(*entry));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline bool is_write_device_entry(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return unlikely(swp_type(entry) == SWP_DEVICE_WRITE);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline struct page *device_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>

Otherwise, looks ok and fairly standard.
<span class="quote">
&gt;  {</span>
<span class="quote">&gt; diff --git a/kernel/memremap.c b/kernel/memremap.c</span>
<span class="quote">&gt; index 19df1f5..d42f039f 100644</span>
<span class="quote">&gt; --- a/kernel/memremap.c</span>
<span class="quote">&gt; +++ b/kernel/memremap.c</span>
<span class="quote">&gt; @@ -18,6 +18,8 @@</span>
<span class="quote">&gt;  #include &lt;linux/io.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/memory_hotplug.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/swapops.h&gt;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #ifndef ioremap_cache</span>
<span class="quote">&gt;  /* temporary while we convert existing ioremap_cache users to memremap */</span>
<span class="quote">&gt; @@ -203,6 +205,21 @@ void put_zone_device_page(struct page *page)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  EXPORT_SYMBOL(put_zone_device_page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#if IS_ENABLED(CONFIG_DEVICE_UNADDRESSABLE)</span>
<span class="quote">&gt; +int device_entry_fault(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +		       unsigned long addr,</span>
<span class="quote">&gt; +		       swp_entry_t entry,</span>
<span class="quote">&gt; +		       unsigned flags,</span>
<span class="quote">&gt; +		       pmd_t *pmdp)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct page *page = device_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	BUG_ON(!page-&gt;pgmap-&gt;page_fault);</span>
<span class="quote">&gt; +	return page-&gt;pgmap-&gt;page_fault(vma, addr, page, flags, pmdp);</span>
<span class="quote">&gt; +}</span>

The BUG_ON is overkill. It&#39;ll trigger a NULL pointer exception
immediately and would likely be a fairly obvious driver bug. 

More importantly, there should be a description of what the
responsibilities of page_fault are. Saying it&#39;s a fault helpful when it
could say here (or in the struct description) that the handled is
responsible for migrating the data from device memory to CPU-accessible
memory.

What is expected to happen if migration fails? Think something crazy like
a process that is memcg limited, is mostly anonymous memory and there is
no swap. Is it acceptable for the application to be killed?
<span class="quote">
&gt; +EXPORT_SYMBOL(device_entry_fault);</span>
<span class="quote">&gt; +#endif /* CONFIG_DEVICE_UNADDRESSABLE */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  static void pgmap_radix_release(struct resource *res)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	resource_size_t key, align_start, align_size, align_end;</span>
<span class="quote">&gt; @@ -258,7 +275,7 @@ static void devm_memremap_pages_release(struct device *dev, void *data)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	lock_device_hotplug();</span>
<span class="quote">&gt;  	mem_hotplug_begin();</span>
<span class="quote">&gt; -	arch_remove_memory(align_start, align_size, MEMORY_DEVICE);</span>
<span class="quote">&gt; +	arch_remove_memory(align_start, align_size, pgmap-&gt;flags);</span>
<span class="quote">&gt;  	mem_hotplug_done();</span>
<span class="quote">&gt;  	unlock_device_hotplug();</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -338,6 +355,7 @@ void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
<span class="quote">&gt;  	pgmap-&gt;ref = ref;</span>
<span class="quote">&gt;  	pgmap-&gt;res = &amp;page_map-&gt;res;</span>
<span class="quote">&gt;  	pgmap-&gt;flags = MEMORY_DEVICE;</span>
<span class="quote">&gt; +	pgmap-&gt;page_fault = NULL;</span>
<span class="quote">&gt;  	pgmap-&gt;page_free = NULL;</span>
<span class="quote">&gt;  	pgmap-&gt;data = NULL;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -378,7 +396,7 @@ void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	lock_device_hotplug();</span>
<span class="quote">&gt;  	mem_hotplug_begin();</span>
<span class="quote">&gt; -	error = arch_add_memory(nid, align_start, align_size, MEMORY_DEVICE);</span>
<span class="quote">&gt; +	error = arch_add_memory(nid, align_start, align_size, pgmap-&gt;flags);</span>
<span class="quote">&gt;  	mem_hotplug_done();</span>
<span class="quote">&gt;  	unlock_device_hotplug();</span>
<span class="quote">&gt;  	if (error)</span>
<span class="quote">&gt; diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="quote">&gt; index 9b8fccb..9502315 100644</span>
<span class="quote">&gt; --- a/mm/Kconfig</span>
<span class="quote">&gt; +++ b/mm/Kconfig</span>
<span class="quote">&gt; @@ -700,6 +700,18 @@ config ZONE_DEVICE</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	  If FS_DAX is enabled, then say Y.</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +config DEVICE_UNADDRESSABLE</span>
<span class="quote">&gt; +	bool &quot;Un-addressable device memory (GPU memory, ...)&quot;</span>
<span class="quote">&gt; +	depends on ZONE_DEVICE</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	help</span>
<span class="quote">&gt; +	  Allow to create struct page for un-addressable device memory</span>
<span class="quote">&gt; +	  ie memory that is only accessible by the device (or group of</span>
<span class="quote">&gt; +	  devices).</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	  Having struct page is necessary for process memory migration</span>
<span class="quote">&gt; +	  to device memory.</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  config FRAME_VECTOR</span>
<span class="quote">&gt;  	bool</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="quote">&gt; index 235ba51..33aff303 100644</span>
<span class="quote">&gt; --- a/mm/memory.c</span>
<span class="quote">&gt; +++ b/mm/memory.c</span>
<span class="quote">&gt; @@ -49,6 +49,7 @@</span>
<span class="quote">&gt;  #include &lt;linux/swap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/highmem.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/pagemap.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/memremap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/ksm.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/rmap.h&gt;</span>
<span class="quote">&gt;  #include &lt;linux/export.h&gt;</span>
<span class="quote">&gt; @@ -927,6 +928,25 @@ copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
<span class="quote">&gt;  					pte = pte_swp_mksoft_dirty(pte);</span>
<span class="quote">&gt;  				set_pte_at(src_mm, addr, src_pte, pte);</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; +		} else if (is_device_entry(entry)) {</span>
<span class="quote">&gt; +			page = device_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * Update rss count even for un-addressable page as</span>
<span class="quote">&gt; +			 * they should be consider just like any other page.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			get_page(page);</span>
<span class="quote">&gt; +			rss[mm_counter(page)]++;</span>
<span class="quote">&gt; +			page_dup_rmap(page, false);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_write_device_entry(entry) &amp;&amp;</span>
<span class="quote">&gt; +			    is_cow_mapping(vm_flags)) {</span>
<span class="quote">&gt; +				make_device_entry_read(&amp;entry);</span>
<span class="quote">&gt; +				pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +				if (pte_swp_soft_dirty(*src_pte))</span>
<span class="quote">&gt; +					pte = pte_swp_mksoft_dirty(pte);</span>
<span class="quote">&gt; +				set_pte_at(src_mm, addr, src_pte, pte);</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		goto out_set_pte;</span>
<span class="quote">&gt;  	}</span>

I&#39;m curious about the soft dirty page handling part. One of the main
reasons for that is for features like checkpoint/restore but the memory in
question in inaccessible so how can should that be handled?  Superficially,
it looks like soft dirty handling with inaccessible memory is a no-go. I
would think that is even a reasonable restriction but it should be clear.

Presumably if an entry needs COW at some point in the future, it&#39;s the
device callback that handles it.
<span class="quote">
&gt; @@ -3679,6 +3735,7 @@ static int wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)</span>
<span class="quote">&gt;  static int handle_pte_fault(struct vm_fault *vmf)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pte_t entry;</span>
<span class="quote">&gt; +	struct page *page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	if (unlikely(pmd_none(*vmf-&gt;pmd))) {</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; @@ -3729,9 +3786,16 @@ static int handle_pte_fault(struct vm_fault *vmf)</span>
<span class="quote">&gt;  	if (pte_protnone(vmf-&gt;orig_pte) &amp;&amp; vma_is_accessible(vmf-&gt;vma))</span>
<span class="quote">&gt;  		return do_numa_page(vmf);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/* Catch mapping of un-addressable memory this should never happen */</span>
<span class="quote">&gt; +	entry = vmf-&gt;orig_pte;</span>
<span class="quote">&gt; +	page = pfn_to_page(pte_pfn(entry));</span>
<span class="quote">&gt; +	if (!is_addressable_page(page)) {</span>
<span class="quote">&gt; +		print_bad_pte(vmf-&gt;vma, vmf-&gt;address, entry, page);</span>
<span class="quote">&gt; +		return VM_FAULT_SIGBUS;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>

You&#39;re adding a new pfn_to_page on every PTE fault and this happens
unconditionally whether DEVICE_INACCESSIBLE is configured or not and it&#39;s
for a debugging check. It&#39;s also a seriously paranoid check because at
this point the PTE is present so something seriously bad happened in a
fault handler in the past.

Consider removing this entirely or making it a debug-only check. What
happens if such a PTE is accessed anyway and the CPU accesses it?
SIGBUS? If it&#39;s harmless (other than an application crash), remove it.
If it&#39;s actively dangerous then move this behind a static branch in a
separate patch that is activate iff there is cpu-inaccessible memory in
the system.
<span class="quote">
&gt;  	vmf-&gt;ptl = pte_lockptr(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;pmd);</span>
<span class="quote">&gt;  	spin_lock(vmf-&gt;ptl);</span>
<span class="quote">&gt; -	entry = vmf-&gt;orig_pte;</span>
<span class="quote">&gt;  	if (unlikely(!pte_same(*vmf-&gt;pte, entry)))</span>
<span class="quote">&gt;  		goto unlock;</span>
<span class="quote">&gt;  	if (vmf-&gt;flags &amp; FAULT_FLAG_WRITE) {</span>
<span class="quote">&gt; diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="quote">&gt; index 46960b3..4dcc003 100644</span>
<span class="quote">&gt; --- a/mm/memory_hotplug.c</span>
<span class="quote">&gt; +++ b/mm/memory_hotplug.c</span>
<span class="quote">&gt; @@ -152,7 +152,7 @@ void mem_hotplug_done(void)</span>
<span class="quote">&gt;  /* add this memory to iomem resource */</span>
<span class="quote">&gt;  static struct resource *register_memory_resource(u64 start, u64 size)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct resource *res;</span>
<span class="quote">&gt; +	struct resource *res, *conflict;</span>
<span class="quote">&gt;  	res = kzalloc(sizeof(struct resource), GFP_KERNEL);</span>
<span class="quote">&gt;  	if (!res)</span>
<span class="quote">&gt;  		return ERR_PTR(-ENOMEM);</span>
<span class="quote">&gt; @@ -161,7 +161,13 @@ static struct resource *register_memory_resource(u64 start, u64 size)</span>
<span class="quote">&gt;  	res-&gt;start = start;</span>
<span class="quote">&gt;  	res-&gt;end = start + size - 1;</span>
<span class="quote">&gt;  	res-&gt;flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;</span>
<span class="quote">&gt; -	if (request_resource(&amp;iomem_resource, res) &lt; 0) {</span>
<span class="quote">&gt; +	conflict =  request_resource_conflict(&amp;iomem_resource, res);</span>
<span class="quote">&gt; +	if (conflict) {</span>
<span class="quote">&gt; +		if (conflict-&gt;desc == IORES_DESC_UNADDRESSABLE_MEMORY) {</span>
<span class="quote">&gt; +			pr_debug(&quot;Device un-addressable memory block &quot;</span>
<span class="quote">&gt; +				 &quot;memory hotplug at %#010llx !\n&quot;,</span>
<span class="quote">&gt; +				 (unsigned long long)start);</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  		pr_debug(&quot;System RAM resource %pR cannot be added\n&quot;, res);</span>
<span class="quote">&gt;  		kfree(res);</span>
<span class="quote">&gt;  		return ERR_PTR(-EEXIST);</span>
<span class="quote">&gt; diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="quote">&gt; index 8edd0d5..50ac297 100644</span>
<span class="quote">&gt; --- a/mm/mprotect.c</span>
<span class="quote">&gt; +++ b/mm/mprotect.c</span>
<span class="quote">&gt; @@ -126,6 +126,18 @@ static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  				pages++;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			if (is_write_device_entry(entry)) {</span>
<span class="quote">&gt; +				pte_t newpte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				make_device_entry_read(&amp;entry);</span>
<span class="quote">&gt; +				newpte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +				if (pte_swp_soft_dirty(oldpte))</span>
<span class="quote">&gt; +					newpte = pte_swp_mksoft_dirty(newpte);</span>
<span class="quote">&gt; +				set_pte_at(mm, addr, pte, newpte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				pages++;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  	} while (pte++, addr += PAGE_SIZE, addr != end);</span>
<span class="quote">&gt;  	arch_leave_lazy_mmu_mode();</span>

Again, the soft dirty handling puzzles me.

Overall though, other than the pfn_to_page stuck into the page fault
path for a debugging check I don&#39;t have major objections.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index f08bd31..d2dea5c 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -538,6 +538,8 @@</span> <span class="p_context"> static void smaps_pte_entry(pte_t *pte, unsigned long addr,</span>
 			}
 		} else if (is_migration_entry(swpent))
 			page = migration_entry_to_page(swpent);
<span class="p_add">+		else if (is_device_entry(swpent))</span>
<span class="p_add">+			page = device_entry_to_page(swpent);</span>
 	} else if (unlikely(IS_ENABLED(CONFIG_SHMEM) &amp;&amp; mss-&gt;check_shmem_swap
 							&amp;&amp; pte_none(*pte))) {
 		page = find_get_entry(vma-&gt;vm_file-&gt;f_mapping,
<span class="p_chunk">@@ -700,6 +702,8 @@</span> <span class="p_context"> static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
 
 		if (is_migration_entry(swpent))
 			page = migration_entry_to_page(swpent);
<span class="p_add">+		else if (is_device_entry(swpent))</span>
<span class="p_add">+			page = device_entry_to_page(swpent);</span>
 	}
 	if (page) {
 		int mapcount = page_mapcount(page);
<span class="p_chunk">@@ -1183,6 +1187,9 @@</span> <span class="p_context"> static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
 		flags |= PM_SWAP;
 		if (is_migration_entry(entry))
 			page = migration_entry_to_page(entry);
<span class="p_add">+</span>
<span class="p_add">+		if (is_device_entry(entry))</span>
<span class="p_add">+			page = device_entry_to_page(entry);</span>
 	}
 
 	if (page &amp;&amp; !PageAnon(page))
<span class="p_header">diff --git a/include/linux/ioport.h b/include/linux/ioport.h</span>
<span class="p_header">index 6230064..d154a18 100644</span>
<span class="p_header">--- a/include/linux/ioport.h</span>
<span class="p_header">+++ b/include/linux/ioport.h</span>
<span class="p_chunk">@@ -130,6 +130,7 @@</span> <span class="p_context"> enum {</span>
 	IORES_DESC_ACPI_NV_STORAGE		= 3,
 	IORES_DESC_PERSISTENT_MEMORY		= 4,
 	IORES_DESC_PERSISTENT_MEMORY_LEGACY	= 5,
<span class="p_add">+	IORES_DESC_UNADDRESSABLE_MEMORY		= 6,</span>
 };
 
 /* helpers to define resources */
<span class="p_header">diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h</span>
<span class="p_header">index 30253da..69aabab 100644</span>
<span class="p_header">--- a/include/linux/memory_hotplug.h</span>
<span class="p_header">+++ b/include/linux/memory_hotplug.h</span>
<span class="p_chunk">@@ -286,15 +286,22 @@</span> <span class="p_context"> extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,</span>
  * never relied on struct page migration so far and new user of might also
  * prefer avoiding struct page migration.
  *
<span class="p_add">+ * For device memory (which use ZONE_DEVICE) we want differentiate between CPU</span>
<span class="p_add">+ * accessible memory (persitent memory, device memory on an architecture with a</span>
<span class="p_add">+ * system bus that allow transparent access to device memory) and unaddressable</span>
<span class="p_add">+ * memory (device memory that can not be accessed by CPU directly).</span>
<span class="p_add">+ *</span>
  * New non device memory specific flags can be added if ever needed.
  *
  * MEMORY_REGULAR: regular system memory
  * DEVICE_MEMORY: device memory create a ZONE_DEVICE zone for it
  * DEVICE_MEMORY_ALLOW_MIGRATE: page in that device memory ca be migrated
<span class="p_add">+ * MEMORY_DEVICE_UNADDRESSABLE: un-addressable memory (CPU can not access it)</span>
  */
 #define MEMORY_NORMAL 0
 #define MEMORY_DEVICE (1 &lt;&lt; 0)
 #define MEMORY_DEVICE_ALLOW_MIGRATE (1 &lt;&lt; 1)
<span class="p_add">+#define MEMORY_DEVICE_UNADDRESSABLE (1 &lt;&lt; 2)</span>
 
 extern int arch_add_memory(int nid, u64 start, u64 size, int flags);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
<span class="p_header">diff --git a/include/linux/memremap.h b/include/linux/memremap.h</span>
<span class="p_header">index 3e04f58..0ae7548 100644</span>
<span class="p_header">--- a/include/linux/memremap.h</span>
<span class="p_header">+++ b/include/linux/memremap.h</span>
<span class="p_chunk">@@ -35,10 +35,16 @@</span> <span class="p_context"> static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
 }
 #endif
 
<span class="p_add">+typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,</span>
<span class="p_add">+				unsigned long addr,</span>
<span class="p_add">+				struct page *page,</span>
<span class="p_add">+				unsigned flags,</span>
<span class="p_add">+				pmd_t *pmdp);</span>
 typedef void (*dev_page_free_t)(struct page *page, void *data);
 
 /**
  * struct dev_pagemap - metadata for ZONE_DEVICE mappings
<span class="p_add">+ * @page_fault: callback when CPU fault on an un-addressable device page</span>
  * @page_free: free page callback when page refcount reach 1
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
  * @res: physical address range covered by @ref
<span class="p_chunk">@@ -48,6 +54,7 @@</span> <span class="p_context"> typedef void (*dev_page_free_t)(struct page *page, void *data);</span>
  * @flags: memory flags see MEMORY_* in memory_hotplug.h
  */
 struct dev_pagemap {
<span class="p_add">+	dev_page_fault_t page_fault;</span>
 	dev_page_free_t page_free;
 	struct vmem_altmap *altmap;
 	const struct resource *res;
<span class="p_chunk">@@ -67,6 +74,12 @@</span> <span class="p_context"> static inline bool dev_page_allow_migrate(const struct page *page)</span>
 	return ((page_zonenum(page) == ZONE_DEVICE) &amp;&amp;
 		(page-&gt;pgmap-&gt;flags &amp; MEMORY_DEVICE_ALLOW_MIGRATE));
 }
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_addressable_page(const struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ((page_zonenum(page) != ZONE_DEVICE) ||</span>
<span class="p_add">+		!(page-&gt;pgmap-&gt;flags &amp; MEMORY_DEVICE_UNADDRESSABLE));</span>
<span class="p_add">+}</span>
 #else
 static inline void *devm_memremap_pages(struct device *dev,
 		struct resource *res, struct percpu_ref *ref,
<span class="p_chunk">@@ -90,6 +103,11 @@</span> <span class="p_context"> static inline bool dev_page_allow_migrate(const struct page *page)</span>
 {
 	return false;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_addressable_page(const struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
 #endif
 
 /**
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index 45e91dd..ba564bc 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -51,6 +51,17 @@</span> <span class="p_context"> static inline int current_is_kswapd(void)</span>
  */
 
 /*
<span class="p_add">+ * Un-addressable device memory support</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_DEVICE_UNADDRESSABLE</span>
<span class="p_add">+#define SWP_DEVICE_NUM 2</span>
<span class="p_add">+#define SWP_DEVICE_WRITE (MAX_SWAPFILES + SWP_HWPOISON_NUM + SWP_MIGRATION_NUM)</span>
<span class="p_add">+#define SWP_DEVICE (MAX_SWAPFILES + SWP_HWPOISON_NUM + SWP_MIGRATION_NUM + 1)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define SWP_DEVICE_NUM 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * NUMA node memory migration support
  */
 #ifdef CONFIG_MIGRATION
<span class="p_chunk">@@ -72,7 +83,8 @@</span> <span class="p_context"> static inline int current_is_kswapd(void)</span>
 #endif
 
 #define MAX_SWAPFILES \
<span class="p_del">-	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
<span class="p_add">+	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_DEVICE_NUM - \</span>
<span class="p_add">+	SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
 
 /*
  * Magic header for a swap area. The first part of the union is
<span class="p_chunk">@@ -435,8 +447,8 @@</span> <span class="p_context"> static inline void show_swap_cache_info(void)</span>
 {
 }
 
<span class="p_del">-#define free_swap_and_cache(swp)	is_migration_entry(swp)</span>
<span class="p_del">-#define swapcache_prepare(swp)		is_migration_entry(swp)</span>
<span class="p_add">+#define free_swap_and_cache(e) (is_migration_entry(e) || is_device_entry(e))</span>
<span class="p_add">+#define swapcache_prepare(e) (is_migration_entry(e) || is_device_entry(e))</span>
 
 static inline int add_swap_count_continuation(swp_entry_t swp, gfp_t gfp_mask)
 {
<span class="p_header">diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="p_header">index 5c3a5f3..0e339f0 100644</span>
<span class="p_header">--- a/include/linux/swapops.h</span>
<span class="p_header">+++ b/include/linux/swapops.h</span>
<span class="p_chunk">@@ -100,6 +100,73 @@</span> <span class="p_context"> static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
 	return (void *)(value | RADIX_TREE_EXCEPTIONAL_ENTRY);
 }
 
<span class="p_add">+#if IS_ENABLED(CONFIG_DEVICE_UNADDRESSABLE)</span>
<span class="p_add">+static inline swp_entry_t make_device_entry(struct page *page, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swp_entry(write?SWP_DEVICE_WRITE:SWP_DEVICE, page_to_pfn(page));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int type = swp_type(entry);</span>
<span class="p_add">+	return type == SWP_DEVICE || type == SWP_DEVICE_WRITE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void make_device_entry_read(swp_entry_t *entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*entry = swp_entry(SWP_DEVICE, swp_offset(*entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_write_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return unlikely(swp_type(entry) == SWP_DEVICE_WRITE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *device_entry_to_page(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int device_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+		       unsigned long addr,</span>
<span class="p_add">+		       swp_entry_t entry,</span>
<span class="p_add">+		       unsigned flags,</span>
<span class="p_add">+		       pmd_t *pmdp);</span>
<span class="p_add">+#else /* CONFIG_DEVICE_UNADDRESSABLE */</span>
<span class="p_add">+static inline swp_entry_t make_device_entry(struct page *page, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swp_entry(0, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void make_device_entry_read(swp_entry_t *entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_write_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *device_entry_to_page(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int device_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+				     unsigned long addr,</span>
<span class="p_add">+				     swp_entry_t entry,</span>
<span class="p_add">+				     unsigned flags,</span>
<span class="p_add">+				     pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return VM_FAULT_SIGBUS;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_DEVICE_UNADDRESSABLE */</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MIGRATION
 static inline swp_entry_t make_migration_entry(struct page *page, int write)
 {
<span class="p_header">diff --git a/kernel/memremap.c b/kernel/memremap.c</span>
<span class="p_header">index 19df1f5..d42f039f 100644</span>
<span class="p_header">--- a/kernel/memremap.c</span>
<span class="p_header">+++ b/kernel/memremap.c</span>
<span class="p_chunk">@@ -18,6 +18,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/io.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/memory_hotplug.h&gt;
<span class="p_add">+#include &lt;linux/swap.h&gt;</span>
<span class="p_add">+#include &lt;linux/swapops.h&gt;</span>
 
 #ifndef ioremap_cache
 /* temporary while we convert existing ioremap_cache users to memremap */
<span class="p_chunk">@@ -203,6 +205,21 @@</span> <span class="p_context"> void put_zone_device_page(struct page *page)</span>
 }
 EXPORT_SYMBOL(put_zone_device_page);
 
<span class="p_add">+#if IS_ENABLED(CONFIG_DEVICE_UNADDRESSABLE)</span>
<span class="p_add">+int device_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+		       unsigned long addr,</span>
<span class="p_add">+		       swp_entry_t entry,</span>
<span class="p_add">+		       unsigned flags,</span>
<span class="p_add">+		       pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page = device_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+	BUG_ON(!page-&gt;pgmap-&gt;page_fault);</span>
<span class="p_add">+	return page-&gt;pgmap-&gt;page_fault(vma, addr, page, flags, pmdp);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(device_entry_fault);</span>
<span class="p_add">+#endif /* CONFIG_DEVICE_UNADDRESSABLE */</span>
<span class="p_add">+</span>
 static void pgmap_radix_release(struct resource *res)
 {
 	resource_size_t key, align_start, align_size, align_end;
<span class="p_chunk">@@ -258,7 +275,7 @@</span> <span class="p_context"> static void devm_memremap_pages_release(struct device *dev, void *data)</span>
 
 	lock_device_hotplug();
 	mem_hotplug_begin();
<span class="p_del">-	arch_remove_memory(align_start, align_size, MEMORY_DEVICE);</span>
<span class="p_add">+	arch_remove_memory(align_start, align_size, pgmap-&gt;flags);</span>
 	mem_hotplug_done();
 	unlock_device_hotplug();
 
<span class="p_chunk">@@ -338,6 +355,7 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 	pgmap-&gt;ref = ref;
 	pgmap-&gt;res = &amp;page_map-&gt;res;
 	pgmap-&gt;flags = MEMORY_DEVICE;
<span class="p_add">+	pgmap-&gt;page_fault = NULL;</span>
 	pgmap-&gt;page_free = NULL;
 	pgmap-&gt;data = NULL;
 
<span class="p_chunk">@@ -378,7 +396,7 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 
 	lock_device_hotplug();
 	mem_hotplug_begin();
<span class="p_del">-	error = arch_add_memory(nid, align_start, align_size, MEMORY_DEVICE);</span>
<span class="p_add">+	error = arch_add_memory(nid, align_start, align_size, pgmap-&gt;flags);</span>
 	mem_hotplug_done();
 	unlock_device_hotplug();
 	if (error)
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index 9b8fccb..9502315 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -700,6 +700,18 @@</span> <span class="p_context"> config ZONE_DEVICE</span>
 
 	  If FS_DAX is enabled, then say Y.
 
<span class="p_add">+config DEVICE_UNADDRESSABLE</span>
<span class="p_add">+	bool &quot;Un-addressable device memory (GPU memory, ...)&quot;</span>
<span class="p_add">+	depends on ZONE_DEVICE</span>
<span class="p_add">+</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Allow to create struct page for un-addressable device memory</span>
<span class="p_add">+	  ie memory that is only accessible by the device (or group of</span>
<span class="p_add">+	  devices).</span>
<span class="p_add">+</span>
<span class="p_add">+	  Having struct page is necessary for process memory migration</span>
<span class="p_add">+	  to device memory.</span>
<span class="p_add">+</span>
 config FRAME_VECTOR
 	bool
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 235ba51..33aff303 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -49,6 +49,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/swap.h&gt;
 #include &lt;linux/highmem.h&gt;
 #include &lt;linux/pagemap.h&gt;
<span class="p_add">+#include &lt;linux/memremap.h&gt;</span>
 #include &lt;linux/ksm.h&gt;
 #include &lt;linux/rmap.h&gt;
 #include &lt;linux/export.h&gt;
<span class="p_chunk">@@ -927,6 +928,25 @@</span> <span class="p_context"> copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 					pte = pte_swp_mksoft_dirty(pte);
 				set_pte_at(src_mm, addr, src_pte, pte);
 			}
<span class="p_add">+		} else if (is_device_entry(entry)) {</span>
<span class="p_add">+			page = device_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Update rss count even for un-addressable page as</span>
<span class="p_add">+			 * they should be consider just like any other page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			get_page(page);</span>
<span class="p_add">+			rss[mm_counter(page)]++;</span>
<span class="p_add">+			page_dup_rmap(page, false);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (is_write_device_entry(entry) &amp;&amp;</span>
<span class="p_add">+			    is_cow_mapping(vm_flags)) {</span>
<span class="p_add">+				make_device_entry_read(&amp;entry);</span>
<span class="p_add">+				pte = swp_entry_to_pte(entry);</span>
<span class="p_add">+				if (pte_swp_soft_dirty(*src_pte))</span>
<span class="p_add">+					pte = pte_swp_mksoft_dirty(pte);</span>
<span class="p_add">+				set_pte_at(src_mm, addr, src_pte, pte);</span>
<span class="p_add">+			}</span>
 		}
 		goto out_set_pte;
 	}
<span class="p_chunk">@@ -1243,6 +1263,34 @@</span> <span class="p_context"> static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
 			}
 			continue;
 		}
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Un-addressable page must always be check that are not like</span>
<span class="p_add">+		 * other swap entries and thus should be check no matter what</span>
<span class="p_add">+		 * details-&gt;check_swap_entries value is.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		entry = pte_to_swp_entry(ptent);</span>
<span class="p_add">+		if (non_swap_entry(entry) &amp;&amp; is_device_entry(entry)) {</span>
<span class="p_add">+			struct page *page = device_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (unlikely(details &amp;&amp; details-&gt;check_mapping)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * unmap_shared_mapping_pages() wants to</span>
<span class="p_add">+				 * invalidate cache without truncating:</span>
<span class="p_add">+				 * unmap shared but keep private pages.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (details-&gt;check_mapping !=</span>
<span class="p_add">+				    page_rmapping(page))</span>
<span class="p_add">+					continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			pte_clear_not_present_full(mm, addr, pte, tlb-&gt;fullmm);</span>
<span class="p_add">+			rss[mm_counter(page)]--;</span>
<span class="p_add">+			page_remove_rmap(page, false);</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		/* If details-&gt;check_mapping, we leave swap entries. */
 		if (unlikely(details))
 			continue;
<span class="p_chunk">@@ -2690,6 +2738,14 @@</span> <span class="p_context"> int do_swap_page(struct vm_fault *vmf)</span>
 		if (is_migration_entry(entry)) {
 			migration_entry_wait(vma-&gt;vm_mm, vmf-&gt;pmd,
 					     vmf-&gt;address);
<span class="p_add">+		} else if (is_device_entry(entry)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * For un-addressable device memory we call the pgmap</span>
<span class="p_add">+			 * fault handler callback. The callback must migrate</span>
<span class="p_add">+			 * the page back to some CPU accessible page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			ret = device_entry_fault(vma, vmf-&gt;address, entry,</span>
<span class="p_add">+						 vmf-&gt;flags, vmf-&gt;pmd);</span>
 		} else if (is_hwpoison_entry(entry)) {
 			ret = VM_FAULT_HWPOISON;
 		} else {
<span class="p_chunk">@@ -3679,6 +3735,7 @@</span> <span class="p_context"> static int wp_huge_pud(struct vm_fault *vmf, pud_t orig_pud)</span>
 static int handle_pte_fault(struct vm_fault *vmf)
 {
 	pte_t entry;
<span class="p_add">+	struct page *page;</span>
 
 	if (unlikely(pmd_none(*vmf-&gt;pmd))) {
 		/*
<span class="p_chunk">@@ -3729,9 +3786,16 @@</span> <span class="p_context"> static int handle_pte_fault(struct vm_fault *vmf)</span>
 	if (pte_protnone(vmf-&gt;orig_pte) &amp;&amp; vma_is_accessible(vmf-&gt;vma))
 		return do_numa_page(vmf);
 
<span class="p_add">+	/* Catch mapping of un-addressable memory this should never happen */</span>
<span class="p_add">+	entry = vmf-&gt;orig_pte;</span>
<span class="p_add">+	page = pfn_to_page(pte_pfn(entry));</span>
<span class="p_add">+	if (!is_addressable_page(page)) {</span>
<span class="p_add">+		print_bad_pte(vmf-&gt;vma, vmf-&gt;address, entry, page);</span>
<span class="p_add">+		return VM_FAULT_SIGBUS;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	vmf-&gt;ptl = pte_lockptr(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;pmd);
 	spin_lock(vmf-&gt;ptl);
<span class="p_del">-	entry = vmf-&gt;orig_pte;</span>
 	if (unlikely(!pte_same(*vmf-&gt;pte, entry)))
 		goto unlock;
 	if (vmf-&gt;flags &amp; FAULT_FLAG_WRITE) {
<span class="p_header">diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="p_header">index 46960b3..4dcc003 100644</span>
<span class="p_header">--- a/mm/memory_hotplug.c</span>
<span class="p_header">+++ b/mm/memory_hotplug.c</span>
<span class="p_chunk">@@ -152,7 +152,7 @@</span> <span class="p_context"> void mem_hotplug_done(void)</span>
 /* add this memory to iomem resource */
 static struct resource *register_memory_resource(u64 start, u64 size)
 {
<span class="p_del">-	struct resource *res;</span>
<span class="p_add">+	struct resource *res, *conflict;</span>
 	res = kzalloc(sizeof(struct resource), GFP_KERNEL);
 	if (!res)
 		return ERR_PTR(-ENOMEM);
<span class="p_chunk">@@ -161,7 +161,13 @@</span> <span class="p_context"> static struct resource *register_memory_resource(u64 start, u64 size)</span>
 	res-&gt;start = start;
 	res-&gt;end = start + size - 1;
 	res-&gt;flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;
<span class="p_del">-	if (request_resource(&amp;iomem_resource, res) &lt; 0) {</span>
<span class="p_add">+	conflict =  request_resource_conflict(&amp;iomem_resource, res);</span>
<span class="p_add">+	if (conflict) {</span>
<span class="p_add">+		if (conflict-&gt;desc == IORES_DESC_UNADDRESSABLE_MEMORY) {</span>
<span class="p_add">+			pr_debug(&quot;Device un-addressable memory block &quot;</span>
<span class="p_add">+				 &quot;memory hotplug at %#010llx !\n&quot;,</span>
<span class="p_add">+				 (unsigned long long)start);</span>
<span class="p_add">+		}</span>
 		pr_debug(&quot;System RAM resource %pR cannot be added\n&quot;, res);
 		kfree(res);
 		return ERR_PTR(-EEXIST);
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index 8edd0d5..50ac297 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -126,6 +126,18 @@</span> <span class="p_context"> static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,</span>
 
 				pages++;
 			}
<span class="p_add">+</span>
<span class="p_add">+			if (is_write_device_entry(entry)) {</span>
<span class="p_add">+				pte_t newpte;</span>
<span class="p_add">+</span>
<span class="p_add">+				make_device_entry_read(&amp;entry);</span>
<span class="p_add">+				newpte = swp_entry_to_pte(entry);</span>
<span class="p_add">+				if (pte_swp_soft_dirty(oldpte))</span>
<span class="p_add">+					newpte = pte_swp_mksoft_dirty(newpte);</span>
<span class="p_add">+				set_pte_at(mm, addr, pte, newpte);</span>
<span class="p_add">+</span>
<span class="p_add">+				pages++;</span>
<span class="p_add">+			}</span>
 		}
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	arch_leave_lazy_mmu_mode();

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



