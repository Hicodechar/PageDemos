
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[RFC,2/4] hugetlb: add support for preferred node to alloc_huge_page_nodemask - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [RFC,2/4] hugetlb: add support for preferred node to alloc_huge_page_nodemask</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 13, 2017, 9 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170613090039.14393-3-mhocko@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9783373/mbox/"
   >mbox</a>
|
   <a href="/patch/9783373/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9783373/">/patch/9783373/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	F2B4860325 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 13 Jun 2017 09:01:33 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id E6FE4274D1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 13 Jun 2017 09:01:33 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id DBB68286E7; Tue, 13 Jun 2017 09:01:33 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id BEE61274D1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 13 Jun 2017 09:01:32 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752591AbdFMJA5 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 13 Jun 2017 05:00:57 -0400
Received: from mail-wr0-f193.google.com ([209.85.128.193]:33026 &quot;EHLO
	mail-wr0-f193.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752436AbdFMJAx (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 13 Jun 2017 05:00:53 -0400
Received: by mail-wr0-f193.google.com with SMTP id v104so27755383wrb.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 13 Jun 2017 02:00:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=K8553EIcyzpn7OIUZQfLMJwQG9lursCWYzZ2U2EGKW0=;
	b=cPebhvImjyTGrxQQGbHFnUUFINQ02IUk9W1uiJU+2L02vw7fnlztvBezTibVtPnvbd
	z40pdvWcXm+SlbQsdB2F7unFsnKbDUjunIahlaM71T+BItQT/bkVJrF2atawsj2dUFEK
	FtYBnen7yPYpEe0tVysF55dGikXg3GGIJqF+gEUTFUXG1j6uEbSzVOdqyL3rUwIyStqu
	bUsuGe942xrqg8heopGrG6EAxR05d4+ibFt157nXdoHYd+lnHrfgFeKp0Wxbd1FhmcMB
	M+IvOryymhJxwrdhQKl0Yfa1e6MNUQE+E3oIU2TieFpPTn2cGCjRZ7eiNaxoeAZoB36N
	npYg==
X-Gm-Message-State: AKS2vOxlDa6nFLNpVlyiSVX/LZEp8HGpNzTnBRraebgrRrBR2zLaubnu
	dYj4tApVrKHw3w==
X-Received: by 10.223.133.167 with SMTP id 36mr2187849wrt.86.1497344450910; 
	Tue, 13 Jun 2017 02:00:50 -0700 (PDT)
Received: from tiehlicka.suse.cz (prg-ext-pat.suse.com. [213.151.95.130])
	by smtp.gmail.com with ESMTPSA id
	c11sm6041078wrb.58.2017.06.13.02.00.50
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 13 Jun 2017 02:00:50 -0700 (PDT)
From: Michal Hocko &lt;mhocko@kernel.org&gt;
To: &lt;linux-mm@kvack.org&gt;
Cc: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Mel Gorman &lt;mgorman@suse.de&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;, Michal Hocko &lt;mhocko@suse.com&gt;
Subject: [RFC PATCH 2/4] hugetlb: add support for preferred node to
	alloc_huge_page_nodemask
Date: Tue, 13 Jun 2017 11:00:37 +0200
Message-Id: &lt;20170613090039.14393-3-mhocko@kernel.org&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170613090039.14393-1-mhocko@kernel.org&gt;
References: &lt;20170613090039.14393-1-mhocko@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 13, 2017, 9 a.m.</div>
<pre class="content">
<span class="from">From: Michal Hocko &lt;mhocko@suse.com&gt;</span>

alloc_huge_page_nodemask tries to allocate from any numa node in the
allowed node mask starting from lower numa nodes. This might lead to
filling up those low NUMA nodes while others are not used. We can reduce
this risk by introducing a concept of the preferred node similar to what
we have in the regular page allocator. We will start allocating from the
preferred nid and then iterate over all allowed nodes in the zonelist
order until we try them all.

This is mimicking the page allocator logic except it operates on
per-node mempools. dequeue_huge_page_vma already does this so distill
the zonelist logic into a more generic dequeue_huge_page_nodemask
and use it in alloc_huge_page_nodemask.
<span class="signed-off-by">
Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
---
 include/linux/hugetlb.h |   3 +-
 include/linux/migrate.h |   2 +-
 mm/hugetlb.c            | 106 +++++++++++++++++++++++++-----------------------
 3 files changed, 59 insertions(+), 52 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - June 14, 2017, 4:17 p.m.</div>
<pre class="content">
On 06/13/2017 11:00 AM, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; alloc_huge_page_nodemask tries to allocate from any numa node in the</span>
<span class="quote">&gt; allowed node mask starting from lower numa nodes. This might lead to</span>
<span class="quote">&gt; filling up those low NUMA nodes while others are not used. We can reduce</span>
<span class="quote">&gt; this risk by introducing a concept of the preferred node similar to what</span>
<span class="quote">&gt; we have in the regular page allocator. We will start allocating from the</span>
<span class="quote">&gt; preferred nid and then iterate over all allowed nodes in the zonelist</span>
<span class="quote">&gt; order until we try them all.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is mimicking the page allocator logic except it operates on</span>
<span class="quote">&gt; per-node mempools. dequeue_huge_page_vma already does this so distill</span>
<span class="quote">&gt; the zonelist logic into a more generic dequeue_huge_page_nodemask</span>
<span class="quote">&gt; and use it in alloc_huge_page_nodemask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>

I&#39;ve reviewed the current version in git, where patch 3/4 is folded.

Noticed some things below, but after fixing:
<span class="acked-by">Acked-by: Vlastimil Babka &lt;vbabka@suse.cz&gt;</span>
<span class="quote">

&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -897,29 +897,58 @@ static struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt; +/* Movability of hugepages depends on migration support. */</span>
<span class="quote">&gt; +static inline gfp_t htlb_alloc_mask(struct hstate *h)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	struct page *page;</span>
<span class="quote">&gt; -	int node;</span>
<span class="quote">&gt; +	if (hugepages_treat_as_movable || hugepage_migration_supported(h))</span>
<span class="quote">&gt; +		return GFP_HIGHUSER_MOVABLE;</span>
<span class="quote">&gt; +	else</span>
<span class="quote">&gt; +		return GFP_HIGHUSER;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (nid != NUMA_NO_NODE)</span>
<span class="quote">&gt; -		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="quote">&gt; +static struct page *dequeue_huge_page_nodemask(struct hstate *h, int nid,</span>
<span class="quote">&gt; +		nodemask_t *nmask)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned int cpuset_mems_cookie;</span>
<span class="quote">&gt; +	struct zonelist *zonelist;</span>
<span class="quote">&gt; +	struct page *page = NULL;</span>
<span class="quote">&gt; +	struct zone *zone;</span>
<span class="quote">&gt; +	struct zoneref *z;</span>
<span class="quote">&gt; +	gfp_t gfp_mask;</span>
<span class="quote">&gt; +	int node = -1;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	gfp_mask = htlb_alloc_mask(h);</span>
<span class="quote">&gt; +	zonelist = node_zonelist(nid, gfp_mask);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +retry_cpuset:</span>
<span class="quote">&gt; +	cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="quote">&gt; +	for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {</span>
<span class="quote">&gt; +		if (!cpuset_zone_allowed(zone, gfp_mask))</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * no need to ask again on the same node. Pool is node rather than</span>
<span class="quote">&gt; +		 * zone aware</span>
<span class="quote">&gt; +		 */</span>
<span class="quote">&gt; +		if (zone_to_nid(zone) == node)</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		node = zone_to_nid(zone);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	for_each_online_node(node) {</span>
<span class="quote">&gt;  		page = dequeue_huge_page_node_exact(h, node);</span>
<span class="quote">&gt;  		if (page)</span>
<span class="quote">&gt; -			return page;</span>
<span class="quote">&gt; +			break;</span>

Either keep return page here...
<span class="quote">
&gt;  	}</span>
<span class="quote">&gt; +	if (unlikely(!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie)))</span>
<span class="quote">&gt; +		goto retry_cpuset;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	return NULL;</span>

... or return page here.
<span class="quote">
&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -/* Movability of hugepages depends on migration support. */</span>
<span class="quote">&gt; -static inline gfp_t htlb_alloc_mask(struct hstate *h)</span>
<span class="quote">&gt; +static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	if (hugepages_treat_as_movable || hugepage_migration_supported(h))</span>
<span class="quote">&gt; -		return GFP_HIGHUSER_MOVABLE;</span>
<span class="quote">&gt; -	else</span>
<span class="quote">&gt; -		return GFP_HIGHUSER;</span>
<span class="quote">&gt; +	if (nid != NUMA_NO_NODE)</span>
<span class="quote">&gt; +		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return dequeue_huge_page_nodemask(h, nid, NULL);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>

...
<span class="quote">
&gt; @@ -1655,25 +1661,25 @@ struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
<span class="quote">&gt;  	return page;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask)</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="quote">&gt; +		nodemask_t *nmask)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	struct page *page = NULL;</span>
<span class="quote">&gt; -	int node;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	spin_lock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {</span>
<span class="quote">&gt; -		for_each_node_mask(node, *nmask) {</span>
<span class="quote">&gt; -			page = dequeue_huge_page_node_exact(h, node);</span>
<span class="quote">&gt; -			if (page)</span>
<span class="quote">&gt; -				break;</span>
<span class="quote">&gt; -		}</span>
<span class="quote">&gt; +		page = dequeue_huge_page_nodemask(h, preferred_nid, nmask);</span>
<span class="quote">


&gt; +		if (page)</span>
<span class="quote">&gt; +			goto unlock;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +unlock:</span>

This doesn&#39;t seem needed?
<span class="quote">
&gt;  	spin_unlock(&amp;hugetlb_lock);</span>
<span class="quote">&gt;  	if (page)</span>
<span class="quote">&gt;  		return page;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* No reservations, try to overcommit */</span>
<span class="quote">&gt; -	return __alloc_buddy_huge_page(h, NUMA_NO_NODE, nmask);</span>
<span class="quote">&gt; +	return __alloc_buddy_huge_page(h, preferred_nid, nmask);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - June 14, 2017, 10:12 p.m.</div>
<pre class="content">
On 06/13/2017 02:00 AM, Michal Hocko wrote:
<span class="quote">&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; alloc_huge_page_nodemask tries to allocate from any numa node in the</span>
<span class="quote">&gt; allowed node mask starting from lower numa nodes. This might lead to</span>
<span class="quote">&gt; filling up those low NUMA nodes while others are not used. We can reduce</span>
<span class="quote">&gt; this risk by introducing a concept of the preferred node similar to what</span>
<span class="quote">&gt; we have in the regular page allocator. We will start allocating from the</span>
<span class="quote">&gt; preferred nid and then iterate over all allowed nodes in the zonelist</span>
<span class="quote">&gt; order until we try them all.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is mimicking the page allocator logic except it operates on</span>
<span class="quote">&gt; per-node mempools. dequeue_huge_page_vma already does this so distill</span>
<span class="quote">&gt; the zonelist logic into a more generic dequeue_huge_page_nodemask</span>
<span class="quote">&gt; and use it in alloc_huge_page_nodemask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; ---</span>


I built attempts/hugetlb-zonelists, threw it on a test machine, ran the
libhugetlbfs test suite and saw failures.  The failures started with this
patch: commit 7e8b09f14495 in your tree.  I have not yet started to look
into the failures.  It is even possible that the tests are making bad
assumptions, but there certainly appears to be changes in behavior visible
to the application(s).

FYI - My &#39;test machine&#39; is an x86 KVM insatnce with 8GB memory simulating
2 nodes.  Huge page allocations before running tests:
node0
512	free_hugepages
512	nr_hugepages
0	surplus_hugepages
node1
512	free_hugepages
512	nr_hugepages
0	surplus_hugepages

I can take a closer look at the failures tomorrow.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - June 15, 2017, 12:12 a.m.</div>
<pre class="content">
On 06/14/2017 03:12 PM, Mike Kravetz wrote:
<span class="quote">&gt; On 06/13/2017 02:00 AM, Michal Hocko wrote:</span>
<span class="quote">&gt;&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; alloc_huge_page_nodemask tries to allocate from any numa node in the</span>
<span class="quote">&gt;&gt; allowed node mask starting from lower numa nodes. This might lead to</span>
<span class="quote">&gt;&gt; filling up those low NUMA nodes while others are not used. We can reduce</span>
<span class="quote">&gt;&gt; this risk by introducing a concept of the preferred node similar to what</span>
<span class="quote">&gt;&gt; we have in the regular page allocator. We will start allocating from the</span>
<span class="quote">&gt;&gt; preferred nid and then iterate over all allowed nodes in the zonelist</span>
<span class="quote">&gt;&gt; order until we try them all.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This is mimicking the page allocator logic except it operates on</span>
<span class="quote">&gt;&gt; per-node mempools. dequeue_huge_page_vma already does this so distill</span>
<span class="quote">&gt;&gt; the zonelist logic into a more generic dequeue_huge_page_nodemask</span>
<span class="quote">&gt;&gt; and use it in alloc_huge_page_nodemask.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I built attempts/hugetlb-zonelists, threw it on a test machine, ran the</span>
<span class="quote">&gt; libhugetlbfs test suite and saw failures.  The failures started with this</span>
<span class="quote">&gt; patch: commit 7e8b09f14495 in your tree.  I have not yet started to look</span>
<span class="quote">&gt; into the failures.  It is even possible that the tests are making bad</span>
<span class="quote">&gt; assumptions, but there certainly appears to be changes in behavior visible</span>
<span class="quote">&gt; to the application(s).</span>

nm.  The failures were the result of dequeue_huge_page_nodemask() always
returning NULL.  Vlastimil already noticed this issue and provided a
solution.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 15, 2017, 8:12 a.m.</div>
<pre class="content">
On Wed 14-06-17 17:12:31, Mike Kravetz wrote:
<span class="quote">&gt; On 06/14/2017 03:12 PM, Mike Kravetz wrote:</span>
<span class="quote">&gt; &gt; On 06/13/2017 02:00 AM, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt;&gt; From: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; alloc_huge_page_nodemask tries to allocate from any numa node in the</span>
<span class="quote">&gt; &gt;&gt; allowed node mask starting from lower numa nodes. This might lead to</span>
<span class="quote">&gt; &gt;&gt; filling up those low NUMA nodes while others are not used. We can reduce</span>
<span class="quote">&gt; &gt;&gt; this risk by introducing a concept of the preferred node similar to what</span>
<span class="quote">&gt; &gt;&gt; we have in the regular page allocator. We will start allocating from the</span>
<span class="quote">&gt; &gt;&gt; preferred nid and then iterate over all allowed nodes in the zonelist</span>
<span class="quote">&gt; &gt;&gt; order until we try them all.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; This is mimicking the page allocator logic except it operates on</span>
<span class="quote">&gt; &gt;&gt; per-node mempools. dequeue_huge_page_vma already does this so distill</span>
<span class="quote">&gt; &gt;&gt; the zonelist logic into a more generic dequeue_huge_page_nodemask</span>
<span class="quote">&gt; &gt;&gt; and use it in alloc_huge_page_nodemask.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Signed-off-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt;&gt; ---</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I built attempts/hugetlb-zonelists, threw it on a test machine, ran the</span>
<span class="quote">&gt; &gt; libhugetlbfs test suite and saw failures.  The failures started with this</span>
<span class="quote">&gt; &gt; patch: commit 7e8b09f14495 in your tree.  I have not yet started to look</span>
<span class="quote">&gt; &gt; into the failures.  It is even possible that the tests are making bad</span>
<span class="quote">&gt; &gt; assumptions, but there certainly appears to be changes in behavior visible</span>
<span class="quote">&gt; &gt; to the application(s).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; nm.  The failures were the result of dequeue_huge_page_nodemask() always</span>
<span class="quote">&gt; returning NULL.  Vlastimil already noticed this issue and provided a</span>
<span class="quote">&gt; solution.</span>

I have pushed my current version to the same branch.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index 016831fcdca1..d4c33a8583be 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -349,7 +349,8 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 struct page *alloc_huge_page_node(struct hstate *h, int nid);
 struct page *alloc_huge_page_noerr(struct vm_area_struct *vma,
 				unsigned long addr, int avoid_reserve);
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask);</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="p_add">+				nodemask_t *nmask);</span>
 int huge_add_to_page_cache(struct page *page, struct address_space *mapping,
 			pgoff_t idx);
 
<span class="p_header">diff --git a/include/linux/migrate.h b/include/linux/migrate.h</span>
<span class="p_header">index f80c9882403a..af3ccf93efaa 100644</span>
<span class="p_header">--- a/include/linux/migrate.h</span>
<span class="p_header">+++ b/include/linux/migrate.h</span>
<span class="p_chunk">@@ -38,7 +38,7 @@</span> <span class="p_context"> static inline struct page *new_page_nodemask(struct page *page, int preferred_ni</span>
 
 	if (PageHuge(page))
 		return alloc_huge_page_nodemask(page_hstate(compound_head(page)),
<span class="p_del">-				nodemask);</span>
<span class="p_add">+				preferred_nid, nodemask);</span>
 
 	if (PageHighMem(page)
 	    || (zone_idx(page_zone(page)) == ZONE_MOVABLE))
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 3d5f25d589b3..696de029f0fa 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -897,29 +897,58 @@</span> <span class="p_context"> static struct page *dequeue_huge_page_node_exact(struct hstate *h, int nid)</span>
 	return page;
 }
 
<span class="p_del">-static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
<span class="p_add">+/* Movability of hugepages depends on migration support. */</span>
<span class="p_add">+static inline gfp_t htlb_alloc_mask(struct hstate *h)</span>
 {
<span class="p_del">-	struct page *page;</span>
<span class="p_del">-	int node;</span>
<span class="p_add">+	if (hugepages_treat_as_movable || hugepage_migration_supported(h))</span>
<span class="p_add">+		return GFP_HIGHUSER_MOVABLE;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return GFP_HIGHUSER;</span>
<span class="p_add">+}</span>
 
<span class="p_del">-	if (nid != NUMA_NO_NODE)</span>
<span class="p_del">-		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="p_add">+static struct page *dequeue_huge_page_nodemask(struct hstate *h, int nid,</span>
<span class="p_add">+		nodemask_t *nmask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int cpuset_mems_cookie;</span>
<span class="p_add">+	struct zonelist *zonelist;</span>
<span class="p_add">+	struct page *page = NULL;</span>
<span class="p_add">+	struct zone *zone;</span>
<span class="p_add">+	struct zoneref *z;</span>
<span class="p_add">+	gfp_t gfp_mask;</span>
<span class="p_add">+	int node = -1;</span>
<span class="p_add">+</span>
<span class="p_add">+	gfp_mask = htlb_alloc_mask(h);</span>
<span class="p_add">+	zonelist = node_zonelist(nid, gfp_mask);</span>
<span class="p_add">+</span>
<span class="p_add">+retry_cpuset:</span>
<span class="p_add">+	cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="p_add">+	for_each_zone_zonelist_nodemask(zone, z, zonelist, gfp_zone(gfp_mask), nmask) {</span>
<span class="p_add">+		if (!cpuset_zone_allowed(zone, gfp_mask))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * no need to ask again on the same node. Pool is node rather than</span>
<span class="p_add">+		 * zone aware</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (zone_to_nid(zone) == node)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		node = zone_to_nid(zone);</span>
 
<span class="p_del">-	for_each_online_node(node) {</span>
 		page = dequeue_huge_page_node_exact(h, node);
 		if (page)
<span class="p_del">-			return page;</span>
<span class="p_add">+			break;</span>
 	}
<span class="p_add">+	if (unlikely(!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie)))</span>
<span class="p_add">+		goto retry_cpuset;</span>
<span class="p_add">+</span>
 	return NULL;
 }
 
<span class="p_del">-/* Movability of hugepages depends on migration support. */</span>
<span class="p_del">-static inline gfp_t htlb_alloc_mask(struct hstate *h)</span>
<span class="p_add">+static struct page *dequeue_huge_page_node(struct hstate *h, int nid)</span>
 {
<span class="p_del">-	if (hugepages_treat_as_movable || hugepage_migration_supported(h))</span>
<span class="p_del">-		return GFP_HIGHUSER_MOVABLE;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		return GFP_HIGHUSER;</span>
<span class="p_add">+	if (nid != NUMA_NO_NODE)</span>
<span class="p_add">+		return dequeue_huge_page_node_exact(h, nid);</span>
<span class="p_add">+</span>
<span class="p_add">+	return dequeue_huge_page_nodemask(h, nid, NULL);</span>
 }
 
 static struct page *dequeue_huge_page_vma(struct hstate *h,
<span class="p_chunk">@@ -927,15 +956,10 @@</span> <span class="p_context"> static struct page *dequeue_huge_page_vma(struct hstate *h,</span>
 				unsigned long address, int avoid_reserve,
 				long chg)
 {
<span class="p_del">-	struct page *page = NULL;</span>
<span class="p_add">+	struct page *page;</span>
 	struct mempolicy *mpol;
 	nodemask_t *nodemask;
<span class="p_del">-	gfp_t gfp_mask;</span>
 	int nid;
<span class="p_del">-	struct zonelist *zonelist;</span>
<span class="p_del">-	struct zone *zone;</span>
<span class="p_del">-	struct zoneref *z;</span>
<span class="p_del">-	unsigned int cpuset_mems_cookie;</span>
 
 	/*
 	 * A child process with MAP_PRIVATE mappings created by their parent
<span class="p_chunk">@@ -950,32 +974,14 @@</span> <span class="p_context"> static struct page *dequeue_huge_page_vma(struct hstate *h,</span>
 	if (avoid_reserve &amp;&amp; h-&gt;free_huge_pages - h-&gt;resv_huge_pages == 0)
 		goto err;
 
<span class="p_del">-retry_cpuset:</span>
<span class="p_del">-	cpuset_mems_cookie = read_mems_allowed_begin();</span>
<span class="p_del">-	gfp_mask = htlb_alloc_mask(h);</span>
<span class="p_del">-	nid = huge_node(vma, address, gfp_mask, &amp;mpol, &amp;nodemask);</span>
<span class="p_del">-	zonelist = node_zonelist(nid, gfp_mask);</span>
<span class="p_del">-</span>
<span class="p_del">-	for_each_zone_zonelist_nodemask(zone, z, zonelist,</span>
<span class="p_del">-						MAX_NR_ZONES - 1, nodemask) {</span>
<span class="p_del">-		if (cpuset_zone_allowed(zone, gfp_mask)) {</span>
<span class="p_del">-			page = dequeue_huge_page_node(h, zone_to_nid(zone));</span>
<span class="p_del">-			if (page) {</span>
<span class="p_del">-				if (avoid_reserve)</span>
<span class="p_del">-					break;</span>
<span class="p_del">-				if (!vma_has_reserves(vma, chg))</span>
<span class="p_del">-					break;</span>
<span class="p_del">-</span>
<span class="p_del">-				SetPagePrivate(page);</span>
<span class="p_del">-				h-&gt;resv_huge_pages--;</span>
<span class="p_del">-				break;</span>
<span class="p_del">-			}</span>
<span class="p_del">-		}</span>
<span class="p_add">+	nid = huge_node(vma, address, htlb_alloc_mask(h), &amp;mpol, &amp;nodemask);</span>
<span class="p_add">+	page = dequeue_huge_page_nodemask(h, nid, nodemask);</span>
<span class="p_add">+	if (page &amp;&amp; !avoid_reserve &amp;&amp; vma_has_reserves(vma, chg)) {</span>
<span class="p_add">+		SetPagePrivate(page);</span>
<span class="p_add">+		h-&gt;resv_huge_pages--;</span>
 	}
 
 	mpol_cond_put(mpol);
<span class="p_del">-	if (unlikely(!page &amp;&amp; read_mems_allowed_retry(cpuset_mems_cookie)))</span>
<span class="p_del">-		goto retry_cpuset;</span>
 	return page;
 
 err:
<span class="p_chunk">@@ -1655,25 +1661,25 @@</span> <span class="p_context"> struct page *alloc_huge_page_node(struct hstate *h, int nid)</span>
 	return page;
 }
 
<span class="p_del">-struct page *alloc_huge_page_nodemask(struct hstate *h, nodemask_t *nmask)</span>
<span class="p_add">+</span>
<span class="p_add">+struct page *alloc_huge_page_nodemask(struct hstate *h, int preferred_nid,</span>
<span class="p_add">+		nodemask_t *nmask)</span>
 {
 	struct page *page = NULL;
<span class="p_del">-	int node;</span>
 
 	spin_lock(&amp;hugetlb_lock);
 	if (h-&gt;free_huge_pages - h-&gt;resv_huge_pages &gt; 0) {
<span class="p_del">-		for_each_node_mask(node, *nmask) {</span>
<span class="p_del">-			page = dequeue_huge_page_node_exact(h, node);</span>
<span class="p_del">-			if (page)</span>
<span class="p_del">-				break;</span>
<span class="p_del">-		}</span>
<span class="p_add">+		page = dequeue_huge_page_nodemask(h, preferred_nid, nmask);</span>
<span class="p_add">+		if (page)</span>
<span class="p_add">+			goto unlock;</span>
 	}
<span class="p_add">+unlock:</span>
 	spin_unlock(&amp;hugetlb_lock);
 	if (page)
 		return page;
 
 	/* No reservations, try to overcommit */
<span class="p_del">-	return __alloc_buddy_huge_page(h, NUMA_NO_NODE, nmask);</span>
<span class="p_add">+	return __alloc_buddy_huge_page(h, preferred_nid, nmask);</span>
 }
 
 /*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



