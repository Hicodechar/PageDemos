
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>memcg: hugetlbfs basic usage accounting - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    memcg: hugetlbfs basic usage accounting</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=174271">Roman Gushchin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Nov. 14, 2017, 5:24 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171114172429.8916-1-guro@fb.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10057949/mbox/"
   >mbox</a>
|
   <a href="/patch/10057949/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10057949/">/patch/10057949/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	64075602A7 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Nov 2017 17:25:27 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 44069295FD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Nov 2017 17:25:27 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 37F7C296B6; Tue, 14 Nov 2017 17:25:27 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 3C27F296CD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 14 Nov 2017 17:25:26 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756008AbdKNRZY (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 14 Nov 2017 12:25:24 -0500
Received: from mx0a-00082601.pphosted.com ([67.231.145.42]:35538 &quot;EHLO
	mx0a-00082601.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1754849AbdKNRZO (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 14 Nov 2017 12:25:14 -0500
Received: from pps.filterd (m0044008.ppops.net [127.0.0.1])
	by mx0a-00082601.pphosted.com (8.16.0.21/8.16.0.21) with SMTP id
	vAEHNX4M020546; Tue, 14 Nov 2017 09:24:59 -0800
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=fb.com;
	h=from : to : cc : subject
	: date : message-id : mime-version : content-type; s=facebook;
	bh=I3GLreKo9y9fIboyX/32ScjNe1pfuH9NJN8d52uuoIQ=;
	b=MZv3woHVkhIRXl+IB2ZS4SeIE4lQ64WnD8mcvCUMGT2rLZULbeGv5Ll9E5m8k4Bt6bTG
	IAfZyXMeKY7UejdjFf+PyFjdhbRZjkhRsPClK3A0Tf6L/2W+DDdVylaMF/Lc2ISa6+K/
	Je1g1ALHFu8GovbETlGJ7PfvMv/InRs+/nQ= 
Received: from mail.thefacebook.com ([199.201.64.23])
	by mx0a-00082601.pphosted.com with ESMTP id 2e80dmrxyt-1
	(version=TLSv1 cipher=ECDHE-RSA-AES256-SHA bits=256 verify=NOT);
	Tue, 14 Nov 2017 09:24:59 -0800
Received: from NAM02-CY1-obe.outbound.protection.outlook.com (192.168.54.28)
	by o365-in.thefacebook.com (192.168.16.24) with Microsoft SMTP
	Server (TLS) id 14.3.361.1; Tue, 14 Nov 2017 09:24:58 -0800
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=fb.onmicrosoft.com; 
	s=selector1-fb-com;
	h=From:Date:Subject:Message-ID:Content-Type:MIME-Version; 
	bh=I3GLreKo9y9fIboyX/32ScjNe1pfuH9NJN8d52uuoIQ=;
	b=k2eyolzdCKy3vDaVDM0K1yoFCy0E9dCjbG4XAjEHXv1wmvTOYnp5KnIovoT4/kugzu0pV8x0Z+TmSSAtqxUoiYNOYy38CfFK+IX3gfYAeL5GuFLdMJwSIfQdAm4fXAQPtCWiPVX7D9d1FmKUdfyf9u2Oor9szk6RUPk/6NxbWno=
Received: from castle.thefacebook.com (2620:10d:c092:200::1:3163) by
	CO1PR15MB1080.namprd15.prod.outlook.com (2a01:111:e400:7b66::10) with
	Microsoft SMTP Server (version=TLS1_2,
	cipher=TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384_P256) id 15.20.218.12;
	Tue, 14 Nov 2017 17:24:51 +0000
From: Roman Gushchin &lt;guro@fb.com&gt;
To: &lt;linux-mm@kvack.org&gt;
CC: Roman Gushchin &lt;guro@fb.com&gt;, Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;,
	Vladimir Davydov &lt;vdavydov.dev@gmail.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, Tejun Heo &lt;tj@kernel.org&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;,
	Dave Hansen &lt;dave.hansen@intel.com&gt;, &lt;kernel-team@fb.com&gt;,
	&lt;cgroups@vger.kernel.org&gt;, &lt;linux-kernel@vger.kernel.org&gt;
Subject: [PATCH] memcg: hugetlbfs basic usage accounting
Date: Tue, 14 Nov 2017 17:24:29 +0000
Message-ID: &lt;20171114172429.8916-1-guro@fb.com&gt;
X-Mailer: git-send-email 2.13.6
MIME-Version: 1.0
Content-Type: text/plain
X-Originating-IP: [2620:10d:c092:200::1:3163]
X-ClientProxiedBy: AM4PR05CA0012.eurprd05.prod.outlook.com
	(2603:10a6:205::25)
	To CO1PR15MB1080.namprd15.prod.outlook.com (2a01:111:e400:7b66::10)
X-MS-PublicTrafficType: Email
X-MS-Office365-Filtering-Correlation-Id: 325afccb-73fe-4190-6ea9-08d52b849ed3
X-Microsoft-Antispam: UriScan:; BCL:0; PCL:0;
	RULEID:(22001)(4534020)(4602075)(4627115)(201703031133081)(201702281549075)(2017052603258);
	SRVR:CO1PR15MB1080; 
X-Microsoft-Exchange-Diagnostics: 1; CO1PR15MB1080;
	3:14KOEz17tU6X6kWJrQRFswLBLyLbFV79RS6f3gNRyBI8OHpr3Ptr25U5NLM5irbg4XPNIPJlvArEkaNnA7ZwNFxXQpohuzZy48qSiwclc8bzeUaMyM9TgGYsO18XFldV0Uhxrp+8qWX0gBWu6Fc3Z1EUeNW1c46JoDIEujK895Z6oxHTJpgRjXtWZiIGUU8KL1ekS/B6EwpuCnWdTncET8sXPEsfmCp8VOqoaPJGo6wkNG5jugf2PcfUsJgxnH3I;
	25:HdTfx56rIhBls1ZJK6SkVO1AHQd+TOZnRwEuwChW3hzIYeQGVoGcep4ztwDPxBRQ60cCLwHICTletLamgakb9b6vbzlEhD1/PuJM9a5kV/sxeeun+c6O5vDipcpHfqeNzTsQXL9+rzZl3Vl3c6DHxyO1vnZYa4tiaZJP72+bmow+DPnBwPLuk3k+ZgjxRpTcspm3KBrSR0UCuYopppktmUfkSbEO6opuiLW67ERxpE3QEiirkyuWmB/6Hc+f7pxa1/zsBtwzPpn3/dXGBRiv+FT72UhjC7FP+nu+ASAiRK36LGbeURQGNZCJ2T5BMNiCn3iPJsbjWczbUb4DTrGSuA==;
	31:QwCCRkzMgwcZvKQvuhv+jrFgp6oo3TwLVWMHrMN0mC9NHbfgCRmOx9xujLKDxwHJeHhngO1RsSXs24FkjjzN63ShTnKpNGP4L1JejSDCkquJ+lb6gz09eRTOCavV2QPUqj9bvscMVxUvfxa2oqSAm7gDCi6x2UKMeL7KptT0OuN3oTYrG86JIxQb4Pb9YIa5mgn7uSYq+vZBXwFYVelSWN3CCSHTyIr1Jh1iufbHnus=
X-MS-TrafficTypeDiagnostic: CO1PR15MB1080:
X-Microsoft-Exchange-Diagnostics: 1; CO1PR15MB1080;
	20:uPLSTKjimoLouSJmnFwQDPzLAWjDoMH99MnffIAISqv+VwSKKmRoqpHgxaEPGXIEGFsSe491/N0sWqN9zIXkluiCjMhfi3fIbcKgivvAIiDB8Y1rB1blQnMoN3gs4oYyohn2a1BXDC/RuuBbb0Dr4Cer6npUoe5KxJlpGDvevJSByk2ak5kYnLD5b1YSHXYpiBDGrz//aZJQo+mtn83DTNQRsP61uIgC0ixLZRvWnMoUUPO6f5RUZAAu/4lqgV3ZMCU8QjScwctuGSMEZKUx7J2ReLiFGmabhxBCPUIS1lUYM/68ZZAEfQVqwLrbMMEMRYD2tdz52M9KynHFQsyUA2gJoC3C5MuH3+AV4pgR7tUfvrqFpCUvFQb9nhqCIGvmI45eUFGPDn20FBUBpHgNjLiyzeidw0FPxBCMmD5aIveRZJyYuYJF7HPVtpUixo7ZeOyjidtt9it46Mg3i0AmB8MqGdmtO1HUSD7vCK0RYBkmbnkt7Zx4hsybfaa8DvO1;
	4:orMt2mZQuiplXUOhdjbepnupch8Z5SzoXt70uRvO7oBG1UEhsKiAEw6rcVqzqgbdG7nsCcbm9AS8FRWVSTAZSFwq6fNcAZdjvcJ5jRck9LEVrfXLLTvocfFFu/SMvYoMTUBP+mvu6toB6kHyhOGxr1+IYh8JnY9bVh/Pvh6cXQYJNXWum/O4qPiv/ju9bYbG8LWg08Ua52SAKenPDRmojnBJXBxpLN1HVuVFJByZ/eJYeFHd0w7HknyOOnM1v7RSfqAV6PHyBdHUePCxgUZEkqS5buMilDkeYsqA9fTcoWUqRF3MNFVPENCdAbhP3OvkNlR6SOQDjsvSCaY9RexkKSvVUjfXT/vAzsPCX2biKMGf5MjEz7TNaNtLzBadznA9270YBsGP0qsLMKk1rf6c4Z97lzq3DbOpU5NXZo
	3Bz+4=
X-Microsoft-Antispam-PRVS: &lt;CO1PR15MB108024646C57A435DA9842BCBE280@CO1PR15MB1080.namprd15.prod.outlook.com&gt;
X-Exchange-Antispam-Report-Test: UriScan:(9452136761055)(67672495146484)(146099531331640)(228905959029699);
X-Exchange-Antispam-Report-CFA-Test: BCL:0; PCL:0;
	RULEID:(100000700101)(100105000095)(100000701101)(100105300095)(100000702101)(100105100095)(11241501159)(6040450)(2401047)(8121501046)(5005006)(3231022)(10201501046)(100000703101)(100105400095)(93006095)(93001095)(3002001)(6041248)(20161123562025)(20161123555025)(20161123558100)(20161123560025)(201703131423075)(201702281528075)(201703061421075)(201703061406153)(20161123564025)(6072148)(201708071742011)(100000704101)(100105200095)(100000705101)(100105500095);
	SRVR:CO1PR15MB1080; BCL:0; PCL:0;
	RULEID:(100000800101)(100110000095)(100000801101)(100110300095)(100000802101)(100110100095)(100000803101)(100110400095)(100000804101)(100110200095)(100000805101)(100110500095);
	SRVR:CO1PR15MB1080; 
X-Forefront-PRVS: 04916EA04C
X-Forefront-Antispam-Report: SFV:NSPM;
	SFS:(10019020)(6009001)(346002)(376002)(189002)(199003)(54906003)(97736004)(48376002)(33646002)(50466002)(68736007)(2361001)(2351001)(16586007)(7416002)(106356001)(69596002)(105586002)(189998001)(86362001)(575784001)(1076002)(6116002)(6512007)(53416004)(39060400002)(53936002)(36756003)(316002)(5660300001)(15650500001)(50226002)(4326008)(50986999)(25786009)(6916009)(101416001)(6666003)(305945005)(7736002)(5003940100001)(47776003)(8936002)(2906002)(6506006)(81166006)(81156014)(8676002)(6486002)(478600001)(21314002)(42262002);
	DIR:OUT; SFP:1102; SCL:1; SRVR:CO1PR15MB1080;
	H:castle.thefacebook.com; FPR:; SPF:None; PTR:InfoNoRecords;
	MX:1; A:1; LANG:en; 
Received-SPF: None (protection.outlook.com: fb.com does not designate
	permitted sender hosts)
X-Microsoft-Exchange-Diagnostics: =?us-ascii?Q?1; CO1PR15MB1080;
	23:HpIRT0T+QS0aMFtD0pEHOBC54raig6t4AwPBHy452?=
	=?us-ascii?Q?4zX8N8mFjt7khS+iVEyxHT1rGla3i1N61paSSClRboYtuZSKyHvHdsHODHnr?=
	=?us-ascii?Q?D+78hhYyI1KKnVKr0tOv19y/OZjrLvZviV2AI61OqSfebzUs1C0cYffNxI5C?=
	=?us-ascii?Q?v5xCt5zXvhle3wBsaXVK6ggljQSNHn2mzlIAehkQIk6HOBM4frNNm9s610KJ?=
	=?us-ascii?Q?f7KlPB2utBd0X+MwE8fcAy/vrJwmHgDf6yirgkPvESBFJ1n+0CEZhbNP1zcF?=
	=?us-ascii?Q?dVjjU4cLC7m5vvMuGeGoJoVvtkYTfRASNWrEzB5M0Yqfejmj7t2dyWo+xbUl?=
	=?us-ascii?Q?CjOllw1qah94Ci4GZ+71SmdmkvO4vkAIJbWFcKM6hrP12TbBCn1YFnz6Z0Zg?=
	=?us-ascii?Q?fpsyg2Vrj+JYQSL3oER8epqqTbeiWYEjYBs65t7X9D2xpx1IBcBAv7cUpy1m?=
	=?us-ascii?Q?yjA/H/br5/aoh2Vp2FqsnxGTmLpt1OzRnPwv2CH4ttSIIUT87hIILgeQAtC6?=
	=?us-ascii?Q?CdJbs21drr9SNRRqmrJBOsU7ekLTicTYys9AYVJyuXdDIa04ZFGFhCeMNcR6?=
	=?us-ascii?Q?5DU6j8wkGosGGN3VcVaHJ+XQcQgX85XeMTmGrhjbLEjAahLGg8hpqDLx0ywJ?=
	=?us-ascii?Q?z6JO3V6pTIbalNF6gBKY0oeu03PeECryosT8M6UYCiix2DTOi9j3K8S6Jetx?=
	=?us-ascii?Q?vC83FmWBED4BjPyw+xzzCT70y225JREkb4V3W8+WYQFQdj9BdDAr42yKZ8vl?=
	=?us-ascii?Q?OTn/TTTgdKPeBN84qtavCeKxjuCtaxpd2UVxWp2MkAoGjdHOvqa6esuHKDt2?=
	=?us-ascii?Q?/PY7/Onz3TwqloYAbA5/Ohmc/jzJUDSdnqXuPdXWC2AEEkHg3fben9xv5Nfi?=
	=?us-ascii?Q?P826eFeQimSSkKV5d+5JM9eoS/x7AHhllcrPv0kILaXGSacq0KLw+vVPT7gQ?=
	=?us-ascii?Q?5rQUJNz/dEmPBufeRIMOUVJ1DFduMoVSccrXEnsAX2N5cB1wfVQaXNdStuQQ?=
	=?us-ascii?Q?jLyYNJG623Wl+Mmy+D+mtNFNXVwVgi5+PZt9usxCnWTtNAwVM8M0TOG0ETBJ?=
	=?us-ascii?Q?E/g5rdWRtDZm0lBXt8cAjRxQufMNPT72QiHziteX2pbeD6kKiAtJwyq0ZoDJ?=
	=?us-ascii?Q?b6dljtviJMlxDZWjQLohjETfRNPyl1wAC+Jnof/IVwI9Rnr8qxP4BHVQNfII?=
	=?us-ascii?Q?czJVvcxFPU7DmL13AD98kX5zUzdUq3T080MnEyyjTm5orbmBtIOzMZJ1Q=3D?=
	=?us-ascii?Q?=3D?=
X-Microsoft-Exchange-Diagnostics: 1; CO1PR15MB1080;
	6:lR1KA6wYBscOhQNQLmtK7PhZ/l/FUWQqxdmYDJhiacm8EjK31Kk4w4YKMFAu9RADNsZi0jJiCGBqKTdxqVEUV7LaCQ73I8VzMtlCHXXgMiGVyDKXMU6PQEwckjA3zYJfey5JYRPhG/7quYPxbHLFGJM/FL0HOe+HqFw9n6fgfZ0qw8IxM7IhpjO7fHFVDlzgxFZqO8eD4aHWC1KjYUXgEOSKLMAAXoCZyWqI4rndwbmFPtUQC76m6zlQfh7INLDRfIIePAJvzzPse/sixh7P7ncSMKNOOCvThYFmLMwbR6EQ4pJ8cTiQw697a2UBaTtCQOwbL+KH+ZCQJhjyQPjwNoUMXznP1FU8OgM4HlvRfWo=;
	5:tvL/xyp8ozNxTUQh/UYM5q7fJbdrqXqE1fv62kky3zc4A4KIVvAH9RAKbRSP8Uf24GgIA0oHhPfvvox+2wOOa0EGhyDQRQ65ltOsmFcKuWmrf0AIp0VScApenaVTNwB4JV0lBnVP8ZgPOSYCy4s/sRwXVlqYWREY+SkzW43MJYo=;
	24:wk4dnelgshNvO25/o1kWz8jLWvbyeQTbNcNwAvqcuD3pN4hm4ooggbXAAwTEUxGo4XzkymAC5a7+uq0x2QF++IgJOqGDVNL1B7lPDM8UV3w=;
	7:xgnfICH9RsXkynOO5nvKcgpAqVshx6IFjpYSgSY331huboGKHLliQUf8irScfsrca14TFUW4M0d2Dpfz4mNNuVAH7Le5sXmaYIsPKYlk9pCb+qKrvwfYMpnFDJO4uIIG1M6eaOWmOVmCjZzCw3x2ZgvwvdmiHDKjafSmb/XbyS/xK3BeSWbJX1RAJRZcEc40ZvonTG/tKWZYLLCqQmqeR7iyh/TQxCuW9DoHxl4Mm7vfzySXUWYioej/AZ7zjAW5
SpamDiagnosticOutput: 1:99
SpamDiagnosticMetadata: NSPM
X-Microsoft-Exchange-Diagnostics: 1; CO1PR15MB1080;
	20:R334aDcQLWyiXDmyPbGjH82TP0pHbJrIIePOccmkWRt8yP0jVFl6u6WRrZyQ9hXL3YRwE4LhZKu7go4i9uQxsdzrcCg1S7eL212FgRtxhF3dbnTp1aPUcDXZno7OFgvpNOyKVLVB4effVOQmdWXrRLa+QBaaZyQVZAmVZmsXOLg=
X-MS-Exchange-CrossTenant-OriginalArrivalTime: 14 Nov 2017 17:24:51.8438
	(UTC)
X-MS-Exchange-CrossTenant-Network-Message-Id: 325afccb-73fe-4190-6ea9-08d52b849ed3
X-MS-Exchange-CrossTenant-FromEntityHeader: Hosted
X-MS-Exchange-CrossTenant-Id: 8ae927fe-1255-47a7-a2af-5f3a069daaa2
X-MS-Exchange-Transport-CrossTenantHeadersStamped: CO1PR15MB1080
X-OriginatorOrg: fb.com
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-11-14_08:, , signatures=0
X-Proofpoint-Spam-Reason: safe
X-FB-Internal: Safe
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174271">Roman Gushchin</a> - Nov. 14, 2017, 5:24 p.m.</div>
<pre class="content">
This patch implements basic accounting of memory consumption
by hugetlbfs pages for cgroup v2 memory controller.

Cgroup v2 memory controller lacks any visibility into the
hugetlbfs memory consumption. Cgroup v1 implemented a separate
hugetlbfs controller, which provided such stats, and also
provided some control abilities. Although porting of the
hugetlbfs controller to cgroup v2 is arguable a good idea and
is outside of scope of this patch, it&#39;s very useful to have
basic stats provided by memory.stat.

As hugetlbfs memory can easily represent a big portion of total
memory, it&#39;s important to understand who (which memcg/container)
is using it.

The number is represented in memory.stat as &quot;hugetlb&quot; in bytes and
is printed unconditionally. Accounting code doesn&#39;t depend on
cgroup v1 hugetlb controller.

Example:
  $ cat /sys/fs/cgroup/user.slice/user-0.slice/session-1.scope/memory.stat
  anon 1634304
  file 1163264
  kernel_stack 16384
  slab 737280
  sock 0
  shmem 0
  file_mapped 32768
  file_dirty 4096
  file_writeback 0
  inactive_anon 0
  active_anon 1634304
  inactive_file 65536
  active_file 1097728
  unevictable 0
  slab_reclaimable 282624
  slab_unreclaimable 454656
  hugetlb 1073741824
  pgfault 4580
  pgmajfault 13
  ...
<span class="signed-off-by">
Signed-off-by: Roman Gushchin &lt;guro@fb.com&gt;</span>
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;
Cc: Michal Hocko &lt;mhocko@kernel.org&gt;
Cc: Vladimir Davydov &lt;vdavydov.dev@gmail.com&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Tejun Heo &lt;tj@kernel.org&gt;
Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;
Cc: kernel-team@fb.com
Cc: cgroups@vger.kernel.org
Cc: linux-mm@kvack.org
Cc: linux-kernel@vger.kernel.org
---
 include/linux/memcontrol.h | 48 ++++++++++++++++++++++++++++++++++++++++++++++
 mm/hugetlb.c               |  5 +++++
 mm/memcontrol.c            |  3 +++
 3 files changed, 56 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=482">Tejun Heo</a> - Nov. 14, 2017, 9:06 p.m.</div>
<pre class="content">
On Tue, Nov 14, 2017 at 05:24:29PM +0000, Roman Gushchin wrote:
<span class="quote">&gt; This patch implements basic accounting of memory consumption</span>
<span class="quote">&gt; by hugetlbfs pages for cgroup v2 memory controller.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cgroup v2 memory controller lacks any visibility into the</span>
<span class="quote">&gt; hugetlbfs memory consumption. Cgroup v1 implemented a separate</span>
<span class="quote">&gt; hugetlbfs controller, which provided such stats, and also</span>
<span class="quote">&gt; provided some control abilities. Although porting of the</span>
<span class="quote">&gt; hugetlbfs controller to cgroup v2 is arguable a good idea and</span>
<span class="quote">&gt; is outside of scope of this patch, it&#39;s very useful to have</span>
<span class="quote">&gt; basic stats provided by memory.stat.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As hugetlbfs memory can easily represent a big portion of total</span>
<span class="quote">&gt; memory, it&#39;s important to understand who (which memcg/container)</span>
<span class="quote">&gt; is using it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The number is represented in memory.stat as &quot;hugetlb&quot; in bytes and</span>
<span class="quote">&gt; is printed unconditionally. Accounting code doesn&#39;t depend on</span>
<span class="quote">&gt; cgroup v1 hugetlb controller.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Example:</span>
<span class="quote">&gt;   $ cat /sys/fs/cgroup/user.slice/user-0.slice/session-1.scope/memory.stat</span>
<span class="quote">&gt;   anon 1634304</span>
<span class="quote">&gt;   file 1163264</span>
<span class="quote">&gt;   kernel_stack 16384</span>
<span class="quote">&gt;   slab 737280</span>
<span class="quote">&gt;   sock 0</span>
<span class="quote">&gt;   shmem 0</span>
<span class="quote">&gt;   file_mapped 32768</span>
<span class="quote">&gt;   file_dirty 4096</span>
<span class="quote">&gt;   file_writeback 0</span>
<span class="quote">&gt;   inactive_anon 0</span>
<span class="quote">&gt;   active_anon 1634304</span>
<span class="quote">&gt;   inactive_file 65536</span>
<span class="quote">&gt;   active_file 1097728</span>
<span class="quote">&gt;   unevictable 0</span>
<span class="quote">&gt;   slab_reclaimable 282624</span>
<span class="quote">&gt;   slab_unreclaimable 454656</span>
<span class="quote">&gt;   hugetlb 1073741824</span>
<span class="quote">&gt;   pgfault 4580</span>
<span class="quote">&gt;   pgmajfault 13</span>
<span class="quote">&gt;   ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Roman Gushchin &lt;guro@fb.com&gt;</span>
<span class="quote">&gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Cc: Michal Hocko &lt;mhocko@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Vladimir Davydov &lt;vdavydov.dev@gmail.com&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Cc: Tejun Heo &lt;tj@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt; Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;</span>
<span class="quote">&gt; Cc: kernel-team@fb.com</span>
<span class="quote">&gt; Cc: cgroups@vger.kernel.org</span>
<span class="quote">&gt; Cc: linux-mm@kvack.org</span>
<span class="quote">&gt; Cc: linux-kernel@vger.kernel.org</span>
<span class="acked-by">
Acked-by: Tejun Heo &lt;tj@kernel.org&gt;</span>

Thanks.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 15, 2017, 8:35 a.m.</div>
<pre class="content">
On Tue 14-11-17 17:24:29, Roman Gushchin wrote:
<span class="quote">&gt; This patch implements basic accounting of memory consumption</span>
<span class="quote">&gt; by hugetlbfs pages for cgroup v2 memory controller.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cgroup v2 memory controller lacks any visibility into the</span>
<span class="quote">&gt; hugetlbfs memory consumption. Cgroup v1 implemented a separate</span>
<span class="quote">&gt; hugetlbfs controller, which provided such stats, and also</span>
<span class="quote">&gt; provided some control abilities. Although porting of the</span>
<span class="quote">&gt; hugetlbfs controller to cgroup v2 is arguable a good idea and</span>
<span class="quote">&gt; is outside of scope of this patch, it&#39;s very useful to have</span>
<span class="quote">&gt; basic stats provided by memory.stat.</span>

Separate hugetlb cgroup controller was really a deliberate decision.
We didn&#39;t want to mix hugetlb with the reclaimable memory. There is no
reasonable way to enforce memcg limits if hugetlb pages are involved.

AFAICS your patch shouldn&#39;t break the hugetlb controller because that
one (ab)uses page[2].private to store the hstate for the accounting.
You also do not really charge those hugetlb pages so the memcg
accounting will work unchaged.

So my primary question is, why don&#39;t you simply allow hugetlb controller
rather than tweak stats for memcg? Is there any fundamental reason why
hugetlb controller is not v2 compatible?

It feels really strange to keeps stats of something the controller
doesn&#39;t really control. I can imagine confused users claiming that
numbers just do not add up...
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174271">Roman Gushchin</a> - Nov. 15, 2017, 11:18 a.m.</div>
<pre class="content">
On Wed, Nov 15, 2017 at 09:35:04AM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Tue 14-11-17 17:24:29, Roman Gushchin wrote:</span>
<span class="quote">&gt; &gt; This patch implements basic accounting of memory consumption</span>
<span class="quote">&gt; &gt; by hugetlbfs pages for cgroup v2 memory controller.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Cgroup v2 memory controller lacks any visibility into the</span>
<span class="quote">&gt; &gt; hugetlbfs memory consumption. Cgroup v1 implemented a separate</span>
<span class="quote">&gt; &gt; hugetlbfs controller, which provided such stats, and also</span>
<span class="quote">&gt; &gt; provided some control abilities. Although porting of the</span>
<span class="quote">&gt; &gt; hugetlbfs controller to cgroup v2 is arguable a good idea and</span>
<span class="quote">&gt; &gt; is outside of scope of this patch, it&#39;s very useful to have</span>
<span class="quote">&gt; &gt; basic stats provided by memory.stat.</span>

Hi, Michal!
<span class="quote">
&gt; Separate hugetlb cgroup controller was really a deliberate decision.</span>
<span class="quote">&gt; We didn&#39;t want to mix hugetlb with the reclaimable memory. There is no</span>
<span class="quote">&gt; reasonable way to enforce memcg limits if hugetlb pages are involved.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; AFAICS your patch shouldn&#39;t break the hugetlb controller because that</span>
<span class="quote">&gt; one (ab)uses page[2].private to store the hstate for the accounting.</span>
<span class="quote">&gt; You also do not really charge those hugetlb pages so the memcg</span>
<span class="quote">&gt; accounting will work unchaged.</span>

Yes, you are right.
<span class="quote">
&gt; </span>
<span class="quote">&gt; So my primary question is, why don&#39;t you simply allow hugetlb controller</span>
<span class="quote">&gt; rather than tweak stats for memcg? Is there any fundamental reason why</span>
<span class="quote">&gt; hugetlb controller is not v2 compatible?</span>

I really don&#39;t know if the hugetlb controller has enough users to deserve
full support in v2 interface: adding knobs like memory.hugetlb.current,
memory.hugetlb.min, memory.hugetlb.high, memory.hugetlb.max, etc.

I&#39;d be rather conservative here and avoid adding a lot to the interface
without clear demand. Also, hugetlb pages are really special, and it&#39;s
at least not obvious how, say, memory.high should work for it.

At the same time we don&#39;t really have any accounting of hugetlb page
usage (except system-wide stats in sysfs). And providing such stats
is really useful.
In my particular case, I have some number of pre-allocated hugepages,
and I have several containerized workloads, which are potentially
using them to get performance bonuses. Having these stats allows to
attribute the memory holding by hugetlb pages to one of the workloads.
<span class="quote">
&gt; It feels really strange to keeps stats of something the controller</span>
<span class="quote">&gt; doesn&#39;t really control. I can imagine confused users claiming that</span>
<span class="quote">&gt; numbers just do not add up...</span>

This is why I do not add this number to memory.current. At the same
time numbers in memory.stat are not intended to be summed (we have
event counters there, dirty pages counter, etc), so I don&#39;t see a problem.

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 15, 2017, 11:42 a.m.</div>
<pre class="content">
On Wed 15-11-17 11:18:13, Roman Gushchin wrote:
<span class="quote">&gt; On Wed, Nov 15, 2017 at 09:35:04AM +0100, Michal Hocko wrote:</span>
[...]
<span class="quote">&gt; &gt; So my primary question is, why don&#39;t you simply allow hugetlb controller</span>
<span class="quote">&gt; &gt; rather than tweak stats for memcg? Is there any fundamental reason why</span>
<span class="quote">&gt; &gt; hugetlb controller is not v2 compatible?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I really don&#39;t know if the hugetlb controller has enough users to deserve</span>
<span class="quote">&gt; full support in v2 interface: adding knobs like memory.hugetlb.current,</span>
<span class="quote">&gt; memory.hugetlb.min, memory.hugetlb.high, memory.hugetlb.max, etc.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;d be rather conservative here and avoid adding a lot to the interface</span>
<span class="quote">&gt; without clear demand. Also, hugetlb pages are really special, and it&#39;s</span>
<span class="quote">&gt; at least not obvious how, say, memory.high should work for it.</span>

But you clearly want the hugetlb accoutning and that is what hugetlb
controller is for. You might not be interested in the limit enforcement
but that is not strictly required. So my question remains. Why don&#39;t we
reuse an existing infrastructure and add a new which might confuse users
in an extreme case?

Please note that I am not saying your patch is wrong, I just do not see
why we should handle hugetlb pages 2 different ways to achieve a common
infrastructure.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Nov. 15, 2017, 12:15 p.m.</div>
<pre class="content">
On Tue, Nov 14, 2017 at 05:24:29PM +0000, Roman Gushchin wrote:
<span class="quote">&gt; This patch implements basic accounting of memory consumption</span>
<span class="quote">&gt; by hugetlbfs pages for cgroup v2 memory controller.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cgroup v2 memory controller lacks any visibility into the</span>
<span class="quote">&gt; hugetlbfs memory consumption. Cgroup v1 implemented a separate</span>
<span class="quote">&gt; hugetlbfs controller, which provided such stats, and also</span>
<span class="quote">&gt; provided some control abilities. Although porting of the</span>
<span class="quote">&gt; hugetlbfs controller to cgroup v2 is arguable a good idea and</span>
<span class="quote">&gt; is outside of scope of this patch, it&#39;s very useful to have</span>
<span class="quote">&gt; basic stats provided by memory.stat.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As hugetlbfs memory can easily represent a big portion of total</span>
<span class="quote">&gt; memory, it&#39;s important to understand who (which memcg/container)</span>
<span class="quote">&gt; is using it.</span>

I&#39;m not really buying this argument.

Hugetlb setups tend to be static configurations that require intimate
coordination between booting the kernel with a hugetlb reservation and
precisely setting up the application(s).

In the few cases where you need introspection, you can check the the
HugetlbPages entry in /proc/&lt;pid&gt;/status. The minor convenience
provided by adding an aggregate cgroup counter IMO doesn&#39;t outweigh
the weirdness of listing a type of resource in memory.stat that isn&#39;t
otherwise acknowledged or controllable as memory.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=174271">Roman Gushchin</a> - Nov. 15, 2017, 12:23 p.m.</div>
<pre class="content">
On Wed, Nov 15, 2017 at 12:42:23PM +0100, Michal Hocko wrote:
<span class="quote">&gt; On Wed 15-11-17 11:18:13, Roman Gushchin wrote:</span>
<span class="quote">&gt; &gt; On Wed, Nov 15, 2017 at 09:35:04AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; &gt; &gt; So my primary question is, why don&#39;t you simply allow hugetlb controller</span>
<span class="quote">&gt; &gt; &gt; rather than tweak stats for memcg? Is there any fundamental reason why</span>
<span class="quote">&gt; &gt; &gt; hugetlb controller is not v2 compatible?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I really don&#39;t know if the hugetlb controller has enough users to deserve</span>
<span class="quote">&gt; &gt; full support in v2 interface: adding knobs like memory.hugetlb.current,</span>
<span class="quote">&gt; &gt; memory.hugetlb.min, memory.hugetlb.high, memory.hugetlb.max, etc.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I&#39;d be rather conservative here and avoid adding a lot to the interface</span>
<span class="quote">&gt; &gt; without clear demand. Also, hugetlb pages are really special, and it&#39;s</span>
<span class="quote">&gt; &gt; at least not obvious how, say, memory.high should work for it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But you clearly want the hugetlb accoutning and that is what hugetlb</span>
<span class="quote">&gt; controller is for. You might not be interested in the limit enforcement</span>
<span class="quote">&gt; but that is not strictly required. So my question remains. Why don&#39;t we</span>
<span class="quote">&gt; reuse an existing infrastructure and add a new which might confuse users</span>
<span class="quote">&gt; in an extreme case?</span>

Hm, but to use a small part of hugetlb controller infrastructure I would
have to add a whole set of cgroup v2 controls.
And control knobs (like memory.hugetlb.current) are much more obligatory
than an entry in memory.stat, where we have some internal stats as well.

So, I don&#39;t really know why confusion should come from in this case?
It would be confusing, if we&#39;d add hugetlb stats to the memory.current,
so that it could be larger then memory.max.
But as separate entry in memory.stat it should not confuse anyone,
at least not more than the existing state of things, when hugetlb pages
are a black hole.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Please note that I am not saying your patch is wrong, I just do not see</span>
<span class="quote">&gt; why we should handle hugetlb pages 2 different ways to achieve a common</span>
<span class="quote">&gt; infrastructure.</span>

This is perfectly fine, and I do understand it.
My point is that it&#39;s a cheap way to solve a real problem, which is also
not binding us too much.

Thanks!
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Nov. 15, 2017, 12:34 p.m.</div>
<pre class="content">
On Wed 15-11-17 12:23:14, Roman Gushchin wrote:
<span class="quote">&gt; On Wed, Nov 15, 2017 at 12:42:23PM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Wed 15-11-17 11:18:13, Roman Gushchin wrote:</span>
<span class="quote">&gt; &gt; &gt; On Wed, Nov 15, 2017 at 09:35:04AM +0100, Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; [...]</span>
<span class="quote">&gt; &gt; &gt; &gt; So my primary question is, why don&#39;t you simply allow hugetlb controller</span>
<span class="quote">&gt; &gt; &gt; &gt; rather than tweak stats for memcg? Is there any fundamental reason why</span>
<span class="quote">&gt; &gt; &gt; &gt; hugetlb controller is not v2 compatible?</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I really don&#39;t know if the hugetlb controller has enough users to deserve</span>
<span class="quote">&gt; &gt; &gt; full support in v2 interface: adding knobs like memory.hugetlb.current,</span>
<span class="quote">&gt; &gt; &gt; memory.hugetlb.min, memory.hugetlb.high, memory.hugetlb.max, etc.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; I&#39;d be rather conservative here and avoid adding a lot to the interface</span>
<span class="quote">&gt; &gt; &gt; without clear demand. Also, hugetlb pages are really special, and it&#39;s</span>
<span class="quote">&gt; &gt; &gt; at least not obvious how, say, memory.high should work for it.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; But you clearly want the hugetlb accoutning and that is what hugetlb</span>
<span class="quote">&gt; &gt; controller is for. You might not be interested in the limit enforcement</span>
<span class="quote">&gt; &gt; but that is not strictly required. So my question remains. Why don&#39;t we</span>
<span class="quote">&gt; &gt; reuse an existing infrastructure and add a new which might confuse users</span>
<span class="quote">&gt; &gt; in an extreme case?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hm, but to use a small part of hugetlb controller infrastructure I would</span>
<span class="quote">&gt; have to add a whole set of cgroup v2 controls.</span>

And? I mean how does that differ from somebody using memcg only for stat
purposes?
<span class="quote">
&gt; And control knobs (like memory.hugetlb.current) are much more obligatory</span>
<span class="quote">&gt; than an entry in memory.stat, where we have some internal stats as well.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So, I don&#39;t really know why confusion should come from in this case?</span>

Because you stat data that is not controlled by the controller. Just
imagine if somebody wanted to sum counters to get the resulting
accounted and enforced memory.

You are simply trying to push a square through circle without a good
reason. Unless there is a fundamental reason to not enable hugetlb
controller to v2 I would rather go that way rather than to have another
hugetlb weirdness. Enabling the controller should be a matter of
exporting knobs. Trivial thing AFAICS.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="p_header">index 69966c461d1c..e0dfb64d2918 100644</span>
<span class="p_header">--- a/include/linux/memcontrol.h</span>
<span class="p_header">+++ b/include/linux/memcontrol.h</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"> enum memcg_stat_item {</span>
 	MEMCG_SOCK,
 	/* XXX: why are these zone and not node counters? */
 	MEMCG_KERNEL_STACK_KB,
<span class="p_add">+	MEMCG_HUGETLB,</span>
 	MEMCG_NR_STAT,
 };
 
<span class="p_chunk">@@ -664,6 +665,39 @@</span> <span class="p_context"> static inline void count_memcg_event_mm(struct mm_struct *mm,</span>
 void mem_cgroup_split_huge_fixup(struct page *head);
 #endif
 
<span class="p_add">+#ifdef CONFIG_HUGETLBFS</span>
<span class="p_add">+static inline void mem_cgroup_add_hugetlb_page(struct page *page,</span>
<span class="p_add">+					       unsigned int count)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (mem_cgroup_disabled())</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	rcu_read_lock();</span>
<span class="p_add">+	page-&gt;mem_cgroup = mem_cgroup_from_task(current);</span>
<span class="p_add">+	css_get(&amp;page-&gt;mem_cgroup-&gt;css);</span>
<span class="p_add">+	rcu_read_unlock();</span>
<span class="p_add">+</span>
<span class="p_add">+	mod_memcg_page_state(page, MEMCG_HUGETLB, count);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void mem_cgroup_remove_hugetlb_page(struct page *page,</span>
<span class="p_add">+						  unsigned int count)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (mem_cgroup_disabled() || !page-&gt;mem_cgroup)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	mod_memcg_page_state(page, MEMCG_HUGETLB, -count);</span>
<span class="p_add">+</span>
<span class="p_add">+	css_put(&amp;page-&gt;mem_cgroup-&gt;css);</span>
<span class="p_add">+	page-&gt;mem_cgroup = NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void mem_cgroup_reset_hugetlb_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	page-&gt;mem_cgroup = NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #else /* CONFIG_MEMCG */
 
 #define MEM_CGROUP_ID_SHIFT	0
<span class="p_chunk">@@ -936,6 +970,20 @@</span> <span class="p_context"> static inline</span>
 void count_memcg_event_mm(struct mm_struct *mm, enum vm_event_item idx)
 {
 }
<span class="p_add">+</span>
<span class="p_add">+static inline void mem_cgroup_add_hugetlb_page(struct page *page,</span>
<span class="p_add">+					       unsigned int count)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void mem_cgroup_remove_hugetlb_page(struct page *page,</span>
<span class="p_add">+						  unsigned int count)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void mem_cgroup_reset_hugetlb_page(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
 #endif /* CONFIG_MEMCG */
 
 /* idx can be of type enum memcg_stat_item or node_stat_item */
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index 4b3bbd2980bb..d1a2a9fa549a 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -1279,6 +1279,8 @@</span> <span class="p_context"> void free_huge_page(struct page *page)</span>
 	clear_page_huge_active(page);
 	hugetlb_cgroup_uncharge_page(hstate_index(h),
 				     pages_per_huge_page(h), page);
<span class="p_add">+	mem_cgroup_remove_hugetlb_page(page, pages_per_huge_page(h));</span>
<span class="p_add">+</span>
 	if (restore_reserve)
 		h-&gt;resv_huge_pages++;
 
<span class="p_chunk">@@ -1301,6 +1303,7 @@</span> <span class="p_context"> static void prep_new_huge_page(struct hstate *h, struct page *page, int nid)</span>
 	set_compound_page_dtor(page, HUGETLB_PAGE_DTOR);
 	spin_lock(&amp;hugetlb_lock);
 	set_hugetlb_cgroup(page, NULL);
<span class="p_add">+	mem_cgroup_reset_hugetlb_page(page);</span>
 	h-&gt;nr_huge_pages++;
 	h-&gt;nr_huge_pages_node[nid]++;
 	spin_unlock(&amp;hugetlb_lock);
<span class="p_chunk">@@ -1584,6 +1587,7 @@</span> <span class="p_context"> static struct page *__alloc_buddy_huge_page(struct hstate *h, gfp_t gfp_mask,</span>
 		r_nid = page_to_nid(page);
 		set_compound_page_dtor(page, HUGETLB_PAGE_DTOR);
 		set_hugetlb_cgroup(page, NULL);
<span class="p_add">+		mem_cgroup_reset_hugetlb_page(page);</span>
 		/*
 		 * We incremented the global counters already
 		 */
<span class="p_chunk">@@ -2041,6 +2045,7 @@</span> <span class="p_context"> struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 		/* Fall through */
 	}
 	hugetlb_cgroup_commit_charge(idx, pages_per_huge_page(h), h_cg, page);
<span class="p_add">+	mem_cgroup_add_hugetlb_page(page, pages_per_huge_page(h));</span>
 	spin_unlock(&amp;hugetlb_lock);
 
 	set_page_private(page, (unsigned long)spool);
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 50e6906314f8..f2323a9405a4 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -5338,6 +5338,9 @@</span> <span class="p_context"> static int memory_stat_show(struct seq_file *m, void *v)</span>
 	seq_printf(m, &quot;slab_unreclaimable %llu\n&quot;,
 		   (u64)stat[NR_SLAB_UNRECLAIMABLE] * PAGE_SIZE);
 
<span class="p_add">+	seq_printf(m, &quot;hugetlb %llu\n&quot;,</span>
<span class="p_add">+		   (u64)stat[MEMCG_HUGETLB] * PAGE_SIZE);</span>
<span class="p_add">+</span>
 	/* Accumulated memory events */
 
 	seq_printf(m, &quot;pgfault %lu\n&quot;, events[PGFAULT]);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



