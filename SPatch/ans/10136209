
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[07/17] mm: pass the vmem_altmap to vmemmap_free - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [07/17] mm: pass the vmem_altmap to vmemmap_free</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Dec. 29, 2017, 7:53 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20171229075406.1936-8-hch@lst.de&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10136209/mbox/"
   >mbox</a>
|
   <a href="/patch/10136209/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10136209/">/patch/10136209/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	485B36037D for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 29 Dec 2017 07:55:02 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 37FBE2CC89
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 29 Dec 2017 07:55:02 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 2A7B92DEEC; Fri, 29 Dec 2017 07:55:02 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.8 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,T_DKIM_INVALID autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EF0682CC89
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 29 Dec 2017 07:55:00 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1755654AbdL2Hy5 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 29 Dec 2017 02:54:57 -0500
Received: from bombadil.infradead.org ([65.50.211.133]:35458 &quot;EHLO
	bombadil.infradead.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1755635AbdL2Hyx (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 29 Dec 2017 02:54:53 -0500
DKIM-Signature: v=1; a=rsa-sha256; q=dns/txt; c=relaxed/relaxed;
	d=infradead.org; s=bombadil.20170209;
	h=References:In-Reply-To:Message-Id:
	Date:Subject:Cc:To:From:Sender:Reply-To:MIME-Version:Content-Type:
	Content-Transfer-Encoding:Content-ID:Content-Description:Resent-Date:
	Resent-From:Resent-Sender:Resent-To:Resent-Cc:Resent-Message-ID:List-Id:
	List-Help:List-Unsubscribe:List-Subscribe:List-Post:List-Owner:List-Archive;
	bh=SynVN8vYrp28tHI4obRy3kt2Gp2DOIX4Y2Y/A4WP8hQ=;
	b=NWKBf0KMvWEqkE9mPr+WHnA6J
	2w2lDdyDsPGvA54omgsNw7/E1SMDdQSuRfxNncgRJ/IBKgMw5xMKEg5z7d5vJCN25EGr5w5/CXWQn
	MdQ50UvquWddy0b6hfMngoGWr/oMf3c8d9vEg7jWzvy7wQqZpio3BWZOE+Dz5vbi4lb8qXtV8Kif9
	7VArtZa3jgVpYo/L3Cr4qtELB/QV4LSMpcbV6HKkMbHi69IPE2gs6gE2XZxpPxYUNT8l8qIAVbqne
	uv5ZKFMNhANmInzxh1xiqeWPQaypKOxz4LEIIsg0KaC9j1FSLVyLmZ/D7PmDYgyzsLnaRxmgaNbfU
	wRg7vbAmQ==;
Received: from 77.117.237.29.wireless.dyn.drei.com ([77.117.237.29]
	helo=localhost)
	by bombadil.infradead.org with esmtpsa (Exim 4.89 #1 (Red Hat Linux))
	id 1eUpV3-0001aJ-Ps; Fri, 29 Dec 2017 07:54:42 +0000
From: Christoph Hellwig &lt;hch@lst.de&gt;
To: Dan Williams &lt;dan.j.williams@intel.com&gt;
Cc: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Logan Gunthorpe &lt;logang@deltatee.com&gt;,
	Michal Hocko &lt;mhocko@kernel.org&gt;, linux-nvdimm@lists.01.org,
	linuxppc-dev@lists.ozlabs.org, x86@kernel.org, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org
Subject: [PATCH 07/17] mm: pass the vmem_altmap to vmemmap_free
Date: Fri, 29 Dec 2017 08:53:56 +0100
Message-Id: &lt;20171229075406.1936-8-hch@lst.de&gt;
X-Mailer: git-send-email 2.14.2
In-Reply-To: &lt;20171229075406.1936-1-hch@lst.de&gt;
References: &lt;20171229075406.1936-1-hch@lst.de&gt;
X-SRS-Rewrite: SMTP reverse-path rewritten from &lt;hch@infradead.org&gt; by
	bombadil.infradead.org. See http://www.infradead.org/rpr.html
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - Dec. 29, 2017, 7:53 a.m.</div>
<pre class="content">
We can just pass this on instead of having to do a radix tree lookup
without proper locking a few levels into the callchain.
<span class="signed-off-by">
Signed-off-by: Christoph Hellwig &lt;hch@lst.de&gt;</span>
<span class="reviewed-by">Reviewed-by: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
---
 arch/arm64/mm/mmu.c            |  3 +-
 arch/ia64/mm/discontig.c       |  3 +-
 arch/powerpc/mm/init_64.c      |  5 ++--
 arch/s390/mm/vmem.c            |  3 +-
 arch/sparc/mm/init_64.c        |  3 +-
 arch/x86/mm/init_64.c          | 67 ++++++++++++++++++++++++------------------
 include/linux/memory_hotplug.h |  2 +-
 include/linux/mm.h             |  3 +-
 mm/memory_hotplug.c            |  7 +++--
 mm/sparse.c                    | 23 ++++++++-------
 10 files changed, 68 insertions(+), 51 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/mm/mmu.c b/arch/arm64/mm/mmu.c</span>
<span class="p_header">index ec8952ff13be..0b1f13e0b4b3 100644</span>
<span class="p_header">--- a/arch/arm64/mm/mmu.c</span>
<span class="p_header">+++ b/arch/arm64/mm/mmu.c</span>
<span class="p_chunk">@@ -696,7 +696,8 @@</span> <span class="p_context"> int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node,</span>
 	return 0;
 }
 #endif	/* CONFIG_ARM64_64K_PAGES */
<span class="p_del">-void vmemmap_free(unsigned long start, unsigned long end)</span>
<span class="p_add">+void vmemmap_free(unsigned long start, unsigned long end,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 }
 #endif	/* CONFIG_SPARSEMEM_VMEMMAP */
<span class="p_header">diff --git a/arch/ia64/mm/discontig.c b/arch/ia64/mm/discontig.c</span>
<span class="p_header">index 1555aecaaf85..5ea0d8d0968b 100644</span>
<span class="p_header">--- a/arch/ia64/mm/discontig.c</span>
<span class="p_header">+++ b/arch/ia64/mm/discontig.c</span>
<span class="p_chunk">@@ -760,7 +760,8 @@</span> <span class="p_context"> int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node,</span>
 	return vmemmap_populate_basepages(start, end, node);
 }
 
<span class="p_del">-void vmemmap_free(unsigned long start, unsigned long end)</span>
<span class="p_add">+void vmemmap_free(unsigned long start, unsigned long end,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 }
 #endif
<span class="p_header">diff --git a/arch/powerpc/mm/init_64.c b/arch/powerpc/mm/init_64.c</span>
<span class="p_header">index 779b74a96b8f..db7d4e092157 100644</span>
<span class="p_header">--- a/arch/powerpc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/powerpc/mm/init_64.c</span>
<span class="p_chunk">@@ -254,7 +254,8 @@</span> <span class="p_context"> static unsigned long vmemmap_list_free(unsigned long start)</span>
 	return vmem_back-&gt;phys;
 }
 
<span class="p_del">-void __ref vmemmap_free(unsigned long start, unsigned long end)</span>
<span class="p_add">+void __ref vmemmap_free(unsigned long start, unsigned long end,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 	unsigned long page_size = 1 &lt;&lt; mmu_psize_defs[mmu_vmemmap_psize].shift;
 	unsigned long page_order = get_order(page_size);
<span class="p_chunk">@@ -265,7 +266,6 @@</span> <span class="p_context"> void __ref vmemmap_free(unsigned long start, unsigned long end)</span>
 
 	for (; start &lt; end; start += page_size) {
 		unsigned long nr_pages, addr;
<span class="p_del">-		struct vmem_altmap *altmap;</span>
 		struct page *section_base;
 		struct page *page;
 
<span class="p_chunk">@@ -285,7 +285,6 @@</span> <span class="p_context"> void __ref vmemmap_free(unsigned long start, unsigned long end)</span>
 		section_base = pfn_to_page(vmemmap_section_start(start));
 		nr_pages = 1 &lt;&lt; page_order;
 
<span class="p_del">-		altmap = to_vmem_altmap((unsigned long) section_base);</span>
 		if (altmap) {
 			vmem_altmap_free(altmap, nr_pages);
 		} else if (PageReserved(page)) {
<span class="p_header">diff --git a/arch/s390/mm/vmem.c b/arch/s390/mm/vmem.c</span>
<span class="p_header">index c44ef0e7c466..db55561c5981 100644</span>
<span class="p_header">--- a/arch/s390/mm/vmem.c</span>
<span class="p_header">+++ b/arch/s390/mm/vmem.c</span>
<span class="p_chunk">@@ -297,7 +297,8 @@</span> <span class="p_context"> int __meminit vmemmap_populate(unsigned long start, unsigned long end, int node,</span>
 	return ret;
 }
 
<span class="p_del">-void vmemmap_free(unsigned long start, unsigned long end)</span>
<span class="p_add">+void vmemmap_free(unsigned long start, unsigned long end,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 }
 
<span class="p_header">diff --git a/arch/sparc/mm/init_64.c b/arch/sparc/mm/init_64.c</span>
<span class="p_header">index 42d27a1a042a..995f9490334d 100644</span>
<span class="p_header">--- a/arch/sparc/mm/init_64.c</span>
<span class="p_header">+++ b/arch/sparc/mm/init_64.c</span>
<span class="p_chunk">@@ -2671,7 +2671,8 @@</span> <span class="p_context"> int __meminit vmemmap_populate(unsigned long vstart, unsigned long vend,</span>
 	return 0;
 }
 
<span class="p_del">-void vmemmap_free(unsigned long start, unsigned long end)</span>
<span class="p_add">+void vmemmap_free(unsigned long start, unsigned long end,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 }
 #endif /* CONFIG_SPARSEMEM_VMEMMAP */
<span class="p_header">diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c</span>
<span class="p_header">index 3c046618cc7e..0cab4b5b59ba 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_64.c</span>
<span class="p_chunk">@@ -800,11 +800,11 @@</span> <span class="p_context"> int arch_add_memory(int nid, u64 start, u64 size, struct vmem_altmap *altmap,</span>
 
 #define PAGE_INUSE 0xFD
 
<span class="p_del">-static void __meminit free_pagetable(struct page *page, int order)</span>
<span class="p_add">+static void __meminit free_pagetable(struct page *page, int order,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 	unsigned long magic;
 	unsigned int nr_pages = 1 &lt;&lt; order;
<span class="p_del">-	struct vmem_altmap *altmap = to_vmem_altmap((unsigned long) page);</span>
 
 	if (altmap) {
 		vmem_altmap_free(altmap, nr_pages);
<span class="p_chunk">@@ -826,7 +826,8 @@</span> <span class="p_context"> static void __meminit free_pagetable(struct page *page, int order)</span>
 		free_pages((unsigned long)page_address(page), order);
 }
 
<span class="p_del">-static void __meminit free_pte_table(pte_t *pte_start, pmd_t *pmd)</span>
<span class="p_add">+static void __meminit free_pte_table(pte_t *pte_start, pmd_t *pmd,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 	pte_t *pte;
 	int i;
<span class="p_chunk">@@ -838,13 +839,14 @@</span> <span class="p_context"> static void __meminit free_pte_table(pte_t *pte_start, pmd_t *pmd)</span>
 	}
 
 	/* free a pte talbe */
<span class="p_del">-	free_pagetable(pmd_page(*pmd), 0);</span>
<span class="p_add">+	free_pagetable(pmd_page(*pmd), 0, altmap);</span>
 	spin_lock(&amp;init_mm.page_table_lock);
 	pmd_clear(pmd);
 	spin_unlock(&amp;init_mm.page_table_lock);
 }
 
<span class="p_del">-static void __meminit free_pmd_table(pmd_t *pmd_start, pud_t *pud)</span>
<span class="p_add">+static void __meminit free_pmd_table(pmd_t *pmd_start, pud_t *pud,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 	pmd_t *pmd;
 	int i;
<span class="p_chunk">@@ -856,13 +858,14 @@</span> <span class="p_context"> static void __meminit free_pmd_table(pmd_t *pmd_start, pud_t *pud)</span>
 	}
 
 	/* free a pmd talbe */
<span class="p_del">-	free_pagetable(pud_page(*pud), 0);</span>
<span class="p_add">+	free_pagetable(pud_page(*pud), 0, altmap);</span>
 	spin_lock(&amp;init_mm.page_table_lock);
 	pud_clear(pud);
 	spin_unlock(&amp;init_mm.page_table_lock);
 }
 
<span class="p_del">-static void __meminit free_pud_table(pud_t *pud_start, p4d_t *p4d)</span>
<span class="p_add">+static void __meminit free_pud_table(pud_t *pud_start, p4d_t *p4d,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 	pud_t *pud;
 	int i;
<span class="p_chunk">@@ -874,7 +877,7 @@</span> <span class="p_context"> static void __meminit free_pud_table(pud_t *pud_start, p4d_t *p4d)</span>
 	}
 
 	/* free a pud talbe */
<span class="p_del">-	free_pagetable(p4d_page(*p4d), 0);</span>
<span class="p_add">+	free_pagetable(p4d_page(*p4d), 0, altmap);</span>
 	spin_lock(&amp;init_mm.page_table_lock);
 	p4d_clear(p4d);
 	spin_unlock(&amp;init_mm.page_table_lock);
<span class="p_chunk">@@ -882,7 +885,7 @@</span> <span class="p_context"> static void __meminit free_pud_table(pud_t *pud_start, p4d_t *p4d)</span>
 
 static void __meminit
 remove_pte_table(pte_t *pte_start, unsigned long addr, unsigned long end,
<span class="p_del">-		 bool direct)</span>
<span class="p_add">+		 struct vmem_altmap *altmap, bool direct)</span>
 {
 	unsigned long next, pages = 0;
 	pte_t *pte;
<span class="p_chunk">@@ -913,7 +916,7 @@</span> <span class="p_context"> remove_pte_table(pte_t *pte_start, unsigned long addr, unsigned long end,</span>
 			 * freed when offlining, or simplely not in use.
 			 */
 			if (!direct)
<span class="p_del">-				free_pagetable(pte_page(*pte), 0);</span>
<span class="p_add">+				free_pagetable(pte_page(*pte), 0, altmap);</span>
 
 			spin_lock(&amp;init_mm.page_table_lock);
 			pte_clear(&amp;init_mm, addr, pte);
<span class="p_chunk">@@ -936,7 +939,7 @@</span> <span class="p_context"> remove_pte_table(pte_t *pte_start, unsigned long addr, unsigned long end,</span>
 
 			page_addr = page_address(pte_page(*pte));
 			if (!memchr_inv(page_addr, PAGE_INUSE, PAGE_SIZE)) {
<span class="p_del">-				free_pagetable(pte_page(*pte), 0);</span>
<span class="p_add">+				free_pagetable(pte_page(*pte), 0, altmap);</span>
 
 				spin_lock(&amp;init_mm.page_table_lock);
 				pte_clear(&amp;init_mm, addr, pte);
<span class="p_chunk">@@ -953,7 +956,7 @@</span> <span class="p_context"> remove_pte_table(pte_t *pte_start, unsigned long addr, unsigned long end,</span>
 
 static void __meminit
 remove_pmd_table(pmd_t *pmd_start, unsigned long addr, unsigned long end,
<span class="p_del">-		 bool direct)</span>
<span class="p_add">+		 bool direct, struct vmem_altmap *altmap)</span>
 {
 	unsigned long next, pages = 0;
 	pte_t *pte_base;
<span class="p_chunk">@@ -972,7 +975,8 @@</span> <span class="p_context"> remove_pmd_table(pmd_t *pmd_start, unsigned long addr, unsigned long end,</span>
 			    IS_ALIGNED(next, PMD_SIZE)) {
 				if (!direct)
 					free_pagetable(pmd_page(*pmd),
<span class="p_del">-						       get_order(PMD_SIZE));</span>
<span class="p_add">+						       get_order(PMD_SIZE),</span>
<span class="p_add">+						       altmap);</span>
 
 				spin_lock(&amp;init_mm.page_table_lock);
 				pmd_clear(pmd);
<span class="p_chunk">@@ -986,7 +990,8 @@</span> <span class="p_context"> remove_pmd_table(pmd_t *pmd_start, unsigned long addr, unsigned long end,</span>
 				if (!memchr_inv(page_addr, PAGE_INUSE,
 						PMD_SIZE)) {
 					free_pagetable(pmd_page(*pmd),
<span class="p_del">-						       get_order(PMD_SIZE));</span>
<span class="p_add">+						       get_order(PMD_SIZE),</span>
<span class="p_add">+						       altmap);</span>
 
 					spin_lock(&amp;init_mm.page_table_lock);
 					pmd_clear(pmd);
<span class="p_chunk">@@ -998,8 +1003,8 @@</span> <span class="p_context"> remove_pmd_table(pmd_t *pmd_start, unsigned long addr, unsigned long end,</span>
 		}
 
 		pte_base = (pte_t *)pmd_page_vaddr(*pmd);
<span class="p_del">-		remove_pte_table(pte_base, addr, next, direct);</span>
<span class="p_del">-		free_pte_table(pte_base, pmd);</span>
<span class="p_add">+		remove_pte_table(pte_base, addr, next, altmap, direct);</span>
<span class="p_add">+		free_pte_table(pte_base, pmd, altmap);</span>
 	}
 
 	/* Call free_pmd_table() in remove_pud_table(). */
<span class="p_chunk">@@ -1009,7 +1014,7 @@</span> <span class="p_context"> remove_pmd_table(pmd_t *pmd_start, unsigned long addr, unsigned long end,</span>
 
 static void __meminit
 remove_pud_table(pud_t *pud_start, unsigned long addr, unsigned long end,
<span class="p_del">-		 bool direct)</span>
<span class="p_add">+		 struct vmem_altmap *altmap, bool direct)</span>
 {
 	unsigned long next, pages = 0;
 	pmd_t *pmd_base;
<span class="p_chunk">@@ -1028,7 +1033,8 @@</span> <span class="p_context"> remove_pud_table(pud_t *pud_start, unsigned long addr, unsigned long end,</span>
 			    IS_ALIGNED(next, PUD_SIZE)) {
 				if (!direct)
 					free_pagetable(pud_page(*pud),
<span class="p_del">-						       get_order(PUD_SIZE));</span>
<span class="p_add">+						       get_order(PUD_SIZE),</span>
<span class="p_add">+						       altmap);</span>
 
 				spin_lock(&amp;init_mm.page_table_lock);
 				pud_clear(pud);
<span class="p_chunk">@@ -1042,7 +1048,8 @@</span> <span class="p_context"> remove_pud_table(pud_t *pud_start, unsigned long addr, unsigned long end,</span>
 				if (!memchr_inv(page_addr, PAGE_INUSE,
 						PUD_SIZE)) {
 					free_pagetable(pud_page(*pud),
<span class="p_del">-						       get_order(PUD_SIZE));</span>
<span class="p_add">+						       get_order(PUD_SIZE),</span>
<span class="p_add">+						       altmap);</span>
 
 					spin_lock(&amp;init_mm.page_table_lock);
 					pud_clear(pud);
<span class="p_chunk">@@ -1054,8 +1061,8 @@</span> <span class="p_context"> remove_pud_table(pud_t *pud_start, unsigned long addr, unsigned long end,</span>
 		}
 
 		pmd_base = pmd_offset(pud, 0);
<span class="p_del">-		remove_pmd_table(pmd_base, addr, next, direct);</span>
<span class="p_del">-		free_pmd_table(pmd_base, pud);</span>
<span class="p_add">+		remove_pmd_table(pmd_base, addr, next, direct, altmap);</span>
<span class="p_add">+		free_pmd_table(pmd_base, pud, altmap);</span>
 	}
 
 	if (direct)
<span class="p_chunk">@@ -1064,7 +1071,7 @@</span> <span class="p_context"> remove_pud_table(pud_t *pud_start, unsigned long addr, unsigned long end,</span>
 
 static void __meminit
 remove_p4d_table(p4d_t *p4d_start, unsigned long addr, unsigned long end,
<span class="p_del">-		 bool direct)</span>
<span class="p_add">+		 struct vmem_altmap *altmap, bool direct)</span>
 {
 	unsigned long next, pages = 0;
 	pud_t *pud_base;
<span class="p_chunk">@@ -1080,14 +1087,14 @@</span> <span class="p_context"> remove_p4d_table(p4d_t *p4d_start, unsigned long addr, unsigned long end,</span>
 		BUILD_BUG_ON(p4d_large(*p4d));
 
 		pud_base = pud_offset(p4d, 0);
<span class="p_del">-		remove_pud_table(pud_base, addr, next, direct);</span>
<span class="p_add">+		remove_pud_table(pud_base, addr, next, altmap, direct);</span>
 		/*
 		 * For 4-level page tables we do not want to free PUDs, but in the
 		 * 5-level case we should free them. This code will have to change
 		 * to adapt for boot-time switching between 4 and 5 level page tables.
 		 */
 		if (CONFIG_PGTABLE_LEVELS == 5)
<span class="p_del">-			free_pud_table(pud_base, p4d);</span>
<span class="p_add">+			free_pud_table(pud_base, p4d, altmap);</span>
 	}
 
 	if (direct)
<span class="p_chunk">@@ -1096,7 +1103,8 @@</span> <span class="p_context"> remove_p4d_table(p4d_t *p4d_start, unsigned long addr, unsigned long end,</span>
 
 /* start and end are both virtual address. */
 static void __meminit
<span class="p_del">-remove_pagetable(unsigned long start, unsigned long end, bool direct)</span>
<span class="p_add">+remove_pagetable(unsigned long start, unsigned long end, bool direct,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 	unsigned long next;
 	unsigned long addr;
<span class="p_chunk">@@ -1111,15 +1119,16 @@</span> <span class="p_context"> remove_pagetable(unsigned long start, unsigned long end, bool direct)</span>
 			continue;
 
 		p4d = p4d_offset(pgd, 0);
<span class="p_del">-		remove_p4d_table(p4d, addr, next, direct);</span>
<span class="p_add">+		remove_p4d_table(p4d, addr, next, altmap, direct);</span>
 	}
 
 	flush_tlb_all();
 }
 
<span class="p_del">-void __ref vmemmap_free(unsigned long start, unsigned long end)</span>
<span class="p_add">+void __ref vmemmap_free(unsigned long start, unsigned long end,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
<span class="p_del">-	remove_pagetable(start, end, false);</span>
<span class="p_add">+	remove_pagetable(start, end, false, altmap);</span>
 }
 
 #ifdef CONFIG_MEMORY_HOTREMOVE
<span class="p_chunk">@@ -1129,7 +1138,7 @@</span> <span class="p_context"> kernel_physical_mapping_remove(unsigned long start, unsigned long end)</span>
 	start = (unsigned long)__va(start);
 	end = (unsigned long)__va(end);
 
<span class="p_del">-	remove_pagetable(start, end, true);</span>
<span class="p_add">+	remove_pagetable(start, end, true, NULL);</span>
 }
 
 int __ref arch_remove_memory(u64 start, u64 size, struct vmem_altmap *altmap)
<span class="p_header">diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h</span>
<span class="p_header">index e71927d0d46b..20dd98ad44a0 100644</span>
<span class="p_header">--- a/include/linux/memory_hotplug.h</span>
<span class="p_header">+++ b/include/linux/memory_hotplug.h</span>
<span class="p_chunk">@@ -331,7 +331,7 @@</span> <span class="p_context"> extern void remove_memory(int nid, u64 start, u64 size);</span>
 extern int sparse_add_one_section(struct pglist_data *pgdat,
 		unsigned long start_pfn, struct vmem_altmap *altmap);
 extern void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
<span class="p_del">-		unsigned long map_offset);</span>
<span class="p_add">+		unsigned long map_offset, struct vmem_altmap *altmap);</span>
 extern struct page *sparse_decode_mem_map(unsigned long coded_mem_map,
 					  unsigned long pnum);
 extern bool allow_online_pfn_range(int nid, unsigned long pfn, unsigned long nr_pages,
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 2f3a7ebecbe2..9d4cd4c1dc6d 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -2561,7 +2561,8 @@</span> <span class="p_context"> int vmemmap_populate(unsigned long start, unsigned long end, int node,</span>
 		struct vmem_altmap *altmap);
 void vmemmap_populate_print_last(void);
 #ifdef CONFIG_MEMORY_HOTPLUG
<span class="p_del">-void vmemmap_free(unsigned long start, unsigned long end);</span>
<span class="p_add">+void vmemmap_free(unsigned long start, unsigned long end,</span>
<span class="p_add">+		struct vmem_altmap *altmap);</span>
 #endif
 void register_page_bootmem_memmap(unsigned long section_nr, struct page *map,
 				  unsigned long nr_pages);
<span class="p_header">diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="p_header">index eae6bf47caf7..a8dde9734120 100644</span>
<span class="p_header">--- a/mm/memory_hotplug.c</span>
<span class="p_header">+++ b/mm/memory_hotplug.c</span>
<span class="p_chunk">@@ -536,7 +536,7 @@</span> <span class="p_context"> static void __remove_zone(struct zone *zone, unsigned long start_pfn)</span>
 }
 
 static int __remove_section(struct zone *zone, struct mem_section *ms,
<span class="p_del">-		unsigned long map_offset)</span>
<span class="p_add">+		unsigned long map_offset, struct vmem_altmap *altmap)</span>
 {
 	unsigned long start_pfn;
 	int scn_nr;
<span class="p_chunk">@@ -553,7 +553,7 @@</span> <span class="p_context"> static int __remove_section(struct zone *zone, struct mem_section *ms,</span>
 	start_pfn = section_nr_to_pfn((unsigned long)scn_nr);
 	__remove_zone(zone, start_pfn);
 
<span class="p_del">-	sparse_remove_one_section(zone, ms, map_offset);</span>
<span class="p_add">+	sparse_remove_one_section(zone, ms, map_offset, altmap);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -607,7 +607,8 @@</span> <span class="p_context"> int __remove_pages(struct zone *zone, unsigned long phys_start_pfn,</span>
 	for (i = 0; i &lt; sections_to_remove; i++) {
 		unsigned long pfn = phys_start_pfn + i*PAGES_PER_SECTION;
 
<span class="p_del">-		ret = __remove_section(zone, __pfn_to_section(pfn), map_offset);</span>
<span class="p_add">+		ret = __remove_section(zone, __pfn_to_section(pfn), map_offset,</span>
<span class="p_add">+				altmap);</span>
 		map_offset = 0;
 		if (ret)
 			break;
<span class="p_header">diff --git a/mm/sparse.c b/mm/sparse.c</span>
<span class="p_header">index 5f4a0dac7836..06130c13dc99 100644</span>
<span class="p_header">--- a/mm/sparse.c</span>
<span class="p_header">+++ b/mm/sparse.c</span>
<span class="p_chunk">@@ -685,12 +685,13 @@</span> <span class="p_context"> static inline struct page *kmalloc_section_memmap(unsigned long pnum, int nid,</span>
 	/* This will make the necessary allocations eventually. */
 	return sparse_mem_map_populate(pnum, nid, altmap);
 }
<span class="p_del">-static void __kfree_section_memmap(struct page *memmap)</span>
<span class="p_add">+static void __kfree_section_memmap(struct page *memmap,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 	unsigned long start = (unsigned long)memmap;
 	unsigned long end = (unsigned long)(memmap + PAGES_PER_SECTION);
 
<span class="p_del">-	vmemmap_free(start, end);</span>
<span class="p_add">+	vmemmap_free(start, end, altmap);</span>
 }
 #ifdef CONFIG_MEMORY_HOTREMOVE
 static void free_map_bootmem(struct page *memmap)
<span class="p_chunk">@@ -698,7 +699,7 @@</span> <span class="p_context"> static void free_map_bootmem(struct page *memmap)</span>
 	unsigned long start = (unsigned long)memmap;
 	unsigned long end = (unsigned long)(memmap + PAGES_PER_SECTION);
 
<span class="p_del">-	vmemmap_free(start, end);</span>
<span class="p_add">+	vmemmap_free(start, end, NULL);</span>
 }
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 #else
<span class="p_chunk">@@ -729,7 +730,8 @@</span> <span class="p_context"> static inline struct page *kmalloc_section_memmap(unsigned long pnum, int nid,</span>
 	return __kmalloc_section_memmap();
 }
 
<span class="p_del">-static void __kfree_section_memmap(struct page *memmap)</span>
<span class="p_add">+static void __kfree_section_memmap(struct page *memmap,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 	if (is_vmalloc_addr(memmap))
 		vfree(memmap);
<span class="p_chunk">@@ -798,7 +800,7 @@</span> <span class="p_context"> int __meminit sparse_add_one_section(struct pglist_data *pgdat,</span>
 		return -ENOMEM;
 	usemap = __kmalloc_section_usemap();
 	if (!usemap) {
<span class="p_del">-		__kfree_section_memmap(memmap);</span>
<span class="p_add">+		__kfree_section_memmap(memmap, altmap);</span>
 		return -ENOMEM;
 	}
 
<span class="p_chunk">@@ -820,7 +822,7 @@</span> <span class="p_context"> int __meminit sparse_add_one_section(struct pglist_data *pgdat,</span>
 	pgdat_resize_unlock(pgdat, &amp;flags);
 	if (ret &lt;= 0) {
 		kfree(usemap);
<span class="p_del">-		__kfree_section_memmap(memmap);</span>
<span class="p_add">+		__kfree_section_memmap(memmap, altmap);</span>
 	}
 	return ret;
 }
<span class="p_chunk">@@ -847,7 +849,8 @@</span> <span class="p_context"> static inline void clear_hwpoisoned_pages(struct page *memmap, int nr_pages)</span>
 }
 #endif
 
<span class="p_del">-static void free_section_usemap(struct page *memmap, unsigned long *usemap)</span>
<span class="p_add">+static void free_section_usemap(struct page *memmap, unsigned long *usemap,</span>
<span class="p_add">+		struct vmem_altmap *altmap)</span>
 {
 	struct page *usemap_page;
 
<span class="p_chunk">@@ -861,7 +864,7 @@</span> <span class="p_context"> static void free_section_usemap(struct page *memmap, unsigned long *usemap)</span>
 	if (PageSlab(usemap_page) || PageCompound(usemap_page)) {
 		kfree(usemap);
 		if (memmap)
<span class="p_del">-			__kfree_section_memmap(memmap);</span>
<span class="p_add">+			__kfree_section_memmap(memmap, altmap);</span>
 		return;
 	}
 
<span class="p_chunk">@@ -875,7 +878,7 @@</span> <span class="p_context"> static void free_section_usemap(struct page *memmap, unsigned long *usemap)</span>
 }
 
 void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,
<span class="p_del">-		unsigned long map_offset)</span>
<span class="p_add">+		unsigned long map_offset, struct vmem_altmap *altmap)</span>
 {
 	struct page *memmap = NULL;
 	unsigned long *usemap = NULL, flags;
<span class="p_chunk">@@ -893,7 +896,7 @@</span> <span class="p_context"> void sparse_remove_one_section(struct zone *zone, struct mem_section *ms,</span>
 
 	clear_hwpoisoned_pages(memmap + map_offset,
 			PAGES_PER_SECTION - map_offset);
<span class="p_del">-	free_section_usemap(memmap, usemap);</span>
<span class="p_add">+	free_section_usemap(memmap, usemap, altmap);</span>
 }
 #endif /* CONFIG_MEMORY_HOTREMOVE */
 #endif /* CONFIG_MEMORY_HOTPLUG */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



