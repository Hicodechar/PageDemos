
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>Linux 4.14.11 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    Linux 4.14.11</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 3, 2018, 2:17 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180103141749.GB14971@kroah.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10142459/mbox/"
   >mbox</a>
|
   <a href="/patch/10142459/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10142459/">/patch/10142459/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	05FE96034B for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 14:18:44 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id A9797290C3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 14:18:43 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 9CC71290CB; Wed,  3 Jan 2018 14:18:43 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id CB996290C3
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  3 Jan 2018 14:18:32 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753087AbeACOS0 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 3 Jan 2018 09:18:26 -0500
Received: from mail.linuxfoundation.org ([140.211.169.12]:39428 &quot;EHLO
	mail.linuxfoundation.org&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1752895AbeACORq (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 3 Jan 2018 09:17:46 -0500
Received: from localhost (LFbn-1-12258-90.w90-92.abo.wanadoo.fr
	[90.92.71.90])
	by mail.linuxfoundation.org (Postfix) with ESMTPSA id 0EE84BAC;
	Wed,  3 Jan 2018 14:17:44 +0000 (UTC)
Date: Wed, 3 Jan 2018 15:17:49 +0100
From: Greg KH &lt;gregkh@linuxfoundation.org&gt;
To: linux-kernel@vger.kernel.org, Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	torvalds@linux-foundation.org, stable@vger.kernel.org
Cc: lwn@lwn.net, Jiri Slaby &lt;jslaby@suse.cz&gt;
Subject: Re: Linux 4.14.11
Message-ID: &lt;20180103141749.GB14971@kroah.com&gt;
References: &lt;20180103141733.GA14971@kroah.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
In-Reply-To: &lt;20180103141733.GA14971@kroah.com&gt;
User-Agent: Mutt/1.9.2 (2017-12-15)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=37061">gregkh@linuxfoundation.org</a> - Jan. 3, 2018, 2:17 p.m.</div>
<pre class="content">

</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">index 05496622b4ef..520fdec15bbb 100644</span>
<span class="p_header">--- a/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2685,6 +2685,8 @@</span> <span class="p_context"></span>
 			steal time is computed, but won&#39;t influence scheduler
 			behaviour
 
<span class="p_add">+	nopti		[X86-64] Disable kernel page table isolation</span>
<span class="p_add">+</span>
 	nolapic		[X86-32,APIC] Do not enable or use the local APIC.
 
 	nolapic_timer	[X86-32,APIC] Do not use the local APIC timer.
<span class="p_chunk">@@ -3253,6 +3255,12 @@</span> <span class="p_context"></span>
 	pt.		[PARIDE]
 			See Documentation/blockdev/paride.txt.
 
<span class="p_add">+	pti=		[X86_64]</span>
<span class="p_add">+			Control user/kernel address space isolation:</span>
<span class="p_add">+			on - enable</span>
<span class="p_add">+			off - disable</span>
<span class="p_add">+			auto - default setting</span>
<span class="p_add">+</span>
 	pty.legacy_count=
 			[KNL] Number of legacy pty&#39;s. Overwrites compiled-in
 			default number.
<span class="p_header">diff --git a/Documentation/x86/x86_64/mm.txt b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">index 51101708a03a..ad41b3813f0a 100644</span>
<span class="p_header">--- a/Documentation/x86/x86_64/mm.txt</span>
<span class="p_header">+++ b/Documentation/x86/x86_64/mm.txt</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"> ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)</span>
 ... unused hole ...
 ffffec0000000000 - fffffbffffffffff (=44 bits) kasan shadow memory (16TB)
 ... unused hole ...
<span class="p_add">+fffffe0000000000 - fffffe7fffffffff (=39 bits) LDT remap for PTI</span>
 fffffe8000000000 - fffffeffffffffff (=39 bits) cpu_entry_area mapping
 ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks
 ... unused hole ...
<span class="p_chunk">@@ -29,8 +30,8 @@</span> <span class="p_context"> Virtual memory map with 5 level page tables:</span>
 hole caused by [56:63] sign extension
 ff00000000000000 - ff0fffffffffffff (=52 bits) guard hole, reserved for hypervisor
 ff10000000000000 - ff8fffffffffffff (=55 bits) direct mapping of all phys. memory
<span class="p_del">-ff90000000000000 - ff91ffffffffffff (=49 bits) hole</span>
<span class="p_del">-ff92000000000000 - ffd1ffffffffffff (=54 bits) vmalloc/ioremap space</span>
<span class="p_add">+ff90000000000000 - ff9fffffffffffff (=52 bits) LDT remap for PTI</span>
<span class="p_add">+ffa0000000000000 - ffd1ffffffffffff (=54 bits) vmalloc/ioremap space (12800 TB)</span>
 ffd2000000000000 - ffd3ffffffffffff (=49 bits) hole
 ffd4000000000000 - ffd5ffffffffffff (=49 bits) virtual memory map (512TB)
 ... unused hole ...
<span class="p_header">diff --git a/Makefile b/Makefile</span>
<span class="p_header">index 9edfb78836a9..655887067dc7 100644</span>
<span class="p_header">--- a/Makefile</span>
<span class="p_header">+++ b/Makefile</span>
<span class="p_chunk">@@ -1,7 +1,7 @@</span> <span class="p_context"></span>
 # SPDX-License-Identifier: GPL-2.0
 VERSION = 4
 PATCHLEVEL = 14
<span class="p_del">-SUBLEVEL = 10</span>
<span class="p_add">+SUBLEVEL = 11</span>
 EXTRAVERSION =
 NAME = Petit Gorille
 
<span class="p_chunk">@@ -802,6 +802,9 @@</span> <span class="p_context"> KBUILD_CFLAGS += $(call cc-disable-warning, pointer-sign)</span>
 # disable invalid &quot;can&#39;t wrap&quot; optimizations for signed / pointers
 KBUILD_CFLAGS	+= $(call cc-option,-fno-strict-overflow)
 
<span class="p_add">+# Make sure -fstack-check isn&#39;t enabled (like gentoo apparently did)</span>
<span class="p_add">+KBUILD_CFLAGS  += $(call cc-option,-fno-stack-check,)</span>
<span class="p_add">+</span>
 # conserve stack if available
 KBUILD_CFLAGS   += $(call cc-option,-fconserve-stack)
 
<span class="p_header">diff --git a/arch/sparc/lib/hweight.S b/arch/sparc/lib/hweight.S</span>
<span class="p_header">index e5547b22cd18..0ddbbb031822 100644</span>
<span class="p_header">--- a/arch/sparc/lib/hweight.S</span>
<span class="p_header">+++ b/arch/sparc/lib/hweight.S</span>
<span class="p_chunk">@@ -44,8 +44,8 @@</span> <span class="p_context"> EXPORT_SYMBOL(__arch_hweight32)</span>
 	.previous
 
 ENTRY(__arch_hweight64)
<span class="p_del">-	sethi	%hi(__sw_hweight16), %g1</span>
<span class="p_del">-	jmpl	%g1 + %lo(__sw_hweight16), %g0</span>
<span class="p_add">+	sethi	%hi(__sw_hweight64), %g1</span>
<span class="p_add">+	jmpl	%g1 + %lo(__sw_hweight64), %g0</span>
 	 nop
 ENDPROC(__arch_hweight64)
 EXPORT_SYMBOL(__arch_hweight64)
<span class="p_header">diff --git a/arch/x86/boot/compressed/pagetable.c b/arch/x86/boot/compressed/pagetable.c</span>
<span class="p_header">index 972319ff5b01..e691ff734cb5 100644</span>
<span class="p_header">--- a/arch/x86/boot/compressed/pagetable.c</span>
<span class="p_header">+++ b/arch/x86/boot/compressed/pagetable.c</span>
<span class="p_chunk">@@ -23,6 +23,9 @@</span> <span class="p_context"></span>
  */
 #undef CONFIG_AMD_MEM_ENCRYPT
 
<span class="p_add">+/* No PAGE_TABLE_ISOLATION support needed either: */</span>
<span class="p_add">+#undef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+</span>
 #include &quot;misc.h&quot;
 
 /* These actually do the work of building the kernel identity maps. */
<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index 3fd8bc560fae..45a63e00a6af 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -1,6 +1,11 @@</span> <span class="p_context"></span>
 /* SPDX-License-Identifier: GPL-2.0 */
 #include &lt;linux/jump_label.h&gt;
 #include &lt;asm/unwind_hints.h&gt;
<span class="p_add">+#include &lt;asm/cpufeatures.h&gt;</span>
<span class="p_add">+#include &lt;asm/page_types.h&gt;</span>
<span class="p_add">+#include &lt;asm/percpu.h&gt;</span>
<span class="p_add">+#include &lt;asm/asm-offsets.h&gt;</span>
<span class="p_add">+#include &lt;asm/processor-flags.h&gt;</span>
 
 /*
 
<span class="p_chunk">@@ -187,6 +192,146 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 #endif
 .endm
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * PAGE_TABLE_ISOLATION PGDs are 8k.  Flip bit 12 to switch between the two</span>
<span class="p_add">+ * halves:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PTI_SWITCH_PGTABLES_MASK	(1&lt;&lt;PAGE_SHIFT)</span>
<span class="p_add">+#define PTI_SWITCH_MASK		(PTI_SWITCH_PGTABLES_MASK|(1&lt;&lt;X86_CR3_PTI_SWITCH_BIT))</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SET_NOFLUSH_BIT	reg:req</span>
<span class="p_add">+	bts	$X86_CR3_PCID_NOFLUSH_BIT, \reg</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro ADJUST_KERNEL_CR3 reg:req</span>
<span class="p_add">+	ALTERNATIVE &quot;&quot;, &quot;SET_NOFLUSH_BIT \reg&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+	/* Clear PCID and &quot;PAGE_TABLE_ISOLATION bit&quot;, point CR3 at kernel pagetables: */</span>
<span class="p_add">+	andq    $(~PTI_SWITCH_MASK), \reg</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_TO_KERNEL_CR3 scratch_reg:req</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="p_add">+	mov	%cr3, \scratch_reg</span>
<span class="p_add">+	ADJUST_KERNEL_CR3 \scratch_reg</span>
<span class="p_add">+	mov	\scratch_reg, %cr3</span>
<span class="p_add">+.Lend_\@:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#define THIS_CPU_user_pcid_flush_mask   \</span>
<span class="p_add">+	PER_CPU_VAR(cpu_tlbstate) + TLB_STATE_user_pcid_flush_mask</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_TO_USER_CR3_NOSTACK scratch_reg:req scratch_reg2:req</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="p_add">+	mov	%cr3, \scratch_reg</span>
<span class="p_add">+</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lwrcr3_\@&quot;, &quot;&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Test if the ASID needs a flush.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq	\scratch_reg, \scratch_reg2</span>
<span class="p_add">+	andq	$(0x7FF), \scratch_reg		/* mask ASID */</span>
<span class="p_add">+	bt	\scratch_reg, THIS_CPU_user_pcid_flush_mask</span>
<span class="p_add">+	jnc	.Lnoflush_\@</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Flush needed, clear the bit */</span>
<span class="p_add">+	btr	\scratch_reg, THIS_CPU_user_pcid_flush_mask</span>
<span class="p_add">+	movq	\scratch_reg2, \scratch_reg</span>
<span class="p_add">+	jmp	.Lwrcr3_\@</span>
<span class="p_add">+</span>
<span class="p_add">+.Lnoflush_\@:</span>
<span class="p_add">+	movq	\scratch_reg2, \scratch_reg</span>
<span class="p_add">+	SET_NOFLUSH_BIT \scratch_reg</span>
<span class="p_add">+</span>
<span class="p_add">+.Lwrcr3_\@:</span>
<span class="p_add">+	/* Flip the PGD and ASID to the user version */</span>
<span class="p_add">+	orq     $(PTI_SWITCH_MASK), \scratch_reg</span>
<span class="p_add">+	mov	\scratch_reg, %cr3</span>
<span class="p_add">+.Lend_\@:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_TO_USER_CR3_STACK	scratch_reg:req</span>
<span class="p_add">+	pushq	%rax</span>
<span class="p_add">+	SWITCH_TO_USER_CR3_NOSTACK scratch_reg=\scratch_reg scratch_reg2=%rax</span>
<span class="p_add">+	popq	%rax</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SAVE_AND_SWITCH_TO_KERNEL_CR3 scratch_reg:req save_reg:req</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Ldone_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="p_add">+	movq	%cr3, \scratch_reg</span>
<span class="p_add">+	movq	\scratch_reg, \save_reg</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Is the &quot;switch mask&quot; all zero?  That means that both of</span>
<span class="p_add">+	 * these are zero:</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *	1. The user/kernel PCID bit, and</span>
<span class="p_add">+	 *	2. The user/kernel &quot;bit&quot; that points CR3 to the</span>
<span class="p_add">+	 *	   bottom half of the 8k PGD</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * That indicates a kernel CR3 value, not a user CR3.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	testq	$(PTI_SWITCH_MASK), \scratch_reg</span>
<span class="p_add">+	jz	.Ldone_\@</span>
<span class="p_add">+</span>
<span class="p_add">+	ADJUST_KERNEL_CR3 \scratch_reg</span>
<span class="p_add">+	movq	\scratch_reg, %cr3</span>
<span class="p_add">+</span>
<span class="p_add">+.Ldone_\@:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro RESTORE_CR3 scratch_reg:req save_reg:req</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lend_\@&quot;, &quot;&quot;, X86_FEATURE_PTI</span>
<span class="p_add">+</span>
<span class="p_add">+	ALTERNATIVE &quot;jmp .Lwrcr3_\@&quot;, &quot;&quot;, X86_FEATURE_PCID</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * KERNEL pages can always resume with NOFLUSH as we do</span>
<span class="p_add">+	 * explicit flushes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	bt	$X86_CR3_PTI_SWITCH_BIT, \save_reg</span>
<span class="p_add">+	jnc	.Lnoflush_\@</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Check if there&#39;s a pending flush for the user ASID we&#39;re</span>
<span class="p_add">+	 * about to set.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq	\save_reg, \scratch_reg</span>
<span class="p_add">+	andq	$(0x7FF), \scratch_reg</span>
<span class="p_add">+	bt	\scratch_reg, THIS_CPU_user_pcid_flush_mask</span>
<span class="p_add">+	jnc	.Lnoflush_\@</span>
<span class="p_add">+</span>
<span class="p_add">+	btr	\scratch_reg, THIS_CPU_user_pcid_flush_mask</span>
<span class="p_add">+	jmp	.Lwrcr3_\@</span>
<span class="p_add">+</span>
<span class="p_add">+.Lnoflush_\@:</span>
<span class="p_add">+	SET_NOFLUSH_BIT \save_reg</span>
<span class="p_add">+</span>
<span class="p_add">+.Lwrcr3_\@:</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The CR3 write could be avoided when not changing its value,</span>
<span class="p_add">+	 * but would require a CR3 read *and* a scratch register.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	movq	\save_reg, %cr3</span>
<span class="p_add">+.Lend_\@:</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* CONFIG_PAGE_TABLE_ISOLATION=n: */</span>
<span class="p_add">+</span>
<span class="p_add">+.macro SWITCH_TO_KERNEL_CR3 scratch_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SWITCH_TO_USER_CR3_NOSTACK scratch_reg:req scratch_reg2:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SWITCH_TO_USER_CR3_STACK scratch_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro SAVE_AND_SWITCH_TO_KERNEL_CR3 scratch_reg:req save_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+.macro RESTORE_CR3 scratch_reg:req save_reg:req</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #endif /* CONFIG_X86_64 */
 
 /*
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index 22c891c3b78d..dd696b966e58 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -23,7 +23,6 @@</span> <span class="p_context"></span>
 #include &lt;asm/segment.h&gt;
 #include &lt;asm/cache.h&gt;
 #include &lt;asm/errno.h&gt;
<span class="p_del">-#include &quot;calling.h&quot;</span>
 #include &lt;asm/asm-offsets.h&gt;
 #include &lt;asm/msr.h&gt;
 #include &lt;asm/unistd.h&gt;
<span class="p_chunk">@@ -40,6 +39,8 @@</span> <span class="p_context"></span>
 #include &lt;asm/frame.h&gt;
 #include &lt;linux/err.h&gt;
 
<span class="p_add">+#include &quot;calling.h&quot;</span>
<span class="p_add">+</span>
 .code64
 .section .entry.text, &quot;ax&quot;
 
<span class="p_chunk">@@ -164,6 +165,9 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_64_trampoline)</span>
 	/* Stash the user RSP. */
 	movq	%rsp, RSP_SCRATCH
 
<span class="p_add">+	/* Note: using %rsp as a scratch reg. */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp</span>
<span class="p_add">+</span>
 	/* Load the top of the task stack into RSP */
 	movq	CPU_ENTRY_AREA_tss + TSS_sp1 + CPU_ENTRY_AREA, %rsp
 
<span class="p_chunk">@@ -203,6 +207,10 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_64)</span>
 	 */
 
 	swapgs
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This path is not taken when PAGE_TABLE_ISOLATION is disabled so it</span>
<span class="p_add">+	 * is not required to switch CR3.</span>
<span class="p_add">+	 */</span>
 	movq	%rsp, PER_CPU_VAR(rsp_scratch)
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 
<span class="p_chunk">@@ -399,6 +407,7 @@</span> <span class="p_context"> syscall_return_via_sysret:</span>
 	 * We are on the trampoline stack.  All regs except RDI are live.
 	 * We can do future final exit work right here.
 	 */
<span class="p_add">+	SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi</span>
 
 	popq	%rdi
 	popq	%rsp
<span class="p_chunk">@@ -736,6 +745,8 @@</span> <span class="p_context"> GLOBAL(swapgs_restore_regs_and_return_to_usermode)</span>
 	 * We can do future final exit work right here.
 	 */
 
<span class="p_add">+	SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi</span>
<span class="p_add">+</span>
 	/* Restore RDI. */
 	popq	%rdi
 	SWAPGS
<span class="p_chunk">@@ -818,7 +829,9 @@</span> <span class="p_context"> native_irq_return_ldt:</span>
 	 */
 
 	pushq	%rdi				/* Stash user RDI */
<span class="p_del">-	SWAPGS</span>
<span class="p_add">+	SWAPGS					/* to kernel GS */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdi	/* to kernel CR3 */</span>
<span class="p_add">+</span>
 	movq	PER_CPU_VAR(espfix_waddr), %rdi
 	movq	%rax, (0*8)(%rdi)		/* user RAX */
 	movq	(1*8)(%rsp), %rax		/* user RIP */
<span class="p_chunk">@@ -834,7 +847,6 @@</span> <span class="p_context"> native_irq_return_ldt:</span>
 	/* Now RAX == RSP. */
 
 	andl	$0xffff0000, %eax		/* RAX = (RSP &amp; 0xffff0000) */
<span class="p_del">-	popq	%rdi				/* Restore user RDI */</span>
 
 	/*
 	 * espfix_stack[31:16] == 0.  The page tables are set up such that
<span class="p_chunk">@@ -845,7 +857,11 @@</span> <span class="p_context"> native_irq_return_ldt:</span>
 	 * still points to an RO alias of the ESPFIX stack.
 	 */
 	orq	PER_CPU_VAR(espfix_stack), %rax
<span class="p_del">-	SWAPGS</span>
<span class="p_add">+</span>
<span class="p_add">+	SWITCH_TO_USER_CR3_STACK scratch_reg=%rdi</span>
<span class="p_add">+	SWAPGS					/* to user GS */</span>
<span class="p_add">+	popq	%rdi				/* Restore user RDI */</span>
<span class="p_add">+</span>
 	movq	%rax, %rsp
 	UNWIND_HINT_IRET_REGS offset=8
 
<span class="p_chunk">@@ -945,6 +961,8 @@</span> <span class="p_context"> ENTRY(switch_to_thread_stack)</span>
 	UNWIND_HINT_FUNC
 
 	pushq	%rdi
<span class="p_add">+	/* Need to switch before accessing the thread stack. */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdi</span>
 	movq	%rsp, %rdi
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 	UNWIND_HINT sp_offset=16 sp_reg=ORC_REG_DI
<span class="p_chunk">@@ -1244,7 +1262,11 @@</span> <span class="p_context"> ENTRY(paranoid_entry)</span>
 	js	1f				/* negative -&gt; in kernel */
 	SWAPGS
 	xorl	%ebx, %ebx
<span class="p_del">-1:	ret</span>
<span class="p_add">+</span>
<span class="p_add">+1:</span>
<span class="p_add">+	SAVE_AND_SWITCH_TO_KERNEL_CR3 scratch_reg=%rax save_reg=%r14</span>
<span class="p_add">+</span>
<span class="p_add">+	ret</span>
 END(paranoid_entry)
 
 /*
<span class="p_chunk">@@ -1266,6 +1288,7 @@</span> <span class="p_context"> ENTRY(paranoid_exit)</span>
 	testl	%ebx, %ebx			/* swapgs needed? */
 	jnz	.Lparanoid_exit_no_swapgs
 	TRACE_IRQS_IRETQ
<span class="p_add">+	RESTORE_CR3	scratch_reg=%rbx save_reg=%r14</span>
 	SWAPGS_UNSAFE_STACK
 	jmp	.Lparanoid_exit_restore
 .Lparanoid_exit_no_swapgs:
<span class="p_chunk">@@ -1293,6 +1316,8 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 	 * from user mode due to an IRET fault.
 	 */
 	SWAPGS
<span class="p_add">+	/* We have user CR3.  Change to kernel CR3. */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax</span>
 
 .Lerror_entry_from_usermode_after_swapgs:
 	/* Put us onto the real thread stack. */
<span class="p_chunk">@@ -1339,6 +1364,7 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 	 * .Lgs_change&#39;s error handler with kernel gsbase.
 	 */
 	SWAPGS
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax</span>
 	jmp .Lerror_entry_done
 
 .Lbstep_iret:
<span class="p_chunk">@@ -1348,10 +1374,11 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 
 .Lerror_bad_iret:
 	/*
<span class="p_del">-	 * We came from an IRET to user mode, so we have user gsbase.</span>
<span class="p_del">-	 * Switch to kernel gsbase:</span>
<span class="p_add">+	 * We came from an IRET to user mode, so we have user</span>
<span class="p_add">+	 * gsbase and CR3.  Switch to kernel gsbase and CR3:</span>
 	 */
 	SWAPGS
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rax</span>
 
 	/*
 	 * Pretend that the exception came from user mode: set up pt_regs
<span class="p_chunk">@@ -1383,6 +1410,10 @@</span> <span class="p_context"> END(error_exit)</span>
 /*
  * Runs on exception stack.  Xen PV does not go through this path at all,
  * so we can use real assembly here.
<span class="p_add">+ *</span>
<span class="p_add">+ * Registers:</span>
<span class="p_add">+ *	%r14: Used to save/restore the CR3 of the interrupted context</span>
<span class="p_add">+ *	      when PAGE_TABLE_ISOLATION is in use.  Do not clobber.</span>
  */
 ENTRY(nmi)
 	UNWIND_HINT_IRET_REGS
<span class="p_chunk">@@ -1446,6 +1477,7 @@</span> <span class="p_context"> ENTRY(nmi)</span>
 
 	swapgs
 	cld
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdx</span>
 	movq	%rsp, %rdx
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 	UNWIND_HINT_IRET_REGS base=%rdx offset=8
<span class="p_chunk">@@ -1698,6 +1730,8 @@</span> <span class="p_context"> end_repeat_nmi:</span>
 	movq	$-1, %rsi
 	call	do_nmi
 
<span class="p_add">+	RESTORE_CR3 scratch_reg=%r15 save_reg=%r14</span>
<span class="p_add">+</span>
 	testl	%ebx, %ebx			/* swapgs needed? */
 	jnz	nmi_restore
 nmi_swapgs:
<span class="p_header">diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">index 95ad40eb7eff..40f17009ec20 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_chunk">@@ -49,6 +49,10 @@</span> <span class="p_context"></span>
 ENTRY(entry_SYSENTER_compat)
 	/* Interrupts are off on entry. */
 	SWAPGS
<span class="p_add">+</span>
<span class="p_add">+	/* We are about to clobber %rsp anyway, clobbering here is OK */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rsp</span>
<span class="p_add">+</span>
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
 
 	/*
<span class="p_chunk">@@ -215,6 +219,12 @@</span> <span class="p_context"> GLOBAL(entry_SYSCALL_compat_after_hwframe)</span>
 	pushq   $0			/* pt_regs-&gt;r14 = 0 */
 	pushq   $0			/* pt_regs-&gt;r15 = 0 */
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We just saved %rdi so it is safe to clobber.  It is not</span>
<span class="p_add">+	 * preserved during the C calls inside TRACE_IRQS_OFF anyway.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_TO_KERNEL_CR3 scratch_reg=%rdi</span>
<span class="p_add">+</span>
 	/*
 	 * User mode is traced as though IRQs are on, and SYSENTER
 	 * turned them off.
<span class="p_chunk">@@ -256,10 +266,22 @@</span> <span class="p_context"> sysret32_from_system_call:</span>
 	 * when the system call started, which is already known to user
 	 * code.  We zero R8-R10 to avoid info leaks.
          */
<span class="p_add">+	movq	RSP-ORIG_RAX(%rsp), %rsp</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The original userspace %rsp (RSP-ORIG_RAX(%rsp)) is stored</span>
<span class="p_add">+	 * on the process stack which is not mapped to userspace and</span>
<span class="p_add">+	 * not readable after we SWITCH_TO_USER_CR3.  Delay the CR3</span>
<span class="p_add">+	 * switch until after after the last reference to the process</span>
<span class="p_add">+	 * stack.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * %r8/%r9 are zeroed before the sysret, thus safe to clobber.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	SWITCH_TO_USER_CR3_NOSTACK scratch_reg=%r8 scratch_reg2=%r9</span>
<span class="p_add">+</span>
 	xorq	%r8, %r8
 	xorq	%r9, %r9
 	xorq	%r10, %r10
<span class="p_del">-	movq	RSP-ORIG_RAX(%rsp), %rsp</span>
 	swapgs
 	sysretl
 END(entry_SYSCALL_compat)
<span class="p_header">diff --git a/arch/x86/entry/vsyscall/vsyscall_64.c b/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_header">index 1faf40f2dda9..577fa8adb785 100644</span>
<span class="p_header">--- a/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_header">+++ b/arch/x86/entry/vsyscall/vsyscall_64.c</span>
<span class="p_chunk">@@ -344,14 +344,14 @@</span> <span class="p_context"> int in_gate_area_no_mm(unsigned long addr)</span>
  * vsyscalls but leave the page not present.  If so, we skip calling
  * this.
  */
<span class="p_del">-static void __init set_vsyscall_pgtable_user_bits(void)</span>
<span class="p_add">+void __init set_vsyscall_pgtable_user_bits(pgd_t *root)</span>
 {
 	pgd_t *pgd;
 	p4d_t *p4d;
 	pud_t *pud;
 	pmd_t *pmd;
 
<span class="p_del">-	pgd = pgd_offset_k(VSYSCALL_ADDR);</span>
<span class="p_add">+	pgd = pgd_offset_pgd(root, VSYSCALL_ADDR);</span>
 	set_pgd(pgd, __pgd(pgd_val(*pgd) | _PAGE_USER));
 	p4d = p4d_offset(pgd, VSYSCALL_ADDR);
 #if CONFIG_PGTABLE_LEVELS &gt;= 5
<span class="p_chunk">@@ -373,7 +373,7 @@</span> <span class="p_context"> void __init map_vsyscall(void)</span>
 			     vsyscall_mode == NATIVE
 			     ? PAGE_KERNEL_VSYSCALL
 			     : PAGE_KERNEL_VVAR);
<span class="p_del">-		set_vsyscall_pgtable_user_bits();</span>
<span class="p_add">+		set_vsyscall_pgtable_user_bits(swapper_pg_dir);</span>
 	}
 
 	BUILD_BUG_ON((unsigned long)__fix_to_virt(VSYSCALL_PAGE) !=
<span class="p_header">diff --git a/arch/x86/events/intel/ds.c b/arch/x86/events/intel/ds.c</span>
<span class="p_header">index 3674a4b6f8bd..8f0aace08b87 100644</span>
<span class="p_header">--- a/arch/x86/events/intel/ds.c</span>
<span class="p_header">+++ b/arch/x86/events/intel/ds.c</span>
<span class="p_chunk">@@ -3,16 +3,18 @@</span> <span class="p_context"></span>
 #include &lt;linux/types.h&gt;
 #include &lt;linux/slab.h&gt;
 
<span class="p_add">+#include &lt;asm/cpu_entry_area.h&gt;</span>
 #include &lt;asm/perf_event.h&gt;
 #include &lt;asm/insn.h&gt;
 
 #include &quot;../perf_event.h&quot;
 
<span class="p_add">+/* Waste a full page so it can be mapped into the cpu_entry_area */</span>
<span class="p_add">+DEFINE_PER_CPU_PAGE_ALIGNED(struct debug_store, cpu_debug_store);</span>
<span class="p_add">+</span>
 /* The size of a BTS record in bytes: */
 #define BTS_RECORD_SIZE		24
 
<span class="p_del">-#define BTS_BUFFER_SIZE		(PAGE_SIZE &lt;&lt; 4)</span>
<span class="p_del">-#define PEBS_BUFFER_SIZE	(PAGE_SIZE &lt;&lt; 4)</span>
 #define PEBS_FIXUP_SIZE		PAGE_SIZE
 
 /*
<span class="p_chunk">@@ -279,17 +281,52 @@</span> <span class="p_context"> void fini_debug_store_on_cpu(int cpu)</span>
 
 static DEFINE_PER_CPU(void *, insn_buffer);
 
<span class="p_del">-static int alloc_pebs_buffer(int cpu)</span>
<span class="p_add">+static void ds_update_cea(void *cea, void *addr, size_t size, pgprot_t prot)</span>
 {
<span class="p_del">-	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;</span>
<span class="p_add">+	phys_addr_t pa;</span>
<span class="p_add">+	size_t msz = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	pa = virt_to_phys(addr);</span>
<span class="p_add">+	for (; msz &lt; size; msz += PAGE_SIZE, pa += PAGE_SIZE, cea += PAGE_SIZE)</span>
<span class="p_add">+		cea_set_pte(cea, pa, prot);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void ds_clear_cea(void *cea, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	size_t msz = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (; msz &lt; size; msz += PAGE_SIZE, cea += PAGE_SIZE)</span>
<span class="p_add">+		cea_set_pte(cea, 0, PAGE_NONE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void *dsalloc_pages(size_t size, gfp_t flags, int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int order = get_order(size);</span>
 	int node = cpu_to_node(cpu);
<span class="p_del">-	int max;</span>
<span class="p_del">-	void *buffer, *ibuffer;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	page = __alloc_pages_node(node, flags | __GFP_ZERO, order);</span>
<span class="p_add">+	return page ? page_address(page) : NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void dsfree_pages(const void *buffer, size_t size)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (buffer)</span>
<span class="p_add">+		free_pages((unsigned long)buffer, get_order(size));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int alloc_pebs_buffer(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct cpu_hw_events *hwev = per_cpu_ptr(&amp;cpu_hw_events, cpu);</span>
<span class="p_add">+	struct debug_store *ds = hwev-&gt;ds;</span>
<span class="p_add">+	size_t bsiz = x86_pmu.pebs_buffer_size;</span>
<span class="p_add">+	int max, node = cpu_to_node(cpu);</span>
<span class="p_add">+	void *buffer, *ibuffer, *cea;</span>
 
 	if (!x86_pmu.pebs)
 		return 0;
 
<span class="p_del">-	buffer = kzalloc_node(x86_pmu.pebs_buffer_size, GFP_KERNEL, node);</span>
<span class="p_add">+	buffer = dsalloc_pages(bsiz, GFP_KERNEL, cpu);</span>
 	if (unlikely(!buffer))
 		return -ENOMEM;
 
<span class="p_chunk">@@ -300,25 +337,27 @@</span> <span class="p_context"> static int alloc_pebs_buffer(int cpu)</span>
 	if (x86_pmu.intel_cap.pebs_format &lt; 2) {
 		ibuffer = kzalloc_node(PEBS_FIXUP_SIZE, GFP_KERNEL, node);
 		if (!ibuffer) {
<span class="p_del">-			kfree(buffer);</span>
<span class="p_add">+			dsfree_pages(buffer, bsiz);</span>
 			return -ENOMEM;
 		}
 		per_cpu(insn_buffer, cpu) = ibuffer;
 	}
<span class="p_del">-</span>
<span class="p_del">-	max = x86_pmu.pebs_buffer_size / x86_pmu.pebs_record_size;</span>
<span class="p_del">-</span>
<span class="p_del">-	ds-&gt;pebs_buffer_base = (u64)(unsigned long)buffer;</span>
<span class="p_add">+	hwev-&gt;ds_pebs_vaddr = buffer;</span>
<span class="p_add">+	/* Update the cpu entry area mapping */</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_buffers.pebs_buffer;</span>
<span class="p_add">+	ds-&gt;pebs_buffer_base = (unsigned long) cea;</span>
<span class="p_add">+	ds_update_cea(cea, buffer, bsiz, PAGE_KERNEL);</span>
 	ds-&gt;pebs_index = ds-&gt;pebs_buffer_base;
<span class="p_del">-	ds-&gt;pebs_absolute_maximum = ds-&gt;pebs_buffer_base +</span>
<span class="p_del">-		max * x86_pmu.pebs_record_size;</span>
<span class="p_del">-</span>
<span class="p_add">+	max = x86_pmu.pebs_record_size * (bsiz / x86_pmu.pebs_record_size);</span>
<span class="p_add">+	ds-&gt;pebs_absolute_maximum = ds-&gt;pebs_buffer_base + max;</span>
 	return 0;
 }
 
 static void release_pebs_buffer(int cpu)
 {
<span class="p_del">-	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;</span>
<span class="p_add">+	struct cpu_hw_events *hwev = per_cpu_ptr(&amp;cpu_hw_events, cpu);</span>
<span class="p_add">+	struct debug_store *ds = hwev-&gt;ds;</span>
<span class="p_add">+	void *cea;</span>
 
 	if (!ds || !x86_pmu.pebs)
 		return;
<span class="p_chunk">@@ -326,73 +365,70 @@</span> <span class="p_context"> static void release_pebs_buffer(int cpu)</span>
 	kfree(per_cpu(insn_buffer, cpu));
 	per_cpu(insn_buffer, cpu) = NULL;
 
<span class="p_del">-	kfree((void *)(unsigned long)ds-&gt;pebs_buffer_base);</span>
<span class="p_add">+	/* Clear the fixmap */</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_buffers.pebs_buffer;</span>
<span class="p_add">+	ds_clear_cea(cea, x86_pmu.pebs_buffer_size);</span>
 	ds-&gt;pebs_buffer_base = 0;
<span class="p_add">+	dsfree_pages(hwev-&gt;ds_pebs_vaddr, x86_pmu.pebs_buffer_size);</span>
<span class="p_add">+	hwev-&gt;ds_pebs_vaddr = NULL;</span>
 }
 
 static int alloc_bts_buffer(int cpu)
 {
<span class="p_del">-	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;</span>
<span class="p_del">-	int node = cpu_to_node(cpu);</span>
<span class="p_del">-	int max, thresh;</span>
<span class="p_del">-	void *buffer;</span>
<span class="p_add">+	struct cpu_hw_events *hwev = per_cpu_ptr(&amp;cpu_hw_events, cpu);</span>
<span class="p_add">+	struct debug_store *ds = hwev-&gt;ds;</span>
<span class="p_add">+	void *buffer, *cea;</span>
<span class="p_add">+	int max;</span>
 
 	if (!x86_pmu.bts)
 		return 0;
 
<span class="p_del">-	buffer = kzalloc_node(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_NOWARN, node);</span>
<span class="p_add">+	buffer = dsalloc_pages(BTS_BUFFER_SIZE, GFP_KERNEL | __GFP_NOWARN, cpu);</span>
 	if (unlikely(!buffer)) {
 		WARN_ONCE(1, &quot;%s: BTS buffer allocation failure\n&quot;, __func__);
 		return -ENOMEM;
 	}
<span class="p_del">-</span>
<span class="p_del">-	max = BTS_BUFFER_SIZE / BTS_RECORD_SIZE;</span>
<span class="p_del">-	thresh = max / 16;</span>
<span class="p_del">-</span>
<span class="p_del">-	ds-&gt;bts_buffer_base = (u64)(unsigned long)buffer;</span>
<span class="p_add">+	hwev-&gt;ds_bts_vaddr = buffer;</span>
<span class="p_add">+	/* Update the fixmap */</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_buffers.bts_buffer;</span>
<span class="p_add">+	ds-&gt;bts_buffer_base = (unsigned long) cea;</span>
<span class="p_add">+	ds_update_cea(cea, buffer, BTS_BUFFER_SIZE, PAGE_KERNEL);</span>
 	ds-&gt;bts_index = ds-&gt;bts_buffer_base;
<span class="p_del">-	ds-&gt;bts_absolute_maximum = ds-&gt;bts_buffer_base +</span>
<span class="p_del">-		max * BTS_RECORD_SIZE;</span>
<span class="p_del">-	ds-&gt;bts_interrupt_threshold = ds-&gt;bts_absolute_maximum -</span>
<span class="p_del">-		thresh * BTS_RECORD_SIZE;</span>
<span class="p_del">-</span>
<span class="p_add">+	max = BTS_RECORD_SIZE * (BTS_BUFFER_SIZE / BTS_RECORD_SIZE);</span>
<span class="p_add">+	ds-&gt;bts_absolute_maximum = ds-&gt;bts_buffer_base + max;</span>
<span class="p_add">+	ds-&gt;bts_interrupt_threshold = ds-&gt;bts_absolute_maximum - (max / 16);</span>
 	return 0;
 }
 
 static void release_bts_buffer(int cpu)
 {
<span class="p_del">-	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;</span>
<span class="p_add">+	struct cpu_hw_events *hwev = per_cpu_ptr(&amp;cpu_hw_events, cpu);</span>
<span class="p_add">+	struct debug_store *ds = hwev-&gt;ds;</span>
<span class="p_add">+	void *cea;</span>
 
 	if (!ds || !x86_pmu.bts)
 		return;
 
<span class="p_del">-	kfree((void *)(unsigned long)ds-&gt;bts_buffer_base);</span>
<span class="p_add">+	/* Clear the fixmap */</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_buffers.bts_buffer;</span>
<span class="p_add">+	ds_clear_cea(cea, BTS_BUFFER_SIZE);</span>
 	ds-&gt;bts_buffer_base = 0;
<span class="p_add">+	dsfree_pages(hwev-&gt;ds_bts_vaddr, BTS_BUFFER_SIZE);</span>
<span class="p_add">+	hwev-&gt;ds_bts_vaddr = NULL;</span>
 }
 
 static int alloc_ds_buffer(int cpu)
 {
<span class="p_del">-	int node = cpu_to_node(cpu);</span>
<span class="p_del">-	struct debug_store *ds;</span>
<span class="p_del">-</span>
<span class="p_del">-	ds = kzalloc_node(sizeof(*ds), GFP_KERNEL, node);</span>
<span class="p_del">-	if (unlikely(!ds))</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_add">+	struct debug_store *ds = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_store;</span>
 
<span class="p_add">+	memset(ds, 0, sizeof(*ds));</span>
 	per_cpu(cpu_hw_events, cpu).ds = ds;
<span class="p_del">-</span>
 	return 0;
 }
 
 static void release_ds_buffer(int cpu)
 {
<span class="p_del">-	struct debug_store *ds = per_cpu(cpu_hw_events, cpu).ds;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!ds)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
 	per_cpu(cpu_hw_events, cpu).ds = NULL;
<span class="p_del">-	kfree(ds);</span>
 }
 
 void release_ds_buffers(void)
<span class="p_header">diff --git a/arch/x86/events/perf_event.h b/arch/x86/events/perf_event.h</span>
<span class="p_header">index f7aaadf9331f..8e4ea143ed96 100644</span>
<span class="p_header">--- a/arch/x86/events/perf_event.h</span>
<span class="p_header">+++ b/arch/x86/events/perf_event.h</span>
<span class="p_chunk">@@ -14,6 +14,8 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/perf_event.h&gt;
 
<span class="p_add">+#include &lt;asm/intel_ds.h&gt;</span>
<span class="p_add">+</span>
 /* To enable MSR tracing please use the generic trace points. */
 
 /*
<span class="p_chunk">@@ -77,8 +79,6 @@</span> <span class="p_context"> struct amd_nb {</span>
 	struct event_constraint event_constraints[X86_PMC_IDX_MAX];
 };
 
<span class="p_del">-/* The maximal number of PEBS events: */</span>
<span class="p_del">-#define MAX_PEBS_EVENTS		8</span>
 #define PEBS_COUNTER_MASK	((1ULL &lt;&lt; MAX_PEBS_EVENTS) - 1)
 
 /*
<span class="p_chunk">@@ -95,23 +95,6 @@</span> <span class="p_context"> struct amd_nb {</span>
 	PERF_SAMPLE_TRANSACTION | PERF_SAMPLE_PHYS_ADDR | \
 	PERF_SAMPLE_REGS_INTR | PERF_SAMPLE_REGS_USER)
 
<span class="p_del">-/*</span>
<span class="p_del">- * A debug store configuration.</span>
<span class="p_del">- *</span>
<span class="p_del">- * We only support architectures that use 64bit fields.</span>
<span class="p_del">- */</span>
<span class="p_del">-struct debug_store {</span>
<span class="p_del">-	u64	bts_buffer_base;</span>
<span class="p_del">-	u64	bts_index;</span>
<span class="p_del">-	u64	bts_absolute_maximum;</span>
<span class="p_del">-	u64	bts_interrupt_threshold;</span>
<span class="p_del">-	u64	pebs_buffer_base;</span>
<span class="p_del">-	u64	pebs_index;</span>
<span class="p_del">-	u64	pebs_absolute_maximum;</span>
<span class="p_del">-	u64	pebs_interrupt_threshold;</span>
<span class="p_del">-	u64	pebs_event_reset[MAX_PEBS_EVENTS];</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 #define PEBS_REGS \
 	(PERF_REG_X86_AX | \
 	 PERF_REG_X86_BX | \
<span class="p_chunk">@@ -216,6 +199,8 @@</span> <span class="p_context"> struct cpu_hw_events {</span>
 	 * Intel DebugStore bits
 	 */
 	struct debug_store	*ds;
<span class="p_add">+	void			*ds_pebs_vaddr;</span>
<span class="p_add">+	void			*ds_bts_vaddr;</span>
 	u64			pebs_enabled;
 	int			n_pebs;
 	int			n_large_pebs;
<span class="p_header">diff --git a/arch/x86/include/asm/cpu_entry_area.h b/arch/x86/include/asm/cpu_entry_area.h</span>
<span class="p_header">index 2fbc69a0916e..4a7884b8dca5 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpu_entry_area.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpu_entry_area.h</span>
<span class="p_chunk">@@ -5,6 +5,7 @@</span> <span class="p_context"></span>
 
 #include &lt;linux/percpu-defs.h&gt;
 #include &lt;asm/processor.h&gt;
<span class="p_add">+#include &lt;asm/intel_ds.h&gt;</span>
 
 /*
  * cpu_entry_area is a percpu region that contains things needed by the CPU
<span class="p_chunk">@@ -40,6 +41,18 @@</span> <span class="p_context"> struct cpu_entry_area {</span>
 	 */
 	char exception_stacks[(N_EXCEPTION_STACKS - 1) * EXCEPTION_STKSZ + DEBUG_STKSZ];
 #endif
<span class="p_add">+#ifdef CONFIG_CPU_SUP_INTEL</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Per CPU debug store for Intel performance monitoring. Wastes a</span>
<span class="p_add">+	 * full page at the moment.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct debug_store cpu_debug_store;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The actual PEBS/BTS buffers must be mapped to user space</span>
<span class="p_add">+	 * Reserve enough fixmap PTEs.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	struct debug_store_buffers cpu_debug_buffers;</span>
<span class="p_add">+#endif</span>
 };
 
 #define CPU_ENTRY_AREA_SIZE	(sizeof(struct cpu_entry_area))
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeatures.h b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">index 800104c8a3ed..07cdd1715705 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeatures.h</span>
<span class="p_chunk">@@ -197,11 +197,12 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_CAT_L3		( 7*32+ 4) /* Cache Allocation Technology L3 */
 #define X86_FEATURE_CAT_L2		( 7*32+ 5) /* Cache Allocation Technology L2 */
 #define X86_FEATURE_CDP_L3		( 7*32+ 6) /* Code and Data Prioritization L3 */
<span class="p_add">+#define X86_FEATURE_INVPCID_SINGLE	( 7*32+ 7) /* Effectively INVPCID &amp;&amp; CR4.PCIDE=1 */</span>
 
 #define X86_FEATURE_HW_PSTATE		( 7*32+ 8) /* AMD HW-PState */
 #define X86_FEATURE_PROC_FEEDBACK	( 7*32+ 9) /* AMD ProcFeedbackInterface */
 #define X86_FEATURE_SME			( 7*32+10) /* AMD Secure Memory Encryption */
<span class="p_del">-</span>
<span class="p_add">+#define X86_FEATURE_PTI			( 7*32+11) /* Kernel Page Table Isolation enabled */</span>
 #define X86_FEATURE_INTEL_PPIN		( 7*32+14) /* Intel Processor Inventory Number */
 #define X86_FEATURE_INTEL_PT		( 7*32+15) /* Intel Processor Trace */
 #define X86_FEATURE_AVX512_4VNNIW	( 7*32+16) /* AVX-512 Neural Network Instructions */
<span class="p_chunk">@@ -340,5 +341,6 @@</span> <span class="p_context"></span>
 #define X86_BUG_SWAPGS_FENCE		X86_BUG(11) /* SWAPGS without input dep on GS */
 #define X86_BUG_MONITOR			X86_BUG(12) /* IPI required to wake up remote CPU */
 #define X86_BUG_AMD_E400		X86_BUG(13) /* CPU is among the affected by Erratum 400 */
<span class="p_add">+#define X86_BUG_CPU_INSECURE		X86_BUG(14) /* CPU is insecure and needs kernel page table isolation */</span>
 
 #endif /* _ASM_X86_CPUFEATURES_H */
<span class="p_header">diff --git a/arch/x86/include/asm/desc.h b/arch/x86/include/asm/desc.h</span>
<span class="p_header">index bc359dd2f7f6..85e23bb7b34e 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/desc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/desc.h</span>
<span class="p_chunk">@@ -21,6 +21,8 @@</span> <span class="p_context"> static inline void fill_ldt(struct desc_struct *desc, const struct user_desc *in</span>
 
 	desc-&gt;type		= (info-&gt;read_exec_only ^ 1) &lt;&lt; 1;
 	desc-&gt;type	       |= info-&gt;contents &lt;&lt; 2;
<span class="p_add">+	/* Set the ACCESS bit so it can be mapped RO */</span>
<span class="p_add">+	desc-&gt;type	       |= 1;</span>
 
 	desc-&gt;s			= 1;
 	desc-&gt;dpl		= 0x3;
<span class="p_header">diff --git a/arch/x86/include/asm/disabled-features.h b/arch/x86/include/asm/disabled-features.h</span>
<span class="p_header">index c10c9128f54e..e428e16dd822 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/disabled-features.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/disabled-features.h</span>
<span class="p_chunk">@@ -44,6 +44,12 @@</span> <span class="p_context"></span>
 # define DISABLE_LA57	(1&lt;&lt;(X86_FEATURE_LA57 &amp; 31))
 #endif
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+# define DISABLE_PTI		0</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define DISABLE_PTI		(1 &lt;&lt; (X86_FEATURE_PTI &amp; 31))</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * Make sure to add features to the correct mask
  */
<span class="p_chunk">@@ -54,7 +60,7 @@</span> <span class="p_context"></span>
 #define DISABLED_MASK4	(DISABLE_PCID)
 #define DISABLED_MASK5	0
 #define DISABLED_MASK6	0
<span class="p_del">-#define DISABLED_MASK7	0</span>
<span class="p_add">+#define DISABLED_MASK7	(DISABLE_PTI)</span>
 #define DISABLED_MASK8	0
 #define DISABLED_MASK9	(DISABLE_MPX)
 #define DISABLED_MASK10	0
<span class="p_header">diff --git a/arch/x86/include/asm/intel_ds.h b/arch/x86/include/asm/intel_ds.h</span>
new file mode 100644
<span class="p_header">index 000000000000..62a9f4966b42</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/intel_ds.h</span>
<span class="p_chunk">@@ -0,0 +1,36 @@</span> <span class="p_context"></span>
<span class="p_add">+#ifndef _ASM_INTEL_DS_H</span>
<span class="p_add">+#define _ASM_INTEL_DS_H</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/percpu-defs.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#define BTS_BUFFER_SIZE		(PAGE_SIZE &lt;&lt; 4)</span>
<span class="p_add">+#define PEBS_BUFFER_SIZE	(PAGE_SIZE &lt;&lt; 4)</span>
<span class="p_add">+</span>
<span class="p_add">+/* The maximal number of PEBS events: */</span>
<span class="p_add">+#define MAX_PEBS_EVENTS		8</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * A debug store configuration.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We only support architectures that use 64bit fields.</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct debug_store {</span>
<span class="p_add">+	u64	bts_buffer_base;</span>
<span class="p_add">+	u64	bts_index;</span>
<span class="p_add">+	u64	bts_absolute_maximum;</span>
<span class="p_add">+	u64	bts_interrupt_threshold;</span>
<span class="p_add">+	u64	pebs_buffer_base;</span>
<span class="p_add">+	u64	pebs_index;</span>
<span class="p_add">+	u64	pebs_absolute_maximum;</span>
<span class="p_add">+	u64	pebs_interrupt_threshold;</span>
<span class="p_add">+	u64	pebs_event_reset[MAX_PEBS_EVENTS];</span>
<span class="p_add">+} __aligned(PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+DECLARE_PER_CPU_PAGE_ALIGNED(struct debug_store, cpu_debug_store);</span>
<span class="p_add">+</span>
<span class="p_add">+struct debug_store_buffers {</span>
<span class="p_add">+	char	bts_buffer[BTS_BUFFER_SIZE];</span>
<span class="p_add">+	char	pebs_buffer[PEBS_BUFFER_SIZE];</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/arch/x86/include/asm/mmu_context.h b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">index 5ede7cae1d67..c931b88982a0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/mmu_context.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/mmu_context.h</span>
<span class="p_chunk">@@ -50,10 +50,33 @@</span> <span class="p_context"> struct ldt_struct {</span>
 	 * call gates.  On native, we could merge the ldt_struct and LDT
 	 * allocations, but it&#39;s not worth trying to optimize.
 	 */
<span class="p_del">-	struct desc_struct *entries;</span>
<span class="p_del">-	unsigned int nr_entries;</span>
<span class="p_add">+	struct desc_struct	*entries;</span>
<span class="p_add">+	unsigned int		nr_entries;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If PTI is in use, then the entries array is not mapped while we&#39;re</span>
<span class="p_add">+	 * in user mode.  The whole array will be aliased at the addressed</span>
<span class="p_add">+	 * given by ldt_slot_va(slot).  We use two slots so that we can allocate</span>
<span class="p_add">+	 * and map, and enable a new LDT without invalidating the mapping</span>
<span class="p_add">+	 * of an older, still-in-use LDT.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * slot will be -1 if this LDT doesn&#39;t have an alias mapping.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	int			slot;</span>
 };
 
<span class="p_add">+/* This is a multiple of PAGE_SIZE. */</span>
<span class="p_add">+#define LDT_SLOT_STRIDE (LDT_ENTRIES * LDT_ENTRY_SIZE)</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void *ldt_slot_va(int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_64</span>
<span class="p_add">+	return (void *)(LDT_BASE_ADDR + LDT_SLOT_STRIDE * slot);</span>
<span class="p_add">+#else</span>
<span class="p_add">+	BUG();</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Used for LDT copy/destruction.
  */
<span class="p_chunk">@@ -64,6 +87,7 @@</span> <span class="p_context"> static inline void init_new_context_ldt(struct mm_struct *mm)</span>
 }
 int ldt_dup_context(struct mm_struct *oldmm, struct mm_struct *mm);
 void destroy_context_ldt(struct mm_struct *mm);
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm);</span>
 #else	/* CONFIG_MODIFY_LDT_SYSCALL */
 static inline void init_new_context_ldt(struct mm_struct *mm) { }
 static inline int ldt_dup_context(struct mm_struct *oldmm,
<span class="p_chunk">@@ -71,7 +95,8 @@</span> <span class="p_context"> static inline int ldt_dup_context(struct mm_struct *oldmm,</span>
 {
 	return 0;
 }
<span class="p_del">-static inline void destroy_context_ldt(struct mm_struct *mm) {}</span>
<span class="p_add">+static inline void destroy_context_ldt(struct mm_struct *mm) { }</span>
<span class="p_add">+static inline void ldt_arch_exit_mmap(struct mm_struct *mm) { }</span>
 #endif
 
 static inline void load_mm_ldt(struct mm_struct *mm)
<span class="p_chunk">@@ -96,10 +121,31 @@</span> <span class="p_context"> static inline void load_mm_ldt(struct mm_struct *mm)</span>
 	 * that we can see.
 	 */
 
<span class="p_del">-	if (unlikely(ldt))</span>
<span class="p_del">-		set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (unlikely(ldt)) {</span>
<span class="p_add">+		if (static_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="p_add">+			if (WARN_ON_ONCE((unsigned long)ldt-&gt;slot &gt; 1)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * Whoops -- either the new LDT isn&#39;t mapped</span>
<span class="p_add">+				 * (if slot == -1) or is mapped into a bogus</span>
<span class="p_add">+				 * slot (if slot &gt; 1).</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				clear_LDT();</span>
<span class="p_add">+				return;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * If page table isolation is enabled, ldt-&gt;entries</span>
<span class="p_add">+			 * will not be mapped in the userspace pagetables.</span>
<span class="p_add">+			 * Tell the CPU to access the LDT through the alias</span>
<span class="p_add">+			 * at ldt_slot_va(ldt-&gt;slot).</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			set_ldt(ldt_slot_va(ldt-&gt;slot), ldt-&gt;nr_entries);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			set_ldt(ldt-&gt;entries, ldt-&gt;nr_entries);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	} else {</span>
 		clear_LDT();
<span class="p_add">+	}</span>
 #else
 	clear_LDT();
 #endif
<span class="p_chunk">@@ -194,6 +240,7 @@</span> <span class="p_context"> static inline int arch_dup_mmap(struct mm_struct *oldmm, struct mm_struct *mm)</span>
 static inline void arch_exit_mmap(struct mm_struct *mm)
 {
 	paravirt_arch_exit_mmap(mm);
<span class="p_add">+	ldt_arch_exit_mmap(mm);</span>
 }
 
 #ifdef CONFIG_X86_64
<span class="p_header">diff --git a/arch/x86/include/asm/pgalloc.h b/arch/x86/include/asm/pgalloc.h</span>
<span class="p_header">index 4b5e1eafada7..aff42e1da6ee 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgalloc.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgalloc.h</span>
<span class="p_chunk">@@ -30,6 +30,17 @@</span> <span class="p_context"> static inline void paravirt_release_p4d(unsigned long pfn) {}</span>
  */
 extern gfp_t __userpte_alloc_gfp;
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Instead of one PGD, we acquire two PGDs.  Being order-1, it is</span>
<span class="p_add">+ * both 8k in size and 8k-aligned.  That lets us just flip bit 12</span>
<span class="p_add">+ * in a pointer to swap between the two 4k halves.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PGD_ALLOCATION_ORDER 1</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define PGD_ALLOCATION_ORDER 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /*
  * Allocate and free page tables.
  */
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index f02de8bc1f72..211368922cad 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -28,6 +28,7 @@</span> <span class="p_context"> extern pgd_t early_top_pgt[PTRS_PER_PGD];</span>
 int __init __early_make_pgtable(unsigned long address, pmdval_t pmd);
 
 void ptdump_walk_pgd_level(struct seq_file *m, pgd_t *pgd);
<span class="p_add">+void ptdump_walk_pgd_level_debugfs(struct seq_file *m, pgd_t *pgd, bool user);</span>
 void ptdump_walk_pgd_level_checkwx(void);
 
 #ifdef CONFIG_DEBUG_WX
<span class="p_chunk">@@ -846,7 +847,12 @@</span> <span class="p_context"> static inline pud_t *pud_offset(p4d_t *p4d, unsigned long address)</span>
 
 static inline int p4d_bad(p4d_t p4d)
 {
<span class="p_del">-	return (p4d_flags(p4d) &amp; ~(_KERNPG_TABLE | _PAGE_USER)) != 0;</span>
<span class="p_add">+	unsigned long ignore_flags = _KERNPG_TABLE | _PAGE_USER;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))</span>
<span class="p_add">+		ignore_flags |= _PAGE_NX;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (p4d_flags(p4d) &amp; ~ignore_flags) != 0;</span>
 }
 #endif  /* CONFIG_PGTABLE_LEVELS &gt; 3 */
 
<span class="p_chunk">@@ -880,7 +886,12 @@</span> <span class="p_context"> static inline p4d_t *p4d_offset(pgd_t *pgd, unsigned long address)</span>
 
 static inline int pgd_bad(pgd_t pgd)
 {
<span class="p_del">-	return (pgd_flags(pgd) &amp; ~_PAGE_USER) != _KERNPG_TABLE;</span>
<span class="p_add">+	unsigned long ignore_flags = _PAGE_USER;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))</span>
<span class="p_add">+		ignore_flags |= _PAGE_NX;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (pgd_flags(pgd) &amp; ~ignore_flags) != _KERNPG_TABLE;</span>
 }
 
 static inline int pgd_none(pgd_t pgd)
<span class="p_chunk">@@ -909,7 +920,11 @@</span> <span class="p_context"> static inline int pgd_none(pgd_t pgd)</span>
  * pgd_offset() returns a (pgd_t *)
  * pgd_index() is used get the offset into the pgd page&#39;s array of pgd_t&#39;s;
  */
<span class="p_del">-#define pgd_offset(mm, address) ((mm)-&gt;pgd + pgd_index((address)))</span>
<span class="p_add">+#define pgd_offset_pgd(pgd, address) (pgd + pgd_index((address)))</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * a shortcut to get a pgd_t in a given mm</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define pgd_offset(mm, address) pgd_offset_pgd((mm)-&gt;pgd, (address))</span>
 /*
  * a shortcut which implies the use of the kernel&#39;s pgd, instead
  * of a process&#39;s
<span class="p_chunk">@@ -1111,7 +1126,14 @@</span> <span class="p_context"> static inline int pud_write(pud_t pud)</span>
  */
 static inline void clone_pgd_range(pgd_t *dst, pgd_t *src, int count)
 {
<span class="p_del">-       memcpy(dst, src, count * sizeof(pgd_t));</span>
<span class="p_add">+	memcpy(dst, src, count * sizeof(pgd_t));</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	/* Clone the user space pgd as well */</span>
<span class="p_add">+	memcpy(kernel_to_user_pgdp(dst), kernel_to_user_pgdp(src),</span>
<span class="p_add">+	       count * sizeof(pgd_t));</span>
<span class="p_add">+#endif</span>
 }
 
 #define PTE_SHIFT ilog2(PTRS_PER_PTE)
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index e9f05331e732..81462e9a34f6 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -131,9 +131,97 @@</span> <span class="p_context"> static inline pud_t native_pudp_get_and_clear(pud_t *xp)</span>
 #endif
 }
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * All top-level PAGE_TABLE_ISOLATION page tables are order-1 pages</span>
<span class="p_add">+ * (8k-aligned and 8k in size).  The kernel one is at the beginning 4k and</span>
<span class="p_add">+ * the user one is in the last 4k.  To switch between them, you</span>
<span class="p_add">+ * just need to flip the 12th bit in their addresses.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PTI_PGTABLE_SWITCH_BIT	PAGE_SHIFT</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * This generates better code than the inline assembly in</span>
<span class="p_add">+ * __set_bit().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void *ptr_set_bit(void *ptr, int bit)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long __ptr = (unsigned long)ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	__ptr |= BIT(bit);</span>
<span class="p_add">+	return (void *)__ptr;</span>
<span class="p_add">+}</span>
<span class="p_add">+static inline void *ptr_clear_bit(void *ptr, int bit)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long __ptr = (unsigned long)ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	__ptr &amp;= ~BIT(bit);</span>
<span class="p_add">+	return (void *)__ptr;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *kernel_to_user_pgdp(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_set_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pgd_t *user_to_kernel_pgdp(pgd_t *pgdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_clear_bit(pgdp, PTI_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline p4d_t *kernel_to_user_p4dp(p4d_t *p4dp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_set_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline p4d_t *user_to_kernel_p4dp(p4d_t *p4dp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ptr_clear_bit(p4dp, PTI_PGTABLE_SWITCH_BIT);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_PAGE_TABLE_ISOLATION */</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Page table pages are page-aligned.  The lower half of the top</span>
<span class="p_add">+ * level is used for userspace and the top half for the kernel.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns true for parts of the PGD that map userspace and</span>
<span class="p_add">+ * false for the parts that map the kernel.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline bool pgdp_maps_userspace(void *__ptr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long ptr = (unsigned long)__ptr;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (ptr &amp; ~PAGE_MASK) &lt; (PAGE_SIZE / 2);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+pgd_t __pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Take a PGD location (pgdp) and a pgd value that needs to be set there.</span>
<span class="p_add">+ * Populates the user and returns the resulting PGD that must be set in</span>
<span class="p_add">+ * the kernel copy of the page tables.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline pgd_t pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return pgd;</span>
<span class="p_add">+	return __pti_set_user_pgd(pgdp, pgd);</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline pgd_t pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 static inline void native_set_p4d(p4d_t *p4dp, p4d_t p4d)
 {
<span class="p_add">+#if defined(CONFIG_PAGE_TABLE_ISOLATION) &amp;&amp; !defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	p4dp-&gt;pgd = pti_set_user_pgd(&amp;p4dp-&gt;pgd, p4d.pgd);</span>
<span class="p_add">+#else</span>
 	*p4dp = p4d;
<span class="p_add">+#endif</span>
 }
 
 static inline void native_p4d_clear(p4d_t *p4d)
<span class="p_chunk">@@ -147,7 +235,11 @@</span> <span class="p_context"> static inline void native_p4d_clear(p4d_t *p4d)</span>
 
 static inline void native_set_pgd(pgd_t *pgdp, pgd_t pgd)
 {
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	*pgdp = pti_set_user_pgd(pgdp, pgd);</span>
<span class="p_add">+#else</span>
 	*pgdp = pgd;
<span class="p_add">+#endif</span>
 }
 
 static inline void native_pgd_clear(pgd_t *pgd)
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64_types.h b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">index 3d27831bc58d..b97a539bcdee 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64_types.h</span>
<span class="p_chunk">@@ -79,13 +79,17 @@</span> <span class="p_context"> typedef struct { pteval_t pte; } pte_t;</span>
 #define MAXMEM			_AC(__AC(1, UL) &lt;&lt; MAX_PHYSMEM_BITS, UL)
 
 #ifdef CONFIG_X86_5LEVEL
<span class="p_del">-# define VMALLOC_SIZE_TB	_AC(16384, UL)</span>
<span class="p_del">-# define __VMALLOC_BASE		_AC(0xff92000000000000, UL)</span>
<span class="p_add">+# define VMALLOC_SIZE_TB	_AC(12800, UL)</span>
<span class="p_add">+# define __VMALLOC_BASE		_AC(0xffa0000000000000, UL)</span>
 # define __VMEMMAP_BASE		_AC(0xffd4000000000000, UL)
<span class="p_add">+# define LDT_PGD_ENTRY		_AC(-112, UL)</span>
<span class="p_add">+# define LDT_BASE_ADDR		(LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #else
 # define VMALLOC_SIZE_TB	_AC(32, UL)
 # define __VMALLOC_BASE		_AC(0xffffc90000000000, UL)
 # define __VMEMMAP_BASE		_AC(0xffffea0000000000, UL)
<span class="p_add">+# define LDT_PGD_ENTRY		_AC(-4, UL)</span>
<span class="p_add">+# define LDT_BASE_ADDR		(LDT_PGD_ENTRY &lt;&lt; PGDIR_SHIFT)</span>
 #endif
 
 #ifdef CONFIG_RANDOMIZE_MEMORY
<span class="p_header">diff --git a/arch/x86/include/asm/processor-flags.h b/arch/x86/include/asm/processor-flags.h</span>
<span class="p_header">index 43212a43ee69..6a60fea90b9d 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor-flags.h</span>
<span class="p_chunk">@@ -38,6 +38,11 @@</span> <span class="p_context"></span>
 #define CR3_ADDR_MASK	__sme_clr(0x7FFFFFFFFFFFF000ull)
 #define CR3_PCID_MASK	0xFFFull
 #define CR3_NOFLUSH	BIT_ULL(63)
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+# define X86_CR3_PTI_SWITCH_BIT	11</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #else
 /*
  * CR3_ADDR_MASK needs at least bits 31:5 set on PAE systems, and we save
<span class="p_header">diff --git a/arch/x86/include/asm/processor.h b/arch/x86/include/asm/processor.h</span>
<span class="p_header">index 9e482d8b0b97..9c18da64daa9 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/processor.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/processor.h</span>
<span class="p_chunk">@@ -851,13 +851,22 @@</span> <span class="p_context"> static inline void spin_lock_prefetch(const void *x)</span>
 
 #else
 /*
<span class="p_del">- * User space process size. 47bits minus one guard page.  The guard</span>
<span class="p_del">- * page is necessary on Intel CPUs: if a SYSCALL instruction is at</span>
<span class="p_del">- * the highest possible canonical userspace address, then that</span>
<span class="p_del">- * syscall will enter the kernel with a non-canonical return</span>
<span class="p_del">- * address, and SYSRET will explode dangerously.  We avoid this</span>
<span class="p_del">- * particular problem by preventing anything from being mapped</span>
<span class="p_del">- * at the maximum canonical address.</span>
<span class="p_add">+ * User space process size.  This is the first address outside the user range.</span>
<span class="p_add">+ * There are a few constraints that determine this:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On Intel CPUs, if a SYSCALL instruction is at the highest canonical</span>
<span class="p_add">+ * address, then that syscall will enter the kernel with a</span>
<span class="p_add">+ * non-canonical return address, and SYSRET will explode dangerously.</span>
<span class="p_add">+ * We avoid this particular problem by preventing anything executable</span>
<span class="p_add">+ * from being mapped at the maximum canonical address.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * On AMD CPUs in the Ryzen family, there&#39;s a nasty bug in which the</span>
<span class="p_add">+ * CPUs malfunction if they execute code from the highest canonical page.</span>
<span class="p_add">+ * They&#39;ll speculate right off the end of the canonical space, and</span>
<span class="p_add">+ * bad things happen.  This is worked around in the same way as the</span>
<span class="p_add">+ * Intel problem.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * With page table isolation enabled, we map the LDT in ... [stay tuned]</span>
  */
 #define TASK_SIZE_MAX	((1UL &lt;&lt; __VIRTUAL_MASK_SHIFT) - PAGE_SIZE)
 
<span class="p_header">diff --git a/arch/x86/include/asm/pti.h b/arch/x86/include/asm/pti.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0b5ef05b2d2d</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/include/asm/pti.h</span>
<span class="p_chunk">@@ -0,0 +1,14 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+#ifndef _ASM_X86_PTI_H</span>
<span class="p_add">+#define _ASM_X86_PTI_H</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+extern void pti_init(void);</span>
<span class="p_add">+extern void pti_check_boottime_disable(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void pti_check_boottime_disable(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* __ASSEMBLY__ */</span>
<span class="p_add">+#endif /* _ASM_X86_PTI_H */</span>
<span class="p_header">diff --git a/arch/x86/include/asm/tlbflush.h b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">index 171b429f43a2..f9b48ce152eb 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/tlbflush.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/tlbflush.h</span>
<span class="p_chunk">@@ -10,38 +10,90 @@</span> <span class="p_context"></span>
 #include &lt;asm/special_insns.h&gt;
 #include &lt;asm/smp.h&gt;
 #include &lt;asm/invpcid.h&gt;
<span class="p_add">+#include &lt;asm/pti.h&gt;</span>
<span class="p_add">+#include &lt;asm/processor-flags.h&gt;</span>
 
<span class="p_del">-static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
<span class="p_del">-{</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Bump the generation count.  This also serves as a full barrier</span>
<span class="p_del">-	 * that synchronizes with switch_mm(): callers are required to order</span>
<span class="p_del">-	 * their read of mm_cpumask after their writes to the paging</span>
<span class="p_del">-	 * structures.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	return atomic64_inc_return(&amp;mm-&gt;context.tlb_gen);</span>
<span class="p_del">-}</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * The x86 feature is called PCID (Process Context IDentifier). It is similar</span>
<span class="p_add">+ * to what is traditionally called ASID on the RISC processors.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We don&#39;t use the traditional ASID implementation, where each process/mm gets</span>
<span class="p_add">+ * its own ASID and flush/restart when we run out of ASID space.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Instead we have a small per-cpu array of ASIDs and cache the last few mm&#39;s</span>
<span class="p_add">+ * that came by on this CPU, allowing cheaper switch_mm between processes on</span>
<span class="p_add">+ * this CPU.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * We end up with different spaces for different things. To avoid confusion we</span>
<span class="p_add">+ * use different names for each of them:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * ASID  - [0, TLB_NR_DYN_ASIDS-1]</span>
<span class="p_add">+ *         the canonical identifier for an mm</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * kPCID - [1, TLB_NR_DYN_ASIDS]</span>
<span class="p_add">+ *         the value we write into the PCID part of CR3; corresponds to the</span>
<span class="p_add">+ *         ASID+1, because PCID 0 is special.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * uPCID - [2048 + 1, 2048 + TLB_NR_DYN_ASIDS]</span>
<span class="p_add">+ *         for KPTI each mm has two address spaces and thus needs two</span>
<span class="p_add">+ *         PCID values, but we can still do with a single ASID denomination</span>
<span class="p_add">+ *         for each mm. Corresponds to kPCID + 2048.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ */</span>
 
 /* There are 12 bits of space for ASIDS in CR3 */
 #define CR3_HW_ASID_BITS		12
<span class="p_add">+</span>
 /*
  * When enabled, PAGE_TABLE_ISOLATION consumes a single bit for
  * user/kernel switches
  */
<span class="p_del">-#define PTI_CONSUMED_ASID_BITS		0</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+# define PTI_CONSUMED_PCID_BITS	1</span>
<span class="p_add">+#else</span>
<span class="p_add">+# define PTI_CONSUMED_PCID_BITS	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#define CR3_AVAIL_PCID_BITS (X86_CR3_PCID_BITS - PTI_CONSUMED_PCID_BITS)</span>
 
<span class="p_del">-#define CR3_AVAIL_ASID_BITS (CR3_HW_ASID_BITS - PTI_CONSUMED_ASID_BITS)</span>
 /*
  * ASIDs are zero-based: 0-&gt;MAX_AVAIL_ASID are valid.  -1 below to account
<span class="p_del">- * for them being zero-based.  Another -1 is because ASID 0 is reserved for</span>
<span class="p_add">+ * for them being zero-based.  Another -1 is because PCID 0 is reserved for</span>
  * use by non-PCID-aware users.
  */
<span class="p_del">-#define MAX_ASID_AVAILABLE ((1 &lt;&lt; CR3_AVAIL_ASID_BITS) - 2)</span>
<span class="p_add">+#define MAX_ASID_AVAILABLE ((1 &lt;&lt; CR3_AVAIL_PCID_BITS) - 2)</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * 6 because 6 should be plenty and struct tlb_state will fit in two cache</span>
<span class="p_add">+ * lines.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define TLB_NR_DYN_ASIDS	6</span>
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Given @asid, compute kPCID</span>
<span class="p_add">+ */</span>
 static inline u16 kern_pcid(u16 asid)
 {
 	VM_WARN_ON_ONCE(asid &gt; MAX_ASID_AVAILABLE);
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Make sure that the dynamic ASID space does not confict with the</span>
<span class="p_add">+	 * bit we are using to switch between user and kernel ASIDs.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	BUILD_BUG_ON(TLB_NR_DYN_ASIDS &gt;= (1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT));</span>
<span class="p_add">+</span>
 	/*
<span class="p_add">+	 * The ASID being passed in here should have respected the</span>
<span class="p_add">+	 * MAX_ASID_AVAILABLE and thus never have the switch bit set.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	VM_WARN_ON_ONCE(asid &amp; (1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT));</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The dynamically-assigned ASIDs that get passed in are small</span>
<span class="p_add">+	 * (&lt;TLB_NR_DYN_ASIDS).  They never have the high switch bit set,</span>
<span class="p_add">+	 * so do not bother to clear it.</span>
<span class="p_add">+	 *</span>
 	 * If PCID is on, ASID-aware code paths put the ASID+1 into the
 	 * PCID bits.  This serves two purposes.  It prevents a nasty
 	 * situation in which PCID-unaware code saves CR3, loads some other
<span class="p_chunk">@@ -53,6 +105,18 @@</span> <span class="p_context"> static inline u16 kern_pcid(u16 asid)</span>
 	return asid + 1;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Given @asid, compute uPCID</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline u16 user_pcid(u16 asid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u16 ret = kern_pcid(asid);</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	ret |= 1 &lt;&lt; X86_CR3_PTI_SWITCH_BIT;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return ret;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 struct pgd_t;
 static inline unsigned long build_cr3(pgd_t *pgd, u16 asid)
 {
<span class="p_chunk">@@ -95,12 +159,6 @@</span> <span class="p_context"> static inline bool tlb_defer_switch_to_init_mm(void)</span>
 	return !static_cpu_has(X86_FEATURE_PCID);
 }
 
<span class="p_del">-/*</span>
<span class="p_del">- * 6 because 6 should be plenty and struct tlb_state will fit in</span>
<span class="p_del">- * two cache lines.</span>
<span class="p_del">- */</span>
<span class="p_del">-#define TLB_NR_DYN_ASIDS 6</span>
<span class="p_del">-</span>
 struct tlb_context {
 	u64 ctx_id;
 	u64 tlb_gen;
<span class="p_chunk">@@ -134,6 +192,24 @@</span> <span class="p_context"> struct tlb_state {</span>
 	 */
 	bool is_lazy;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If set we changed the page tables in such a way that we</span>
<span class="p_add">+	 * needed an invalidation of all contexts (aka. PCIDs / ASIDs).</span>
<span class="p_add">+	 * This tells us to go invalidate all the non-loaded ctxs[]</span>
<span class="p_add">+	 * on the next context switch.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * The current ctx was kept up-to-date as it ran and does not</span>
<span class="p_add">+	 * need to be invalidated.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	bool invalidate_other;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Mask that contains TLB_NR_DYN_ASIDS+1 bits to indicate</span>
<span class="p_add">+	 * the corresponding user PCID needs a flush next time we</span>
<span class="p_add">+	 * switch to it; see SWITCH_TO_USER_CR3.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	unsigned short user_pcid_flush_mask;</span>
<span class="p_add">+</span>
 	/*
 	 * Access to this CR4 shadow and to H/W CR4 is protected by
 	 * disabling interrupts when modifying either one.
<span class="p_chunk">@@ -211,6 +287,14 @@</span> <span class="p_context"> static inline unsigned long cr4_read_shadow(void)</span>
 	return this_cpu_read(cpu_tlbstate.cr4);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Mark all other ASIDs as invalid, preserves the current.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void invalidate_other_asid(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.invalidate_other, true);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Save some of cr4 feature set we&#39;re using (e.g.  Pentium 4MB
  * enable and PPro Global page enable), so that any CPU&#39;s that boot
<span class="p_chunk">@@ -230,19 +314,48 @@</span> <span class="p_context"> static inline void cr4_set_bits_and_update_boot(unsigned long mask)</span>
 
 extern void initialize_tlbstate_and_flush(void);
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Given an ASID, flush the corresponding user ASID.  We can delay this</span>
<span class="p_add">+ * until the next time we switch to it.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * See SWITCH_TO_USER_CR3.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline void invalidate_user_asid(u16 asid)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* There is no user ASID if address space separation is off */</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * We only have a single ASID if PCID is off and the CR3</span>
<span class="p_add">+	 * write will have flushed it.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!cpu_feature_enabled(X86_FEATURE_PCID))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	__set_bit(kern_pcid(asid),</span>
<span class="p_add">+		  (unsigned long *)this_cpu_ptr(&amp;cpu_tlbstate.user_pcid_flush_mask));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * flush the entire current user mapping
  */
 static inline void __native_flush_tlb(void)
 {
 	/*
<span class="p_del">-	 * If current-&gt;mm == NULL then we borrow a mm which may change during a</span>
<span class="p_del">-	 * task switch and therefore we must not be preempted while we write CR3</span>
<span class="p_del">-	 * back:</span>
<span class="p_add">+	 * Preemption or interrupts must be disabled to protect the access</span>
<span class="p_add">+	 * to the per CPU variable and to prevent being preempted between</span>
<span class="p_add">+	 * read_cr3() and write_cr3().</span>
 	 */
<span class="p_del">-	preempt_disable();</span>
<span class="p_add">+	WARN_ON_ONCE(preemptible());</span>
<span class="p_add">+</span>
<span class="p_add">+	invalidate_user_asid(this_cpu_read(cpu_tlbstate.loaded_mm_asid));</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If current-&gt;mm == NULL then the read_cr3() &quot;borrows&quot; an mm */</span>
 	native_write_cr3(__native_read_cr3());
<span class="p_del">-	preempt_enable();</span>
 }
 
 /*
<span class="p_chunk">@@ -256,6 +369,8 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
 		/*
 		 * Using INVPCID is considerably faster than a pair of writes
 		 * to CR4 sandwiched inside an IRQ flag save/restore.
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Note, this works with CR4.PCIDE=0 or 1.</span>
 		 */
 		invpcid_flush_all();
 		return;
<span class="p_chunk">@@ -282,7 +397,21 @@</span> <span class="p_context"> static inline void __native_flush_tlb_global(void)</span>
  */
 static inline void __native_flush_tlb_single(unsigned long addr)
 {
<span class="p_add">+	u32 loaded_mm_asid = this_cpu_read(cpu_tlbstate.loaded_mm_asid);</span>
<span class="p_add">+</span>
 	asm volatile(&quot;invlpg (%0)&quot; ::&quot;r&quot; (addr) : &quot;memory&quot;);
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Some platforms #GP if we call invpcid(type=1/2) before CR4.PCIDE=1.</span>
<span class="p_add">+	 * Just use invalidate_user_asid() in case we are called early.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!this_cpu_has(X86_FEATURE_INVPCID_SINGLE))</span>
<span class="p_add">+		invalidate_user_asid(loaded_mm_asid);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		invpcid_flush_one(user_pcid(loaded_mm_asid), addr);</span>
 }
 
 /*
<span class="p_chunk">@@ -298,14 +427,6 @@</span> <span class="p_context"> static inline void __flush_tlb_all(void)</span>
 		 */
 		__flush_tlb();
 	}
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Note: if we somehow had PCID but not PGE, then this wouldn&#39;t work --</span>
<span class="p_del">-	 * we&#39;d end up flushing kernel translations for the current ASID but</span>
<span class="p_del">-	 * we might fail to flush kernel translations for other cached ASIDs.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * To avoid this issue, we force PCID off if PGE is off.</span>
<span class="p_del">-	 */</span>
 }
 
 /*
<span class="p_chunk">@@ -315,6 +436,16 @@</span> <span class="p_context"> static inline void __flush_tlb_one(unsigned long addr)</span>
 {
 	count_vm_tlb_event(NR_TLB_LOCAL_FLUSH_ONE);
 	__flush_tlb_single(addr);
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * __flush_tlb_single() will have cleared the TLB entry for this ASID,</span>
<span class="p_add">+	 * but since kernel space is replicated across all, we must also</span>
<span class="p_add">+	 * invalidate all others.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	invalidate_other_asid();</span>
 }
 
 #define TLB_FLUSH_ALL	-1UL
<span class="p_chunk">@@ -375,6 +506,17 @@</span> <span class="p_context"> static inline void flush_tlb_page(struct vm_area_struct *vma, unsigned long a)</span>
 void native_flush_tlb_others(const struct cpumask *cpumask,
 			     const struct flush_tlb_info *info);
 
<span class="p_add">+static inline u64 inc_mm_tlb_gen(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Bump the generation count.  This also serves as a full barrier</span>
<span class="p_add">+	 * that synchronizes with switch_mm(): callers are required to order</span>
<span class="p_add">+	 * their read of mm_cpumask after their writes to the paging</span>
<span class="p_add">+	 * structures.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	return atomic64_inc_return(&amp;mm-&gt;context.tlb_gen);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline void arch_tlbbatch_add_mm(struct arch_tlbflush_unmap_batch *batch,
 					struct mm_struct *mm)
 {
<span class="p_header">diff --git a/arch/x86/include/asm/vsyscall.h b/arch/x86/include/asm/vsyscall.h</span>
<span class="p_header">index d9a7c659009c..b986b2ca688a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/vsyscall.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/vsyscall.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 
 #ifdef CONFIG_X86_VSYSCALL_EMULATION
 extern void map_vsyscall(void);
<span class="p_add">+extern void set_vsyscall_pgtable_user_bits(pgd_t *root);</span>
 
 /*
  * Called on instruction fetch fault in vsyscall page.
<span class="p_header">diff --git a/arch/x86/include/uapi/asm/processor-flags.h b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">index 53b4ca55ebb6..97abdaab9535 100644</span>
<span class="p_header">--- a/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_header">+++ b/arch/x86/include/uapi/asm/processor-flags.h</span>
<span class="p_chunk">@@ -78,7 +78,12 @@</span> <span class="p_context"></span>
 #define X86_CR3_PWT		_BITUL(X86_CR3_PWT_BIT)
 #define X86_CR3_PCD_BIT		4 /* Page Cache Disable */
 #define X86_CR3_PCD		_BITUL(X86_CR3_PCD_BIT)
<span class="p_del">-#define X86_CR3_PCID_MASK	_AC(0x00000fff,UL) /* PCID Mask */</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_CR3_PCID_BITS	12</span>
<span class="p_add">+#define X86_CR3_PCID_MASK	(_AC((1UL &lt;&lt; X86_CR3_PCID_BITS) - 1, UL))</span>
<span class="p_add">+</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH_BIT 63 /* Preserve old PCID */</span>
<span class="p_add">+#define X86_CR3_PCID_NOFLUSH    _BITULL(X86_CR3_PCID_NOFLUSH_BIT)</span>
 
 /*
  * Intel CPU features in CR4
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">index 676b7cf4b62b..76417a9aab73 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_chunk">@@ -17,6 +17,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/sigframe.h&gt;
 #include &lt;asm/bootparam.h&gt;
 #include &lt;asm/suspend.h&gt;
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
 
 #ifdef CONFIG_XEN
 #include &lt;xen/interface/xen.h&gt;
<span class="p_chunk">@@ -94,6 +95,9 @@</span> <span class="p_context"> void common(void) {</span>
 	BLANK();
 	DEFINE(PTREGS_SIZE, sizeof(struct pt_regs));
 
<span class="p_add">+	/* TLB state for the entry code */</span>
<span class="p_add">+	OFFSET(TLB_STATE_user_pcid_flush_mask, tlb_state, user_pcid_flush_mask);</span>
<span class="p_add">+</span>
 	/* Layout info for cpu_entry_area */
 	OFFSET(CPU_ENTRY_AREA_tss, cpu_entry_area, tss);
 	OFFSET(CPU_ENTRY_AREA_entry_trampoline, cpu_entry_area, entry_trampoline);
<span class="p_header">diff --git a/arch/x86/kernel/cpu/common.c b/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">index 8ddcfa4d4165..f2a94dfb434e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/cpu/common.c</span>
<span class="p_header">+++ b/arch/x86/kernel/cpu/common.c</span>
<span class="p_chunk">@@ -898,6 +898,10 @@</span> <span class="p_context"> static void __init early_identify_cpu(struct cpuinfo_x86 *c)</span>
 	}
 
 	setup_force_cpu_cap(X86_FEATURE_ALWAYS);
<span class="p_add">+</span>
<span class="p_add">+	/* Assume for now that ALL x86 CPUs are insecure */</span>
<span class="p_add">+	setup_force_cpu_bug(X86_BUG_CPU_INSECURE);</span>
<span class="p_add">+</span>
 	fpu__init_system(c);
 
 #ifdef CONFIG_X86_32
<span class="p_chunk">@@ -1335,7 +1339,10 @@</span> <span class="p_context"> void syscall_init(void)</span>
 		(entry_SYSCALL_64_trampoline - _entry_trampoline);
 
 	wrmsr(MSR_STAR, 0, (__USER32_CS &lt;&lt; 16) | __KERNEL_CS);
<span class="p_del">-	wrmsrl(MSR_LSTAR, SYSCALL64_entry_trampoline);</span>
<span class="p_add">+	if (static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		wrmsrl(MSR_LSTAR, SYSCALL64_entry_trampoline);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		wrmsrl(MSR_LSTAR, (unsigned long)entry_SYSCALL_64);</span>
 
 #ifdef CONFIG_IA32_EMULATION
 	wrmsrl(MSR_CSTAR, (unsigned long)entry_SYSCALL_compat);
<span class="p_header">diff --git a/arch/x86/kernel/dumpstack.c b/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">index 36b17e0febe8..5fa110699ed2 100644</span>
<span class="p_header">--- a/arch/x86/kernel/dumpstack.c</span>
<span class="p_header">+++ b/arch/x86/kernel/dumpstack.c</span>
<span class="p_chunk">@@ -297,11 +297,13 @@</span> <span class="p_context"> int __die(const char *str, struct pt_regs *regs, long err)</span>
 	unsigned long sp;
 #endif
 	printk(KERN_DEFAULT
<span class="p_del">-	       &quot;%s: %04lx [#%d]%s%s%s%s\n&quot;, str, err &amp; 0xffff, ++die_counter,</span>
<span class="p_add">+	       &quot;%s: %04lx [#%d]%s%s%s%s%s\n&quot;, str, err &amp; 0xffff, ++die_counter,</span>
 	       IS_ENABLED(CONFIG_PREEMPT) ? &quot; PREEMPT&quot;         : &quot;&quot;,
 	       IS_ENABLED(CONFIG_SMP)     ? &quot; SMP&quot;             : &quot;&quot;,
 	       debug_pagealloc_enabled()  ? &quot; DEBUG_PAGEALLOC&quot; : &quot;&quot;,
<span class="p_del">-	       IS_ENABLED(CONFIG_KASAN)   ? &quot; KASAN&quot;           : &quot;&quot;);</span>
<span class="p_add">+	       IS_ENABLED(CONFIG_KASAN)   ? &quot; KASAN&quot;           : &quot;&quot;,</span>
<span class="p_add">+	       IS_ENABLED(CONFIG_PAGE_TABLE_ISOLATION) ?</span>
<span class="p_add">+	       (boot_cpu_has(X86_FEATURE_PTI) ? &quot; PTI&quot; : &quot; NOPTI&quot;) : &quot;&quot;);</span>
 
 	if (notify_die(DIE_OOPS, str, regs, err,
 			current-&gt;thread.trap_nr, SIGSEGV) == NOTIFY_STOP)
<span class="p_header">diff --git a/arch/x86/kernel/head_64.S b/arch/x86/kernel/head_64.S</span>
<span class="p_header">index 7dca675fe78d..04a625f0fcda 100644</span>
<span class="p_header">--- a/arch/x86/kernel/head_64.S</span>
<span class="p_header">+++ b/arch/x86/kernel/head_64.S</span>
<span class="p_chunk">@@ -341,6 +341,27 @@</span> <span class="p_context"> GLOBAL(early_recursion_flag)</span>
 	.balign	PAGE_SIZE; \
 GLOBAL(name)
 
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Each PGD needs to be 8k long and 8k aligned.  We do not</span>
<span class="p_add">+ * ever go out to userspace with these, so we do not</span>
<span class="p_add">+ * strictly *need* the second page, but this allows us to</span>
<span class="p_add">+ * have a single set_pgd() implementation that does not</span>
<span class="p_add">+ * need to worry about whether it has 4k or 8k to work</span>
<span class="p_add">+ * with.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This ensures PGDs are 8k long:</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define PTI_USER_PGD_FILL	512</span>
<span class="p_add">+/* This ensures they are 8k-aligned: */</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) \</span>
<span class="p_add">+	.balign 2 * PAGE_SIZE; \</span>
<span class="p_add">+GLOBAL(name)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define NEXT_PGD_PAGE(name) NEXT_PAGE(name)</span>
<span class="p_add">+#define PTI_USER_PGD_FILL	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* Automate the creation of 1 to 1 mapping pmd entries */
 #define PMDS(START, PERM, COUNT)			\
 	i = 0 ;						\
<span class="p_chunk">@@ -350,13 +371,14 @@</span> <span class="p_context"> GLOBAL(name)</span>
 	.endr
 
 	__INITDATA
<span class="p_del">-NEXT_PAGE(early_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(early_top_pgt)</span>
 	.fill	511,8,0
 #ifdef CONFIG_X86_5LEVEL
 	.quad	level4_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC
 #else
 	.quad	level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC
 #endif
<span class="p_add">+	.fill	PTI_USER_PGD_FILL,8,0</span>
 
 NEXT_PAGE(early_dynamic_pgts)
 	.fill	512*EARLY_DYNAMIC_PAGE_TABLES,8,0
<span class="p_chunk">@@ -364,13 +386,14 @@</span> <span class="p_context"> NEXT_PAGE(early_dynamic_pgts)</span>
 	.data
 
 #if defined(CONFIG_XEN_PV) || defined(CONFIG_XEN_PVH)
<span class="p_del">-NEXT_PAGE(init_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_top_pgt)</span>
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
 	.org    init_top_pgt + PGD_PAGE_OFFSET*8, 0
 	.quad   level3_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
 	.org    init_top_pgt + PGD_START_KERNEL*8, 0
 	/* (2^48-(2*1024*1024*1024))/(2^39) = 511 */
 	.quad   level3_kernel_pgt - __START_KERNEL_map + _PAGE_TABLE_NOENC
<span class="p_add">+	.fill	PTI_USER_PGD_FILL,8,0</span>
 
 NEXT_PAGE(level3_ident_pgt)
 	.quad	level2_ident_pgt - __START_KERNEL_map + _KERNPG_TABLE_NOENC
<span class="p_chunk">@@ -381,8 +404,9 @@</span> <span class="p_context"> NEXT_PAGE(level2_ident_pgt)</span>
 	 */
 	PMDS(0, __PAGE_KERNEL_IDENT_LARGE_EXEC, PTRS_PER_PMD)
 #else
<span class="p_del">-NEXT_PAGE(init_top_pgt)</span>
<span class="p_add">+NEXT_PGD_PAGE(init_top_pgt)</span>
 	.fill	512,8,0
<span class="p_add">+	.fill	PTI_USER_PGD_FILL,8,0</span>
 #endif
 
 #ifdef CONFIG_X86_5LEVEL
<span class="p_header">diff --git a/arch/x86/kernel/ldt.c b/arch/x86/kernel/ldt.c</span>
<span class="p_header">index a6b5d62f45a7..26d713ecad34 100644</span>
<span class="p_header">--- a/arch/x86/kernel/ldt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/ldt.c</span>
<span class="p_chunk">@@ -24,6 +24,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/uaccess.h&gt;
 
 #include &lt;asm/ldt.h&gt;
<span class="p_add">+#include &lt;asm/tlb.h&gt;</span>
 #include &lt;asm/desc.h&gt;
 #include &lt;asm/mmu_context.h&gt;
 #include &lt;asm/syscalls.h&gt;
<span class="p_chunk">@@ -51,13 +52,11 @@</span> <span class="p_context"> static void refresh_ldt_segments(void)</span>
 static void flush_ldt(void *__mm)
 {
 	struct mm_struct *mm = __mm;
<span class="p_del">-	mm_context_t *pc;</span>
 
 	if (this_cpu_read(cpu_tlbstate.loaded_mm) != mm)
 		return;
 
<span class="p_del">-	pc = &amp;mm-&gt;context;</span>
<span class="p_del">-	set_ldt(pc-&gt;ldt-&gt;entries, pc-&gt;ldt-&gt;nr_entries);</span>
<span class="p_add">+	load_mm_ldt(mm);</span>
 
 	refresh_ldt_segments();
 }
<span class="p_chunk">@@ -94,10 +93,126 @@</span> <span class="p_context"> static struct ldt_struct *alloc_ldt_struct(unsigned int num_entries)</span>
 		return NULL;
 	}
 
<span class="p_add">+	/* The new LDT isn&#39;t aliased for PTI yet. */</span>
<span class="p_add">+	new_ldt-&gt;slot = -1;</span>
<span class="p_add">+</span>
 	new_ldt-&gt;nr_entries = num_entries;
 	return new_ldt;
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * If PTI is enabled, this maps the LDT into the kernelmode and</span>
<span class="p_add">+ * usermode tables for the given mm.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * There is no corresponding unmap function.  Even if the LDT is freed, we</span>
<span class="p_add">+ * leave the PTEs around until the slot is reused or the mm is destroyed.</span>
<span class="p_add">+ * This is harmless: the LDT is always in ordinary memory, and no one will</span>
<span class="p_add">+ * access the freed slot.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * If we wanted to unmap freed LDTs, we&#39;d also need to do a flush to make</span>
<span class="p_add">+ * it useful, and the flush would slow down modify_ldt().</span>
<span class="p_add">+ */</span>
<span class="p_add">+static int</span>
<span class="p_add">+map_ldt_struct(struct mm_struct *mm, struct ldt_struct *ldt, int slot)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	bool is_vmalloc, had_top_level_entry;</span>
<span class="p_add">+	unsigned long va;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	pgd_t *pgd;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Any given ldt_struct should have map_ldt_struct() called at most</span>
<span class="p_add">+	 * once.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WARN_ON(ldt-&gt;slot != -1);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Did we already have the top level entry allocated?  We can&#39;t</span>
<span class="p_add">+	 * use pgd_none() for this because it doens&#39;t do anything on</span>
<span class="p_add">+	 * 4-level page table kernels.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgd = pgd_offset(mm, LDT_BASE_ADDR);</span>
<span class="p_add">+	had_top_level_entry = (pgd-&gt;pgd != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	is_vmalloc = is_vmalloc_addr(ldt-&gt;entries);</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i * PAGE_SIZE &lt; ldt-&gt;nr_entries * LDT_ENTRY_SIZE; i++) {</span>
<span class="p_add">+		unsigned long offset = i &lt;&lt; PAGE_SHIFT;</span>
<span class="p_add">+		const void *src = (char *)ldt-&gt;entries + offset;</span>
<span class="p_add">+		unsigned long pfn;</span>
<span class="p_add">+		pte_t pte, *ptep;</span>
<span class="p_add">+</span>
<span class="p_add">+		va = (unsigned long)ldt_slot_va(slot) + offset;</span>
<span class="p_add">+		pfn = is_vmalloc ? vmalloc_to_pfn(src) :</span>
<span class="p_add">+			page_to_pfn(virt_to_page(src));</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Treat the PTI LDT range as a *userspace* range.</span>
<span class="p_add">+		 * get_locked_pte() will allocate all needed pagetables</span>
<span class="p_add">+		 * and account for them in this mm.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ptep = get_locked_pte(mm, va, &amp;ptl);</span>
<span class="p_add">+		if (!ptep)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Map it RO so the easy to find address is not a primary</span>
<span class="p_add">+		 * target via some kernel interface which misses a</span>
<span class="p_add">+		 * permission check.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		pte = pfn_pte(pfn, __pgprot(__PAGE_KERNEL_RO &amp; ~_PAGE_GLOBAL));</span>
<span class="p_add">+		set_pte_at(mm, va, ptep, pte);</span>
<span class="p_add">+		pte_unmap_unlock(ptep, ptl);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (mm-&gt;context.ldt) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We already had an LDT.  The top-level entry should already</span>
<span class="p_add">+		 * have been allocated and synchronized with the usermode</span>
<span class="p_add">+		 * tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(!had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+			WARN_ON(!kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This is the first time we&#39;re mapping an LDT for this process.</span>
<span class="p_add">+		 * Sync the pgd to the usermode tables.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		WARN_ON(had_top_level_entry);</span>
<span class="p_add">+		if (static_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="p_add">+			WARN_ON(kernel_to_user_pgdp(pgd)-&gt;pgd);</span>
<span class="p_add">+			set_pgd(kernel_to_user_pgdp(pgd), *pgd);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	va = (unsigned long)ldt_slot_va(slot);</span>
<span class="p_add">+	flush_tlb_mm_range(mm, va, va + LDT_SLOT_STRIDE, 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	ldt-&gt;slot = slot;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void free_ldt_pgtables(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	struct mmu_gather tlb;</span>
<span class="p_add">+	unsigned long start = LDT_BASE_ADDR;</span>
<span class="p_add">+	unsigned long end = start + (1UL &lt;&lt; PGDIR_SHIFT);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	tlb_gather_mmu(&amp;tlb, mm, start, end);</span>
<span class="p_add">+	free_pgd_range(&amp;tlb, start, end, start, end);</span>
<span class="p_add">+	tlb_finish_mmu(&amp;tlb, start, end);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* After calling this, the LDT is immutable. */
 static void finalize_ldt_struct(struct ldt_struct *ldt)
 {
<span class="p_chunk">@@ -156,6 +271,12 @@</span> <span class="p_context"> int ldt_dup_context(struct mm_struct *old_mm, struct mm_struct *mm)</span>
 	       new_ldt-&gt;nr_entries * LDT_ENTRY_SIZE);
 	finalize_ldt_struct(new_ldt);
 
<span class="p_add">+	retval = map_ldt_struct(mm, new_ldt, 0);</span>
<span class="p_add">+	if (retval) {</span>
<span class="p_add">+		free_ldt_pgtables(mm);</span>
<span class="p_add">+		free_ldt_struct(new_ldt);</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
 	mm-&gt;context.ldt = new_ldt;
 
 out_unlock:
<span class="p_chunk">@@ -174,6 +295,11 @@</span> <span class="p_context"> void destroy_context_ldt(struct mm_struct *mm)</span>
 	mm-&gt;context.ldt = NULL;
 }
 
<span class="p_add">+void ldt_arch_exit_mmap(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+	free_ldt_pgtables(mm);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int read_ldt(void __user *ptr, unsigned long bytecount)
 {
 	struct mm_struct *mm = current-&gt;mm;
<span class="p_chunk">@@ -287,6 +413,25 @@</span> <span class="p_context"> static int write_ldt(void __user *ptr, unsigned long bytecount, int oldmode)</span>
 	new_ldt-&gt;entries[ldt_info.entry_number] = ldt;
 	finalize_ldt_struct(new_ldt);
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If we are using PTI, map the new LDT into the userspace pagetables.</span>
<span class="p_add">+	 * If there is already an LDT, use the other slot so that other CPUs</span>
<span class="p_add">+	 * will continue to use the old LDT until install_ldt() switches</span>
<span class="p_add">+	 * them over to the new LDT.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	error = map_ldt_struct(mm, new_ldt, old_ldt ? !old_ldt-&gt;slot : 0);</span>
<span class="p_add">+	if (error) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This only can fail for the first LDT setup. If an LDT is</span>
<span class="p_add">+		 * already installed then the PTE page is already</span>
<span class="p_add">+		 * populated. Mop up a half populated page table.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!WARN_ON_ONCE(old_ldt))</span>
<span class="p_add">+			free_ldt_pgtables(mm);</span>
<span class="p_add">+		free_ldt_struct(new_ldt);</span>
<span class="p_add">+		goto out_unlock;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	install_ldt(mm, new_ldt);
 	free_ldt_struct(old_ldt);
 	error = 0;
<span class="p_header">diff --git a/arch/x86/kernel/machine_kexec_32.c b/arch/x86/kernel/machine_kexec_32.c</span>
<span class="p_header">index 00bc751c861c..edfede768688 100644</span>
<span class="p_header">--- a/arch/x86/kernel/machine_kexec_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/machine_kexec_32.c</span>
<span class="p_chunk">@@ -48,8 +48,6 @@</span> <span class="p_context"> static void load_segments(void)</span>
 		&quot;\tmovl $&quot;STR(__KERNEL_DS)&quot;,%%eax\n&quot;
 		&quot;\tmovl %%eax,%%ds\n&quot;
 		&quot;\tmovl %%eax,%%es\n&quot;
<span class="p_del">-		&quot;\tmovl %%eax,%%fs\n&quot;</span>
<span class="p_del">-		&quot;\tmovl %%eax,%%gs\n&quot;</span>
 		&quot;\tmovl %%eax,%%ss\n&quot;
 		: : : &quot;eax&quot;, &quot;memory&quot;);
 #undef STR
<span class="p_chunk">@@ -232,8 +230,8 @@</span> <span class="p_context"> void machine_kexec(struct kimage *image)</span>
 	 * The gdt &amp; idt are now invalid.
 	 * If you want to load them you must set up your own idt &amp; gdt.
 	 */
<span class="p_del">-	set_gdt(phys_to_virt(0), 0);</span>
 	idt_invalidate(phys_to_virt(0));
<span class="p_add">+	set_gdt(phys_to_virt(0), 0);</span>
 
 	/* now call it */
 	image-&gt;start = relocate_kernel_ptr((unsigned long)image-&gt;head,
<span class="p_header">diff --git a/arch/x86/kernel/smpboot.c b/arch/x86/kernel/smpboot.c</span>
<span class="p_header">index 12bf07d44dfe..2651ca2112c4 100644</span>
<span class="p_header">--- a/arch/x86/kernel/smpboot.c</span>
<span class="p_header">+++ b/arch/x86/kernel/smpboot.c</span>
<span class="p_chunk">@@ -128,25 +128,16 @@</span> <span class="p_context"> static inline void smpboot_setup_warm_reset_vector(unsigned long start_eip)</span>
 	spin_lock_irqsave(&amp;rtc_lock, flags);
 	CMOS_WRITE(0xa, 0xf);
 	spin_unlock_irqrestore(&amp;rtc_lock, flags);
<span class="p_del">-	local_flush_tlb();</span>
<span class="p_del">-	pr_debug(&quot;1.\n&quot;);</span>
 	*((volatile unsigned short *)phys_to_virt(TRAMPOLINE_PHYS_HIGH)) =
 							start_eip &gt;&gt; 4;
<span class="p_del">-	pr_debug(&quot;2.\n&quot;);</span>
 	*((volatile unsigned short *)phys_to_virt(TRAMPOLINE_PHYS_LOW)) =
 							start_eip &amp; 0xf;
<span class="p_del">-	pr_debug(&quot;3.\n&quot;);</span>
 }
 
 static inline void smpboot_restore_warm_reset_vector(void)
 {
 	unsigned long flags;
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Install writable page 0 entry to set BIOS data area.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	local_flush_tlb();</span>
<span class="p_del">-</span>
 	/*
 	 * Paranoid:  Set warm reset code and vector here back
 	 * to default values.
<span class="p_header">diff --git a/arch/x86/kernel/tls.c b/arch/x86/kernel/tls.c</span>
<span class="p_header">index 9a9c9b076955..a5b802a12212 100644</span>
<span class="p_header">--- a/arch/x86/kernel/tls.c</span>
<span class="p_header">+++ b/arch/x86/kernel/tls.c</span>
<span class="p_chunk">@@ -93,17 +93,10 @@</span> <span class="p_context"> static void set_tls_desc(struct task_struct *p, int idx,</span>
 	cpu = get_cpu();
 
 	while (n-- &gt; 0) {
<span class="p_del">-		if (LDT_empty(info) || LDT_zero(info)) {</span>
<span class="p_add">+		if (LDT_empty(info) || LDT_zero(info))</span>
 			memset(desc, 0, sizeof(*desc));
<span class="p_del">-		} else {</span>
<span class="p_add">+		else</span>
 			fill_ldt(desc, info);
<span class="p_del">-</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * Always set the accessed bit so that the CPU</span>
<span class="p_del">-			 * doesn&#39;t try to write to the (read-only) GDT.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			desc-&gt;type |= 1;</span>
<span class="p_del">-		}</span>
 		++info;
 		++desc;
 	}
<span class="p_header">diff --git a/arch/x86/kernel/traps.c b/arch/x86/kernel/traps.c</span>
<span class="p_header">index 7c16fe0b60c2..b33e860d32fe 100644</span>
<span class="p_header">--- a/arch/x86/kernel/traps.c</span>
<span class="p_header">+++ b/arch/x86/kernel/traps.c</span>
<span class="p_chunk">@@ -361,7 +361,7 @@</span> <span class="p_context"> dotraplinkage void do_double_fault(struct pt_regs *regs, long error_code)</span>
 	 *
 	 * No need for ist_enter here because we don&#39;t use RCU.
 	 */
<span class="p_del">-	if (((long)regs-&gt;sp &gt;&gt; PGDIR_SHIFT) == ESPFIX_PGD_ENTRY &amp;&amp;</span>
<span class="p_add">+	if (((long)regs-&gt;sp &gt;&gt; P4D_SHIFT) == ESPFIX_PGD_ENTRY &amp;&amp;</span>
 		regs-&gt;cs == __KERNEL_CS &amp;&amp;
 		regs-&gt;ip == (unsigned long)native_irq_return_iret)
 	{
<span class="p_header">diff --git a/arch/x86/kernel/vmlinux.lds.S b/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_header">index d2a8b5a24a44..1e413a9326aa 100644</span>
<span class="p_header">--- a/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_header">+++ b/arch/x86/kernel/vmlinux.lds.S</span>
<span class="p_chunk">@@ -61,11 +61,17 @@</span> <span class="p_context"> jiffies_64 = jiffies;</span>
 		. = ALIGN(HPAGE_SIZE);				\
 		__end_rodata_hpage_align = .;
 
<span class="p_add">+#define ALIGN_ENTRY_TEXT_BEGIN	. = ALIGN(PMD_SIZE);</span>
<span class="p_add">+#define ALIGN_ENTRY_TEXT_END	. = ALIGN(PMD_SIZE);</span>
<span class="p_add">+</span>
 #else
 
 #define X64_ALIGN_RODATA_BEGIN
 #define X64_ALIGN_RODATA_END
 
<span class="p_add">+#define ALIGN_ENTRY_TEXT_BEGIN</span>
<span class="p_add">+#define ALIGN_ENTRY_TEXT_END</span>
<span class="p_add">+</span>
 #endif
 
 PHDRS {
<span class="p_chunk">@@ -102,8 +108,10 @@</span> <span class="p_context"> SECTIONS</span>
 		CPUIDLE_TEXT
 		LOCK_TEXT
 		KPROBES_TEXT
<span class="p_add">+		ALIGN_ENTRY_TEXT_BEGIN</span>
 		ENTRY_TEXT
 		IRQENTRY_TEXT
<span class="p_add">+		ALIGN_ENTRY_TEXT_END</span>
 		SOFTIRQENTRY_TEXT
 		*(.fixup)
 		*(.gnu.warning)
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 2e0017af8f9b..52906808e277 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -43,9 +43,10 @@</span> <span class="p_context"> obj-$(CONFIG_AMD_NUMA)		+= amdtopology.o</span>
 obj-$(CONFIG_ACPI_NUMA)		+= srat.o
 obj-$(CONFIG_NUMA_EMU)		+= numa_emulation.o
 
<span class="p_del">-obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o</span>
<span class="p_del">-obj-$(CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS) += pkeys.o</span>
<span class="p_del">-obj-$(CONFIG_RANDOMIZE_MEMORY) += kaslr.o</span>
<span class="p_add">+obj-$(CONFIG_X86_INTEL_MPX)			+= mpx.o</span>
<span class="p_add">+obj-$(CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS)	+= pkeys.o</span>
<span class="p_add">+obj-$(CONFIG_RANDOMIZE_MEMORY)			+= kaslr.o</span>
<span class="p_add">+obj-$(CONFIG_PAGE_TABLE_ISOLATION)		+= pti.o</span>
 
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt.o
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_boot.o
<span class="p_header">diff --git a/arch/x86/mm/cpu_entry_area.c b/arch/x86/mm/cpu_entry_area.c</span>
<span class="p_header">index fe814fd5e014..b9283cc27622 100644</span>
<span class="p_header">--- a/arch/x86/mm/cpu_entry_area.c</span>
<span class="p_header">+++ b/arch/x86/mm/cpu_entry_area.c</span>
<span class="p_chunk">@@ -38,6 +38,32 @@</span> <span class="p_context"> cea_map_percpu_pages(void *cea_vaddr, void *ptr, int pages, pgprot_t prot)</span>
 		cea_set_pte(cea_vaddr, per_cpu_ptr_to_phys(ptr), prot);
 }
 
<span class="p_add">+static void percpu_setup_debug_store(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_CPU_SUP_INTEL</span>
<span class="p_add">+	int npages;</span>
<span class="p_add">+	void *cea;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (boot_cpu_data.x86_vendor != X86_VENDOR_INTEL)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_store;</span>
<span class="p_add">+	npages = sizeof(struct debug_store) / PAGE_SIZE;</span>
<span class="p_add">+	BUILD_BUG_ON(sizeof(struct debug_store) % PAGE_SIZE != 0);</span>
<span class="p_add">+	cea_map_percpu_pages(cea, &amp;per_cpu(cpu_debug_store, cpu), npages,</span>
<span class="p_add">+			     PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	cea = &amp;get_cpu_entry_area(cpu)-&gt;cpu_debug_buffers;</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Force the population of PMDs for not yet allocated per cpu</span>
<span class="p_add">+	 * memory like debug store buffers.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	npages = sizeof(struct debug_store_buffers) / PAGE_SIZE;</span>
<span class="p_add">+	for (; npages; npages--, cea += PAGE_SIZE)</span>
<span class="p_add">+		cea_set_pte(cea, 0, PAGE_NONE);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* Setup the fixmap mappings only once per-processor */
 static void __init setup_cpu_entry_area(int cpu)
 {
<span class="p_chunk">@@ -109,6 +135,7 @@</span> <span class="p_context"> static void __init setup_cpu_entry_area(int cpu)</span>
 	cea_set_pte(&amp;get_cpu_entry_area(cpu)-&gt;entry_trampoline,
 		     __pa_symbol(_entry_trampoline), PAGE_KERNEL_RX);
 #endif
<span class="p_add">+	percpu_setup_debug_store(cpu);</span>
 }
 
 static __init void setup_cpu_entry_area_ptes(void)
<span class="p_header">diff --git a/arch/x86/mm/debug_pagetables.c b/arch/x86/mm/debug_pagetables.c</span>
<span class="p_header">index bfcffdf6c577..421f2664ffa0 100644</span>
<span class="p_header">--- a/arch/x86/mm/debug_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/debug_pagetables.c</span>
<span class="p_chunk">@@ -5,7 +5,7 @@</span> <span class="p_context"></span>
 
 static int ptdump_show(struct seq_file *m, void *v)
 {
<span class="p_del">-	ptdump_walk_pgd_level(m, NULL);</span>
<span class="p_add">+	ptdump_walk_pgd_level_debugfs(m, NULL, false);</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -22,21 +22,89 @@</span> <span class="p_context"> static const struct file_operations ptdump_fops = {</span>
 	.release	= single_release,
 };
 
<span class="p_del">-static struct dentry *pe;</span>
<span class="p_add">+static int ptdump_show_curknl(struct seq_file *m, void *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (current-&gt;mm-&gt;pgd) {</span>
<span class="p_add">+		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		ptdump_walk_pgd_level_debugfs(m, current-&gt;mm-&gt;pgd, false);</span>
<span class="p_add">+		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int ptdump_open_curknl(struct inode *inode, struct file *filp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return single_open(filp, ptdump_show_curknl, NULL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct file_operations ptdump_curknl_fops = {</span>
<span class="p_add">+	.owner		= THIS_MODULE,</span>
<span class="p_add">+	.open		= ptdump_open_curknl,</span>
<span class="p_add">+	.read		= seq_read,</span>
<span class="p_add">+	.llseek		= seq_lseek,</span>
<span class="p_add">+	.release	= single_release,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+static struct dentry *pe_curusr;</span>
<span class="p_add">+</span>
<span class="p_add">+static int ptdump_show_curusr(struct seq_file *m, void *v)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (current-&gt;mm-&gt;pgd) {</span>
<span class="p_add">+		down_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+		ptdump_walk_pgd_level_debugfs(m, current-&gt;mm-&gt;pgd, true);</span>
<span class="p_add">+		up_read(&amp;current-&gt;mm-&gt;mmap_sem);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int ptdump_open_curusr(struct inode *inode, struct file *filp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return single_open(filp, ptdump_show_curusr, NULL);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static const struct file_operations ptdump_curusr_fops = {</span>
<span class="p_add">+	.owner		= THIS_MODULE,</span>
<span class="p_add">+	.open		= ptdump_open_curusr,</span>
<span class="p_add">+	.read		= seq_read,</span>
<span class="p_add">+	.llseek		= seq_lseek,</span>
<span class="p_add">+	.release	= single_release,</span>
<span class="p_add">+};</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static struct dentry *dir, *pe_knl, *pe_curknl;</span>
 
 static int __init pt_dump_debug_init(void)
 {
<span class="p_del">-	pe = debugfs_create_file(&quot;kernel_page_tables&quot;, S_IRUSR, NULL, NULL,</span>
<span class="p_del">-				 &amp;ptdump_fops);</span>
<span class="p_del">-	if (!pe)</span>
<span class="p_add">+	dir = debugfs_create_dir(&quot;page_tables&quot;, NULL);</span>
<span class="p_add">+	if (!dir)</span>
 		return -ENOMEM;
 
<span class="p_add">+	pe_knl = debugfs_create_file(&quot;kernel&quot;, 0400, dir, NULL,</span>
<span class="p_add">+				     &amp;ptdump_fops);</span>
<span class="p_add">+	if (!pe_knl)</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+</span>
<span class="p_add">+	pe_curknl = debugfs_create_file(&quot;current_kernel&quot;, 0400,</span>
<span class="p_add">+					dir, NULL, &amp;ptdump_curknl_fops);</span>
<span class="p_add">+	if (!pe_curknl)</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	pe_curusr = debugfs_create_file(&quot;current_user&quot;, 0400,</span>
<span class="p_add">+					dir, NULL, &amp;ptdump_curusr_fops);</span>
<span class="p_add">+	if (!pe_curusr)</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+#endif</span>
 	return 0;
<span class="p_add">+err:</span>
<span class="p_add">+	debugfs_remove_recursive(dir);</span>
<span class="p_add">+	return -ENOMEM;</span>
 }
 
 static void __exit pt_dump_debug_exit(void)
 {
<span class="p_del">-	debugfs_remove_recursive(pe);</span>
<span class="p_add">+	debugfs_remove_recursive(dir);</span>
 }
 
 module_init(pt_dump_debug_init);
<span class="p_header">diff --git a/arch/x86/mm/dump_pagetables.c b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">index 43dedbfb7257..f56902c1f04b 100644</span>
<span class="p_header">--- a/arch/x86/mm/dump_pagetables.c</span>
<span class="p_header">+++ b/arch/x86/mm/dump_pagetables.c</span>
<span class="p_chunk">@@ -52,11 +52,17 @@</span> <span class="p_context"> enum address_markers_idx {</span>
 	USER_SPACE_NR = 0,
 	KERNEL_SPACE_NR,
 	LOW_KERNEL_NR,
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	LDT_NR,</span>
<span class="p_add">+#endif</span>
 	VMALLOC_START_NR,
 	VMEMMAP_START_NR,
 #ifdef CONFIG_KASAN
 	KASAN_SHADOW_START_NR,
 	KASAN_SHADOW_END_NR,
<span class="p_add">+#endif</span>
<span class="p_add">+#if defined(CONFIG_MODIFY_LDT_SYSCALL) &amp;&amp; !defined(CONFIG_X86_5LEVEL)</span>
<span class="p_add">+	LDT_NR,</span>
 #endif
 	CPU_ENTRY_AREA_NR,
 #ifdef CONFIG_X86_ESPFIX64
<span class="p_chunk">@@ -81,6 +87,9 @@</span> <span class="p_context"> static struct addr_marker address_markers[] = {</span>
 #ifdef CONFIG_KASAN
 	[KASAN_SHADOW_START_NR]	= { KASAN_SHADOW_START,	&quot;KASAN shadow&quot; },
 	[KASAN_SHADOW_END_NR]	= { KASAN_SHADOW_END,	&quot;KASAN shadow end&quot; },
<span class="p_add">+#endif</span>
<span class="p_add">+#ifdef CONFIG_MODIFY_LDT_SYSCALL</span>
<span class="p_add">+	[LDT_NR]		= { LDT_BASE_ADDR,	&quot;LDT remap&quot; },</span>
 #endif
 	[CPU_ENTRY_AREA_NR]	= { CPU_ENTRY_AREA_BASE,&quot;CPU entry Area&quot; },
 #ifdef CONFIG_X86_ESPFIX64
<span class="p_chunk">@@ -467,7 +476,7 @@</span> <span class="p_context"> static inline bool is_hypervisor_range(int idx)</span>
 }
 
 static void ptdump_walk_pgd_level_core(struct seq_file *m, pgd_t *pgd,
<span class="p_del">-				       bool checkwx)</span>
<span class="p_add">+				       bool checkwx, bool dmesg)</span>
 {
 #ifdef CONFIG_X86_64
 	pgd_t *start = (pgd_t *) &amp;init_top_pgt;
<span class="p_chunk">@@ -480,7 +489,7 @@</span> <span class="p_context"> static void ptdump_walk_pgd_level_core(struct seq_file *m, pgd_t *pgd,</span>
 
 	if (pgd) {
 		start = pgd;
<span class="p_del">-		st.to_dmesg = true;</span>
<span class="p_add">+		st.to_dmesg = dmesg;</span>
 	}
 
 	st.check_wx = checkwx;
<span class="p_chunk">@@ -518,13 +527,37 @@</span> <span class="p_context"> static void ptdump_walk_pgd_level_core(struct seq_file *m, pgd_t *pgd,</span>
 
 void ptdump_walk_pgd_level(struct seq_file *m, pgd_t *pgd)
 {
<span class="p_del">-	ptdump_walk_pgd_level_core(m, pgd, false);</span>
<span class="p_add">+	ptdump_walk_pgd_level_core(m, pgd, false, true);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void ptdump_walk_pgd_level_debugfs(struct seq_file *m, pgd_t *pgd, bool user)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	if (user &amp;&amp; static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		pgd = kernel_to_user_pgdp(pgd);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	ptdump_walk_pgd_level_core(m, pgd, false, false);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL_GPL(ptdump_walk_pgd_level_debugfs);</span>
<span class="p_add">+</span>
<span class="p_add">+static void ptdump_walk_user_pgd_level_checkwx(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	pgd_t *pgd = (pgd_t *) &amp;init_top_pgt;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;x86/mm: Checking user space page tables\n&quot;);</span>
<span class="p_add">+	pgd = kernel_to_user_pgdp(pgd);</span>
<span class="p_add">+	ptdump_walk_pgd_level_core(NULL, pgd, true, false);</span>
<span class="p_add">+#endif</span>
 }
<span class="p_del">-EXPORT_SYMBOL_GPL(ptdump_walk_pgd_level);</span>
 
 void ptdump_walk_pgd_level_checkwx(void)
 {
<span class="p_del">-	ptdump_walk_pgd_level_core(NULL, NULL, true);</span>
<span class="p_add">+	ptdump_walk_pgd_level_core(NULL, NULL, true, false);</span>
<span class="p_add">+	ptdump_walk_user_pgd_level_checkwx();</span>
 }
 
 static int __init pt_dump_init(void)
<span class="p_header">diff --git a/arch/x86/mm/init.c b/arch/x86/mm/init.c</span>
<span class="p_header">index a22c2b95e513..80259ad8c386 100644</span>
<span class="p_header">--- a/arch/x86/mm/init.c</span>
<span class="p_header">+++ b/arch/x86/mm/init.c</span>
<span class="p_chunk">@@ -20,6 +20,7 @@</span> <span class="p_context"></span>
 #include &lt;asm/kaslr.h&gt;
 #include &lt;asm/hypervisor.h&gt;
 #include &lt;asm/cpufeature.h&gt;
<span class="p_add">+#include &lt;asm/pti.h&gt;</span>
 
 /*
  * We need to define the tracepoints somewhere, and tlb.c
<span class="p_chunk">@@ -161,6 +162,12 @@</span> <span class="p_context"> struct map_range {</span>
 
 static int page_size_mask;
 
<span class="p_add">+static void enable_global_pages(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		__supported_pte_mask |= _PAGE_GLOBAL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void __init probe_page_size_mask(void)
 {
 	/*
<span class="p_chunk">@@ -179,11 +186,11 @@</span> <span class="p_context"> static void __init probe_page_size_mask(void)</span>
 		cr4_set_bits_and_update_boot(X86_CR4_PSE);
 
 	/* Enable PGE if available */
<span class="p_add">+	__supported_pte_mask &amp;= ~_PAGE_GLOBAL;</span>
 	if (boot_cpu_has(X86_FEATURE_PGE)) {
 		cr4_set_bits_and_update_boot(X86_CR4_PGE);
<span class="p_del">-		__supported_pte_mask |= _PAGE_GLOBAL;</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		__supported_pte_mask &amp;= ~_PAGE_GLOBAL;</span>
<span class="p_add">+		enable_global_pages();</span>
<span class="p_add">+	}</span>
 
 	/* Enable 1 GB linear kernel mappings if available: */
 	if (direct_gbpages &amp;&amp; boot_cpu_has(X86_FEATURE_GBPAGES)) {
<span class="p_chunk">@@ -196,34 +203,44 @@</span> <span class="p_context"> static void __init probe_page_size_mask(void)</span>
 
 static void setup_pcid(void)
 {
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-	if (boot_cpu_has(X86_FEATURE_PCID)) {</span>
<span class="p_del">-		if (boot_cpu_has(X86_FEATURE_PGE)) {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * This can&#39;t be cr4_set_bits_and_update_boot() --</span>
<span class="p_del">-			 * the trampoline code can&#39;t handle CR4.PCIDE and</span>
<span class="p_del">-			 * it wouldn&#39;t do any good anyway.  Despite the name,</span>
<span class="p_del">-			 * cr4_set_bits_and_update_boot() doesn&#39;t actually</span>
<span class="p_del">-			 * cause the bits in question to remain set all the</span>
<span class="p_del">-			 * way through the secondary boot asm.</span>
<span class="p_del">-			 *</span>
<span class="p_del">-			 * Instead, we brute-force it and set CR4.PCIDE</span>
<span class="p_del">-			 * manually in start_secondary().</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			cr4_set_bits(X86_CR4_PCIDE);</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * flush_tlb_all(), as currently implemented, won&#39;t</span>
<span class="p_del">-			 * work if PCID is on but PGE is not.  Since that</span>
<span class="p_del">-			 * combination doesn&#39;t exist on real hardware, there&#39;s</span>
<span class="p_del">-			 * no reason to try to fully support it, but it&#39;s</span>
<span class="p_del">-			 * polite to avoid corrupting data if we&#39;re on</span>
<span class="p_del">-			 * an improperly configured VM.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			setup_clear_cpu_cap(X86_FEATURE_PCID);</span>
<span class="p_del">-		}</span>
<span class="p_add">+	if (!IS_ENABLED(CONFIG_X86_64))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!boot_cpu_has(X86_FEATURE_PCID))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (boot_cpu_has(X86_FEATURE_PGE)) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * This can&#39;t be cr4_set_bits_and_update_boot() -- the</span>
<span class="p_add">+		 * trampoline code can&#39;t handle CR4.PCIDE and it wouldn&#39;t</span>
<span class="p_add">+		 * do any good anyway.  Despite the name,</span>
<span class="p_add">+		 * cr4_set_bits_and_update_boot() doesn&#39;t actually cause</span>
<span class="p_add">+		 * the bits in question to remain set all the way through</span>
<span class="p_add">+		 * the secondary boot asm.</span>
<span class="p_add">+		 *</span>
<span class="p_add">+		 * Instead, we brute-force it and set CR4.PCIDE manually in</span>
<span class="p_add">+		 * start_secondary().</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		cr4_set_bits(X86_CR4_PCIDE);</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * INVPCID&#39;s single-context modes (2/3) only work if we set</span>
<span class="p_add">+		 * X86_CR4_PCIDE, *and* we INVPCID support.  It&#39;s unusable</span>
<span class="p_add">+		 * on systems that have X86_CR4_PCIDE clear, or that have</span>
<span class="p_add">+		 * no INVPCID support at all.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (boot_cpu_has(X86_FEATURE_INVPCID))</span>
<span class="p_add">+			setup_force_cpu_cap(X86_FEATURE_INVPCID_SINGLE);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * flush_tlb_all(), as currently implemented, won&#39;t work if</span>
<span class="p_add">+		 * PCID is on but PGE is not.  Since that combination</span>
<span class="p_add">+		 * doesn&#39;t exist on real hardware, there&#39;s no reason to try</span>
<span class="p_add">+		 * to fully support it, but it&#39;s polite to avoid corrupting</span>
<span class="p_add">+		 * data if we&#39;re on an improperly configured VM.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		setup_clear_cpu_cap(X86_FEATURE_PCID);</span>
 	}
<span class="p_del">-#endif</span>
 }
 
 #ifdef CONFIG_X86_32
<span class="p_chunk">@@ -624,6 +641,7 @@</span> <span class="p_context"> void __init init_mem_mapping(void)</span>
 {
 	unsigned long end;
 
<span class="p_add">+	pti_check_boottime_disable();</span>
 	probe_page_size_mask();
 	setup_pcid();
 
<span class="p_chunk">@@ -847,7 +865,7 @@</span> <span class="p_context"> void __init zone_sizes_init(void)</span>
 	free_area_init_nodes(max_zone_pfns);
 }
 
<span class="p_del">-DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {</span>
<span class="p_add">+__visible DEFINE_PER_CPU_SHARED_ALIGNED(struct tlb_state, cpu_tlbstate) = {</span>
 	.loaded_mm = &amp;init_mm,
 	.next_asid = 1,
 	.cr4 = ~0UL,	/* fail hard if we screw up cr4 shadow initialization */
<span class="p_header">diff --git a/arch/x86/mm/pgtable.c b/arch/x86/mm/pgtable.c</span>
<span class="p_header">index 17ebc5a978cc..9b7bcbd33cc2 100644</span>
<span class="p_header">--- a/arch/x86/mm/pgtable.c</span>
<span class="p_header">+++ b/arch/x86/mm/pgtable.c</span>
<span class="p_chunk">@@ -355,14 +355,15 @@</span> <span class="p_context"> static inline void _pgd_free(pgd_t *pgd)</span>
 		kmem_cache_free(pgd_cache, pgd);
 }
 #else
<span class="p_add">+</span>
 static inline pgd_t *_pgd_alloc(void)
 {
<span class="p_del">-	return (pgd_t *)__get_free_page(PGALLOC_GFP);</span>
<span class="p_add">+	return (pgd_t *)__get_free_pages(PGALLOC_GFP, PGD_ALLOCATION_ORDER);</span>
 }
 
 static inline void _pgd_free(pgd_t *pgd)
 {
<span class="p_del">-	free_page((unsigned long)pgd);</span>
<span class="p_add">+	free_pages((unsigned long)pgd, PGD_ALLOCATION_ORDER);</span>
 }
 #endif /* CONFIG_X86_PAE */
 
<span class="p_header">diff --git a/arch/x86/mm/pti.c b/arch/x86/mm/pti.c</span>
new file mode 100644
<span class="p_header">index 000000000000..bce8aea65606</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/pti.c</span>
<span class="p_chunk">@@ -0,0 +1,387 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright(c) 2017 Intel Corporation. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of version 2 of the GNU General Public License as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is distributed in the hope that it will be useful, but</span>
<span class="p_add">+ * WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="p_add">+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU</span>
<span class="p_add">+ * General Public License for more details.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This code is based in part on work published here:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *	https://github.com/IAIK/KAISER</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * The original work was written by and and signed off by for the Linux</span>
<span class="p_add">+ * kernel by:</span>
<span class="p_add">+ *</span>
<span class="p_add">+ *   Signed-off-by: Richard Fellner &lt;richard.fellner@student.tugraz.at&gt;</span>
<span class="p_add">+ *   Signed-off-by: Moritz Lipp &lt;moritz.lipp@iaik.tugraz.at&gt;</span>
<span class="p_add">+ *   Signed-off-by: Daniel Gruss &lt;daniel.gruss@iaik.tugraz.at&gt;</span>
<span class="p_add">+ *   Signed-off-by: Michael Schwarz &lt;michael.schwarz@iaik.tugraz.at&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Major changes to the original code by: Dave Hansen &lt;dave.hansen@intel.com&gt;</span>
<span class="p_add">+ * Mostly rewritten by Thomas Gleixner &lt;tglx@linutronix.de&gt; and</span>
<span class="p_add">+ *		       Andy Lutomirsky &lt;luto@amacapital.net&gt;</span>
<span class="p_add">+ */</span>
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
<span class="p_add">+#include &lt;linux/errno.h&gt;</span>
<span class="p_add">+#include &lt;linux/string.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+#include &lt;linux/bug.h&gt;</span>
<span class="p_add">+#include &lt;linux/init.h&gt;</span>
<span class="p_add">+#include &lt;linux/spinlock.h&gt;</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/uaccess.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/cpufeature.h&gt;</span>
<span class="p_add">+#include &lt;asm/hypervisor.h&gt;</span>
<span class="p_add">+#include &lt;asm/vsyscall.h&gt;</span>
<span class="p_add">+#include &lt;asm/cmdline.h&gt;</span>
<span class="p_add">+#include &lt;asm/pti.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgtable.h&gt;</span>
<span class="p_add">+#include &lt;asm/pgalloc.h&gt;</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+#include &lt;asm/desc.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#undef pr_fmt</span>
<span class="p_add">+#define pr_fmt(fmt)     &quot;Kernel/User page tables isolation: &quot; fmt</span>
<span class="p_add">+</span>
<span class="p_add">+/* Backporting helper */</span>
<span class="p_add">+#ifndef __GFP_NOTRACK</span>
<span class="p_add">+#define __GFP_NOTRACK	0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init pti_print_if_insecure(const char *reason)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+		pr_info(&quot;%s\n&quot;, reason);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init pti_print_if_secure(const char *reason)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+		pr_info(&quot;%s\n&quot;, reason);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init pti_check_boottime_disable(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	char arg[5];</span>
<span class="p_add">+	int ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (hypervisor_is_type(X86_HYPER_XEN_PV)) {</span>
<span class="p_add">+		pti_print_if_insecure(&quot;disabled on XEN PV.&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ret = cmdline_find_option(boot_command_line, &quot;pti&quot;, arg, sizeof(arg));</span>
<span class="p_add">+	if (ret &gt; 0)  {</span>
<span class="p_add">+		if (ret == 3 &amp;&amp; !strncmp(arg, &quot;off&quot;, 3)) {</span>
<span class="p_add">+			pti_print_if_insecure(&quot;disabled on command line.&quot;);</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (ret == 2 &amp;&amp; !strncmp(arg, &quot;on&quot;, 2)) {</span>
<span class="p_add">+			pti_print_if_secure(&quot;force enabled on command line.&quot;);</span>
<span class="p_add">+			goto enable;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (ret == 4 &amp;&amp; !strncmp(arg, &quot;auto&quot;, 4))</span>
<span class="p_add">+			goto autosel;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (cmdline_find_option_bool(boot_command_line, &quot;nopti&quot;)) {</span>
<span class="p_add">+		pti_print_if_insecure(&quot;disabled on command line.&quot;);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+autosel:</span>
<span class="p_add">+	if (!boot_cpu_has_bug(X86_BUG_CPU_INSECURE))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+enable:</span>
<span class="p_add">+	setup_force_cpu_cap(X86_FEATURE_PTI);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+pgd_t __pti_set_user_pgd(pgd_t *pgdp, pgd_t pgd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Changes to the high (kernel) portion of the kernelmode page</span>
<span class="p_add">+	 * tables are not automatically propagated to the usermode tables.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Users should keep in mind that, unlike the kernelmode tables,</span>
<span class="p_add">+	 * there is no vmalloc_fault equivalent for the usermode tables.</span>
<span class="p_add">+	 * Top-level entries added to init_mm&#39;s usermode pgd after boot</span>
<span class="p_add">+	 * will not be automatically propagated to other mms.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!pgdp_maps_userspace(pgdp))</span>
<span class="p_add">+		return pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The user page tables get the full PGD, accessible from</span>
<span class="p_add">+	 * userspace:</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	kernel_to_user_pgdp(pgdp)-&gt;pgd = pgd.pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * If this is normal user memory, make it NX in the kernel</span>
<span class="p_add">+	 * pagetables so that, if we somehow screw up and return to</span>
<span class="p_add">+	 * usermode with the kernel CR3 loaded, we&#39;ll get a page fault</span>
<span class="p_add">+	 * instead of allowing user code to execute with the wrong CR3.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * As exceptions, we don&#39;t set NX if:</span>
<span class="p_add">+	 *  - _PAGE_USER is not set.  This could be an executable</span>
<span class="p_add">+	 *     EFI runtime mapping or something similar, and the kernel</span>
<span class="p_add">+	 *     may execute from it</span>
<span class="p_add">+	 *  - we don&#39;t have NX support</span>
<span class="p_add">+	 *  - we&#39;re clearing the PGD (i.e. the new pgd is not present).</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((pgd.pgd &amp; (_PAGE_USER|_PAGE_PRESENT)) == (_PAGE_USER|_PAGE_PRESENT) &amp;&amp;</span>
<span class="p_add">+	    (__supported_pte_mask &amp; _PAGE_NX))</span>
<span class="p_add">+		pgd.pgd |= _PAGE_NX;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* return the copy of the PGD we want the kernel to use: */</span>
<span class="p_add">+	return pgd;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Walk the user copy of the page tables (optionally) trying to allocate</span>
<span class="p_add">+ * page table pages on the way down.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns a pointer to a P4D on success, or NULL on failure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static p4d_t *pti_user_pagetable_walk_p4d(unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd = kernel_to_user_pgdp(pgd_offset_k(address));</span>
<span class="p_add">+	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (address &lt; PAGE_OFFSET) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;attempt to walk user address\n&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pgd_none(*pgd)) {</span>
<span class="p_add">+		unsigned long new_p4d_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_p4d_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pgd_none(*pgd)) {</span>
<span class="p_add">+			set_pgd(pgd, __pgd(_KERNPG_TABLE | __pa(new_p4d_page)));</span>
<span class="p_add">+			new_p4d_page = 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (new_p4d_page)</span>
<span class="p_add">+			free_page(new_p4d_page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	BUILD_BUG_ON(pgd_large(*pgd) != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	return p4d_offset(pgd, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Walk the user copy of the page tables (optionally) trying to allocate</span>
<span class="p_add">+ * page table pages on the way down.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns a pointer to a PMD on success, or NULL on failure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static pmd_t *pti_user_pagetable_walk_pmd(unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
<span class="p_add">+	p4d_t *p4d = pti_user_pagetable_walk_p4d(address);</span>
<span class="p_add">+	pud_t *pud;</span>
<span class="p_add">+</span>
<span class="p_add">+	BUILD_BUG_ON(p4d_large(*p4d) != 0);</span>
<span class="p_add">+	if (p4d_none(*p4d)) {</span>
<span class="p_add">+		unsigned long new_pud_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pud_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (p4d_none(*p4d)) {</span>
<span class="p_add">+			set_p4d(p4d, __p4d(_KERNPG_TABLE | __pa(new_pud_page)));</span>
<span class="p_add">+			new_pud_page = 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (new_pud_page)</span>
<span class="p_add">+			free_page(new_pud_page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pud = pud_offset(p4d, address);</span>
<span class="p_add">+	/* The user page tables do not use large mappings: */</span>
<span class="p_add">+	if (pud_large(*pud)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (pud_none(*pud)) {</span>
<span class="p_add">+		unsigned long new_pmd_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pmd_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pud_none(*pud)) {</span>
<span class="p_add">+			set_pud(pud, __pud(_KERNPG_TABLE | __pa(new_pmd_page)));</span>
<span class="p_add">+			new_pmd_page = 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (new_pmd_page)</span>
<span class="p_add">+			free_page(new_pmd_page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return pmd_offset(pud, address);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_X86_VSYSCALL_EMULATION</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Walk the shadow copy of the page tables (optionally) trying to allocate</span>
<span class="p_add">+ * page table pages on the way down.  Does not support large pages.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Note: this is only used when mapping *new* kernel data into the</span>
<span class="p_add">+ * user/shadow page tables.  It is never used for userspace data.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Returns a pointer to a PTE on success, or NULL on failure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static __init pte_t *pti_user_pagetable_walk_pte(unsigned long address)</span>
<span class="p_add">+{</span>
<span class="p_add">+	gfp_t gfp = (GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO);</span>
<span class="p_add">+	pmd_t *pmd = pti_user_pagetable_walk_pmd(address);</span>
<span class="p_add">+	pte_t *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* We can&#39;t do anything sensible if we hit a large mapping. */</span>
<span class="p_add">+	if (pmd_large(*pmd)) {</span>
<span class="p_add">+		WARN_ON(1);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pmd_none(*pmd)) {</span>
<span class="p_add">+		unsigned long new_pte_page = __get_free_page(gfp);</span>
<span class="p_add">+		if (!new_pte_page)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (pmd_none(*pmd)) {</span>
<span class="p_add">+			set_pmd(pmd, __pmd(_KERNPG_TABLE | __pa(new_pte_page)));</span>
<span class="p_add">+			new_pte_page = 0;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		if (new_pte_page)</span>
<span class="p_add">+			free_page(new_pte_page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = pte_offset_kernel(pmd, address);</span>
<span class="p_add">+	if (pte_flags(*pte) &amp; _PAGE_USER) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;attempt to walk to user pte\n&quot;);</span>
<span class="p_add">+		return NULL;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return pte;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init pti_setup_vsyscall(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pte_t *pte, *target_pte;</span>
<span class="p_add">+	unsigned int level;</span>
<span class="p_add">+</span>
<span class="p_add">+	pte = lookup_address(VSYSCALL_ADDR, &amp;level);</span>
<span class="p_add">+	if (!pte || WARN_ON(level != PG_LEVEL_4K) || pte_none(*pte))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	target_pte = pti_user_pagetable_walk_pte(VSYSCALL_ADDR);</span>
<span class="p_add">+	if (WARN_ON(!target_pte))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	*target_pte = *pte;</span>
<span class="p_add">+	set_vsyscall_pgtable_user_bits(kernel_to_user_pgdp(swapper_pg_dir));</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static void __init pti_setup_vsyscall(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init</span>
<span class="p_add">+pti_clone_pmds(unsigned long start, unsigned long end, pmdval_t clear)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long addr;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Clone the populated PMDs which cover start to end. These PMD areas</span>
<span class="p_add">+	 * can have holes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (addr = start; addr &lt; end; addr += PMD_SIZE) {</span>
<span class="p_add">+		pmd_t *pmd, *target_pmd;</span>
<span class="p_add">+		pgd_t *pgd;</span>
<span class="p_add">+		p4d_t *p4d;</span>
<span class="p_add">+		pud_t *pud;</span>
<span class="p_add">+</span>
<span class="p_add">+		pgd = pgd_offset_k(addr);</span>
<span class="p_add">+		if (WARN_ON(pgd_none(*pgd)))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		p4d = p4d_offset(pgd, addr);</span>
<span class="p_add">+		if (WARN_ON(p4d_none(*p4d)))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+		pud = pud_offset(p4d, addr);</span>
<span class="p_add">+		if (pud_none(*pud))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		pmd = pmd_offset(pud, addr);</span>
<span class="p_add">+		if (pmd_none(*pmd))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		target_pmd = pti_user_pagetable_walk_pmd(addr);</span>
<span class="p_add">+		if (WARN_ON(!target_pmd))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Copy the PMD.  That is, the kernelmode and usermode</span>
<span class="p_add">+		 * tables will share the last-level page tables of this</span>
<span class="p_add">+		 * address range</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		*target_pmd = pmd_clear_flags(*pmd, clear);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Clone a single p4d (i.e. a top-level entry on 4-level systems and a</span>
<span class="p_add">+ * next-level entry on 5-level systems.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init pti_clone_p4d(unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	p4d_t *kernel_p4d, *user_p4d;</span>
<span class="p_add">+	pgd_t *kernel_pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	user_p4d = pti_user_pagetable_walk_p4d(addr);</span>
<span class="p_add">+	kernel_pgd = pgd_offset_k(addr);</span>
<span class="p_add">+	kernel_p4d = p4d_offset(kernel_pgd, addr);</span>
<span class="p_add">+	*user_p4d = *kernel_p4d;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Clone the CPU_ENTRY_AREA into the user space visible page table.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init pti_clone_user_shared(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pti_clone_p4d(CPU_ENTRY_AREA_BASE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Clone the ESPFIX P4D into the user space visinble page table</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init pti_setup_espfix64(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifdef CONFIG_X86_ESPFIX64</span>
<span class="p_add">+	pti_clone_p4d(ESPFIX_BASE_ADDR);</span>
<span class="p_add">+#endif</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Clone the populated PMDs of the entry and irqentry text and force it RO.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void __init pti_clone_entry_text(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pti_clone_pmds((unsigned long) __entry_text_start,</span>
<span class="p_add">+			(unsigned long) __irqentry_text_end, _PAGE_RW);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Initialize kernel page table isolation</span>
<span class="p_add">+ */</span>
<span class="p_add">+void __init pti_init(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pr_info(&quot;enabled\n&quot;);</span>
<span class="p_add">+</span>
<span class="p_add">+	pti_clone_user_shared();</span>
<span class="p_add">+	pti_clone_entry_text();</span>
<span class="p_add">+	pti_setup_espfix64();</span>
<span class="p_add">+	pti_setup_vsyscall();</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/mm/tlb.c b/arch/x86/mm/tlb.c</span>
<span class="p_header">index 0a1be3adc97e..a1561957dccb 100644</span>
<span class="p_header">--- a/arch/x86/mm/tlb.c</span>
<span class="p_header">+++ b/arch/x86/mm/tlb.c</span>
<span class="p_chunk">@@ -28,6 +28,38 @@</span> <span class="p_context"></span>
  *	Implement flush IPI by CALL_FUNCTION_VECTOR, Alex Shi
  */
 
<span class="p_add">+/*</span>
<span class="p_add">+ * We get here when we do something requiring a TLB invalidation</span>
<span class="p_add">+ * but could not go invalidate all of the contexts.  We do the</span>
<span class="p_add">+ * necessary invalidation by clearing out the &#39;ctx_id&#39; which</span>
<span class="p_add">+ * forces a TLB flush when the context is loaded.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void clear_asid_other(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	u16 asid;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * This is only expected to be set if we have disabled</span>
<span class="p_add">+	 * kernel _PAGE_GLOBAL pages.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!static_cpu_has(X86_FEATURE_PTI)) {</span>
<span class="p_add">+		WARN_ON_ONCE(1);</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	for (asid = 0; asid &lt; TLB_NR_DYN_ASIDS; asid++) {</span>
<span class="p_add">+		/* Do not need to flush the current asid */</span>
<span class="p_add">+		if (asid == this_cpu_read(cpu_tlbstate.loaded_mm_asid))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Make sure the next time we go to switch to</span>
<span class="p_add">+		 * this asid, we do a flush:</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		this_cpu_write(cpu_tlbstate.ctxs[asid].ctx_id, 0);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	this_cpu_write(cpu_tlbstate.invalidate_other, false);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 atomic64_t last_mm_ctx_id = ATOMIC64_INIT(1);
 
 
<span class="p_chunk">@@ -42,6 +74,9 @@</span> <span class="p_context"> static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,</span>
 		return;
 	}
 
<span class="p_add">+	if (this_cpu_read(cpu_tlbstate.invalidate_other))</span>
<span class="p_add">+		clear_asid_other();</span>
<span class="p_add">+</span>
 	for (asid = 0; asid &lt; TLB_NR_DYN_ASIDS; asid++) {
 		if (this_cpu_read(cpu_tlbstate.ctxs[asid].ctx_id) !=
 		    next-&gt;context.ctx_id)
<span class="p_chunk">@@ -65,6 +100,25 @@</span> <span class="p_context"> static void choose_new_asid(struct mm_struct *next, u64 next_tlb_gen,</span>
 	*need_flush = true;
 }
 
<span class="p_add">+static void load_new_mm_cr3(pgd_t *pgdir, u16 new_asid, bool need_flush)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long new_mm_cr3;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (need_flush) {</span>
<span class="p_add">+		invalidate_user_asid(new_asid);</span>
<span class="p_add">+		new_mm_cr3 = build_cr3(pgdir, new_asid);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		new_mm_cr3 = build_cr3_noflush(pgdir, new_asid);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Caution: many callers of this function expect</span>
<span class="p_add">+	 * that load_cr3() is serializing and orders TLB</span>
<span class="p_add">+	 * fills with respect to the mm_cpumask writes.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	write_cr3(new_mm_cr3);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 void leave_mm(int cpu)
 {
 	struct mm_struct *loaded_mm = this_cpu_read(cpu_tlbstate.loaded_mm);
<span class="p_chunk">@@ -195,7 +249,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 		if (need_flush) {
 			this_cpu_write(cpu_tlbstate.ctxs[new_asid].ctx_id, next-&gt;context.ctx_id);
 			this_cpu_write(cpu_tlbstate.ctxs[new_asid].tlb_gen, next_tlb_gen);
<span class="p_del">-			write_cr3(build_cr3(next-&gt;pgd, new_asid));</span>
<span class="p_add">+			load_new_mm_cr3(next-&gt;pgd, new_asid, true);</span>
 
 			/*
 			 * NB: This gets called via leave_mm() in the idle path
<span class="p_chunk">@@ -208,7 +262,7 @@</span> <span class="p_context"> void switch_mm_irqs_off(struct mm_struct *prev, struct mm_struct *next,</span>
 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, TLB_FLUSH_ALL);
 		} else {
 			/* The new ASID is already up to date. */
<span class="p_del">-			write_cr3(build_cr3_noflush(next-&gt;pgd, new_asid));</span>
<span class="p_add">+			load_new_mm_cr3(next-&gt;pgd, new_asid, false);</span>
 
 			/* See above wrt _rcuidle. */
 			trace_tlb_flush_rcuidle(TLB_FLUSH_ON_TASK_SWITCH, 0);
<span class="p_header">diff --git a/arch/x86/platform/efi/efi_64.c b/arch/x86/platform/efi/efi_64.c</span>
<span class="p_header">index 20fb31579b69..39c4b35ac7a4 100644</span>
<span class="p_header">--- a/arch/x86/platform/efi/efi_64.c</span>
<span class="p_header">+++ b/arch/x86/platform/efi/efi_64.c</span>
<span class="p_chunk">@@ -195,6 +195,9 @@</span> <span class="p_context"> static pgd_t *efi_pgd;</span>
  * because we want to avoid inserting EFI region mappings (EFI_VA_END
  * to EFI_VA_START) into the standard kernel page tables. Everything
  * else can be shared, see efi_sync_low_kernel_mappings().
<span class="p_add">+ *</span>
<span class="p_add">+ * We don&#39;t want the pgd on the pgd_list and cannot use pgd_alloc() for the</span>
<span class="p_add">+ * allocation.</span>
  */
 int __init efi_alloc_page_tables(void)
 {
<span class="p_chunk">@@ -207,7 +210,7 @@</span> <span class="p_context"> int __init efi_alloc_page_tables(void)</span>
 		return 0;
 
 	gfp_mask = GFP_KERNEL | __GFP_NOTRACK | __GFP_ZERO;
<span class="p_del">-	efi_pgd = (pgd_t *)__get_free_page(gfp_mask);</span>
<span class="p_add">+	efi_pgd = (pgd_t *)__get_free_pages(gfp_mask, PGD_ALLOCATION_ORDER);</span>
 	if (!efi_pgd)
 		return -ENOMEM;
 
<span class="p_header">diff --git a/block/blk-map.c b/block/blk-map.c</span>
<span class="p_header">index d5251edcc0dd..368daa02714e 100644</span>
<span class="p_header">--- a/block/blk-map.c</span>
<span class="p_header">+++ b/block/blk-map.c</span>
<span class="p_chunk">@@ -12,22 +12,29 @@</span> <span class="p_context"></span>
 #include &quot;blk.h&quot;
 
 /*
<span class="p_del">- * Append a bio to a passthrough request.  Only works can be merged into</span>
<span class="p_del">- * the request based on the driver constraints.</span>
<span class="p_add">+ * Append a bio to a passthrough request.  Only works if the bio can be merged</span>
<span class="p_add">+ * into the request based on the driver constraints.</span>
  */
<span class="p_del">-int blk_rq_append_bio(struct request *rq, struct bio *bio)</span>
<span class="p_add">+int blk_rq_append_bio(struct request *rq, struct bio **bio)</span>
 {
<span class="p_del">-	blk_queue_bounce(rq-&gt;q, &amp;bio);</span>
<span class="p_add">+	struct bio *orig_bio = *bio;</span>
<span class="p_add">+</span>
<span class="p_add">+	blk_queue_bounce(rq-&gt;q, bio);</span>
 
 	if (!rq-&gt;bio) {
<span class="p_del">-		blk_rq_bio_prep(rq-&gt;q, rq, bio);</span>
<span class="p_add">+		blk_rq_bio_prep(rq-&gt;q, rq, *bio);</span>
 	} else {
<span class="p_del">-		if (!ll_back_merge_fn(rq-&gt;q, rq, bio))</span>
<span class="p_add">+		if (!ll_back_merge_fn(rq-&gt;q, rq, *bio)) {</span>
<span class="p_add">+			if (orig_bio != *bio) {</span>
<span class="p_add">+				bio_put(*bio);</span>
<span class="p_add">+				*bio = orig_bio;</span>
<span class="p_add">+			}</span>
 			return -EINVAL;
<span class="p_add">+		}</span>
 
<span class="p_del">-		rq-&gt;biotail-&gt;bi_next = bio;</span>
<span class="p_del">-		rq-&gt;biotail = bio;</span>
<span class="p_del">-		rq-&gt;__data_len += bio-&gt;bi_iter.bi_size;</span>
<span class="p_add">+		rq-&gt;biotail-&gt;bi_next = *bio;</span>
<span class="p_add">+		rq-&gt;biotail = *bio;</span>
<span class="p_add">+		rq-&gt;__data_len += (*bio)-&gt;bi_iter.bi_size;</span>
 	}
 
 	return 0;
<span class="p_chunk">@@ -80,14 +87,12 @@</span> <span class="p_context"> static int __blk_rq_map_user_iov(struct request *rq,</span>
 	 * We link the bounce buffer in and could have to traverse it
 	 * later so we have to get a ref to prevent it from being freed
 	 */
<span class="p_del">-	ret = blk_rq_append_bio(rq, bio);</span>
<span class="p_del">-	bio_get(bio);</span>
<span class="p_add">+	ret = blk_rq_append_bio(rq, &amp;bio);</span>
 	if (ret) {
<span class="p_del">-		bio_endio(bio);</span>
 		__blk_rq_unmap_user(orig_bio);
<span class="p_del">-		bio_put(bio);</span>
 		return ret;
 	}
<span class="p_add">+	bio_get(bio);</span>
 
 	return 0;
 }
<span class="p_chunk">@@ -220,7 +225,7 @@</span> <span class="p_context"> int blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,</span>
 	int reading = rq_data_dir(rq) == READ;
 	unsigned long addr = (unsigned long) kbuf;
 	int do_copy = 0;
<span class="p_del">-	struct bio *bio;</span>
<span class="p_add">+	struct bio *bio, *orig_bio;</span>
 	int ret;
 
 	if (len &gt; (queue_max_hw_sectors(q) &lt;&lt; 9))
<span class="p_chunk">@@ -243,10 +248,11 @@</span> <span class="p_context"> int blk_rq_map_kern(struct request_queue *q, struct request *rq, void *kbuf,</span>
 	if (do_copy)
 		rq-&gt;rq_flags |= RQF_COPY_USER;
 
<span class="p_del">-	ret = blk_rq_append_bio(rq, bio);</span>
<span class="p_add">+	orig_bio = bio;</span>
<span class="p_add">+	ret = blk_rq_append_bio(rq, &amp;bio);</span>
 	if (unlikely(ret)) {
 		/* request is too big */
<span class="p_del">-		bio_put(bio);</span>
<span class="p_add">+		bio_put(orig_bio);</span>
 		return ret;
 	}
 
<span class="p_header">diff --git a/block/bounce.c b/block/bounce.c</span>
<span class="p_header">index fceb1a96480b..1d05c422c932 100644</span>
<span class="p_header">--- a/block/bounce.c</span>
<span class="p_header">+++ b/block/bounce.c</span>
<span class="p_chunk">@@ -200,6 +200,7 @@</span> <span class="p_context"> static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,</span>
 	unsigned i = 0;
 	bool bounce = false;
 	int sectors = 0;
<span class="p_add">+	bool passthrough = bio_is_passthrough(*bio_orig);</span>
 
 	bio_for_each_segment(from, *bio_orig, iter) {
 		if (i++ &lt; BIO_MAX_PAGES)
<span class="p_chunk">@@ -210,13 +211,14 @@</span> <span class="p_context"> static void __blk_queue_bounce(struct request_queue *q, struct bio **bio_orig,</span>
 	if (!bounce)
 		return;
 
<span class="p_del">-	if (sectors &lt; bio_sectors(*bio_orig)) {</span>
<span class="p_add">+	if (!passthrough &amp;&amp; sectors &lt; bio_sectors(*bio_orig)) {</span>
 		bio = bio_split(*bio_orig, sectors, GFP_NOIO, bounce_bio_split);
 		bio_chain(bio, *bio_orig);
 		generic_make_request(*bio_orig);
 		*bio_orig = bio;
 	}
<span class="p_del">-	bio = bio_clone_bioset(*bio_orig, GFP_NOIO, bounce_bio_set);</span>
<span class="p_add">+	bio = bio_clone_bioset(*bio_orig, GFP_NOIO, passthrough ? NULL :</span>
<span class="p_add">+			bounce_bio_set);</span>
 
 	bio_for_each_segment_all(to, bio, i) {
 		struct page *page = to-&gt;bv_page;
<span class="p_header">diff --git a/drivers/android/binder.c b/drivers/android/binder.c</span>
<span class="p_header">index 88b4bbe58100..a340766b51fe 100644</span>
<span class="p_header">--- a/drivers/android/binder.c</span>
<span class="p_header">+++ b/drivers/android/binder.c</span>
<span class="p_chunk">@@ -482,7 +482,8 @@</span> <span class="p_context"> enum binder_deferred_state {</span>
  * @tsk                   task_struct for group_leader of process
  *                        (invariant after initialized)
  * @files                 files_struct for process
<span class="p_del">- *                        (invariant after initialized)</span>
<span class="p_add">+ *                        (protected by @files_lock)</span>
<span class="p_add">+ * @files_lock            mutex to protect @files</span>
  * @deferred_work_node:   element for binder_deferred_list
  *                        (protected by binder_deferred_lock)
  * @deferred_work:        bitmap of deferred work to perform
<span class="p_chunk">@@ -530,6 +531,7 @@</span> <span class="p_context"> struct binder_proc {</span>
 	int pid;
 	struct task_struct *tsk;
 	struct files_struct *files;
<span class="p_add">+	struct mutex files_lock;</span>
 	struct hlist_node deferred_work_node;
 	int deferred_work;
 	bool is_dead;
<span class="p_chunk">@@ -877,20 +879,26 @@</span> <span class="p_context"> static void binder_inc_node_tmpref_ilocked(struct binder_node *node);</span>
 
 static int task_get_unused_fd_flags(struct binder_proc *proc, int flags)
 {
<span class="p_del">-	struct files_struct *files = proc-&gt;files;</span>
 	unsigned long rlim_cur;
 	unsigned long irqs;
<span class="p_add">+	int ret;</span>
 
<span class="p_del">-	if (files == NULL)</span>
<span class="p_del">-		return -ESRCH;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!lock_task_sighand(proc-&gt;tsk, &amp;irqs))</span>
<span class="p_del">-		return -EMFILE;</span>
<span class="p_del">-</span>
<span class="p_add">+	mutex_lock(&amp;proc-&gt;files_lock);</span>
<span class="p_add">+	if (proc-&gt;files == NULL) {</span>
<span class="p_add">+		ret = -ESRCH;</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	if (!lock_task_sighand(proc-&gt;tsk, &amp;irqs)) {</span>
<span class="p_add">+		ret = -EMFILE;</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+	}</span>
 	rlim_cur = task_rlimit(proc-&gt;tsk, RLIMIT_NOFILE);
 	unlock_task_sighand(proc-&gt;tsk, &amp;irqs);
 
<span class="p_del">-	return __alloc_fd(files, 0, rlim_cur, flags);</span>
<span class="p_add">+	ret = __alloc_fd(proc-&gt;files, 0, rlim_cur, flags);</span>
<span class="p_add">+err:</span>
<span class="p_add">+	mutex_unlock(&amp;proc-&gt;files_lock);</span>
<span class="p_add">+	return ret;</span>
 }
 
 /*
<span class="p_chunk">@@ -899,8 +907,10 @@</span> <span class="p_context"> static int task_get_unused_fd_flags(struct binder_proc *proc, int flags)</span>
 static void task_fd_install(
 	struct binder_proc *proc, unsigned int fd, struct file *file)
 {
<span class="p_add">+	mutex_lock(&amp;proc-&gt;files_lock);</span>
 	if (proc-&gt;files)
 		__fd_install(proc-&gt;files, fd, file);
<span class="p_add">+	mutex_unlock(&amp;proc-&gt;files_lock);</span>
 }
 
 /*
<span class="p_chunk">@@ -910,9 +920,11 @@</span> <span class="p_context"> static long task_close_fd(struct binder_proc *proc, unsigned int fd)</span>
 {
 	int retval;
 
<span class="p_del">-	if (proc-&gt;files == NULL)</span>
<span class="p_del">-		return -ESRCH;</span>
<span class="p_del">-</span>
<span class="p_add">+	mutex_lock(&amp;proc-&gt;files_lock);</span>
<span class="p_add">+	if (proc-&gt;files == NULL) {</span>
<span class="p_add">+		retval = -ESRCH;</span>
<span class="p_add">+		goto err;</span>
<span class="p_add">+	}</span>
 	retval = __close_fd(proc-&gt;files, fd);
 	/* can&#39;t restart close syscall because file table entry was cleared */
 	if (unlikely(retval == -ERESTARTSYS ||
<span class="p_chunk">@@ -920,7 +932,8 @@</span> <span class="p_context"> static long task_close_fd(struct binder_proc *proc, unsigned int fd)</span>
 		     retval == -ERESTARTNOHAND ||
 		     retval == -ERESTART_RESTARTBLOCK))
 		retval = -EINTR;
<span class="p_del">-</span>
<span class="p_add">+err:</span>
<span class="p_add">+	mutex_unlock(&amp;proc-&gt;files_lock);</span>
 	return retval;
 }
 
<span class="p_chunk">@@ -4627,7 +4640,9 @@</span> <span class="p_context"> static int binder_mmap(struct file *filp, struct vm_area_struct *vma)</span>
 	ret = binder_alloc_mmap_handler(&amp;proc-&gt;alloc, vma);
 	if (ret)
 		return ret;
<span class="p_add">+	mutex_lock(&amp;proc-&gt;files_lock);</span>
 	proc-&gt;files = get_files_struct(current);
<span class="p_add">+	mutex_unlock(&amp;proc-&gt;files_lock);</span>
 	return 0;
 
 err_bad_arg:
<span class="p_chunk">@@ -4651,6 +4666,7 @@</span> <span class="p_context"> static int binder_open(struct inode *nodp, struct file *filp)</span>
 	spin_lock_init(&amp;proc-&gt;outer_lock);
 	get_task_struct(current-&gt;group_leader);
 	proc-&gt;tsk = current-&gt;group_leader;
<span class="p_add">+	mutex_init(&amp;proc-&gt;files_lock);</span>
 	INIT_LIST_HEAD(&amp;proc-&gt;todo);
 	proc-&gt;default_priority = task_nice(current);
 	binder_dev = container_of(filp-&gt;private_data, struct binder_device,
<span class="p_chunk">@@ -4903,9 +4919,11 @@</span> <span class="p_context"> static void binder_deferred_func(struct work_struct *work)</span>
 
 		files = NULL;
 		if (defer &amp; BINDER_DEFERRED_PUT_FILES) {
<span class="p_add">+			mutex_lock(&amp;proc-&gt;files_lock);</span>
 			files = proc-&gt;files;
 			if (files)
 				proc-&gt;files = NULL;
<span class="p_add">+			mutex_unlock(&amp;proc-&gt;files_lock);</span>
 		}
 
 		if (defer &amp; BINDER_DEFERRED_FLUSH)
<span class="p_header">diff --git a/drivers/base/cacheinfo.c b/drivers/base/cacheinfo.c</span>
<span class="p_header">index eb3af2739537..07532d83be0b 100644</span>
<span class="p_header">--- a/drivers/base/cacheinfo.c</span>
<span class="p_header">+++ b/drivers/base/cacheinfo.c</span>
<span class="p_chunk">@@ -186,6 +186,11 @@</span> <span class="p_context"> static void cache_associativity(struct cacheinfo *this_leaf)</span>
 		this_leaf-&gt;ways_of_associativity = (size / nr_sets) / line_size;
 }
 
<span class="p_add">+static bool cache_node_is_unified(struct cacheinfo *this_leaf)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return of_property_read_bool(this_leaf-&gt;of_node, &quot;cache-unified&quot;);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void cache_of_override_properties(unsigned int cpu)
 {
 	int index;
<span class="p_chunk">@@ -194,6 +199,14 @@</span> <span class="p_context"> static void cache_of_override_properties(unsigned int cpu)</span>
 
 	for (index = 0; index &lt; cache_leaves(cpu); index++) {
 		this_leaf = this_cpu_ci-&gt;info_list + index;
<span class="p_add">+		/*</span>
<span class="p_add">+		 * init_cache_level must setup the cache level correctly</span>
<span class="p_add">+		 * overriding the architecturally specified levels, so</span>
<span class="p_add">+		 * if type is NONE at this stage, it should be unified</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (this_leaf-&gt;type == CACHE_TYPE_NOCACHE &amp;&amp;</span>
<span class="p_add">+		    cache_node_is_unified(this_leaf))</span>
<span class="p_add">+			this_leaf-&gt;type = CACHE_TYPE_UNIFIED;</span>
 		cache_size(this_leaf);
 		cache_get_line_size(this_leaf);
 		cache_nr_sets(this_leaf);
<span class="p_header">diff --git a/drivers/gpio/gpiolib-acpi.c b/drivers/gpio/gpiolib-acpi.c</span>
<span class="p_header">index eb4528c87c0b..d6f3d9ee1350 100644</span>
<span class="p_header">--- a/drivers/gpio/gpiolib-acpi.c</span>
<span class="p_header">+++ b/drivers/gpio/gpiolib-acpi.c</span>
<span class="p_chunk">@@ -1074,7 +1074,7 @@</span> <span class="p_context"> void acpi_gpiochip_add(struct gpio_chip *chip)</span>
 	}
 
 	if (!chip-&gt;names)
<span class="p_del">-		devprop_gpiochip_set_names(chip);</span>
<span class="p_add">+		devprop_gpiochip_set_names(chip, dev_fwnode(chip-&gt;parent));</span>
 
 	acpi_gpiochip_request_regions(acpi_gpio);
 	acpi_gpiochip_scan_gpios(acpi_gpio);
<span class="p_header">diff --git a/drivers/gpio/gpiolib-devprop.c b/drivers/gpio/gpiolib-devprop.c</span>
<span class="p_header">index 27f383bda7d9..f748aa3e77f7 100644</span>
<span class="p_header">--- a/drivers/gpio/gpiolib-devprop.c</span>
<span class="p_header">+++ b/drivers/gpio/gpiolib-devprop.c</span>
<span class="p_chunk">@@ -19,30 +19,27 @@</span> <span class="p_context"></span>
 /**
  * devprop_gpiochip_set_names - Set GPIO line names using device properties
  * @chip: GPIO chip whose lines should be named, if possible
<span class="p_add">+ * @fwnode: Property Node containing the gpio-line-names property</span>
  *
  * Looks for device property &quot;gpio-line-names&quot; and if it exists assigns
  * GPIO line names for the chip. The memory allocated for the assigned
  * names belong to the underlying firmware node and should not be released
  * by the caller.
  */
<span class="p_del">-void devprop_gpiochip_set_names(struct gpio_chip *chip)</span>
<span class="p_add">+void devprop_gpiochip_set_names(struct gpio_chip *chip,</span>
<span class="p_add">+				const struct fwnode_handle *fwnode)</span>
 {
 	struct gpio_device *gdev = chip-&gt;gpiodev;
 	const char **names;
 	int ret, i;
 
<span class="p_del">-	if (!chip-&gt;parent) {</span>
<span class="p_del">-		dev_warn(&amp;gdev-&gt;dev, &quot;GPIO chip parent is NULL\n&quot;);</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = device_property_read_string_array(chip-&gt;parent, &quot;gpio-line-names&quot;,</span>
<span class="p_add">+	ret = fwnode_property_read_string_array(fwnode, &quot;gpio-line-names&quot;,</span>
 						NULL, 0);
 	if (ret &lt; 0)
 		return;
 
 	if (ret != gdev-&gt;ngpio) {
<span class="p_del">-		dev_warn(chip-&gt;parent,</span>
<span class="p_add">+		dev_warn(&amp;gdev-&gt;dev,</span>
 			 &quot;names %d do not match number of GPIOs %d\n&quot;, ret,
 			 gdev-&gt;ngpio);
 		return;
<span class="p_chunk">@@ -52,10 +49,10 @@</span> <span class="p_context"> void devprop_gpiochip_set_names(struct gpio_chip *chip)</span>
 	if (!names)
 		return;
 
<span class="p_del">-	ret = device_property_read_string_array(chip-&gt;parent, &quot;gpio-line-names&quot;,</span>
<span class="p_add">+	ret = fwnode_property_read_string_array(fwnode, &quot;gpio-line-names&quot;,</span>
 						names, gdev-&gt;ngpio);
 	if (ret &lt; 0) {
<span class="p_del">-		dev_warn(chip-&gt;parent, &quot;failed to read GPIO line names\n&quot;);</span>
<span class="p_add">+		dev_warn(&amp;gdev-&gt;dev, &quot;failed to read GPIO line names\n&quot;);</span>
 		kfree(names);
 		return;
 	}
<span class="p_header">diff --git a/drivers/gpio/gpiolib-of.c b/drivers/gpio/gpiolib-of.c</span>
<span class="p_header">index bfcd20699ec8..ba38f530e403 100644</span>
<span class="p_header">--- a/drivers/gpio/gpiolib-of.c</span>
<span class="p_header">+++ b/drivers/gpio/gpiolib-of.c</span>
<span class="p_chunk">@@ -493,7 +493,8 @@</span> <span class="p_context"> int of_gpiochip_add(struct gpio_chip *chip)</span>
 
 	/* If the chip defines names itself, these take precedence */
 	if (!chip-&gt;names)
<span class="p_del">-		devprop_gpiochip_set_names(chip);</span>
<span class="p_add">+		devprop_gpiochip_set_names(chip,</span>
<span class="p_add">+					   of_fwnode_handle(chip-&gt;of_node));</span>
 
 	of_node_get(chip-&gt;of_node);
 
<span class="p_header">diff --git a/drivers/gpio/gpiolib.h b/drivers/gpio/gpiolib.h</span>
<span class="p_header">index d003ccb12781..3d4d0634c9dd 100644</span>
<span class="p_header">--- a/drivers/gpio/gpiolib.h</span>
<span class="p_header">+++ b/drivers/gpio/gpiolib.h</span>
<span class="p_chunk">@@ -224,7 +224,8 @@</span> <span class="p_context"> static inline int gpio_chip_hwgpio(const struct gpio_desc *desc)</span>
 	return desc - &amp;desc-&gt;gdev-&gt;descs[0];
 }
 
<span class="p_del">-void devprop_gpiochip_set_names(struct gpio_chip *chip);</span>
<span class="p_add">+void devprop_gpiochip_set_names(struct gpio_chip *chip,</span>
<span class="p_add">+				const struct fwnode_handle *fwnode);</span>
 
 /* With descriptor prefix */
 
<span class="p_header">diff --git a/drivers/infiniband/core/security.c b/drivers/infiniband/core/security.c</span>
<span class="p_header">index feafdb961c48..59b2f96d986a 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/security.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/security.c</span>
<span class="p_chunk">@@ -386,6 +386,9 @@</span> <span class="p_context"> int ib_open_shared_qp_security(struct ib_qp *qp, struct ib_device *dev)</span>
 	if (ret)
 		return ret;
 
<span class="p_add">+	if (!qp-&gt;qp_sec)</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	mutex_lock(&amp;real_qp-&gt;qp_sec-&gt;mutex);
 	ret = check_qp_port_pkey_settings(real_qp-&gt;qp_sec-&gt;ports_pkeys,
 					  qp-&gt;qp_sec);
<span class="p_header">diff --git a/drivers/infiniband/core/uverbs_cmd.c b/drivers/infiniband/core/uverbs_cmd.c</span>
<span class="p_header">index d8f540054392..93c1a57dbff1 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/uverbs_cmd.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/uverbs_cmd.c</span>
<span class="p_chunk">@@ -2085,8 +2085,8 @@</span> <span class="p_context"> int ib_uverbs_ex_modify_qp(struct ib_uverbs_file *file,</span>
 		return -EOPNOTSUPP;
 
 	if (ucore-&gt;inlen &gt; sizeof(cmd)) {
<span class="p_del">-		if (ib_is_udata_cleared(ucore, sizeof(cmd),</span>
<span class="p_del">-					ucore-&gt;inlen - sizeof(cmd)))</span>
<span class="p_add">+		if (!ib_is_udata_cleared(ucore, sizeof(cmd),</span>
<span class="p_add">+					 ucore-&gt;inlen - sizeof(cmd)))</span>
 			return -EOPNOTSUPP;
 	}
 
<span class="p_header">diff --git a/drivers/infiniband/core/verbs.c b/drivers/infiniband/core/verbs.c</span>
<span class="p_header">index de57d6c11a25..9032f77cc38d 100644</span>
<span class="p_header">--- a/drivers/infiniband/core/verbs.c</span>
<span class="p_header">+++ b/drivers/infiniband/core/verbs.c</span>
<span class="p_chunk">@@ -1400,7 +1400,8 @@</span> <span class="p_context"> int ib_close_qp(struct ib_qp *qp)</span>
 	spin_unlock_irqrestore(&amp;real_qp-&gt;device-&gt;event_handler_lock, flags);
 
 	atomic_dec(&amp;real_qp-&gt;usecnt);
<span class="p_del">-	ib_close_shared_qp_security(qp-&gt;qp_sec);</span>
<span class="p_add">+	if (qp-&gt;qp_sec)</span>
<span class="p_add">+		ib_close_shared_qp_security(qp-&gt;qp_sec);</span>
 	kfree(qp);
 
 	return 0;
<span class="p_header">diff --git a/drivers/infiniband/hw/cxgb4/cq.c b/drivers/infiniband/hw/cxgb4/cq.c</span>
<span class="p_header">index eae8ea81c6e2..514c1000ded1 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/cxgb4/cq.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/cxgb4/cq.c</span>
<span class="p_chunk">@@ -586,10 +586,10 @@</span> <span class="p_context"> static int poll_cq(struct t4_wq *wq, struct t4_cq *cq, struct t4_cqe *cqe,</span>
 			ret = -EAGAIN;
 			goto skip_cqe;
 		}
<span class="p_del">-		if (unlikely((CQE_WRID_MSN(hw_cqe) != (wq-&gt;rq.msn)))) {</span>
<span class="p_add">+		if (unlikely(!CQE_STATUS(hw_cqe) &amp;&amp;</span>
<span class="p_add">+			     CQE_WRID_MSN(hw_cqe) != wq-&gt;rq.msn)) {</span>
 			t4_set_wq_in_error(wq);
<span class="p_del">-			hw_cqe-&gt;header |= htonl(CQE_STATUS_V(T4_ERR_MSN));</span>
<span class="p_del">-			goto proc_cqe;</span>
<span class="p_add">+			hw_cqe-&gt;header |= cpu_to_be32(CQE_STATUS_V(T4_ERR_MSN));</span>
 		}
 		goto proc_cqe;
 	}
<span class="p_header">diff --git a/drivers/infiniband/hw/hfi1/hfi.h b/drivers/infiniband/hw/hfi1/hfi.h</span>
<span class="p_header">index 6ff44dc606eb..3409eee16092 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/hfi1/hfi.h</span>
<span class="p_header">+++ b/drivers/infiniband/hw/hfi1/hfi.h</span>
<span class="p_chunk">@@ -1129,7 +1129,6 @@</span> <span class="p_context"> struct hfi1_devdata {</span>
 	u16 pcie_lnkctl;
 	u16 pcie_devctl2;
 	u32 pci_msix0;
<span class="p_del">-	u32 pci_lnkctl3;</span>
 	u32 pci_tph2;
 
 	/*
<span class="p_header">diff --git a/drivers/infiniband/hw/hfi1/pcie.c b/drivers/infiniband/hw/hfi1/pcie.c</span>
<span class="p_header">index 09e50fd2a08f..8c7e7a60b715 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/hfi1/pcie.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/hfi1/pcie.c</span>
<span class="p_chunk">@@ -411,15 +411,12 @@</span> <span class="p_context"> int restore_pci_variables(struct hfi1_devdata *dd)</span>
 	if (ret)
 		goto error;
 
<span class="p_del">-	ret = pci_write_config_dword(dd-&gt;pcidev, PCIE_CFG_SPCIE1,</span>
<span class="p_del">-				     dd-&gt;pci_lnkctl3);</span>
<span class="p_del">-	if (ret)</span>
<span class="p_del">-		goto error;</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = pci_write_config_dword(dd-&gt;pcidev, PCIE_CFG_TPH2, dd-&gt;pci_tph2);</span>
<span class="p_del">-	if (ret)</span>
<span class="p_del">-		goto error;</span>
<span class="p_del">-</span>
<span class="p_add">+	if (pci_find_ext_capability(dd-&gt;pcidev, PCI_EXT_CAP_ID_TPH)) {</span>
<span class="p_add">+		ret = pci_write_config_dword(dd-&gt;pcidev, PCIE_CFG_TPH2,</span>
<span class="p_add">+					     dd-&gt;pci_tph2);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto error;</span>
<span class="p_add">+	}</span>
 	return 0;
 
 error:
<span class="p_chunk">@@ -469,15 +466,12 @@</span> <span class="p_context"> int save_pci_variables(struct hfi1_devdata *dd)</span>
 	if (ret)
 		goto error;
 
<span class="p_del">-	ret = pci_read_config_dword(dd-&gt;pcidev, PCIE_CFG_SPCIE1,</span>
<span class="p_del">-				    &amp;dd-&gt;pci_lnkctl3);</span>
<span class="p_del">-	if (ret)</span>
<span class="p_del">-		goto error;</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = pci_read_config_dword(dd-&gt;pcidev, PCIE_CFG_TPH2, &amp;dd-&gt;pci_tph2);</span>
<span class="p_del">-	if (ret)</span>
<span class="p_del">-		goto error;</span>
<span class="p_del">-</span>
<span class="p_add">+	if (pci_find_ext_capability(dd-&gt;pcidev, PCI_EXT_CAP_ID_TPH)) {</span>
<span class="p_add">+		ret = pci_read_config_dword(dd-&gt;pcidev, PCIE_CFG_TPH2,</span>
<span class="p_add">+					    &amp;dd-&gt;pci_tph2);</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto error;</span>
<span class="p_add">+	}</span>
 	return 0;
 
 error:
<span class="p_header">diff --git a/drivers/infiniband/hw/mlx5/main.c b/drivers/infiniband/hw/mlx5/main.c</span>
<span class="p_header">index 5aff1e33d984..30d479f87cb8 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/mlx5/main.c</span>
<span class="p_header">+++ b/drivers/infiniband/hw/mlx5/main.c</span>
<span class="p_chunk">@@ -1415,6 +1415,7 @@</span> <span class="p_context"> static struct ib_ucontext *mlx5_ib_alloc_ucontext(struct ib_device *ibdev,</span>
 	}
 
 	INIT_LIST_HEAD(&amp;context-&gt;vma_private_list);
<span class="p_add">+	mutex_init(&amp;context-&gt;vma_private_list_mutex);</span>
 	INIT_LIST_HEAD(&amp;context-&gt;db_page_list);
 	mutex_init(&amp;context-&gt;db_page_mutex);
 
<span class="p_chunk">@@ -1576,7 +1577,9 @@</span> <span class="p_context"> static void  mlx5_ib_vma_close(struct vm_area_struct *area)</span>
 	 * mlx5_ib_disassociate_ucontext().
 	 */
 	mlx5_ib_vma_priv_data-&gt;vma = NULL;
<span class="p_add">+	mutex_lock(mlx5_ib_vma_priv_data-&gt;vma_private_list_mutex);</span>
 	list_del(&amp;mlx5_ib_vma_priv_data-&gt;list);
<span class="p_add">+	mutex_unlock(mlx5_ib_vma_priv_data-&gt;vma_private_list_mutex);</span>
 	kfree(mlx5_ib_vma_priv_data);
 }
 
<span class="p_chunk">@@ -1596,10 +1599,13 @@</span> <span class="p_context"> static int mlx5_ib_set_vma_data(struct vm_area_struct *vma,</span>
 		return -ENOMEM;
 
 	vma_prv-&gt;vma = vma;
<span class="p_add">+	vma_prv-&gt;vma_private_list_mutex = &amp;ctx-&gt;vma_private_list_mutex;</span>
 	vma-&gt;vm_private_data = vma_prv;
 	vma-&gt;vm_ops =  &amp;mlx5_ib_vm_ops;
 
<span class="p_add">+	mutex_lock(&amp;ctx-&gt;vma_private_list_mutex);</span>
 	list_add(&amp;vma_prv-&gt;list, vma_head);
<span class="p_add">+	mutex_unlock(&amp;ctx-&gt;vma_private_list_mutex);</span>
 
 	return 0;
 }
<span class="p_chunk">@@ -1642,6 +1648,7 @@</span> <span class="p_context"> static void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 	 * mlx5_ib_vma_close.
 	 */
 	down_write(&amp;owning_mm-&gt;mmap_sem);
<span class="p_add">+	mutex_lock(&amp;context-&gt;vma_private_list_mutex);</span>
 	list_for_each_entry_safe(vma_private, n, &amp;context-&gt;vma_private_list,
 				 list) {
 		vma = vma_private-&gt;vma;
<span class="p_chunk">@@ -1656,6 +1663,7 @@</span> <span class="p_context"> static void mlx5_ib_disassociate_ucontext(struct ib_ucontext *ibcontext)</span>
 		list_del(&amp;vma_private-&gt;list);
 		kfree(vma_private);
 	}
<span class="p_add">+	mutex_unlock(&amp;context-&gt;vma_private_list_mutex);</span>
 	up_write(&amp;owning_mm-&gt;mmap_sem);
 	mmput(owning_mm);
 	put_task_struct(owning_process);
<span class="p_header">diff --git a/drivers/infiniband/hw/mlx5/mlx5_ib.h b/drivers/infiniband/hw/mlx5/mlx5_ib.h</span>
<span class="p_header">index 189e80cd6b2f..754103372faa 100644</span>
<span class="p_header">--- a/drivers/infiniband/hw/mlx5/mlx5_ib.h</span>
<span class="p_header">+++ b/drivers/infiniband/hw/mlx5/mlx5_ib.h</span>
<span class="p_chunk">@@ -115,6 +115,8 @@</span> <span class="p_context"> enum {</span>
 struct mlx5_ib_vma_private_data {
 	struct list_head list;
 	struct vm_area_struct *vma;
<span class="p_add">+	/* protect vma_private_list add/del */</span>
<span class="p_add">+	struct mutex *vma_private_list_mutex;</span>
 };
 
 struct mlx5_ib_ucontext {
<span class="p_chunk">@@ -129,6 +131,8 @@</span> <span class="p_context"> struct mlx5_ib_ucontext {</span>
 	/* Transport Domain number */
 	u32			tdn;
 	struct list_head	vma_private_list;
<span class="p_add">+	/* protect vma_private_list add/del */</span>
<span class="p_add">+	struct mutex		vma_private_list_mutex;</span>
 
 	unsigned long		upd_xlt_page;
 	/* protect ODP/KSM */
<span class="p_header">diff --git a/drivers/net/dsa/bcm_sf2.c b/drivers/net/dsa/bcm_sf2.c</span>
<span class="p_header">index d7b53d53c116..72d6ffbfd638 100644</span>
<span class="p_header">--- a/drivers/net/dsa/bcm_sf2.c</span>
<span class="p_header">+++ b/drivers/net/dsa/bcm_sf2.c</span>
<span class="p_chunk">@@ -167,7 +167,7 @@</span> <span class="p_context"> static void bcm_sf2_gphy_enable_set(struct dsa_switch *ds, bool enable)</span>
 	reg = reg_readl(priv, REG_SPHY_CNTRL);
 	if (enable) {
 		reg |= PHY_RESET;
<span class="p_del">-		reg &amp;= ~(EXT_PWR_DOWN | IDDQ_BIAS | CK25_DIS);</span>
<span class="p_add">+		reg &amp;= ~(EXT_PWR_DOWN | IDDQ_BIAS | IDDQ_GLOBAL_PWR | CK25_DIS);</span>
 		reg_writel(priv, reg, REG_SPHY_CNTRL);
 		udelay(21);
 		reg = reg_readl(priv, REG_SPHY_CNTRL);
<span class="p_header">diff --git a/drivers/net/ethernet/broadcom/bnxt/bnxt.c b/drivers/net/ethernet/broadcom/bnxt/bnxt.c</span>
<span class="p_header">index dc5de275352a..aa764c5e3c6b 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/broadcom/bnxt/bnxt.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/broadcom/bnxt/bnxt.c</span>
<span class="p_chunk">@@ -1875,7 +1875,7 @@</span> <span class="p_context"> static int bnxt_poll_work(struct bnxt *bp, struct bnxt_napi *bnapi, int budget)</span>
 			 * here forever if we consistently cannot allocate
 			 * buffers.
 			 */
<span class="p_del">-			else if (rc == -ENOMEM)</span>
<span class="p_add">+			else if (rc == -ENOMEM &amp;&amp; budget)</span>
 				rx_pkts++;
 			else if (rc == -EBUSY)	/* partial completion */
 				break;
<span class="p_chunk">@@ -1961,7 +1961,7 @@</span> <span class="p_context"> static int bnxt_poll_nitroa0(struct napi_struct *napi, int budget)</span>
 				cpu_to_le32(RX_CMPL_ERRORS_CRC_ERROR);
 
 			rc = bnxt_rx_pkt(bp, bnapi, &amp;raw_cons, &amp;event);
<span class="p_del">-			if (likely(rc == -EIO))</span>
<span class="p_add">+			if (likely(rc == -EIO) &amp;&amp; budget)</span>
 				rx_pkts++;
 			else if (rc == -EBUSY)	/* partial completion */
 				break;
<span class="p_header">diff --git a/drivers/net/ethernet/broadcom/tg3.c b/drivers/net/ethernet/broadcom/tg3.c</span>
<span class="p_header">index 656e6af70f0a..aef3fcf2f5b9 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/broadcom/tg3.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/broadcom/tg3.c</span>
<span class="p_chunk">@@ -14227,7 +14227,9 @@</span> <span class="p_context"> static int tg3_change_mtu(struct net_device *dev, int new_mtu)</span>
 	/* Reset PHY, otherwise the read DMA engine will be in a mode that
 	 * breaks all requests to 256 bytes.
 	 */
<span class="p_del">-	if (tg3_asic_rev(tp) == ASIC_REV_57766)</span>
<span class="p_add">+	if (tg3_asic_rev(tp) == ASIC_REV_57766 ||</span>
<span class="p_add">+	    tg3_asic_rev(tp) == ASIC_REV_5717 ||</span>
<span class="p_add">+	    tg3_asic_rev(tp) == ASIC_REV_5719)</span>
 		reset_phy = true;
 
 	err = tg3_restart_hw(tp, reset_phy);
<span class="p_header">diff --git a/drivers/net/ethernet/freescale/fec_main.c b/drivers/net/ethernet/freescale/fec_main.c</span>
<span class="p_header">index 3dc2d771a222..faf7cdc97ebf 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/freescale/fec_main.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/freescale/fec_main.c</span>
<span class="p_chunk">@@ -818,6 +818,12 @@</span> <span class="p_context"> static void fec_enet_bd_init(struct net_device *dev)</span>
 		for (i = 0; i &lt; txq-&gt;bd.ring_size; i++) {
 			/* Initialize the BD for every fragment in the page. */
 			bdp-&gt;cbd_sc = cpu_to_fec16(0);
<span class="p_add">+			if (bdp-&gt;cbd_bufaddr &amp;&amp;</span>
<span class="p_add">+			    !IS_TSO_HEADER(txq, fec32_to_cpu(bdp-&gt;cbd_bufaddr)))</span>
<span class="p_add">+				dma_unmap_single(&amp;fep-&gt;pdev-&gt;dev,</span>
<span class="p_add">+						 fec32_to_cpu(bdp-&gt;cbd_bufaddr),</span>
<span class="p_add">+						 fec16_to_cpu(bdp-&gt;cbd_datlen),</span>
<span class="p_add">+						 DMA_TO_DEVICE);</span>
 			if (txq-&gt;tx_skbuff[i]) {
 				dev_kfree_skb_any(txq-&gt;tx_skbuff[i]);
 				txq-&gt;tx_skbuff[i] = NULL;
<span class="p_header">diff --git a/drivers/net/ethernet/marvell/mvmdio.c b/drivers/net/ethernet/marvell/mvmdio.c</span>
<span class="p_header">index c9798210fa0f..0495487f7b42 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/marvell/mvmdio.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/marvell/mvmdio.c</span>
<span class="p_chunk">@@ -344,7 +344,8 @@</span> <span class="p_context"> static int orion_mdio_probe(struct platform_device *pdev)</span>
 			dev-&gt;regs + MVMDIO_ERR_INT_MASK);
 
 	} else if (dev-&gt;err_interrupt == -EPROBE_DEFER) {
<span class="p_del">-		return -EPROBE_DEFER;</span>
<span class="p_add">+		ret = -EPROBE_DEFER;</span>
<span class="p_add">+		goto out_mdio;</span>
 	}
 
 	if (pdev-&gt;dev.of_node)
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlx5/core/cmd.c b/drivers/net/ethernet/mellanox/mlx5/core/cmd.c</span>
<span class="p_header">index 1fffdebbc9e8..e9a1fbcc4adf 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlx5/core/cmd.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlx5/core/cmd.c</span>
<span class="p_chunk">@@ -362,7 +362,7 @@</span> <span class="p_context"> static int mlx5_internal_err_ret_value(struct mlx5_core_dev *dev, u16 op,</span>
 	case MLX5_CMD_OP_QUERY_VPORT_COUNTER:
 	case MLX5_CMD_OP_ALLOC_Q_COUNTER:
 	case MLX5_CMD_OP_QUERY_Q_COUNTER:
<span class="p_del">-	case MLX5_CMD_OP_SET_RATE_LIMIT:</span>
<span class="p_add">+	case MLX5_CMD_OP_SET_PP_RATE_LIMIT:</span>
 	case MLX5_CMD_OP_QUERY_RATE_LIMIT:
 	case MLX5_CMD_OP_CREATE_SCHEDULING_ELEMENT:
 	case MLX5_CMD_OP_QUERY_SCHEDULING_ELEMENT:
<span class="p_chunk">@@ -505,7 +505,7 @@</span> <span class="p_context"> const char *mlx5_command_str(int command)</span>
 	MLX5_COMMAND_STR_CASE(ALLOC_Q_COUNTER);
 	MLX5_COMMAND_STR_CASE(DEALLOC_Q_COUNTER);
 	MLX5_COMMAND_STR_CASE(QUERY_Q_COUNTER);
<span class="p_del">-	MLX5_COMMAND_STR_CASE(SET_RATE_LIMIT);</span>
<span class="p_add">+	MLX5_COMMAND_STR_CASE(SET_PP_RATE_LIMIT);</span>
 	MLX5_COMMAND_STR_CASE(QUERY_RATE_LIMIT);
 	MLX5_COMMAND_STR_CASE(CREATE_SCHEDULING_ELEMENT);
 	MLX5_COMMAND_STR_CASE(DESTROY_SCHEDULING_ELEMENT);
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h</span>
<span class="p_header">index 13b5ef9d8703..5fa071620104 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h</span>
<span class="p_chunk">@@ -590,6 +590,7 @@</span> <span class="p_context"> struct mlx5e_channel {</span>
 	struct mlx5_core_dev      *mdev;
 	struct mlx5e_tstamp       *tstamp;
 	int                        ix;
<span class="p_add">+	int                        cpu;</span>
 };
 
 struct mlx5e_channels {
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c</span>
<span class="p_header">index cc11bbbd0309..3cdb932cae76 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlx5/core/en_main.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_main.c</span>
<span class="p_chunk">@@ -71,11 +71,6 @@</span> <span class="p_context"> struct mlx5e_channel_param {</span>
 	struct mlx5e_cq_param      icosq_cq;
 };
 
<span class="p_del">-static int mlx5e_get_node(struct mlx5e_priv *priv, int ix)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return pci_irq_get_node(priv-&gt;mdev-&gt;pdev, MLX5_EQ_VEC_COMP_BASE + ix);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static bool mlx5e_check_fragmented_striding_rq_cap(struct mlx5_core_dev *mdev)
 {
 	return MLX5_CAP_GEN(mdev, striding_rq) &amp;&amp;
<span class="p_chunk">@@ -452,17 +447,16 @@</span> <span class="p_context"> static int mlx5e_rq_alloc_mpwqe_info(struct mlx5e_rq *rq,</span>
 	int wq_sz = mlx5_wq_ll_get_size(&amp;rq-&gt;wq);
 	int mtt_sz = mlx5e_get_wqe_mtt_sz();
 	int mtt_alloc = mtt_sz + MLX5_UMR_ALIGN - 1;
<span class="p_del">-	int node = mlx5e_get_node(c-&gt;priv, c-&gt;ix);</span>
 	int i;
 
 	rq-&gt;mpwqe.info = kzalloc_node(wq_sz * sizeof(*rq-&gt;mpwqe.info),
<span class="p_del">-					GFP_KERNEL, node);</span>
<span class="p_add">+				      GFP_KERNEL, cpu_to_node(c-&gt;cpu));</span>
 	if (!rq-&gt;mpwqe.info)
 		goto err_out;
 
 	/* We allocate more than mtt_sz as we will align the pointer */
<span class="p_del">-	rq-&gt;mpwqe.mtt_no_align = kzalloc_node(mtt_alloc * wq_sz,</span>
<span class="p_del">-					GFP_KERNEL, node);</span>
<span class="p_add">+	rq-&gt;mpwqe.mtt_no_align = kzalloc_node(mtt_alloc * wq_sz, GFP_KERNEL,</span>
<span class="p_add">+					cpu_to_node(c-&gt;cpu));</span>
 	if (unlikely(!rq-&gt;mpwqe.mtt_no_align))
 		goto err_free_wqe_info;
 
<span class="p_chunk">@@ -570,7 +564,7 @@</span> <span class="p_context"> static int mlx5e_alloc_rq(struct mlx5e_channel *c,</span>
 	int err;
 	int i;
 
<span class="p_del">-	rqp-&gt;wq.db_numa_node = mlx5e_get_node(c-&gt;priv, c-&gt;ix);</span>
<span class="p_add">+	rqp-&gt;wq.db_numa_node = cpu_to_node(c-&gt;cpu);</span>
 
 	err = mlx5_wq_ll_create(mdev, &amp;rqp-&gt;wq, rqc_wq, &amp;rq-&gt;wq,
 				&amp;rq-&gt;wq_ctrl);
<span class="p_chunk">@@ -636,8 +630,7 @@</span> <span class="p_context"> static int mlx5e_alloc_rq(struct mlx5e_channel *c,</span>
 	default: /* MLX5_WQ_TYPE_LINKED_LIST */
 		rq-&gt;wqe.frag_info =
 			kzalloc_node(wq_sz * sizeof(*rq-&gt;wqe.frag_info),
<span class="p_del">-				     GFP_KERNEL,</span>
<span class="p_del">-				     mlx5e_get_node(c-&gt;priv, c-&gt;ix));</span>
<span class="p_add">+				     GFP_KERNEL, cpu_to_node(c-&gt;cpu));</span>
 		if (!rq-&gt;wqe.frag_info) {
 			err = -ENOMEM;
 			goto err_rq_wq_destroy;
<span class="p_chunk">@@ -1007,13 +1000,13 @@</span> <span class="p_context"> static int mlx5e_alloc_xdpsq(struct mlx5e_channel *c,</span>
 	sq-&gt;uar_map   = mdev-&gt;mlx5e_res.bfreg.map;
 	sq-&gt;min_inline_mode = params-&gt;tx_min_inline_mode;
 
<span class="p_del">-	param-&gt;wq.db_numa_node = mlx5e_get_node(c-&gt;priv, c-&gt;ix);</span>
<span class="p_add">+	param-&gt;wq.db_numa_node = cpu_to_node(c-&gt;cpu);</span>
 	err = mlx5_wq_cyc_create(mdev, &amp;param-&gt;wq, sqc_wq, &amp;sq-&gt;wq, &amp;sq-&gt;wq_ctrl);
 	if (err)
 		return err;
 	sq-&gt;wq.db = &amp;sq-&gt;wq.db[MLX5_SND_DBR];
 
<span class="p_del">-	err = mlx5e_alloc_xdpsq_db(sq, mlx5e_get_node(c-&gt;priv, c-&gt;ix));</span>
<span class="p_add">+	err = mlx5e_alloc_xdpsq_db(sq, cpu_to_node(c-&gt;cpu));</span>
 	if (err)
 		goto err_sq_wq_destroy;
 
<span class="p_chunk">@@ -1060,13 +1053,13 @@</span> <span class="p_context"> static int mlx5e_alloc_icosq(struct mlx5e_channel *c,</span>
 	sq-&gt;channel   = c;
 	sq-&gt;uar_map   = mdev-&gt;mlx5e_res.bfreg.map;
 
<span class="p_del">-	param-&gt;wq.db_numa_node = mlx5e_get_node(c-&gt;priv, c-&gt;ix);</span>
<span class="p_add">+	param-&gt;wq.db_numa_node = cpu_to_node(c-&gt;cpu);</span>
 	err = mlx5_wq_cyc_create(mdev, &amp;param-&gt;wq, sqc_wq, &amp;sq-&gt;wq, &amp;sq-&gt;wq_ctrl);
 	if (err)
 		return err;
 	sq-&gt;wq.db = &amp;sq-&gt;wq.db[MLX5_SND_DBR];
 
<span class="p_del">-	err = mlx5e_alloc_icosq_db(sq, mlx5e_get_node(c-&gt;priv, c-&gt;ix));</span>
<span class="p_add">+	err = mlx5e_alloc_icosq_db(sq, cpu_to_node(c-&gt;cpu));</span>
 	if (err)
 		goto err_sq_wq_destroy;
 
<span class="p_chunk">@@ -1132,13 +1125,13 @@</span> <span class="p_context"> static int mlx5e_alloc_txqsq(struct mlx5e_channel *c,</span>
 	if (MLX5_IPSEC_DEV(c-&gt;priv-&gt;mdev))
 		set_bit(MLX5E_SQ_STATE_IPSEC, &amp;sq-&gt;state);
 
<span class="p_del">-	param-&gt;wq.db_numa_node = mlx5e_get_node(c-&gt;priv, c-&gt;ix);</span>
<span class="p_add">+	param-&gt;wq.db_numa_node = cpu_to_node(c-&gt;cpu);</span>
 	err = mlx5_wq_cyc_create(mdev, &amp;param-&gt;wq, sqc_wq, &amp;sq-&gt;wq, &amp;sq-&gt;wq_ctrl);
 	if (err)
 		return err;
 	sq-&gt;wq.db    = &amp;sq-&gt;wq.db[MLX5_SND_DBR];
 
<span class="p_del">-	err = mlx5e_alloc_txqsq_db(sq, mlx5e_get_node(c-&gt;priv, c-&gt;ix));</span>
<span class="p_add">+	err = mlx5e_alloc_txqsq_db(sq, cpu_to_node(c-&gt;cpu));</span>
 	if (err)
 		goto err_sq_wq_destroy;
 
<span class="p_chunk">@@ -1510,8 +1503,8 @@</span> <span class="p_context"> static int mlx5e_alloc_cq(struct mlx5e_channel *c,</span>
 	struct mlx5_core_dev *mdev = c-&gt;priv-&gt;mdev;
 	int err;
 
<span class="p_del">-	param-&gt;wq.buf_numa_node = mlx5e_get_node(c-&gt;priv, c-&gt;ix);</span>
<span class="p_del">-	param-&gt;wq.db_numa_node  = mlx5e_get_node(c-&gt;priv, c-&gt;ix);</span>
<span class="p_add">+	param-&gt;wq.buf_numa_node = cpu_to_node(c-&gt;cpu);</span>
<span class="p_add">+	param-&gt;wq.db_numa_node  = cpu_to_node(c-&gt;cpu);</span>
 	param-&gt;eq_ix   = c-&gt;ix;
 
 	err = mlx5e_alloc_cq_common(mdev, param, cq);
<span class="p_chunk">@@ -1610,6 +1603,11 @@</span> <span class="p_context"> static void mlx5e_close_cq(struct mlx5e_cq *cq)</span>
 	mlx5e_free_cq(cq);
 }
 
<span class="p_add">+static int mlx5e_get_cpu(struct mlx5e_priv *priv, int ix)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return cpumask_first(priv-&gt;mdev-&gt;priv.irq_info[ix].mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int mlx5e_open_tx_cqs(struct mlx5e_channel *c,
 			     struct mlx5e_params *params,
 			     struct mlx5e_channel_param *cparam)
<span class="p_chunk">@@ -1758,12 +1756,13 @@</span> <span class="p_context"> static int mlx5e_open_channel(struct mlx5e_priv *priv, int ix,</span>
 {
 	struct mlx5e_cq_moder icocq_moder = {0, 0};
 	struct net_device *netdev = priv-&gt;netdev;
<span class="p_add">+	int cpu = mlx5e_get_cpu(priv, ix);</span>
 	struct mlx5e_channel *c;
 	unsigned int irq;
 	int err;
 	int eqn;
 
<span class="p_del">-	c = kzalloc_node(sizeof(*c), GFP_KERNEL, mlx5e_get_node(priv, ix));</span>
<span class="p_add">+	c = kzalloc_node(sizeof(*c), GFP_KERNEL, cpu_to_node(cpu));</span>
 	if (!c)
 		return -ENOMEM;
 
<span class="p_chunk">@@ -1771,6 +1770,7 @@</span> <span class="p_context"> static int mlx5e_open_channel(struct mlx5e_priv *priv, int ix,</span>
 	c-&gt;mdev     = priv-&gt;mdev;
 	c-&gt;tstamp   = &amp;priv-&gt;tstamp;
 	c-&gt;ix       = ix;
<span class="p_add">+	c-&gt;cpu      = cpu;</span>
 	c-&gt;pdev     = &amp;priv-&gt;mdev-&gt;pdev-&gt;dev;
 	c-&gt;netdev   = priv-&gt;netdev;
 	c-&gt;mkey_be  = cpu_to_be32(priv-&gt;mdev-&gt;mlx5e_res.mkey.key);
<span class="p_chunk">@@ -1859,8 +1859,7 @@</span> <span class="p_context"> static void mlx5e_activate_channel(struct mlx5e_channel *c)</span>
 	for (tc = 0; tc &lt; c-&gt;num_tc; tc++)
 		mlx5e_activate_txqsq(&amp;c-&gt;sq[tc]);
 	mlx5e_activate_rq(&amp;c-&gt;rq);
<span class="p_del">-	netif_set_xps_queue(c-&gt;netdev,</span>
<span class="p_del">-		mlx5_get_vector_affinity(c-&gt;priv-&gt;mdev, c-&gt;ix), c-&gt;ix);</span>
<span class="p_add">+	netif_set_xps_queue(c-&gt;netdev, get_cpu_mask(c-&gt;cpu), c-&gt;ix);</span>
 }
 
 static void mlx5e_deactivate_channel(struct mlx5e_channel *c)
<span class="p_chunk">@@ -3554,6 +3553,7 @@</span> <span class="p_context"> static netdev_features_t mlx5e_tunnel_features_check(struct mlx5e_priv *priv,</span>
 						     struct sk_buff *skb,
 						     netdev_features_t features)
 {
<span class="p_add">+	unsigned int offset = 0;</span>
 	struct udphdr *udph;
 	u8 proto;
 	u16 port;
<span class="p_chunk">@@ -3563,7 +3563,7 @@</span> <span class="p_context"> static netdev_features_t mlx5e_tunnel_features_check(struct mlx5e_priv *priv,</span>
 		proto = ip_hdr(skb)-&gt;protocol;
 		break;
 	case htons(ETH_P_IPV6):
<span class="p_del">-		proto = ipv6_hdr(skb)-&gt;nexthdr;</span>
<span class="p_add">+		proto = ipv6_find_hdr(skb, &amp;offset, -1, NULL, NULL);</span>
 		break;
 	default:
 		goto out;
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlx5/core/fpga/sdk.c b/drivers/net/ethernet/mellanox/mlx5/core/fpga/sdk.c</span>
<span class="p_header">index 3c11d6e2160a..14962969c5ba 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlx5/core/fpga/sdk.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlx5/core/fpga/sdk.c</span>
<span class="p_chunk">@@ -66,6 +66,9 @@</span> <span class="p_context"> static int mlx5_fpga_mem_read_i2c(struct mlx5_fpga_device *fdev, size_t size,</span>
 	u8 actual_size;
 	int err;
 
<span class="p_add">+	if (!size)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	if (!fdev-&gt;mdev)
 		return -ENOTCONN;
 
<span class="p_chunk">@@ -95,6 +98,9 @@</span> <span class="p_context"> static int mlx5_fpga_mem_write_i2c(struct mlx5_fpga_device *fdev, size_t size,</span>
 	u8 actual_size;
 	int err;
 
<span class="p_add">+	if (!size)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	if (!fdev-&gt;mdev)
 		return -ENOTCONN;
 
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlx5/core/main.c b/drivers/net/ethernet/mellanox/mlx5/core/main.c</span>
<span class="p_header">index 06562c9a6b9c..8bfc37e4ec87 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlx5/core/main.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlx5/core/main.c</span>
<span class="p_chunk">@@ -316,9 +316,6 @@</span> <span class="p_context"> static int mlx5_alloc_irq_vectors(struct mlx5_core_dev *dev)</span>
 {
 	struct mlx5_priv *priv = &amp;dev-&gt;priv;
 	struct mlx5_eq_table *table = &amp;priv-&gt;eq_table;
<span class="p_del">-	struct irq_affinity irqdesc = {</span>
<span class="p_del">-		.pre_vectors = MLX5_EQ_VEC_COMP_BASE,</span>
<span class="p_del">-	};</span>
 	int num_eqs = 1 &lt;&lt; MLX5_CAP_GEN(dev, log_max_eq);
 	int nvec;
 
<span class="p_chunk">@@ -332,10 +329,9 @@</span> <span class="p_context"> static int mlx5_alloc_irq_vectors(struct mlx5_core_dev *dev)</span>
 	if (!priv-&gt;irq_info)
 		goto err_free_msix;
 
<span class="p_del">-	nvec = pci_alloc_irq_vectors_affinity(dev-&gt;pdev,</span>
<span class="p_add">+	nvec = pci_alloc_irq_vectors(dev-&gt;pdev,</span>
 			MLX5_EQ_VEC_COMP_BASE + 1, nvec,
<span class="p_del">-			PCI_IRQ_MSIX | PCI_IRQ_AFFINITY,</span>
<span class="p_del">-			&amp;irqdesc);</span>
<span class="p_add">+			PCI_IRQ_MSIX);</span>
 	if (nvec &lt; 0)
 		return nvec;
 
<span class="p_chunk">@@ -621,6 +617,63 @@</span> <span class="p_context"> u64 mlx5_read_internal_timer(struct mlx5_core_dev *dev)</span>
 	return (u64)timer_l | (u64)timer_h1 &lt;&lt; 32;
 }
 
<span class="p_add">+static int mlx5_irq_set_affinity_hint(struct mlx5_core_dev *mdev, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mlx5_priv *priv  = &amp;mdev-&gt;priv;</span>
<span class="p_add">+	int irq = pci_irq_vector(mdev-&gt;pdev, MLX5_EQ_VEC_COMP_BASE + i);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!zalloc_cpumask_var(&amp;priv-&gt;irq_info[i].mask, GFP_KERNEL)) {</span>
<span class="p_add">+		mlx5_core_warn(mdev, &quot;zalloc_cpumask_var failed&quot;);</span>
<span class="p_add">+		return -ENOMEM;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	cpumask_set_cpu(cpumask_local_spread(i, priv-&gt;numa_node),</span>
<span class="p_add">+			priv-&gt;irq_info[i].mask);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_SMP) &amp;&amp;</span>
<span class="p_add">+	    irq_set_affinity_hint(irq, priv-&gt;irq_info[i].mask))</span>
<span class="p_add">+		mlx5_core_warn(mdev, &quot;irq_set_affinity_hint failed, irq 0x%.4x&quot;, irq);</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mlx5_irq_clear_affinity_hint(struct mlx5_core_dev *mdev, int i)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mlx5_priv *priv  = &amp;mdev-&gt;priv;</span>
<span class="p_add">+	int irq = pci_irq_vector(mdev-&gt;pdev, MLX5_EQ_VEC_COMP_BASE + i);</span>
<span class="p_add">+</span>
<span class="p_add">+	irq_set_affinity_hint(irq, NULL);</span>
<span class="p_add">+	free_cpumask_var(priv-&gt;irq_info[i].mask);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static int mlx5_irq_set_affinity_hints(struct mlx5_core_dev *mdev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int err;</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; mdev-&gt;priv.eq_table.num_comp_vectors; i++) {</span>
<span class="p_add">+		err = mlx5_irq_set_affinity_hint(mdev, i);</span>
<span class="p_add">+		if (err)</span>
<span class="p_add">+			goto err_out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+err_out:</span>
<span class="p_add">+	for (i--; i &gt;= 0; i--)</span>
<span class="p_add">+		mlx5_irq_clear_affinity_hint(mdev, i);</span>
<span class="p_add">+</span>
<span class="p_add">+	return err;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void mlx5_irq_clear_affinity_hints(struct mlx5_core_dev *mdev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; mdev-&gt;priv.eq_table.num_comp_vectors; i++)</span>
<span class="p_add">+		mlx5_irq_clear_affinity_hint(mdev, i);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int mlx5_vector2eqn(struct mlx5_core_dev *dev, int vector, int *eqn,
 		    unsigned int *irqn)
 {
<span class="p_chunk">@@ -1093,6 +1146,12 @@</span> <span class="p_context"> static int mlx5_load_one(struct mlx5_core_dev *dev, struct mlx5_priv *priv,</span>
 		goto err_stop_eqs;
 	}
 
<span class="p_add">+	err = mlx5_irq_set_affinity_hints(dev);</span>
<span class="p_add">+	if (err) {</span>
<span class="p_add">+		dev_err(&amp;pdev-&gt;dev, &quot;Failed to alloc affinity hint cpumask\n&quot;);</span>
<span class="p_add">+		goto err_affinity_hints;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	err = mlx5_init_fs(dev);
 	if (err) {
 		dev_err(&amp;pdev-&gt;dev, &quot;Failed to init flow steering\n&quot;);
<span class="p_chunk">@@ -1150,6 +1209,9 @@</span> <span class="p_context"> static int mlx5_load_one(struct mlx5_core_dev *dev, struct mlx5_priv *priv,</span>
 	mlx5_cleanup_fs(dev);
 
 err_fs:
<span class="p_add">+	mlx5_irq_clear_affinity_hints(dev);</span>
<span class="p_add">+</span>
<span class="p_add">+err_affinity_hints:</span>
 	free_comp_eqs(dev);
 
 err_stop_eqs:
<span class="p_chunk">@@ -1218,6 +1280,7 @@</span> <span class="p_context"> static int mlx5_unload_one(struct mlx5_core_dev *dev, struct mlx5_priv *priv,</span>
 
 	mlx5_sriov_detach(dev);
 	mlx5_cleanup_fs(dev);
<span class="p_add">+	mlx5_irq_clear_affinity_hints(dev);</span>
 	free_comp_eqs(dev);
 	mlx5_stop_eqs(dev);
 	mlx5_put_uars_page(dev, priv-&gt;uar);
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlx5/core/qp.c b/drivers/net/ethernet/mellanox/mlx5/core/qp.c</span>
<span class="p_header">index db9e665ab104..889130edb715 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlx5/core/qp.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlx5/core/qp.c</span>
<span class="p_chunk">@@ -213,8 +213,8 @@</span> <span class="p_context"> int mlx5_core_create_qp(struct mlx5_core_dev *dev,</span>
 err_cmd:
 	memset(din, 0, sizeof(din));
 	memset(dout, 0, sizeof(dout));
<span class="p_del">-	MLX5_SET(destroy_qp_in, in, opcode, MLX5_CMD_OP_DESTROY_QP);</span>
<span class="p_del">-	MLX5_SET(destroy_qp_in, in, qpn, qp-&gt;qpn);</span>
<span class="p_add">+	MLX5_SET(destroy_qp_in, din, opcode, MLX5_CMD_OP_DESTROY_QP);</span>
<span class="p_add">+	MLX5_SET(destroy_qp_in, din, qpn, qp-&gt;qpn);</span>
 	mlx5_cmd_exec(dev, din, sizeof(din), dout, sizeof(dout));
 	return err;
 }
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlx5/core/rl.c b/drivers/net/ethernet/mellanox/mlx5/core/rl.c</span>
<span class="p_header">index e651e4c02867..d3c33e9eea72 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlx5/core/rl.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlx5/core/rl.c</span>
<span class="p_chunk">@@ -125,16 +125,16 @@</span> <span class="p_context"> static struct mlx5_rl_entry *find_rl_entry(struct mlx5_rl_table *table,</span>
 	return ret_entry;
 }
 
<span class="p_del">-static int mlx5_set_rate_limit_cmd(struct mlx5_core_dev *dev,</span>
<span class="p_add">+static int mlx5_set_pp_rate_limit_cmd(struct mlx5_core_dev *dev,</span>
 				   u32 rate, u16 index)
 {
<span class="p_del">-	u32 in[MLX5_ST_SZ_DW(set_rate_limit_in)]   = {0};</span>
<span class="p_del">-	u32 out[MLX5_ST_SZ_DW(set_rate_limit_out)] = {0};</span>
<span class="p_add">+	u32 in[MLX5_ST_SZ_DW(set_pp_rate_limit_in)]   = {0};</span>
<span class="p_add">+	u32 out[MLX5_ST_SZ_DW(set_pp_rate_limit_out)] = {0};</span>
 
<span class="p_del">-	MLX5_SET(set_rate_limit_in, in, opcode,</span>
<span class="p_del">-		 MLX5_CMD_OP_SET_RATE_LIMIT);</span>
<span class="p_del">-	MLX5_SET(set_rate_limit_in, in, rate_limit_index, index);</span>
<span class="p_del">-	MLX5_SET(set_rate_limit_in, in, rate_limit, rate);</span>
<span class="p_add">+	MLX5_SET(set_pp_rate_limit_in, in, opcode,</span>
<span class="p_add">+		 MLX5_CMD_OP_SET_PP_RATE_LIMIT);</span>
<span class="p_add">+	MLX5_SET(set_pp_rate_limit_in, in, rate_limit_index, index);</span>
<span class="p_add">+	MLX5_SET(set_pp_rate_limit_in, in, rate_limit, rate);</span>
 	return mlx5_cmd_exec(dev, in, sizeof(in), out, sizeof(out));
 }
 
<span class="p_chunk">@@ -173,7 +173,7 @@</span> <span class="p_context"> int mlx5_rl_add_rate(struct mlx5_core_dev *dev, u32 rate, u16 *index)</span>
 		entry-&gt;refcount++;
 	} else {
 		/* new rate limit */
<span class="p_del">-		err = mlx5_set_rate_limit_cmd(dev, rate, entry-&gt;index);</span>
<span class="p_add">+		err = mlx5_set_pp_rate_limit_cmd(dev, rate, entry-&gt;index);</span>
 		if (err) {
 			mlx5_core_err(dev, &quot;Failed configuring rate: %u (%d)\n&quot;,
 				      rate, err);
<span class="p_chunk">@@ -209,7 +209,7 @@</span> <span class="p_context"> void mlx5_rl_remove_rate(struct mlx5_core_dev *dev, u32 rate)</span>
 	entry-&gt;refcount--;
 	if (!entry-&gt;refcount) {
 		/* need to remove rate */
<span class="p_del">-		mlx5_set_rate_limit_cmd(dev, 0, entry-&gt;index);</span>
<span class="p_add">+		mlx5_set_pp_rate_limit_cmd(dev, 0, entry-&gt;index);</span>
 		entry-&gt;rate = 0;
 	}
 
<span class="p_chunk">@@ -262,8 +262,8 @@</span> <span class="p_context"> void mlx5_cleanup_rl_table(struct mlx5_core_dev *dev)</span>
 	/* Clear all configured rates */
 	for (i = 0; i &lt; table-&gt;max_size; i++)
 		if (table-&gt;rl_entry[i].rate)
<span class="p_del">-			mlx5_set_rate_limit_cmd(dev, 0,</span>
<span class="p_del">-						table-&gt;rl_entry[i].index);</span>
<span class="p_add">+			mlx5_set_pp_rate_limit_cmd(dev, 0,</span>
<span class="p_add">+						   table-&gt;rl_entry[i].index);</span>
 
 	kfree(dev-&gt;priv.rl_table.rl_entry);
 }
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlx5/core/vxlan.c b/drivers/net/ethernet/mellanox/mlx5/core/vxlan.c</span>
<span class="p_header">index 07a9ba6cfc70..2f74953e4561 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlx5/core/vxlan.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlx5/core/vxlan.c</span>
<span class="p_chunk">@@ -71,9 +71,9 @@</span> <span class="p_context"> struct mlx5e_vxlan *mlx5e_vxlan_lookup_port(struct mlx5e_priv *priv, u16 port)</span>
 	struct mlx5e_vxlan_db *vxlan_db = &amp;priv-&gt;vxlan;
 	struct mlx5e_vxlan *vxlan;
 
<span class="p_del">-	spin_lock(&amp;vxlan_db-&gt;lock);</span>
<span class="p_add">+	spin_lock_bh(&amp;vxlan_db-&gt;lock);</span>
 	vxlan = radix_tree_lookup(&amp;vxlan_db-&gt;tree, port);
<span class="p_del">-	spin_unlock(&amp;vxlan_db-&gt;lock);</span>
<span class="p_add">+	spin_unlock_bh(&amp;vxlan_db-&gt;lock);</span>
 
 	return vxlan;
 }
<span class="p_chunk">@@ -88,8 +88,12 @@</span> <span class="p_context"> static void mlx5e_vxlan_add_port(struct work_struct *work)</span>
 	struct mlx5e_vxlan *vxlan;
 	int err;
 
<span class="p_del">-	if (mlx5e_vxlan_lookup_port(priv, port))</span>
<span class="p_add">+	mutex_lock(&amp;priv-&gt;state_lock);</span>
<span class="p_add">+	vxlan = mlx5e_vxlan_lookup_port(priv, port);</span>
<span class="p_add">+	if (vxlan) {</span>
<span class="p_add">+		atomic_inc(&amp;vxlan-&gt;refcount);</span>
 		goto free_work;
<span class="p_add">+	}</span>
 
 	if (mlx5e_vxlan_core_add_port_cmd(priv-&gt;mdev, port))
 		goto free_work;
<span class="p_chunk">@@ -99,10 +103,11 @@</span> <span class="p_context"> static void mlx5e_vxlan_add_port(struct work_struct *work)</span>
 		goto err_delete_port;
 
 	vxlan-&gt;udp_port = port;
<span class="p_add">+	atomic_set(&amp;vxlan-&gt;refcount, 1);</span>
 
<span class="p_del">-	spin_lock_irq(&amp;vxlan_db-&gt;lock);</span>
<span class="p_add">+	spin_lock_bh(&amp;vxlan_db-&gt;lock);</span>
 	err = radix_tree_insert(&amp;vxlan_db-&gt;tree, vxlan-&gt;udp_port, vxlan);
<span class="p_del">-	spin_unlock_irq(&amp;vxlan_db-&gt;lock);</span>
<span class="p_add">+	spin_unlock_bh(&amp;vxlan_db-&gt;lock);</span>
 	if (err)
 		goto err_free;
 
<span class="p_chunk">@@ -113,35 +118,39 @@</span> <span class="p_context"> static void mlx5e_vxlan_add_port(struct work_struct *work)</span>
 err_delete_port:
 	mlx5e_vxlan_core_del_port_cmd(priv-&gt;mdev, port);
 free_work:
<span class="p_add">+	mutex_unlock(&amp;priv-&gt;state_lock);</span>
 	kfree(vxlan_work);
 }
 
<span class="p_del">-static void __mlx5e_vxlan_core_del_port(struct mlx5e_priv *priv, u16 port)</span>
<span class="p_add">+static void mlx5e_vxlan_del_port(struct work_struct *work)</span>
 {
<span class="p_add">+	struct mlx5e_vxlan_work *vxlan_work =</span>
<span class="p_add">+		container_of(work, struct mlx5e_vxlan_work, work);</span>
<span class="p_add">+	struct mlx5e_priv *priv         = vxlan_work-&gt;priv;</span>
 	struct mlx5e_vxlan_db *vxlan_db = &amp;priv-&gt;vxlan;
<span class="p_add">+	u16 port = vxlan_work-&gt;port;</span>
 	struct mlx5e_vxlan *vxlan;
<span class="p_add">+	bool remove = false;</span>
 
<span class="p_del">-	spin_lock_irq(&amp;vxlan_db-&gt;lock);</span>
<span class="p_del">-	vxlan = radix_tree_delete(&amp;vxlan_db-&gt;tree, port);</span>
<span class="p_del">-	spin_unlock_irq(&amp;vxlan_db-&gt;lock);</span>
<span class="p_del">-</span>
<span class="p_add">+	mutex_lock(&amp;priv-&gt;state_lock);</span>
<span class="p_add">+	spin_lock_bh(&amp;vxlan_db-&gt;lock);</span>
<span class="p_add">+	vxlan = radix_tree_lookup(&amp;vxlan_db-&gt;tree, port);</span>
 	if (!vxlan)
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	mlx5e_vxlan_core_del_port_cmd(priv-&gt;mdev, vxlan-&gt;udp_port);</span>
<span class="p_del">-</span>
<span class="p_del">-	kfree(vxlan);</span>
<span class="p_del">-}</span>
<span class="p_add">+		goto out_unlock;</span>
 
<span class="p_del">-static void mlx5e_vxlan_del_port(struct work_struct *work)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct mlx5e_vxlan_work *vxlan_work =</span>
<span class="p_del">-		container_of(work, struct mlx5e_vxlan_work, work);</span>
<span class="p_del">-	struct mlx5e_priv *priv = vxlan_work-&gt;priv;</span>
<span class="p_del">-	u16 port = vxlan_work-&gt;port;</span>
<span class="p_add">+	if (atomic_dec_and_test(&amp;vxlan-&gt;refcount)) {</span>
<span class="p_add">+		radix_tree_delete(&amp;vxlan_db-&gt;tree, port);</span>
<span class="p_add">+		remove = true;</span>
<span class="p_add">+	}</span>
 
<span class="p_del">-	__mlx5e_vxlan_core_del_port(priv, port);</span>
<span class="p_add">+out_unlock:</span>
<span class="p_add">+	spin_unlock_bh(&amp;vxlan_db-&gt;lock);</span>
 
<span class="p_add">+	if (remove) {</span>
<span class="p_add">+		mlx5e_vxlan_core_del_port_cmd(priv-&gt;mdev, port);</span>
<span class="p_add">+		kfree(vxlan);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	mutex_unlock(&amp;priv-&gt;state_lock);</span>
 	kfree(vxlan_work);
 }
 
<span class="p_chunk">@@ -171,12 +180,11 @@</span> <span class="p_context"> void mlx5e_vxlan_cleanup(struct mlx5e_priv *priv)</span>
 	struct mlx5e_vxlan *vxlan;
 	unsigned int port = 0;
 
<span class="p_del">-	spin_lock_irq(&amp;vxlan_db-&gt;lock);</span>
<span class="p_add">+	/* Lockless since we are the only radix-tree consumers, wq is disabled */</span>
 	while (radix_tree_gang_lookup(&amp;vxlan_db-&gt;tree, (void **)&amp;vxlan, port, 1)) {
 		port = vxlan-&gt;udp_port;
<span class="p_del">-		spin_unlock_irq(&amp;vxlan_db-&gt;lock);</span>
<span class="p_del">-		__mlx5e_vxlan_core_del_port(priv, (u16)port);</span>
<span class="p_del">-		spin_lock_irq(&amp;vxlan_db-&gt;lock);</span>
<span class="p_add">+		radix_tree_delete(&amp;vxlan_db-&gt;tree, port);</span>
<span class="p_add">+		mlx5e_vxlan_core_del_port_cmd(priv-&gt;mdev, port);</span>
<span class="p_add">+		kfree(vxlan);</span>
 	}
<span class="p_del">-	spin_unlock_irq(&amp;vxlan_db-&gt;lock);</span>
 }
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlx5/core/vxlan.h b/drivers/net/ethernet/mellanox/mlx5/core/vxlan.h</span>
<span class="p_header">index 5def12c048e3..5ef6ae7d568a 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlx5/core/vxlan.h</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlx5/core/vxlan.h</span>
<span class="p_chunk">@@ -36,6 +36,7 @@</span> <span class="p_context"></span>
 #include &quot;en.h&quot;
 
 struct mlx5e_vxlan {
<span class="p_add">+	atomic_t refcount;</span>
 	u16 udp_port;
 };
 
<span class="p_header">diff --git a/drivers/net/ethernet/mellanox/mlxsw/spectrum.c b/drivers/net/ethernet/mellanox/mlxsw/spectrum.c</span>
<span class="p_header">index db38880f54b4..3ead7439821c 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/mellanox/mlxsw/spectrum.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/mellanox/mlxsw/spectrum.c</span>
<span class="p_chunk">@@ -4164,6 +4164,7 @@</span> <span class="p_context"> static int mlxsw_sp_port_stp_set(struct mlxsw_sp_port *mlxsw_sp_port,</span>
 
 static int mlxsw_sp_port_ovs_join(struct mlxsw_sp_port *mlxsw_sp_port)
 {
<span class="p_add">+	u16 vid = 1;</span>
 	int err;
 
 	err = mlxsw_sp_port_vp_mode_set(mlxsw_sp_port, true);
<span class="p_chunk">@@ -4176,8 +4177,19 @@</span> <span class="p_context"> static int mlxsw_sp_port_ovs_join(struct mlxsw_sp_port *mlxsw_sp_port)</span>
 				     true, false);
 	if (err)
 		goto err_port_vlan_set;
<span class="p_add">+</span>
<span class="p_add">+	for (; vid &lt;= VLAN_N_VID - 1; vid++) {</span>
<span class="p_add">+		err = mlxsw_sp_port_vid_learning_set(mlxsw_sp_port,</span>
<span class="p_add">+						     vid, false);</span>
<span class="p_add">+		if (err)</span>
<span class="p_add">+			goto err_vid_learning_set;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	return 0;
 
<span class="p_add">+err_vid_learning_set:</span>
<span class="p_add">+	for (vid--; vid &gt;= 1; vid--)</span>
<span class="p_add">+		mlxsw_sp_port_vid_learning_set(mlxsw_sp_port, vid, true);</span>
 err_port_vlan_set:
 	mlxsw_sp_port_stp_set(mlxsw_sp_port, false);
 err_port_stp_set:
<span class="p_chunk">@@ -4187,6 +4199,12 @@</span> <span class="p_context"> static int mlxsw_sp_port_ovs_join(struct mlxsw_sp_port *mlxsw_sp_port)</span>
 
 static void mlxsw_sp_port_ovs_leave(struct mlxsw_sp_port *mlxsw_sp_port)
 {
<span class="p_add">+	u16 vid;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (vid = VLAN_N_VID - 1; vid &gt;= 1; vid--)</span>
<span class="p_add">+		mlxsw_sp_port_vid_learning_set(mlxsw_sp_port,</span>
<span class="p_add">+					       vid, true);</span>
<span class="p_add">+</span>
 	mlxsw_sp_port_vlan_set(mlxsw_sp_port, 2, VLAN_N_VID - 1,
 			       false, false);
 	mlxsw_sp_port_stp_set(mlxsw_sp_port, false);
<span class="p_header">diff --git a/drivers/net/ethernet/sfc/tx.c b/drivers/net/ethernet/sfc/tx.c</span>
<span class="p_header">index 32bf1fecf864..9b85cbd5a231 100644</span>
<span class="p_header">--- a/drivers/net/ethernet/sfc/tx.c</span>
<span class="p_header">+++ b/drivers/net/ethernet/sfc/tx.c</span>
<span class="p_chunk">@@ -77,6 +77,7 @@</span> <span class="p_context"> static void efx_dequeue_buffer(struct efx_tx_queue *tx_queue,</span>
 	}
 
 	if (buffer-&gt;flags &amp; EFX_TX_BUF_SKB) {
<span class="p_add">+		EFX_WARN_ON_PARANOID(!pkts_compl || !bytes_compl);</span>
 		(*pkts_compl)++;
 		(*bytes_compl) += buffer-&gt;skb-&gt;len;
 		dev_consume_skb_any((struct sk_buff *)buffer-&gt;skb);
<span class="p_chunk">@@ -426,12 +427,14 @@</span> <span class="p_context"> static int efx_tx_map_data(struct efx_tx_queue *tx_queue, struct sk_buff *skb,</span>
 static void efx_enqueue_unwind(struct efx_tx_queue *tx_queue)
 {
 	struct efx_tx_buffer *buffer;
<span class="p_add">+	unsigned int bytes_compl = 0;</span>
<span class="p_add">+	unsigned int pkts_compl = 0;</span>
 
 	/* Work backwards until we hit the original insert pointer value */
 	while (tx_queue-&gt;insert_count != tx_queue-&gt;write_count) {
 		--tx_queue-&gt;insert_count;
 		buffer = __efx_tx_queue_get_insert_buffer(tx_queue);
<span class="p_del">-		efx_dequeue_buffer(tx_queue, buffer, NULL, NULL);</span>
<span class="p_add">+		efx_dequeue_buffer(tx_queue, buffer, &amp;pkts_compl, &amp;bytes_compl);</span>
 	}
 }
 
<span class="p_header">diff --git a/drivers/net/phy/marvell.c b/drivers/net/phy/marvell.c</span>
<span class="p_header">index 4d02b27df044..a3f456b91c99 100644</span>
<span class="p_header">--- a/drivers/net/phy/marvell.c</span>
<span class="p_header">+++ b/drivers/net/phy/marvell.c</span>
<span class="p_chunk">@@ -2069,7 +2069,7 @@</span> <span class="p_context"> static struct phy_driver marvell_drivers[] = {</span>
 		.flags = PHY_HAS_INTERRUPT,
 		.probe = marvell_probe,
 		.config_init = &amp;m88e1145_config_init,
<span class="p_del">-		.config_aneg = &amp;marvell_config_aneg,</span>
<span class="p_add">+		.config_aneg = &amp;m88e1101_config_aneg,</span>
 		.read_status = &amp;genphy_read_status,
 		.ack_interrupt = &amp;marvell_ack_interrupt,
 		.config_intr = &amp;marvell_config_intr,
<span class="p_header">diff --git a/drivers/net/phy/micrel.c b/drivers/net/phy/micrel.c</span>
<span class="p_header">index fdb43dd9b5cd..6c45ff650ec7 100644</span>
<span class="p_header">--- a/drivers/net/phy/micrel.c</span>
<span class="p_header">+++ b/drivers/net/phy/micrel.c</span>
<span class="p_chunk">@@ -622,6 +622,7 @@</span> <span class="p_context"> static int ksz9031_read_status(struct phy_device *phydev)</span>
 		phydev-&gt;link = 0;
 		if (phydev-&gt;drv-&gt;config_intr &amp;&amp; phy_interrupt_is_valid(phydev))
 			phydev-&gt;drv-&gt;config_intr(phydev);
<span class="p_add">+		return genphy_config_aneg(phydev);</span>
 	}
 
 	return 0;
<span class="p_header">diff --git a/drivers/net/phy/phylink.c b/drivers/net/phy/phylink.c</span>
<span class="p_header">index bcb4755bcd95..4b377b978a0b 100644</span>
<span class="p_header">--- a/drivers/net/phy/phylink.c</span>
<span class="p_header">+++ b/drivers/net/phy/phylink.c</span>
<span class="p_chunk">@@ -525,6 +525,7 @@</span> <span class="p_context"> struct phylink *phylink_create(struct net_device *ndev, struct device_node *np,</span>
 	pl-&gt;link_config.pause = MLO_PAUSE_AN;
 	pl-&gt;link_config.speed = SPEED_UNKNOWN;
 	pl-&gt;link_config.duplex = DUPLEX_UNKNOWN;
<span class="p_add">+	pl-&gt;link_config.an_enabled = true;</span>
 	pl-&gt;ops = ops;
 	__set_bit(PHYLINK_DISABLE_STOPPED, &amp;pl-&gt;phylink_disable_state);
 
<span class="p_chunk">@@ -948,6 +949,7 @@</span> <span class="p_context"> int phylink_ethtool_ksettings_set(struct phylink *pl,</span>
 	mutex_lock(&amp;pl-&gt;state_mutex);
 	/* Configure the MAC to match the new settings */
 	linkmode_copy(pl-&gt;link_config.advertising, our_kset.link_modes.advertising);
<span class="p_add">+	pl-&gt;link_config.interface = config.interface;</span>
 	pl-&gt;link_config.speed = our_kset.base.speed;
 	pl-&gt;link_config.duplex = our_kset.base.duplex;
 	pl-&gt;link_config.an_enabled = our_kset.base.autoneg != AUTONEG_DISABLE;
<span class="p_header">diff --git a/drivers/net/usb/qmi_wwan.c b/drivers/net/usb/qmi_wwan.c</span>
<span class="p_header">index 81394a4b2803..2092febfcb42 100644</span>
<span class="p_header">--- a/drivers/net/usb/qmi_wwan.c</span>
<span class="p_header">+++ b/drivers/net/usb/qmi_wwan.c</span>
<span class="p_chunk">@@ -1204,6 +1204,7 @@</span> <span class="p_context"> static const struct usb_device_id products[] = {</span>
 	{QMI_FIXED_INTF(0x1199, 0x9079, 10)},	/* Sierra Wireless EM74xx */
 	{QMI_FIXED_INTF(0x1199, 0x907b, 8)},	/* Sierra Wireless EM74xx */
 	{QMI_FIXED_INTF(0x1199, 0x907b, 10)},	/* Sierra Wireless EM74xx */
<span class="p_add">+	{QMI_FIXED_INTF(0x1199, 0x9091, 8)},	/* Sierra Wireless EM7565 */</span>
 	{QMI_FIXED_INTF(0x1bbb, 0x011e, 4)},	/* Telekom Speedstick LTE II (Alcatel One Touch L100V LTE) */
 	{QMI_FIXED_INTF(0x1bbb, 0x0203, 2)},	/* Alcatel L800MA */
 	{QMI_FIXED_INTF(0x2357, 0x0201, 4)},	/* TP-LINK HSUPA Modem MA180 */
<span class="p_header">diff --git a/drivers/net/vxlan.c b/drivers/net/vxlan.c</span>
<span class="p_header">index a2f4e52fadb5..9e9202b50e73 100644</span>
<span class="p_header">--- a/drivers/net/vxlan.c</span>
<span class="p_header">+++ b/drivers/net/vxlan.c</span>
<span class="p_chunk">@@ -3105,6 +3105,11 @@</span> <span class="p_context"> static void vxlan_config_apply(struct net_device *dev,</span>
 
 		max_mtu = lowerdev-&gt;mtu - (use_ipv6 ? VXLAN6_HEADROOM :
 					   VXLAN_HEADROOM);
<span class="p_add">+		if (max_mtu &lt; ETH_MIN_MTU)</span>
<span class="p_add">+			max_mtu = ETH_MIN_MTU;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!changelink &amp;&amp; !conf-&gt;mtu)</span>
<span class="p_add">+			dev-&gt;mtu = max_mtu;</span>
 	}
 
 	if (dev-&gt;mtu &gt; max_mtu)
<span class="p_header">diff --git a/drivers/phy/tegra/xusb.c b/drivers/phy/tegra/xusb.c</span>
<span class="p_header">index 4307bf0013e1..63e916d4d069 100644</span>
<span class="p_header">--- a/drivers/phy/tegra/xusb.c</span>
<span class="p_header">+++ b/drivers/phy/tegra/xusb.c</span>
<span class="p_chunk">@@ -75,14 +75,14 @@</span> <span class="p_context"> MODULE_DEVICE_TABLE(of, tegra_xusb_padctl_of_match);</span>
 static struct device_node *
 tegra_xusb_find_pad_node(struct tegra_xusb_padctl *padctl, const char *name)
 {
<span class="p_del">-	/*</span>
<span class="p_del">-	 * of_find_node_by_name() drops a reference, so make sure to grab one.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	struct device_node *np = of_node_get(padctl-&gt;dev-&gt;of_node);</span>
<span class="p_add">+	struct device_node *pads, *np;</span>
<span class="p_add">+</span>
<span class="p_add">+	pads = of_get_child_by_name(padctl-&gt;dev-&gt;of_node, &quot;pads&quot;);</span>
<span class="p_add">+	if (!pads)</span>
<span class="p_add">+		return NULL;</span>
 
<span class="p_del">-	np = of_find_node_by_name(np, &quot;pads&quot;);</span>
<span class="p_del">-	if (np)</span>
<span class="p_del">-		np = of_find_node_by_name(np, name);</span>
<span class="p_add">+	np = of_get_child_by_name(pads, name);</span>
<span class="p_add">+	of_node_put(pads);</span>
 
 	return np;
 }
<span class="p_chunk">@@ -90,16 +90,16 @@</span> <span class="p_context"> tegra_xusb_find_pad_node(struct tegra_xusb_padctl *padctl, const char *name)</span>
 static struct device_node *
 tegra_xusb_pad_find_phy_node(struct tegra_xusb_pad *pad, unsigned int index)
 {
<span class="p_del">-	/*</span>
<span class="p_del">-	 * of_find_node_by_name() drops a reference, so make sure to grab one.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	struct device_node *np = of_node_get(pad-&gt;dev.of_node);</span>
<span class="p_add">+	struct device_node *np, *lanes;</span>
 
<span class="p_del">-	np = of_find_node_by_name(np, &quot;lanes&quot;);</span>
<span class="p_del">-	if (!np)</span>
<span class="p_add">+	lanes = of_get_child_by_name(pad-&gt;dev.of_node, &quot;lanes&quot;);</span>
<span class="p_add">+	if (!lanes)</span>
 		return NULL;
 
<span class="p_del">-	return of_find_node_by_name(np, pad-&gt;soc-&gt;lanes[index].name);</span>
<span class="p_add">+	np = of_get_child_by_name(lanes, pad-&gt;soc-&gt;lanes[index].name);</span>
<span class="p_add">+	of_node_put(lanes);</span>
<span class="p_add">+</span>
<span class="p_add">+	return np;</span>
 }
 
 static int
<span class="p_chunk">@@ -195,7 +195,7 @@</span> <span class="p_context"> int tegra_xusb_pad_register(struct tegra_xusb_pad *pad,</span>
 	unsigned int i;
 	int err;
 
<span class="p_del">-	children = of_find_node_by_name(pad-&gt;dev.of_node, &quot;lanes&quot;);</span>
<span class="p_add">+	children = of_get_child_by_name(pad-&gt;dev.of_node, &quot;lanes&quot;);</span>
 	if (!children)
 		return -ENODEV;
 
<span class="p_chunk">@@ -444,21 +444,21 @@</span> <span class="p_context"> static struct device_node *</span>
 tegra_xusb_find_port_node(struct tegra_xusb_padctl *padctl, const char *type,
 			  unsigned int index)
 {
<span class="p_del">-	/*</span>
<span class="p_del">-	 * of_find_node_by_name() drops a reference, so make sure to grab one.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	struct device_node *np = of_node_get(padctl-&gt;dev-&gt;of_node);</span>
<span class="p_add">+	struct device_node *ports, *np;</span>
<span class="p_add">+	char *name;</span>
 
<span class="p_del">-	np = of_find_node_by_name(np, &quot;ports&quot;);</span>
<span class="p_del">-	if (np) {</span>
<span class="p_del">-		char *name;</span>
<span class="p_add">+	ports = of_get_child_by_name(padctl-&gt;dev-&gt;of_node, &quot;ports&quot;);</span>
<span class="p_add">+	if (!ports)</span>
<span class="p_add">+		return NULL;</span>
 
<span class="p_del">-		name = kasprintf(GFP_KERNEL, &quot;%s-%u&quot;, type, index);</span>
<span class="p_del">-		if (!name)</span>
<span class="p_del">-			return ERR_PTR(-ENOMEM);</span>
<span class="p_del">-		np = of_find_node_by_name(np, name);</span>
<span class="p_del">-		kfree(name);</span>
<span class="p_add">+	name = kasprintf(GFP_KERNEL, &quot;%s-%u&quot;, type, index);</span>
<span class="p_add">+	if (!name) {</span>
<span class="p_add">+		of_node_put(ports);</span>
<span class="p_add">+		return ERR_PTR(-ENOMEM);</span>
 	}
<span class="p_add">+	np = of_get_child_by_name(ports, name);</span>
<span class="p_add">+	kfree(name);</span>
<span class="p_add">+	of_node_put(ports);</span>
 
 	return np;
 }
<span class="p_chunk">@@ -847,7 +847,7 @@</span> <span class="p_context"> static void tegra_xusb_remove_ports(struct tegra_xusb_padctl *padctl)</span>
 
 static int tegra_xusb_padctl_probe(struct platform_device *pdev)
 {
<span class="p_del">-	struct device_node *np = of_node_get(pdev-&gt;dev.of_node);</span>
<span class="p_add">+	struct device_node *np = pdev-&gt;dev.of_node;</span>
 	const struct tegra_xusb_padctl_soc *soc;
 	struct tegra_xusb_padctl *padctl;
 	const struct of_device_id *match;
<span class="p_chunk">@@ -855,7 +855,7 @@</span> <span class="p_context"> static int tegra_xusb_padctl_probe(struct platform_device *pdev)</span>
 	int err;
 
 	/* for backwards compatibility with old device trees */
<span class="p_del">-	np = of_find_node_by_name(np, &quot;pads&quot;);</span>
<span class="p_add">+	np = of_get_child_by_name(np, &quot;pads&quot;);</span>
 	if (!np) {
 		dev_warn(&amp;pdev-&gt;dev, &quot;deprecated DT, using legacy driver\n&quot;);
 		return tegra_xusb_padctl_legacy_probe(pdev);
<span class="p_header">diff --git a/drivers/s390/net/qeth_core.h b/drivers/s390/net/qeth_core.h</span>
<span class="p_header">index 5340efc673a9..92dd4aef21a3 100644</span>
<span class="p_header">--- a/drivers/s390/net/qeth_core.h</span>
<span class="p_header">+++ b/drivers/s390/net/qeth_core.h</span>
<span class="p_chunk">@@ -564,9 +564,9 @@</span> <span class="p_context"> enum qeth_cq {</span>
 };
 
 struct qeth_ipato {
<span class="p_del">-	int enabled;</span>
<span class="p_del">-	int invert4;</span>
<span class="p_del">-	int invert6;</span>
<span class="p_add">+	bool enabled;</span>
<span class="p_add">+	bool invert4;</span>
<span class="p_add">+	bool invert6;</span>
 	struct list_head entries;
 };
 
<span class="p_header">diff --git a/drivers/s390/net/qeth_core_main.c b/drivers/s390/net/qeth_core_main.c</span>
<span class="p_header">index 330e5d3dadf3..7c7a244b6684 100644</span>
<span class="p_header">--- a/drivers/s390/net/qeth_core_main.c</span>
<span class="p_header">+++ b/drivers/s390/net/qeth_core_main.c</span>
<span class="p_chunk">@@ -1479,9 +1479,9 @@</span> <span class="p_context"> static int qeth_setup_card(struct qeth_card *card)</span>
 	qeth_set_intial_options(card);
 	/* IP address takeover */
 	INIT_LIST_HEAD(&amp;card-&gt;ipato.entries);
<span class="p_del">-	card-&gt;ipato.enabled = 0;</span>
<span class="p_del">-	card-&gt;ipato.invert4 = 0;</span>
<span class="p_del">-	card-&gt;ipato.invert6 = 0;</span>
<span class="p_add">+	card-&gt;ipato.enabled = false;</span>
<span class="p_add">+	card-&gt;ipato.invert4 = false;</span>
<span class="p_add">+	card-&gt;ipato.invert6 = false;</span>
 	/* init QDIO stuff */
 	qeth_init_qdio_info(card);
 	INIT_DELAYED_WORK(&amp;card-&gt;buffer_reclaim_work, qeth_buffer_reclaim_work);
<span class="p_chunk">@@ -5445,6 +5445,13 @@</span> <span class="p_context"> int qeth_poll(struct napi_struct *napi, int budget)</span>
 }
 EXPORT_SYMBOL_GPL(qeth_poll);
 
<span class="p_add">+static int qeth_setassparms_inspect_rc(struct qeth_ipa_cmd *cmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!cmd-&gt;hdr.return_code)</span>
<span class="p_add">+		cmd-&gt;hdr.return_code = cmd-&gt;data.setassparms.hdr.return_code;</span>
<span class="p_add">+	return cmd-&gt;hdr.return_code;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int qeth_setassparms_cb(struct qeth_card *card,
 			struct qeth_reply *reply, unsigned long data)
 {
<span class="p_chunk">@@ -6304,7 +6311,7 @@</span> <span class="p_context"> static int qeth_ipa_checksum_run_cmd_cb(struct qeth_card *card,</span>
 				(struct qeth_checksum_cmd *)reply-&gt;param;
 
 	QETH_CARD_TEXT(card, 4, &quot;chkdoccb&quot;);
<span class="p_del">-	if (cmd-&gt;hdr.return_code)</span>
<span class="p_add">+	if (qeth_setassparms_inspect_rc(cmd))</span>
 		return 0;
 
 	memset(chksum_cb, 0, sizeof(*chksum_cb));
<span class="p_header">diff --git a/drivers/s390/net/qeth_l3.h b/drivers/s390/net/qeth_l3.h</span>
<span class="p_header">index 194ae9b577cc..e5833837b799 100644</span>
<span class="p_header">--- a/drivers/s390/net/qeth_l3.h</span>
<span class="p_header">+++ b/drivers/s390/net/qeth_l3.h</span>
<span class="p_chunk">@@ -82,7 +82,7 @@</span> <span class="p_context"> void qeth_l3_del_vipa(struct qeth_card *, enum qeth_prot_versions, const u8 *);</span>
 int qeth_l3_add_rxip(struct qeth_card *, enum qeth_prot_versions, const u8 *);
 void qeth_l3_del_rxip(struct qeth_card *card, enum qeth_prot_versions,
 			const u8 *);
<span class="p_del">-int qeth_l3_is_addr_covered_by_ipato(struct qeth_card *, struct qeth_ipaddr *);</span>
<span class="p_add">+void qeth_l3_update_ipato(struct qeth_card *card);</span>
 struct qeth_ipaddr *qeth_l3_get_addr_buffer(enum qeth_prot_versions);
 int qeth_l3_add_ip(struct qeth_card *, struct qeth_ipaddr *);
 int qeth_l3_delete_ip(struct qeth_card *, struct qeth_ipaddr *);
<span class="p_header">diff --git a/drivers/s390/net/qeth_l3_main.c b/drivers/s390/net/qeth_l3_main.c</span>
<span class="p_header">index 27185ab38136..36dee176f8e2 100644</span>
<span class="p_header">--- a/drivers/s390/net/qeth_l3_main.c</span>
<span class="p_header">+++ b/drivers/s390/net/qeth_l3_main.c</span>
<span class="p_chunk">@@ -163,8 +163,8 @@</span> <span class="p_context"> static void qeth_l3_convert_addr_to_bits(u8 *addr, u8 *bits, int len)</span>
 	}
 }
 
<span class="p_del">-int qeth_l3_is_addr_covered_by_ipato(struct qeth_card *card,</span>
<span class="p_del">-						struct qeth_ipaddr *addr)</span>
<span class="p_add">+static bool qeth_l3_is_addr_covered_by_ipato(struct qeth_card *card,</span>
<span class="p_add">+					     struct qeth_ipaddr *addr)</span>
 {
 	struct qeth_ipato_entry *ipatoe;
 	u8 addr_bits[128] = {0, };
<span class="p_chunk">@@ -173,6 +173,8 @@</span> <span class="p_context"> int qeth_l3_is_addr_covered_by_ipato(struct qeth_card *card,</span>
 
 	if (!card-&gt;ipato.enabled)
 		return 0;
<span class="p_add">+	if (addr-&gt;type != QETH_IP_TYPE_NORMAL)</span>
<span class="p_add">+		return 0;</span>
 
 	qeth_l3_convert_addr_to_bits((u8 *) &amp;addr-&gt;u, addr_bits,
 				  (addr-&gt;proto == QETH_PROT_IPV4)? 4:16);
<span class="p_chunk">@@ -289,8 +291,7 @@</span> <span class="p_context"> int qeth_l3_add_ip(struct qeth_card *card, struct qeth_ipaddr *tmp_addr)</span>
 		memcpy(addr, tmp_addr, sizeof(struct qeth_ipaddr));
 		addr-&gt;ref_counter = 1;
 
<span class="p_del">-		if (addr-&gt;type == QETH_IP_TYPE_NORMAL  &amp;&amp;</span>
<span class="p_del">-				qeth_l3_is_addr_covered_by_ipato(card, addr)) {</span>
<span class="p_add">+		if (qeth_l3_is_addr_covered_by_ipato(card, addr)) {</span>
 			QETH_CARD_TEXT(card, 2, &quot;tkovaddr&quot;);
 			addr-&gt;set_flags |= QETH_IPA_SETIP_TAKEOVER_FLAG;
 		}
<span class="p_chunk">@@ -604,6 +605,27 @@</span> <span class="p_context"> int qeth_l3_setrouting_v6(struct qeth_card *card)</span>
 /*
  * IP address takeover related functions
  */
<span class="p_add">+</span>
<span class="p_add">+/**</span>
<span class="p_add">+ * qeth_l3_update_ipato() - Update &#39;takeover&#39; property, for all NORMAL IPs.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Caller must hold ip_lock.</span>
<span class="p_add">+ */</span>
<span class="p_add">+void qeth_l3_update_ipato(struct qeth_card *card)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct qeth_ipaddr *addr;</span>
<span class="p_add">+	unsigned int i;</span>
<span class="p_add">+</span>
<span class="p_add">+	hash_for_each(card-&gt;ip_htable, i, addr, hnode) {</span>
<span class="p_add">+		if (addr-&gt;type != QETH_IP_TYPE_NORMAL)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		if (qeth_l3_is_addr_covered_by_ipato(card, addr))</span>
<span class="p_add">+			addr-&gt;set_flags |= QETH_IPA_SETIP_TAKEOVER_FLAG;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			addr-&gt;set_flags &amp;= ~QETH_IPA_SETIP_TAKEOVER_FLAG;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static void qeth_l3_clear_ipato_list(struct qeth_card *card)
 {
 	struct qeth_ipato_entry *ipatoe, *tmp;
<span class="p_chunk">@@ -615,6 +637,7 @@</span> <span class="p_context"> static void qeth_l3_clear_ipato_list(struct qeth_card *card)</span>
 		kfree(ipatoe);
 	}
 
<span class="p_add">+	qeth_l3_update_ipato(card);</span>
 	spin_unlock_bh(&amp;card-&gt;ip_lock);
 }
 
<span class="p_chunk">@@ -639,8 +662,10 @@</span> <span class="p_context"> int qeth_l3_add_ipato_entry(struct qeth_card *card,</span>
 		}
 	}
 
<span class="p_del">-	if (!rc)</span>
<span class="p_add">+	if (!rc) {</span>
 		list_add_tail(&amp;new-&gt;entry, &amp;card-&gt;ipato.entries);
<span class="p_add">+		qeth_l3_update_ipato(card);</span>
<span class="p_add">+	}</span>
 
 	spin_unlock_bh(&amp;card-&gt;ip_lock);
 
<span class="p_chunk">@@ -663,6 +688,7 @@</span> <span class="p_context"> void qeth_l3_del_ipato_entry(struct qeth_card *card,</span>
 			    (proto == QETH_PROT_IPV4)? 4:16) &amp;&amp;
 		    (ipatoe-&gt;mask_bits == mask_bits)) {
 			list_del(&amp;ipatoe-&gt;entry);
<span class="p_add">+			qeth_l3_update_ipato(card);</span>
 			kfree(ipatoe);
 		}
 	}
<span class="p_header">diff --git a/drivers/s390/net/qeth_l3_sys.c b/drivers/s390/net/qeth_l3_sys.c</span>
<span class="p_header">index 7a829ad77783..1295dd8ec849 100644</span>
<span class="p_header">--- a/drivers/s390/net/qeth_l3_sys.c</span>
<span class="p_header">+++ b/drivers/s390/net/qeth_l3_sys.c</span>
<span class="p_chunk">@@ -370,8 +370,8 @@</span> <span class="p_context"> static ssize_t qeth_l3_dev_ipato_enable_store(struct device *dev,</span>
 		struct device_attribute *attr, const char *buf, size_t count)
 {
 	struct qeth_card *card = dev_get_drvdata(dev);
<span class="p_del">-	struct qeth_ipaddr *addr;</span>
<span class="p_del">-	int i, rc = 0;</span>
<span class="p_add">+	bool enable;</span>
<span class="p_add">+	int rc = 0;</span>
 
 	if (!card)
 		return -EINVAL;
<span class="p_chunk">@@ -384,25 +384,18 @@</span> <span class="p_context"> static ssize_t qeth_l3_dev_ipato_enable_store(struct device *dev,</span>
 	}
 
 	if (sysfs_streq(buf, &quot;toggle&quot;)) {
<span class="p_del">-		card-&gt;ipato.enabled = (card-&gt;ipato.enabled)? 0 : 1;</span>
<span class="p_del">-	} else if (sysfs_streq(buf, &quot;1&quot;)) {</span>
<span class="p_del">-		card-&gt;ipato.enabled = 1;</span>
<span class="p_del">-		hash_for_each(card-&gt;ip_htable, i, addr, hnode) {</span>
<span class="p_del">-				if ((addr-&gt;type == QETH_IP_TYPE_NORMAL) &amp;&amp;</span>
<span class="p_del">-				qeth_l3_is_addr_covered_by_ipato(card, addr))</span>
<span class="p_del">-					addr-&gt;set_flags |=</span>
<span class="p_del">-					QETH_IPA_SETIP_TAKEOVER_FLAG;</span>
<span class="p_del">-			}</span>
<span class="p_del">-	} else if (sysfs_streq(buf, &quot;0&quot;)) {</span>
<span class="p_del">-		card-&gt;ipato.enabled = 0;</span>
<span class="p_del">-		hash_for_each(card-&gt;ip_htable, i, addr, hnode) {</span>
<span class="p_del">-			if (addr-&gt;set_flags &amp;</span>
<span class="p_del">-			QETH_IPA_SETIP_TAKEOVER_FLAG)</span>
<span class="p_del">-				addr-&gt;set_flags &amp;=</span>
<span class="p_del">-				~QETH_IPA_SETIP_TAKEOVER_FLAG;</span>
<span class="p_del">-			}</span>
<span class="p_del">-	} else</span>
<span class="p_add">+		enable = !card-&gt;ipato.enabled;</span>
<span class="p_add">+	} else if (kstrtobool(buf, &amp;enable)) {</span>
 		rc = -EINVAL;
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (card-&gt;ipato.enabled != enable) {</span>
<span class="p_add">+		card-&gt;ipato.enabled = enable;</span>
<span class="p_add">+		spin_lock_bh(&amp;card-&gt;ip_lock);</span>
<span class="p_add">+		qeth_l3_update_ipato(card);</span>
<span class="p_add">+		spin_unlock_bh(&amp;card-&gt;ip_lock);</span>
<span class="p_add">+	}</span>
 out:
 	mutex_unlock(&amp;card-&gt;conf_mutex);
 	return rc ? rc : count;
<span class="p_chunk">@@ -428,20 +421,27 @@</span> <span class="p_context"> static ssize_t qeth_l3_dev_ipato_invert4_store(struct device *dev,</span>
 				const char *buf, size_t count)
 {
 	struct qeth_card *card = dev_get_drvdata(dev);
<span class="p_add">+	bool invert;</span>
 	int rc = 0;
 
 	if (!card)
 		return -EINVAL;
 
 	mutex_lock(&amp;card-&gt;conf_mutex);
<span class="p_del">-	if (sysfs_streq(buf, &quot;toggle&quot;))</span>
<span class="p_del">-		card-&gt;ipato.invert4 = (card-&gt;ipato.invert4)? 0 : 1;</span>
<span class="p_del">-	else if (sysfs_streq(buf, &quot;1&quot;))</span>
<span class="p_del">-		card-&gt;ipato.invert4 = 1;</span>
<span class="p_del">-	else if (sysfs_streq(buf, &quot;0&quot;))</span>
<span class="p_del">-		card-&gt;ipato.invert4 = 0;</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (sysfs_streq(buf, &quot;toggle&quot;)) {</span>
<span class="p_add">+		invert = !card-&gt;ipato.invert4;</span>
<span class="p_add">+	} else if (kstrtobool(buf, &amp;invert)) {</span>
 		rc = -EINVAL;
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (card-&gt;ipato.invert4 != invert) {</span>
<span class="p_add">+		card-&gt;ipato.invert4 = invert;</span>
<span class="p_add">+		spin_lock_bh(&amp;card-&gt;ip_lock);</span>
<span class="p_add">+		qeth_l3_update_ipato(card);</span>
<span class="p_add">+		spin_unlock_bh(&amp;card-&gt;ip_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+out:</span>
 	mutex_unlock(&amp;card-&gt;conf_mutex);
 	return rc ? rc : count;
 }
<span class="p_chunk">@@ -607,20 +607,27 @@</span> <span class="p_context"> static ssize_t qeth_l3_dev_ipato_invert6_store(struct device *dev,</span>
 		struct device_attribute *attr, const char *buf, size_t count)
 {
 	struct qeth_card *card = dev_get_drvdata(dev);
<span class="p_add">+	bool invert;</span>
 	int rc = 0;
 
 	if (!card)
 		return -EINVAL;
 
 	mutex_lock(&amp;card-&gt;conf_mutex);
<span class="p_del">-	if (sysfs_streq(buf, &quot;toggle&quot;))</span>
<span class="p_del">-		card-&gt;ipato.invert6 = (card-&gt;ipato.invert6)? 0 : 1;</span>
<span class="p_del">-	else if (sysfs_streq(buf, &quot;1&quot;))</span>
<span class="p_del">-		card-&gt;ipato.invert6 = 1;</span>
<span class="p_del">-	else if (sysfs_streq(buf, &quot;0&quot;))</span>
<span class="p_del">-		card-&gt;ipato.invert6 = 0;</span>
<span class="p_del">-	else</span>
<span class="p_add">+	if (sysfs_streq(buf, &quot;toggle&quot;)) {</span>
<span class="p_add">+		invert = !card-&gt;ipato.invert6;</span>
<span class="p_add">+	} else if (kstrtobool(buf, &amp;invert)) {</span>
 		rc = -EINVAL;
<span class="p_add">+		goto out;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (card-&gt;ipato.invert6 != invert) {</span>
<span class="p_add">+		card-&gt;ipato.invert6 = invert;</span>
<span class="p_add">+		spin_lock_bh(&amp;card-&gt;ip_lock);</span>
<span class="p_add">+		qeth_l3_update_ipato(card);</span>
<span class="p_add">+		spin_unlock_bh(&amp;card-&gt;ip_lock);</span>
<span class="p_add">+	}</span>
<span class="p_add">+out:</span>
 	mutex_unlock(&amp;card-&gt;conf_mutex);
 	return rc ? rc : count;
 }
<span class="p_header">diff --git a/drivers/scsi/osd/osd_initiator.c b/drivers/scsi/osd/osd_initiator.c</span>
<span class="p_header">index a4f28b7e4c65..e18877177f1b 100644</span>
<span class="p_header">--- a/drivers/scsi/osd/osd_initiator.c</span>
<span class="p_header">+++ b/drivers/scsi/osd/osd_initiator.c</span>
<span class="p_chunk">@@ -1576,7 +1576,9 @@</span> <span class="p_context"> static struct request *_make_request(struct request_queue *q, bool has_write,</span>
 		return req;
 
 	for_each_bio(bio) {
<span class="p_del">-		ret = blk_rq_append_bio(req, bio);</span>
<span class="p_add">+		struct bio *bounce_bio = bio;</span>
<span class="p_add">+</span>
<span class="p_add">+		ret = blk_rq_append_bio(req, &amp;bounce_bio);</span>
 		if (ret)
 			return ERR_PTR(ret);
 	}
<span class="p_header">diff --git a/drivers/staging/android/ion/ion.c b/drivers/staging/android/ion/ion.c</span>
<span class="p_header">index 93e2c90fa77d..83dc3292e9ab 100644</span>
<span class="p_header">--- a/drivers/staging/android/ion/ion.c</span>
<span class="p_header">+++ b/drivers/staging/android/ion/ion.c</span>
<span class="p_chunk">@@ -348,7 +348,7 @@</span> <span class="p_context"> static int ion_dma_buf_begin_cpu_access(struct dma_buf *dmabuf,</span>
 	mutex_lock(&amp;buffer-&gt;lock);
 	list_for_each_entry(a, &amp;buffer-&gt;attachments, list) {
 		dma_sync_sg_for_cpu(a-&gt;dev, a-&gt;table-&gt;sgl, a-&gt;table-&gt;nents,
<span class="p_del">-				    DMA_BIDIRECTIONAL);</span>
<span class="p_add">+				    direction);</span>
 	}
 	mutex_unlock(&amp;buffer-&gt;lock);
 
<span class="p_chunk">@@ -370,7 +370,7 @@</span> <span class="p_context"> static int ion_dma_buf_end_cpu_access(struct dma_buf *dmabuf,</span>
 	mutex_lock(&amp;buffer-&gt;lock);
 	list_for_each_entry(a, &amp;buffer-&gt;attachments, list) {
 		dma_sync_sg_for_device(a-&gt;dev, a-&gt;table-&gt;sgl, a-&gt;table-&gt;nents,
<span class="p_del">-				       DMA_BIDIRECTIONAL);</span>
<span class="p_add">+				       direction);</span>
 	}
 	mutex_unlock(&amp;buffer-&gt;lock);
 
<span class="p_header">diff --git a/drivers/target/target_core_pscsi.c b/drivers/target/target_core_pscsi.c</span>
<span class="p_header">index 7c69b4a9694d..0d99b242e82e 100644</span>
<span class="p_header">--- a/drivers/target/target_core_pscsi.c</span>
<span class="p_header">+++ b/drivers/target/target_core_pscsi.c</span>
<span class="p_chunk">@@ -920,7 +920,7 @@</span> <span class="p_context"> pscsi_map_sg(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,</span>
 					&quot; %d i: %d bio: %p, allocating another&quot;
 					&quot; bio\n&quot;, bio-&gt;bi_vcnt, i, bio);
 
<span class="p_del">-				rc = blk_rq_append_bio(req, bio);</span>
<span class="p_add">+				rc = blk_rq_append_bio(req, &amp;bio);</span>
 				if (rc) {
 					pr_err(&quot;pSCSI: failed to append bio\n&quot;);
 					goto fail;
<span class="p_chunk">@@ -938,7 +938,7 @@</span> <span class="p_context"> pscsi_map_sg(struct se_cmd *cmd, struct scatterlist *sgl, u32 sgl_nents,</span>
 	}
 
 	if (bio) {
<span class="p_del">-		rc = blk_rq_append_bio(req, bio);</span>
<span class="p_add">+		rc = blk_rq_append_bio(req, &amp;bio);</span>
 		if (rc) {
 			pr_err(&quot;pSCSI: failed to append bio\n&quot;);
 			goto fail;
<span class="p_header">diff --git a/drivers/tty/n_tty.c b/drivers/tty/n_tty.c</span>
<span class="p_header">index bdf0e6e89991..faf50df81622 100644</span>
<span class="p_header">--- a/drivers/tty/n_tty.c</span>
<span class="p_header">+++ b/drivers/tty/n_tty.c</span>
<span class="p_chunk">@@ -1764,7 +1764,7 @@</span> <span class="p_context"> static void n_tty_set_termios(struct tty_struct *tty, struct ktermios *old)</span>
 {
 	struct n_tty_data *ldata = tty-&gt;disc_data;
 
<span class="p_del">-	if (!old || (old-&gt;c_lflag ^ tty-&gt;termios.c_lflag) &amp; ICANON) {</span>
<span class="p_add">+	if (!old || (old-&gt;c_lflag ^ tty-&gt;termios.c_lflag) &amp; (ICANON | EXTPROC)) {</span>
 		bitmap_zero(ldata-&gt;read_flags, N_TTY_BUF_SIZE);
 		ldata-&gt;line_start = ldata-&gt;read_tail;
 		if (!L_ICANON(tty) || !read_cnt(ldata)) {
<span class="p_chunk">@@ -2427,7 +2427,7 @@</span> <span class="p_context"> static int n_tty_ioctl(struct tty_struct *tty, struct file *file,</span>
 		return put_user(tty_chars_in_buffer(tty), (int __user *) arg);
 	case TIOCINQ:
 		down_write(&amp;tty-&gt;termios_rwsem);
<span class="p_del">-		if (L_ICANON(tty))</span>
<span class="p_add">+		if (L_ICANON(tty) &amp;&amp; !L_EXTPROC(tty))</span>
 			retval = inq_canon(ldata);
 		else
 			retval = read_cnt(ldata);
<span class="p_header">diff --git a/drivers/tty/tty_buffer.c b/drivers/tty/tty_buffer.c</span>
<span class="p_header">index f8eba1c5412f..677fa99b7747 100644</span>
<span class="p_header">--- a/drivers/tty/tty_buffer.c</span>
<span class="p_header">+++ b/drivers/tty/tty_buffer.c</span>
<span class="p_chunk">@@ -446,7 +446,7 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(tty_prepare_flip_string);</span>
  *	Callers other than flush_to_ldisc() need to exclude the kworker
  *	from concurrent use of the line discipline, see paste_selection().
  *
<span class="p_del">- *	Returns the number of bytes not processed</span>
<span class="p_add">+ *	Returns the number of bytes processed</span>
  */
 int tty_ldisc_receive_buf(struct tty_ldisc *ld, const unsigned char *p,
 			  char *f, int count)
<span class="p_header">diff --git a/drivers/usb/chipidea/ci_hdrc_msm.c b/drivers/usb/chipidea/ci_hdrc_msm.c</span>
<span class="p_header">index bb626120296f..53f3bf459dd1 100644</span>
<span class="p_header">--- a/drivers/usb/chipidea/ci_hdrc_msm.c</span>
<span class="p_header">+++ b/drivers/usb/chipidea/ci_hdrc_msm.c</span>
<span class="p_chunk">@@ -251,7 +251,7 @@</span> <span class="p_context"> static int ci_hdrc_msm_probe(struct platform_device *pdev)</span>
 	if (ret)
 		goto err_mux;
 
<span class="p_del">-	ulpi_node = of_find_node_by_name(of_node_get(pdev-&gt;dev.of_node), &quot;ulpi&quot;);</span>
<span class="p_add">+	ulpi_node = of_get_child_by_name(pdev-&gt;dev.of_node, &quot;ulpi&quot;);</span>
 	if (ulpi_node) {
 		phy_node = of_get_next_available_child(ulpi_node, NULL);
 		ci-&gt;hsic = of_device_is_compatible(phy_node, &quot;qcom,usb-hsic-phy&quot;);
<span class="p_header">diff --git a/drivers/usb/core/config.c b/drivers/usb/core/config.c</span>
<span class="p_header">index 843ef46d2537..9e3355b97396 100644</span>
<span class="p_header">--- a/drivers/usb/core/config.c</span>
<span class="p_header">+++ b/drivers/usb/core/config.c</span>
<span class="p_chunk">@@ -1007,7 +1007,7 @@</span> <span class="p_context"> int usb_get_bos_descriptor(struct usb_device *dev)</span>
 		case USB_SSP_CAP_TYPE:
 			ssp_cap = (struct usb_ssp_cap_descriptor *)buffer;
 			ssac = (le32_to_cpu(ssp_cap-&gt;bmAttributes) &amp;
<span class="p_del">-				USB_SSP_SUBLINK_SPEED_ATTRIBS) + 1;</span>
<span class="p_add">+				USB_SSP_SUBLINK_SPEED_ATTRIBS);</span>
 			if (length &gt;= USB_DT_USB_SSP_CAP_SIZE(ssac))
 				dev-&gt;bos-&gt;ssp_cap = ssp_cap;
 			break;
<span class="p_header">diff --git a/drivers/usb/core/quirks.c b/drivers/usb/core/quirks.c</span>
<span class="p_header">index 50010282c010..c05c4f877750 100644</span>
<span class="p_header">--- a/drivers/usb/core/quirks.c</span>
<span class="p_header">+++ b/drivers/usb/core/quirks.c</span>
<span class="p_chunk">@@ -57,10 +57,11 @@</span> <span class="p_context"> static const struct usb_device_id usb_quirk_list[] = {</span>
 	/* Microsoft LifeCam-VX700 v2.0 */
 	{ USB_DEVICE(0x045e, 0x0770), .driver_info = USB_QUIRK_RESET_RESUME },
 
<span class="p_del">-	/* Logitech HD Pro Webcams C920, C920-C and C930e */</span>
<span class="p_add">+	/* Logitech HD Pro Webcams C920, C920-C, C925e and C930e */</span>
 	{ USB_DEVICE(0x046d, 0x082d), .driver_info = USB_QUIRK_DELAY_INIT },
 	{ USB_DEVICE(0x046d, 0x0841), .driver_info = USB_QUIRK_DELAY_INIT },
 	{ USB_DEVICE(0x046d, 0x0843), .driver_info = USB_QUIRK_DELAY_INIT },
<span class="p_add">+	{ USB_DEVICE(0x046d, 0x085b), .driver_info = USB_QUIRK_DELAY_INIT },</span>
 
 	/* Logitech ConferenceCam CC3000e */
 	{ USB_DEVICE(0x046d, 0x0847), .driver_info = USB_QUIRK_DELAY_INIT },
<span class="p_chunk">@@ -154,6 +155,9 @@</span> <span class="p_context"> static const struct usb_device_id usb_quirk_list[] = {</span>
 	/* Genesys Logic hub, internally used by KY-688 USB 3.1 Type-C Hub */
 	{ USB_DEVICE(0x05e3, 0x0612), .driver_info = USB_QUIRK_NO_LPM },
 
<span class="p_add">+	/* ELSA MicroLink 56K */</span>
<span class="p_add">+	{ USB_DEVICE(0x05cc, 0x2267), .driver_info = USB_QUIRK_RESET_RESUME },</span>
<span class="p_add">+</span>
 	/* Genesys Logic hub, internally used by Moshi USB to Ethernet Adapter */
 	{ USB_DEVICE(0x05e3, 0x0616), .driver_info = USB_QUIRK_NO_LPM },
 
<span class="p_header">diff --git a/drivers/usb/host/xhci-pci.c b/drivers/usb/host/xhci-pci.c</span>
<span class="p_header">index 76f392954733..abb8f19ae40f 100644</span>
<span class="p_header">--- a/drivers/usb/host/xhci-pci.c</span>
<span class="p_header">+++ b/drivers/usb/host/xhci-pci.c</span>
<span class="p_chunk">@@ -189,6 +189,9 @@</span> <span class="p_context"> static void xhci_pci_quirks(struct device *dev, struct xhci_hcd *xhci)</span>
 		xhci-&gt;quirks |= XHCI_TRUST_TX_LENGTH;
 		xhci-&gt;quirks |= XHCI_BROKEN_STREAMS;
 	}
<span class="p_add">+	if (pdev-&gt;vendor == PCI_VENDOR_ID_RENESAS &amp;&amp;</span>
<span class="p_add">+			pdev-&gt;device == 0x0014)</span>
<span class="p_add">+		xhci-&gt;quirks |= XHCI_TRUST_TX_LENGTH;</span>
 	if (pdev-&gt;vendor == PCI_VENDOR_ID_RENESAS &amp;&amp;
 			pdev-&gt;device == 0x0015)
 		xhci-&gt;quirks |= XHCI_RESET_ON_RESUME;
<span class="p_header">diff --git a/drivers/usb/serial/ftdi_sio.c b/drivers/usb/serial/ftdi_sio.c</span>
<span class="p_header">index 49d1b2d4606d..d038e543c246 100644</span>
<span class="p_header">--- a/drivers/usb/serial/ftdi_sio.c</span>
<span class="p_header">+++ b/drivers/usb/serial/ftdi_sio.c</span>
<span class="p_chunk">@@ -1017,6 +1017,7 @@</span> <span class="p_context"> static const struct usb_device_id id_table_combined[] = {</span>
 		.driver_info = (kernel_ulong_t)&amp;ftdi_jtag_quirk },
 	{ USB_DEVICE(CYPRESS_VID, CYPRESS_WICED_BT_USB_PID) },
 	{ USB_DEVICE(CYPRESS_VID, CYPRESS_WICED_WL_USB_PID) },
<span class="p_add">+	{ USB_DEVICE(AIRBUS_DS_VID, AIRBUS_DS_P8GR) },</span>
 	{ }					/* Terminating entry */
 };
 
<span class="p_header">diff --git a/drivers/usb/serial/ftdi_sio_ids.h b/drivers/usb/serial/ftdi_sio_ids.h</span>
<span class="p_header">index 4faa09fe308c..8b4ecd2bd297 100644</span>
<span class="p_header">--- a/drivers/usb/serial/ftdi_sio_ids.h</span>
<span class="p_header">+++ b/drivers/usb/serial/ftdi_sio_ids.h</span>
<span class="p_chunk">@@ -914,6 +914,12 @@</span> <span class="p_context"></span>
 #define ICPDAS_I7561U_PID		0x0104
 #define ICPDAS_I7563U_PID		0x0105
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Airbus Defence and Space</span>
<span class="p_add">+ */</span>
<span class="p_add">+#define AIRBUS_DS_VID			0x1e8e  /* Vendor ID */</span>
<span class="p_add">+#define AIRBUS_DS_P8GR			0x6001  /* Tetra P8GR */</span>
<span class="p_add">+</span>
 /*
  * RT Systems programming cables for various ham radios
  */
<span class="p_header">diff --git a/drivers/usb/serial/option.c b/drivers/usb/serial/option.c</span>
<span class="p_header">index 54e316b1892d..a9400458ccea 100644</span>
<span class="p_header">--- a/drivers/usb/serial/option.c</span>
<span class="p_header">+++ b/drivers/usb/serial/option.c</span>
<span class="p_chunk">@@ -236,6 +236,8 @@</span> <span class="p_context"> static void option_instat_callback(struct urb *urb);</span>
 /* These Quectel products use Qualcomm&#39;s vendor ID */
 #define QUECTEL_PRODUCT_UC20			0x9003
 #define QUECTEL_PRODUCT_UC15			0x9090
<span class="p_add">+/* These Yuga products use Qualcomm&#39;s vendor ID */</span>
<span class="p_add">+#define YUGA_PRODUCT_CLM920_NC5			0x9625</span>
 
 #define QUECTEL_VENDOR_ID			0x2c7c
 /* These Quectel products use Quectel&#39;s vendor ID */
<span class="p_chunk">@@ -283,6 +285,7 @@</span> <span class="p_context"> static void option_instat_callback(struct urb *urb);</span>
 #define TELIT_PRODUCT_LE922_USBCFG3		0x1043
 #define TELIT_PRODUCT_LE922_USBCFG5		0x1045
 #define TELIT_PRODUCT_ME910			0x1100
<span class="p_add">+#define TELIT_PRODUCT_ME910_DUAL_MODEM		0x1101</span>
 #define TELIT_PRODUCT_LE920			0x1200
 #define TELIT_PRODUCT_LE910			0x1201
 #define TELIT_PRODUCT_LE910_USBCFG4		0x1206
<span class="p_chunk">@@ -648,6 +651,11 @@</span> <span class="p_context"> static const struct option_blacklist_info telit_me910_blacklist = {</span>
 	.reserved = BIT(1) | BIT(3),
 };
 
<span class="p_add">+static const struct option_blacklist_info telit_me910_dual_modem_blacklist = {</span>
<span class="p_add">+	.sendsetup = BIT(0),</span>
<span class="p_add">+	.reserved = BIT(3),</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 static const struct option_blacklist_info telit_le910_blacklist = {
 	.sendsetup = BIT(0),
 	.reserved = BIT(1) | BIT(2),
<span class="p_chunk">@@ -677,6 +685,10 @@</span> <span class="p_context"> static const struct option_blacklist_info cinterion_rmnet2_blacklist = {</span>
 	.reserved = BIT(4) | BIT(5),
 };
 
<span class="p_add">+static const struct option_blacklist_info yuga_clm920_nc5_blacklist = {</span>
<span class="p_add">+	.reserved = BIT(1) | BIT(4),</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 static const struct usb_device_id option_ids[] = {
 	{ USB_DEVICE(OPTION_VENDOR_ID, OPTION_PRODUCT_COLT) },
 	{ USB_DEVICE(OPTION_VENDOR_ID, OPTION_PRODUCT_RICOLA) },
<span class="p_chunk">@@ -1181,6 +1193,9 @@</span> <span class="p_context"> static const struct usb_device_id option_ids[] = {</span>
 	{ USB_DEVICE(QUALCOMM_VENDOR_ID, QUECTEL_PRODUCT_UC15)},
 	{ USB_DEVICE(QUALCOMM_VENDOR_ID, QUECTEL_PRODUCT_UC20),
 	  .driver_info = (kernel_ulong_t)&amp;net_intf4_blacklist },
<span class="p_add">+	/* Yuga products use Qualcomm vendor ID */</span>
<span class="p_add">+	{ USB_DEVICE(QUALCOMM_VENDOR_ID, YUGA_PRODUCT_CLM920_NC5),</span>
<span class="p_add">+	  .driver_info = (kernel_ulong_t)&amp;yuga_clm920_nc5_blacklist },</span>
 	/* Quectel products using Quectel vendor ID */
 	{ USB_DEVICE(QUECTEL_VENDOR_ID, QUECTEL_PRODUCT_EC21),
 	  .driver_info = (kernel_ulong_t)&amp;net_intf4_blacklist },
<span class="p_chunk">@@ -1247,6 +1262,8 @@</span> <span class="p_context"> static const struct usb_device_id option_ids[] = {</span>
 		.driver_info = (kernel_ulong_t)&amp;telit_le922_blacklist_usbcfg0 },
 	{ USB_DEVICE(TELIT_VENDOR_ID, TELIT_PRODUCT_ME910),
 		.driver_info = (kernel_ulong_t)&amp;telit_me910_blacklist },
<span class="p_add">+	{ USB_DEVICE(TELIT_VENDOR_ID, TELIT_PRODUCT_ME910_DUAL_MODEM),</span>
<span class="p_add">+		.driver_info = (kernel_ulong_t)&amp;telit_me910_dual_modem_blacklist },</span>
 	{ USB_DEVICE(TELIT_VENDOR_ID, TELIT_PRODUCT_LE910),
 		.driver_info = (kernel_ulong_t)&amp;telit_le910_blacklist },
 	{ USB_DEVICE(TELIT_VENDOR_ID, TELIT_PRODUCT_LE910_USBCFG4),
<span class="p_header">diff --git a/drivers/usb/serial/qcserial.c b/drivers/usb/serial/qcserial.c</span>
<span class="p_header">index 9f9d3a904464..55a8fb25ce2b 100644</span>
<span class="p_header">--- a/drivers/usb/serial/qcserial.c</span>
<span class="p_header">+++ b/drivers/usb/serial/qcserial.c</span>
<span class="p_chunk">@@ -166,6 +166,8 @@</span> <span class="p_context"> static const struct usb_device_id id_table[] = {</span>
 	{DEVICE_SWI(0x1199, 0x9079)},	/* Sierra Wireless EM74xx */
 	{DEVICE_SWI(0x1199, 0x907a)},	/* Sierra Wireless EM74xx QDL */
 	{DEVICE_SWI(0x1199, 0x907b)},	/* Sierra Wireless EM74xx */
<span class="p_add">+	{DEVICE_SWI(0x1199, 0x9090)},	/* Sierra Wireless EM7565 QDL */</span>
<span class="p_add">+	{DEVICE_SWI(0x1199, 0x9091)},	/* Sierra Wireless EM7565 */</span>
 	{DEVICE_SWI(0x413c, 0x81a2)},	/* Dell Wireless 5806 Gobi(TM) 4G LTE Mobile Broadband Card */
 	{DEVICE_SWI(0x413c, 0x81a3)},	/* Dell Wireless 5570 HSPA+ (42Mbps) Mobile Broadband Card */
 	{DEVICE_SWI(0x413c, 0x81a4)},	/* Dell Wireless 5570e HSPA+ (42Mbps) Mobile Broadband Card */
<span class="p_chunk">@@ -346,6 +348,7 @@</span> <span class="p_context"> static int qcprobe(struct usb_serial *serial, const struct usb_device_id *id)</span>
 			break;
 		case 2:
 			dev_dbg(dev, &quot;NMEA GPS interface found\n&quot;);
<span class="p_add">+			sendsetup = true;</span>
 			break;
 		case 3:
 			dev_dbg(dev, &quot;Modem port found\n&quot;);
<span class="p_header">diff --git a/drivers/usb/usbip/stub_dev.c b/drivers/usb/usbip/stub_dev.c</span>
<span class="p_header">index c653ce533430..720408d39f11 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/stub_dev.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/stub_dev.c</span>
<span class="p_chunk">@@ -163,8 +163,7 @@</span> <span class="p_context"> static void stub_shutdown_connection(struct usbip_device *ud)</span>
 	 * step 1?
 	 */
 	if (ud-&gt;tcp_socket) {
<span class="p_del">-		dev_dbg(&amp;sdev-&gt;udev-&gt;dev, &quot;shutdown tcp_socket %p\n&quot;,</span>
<span class="p_del">-			ud-&gt;tcp_socket);</span>
<span class="p_add">+		dev_dbg(&amp;sdev-&gt;udev-&gt;dev, &quot;shutdown sockfd %d\n&quot;, ud-&gt;sockfd);</span>
 		kernel_sock_shutdown(ud-&gt;tcp_socket, SHUT_RDWR);
 	}
 
<span class="p_header">diff --git a/drivers/usb/usbip/stub_main.c b/drivers/usb/usbip/stub_main.c</span>
<span class="p_header">index 7170404e8979..6968c906fa29 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/stub_main.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/stub_main.c</span>
<span class="p_chunk">@@ -251,11 +251,12 @@</span> <span class="p_context"> void stub_device_cleanup_urbs(struct stub_device *sdev)</span>
 	struct stub_priv *priv;
 	struct urb *urb;
 
<span class="p_del">-	dev_dbg(&amp;sdev-&gt;udev-&gt;dev, &quot;free sdev %p\n&quot;, sdev);</span>
<span class="p_add">+	dev_dbg(&amp;sdev-&gt;udev-&gt;dev, &quot;Stub device cleaning up urbs\n&quot;);</span>
 
 	while ((priv = stub_priv_pop(sdev))) {
 		urb = priv-&gt;urb;
<span class="p_del">-		dev_dbg(&amp;sdev-&gt;udev-&gt;dev, &quot;free urb %p\n&quot;, urb);</span>
<span class="p_add">+		dev_dbg(&amp;sdev-&gt;udev-&gt;dev, &quot;free urb seqnum %lu\n&quot;,</span>
<span class="p_add">+			priv-&gt;seqnum);</span>
 		usb_kill_urb(urb);
 
 		kmem_cache_free(stub_priv_cache, priv);
<span class="p_header">diff --git a/drivers/usb/usbip/stub_rx.c b/drivers/usb/usbip/stub_rx.c</span>
<span class="p_header">index 283a9be77a22..5b807185f79e 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/stub_rx.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/stub_rx.c</span>
<span class="p_chunk">@@ -225,9 +225,6 @@</span> <span class="p_context"> static int stub_recv_cmd_unlink(struct stub_device *sdev,</span>
 		if (priv-&gt;seqnum != pdu-&gt;u.cmd_unlink.seqnum)
 			continue;
 
<span class="p_del">-		dev_info(&amp;priv-&gt;urb-&gt;dev-&gt;dev, &quot;unlink urb %p\n&quot;,</span>
<span class="p_del">-			 priv-&gt;urb);</span>
<span class="p_del">-</span>
 		/*
 		 * This matched urb is not completed yet (i.e., be in
 		 * flight in usb hcd hardware/driver). Now we are
<span class="p_chunk">@@ -266,8 +263,8 @@</span> <span class="p_context"> static int stub_recv_cmd_unlink(struct stub_device *sdev,</span>
 		ret = usb_unlink_urb(priv-&gt;urb);
 		if (ret != -EINPROGRESS)
 			dev_err(&amp;priv-&gt;urb-&gt;dev-&gt;dev,
<span class="p_del">-				&quot;failed to unlink a urb %p, ret %d\n&quot;,</span>
<span class="p_del">-				priv-&gt;urb, ret);</span>
<span class="p_add">+				&quot;failed to unlink a urb # %lu, ret %d\n&quot;,</span>
<span class="p_add">+				priv-&gt;seqnum, ret);</span>
 
 		return 0;
 	}
<span class="p_header">diff --git a/drivers/usb/usbip/stub_tx.c b/drivers/usb/usbip/stub_tx.c</span>
<span class="p_header">index 87ff94be4235..96aa375b80d9 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/stub_tx.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/stub_tx.c</span>
<span class="p_chunk">@@ -102,7 +102,7 @@</span> <span class="p_context"> void stub_complete(struct urb *urb)</span>
 	/* link a urb to the queue of tx. */
 	spin_lock_irqsave(&amp;sdev-&gt;priv_lock, flags);
 	if (sdev-&gt;ud.tcp_socket == NULL) {
<span class="p_del">-		usbip_dbg_stub_tx(&quot;ignore urb for closed connection %p&quot;, urb);</span>
<span class="p_add">+		usbip_dbg_stub_tx(&quot;ignore urb for closed connection\n&quot;);</span>
 		/* It will be freed in stub_device_cleanup_urbs(). */
 	} else if (priv-&gt;unlinking) {
 		stub_enqueue_ret_unlink(sdev, priv-&gt;seqnum, urb-&gt;status);
<span class="p_chunk">@@ -204,8 +204,8 @@</span> <span class="p_context"> static int stub_send_ret_submit(struct stub_device *sdev)</span>
 
 		/* 1. setup usbip_header */
 		setup_ret_submit_pdu(&amp;pdu_header, urb);
<span class="p_del">-		usbip_dbg_stub_tx(&quot;setup txdata seqnum: %d urb: %p\n&quot;,</span>
<span class="p_del">-				  pdu_header.base.seqnum, urb);</span>
<span class="p_add">+		usbip_dbg_stub_tx(&quot;setup txdata seqnum: %d\n&quot;,</span>
<span class="p_add">+				  pdu_header.base.seqnum);</span>
 		usbip_header_correct_endian(&amp;pdu_header, 1);
 
 		iov[iovnum].iov_base = &amp;pdu_header;
<span class="p_header">diff --git a/drivers/usb/usbip/usbip_common.c b/drivers/usb/usbip/usbip_common.c</span>
<span class="p_header">index 2281f3562870..17b599b923f3 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/usbip_common.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/usbip_common.c</span>
<span class="p_chunk">@@ -331,26 +331,20 @@</span> <span class="p_context"> int usbip_recv(struct socket *sock, void *buf, int size)</span>
 	struct msghdr msg = {.msg_flags = MSG_NOSIGNAL};
 	int total = 0;
 
<span class="p_add">+	if (!sock || !buf || !size)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
 	iov_iter_kvec(&amp;msg.msg_iter, READ|ITER_KVEC, &amp;iov, 1, size);
 
 	usbip_dbg_xmit(&quot;enter\n&quot;);
 
<span class="p_del">-	if (!sock || !buf || !size) {</span>
<span class="p_del">-		pr_err(&quot;invalid arg, sock %p buff %p size %d\n&quot;, sock, buf,</span>
<span class="p_del">-		       size);</span>
<span class="p_del">-		return -EINVAL;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	do {
<span class="p_del">-		int sz = msg_data_left(&amp;msg);</span>
<span class="p_add">+		msg_data_left(&amp;msg);</span>
 		sock-&gt;sk-&gt;sk_allocation = GFP_NOIO;
 
 		result = sock_recvmsg(sock, &amp;msg, MSG_WAITALL);
<span class="p_del">-		if (result &lt;= 0) {</span>
<span class="p_del">-			pr_debug(&quot;receive sock %p buf %p size %u ret %d total %d\n&quot;,</span>
<span class="p_del">-				 sock, buf + total, sz, result, total);</span>
<span class="p_add">+		if (result &lt;= 0)</span>
 			goto err;
<span class="p_del">-		}</span>
 
 		total += result;
 	} while (msg_data_left(&amp;msg));
<span class="p_header">diff --git a/drivers/usb/usbip/vhci_hcd.c b/drivers/usb/usbip/vhci_hcd.c</span>
<span class="p_header">index 1f0cf81cc145..692cfdef667e 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/vhci_hcd.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/vhci_hcd.c</span>
<span class="p_chunk">@@ -670,9 +670,6 @@</span> <span class="p_context"> static int vhci_urb_enqueue(struct usb_hcd *hcd, struct urb *urb, gfp_t mem_flag</span>
 	struct vhci_device *vdev;
 	unsigned long flags;
 
<span class="p_del">-	usbip_dbg_vhci_hc(&quot;enter, usb_hcd %p urb %p mem_flags %d\n&quot;,</span>
<span class="p_del">-			  hcd, urb, mem_flags);</span>
<span class="p_del">-</span>
 	if (portnum &gt; VHCI_HC_PORTS) {
 		pr_err(&quot;invalid port number %d\n&quot;, portnum);
 		return -ENODEV;
<span class="p_chunk">@@ -836,8 +833,6 @@</span> <span class="p_context"> static int vhci_urb_dequeue(struct usb_hcd *hcd, struct urb *urb, int status)</span>
 	struct vhci_device *vdev;
 	unsigned long flags;
 
<span class="p_del">-	pr_info(&quot;dequeue a urb %p\n&quot;, urb);</span>
<span class="p_del">-</span>
 	spin_lock_irqsave(&amp;vhci-&gt;lock, flags);
 
 	priv = urb-&gt;hcpriv;
<span class="p_chunk">@@ -865,7 +860,6 @@</span> <span class="p_context"> static int vhci_urb_dequeue(struct usb_hcd *hcd, struct urb *urb, int status)</span>
 		/* tcp connection is closed */
 		spin_lock(&amp;vdev-&gt;priv_lock);
 
<span class="p_del">-		pr_info(&quot;device %p seems to be disconnected\n&quot;, vdev);</span>
 		list_del(&amp;priv-&gt;list);
 		kfree(priv);
 		urb-&gt;hcpriv = NULL;
<span class="p_chunk">@@ -877,8 +871,6 @@</span> <span class="p_context"> static int vhci_urb_dequeue(struct usb_hcd *hcd, struct urb *urb, int status)</span>
 		 * vhci_rx will receive RET_UNLINK and give back the URB.
 		 * Otherwise, we give back it here.
 		 */
<span class="p_del">-		pr_info(&quot;gives back urb %p\n&quot;, urb);</span>
<span class="p_del">-</span>
 		usb_hcd_unlink_urb_from_ep(hcd, urb);
 
 		spin_unlock_irqrestore(&amp;vhci-&gt;lock, flags);
<span class="p_chunk">@@ -906,8 +898,6 @@</span> <span class="p_context"> static int vhci_urb_dequeue(struct usb_hcd *hcd, struct urb *urb, int status)</span>
 
 		unlink-&gt;unlink_seqnum = priv-&gt;seqnum;
 
<span class="p_del">-		pr_info(&quot;device %p seems to be still connected\n&quot;, vdev);</span>
<span class="p_del">-</span>
 		/* send cmd_unlink and try to cancel the pending URB in the
 		 * peer */
 		list_add_tail(&amp;unlink-&gt;list, &amp;vdev-&gt;unlink_tx);
<span class="p_chunk">@@ -989,7 +979,7 @@</span> <span class="p_context"> static void vhci_shutdown_connection(struct usbip_device *ud)</span>
 
 	/* need this? see stub_dev.c */
 	if (ud-&gt;tcp_socket) {
<span class="p_del">-		pr_debug(&quot;shutdown tcp_socket %p\n&quot;, ud-&gt;tcp_socket);</span>
<span class="p_add">+		pr_debug(&quot;shutdown tcp_socket %d\n&quot;, ud-&gt;sockfd);</span>
 		kernel_sock_shutdown(ud-&gt;tcp_socket, SHUT_RDWR);
 	}
 
<span class="p_header">diff --git a/drivers/usb/usbip/vhci_rx.c b/drivers/usb/usbip/vhci_rx.c</span>
<span class="p_header">index ef2f2d5ca6b2..1343037d00f9 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/vhci_rx.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/vhci_rx.c</span>
<span class="p_chunk">@@ -37,24 +37,23 @@</span> <span class="p_context"> struct urb *pickup_urb_and_free_priv(struct vhci_device *vdev, __u32 seqnum)</span>
 		urb = priv-&gt;urb;
 		status = urb-&gt;status;
 
<span class="p_del">-		usbip_dbg_vhci_rx(&quot;find urb %p vurb %p seqnum %u\n&quot;,</span>
<span class="p_del">-				urb, priv, seqnum);</span>
<span class="p_add">+		usbip_dbg_vhci_rx(&quot;find urb seqnum %u\n&quot;, seqnum);</span>
 
 		switch (status) {
 		case -ENOENT:
 			/* fall through */
 		case -ECONNRESET:
<span class="p_del">-			dev_info(&amp;urb-&gt;dev-&gt;dev,</span>
<span class="p_del">-				 &quot;urb %p was unlinked %ssynchronuously.\n&quot;, urb,</span>
<span class="p_del">-				 status == -ENOENT ? &quot;&quot; : &quot;a&quot;);</span>
<span class="p_add">+			dev_dbg(&amp;urb-&gt;dev-&gt;dev,</span>
<span class="p_add">+				 &quot;urb seq# %u was unlinked %ssynchronuously\n&quot;,</span>
<span class="p_add">+				 seqnum, status == -ENOENT ? &quot;&quot; : &quot;a&quot;);</span>
 			break;
 		case -EINPROGRESS:
 			/* no info output */
 			break;
 		default:
<span class="p_del">-			dev_info(&amp;urb-&gt;dev-&gt;dev,</span>
<span class="p_del">-				 &quot;urb %p may be in a error, status %d\n&quot;, urb,</span>
<span class="p_del">-				 status);</span>
<span class="p_add">+			dev_dbg(&amp;urb-&gt;dev-&gt;dev,</span>
<span class="p_add">+				 &quot;urb seq# %u may be in a error, status %d\n&quot;,</span>
<span class="p_add">+				 seqnum, status);</span>
 		}
 
 		list_del(&amp;priv-&gt;list);
<span class="p_chunk">@@ -81,8 +80,8 @@</span> <span class="p_context"> static void vhci_recv_ret_submit(struct vhci_device *vdev,</span>
 	spin_unlock_irqrestore(&amp;vdev-&gt;priv_lock, flags);
 
 	if (!urb) {
<span class="p_del">-		pr_err(&quot;cannot find a urb of seqnum %u\n&quot;, pdu-&gt;base.seqnum);</span>
<span class="p_del">-		pr_info(&quot;max seqnum %d\n&quot;,</span>
<span class="p_add">+		pr_err(&quot;cannot find a urb of seqnum %u max seqnum %d\n&quot;,</span>
<span class="p_add">+			pdu-&gt;base.seqnum,</span>
 			atomic_read(&amp;vhci_hcd-&gt;seqnum));
 		usbip_event_add(ud, VDEV_EVENT_ERROR_TCP);
 		return;
<span class="p_chunk">@@ -105,7 +104,7 @@</span> <span class="p_context"> static void vhci_recv_ret_submit(struct vhci_device *vdev,</span>
 	if (usbip_dbg_flag_vhci_rx)
 		usbip_dump_urb(urb);
 
<span class="p_del">-	usbip_dbg_vhci_rx(&quot;now giveback urb %p\n&quot;, urb);</span>
<span class="p_add">+	usbip_dbg_vhci_rx(&quot;now giveback urb %u\n&quot;, pdu-&gt;base.seqnum);</span>
 
 	spin_lock_irqsave(&amp;vhci-&gt;lock, flags);
 	usb_hcd_unlink_urb_from_ep(vhci_hcd_to_hcd(vhci_hcd), urb);
<span class="p_chunk">@@ -172,7 +171,7 @@</span> <span class="p_context"> static void vhci_recv_ret_unlink(struct vhci_device *vdev,</span>
 		pr_info(&quot;the urb (seqnum %d) was already given back\n&quot;,
 			pdu-&gt;base.seqnum);
 	} else {
<span class="p_del">-		usbip_dbg_vhci_rx(&quot;now giveback urb %p\n&quot;, urb);</span>
<span class="p_add">+		usbip_dbg_vhci_rx(&quot;now giveback urb %d\n&quot;, pdu-&gt;base.seqnum);</span>
 
 		/* If unlink is successful, status is -ECONNRESET */
 		urb-&gt;status = pdu-&gt;u.ret_unlink.status;
<span class="p_header">diff --git a/drivers/usb/usbip/vhci_tx.c b/drivers/usb/usbip/vhci_tx.c</span>
<span class="p_header">index 3e7878fe2fd4..a9a663a578b6 100644</span>
<span class="p_header">--- a/drivers/usb/usbip/vhci_tx.c</span>
<span class="p_header">+++ b/drivers/usb/usbip/vhci_tx.c</span>
<span class="p_chunk">@@ -83,7 +83,8 @@</span> <span class="p_context"> static int vhci_send_cmd_submit(struct vhci_device *vdev)</span>
 		memset(&amp;msg, 0, sizeof(msg));
 		memset(&amp;iov, 0, sizeof(iov));
 
<span class="p_del">-		usbip_dbg_vhci_tx(&quot;setup txdata urb %p\n&quot;, urb);</span>
<span class="p_add">+		usbip_dbg_vhci_tx(&quot;setup txdata urb seqnum %lu\n&quot;,</span>
<span class="p_add">+				  priv-&gt;seqnum);</span>
 
 		/* 1. setup usbip_header */
 		setup_cmd_submit_pdu(&amp;pdu_header, urb);
<span class="p_header">diff --git a/include/linux/blkdev.h b/include/linux/blkdev.h</span>
<span class="p_header">index fd47bd96b5d3..6362e3606aa5 100644</span>
<span class="p_header">--- a/include/linux/blkdev.h</span>
<span class="p_header">+++ b/include/linux/blkdev.h</span>
<span class="p_chunk">@@ -241,14 +241,24 @@</span> <span class="p_context"> struct request {</span>
 	struct request *next_rq;
 };
 
<span class="p_add">+static inline bool blk_op_is_scsi(unsigned int op)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return op == REQ_OP_SCSI_IN || op == REQ_OP_SCSI_OUT;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool blk_op_is_private(unsigned int op)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return op == REQ_OP_DRV_IN || op == REQ_OP_DRV_OUT;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline bool blk_rq_is_scsi(struct request *rq)
 {
<span class="p_del">-	return req_op(rq) == REQ_OP_SCSI_IN || req_op(rq) == REQ_OP_SCSI_OUT;</span>
<span class="p_add">+	return blk_op_is_scsi(req_op(rq));</span>
 }
 
 static inline bool blk_rq_is_private(struct request *rq)
 {
<span class="p_del">-	return req_op(rq) == REQ_OP_DRV_IN || req_op(rq) == REQ_OP_DRV_OUT;</span>
<span class="p_add">+	return blk_op_is_private(req_op(rq));</span>
 }
 
 static inline bool blk_rq_is_passthrough(struct request *rq)
<span class="p_chunk">@@ -256,6 +266,13 @@</span> <span class="p_context"> static inline bool blk_rq_is_passthrough(struct request *rq)</span>
 	return blk_rq_is_scsi(rq) || blk_rq_is_private(rq);
 }
 
<span class="p_add">+static inline bool bio_is_passthrough(struct bio *bio)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned op = bio_op(bio);</span>
<span class="p_add">+</span>
<span class="p_add">+	return blk_op_is_scsi(op) || blk_op_is_private(op);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static inline unsigned short req_get_ioprio(struct request *req)
 {
 	return req-&gt;ioprio;
<span class="p_chunk">@@ -952,7 +969,7 @@</span> <span class="p_context"> extern int blk_rq_prep_clone(struct request *rq, struct request *rq_src,</span>
 extern void blk_rq_unprep_clone(struct request *rq);
 extern blk_status_t blk_insert_cloned_request(struct request_queue *q,
 				     struct request *rq);
<span class="p_del">-extern int blk_rq_append_bio(struct request *rq, struct bio *bio);</span>
<span class="p_add">+extern int blk_rq_append_bio(struct request *rq, struct bio **bio);</span>
 extern void blk_delay_queue(struct request_queue *, unsigned long);
 extern void blk_queue_split(struct request_queue *, struct bio **);
 extern void blk_recount_segments(struct request_queue *, struct bio *);
<span class="p_header">diff --git a/include/linux/cpuhotplug.h b/include/linux/cpuhotplug.h</span>
<span class="p_header">index 2477a5cb5bd5..fb83dee528a1 100644</span>
<span class="p_header">--- a/include/linux/cpuhotplug.h</span>
<span class="p_header">+++ b/include/linux/cpuhotplug.h</span>
<span class="p_chunk">@@ -86,7 +86,7 @@</span> <span class="p_context"> enum cpuhp_state {</span>
 	CPUHP_MM_ZSWP_POOL_PREPARE,
 	CPUHP_KVM_PPC_BOOK3S_PREPARE,
 	CPUHP_ZCOMP_PREPARE,
<span class="p_del">-	CPUHP_TIMERS_DEAD,</span>
<span class="p_add">+	CPUHP_TIMERS_PREPARE,</span>
 	CPUHP_MIPS_SOC_PREPARE,
 	CPUHP_BP_PREPARE_DYN,
 	CPUHP_BP_PREPARE_DYN_END		= CPUHP_BP_PREPARE_DYN + 20,
<span class="p_header">diff --git a/include/linux/ipv6.h b/include/linux/ipv6.h</span>
<span class="p_header">index ea04ca024f0d..067a6fa675ed 100644</span>
<span class="p_header">--- a/include/linux/ipv6.h</span>
<span class="p_header">+++ b/include/linux/ipv6.h</span>
<span class="p_chunk">@@ -272,7 +272,8 @@</span> <span class="p_context"> struct ipv6_pinfo {</span>
 						 * 100: prefer care-of address
 						 */
 				dontfrag:1,
<span class="p_del">-				autoflowlabel:1;</span>
<span class="p_add">+				autoflowlabel:1,</span>
<span class="p_add">+				autoflowlabel_set:1;</span>
 	__u8			min_hopcount;
 	__u8			tclass;
 	__be32			rcv_flowinfo;
<span class="p_header">diff --git a/include/linux/mlx5/driver.h b/include/linux/mlx5/driver.h</span>
<span class="p_header">index 401c8972cc3a..8b3d0103c03a 100644</span>
<span class="p_header">--- a/include/linux/mlx5/driver.h</span>
<span class="p_header">+++ b/include/linux/mlx5/driver.h</span>
<span class="p_chunk">@@ -546,6 +546,7 @@</span> <span class="p_context"> struct mlx5_core_sriov {</span>
 };
 
 struct mlx5_irq_info {
<span class="p_add">+	cpumask_var_t mask;</span>
 	char name[MLX5_MAX_IRQ_NAME];
 };
 
<span class="p_header">diff --git a/include/linux/mlx5/mlx5_ifc.h b/include/linux/mlx5/mlx5_ifc.h</span>
<span class="p_header">index 69772347f866..c8091f06eaa4 100644</span>
<span class="p_header">--- a/include/linux/mlx5/mlx5_ifc.h</span>
<span class="p_header">+++ b/include/linux/mlx5/mlx5_ifc.h</span>
<span class="p_chunk">@@ -147,7 +147,7 @@</span> <span class="p_context"> enum {</span>
 	MLX5_CMD_OP_ALLOC_Q_COUNTER               = 0x771,
 	MLX5_CMD_OP_DEALLOC_Q_COUNTER             = 0x772,
 	MLX5_CMD_OP_QUERY_Q_COUNTER               = 0x773,
<span class="p_del">-	MLX5_CMD_OP_SET_RATE_LIMIT                = 0x780,</span>
<span class="p_add">+	MLX5_CMD_OP_SET_PP_RATE_LIMIT             = 0x780,</span>
 	MLX5_CMD_OP_QUERY_RATE_LIMIT              = 0x781,
 	MLX5_CMD_OP_CREATE_SCHEDULING_ELEMENT      = 0x782,
 	MLX5_CMD_OP_DESTROY_SCHEDULING_ELEMENT     = 0x783,
<span class="p_chunk">@@ -7233,7 +7233,7 @@</span> <span class="p_context"> struct mlx5_ifc_add_vxlan_udp_dport_in_bits {</span>
 	u8         vxlan_udp_port[0x10];
 };
 
<span class="p_del">-struct mlx5_ifc_set_rate_limit_out_bits {</span>
<span class="p_add">+struct mlx5_ifc_set_pp_rate_limit_out_bits {</span>
 	u8         status[0x8];
 	u8         reserved_at_8[0x18];
 
<span class="p_chunk">@@ -7242,7 +7242,7 @@</span> <span class="p_context"> struct mlx5_ifc_set_rate_limit_out_bits {</span>
 	u8         reserved_at_40[0x40];
 };
 
<span class="p_del">-struct mlx5_ifc_set_rate_limit_in_bits {</span>
<span class="p_add">+struct mlx5_ifc_set_pp_rate_limit_in_bits {</span>
 	u8         opcode[0x10];
 	u8         reserved_at_10[0x10];
 
<span class="p_chunk">@@ -7255,6 +7255,8 @@</span> <span class="p_context"> struct mlx5_ifc_set_rate_limit_in_bits {</span>
 	u8         reserved_at_60[0x20];
 
 	u8         rate_limit[0x20];
<span class="p_add">+</span>
<span class="p_add">+	u8         reserved_at_a0[0x160];</span>
 };
 
 struct mlx5_ifc_access_register_out_bits {
<span class="p_header">diff --git a/include/linux/pti.h b/include/linux/pti.h</span>
new file mode 100644
<span class="p_header">index 000000000000..0174883a935a</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/pti.h</span>
<span class="p_chunk">@@ -0,0 +1,11 @@</span> <span class="p_context"></span>
<span class="p_add">+// SPDX-License-Identifier: GPL-2.0</span>
<span class="p_add">+#ifndef _INCLUDE_PTI_H</span>
<span class="p_add">+#define _INCLUDE_PTI_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_PAGE_TABLE_ISOLATION</span>
<span class="p_add">+#include &lt;asm/pti.h&gt;</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void pti_init(void) { }</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/include/linux/ptr_ring.h b/include/linux/ptr_ring.h</span>
<span class="p_header">index 37b4bb2545b3..6866df4f31b5 100644</span>
<span class="p_header">--- a/include/linux/ptr_ring.h</span>
<span class="p_header">+++ b/include/linux/ptr_ring.h</span>
<span class="p_chunk">@@ -101,12 +101,18 @@</span> <span class="p_context"> static inline bool ptr_ring_full_bh(struct ptr_ring *r)</span>
 
 /* Note: callers invoking this in a loop must use a compiler barrier,
  * for example cpu_relax(). Callers must hold producer_lock.
<span class="p_add">+ * Callers are responsible for making sure pointer that is being queued</span>
<span class="p_add">+ * points to a valid data.</span>
  */
 static inline int __ptr_ring_produce(struct ptr_ring *r, void *ptr)
 {
 	if (unlikely(!r-&gt;size) || r-&gt;queue[r-&gt;producer])
 		return -ENOSPC;
 
<span class="p_add">+	/* Make sure the pointer we are storing points to a valid data. */</span>
<span class="p_add">+	/* Pairs with smp_read_barrier_depends in __ptr_ring_consume. */</span>
<span class="p_add">+	smp_wmb();</span>
<span class="p_add">+</span>
 	r-&gt;queue[r-&gt;producer++] = ptr;
 	if (unlikely(r-&gt;producer &gt;= r-&gt;size))
 		r-&gt;producer = 0;
<span class="p_chunk">@@ -275,6 +281,9 @@</span> <span class="p_context"> static inline void *__ptr_ring_consume(struct ptr_ring *r)</span>
 	if (ptr)
 		__ptr_ring_discard_one(r);
 
<span class="p_add">+	/* Make sure anyone accessing data through the pointer is up to date. */</span>
<span class="p_add">+	/* Pairs with smp_wmb in __ptr_ring_produce. */</span>
<span class="p_add">+	smp_read_barrier_depends();</span>
 	return ptr;
 }
 
<span class="p_header">diff --git a/include/linux/tcp.h b/include/linux/tcp.h</span>
<span class="p_header">index 4aa40ef02d32..e8418fc77a43 100644</span>
<span class="p_header">--- a/include/linux/tcp.h</span>
<span class="p_header">+++ b/include/linux/tcp.h</span>
<span class="p_chunk">@@ -214,7 +214,8 @@</span> <span class="p_context"> struct tcp_sock {</span>
 	u8	chrono_type:2,	/* current chronograph type */
 		rate_app_limited:1,  /* rate_{delivered,interval_us} limited? */
 		fastopen_connect:1, /* FASTOPEN_CONNECT sockopt */
<span class="p_del">-		unused:4;</span>
<span class="p_add">+		is_sack_reneg:1,    /* in recovery from loss with SACK reneg? */</span>
<span class="p_add">+		unused:3;</span>
 	u8	nonagle     : 4,/* Disable Nagle algorithm?             */
 		thin_lto    : 1,/* Use linear timeouts for thin streams */
 		unused1	    : 1,
<span class="p_header">diff --git a/include/linux/tick.h b/include/linux/tick.h</span>
<span class="p_header">index cf413b344ddb..5cdac11dd317 100644</span>
<span class="p_header">--- a/include/linux/tick.h</span>
<span class="p_header">+++ b/include/linux/tick.h</span>
<span class="p_chunk">@@ -119,6 +119,7 @@</span> <span class="p_context"> extern void tick_nohz_idle_exit(void);</span>
 extern void tick_nohz_irq_exit(void);
 extern ktime_t tick_nohz_get_sleep_length(void);
 extern unsigned long tick_nohz_get_idle_calls(void);
<span class="p_add">+extern unsigned long tick_nohz_get_idle_calls_cpu(int cpu);</span>
 extern u64 get_cpu_idle_time_us(int cpu, u64 *last_update_time);
 extern u64 get_cpu_iowait_time_us(int cpu, u64 *last_update_time);
 #else /* !CONFIG_NO_HZ_COMMON */
<span class="p_header">diff --git a/include/linux/timer.h b/include/linux/timer.h</span>
<span class="p_header">index ac66f29c6916..e0ea1fe87572 100644</span>
<span class="p_header">--- a/include/linux/timer.h</span>
<span class="p_header">+++ b/include/linux/timer.h</span>
<span class="p_chunk">@@ -246,9 +246,11 @@</span> <span class="p_context"> unsigned long round_jiffies_up(unsigned long j);</span>
 unsigned long round_jiffies_up_relative(unsigned long j);
 
 #ifdef CONFIG_HOTPLUG_CPU
<span class="p_add">+int timers_prepare_cpu(unsigned int cpu);</span>
 int timers_dead_cpu(unsigned int cpu);
 #else
<span class="p_del">-#define timers_dead_cpu NULL</span>
<span class="p_add">+#define timers_prepare_cpu	NULL</span>
<span class="p_add">+#define timers_dead_cpu		NULL</span>
 #endif
 
 #endif
<span class="p_header">diff --git a/include/net/ip.h b/include/net/ip.h</span>
<span class="p_header">index 9896f46cbbf1..af8addbaa3c1 100644</span>
<span class="p_header">--- a/include/net/ip.h</span>
<span class="p_header">+++ b/include/net/ip.h</span>
<span class="p_chunk">@@ -34,6 +34,7 @@</span> <span class="p_context"></span>
 #include &lt;net/flow_dissector.h&gt;
 
 #define IPV4_MAX_PMTU		65535U		/* RFC 2675, Section 5.1 */
<span class="p_add">+#define IPV4_MIN_MTU		68			/* RFC 791 */</span>
 
 struct sock;
 
<span class="p_header">diff --git a/include/net/tcp.h b/include/net/tcp.h</span>
<span class="p_header">index 6ced69940f5c..0a13574134b8 100644</span>
<span class="p_header">--- a/include/net/tcp.h</span>
<span class="p_header">+++ b/include/net/tcp.h</span>
<span class="p_chunk">@@ -1085,7 +1085,7 @@</span> <span class="p_context"> void tcp_rate_skb_sent(struct sock *sk, struct sk_buff *skb);</span>
 void tcp_rate_skb_delivered(struct sock *sk, struct sk_buff *skb,
 			    struct rate_sample *rs);
 void tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,
<span class="p_del">-		  struct rate_sample *rs);</span>
<span class="p_add">+		  bool is_sack_reneg, struct rate_sample *rs);</span>
 void tcp_rate_check_app_limited(struct sock *sk);
 
 /* These functions determine how the current flow behaves in respect of SACK
<span class="p_header">diff --git a/init/main.c b/init/main.c</span>
<span class="p_header">index 8a390f60ec81..b32ec72cdf3d 100644</span>
<span class="p_header">--- a/init/main.c</span>
<span class="p_header">+++ b/init/main.c</span>
<span class="p_chunk">@@ -75,6 +75,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/slab.h&gt;
 #include &lt;linux/perf_event.h&gt;
 #include &lt;linux/ptrace.h&gt;
<span class="p_add">+#include &lt;linux/pti.h&gt;</span>
 #include &lt;linux/blkdev.h&gt;
 #include &lt;linux/elevator.h&gt;
 #include &lt;linux/sched_clock.h&gt;
<span class="p_chunk">@@ -506,6 +507,8 @@</span> <span class="p_context"> static void __init mm_init(void)</span>
 	ioremap_huge_init();
 	/* Should be run before the first non-init thread is created */
 	init_espfix_bsp();
<span class="p_add">+	/* Should be run after espfix64 is set up. */</span>
<span class="p_add">+	pti_init();</span>
 }
 
 asmlinkage __visible void __init start_kernel(void)
<span class="p_header">diff --git a/kernel/cpu.c b/kernel/cpu.c</span>
<span class="p_header">index 7891aecc6aec..f21bfa3172d8 100644</span>
<span class="p_header">--- a/kernel/cpu.c</span>
<span class="p_header">+++ b/kernel/cpu.c</span>
<span class="p_chunk">@@ -1277,9 +1277,9 @@</span> <span class="p_context"> static struct cpuhp_step cpuhp_bp_states[] = {</span>
 	 * before blk_mq_queue_reinit_notify() from notify_dead(),
 	 * otherwise a RCU stall occurs.
 	 */
<span class="p_del">-	[CPUHP_TIMERS_DEAD] = {</span>
<span class="p_add">+	[CPUHP_TIMERS_PREPARE] = {</span>
 		.name			= &quot;timers:dead&quot;,
<span class="p_del">-		.startup.single		= NULL,</span>
<span class="p_add">+		.startup.single		= timers_prepare_cpu,</span>
 		.teardown.single	= timers_dead_cpu,
 	},
 	/* Kicks the plugged cpu into life */
<span class="p_header">diff --git a/kernel/sched/cpufreq_schedutil.c b/kernel/sched/cpufreq_schedutil.c</span>
<span class="p_header">index 2f52ec0f1539..d6717a3331a1 100644</span>
<span class="p_header">--- a/kernel/sched/cpufreq_schedutil.c</span>
<span class="p_header">+++ b/kernel/sched/cpufreq_schedutil.c</span>
<span class="p_chunk">@@ -244,7 +244,7 @@</span> <span class="p_context"> static void sugov_iowait_boost(struct sugov_cpu *sg_cpu, unsigned long *util,</span>
 #ifdef CONFIG_NO_HZ_COMMON
 static bool sugov_cpu_is_busy(struct sugov_cpu *sg_cpu)
 {
<span class="p_del">-	unsigned long idle_calls = tick_nohz_get_idle_calls();</span>
<span class="p_add">+	unsigned long idle_calls = tick_nohz_get_idle_calls_cpu(sg_cpu-&gt;cpu);</span>
 	bool ret = idle_calls == sg_cpu-&gt;saved_idle_calls;
 
 	sg_cpu-&gt;saved_idle_calls = idle_calls;
<span class="p_header">diff --git a/kernel/time/tick-sched.c b/kernel/time/tick-sched.c</span>
<span class="p_header">index c7a899c5ce64..dfa4a117fee3 100644</span>
<span class="p_header">--- a/kernel/time/tick-sched.c</span>
<span class="p_header">+++ b/kernel/time/tick-sched.c</span>
<span class="p_chunk">@@ -674,6 +674,11 @@</span> <span class="p_context"> static void tick_nohz_restart(struct tick_sched *ts, ktime_t now)</span>
 	ts-&gt;next_tick = 0;
 }
 
<span class="p_add">+static inline bool local_timer_softirq_pending(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return local_softirq_pending() &amp; TIMER_SOFTIRQ;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static ktime_t tick_nohz_stop_sched_tick(struct tick_sched *ts,
 					 ktime_t now, int cpu)
 {
<span class="p_chunk">@@ -690,8 +695,18 @@</span> <span class="p_context"> static ktime_t tick_nohz_stop_sched_tick(struct tick_sched *ts,</span>
 	} while (read_seqretry(&amp;jiffies_lock, seq));
 	ts-&gt;last_jiffies = basejiff;
 
<span class="p_del">-	if (rcu_needs_cpu(basemono, &amp;next_rcu) ||</span>
<span class="p_del">-	    arch_needs_cpu() || irq_work_needs_cpu()) {</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Keep the periodic tick, when RCU, architecture or irq_work</span>
<span class="p_add">+	 * requests it.</span>
<span class="p_add">+	 * Aside of that check whether the local timer softirq is</span>
<span class="p_add">+	 * pending. If so its a bad idea to call get_next_timer_interrupt()</span>
<span class="p_add">+	 * because there is an already expired timer, so it will request</span>
<span class="p_add">+	 * immeditate expiry, which rearms the hardware timer with a</span>
<span class="p_add">+	 * minimal delta which brings us back to this place</span>
<span class="p_add">+	 * immediately. Lather, rinse and repeat...</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (rcu_needs_cpu(basemono, &amp;next_rcu) || arch_needs_cpu() ||</span>
<span class="p_add">+	    irq_work_needs_cpu() || local_timer_softirq_pending()) {</span>
 		next_tick = basemono + TICK_NSEC;
 	} else {
 		/*
<span class="p_chunk">@@ -1009,6 +1024,19 @@</span> <span class="p_context"> ktime_t tick_nohz_get_sleep_length(void)</span>
 	return ts-&gt;sleep_length;
 }
 
<span class="p_add">+/**</span>
<span class="p_add">+ * tick_nohz_get_idle_calls_cpu - return the current idle calls counter value</span>
<span class="p_add">+ * for a particular CPU.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Called from the schedutil frequency scaling governor in scheduler context.</span>
<span class="p_add">+ */</span>
<span class="p_add">+unsigned long tick_nohz_get_idle_calls_cpu(int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct tick_sched *ts = tick_get_tick_sched(cpu);</span>
<span class="p_add">+</span>
<span class="p_add">+	return ts-&gt;idle_calls;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /**
  * tick_nohz_get_idle_calls - return the current idle calls counter value
  *
<span class="p_header">diff --git a/kernel/time/timer.c b/kernel/time/timer.c</span>
<span class="p_header">index f2674a056c26..73e3cdbc61f1 100644</span>
<span class="p_header">--- a/kernel/time/timer.c</span>
<span class="p_header">+++ b/kernel/time/timer.c</span>
<span class="p_chunk">@@ -814,11 +814,10 @@</span> <span class="p_context"> static inline struct timer_base *get_timer_cpu_base(u32 tflags, u32 cpu)</span>
 	struct timer_base *base = per_cpu_ptr(&amp;timer_bases[BASE_STD], cpu);
 
 	/*
<span class="p_del">-	 * If the timer is deferrable and nohz is active then we need to use</span>
<span class="p_del">-	 * the deferrable base.</span>
<span class="p_add">+	 * If the timer is deferrable and NO_HZ_COMMON is set then we need</span>
<span class="p_add">+	 * to use the deferrable base.</span>
 	 */
<span class="p_del">-	if (IS_ENABLED(CONFIG_NO_HZ_COMMON) &amp;&amp; base-&gt;nohz_active &amp;&amp;</span>
<span class="p_del">-	    (tflags &amp; TIMER_DEFERRABLE))</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_NO_HZ_COMMON) &amp;&amp; (tflags &amp; TIMER_DEFERRABLE))</span>
 		base = per_cpu_ptr(&amp;timer_bases[BASE_DEF], cpu);
 	return base;
 }
<span class="p_chunk">@@ -828,11 +827,10 @@</span> <span class="p_context"> static inline struct timer_base *get_timer_this_cpu_base(u32 tflags)</span>
 	struct timer_base *base = this_cpu_ptr(&amp;timer_bases[BASE_STD]);
 
 	/*
<span class="p_del">-	 * If the timer is deferrable and nohz is active then we need to use</span>
<span class="p_del">-	 * the deferrable base.</span>
<span class="p_add">+	 * If the timer is deferrable and NO_HZ_COMMON is set then we need</span>
<span class="p_add">+	 * to use the deferrable base.</span>
 	 */
<span class="p_del">-	if (IS_ENABLED(CONFIG_NO_HZ_COMMON) &amp;&amp; base-&gt;nohz_active &amp;&amp;</span>
<span class="p_del">-	    (tflags &amp; TIMER_DEFERRABLE))</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_NO_HZ_COMMON) &amp;&amp; (tflags &amp; TIMER_DEFERRABLE))</span>
 		base = this_cpu_ptr(&amp;timer_bases[BASE_DEF]);
 	return base;
 }
<span class="p_chunk">@@ -984,8 +982,6 @@</span> <span class="p_context"> __mod_timer(struct timer_list *timer, unsigned long expires, bool pending_only)</span>
 	if (!ret &amp;&amp; pending_only)
 		goto out_unlock;
 
<span class="p_del">-	debug_activate(timer, expires);</span>
<span class="p_del">-</span>
 	new_base = get_target_base(base, timer-&gt;flags);
 
 	if (base != new_base) {
<span class="p_chunk">@@ -1009,6 +1005,8 @@</span> <span class="p_context"> __mod_timer(struct timer_list *timer, unsigned long expires, bool pending_only)</span>
 		}
 	}
 
<span class="p_add">+	debug_activate(timer, expires);</span>
<span class="p_add">+</span>
 	timer-&gt;expires = expires;
 	/*
 	 * If &#39;idx&#39; was calculated above and the base time did not advance
<span class="p_chunk">@@ -1644,7 +1642,7 @@</span> <span class="p_context"> static __latent_entropy void run_timer_softirq(struct softirq_action *h)</span>
 	base-&gt;must_forward_clk = false;
 
 	__run_timers(base);
<span class="p_del">-	if (IS_ENABLED(CONFIG_NO_HZ_COMMON) &amp;&amp; base-&gt;nohz_active)</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_NO_HZ_COMMON))</span>
 		__run_timers(this_cpu_ptr(&amp;timer_bases[BASE_DEF]));
 }
 
<span class="p_chunk">@@ -1803,6 +1801,21 @@</span> <span class="p_context"> static void migrate_timer_list(struct timer_base *new_base, struct hlist_head *h</span>
 	}
 }
 
<span class="p_add">+int timers_prepare_cpu(unsigned int cpu)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct timer_base *base;</span>
<span class="p_add">+	int b;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (b = 0; b &lt; NR_BASES; b++) {</span>
<span class="p_add">+		base = per_cpu_ptr(&amp;timer_bases[b], cpu);</span>
<span class="p_add">+		base-&gt;clk = jiffies;</span>
<span class="p_add">+		base-&gt;next_expiry = base-&gt;clk + NEXT_TIMER_MAX_DELTA;</span>
<span class="p_add">+		base-&gt;is_idle = false;</span>
<span class="p_add">+		base-&gt;must_forward_clk = true;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int timers_dead_cpu(unsigned int cpu)
 {
 	struct timer_base *old_base;
<span class="p_header">diff --git a/kernel/trace/ring_buffer.c b/kernel/trace/ring_buffer.c</span>
<span class="p_header">index 81279c6602ff..0476a9372014 100644</span>
<span class="p_header">--- a/kernel/trace/ring_buffer.c</span>
<span class="p_header">+++ b/kernel/trace/ring_buffer.c</span>
<span class="p_chunk">@@ -281,6 +281,8 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(ring_buffer_event_data);</span>
 /* Missed count stored at end */
 #define RB_MISSED_STORED	(1 &lt;&lt; 30)
 
<span class="p_add">+#define RB_MISSED_FLAGS		(RB_MISSED_EVENTS|RB_MISSED_STORED)</span>
<span class="p_add">+</span>
 struct buffer_data_page {
 	u64		 time_stamp;	/* page time stamp */
 	local_t		 commit;	/* write committed index */
<span class="p_chunk">@@ -332,7 +334,9 @@</span> <span class="p_context"> static void rb_init_page(struct buffer_data_page *bpage)</span>
  */
 size_t ring_buffer_page_len(void *page)
 {
<span class="p_del">-	return local_read(&amp;((struct buffer_data_page *)page)-&gt;commit)</span>
<span class="p_add">+	struct buffer_data_page *bpage = page;</span>
<span class="p_add">+</span>
<span class="p_add">+	return (local_read(&amp;bpage-&gt;commit) &amp; ~RB_MISSED_FLAGS)</span>
 		+ BUF_PAGE_HDR_SIZE;
 }
 
<span class="p_chunk">@@ -4439,8 +4443,13 @@</span> <span class="p_context"> void ring_buffer_free_read_page(struct ring_buffer *buffer, int cpu, void *data)</span>
 {
 	struct ring_buffer_per_cpu *cpu_buffer = buffer-&gt;buffers[cpu];
 	struct buffer_data_page *bpage = data;
<span class="p_add">+	struct page *page = virt_to_page(bpage);</span>
 	unsigned long flags;
 
<span class="p_add">+	/* If the page is still in use someplace else, we can&#39;t reuse it */</span>
<span class="p_add">+	if (page_ref_count(page) &gt; 1)</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
 	local_irq_save(flags);
 	arch_spin_lock(&amp;cpu_buffer-&gt;lock);
 
<span class="p_chunk">@@ -4452,6 +4461,7 @@</span> <span class="p_context"> void ring_buffer_free_read_page(struct ring_buffer *buffer, int cpu, void *data)</span>
 	arch_spin_unlock(&amp;cpu_buffer-&gt;lock);
 	local_irq_restore(flags);
 
<span class="p_add">+ out:</span>
 	free_page((unsigned long)bpage);
 }
 EXPORT_SYMBOL_GPL(ring_buffer_free_read_page);
<span class="p_header">diff --git a/kernel/trace/trace.c b/kernel/trace/trace.c</span>
<span class="p_header">index 80de14973b42..76bcc80b893e 100644</span>
<span class="p_header">--- a/kernel/trace/trace.c</span>
<span class="p_header">+++ b/kernel/trace/trace.c</span>
<span class="p_chunk">@@ -6769,7 +6769,7 @@</span> <span class="p_context"> tracing_buffers_splice_read(struct file *file, loff_t *ppos,</span>
 		.spd_release	= buffer_spd_release,
 	};
 	struct buffer_ref *ref;
<span class="p_del">-	int entries, size, i;</span>
<span class="p_add">+	int entries, i;</span>
 	ssize_t ret = 0;
 
 #ifdef CONFIG_TRACER_MAX_TRACE
<span class="p_chunk">@@ -6823,14 +6823,6 @@</span> <span class="p_context"> tracing_buffers_splice_read(struct file *file, loff_t *ppos,</span>
 			break;
 		}
 
<span class="p_del">-		/*</span>
<span class="p_del">-		 * zero out any left over data, this is going to</span>
<span class="p_del">-		 * user land.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		size = ring_buffer_page_len(ref-&gt;page);</span>
<span class="p_del">-		if (size &lt; PAGE_SIZE)</span>
<span class="p_del">-			memset(ref-&gt;page + size, 0, PAGE_SIZE - size);</span>
<span class="p_del">-</span>
 		page = virt_to_page(ref-&gt;page);
 
 		spd.pages[i] = page;
<span class="p_chunk">@@ -7588,6 +7580,7 @@</span> <span class="p_context"> allocate_trace_buffer(struct trace_array *tr, struct trace_buffer *buf, int size</span>
 	buf-&gt;data = alloc_percpu(struct trace_array_cpu);
 	if (!buf-&gt;data) {
 		ring_buffer_free(buf-&gt;buffer);
<span class="p_add">+		buf-&gt;buffer = NULL;</span>
 		return -ENOMEM;
 	}
 
<span class="p_chunk">@@ -7611,7 +7604,9 @@</span> <span class="p_context"> static int allocate_trace_buffers(struct trace_array *tr, int size)</span>
 				    allocate_snapshot ? size : 1);
 	if (WARN_ON(ret)) {
 		ring_buffer_free(tr-&gt;trace_buffer.buffer);
<span class="p_add">+		tr-&gt;trace_buffer.buffer = NULL;</span>
 		free_percpu(tr-&gt;trace_buffer.data);
<span class="p_add">+		tr-&gt;trace_buffer.data = NULL;</span>
 		return -ENOMEM;
 	}
 	tr-&gt;allocated_snapshot = allocate_snapshot;
<span class="p_header">diff --git a/net/bridge/br_netlink.c b/net/bridge/br_netlink.c</span>
<span class="p_header">index de2152730809..08190db0a2dc 100644</span>
<span class="p_header">--- a/net/bridge/br_netlink.c</span>
<span class="p_header">+++ b/net/bridge/br_netlink.c</span>
<span class="p_chunk">@@ -1223,19 +1223,20 @@</span> <span class="p_context"> static int br_dev_newlink(struct net *src_net, struct net_device *dev,</span>
 	struct net_bridge *br = netdev_priv(dev);
 	int err;
 
<span class="p_add">+	err = register_netdevice(dev);</span>
<span class="p_add">+	if (err)</span>
<span class="p_add">+		return err;</span>
<span class="p_add">+</span>
 	if (tb[IFLA_ADDRESS]) {
 		spin_lock_bh(&amp;br-&gt;lock);
 		br_stp_change_bridge_id(br, nla_data(tb[IFLA_ADDRESS]));
 		spin_unlock_bh(&amp;br-&gt;lock);
 	}
 
<span class="p_del">-	err = register_netdevice(dev);</span>
<span class="p_del">-	if (err)</span>
<span class="p_del">-		return err;</span>
<span class="p_del">-</span>
 	err = br_changelink(dev, tb, data, extack);
 	if (err)
<span class="p_del">-		unregister_netdevice(dev);</span>
<span class="p_add">+		br_dev_delete(dev, NULL);</span>
<span class="p_add">+</span>
 	return err;
 }
 
<span class="p_header">diff --git a/net/core/net_namespace.c b/net/core/net_namespace.c</span>
<span class="p_header">index 6cfdc7c84c48..0dd6359e5924 100644</span>
<span class="p_header">--- a/net/core/net_namespace.c</span>
<span class="p_header">+++ b/net/core/net_namespace.c</span>
<span class="p_chunk">@@ -266,7 +266,7 @@</span> <span class="p_context"> struct net *get_net_ns_by_id(struct net *net, int id)</span>
 	spin_lock_bh(&amp;net-&gt;nsid_lock);
 	peer = idr_find(&amp;net-&gt;netns_ids, id);
 	if (peer)
<span class="p_del">-		get_net(peer);</span>
<span class="p_add">+		peer = maybe_get_net(peer);</span>
 	spin_unlock_bh(&amp;net-&gt;nsid_lock);
 	rcu_read_unlock();
 
<span class="p_header">diff --git a/net/core/skbuff.c b/net/core/skbuff.c</span>
<span class="p_header">index e140ba49b30a..15fa5baa8fae 100644</span>
<span class="p_header">--- a/net/core/skbuff.c</span>
<span class="p_header">+++ b/net/core/skbuff.c</span>
<span class="p_chunk">@@ -1181,12 +1181,12 @@</span> <span class="p_context"> int skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask)</span>
 	int i, new_frags;
 	u32 d_off;
 
<span class="p_del">-	if (!num_frags)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
 	if (skb_shared(skb) || skb_unclone(skb, gfp_mask))
 		return -EINVAL;
 
<span class="p_add">+	if (!num_frags)</span>
<span class="p_add">+		goto release;</span>
<span class="p_add">+</span>
 	new_frags = (__skb_pagelen(skb) + PAGE_SIZE - 1) &gt;&gt; PAGE_SHIFT;
 	for (i = 0; i &lt; new_frags; i++) {
 		page = alloc_page(gfp_mask);
<span class="p_chunk">@@ -1242,6 +1242,7 @@</span> <span class="p_context"> int skb_copy_ubufs(struct sk_buff *skb, gfp_t gfp_mask)</span>
 	__skb_fill_page_desc(skb, new_frags - 1, head, 0, d_off);
 	skb_shinfo(skb)-&gt;nr_frags = new_frags;
 
<span class="p_add">+release:</span>
 	skb_zcopy_clear(skb, false);
 	return 0;
 }
<span class="p_chunk">@@ -3657,8 +3658,6 @@</span> <span class="p_context"> struct sk_buff *skb_segment(struct sk_buff *head_skb,</span>
 
 		skb_shinfo(nskb)-&gt;tx_flags |= skb_shinfo(head_skb)-&gt;tx_flags &amp;
 					      SKBTX_SHARED_FRAG;
<span class="p_del">-		if (skb_zerocopy_clone(nskb, head_skb, GFP_ATOMIC))</span>
<span class="p_del">-			goto err;</span>
 
 		while (pos &lt; offset + len) {
 			if (i &gt;= nfrags) {
<span class="p_chunk">@@ -3684,6 +3683,8 @@</span> <span class="p_context"> struct sk_buff *skb_segment(struct sk_buff *head_skb,</span>
 
 			if (unlikely(skb_orphan_frags(frag_skb, GFP_ATOMIC)))
 				goto err;
<span class="p_add">+			if (skb_zerocopy_clone(nskb, frag_skb, GFP_ATOMIC))</span>
<span class="p_add">+				goto err;</span>
 
 			*nskb_frag = *frag;
 			__skb_frag_ref(nskb_frag);
<span class="p_chunk">@@ -4296,7 +4297,7 @@</span> <span class="p_context"> void skb_complete_tx_timestamp(struct sk_buff *skb,</span>
 	struct sock *sk = skb-&gt;sk;
 
 	if (!skb_may_tx_timestamp(sk, false))
<span class="p_del">-		return;</span>
<span class="p_add">+		goto err;</span>
 
 	/* Take a reference to prevent skb_orphan() from freeing the socket,
 	 * but only if the socket refcount is not zero.
<span class="p_chunk">@@ -4305,7 +4306,11 @@</span> <span class="p_context"> void skb_complete_tx_timestamp(struct sk_buff *skb,</span>
 		*skb_hwtstamps(skb) = *hwtstamps;
 		__skb_complete_tx_timestamp(skb, sk, SCM_TSTAMP_SND, false);
 		sock_put(sk);
<span class="p_add">+		return;</span>
 	}
<span class="p_add">+</span>
<span class="p_add">+err:</span>
<span class="p_add">+	kfree_skb(skb);</span>
 }
 EXPORT_SYMBOL_GPL(skb_complete_tx_timestamp);
 
<span class="p_header">diff --git a/net/ipv4/devinet.c b/net/ipv4/devinet.c</span>
<span class="p_header">index d7adc0616599..bffa88ecc534 100644</span>
<span class="p_header">--- a/net/ipv4/devinet.c</span>
<span class="p_header">+++ b/net/ipv4/devinet.c</span>
<span class="p_chunk">@@ -1420,7 +1420,7 @@</span> <span class="p_context"> static void inetdev_changename(struct net_device *dev, struct in_device *in_dev)</span>
 
 static bool inetdev_valid_mtu(unsigned int mtu)
 {
<span class="p_del">-	return mtu &gt;= 68;</span>
<span class="p_add">+	return mtu &gt;= IPV4_MIN_MTU;</span>
 }
 
 static void inetdev_send_gratuitous_arp(struct net_device *dev,
<span class="p_header">diff --git a/net/ipv4/fib_frontend.c b/net/ipv4/fib_frontend.c</span>
<span class="p_header">index 37819ab4cc74..d72874150905 100644</span>
<span class="p_header">--- a/net/ipv4/fib_frontend.c</span>
<span class="p_header">+++ b/net/ipv4/fib_frontend.c</span>
<span class="p_chunk">@@ -1274,14 +1274,19 @@</span> <span class="p_context"> static int __net_init ip_fib_net_init(struct net *net)</span>
 
 static void ip_fib_net_exit(struct net *net)
 {
<span class="p_del">-	unsigned int i;</span>
<span class="p_add">+	int i;</span>
 
 	rtnl_lock();
 #ifdef CONFIG_IP_MULTIPLE_TABLES
 	RCU_INIT_POINTER(net-&gt;ipv4.fib_main, NULL);
 	RCU_INIT_POINTER(net-&gt;ipv4.fib_default, NULL);
 #endif
<span class="p_del">-	for (i = 0; i &lt; FIB_TABLE_HASHSZ; i++) {</span>
<span class="p_add">+	/* Destroy the tables in reverse order to guarantee that the</span>
<span class="p_add">+	 * local table, ID 255, is destroyed before the main table, ID</span>
<span class="p_add">+	 * 254. This is necessary as the local table may contain</span>
<span class="p_add">+	 * references to data contained in the main table.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	for (i = FIB_TABLE_HASHSZ - 1; i &gt;= 0; i--) {</span>
 		struct hlist_head *head = &amp;net-&gt;ipv4.fib_table_hash[i];
 		struct hlist_node *tmp;
 		struct fib_table *tb;
<span class="p_header">diff --git a/net/ipv4/fib_semantics.c b/net/ipv4/fib_semantics.c</span>
<span class="p_header">index 01ed22139ac2..aff3751df950 100644</span>
<span class="p_header">--- a/net/ipv4/fib_semantics.c</span>
<span class="p_header">+++ b/net/ipv4/fib_semantics.c</span>
<span class="p_chunk">@@ -706,7 +706,7 @@</span> <span class="p_context"> bool fib_metrics_match(struct fib_config *cfg, struct fib_info *fi)</span>
 
 	nla_for_each_attr(nla, cfg-&gt;fc_mx, cfg-&gt;fc_mx_len, remaining) {
 		int type = nla_type(nla);
<span class="p_del">-		u32 val;</span>
<span class="p_add">+		u32 fi_val, val;</span>
 
 		if (!type)
 			continue;
<span class="p_chunk">@@ -723,7 +723,11 @@</span> <span class="p_context"> bool fib_metrics_match(struct fib_config *cfg, struct fib_info *fi)</span>
 			val = nla_get_u32(nla);
 		}
 
<span class="p_del">-		if (fi-&gt;fib_metrics-&gt;metrics[type - 1] != val)</span>
<span class="p_add">+		fi_val = fi-&gt;fib_metrics-&gt;metrics[type - 1];</span>
<span class="p_add">+		if (type == RTAX_FEATURES)</span>
<span class="p_add">+			fi_val &amp;= ~DST_FEATURE_ECN_CA;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (fi_val != val)</span>
 			return false;
 	}
 
<span class="p_header">diff --git a/net/ipv4/igmp.c b/net/ipv4/igmp.c</span>
<span class="p_header">index ab183af0b5b6..c621266e0306 100644</span>
<span class="p_header">--- a/net/ipv4/igmp.c</span>
<span class="p_header">+++ b/net/ipv4/igmp.c</span>
<span class="p_chunk">@@ -89,6 +89,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/rtnetlink.h&gt;
 #include &lt;linux/times.h&gt;
 #include &lt;linux/pkt_sched.h&gt;
<span class="p_add">+#include &lt;linux/byteorder/generic.h&gt;</span>
 
 #include &lt;net/net_namespace.h&gt;
 #include &lt;net/arp.h&gt;
<span class="p_chunk">@@ -321,6 +322,23 @@</span> <span class="p_context"> igmp_scount(struct ip_mc_list *pmc, int type, int gdeleted, int sdeleted)</span>
 	return scount;
 }
 
<span class="p_add">+/* source address selection per RFC 3376 section 4.2.13 */</span>
<span class="p_add">+static __be32 igmpv3_get_srcaddr(struct net_device *dev,</span>
<span class="p_add">+				 const struct flowi4 *fl4)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct in_device *in_dev = __in_dev_get_rcu(dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!in_dev)</span>
<span class="p_add">+		return htonl(INADDR_ANY);</span>
<span class="p_add">+</span>
<span class="p_add">+	for_ifa(in_dev) {</span>
<span class="p_add">+		if (inet_ifa_match(fl4-&gt;saddr, ifa))</span>
<span class="p_add">+			return fl4-&gt;saddr;</span>
<span class="p_add">+	} endfor_ifa(in_dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	return htonl(INADDR_ANY);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static struct sk_buff *igmpv3_newpack(struct net_device *dev, unsigned int mtu)
 {
 	struct sk_buff *skb;
<span class="p_chunk">@@ -368,7 +386,7 @@</span> <span class="p_context"> static struct sk_buff *igmpv3_newpack(struct net_device *dev, unsigned int mtu)</span>
 	pip-&gt;frag_off = htons(IP_DF);
 	pip-&gt;ttl      = 1;
 	pip-&gt;daddr    = fl4.daddr;
<span class="p_del">-	pip-&gt;saddr    = fl4.saddr;</span>
<span class="p_add">+	pip-&gt;saddr    = igmpv3_get_srcaddr(dev, &amp;fl4);</span>
 	pip-&gt;protocol = IPPROTO_IGMP;
 	pip-&gt;tot_len  = 0;	/* filled in later */
 	ip_select_ident(net, skb, NULL);
<span class="p_chunk">@@ -404,16 +422,17 @@</span> <span class="p_context"> static int grec_size(struct ip_mc_list *pmc, int type, int gdel, int sdel)</span>
 }
 
 static struct sk_buff *add_grhead(struct sk_buff *skb, struct ip_mc_list *pmc,
<span class="p_del">-	int type, struct igmpv3_grec **ppgr)</span>
<span class="p_add">+	int type, struct igmpv3_grec **ppgr, unsigned int mtu)</span>
 {
 	struct net_device *dev = pmc-&gt;interface-&gt;dev;
 	struct igmpv3_report *pih;
 	struct igmpv3_grec *pgr;
 
<span class="p_del">-	if (!skb)</span>
<span class="p_del">-		skb = igmpv3_newpack(dev, dev-&gt;mtu);</span>
<span class="p_del">-	if (!skb)</span>
<span class="p_del">-		return NULL;</span>
<span class="p_add">+	if (!skb) {</span>
<span class="p_add">+		skb = igmpv3_newpack(dev, mtu);</span>
<span class="p_add">+		if (!skb)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+	}</span>
 	pgr = skb_put(skb, sizeof(struct igmpv3_grec));
 	pgr-&gt;grec_type = type;
 	pgr-&gt;grec_auxwords = 0;
<span class="p_chunk">@@ -436,12 +455,17 @@</span> <span class="p_context"> static struct sk_buff *add_grec(struct sk_buff *skb, struct ip_mc_list *pmc,</span>
 	struct igmpv3_grec *pgr = NULL;
 	struct ip_sf_list *psf, *psf_next, *psf_prev, **psf_list;
 	int scount, stotal, first, isquery, truncate;
<span class="p_add">+	unsigned int mtu;</span>
 
 	if (pmc-&gt;multiaddr == IGMP_ALL_HOSTS)
 		return skb;
 	if (ipv4_is_local_multicast(pmc-&gt;multiaddr) &amp;&amp; !net-&gt;ipv4.sysctl_igmp_llm_reports)
 		return skb;
 
<span class="p_add">+	mtu = READ_ONCE(dev-&gt;mtu);</span>
<span class="p_add">+	if (mtu &lt; IPV4_MIN_MTU)</span>
<span class="p_add">+		return skb;</span>
<span class="p_add">+</span>
 	isquery = type == IGMPV3_MODE_IS_INCLUDE ||
 		  type == IGMPV3_MODE_IS_EXCLUDE;
 	truncate = type == IGMPV3_MODE_IS_EXCLUDE ||
<span class="p_chunk">@@ -462,7 +486,7 @@</span> <span class="p_context"> static struct sk_buff *add_grec(struct sk_buff *skb, struct ip_mc_list *pmc,</span>
 		    AVAILABLE(skb) &lt; grec_size(pmc, type, gdeleted, sdeleted)) {
 			if (skb)
 				igmpv3_sendpack(skb);
<span class="p_del">-			skb = igmpv3_newpack(dev, dev-&gt;mtu);</span>
<span class="p_add">+			skb = igmpv3_newpack(dev, mtu);</span>
 		}
 	}
 	first = 1;
<span class="p_chunk">@@ -498,12 +522,12 @@</span> <span class="p_context"> static struct sk_buff *add_grec(struct sk_buff *skb, struct ip_mc_list *pmc,</span>
 				pgr-&gt;grec_nsrcs = htons(scount);
 			if (skb)
 				igmpv3_sendpack(skb);
<span class="p_del">-			skb = igmpv3_newpack(dev, dev-&gt;mtu);</span>
<span class="p_add">+			skb = igmpv3_newpack(dev, mtu);</span>
 			first = 1;
 			scount = 0;
 		}
 		if (first) {
<span class="p_del">-			skb = add_grhead(skb, pmc, type, &amp;pgr);</span>
<span class="p_add">+			skb = add_grhead(skb, pmc, type, &amp;pgr, mtu);</span>
 			first = 0;
 		}
 		if (!skb)
<span class="p_chunk">@@ -538,7 +562,7 @@</span> <span class="p_context"> static struct sk_buff *add_grec(struct sk_buff *skb, struct ip_mc_list *pmc,</span>
 				igmpv3_sendpack(skb);
 				skb = NULL; /* add_grhead will get a new one */
 			}
<span class="p_del">-			skb = add_grhead(skb, pmc, type, &amp;pgr);</span>
<span class="p_add">+			skb = add_grhead(skb, pmc, type, &amp;pgr, mtu);</span>
 		}
 	}
 	if (pgr)
<span class="p_header">diff --git a/net/ipv4/ip_tunnel.c b/net/ipv4/ip_tunnel.c</span>
<span class="p_header">index e9805ad664ac..4e90082b23a6 100644</span>
<span class="p_header">--- a/net/ipv4/ip_tunnel.c</span>
<span class="p_header">+++ b/net/ipv4/ip_tunnel.c</span>
<span class="p_chunk">@@ -349,8 +349,8 @@</span> <span class="p_context"> static int ip_tunnel_bind_dev(struct net_device *dev)</span>
 	dev-&gt;needed_headroom = t_hlen + hlen;
 	mtu -= (dev-&gt;hard_header_len + t_hlen);
 
<span class="p_del">-	if (mtu &lt; 68)</span>
<span class="p_del">-		mtu = 68;</span>
<span class="p_add">+	if (mtu &lt; IPV4_MIN_MTU)</span>
<span class="p_add">+		mtu = IPV4_MIN_MTU;</span>
 
 	return mtu;
 }
<span class="p_header">diff --git a/net/ipv4/raw.c b/net/ipv4/raw.c</span>
<span class="p_header">index 33b70bfd1122..125c1eab3eaa 100644</span>
<span class="p_header">--- a/net/ipv4/raw.c</span>
<span class="p_header">+++ b/net/ipv4/raw.c</span>
<span class="p_chunk">@@ -513,11 +513,16 @@</span> <span class="p_context"> static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)</span>
 	int err;
 	struct ip_options_data opt_copy;
 	struct raw_frag_vec rfv;
<span class="p_add">+	int hdrincl;</span>
 
 	err = -EMSGSIZE;
 	if (len &gt; 0xFFFF)
 		goto out;
 
<span class="p_add">+	/* hdrincl should be READ_ONCE(inet-&gt;hdrincl)</span>
<span class="p_add">+	 * but READ_ONCE() doesn&#39;t work with bit fields</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	hdrincl = inet-&gt;hdrincl;</span>
 	/*
 	 *	Check the flags.
 	 */
<span class="p_chunk">@@ -593,7 +598,7 @@</span> <span class="p_context"> static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)</span>
 		/* Linux does not mangle headers on raw sockets,
 		 * so that IP options + IP_HDRINCL is non-sense.
 		 */
<span class="p_del">-		if (inet-&gt;hdrincl)</span>
<span class="p_add">+		if (hdrincl)</span>
 			goto done;
 		if (ipc.opt-&gt;opt.srr) {
 			if (!daddr)
<span class="p_chunk">@@ -615,12 +620,12 @@</span> <span class="p_context"> static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)</span>
 
 	flowi4_init_output(&amp;fl4, ipc.oif, sk-&gt;sk_mark, tos,
 			   RT_SCOPE_UNIVERSE,
<span class="p_del">-			   inet-&gt;hdrincl ? IPPROTO_RAW : sk-&gt;sk_protocol,</span>
<span class="p_add">+			   hdrincl ? IPPROTO_RAW : sk-&gt;sk_protocol,</span>
 			   inet_sk_flowi_flags(sk) |
<span class="p_del">-			    (inet-&gt;hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),</span>
<span class="p_add">+			    (hdrincl ? FLOWI_FLAG_KNOWN_NH : 0),</span>
 			   daddr, saddr, 0, 0, sk-&gt;sk_uid);
 
<span class="p_del">-	if (!inet-&gt;hdrincl) {</span>
<span class="p_add">+	if (!hdrincl) {</span>
 		rfv.msg = msg;
 		rfv.hlen = 0;
 
<span class="p_chunk">@@ -645,7 +650,7 @@</span> <span class="p_context"> static int raw_sendmsg(struct sock *sk, struct msghdr *msg, size_t len)</span>
 		goto do_confirm;
 back_from_confirm:
 
<span class="p_del">-	if (inet-&gt;hdrincl)</span>
<span class="p_add">+	if (hdrincl)</span>
 		err = raw_send_hdrinc(sk, &amp;fl4, msg, len,
 				      &amp;rt, msg-&gt;msg_flags, &amp;ipc.sockc);
 
<span class="p_header">diff --git a/net/ipv4/tcp.c b/net/ipv4/tcp.c</span>
<span class="p_header">index 5091402720ab..a0c72b09cefc 100644</span>
<span class="p_header">--- a/net/ipv4/tcp.c</span>
<span class="p_header">+++ b/net/ipv4/tcp.c</span>
<span class="p_chunk">@@ -2356,6 +2356,7 @@</span> <span class="p_context"> int tcp_disconnect(struct sock *sk, int flags)</span>
 	tp-&gt;snd_cwnd_cnt = 0;
 	tp-&gt;window_clamp = 0;
 	tcp_set_ca_state(sk, TCP_CA_Open);
<span class="p_add">+	tp-&gt;is_sack_reneg = 0;</span>
 	tcp_clear_retrans(tp);
 	inet_csk_delack_init(sk);
 	/* Initialize rcv_mss to TCP_MIN_MSS to avoid division by 0
<span class="p_header">diff --git a/net/ipv4/tcp_bbr.c b/net/ipv4/tcp_bbr.c</span>
<span class="p_header">index 69ee877574d0..8322f26e770e 100644</span>
<span class="p_header">--- a/net/ipv4/tcp_bbr.c</span>
<span class="p_header">+++ b/net/ipv4/tcp_bbr.c</span>
<span class="p_chunk">@@ -110,7 +110,8 @@</span> <span class="p_context"> struct bbr {</span>
 	u32	lt_last_lost;	     /* LT intvl start: tp-&gt;lost */
 	u32	pacing_gain:10,	/* current gain for setting pacing rate */
 		cwnd_gain:10,	/* current gain for setting cwnd */
<span class="p_del">-		full_bw_cnt:3,	/* number of rounds without large bw gains */</span>
<span class="p_add">+		full_bw_reached:1,   /* reached full bw in Startup? */</span>
<span class="p_add">+		full_bw_cnt:2,	/* number of rounds without large bw gains */</span>
 		cycle_idx:3,	/* current index in pacing_gain cycle array */
 		has_seen_rtt:1, /* have we seen an RTT sample yet? */
 		unused_b:5;
<span class="p_chunk">@@ -180,7 +181,7 @@</span> <span class="p_context"> static bool bbr_full_bw_reached(const struct sock *sk)</span>
 {
 	const struct bbr *bbr = inet_csk_ca(sk);
 
<span class="p_del">-	return bbr-&gt;full_bw_cnt &gt;= bbr_full_bw_cnt;</span>
<span class="p_add">+	return bbr-&gt;full_bw_reached;</span>
 }
 
 /* Return the windowed max recent bandwidth sample, in pkts/uS &lt;&lt; BW_SCALE. */
<span class="p_chunk">@@ -717,6 +718,7 @@</span> <span class="p_context"> static void bbr_check_full_bw_reached(struct sock *sk,</span>
 		return;
 	}
 	++bbr-&gt;full_bw_cnt;
<span class="p_add">+	bbr-&gt;full_bw_reached = bbr-&gt;full_bw_cnt &gt;= bbr_full_bw_cnt;</span>
 }
 
 /* If pipe is probably full, drain the queue and then enter steady-state. */
<span class="p_chunk">@@ -850,6 +852,7 @@</span> <span class="p_context"> static void bbr_init(struct sock *sk)</span>
 	bbr-&gt;restore_cwnd = 0;
 	bbr-&gt;round_start = 0;
 	bbr-&gt;idle_restart = 0;
<span class="p_add">+	bbr-&gt;full_bw_reached = 0;</span>
 	bbr-&gt;full_bw = 0;
 	bbr-&gt;full_bw_cnt = 0;
 	bbr-&gt;cycle_mstamp = 0;
<span class="p_chunk">@@ -871,6 +874,11 @@</span> <span class="p_context"> static u32 bbr_sndbuf_expand(struct sock *sk)</span>
  */
 static u32 bbr_undo_cwnd(struct sock *sk)
 {
<span class="p_add">+	struct bbr *bbr = inet_csk_ca(sk);</span>
<span class="p_add">+</span>
<span class="p_add">+	bbr-&gt;full_bw = 0;   /* spurious slow-down; reset full pipe detection */</span>
<span class="p_add">+	bbr-&gt;full_bw_cnt = 0;</span>
<span class="p_add">+	bbr_reset_lt_bw_sampling(sk);</span>
 	return tcp_sk(sk)-&gt;snd_cwnd;
 }
 
<span class="p_header">diff --git a/net/ipv4/tcp_input.c b/net/ipv4/tcp_input.c</span>
<span class="p_header">index c5447b9f8517..ff48ac654e5a 100644</span>
<span class="p_header">--- a/net/ipv4/tcp_input.c</span>
<span class="p_header">+++ b/net/ipv4/tcp_input.c</span>
<span class="p_chunk">@@ -521,9 +521,6 @@</span> <span class="p_context"> static void tcp_rcv_rtt_update(struct tcp_sock *tp, u32 sample, int win_dep)</span>
 	u32 new_sample = tp-&gt;rcv_rtt_est.rtt_us;
 	long m = sample;
 
<span class="p_del">-	if (m == 0)</span>
<span class="p_del">-		m = 1;</span>
<span class="p_del">-</span>
 	if (new_sample != 0) {
 		/* If we sample in larger samples in the non-timestamp
 		 * case, we could grossly overestimate the RTT especially
<span class="p_chunk">@@ -560,6 +557,8 @@</span> <span class="p_context"> static inline void tcp_rcv_rtt_measure(struct tcp_sock *tp)</span>
 	if (before(tp-&gt;rcv_nxt, tp-&gt;rcv_rtt_est.seq))
 		return;
 	delta_us = tcp_stamp_us_delta(tp-&gt;tcp_mstamp, tp-&gt;rcv_rtt_est.time);
<span class="p_add">+	if (!delta_us)</span>
<span class="p_add">+		delta_us = 1;</span>
 	tcp_rcv_rtt_update(tp, delta_us, 1);
 
 new_measure:
<span class="p_chunk">@@ -576,8 +575,11 @@</span> <span class="p_context"> static inline void tcp_rcv_rtt_measure_ts(struct sock *sk,</span>
 	    (TCP_SKB_CB(skb)-&gt;end_seq -
 	     TCP_SKB_CB(skb)-&gt;seq &gt;= inet_csk(sk)-&gt;icsk_ack.rcv_mss)) {
 		u32 delta = tcp_time_stamp(tp) - tp-&gt;rx_opt.rcv_tsecr;
<span class="p_del">-		u32 delta_us = delta * (USEC_PER_SEC / TCP_TS_HZ);</span>
<span class="p_add">+		u32 delta_us;</span>
 
<span class="p_add">+		if (!delta)</span>
<span class="p_add">+			delta = 1;</span>
<span class="p_add">+		delta_us = delta * (USEC_PER_SEC / TCP_TS_HZ);</span>
 		tcp_rcv_rtt_update(tp, delta_us, 0);
 	}
 }
<span class="p_chunk">@@ -1975,6 +1977,8 @@</span> <span class="p_context"> void tcp_enter_loss(struct sock *sk)</span>
 		NET_INC_STATS(sock_net(sk), LINUX_MIB_TCPSACKRENEGING);
 		tp-&gt;sacked_out = 0;
 		tp-&gt;fackets_out = 0;
<span class="p_add">+		/* Mark SACK reneging until we recover from this loss event. */</span>
<span class="p_add">+		tp-&gt;is_sack_reneg = 1;</span>
 	}
 	tcp_clear_all_retrans_hints(tp);
 
<span class="p_chunk">@@ -2428,6 +2432,7 @@</span> <span class="p_context"> static bool tcp_try_undo_recovery(struct sock *sk)</span>
 		return true;
 	}
 	tcp_set_ca_state(sk, TCP_CA_Open);
<span class="p_add">+	tp-&gt;is_sack_reneg = 0;</span>
 	return false;
 }
 
<span class="p_chunk">@@ -2459,8 +2464,10 @@</span> <span class="p_context"> static bool tcp_try_undo_loss(struct sock *sk, bool frto_undo)</span>
 			NET_INC_STATS(sock_net(sk),
 					LINUX_MIB_TCPSPURIOUSRTOS);
 		inet_csk(sk)-&gt;icsk_retransmits = 0;
<span class="p_del">-		if (frto_undo || tcp_is_sack(tp))</span>
<span class="p_add">+		if (frto_undo || tcp_is_sack(tp)) {</span>
 			tcp_set_ca_state(sk, TCP_CA_Open);
<span class="p_add">+			tp-&gt;is_sack_reneg = 0;</span>
<span class="p_add">+		}</span>
 		return true;
 	}
 	return false;
<span class="p_chunk">@@ -3551,6 +3558,7 @@</span> <span class="p_context"> static int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)</span>
 	struct tcp_sacktag_state sack_state;
 	struct rate_sample rs = { .prior_delivered = 0 };
 	u32 prior_snd_una = tp-&gt;snd_una;
<span class="p_add">+	bool is_sack_reneg = tp-&gt;is_sack_reneg;</span>
 	u32 ack_seq = TCP_SKB_CB(skb)-&gt;seq;
 	u32 ack = TCP_SKB_CB(skb)-&gt;ack_seq;
 	bool is_dupack = false;
<span class="p_chunk">@@ -3666,7 +3674,7 @@</span> <span class="p_context"> static int tcp_ack(struct sock *sk, const struct sk_buff *skb, int flag)</span>
 
 	delivered = tp-&gt;delivered - delivered;	/* freshly ACKed or SACKed */
 	lost = tp-&gt;lost - lost;			/* freshly marked lost */
<span class="p_del">-	tcp_rate_gen(sk, delivered, lost, sack_state.rate);</span>
<span class="p_add">+	tcp_rate_gen(sk, delivered, lost, is_sack_reneg, sack_state.rate);</span>
 	tcp_cong_control(sk, ack, delivered, flag, sack_state.rate);
 	tcp_xmit_recovery(sk, rexmit);
 	return 1;
<span class="p_header">diff --git a/net/ipv4/tcp_ipv4.c b/net/ipv4/tcp_ipv4.c</span>
<span class="p_header">index 5a5ed4f14678..cab4b935e474 100644</span>
<span class="p_header">--- a/net/ipv4/tcp_ipv4.c</span>
<span class="p_header">+++ b/net/ipv4/tcp_ipv4.c</span>
<span class="p_chunk">@@ -844,7 +844,7 @@</span> <span class="p_context"> static void tcp_v4_reqsk_send_ack(const struct sock *sk, struct sk_buff *skb,</span>
 			tcp_time_stamp_raw() + tcp_rsk(req)-&gt;ts_off,
 			req-&gt;ts_recent,
 			0,
<span class="p_del">-			tcp_md5_do_lookup(sk, (union tcp_md5_addr *)&amp;ip_hdr(skb)-&gt;daddr,</span>
<span class="p_add">+			tcp_md5_do_lookup(sk, (union tcp_md5_addr *)&amp;ip_hdr(skb)-&gt;saddr,</span>
 					  AF_INET),
 			inet_rsk(req)-&gt;no_srccheck ? IP_REPLY_ARG_NOSRCCHECK : 0,
 			ip_hdr(skb)-&gt;tos);
<span class="p_header">diff --git a/net/ipv4/tcp_rate.c b/net/ipv4/tcp_rate.c</span>
<span class="p_header">index 3330a370d306..c61240e43923 100644</span>
<span class="p_header">--- a/net/ipv4/tcp_rate.c</span>
<span class="p_header">+++ b/net/ipv4/tcp_rate.c</span>
<span class="p_chunk">@@ -106,7 +106,7 @@</span> <span class="p_context"> void tcp_rate_skb_delivered(struct sock *sk, struct sk_buff *skb,</span>
 
 /* Update the connection delivery information and generate a rate sample. */
 void tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,
<span class="p_del">-		  struct rate_sample *rs)</span>
<span class="p_add">+		  bool is_sack_reneg, struct rate_sample *rs)</span>
 {
 	struct tcp_sock *tp = tcp_sk(sk);
 	u32 snd_us, ack_us;
<span class="p_chunk">@@ -124,8 +124,12 @@</span> <span class="p_context"> void tcp_rate_gen(struct sock *sk, u32 delivered, u32 lost,</span>
 
 	rs-&gt;acked_sacked = delivered;	/* freshly ACKed or SACKed */
 	rs-&gt;losses = lost;		/* freshly marked lost */
<span class="p_del">-	/* Return an invalid sample if no timing information is available. */</span>
<span class="p_del">-	if (!rs-&gt;prior_mstamp) {</span>
<span class="p_add">+	/* Return an invalid sample if no timing information is available or</span>
<span class="p_add">+	 * in recovery from loss with SACK reneging. Rate samples taken during</span>
<span class="p_add">+	 * a SACK reneging event may overestimate bw by including packets that</span>
<span class="p_add">+	 * were SACKed before the reneg.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (!rs-&gt;prior_mstamp || is_sack_reneg) {</span>
 		rs-&gt;delivered = -1;
 		rs-&gt;interval_us = -1;
 		return;
<span class="p_header">diff --git a/net/ipv4/tcp_timer.c b/net/ipv4/tcp_timer.c</span>
<span class="p_header">index 655dd8d7f064..e9af1879cd53 100644</span>
<span class="p_header">--- a/net/ipv4/tcp_timer.c</span>
<span class="p_header">+++ b/net/ipv4/tcp_timer.c</span>
<span class="p_chunk">@@ -264,6 +264,7 @@</span> <span class="p_context"> void tcp_delack_timer_handler(struct sock *sk)</span>
 			icsk-&gt;icsk_ack.pingpong = 0;
 			icsk-&gt;icsk_ack.ato      = TCP_ATO_MIN;
 		}
<span class="p_add">+		tcp_mstamp_refresh(tcp_sk(sk));</span>
 		tcp_send_ack(sk);
 		__NET_INC_STATS(sock_net(sk), LINUX_MIB_DELAYEDACKS);
 	}
<span class="p_chunk">@@ -627,6 +628,7 @@</span> <span class="p_context"> static void tcp_keepalive_timer (unsigned long data)</span>
 		goto out;
 	}
 
<span class="p_add">+	tcp_mstamp_refresh(tp);</span>
 	if (sk-&gt;sk_state == TCP_FIN_WAIT2 &amp;&amp; sock_flag(sk, SOCK_DEAD)) {
 		if (tp-&gt;linger2 &gt;= 0) {
 			const int tmo = tcp_fin_time(sk) - TCP_TIMEWAIT_LEN;
<span class="p_header">diff --git a/net/ipv6/addrconf.c b/net/ipv6/addrconf.c</span>
<span class="p_header">index 2ec39404c449..c5318f5f6a14 100644</span>
<span class="p_header">--- a/net/ipv6/addrconf.c</span>
<span class="p_header">+++ b/net/ipv6/addrconf.c</span>
<span class="p_chunk">@@ -231,7 +231,7 @@</span> <span class="p_context"> static struct ipv6_devconf ipv6_devconf __read_mostly = {</span>
 	.proxy_ndp		= 0,
 	.accept_source_route	= 0,	/* we do not accept RH0 by default. */
 	.disable_ipv6		= 0,
<span class="p_del">-	.accept_dad		= 1,</span>
<span class="p_add">+	.accept_dad		= 0,</span>
 	.suppress_frag_ndisc	= 1,
 	.accept_ra_mtu		= 1,
 	.stable_secret		= {
<span class="p_header">diff --git a/net/ipv6/af_inet6.c b/net/ipv6/af_inet6.c</span>
<span class="p_header">index fe5262fd6aa5..bcbd5f3bf8bd 100644</span>
<span class="p_header">--- a/net/ipv6/af_inet6.c</span>
<span class="p_header">+++ b/net/ipv6/af_inet6.c</span>
<span class="p_chunk">@@ -210,7 +210,6 @@</span> <span class="p_context"> static int inet6_create(struct net *net, struct socket *sock, int protocol,</span>
 	np-&gt;mcast_hops	= IPV6_DEFAULT_MCASTHOPS;
 	np-&gt;mc_loop	= 1;
 	np-&gt;pmtudisc	= IPV6_PMTUDISC_WANT;
<span class="p_del">-	np-&gt;autoflowlabel = ip6_default_np_autolabel(net);</span>
 	np-&gt;repflow	= net-&gt;ipv6.sysctl.flowlabel_reflect;
 	sk-&gt;sk_ipv6only	= net-&gt;ipv6.sysctl.bindv6only;
 
<span class="p_header">diff --git a/net/ipv6/ip6_gre.c b/net/ipv6/ip6_gre.c</span>
<span class="p_header">index 5d6bee070871..7a2df6646486 100644</span>
<span class="p_header">--- a/net/ipv6/ip6_gre.c</span>
<span class="p_header">+++ b/net/ipv6/ip6_gre.c</span>
<span class="p_chunk">@@ -1020,6 +1020,36 @@</span> <span class="p_context"> static void ip6gre_tunnel_setup(struct net_device *dev)</span>
 	eth_random_addr(dev-&gt;perm_addr);
 }
 
<span class="p_add">+#define GRE6_FEATURES (NETIF_F_SG |		\</span>
<span class="p_add">+		       NETIF_F_FRAGLIST |	\</span>
<span class="p_add">+		       NETIF_F_HIGHDMA |	\</span>
<span class="p_add">+		       NETIF_F_HW_CSUM)</span>
<span class="p_add">+</span>
<span class="p_add">+static void ip6gre_tnl_init_features(struct net_device *dev)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct ip6_tnl *nt = netdev_priv(dev);</span>
<span class="p_add">+</span>
<span class="p_add">+	dev-&gt;features		|= GRE6_FEATURES;</span>
<span class="p_add">+	dev-&gt;hw_features	|= GRE6_FEATURES;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!(nt-&gt;parms.o_flags &amp; TUNNEL_SEQ)) {</span>
<span class="p_add">+		/* TCP offload with GRE SEQ is not supported, nor</span>
<span class="p_add">+		 * can we support 2 levels of outer headers requiring</span>
<span class="p_add">+		 * an update.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!(nt-&gt;parms.o_flags &amp; TUNNEL_CSUM) ||</span>
<span class="p_add">+		    nt-&gt;encap.type == TUNNEL_ENCAP_NONE) {</span>
<span class="p_add">+			dev-&gt;features    |= NETIF_F_GSO_SOFTWARE;</span>
<span class="p_add">+			dev-&gt;hw_features |= NETIF_F_GSO_SOFTWARE;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Can use a lockless transmit, unless we generate</span>
<span class="p_add">+		 * output sequences</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		dev-&gt;features |= NETIF_F_LLTX;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static int ip6gre_tunnel_init_common(struct net_device *dev)
 {
 	struct ip6_tnl *tunnel;
<span class="p_chunk">@@ -1054,6 +1084,8 @@</span> <span class="p_context"> static int ip6gre_tunnel_init_common(struct net_device *dev)</span>
 	if (!(tunnel-&gt;parms.flags &amp; IP6_TNL_F_IGN_ENCAP_LIMIT))
 		dev-&gt;mtu -= 8;
 
<span class="p_add">+	ip6gre_tnl_init_features(dev);</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -1302,11 +1334,6 @@</span> <span class="p_context"> static const struct net_device_ops ip6gre_tap_netdev_ops = {</span>
 	.ndo_get_iflink = ip6_tnl_get_iflink,
 };
 
<span class="p_del">-#define GRE6_FEATURES (NETIF_F_SG |		\</span>
<span class="p_del">-		       NETIF_F_FRAGLIST |	\</span>
<span class="p_del">-		       NETIF_F_HIGHDMA |		\</span>
<span class="p_del">-		       NETIF_F_HW_CSUM)</span>
<span class="p_del">-</span>
 static void ip6gre_tap_setup(struct net_device *dev)
 {
 
<span class="p_chunk">@@ -1386,26 +1413,6 @@</span> <span class="p_context"> static int ip6gre_newlink(struct net *src_net, struct net_device *dev,</span>
 	nt-&gt;net = dev_net(dev);
 	ip6gre_tnl_link_config(nt, !tb[IFLA_MTU]);
 
<span class="p_del">-	dev-&gt;features		|= GRE6_FEATURES;</span>
<span class="p_del">-	dev-&gt;hw_features	|= GRE6_FEATURES;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!(nt-&gt;parms.o_flags &amp; TUNNEL_SEQ)) {</span>
<span class="p_del">-		/* TCP offload with GRE SEQ is not supported, nor</span>
<span class="p_del">-		 * can we support 2 levels of outer headers requiring</span>
<span class="p_del">-		 * an update.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (!(nt-&gt;parms.o_flags &amp; TUNNEL_CSUM) ||</span>
<span class="p_del">-		    (nt-&gt;encap.type == TUNNEL_ENCAP_NONE)) {</span>
<span class="p_del">-			dev-&gt;features    |= NETIF_F_GSO_SOFTWARE;</span>
<span class="p_del">-			dev-&gt;hw_features |= NETIF_F_GSO_SOFTWARE;</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Can use a lockless transmit, unless we generate</span>
<span class="p_del">-		 * output sequences</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		dev-&gt;features |= NETIF_F_LLTX;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	err = register_netdevice(dev);
 	if (err)
 		goto out;
<span class="p_header">diff --git a/net/ipv6/ip6_output.c b/net/ipv6/ip6_output.c</span>
<span class="p_header">index 5110a418cc4d..f7dd51c42314 100644</span>
<span class="p_header">--- a/net/ipv6/ip6_output.c</span>
<span class="p_header">+++ b/net/ipv6/ip6_output.c</span>
<span class="p_chunk">@@ -166,6 +166,14 @@</span> <span class="p_context"> int ip6_output(struct net *net, struct sock *sk, struct sk_buff *skb)</span>
 			    !(IP6CB(skb)-&gt;flags &amp; IP6SKB_REROUTED));
 }
 
<span class="p_add">+static bool ip6_autoflowlabel(struct net *net, const struct ipv6_pinfo *np)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (!np-&gt;autoflowlabel_set)</span>
<span class="p_add">+		return ip6_default_np_autolabel(net);</span>
<span class="p_add">+	else</span>
<span class="p_add">+		return np-&gt;autoflowlabel;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * xmit an sk_buff (used by TCP, SCTP and DCCP)
  * Note : socket lock is not held for SYNACK packets, but might be modified
<span class="p_chunk">@@ -230,7 +238,7 @@</span> <span class="p_context"> int ip6_xmit(const struct sock *sk, struct sk_buff *skb, struct flowi6 *fl6,</span>
 		hlimit = ip6_dst_hoplimit(dst);
 
 	ip6_flow_hdr(hdr, tclass, ip6_make_flowlabel(net, skb, fl6-&gt;flowlabel,
<span class="p_del">-						     np-&gt;autoflowlabel, fl6));</span>
<span class="p_add">+				ip6_autoflowlabel(net, np), fl6));</span>
 
 	hdr-&gt;payload_len = htons(seg_len);
 	hdr-&gt;nexthdr = proto;
<span class="p_chunk">@@ -1626,7 +1634,7 @@</span> <span class="p_context"> struct sk_buff *__ip6_make_skb(struct sock *sk,</span>
 
 	ip6_flow_hdr(hdr, v6_cork-&gt;tclass,
 		     ip6_make_flowlabel(net, skb, fl6-&gt;flowlabel,
<span class="p_del">-					np-&gt;autoflowlabel, fl6));</span>
<span class="p_add">+					ip6_autoflowlabel(net, np), fl6));</span>
 	hdr-&gt;hop_limit = v6_cork-&gt;hop_limit;
 	hdr-&gt;nexthdr = proto;
 	hdr-&gt;saddr = fl6-&gt;saddr;
<span class="p_header">diff --git a/net/ipv6/ip6_tunnel.c b/net/ipv6/ip6_tunnel.c</span>
<span class="p_header">index a1c24443cd9e..ef958d50746b 100644</span>
<span class="p_header">--- a/net/ipv6/ip6_tunnel.c</span>
<span class="p_header">+++ b/net/ipv6/ip6_tunnel.c</span>
<span class="p_chunk">@@ -912,7 +912,7 @@</span> <span class="p_context"> static int ipxip6_rcv(struct sk_buff *skb, u8 ipproto,</span>
 		if (t-&gt;parms.collect_md) {
 			tun_dst = ipv6_tun_rx_dst(skb, 0, 0, 0);
 			if (!tun_dst)
<span class="p_del">-				return 0;</span>
<span class="p_add">+				goto drop;</span>
 		}
 		ret = __ip6_tnl_rcv(t, skb, tpi, tun_dst, dscp_ecn_decapsulate,
 				    log_ecn_error);
<span class="p_header">diff --git a/net/ipv6/ipv6_sockglue.c b/net/ipv6/ipv6_sockglue.c</span>
<span class="p_header">index a5e466d4e093..90dbfa78a390 100644</span>
<span class="p_header">--- a/net/ipv6/ipv6_sockglue.c</span>
<span class="p_header">+++ b/net/ipv6/ipv6_sockglue.c</span>
<span class="p_chunk">@@ -878,6 +878,7 @@</span> <span class="p_context"> static int do_ipv6_setsockopt(struct sock *sk, int level, int optname,</span>
 		break;
 	case IPV6_AUTOFLOWLABEL:
 		np-&gt;autoflowlabel = valbool;
<span class="p_add">+		np-&gt;autoflowlabel_set = 1;</span>
 		retv = 0;
 		break;
 	case IPV6_RECVFRAGSIZE:
<span class="p_header">diff --git a/net/ipv6/mcast.c b/net/ipv6/mcast.c</span>
<span class="p_header">index 12b7c27ce5ce..9a38a2c641fa 100644</span>
<span class="p_header">--- a/net/ipv6/mcast.c</span>
<span class="p_header">+++ b/net/ipv6/mcast.c</span>
<span class="p_chunk">@@ -1682,16 +1682,16 @@</span> <span class="p_context"> static int grec_size(struct ifmcaddr6 *pmc, int type, int gdel, int sdel)</span>
 }
 
 static struct sk_buff *add_grhead(struct sk_buff *skb, struct ifmcaddr6 *pmc,
<span class="p_del">-	int type, struct mld2_grec **ppgr)</span>
<span class="p_add">+	int type, struct mld2_grec **ppgr, unsigned int mtu)</span>
 {
<span class="p_del">-	struct net_device *dev = pmc-&gt;idev-&gt;dev;</span>
 	struct mld2_report *pmr;
 	struct mld2_grec *pgr;
 
<span class="p_del">-	if (!skb)</span>
<span class="p_del">-		skb = mld_newpack(pmc-&gt;idev, dev-&gt;mtu);</span>
<span class="p_del">-	if (!skb)</span>
<span class="p_del">-		return NULL;</span>
<span class="p_add">+	if (!skb) {</span>
<span class="p_add">+		skb = mld_newpack(pmc-&gt;idev, mtu);</span>
<span class="p_add">+		if (!skb)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+	}</span>
 	pgr = skb_put(skb, sizeof(struct mld2_grec));
 	pgr-&gt;grec_type = type;
 	pgr-&gt;grec_auxwords = 0;
<span class="p_chunk">@@ -1714,10 +1714,15 @@</span> <span class="p_context"> static struct sk_buff *add_grec(struct sk_buff *skb, struct ifmcaddr6 *pmc,</span>
 	struct mld2_grec *pgr = NULL;
 	struct ip6_sf_list *psf, *psf_next, *psf_prev, **psf_list;
 	int scount, stotal, first, isquery, truncate;
<span class="p_add">+	unsigned int mtu;</span>
 
 	if (pmc-&gt;mca_flags &amp; MAF_NOREPORT)
 		return skb;
 
<span class="p_add">+	mtu = READ_ONCE(dev-&gt;mtu);</span>
<span class="p_add">+	if (mtu &lt; IPV6_MIN_MTU)</span>
<span class="p_add">+		return skb;</span>
<span class="p_add">+</span>
 	isquery = type == MLD2_MODE_IS_INCLUDE ||
 		  type == MLD2_MODE_IS_EXCLUDE;
 	truncate = type == MLD2_MODE_IS_EXCLUDE ||
<span class="p_chunk">@@ -1738,7 +1743,7 @@</span> <span class="p_context"> static struct sk_buff *add_grec(struct sk_buff *skb, struct ifmcaddr6 *pmc,</span>
 		    AVAILABLE(skb) &lt; grec_size(pmc, type, gdeleted, sdeleted)) {
 			if (skb)
 				mld_sendpack(skb);
<span class="p_del">-			skb = mld_newpack(idev, dev-&gt;mtu);</span>
<span class="p_add">+			skb = mld_newpack(idev, mtu);</span>
 		}
 	}
 	first = 1;
<span class="p_chunk">@@ -1774,12 +1779,12 @@</span> <span class="p_context"> static struct sk_buff *add_grec(struct sk_buff *skb, struct ifmcaddr6 *pmc,</span>
 				pgr-&gt;grec_nsrcs = htons(scount);
 			if (skb)
 				mld_sendpack(skb);
<span class="p_del">-			skb = mld_newpack(idev, dev-&gt;mtu);</span>
<span class="p_add">+			skb = mld_newpack(idev, mtu);</span>
 			first = 1;
 			scount = 0;
 		}
 		if (first) {
<span class="p_del">-			skb = add_grhead(skb, pmc, type, &amp;pgr);</span>
<span class="p_add">+			skb = add_grhead(skb, pmc, type, &amp;pgr, mtu);</span>
 			first = 0;
 		}
 		if (!skb)
<span class="p_chunk">@@ -1814,7 +1819,7 @@</span> <span class="p_context"> static struct sk_buff *add_grec(struct sk_buff *skb, struct ifmcaddr6 *pmc,</span>
 				mld_sendpack(skb);
 				skb = NULL; /* add_grhead will get a new one */
 			}
<span class="p_del">-			skb = add_grhead(skb, pmc, type, &amp;pgr);</span>
<span class="p_add">+			skb = add_grhead(skb, pmc, type, &amp;pgr, mtu);</span>
 		}
 	}
 	if (pgr)
<span class="p_header">diff --git a/net/ipv6/route.c b/net/ipv6/route.c</span>
<span class="p_header">index 598efa8cfe25..ca8d3266e92e 100644</span>
<span class="p_header">--- a/net/ipv6/route.c</span>
<span class="p_header">+++ b/net/ipv6/route.c</span>
<span class="p_chunk">@@ -3700,19 +3700,13 @@</span> <span class="p_context"> static int inet6_rtm_getroute(struct sk_buff *in_skb, struct nlmsghdr *nlh,</span>
 		if (!ipv6_addr_any(&amp;fl6.saddr))
 			flags |= RT6_LOOKUP_F_HAS_SADDR;
 
<span class="p_del">-		if (!fibmatch)</span>
<span class="p_del">-			dst = ip6_route_input_lookup(net, dev, &amp;fl6, flags);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			dst = ip6_route_lookup(net, &amp;fl6, 0);</span>
<span class="p_add">+		dst = ip6_route_input_lookup(net, dev, &amp;fl6, flags);</span>
 
 		rcu_read_unlock();
 	} else {
 		fl6.flowi6_oif = oif;
 
<span class="p_del">-		if (!fibmatch)</span>
<span class="p_del">-			dst = ip6_route_output(net, NULL, &amp;fl6);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			dst = ip6_route_lookup(net, &amp;fl6, 0);</span>
<span class="p_add">+		dst = ip6_route_output(net, NULL, &amp;fl6);</span>
 	}
 
 
<span class="p_chunk">@@ -3729,6 +3723,15 @@</span> <span class="p_context"> static int inet6_rtm_getroute(struct sk_buff *in_skb, struct nlmsghdr *nlh,</span>
 		goto errout;
 	}
 
<span class="p_add">+	if (fibmatch &amp;&amp; rt-&gt;dst.from) {</span>
<span class="p_add">+		struct rt6_info *ort = container_of(rt-&gt;dst.from,</span>
<span class="p_add">+						    struct rt6_info, dst);</span>
<span class="p_add">+</span>
<span class="p_add">+		dst_hold(&amp;ort-&gt;dst);</span>
<span class="p_add">+		ip6_rt_put(rt);</span>
<span class="p_add">+		rt = ort;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	skb = alloc_skb(NLMSG_GOODSIZE, GFP_KERNEL);
 	if (!skb) {
 		ip6_rt_put(rt);
<span class="p_header">diff --git a/net/ipv6/tcp_ipv6.c b/net/ipv6/tcp_ipv6.c</span>
<span class="p_header">index 32ded300633d..237cc6187c5a 100644</span>
<span class="p_header">--- a/net/ipv6/tcp_ipv6.c</span>
<span class="p_header">+++ b/net/ipv6/tcp_ipv6.c</span>
<span class="p_chunk">@@ -988,7 +988,7 @@</span> <span class="p_context"> static void tcp_v6_reqsk_send_ack(const struct sock *sk, struct sk_buff *skb,</span>
 			req-&gt;rsk_rcv_wnd &gt;&gt; inet_rsk(req)-&gt;rcv_wscale,
 			tcp_time_stamp_raw() + tcp_rsk(req)-&gt;ts_off,
 			req-&gt;ts_recent, sk-&gt;sk_bound_dev_if,
<span class="p_del">-			tcp_v6_md5_do_lookup(sk, &amp;ipv6_hdr(skb)-&gt;daddr),</span>
<span class="p_add">+			tcp_v6_md5_do_lookup(sk, &amp;ipv6_hdr(skb)-&gt;saddr),</span>
 			0, 0);
 }
 
<span class="p_header">diff --git a/net/netlink/af_netlink.c b/net/netlink/af_netlink.c</span>
<span class="p_header">index 15c99dfa3d72..aac9d68b4636 100644</span>
<span class="p_header">--- a/net/netlink/af_netlink.c</span>
<span class="p_header">+++ b/net/netlink/af_netlink.c</span>
<span class="p_chunk">@@ -254,6 +254,9 @@</span> <span class="p_context"> static int __netlink_deliver_tap_skb(struct sk_buff *skb,</span>
 	struct sock *sk = skb-&gt;sk;
 	int ret = -ENOMEM;
 
<span class="p_add">+	if (!net_eq(dev_net(dev), sock_net(sk)))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
 	dev_hold(dev);
 
 	if (is_vmalloc_addr(skb-&gt;head))
<span class="p_header">diff --git a/net/openvswitch/flow.c b/net/openvswitch/flow.c</span>
<span class="p_header">index cfb652a4e007..dbe1079a1651 100644</span>
<span class="p_header">--- a/net/openvswitch/flow.c</span>
<span class="p_header">+++ b/net/openvswitch/flow.c</span>
<span class="p_chunk">@@ -532,6 +532,7 @@</span> <span class="p_context"> static int key_extract(struct sk_buff *skb, struct sw_flow_key *key)</span>
 			return -EINVAL;
 
 		skb_reset_network_header(skb);
<span class="p_add">+		key-&gt;eth.type = skb-&gt;protocol;</span>
 	} else {
 		eth = eth_hdr(skb);
 		ether_addr_copy(key-&gt;eth.src, eth-&gt;h_source);
<span class="p_chunk">@@ -545,15 +546,23 @@</span> <span class="p_context"> static int key_extract(struct sk_buff *skb, struct sw_flow_key *key)</span>
 		if (unlikely(parse_vlan(skb, key)))
 			return -ENOMEM;
 
<span class="p_del">-		skb-&gt;protocol = parse_ethertype(skb);</span>
<span class="p_del">-		if (unlikely(skb-&gt;protocol == htons(0)))</span>
<span class="p_add">+		key-&gt;eth.type = parse_ethertype(skb);</span>
<span class="p_add">+		if (unlikely(key-&gt;eth.type == htons(0)))</span>
 			return -ENOMEM;
 
<span class="p_add">+		/* Multiple tagged packets need to retain TPID to satisfy</span>
<span class="p_add">+		 * skb_vlan_pop(), which will later shift the ethertype into</span>
<span class="p_add">+		 * skb-&gt;protocol.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (key-&gt;eth.cvlan.tci &amp; htons(VLAN_TAG_PRESENT))</span>
<span class="p_add">+			skb-&gt;protocol = key-&gt;eth.cvlan.tpid;</span>
<span class="p_add">+		else</span>
<span class="p_add">+			skb-&gt;protocol = key-&gt;eth.type;</span>
<span class="p_add">+</span>
 		skb_reset_network_header(skb);
 		__skb_push(skb, skb-&gt;data - skb_mac_header(skb));
 	}
 	skb_reset_mac_len(skb);
<span class="p_del">-	key-&gt;eth.type = skb-&gt;protocol;</span>
 
 	/* Network layer. */
 	if (key-&gt;eth.type == htons(ETH_P_IP)) {
<span class="p_header">diff --git a/net/rds/send.c b/net/rds/send.c</span>
<span class="p_header">index b52cdc8ae428..f72466c63f0c 100644</span>
<span class="p_header">--- a/net/rds/send.c</span>
<span class="p_header">+++ b/net/rds/send.c</span>
<span class="p_chunk">@@ -1009,6 +1009,9 @@</span> <span class="p_context"> static int rds_rdma_bytes(struct msghdr *msg, size_t *rdma_bytes)</span>
 			continue;
 
 		if (cmsg-&gt;cmsg_type == RDS_CMSG_RDMA_ARGS) {
<span class="p_add">+			if (cmsg-&gt;cmsg_len &lt;</span>
<span class="p_add">+			    CMSG_LEN(sizeof(struct rds_rdma_args)))</span>
<span class="p_add">+				return -EINVAL;</span>
 			args = CMSG_DATA(cmsg);
 			*rdma_bytes += args-&gt;remote_vec.bytes;
 		}
<span class="p_header">diff --git a/net/sched/sch_ingress.c b/net/sched/sch_ingress.c</span>
<span class="p_header">index 44de4ee51ce9..a08a32fa0949 100644</span>
<span class="p_header">--- a/net/sched/sch_ingress.c</span>
<span class="p_header">+++ b/net/sched/sch_ingress.c</span>
<span class="p_chunk">@@ -59,11 +59,12 @@</span> <span class="p_context"> static int ingress_init(struct Qdisc *sch, struct nlattr *opt)</span>
 	struct net_device *dev = qdisc_dev(sch);
 	int err;
 
<span class="p_add">+	net_inc_ingress_queue();</span>
<span class="p_add">+</span>
 	err = tcf_block_get(&amp;q-&gt;block, &amp;dev-&gt;ingress_cl_list);
 	if (err)
 		return err;
 
<span class="p_del">-	net_inc_ingress_queue();</span>
 	sch-&gt;flags |= TCQ_F_CPUSTATS;
 
 	return 0;
<span class="p_chunk">@@ -153,6 +154,9 @@</span> <span class="p_context"> static int clsact_init(struct Qdisc *sch, struct nlattr *opt)</span>
 	struct net_device *dev = qdisc_dev(sch);
 	int err;
 
<span class="p_add">+	net_inc_ingress_queue();</span>
<span class="p_add">+	net_inc_egress_queue();</span>
<span class="p_add">+</span>
 	err = tcf_block_get(&amp;q-&gt;ingress_block, &amp;dev-&gt;ingress_cl_list);
 	if (err)
 		return err;
<span class="p_chunk">@@ -161,9 +165,6 @@</span> <span class="p_context"> static int clsact_init(struct Qdisc *sch, struct nlattr *opt)</span>
 	if (err)
 		return err;
 
<span class="p_del">-	net_inc_ingress_queue();</span>
<span class="p_del">-	net_inc_egress_queue();</span>
<span class="p_del">-</span>
 	sch-&gt;flags |= TCQ_F_CPUSTATS;
 
 	return 0;
<span class="p_header">diff --git a/net/sctp/socket.c b/net/sctp/socket.c</span>
<span class="p_header">index d6163f7aefb1..df806b8819aa 100644</span>
<span class="p_header">--- a/net/sctp/socket.c</span>
<span class="p_header">+++ b/net/sctp/socket.c</span>
<span class="p_chunk">@@ -3874,13 +3874,17 @@</span> <span class="p_context"> static int sctp_setsockopt_reset_streams(struct sock *sk,</span>
 	struct sctp_association *asoc;
 	int retval = -EINVAL;
 
<span class="p_del">-	if (optlen &lt; sizeof(struct sctp_reset_streams))</span>
<span class="p_add">+	if (optlen &lt; sizeof(*params))</span>
 		return -EINVAL;
 
 	params = memdup_user(optval, optlen);
 	if (IS_ERR(params))
 		return PTR_ERR(params);
 
<span class="p_add">+	if (params-&gt;srs_number_streams * sizeof(__u16) &gt;</span>
<span class="p_add">+	    optlen - sizeof(*params))</span>
<span class="p_add">+		goto out;</span>
<span class="p_add">+</span>
 	asoc = sctp_id2assoc(sk, params-&gt;srs_assoc_id);
 	if (!asoc)
 		goto out;
<span class="p_chunk">@@ -4413,7 +4417,7 @@</span> <span class="p_context"> static int sctp_init_sock(struct sock *sk)</span>
 	SCTP_DBG_OBJCNT_INC(sock);
 
 	local_bh_disable();
<span class="p_del">-	percpu_counter_inc(&amp;sctp_sockets_allocated);</span>
<span class="p_add">+	sk_sockets_allocated_inc(sk);</span>
 	sock_prot_inuse_add(net, sk-&gt;sk_prot, 1);
 
 	/* Nothing can fail after this block, otherwise
<span class="p_chunk">@@ -4457,7 +4461,7 @@</span> <span class="p_context"> static void sctp_destroy_sock(struct sock *sk)</span>
 	}
 	sctp_endpoint_free(sp-&gt;ep);
 	local_bh_disable();
<span class="p_del">-	percpu_counter_dec(&amp;sctp_sockets_allocated);</span>
<span class="p_add">+	sk_sockets_allocated_dec(sk);</span>
 	sock_prot_inuse_add(sock_net(sk), sk-&gt;sk_prot, -1);
 	local_bh_enable();
 }
<span class="p_header">diff --git a/net/tipc/socket.c b/net/tipc/socket.c</span>
<span class="p_header">index d50edd6e0019..98a44ecb11e7 100644</span>
<span class="p_header">--- a/net/tipc/socket.c</span>
<span class="p_header">+++ b/net/tipc/socket.c</span>
<span class="p_chunk">@@ -709,11 +709,11 @@</span> <span class="p_context"> static unsigned int tipc_poll(struct file *file, struct socket *sock,</span>
 
 	switch (sk-&gt;sk_state) {
 	case TIPC_ESTABLISHED:
<span class="p_add">+	case TIPC_CONNECTING:</span>
 		if (!tsk-&gt;cong_link_cnt &amp;&amp; !tsk_conn_cong(tsk))
 			mask |= POLLOUT;
 		/* fall thru&#39; */
 	case TIPC_LISTEN:
<span class="p_del">-	case TIPC_CONNECTING:</span>
 		if (!skb_queue_empty(&amp;sk-&gt;sk_receive_queue))
 			mask |= (POLLIN | POLLRDNORM);
 		break;
<span class="p_header">diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="p_header">index e8e449444e65..6614b9312b45 100644</span>
<span class="p_header">--- a/security/Kconfig</span>
<span class="p_header">+++ b/security/Kconfig</span>
<span class="p_chunk">@@ -54,6 +54,17 @@</span> <span class="p_context"> config SECURITY_NETWORK</span>
 	  implement socket and networking access controls.
 	  If you are unsure how to answer this question, answer N.
 
<span class="p_add">+config PAGE_TABLE_ISOLATION</span>
<span class="p_add">+	bool &quot;Remove the kernel mapping in user mode&quot;</span>
<span class="p_add">+	depends on X86_64 &amp;&amp; !UML</span>
<span class="p_add">+	default y</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  This feature reduces the number of hardware side channels by</span>
<span class="p_add">+	  ensuring that the majority of kernel addresses are not mapped</span>
<span class="p_add">+	  into userspace.</span>
<span class="p_add">+</span>
<span class="p_add">+	  See Documentation/x86/pagetable-isolation.txt for more details.</span>
<span class="p_add">+</span>
 config SECURITY_INFINIBAND
 	bool &quot;Infiniband Security Hooks&quot;
 	depends on SECURITY &amp;&amp; INFINIBAND
<span class="p_header">diff --git a/sound/hda/hdac_i915.c b/sound/hda/hdac_i915.c</span>
<span class="p_header">index 038a180d3f81..cbe818eda336 100644</span>
<span class="p_header">--- a/sound/hda/hdac_i915.c</span>
<span class="p_header">+++ b/sound/hda/hdac_i915.c</span>
<span class="p_chunk">@@ -325,7 +325,7 @@</span> <span class="p_context"> static int hdac_component_master_match(struct device *dev, void *data)</span>
  */
 int snd_hdac_i915_register_notifier(const struct i915_audio_component_audio_ops *aops)
 {
<span class="p_del">-	if (WARN_ON(!hdac_acomp))</span>
<span class="p_add">+	if (!hdac_acomp)</span>
 		return -ENODEV;
 
 	hdac_acomp-&gt;audio_ops = aops;
<span class="p_header">diff --git a/sound/pci/hda/patch_conexant.c b/sound/pci/hda/patch_conexant.c</span>
<span class="p_header">index a81aacf684b2..37e1cf8218ff 100644</span>
<span class="p_header">--- a/sound/pci/hda/patch_conexant.c</span>
<span class="p_header">+++ b/sound/pci/hda/patch_conexant.c</span>
<span class="p_chunk">@@ -271,6 +271,8 @@</span> <span class="p_context"> enum {</span>
 	CXT_FIXUP_HP_SPECTRE,
 	CXT_FIXUP_HP_GATE_MIC,
 	CXT_FIXUP_MUTE_LED_GPIO,
<span class="p_add">+	CXT_FIXUP_HEADSET_MIC,</span>
<span class="p_add">+	CXT_FIXUP_HP_MIC_NO_PRESENCE,</span>
 };
 
 /* for hda_fixup_thinkpad_acpi() */
<span class="p_chunk">@@ -350,6 +352,18 @@</span> <span class="p_context"> static void cxt_fixup_headphone_mic(struct hda_codec *codec,</span>
 	}
 }
 
<span class="p_add">+static void cxt_fixup_headset_mic(struct hda_codec *codec,</span>
<span class="p_add">+				    const struct hda_fixup *fix, int action)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct conexant_spec *spec = codec-&gt;spec;</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (action) {</span>
<span class="p_add">+	case HDA_FIXUP_ACT_PRE_PROBE:</span>
<span class="p_add">+		spec-&gt;parse_flags |= HDA_PINCFG_HEADSET_MIC;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /* OPLC XO 1.5 fixup */
 
 /* OLPC XO-1.5 supports DC input mode (e.g. for use with analog sensors)
<span class="p_chunk">@@ -880,6 +894,19 @@</span> <span class="p_context"> static const struct hda_fixup cxt_fixups[] = {</span>
 		.type = HDA_FIXUP_FUNC,
 		.v.func = cxt_fixup_mute_led_gpio,
 	},
<span class="p_add">+	[CXT_FIXUP_HEADSET_MIC] = {</span>
<span class="p_add">+		.type = HDA_FIXUP_FUNC,</span>
<span class="p_add">+		.v.func = cxt_fixup_headset_mic,</span>
<span class="p_add">+	},</span>
<span class="p_add">+	[CXT_FIXUP_HP_MIC_NO_PRESENCE] = {</span>
<span class="p_add">+		.type = HDA_FIXUP_PINS,</span>
<span class="p_add">+		.v.pins = (const struct hda_pintbl[]) {</span>
<span class="p_add">+			{ 0x1a, 0x02a1113c },</span>
<span class="p_add">+			{ }</span>
<span class="p_add">+		},</span>
<span class="p_add">+		.chained = true,</span>
<span class="p_add">+		.chain_id = CXT_FIXUP_HEADSET_MIC,</span>
<span class="p_add">+	},</span>
 };
 
 static const struct snd_pci_quirk cxt5045_fixups[] = {
<span class="p_chunk">@@ -934,6 +961,8 @@</span> <span class="p_context"> static const struct snd_pci_quirk cxt5066_fixups[] = {</span>
 	SND_PCI_QUIRK(0x103c, 0x8115, &quot;HP Z1 Gen3&quot;, CXT_FIXUP_HP_GATE_MIC),
 	SND_PCI_QUIRK(0x103c, 0x814f, &quot;HP ZBook 15u G3&quot;, CXT_FIXUP_MUTE_LED_GPIO),
 	SND_PCI_QUIRK(0x103c, 0x822e, &quot;HP ProBook 440 G4&quot;, CXT_FIXUP_MUTE_LED_GPIO),
<span class="p_add">+	SND_PCI_QUIRK(0x103c, 0x8299, &quot;HP 800 G3 SFF&quot;, CXT_FIXUP_HP_MIC_NO_PRESENCE),</span>
<span class="p_add">+	SND_PCI_QUIRK(0x103c, 0x829a, &quot;HP 800 G3 DM&quot;, CXT_FIXUP_HP_MIC_NO_PRESENCE),</span>
 	SND_PCI_QUIRK(0x1043, 0x138d, &quot;Asus&quot;, CXT_FIXUP_HEADPHONE_MIC_PIN),
 	SND_PCI_QUIRK(0x152d, 0x0833, &quot;OLPC XO-1.5&quot;, CXT_FIXUP_OLPC_XO),
 	SND_PCI_QUIRK(0x17aa, 0x20f2, &quot;Lenovo T400&quot;, CXT_PINCFG_LENOVO_TP410),
<span class="p_header">diff --git a/sound/pci/hda/patch_realtek.c b/sound/pci/hda/patch_realtek.c</span>
<span class="p_header">index 9ac4b9076ee2..acdb196ddb44 100644</span>
<span class="p_header">--- a/sound/pci/hda/patch_realtek.c</span>
<span class="p_header">+++ b/sound/pci/hda/patch_realtek.c</span>
<span class="p_chunk">@@ -324,8 +324,12 @@</span> <span class="p_context"> static void alc_fill_eapd_coef(struct hda_codec *codec)</span>
 	case 0x10ec0292:
 		alc_update_coef_idx(codec, 0x4, 1&lt;&lt;15, 0);
 		break;
<span class="p_del">-	case 0x10ec0215:</span>
 	case 0x10ec0225:
<span class="p_add">+	case 0x10ec0295:</span>
<span class="p_add">+	case 0x10ec0299:</span>
<span class="p_add">+		alc_update_coef_idx(codec, 0x67, 0xf000, 0x3000);</span>
<span class="p_add">+		/* fallthrough */</span>
<span class="p_add">+	case 0x10ec0215:</span>
 	case 0x10ec0233:
 	case 0x10ec0236:
 	case 0x10ec0255:
<span class="p_chunk">@@ -336,10 +340,8 @@</span> <span class="p_context"> static void alc_fill_eapd_coef(struct hda_codec *codec)</span>
 	case 0x10ec0286:
 	case 0x10ec0288:
 	case 0x10ec0285:
<span class="p_del">-	case 0x10ec0295:</span>
 	case 0x10ec0298:
 	case 0x10ec0289:
<span class="p_del">-	case 0x10ec0299:</span>
 		alc_update_coef_idx(codec, 0x10, 1&lt;&lt;9, 0);
 		break;
 	case 0x10ec0275:
<span class="p_chunk">@@ -6305,6 +6307,7 @@</span> <span class="p_context"> static const struct snd_pci_quirk alc269_fixup_tbl[] = {</span>
 	SND_PCI_QUIRK(0x17aa, 0x30bb, &quot;ThinkCentre AIO&quot;, ALC233_FIXUP_LENOVO_LINE2_MIC_HOTKEY),
 	SND_PCI_QUIRK(0x17aa, 0x30e2, &quot;ThinkCentre AIO&quot;, ALC233_FIXUP_LENOVO_LINE2_MIC_HOTKEY),
 	SND_PCI_QUIRK(0x17aa, 0x310c, &quot;ThinkCentre Station&quot;, ALC294_FIXUP_LENOVO_MIC_LOCATION),
<span class="p_add">+	SND_PCI_QUIRK(0x17aa, 0x313c, &quot;ThinkCentre Station&quot;, ALC294_FIXUP_LENOVO_MIC_LOCATION),</span>
 	SND_PCI_QUIRK(0x17aa, 0x3112, &quot;ThinkCentre AIO&quot;, ALC233_FIXUP_LENOVO_LINE2_MIC_HOTKEY),
 	SND_PCI_QUIRK(0x17aa, 0x3902, &quot;Lenovo E50-80&quot;, ALC269_FIXUP_DMIC_THINKPAD_ACPI),
 	SND_PCI_QUIRK(0x17aa, 0x3977, &quot;IdeaPad S210&quot;, ALC283_FIXUP_INT_MIC),
<span class="p_chunk">@@ -6557,6 +6560,11 @@</span> <span class="p_context"> static const struct snd_hda_pin_quirk alc269_pin_fixup_tbl[] = {</span>
 	SND_HDA_PIN_QUIRK(0x10ec0255, 0x1028, &quot;Dell&quot;, ALC255_FIXUP_DELL1_MIC_NO_PRESENCE,
 		{0x1b, 0x01011020},
 		{0x21, 0x02211010}),
<span class="p_add">+	SND_HDA_PIN_QUIRK(0x10ec0256, 0x1028, &quot;Dell&quot;, ALC255_FIXUP_DELL1_MIC_NO_PRESENCE,</span>
<span class="p_add">+		{0x12, 0x90a60130},</span>
<span class="p_add">+		{0x14, 0x90170110},</span>
<span class="p_add">+		{0x1b, 0x01011020},</span>
<span class="p_add">+		{0x21, 0x0221101f}),</span>
 	SND_HDA_PIN_QUIRK(0x10ec0256, 0x1028, &quot;Dell&quot;, ALC255_FIXUP_DELL1_MIC_NO_PRESENCE,
 		{0x12, 0x90a60160},
 		{0x14, 0x90170120},
<span class="p_header">diff --git a/sound/soc/codecs/da7218.c b/sound/soc/codecs/da7218.c</span>
<span class="p_header">index b2d42ec1dcd9..56564ce90cb6 100644</span>
<span class="p_header">--- a/sound/soc/codecs/da7218.c</span>
<span class="p_header">+++ b/sound/soc/codecs/da7218.c</span>
<span class="p_chunk">@@ -2520,7 +2520,7 @@</span> <span class="p_context"> static struct da7218_pdata *da7218_of_to_pdata(struct snd_soc_codec *codec)</span>
 	}
 
 	if (da7218-&gt;dev_id == DA7218_DEV_ID) {
<span class="p_del">-		hpldet_np = of_find_node_by_name(np, &quot;da7218_hpldet&quot;);</span>
<span class="p_add">+		hpldet_np = of_get_child_by_name(np, &quot;da7218_hpldet&quot;);</span>
 		if (!hpldet_np)
 			return pdata;
 
<span class="p_header">diff --git a/sound/soc/codecs/msm8916-wcd-analog.c b/sound/soc/codecs/msm8916-wcd-analog.c</span>
<span class="p_header">index 18933bf6473f..8c7063e1aa46 100644</span>
<span class="p_header">--- a/sound/soc/codecs/msm8916-wcd-analog.c</span>
<span class="p_header">+++ b/sound/soc/codecs/msm8916-wcd-analog.c</span>
<span class="p_chunk">@@ -267,7 +267,7 @@</span> <span class="p_context"></span>
 #define MSM8916_WCD_ANALOG_RATES (SNDRV_PCM_RATE_8000 | SNDRV_PCM_RATE_16000 |\
 			SNDRV_PCM_RATE_32000 | SNDRV_PCM_RATE_48000)
 #define MSM8916_WCD_ANALOG_FORMATS (SNDRV_PCM_FMTBIT_S16_LE |\
<span class="p_del">-				    SNDRV_PCM_FMTBIT_S24_LE)</span>
<span class="p_add">+				    SNDRV_PCM_FMTBIT_S32_LE)</span>
 
 static int btn_mask = SND_JACK_BTN_0 | SND_JACK_BTN_1 |
 	       SND_JACK_BTN_2 | SND_JACK_BTN_3 | SND_JACK_BTN_4;
<span class="p_header">diff --git a/sound/soc/codecs/msm8916-wcd-digital.c b/sound/soc/codecs/msm8916-wcd-digital.c</span>
<span class="p_header">index 66df8f810f0d..694db27b11fa 100644</span>
<span class="p_header">--- a/sound/soc/codecs/msm8916-wcd-digital.c</span>
<span class="p_header">+++ b/sound/soc/codecs/msm8916-wcd-digital.c</span>
<span class="p_chunk">@@ -194,7 +194,7 @@</span> <span class="p_context"></span>
 				   SNDRV_PCM_RATE_32000 | \
 				   SNDRV_PCM_RATE_48000)
 #define MSM8916_WCD_DIGITAL_FORMATS (SNDRV_PCM_FMTBIT_S16_LE |\
<span class="p_del">-				     SNDRV_PCM_FMTBIT_S24_LE)</span>
<span class="p_add">+				     SNDRV_PCM_FMTBIT_S32_LE)</span>
 
 struct msm8916_wcd_digital_priv {
 	struct clk *ahbclk, *mclk;
<span class="p_chunk">@@ -645,7 +645,7 @@</span> <span class="p_context"> static int msm8916_wcd_digital_hw_params(struct snd_pcm_substream *substream,</span>
 				    RX_I2S_CTL_RX_I2S_MODE_MASK,
 				    RX_I2S_CTL_RX_I2S_MODE_16);
 		break;
<span class="p_del">-	case SNDRV_PCM_FORMAT_S24_LE:</span>
<span class="p_add">+	case SNDRV_PCM_FORMAT_S32_LE:</span>
 		snd_soc_update_bits(dai-&gt;codec, LPASS_CDC_CLK_TX_I2S_CTL,
 				    TX_I2S_CTL_TX_I2S_MODE_MASK,
 				    TX_I2S_CTL_TX_I2S_MODE_32);
<span class="p_header">diff --git a/sound/soc/codecs/tlv320aic31xx.h b/sound/soc/codecs/tlv320aic31xx.h</span>
<span class="p_header">index 730fb2058869..1ff3edb7bbb6 100644</span>
<span class="p_header">--- a/sound/soc/codecs/tlv320aic31xx.h</span>
<span class="p_header">+++ b/sound/soc/codecs/tlv320aic31xx.h</span>
<span class="p_chunk">@@ -116,7 +116,7 @@</span> <span class="p_context"> struct aic31xx_pdata {</span>
 /* INT2 interrupt control */
 #define AIC31XX_INT2CTRL	AIC31XX_REG(0, 49)
 /* GPIO1 control */
<span class="p_del">-#define AIC31XX_GPIO1		AIC31XX_REG(0, 50)</span>
<span class="p_add">+#define AIC31XX_GPIO1		AIC31XX_REG(0, 51)</span>
 
 #define AIC31XX_DACPRB		AIC31XX_REG(0, 60)
 /* ADC Instruction Set Register */
<span class="p_header">diff --git a/sound/soc/codecs/twl4030.c b/sound/soc/codecs/twl4030.c</span>
<span class="p_header">index c482b2e7a7d2..cfe72b9d4356 100644</span>
<span class="p_header">--- a/sound/soc/codecs/twl4030.c</span>
<span class="p_header">+++ b/sound/soc/codecs/twl4030.c</span>
<span class="p_chunk">@@ -232,7 +232,7 @@</span> <span class="p_context"> static struct twl4030_codec_data *twl4030_get_pdata(struct snd_soc_codec *codec)</span>
 	struct twl4030_codec_data *pdata = dev_get_platdata(codec-&gt;dev);
 	struct device_node *twl4030_codec_node = NULL;
 
<span class="p_del">-	twl4030_codec_node = of_find_node_by_name(codec-&gt;dev-&gt;parent-&gt;of_node,</span>
<span class="p_add">+	twl4030_codec_node = of_get_child_by_name(codec-&gt;dev-&gt;parent-&gt;of_node,</span>
 						  &quot;codec&quot;);
 
 	if (!pdata &amp;&amp; twl4030_codec_node) {
<span class="p_chunk">@@ -241,9 +241,11 @@</span> <span class="p_context"> static struct twl4030_codec_data *twl4030_get_pdata(struct snd_soc_codec *codec)</span>
 				     GFP_KERNEL);
 		if (!pdata) {
 			dev_err(codec-&gt;dev, &quot;Can not allocate memory\n&quot;);
<span class="p_add">+			of_node_put(twl4030_codec_node);</span>
 			return NULL;
 		}
 		twl4030_setup_pdata_of(pdata, twl4030_codec_node);
<span class="p_add">+		of_node_put(twl4030_codec_node);</span>
 	}
 
 	return pdata;
<span class="p_header">diff --git a/sound/soc/codecs/wm_adsp.c b/sound/soc/codecs/wm_adsp.c</span>
<span class="p_header">index 65c059b5ffd7..66e32f5d2917 100644</span>
<span class="p_header">--- a/sound/soc/codecs/wm_adsp.c</span>
<span class="p_header">+++ b/sound/soc/codecs/wm_adsp.c</span>
<span class="p_chunk">@@ -1733,7 +1733,7 @@</span> <span class="p_context"> static int wm_adsp_load(struct wm_adsp *dsp)</span>
 		 le64_to_cpu(footer-&gt;timestamp));
 
 	while (pos &lt; firmware-&gt;size &amp;&amp;
<span class="p_del">-	       pos - firmware-&gt;size &gt; sizeof(*region)) {</span>
<span class="p_add">+	       sizeof(*region) &lt; firmware-&gt;size - pos) {</span>
 		region = (void *)&amp;(firmware-&gt;data[pos]);
 		region_name = &quot;Unknown&quot;;
 		reg = 0;
<span class="p_chunk">@@ -1782,8 +1782,8 @@</span> <span class="p_context"> static int wm_adsp_load(struct wm_adsp *dsp)</span>
 			 regions, le32_to_cpu(region-&gt;len), offset,
 			 region_name);
 
<span class="p_del">-		if ((pos + le32_to_cpu(region-&gt;len) + sizeof(*region)) &gt;</span>
<span class="p_del">-		    firmware-&gt;size) {</span>
<span class="p_add">+		if (le32_to_cpu(region-&gt;len) &gt;</span>
<span class="p_add">+		    firmware-&gt;size - pos - sizeof(*region)) {</span>
 			adsp_err(dsp,
 				 &quot;%s.%d: %s region len %d bytes exceeds file length %zu\n&quot;,
 				 file, regions, region_name,
<span class="p_chunk">@@ -2253,7 +2253,7 @@</span> <span class="p_context"> static int wm_adsp_load_coeff(struct wm_adsp *dsp)</span>
 
 	blocks = 0;
 	while (pos &lt; firmware-&gt;size &amp;&amp;
<span class="p_del">-	       pos - firmware-&gt;size &gt; sizeof(*blk)) {</span>
<span class="p_add">+	       sizeof(*blk) &lt; firmware-&gt;size - pos) {</span>
 		blk = (void *)(&amp;firmware-&gt;data[pos]);
 
 		type = le16_to_cpu(blk-&gt;type);
<span class="p_chunk">@@ -2327,8 +2327,8 @@</span> <span class="p_context"> static int wm_adsp_load_coeff(struct wm_adsp *dsp)</span>
 		}
 
 		if (reg) {
<span class="p_del">-			if ((pos + le32_to_cpu(blk-&gt;len) + sizeof(*blk)) &gt;</span>
<span class="p_del">-			    firmware-&gt;size) {</span>
<span class="p_add">+			if (le32_to_cpu(blk-&gt;len) &gt;</span>
<span class="p_add">+			    firmware-&gt;size - pos - sizeof(*blk)) {</span>
 				adsp_err(dsp,
 					 &quot;%s.%d: %s region len %d bytes exceeds file length %zu\n&quot;,
 					 file, blocks, region_name,
<span class="p_header">diff --git a/sound/soc/fsl/fsl_ssi.c b/sound/soc/fsl/fsl_ssi.c</span>
<span class="p_header">index 64598d1183f8..3ffbb498cc70 100644</span>
<span class="p_header">--- a/sound/soc/fsl/fsl_ssi.c</span>
<span class="p_header">+++ b/sound/soc/fsl/fsl_ssi.c</span>
<span class="p_chunk">@@ -1452,12 +1452,6 @@</span> <span class="p_context"> static int fsl_ssi_probe(struct platform_device *pdev)</span>
 				sizeof(fsl_ssi_ac97_dai));
 
 		fsl_ac97_data = ssi_private;
<span class="p_del">-</span>
<span class="p_del">-		ret = snd_soc_set_ac97_ops_of_reset(&amp;fsl_ssi_ac97_ops, pdev);</span>
<span class="p_del">-		if (ret) {</span>
<span class="p_del">-			dev_err(&amp;pdev-&gt;dev, &quot;could not set AC&#39;97 ops\n&quot;);</span>
<span class="p_del">-			return ret;</span>
<span class="p_del">-		}</span>
 	} else {
 		/* Initialize this copy of the CPU DAI driver structure */
 		memcpy(&amp;ssi_private-&gt;cpu_dai_drv, &amp;fsl_ssi_dai_template,
<span class="p_chunk">@@ -1568,6 +1562,14 @@</span> <span class="p_context"> static int fsl_ssi_probe(struct platform_device *pdev)</span>
 			return ret;
 	}
 
<span class="p_add">+	if (fsl_ssi_is_ac97(ssi_private)) {</span>
<span class="p_add">+		ret = snd_soc_set_ac97_ops_of_reset(&amp;fsl_ssi_ac97_ops, pdev);</span>
<span class="p_add">+		if (ret) {</span>
<span class="p_add">+			dev_err(&amp;pdev-&gt;dev, &quot;could not set AC&#39;97 ops\n&quot;);</span>
<span class="p_add">+			goto error_ac97_ops;</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	ret = devm_snd_soc_register_component(&amp;pdev-&gt;dev, &amp;fsl_ssi_component,
 					      &amp;ssi_private-&gt;cpu_dai_drv, 1);
 	if (ret) {
<span class="p_chunk">@@ -1651,6 +1653,10 @@</span> <span class="p_context"> static int fsl_ssi_probe(struct platform_device *pdev)</span>
 	fsl_ssi_debugfs_remove(&amp;ssi_private-&gt;dbg_stats);
 
 error_asoc_register:
<span class="p_add">+	if (fsl_ssi_is_ac97(ssi_private))</span>
<span class="p_add">+		snd_soc_set_ac97_ops(NULL);</span>
<span class="p_add">+</span>
<span class="p_add">+error_ac97_ops:</span>
 	if (ssi_private-&gt;soc-&gt;imx)
 		fsl_ssi_imx_clean(pdev, ssi_private);
 
<span class="p_header">diff --git a/tools/testing/selftests/x86/ldt_gdt.c b/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_header">index 0304ffb714f2..1aef72df20a1 100644</span>
<span class="p_header">--- a/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_header">+++ b/tools/testing/selftests/x86/ldt_gdt.c</span>
<span class="p_chunk">@@ -122,8 +122,7 @@</span> <span class="p_context"> static void check_valid_segment(uint16_t index, int ldt,</span>
 	 * NB: Different Linux versions do different things with the
 	 * accessed bit in set_thread_area().
 	 */
<span class="p_del">-	if (ar != expected_ar &amp;&amp;</span>
<span class="p_del">-	    (ldt || ar != (expected_ar | AR_ACCESSED))) {</span>
<span class="p_add">+	if (ar != expected_ar &amp;&amp; ar != (expected_ar | AR_ACCESSED)) {</span>
 		printf(&quot;[FAIL]\t%s entry %hu has AR 0x%08X but expected 0x%08X\n&quot;,
 		       (ldt ? &quot;LDT&quot; : &quot;GDT&quot;), index, ar, expected_ar);
 		nerrs++;
<span class="p_header">diff --git a/tools/usb/usbip/src/utils.c b/tools/usb/usbip/src/utils.c</span>
<span class="p_header">index 2b3d6d235015..3d7b42e77299 100644</span>
<span class="p_header">--- a/tools/usb/usbip/src/utils.c</span>
<span class="p_header">+++ b/tools/usb/usbip/src/utils.c</span>
<span class="p_chunk">@@ -30,6 +30,7 @@</span> <span class="p_context"> int modify_match_busid(char *busid, int add)</span>
 	char command[SYSFS_BUS_ID_SIZE + 4];
 	char match_busid_attr_path[SYSFS_PATH_MAX];
 	int rc;
<span class="p_add">+	int cmd_size;</span>
 
 	snprintf(match_busid_attr_path, sizeof(match_busid_attr_path),
 		 &quot;%s/%s/%s/%s/%s/%s&quot;, SYSFS_MNT_PATH, SYSFS_BUS_NAME,
<span class="p_chunk">@@ -37,12 +38,14 @@</span> <span class="p_context"> int modify_match_busid(char *busid, int add)</span>
 		 attr_name);
 
 	if (add)
<span class="p_del">-		snprintf(command, SYSFS_BUS_ID_SIZE + 4, &quot;add %s&quot;, busid);</span>
<span class="p_add">+		cmd_size = snprintf(command, SYSFS_BUS_ID_SIZE + 4, &quot;add %s&quot;,</span>
<span class="p_add">+				    busid);</span>
 	else
<span class="p_del">-		snprintf(command, SYSFS_BUS_ID_SIZE + 4, &quot;del %s&quot;, busid);</span>
<span class="p_add">+		cmd_size = snprintf(command, SYSFS_BUS_ID_SIZE + 4, &quot;del %s&quot;,</span>
<span class="p_add">+				    busid);</span>
 
 	rc = write_sysfs_attribute(match_busid_attr_path, command,
<span class="p_del">-				   sizeof(command));</span>
<span class="p_add">+				   cmd_size);</span>
 	if (rc &lt; 0) {
 		dbg(&quot;failed to write match_busid: %s&quot;, strerror(errno));
 		return -1;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



