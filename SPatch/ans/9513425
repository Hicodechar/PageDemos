
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[HMM,v16,04/15] mm/ZONE_DEVICE/unaddressable: add support for un-addressable device memory v2 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [HMM,v16,04/15] mm/ZONE_DEVICE/unaddressable: add support for un-addressable device memory v2</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 12, 2017, 4:30 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1484238642-10674-5-git-send-email-jglisse@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9513425/mbox/"
   >mbox</a>
|
   <a href="/patch/9513425/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9513425/">/patch/9513425/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	8D21D60710 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Jan 2017 15:29:58 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7E061286DF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Jan 2017 15:29:58 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 72EE2286E1; Thu, 12 Jan 2017 15:29:58 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 05F49286E2
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu, 12 Jan 2017 15:29:57 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751298AbdALP3x (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 12 Jan 2017 10:29:53 -0500
Received: from mx1.redhat.com ([209.132.183.28]:45932 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1751267AbdALP3u (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 12 Jan 2017 10:29:50 -0500
Received: from int-mx10.intmail.prod.int.phx2.redhat.com
	(int-mx10.intmail.prod.int.phx2.redhat.com [10.5.11.23])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id D6502C05005F;
	Thu, 12 Jan 2017 15:29:35 +0000 (UTC)
Received: from xgl-cortex.ml2.eng.bos.redhat.com
	(xgl-cortex.ml2.eng.bos.redhat.com [10.19.160.80])
	by int-mx10.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id v0CFTStR025430; Thu, 12 Jan 2017 10:29:35 -0500
From: =?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;
To: akpm@linux-foundation.org, &lt;linux-kernel@vger.kernel.org&gt;,
	linux-mm@kvack.org
Cc: John Hubbard &lt;jhubbard@nvidia.com&gt;,
	=?UTF-8?q?J=C3=A9r=C3=B4me=20Glisse?= &lt;jglisse@redhat.com&gt;,
	Dan Williams &lt;dan.j.williams@intel.com&gt;,
	Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;
Subject: [HMM v16 04/15] mm/ZONE_DEVICE/unaddressable: add support for
	un-addressable device memory v2
Date: Thu, 12 Jan 2017 11:30:31 -0500
Message-Id: &lt;1484238642-10674-5-git-send-email-jglisse@redhat.com&gt;
In-Reply-To: &lt;1484238642-10674-1-git-send-email-jglisse@redhat.com&gt;
References: &lt;1484238642-10674-1-git-send-email-jglisse@redhat.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 8bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.23
X-Greylist: Sender IP whitelisted, not delayed by milter-greylist-4.5.16
	(mx1.redhat.com [10.5.110.31]);
	Thu, 12 Jan 2017 15:29:36 +0000 (UTC)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Jan. 12, 2017, 4:30 p.m.</div>
<pre class="content">
This add support for un-addressable device memory. Such memory is hotpluged
only so we can have struct page but we should never map them as such memory
can not be accessed by CPU. For that reason it uses a special swap entry for
CPU page table entry.

This patch implement all the logic from special swap type to handling CPU
page fault through a callback specified in the ZONE_DEVICE pgmap struct.

Architecture that wish to support un-addressable device memory should make
sure to never populate the kernel linar mapping for the physical range.

This feature potentially breaks memory hotplug unless every driver using it
magically predicts the future addresses of where memory will be hotplugged.

Changed since v1:
  - Add unaddressable memory resource descriptor enum
  - Explain why memory hotplug can fail because of un-addressable memory
<span class="signed-off-by">
Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
Cc: Dan Williams &lt;dan.j.williams@intel.com&gt;
Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;
---
 drivers/dax/pmem.c                |  4 +--
 drivers/nvdimm/pmem.c             |  6 ++--
 fs/proc/task_mmu.c                | 10 +++++-
 include/linux/ioport.h            |  1 +
 include/linux/memory_hotplug.h    |  7 ++++
 include/linux/memremap.h          | 29 +++++++++++++++--
 include/linux/swap.h              | 18 +++++++++--
 include/linux/swapops.h           | 67 +++++++++++++++++++++++++++++++++++++++
 kernel/memremap.c                 | 43 +++++++++++++++++++++++--
 mm/Kconfig                        | 12 +++++++
 mm/memory.c                       | 64 ++++++++++++++++++++++++++++++++++++-
 mm/memory_hotplug.c               | 10 ++++--
 mm/mprotect.c                     | 12 +++++++
 tools/testing/nvdimm/test/iomap.c |  3 +-
 14 files changed, 269 insertions(+), 17 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=320">Dan Williams</a> - Jan. 16, 2017, 7:05 a.m.</div>
<pre class="content">
On Thu, Jan 12, 2017 at 8:30 AM, Jérôme Glisse &lt;jglisse@redhat.com&gt; wrote:
<span class="quote">&gt; This add support for un-addressable device memory. Such memory is hotpluged</span>
<span class="quote">&gt; only so we can have struct page but we should never map them as such memory</span>
<span class="quote">&gt; can not be accessed by CPU. For that reason it uses a special swap entry for</span>
<span class="quote">&gt; CPU page table entry.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch implement all the logic from special swap type to handling CPU</span>
<span class="quote">&gt; page fault through a callback specified in the ZONE_DEVICE pgmap struct.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Architecture that wish to support un-addressable device memory should make</span>
<span class="quote">&gt; sure to never populate the kernel linar mapping for the physical range.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This feature potentially breaks memory hotplug unless every driver using it</span>
<span class="quote">&gt; magically predicts the future addresses of where memory will be hotplugged.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Changed since v1:</span>
<span class="quote">&gt;   - Add unaddressable memory resource descriptor enum</span>
<span class="quote">&gt;   - Explain why memory hotplug can fail because of un-addressable memory</span>

How can we merge this with a known potential regression?
<span class="quote">
&gt;</span>
<span class="quote">&gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
<span class="quote">&gt; Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  drivers/dax/pmem.c                |  4 +--</span>
<span class="quote">&gt;  drivers/nvdimm/pmem.c             |  6 ++--</span>
<span class="quote">&gt;  fs/proc/task_mmu.c                | 10 +++++-</span>
<span class="quote">&gt;  include/linux/ioport.h            |  1 +</span>
<span class="quote">&gt;  include/linux/memory_hotplug.h    |  7 ++++</span>
<span class="quote">&gt;  include/linux/memremap.h          | 29 +++++++++++++++--</span>
<span class="quote">&gt;  include/linux/swap.h              | 18 +++++++++--</span>
<span class="quote">&gt;  include/linux/swapops.h           | 67 +++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  kernel/memremap.c                 | 43 +++++++++++++++++++++++--</span>
<span class="quote">&gt;  mm/Kconfig                        | 12 +++++++</span>
<span class="quote">&gt;  mm/memory.c                       | 64 ++++++++++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;  mm/memory_hotplug.c               | 10 ++++--</span>
<span class="quote">&gt;  mm/mprotect.c                     | 12 +++++++</span>
<span class="quote">&gt;  tools/testing/nvdimm/test/iomap.c |  3 +-</span>
<span class="quote">&gt;  14 files changed, 269 insertions(+), 17 deletions(-)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/drivers/dax/pmem.c b/drivers/dax/pmem.c</span>
<span class="quote">&gt; index 66af7b1..c50b58d 100644</span>
<span class="quote">&gt; --- a/drivers/dax/pmem.c</span>
<span class="quote">&gt; +++ b/drivers/dax/pmem.c</span>
<span class="quote">&gt; @@ -111,8 +111,8 @@ static int dax_pmem_probe(struct device *dev)</span>
<span class="quote">&gt;         if (rc)</span>
<span class="quote">&gt;                 return rc;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; -       addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref,</span>
<span class="quote">&gt; -                                  altmap, NULL, NULL);</span>
<span class="quote">&gt; +       addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref, altmap,</span>
<span class="quote">&gt; +                                  NULL, NULL, NULL, NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt;         if (IS_ERR(addr))</span>
<span class="quote">&gt;                 return PTR_ERR(addr);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; diff --git a/drivers/nvdimm/pmem.c b/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt; index f2f1904..8166a56 100644</span>
<span class="quote">&gt; --- a/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt; +++ b/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt; @@ -282,7 +282,8 @@ static int pmem_attach_disk(struct device *dev,</span>
<span class="quote">&gt;         pmem-&gt;pfn_flags = PFN_DEV;</span>
<span class="quote">&gt;         if (is_nd_pfn(dev)) {</span>
<span class="quote">&gt;                 addr = devm_memremap_pages(dev, &amp;pfn_res, &amp;q-&gt;q_usage_counter,</span>
<span class="quote">&gt; -                                          altmap, NULL, NULL);</span>
<span class="quote">&gt; +                                          altmap, NULL, NULL, NULL,</span>
<span class="quote">&gt; +                                          NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt;                 pfn_sb = nd_pfn-&gt;pfn_sb;</span>
<span class="quote">&gt;                 pmem-&gt;data_offset = le64_to_cpu(pfn_sb-&gt;dataoff);</span>
<span class="quote">&gt;                 pmem-&gt;pfn_pad = resource_size(res) - resource_size(&amp;pfn_res);</span>
<span class="quote">&gt; @@ -292,7 +293,8 @@ static int pmem_attach_disk(struct device *dev,</span>
<span class="quote">&gt;         } else if (pmem_should_map_pages(dev)) {</span>
<span class="quote">&gt;                 addr = devm_memremap_pages(dev, &amp;nsio-&gt;res,</span>
<span class="quote">&gt;                                            &amp;q-&gt;q_usage_counter,</span>
<span class="quote">&gt; -                                          NULL, NULL, NULL);</span>
<span class="quote">&gt; +                                          NULL, NULL, NULL, NULL,</span>
<span class="quote">&gt; +                                          NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt;                 pmem-&gt;pfn_flags |= PFN_MAP;</span>
<span class="quote">&gt;         } else</span>
<span class="quote">&gt;                 addr = devm_memremap(dev, pmem-&gt;phys_addr,</span>
<span class="quote">&gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; index 958f325..9a6ab71 100644</span>
<span class="quote">&gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; @@ -535,8 +535,11 @@ static void smaps_pte_entry(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt;                         } else {</span>
<span class="quote">&gt;                                 mss-&gt;swap_pss += (u64)PAGE_SIZE &lt;&lt; PSS_SHIFT;</span>
<span class="quote">&gt;                         }</span>
<span class="quote">&gt; -               } else if (is_migration_entry(swpent))</span>
<span class="quote">&gt; +               } else if (is_migration_entry(swpent)) {</span>
<span class="quote">&gt;                         page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt; +               } else if (is_device_entry(swpent)) {</span>
<span class="quote">&gt; +                       page = device_entry_to_page(swpent);</span>
<span class="quote">&gt; +               }</span>
<span class="quote">&gt;         } else if (unlikely(IS_ENABLED(CONFIG_SHMEM) &amp;&amp; mss-&gt;check_shmem_swap</span>
<span class="quote">&gt;                                                         &amp;&amp; pte_none(*pte))) {</span>
<span class="quote">&gt;                 page = find_get_entry(vma-&gt;vm_file-&gt;f_mapping,</span>
<span class="quote">&gt; @@ -699,6 +702,8 @@ static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;                 if (is_migration_entry(swpent))</span>
<span class="quote">&gt;                         page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt; +               if (is_device_entry(swpent))</span>
<span class="quote">&gt; +                       page = device_entry_to_page(swpent);</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;         if (page) {</span>
<span class="quote">&gt;                 int mapcount = page_mapcount(page);</span>
<span class="quote">&gt; @@ -1182,6 +1187,9 @@ static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt;                 flags |= PM_SWAP;</span>
<span class="quote">&gt;                 if (is_migration_entry(entry))</span>
<span class="quote">&gt;                         page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +               if (is_device_entry(entry))</span>
<span class="quote">&gt; +                       page = device_entry_to_page(entry);</span>
<span class="quote">&gt;         }</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;         if (page &amp;&amp; !PageAnon(page))</span>
<span class="quote">&gt; diff --git a/include/linux/ioport.h b/include/linux/ioport.h</span>
<span class="quote">&gt; index 6230064..d154a18 100644</span>
<span class="quote">&gt; --- a/include/linux/ioport.h</span>
<span class="quote">&gt; +++ b/include/linux/ioport.h</span>
<span class="quote">&gt; @@ -130,6 +130,7 @@ enum {</span>
<span class="quote">&gt;         IORES_DESC_ACPI_NV_STORAGE              = 3,</span>
<span class="quote">&gt;         IORES_DESC_PERSISTENT_MEMORY            = 4,</span>
<span class="quote">&gt;         IORES_DESC_PERSISTENT_MEMORY_LEGACY     = 5,</span>
<span class="quote">&gt; +       IORES_DESC_UNADDRESSABLE_MEMORY         = 6,</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /* helpers to define resources */</span>
<span class="quote">&gt; diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h</span>
<span class="quote">&gt; index 3f50eb8..e7c5dc6 100644</span>
<span class="quote">&gt; --- a/include/linux/memory_hotplug.h</span>
<span class="quote">&gt; +++ b/include/linux/memory_hotplug.h</span>
<span class="quote">&gt; @@ -285,15 +285,22 @@ extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,</span>
<span class="quote">&gt;   * never relied on struct page migration so far and new user of might also</span>
<span class="quote">&gt;   * prefer avoiding struct page migration.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt; + * For device memory (which use ZONE_DEVICE) we want differentiate between CPU</span>
<span class="quote">&gt; + * accessible memory (persitent memory, device memory on an architecture with a</span>
<span class="quote">&gt; + * system bus that allow transparent access to device memory) and unaddressable</span>
<span class="quote">&gt; + * memory (device memory that can not be accessed by CPU directly).</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt;   * New non device memory specific flags can be added if ever needed.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * MEMORY_REGULAR: regular system memory</span>
<span class="quote">&gt;   * DEVICE_MEMORY: device memory create a ZONE_DEVICE zone for it</span>
<span class="quote">&gt;   * DEVICE_MEMORY_ALLOW_MIGRATE: page in that device memory ca be migrated</span>
<span class="quote">&gt; + * MEMORY_DEVICE_UNADDRESSABLE: un-addressable memory (CPU can not access it)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  #define MEMORY_NORMAL 0</span>
<span class="quote">&gt;  #define MEMORY_DEVICE (1 &lt;&lt; 0)</span>
<span class="quote">&gt;  #define MEMORY_DEVICE_ALLOW_MIGRATE (1 &lt;&lt; 1)</span>
<span class="quote">&gt; +#define MEMORY_DEVICE_UNADDRESSABLE (1 &lt;&lt; 2)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  extern int arch_add_memory(int nid, u64 start, u64 size, int flags);</span>
<span class="quote">&gt;  extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);</span>
<span class="quote">&gt; diff --git a/include/linux/memremap.h b/include/linux/memremap.h</span>
<span class="quote">&gt; index 582561f..4b9f02c 100644</span>
<span class="quote">&gt; --- a/include/linux/memremap.h</span>
<span class="quote">&gt; +++ b/include/linux/memremap.h</span>
<span class="quote">&gt; @@ -35,31 +35,42 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; +typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; +                               unsigned long addr,</span>
<span class="quote">&gt; +                               struct page *page,</span>
<span class="quote">&gt; +                               unsigned flags,</span>
<span class="quote">&gt; +                               pmd_t *pmdp);</span>
<span class="quote">&gt;  typedef void (*dev_page_free_t)(struct page *page, void *data);</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  /**</span>
<span class="quote">&gt;   * struct dev_pagemap - metadata for ZONE_DEVICE mappings</span>
<span class="quote">&gt; + * @page_fault: callback when CPU fault on an un-addressable device page</span>
<span class="quote">&gt;   * @page_free: free page callback when page refcount reach 1</span>
<span class="quote">&gt;   * @altmap: pre-allocated/reserved memory for vmemmap allocations</span>
<span class="quote">&gt;   * @res: physical address range covered by @ref</span>
<span class="quote">&gt;   * @ref: reference count that pins the devm_memremap_pages() mapping</span>
<span class="quote">&gt;   * @dev: host device of the mapping for debug</span>
<span class="quote">&gt;   * @data: privata data pointer for page_free</span>
<span class="quote">&gt; + * @flags: device memory flags (look for MEMORY_DEVICE_* memory_hotplug.h)</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  struct dev_pagemap {</span>
<span class="quote">&gt; +       dev_page_fault_t page_fault;</span>
<span class="quote">&gt;         dev_page_free_t page_free;</span>
<span class="quote">&gt;         struct vmem_altmap *altmap;</span>
<span class="quote">&gt;         const struct resource *res;</span>
<span class="quote">&gt;         struct percpu_ref *ref;</span>
<span class="quote">&gt;         struct device *dev;</span>
<span class="quote">&gt;         void *data;</span>
<span class="quote">&gt; +       int flags;</span>
<span class="quote">&gt;  };</span>

dev_pagemap is only meant for get_user_pages() to do lookups of ptes
with _PAGE_DEVMAP and take a reference against the hosting device..

Why can&#39;t HMM use the typical vm_operations_struct fault path and push
more of these details to a driver rather than the core?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Jan. 16, 2017, 3:17 p.m.</div>
<pre class="content">
On Sun, Jan 15, 2017 at 11:05:43PM -0800, Dan Williams wrote:
<span class="quote">&gt; On Thu, Jan 12, 2017 at 8:30 AM, Jérôme Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt; &gt; This add support for un-addressable device memory. Such memory is hotpluged</span>
<span class="quote">&gt; &gt; only so we can have struct page but we should never map them as such memory</span>
<span class="quote">&gt; &gt; can not be accessed by CPU. For that reason it uses a special swap entry for</span>
<span class="quote">&gt; &gt; CPU page table entry.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This patch implement all the logic from special swap type to handling CPU</span>
<span class="quote">&gt; &gt; page fault through a callback specified in the ZONE_DEVICE pgmap struct.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Architecture that wish to support un-addressable device memory should make</span>
<span class="quote">&gt; &gt; sure to never populate the kernel linar mapping for the physical range.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This feature potentially breaks memory hotplug unless every driver using it</span>
<span class="quote">&gt; &gt; magically predicts the future addresses of where memory will be hotplugged.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Changed since v1:</span>
<span class="quote">&gt; &gt;   - Add unaddressable memory resource descriptor enum</span>
<span class="quote">&gt; &gt;   - Explain why memory hotplug can fail because of un-addressable memory</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How can we merge this with a known potential regression?</span>

This only affect people using HMM it does not affect people that do not
use it. Like i said for time being people that want HMM are ok with that.
I haven&#39;t look yet on how to allow struct page about MAX_PHYSMEM_BITS
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  drivers/dax/pmem.c                |  4 +--</span>
<span class="quote">&gt; &gt;  drivers/nvdimm/pmem.c             |  6 ++--</span>
<span class="quote">&gt; &gt;  fs/proc/task_mmu.c                | 10 +++++-</span>
<span class="quote">&gt; &gt;  include/linux/ioport.h            |  1 +</span>
<span class="quote">&gt; &gt;  include/linux/memory_hotplug.h    |  7 ++++</span>
<span class="quote">&gt; &gt;  include/linux/memremap.h          | 29 +++++++++++++++--</span>
<span class="quote">&gt; &gt;  include/linux/swap.h              | 18 +++++++++--</span>
<span class="quote">&gt; &gt;  include/linux/swapops.h           | 67 +++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt; &gt;  kernel/memremap.c                 | 43 +++++++++++++++++++++++--</span>
<span class="quote">&gt; &gt;  mm/Kconfig                        | 12 +++++++</span>
<span class="quote">&gt; &gt;  mm/memory.c                       | 64 ++++++++++++++++++++++++++++++++++++-</span>
<span class="quote">&gt; &gt;  mm/memory_hotplug.c               | 10 ++++--</span>
<span class="quote">&gt; &gt;  mm/mprotect.c                     | 12 +++++++</span>
<span class="quote">&gt; &gt;  tools/testing/nvdimm/test/iomap.c |  3 +-</span>
<span class="quote">&gt; &gt;  14 files changed, 269 insertions(+), 17 deletions(-)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; diff --git a/drivers/dax/pmem.c b/drivers/dax/pmem.c</span>
<span class="quote">&gt; &gt; index 66af7b1..c50b58d 100644</span>
<span class="quote">&gt; &gt; --- a/drivers/dax/pmem.c</span>
<span class="quote">&gt; &gt; +++ b/drivers/dax/pmem.c</span>
<span class="quote">&gt; &gt; @@ -111,8 +111,8 @@ static int dax_pmem_probe(struct device *dev)</span>
<span class="quote">&gt; &gt;         if (rc)</span>
<span class="quote">&gt; &gt;                 return rc;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; -       addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref,</span>
<span class="quote">&gt; &gt; -                                  altmap, NULL, NULL);</span>
<span class="quote">&gt; &gt; +       addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref, altmap,</span>
<span class="quote">&gt; &gt; +                                  NULL, NULL, NULL, NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt; &gt;         if (IS_ERR(addr))</span>
<span class="quote">&gt; &gt;                 return PTR_ERR(addr);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; diff --git a/drivers/nvdimm/pmem.c b/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt; &gt; index f2f1904..8166a56 100644</span>
<span class="quote">&gt; &gt; --- a/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt; &gt; +++ b/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt; &gt; @@ -282,7 +282,8 @@ static int pmem_attach_disk(struct device *dev,</span>
<span class="quote">&gt; &gt;         pmem-&gt;pfn_flags = PFN_DEV;</span>
<span class="quote">&gt; &gt;         if (is_nd_pfn(dev)) {</span>
<span class="quote">&gt; &gt;                 addr = devm_memremap_pages(dev, &amp;pfn_res, &amp;q-&gt;q_usage_counter,</span>
<span class="quote">&gt; &gt; -                                          altmap, NULL, NULL);</span>
<span class="quote">&gt; &gt; +                                          altmap, NULL, NULL, NULL,</span>
<span class="quote">&gt; &gt; +                                          NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt; &gt;                 pfn_sb = nd_pfn-&gt;pfn_sb;</span>
<span class="quote">&gt; &gt;                 pmem-&gt;data_offset = le64_to_cpu(pfn_sb-&gt;dataoff);</span>
<span class="quote">&gt; &gt;                 pmem-&gt;pfn_pad = resource_size(res) - resource_size(&amp;pfn_res);</span>
<span class="quote">&gt; &gt; @@ -292,7 +293,8 @@ static int pmem_attach_disk(struct device *dev,</span>
<span class="quote">&gt; &gt;         } else if (pmem_should_map_pages(dev)) {</span>
<span class="quote">&gt; &gt;                 addr = devm_memremap_pages(dev, &amp;nsio-&gt;res,</span>
<span class="quote">&gt; &gt;                                            &amp;q-&gt;q_usage_counter,</span>
<span class="quote">&gt; &gt; -                                          NULL, NULL, NULL);</span>
<span class="quote">&gt; &gt; +                                          NULL, NULL, NULL, NULL,</span>
<span class="quote">&gt; &gt; +                                          NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt; &gt;                 pmem-&gt;pfn_flags |= PFN_MAP;</span>
<span class="quote">&gt; &gt;         } else</span>
<span class="quote">&gt; &gt;                 addr = devm_memremap(dev, pmem-&gt;phys_addr,</span>
<span class="quote">&gt; &gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt; index 958f325..9a6ab71 100644</span>
<span class="quote">&gt; &gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt; @@ -535,8 +535,11 @@ static void smaps_pte_entry(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt; &gt;                         } else {</span>
<span class="quote">&gt; &gt;                                 mss-&gt;swap_pss += (u64)PAGE_SIZE &lt;&lt; PSS_SHIFT;</span>
<span class="quote">&gt; &gt;                         }</span>
<span class="quote">&gt; &gt; -               } else if (is_migration_entry(swpent))</span>
<span class="quote">&gt; &gt; +               } else if (is_migration_entry(swpent)) {</span>
<span class="quote">&gt; &gt;                         page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt; &gt; +               } else if (is_device_entry(swpent)) {</span>
<span class="quote">&gt; &gt; +                       page = device_entry_to_page(swpent);</span>
<span class="quote">&gt; &gt; +               }</span>
<span class="quote">&gt; &gt;         } else if (unlikely(IS_ENABLED(CONFIG_SHMEM) &amp;&amp; mss-&gt;check_shmem_swap</span>
<span class="quote">&gt; &gt;                                                         &amp;&amp; pte_none(*pte))) {</span>
<span class="quote">&gt; &gt;                 page = find_get_entry(vma-&gt;vm_file-&gt;f_mapping,</span>
<span class="quote">&gt; &gt; @@ -699,6 +702,8 @@ static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;                 if (is_migration_entry(swpent))</span>
<span class="quote">&gt; &gt;                         page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt; &gt; +               if (is_device_entry(swpent))</span>
<span class="quote">&gt; &gt; +                       page = device_entry_to_page(swpent);</span>
<span class="quote">&gt; &gt;         }</span>
<span class="quote">&gt; &gt;         if (page) {</span>
<span class="quote">&gt; &gt;                 int mapcount = page_mapcount(page);</span>
<span class="quote">&gt; &gt; @@ -1182,6 +1187,9 @@ static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt; &gt;                 flags |= PM_SWAP;</span>
<span class="quote">&gt; &gt;                 if (is_migration_entry(entry))</span>
<span class="quote">&gt; &gt;                         page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +               if (is_device_entry(entry))</span>
<span class="quote">&gt; &gt; +                       page = device_entry_to_page(entry);</span>
<span class="quote">&gt; &gt;         }</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;         if (page &amp;&amp; !PageAnon(page))</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/ioport.h b/include/linux/ioport.h</span>
<span class="quote">&gt; &gt; index 6230064..d154a18 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/ioport.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/ioport.h</span>
<span class="quote">&gt; &gt; @@ -130,6 +130,7 @@ enum {</span>
<span class="quote">&gt; &gt;         IORES_DESC_ACPI_NV_STORAGE              = 3,</span>
<span class="quote">&gt; &gt;         IORES_DESC_PERSISTENT_MEMORY            = 4,</span>
<span class="quote">&gt; &gt;         IORES_DESC_PERSISTENT_MEMORY_LEGACY     = 5,</span>
<span class="quote">&gt; &gt; +       IORES_DESC_UNADDRESSABLE_MEMORY         = 6,</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  /* helpers to define resources */</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h</span>
<span class="quote">&gt; &gt; index 3f50eb8..e7c5dc6 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/memory_hotplug.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/memory_hotplug.h</span>
<span class="quote">&gt; &gt; @@ -285,15 +285,22 @@ extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,</span>
<span class="quote">&gt; &gt;   * never relied on struct page migration so far and new user of might also</span>
<span class="quote">&gt; &gt;   * prefer avoiding struct page migration.</span>
<span class="quote">&gt; &gt;   *</span>
<span class="quote">&gt; &gt; + * For device memory (which use ZONE_DEVICE) we want differentiate between CPU</span>
<span class="quote">&gt; &gt; + * accessible memory (persitent memory, device memory on an architecture with a</span>
<span class="quote">&gt; &gt; + * system bus that allow transparent access to device memory) and unaddressable</span>
<span class="quote">&gt; &gt; + * memory (device memory that can not be accessed by CPU directly).</span>
<span class="quote">&gt; &gt; + *</span>
<span class="quote">&gt; &gt;   * New non device memory specific flags can be added if ever needed.</span>
<span class="quote">&gt; &gt;   *</span>
<span class="quote">&gt; &gt;   * MEMORY_REGULAR: regular system memory</span>
<span class="quote">&gt; &gt;   * DEVICE_MEMORY: device memory create a ZONE_DEVICE zone for it</span>
<span class="quote">&gt; &gt;   * DEVICE_MEMORY_ALLOW_MIGRATE: page in that device memory ca be migrated</span>
<span class="quote">&gt; &gt; + * MEMORY_DEVICE_UNADDRESSABLE: un-addressable memory (CPU can not access it)</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt;  #define MEMORY_NORMAL 0</span>
<span class="quote">&gt; &gt;  #define MEMORY_DEVICE (1 &lt;&lt; 0)</span>
<span class="quote">&gt; &gt;  #define MEMORY_DEVICE_ALLOW_MIGRATE (1 &lt;&lt; 1)</span>
<span class="quote">&gt; &gt; +#define MEMORY_DEVICE_UNADDRESSABLE (1 &lt;&lt; 2)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  extern int arch_add_memory(int nid, u64 start, u64 size, int flags);</span>
<span class="quote">&gt; &gt;  extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);</span>
<span class="quote">&gt; &gt; diff --git a/include/linux/memremap.h b/include/linux/memremap.h</span>
<span class="quote">&gt; &gt; index 582561f..4b9f02c 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/memremap.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/memremap.h</span>
<span class="quote">&gt; &gt; @@ -35,31 +35,42 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
<span class="quote">&gt; &gt;  }</span>
<span class="quote">&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; +typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt; +                               unsigned long addr,</span>
<span class="quote">&gt; &gt; +                               struct page *page,</span>
<span class="quote">&gt; &gt; +                               unsigned flags,</span>
<span class="quote">&gt; &gt; +                               pmd_t *pmdp);</span>
<span class="quote">&gt; &gt;  typedef void (*dev_page_free_t)(struct page *page, void *data);</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  /**</span>
<span class="quote">&gt; &gt;   * struct dev_pagemap - metadata for ZONE_DEVICE mappings</span>
<span class="quote">&gt; &gt; + * @page_fault: callback when CPU fault on an un-addressable device page</span>
<span class="quote">&gt; &gt;   * @page_free: free page callback when page refcount reach 1</span>
<span class="quote">&gt; &gt;   * @altmap: pre-allocated/reserved memory for vmemmap allocations</span>
<span class="quote">&gt; &gt;   * @res: physical address range covered by @ref</span>
<span class="quote">&gt; &gt;   * @ref: reference count that pins the devm_memremap_pages() mapping</span>
<span class="quote">&gt; &gt;   * @dev: host device of the mapping for debug</span>
<span class="quote">&gt; &gt;   * @data: privata data pointer for page_free</span>
<span class="quote">&gt; &gt; + * @flags: device memory flags (look for MEMORY_DEVICE_* memory_hotplug.h)</span>
<span class="quote">&gt; &gt;   */</span>
<span class="quote">&gt; &gt;  struct dev_pagemap {</span>
<span class="quote">&gt; &gt; +       dev_page_fault_t page_fault;</span>
<span class="quote">&gt; &gt;         dev_page_free_t page_free;</span>
<span class="quote">&gt; &gt;         struct vmem_altmap *altmap;</span>
<span class="quote">&gt; &gt;         const struct resource *res;</span>
<span class="quote">&gt; &gt;         struct percpu_ref *ref;</span>
<span class="quote">&gt; &gt;         struct device *dev;</span>
<span class="quote">&gt; &gt;         void *data;</span>
<span class="quote">&gt; &gt; +       int flags;</span>
<span class="quote">&gt; &gt;  };</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; dev_pagemap is only meant for get_user_pages() to do lookups of ptes</span>
<span class="quote">&gt; with _PAGE_DEVMAP and take a reference against the hosting device..</span>

And i want to build on top of that to extend _PAGE_DEVMAP to support
a new usecase for unaddressable device memory.
<span class="quote">
&gt; </span>
<span class="quote">&gt; Why can&#39;t HMM use the typical vm_operations_struct fault path and push</span>
<span class="quote">&gt; more of these details to a driver rather than the core?</span>

Because the vm_operations_struct has nothing to do with the device.
We are talking about regular vma here. Think malloc, mmap, share
memory, ...  not about mmap(/dev/thedevice,...)

So the vm_operations_struct is never under device control and we can
not, nor want to, rely on that.

So what we looking for here is struct page that can behave mostly
like anyother except that we do not want to allow GUP to take a
reference almost exactly what ZONE_DEVICE already provide.

So do you have any fundamental objections to this patchset ? And if
so, how do you propose i solve the problem i am trying to address ?
Because hardware exist today and without something like HMM we will
not be able to support such hardware.

Do i wish all platform (AMD, ARM, IBM, Intel, ...) were moving in the
same direction ? Yes, but reality is that some of them are not planing
to have a cache coherent system bus like CCIX or CAPI. So for some of
the above we need something like HMM.


Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=320">Dan Williams</a> - Jan. 16, 2017, 7:31 p.m.</div>
<pre class="content">
On Mon, Jan 16, 2017 at 7:17 AM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:
<span class="quote">&gt; On Sun, Jan 15, 2017 at 11:05:43PM -0800, Dan Williams wrote:</span>
<span class="quote">&gt;&gt; On Thu, Jan 12, 2017 at 8:30 AM, Jérôme Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt; This add support for un-addressable device memory. Such memory is hotpluged</span>
<span class="quote">&gt;&gt; &gt; only so we can have struct page but we should never map them as such memory</span>
<span class="quote">&gt;&gt; &gt; can not be accessed by CPU. For that reason it uses a special swap entry for</span>
<span class="quote">&gt;&gt; &gt; CPU page table entry.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; This patch implement all the logic from special swap type to handling CPU</span>
<span class="quote">&gt;&gt; &gt; page fault through a callback specified in the ZONE_DEVICE pgmap struct.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Architecture that wish to support un-addressable device memory should make</span>
<span class="quote">&gt;&gt; &gt; sure to never populate the kernel linar mapping for the physical range.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; This feature potentially breaks memory hotplug unless every driver using it</span>
<span class="quote">&gt;&gt; &gt; magically predicts the future addresses of where memory will be hotplugged.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Changed since v1:</span>
<span class="quote">&gt;&gt; &gt;   - Add unaddressable memory resource descriptor enum</span>
<span class="quote">&gt;&gt; &gt;   - Explain why memory hotplug can fail because of un-addressable memory</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; How can we merge this with a known potential regression?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This only affect people using HMM it does not affect people that do not</span>
<span class="quote">&gt; use it. Like i said for time being people that want HMM are ok with that.</span>
<span class="quote">&gt; I haven&#39;t look yet on how to allow struct page about MAX_PHYSMEM_BITS</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt;&gt; &gt; Cc: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
<span class="quote">&gt;&gt; &gt; Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt; &gt; ---</span>
<span class="quote">&gt;&gt; &gt;  drivers/dax/pmem.c                |  4 +--</span>
<span class="quote">&gt;&gt; &gt;  drivers/nvdimm/pmem.c             |  6 ++--</span>
<span class="quote">&gt;&gt; &gt;  fs/proc/task_mmu.c                | 10 +++++-</span>
<span class="quote">&gt;&gt; &gt;  include/linux/ioport.h            |  1 +</span>
<span class="quote">&gt;&gt; &gt;  include/linux/memory_hotplug.h    |  7 ++++</span>
<span class="quote">&gt;&gt; &gt;  include/linux/memremap.h          | 29 +++++++++++++++--</span>
<span class="quote">&gt;&gt; &gt;  include/linux/swap.h              | 18 +++++++++--</span>
<span class="quote">&gt;&gt; &gt;  include/linux/swapops.h           | 67 +++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt; &gt;  kernel/memremap.c                 | 43 +++++++++++++++++++++++--</span>
<span class="quote">&gt;&gt; &gt;  mm/Kconfig                        | 12 +++++++</span>
<span class="quote">&gt;&gt; &gt;  mm/memory.c                       | 64 ++++++++++++++++++++++++++++++++++++-</span>
<span class="quote">&gt;&gt; &gt;  mm/memory_hotplug.c               | 10 ++++--</span>
<span class="quote">&gt;&gt; &gt;  mm/mprotect.c                     | 12 +++++++</span>
<span class="quote">&gt;&gt; &gt;  tools/testing/nvdimm/test/iomap.c |  3 +-</span>
<span class="quote">&gt;&gt; &gt;  14 files changed, 269 insertions(+), 17 deletions(-)</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; diff --git a/drivers/dax/pmem.c b/drivers/dax/pmem.c</span>
<span class="quote">&gt;&gt; &gt; index 66af7b1..c50b58d 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/drivers/dax/pmem.c</span>
<span class="quote">&gt;&gt; &gt; +++ b/drivers/dax/pmem.c</span>
<span class="quote">&gt;&gt; &gt; @@ -111,8 +111,8 @@ static int dax_pmem_probe(struct device *dev)</span>
<span class="quote">&gt;&gt; &gt;         if (rc)</span>
<span class="quote">&gt;&gt; &gt;                 return rc;</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; -       addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref,</span>
<span class="quote">&gt;&gt; &gt; -                                  altmap, NULL, NULL);</span>
<span class="quote">&gt;&gt; &gt; +       addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref, altmap,</span>
<span class="quote">&gt;&gt; &gt; +                                  NULL, NULL, NULL, NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt;&gt; &gt;         if (IS_ERR(addr))</span>
<span class="quote">&gt;&gt; &gt;                 return PTR_ERR(addr);</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; diff --git a/drivers/nvdimm/pmem.c b/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt;&gt; &gt; index f2f1904..8166a56 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt;&gt; &gt; +++ b/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt;&gt; &gt; @@ -282,7 +282,8 @@ static int pmem_attach_disk(struct device *dev,</span>
<span class="quote">&gt;&gt; &gt;         pmem-&gt;pfn_flags = PFN_DEV;</span>
<span class="quote">&gt;&gt; &gt;         if (is_nd_pfn(dev)) {</span>
<span class="quote">&gt;&gt; &gt;                 addr = devm_memremap_pages(dev, &amp;pfn_res, &amp;q-&gt;q_usage_counter,</span>
<span class="quote">&gt;&gt; &gt; -                                          altmap, NULL, NULL);</span>
<span class="quote">&gt;&gt; &gt; +                                          altmap, NULL, NULL, NULL,</span>
<span class="quote">&gt;&gt; &gt; +                                          NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt;&gt; &gt;                 pfn_sb = nd_pfn-&gt;pfn_sb;</span>
<span class="quote">&gt;&gt; &gt;                 pmem-&gt;data_offset = le64_to_cpu(pfn_sb-&gt;dataoff);</span>
<span class="quote">&gt;&gt; &gt;                 pmem-&gt;pfn_pad = resource_size(res) - resource_size(&amp;pfn_res);</span>
<span class="quote">&gt;&gt; &gt; @@ -292,7 +293,8 @@ static int pmem_attach_disk(struct device *dev,</span>
<span class="quote">&gt;&gt; &gt;         } else if (pmem_should_map_pages(dev)) {</span>
<span class="quote">&gt;&gt; &gt;                 addr = devm_memremap_pages(dev, &amp;nsio-&gt;res,</span>
<span class="quote">&gt;&gt; &gt;                                            &amp;q-&gt;q_usage_counter,</span>
<span class="quote">&gt;&gt; &gt; -                                          NULL, NULL, NULL);</span>
<span class="quote">&gt;&gt; &gt; +                                          NULL, NULL, NULL, NULL,</span>
<span class="quote">&gt;&gt; &gt; +                                          NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt;&gt; &gt;                 pmem-&gt;pfn_flags |= PFN_MAP;</span>
<span class="quote">&gt;&gt; &gt;         } else</span>
<span class="quote">&gt;&gt; &gt;                 addr = devm_memremap(dev, pmem-&gt;phys_addr,</span>
<span class="quote">&gt;&gt; &gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt;&gt; &gt; index 958f325..9a6ab71 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt;&gt; &gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt;&gt; &gt; @@ -535,8 +535,11 @@ static void smaps_pte_entry(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt;&gt; &gt;                         } else {</span>
<span class="quote">&gt;&gt; &gt;                                 mss-&gt;swap_pss += (u64)PAGE_SIZE &lt;&lt; PSS_SHIFT;</span>
<span class="quote">&gt;&gt; &gt;                         }</span>
<span class="quote">&gt;&gt; &gt; -               } else if (is_migration_entry(swpent))</span>
<span class="quote">&gt;&gt; &gt; +               } else if (is_migration_entry(swpent)) {</span>
<span class="quote">&gt;&gt; &gt;                         page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt;&gt; &gt; +               } else if (is_device_entry(swpent)) {</span>
<span class="quote">&gt;&gt; &gt; +                       page = device_entry_to_page(swpent);</span>
<span class="quote">&gt;&gt; &gt; +               }</span>
<span class="quote">&gt;&gt; &gt;         } else if (unlikely(IS_ENABLED(CONFIG_SHMEM) &amp;&amp; mss-&gt;check_shmem_swap</span>
<span class="quote">&gt;&gt; &gt;                                                         &amp;&amp; pte_none(*pte))) {</span>
<span class="quote">&gt;&gt; &gt;                 page = find_get_entry(vma-&gt;vm_file-&gt;f_mapping,</span>
<span class="quote">&gt;&gt; &gt; @@ -699,6 +702,8 @@ static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;                 if (is_migration_entry(swpent))</span>
<span class="quote">&gt;&gt; &gt;                         page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt;&gt; &gt; +               if (is_device_entry(swpent))</span>
<span class="quote">&gt;&gt; &gt; +                       page = device_entry_to_page(swpent);</span>
<span class="quote">&gt;&gt; &gt;         }</span>
<span class="quote">&gt;&gt; &gt;         if (page) {</span>
<span class="quote">&gt;&gt; &gt;                 int mapcount = page_mapcount(page);</span>
<span class="quote">&gt;&gt; &gt; @@ -1182,6 +1187,9 @@ static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt;&gt; &gt;                 flags |= PM_SWAP;</span>
<span class="quote">&gt;&gt; &gt;                 if (is_migration_entry(entry))</span>
<span class="quote">&gt;&gt; &gt;                         page = migration_entry_to_page(entry);</span>
<span class="quote">&gt;&gt; &gt; +</span>
<span class="quote">&gt;&gt; &gt; +               if (is_device_entry(entry))</span>
<span class="quote">&gt;&gt; &gt; +                       page = device_entry_to_page(entry);</span>
<span class="quote">&gt;&gt; &gt;         }</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;         if (page &amp;&amp; !PageAnon(page))</span>
<span class="quote">&gt;&gt; &gt; diff --git a/include/linux/ioport.h b/include/linux/ioport.h</span>
<span class="quote">&gt;&gt; &gt; index 6230064..d154a18 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/include/linux/ioport.h</span>
<span class="quote">&gt;&gt; &gt; +++ b/include/linux/ioport.h</span>
<span class="quote">&gt;&gt; &gt; @@ -130,6 +130,7 @@ enum {</span>
<span class="quote">&gt;&gt; &gt;         IORES_DESC_ACPI_NV_STORAGE              = 3,</span>
<span class="quote">&gt;&gt; &gt;         IORES_DESC_PERSISTENT_MEMORY            = 4,</span>
<span class="quote">&gt;&gt; &gt;         IORES_DESC_PERSISTENT_MEMORY_LEGACY     = 5,</span>
<span class="quote">&gt;&gt; &gt; +       IORES_DESC_UNADDRESSABLE_MEMORY         = 6,</span>
<span class="quote">&gt;&gt; &gt;  };</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;  /* helpers to define resources */</span>
<span class="quote">&gt;&gt; &gt; diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h</span>
<span class="quote">&gt;&gt; &gt; index 3f50eb8..e7c5dc6 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/include/linux/memory_hotplug.h</span>
<span class="quote">&gt;&gt; &gt; +++ b/include/linux/memory_hotplug.h</span>
<span class="quote">&gt;&gt; &gt; @@ -285,15 +285,22 @@ extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,</span>
<span class="quote">&gt;&gt; &gt;   * never relied on struct page migration so far and new user of might also</span>
<span class="quote">&gt;&gt; &gt;   * prefer avoiding struct page migration.</span>
<span class="quote">&gt;&gt; &gt;   *</span>
<span class="quote">&gt;&gt; &gt; + * For device memory (which use ZONE_DEVICE) we want differentiate between CPU</span>
<span class="quote">&gt;&gt; &gt; + * accessible memory (persitent memory, device memory on an architecture with a</span>
<span class="quote">&gt;&gt; &gt; + * system bus that allow transparent access to device memory) and unaddressable</span>
<span class="quote">&gt;&gt; &gt; + * memory (device memory that can not be accessed by CPU directly).</span>
<span class="quote">&gt;&gt; &gt; + *</span>
<span class="quote">&gt;&gt; &gt;   * New non device memory specific flags can be added if ever needed.</span>
<span class="quote">&gt;&gt; &gt;   *</span>
<span class="quote">&gt;&gt; &gt;   * MEMORY_REGULAR: regular system memory</span>
<span class="quote">&gt;&gt; &gt;   * DEVICE_MEMORY: device memory create a ZONE_DEVICE zone for it</span>
<span class="quote">&gt;&gt; &gt;   * DEVICE_MEMORY_ALLOW_MIGRATE: page in that device memory ca be migrated</span>
<span class="quote">&gt;&gt; &gt; + * MEMORY_DEVICE_UNADDRESSABLE: un-addressable memory (CPU can not access it)</span>
<span class="quote">&gt;&gt; &gt;   */</span>
<span class="quote">&gt;&gt; &gt;  #define MEMORY_NORMAL 0</span>
<span class="quote">&gt;&gt; &gt;  #define MEMORY_DEVICE (1 &lt;&lt; 0)</span>
<span class="quote">&gt;&gt; &gt;  #define MEMORY_DEVICE_ALLOW_MIGRATE (1 &lt;&lt; 1)</span>
<span class="quote">&gt;&gt; &gt; +#define MEMORY_DEVICE_UNADDRESSABLE (1 &lt;&lt; 2)</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;  extern int arch_add_memory(int nid, u64 start, u64 size, int flags);</span>
<span class="quote">&gt;&gt; &gt;  extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);</span>
<span class="quote">&gt;&gt; &gt; diff --git a/include/linux/memremap.h b/include/linux/memremap.h</span>
<span class="quote">&gt;&gt; &gt; index 582561f..4b9f02c 100644</span>
<span class="quote">&gt;&gt; &gt; --- a/include/linux/memremap.h</span>
<span class="quote">&gt;&gt; &gt; +++ b/include/linux/memremap.h</span>
<span class="quote">&gt;&gt; &gt; @@ -35,31 +35,42 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
<span class="quote">&gt;&gt; &gt;  }</span>
<span class="quote">&gt;&gt; &gt;  #endif</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; +typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt; &gt; +                               unsigned long addr,</span>
<span class="quote">&gt;&gt; &gt; +                               struct page *page,</span>
<span class="quote">&gt;&gt; &gt; +                               unsigned flags,</span>
<span class="quote">&gt;&gt; &gt; +                               pmd_t *pmdp);</span>
<span class="quote">&gt;&gt; &gt;  typedef void (*dev_page_free_t)(struct page *page, void *data);</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;  /**</span>
<span class="quote">&gt;&gt; &gt;   * struct dev_pagemap - metadata for ZONE_DEVICE mappings</span>
<span class="quote">&gt;&gt; &gt; + * @page_fault: callback when CPU fault on an un-addressable device page</span>
<span class="quote">&gt;&gt; &gt;   * @page_free: free page callback when page refcount reach 1</span>
<span class="quote">&gt;&gt; &gt;   * @altmap: pre-allocated/reserved memory for vmemmap allocations</span>
<span class="quote">&gt;&gt; &gt;   * @res: physical address range covered by @ref</span>
<span class="quote">&gt;&gt; &gt;   * @ref: reference count that pins the devm_memremap_pages() mapping</span>
<span class="quote">&gt;&gt; &gt;   * @dev: host device of the mapping for debug</span>
<span class="quote">&gt;&gt; &gt;   * @data: privata data pointer for page_free</span>
<span class="quote">&gt;&gt; &gt; + * @flags: device memory flags (look for MEMORY_DEVICE_* memory_hotplug.h)</span>
<span class="quote">&gt;&gt; &gt;   */</span>
<span class="quote">&gt;&gt; &gt;  struct dev_pagemap {</span>
<span class="quote">&gt;&gt; &gt; +       dev_page_fault_t page_fault;</span>
<span class="quote">&gt;&gt; &gt;         dev_page_free_t page_free;</span>
<span class="quote">&gt;&gt; &gt;         struct vmem_altmap *altmap;</span>
<span class="quote">&gt;&gt; &gt;         const struct resource *res;</span>
<span class="quote">&gt;&gt; &gt;         struct percpu_ref *ref;</span>
<span class="quote">&gt;&gt; &gt;         struct device *dev;</span>
<span class="quote">&gt;&gt; &gt;         void *data;</span>
<span class="quote">&gt;&gt; &gt; +       int flags;</span>
<span class="quote">&gt;&gt; &gt;  };</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; dev_pagemap is only meant for get_user_pages() to do lookups of ptes</span>
<span class="quote">&gt;&gt; with _PAGE_DEVMAP and take a reference against the hosting device..</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And i want to build on top of that to extend _PAGE_DEVMAP to support</span>
<span class="quote">&gt; a new usecase for unaddressable device memory.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Why can&#39;t HMM use the typical vm_operations_struct fault path and push</span>
<span class="quote">&gt;&gt; more of these details to a driver rather than the core?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Because the vm_operations_struct has nothing to do with the device.</span>
<span class="quote">&gt; We are talking about regular vma here. Think malloc, mmap, share</span>
<span class="quote">&gt; memory, ...  not about mmap(/dev/thedevice,...)</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So the vm_operations_struct is never under device control and we can</span>
<span class="quote">&gt; not, nor want to, rely on that.</span>

Can you explain more what&#39;s behind that &quot;can not, nor want to&quot;
statement? It seems to me that any awkwardness of moving to a
standalone device file interface is less than a maintaining a new /
parallel mm fault path through dev_pagemap.
<span class="quote">
&gt;</span>
<span class="quote">&gt; So what we looking for here is struct page that can behave mostly</span>
<span class="quote">&gt; like anyother except that we do not want to allow GUP to take a</span>
<span class="quote">&gt; reference almost exactly what ZONE_DEVICE already provide.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; So do you have any fundamental objections to this patchset ? And if</span>
<span class="quote">&gt; so, how do you propose i solve the problem i am trying to address ?</span>
<span class="quote">&gt; Because hardware exist today and without something like HMM we will</span>
<span class="quote">&gt; not be able to support such hardware.</span>

My pushback stems from it being a completely different use case for
devm_memremap_pages(), as evidenced by it growing from 4 arguments to
9, and the ongoing maintenance overhead of understanding HMM
requirements when updating the pmem usage of ZONE_DEVICE.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Jan. 16, 2017, 8:13 p.m.</div>
<pre class="content">
On Mon, Jan 16, 2017 at 11:31:39AM -0800, Dan Williams wrote:
<span class="quote">&gt; On Mon, Jan 16, 2017 at 7:17 AM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt; &gt; On Sun, Jan 15, 2017 at 11:05:43PM -0800, Dan Williams wrote:</span>
<span class="quote">&gt; &gt;&gt; On Thu, Jan 12, 2017 at 8:30 AM, Jérôme Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt; &gt;&gt; &gt; This add support for un-addressable device memory. Such memory is hotpluged</span>
<span class="quote">&gt; &gt;&gt; &gt; only so we can have struct page but we should never map them as such memory</span>
<span class="quote">&gt; &gt;&gt; &gt; can not be accessed by CPU. For that reason it uses a special swap entry for</span>
<span class="quote">&gt; &gt;&gt; &gt; CPU page table entry.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; This patch implement all the logic from special swap type to handling CPU</span>
<span class="quote">&gt; &gt;&gt; &gt; page fault through a callback specified in the ZONE_DEVICE pgmap struct.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; Architecture that wish to support un-addressable device memory should make</span>
<span class="quote">&gt; &gt;&gt; &gt; sure to never populate the kernel linar mapping for the physical range.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; This feature potentially breaks memory hotplug unless every driver using it</span>
<span class="quote">&gt; &gt;&gt; &gt; magically predicts the future addresses of where memory will be hotplugged.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; Changed since v1:</span>
<span class="quote">&gt; &gt;&gt; &gt;   - Add unaddressable memory resource descriptor enum</span>
<span class="quote">&gt; &gt;&gt; &gt;   - Explain why memory hotplug can fail because of un-addressable memory</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; How can we merge this with a known potential regression?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; This only affect people using HMM it does not affect people that do not</span>
<span class="quote">&gt; &gt; use it. Like i said for time being people that want HMM are ok with that.</span>
<span class="quote">&gt; &gt; I haven&#39;t look yet on how to allow struct page about MAX_PHYSMEM_BITS</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; Signed-off-by: Jérôme Glisse &lt;jglisse@redhat.com&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; Cc: Dan Williams &lt;dan.j.williams@intel.com&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; Cc: Ross Zwisler &lt;ross.zwisler@linux.intel.com&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; ---</span>
<span class="quote">&gt; &gt;&gt; &gt;  drivers/dax/pmem.c                |  4 +--</span>
<span class="quote">&gt; &gt;&gt; &gt;  drivers/nvdimm/pmem.c             |  6 ++--</span>
<span class="quote">&gt; &gt;&gt; &gt;  fs/proc/task_mmu.c                | 10 +++++-</span>
<span class="quote">&gt; &gt;&gt; &gt;  include/linux/ioport.h            |  1 +</span>
<span class="quote">&gt; &gt;&gt; &gt;  include/linux/memory_hotplug.h    |  7 ++++</span>
<span class="quote">&gt; &gt;&gt; &gt;  include/linux/memremap.h          | 29 +++++++++++++++--</span>
<span class="quote">&gt; &gt;&gt; &gt;  include/linux/swap.h              | 18 +++++++++--</span>
<span class="quote">&gt; &gt;&gt; &gt;  include/linux/swapops.h           | 67 +++++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt; &gt;&gt; &gt;  kernel/memremap.c                 | 43 +++++++++++++++++++++++--</span>
<span class="quote">&gt; &gt;&gt; &gt;  mm/Kconfig                        | 12 +++++++</span>
<span class="quote">&gt; &gt;&gt; &gt;  mm/memory.c                       | 64 ++++++++++++++++++++++++++++++++++++-</span>
<span class="quote">&gt; &gt;&gt; &gt;  mm/memory_hotplug.c               | 10 ++++--</span>
<span class="quote">&gt; &gt;&gt; &gt;  mm/mprotect.c                     | 12 +++++++</span>
<span class="quote">&gt; &gt;&gt; &gt;  tools/testing/nvdimm/test/iomap.c |  3 +-</span>
<span class="quote">&gt; &gt;&gt; &gt;  14 files changed, 269 insertions(+), 17 deletions(-)</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/drivers/dax/pmem.c b/drivers/dax/pmem.c</span>
<span class="quote">&gt; &gt;&gt; &gt; index 66af7b1..c50b58d 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/drivers/dax/pmem.c</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/drivers/dax/pmem.c</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -111,8 +111,8 @@ static int dax_pmem_probe(struct device *dev)</span>
<span class="quote">&gt; &gt;&gt; &gt;         if (rc)</span>
<span class="quote">&gt; &gt;&gt; &gt;                 return rc;</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; -       addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref,</span>
<span class="quote">&gt; &gt;&gt; &gt; -                                  altmap, NULL, NULL);</span>
<span class="quote">&gt; &gt;&gt; &gt; +       addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref, altmap,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                  NULL, NULL, NULL, NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt; &gt;&gt; &gt;         if (IS_ERR(addr))</span>
<span class="quote">&gt; &gt;&gt; &gt;                 return PTR_ERR(addr);</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/drivers/nvdimm/pmem.c b/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt; &gt;&gt; &gt; index f2f1904..8166a56 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/drivers/nvdimm/pmem.c</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -282,7 +282,8 @@ static int pmem_attach_disk(struct device *dev,</span>
<span class="quote">&gt; &gt;&gt; &gt;         pmem-&gt;pfn_flags = PFN_DEV;</span>
<span class="quote">&gt; &gt;&gt; &gt;         if (is_nd_pfn(dev)) {</span>
<span class="quote">&gt; &gt;&gt; &gt;                 addr = devm_memremap_pages(dev, &amp;pfn_res, &amp;q-&gt;q_usage_counter,</span>
<span class="quote">&gt; &gt;&gt; &gt; -                                          altmap, NULL, NULL);</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                          altmap, NULL, NULL, NULL,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                          NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt; &gt;&gt; &gt;                 pfn_sb = nd_pfn-&gt;pfn_sb;</span>
<span class="quote">&gt; &gt;&gt; &gt;                 pmem-&gt;data_offset = le64_to_cpu(pfn_sb-&gt;dataoff);</span>
<span class="quote">&gt; &gt;&gt; &gt;                 pmem-&gt;pfn_pad = resource_size(res) - resource_size(&amp;pfn_res);</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -292,7 +293,8 @@ static int pmem_attach_disk(struct device *dev,</span>
<span class="quote">&gt; &gt;&gt; &gt;         } else if (pmem_should_map_pages(dev)) {</span>
<span class="quote">&gt; &gt;&gt; &gt;                 addr = devm_memremap_pages(dev, &amp;nsio-&gt;res,</span>
<span class="quote">&gt; &gt;&gt; &gt;                                            &amp;q-&gt;q_usage_counter,</span>
<span class="quote">&gt; &gt;&gt; &gt; -                                          NULL, NULL, NULL);</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                          NULL, NULL, NULL, NULL,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                                          NULL, MEMORY_DEVICE);</span>
<span class="quote">&gt; &gt;&gt; &gt;                 pmem-&gt;pfn_flags |= PFN_MAP;</span>
<span class="quote">&gt; &gt;&gt; &gt;         } else</span>
<span class="quote">&gt; &gt;&gt; &gt;                 addr = devm_memremap(dev, pmem-&gt;phys_addr,</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt;&gt; &gt; index 958f325..9a6ab71 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/fs/proc/task_mmu.c</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -535,8 +535,11 @@ static void smaps_pte_entry(pte_t *pte, unsigned long addr,</span>
<span class="quote">&gt; &gt;&gt; &gt;                         } else {</span>
<span class="quote">&gt; &gt;&gt; &gt;                                 mss-&gt;swap_pss += (u64)PAGE_SIZE &lt;&lt; PSS_SHIFT;</span>
<span class="quote">&gt; &gt;&gt; &gt;                         }</span>
<span class="quote">&gt; &gt;&gt; &gt; -               } else if (is_migration_entry(swpent))</span>
<span class="quote">&gt; &gt;&gt; &gt; +               } else if (is_migration_entry(swpent)) {</span>
<span class="quote">&gt; &gt;&gt; &gt;                         page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt; &gt;&gt; &gt; +               } else if (is_device_entry(swpent)) {</span>
<span class="quote">&gt; &gt;&gt; &gt; +                       page = device_entry_to_page(swpent);</span>
<span class="quote">&gt; &gt;&gt; &gt; +               }</span>
<span class="quote">&gt; &gt;&gt; &gt;         } else if (unlikely(IS_ENABLED(CONFIG_SHMEM) &amp;&amp; mss-&gt;check_shmem_swap</span>
<span class="quote">&gt; &gt;&gt; &gt;                                                         &amp;&amp; pte_none(*pte))) {</span>
<span class="quote">&gt; &gt;&gt; &gt;                 page = find_get_entry(vma-&gt;vm_file-&gt;f_mapping,</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -699,6 +702,8 @@ static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;                 if (is_migration_entry(swpent))</span>
<span class="quote">&gt; &gt;&gt; &gt;                         page = migration_entry_to_page(swpent);</span>
<span class="quote">&gt; &gt;&gt; &gt; +               if (is_device_entry(swpent))</span>
<span class="quote">&gt; &gt;&gt; &gt; +                       page = device_entry_to_page(swpent);</span>
<span class="quote">&gt; &gt;&gt; &gt;         }</span>
<span class="quote">&gt; &gt;&gt; &gt;         if (page) {</span>
<span class="quote">&gt; &gt;&gt; &gt;                 int mapcount = page_mapcount(page);</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -1182,6 +1187,9 @@ static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
<span class="quote">&gt; &gt;&gt; &gt;                 flags |= PM_SWAP;</span>
<span class="quote">&gt; &gt;&gt; &gt;                 if (is_migration_entry(entry))</span>
<span class="quote">&gt; &gt;&gt; &gt;                         page = migration_entry_to_page(entry);</span>
<span class="quote">&gt; &gt;&gt; &gt; +</span>
<span class="quote">&gt; &gt;&gt; &gt; +               if (is_device_entry(entry))</span>
<span class="quote">&gt; &gt;&gt; &gt; +                       page = device_entry_to_page(entry);</span>
<span class="quote">&gt; &gt;&gt; &gt;         }</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;         if (page &amp;&amp; !PageAnon(page))</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/include/linux/ioport.h b/include/linux/ioport.h</span>
<span class="quote">&gt; &gt;&gt; &gt; index 6230064..d154a18 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/include/linux/ioport.h</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/include/linux/ioport.h</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -130,6 +130,7 @@ enum {</span>
<span class="quote">&gt; &gt;&gt; &gt;         IORES_DESC_ACPI_NV_STORAGE              = 3,</span>
<span class="quote">&gt; &gt;&gt; &gt;         IORES_DESC_PERSISTENT_MEMORY            = 4,</span>
<span class="quote">&gt; &gt;&gt; &gt;         IORES_DESC_PERSISTENT_MEMORY_LEGACY     = 5,</span>
<span class="quote">&gt; &gt;&gt; &gt; +       IORES_DESC_UNADDRESSABLE_MEMORY         = 6,</span>
<span class="quote">&gt; &gt;&gt; &gt;  };</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;  /* helpers to define resources */</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h</span>
<span class="quote">&gt; &gt;&gt; &gt; index 3f50eb8..e7c5dc6 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/include/linux/memory_hotplug.h</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/include/linux/memory_hotplug.h</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -285,15 +285,22 @@ extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,</span>
<span class="quote">&gt; &gt;&gt; &gt;   * never relied on struct page migration so far and new user of might also</span>
<span class="quote">&gt; &gt;&gt; &gt;   * prefer avoiding struct page migration.</span>
<span class="quote">&gt; &gt;&gt; &gt;   *</span>
<span class="quote">&gt; &gt;&gt; &gt; + * For device memory (which use ZONE_DEVICE) we want differentiate between CPU</span>
<span class="quote">&gt; &gt;&gt; &gt; + * accessible memory (persitent memory, device memory on an architecture with a</span>
<span class="quote">&gt; &gt;&gt; &gt; + * system bus that allow transparent access to device memory) and unaddressable</span>
<span class="quote">&gt; &gt;&gt; &gt; + * memory (device memory that can not be accessed by CPU directly).</span>
<span class="quote">&gt; &gt;&gt; &gt; + *</span>
<span class="quote">&gt; &gt;&gt; &gt;   * New non device memory specific flags can be added if ever needed.</span>
<span class="quote">&gt; &gt;&gt; &gt;   *</span>
<span class="quote">&gt; &gt;&gt; &gt;   * MEMORY_REGULAR: regular system memory</span>
<span class="quote">&gt; &gt;&gt; &gt;   * DEVICE_MEMORY: device memory create a ZONE_DEVICE zone for it</span>
<span class="quote">&gt; &gt;&gt; &gt;   * DEVICE_MEMORY_ALLOW_MIGRATE: page in that device memory ca be migrated</span>
<span class="quote">&gt; &gt;&gt; &gt; + * MEMORY_DEVICE_UNADDRESSABLE: un-addressable memory (CPU can not access it)</span>
<span class="quote">&gt; &gt;&gt; &gt;   */</span>
<span class="quote">&gt; &gt;&gt; &gt;  #define MEMORY_NORMAL 0</span>
<span class="quote">&gt; &gt;&gt; &gt;  #define MEMORY_DEVICE (1 &lt;&lt; 0)</span>
<span class="quote">&gt; &gt;&gt; &gt;  #define MEMORY_DEVICE_ALLOW_MIGRATE (1 &lt;&lt; 1)</span>
<span class="quote">&gt; &gt;&gt; &gt; +#define MEMORY_DEVICE_UNADDRESSABLE (1 &lt;&lt; 2)</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;  extern int arch_add_memory(int nid, u64 start, u64 size, int flags);</span>
<span class="quote">&gt; &gt;&gt; &gt;  extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);</span>
<span class="quote">&gt; &gt;&gt; &gt; diff --git a/include/linux/memremap.h b/include/linux/memremap.h</span>
<span class="quote">&gt; &gt;&gt; &gt; index 582561f..4b9f02c 100644</span>
<span class="quote">&gt; &gt;&gt; &gt; --- a/include/linux/memremap.h</span>
<span class="quote">&gt; &gt;&gt; &gt; +++ b/include/linux/memremap.h</span>
<span class="quote">&gt; &gt;&gt; &gt; @@ -35,31 +35,42 @@ static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
<span class="quote">&gt; &gt;&gt; &gt;  }</span>
<span class="quote">&gt; &gt;&gt; &gt;  #endif</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; +typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                               unsigned long addr,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                               struct page *page,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                               unsigned flags,</span>
<span class="quote">&gt; &gt;&gt; &gt; +                               pmd_t *pmdp);</span>
<span class="quote">&gt; &gt;&gt; &gt;  typedef void (*dev_page_free_t)(struct page *page, void *data);</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;  /**</span>
<span class="quote">&gt; &gt;&gt; &gt;   * struct dev_pagemap - metadata for ZONE_DEVICE mappings</span>
<span class="quote">&gt; &gt;&gt; &gt; + * @page_fault: callback when CPU fault on an un-addressable device page</span>
<span class="quote">&gt; &gt;&gt; &gt;   * @page_free: free page callback when page refcount reach 1</span>
<span class="quote">&gt; &gt;&gt; &gt;   * @altmap: pre-allocated/reserved memory for vmemmap allocations</span>
<span class="quote">&gt; &gt;&gt; &gt;   * @res: physical address range covered by @ref</span>
<span class="quote">&gt; &gt;&gt; &gt;   * @ref: reference count that pins the devm_memremap_pages() mapping</span>
<span class="quote">&gt; &gt;&gt; &gt;   * @dev: host device of the mapping for debug</span>
<span class="quote">&gt; &gt;&gt; &gt;   * @data: privata data pointer for page_free</span>
<span class="quote">&gt; &gt;&gt; &gt; + * @flags: device memory flags (look for MEMORY_DEVICE_* memory_hotplug.h)</span>
<span class="quote">&gt; &gt;&gt; &gt;   */</span>
<span class="quote">&gt; &gt;&gt; &gt;  struct dev_pagemap {</span>
<span class="quote">&gt; &gt;&gt; &gt; +       dev_page_fault_t page_fault;</span>
<span class="quote">&gt; &gt;&gt; &gt;         dev_page_free_t page_free;</span>
<span class="quote">&gt; &gt;&gt; &gt;         struct vmem_altmap *altmap;</span>
<span class="quote">&gt; &gt;&gt; &gt;         const struct resource *res;</span>
<span class="quote">&gt; &gt;&gt; &gt;         struct percpu_ref *ref;</span>
<span class="quote">&gt; &gt;&gt; &gt;         struct device *dev;</span>
<span class="quote">&gt; &gt;&gt; &gt;         void *data;</span>
<span class="quote">&gt; &gt;&gt; &gt; +       int flags;</span>
<span class="quote">&gt; &gt;&gt; &gt;  };</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; dev_pagemap is only meant for get_user_pages() to do lookups of ptes</span>
<span class="quote">&gt; &gt;&gt; with _PAGE_DEVMAP and take a reference against the hosting device..</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; And i want to build on top of that to extend _PAGE_DEVMAP to support</span>
<span class="quote">&gt; &gt; a new usecase for unaddressable device memory.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Why can&#39;t HMM use the typical vm_operations_struct fault path and push</span>
<span class="quote">&gt; &gt;&gt; more of these details to a driver rather than the core?</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Because the vm_operations_struct has nothing to do with the device.</span>
<span class="quote">&gt; &gt; We are talking about regular vma here. Think malloc, mmap, share</span>
<span class="quote">&gt; &gt; memory, ...  not about mmap(/dev/thedevice,...)</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; So the vm_operations_struct is never under device control and we can</span>
<span class="quote">&gt; &gt; not, nor want to, rely on that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can you explain more what&#39;s behind that &quot;can not, nor want to&quot;</span>
<span class="quote">&gt; statement? It seems to me that any awkwardness of moving to a</span>
<span class="quote">&gt; standalone device file interface is less than a maintaining a new /</span>
<span class="quote">&gt; parallel mm fault path through dev_pagemap.</span>

The whole point of HMM is to allow transparent usage of process address
space on to a device like GPU. So it imply any vma (vm_area_struct) that
result from usual mmap (ie any mmap either PRIVATE or SHARE as long as it
is not a an mmap of a device file).

It means that application can use malloc or the usual memory allocation
primitive of the langage (c++, rust, python, ...) and directly use the
memory it gets from that with the device.

Device like GPU have a large pool of device memory that is not accessible
by the CPU. This device memory has 10 times more bandwidth than system
memory and has better latency then PCIE. Hence for the whole thing to
make sense you need to allow to use it.

For that you need to allow migration from system memory to device memory.
Because you can not rely on special userspace allocator you have to
assume that the vma (vm_area_struct) is a regular one. So we are left
with having struct page for the device memory to allow migration to
work without requiring too much changes to existing mm.

Because device memory is not accessible by the CPU, you can not allow
anyone to pin it and thus get_user_page* must trigger a migration back
as CPU page fault would.
<span class="quote">

&gt; &gt; So what we looking for here is struct page that can behave mostly</span>
<span class="quote">&gt; &gt; like anyother except that we do not want to allow GUP to take a</span>
<span class="quote">&gt; &gt; reference almost exactly what ZONE_DEVICE already provide.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; So do you have any fundamental objections to this patchset ? And if</span>
<span class="quote">&gt; &gt; so, how do you propose i solve the problem i am trying to address ?</span>
<span class="quote">&gt; &gt; Because hardware exist today and without something like HMM we will</span>
<span class="quote">&gt; &gt; not be able to support such hardware.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; My pushback stems from it being a completely different use case for</span>
<span class="quote">&gt; devm_memremap_pages(), as evidenced by it growing from 4 arguments to</span>
<span class="quote">&gt; 9, and the ongoing maintenance overhead of understanding HMM</span>
<span class="quote">&gt; requirements when updating the pmem usage of ZONE_DEVICE.</span>

I rather reuse something existing and modify it to support more use case
than try to add ZONE_DEVICE2 or ZONE_DEVICE_I_AM_DIFFERENT. I have made
sure that my modifications to ZONE_DEVICE can be use without HMM. It is
just a generic interface to support page fault and to allow to track last
user of a device page. Both can be use indepentently from each other.

To me the whole point of kernel is trying to share infrastructure accross
as many hardware as possible and i am doing just that. I do not think HMM
should be block because something that use to be for one specific use case
now support 2 use cases. I am not breaking anything existing. Is it more
work for you ? Maybe, but at Red Hat we intend to support it for as long
as it is needed so you always have some one to talk to if you want to
update ZONE_DEVICE.

Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=320">Dan Williams</a> - Jan. 17, 2017, 12:58 a.m.</div>
<pre class="content">
On Mon, Jan 16, 2017 at 12:13 PM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:
<span class="quote">&gt; On Mon, Jan 16, 2017 at 11:31:39AM -0800, Dan Williams wrote:</span>
[..]
<span class="quote">&gt;&gt; &gt;&gt; dev_pagemap is only meant for get_user_pages() to do lookups of ptes</span>
<span class="quote">&gt;&gt; &gt;&gt; with _PAGE_DEVMAP and take a reference against the hosting device..</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; And i want to build on top of that to extend _PAGE_DEVMAP to support</span>
<span class="quote">&gt;&gt; &gt; a new usecase for unaddressable device memory.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; Why can&#39;t HMM use the typical vm_operations_struct fault path and push</span>
<span class="quote">&gt;&gt; &gt;&gt; more of these details to a driver rather than the core?</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Because the vm_operations_struct has nothing to do with the device.</span>
<span class="quote">&gt;&gt; &gt; We are talking about regular vma here. Think malloc, mmap, share</span>
<span class="quote">&gt;&gt; &gt; memory, ...  not about mmap(/dev/thedevice,...)</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; So the vm_operations_struct is never under device control and we can</span>
<span class="quote">&gt;&gt; &gt; not, nor want to, rely on that.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Can you explain more what&#39;s behind that &quot;can not, nor want to&quot;</span>
<span class="quote">&gt;&gt; statement? It seems to me that any awkwardness of moving to a</span>
<span class="quote">&gt;&gt; standalone device file interface is less than a maintaining a new /</span>
<span class="quote">&gt;&gt; parallel mm fault path through dev_pagemap.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; The whole point of HMM is to allow transparent usage of process address</span>
<span class="quote">&gt; space on to a device like GPU. So it imply any vma (vm_area_struct) that</span>
<span class="quote">&gt; result from usual mmap (ie any mmap either PRIVATE or SHARE as long as it</span>
<span class="quote">&gt; is not a an mmap of a device file).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; It means that application can use malloc or the usual memory allocation</span>
<span class="quote">&gt; primitive of the langage (c++, rust, python, ...) and directly use the</span>
<span class="quote">&gt; memory it gets from that with the device.</span>

So you need 100% support of all these mm paths for this hardware to be
useful at all? Does a separate device-driver and a userpace helper
library get you something like 80% of the functionality and then we
can debate the core mm changes to get the final 20%? Or am I just
completely off base with how people want to use this hardware?
<span class="quote">
&gt; Device like GPU have a large pool of device memory that is not accessible</span>
<span class="quote">&gt; by the CPU. This device memory has 10 times more bandwidth than system</span>
<span class="quote">&gt; memory and has better latency then PCIE. Hence for the whole thing to</span>
<span class="quote">&gt; make sense you need to allow to use it.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; For that you need to allow migration from system memory to device memory.</span>
<span class="quote">&gt; Because you can not rely on special userspace allocator you have to</span>
<span class="quote">&gt; assume that the vma (vm_area_struct) is a regular one. So we are left</span>
<span class="quote">&gt; with having struct page for the device memory to allow migration to</span>
<span class="quote">&gt; work without requiring too much changes to existing mm.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Because device memory is not accessible by the CPU, you can not allow</span>
<span class="quote">&gt; anyone to pin it and thus get_user_page* must trigger a migration back</span>
<span class="quote">&gt; as CPU page fault would.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; &gt; So what we looking for here is struct page that can behave mostly</span>
<span class="quote">&gt;&gt; &gt; like anyother except that we do not want to allow GUP to take a</span>
<span class="quote">&gt;&gt; &gt; reference almost exactly what ZONE_DEVICE already provide.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; So do you have any fundamental objections to this patchset ? And if</span>
<span class="quote">&gt;&gt; &gt; so, how do you propose i solve the problem i am trying to address ?</span>
<span class="quote">&gt;&gt; &gt; Because hardware exist today and without something like HMM we will</span>
<span class="quote">&gt;&gt; &gt; not be able to support such hardware.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; My pushback stems from it being a completely different use case for</span>
<span class="quote">&gt;&gt; devm_memremap_pages(), as evidenced by it growing from 4 arguments to</span>
<span class="quote">&gt;&gt; 9, and the ongoing maintenance overhead of understanding HMM</span>
<span class="quote">&gt;&gt; requirements when updating the pmem usage of ZONE_DEVICE.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I rather reuse something existing and modify it to support more use case</span>
<span class="quote">&gt; than try to add ZONE_DEVICE2 or ZONE_DEVICE_I_AM_DIFFERENT. I have made</span>
<span class="quote">&gt; sure that my modifications to ZONE_DEVICE can be use without HMM. It is</span>
<span class="quote">&gt; just a generic interface to support page fault and to allow to track last</span>
<span class="quote">&gt; user of a device page. Both can be use indepentently from each other.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To me the whole point of kernel is trying to share infrastructure accross</span>
<span class="quote">&gt; as many hardware as possible and i am doing just that. I do not think HMM</span>
<span class="quote">&gt; should be block because something that use to be for one specific use case</span>
<span class="quote">&gt; now support 2 use cases. I am not breaking anything existing. Is it more</span>
<span class="quote">&gt; work for you ? Maybe, but at Red Hat we intend to support it for as long</span>
<span class="quote">&gt; as it is needed so you always have some one to talk to if you want to</span>
<span class="quote">&gt; update ZONE_DEVICE.</span>

Sharing infrastructure should not come at the expense of type safety
and clear usage rules.

For example the pmem case, before exposing ZONE_DEVICE memory to other
parts of the kernel, introduced the pfn_t type to distinguish DMA
capable pfns from other raw pfns. All programmatic ways of discovering
if a pmem range can support DMA use this type and explicit flags.

While we may not need ZONE_DEVICE2 we obviously need a different
wrapper around arch_add_memory() than devm_memremap_pages() for HMM
and likely a different physical address radix than pgmap_radix because
they are servicing 2 distinct purposes. For example, I don&#39;t think HMM
should be using unmodified arch_add_memory(). We shouldn&#39;t add
unaddressable memory to the linear address mappings when we know there
is nothing behind it, especially when it seems all you need from
arch_add_memory() is pfn_to_page() to be valid.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=2554">Jerome Glisse</a> - Jan. 17, 2017, 2 a.m.</div>
<pre class="content">
On Mon, Jan 16, 2017 at 04:58:24PM -0800, Dan Williams wrote:
<span class="quote">&gt; On Mon, Jan 16, 2017 at 12:13 PM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt; &gt; On Mon, Jan 16, 2017 at 11:31:39AM -0800, Dan Williams wrote:</span>
<span class="quote">&gt; [..]</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; dev_pagemap is only meant for get_user_pages() to do lookups of ptes</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; with _PAGE_DEVMAP and take a reference against the hosting device..</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; And i want to build on top of that to extend _PAGE_DEVMAP to support</span>
<span class="quote">&gt; &gt;&gt; &gt; a new usecase for unaddressable device memory.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; Why can&#39;t HMM use the typical vm_operations_struct fault path and push</span>
<span class="quote">&gt; &gt;&gt; &gt;&gt; more of these details to a driver rather than the core?</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; Because the vm_operations_struct has nothing to do with the device.</span>
<span class="quote">&gt; &gt;&gt; &gt; We are talking about regular vma here. Think malloc, mmap, share</span>
<span class="quote">&gt; &gt;&gt; &gt; memory, ...  not about mmap(/dev/thedevice,...)</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; So the vm_operations_struct is never under device control and we can</span>
<span class="quote">&gt; &gt;&gt; &gt; not, nor want to, rely on that.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; Can you explain more what&#39;s behind that &quot;can not, nor want to&quot;</span>
<span class="quote">&gt; &gt;&gt; statement? It seems to me that any awkwardness of moving to a</span>
<span class="quote">&gt; &gt;&gt; standalone device file interface is less than a maintaining a new /</span>
<span class="quote">&gt; &gt;&gt; parallel mm fault path through dev_pagemap.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; The whole point of HMM is to allow transparent usage of process address</span>
<span class="quote">&gt; &gt; space on to a device like GPU. So it imply any vma (vm_area_struct) that</span>
<span class="quote">&gt; &gt; result from usual mmap (ie any mmap either PRIVATE or SHARE as long as it</span>
<span class="quote">&gt; &gt; is not a an mmap of a device file).</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; It means that application can use malloc or the usual memory allocation</span>
<span class="quote">&gt; &gt; primitive of the langage (c++, rust, python, ...) and directly use the</span>
<span class="quote">&gt; &gt; memory it gets from that with the device.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So you need 100% support of all these mm paths for this hardware to be</span>
<span class="quote">&gt; useful at all? Does a separate device-driver and a userpace helper</span>
<span class="quote">&gt; library get you something like 80% of the functionality and then we</span>
<span class="quote">&gt; can debate the core mm changes to get the final 20%? Or am I just</span>
<span class="quote">&gt; completely off base with how people want to use this hardware?</span>

Can&#39;t do that. Think library want to use GPU but you do not want to update
every single program that use that library and library get its memory from
the application. This is just one scenario. Then you have mmaped file, or
share memory, ...

Transparent address space is where the industry is moving and sadly on some
platform (like Intel) we can not rely on hardware to solve it for us.
<span class="quote">

&gt; &gt; Device like GPU have a large pool of device memory that is not accessible</span>
<span class="quote">&gt; &gt; by the CPU. This device memory has 10 times more bandwidth than system</span>
<span class="quote">&gt; &gt; memory and has better latency then PCIE. Hence for the whole thing to</span>
<span class="quote">&gt; &gt; make sense you need to allow to use it.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; For that you need to allow migration from system memory to device memory.</span>
<span class="quote">&gt; &gt; Because you can not rely on special userspace allocator you have to</span>
<span class="quote">&gt; &gt; assume that the vma (vm_area_struct) is a regular one. So we are left</span>
<span class="quote">&gt; &gt; with having struct page for the device memory to allow migration to</span>
<span class="quote">&gt; &gt; work without requiring too much changes to existing mm.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; Because device memory is not accessible by the CPU, you can not allow</span>
<span class="quote">&gt; &gt; anyone to pin it and thus get_user_page* must trigger a migration back</span>
<span class="quote">&gt; &gt; as CPU page fault would.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; So what we looking for here is struct page that can behave mostly</span>
<span class="quote">&gt; &gt;&gt; &gt; like anyother except that we do not want to allow GUP to take a</span>
<span class="quote">&gt; &gt;&gt; &gt; reference almost exactly what ZONE_DEVICE already provide.</span>
<span class="quote">&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt; &gt;&gt; &gt; So do you have any fundamental objections to this patchset ? And if</span>
<span class="quote">&gt; &gt;&gt; &gt; so, how do you propose i solve the problem i am trying to address ?</span>
<span class="quote">&gt; &gt;&gt; &gt; Because hardware exist today and without something like HMM we will</span>
<span class="quote">&gt; &gt;&gt; &gt; not be able to support such hardware.</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; My pushback stems from it being a completely different use case for</span>
<span class="quote">&gt; &gt;&gt; devm_memremap_pages(), as evidenced by it growing from 4 arguments to</span>
<span class="quote">&gt; &gt;&gt; 9, and the ongoing maintenance overhead of understanding HMM</span>
<span class="quote">&gt; &gt;&gt; requirements when updating the pmem usage of ZONE_DEVICE.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; I rather reuse something existing and modify it to support more use case</span>
<span class="quote">&gt; &gt; than try to add ZONE_DEVICE2 or ZONE_DEVICE_I_AM_DIFFERENT. I have made</span>
<span class="quote">&gt; &gt; sure that my modifications to ZONE_DEVICE can be use without HMM. It is</span>
<span class="quote">&gt; &gt; just a generic interface to support page fault and to allow to track last</span>
<span class="quote">&gt; &gt; user of a device page. Both can be use indepentently from each other.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; To me the whole point of kernel is trying to share infrastructure accross</span>
<span class="quote">&gt; &gt; as many hardware as possible and i am doing just that. I do not think HMM</span>
<span class="quote">&gt; &gt; should be block because something that use to be for one specific use case</span>
<span class="quote">&gt; &gt; now support 2 use cases. I am not breaking anything existing. Is it more</span>
<span class="quote">&gt; &gt; work for you ? Maybe, but at Red Hat we intend to support it for as long</span>
<span class="quote">&gt; &gt; as it is needed so you always have some one to talk to if you want to</span>
<span class="quote">&gt; &gt; update ZONE_DEVICE.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sharing infrastructure should not come at the expense of type safety</span>
<span class="quote">&gt; and clear usage rules.</span>

And where exactly do i violate that ?
<span class="quote"> 
&gt; For example the pmem case, before exposing ZONE_DEVICE memory to other</span>
<span class="quote">&gt; parts of the kernel, introduced the pfn_t type to distinguish DMA</span>
<span class="quote">&gt; capable pfns from other raw pfns. All programmatic ways of discovering</span>
<span class="quote">&gt; if a pmem range can support DMA use this type and explicit flags.</span>

I am protected from this because i do not allow GUP. GUP trigger migration
back to regular system memory.
<span class="quote">
&gt; </span>
<span class="quote">&gt; While we may not need ZONE_DEVICE2 we obviously need a different</span>
<span class="quote">&gt; wrapper around arch_add_memory() than devm_memremap_pages() for HMM</span>
<span class="quote">&gt; and likely a different physical address radix than pgmap_radix because</span>
<span class="quote">&gt; they are servicing 2 distinct purposes. For example, I don&#39;t think HMM</span>
<span class="quote">&gt; should be using unmodified arch_add_memory(). We shouldn&#39;t add</span>
<span class="quote">&gt; unaddressable memory to the linear address mappings when we know there</span>
<span class="quote">&gt; is nothing behind it, especially when it seems all you need from</span>
<span class="quote">&gt; arch_add_memory() is pfn_to_page() to be valid.</span>

And my patchset does just that, i do not add the device pfn to the linear
mapping because there is nothing there. In arch_add_memory() x86, ppc, arm
do barely more than setting up linear mapping and adding struct page. So
instead of splitting in two this function i just made the linear mapping
conditional.

I can split HMM from devm_memremap_pages() and thus from a different
pgmap_radix. You have to understand that this will not change most of
my patchset.


Cheers,
Jérôme
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=320">Dan Williams</a> - Jan. 17, 2017, 2:57 a.m.</div>
<pre class="content">
On Mon, Jan 16, 2017 at 6:00 PM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:
<span class="quote">&gt; On Mon, Jan 16, 2017 at 04:58:24PM -0800, Dan Williams wrote:</span>
<span class="quote">&gt;&gt; On Mon, Jan 16, 2017 at 12:13 PM, Jerome Glisse &lt;jglisse@redhat.com&gt; wrote:</span>
<span class="quote">&gt;&gt; &gt; On Mon, Jan 16, 2017 at 11:31:39AM -0800, Dan Williams wrote:</span>
<span class="quote">&gt;&gt; [..]</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt; dev_pagemap is only meant for get_user_pages() to do lookups of ptes</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt; with _PAGE_DEVMAP and take a reference against the hosting device..</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; And i want to build on top of that to extend _PAGE_DEVMAP to support</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; a new usecase for unaddressable device memory.</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt; Why can&#39;t HMM use the typical vm_operations_struct fault path and push</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;&gt; more of these details to a driver rather than the core?</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; Because the vm_operations_struct has nothing to do with the device.</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; We are talking about regular vma here. Think malloc, mmap, share</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; memory, ...  not about mmap(/dev/thedevice,...)</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; So the vm_operations_struct is never under device control and we can</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; not, nor want to, rely on that.</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; Can you explain more what&#39;s behind that &quot;can not, nor want to&quot;</span>
<span class="quote">&gt;&gt; &gt;&gt; statement? It seems to me that any awkwardness of moving to a</span>
<span class="quote">&gt;&gt; &gt;&gt; standalone device file interface is less than a maintaining a new /</span>
<span class="quote">&gt;&gt; &gt;&gt; parallel mm fault path through dev_pagemap.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; The whole point of HMM is to allow transparent usage of process address</span>
<span class="quote">&gt;&gt; &gt; space on to a device like GPU. So it imply any vma (vm_area_struct) that</span>
<span class="quote">&gt;&gt; &gt; result from usual mmap (ie any mmap either PRIVATE or SHARE as long as it</span>
<span class="quote">&gt;&gt; &gt; is not a an mmap of a device file).</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; It means that application can use malloc or the usual memory allocation</span>
<span class="quote">&gt;&gt; &gt; primitive of the langage (c++, rust, python, ...) and directly use the</span>
<span class="quote">&gt;&gt; &gt; memory it gets from that with the device.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; So you need 100% support of all these mm paths for this hardware to be</span>
<span class="quote">&gt;&gt; useful at all? Does a separate device-driver and a userpace helper</span>
<span class="quote">&gt;&gt; library get you something like 80% of the functionality and then we</span>
<span class="quote">&gt;&gt; can debate the core mm changes to get the final 20%? Or am I just</span>
<span class="quote">&gt;&gt; completely off base with how people want to use this hardware?</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Can&#39;t do that. Think library want to use GPU but you do not want to update</span>
<span class="quote">&gt; every single program that use that library and library get its memory from</span>
<span class="quote">&gt; the application. This is just one scenario. Then you have mmaped file, or</span>
<span class="quote">&gt; share memory, ...</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Transparent address space is where the industry is moving and sadly on some</span>
<span class="quote">&gt; platform (like Intel) we can not rely on hardware to solve it for us.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt; &gt; Device like GPU have a large pool of device memory that is not accessible</span>
<span class="quote">&gt;&gt; &gt; by the CPU. This device memory has 10 times more bandwidth than system</span>
<span class="quote">&gt;&gt; &gt; memory and has better latency then PCIE. Hence for the whole thing to</span>
<span class="quote">&gt;&gt; &gt; make sense you need to allow to use it.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; For that you need to allow migration from system memory to device memory.</span>
<span class="quote">&gt;&gt; &gt; Because you can not rely on special userspace allocator you have to</span>
<span class="quote">&gt;&gt; &gt; assume that the vma (vm_area_struct) is a regular one. So we are left</span>
<span class="quote">&gt;&gt; &gt; with having struct page for the device memory to allow migration to</span>
<span class="quote">&gt;&gt; &gt; work without requiring too much changes to existing mm.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; Because device memory is not accessible by the CPU, you can not allow</span>
<span class="quote">&gt;&gt; &gt; anyone to pin it and thus get_user_page* must trigger a migration back</span>
<span class="quote">&gt;&gt; &gt; as CPU page fault would.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; So what we looking for here is struct page that can behave mostly</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; like anyother except that we do not want to allow GUP to take a</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; reference almost exactly what ZONE_DEVICE already provide.</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; So do you have any fundamental objections to this patchset ? And if</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; so, how do you propose i solve the problem i am trying to address ?</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; Because hardware exist today and without something like HMM we will</span>
<span class="quote">&gt;&gt; &gt;&gt; &gt; not be able to support such hardware.</span>
<span class="quote">&gt;&gt; &gt;&gt;</span>
<span class="quote">&gt;&gt; &gt;&gt; My pushback stems from it being a completely different use case for</span>
<span class="quote">&gt;&gt; &gt;&gt; devm_memremap_pages(), as evidenced by it growing from 4 arguments to</span>
<span class="quote">&gt;&gt; &gt;&gt; 9, and the ongoing maintenance overhead of understanding HMM</span>
<span class="quote">&gt;&gt; &gt;&gt; requirements when updating the pmem usage of ZONE_DEVICE.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; I rather reuse something existing and modify it to support more use case</span>
<span class="quote">&gt;&gt; &gt; than try to add ZONE_DEVICE2 or ZONE_DEVICE_I_AM_DIFFERENT. I have made</span>
<span class="quote">&gt;&gt; &gt; sure that my modifications to ZONE_DEVICE can be use without HMM. It is</span>
<span class="quote">&gt;&gt; &gt; just a generic interface to support page fault and to allow to track last</span>
<span class="quote">&gt;&gt; &gt; user of a device page. Both can be use indepentently from each other.</span>
<span class="quote">&gt;&gt; &gt;</span>
<span class="quote">&gt;&gt; &gt; To me the whole point of kernel is trying to share infrastructure accross</span>
<span class="quote">&gt;&gt; &gt; as many hardware as possible and i am doing just that. I do not think HMM</span>
<span class="quote">&gt;&gt; &gt; should be block because something that use to be for one specific use case</span>
<span class="quote">&gt;&gt; &gt; now support 2 use cases. I am not breaking anything existing. Is it more</span>
<span class="quote">&gt;&gt; &gt; work for you ? Maybe, but at Red Hat we intend to support it for as long</span>
<span class="quote">&gt;&gt; &gt; as it is needed so you always have some one to talk to if you want to</span>
<span class="quote">&gt;&gt; &gt; update ZONE_DEVICE.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Sharing infrastructure should not come at the expense of type safety</span>
<span class="quote">&gt;&gt; and clear usage rules.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And where exactly do i violate that ?</span>

It&#39;s hard to judge without a user. For example, we found that fs/dax.c
was violating block device safety lifetime rules that we solved with
dax_map_atomic(), but that couldn&#39;t have been done without seeing both
sides of the interface.

...but as I say that I&#39;m aware that I don&#39;t have the background in
graphics memory management like I do the block stack to review the
usages.
<span class="quote">
&gt;&gt; For example the pmem case, before exposing ZONE_DEVICE memory to other</span>
<span class="quote">&gt;&gt; parts of the kernel, introduced the pfn_t type to distinguish DMA</span>
<span class="quote">&gt;&gt; capable pfns from other raw pfns. All programmatic ways of discovering</span>
<span class="quote">&gt;&gt; if a pmem range can support DMA use this type and explicit flags.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; I am protected from this because i do not allow GUP. GUP trigger migration</span>
<span class="quote">&gt; back to regular system memory.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; While we may not need ZONE_DEVICE2 we obviously need a different</span>
<span class="quote">&gt;&gt; wrapper around arch_add_memory() than devm_memremap_pages() for HMM</span>
<span class="quote">&gt;&gt; and likely a different physical address radix than pgmap_radix because</span>
<span class="quote">&gt;&gt; they are servicing 2 distinct purposes. For example, I don&#39;t think HMM</span>
<span class="quote">&gt;&gt; should be using unmodified arch_add_memory(). We shouldn&#39;t add</span>
<span class="quote">&gt;&gt; unaddressable memory to the linear address mappings when we know there</span>
<span class="quote">&gt;&gt; is nothing behind it, especially when it seems all you need from</span>
<span class="quote">&gt;&gt; arch_add_memory() is pfn_to_page() to be valid.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; And my patchset does just that, i do not add the device pfn to the linear</span>
<span class="quote">&gt; mapping because there is nothing there. In arch_add_memory() x86, ppc, arm</span>
<span class="quote">&gt; do barely more than setting up linear mapping and adding struct page. So</span>
<span class="quote">&gt; instead of splitting in two this function i just made the linear mapping</span>
<span class="quote">&gt; conditional.</span>

Sorry, I missed that.
<span class="quote">
&gt; I can split HMM from devm_memremap_pages() and thus from a different</span>
<span class="quote">&gt; pgmap_radix. You have to understand that this will not change most of</span>
<span class="quote">&gt; my patchset.</span>
<span class="quote">&gt;</span>

Sure, but I think it would worth it from a readability / maintenance
perspective. With HMM being a superset of the existing dev_pagemap()
usage it might make sense to just use struct dev_pagemap as a
sub-structure of the hmm data.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/drivers/dax/pmem.c b/drivers/dax/pmem.c</span>
<span class="p_header">index 66af7b1..c50b58d 100644</span>
<span class="p_header">--- a/drivers/dax/pmem.c</span>
<span class="p_header">+++ b/drivers/dax/pmem.c</span>
<span class="p_chunk">@@ -111,8 +111,8 @@</span> <span class="p_context"> static int dax_pmem_probe(struct device *dev)</span>
 	if (rc)
 		return rc;
 
<span class="p_del">-	addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref,</span>
<span class="p_del">-				   altmap, NULL, NULL);</span>
<span class="p_add">+	addr = devm_memremap_pages(dev, &amp;res, &amp;dax_pmem-&gt;ref, altmap,</span>
<span class="p_add">+				   NULL, NULL, NULL, NULL, MEMORY_DEVICE);</span>
 	if (IS_ERR(addr))
 		return PTR_ERR(addr);
 
<span class="p_header">diff --git a/drivers/nvdimm/pmem.c b/drivers/nvdimm/pmem.c</span>
<span class="p_header">index f2f1904..8166a56 100644</span>
<span class="p_header">--- a/drivers/nvdimm/pmem.c</span>
<span class="p_header">+++ b/drivers/nvdimm/pmem.c</span>
<span class="p_chunk">@@ -282,7 +282,8 @@</span> <span class="p_context"> static int pmem_attach_disk(struct device *dev,</span>
 	pmem-&gt;pfn_flags = PFN_DEV;
 	if (is_nd_pfn(dev)) {
 		addr = devm_memremap_pages(dev, &amp;pfn_res, &amp;q-&gt;q_usage_counter,
<span class="p_del">-					   altmap, NULL, NULL);</span>
<span class="p_add">+					   altmap, NULL, NULL, NULL,</span>
<span class="p_add">+					   NULL, MEMORY_DEVICE);</span>
 		pfn_sb = nd_pfn-&gt;pfn_sb;
 		pmem-&gt;data_offset = le64_to_cpu(pfn_sb-&gt;dataoff);
 		pmem-&gt;pfn_pad = resource_size(res) - resource_size(&amp;pfn_res);
<span class="p_chunk">@@ -292,7 +293,8 @@</span> <span class="p_context"> static int pmem_attach_disk(struct device *dev,</span>
 	} else if (pmem_should_map_pages(dev)) {
 		addr = devm_memremap_pages(dev, &amp;nsio-&gt;res,
 					   &amp;q-&gt;q_usage_counter,
<span class="p_del">-					   NULL, NULL, NULL);</span>
<span class="p_add">+					   NULL, NULL, NULL, NULL,</span>
<span class="p_add">+					   NULL, MEMORY_DEVICE);</span>
 		pmem-&gt;pfn_flags |= PFN_MAP;
 	} else
 		addr = devm_memremap(dev, pmem-&gt;phys_addr,
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 958f325..9a6ab71 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -535,8 +535,11 @@</span> <span class="p_context"> static void smaps_pte_entry(pte_t *pte, unsigned long addr,</span>
 			} else {
 				mss-&gt;swap_pss += (u64)PAGE_SIZE &lt;&lt; PSS_SHIFT;
 			}
<span class="p_del">-		} else if (is_migration_entry(swpent))</span>
<span class="p_add">+		} else if (is_migration_entry(swpent)) {</span>
 			page = migration_entry_to_page(swpent);
<span class="p_add">+		} else if (is_device_entry(swpent)) {</span>
<span class="p_add">+			page = device_entry_to_page(swpent);</span>
<span class="p_add">+		}</span>
 	} else if (unlikely(IS_ENABLED(CONFIG_SHMEM) &amp;&amp; mss-&gt;check_shmem_swap
 							&amp;&amp; pte_none(*pte))) {
 		page = find_get_entry(vma-&gt;vm_file-&gt;f_mapping,
<span class="p_chunk">@@ -699,6 +702,8 @@</span> <span class="p_context"> static int smaps_hugetlb_range(pte_t *pte, unsigned long hmask,</span>
 
 		if (is_migration_entry(swpent))
 			page = migration_entry_to_page(swpent);
<span class="p_add">+		if (is_device_entry(swpent))</span>
<span class="p_add">+			page = device_entry_to_page(swpent);</span>
 	}
 	if (page) {
 		int mapcount = page_mapcount(page);
<span class="p_chunk">@@ -1182,6 +1187,9 @@</span> <span class="p_context"> static pagemap_entry_t pte_to_pagemap_entry(struct pagemapread *pm,</span>
 		flags |= PM_SWAP;
 		if (is_migration_entry(entry))
 			page = migration_entry_to_page(entry);
<span class="p_add">+</span>
<span class="p_add">+		if (is_device_entry(entry))</span>
<span class="p_add">+			page = device_entry_to_page(entry);</span>
 	}
 
 	if (page &amp;&amp; !PageAnon(page))
<span class="p_header">diff --git a/include/linux/ioport.h b/include/linux/ioport.h</span>
<span class="p_header">index 6230064..d154a18 100644</span>
<span class="p_header">--- a/include/linux/ioport.h</span>
<span class="p_header">+++ b/include/linux/ioport.h</span>
<span class="p_chunk">@@ -130,6 +130,7 @@</span> <span class="p_context"> enum {</span>
 	IORES_DESC_ACPI_NV_STORAGE		= 3,
 	IORES_DESC_PERSISTENT_MEMORY		= 4,
 	IORES_DESC_PERSISTENT_MEMORY_LEGACY	= 5,
<span class="p_add">+	IORES_DESC_UNADDRESSABLE_MEMORY		= 6,</span>
 };
 
 /* helpers to define resources */
<span class="p_header">diff --git a/include/linux/memory_hotplug.h b/include/linux/memory_hotplug.h</span>
<span class="p_header">index 3f50eb8..e7c5dc6 100644</span>
<span class="p_header">--- a/include/linux/memory_hotplug.h</span>
<span class="p_header">+++ b/include/linux/memory_hotplug.h</span>
<span class="p_chunk">@@ -285,15 +285,22 @@</span> <span class="p_context"> extern int zone_for_memory(int nid, u64 start, u64 size, int zone_default,</span>
  * never relied on struct page migration so far and new user of might also
  * prefer avoiding struct page migration.
  *
<span class="p_add">+ * For device memory (which use ZONE_DEVICE) we want differentiate between CPU</span>
<span class="p_add">+ * accessible memory (persitent memory, device memory on an architecture with a</span>
<span class="p_add">+ * system bus that allow transparent access to device memory) and unaddressable</span>
<span class="p_add">+ * memory (device memory that can not be accessed by CPU directly).</span>
<span class="p_add">+ *</span>
  * New non device memory specific flags can be added if ever needed.
  *
  * MEMORY_REGULAR: regular system memory
  * DEVICE_MEMORY: device memory create a ZONE_DEVICE zone for it
  * DEVICE_MEMORY_ALLOW_MIGRATE: page in that device memory ca be migrated
<span class="p_add">+ * MEMORY_DEVICE_UNADDRESSABLE: un-addressable memory (CPU can not access it)</span>
  */
 #define MEMORY_NORMAL 0
 #define MEMORY_DEVICE (1 &lt;&lt; 0)
 #define MEMORY_DEVICE_ALLOW_MIGRATE (1 &lt;&lt; 1)
<span class="p_add">+#define MEMORY_DEVICE_UNADDRESSABLE (1 &lt;&lt; 2)</span>
 
 extern int arch_add_memory(int nid, u64 start, u64 size, int flags);
 extern int offline_pages(unsigned long start_pfn, unsigned long nr_pages);
<span class="p_header">diff --git a/include/linux/memremap.h b/include/linux/memremap.h</span>
<span class="p_header">index 582561f..4b9f02c 100644</span>
<span class="p_header">--- a/include/linux/memremap.h</span>
<span class="p_header">+++ b/include/linux/memremap.h</span>
<span class="p_chunk">@@ -35,31 +35,42 @@</span> <span class="p_context"> static inline struct vmem_altmap *to_vmem_altmap(unsigned long memmap_start)</span>
 }
 #endif
 
<span class="p_add">+typedef int (*dev_page_fault_t)(struct vm_area_struct *vma,</span>
<span class="p_add">+				unsigned long addr,</span>
<span class="p_add">+				struct page *page,</span>
<span class="p_add">+				unsigned flags,</span>
<span class="p_add">+				pmd_t *pmdp);</span>
 typedef void (*dev_page_free_t)(struct page *page, void *data);
 
 /**
  * struct dev_pagemap - metadata for ZONE_DEVICE mappings
<span class="p_add">+ * @page_fault: callback when CPU fault on an un-addressable device page</span>
  * @page_free: free page callback when page refcount reach 1
  * @altmap: pre-allocated/reserved memory for vmemmap allocations
  * @res: physical address range covered by @ref
  * @ref: reference count that pins the devm_memremap_pages() mapping
  * @dev: host device of the mapping for debug
  * @data: privata data pointer for page_free
<span class="p_add">+ * @flags: device memory flags (look for MEMORY_DEVICE_* memory_hotplug.h)</span>
  */
 struct dev_pagemap {
<span class="p_add">+	dev_page_fault_t page_fault;</span>
 	dev_page_free_t page_free;
 	struct vmem_altmap *altmap;
 	const struct resource *res;
 	struct percpu_ref *ref;
 	struct device *dev;
 	void *data;
<span class="p_add">+	int flags;</span>
 };
 
 #ifdef CONFIG_ZONE_DEVICE
 void *devm_memremap_pages(struct device *dev, struct resource *res,
 			  struct percpu_ref *ref, struct vmem_altmap *altmap,
<span class="p_add">+			  struct dev_pagemap **ppgmap,</span>
<span class="p_add">+			  dev_page_fault_t page_fault,</span>
 			  dev_page_free_t page_free,
<span class="p_del">-			  void *data);</span>
<span class="p_add">+			  void *data, int flags);</span>
 struct dev_pagemap *find_dev_pagemap(resource_size_t phys);
 int devm_memunmap_pages(struct device *dev, void *start);
 
<span class="p_chunk">@@ -68,13 +79,22 @@</span> <span class="p_context"> static inline bool dev_page_allow_migrate(const struct page *page)</span>
 	return ((page_zonenum(page) == ZONE_DEVICE) &amp;&amp;
 		(page-&gt;pgmap-&gt;flags &amp; MEMORY_DEVICE_ALLOW_MIGRATE));
 }
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_addressable_page(const struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return ((page_zonenum(page) != ZONE_DEVICE) ||</span>
<span class="p_add">+		!(page-&gt;pgmap-&gt;flags &amp; MEMORY_DEVICE_UNADDRESSABLE));</span>
<span class="p_add">+}</span>
 #else
 static inline void *devm_memremap_pages(struct device *dev,
 					struct resource *res,
 					struct percpu_ref *ref,
 					struct vmem_altmap *altmap,
<span class="p_add">+					struct dev_pagemap **ppgmap,</span>
<span class="p_add">+					dev_page_fault_t page_fault,</span>
 					dev_page_free_t page_free,
<span class="p_del">-					void *data)</span>
<span class="p_add">+					void *data,</span>
<span class="p_add">+					int flags)</span>
 {
 	/*
 	 * Fail attempts to call devm_memremap_pages() without
<span class="p_chunk">@@ -99,6 +119,11 @@</span> <span class="p_context"> static inline bool dev_page_allow_migrate(const struct page *page)</span>
 {
 	return false;
 }
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_addressable_page(const struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
 #endif
 
 /**
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index 09b212d..81b44ea 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -50,6 +50,17 @@</span> <span class="p_context"> static inline int current_is_kswapd(void)</span>
  */
 
 /*
<span class="p_add">+ * Un-addressable device memory support</span>
<span class="p_add">+ */</span>
<span class="p_add">+#ifdef CONFIG_DEVICE_UNADDRESSABLE</span>
<span class="p_add">+#define SWP_DEVICE_NUM 2</span>
<span class="p_add">+#define SWP_DEVICE_WRITE (MAX_SWAPFILES + SWP_HWPOISON_NUM + SWP_MIGRATION_NUM)</span>
<span class="p_add">+#define SWP_DEVICE (MAX_SWAPFILES + SWP_HWPOISON_NUM + SWP_MIGRATION_NUM + 1)</span>
<span class="p_add">+#else</span>
<span class="p_add">+#define SWP_DEVICE_NUM 0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * NUMA node memory migration support
  */
 #ifdef CONFIG_MIGRATION
<span class="p_chunk">@@ -71,7 +82,8 @@</span> <span class="p_context"> static inline int current_is_kswapd(void)</span>
 #endif
 
 #define MAX_SWAPFILES \
<span class="p_del">-	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
<span class="p_add">+	((1 &lt;&lt; MAX_SWAPFILES_SHIFT) - SWP_DEVICE_NUM - \</span>
<span class="p_add">+	SWP_MIGRATION_NUM - SWP_HWPOISON_NUM)</span>
 
 /*
  * Magic header for a swap area. The first part of the union is
<span class="p_chunk">@@ -410,8 +422,8 @@</span> <span class="p_context"> static inline void show_swap_cache_info(void)</span>
 {
 }
 
<span class="p_del">-#define free_swap_and_cache(swp)	is_migration_entry(swp)</span>
<span class="p_del">-#define swapcache_prepare(swp)		is_migration_entry(swp)</span>
<span class="p_add">+#define free_swap_and_cache(e) (is_migration_entry(e) || is_device_entry(e))</span>
<span class="p_add">+#define swapcache_prepare(e) (is_migration_entry(e) || is_device_entry(e))</span>
 
 static inline int add_swap_count_continuation(swp_entry_t swp, gfp_t gfp_mask)
 {
<span class="p_header">diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="p_header">index 5c3a5f3..0e339f0 100644</span>
<span class="p_header">--- a/include/linux/swapops.h</span>
<span class="p_header">+++ b/include/linux/swapops.h</span>
<span class="p_chunk">@@ -100,6 +100,73 @@</span> <span class="p_context"> static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
 	return (void *)(value | RADIX_TREE_EXCEPTIONAL_ENTRY);
 }
 
<span class="p_add">+#if IS_ENABLED(CONFIG_DEVICE_UNADDRESSABLE)</span>
<span class="p_add">+static inline swp_entry_t make_device_entry(struct page *page, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swp_entry(write?SWP_DEVICE_WRITE:SWP_DEVICE, page_to_pfn(page));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int type = swp_type(entry);</span>
<span class="p_add">+	return type == SWP_DEVICE || type == SWP_DEVICE_WRITE;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void make_device_entry_read(swp_entry_t *entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	*entry = swp_entry(SWP_DEVICE, swp_offset(*entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_write_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return unlikely(swp_type(entry) == SWP_DEVICE_WRITE);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *device_entry_to_page(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+int device_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+		       unsigned long addr,</span>
<span class="p_add">+		       swp_entry_t entry,</span>
<span class="p_add">+		       unsigned flags,</span>
<span class="p_add">+		       pmd_t *pmdp);</span>
<span class="p_add">+#else /* CONFIG_DEVICE_UNADDRESSABLE */</span>
<span class="p_add">+static inline swp_entry_t make_device_entry(struct page *page, bool write)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return swp_entry(0, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void make_device_entry_read(swp_entry_t *entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline bool is_write_device_entry(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return false;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct page *device_entry_to_page(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int device_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+				     unsigned long addr,</span>
<span class="p_add">+				     swp_entry_t entry,</span>
<span class="p_add">+				     unsigned flags,</span>
<span class="p_add">+				     pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return VM_FAULT_SIGBUS;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif /* CONFIG_DEVICE_UNADDRESSABLE */</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MIGRATION
 static inline swp_entry_t make_migration_entry(struct page *page, int write)
 {
<span class="p_header">diff --git a/kernel/memremap.c b/kernel/memremap.c</span>
<span class="p_header">index 7e47e64..1f9d771 100644</span>
<span class="p_header">--- a/kernel/memremap.c</span>
<span class="p_header">+++ b/kernel/memremap.c</span>
<span class="p_chunk">@@ -18,6 +18,8 @@</span> <span class="p_context"></span>
 #include &lt;linux/io.h&gt;
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/memory_hotplug.h&gt;
<span class="p_add">+#include &lt;linux/swap.h&gt;</span>
<span class="p_add">+#include &lt;linux/swapops.h&gt;</span>
 
 #ifndef ioremap_cache
 /* temporary while we convert existing ioremap_cache users to memremap */
<span class="p_chunk">@@ -200,6 +202,21 @@</span> <span class="p_context"> void put_zone_device_page(struct page *page)</span>
 }
 EXPORT_SYMBOL(put_zone_device_page);
 
<span class="p_add">+#if IS_ENABLED(CONFIG_DEVICE_UNADDRESSABLE)</span>
<span class="p_add">+int device_entry_fault(struct vm_area_struct *vma,</span>
<span class="p_add">+		       unsigned long addr,</span>
<span class="p_add">+		       swp_entry_t entry,</span>
<span class="p_add">+		       unsigned flags,</span>
<span class="p_add">+		       pmd_t *pmdp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct page *page = device_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+	BUG_ON(!page-&gt;pgmap-&gt;page_fault);</span>
<span class="p_add">+	return page-&gt;pgmap-&gt;page_fault(vma, addr, page, flags, pmdp);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(device_entry_fault);</span>
<span class="p_add">+#endif /* CONFIG_DEVICE_UNADDRESSABLE */</span>
<span class="p_add">+</span>
 static void pgmap_radix_release(struct resource *res)
 {
 	resource_size_t key, align_start, align_size, align_end;
<span class="p_chunk">@@ -252,7 +269,7 @@</span> <span class="p_context"> static void devm_memremap_pages_release(struct device *dev, void *data)</span>
 	/* pages are dead and unused, undo the arch mapping */
 	align_start = res-&gt;start &amp; ~(SECTION_SIZE - 1);
 	align_size = ALIGN(resource_size(res), SECTION_SIZE);
<span class="p_del">-	arch_remove_memory(align_start, align_size, MEMORY_DEVICE);</span>
<span class="p_add">+	arch_remove_memory(align_start, align_size, pgmap-&gt;flags);</span>
 	untrack_pfn(NULL, PHYS_PFN(align_start), align_size);
 	pgmap_radix_release(res);
 	dev_WARN_ONCE(dev, pgmap-&gt;altmap &amp;&amp; pgmap-&gt;altmap-&gt;alloc,
<span class="p_chunk">@@ -276,8 +293,11 @@</span> <span class="p_context"> struct dev_pagemap *find_dev_pagemap(resource_size_t phys)</span>
  * @res: &quot;host memory&quot; address range
  * @ref: a live per-cpu reference count
  * @altmap: optional descriptor for allocating the memmap from @res
<span class="p_add">+ * @ppgmap: pointer set to new page dev_pagemap on success</span>
<span class="p_add">+ * @page_fault: callback for CPU page fault on un-addressable memory</span>
  * @page_free: callback call when page refcount reach 1 ie it is free
  * @data: privata data pointer for page_free
<span class="p_add">+ * @flags: device memory flags (look for MEMORY_DEVICE_* memory_hotplug.h)</span>
  *
  * Notes:
  * 1/ @ref must be &#39;live&#39; on entry and &#39;dead&#39; before devm_memunmap_pages() time
<span class="p_chunk">@@ -289,8 +309,10 @@</span> <span class="p_context"> struct dev_pagemap *find_dev_pagemap(resource_size_t phys)</span>
  */
 void *devm_memremap_pages(struct device *dev, struct resource *res,
 			  struct percpu_ref *ref, struct vmem_altmap *altmap,
<span class="p_add">+			  struct dev_pagemap **ppgmap,</span>
<span class="p_add">+			  dev_page_fault_t page_fault,</span>
 			  dev_page_free_t page_free,
<span class="p_del">-			  void *data)</span>
<span class="p_add">+			  void *data, int flags)</span>
 {
 	resource_size_t key, align_start, align_size, align_end;
 	pgprot_t pgprot = PAGE_KERNEL;
<span class="p_chunk">@@ -299,6 +321,17 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 	int error, nid, is_ram;
 	unsigned long pfn;
 
<span class="p_add">+	if (!(flags &amp; MEMORY_DEVICE)) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;%s attempted on non device memory\n&quot;, __func__);</span>
<span class="p_add">+		return ERR_PTR(-EINVAL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (altmap &amp;&amp; (flags &amp; MEMORY_DEVICE_UNADDRESSABLE)) {</span>
<span class="p_add">+		WARN_ONCE(1, &quot;%s with altmap for un-addressable &quot;</span>
<span class="p_add">+			  &quot;device memory\n&quot;, __func__);</span>
<span class="p_add">+		return ERR_PTR(-EINVAL);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	align_start = res-&gt;start &amp; ~(SECTION_SIZE - 1);
 	align_size = ALIGN(res-&gt;start + resource_size(res), SECTION_SIZE)
 		- align_start;
<span class="p_chunk">@@ -332,8 +365,10 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 	}
 	pgmap-&gt;ref = ref;
 	pgmap-&gt;res = &amp;page_map-&gt;res;
<span class="p_add">+	pgmap-&gt;page_fault = page_fault;</span>
 	pgmap-&gt;page_free = page_free;
 	pgmap-&gt;data = data;
<span class="p_add">+	pgmap-&gt;flags = flags;</span>
 
 	mutex_lock(&amp;pgmap_lock);
 	error = 0;
<span class="p_chunk">@@ -370,7 +405,7 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 	if (error)
 		goto err_pfn_remap;
 
<span class="p_del">-	error = arch_add_memory(nid, align_start, align_size, MEMORY_DEVICE);</span>
<span class="p_add">+	error = arch_add_memory(nid, align_start, align_size, pgmap-&gt;flags);</span>
 	if (error)
 		goto err_add_memory;
 
<span class="p_chunk">@@ -387,6 +422,8 @@</span> <span class="p_context"> void *devm_memremap_pages(struct device *dev, struct resource *res,</span>
 		page-&gt;pgmap = pgmap;
 	}
 	devres_add(dev, page_map);
<span class="p_add">+	if (ppgmap)</span>
<span class="p_add">+		*ppgmap = pgmap;</span>
 	return __va(res-&gt;start);
 
  err_add_memory:
<span class="p_header">diff --git a/mm/Kconfig b/mm/Kconfig</span>
<span class="p_header">index e9b7c7e..0c33f46 100644</span>
<span class="p_header">--- a/mm/Kconfig</span>
<span class="p_header">+++ b/mm/Kconfig</span>
<span class="p_chunk">@@ -700,6 +700,18 @@</span> <span class="p_context"> config ZONE_DEVICE</span>
 
 	  If FS_DAX is enabled, then say Y.
 
<span class="p_add">+config DEVICE_UNADDRESSABLE</span>
<span class="p_add">+	bool &quot;Un-addressable device memory (GPU memory, ...)&quot;</span>
<span class="p_add">+	depends on ZONE_DEVICE</span>
<span class="p_add">+</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  Allow to create struct page for un-addressable device memory</span>
<span class="p_add">+	  ie memory that is only accessible by the device (or group of</span>
<span class="p_add">+	  devices).</span>
<span class="p_add">+</span>
<span class="p_add">+	  Having struct page is necessary for process memory migration</span>
<span class="p_add">+	  to device memory.</span>
<span class="p_add">+</span>
 config FRAME_VECTOR
 	bool
 
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index e870322..69bede9 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -45,6 +45,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/swap.h&gt;
 #include &lt;linux/highmem.h&gt;
 #include &lt;linux/pagemap.h&gt;
<span class="p_add">+#include &lt;linux/memremap.h&gt;</span>
 #include &lt;linux/ksm.h&gt;
 #include &lt;linux/rmap.h&gt;
 #include &lt;linux/export.h&gt;
<span class="p_chunk">@@ -890,6 +891,25 @@</span> <span class="p_context"> copy_one_pte(struct mm_struct *dst_mm, struct mm_struct *src_mm,</span>
 					pte = pte_swp_mksoft_dirty(pte);
 				set_pte_at(src_mm, addr, src_pte, pte);
 			}
<span class="p_add">+		} else if (is_device_entry(entry)) {</span>
<span class="p_add">+			page = device_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Update rss count even for un-addressable page as</span>
<span class="p_add">+			 * they should be consider just like any other page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			get_page(page);</span>
<span class="p_add">+			rss[mm_counter(page)]++;</span>
<span class="p_add">+			page_dup_rmap(page, false);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (is_write_device_entry(entry) &amp;&amp;</span>
<span class="p_add">+			    is_cow_mapping(vm_flags)) {</span>
<span class="p_add">+				make_device_entry_read(&amp;entry);</span>
<span class="p_add">+				pte = swp_entry_to_pte(entry);</span>
<span class="p_add">+				if (pte_swp_soft_dirty(*src_pte))</span>
<span class="p_add">+					pte = pte_swp_mksoft_dirty(pte);</span>
<span class="p_add">+				set_pte_at(src_mm, addr, src_pte, pte);</span>
<span class="p_add">+			}</span>
 		}
 		goto out_set_pte;
 	}
<span class="p_chunk">@@ -1179,6 +1199,32 @@</span> <span class="p_context"> static unsigned long zap_pte_range(struct mmu_gather *tlb,</span>
 			}
 			continue;
 		}
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Un-addressable page must always be check that are not like</span>
<span class="p_add">+		 * other swap entries and thus should be check no matter what</span>
<span class="p_add">+		 * details-&gt;check_swap_entries value is.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		entry = pte_to_swp_entry(ptent);</span>
<span class="p_add">+		if (non_swap_entry(entry) &amp;&amp; is_device_entry(entry)) {</span>
<span class="p_add">+			struct page *page = device_entry_to_page(entry);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (unlikely(details)) {</span>
<span class="p_add">+				/*</span>
<span class="p_add">+				 * unmap_shared_mapping_pages() wants to</span>
<span class="p_add">+				 * invalidate cache without truncating:</span>
<span class="p_add">+				 * unmap shared but keep private pages.</span>
<span class="p_add">+				 */</span>
<span class="p_add">+				if (details-&gt;check_mapping &amp;&amp;</span>
<span class="p_add">+				    details-&gt;check_mapping != page_rmapping(page))</span>
<span class="p_add">+					continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+</span>
<span class="p_add">+			rss[mm_counter(page)]--;</span>
<span class="p_add">+			page_remove_rmap(page, false);</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		/* only check swap_entries if explicitly asked for in details */
 		if (unlikely(details &amp;&amp; !details-&gt;check_swap_entries))
 			continue;
<span class="p_chunk">@@ -2550,6 +2596,14 @@</span> <span class="p_context"> int do_swap_page(struct vm_fault *vmf)</span>
 		if (is_migration_entry(entry)) {
 			migration_entry_wait(vma-&gt;vm_mm, vmf-&gt;pmd,
 					     vmf-&gt;address);
<span class="p_add">+		} else if (is_device_entry(entry)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * For un-addressable device memory we call the pgmap</span>
<span class="p_add">+			 * fault handler callback. The callback must migrate</span>
<span class="p_add">+			 * the page back to some CPU accessible page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			ret = device_entry_fault(vma, vmf-&gt;address, entry,</span>
<span class="p_add">+						 vmf-&gt;flags, vmf-&gt;pmd);</span>
 		} else if (is_hwpoison_entry(entry)) {
 			ret = VM_FAULT_HWPOISON;
 		} else {
<span class="p_chunk">@@ -3518,6 +3572,7 @@</span> <span class="p_context"> static inline bool vma_is_accessible(struct vm_area_struct *vma)</span>
 static int handle_pte_fault(struct vm_fault *vmf)
 {
 	pte_t entry;
<span class="p_add">+	struct page *page;</span>
 
 	if (unlikely(pmd_none(*vmf-&gt;pmd))) {
 		/*
<span class="p_chunk">@@ -3568,9 +3623,16 @@</span> <span class="p_context"> static int handle_pte_fault(struct vm_fault *vmf)</span>
 	if (pte_protnone(vmf-&gt;orig_pte) &amp;&amp; vma_is_accessible(vmf-&gt;vma))
 		return do_numa_page(vmf);
 
<span class="p_add">+	/* Catch mapping of un-addressable memory this should never happen */</span>
<span class="p_add">+	entry = vmf-&gt;orig_pte;</span>
<span class="p_add">+	page = pfn_to_page(pte_pfn(entry));</span>
<span class="p_add">+	if (!is_addressable_page(page)) {</span>
<span class="p_add">+		print_bad_pte(vmf-&gt;vma, vmf-&gt;address, entry, page);</span>
<span class="p_add">+		return VM_FAULT_SIGBUS;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	vmf-&gt;ptl = pte_lockptr(vmf-&gt;vma-&gt;vm_mm, vmf-&gt;pmd);
 	spin_lock(vmf-&gt;ptl);
<span class="p_del">-	entry = vmf-&gt;orig_pte;</span>
 	if (unlikely(!pte_same(*vmf-&gt;pte, entry)))
 		goto unlock;
 	if (vmf-&gt;flags &amp; FAULT_FLAG_WRITE) {
<span class="p_header">diff --git a/mm/memory_hotplug.c b/mm/memory_hotplug.c</span>
<span class="p_header">index 096c651..76f5359 100644</span>
<span class="p_header">--- a/mm/memory_hotplug.c</span>
<span class="p_header">+++ b/mm/memory_hotplug.c</span>
<span class="p_chunk">@@ -149,7 +149,7 @@</span> <span class="p_context"> void mem_hotplug_done(void)</span>
 /* add this memory to iomem resource */
 static struct resource *register_memory_resource(u64 start, u64 size)
 {
<span class="p_del">-	struct resource *res;</span>
<span class="p_add">+	struct resource *res, *conflict;</span>
 	res = kzalloc(sizeof(struct resource), GFP_KERNEL);
 	if (!res)
 		return ERR_PTR(-ENOMEM);
<span class="p_chunk">@@ -158,7 +158,13 @@</span> <span class="p_context"> static struct resource *register_memory_resource(u64 start, u64 size)</span>
 	res-&gt;start = start;
 	res-&gt;end = start + size - 1;
 	res-&gt;flags = IORESOURCE_SYSTEM_RAM | IORESOURCE_BUSY;
<span class="p_del">-	if (request_resource(&amp;iomem_resource, res) &lt; 0) {</span>
<span class="p_add">+	conflict =  request_resource_conflict(&amp;iomem_resource, res);</span>
<span class="p_add">+	if (conflict) {</span>
<span class="p_add">+		if (conflict-&gt;desc == IORES_DESC_UNADDRESSABLE_MEMORY) {</span>
<span class="p_add">+			pr_debug(&quot;Device un-addressable memory block &quot;</span>
<span class="p_add">+				 &quot;memory hotplug at %#010llx !\n&quot;,</span>
<span class="p_add">+				 (unsigned long long)start);</span>
<span class="p_add">+		}</span>
 		pr_debug(&quot;System RAM resource %pR cannot be added\n&quot;, res);
 		kfree(res);
 		return ERR_PTR(-EEXIST);
<span class="p_header">diff --git a/mm/mprotect.c b/mm/mprotect.c</span>
<span class="p_header">index cc2459c..fc3dd08 100644</span>
<span class="p_header">--- a/mm/mprotect.c</span>
<span class="p_header">+++ b/mm/mprotect.c</span>
<span class="p_chunk">@@ -140,6 +140,18 @@</span> <span class="p_context"> static unsigned long change_pte_range(struct vm_area_struct *vma, pmd_t *pmd,</span>
 
 				pages++;
 			}
<span class="p_add">+</span>
<span class="p_add">+			if (is_write_device_entry(entry)) {</span>
<span class="p_add">+				pte_t newpte;</span>
<span class="p_add">+</span>
<span class="p_add">+				make_device_entry_read(&amp;entry);</span>
<span class="p_add">+				newpte = swp_entry_to_pte(entry);</span>
<span class="p_add">+				if (pte_swp_soft_dirty(oldpte))</span>
<span class="p_add">+					newpte = pte_swp_mksoft_dirty(newpte);</span>
<span class="p_add">+				set_pte_at(mm, addr, pte, newpte);</span>
<span class="p_add">+</span>
<span class="p_add">+				pages++;</span>
<span class="p_add">+			}</span>
 		}
 	} while (pte++, addr += PAGE_SIZE, addr != end);
 	arch_leave_lazy_mmu_mode();
<span class="p_header">diff --git a/tools/testing/nvdimm/test/iomap.c b/tools/testing/nvdimm/test/iomap.c</span>
<span class="p_header">index 9992a7c..19902c9 100644</span>
<span class="p_header">--- a/tools/testing/nvdimm/test/iomap.c</span>
<span class="p_header">+++ b/tools/testing/nvdimm/test/iomap.c</span>
<span class="p_chunk">@@ -112,7 +112,8 @@</span> <span class="p_context"> void *__wrap_devm_memremap_pages(struct device *dev, struct resource *res,</span>
 
 	if (nfit_res)
 		return nfit_res-&gt;buf + offset - nfit_res-&gt;res.start;
<span class="p_del">-	return devm_memremap_pages(dev, res, ref, altmap, NULL, NULL);</span>
<span class="p_add">+	return devm_memremap_pages(dev, res, ref, altmap, NULL,</span>
<span class="p_add">+				   NULL, NULL, NULL, MEMORY_DEVICE);</span>
 }
 EXPORT_SYMBOL(__wrap_devm_memremap_pages);
 

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



