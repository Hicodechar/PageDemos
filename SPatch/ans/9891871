
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v5,02/10] mm, x86: Add support for eXclusive Page Frame Ownership (XPFO) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v5,02/10] mm, x86: Add support for eXclusive Page Frame Ownership (XPFO)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 9, 2017, 8:07 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170809200755.11234-3-tycho@docker.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9891871/mbox/"
   >mbox</a>
|
   <a href="/patch/9891871/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9891871/">/patch/9891871/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	C73B9603F2 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  9 Aug 2017 20:09:33 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B6FE0289BD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  9 Aug 2017 20:09:33 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id ABC5828A84; Wed,  9 Aug 2017 20:09:33 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.5 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, RCVD_IN_DNSWL_HI,
	RCVD_IN_SORBS_SPAM autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 39CA0289BD
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed,  9 Aug 2017 20:09:32 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752113AbdHIUJB (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 9 Aug 2017 16:09:01 -0400
Received: from mail-it0-f49.google.com ([209.85.214.49]:38204 &quot;EHLO
	mail-it0-f49.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751890AbdHIUI5 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 9 Aug 2017 16:08:57 -0400
Received: by mail-it0-f49.google.com with SMTP id m34so3264580iti.1
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Wed, 09 Aug 2017 13:08:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=docker.com; s=google;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=a9Jh5t8+NiSSiiYfOIj43F079HV8SJYxNKEqOIM+RF4=;
	b=CmRC31xrBUjMXoa/0cg9ghI5J4cz2bkpEl68/WYqRGniLqEzqCG9BycInAoHRAgQRQ
	EaTls8kisUok4AhTMQMoAR8y0OEY9aAHvig9IYrKkdtEPQCzueR4WJqcfEeeN0Szcfnq
	5jXmzOUhBmcCrOCQ4Ab1/m5f43fZQV2h2Esi0=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20161025;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=a9Jh5t8+NiSSiiYfOIj43F079HV8SJYxNKEqOIM+RF4=;
	b=PjW4Zu9g3T+ZFyErlB6XKQ1nZ5sVOIBqhvN6n3HtaCjn33+cwRy20h1G5t/bInav3t
	lsSOkoLi4gXSDHK+hRQ025GZ/zlucawOR8/yJwcxIKl42s354GmqAzqILB+XmcsF5jYC
	Ngz9dPpsMhTRJDbOG+hH0ExM8+OkCSzYSI/h9Bp7LrgwXFe8/3SiiwjZasZMlqqC4q1r
	fprj2KdZI4Pr8+75O6oQyhpZy6V8Xc3+V+JZtnaz7o8v2vF5KEF95iJM6G42LvFM8720
	02kw8HAQWClf8Bt4mkN/uphRksOoZb3848HRBX9C4uPsQaN9zMNsOkbzxlXIBaHIo30U
	gUEQ==
X-Gm-Message-State: AHYfb5hwjy8F9h65FpHlBoBz6OU2uwBIp/q+B56MSNUuMXD86jFfHXpU
	sdv2udgm0XNoTAsV7ZtQQQ==
X-Received: by 10.36.25.70 with SMTP id b67mr7842180itb.72.1502309336567;
	Wed, 09 Aug 2017 13:08:56 -0700 (PDT)
Received: from localhost.localdomain ([8.24.24.129])
	by smtp.gmail.com with ESMTPSA id
	p63sm2324422itg.32.2017.08.09.13.08.55
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Wed, 09 Aug 2017 13:08:55 -0700 (PDT)
From: Tycho Andersen &lt;tycho@docker.com&gt;
To: linux-kernel@vger.kernel.org
Cc: linux-mm@kvack.org, kernel-hardening@lists.openwall.com,
	Marco Benatto &lt;marco.antonio.780@gmail.com&gt;,
	Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;,
	Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;,
	Tycho Andersen &lt;tycho@docker.com&gt;
Subject: [PATCH v5 02/10] mm,
	x86: Add support for eXclusive Page Frame Ownership (XPFO)
Date: Wed,  9 Aug 2017 14:07:47 -0600
Message-Id: &lt;20170809200755.11234-3-tycho@docker.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170809200755.11234-1-tycho@docker.com&gt;
References: &lt;20170809200755.11234-1-tycho@docker.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Aug. 9, 2017, 8:07 p.m.</div>
<pre class="content">
<span class="from">From: Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>

This patch adds support for XPFO which protects against &#39;ret2dir&#39; kernel
attacks. The basic idea is to enforce exclusive ownership of page frames
by either the kernel or userspace, unless explicitly requested by the
kernel. Whenever a page destined for userspace is allocated, it is
unmapped from physmap (the kernel&#39;s page table). When such a page is
reclaimed from userspace, it is mapped back to physmap.

Additional fields in the page_ext struct are used for XPFO housekeeping,
specifically:
  - two flags to distinguish user vs. kernel pages and to tag unmapped
    pages.
  - a reference counter to balance kmap/kunmap operations.
  - a lock to serialize access to the XPFO fields.

This patch is based on the work of Vasileios P. Kemerlis et al. who
published their work in this paper:
  http://www.cs.columbia.edu/~vpk/papers/ret2dir.sec14.pdf

Suggested-by: Vasileios P. Kemerlis &lt;vpk@cs.columbia.edu&gt;
<span class="signed-off-by">Signed-off-by: Juerg Haefliger &lt;juerg.haefliger@canonical.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Tycho Andersen &lt;tycho@docker.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Marco Benatto &lt;marco.antonio.780@gmail.com&gt;</span>
---
 Documentation/admin-guide/kernel-parameters.txt |   2 +
 arch/x86/Kconfig                                |   1 +
 arch/x86/include/asm/pgtable.h                  |  23 +++
 arch/x86/mm/Makefile                            |   1 +
 arch/x86/mm/pageattr.c                          |  24 +--
 arch/x86/mm/xpfo.c                              |  96 +++++++++++
 include/linux/highmem.h                         |  15 +-
 include/linux/xpfo.h                            |  39 +++++
 mm/Makefile                                     |   1 +
 mm/page_alloc.c                                 |   2 +
 mm/page_ext.c                                   |   4 +
 mm/xpfo.c                                       | 208 ++++++++++++++++++++++++
 security/Kconfig                                |  19 +++
 13 files changed, 413 insertions(+), 22 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - Aug. 14, 2017, 6:51 p.m.</div>
<pre class="content">
On 08/09/2017 01:07 PM, Tycho Andersen wrote:
<span class="quote">&gt; diff --git a/mm/xpfo.c b/mm/xpfo.c</span>
<span class="quote">&gt; new file mode 100644</span>
<span class="quote">&gt; index 000000000000..3cd45f68b5ad</span>
<span class="quote">&gt; --- /dev/null</span>
<span class="quote">&gt; +++ b/mm/xpfo.c</span>
<span class="quote">&gt; @@ -0,0 +1,208 @@</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt; + * Copyright (C) 2017 Hewlett Packard Enterprise Development, L.P.</span>
<span class="quote">&gt; + * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Authors:</span>
<span class="quote">&gt; + *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="quote">&gt; + *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * This program is free software; you can redistribute it and/or modify it</span>
<span class="quote">&gt; + * under the terms of the GNU General Public License version 2 as published by</span>
<span class="quote">&gt; + * the Free Software Foundation.</span>
<span class="quote">&gt; + */</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;linux/mm.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/module.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/page_ext.h&gt;</span>
<span class="quote">&gt; +#include &lt;linux/xpfo.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#include &lt;asm/tlbflush.h&gt;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* XPFO page state flags */</span>
<span class="quote">&gt; +enum xpfo_flags {</span>
<span class="quote">&gt; +	XPFO_PAGE_USER,		/* Page is allocated to user-space */</span>
<span class="quote">&gt; +	XPFO_PAGE_UNMAPPED,	/* Page is unmapped from the linear map */</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/* Per-page XPFO house-keeping data */</span>
<span class="quote">&gt; +struct xpfo {</span>
<span class="quote">&gt; +	unsigned long flags;	/* Page state */</span>
<span class="quote">&gt; +	bool inited;		/* Map counter and lock initialized */</span>
<span class="quote">&gt; +	atomic_t mapcount;	/* Counter for balancing map/unmap requests */</span>
<span class="quote">&gt; +	spinlock_t maplock;	/* Lock to serialize map/unmap requests */</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +DEFINE_STATIC_KEY_FALSE(xpfo_inited);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static bool xpfo_disabled __initdata;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static int __init noxpfo_param(char *str)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	xpfo_disabled = true;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +early_param(&quot;noxpfo&quot;, noxpfo_param);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static bool __init need_xpfo(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (xpfo_disabled) {</span>
<span class="quote">&gt; +		printk(KERN_INFO &quot;XPFO disabled\n&quot;);</span>
<span class="quote">&gt; +		return false;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	return true;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static void init_xpfo(void)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	printk(KERN_INFO &quot;XPFO enabled\n&quot;);</span>
<span class="quote">&gt; +	static_branch_enable(&amp;xpfo_inited);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +struct page_ext_operations page_xpfo_ops = {</span>
<span class="quote">&gt; +	.size = sizeof(struct xpfo),</span>
<span class="quote">&gt; +	.need = need_xpfo,</span>
<span class="quote">&gt; +	.init = init_xpfo,</span>
<span class="quote">&gt; +};</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline struct xpfo *lookup_xpfo(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return (void *)lookup_page_ext(page) + page_xpfo_ops.offset;</span>
<span class="quote">&gt; +}</span>

lookup_page_ext can return NULL so this function and its callers
need to account for that.

Thanks,
Laura
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=130411">Laura Abbott</a> - Aug. 14, 2017, 10:30 p.m.</div>
<pre class="content">
On 08/09/2017 01:07 PM, Tycho Andersen wrote:
<span class="quote">&gt; +/* Update a single kernel page table entry */</span>
<span class="quote">&gt; +inline void set_kpte(void *kaddr, struct page *page, pgprot_t prot)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	unsigned int level;</span>
<span class="quote">&gt; +	pgprot_t msk_clr;</span>
<span class="quote">&gt; +	pte_t *pte = lookup_address((unsigned long)kaddr, &amp;level);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	BUG_ON(!pte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	switch (level) {</span>
<span class="quote">&gt; +	case PG_LEVEL_4K:</span>
<span class="quote">&gt; +		set_pte_atomic(pte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	case PG_LEVEL_2M:</span>
<span class="quote">&gt; +		/* We need to check if it&#39;s a 2M page or 1GB page before retrieve</span>
<span class="quote">&gt; +		 * pgprot info, as each one will be extracted from a different</span>
<span class="quote">&gt; +		 * page table levels */</span>
<span class="quote">&gt; +		msk_clr = pmd_pgprot(*(pmd_t*)pte);</span>
<span class="quote">&gt; +	case PG_LEVEL_1G: {</span>
<span class="quote">&gt; +		struct cpa_data cpa;</span>
<span class="quote">&gt; +		int do_split;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		msk_clr = pud_pgprot(*(pud_t*)pte);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		memset(&amp;cpa, 0, sizeof(cpa));</span>
<span class="quote">&gt; +		cpa.vaddr = kaddr;</span>
<span class="quote">&gt; +		cpa.pages = &amp;page;</span>
<span class="quote">&gt; +		cpa.mask_set = prot;</span>
<span class="quote">&gt; +		cpa.mask_clr = msk_clr;</span>
<span class="quote">&gt; +		cpa.numpages = 1;</span>
<span class="quote">&gt; +		cpa.flags = 0;</span>
<span class="quote">&gt; +		cpa.curpage = 0;</span>
<span class="quote">&gt; +		cpa.force_split = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		do_split = try_preserve_large_page(pte, (unsigned long)kaddr, &amp;cpa);</span>
<span class="quote">&gt; +		if (do_split) {</span>
<span class="quote">&gt; +			spin_lock(&amp;cpa_lock);</span>
<span class="quote">&gt; +			BUG_ON(split_large_page(&amp;cpa, pte, (unsigned long)kaddr));</span>
<span class="quote">&gt; +			spin_unlock(&amp;cpa_lock);</span>
<span class="quote">&gt; +		}</span>

This doesn&#39;t work in atomic contexts:

[   28.263571] BUG: sleeping function called from invalid context at 
mm/page_alloc.c:4048
[   28.263575] in_atomic(): 1, irqs_disabled(): 1, pid: 2433, name: 
gnome-terminal
[   28.263576] INFO: lockdep is turned off.
[   28.263578] irq event stamp: 0
[   28.263580] hardirqs last  enabled at (0): [&lt;          (null)&gt;] 
     (null)
[   28.263584] hardirqs last disabled at (0): [&lt;ffffffff840af28a&gt;] 
copy_process.part.25+0x62a/0x1e90
[   28.263587] softirqs last  enabled at (0): [&lt;ffffffff840af28a&gt;] 
copy_process.part.25+0x62a/0x1e90
[   28.263588] softirqs last disabled at (0): [&lt;          (null)&gt;] 
     (null)
[   28.263591] CPU: 0 PID: 2433 Comm: gnome-terminal Tainted: G        W 
       4.13.0-rc5-xpfo+ #86
[   28.263592] Hardware name: LENOVO 20BTS1N700/20BTS1N700, BIOS 
N14ET28W (1.06 ) 03/12/2015
[   28.263593] Call Trace:
[   28.263598]  dump_stack+0x8e/0xd6
[   28.263601]  ___might_sleep+0x164/0x250
[   28.263604]  __might_sleep+0x4a/0x80
[   28.263607]  __alloc_pages_nodemask+0x2b3/0x3e0
[   28.263611]  alloc_pages_current+0x6a/0xe0
[   28.263614]  split_large_page+0x4e/0x360
[   28.263618]  set_kpte+0x12c/0x150
[   28.263623]  xpfo_kunmap+0x7e/0xa0
[   28.263627]  wp_page_copy+0x16e/0x800
[   28.263631]  do_wp_page+0x9a/0x580
[   28.263633]  __handle_mm_fault+0xb1c/0x1130
[   28.263638]  handle_mm_fault+0x178/0x350
[   28.263641]  __do_page_fault+0x26e/0x510
[   28.263644]  do_page_fault+0x30/0x80
[   28.263647]  page_fault+0x28/0x30


split_large_page calls alloc_page with GFP_KERNEL. switching to
use GFP_ATOMIC in this path works locally for me.

Thanks,
Laura
<span class="quote">
&gt; +</span>
<span class="quote">&gt; +		break;</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +	case PG_LEVEL_512G:</span>
<span class="quote">&gt; +		/* fallthrough, splitting infrastructure doesn&#39;t</span>
<span class="quote">&gt; +		 * support 512G pages. */</span>
<span class="quote">&gt; +	default:</span>
<span class="quote">&gt; +		BUG();</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +}</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=173139">Tycho Andersen</a> - Aug. 15, 2017, 3:47 a.m.</div>
<pre class="content">
Hi Laura,

On Mon, Aug 14, 2017 at 03:30:00PM -0700, Laura Abbott wrote:
<span class="quote">&gt; On 08/09/2017 01:07 PM, Tycho Andersen wrote:</span>
<span class="quote">&gt; &gt; +/* Update a single kernel page table entry */</span>
<span class="quote">&gt; &gt; +inline void set_kpte(void *kaddr, struct page *page, pgprot_t prot)</span>
<span class="quote">&gt; &gt; +{</span>
<span class="quote">&gt; &gt; +	unsigned int level;</span>
<span class="quote">&gt; &gt; +	pgprot_t msk_clr;</span>
<span class="quote">&gt; &gt; +	pte_t *pte = lookup_address((unsigned long)kaddr, &amp;level);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	BUG_ON(!pte);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	switch (level) {</span>
<span class="quote">&gt; &gt; +	case PG_LEVEL_4K:</span>
<span class="quote">&gt; &gt; +		set_pte_atomic(pte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="quote">&gt; &gt; +		break;</span>
<span class="quote">&gt; &gt; +	case PG_LEVEL_2M:</span>
<span class="quote">&gt; &gt; +		/* We need to check if it&#39;s a 2M page or 1GB page before retrieve</span>
<span class="quote">&gt; &gt; +		 * pgprot info, as each one will be extracted from a different</span>
<span class="quote">&gt; &gt; +		 * page table levels */</span>
<span class="quote">&gt; &gt; +		msk_clr = pmd_pgprot(*(pmd_t*)pte);</span>
<span class="quote">&gt; &gt; +	case PG_LEVEL_1G: {</span>
<span class="quote">&gt; &gt; +		struct cpa_data cpa;</span>
<span class="quote">&gt; &gt; +		int do_split;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		msk_clr = pud_pgprot(*(pud_t*)pte);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		memset(&amp;cpa, 0, sizeof(cpa));</span>
<span class="quote">&gt; &gt; +		cpa.vaddr = kaddr;</span>
<span class="quote">&gt; &gt; +		cpa.pages = &amp;page;</span>
<span class="quote">&gt; &gt; +		cpa.mask_set = prot;</span>
<span class="quote">&gt; &gt; +		cpa.mask_clr = msk_clr;</span>
<span class="quote">&gt; &gt; +		cpa.numpages = 1;</span>
<span class="quote">&gt; &gt; +		cpa.flags = 0;</span>
<span class="quote">&gt; &gt; +		cpa.curpage = 0;</span>
<span class="quote">&gt; &gt; +		cpa.force_split = 0;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +		do_split = try_preserve_large_page(pte, (unsigned long)kaddr, &amp;cpa);</span>
<span class="quote">&gt; &gt; +		if (do_split) {</span>
<span class="quote">&gt; &gt; +			spin_lock(&amp;cpa_lock);</span>
<span class="quote">&gt; &gt; +			BUG_ON(split_large_page(&amp;cpa, pte, (unsigned long)kaddr));</span>
<span class="quote">&gt; &gt; +			spin_unlock(&amp;cpa_lock);</span>
<span class="quote">&gt; &gt; +		}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This doesn&#39;t work in atomic contexts:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [   28.263571] BUG: sleeping function called from invalid context at</span>
<span class="quote">&gt; mm/page_alloc.c:4048</span>
<span class="quote">&gt; [   28.263575] in_atomic(): 1, irqs_disabled(): 1, pid: 2433, name:</span>
<span class="quote">&gt; gnome-terminal</span>
<span class="quote">&gt; [   28.263576] INFO: lockdep is turned off.</span>
<span class="quote">&gt; [   28.263578] irq event stamp: 0</span>
<span class="quote">&gt; [   28.263580] hardirqs last  enabled at (0): [&lt;          (null)&gt;]</span>
<span class="quote">&gt; (null)</span>
<span class="quote">&gt; [   28.263584] hardirqs last disabled at (0): [&lt;ffffffff840af28a&gt;]</span>
<span class="quote">&gt; copy_process.part.25+0x62a/0x1e90</span>
<span class="quote">&gt; [   28.263587] softirqs last  enabled at (0): [&lt;ffffffff840af28a&gt;]</span>
<span class="quote">&gt; copy_process.part.25+0x62a/0x1e90</span>
<span class="quote">&gt; [   28.263588] softirqs last disabled at (0): [&lt;          (null)&gt;]</span>
<span class="quote">&gt; (null)</span>
<span class="quote">&gt; [   28.263591] CPU: 0 PID: 2433 Comm: gnome-terminal Tainted: G        W</span>
<span class="quote">&gt; 4.13.0-rc5-xpfo+ #86</span>
<span class="quote">&gt; [   28.263592] Hardware name: LENOVO 20BTS1N700/20BTS1N700, BIOS N14ET28W</span>
<span class="quote">&gt; (1.06 ) 03/12/2015</span>
<span class="quote">&gt; [   28.263593] Call Trace:</span>
<span class="quote">&gt; [   28.263598]  dump_stack+0x8e/0xd6</span>
<span class="quote">&gt; [   28.263601]  ___might_sleep+0x164/0x250</span>
<span class="quote">&gt; [   28.263604]  __might_sleep+0x4a/0x80</span>
<span class="quote">&gt; [   28.263607]  __alloc_pages_nodemask+0x2b3/0x3e0</span>
<span class="quote">&gt; [   28.263611]  alloc_pages_current+0x6a/0xe0</span>
<span class="quote">&gt; [   28.263614]  split_large_page+0x4e/0x360</span>
<span class="quote">&gt; [   28.263618]  set_kpte+0x12c/0x150</span>
<span class="quote">&gt; [   28.263623]  xpfo_kunmap+0x7e/0xa0</span>
<span class="quote">&gt; [   28.263627]  wp_page_copy+0x16e/0x800</span>
<span class="quote">&gt; [   28.263631]  do_wp_page+0x9a/0x580</span>
<span class="quote">&gt; [   28.263633]  __handle_mm_fault+0xb1c/0x1130</span>
<span class="quote">&gt; [   28.263638]  handle_mm_fault+0x178/0x350</span>
<span class="quote">&gt; [   28.263641]  __do_page_fault+0x26e/0x510</span>
<span class="quote">&gt; [   28.263644]  do_page_fault+0x30/0x80</span>
<span class="quote">&gt; [   28.263647]  page_fault+0x28/0x30</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; split_large_page calls alloc_page with GFP_KERNEL. switching to</span>
<span class="quote">&gt; use GFP_ATOMIC in this path works locally for me.</span>

Oof, thanks. I&#39;ll do that for the next version, and also CC x86 in
case they may have better suggestions.

Cheers,

Tycho
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/admin-guide/kernel-parameters.txt b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">index d9c171ce4190..444d83183f75 100644</span>
<span class="p_header">--- a/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_header">+++ b/Documentation/admin-guide/kernel-parameters.txt</span>
<span class="p_chunk">@@ -2736,6 +2736,8 @@</span> <span class="p_context"></span>
 
 	nox2apic	[X86-64,APIC] Do not enable x2APIC mode.
 
<span class="p_add">+	noxpfo		[X86-64] Disable XPFO when CONFIG_XPFO is on.</span>
<span class="p_add">+</span>
 	cpu0_hotplug	[X86] Turn on CPU0 hotplug feature when
 			CONFIG_BOOTPARAM_HOTPLUG_CPU0 is off.
 			Some features depend on CPU0. Known dependencies are:
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 781521b7cf9e..f37d408ab1f2 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -184,6 +184,7 @@</span> <span class="p_context"> config X86</span>
 	select USER_STACKTRACE_SUPPORT
 	select VIRT_TO_BUS
 	select X86_FEATURE_NAMES		if PROC_FS
<span class="p_add">+	select ARCH_SUPPORTS_XPFO		if X86_64</span>
 
 config INSTRUCTION_DECODER
 	def_bool y
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable.h b/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">index 77037b6f1caa..0c20379c034c 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable.h</span>
<span class="p_chunk">@@ -1238,6 +1238,29 @@</span> <span class="p_context"> static inline bool pud_access_permitted(pud_t pud, bool write)</span>
 	return __pte_access_permitted(pud_val(pud), write);
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * The current flushing context - we pass it instead of 5 arguments:</span>
<span class="p_add">+ */</span>
<span class="p_add">+struct cpa_data {</span>
<span class="p_add">+	unsigned long	*vaddr;</span>
<span class="p_add">+	pgd_t		*pgd;</span>
<span class="p_add">+	pgprot_t	mask_set;</span>
<span class="p_add">+	pgprot_t	mask_clr;</span>
<span class="p_add">+	unsigned long	numpages;</span>
<span class="p_add">+	int		flags;</span>
<span class="p_add">+	unsigned long	pfn;</span>
<span class="p_add">+	unsigned	force_split : 1;</span>
<span class="p_add">+	int		curpage;</span>
<span class="p_add">+	struct page	**pages;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+int</span>
<span class="p_add">+try_preserve_large_page(pte_t *kpte, unsigned long address,</span>
<span class="p_add">+			struct cpa_data *cpa);</span>
<span class="p_add">+int split_large_page(struct cpa_data *cpa, pte_t *kpte,</span>
<span class="p_add">+		     unsigned long address);</span>
<span class="p_add">+</span>
 #include &lt;asm-generic/pgtable.h&gt;
 #endif	/* __ASSEMBLY__ */
 
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 0fbdcb64f9f8..89ba6d25fb51 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -39,3 +39,4 @@</span> <span class="p_context"> obj-$(CONFIG_X86_INTEL_MPX)	+= mpx.o</span>
 obj-$(CONFIG_X86_INTEL_MEMORY_PROTECTION_KEYS) += pkeys.o
 obj-$(CONFIG_RANDOMIZE_MEMORY) += kaslr.o
 
<span class="p_add">+obj-$(CONFIG_XPFO)		+= xpfo.o</span>
<span class="p_header">diff --git a/arch/x86/mm/pageattr.c b/arch/x86/mm/pageattr.c</span>
<span class="p_header">index 757b0bcdf712..0a40be4708e9 100644</span>
<span class="p_header">--- a/arch/x86/mm/pageattr.c</span>
<span class="p_header">+++ b/arch/x86/mm/pageattr.c</span>
<span class="p_chunk">@@ -27,28 +27,12 @@</span> <span class="p_context"></span>
 #include &lt;asm/set_memory.h&gt;
 
 /*
<span class="p_del">- * The current flushing context - we pass it instead of 5 arguments:</span>
<span class="p_del">- */</span>
<span class="p_del">-struct cpa_data {</span>
<span class="p_del">-	unsigned long	*vaddr;</span>
<span class="p_del">-	pgd_t		*pgd;</span>
<span class="p_del">-	pgprot_t	mask_set;</span>
<span class="p_del">-	pgprot_t	mask_clr;</span>
<span class="p_del">-	unsigned long	numpages;</span>
<span class="p_del">-	int		flags;</span>
<span class="p_del">-	unsigned long	pfn;</span>
<span class="p_del">-	unsigned	force_split : 1;</span>
<span class="p_del">-	int		curpage;</span>
<span class="p_del">-	struct page	**pages;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * Serialize cpa() (for !DEBUG_PAGEALLOC which uses large identity mappings)
  * using cpa_lock. So that we don&#39;t allow any other cpu, with stale large tlb
  * entries change the page attribute in parallel to some other cpu
  * splitting a large page entry along with changing the attribute.
  */
<span class="p_del">-static DEFINE_SPINLOCK(cpa_lock);</span>
<span class="p_add">+DEFINE_SPINLOCK(cpa_lock);</span>
 
 #define CPA_FLUSHTLB 1
 #define CPA_ARRAY 2
<span class="p_chunk">@@ -512,7 +496,7 @@</span> <span class="p_context"> static void __set_pmd_pte(pte_t *kpte, unsigned long address, pte_t pte)</span>
 #endif
 }
 
<span class="p_del">-static int</span>
<span class="p_add">+int</span>
 try_preserve_large_page(pte_t *kpte, unsigned long address,
 			struct cpa_data *cpa)
 {
<span class="p_chunk">@@ -746,8 +730,8 @@</span> <span class="p_context"> __split_large_page(struct cpa_data *cpa, pte_t *kpte, unsigned long address,</span>
 	return 0;
 }
 
<span class="p_del">-static int split_large_page(struct cpa_data *cpa, pte_t *kpte,</span>
<span class="p_del">-			    unsigned long address)</span>
<span class="p_add">+int split_large_page(struct cpa_data *cpa, pte_t *kpte,</span>
<span class="p_add">+		     unsigned long address)</span>
 {
 	struct page *base;
 
<span class="p_header">diff --git a/arch/x86/mm/xpfo.c b/arch/x86/mm/xpfo.c</span>
new file mode 100644
<span class="p_header">index 000000000000..3635b37f2fc5</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/xpfo.c</span>
<span class="p_chunk">@@ -0,0 +1,96 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2017 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+extern spinlock_t cpa_lock;</span>
<span class="p_add">+</span>
<span class="p_add">+/* Update a single kernel page table entry */</span>
<span class="p_add">+inline void set_kpte(void *kaddr, struct page *page, pgprot_t prot)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned int level;</span>
<span class="p_add">+	pgprot_t msk_clr;</span>
<span class="p_add">+	pte_t *pte = lookup_address((unsigned long)kaddr, &amp;level);</span>
<span class="p_add">+</span>
<span class="p_add">+	BUG_ON(!pte);</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (level) {</span>
<span class="p_add">+	case PG_LEVEL_4K:</span>
<span class="p_add">+		set_pte_atomic(pte, pfn_pte(page_to_pfn(page), canon_pgprot(prot)));</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case PG_LEVEL_2M:</span>
<span class="p_add">+		/* We need to check if it&#39;s a 2M page or 1GB page before retrieve</span>
<span class="p_add">+		 * pgprot info, as each one will be extracted from a different</span>
<span class="p_add">+		 * page table levels */</span>
<span class="p_add">+		msk_clr = pmd_pgprot(*(pmd_t*)pte);</span>
<span class="p_add">+	case PG_LEVEL_1G: {</span>
<span class="p_add">+		struct cpa_data cpa;</span>
<span class="p_add">+		int do_split;</span>
<span class="p_add">+</span>
<span class="p_add">+		msk_clr = pud_pgprot(*(pud_t*)pte);</span>
<span class="p_add">+</span>
<span class="p_add">+		memset(&amp;cpa, 0, sizeof(cpa));</span>
<span class="p_add">+		cpa.vaddr = kaddr;</span>
<span class="p_add">+		cpa.pages = &amp;page;</span>
<span class="p_add">+		cpa.mask_set = prot;</span>
<span class="p_add">+		cpa.mask_clr = msk_clr;</span>
<span class="p_add">+		cpa.numpages = 1;</span>
<span class="p_add">+		cpa.flags = 0;</span>
<span class="p_add">+		cpa.curpage = 0;</span>
<span class="p_add">+		cpa.force_split = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+		do_split = try_preserve_large_page(pte, (unsigned long)kaddr, &amp;cpa);</span>
<span class="p_add">+		if (do_split) {</span>
<span class="p_add">+			spin_lock(&amp;cpa_lock);</span>
<span class="p_add">+			BUG_ON(split_large_page(&amp;cpa, pte, (unsigned long)kaddr));</span>
<span class="p_add">+			spin_unlock(&amp;cpa_lock);</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	case PG_LEVEL_512G:</span>
<span class="p_add">+		/* fallthrough, splitting infrastructure doesn&#39;t</span>
<span class="p_add">+		 * support 512G pages. */</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+inline void xpfo_flush_kernel_page(struct page *page, int order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int level;</span>
<span class="p_add">+	unsigned long size, kaddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	kaddr = (unsigned long)page_address(page);</span>
<span class="p_add">+	lookup_address(kaddr, &amp;level);</span>
<span class="p_add">+</span>
<span class="p_add">+	switch (level) {</span>
<span class="p_add">+	case PG_LEVEL_4K:</span>
<span class="p_add">+		size = PAGE_SIZE;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case PG_LEVEL_2M:</span>
<span class="p_add">+		size = PMD_SIZE;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case PG_LEVEL_1G:</span>
<span class="p_add">+		size = PUD_SIZE;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	flush_tlb_kernel_range(kaddr, kaddr + (1 &lt;&lt; order) * size);</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/include/linux/highmem.h b/include/linux/highmem.h</span>
<span class="p_header">index bb3f3297062a..7a17c166532f 100644</span>
<span class="p_header">--- a/include/linux/highmem.h</span>
<span class="p_header">+++ b/include/linux/highmem.h</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mm.h&gt;
 #include &lt;linux/uaccess.h&gt;
 #include &lt;linux/hardirq.h&gt;
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
 
 #include &lt;asm/cacheflush.h&gt;
 
<span class="p_chunk">@@ -55,24 +56,34 @@</span> <span class="p_context"> static inline struct page *kmap_to_page(void *addr)</span>
 #ifndef ARCH_HAS_KMAP
 static inline void *kmap(struct page *page)
 {
<span class="p_add">+	void *kaddr;</span>
<span class="p_add">+</span>
 	might_sleep();
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+	kaddr = page_address(page);</span>
<span class="p_add">+	xpfo_kmap(kaddr, page);</span>
<span class="p_add">+	return kaddr;</span>
 }
 
 static inline void kunmap(struct page *page)
 {
<span class="p_add">+	xpfo_kunmap(page_address(page), page);</span>
 }
 
 static inline void *kmap_atomic(struct page *page)
 {
<span class="p_add">+	void *kaddr;</span>
<span class="p_add">+</span>
 	preempt_disable();
 	pagefault_disable();
<span class="p_del">-	return page_address(page);</span>
<span class="p_add">+	kaddr = page_address(page);</span>
<span class="p_add">+	xpfo_kmap(kaddr, page);</span>
<span class="p_add">+	return kaddr;</span>
 }
 #define kmap_atomic_prot(page, prot)	kmap_atomic(page)
 
 static inline void __kunmap_atomic(void *addr)
 {
<span class="p_add">+	xpfo_kunmap(addr, virt_to_page(addr));</span>
 	pagefault_enable();
 	preempt_enable();
 }
<span class="p_header">diff --git a/include/linux/xpfo.h b/include/linux/xpfo.h</span>
new file mode 100644
<span class="p_header">index 000000000000..1ff2d1976837</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/include/linux/xpfo.h</span>
<span class="p_chunk">@@ -0,0 +1,39 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2017 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef _LINUX_XPFO_H</span>
<span class="p_add">+#define _LINUX_XPFO_H</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+</span>
<span class="p_add">+extern struct page_ext_operations page_xpfo_ops;</span>
<span class="p_add">+</span>
<span class="p_add">+void set_kpte(void *kaddr, struct page *page, pgprot_t prot);</span>
<span class="p_add">+void xpfo_dma_map_unmap_area(bool map, const void *addr, size_t size, int dir);</span>
<span class="p_add">+void xpfo_flush_kernel_page(struct page *page, int order);</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kmap(void *kaddr, struct page *page);</span>
<span class="p_add">+void xpfo_kunmap(void *kaddr, struct page *page);</span>
<span class="p_add">+void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp);</span>
<span class="p_add">+void xpfo_free_pages(struct page *page, int order);</span>
<span class="p_add">+</span>
<span class="p_add">+#else /* !CONFIG_XPFO */</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void xpfo_kmap(void *kaddr, struct page *page) { }</span>
<span class="p_add">+static inline void xpfo_kunmap(void *kaddr, struct page *page) { }</span>
<span class="p_add">+static inline void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp) { }</span>
<span class="p_add">+static inline void xpfo_free_pages(struct page *page, int order) { }</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* CONFIG_XPFO */</span>
<span class="p_add">+</span>
<span class="p_add">+#endif /* _LINUX_XPFO_H */</span>
<span class="p_header">diff --git a/mm/Makefile b/mm/Makefile</span>
<span class="p_header">index 411bd24d4a7c..0be67cac8f6c 100644</span>
<span class="p_header">--- a/mm/Makefile</span>
<span class="p_header">+++ b/mm/Makefile</span>
<span class="p_chunk">@@ -104,3 +104,4 @@</span> <span class="p_context"> obj-$(CONFIG_FRAME_VECTOR) += frame_vector.o</span>
 obj-$(CONFIG_DEBUG_PAGE_REF) += debug_page_ref.o
 obj-$(CONFIG_HARDENED_USERCOPY) += usercopy.o
 obj-$(CONFIG_PERCPU_STATS) += percpu-stats.o
<span class="p_add">+obj-$(CONFIG_XPFO) += xpfo.o</span>
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index fc32aa81f359..f83d8a384fde 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -1058,6 +1058,7 @@</span> <span class="p_context"> static __always_inline bool free_pages_prepare(struct page *page,</span>
 	kernel_poison_pages(page, 1 &lt;&lt; order, 0);
 	kernel_map_pages(page, 1 &lt;&lt; order, 0);
 	kasan_free_pages(page, order);
<span class="p_add">+	xpfo_free_pages(page, order);</span>
 
 	return true;
 }
<span class="p_chunk">@@ -1753,6 +1754,7 @@</span> <span class="p_context"> inline void post_alloc_hook(struct page *page, unsigned int order,</span>
 	kernel_map_pages(page, 1 &lt;&lt; order, 1);
 	kernel_poison_pages(page, 1 &lt;&lt; order, 1);
 	kasan_alloc_pages(page, order);
<span class="p_add">+	xpfo_alloc_pages(page, order, gfp_flags);</span>
 	set_page_owner(page, order, gfp_flags);
 }
 
<span class="p_header">diff --git a/mm/page_ext.c b/mm/page_ext.c</span>
<span class="p_header">index 88ccc044b09a..4899df1f5d66 100644</span>
<span class="p_header">--- a/mm/page_ext.c</span>
<span class="p_header">+++ b/mm/page_ext.c</span>
<span class="p_chunk">@@ -7,6 +7,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/kmemleak.h&gt;
 #include &lt;linux/page_owner.h&gt;
 #include &lt;linux/page_idle.h&gt;
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
 
 /*
  * struct page extension
<span class="p_chunk">@@ -65,6 +66,9 @@</span> <span class="p_context"> static struct page_ext_operations *page_ext_ops[] = {</span>
 #if defined(CONFIG_IDLE_PAGE_TRACKING) &amp;&amp; !defined(CONFIG_64BIT)
 	&amp;page_idle_ops,
 #endif
<span class="p_add">+#ifdef CONFIG_XPFO</span>
<span class="p_add">+	&amp;page_xpfo_ops,</span>
<span class="p_add">+#endif</span>
 };
 
 static unsigned long total_usage;
<span class="p_header">diff --git a/mm/xpfo.c b/mm/xpfo.c</span>
new file mode 100644
<span class="p_header">index 000000000000..3cd45f68b5ad</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/mm/xpfo.c</span>
<span class="p_chunk">@@ -0,0 +1,208 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * Copyright (C) 2017 Hewlett Packard Enterprise Development, L.P.</span>
<span class="p_add">+ * Copyright (C) 2016 Brown University. All rights reserved.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Authors:</span>
<span class="p_add">+ *   Juerg Haefliger &lt;juerg.haefliger@hpe.com&gt;</span>
<span class="p_add">+ *   Vasileios P. Kemerlis &lt;vpk@cs.brown.edu&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify it</span>
<span class="p_add">+ * under the terms of the GNU General Public License version 2 as published by</span>
<span class="p_add">+ * the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+#include &lt;linux/module.h&gt;</span>
<span class="p_add">+#include &lt;linux/page_ext.h&gt;</span>
<span class="p_add">+#include &lt;linux/xpfo.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/tlbflush.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+/* XPFO page state flags */</span>
<span class="p_add">+enum xpfo_flags {</span>
<span class="p_add">+	XPFO_PAGE_USER,		/* Page is allocated to user-space */</span>
<span class="p_add">+	XPFO_PAGE_UNMAPPED,	/* Page is unmapped from the linear map */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+/* Per-page XPFO house-keeping data */</span>
<span class="p_add">+struct xpfo {</span>
<span class="p_add">+	unsigned long flags;	/* Page state */</span>
<span class="p_add">+	bool inited;		/* Map counter and lock initialized */</span>
<span class="p_add">+	atomic_t mapcount;	/* Counter for balancing map/unmap requests */</span>
<span class="p_add">+	spinlock_t maplock;	/* Lock to serialize map/unmap requests */</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+DEFINE_STATIC_KEY_FALSE(xpfo_inited);</span>
<span class="p_add">+</span>
<span class="p_add">+static bool xpfo_disabled __initdata;</span>
<span class="p_add">+</span>
<span class="p_add">+static int __init noxpfo_param(char *str)</span>
<span class="p_add">+{</span>
<span class="p_add">+	xpfo_disabled = true;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+early_param(&quot;noxpfo&quot;, noxpfo_param);</span>
<span class="p_add">+</span>
<span class="p_add">+static bool __init need_xpfo(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (xpfo_disabled) {</span>
<span class="p_add">+		printk(KERN_INFO &quot;XPFO disabled\n&quot;);</span>
<span class="p_add">+		return false;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return true;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void init_xpfo(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	printk(KERN_INFO &quot;XPFO enabled\n&quot;);</span>
<span class="p_add">+	static_branch_enable(&amp;xpfo_inited);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+struct page_ext_operations page_xpfo_ops = {</span>
<span class="p_add">+	.size = sizeof(struct xpfo),</span>
<span class="p_add">+	.need = need_xpfo,</span>
<span class="p_add">+	.init = init_xpfo,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static inline struct xpfo *lookup_xpfo(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (void *)lookup_page_ext(page) + page_xpfo_ops.offset;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_alloc_pages(struct page *page, int order, gfp_t gfp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i, flush_tlb = 0;</span>
<span class="p_add">+	struct xpfo *xpfo;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++)  {</span>
<span class="p_add">+		xpfo = lookup_xpfo(page + i);</span>
<span class="p_add">+</span>
<span class="p_add">+		BUG_ON(test_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags));</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Initialize the map lock and map counter */</span>
<span class="p_add">+		if (unlikely(!xpfo-&gt;inited)) {</span>
<span class="p_add">+			spin_lock_init(&amp;xpfo-&gt;maplock);</span>
<span class="p_add">+			atomic_set(&amp;xpfo-&gt;mapcount, 0);</span>
<span class="p_add">+			xpfo-&gt;inited = true;</span>
<span class="p_add">+		}</span>
<span class="p_add">+		BUG_ON(atomic_read(&amp;xpfo-&gt;mapcount));</span>
<span class="p_add">+</span>
<span class="p_add">+		if ((gfp &amp; GFP_HIGHUSER) == GFP_HIGHUSER) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * Tag the page as a user page and flush the TLB if it</span>
<span class="p_add">+			 * was previously allocated to the kernel.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (!test_and_set_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="p_add">+				flush_tlb = 1;</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			/* Tag the page as a non-user (kernel) page */</span>
<span class="p_add">+			clear_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flush_tlb)</span>
<span class="p_add">+		xpfo_flush_kernel_page(page, order);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_free_pages(struct page *page, int order)</span>
<span class="p_add">+{</span>
<span class="p_add">+	int i;</span>
<span class="p_add">+	struct xpfo *xpfo;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	for (i = 0; i &lt; (1 &lt;&lt; order); i++) {</span>
<span class="p_add">+		xpfo = lookup_xpfo(page + i);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (unlikely(!xpfo-&gt;inited)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * The page was allocated before page_ext was</span>
<span class="p_add">+			 * initialized, so it is a kernel page.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Map the page back into the kernel if it was previously</span>
<span class="p_add">+		 * allocated to user space.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (test_and_clear_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags)) {</span>
<span class="p_add">+			set_kpte(page_address(page + i), page + i,</span>
<span class="p_add">+				 PAGE_KERNEL);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kmap(void *kaddr, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct xpfo *xpfo;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	xpfo = lookup_xpfo(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was allocated before page_ext was initialized (which means</span>
<span class="p_add">+	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="p_add">+	 * do.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (unlikely(!xpfo-&gt;inited) || !test_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;xpfo-&gt;maplock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was previously allocated to user space, so map it back</span>
<span class="p_add">+	 * into the kernel. No TLB flush required.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if ((atomic_inc_return(&amp;xpfo-&gt;mapcount) == 1) &amp;&amp;</span>
<span class="p_add">+	    test_and_clear_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags))</span>
<span class="p_add">+		set_kpte(kaddr, page, PAGE_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;xpfo-&gt;maplock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_kmap);</span>
<span class="p_add">+</span>
<span class="p_add">+void xpfo_kunmap(void *kaddr, struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct xpfo *xpfo;</span>
<span class="p_add">+	unsigned long flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!static_branch_unlikely(&amp;xpfo_inited))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	xpfo = lookup_xpfo(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page was allocated before page_ext was initialized (which means</span>
<span class="p_add">+	 * it&#39;s a kernel page) or it&#39;s allocated to the kernel, so nothing to</span>
<span class="p_add">+	 * do.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (unlikely(!xpfo-&gt;inited) || !test_bit(XPFO_PAGE_USER, &amp;xpfo-&gt;flags))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_lock_irqsave(&amp;xpfo-&gt;maplock, flags);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The page is to be allocated back to user space, so unmap it from the</span>
<span class="p_add">+	 * kernel, flush the TLB and tag it as a user page.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (atomic_dec_return(&amp;xpfo-&gt;mapcount) == 0) {</span>
<span class="p_add">+		BUG_ON(test_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags));</span>
<span class="p_add">+		set_bit(XPFO_PAGE_UNMAPPED, &amp;xpfo-&gt;flags);</span>
<span class="p_add">+		set_kpte(kaddr, page, __pgprot(0));</span>
<span class="p_add">+		__flush_tlb_one((unsigned long)kaddr);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	spin_unlock_irqrestore(&amp;xpfo-&gt;maplock, flags);</span>
<span class="p_add">+}</span>
<span class="p_add">+EXPORT_SYMBOL(xpfo_kunmap);</span>
<span class="p_header">diff --git a/security/Kconfig b/security/Kconfig</span>
<span class="p_header">index e8e449444e65..be5145eeed7d 100644</span>
<span class="p_header">--- a/security/Kconfig</span>
<span class="p_header">+++ b/security/Kconfig</span>
<span class="p_chunk">@@ -6,6 +6,25 @@</span> <span class="p_context"> menu &quot;Security options&quot;</span>
 
 source security/keys/Kconfig
 
<span class="p_add">+config ARCH_SUPPORTS_XPFO</span>
<span class="p_add">+	bool</span>
<span class="p_add">+</span>
<span class="p_add">+config XPFO</span>
<span class="p_add">+	bool &quot;Enable eXclusive Page Frame Ownership (XPFO)&quot;</span>
<span class="p_add">+	default n</span>
<span class="p_add">+	depends on ARCH_SUPPORTS_XPFO</span>
<span class="p_add">+	select PAGE_EXTENSION</span>
<span class="p_add">+	help</span>
<span class="p_add">+	  This option offers protection against &#39;ret2dir&#39; kernel attacks.</span>
<span class="p_add">+	  When enabled, every time a page frame is allocated to user space, it</span>
<span class="p_add">+	  is unmapped from the direct mapped RAM region in kernel space</span>
<span class="p_add">+	  (physmap). Similarly, when a page frame is freed/reclaimed, it is</span>
<span class="p_add">+	  mapped back to physmap.</span>
<span class="p_add">+</span>
<span class="p_add">+	  There is a slight performance impact when this option is enabled.</span>
<span class="p_add">+</span>
<span class="p_add">+	  If in doubt, say &quot;N&quot;.</span>
<span class="p_add">+</span>
 config SECURITY_DMESG_RESTRICT
 	bool &quot;Restrict unprivileged access to the kernel syslog&quot;
 	default n

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



