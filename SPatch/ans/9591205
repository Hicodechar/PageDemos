
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V5,4/6] mm: reclaim MADV_FREE pages - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V5,4/6] mm: reclaim MADV_FREE pages</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Feb. 24, 2017, 9:31 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;14b8eb1d3f6bf6cc492833f183ac8c304e560484.1487965799.git.shli@fb.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9591205/mbox/"
   >mbox</a>
|
   <a href="/patch/9591205/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9591205/">/patch/9591205/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	900E9601AE for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 24 Feb 2017 21:32:49 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 7D78028922
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 24 Feb 2017 21:32:49 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 700062894F; Fri, 24 Feb 2017 21:32:49 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id AB24028922
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 24 Feb 2017 21:32:48 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751385AbdBXVby (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 24 Feb 2017 16:31:54 -0500
Received: from mx0a-00082601.pphosted.com ([67.231.145.42]:33233 &quot;EHLO
	mx0a-00082601.pphosted.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1751219AbdBXVbw (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 24 Feb 2017 16:31:52 -0500
Received: from pps.filterd (m0109334.ppops.net [127.0.0.1])
	by mx0a-00082601.pphosted.com (8.16.0.20/8.16.0.20) with SMTP id
	v1OLMcV1001348
	for &lt;linux-kernel@vger.kernel.org&gt;; Fri, 24 Feb 2017 13:31:52 -0800
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=fb.com;
	h=from : to : cc : subject
	: date : message-id : in-reply-to : references : mime-version :
	content-type; s=facebook;
	bh=EQrVqS8ClavtCwMBizXU9sapz6KfKQHUqEuRl6BMPUA=; 
	b=D9Jq0Qo9mPfQBLGlGHQ4m2s2D97+8MJriOtiKbEseXNL+pUgXp2DPK9/1q8uGKhFFy+E
	UfvIjVLncR7gyLEjmGRFLB6hdeJVpwSgc6qTEbTcY4J1uo4RYEUEDBcUsmKTk30iIgQb
	EGGA1zWwPRD2BLMStrSiyPhgGiql38GWEH0= 
Received: from mail.thefacebook.com ([199.201.64.23])
	by mx0a-00082601.pphosted.com with ESMTP id 28tukj88kq-2
	(version=TLSv1 cipher=ECDHE-RSA-AES256-SHA bits=256 verify=NOT)
	for &lt;linux-kernel@vger.kernel.org&gt;; Fri, 24 Feb 2017 13:31:52 -0800
Received: from mx-out.facebook.com (192.168.52.123) by
	PRN-CHUB07.TheFacebook.com (192.168.16.17) with Microsoft SMTP Server
	(TLS) id 14.3.319.2; Fri, 24 Feb 2017 13:31:50 -0800
Received: from facebook.com (2401:db00:21:603d:face:0:19:0)     by
	mx-out.facebook.com (10.102.107.99) with ESMTP id
	a77383c8fad811e684420002c99293a0-df9d4a00 for
	&lt;linux-kernel@vger.kernel.org&gt;; Fri, 24 Feb 2017 13:31:51 -0800
Received: by devbig638.prn2.facebook.com (Postfix, from userid 11222)   id
	175EF43A2852; Fri, 24 Feb 2017 13:31:50 -0800 (PST)
Smtp-Origin-Hostprefix: devbig
From: Shaohua Li &lt;shli@fb.com&gt;
Smtp-Origin-Hostname: devbig638.prn2.facebook.com
To: &lt;linux-mm@kvack.org&gt;, &lt;linux-kernel@vger.kernel.org&gt;
CC: &lt;Kernel-team@fb.com&gt;, &lt;mhocko@suse.com&gt;, &lt;minchan@kernel.org&gt;,
	&lt;hughd@google.com&gt;, &lt;hannes@cmpxchg.org&gt;, &lt;riel@redhat.com&gt;,
	&lt;mgorman@techsingularity.net&gt;, &lt;akpm@linux-foundation.org&gt;
Smtp-Origin-Cluster: prn2c22
Subject: [PATCH V5 4/6] mm: reclaim MADV_FREE pages
Date: Fri, 24 Feb 2017 13:31:47 -0800
Message-ID: &lt;14b8eb1d3f6bf6cc492833f183ac8c304e560484.1487965799.git.shli@fb.com&gt;
X-Mailer: git-send-email 2.9.3
In-Reply-To: &lt;cover.1487965799.git.shli@fb.com&gt;
References: &lt;cover.1487965799.git.shli@fb.com&gt;
X-FB-Internal: Safe
MIME-Version: 1.0
Content-Type: text/plain
X-Proofpoint-Spam-Reason: safe
X-FB-Internal: Safe
X-Proofpoint-Virus-Version: vendor=fsecure engine=2.50.10432:, ,
	definitions=2017-02-24_13:, , signatures=0
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 24, 2017, 9:31 p.m.</div>
<pre class="content">
When memory pressure is high, we free MADV_FREE pages. If the pages are
not dirty in pte, the pages could be freed immediately. Otherwise we
can&#39;t reclaim them. We put the pages back to anonumous LRU list (by
setting SwapBacked flag) and the pages will be reclaimed in normal
swapout way.

We use normal page reclaim policy. Since MADV_FREE pages are put into
inactive file list, such pages and inactive file pages are reclaimed
according to their age. This is expected, because we don&#39;t want to
reclaim too many MADV_FREE pages before used once pages.

Based on Minchan&#39;s original patch

Cc: Michal Hocko &lt;mhocko@suse.com&gt;
Cc: Minchan Kim &lt;minchan@kernel.org&gt;
Cc: Hugh Dickins &lt;hughd@google.com&gt;
Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;
Cc: Rik van Riel &lt;riel@redhat.com&gt;
Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
<span class="signed-off-by">Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
---
 include/linux/rmap.h |  2 +-
 mm/huge_memory.c     |  2 ++
 mm/madvise.c         |  1 +
 mm/rmap.c            | 40 +++++++++++++++++-----------------------
 mm/vmscan.c          | 34 ++++++++++++++++++++++------------
 5 files changed, 43 insertions(+), 36 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Feb. 27, 2017, 6:33 a.m.</div>
<pre class="content">
Hi Shaohua,

On Fri, Feb 24, 2017 at 01:31:47PM -0800, Shaohua Li wrote:
<span class="quote">&gt; When memory pressure is high, we free MADV_FREE pages. If the pages are</span>
<span class="quote">&gt; not dirty in pte, the pages could be freed immediately. Otherwise we</span>
<span class="quote">&gt; can&#39;t reclaim them. We put the pages back to anonumous LRU list (by</span>
<span class="quote">&gt; setting SwapBacked flag) and the pages will be reclaimed in normal</span>
<span class="quote">&gt; swapout way.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We use normal page reclaim policy. Since MADV_FREE pages are put into</span>
<span class="quote">&gt; inactive file list, such pages and inactive file pages are reclaimed</span>
<span class="quote">&gt; according to their age. This is expected, because we don&#39;t want to</span>
<span class="quote">&gt; reclaim too many MADV_FREE pages before used once pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Based on Minchan&#39;s original patch</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/rmap.h |  2 +-</span>
<span class="quote">&gt;  mm/huge_memory.c     |  2 ++</span>
<span class="quote">&gt;  mm/madvise.c         |  1 +</span>
<span class="quote">&gt;  mm/rmap.c            | 40 +++++++++++++++++-----------------------</span>
<span class="quote">&gt;  mm/vmscan.c          | 34 ++++++++++++++++++++++------------</span>
<span class="quote">&gt;  5 files changed, 43 insertions(+), 36 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="quote">&gt; index 7a39414..fee10d7 100644</span>
<span class="quote">&gt; --- a/include/linux/rmap.h</span>
<span class="quote">&gt; +++ b/include/linux/rmap.h</span>
<span class="quote">&gt; @@ -298,6 +298,6 @@ static inline int page_mkclean(struct page *page)</span>
<span class="quote">&gt;  #define SWAP_AGAIN	1</span>
<span class="quote">&gt;  #define SWAP_FAIL	2</span>
<span class="quote">&gt;  #define SWAP_MLOCK	3</span>
<span class="quote">&gt; -#define SWAP_LZFREE	4</span>
<span class="quote">&gt; +#define SWAP_DIRTY	4</span>

I still don&#39;t convinced why we should introduce SWAP_DIRTY in try_to_unmap.
https://marc.info/?l=linux-mm&amp;m=148797879123238&amp;w=2

We have been SetPageMlocked in there but why cannot we SetPageSwapBacked
in there? It&#39;s not a thing to change LRU type but it&#39;s just indication
we found the page&#39;s status changed in late.
<span class="quote">
&gt;  </span>
<span class="quote">&gt;  #endif	/* _LINUX_RMAP_H */</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index 3b7ee0c..4c7454b 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1571,6 +1571,8 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		set_pmd_at(mm, addr, pmd, orig_pmd);</span>
<span class="quote">&gt;  		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mark_page_lazyfree(page);</span>
<span class="quote">&gt;  	ret = true;</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; index 61e10b1..225af7d 100644</span>
<span class="quote">&gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; @@ -413,6 +413,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  			set_pte_at(mm, addr, pte, ptent);</span>
<span class="quote">&gt;  			tlb_remove_tlb_entry(tlb, pte, addr);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +		mark_page_lazyfree(page);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	if (nr_swap) {</span>
<span class="quote">&gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; index c621088..bb45712 100644</span>
<span class="quote">&gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; @@ -1281,11 +1281,6 @@ void page_remove_rmap(struct page *page, bool compound)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct rmap_private {</span>
<span class="quote">&gt; -	enum ttu_flags flags;</span>
<span class="quote">&gt; -	int lazyfreed;</span>
<span class="quote">&gt; -};</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * @arg: enum ttu_flags will be passed to this argument</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; @@ -1301,8 +1296,7 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	pte_t pteval;</span>
<span class="quote">&gt;  	struct page *subpage;</span>
<span class="quote">&gt;  	int ret = SWAP_AGAIN;</span>
<span class="quote">&gt; -	struct rmap_private *rp = arg;</span>
<span class="quote">&gt; -	enum ttu_flags flags = rp-&gt;flags;</span>
<span class="quote">&gt; +	enum ttu_flags flags = (enum ttu_flags)arg;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* munlock has nothing to gain from examining un-locked vmas */</span>
<span class="quote">&gt;  	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))</span>
<span class="quote">&gt; @@ -1419,11 +1413,21 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			VM_BUG_ON_PAGE(!PageSwapCache(page) &amp;&amp; PageSwapBacked(page),</span>
<span class="quote">&gt;  				page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -			if (!PageDirty(page)) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * swapin page could be clean, it has data stored in</span>
<span class="quote">&gt; +			 * swap. We can&#39;t silently discard it without setting</span>
<span class="quote">&gt; +			 * swap entry in the page table.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (!PageDirty(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="quote">&gt;  				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt;  				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; -				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt;  				goto discard;</span>
<span class="quote">&gt; +			} else if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt; +				/* dirty MADV_FREE page */</span>
<span class="quote">&gt; +				set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; +				ret = SWAP_DIRTY;</span>
<span class="quote">&gt; +				page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; +				break;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			if (swap_duplicate(entry) &lt; 0) {</span>
<span class="quote">&gt; @@ -1491,18 +1495,15 @@ static int page_mapcount_is_zero(struct page *page)</span>
<span class="quote">&gt;   * SWAP_AGAIN	- we missed a mapping, try again later</span>
<span class="quote">&gt;   * SWAP_FAIL	- the page is unswappable</span>
<span class="quote">&gt;   * SWAP_MLOCK	- page is mlocked.</span>
<span class="quote">&gt; + * SWAP_DIRTY	- page is dirty MADV_FREE page</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; -	struct rmap_private rp = {</span>
<span class="quote">&gt; -		.flags = flags,</span>
<span class="quote">&gt; -		.lazyfreed = 0,</span>
<span class="quote">&gt; -	};</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	struct rmap_walk_control rwc = {</span>
<span class="quote">&gt;  		.rmap_one = try_to_unmap_one,</span>
<span class="quote">&gt; -		.arg = &amp;rp,</span>
<span class="quote">&gt; +		.arg = (void *)flags,</span>
<span class="quote">&gt;  		.done = page_mapcount_is_zero,</span>
<span class="quote">&gt;  		.anon_lock = page_lock_anon_vma_read,</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt; @@ -1523,11 +1524,8 @@ int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  		ret = rmap_walk(page, &amp;rwc);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page)) {</span>
<span class="quote">&gt; +	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page))</span>
<span class="quote">&gt;  		ret = SWAP_SUCCESS;</span>
<span class="quote">&gt; -		if (rp.lazyfreed &amp;&amp; !PageDirty(page))</span>
<span class="quote">&gt; -			ret = SWAP_LZFREE;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1554,14 +1552,10 @@ static int page_not_mapped(struct page *page)</span>
<span class="quote">&gt;  int try_to_munlock(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; -	struct rmap_private rp = {</span>
<span class="quote">&gt; -		.flags = TTU_MUNLOCK,</span>
<span class="quote">&gt; -		.lazyfreed = 0,</span>
<span class="quote">&gt; -	};</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	struct rmap_walk_control rwc = {</span>
<span class="quote">&gt;  		.rmap_one = try_to_unmap_one,</span>
<span class="quote">&gt; -		.arg = &amp;rp,</span>
<span class="quote">&gt; +		.arg = (void *)TTU_MUNLOCK,</span>
<span class="quote">&gt;  		.done = page_not_mapped,</span>
<span class="quote">&gt;  		.anon_lock = page_lock_anon_vma_read,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt; index 68ea50d..16ad821 100644</span>
<span class="quote">&gt; --- a/mm/vmscan.c</span>
<span class="quote">&gt; +++ b/mm/vmscan.c</span>
<span class="quote">&gt; @@ -911,7 +911,8 @@ static void page_check_dirty_writeback(struct page *page,</span>
<span class="quote">&gt;  	 * Anonymous pages are not handled by flushers and must be written</span>
<span class="quote">&gt;  	 * from reclaim context. Do not stall reclaim based on them</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (!page_is_file_cache(page)) {</span>
<span class="quote">&gt; +	if (!page_is_file_cache(page) ||</span>
<span class="quote">&gt; +	    (PageAnon(page) &amp;&amp; !PageSwapBacked(page))) {</span>
<span class="quote">&gt;  		*dirty = false;</span>
<span class="quote">&gt;  		*writeback = false;</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; @@ -992,7 +993,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  			goto keep_locked;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* Double the slab pressure for mapped and swapcache pages */</span>
<span class="quote">&gt; -		if (page_mapped(page) || PageSwapCache(page))</span>
<span class="quote">&gt; +		if ((page_mapped(page) || PageSwapCache(page)) &amp;&amp;</span>
<span class="quote">&gt; +		    !(PageAnon(page) &amp;&amp; !PageSwapBacked(page)))</span>
<span class="quote">&gt;  			sc-&gt;nr_scanned++;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		may_enter_fs = (sc-&gt;gfp_mask &amp; __GFP_FS) ||</span>
<span class="quote">&gt; @@ -1118,8 +1120,10 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Anonymous process memory has backing store?</span>
<span class="quote">&gt;  		 * Try to allocate it some swap space here.</span>
<span class="quote">&gt; +		 * Lazyfree page could be freed directly</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="quote">&gt; +		if (PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="quote">&gt; +		    !PageSwapCache(page)) {</span>
<span class="quote">&gt;  			if (!(sc-&gt;gfp_mask &amp; __GFP_IO))</span>
<span class="quote">&gt;  				goto keep_locked;</span>
<span class="quote">&gt;  			if (!add_to_swap(page, page_list))</span>
<span class="quote">&gt; @@ -1140,9 +1144,12 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		 * The page is mapped into the page tables of one or more</span>
<span class="quote">&gt;  		 * processes. Try to unmap it here.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="quote">&gt; +		if (page_mapped(page)) {</span>
<span class="quote">&gt;  			switch (ret = try_to_unmap(page,</span>
<span class="quote">&gt;  				ttu_flags | TTU_BATCH_FLUSH)) {</span>
<span class="quote">&gt; +			case SWAP_DIRTY:</span>
<span class="quote">&gt; +				SetPageSwapBacked(page);</span>
<span class="quote">&gt; +				/* fall through */</span>
<span class="quote">&gt;  			case SWAP_FAIL:</span>
<span class="quote">&gt;  				nr_unmap_fail++;</span>
<span class="quote">&gt;  				goto activate_locked;</span>
<span class="quote">&gt; @@ -1150,8 +1157,6 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  				goto keep_locked;</span>
<span class="quote">&gt;  			case SWAP_MLOCK:</span>
<span class="quote">&gt;  				goto cull_mlocked;</span>
<span class="quote">&gt; -			case SWAP_LZFREE:</span>
<span class="quote">&gt; -				goto lazyfree;</span>
<span class="quote">&gt;  			case SWAP_SUCCESS:</span>
<span class="quote">&gt;  				; /* try to free the page below */</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; @@ -1263,10 +1268,18 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -lazyfree:</span>
<span class="quote">&gt; -		if (!mapping || !__remove_mapping(mapping, page, true))</span>
<span class="quote">&gt; -			goto keep_locked;</span>
<span class="quote">&gt; +		if (PageAnon(page) &amp;&amp; !PageSwapBacked(page)) {</span>
<span class="quote">&gt; +			/* follow __remove_mapping for reference */</span>
<span class="quote">&gt; +			if (!page_ref_freeze(page, 1))</span>
<span class="quote">&gt; +				goto keep_locked;</span>
<span class="quote">&gt; +			if (PageDirty(page)) {</span>
<span class="quote">&gt; +				page_ref_unfreeze(page, 1);</span>
<span class="quote">&gt; +				goto keep_locked;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +			count_vm_event(PGLAZYFREED);</span>
<span class="quote">&gt; +		} else if (!mapping || !__remove_mapping(mapping, page, true))</span>
<span class="quote">&gt; +			goto keep_locked;</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * At this point, we have no other references and there is</span>
<span class="quote">&gt;  		 * no way to pick any more up (removed from LRU, removed</span>
<span class="quote">&gt; @@ -1276,9 +1289,6 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		__ClearPageLocked(page);</span>
<span class="quote">&gt;  free_it:</span>
<span class="quote">&gt; -		if (ret == SWAP_LZFREE)</span>
<span class="quote">&gt; -			count_vm_event(PGLAZYFREED);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  		nr_reclaimed++;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.9.3</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Feb. 27, 2017, 3:05 p.m.</div>
<pre class="content">
On Fri 24-02-17 13:31:47, Shaohua Li wrote:
<span class="quote">&gt; When memory pressure is high, we free MADV_FREE pages. If the pages are</span>
<span class="quote">&gt; not dirty in pte, the pages could be freed immediately. Otherwise we</span>
<span class="quote">&gt; can&#39;t reclaim them. We put the pages back to anonumous LRU list (by</span>
<span class="quote">&gt; setting SwapBacked flag) and the pages will be reclaimed in normal</span>
<span class="quote">&gt; swapout way.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We use normal page reclaim policy. Since MADV_FREE pages are put into</span>
<span class="quote">&gt; inactive file list, such pages and inactive file pages are reclaimed</span>
<span class="quote">&gt; according to their age. This is expected, because we don&#39;t want to</span>
<span class="quote">&gt; reclaim too many MADV_FREE pages before used once pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Based on Minchan&#39;s original patch</span>

OK, this looks much more cleaner and easier to follow than the original
version I have seen.
<span class="quote"> 
&gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="acked-by">
Acked-by: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  include/linux/rmap.h |  2 +-</span>
<span class="quote">&gt;  mm/huge_memory.c     |  2 ++</span>
<span class="quote">&gt;  mm/madvise.c         |  1 +</span>
<span class="quote">&gt;  mm/rmap.c            | 40 +++++++++++++++++-----------------------</span>
<span class="quote">&gt;  mm/vmscan.c          | 34 ++++++++++++++++++++++------------</span>
<span class="quote">&gt;  5 files changed, 43 insertions(+), 36 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="quote">&gt; index 7a39414..fee10d7 100644</span>
<span class="quote">&gt; --- a/include/linux/rmap.h</span>
<span class="quote">&gt; +++ b/include/linux/rmap.h</span>
<span class="quote">&gt; @@ -298,6 +298,6 @@ static inline int page_mkclean(struct page *page)</span>
<span class="quote">&gt;  #define SWAP_AGAIN	1</span>
<span class="quote">&gt;  #define SWAP_FAIL	2</span>
<span class="quote">&gt;  #define SWAP_MLOCK	3</span>
<span class="quote">&gt; -#define SWAP_LZFREE	4</span>
<span class="quote">&gt; +#define SWAP_DIRTY	4</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #endif	/* _LINUX_RMAP_H */</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index 3b7ee0c..4c7454b 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1571,6 +1571,8 @@ bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		set_pmd_at(mm, addr, pmd, orig_pmd);</span>
<span class="quote">&gt;  		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	mark_page_lazyfree(page);</span>
<span class="quote">&gt;  	ret = true;</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	spin_unlock(ptl);</span>
<span class="quote">&gt; diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="quote">&gt; index 61e10b1..225af7d 100644</span>
<span class="quote">&gt; --- a/mm/madvise.c</span>
<span class="quote">&gt; +++ b/mm/madvise.c</span>
<span class="quote">&gt; @@ -413,6 +413,7 @@ static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="quote">&gt;  			set_pte_at(mm, addr, pte, ptent);</span>
<span class="quote">&gt;  			tlb_remove_tlb_entry(tlb, pte, addr);</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +		mark_page_lazyfree(page);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  out:</span>
<span class="quote">&gt;  	if (nr_swap) {</span>
<span class="quote">&gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; index c621088..bb45712 100644</span>
<span class="quote">&gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; @@ -1281,11 +1281,6 @@ void page_remove_rmap(struct page *page, bool compound)</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -struct rmap_private {</span>
<span class="quote">&gt; -	enum ttu_flags flags;</span>
<span class="quote">&gt; -	int lazyfreed;</span>
<span class="quote">&gt; -};</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * @arg: enum ttu_flags will be passed to this argument</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt; @@ -1301,8 +1296,7 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	pte_t pteval;</span>
<span class="quote">&gt;  	struct page *subpage;</span>
<span class="quote">&gt;  	int ret = SWAP_AGAIN;</span>
<span class="quote">&gt; -	struct rmap_private *rp = arg;</span>
<span class="quote">&gt; -	enum ttu_flags flags = rp-&gt;flags;</span>
<span class="quote">&gt; +	enum ttu_flags flags = (enum ttu_flags)arg;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	/* munlock has nothing to gain from examining un-locked vmas */</span>
<span class="quote">&gt;  	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))</span>
<span class="quote">&gt; @@ -1419,11 +1413,21 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			VM_BUG_ON_PAGE(!PageSwapCache(page) &amp;&amp; PageSwapBacked(page),</span>
<span class="quote">&gt;  				page);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -			if (!PageDirty(page)) {</span>
<span class="quote">&gt; +			/*</span>
<span class="quote">&gt; +			 * swapin page could be clean, it has data stored in</span>
<span class="quote">&gt; +			 * swap. We can&#39;t silently discard it without setting</span>
<span class="quote">&gt; +			 * swap entry in the page table.</span>
<span class="quote">&gt; +			 */</span>
<span class="quote">&gt; +			if (!PageDirty(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="quote">&gt;  				/* It&#39;s a freeable page by MADV_FREE */</span>
<span class="quote">&gt;  				dec_mm_counter(mm, MM_ANONPAGES);</span>
<span class="quote">&gt; -				rp-&gt;lazyfreed++;</span>
<span class="quote">&gt;  				goto discard;</span>
<span class="quote">&gt; +			} else if (!PageSwapBacked(page)) {</span>
<span class="quote">&gt; +				/* dirty MADV_FREE page */</span>
<span class="quote">&gt; +				set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="quote">&gt; +				ret = SWAP_DIRTY;</span>
<span class="quote">&gt; +				page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="quote">&gt; +				break;</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  			if (swap_duplicate(entry) &lt; 0) {</span>
<span class="quote">&gt; @@ -1491,18 +1495,15 @@ static int page_mapcount_is_zero(struct page *page)</span>
<span class="quote">&gt;   * SWAP_AGAIN	- we missed a mapping, try again later</span>
<span class="quote">&gt;   * SWAP_FAIL	- the page is unswappable</span>
<span class="quote">&gt;   * SWAP_MLOCK	- page is mlocked.</span>
<span class="quote">&gt; + * SWAP_DIRTY	- page is dirty MADV_FREE page</span>
<span class="quote">&gt;   */</span>
<span class="quote">&gt;  int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; -	struct rmap_private rp = {</span>
<span class="quote">&gt; -		.flags = flags,</span>
<span class="quote">&gt; -		.lazyfreed = 0,</span>
<span class="quote">&gt; -	};</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	struct rmap_walk_control rwc = {</span>
<span class="quote">&gt;  		.rmap_one = try_to_unmap_one,</span>
<span class="quote">&gt; -		.arg = &amp;rp,</span>
<span class="quote">&gt; +		.arg = (void *)flags,</span>
<span class="quote">&gt;  		.done = page_mapcount_is_zero,</span>
<span class="quote">&gt;  		.anon_lock = page_lock_anon_vma_read,</span>
<span class="quote">&gt;  	};</span>
<span class="quote">&gt; @@ -1523,11 +1524,8 @@ int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
<span class="quote">&gt;  	else</span>
<span class="quote">&gt;  		ret = rmap_walk(page, &amp;rwc);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page)) {</span>
<span class="quote">&gt; +	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page))</span>
<span class="quote">&gt;  		ret = SWAP_SUCCESS;</span>
<span class="quote">&gt; -		if (rp.lazyfreed &amp;&amp; !PageDirty(page))</span>
<span class="quote">&gt; -			ret = SWAP_LZFREE;</span>
<span class="quote">&gt; -	}</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -1554,14 +1552,10 @@ static int page_not_mapped(struct page *page)</span>
<span class="quote">&gt;  int try_to_munlock(struct page *page)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	int ret;</span>
<span class="quote">&gt; -	struct rmap_private rp = {</span>
<span class="quote">&gt; -		.flags = TTU_MUNLOCK,</span>
<span class="quote">&gt; -		.lazyfreed = 0,</span>
<span class="quote">&gt; -	};</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	struct rmap_walk_control rwc = {</span>
<span class="quote">&gt;  		.rmap_one = try_to_unmap_one,</span>
<span class="quote">&gt; -		.arg = &amp;rp,</span>
<span class="quote">&gt; +		.arg = (void *)TTU_MUNLOCK,</span>
<span class="quote">&gt;  		.done = page_not_mapped,</span>
<span class="quote">&gt;  		.anon_lock = page_lock_anon_vma_read,</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="quote">&gt; index 68ea50d..16ad821 100644</span>
<span class="quote">&gt; --- a/mm/vmscan.c</span>
<span class="quote">&gt; +++ b/mm/vmscan.c</span>
<span class="quote">&gt; @@ -911,7 +911,8 @@ static void page_check_dirty_writeback(struct page *page,</span>
<span class="quote">&gt;  	 * Anonymous pages are not handled by flushers and must be written</span>
<span class="quote">&gt;  	 * from reclaim context. Do not stall reclaim based on them</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if (!page_is_file_cache(page)) {</span>
<span class="quote">&gt; +	if (!page_is_file_cache(page) ||</span>
<span class="quote">&gt; +	    (PageAnon(page) &amp;&amp; !PageSwapBacked(page))) {</span>
<span class="quote">&gt;  		*dirty = false;</span>
<span class="quote">&gt;  		*writeback = false;</span>
<span class="quote">&gt;  		return;</span>
<span class="quote">&gt; @@ -992,7 +993,8 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  			goto keep_locked;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/* Double the slab pressure for mapped and swapcache pages */</span>
<span class="quote">&gt; -		if (page_mapped(page) || PageSwapCache(page))</span>
<span class="quote">&gt; +		if ((page_mapped(page) || PageSwapCache(page)) &amp;&amp;</span>
<span class="quote">&gt; +		    !(PageAnon(page) &amp;&amp; !PageSwapBacked(page)))</span>
<span class="quote">&gt;  			sc-&gt;nr_scanned++;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		may_enter_fs = (sc-&gt;gfp_mask &amp; __GFP_FS) ||</span>
<span class="quote">&gt; @@ -1118,8 +1120,10 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * Anonymous process memory has backing store?</span>
<span class="quote">&gt;  		 * Try to allocate it some swap space here.</span>
<span class="quote">&gt; +		 * Lazyfree page could be freed directly</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="quote">&gt; +		if (PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="quote">&gt; +		    !PageSwapCache(page)) {</span>
<span class="quote">&gt;  			if (!(sc-&gt;gfp_mask &amp; __GFP_IO))</span>
<span class="quote">&gt;  				goto keep_locked;</span>
<span class="quote">&gt;  			if (!add_to_swap(page, page_list))</span>
<span class="quote">&gt; @@ -1140,9 +1144,12 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		 * The page is mapped into the page tables of one or more</span>
<span class="quote">&gt;  		 * processes. Try to unmap it here.</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt; -		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="quote">&gt; +		if (page_mapped(page)) {</span>
<span class="quote">&gt;  			switch (ret = try_to_unmap(page,</span>
<span class="quote">&gt;  				ttu_flags | TTU_BATCH_FLUSH)) {</span>
<span class="quote">&gt; +			case SWAP_DIRTY:</span>
<span class="quote">&gt; +				SetPageSwapBacked(page);</span>
<span class="quote">&gt; +				/* fall through */</span>
<span class="quote">&gt;  			case SWAP_FAIL:</span>
<span class="quote">&gt;  				nr_unmap_fail++;</span>
<span class="quote">&gt;  				goto activate_locked;</span>
<span class="quote">&gt; @@ -1150,8 +1157,6 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  				goto keep_locked;</span>
<span class="quote">&gt;  			case SWAP_MLOCK:</span>
<span class="quote">&gt;  				goto cull_mlocked;</span>
<span class="quote">&gt; -			case SWAP_LZFREE:</span>
<span class="quote">&gt; -				goto lazyfree;</span>
<span class="quote">&gt;  			case SWAP_SUCCESS:</span>
<span class="quote">&gt;  				; /* try to free the page below */</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt; @@ -1263,10 +1268,18 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  			}</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -lazyfree:</span>
<span class="quote">&gt; -		if (!mapping || !__remove_mapping(mapping, page, true))</span>
<span class="quote">&gt; -			goto keep_locked;</span>
<span class="quote">&gt; +		if (PageAnon(page) &amp;&amp; !PageSwapBacked(page)) {</span>
<span class="quote">&gt; +			/* follow __remove_mapping for reference */</span>
<span class="quote">&gt; +			if (!page_ref_freeze(page, 1))</span>
<span class="quote">&gt; +				goto keep_locked;</span>
<span class="quote">&gt; +			if (PageDirty(page)) {</span>
<span class="quote">&gt; +				page_ref_unfreeze(page, 1);</span>
<span class="quote">&gt; +				goto keep_locked;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +			count_vm_event(PGLAZYFREED);</span>
<span class="quote">&gt; +		} else if (!mapping || !__remove_mapping(mapping, page, true))</span>
<span class="quote">&gt; +			goto keep_locked;</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * At this point, we have no other references and there is</span>
<span class="quote">&gt;  		 * no way to pick any more up (removed from LRU, removed</span>
<span class="quote">&gt; @@ -1276,9 +1289,6 @@ static unsigned long shrink_page_list(struct list_head *page_list,</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		__ClearPageLocked(page);</span>
<span class="quote">&gt;  free_it:</span>
<span class="quote">&gt; -		if (ret == SWAP_LZFREE)</span>
<span class="quote">&gt; -			count_vm_event(PGLAZYFREED);</span>
<span class="quote">&gt; -</span>
<span class="quote">&gt;  		nr_reclaimed++;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.9.3</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=117011">Shaohua Li</a> - Feb. 27, 2017, 4:19 p.m.</div>
<pre class="content">
On Mon, Feb 27, 2017 at 03:33:15PM +0900, Minchan Kim wrote:
<span class="quote">&gt; Hi Shaohua,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Fri, Feb 24, 2017 at 01:31:47PM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; When memory pressure is high, we free MADV_FREE pages. If the pages are</span>
<span class="quote">&gt; &gt; not dirty in pte, the pages could be freed immediately. Otherwise we</span>
<span class="quote">&gt; &gt; can&#39;t reclaim them. We put the pages back to anonumous LRU list (by</span>
<span class="quote">&gt; &gt; setting SwapBacked flag) and the pages will be reclaimed in normal</span>
<span class="quote">&gt; &gt; swapout way.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We use normal page reclaim policy. Since MADV_FREE pages are put into</span>
<span class="quote">&gt; &gt; inactive file list, such pages and inactive file pages are reclaimed</span>
<span class="quote">&gt; &gt; according to their age. This is expected, because we don&#39;t want to</span>
<span class="quote">&gt; &gt; reclaim too many MADV_FREE pages before used once pages.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Based on Minchan&#39;s original patch</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; &gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; &gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; &gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; &gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; &gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  include/linux/rmap.h |  2 +-</span>
<span class="quote">&gt; &gt;  mm/huge_memory.c     |  2 ++</span>
<span class="quote">&gt; &gt;  mm/madvise.c         |  1 +</span>
<span class="quote">&gt; &gt;  mm/rmap.c            | 40 +++++++++++++++++-----------------------</span>
<span class="quote">&gt; &gt;  mm/vmscan.c          | 34 ++++++++++++++++++++++------------</span>
<span class="quote">&gt; &gt;  5 files changed, 43 insertions(+), 36 deletions(-)</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; index 7a39414..fee10d7 100644</span>
<span class="quote">&gt; &gt; --- a/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; +++ b/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; @@ -298,6 +298,6 @@ static inline int page_mkclean(struct page *page)</span>
<span class="quote">&gt; &gt;  #define SWAP_AGAIN	1</span>
<span class="quote">&gt; &gt;  #define SWAP_FAIL	2</span>
<span class="quote">&gt; &gt;  #define SWAP_MLOCK	3</span>
<span class="quote">&gt; &gt; -#define SWAP_LZFREE	4</span>
<span class="quote">&gt; &gt; +#define SWAP_DIRTY	4</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I still don&#39;t convinced why we should introduce SWAP_DIRTY in try_to_unmap.</span>
<span class="quote">&gt; https://marc.info/?l=linux-mm&amp;m=148797879123238&amp;w=2</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We have been SetPageMlocked in there but why cannot we SetPageSwapBacked</span>
<span class="quote">&gt; in there? It&#39;s not a thing to change LRU type but it&#39;s just indication</span>
<span class="quote">&gt; we found the page&#39;s status changed in late.</span>

This one I don&#39;t have strong preference. Personally I agree with Johannes,
handling failure in vmscan sounds better. But since the failure handling is
just one statement, this probably doesn&#39;t make too much difference. If Johannes
and you made an agreement, I&#39;ll follow.

Thanks,
Shaohua
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Feb. 27, 2017, 4:32 p.m.</div>
<pre class="content">
On Mon 27-02-17 08:19:08, Shaohua Li wrote:
<span class="quote">&gt; On Mon, Feb 27, 2017 at 03:33:15PM +0900, Minchan Kim wrote:</span>
[...]
<span class="quote">&gt; &gt; &gt; --- a/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; &gt; +++ b/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; &gt; @@ -298,6 +298,6 @@ static inline int page_mkclean(struct page *page)</span>
<span class="quote">&gt; &gt; &gt;  #define SWAP_AGAIN	1</span>
<span class="quote">&gt; &gt; &gt;  #define SWAP_FAIL	2</span>
<span class="quote">&gt; &gt; &gt;  #define SWAP_MLOCK	3</span>
<span class="quote">&gt; &gt; &gt; -#define SWAP_LZFREE	4</span>
<span class="quote">&gt; &gt; &gt; +#define SWAP_DIRTY	4</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I still don&#39;t convinced why we should introduce SWAP_DIRTY in try_to_unmap.</span>
<span class="quote">&gt; &gt; https://marc.info/?l=linux-mm&amp;m=148797879123238&amp;w=2</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We have been SetPageMlocked in there but why cannot we SetPageSwapBacked</span>
<span class="quote">&gt; &gt; in there? It&#39;s not a thing to change LRU type but it&#39;s just indication</span>
<span class="quote">&gt; &gt; we found the page&#39;s status changed in late.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This one I don&#39;t have strong preference. Personally I agree with Johannes,</span>
<span class="quote">&gt; handling failure in vmscan sounds better. But since the failure handling is</span>
<span class="quote">&gt; just one statement, this probably doesn&#39;t make too much difference. If Johannes</span>
<span class="quote">&gt; and you made an agreement, I&#39;ll follow.</span>

FWIW I like your current SWAP_DIRTY and the later handling at the vmscan
level more.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - Feb. 27, 2017, 5:21 p.m.</div>
<pre class="content">
On Fri, Feb 24, 2017 at 01:31:47PM -0800, Shaohua Li wrote:
<span class="quote">&gt; When memory pressure is high, we free MADV_FREE pages. If the pages are</span>
<span class="quote">&gt; not dirty in pte, the pages could be freed immediately. Otherwise we</span>
<span class="quote">&gt; can&#39;t reclaim them. We put the pages back to anonumous LRU list (by</span>
<span class="quote">&gt; setting SwapBacked flag) and the pages will be reclaimed in normal</span>
<span class="quote">&gt; swapout way.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We use normal page reclaim policy. Since MADV_FREE pages are put into</span>
<span class="quote">&gt; inactive file list, such pages and inactive file pages are reclaimed</span>
<span class="quote">&gt; according to their age. This is expected, because we don&#39;t want to</span>
<span class="quote">&gt; reclaim too many MADV_FREE pages before used once pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Based on Minchan&#39;s original patch</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="acked-by">
Acked-by: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>

FWIW, I agree with Minchan that this could be folded into the previous
patch and would be a little neater. But I don&#39;t feel strongly in this
case since I didn&#39;t have any trouble reviewing the patches like this -
void mark_page_lazyfree(struct page *) is an easy API to remember.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=113021">Hillf Danton</a> - Feb. 28, 2017, 3:21 a.m.</div>
<pre class="content">
On February 25, 2017 5:32 AM Shaohua Li wrote: 
<span class="quote">&gt; </span>
<span class="quote">&gt; When memory pressure is high, we free MADV_FREE pages. If the pages are</span>
<span class="quote">&gt; not dirty in pte, the pages could be freed immediately. Otherwise we</span>
<span class="quote">&gt; can&#39;t reclaim them. We put the pages back to anonumous LRU list (by</span>
<span class="quote">&gt; setting SwapBacked flag) and the pages will be reclaimed in normal</span>
<span class="quote">&gt; swapout way.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We use normal page reclaim policy. Since MADV_FREE pages are put into</span>
<span class="quote">&gt; inactive file list, such pages and inactive file pages are reclaimed</span>
<span class="quote">&gt; according to their age. This is expected, because we don&#39;t want to</span>
<span class="quote">&gt; reclaim too many MADV_FREE pages before used once pages.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Based on Minchan&#39;s original patch</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="acked-by">
Acked-by: Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - Feb. 28, 2017, 5:02 a.m.</div>
<pre class="content">
On Mon, Feb 27, 2017 at 08:19:08AM -0800, Shaohua Li wrote:
<span class="quote">&gt; On Mon, Feb 27, 2017 at 03:33:15PM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; Hi Shaohua,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; On Fri, Feb 24, 2017 at 01:31:47PM -0800, Shaohua Li wrote:</span>
<span class="quote">&gt; &gt; &gt; When memory pressure is high, we free MADV_FREE pages. If the pages are</span>
<span class="quote">&gt; &gt; &gt; not dirty in pte, the pages could be freed immediately. Otherwise we</span>
<span class="quote">&gt; &gt; &gt; can&#39;t reclaim them. We put the pages back to anonumous LRU list (by</span>
<span class="quote">&gt; &gt; &gt; setting SwapBacked flag) and the pages will be reclaimed in normal</span>
<span class="quote">&gt; &gt; &gt; swapout way.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; We use normal page reclaim policy. Since MADV_FREE pages are put into</span>
<span class="quote">&gt; &gt; &gt; inactive file list, such pages and inactive file pages are reclaimed</span>
<span class="quote">&gt; &gt; &gt; according to their age. This is expected, because we don&#39;t want to</span>
<span class="quote">&gt; &gt; &gt; reclaim too many MADV_FREE pages before used once pages.</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Based on Minchan&#39;s original patch</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Cc: Michal Hocko &lt;mhocko@suse.com&gt;</span>
<span class="quote">&gt; &gt; &gt; Cc: Minchan Kim &lt;minchan@kernel.org&gt;</span>
<span class="quote">&gt; &gt; &gt; Cc: Hugh Dickins &lt;hughd@google.com&gt;</span>
<span class="quote">&gt; &gt; &gt; Cc: Johannes Weiner &lt;hannes@cmpxchg.org&gt;</span>
<span class="quote">&gt; &gt; &gt; Cc: Rik van Riel &lt;riel@redhat.com&gt;</span>
<span class="quote">&gt; &gt; &gt; Cc: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; &gt; &gt; Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;</span>
<span class="quote">&gt; &gt; &gt; Signed-off-by: Shaohua Li &lt;shli@fb.com&gt;</span>
<span class="quote">&gt; &gt; &gt; ---</span>
<span class="quote">&gt; &gt; &gt;  include/linux/rmap.h |  2 +-</span>
<span class="quote">&gt; &gt; &gt;  mm/huge_memory.c     |  2 ++</span>
<span class="quote">&gt; &gt; &gt;  mm/madvise.c         |  1 +</span>
<span class="quote">&gt; &gt; &gt;  mm/rmap.c            | 40 +++++++++++++++++-----------------------</span>
<span class="quote">&gt; &gt; &gt;  mm/vmscan.c          | 34 ++++++++++++++++++++++------------</span>
<span class="quote">&gt; &gt; &gt;  5 files changed, 43 insertions(+), 36 deletions(-)</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; &gt; index 7a39414..fee10d7 100644</span>
<span class="quote">&gt; &gt; &gt; --- a/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; &gt; +++ b/include/linux/rmap.h</span>
<span class="quote">&gt; &gt; &gt; @@ -298,6 +298,6 @@ static inline int page_mkclean(struct page *page)</span>
<span class="quote">&gt; &gt; &gt;  #define SWAP_AGAIN	1</span>
<span class="quote">&gt; &gt; &gt;  #define SWAP_FAIL	2</span>
<span class="quote">&gt; &gt; &gt;  #define SWAP_MLOCK	3</span>
<span class="quote">&gt; &gt; &gt; -#define SWAP_LZFREE	4</span>
<span class="quote">&gt; &gt; &gt; +#define SWAP_DIRTY	4</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I still don&#39;t convinced why we should introduce SWAP_DIRTY in try_to_unmap.</span>
<span class="quote">&gt; &gt; https://marc.info/?l=linux-mm&amp;m=148797879123238&amp;w=2</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We have been SetPageMlocked in there but why cannot we SetPageSwapBacked</span>
<span class="quote">&gt; &gt; in there? It&#39;s not a thing to change LRU type but it&#39;s just indication</span>
<span class="quote">&gt; &gt; we found the page&#39;s status changed in late.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This one I don&#39;t have strong preference. Personally I agree with Johannes,</span>
<span class="quote">&gt; handling failure in vmscan sounds better. But since the failure handling is</span>
<span class="quote">&gt; just one statement, this probably doesn&#39;t make too much difference. If Johannes</span>
<span class="quote">&gt; and you made an agreement, I&#39;ll follow.</span>

I don&#39;t want to add unnecessary new return value(i.e., SWAP_DIRTY).
If VM found lazyfree page dirty in try_to_unmap_one, it means &quot;non-swappable page&quot;
so it&#39;s natural to set SetPageSwapBacked in there and return just SWAP_FAIL to
activate it in vmscan.c. SWAP_FAIL means the page is non-swappable so it should be
activated. I don&#39;t see any problem in there like software engineering pov.

However, it seems everyone are happy with introdcuing SWAP_DIRTY so I don&#39;t
insist on it which is not critical for this patchset.
I looked over try_to_unmap and callers. Now, I think we could remove SWAP_MLOCK
and maybe SWAP_AGAIN as well as SWAP_DIRTY that is to make try_to_unmap *bool*.
So, it could be done by separate patchset. I will look into that in more.
<span class="acked-by">
Acked-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>

Thanks.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index 7a39414..fee10d7 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -298,6 +298,6 @@</span> <span class="p_context"> static inline int page_mkclean(struct page *page)</span>
 #define SWAP_AGAIN	1
 #define SWAP_FAIL	2
 #define SWAP_MLOCK	3
<span class="p_del">-#define SWAP_LZFREE	4</span>
<span class="p_add">+#define SWAP_DIRTY	4</span>
 
 #endif	/* _LINUX_RMAP_H */
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 3b7ee0c..4c7454b 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1571,6 +1571,8 @@</span> <span class="p_context"> bool madvise_free_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		set_pmd_at(mm, addr, pmd, orig_pmd);
 		tlb_remove_pmd_tlb_entry(tlb, pmd, addr);
 	}
<span class="p_add">+</span>
<span class="p_add">+	mark_page_lazyfree(page);</span>
 	ret = true;
 out:
 	spin_unlock(ptl);
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 61e10b1..225af7d 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -413,6 +413,7 @@</span> <span class="p_context"> static int madvise_free_pte_range(pmd_t *pmd, unsigned long addr,</span>
 			set_pte_at(mm, addr, pte, ptent);
 			tlb_remove_tlb_entry(tlb, pte, addr);
 		}
<span class="p_add">+		mark_page_lazyfree(page);</span>
 	}
 out:
 	if (nr_swap) {
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index c621088..bb45712 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1281,11 +1281,6 @@</span> <span class="p_context"> void page_remove_rmap(struct page *page, bool compound)</span>
 	 */
 }
 
<span class="p_del">-struct rmap_private {</span>
<span class="p_del">-	enum ttu_flags flags;</span>
<span class="p_del">-	int lazyfreed;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
 /*
  * @arg: enum ttu_flags will be passed to this argument
  */
<span class="p_chunk">@@ -1301,8 +1296,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	pte_t pteval;
 	struct page *subpage;
 	int ret = SWAP_AGAIN;
<span class="p_del">-	struct rmap_private *rp = arg;</span>
<span class="p_del">-	enum ttu_flags flags = rp-&gt;flags;</span>
<span class="p_add">+	enum ttu_flags flags = (enum ttu_flags)arg;</span>
 
 	/* munlock has nothing to gain from examining un-locked vmas */
 	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))
<span class="p_chunk">@@ -1419,11 +1413,21 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 			VM_BUG_ON_PAGE(!PageSwapCache(page) &amp;&amp; PageSwapBacked(page),
 				page);
 
<span class="p_del">-			if (!PageDirty(page)) {</span>
<span class="p_add">+			/*</span>
<span class="p_add">+			 * swapin page could be clean, it has data stored in</span>
<span class="p_add">+			 * swap. We can&#39;t silently discard it without setting</span>
<span class="p_add">+			 * swap entry in the page table.</span>
<span class="p_add">+			 */</span>
<span class="p_add">+			if (!PageDirty(page) &amp;&amp; !PageSwapCache(page)) {</span>
 				/* It&#39;s a freeable page by MADV_FREE */
 				dec_mm_counter(mm, MM_ANONPAGES);
<span class="p_del">-				rp-&gt;lazyfreed++;</span>
 				goto discard;
<span class="p_add">+			} else if (!PageSwapBacked(page)) {</span>
<span class="p_add">+				/* dirty MADV_FREE page */</span>
<span class="p_add">+				set_pte_at(mm, address, pvmw.pte, pteval);</span>
<span class="p_add">+				ret = SWAP_DIRTY;</span>
<span class="p_add">+				page_vma_mapped_walk_done(&amp;pvmw);</span>
<span class="p_add">+				break;</span>
 			}
 
 			if (swap_duplicate(entry) &lt; 0) {
<span class="p_chunk">@@ -1491,18 +1495,15 @@</span> <span class="p_context"> static int page_mapcount_is_zero(struct page *page)</span>
  * SWAP_AGAIN	- we missed a mapping, try again later
  * SWAP_FAIL	- the page is unswappable
  * SWAP_MLOCK	- page is mlocked.
<span class="p_add">+ * SWAP_DIRTY	- page is dirty MADV_FREE page</span>
  */
 int try_to_unmap(struct page *page, enum ttu_flags flags)
 {
 	int ret;
<span class="p_del">-	struct rmap_private rp = {</span>
<span class="p_del">-		.flags = flags,</span>
<span class="p_del">-		.lazyfreed = 0,</span>
<span class="p_del">-	};</span>
 
 	struct rmap_walk_control rwc = {
 		.rmap_one = try_to_unmap_one,
<span class="p_del">-		.arg = &amp;rp,</span>
<span class="p_add">+		.arg = (void *)flags,</span>
 		.done = page_mapcount_is_zero,
 		.anon_lock = page_lock_anon_vma_read,
 	};
<span class="p_chunk">@@ -1523,11 +1524,8 @@</span> <span class="p_context"> int try_to_unmap(struct page *page, enum ttu_flags flags)</span>
 	else
 		ret = rmap_walk(page, &amp;rwc);
 
<span class="p_del">-	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page)) {</span>
<span class="p_add">+	if (ret != SWAP_MLOCK &amp;&amp; !page_mapcount(page))</span>
 		ret = SWAP_SUCCESS;
<span class="p_del">-		if (rp.lazyfreed &amp;&amp; !PageDirty(page))</span>
<span class="p_del">-			ret = SWAP_LZFREE;</span>
<span class="p_del">-	}</span>
 	return ret;
 }
 
<span class="p_chunk">@@ -1554,14 +1552,10 @@</span> <span class="p_context"> static int page_not_mapped(struct page *page)</span>
 int try_to_munlock(struct page *page)
 {
 	int ret;
<span class="p_del">-	struct rmap_private rp = {</span>
<span class="p_del">-		.flags = TTU_MUNLOCK,</span>
<span class="p_del">-		.lazyfreed = 0,</span>
<span class="p_del">-	};</span>
 
 	struct rmap_walk_control rwc = {
 		.rmap_one = try_to_unmap_one,
<span class="p_del">-		.arg = &amp;rp,</span>
<span class="p_add">+		.arg = (void *)TTU_MUNLOCK,</span>
 		.done = page_not_mapped,
 		.anon_lock = page_lock_anon_vma_read,
 
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 68ea50d..16ad821 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -911,7 +911,8 @@</span> <span class="p_context"> static void page_check_dirty_writeback(struct page *page,</span>
 	 * Anonymous pages are not handled by flushers and must be written
 	 * from reclaim context. Do not stall reclaim based on them
 	 */
<span class="p_del">-	if (!page_is_file_cache(page)) {</span>
<span class="p_add">+	if (!page_is_file_cache(page) ||</span>
<span class="p_add">+	    (PageAnon(page) &amp;&amp; !PageSwapBacked(page))) {</span>
 		*dirty = false;
 		*writeback = false;
 		return;
<span class="p_chunk">@@ -992,7 +993,8 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 			goto keep_locked;
 
 		/* Double the slab pressure for mapped and swapcache pages */
<span class="p_del">-		if (page_mapped(page) || PageSwapCache(page))</span>
<span class="p_add">+		if ((page_mapped(page) || PageSwapCache(page)) &amp;&amp;</span>
<span class="p_add">+		    !(PageAnon(page) &amp;&amp; !PageSwapBacked(page)))</span>
 			sc-&gt;nr_scanned++;
 
 		may_enter_fs = (sc-&gt;gfp_mask &amp; __GFP_FS) ||
<span class="p_chunk">@@ -1118,8 +1120,10 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		/*
 		 * Anonymous process memory has backing store?
 		 * Try to allocate it some swap space here.
<span class="p_add">+		 * Lazyfree page could be freed directly</span>
 		 */
<span class="p_del">-		if (PageAnon(page) &amp;&amp; !PageSwapCache(page)) {</span>
<span class="p_add">+		if (PageAnon(page) &amp;&amp; PageSwapBacked(page) &amp;&amp;</span>
<span class="p_add">+		    !PageSwapCache(page)) {</span>
 			if (!(sc-&gt;gfp_mask &amp; __GFP_IO))
 				goto keep_locked;
 			if (!add_to_swap(page, page_list))
<span class="p_chunk">@@ -1140,9 +1144,12 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 * The page is mapped into the page tables of one or more
 		 * processes. Try to unmap it here.
 		 */
<span class="p_del">-		if (page_mapped(page) &amp;&amp; mapping) {</span>
<span class="p_add">+		if (page_mapped(page)) {</span>
 			switch (ret = try_to_unmap(page,
 				ttu_flags | TTU_BATCH_FLUSH)) {
<span class="p_add">+			case SWAP_DIRTY:</span>
<span class="p_add">+				SetPageSwapBacked(page);</span>
<span class="p_add">+				/* fall through */</span>
 			case SWAP_FAIL:
 				nr_unmap_fail++;
 				goto activate_locked;
<span class="p_chunk">@@ -1150,8 +1157,6 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 				goto keep_locked;
 			case SWAP_MLOCK:
 				goto cull_mlocked;
<span class="p_del">-			case SWAP_LZFREE:</span>
<span class="p_del">-				goto lazyfree;</span>
 			case SWAP_SUCCESS:
 				; /* try to free the page below */
 			}
<span class="p_chunk">@@ -1263,10 +1268,18 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 			}
 		}
 
<span class="p_del">-lazyfree:</span>
<span class="p_del">-		if (!mapping || !__remove_mapping(mapping, page, true))</span>
<span class="p_del">-			goto keep_locked;</span>
<span class="p_add">+		if (PageAnon(page) &amp;&amp; !PageSwapBacked(page)) {</span>
<span class="p_add">+			/* follow __remove_mapping for reference */</span>
<span class="p_add">+			if (!page_ref_freeze(page, 1))</span>
<span class="p_add">+				goto keep_locked;</span>
<span class="p_add">+			if (PageDirty(page)) {</span>
<span class="p_add">+				page_ref_unfreeze(page, 1);</span>
<span class="p_add">+				goto keep_locked;</span>
<span class="p_add">+			}</span>
 
<span class="p_add">+			count_vm_event(PGLAZYFREED);</span>
<span class="p_add">+		} else if (!mapping || !__remove_mapping(mapping, page, true))</span>
<span class="p_add">+			goto keep_locked;</span>
 		/*
 		 * At this point, we have no other references and there is
 		 * no way to pick any more up (removed from LRU, removed
<span class="p_chunk">@@ -1276,9 +1289,6 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 */
 		__ClearPageLocked(page);
 free_it:
<span class="p_del">-		if (ret == SWAP_LZFREE)</span>
<span class="p_del">-			count_vm_event(PGLAZYFREED);</span>
<span class="p_del">-</span>
 		nr_reclaimed++;
 
 		/*

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



