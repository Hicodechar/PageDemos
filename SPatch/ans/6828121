
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv9,12/36] thp: drop all split_huge_page()-related code - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv9,12/36] thp: drop all split_huge_page()-related code</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 20, 2015, 2:20 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1437402069-105900-13-git-send-email-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6828121/mbox/"
   >mbox</a>
|
   <a href="/patch/6828121/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6828121/">/patch/6828121/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id 0F7B29F38B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 20 Jul 2015 14:31:25 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 8A9F22064B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 20 Jul 2015 14:31:23 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 213BC20647
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 20 Jul 2015 14:31:21 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1756595AbbGTObI (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 20 Jul 2015 10:31:08 -0400
Received: from mga02.intel.com ([134.134.136.20]:52453 &quot;EHLO mga02.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S932374AbbGTOVZ (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 20 Jul 2015 10:21:25 -0400
Received: from orsmga002.jf.intel.com ([10.7.209.21])
	by orsmga101.jf.intel.com with ESMTP; 20 Jul 2015 07:21:23 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.15,508,1432623600&quot;; d=&quot;scan&#39;208&quot;;a=&quot;767681962&quot;
Received: from black.fi.intel.com ([10.237.72.157])
	by orsmga002.jf.intel.com with ESMTP; 20 Jul 2015 07:21:17 -0700
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 12F706ED; Mon, 20 Jul 2015 17:21:11 +0300 (EEST)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Andrea Arcangeli &lt;aarcange@redhat.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;
Cc: Dave Hansen &lt;dave.hansen@intel.com&gt;, Mel Gorman &lt;mgorman@suse.de&gt;,
	Rik van Riel &lt;riel@redhat.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Christoph Lameter &lt;cl@gentwo.org&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	Steve Capper &lt;steve.capper@linaro.org&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	Jerome Marchand &lt;jmarchan@redhat.com&gt;,
	Sasha Levin &lt;sasha.levin@oracle.com&gt;,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCHv9 12/36] thp: drop all split_huge_page()-related code
Date: Mon, 20 Jul 2015 17:20:45 +0300
Message-Id: &lt;1437402069-105900-13-git-send-email-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.1.4
In-Reply-To: &lt;1437402069-105900-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
References: &lt;1437402069-105900-1-git-send-email-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.1 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - July 20, 2015, 2:20 p.m.</div>
<pre class="content">
We will re-introduce new version with new refcounting later in patchset.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="tested-by">Tested-by: Sasha Levin &lt;sasha.levin@oracle.com&gt;</span>
<span class="tested-by">Tested-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
---
 include/linux/huge_mm.h |  28 +---
 mm/huge_memory.c        | 400 +-----------------------------------------------
 2 files changed, 7 insertions(+), 421 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=1608">Jerome Marchand</a> - July 31, 2015, 2:50 p.m.</div>
<pre class="content">
On 07/20/2015 04:20 PM, Kirill A. Shutemov wrote:
<span class="quote">&gt; We will re-introduce new version with new refcounting later in patchset.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; Tested-by: Sasha Levin &lt;sasha.levin@oracle.com&gt;</span>
<span class="quote">&gt; Tested-by: Aneesh Kumar K.V &lt;aneesh.kumar@linux.vnet.ibm.com&gt;</span>
<span class="acked-by">
Acked-by: Jerome Marchand &lt;jmarchan@redhat.com&gt;</span>
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  include/linux/huge_mm.h |  28 +---</span>
<span class="quote">&gt;  mm/huge_memory.c        | 400 +-----------------------------------------------</span>
<span class="quote">&gt;  2 files changed, 7 insertions(+), 421 deletions(-)</span>
<span class="quote">&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h</span>
<span class="p_header">index f825694a23ee..933c63cc5ed7 100644</span>
<span class="p_header">--- a/include/linux/huge_mm.h</span>
<span class="p_header">+++ b/include/linux/huge_mm.h</span>
<span class="p_chunk">@@ -99,28 +99,12 @@</span> <span class="p_context"> extern bool is_vma_temporary_stack(struct vm_area_struct *vma);</span>
 #endif /* CONFIG_DEBUG_VM */
 
 extern unsigned long transparent_hugepage_flags;
<span class="p_del">-extern int split_huge_page_to_list(struct page *page, struct list_head *list);</span>
<span class="p_del">-static inline int split_huge_page(struct page *page)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return split_huge_page_to_list(page, NULL);</span>
<span class="p_del">-}</span>
<span class="p_del">-extern void __split_huge_page_pmd(struct vm_area_struct *vma,</span>
<span class="p_del">-		unsigned long address, pmd_t *pmd);</span>
<span class="p_del">-#define split_huge_pmd(__vma, __pmd, __address)				\</span>
<span class="p_del">-	do {								\</span>
<span class="p_del">-		pmd_t *____pmd = (__pmd);				\</span>
<span class="p_del">-		if (unlikely(pmd_trans_huge(*____pmd)))			\</span>
<span class="p_del">-			__split_huge_page_pmd(__vma, __address,		\</span>
<span class="p_del">-					____pmd);			\</span>
<span class="p_del">-	}  while (0)</span>
<span class="p_del">-#define wait_split_huge_page(__anon_vma, __pmd)				\</span>
<span class="p_del">-	do {								\</span>
<span class="p_del">-		pmd_t *____pmd = (__pmd);				\</span>
<span class="p_del">-		anon_vma_lock_write(__anon_vma);			\</span>
<span class="p_del">-		anon_vma_unlock_write(__anon_vma);			\</span>
<span class="p_del">-		BUG_ON(pmd_trans_splitting(*____pmd) ||			\</span>
<span class="p_del">-		       pmd_trans_huge(*____pmd));			\</span>
<span class="p_del">-	} while (0)</span>
<span class="p_add">+</span>
<span class="p_add">+#define split_huge_page_to_list(page, list) BUILD_BUG()</span>
<span class="p_add">+#define split_huge_page(page) BUILD_BUG()</span>
<span class="p_add">+#define split_huge_pmd(__vma, __pmd, __address) BUILD_BUG()</span>
<span class="p_add">+</span>
<span class="p_add">+#define wait_split_huge_page(__anon_vma, __pmd) BUILD_BUG()</span>
 #if HPAGE_PMD_ORDER &gt;= MAX_ORDER
 #error &quot;hugepages can&#39;t be allocated by the buddy allocator&quot;
 #endif
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index 330d978c4ffc..25ad3af53f15 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1725,329 +1725,6 @@</span> <span class="p_context"> int pmd_freeable(pmd_t pmd)</span>
 	return !pmd_dirty(pmd);
 }
 
<span class="p_del">-static int __split_huge_page_splitting(struct page *page,</span>
<span class="p_del">-				       struct vm_area_struct *vma,</span>
<span class="p_del">-				       unsigned long address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_del">-	spinlock_t *ptl;</span>
<span class="p_del">-	pmd_t *pmd;</span>
<span class="p_del">-	int ret = 0;</span>
<span class="p_del">-	/* For mmu_notifiers */</span>
<span class="p_del">-	const unsigned long mmun_start = address;</span>
<span class="p_del">-	const unsigned long mmun_end   = address + HPAGE_PMD_SIZE;</span>
<span class="p_del">-</span>
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_del">-	pmd = page_check_address_pmd(page, mm, address,</span>
<span class="p_del">-			PAGE_CHECK_ADDRESS_PMD_NOTSPLITTING_FLAG, &amp;ptl);</span>
<span class="p_del">-	if (pmd) {</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * We can&#39;t temporarily set the pmd to null in order</span>
<span class="p_del">-		 * to split it, the pmd must remain marked huge at all</span>
<span class="p_del">-		 * times or the VM won&#39;t take the pmd_trans_huge paths</span>
<span class="p_del">-		 * and it won&#39;t wait on the anon_vma-&gt;root-&gt;rwsem to</span>
<span class="p_del">-		 * serialize against split_huge_page*.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		pmdp_splitting_flush(vma, address, pmd);</span>
<span class="p_del">-</span>
<span class="p_del">-		ret = 1;</span>
<span class="p_del">-		spin_unlock(ptl);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_del">-</span>
<span class="p_del">-	return ret;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __split_huge_page_refcount(struct page *page,</span>
<span class="p_del">-				       struct list_head *list)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int i;</span>
<span class="p_del">-	struct zone *zone = page_zone(page);</span>
<span class="p_del">-	struct lruvec *lruvec;</span>
<span class="p_del">-	int tail_count = 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* prevent PageLRU to go away from under us, and freeze lru stats */</span>
<span class="p_del">-	spin_lock_irq(&amp;zone-&gt;lru_lock);</span>
<span class="p_del">-	lruvec = mem_cgroup_page_lruvec(page, zone);</span>
<span class="p_del">-</span>
<span class="p_del">-	compound_lock(page);</span>
<span class="p_del">-	/* complete memcg works before add pages to LRU */</span>
<span class="p_del">-	mem_cgroup_split_huge_fixup(page);</span>
<span class="p_del">-</span>
<span class="p_del">-	for (i = HPAGE_PMD_NR - 1; i &gt;= 1; i--) {</span>
<span class="p_del">-		struct page *page_tail = page + i;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* tail_page-&gt;_mapcount cannot change */</span>
<span class="p_del">-		BUG_ON(page_mapcount(page_tail) &lt; 0);</span>
<span class="p_del">-		tail_count += page_mapcount(page_tail);</span>
<span class="p_del">-		/* check for overflow */</span>
<span class="p_del">-		BUG_ON(tail_count &lt; 0);</span>
<span class="p_del">-		BUG_ON(atomic_read(&amp;page_tail-&gt;_count) != 0);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * tail_page-&gt;_count is zero and not changing from</span>
<span class="p_del">-		 * under us. But get_page_unless_zero() may be running</span>
<span class="p_del">-		 * from under us on the tail_page. If we used</span>
<span class="p_del">-		 * atomic_set() below instead of atomic_add(), we</span>
<span class="p_del">-		 * would then run atomic_set() concurrently with</span>
<span class="p_del">-		 * get_page_unless_zero(), and atomic_set() is</span>
<span class="p_del">-		 * implemented in C not using locked ops. spin_unlock</span>
<span class="p_del">-		 * on x86 sometime uses locked ops because of PPro</span>
<span class="p_del">-		 * errata 66, 92, so unless somebody can guarantee</span>
<span class="p_del">-		 * atomic_set() here would be safe on all archs (and</span>
<span class="p_del">-		 * not only on x86), it&#39;s safer to use atomic_add().</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		atomic_add(page_mapcount(page) + page_mapcount(page_tail) + 1,</span>
<span class="p_del">-			   &amp;page_tail-&gt;_count);</span>
<span class="p_del">-</span>
<span class="p_del">-		/* after clearing PageTail the gup refcount can be released */</span>
<span class="p_del">-		smp_mb__after_atomic();</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * retain hwpoison flag of the poisoned tail page:</span>
<span class="p_del">-		 *   fix for the unsuitable process killed on Guest Machine(KVM)</span>
<span class="p_del">-		 *   by the memory-failure.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		page_tail-&gt;flags &amp;= ~PAGE_FLAGS_CHECK_AT_PREP | __PG_HWPOISON;</span>
<span class="p_del">-		page_tail-&gt;flags |= (page-&gt;flags &amp;</span>
<span class="p_del">-				     ((1L &lt;&lt; PG_referenced) |</span>
<span class="p_del">-				      (1L &lt;&lt; PG_swapbacked) |</span>
<span class="p_del">-				      (1L &lt;&lt; PG_mlocked) |</span>
<span class="p_del">-				      (1L &lt;&lt; PG_uptodate) |</span>
<span class="p_del">-				      (1L &lt;&lt; PG_active) |</span>
<span class="p_del">-				      (1L &lt;&lt; PG_unevictable)));</span>
<span class="p_del">-		page_tail-&gt;flags |= (1L &lt;&lt; PG_dirty);</span>
<span class="p_del">-</span>
<span class="p_del">-		/* clear PageTail before overwriting first_page */</span>
<span class="p_del">-		smp_wmb();</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * __split_huge_page_splitting() already set the</span>
<span class="p_del">-		 * splitting bit in all pmd that could map this</span>
<span class="p_del">-		 * hugepage, that will ensure no CPU can alter the</span>
<span class="p_del">-		 * mapcount on the head page. The mapcount is only</span>
<span class="p_del">-		 * accounted in the head page and it has to be</span>
<span class="p_del">-		 * transferred to all tail pages in the below code. So</span>
<span class="p_del">-		 * for this code to be safe, the split the mapcount</span>
<span class="p_del">-		 * can&#39;t change. But that doesn&#39;t mean userland can&#39;t</span>
<span class="p_del">-		 * keep changing and reading the page contents while</span>
<span class="p_del">-		 * we transfer the mapcount, so the pmd splitting</span>
<span class="p_del">-		 * status is achieved setting a reserved bit in the</span>
<span class="p_del">-		 * pmd, not by clearing the present bit.</span>
<span class="p_del">-		*/</span>
<span class="p_del">-		page_tail-&gt;_mapcount = page-&gt;_mapcount;</span>
<span class="p_del">-</span>
<span class="p_del">-		BUG_ON(page_tail-&gt;mapping != TAIL_MAPPING);</span>
<span class="p_del">-		page_tail-&gt;mapping = page-&gt;mapping;</span>
<span class="p_del">-</span>
<span class="p_del">-		page_tail-&gt;index = page-&gt;index + i;</span>
<span class="p_del">-		page_cpupid_xchg_last(page_tail, page_cpupid_last(page));</span>
<span class="p_del">-</span>
<span class="p_del">-		BUG_ON(!PageAnon(page_tail));</span>
<span class="p_del">-		BUG_ON(!PageUptodate(page_tail));</span>
<span class="p_del">-		BUG_ON(!PageDirty(page_tail));</span>
<span class="p_del">-		BUG_ON(!PageSwapBacked(page_tail));</span>
<span class="p_del">-</span>
<span class="p_del">-		lru_add_page_tail(page, page_tail, lruvec, list);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	atomic_sub(tail_count, &amp;page-&gt;_count);</span>
<span class="p_del">-	BUG_ON(atomic_read(&amp;page-&gt;_count) &lt;= 0);</span>
<span class="p_del">-</span>
<span class="p_del">-	__mod_zone_page_state(zone, NR_ANON_TRANSPARENT_HUGEPAGES, -1);</span>
<span class="p_del">-</span>
<span class="p_del">-	ClearPageCompound(page);</span>
<span class="p_del">-	compound_unlock(page);</span>
<span class="p_del">-	spin_unlock_irq(&amp;zone-&gt;lru_lock);</span>
<span class="p_del">-</span>
<span class="p_del">-	for (i = 1; i &lt; HPAGE_PMD_NR; i++) {</span>
<span class="p_del">-		struct page *page_tail = page + i;</span>
<span class="p_del">-		BUG_ON(page_count(page_tail) &lt;= 0);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Tail pages may be freed if there wasn&#39;t any mapping</span>
<span class="p_del">-		 * like if add_to_swap() is running on a lru page that</span>
<span class="p_del">-		 * had its mapping zapped. And freeing these pages</span>
<span class="p_del">-		 * requires taking the lru_lock so we do the put_page</span>
<span class="p_del">-		 * of the tail pages after the split is complete.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		put_page(page_tail);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Only the head page (now become a regular page) is required</span>
<span class="p_del">-	 * to be pinned by the caller.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	BUG_ON(page_count(page) &lt;= 0);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __split_huge_page_map(struct page *page,</span>
<span class="p_del">-				 struct vm_area_struct *vma,</span>
<span class="p_del">-				 unsigned long address)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_del">-	spinlock_t *ptl;</span>
<span class="p_del">-	pmd_t *pmd, _pmd;</span>
<span class="p_del">-	int ret = 0, i;</span>
<span class="p_del">-	pgtable_t pgtable;</span>
<span class="p_del">-	unsigned long haddr;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmd = page_check_address_pmd(page, mm, address,</span>
<span class="p_del">-			PAGE_CHECK_ADDRESS_PMD_SPLITTING_FLAG, &amp;ptl);</span>
<span class="p_del">-	if (pmd) {</span>
<span class="p_del">-		pgtable = pgtable_trans_huge_withdraw(mm, pmd);</span>
<span class="p_del">-		pmd_populate(mm, &amp;_pmd, pgtable);</span>
<span class="p_del">-		if (pmd_write(*pmd))</span>
<span class="p_del">-			BUG_ON(page_mapcount(page) != 1);</span>
<span class="p_del">-</span>
<span class="p_del">-		haddr = address;</span>
<span class="p_del">-		for (i = 0; i &lt; HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {</span>
<span class="p_del">-			pte_t *pte, entry;</span>
<span class="p_del">-			BUG_ON(PageCompound(page+i));</span>
<span class="p_del">-			/*</span>
<span class="p_del">-			 * Note that NUMA hinting access restrictions are not</span>
<span class="p_del">-			 * transferred to avoid any possibility of altering</span>
<span class="p_del">-			 * permissions across VMAs.</span>
<span class="p_del">-			 */</span>
<span class="p_del">-			entry = mk_pte(page + i, vma-&gt;vm_page_prot);</span>
<span class="p_del">-			entry = maybe_mkwrite(pte_mkdirty(entry), vma);</span>
<span class="p_del">-			if (!pmd_write(*pmd))</span>
<span class="p_del">-				entry = pte_wrprotect(entry);</span>
<span class="p_del">-			if (!pmd_young(*pmd))</span>
<span class="p_del">-				entry = pte_mkold(entry);</span>
<span class="p_del">-			pte = pte_offset_map(&amp;_pmd, haddr);</span>
<span class="p_del">-			BUG_ON(!pte_none(*pte));</span>
<span class="p_del">-			set_pte_at(mm, haddr, pte, entry);</span>
<span class="p_del">-			pte_unmap(pte);</span>
<span class="p_del">-		}</span>
<span class="p_del">-</span>
<span class="p_del">-		smp_wmb(); /* make pte visible before pmd */</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Up to this point the pmd is present and huge and</span>
<span class="p_del">-		 * userland has the whole access to the hugepage</span>
<span class="p_del">-		 * during the split (which happens in place). If we</span>
<span class="p_del">-		 * overwrite the pmd with the not-huge version</span>
<span class="p_del">-		 * pointing to the pte here (which of course we could</span>
<span class="p_del">-		 * if all CPUs were bug free), userland could trigger</span>
<span class="p_del">-		 * a small page size TLB miss on the small sized TLB</span>
<span class="p_del">-		 * while the hugepage TLB entry is still established</span>
<span class="p_del">-		 * in the huge TLB. Some CPU doesn&#39;t like that. See</span>
<span class="p_del">-		 * http://support.amd.com/us/Processor_TechDocs/41322.pdf,</span>
<span class="p_del">-		 * Erratum 383 on page 93. Intel should be safe but is</span>
<span class="p_del">-		 * also warns that it&#39;s only safe if the permission</span>
<span class="p_del">-		 * and cache attributes of the two entries loaded in</span>
<span class="p_del">-		 * the two TLB is identical (which should be the case</span>
<span class="p_del">-		 * here). But it is generally safer to never allow</span>
<span class="p_del">-		 * small and huge TLB entries for the same virtual</span>
<span class="p_del">-		 * address to be loaded simultaneously. So instead of</span>
<span class="p_del">-		 * doing &quot;pmd_populate(); flush_tlb_range();&quot; we first</span>
<span class="p_del">-		 * mark the current pmd notpresent (atomically because</span>
<span class="p_del">-		 * here the pmd_trans_huge and pmd_trans_splitting</span>
<span class="p_del">-		 * must remain set at all times on the pmd until the</span>
<span class="p_del">-		 * split is complete for this pmd), then we flush the</span>
<span class="p_del">-		 * SMP TLB and finally we write the non-huge version</span>
<span class="p_del">-		 * of the pmd entry with pmd_populate.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		pmdp_invalidate(vma, address, pmd);</span>
<span class="p_del">-		pmd_populate(mm, pmd, pgtable);</span>
<span class="p_del">-		ret = 1;</span>
<span class="p_del">-		spin_unlock(ptl);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return ret;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/* must be called with anon_vma-&gt;root-&gt;rwsem held */</span>
<span class="p_del">-static void __split_huge_page(struct page *page,</span>
<span class="p_del">-			      struct anon_vma *anon_vma,</span>
<span class="p_del">-			      struct list_head *list)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int mapcount, mapcount2;</span>
<span class="p_del">-	pgoff_t pgoff = page-&gt;index &lt;&lt; (PAGE_CACHE_SHIFT - PAGE_SHIFT);</span>
<span class="p_del">-	struct anon_vma_chain *avc;</span>
<span class="p_del">-</span>
<span class="p_del">-	BUG_ON(!PageHead(page));</span>
<span class="p_del">-	BUG_ON(PageTail(page));</span>
<span class="p_del">-</span>
<span class="p_del">-	mapcount = 0;</span>
<span class="p_del">-	anon_vma_interval_tree_foreach(avc, &amp;anon_vma-&gt;rb_root, pgoff, pgoff) {</span>
<span class="p_del">-		struct vm_area_struct *vma = avc-&gt;vma;</span>
<span class="p_del">-		unsigned long addr = vma_address(page, vma);</span>
<span class="p_del">-		BUG_ON(is_vma_temporary_stack(vma));</span>
<span class="p_del">-		mapcount += __split_huge_page_splitting(page, vma, addr);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * It is critical that new vmas are added to the tail of the</span>
<span class="p_del">-	 * anon_vma list. This guarantes that if copy_huge_pmd() runs</span>
<span class="p_del">-	 * and establishes a child pmd before</span>
<span class="p_del">-	 * __split_huge_page_splitting() freezes the parent pmd (so if</span>
<span class="p_del">-	 * we fail to prevent copy_huge_pmd() from running until the</span>
<span class="p_del">-	 * whole __split_huge_page() is complete), we will still see</span>
<span class="p_del">-	 * the newly established pmd of the child later during the</span>
<span class="p_del">-	 * walk, to be able to set it as pmd_trans_splitting too.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (mapcount != page_mapcount(page)) {</span>
<span class="p_del">-		pr_err(&quot;mapcount %d page_mapcount %d\n&quot;,</span>
<span class="p_del">-			mapcount, page_mapcount(page));</span>
<span class="p_del">-		BUG();</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	__split_huge_page_refcount(page, list);</span>
<span class="p_del">-</span>
<span class="p_del">-	mapcount2 = 0;</span>
<span class="p_del">-	anon_vma_interval_tree_foreach(avc, &amp;anon_vma-&gt;rb_root, pgoff, pgoff) {</span>
<span class="p_del">-		struct vm_area_struct *vma = avc-&gt;vma;</span>
<span class="p_del">-		unsigned long addr = vma_address(page, vma);</span>
<span class="p_del">-		BUG_ON(is_vma_temporary_stack(vma));</span>
<span class="p_del">-		mapcount2 += __split_huge_page_map(page, vma, addr);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	if (mapcount != mapcount2) {</span>
<span class="p_del">-		pr_err(&quot;mapcount %d mapcount2 %d page_mapcount %d\n&quot;,</span>
<span class="p_del">-			mapcount, mapcount2, page_mapcount(page));</span>
<span class="p_del">-		BUG();</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Split a hugepage into normal pages. This doesn&#39;t change the position of head</span>
<span class="p_del">- * page. If @list is null, tail pages will be added to LRU list, otherwise, to</span>
<span class="p_del">- * @list. Both head page and tail pages will inherit mapping, flags, and so on</span>
<span class="p_del">- * from the hugepage.</span>
<span class="p_del">- * Return 0 if the hugepage is split successfully otherwise return 1.</span>
<span class="p_del">- */</span>
<span class="p_del">-int split_huge_page_to_list(struct page *page, struct list_head *list)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct anon_vma *anon_vma;</span>
<span class="p_del">-	int ret = 1;</span>
<span class="p_del">-</span>
<span class="p_del">-	BUG_ON(is_huge_zero_page(page));</span>
<span class="p_del">-	BUG_ON(!PageAnon(page));</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The caller does not necessarily hold an mmap_sem that would prevent</span>
<span class="p_del">-	 * the anon_vma disappearing so we first we take a reference to it</span>
<span class="p_del">-	 * and then lock the anon_vma for write. This is similar to</span>
<span class="p_del">-	 * page_lock_anon_vma_read except the write lock is taken to serialise</span>
<span class="p_del">-	 * against parallel split or collapse operations.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	anon_vma = page_get_anon_vma(page);</span>
<span class="p_del">-	if (!anon_vma)</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	anon_vma_lock_write(anon_vma);</span>
<span class="p_del">-</span>
<span class="p_del">-	ret = 0;</span>
<span class="p_del">-	if (!PageCompound(page))</span>
<span class="p_del">-		goto out_unlock;</span>
<span class="p_del">-</span>
<span class="p_del">-	BUG_ON(!PageSwapBacked(page));</span>
<span class="p_del">-	__split_huge_page(page, anon_vma, list);</span>
<span class="p_del">-	count_vm_event(THP_SPLIT_PAGE);</span>
<span class="p_del">-</span>
<span class="p_del">-	BUG_ON(PageCompound(page));</span>
<span class="p_del">-out_unlock:</span>
<span class="p_del">-	anon_vma_unlock_write(anon_vma);</span>
<span class="p_del">-	put_anon_vma(anon_vma);</span>
<span class="p_del">-out:</span>
<span class="p_del">-	return ret;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 #define VM_NO_THP (VM_SPECIAL | VM_HUGETLB | VM_SHARED | VM_MAYSHARE)
 
 int hugepage_madvise(struct vm_area_struct *vma,
<span class="p_chunk">@@ -2987,81 +2664,6 @@</span> <span class="p_context"> static int khugepaged(void *none)</span>
 	return 0;
 }
 
<span class="p_del">-static void __split_huge_zero_page_pmd(struct vm_area_struct *vma,</span>
<span class="p_del">-		unsigned long haddr, pmd_t *pmd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_del">-	pgtable_t pgtable;</span>
<span class="p_del">-	pmd_t _pmd;</span>
<span class="p_del">-	int i;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmdp_huge_clear_flush_notify(vma, haddr, pmd);</span>
<span class="p_del">-	/* leave pmd empty until pte is filled */</span>
<span class="p_del">-</span>
<span class="p_del">-	pgtable = pgtable_trans_huge_withdraw(mm, pmd);</span>
<span class="p_del">-	pmd_populate(mm, &amp;_pmd, pgtable);</span>
<span class="p_del">-</span>
<span class="p_del">-	for (i = 0; i &lt; HPAGE_PMD_NR; i++, haddr += PAGE_SIZE) {</span>
<span class="p_del">-		pte_t *pte, entry;</span>
<span class="p_del">-		entry = pfn_pte(my_zero_pfn(haddr), vma-&gt;vm_page_prot);</span>
<span class="p_del">-		entry = pte_mkspecial(entry);</span>
<span class="p_del">-		pte = pte_offset_map(&amp;_pmd, haddr);</span>
<span class="p_del">-		VM_BUG_ON(!pte_none(*pte));</span>
<span class="p_del">-		set_pte_at(mm, haddr, pte, entry);</span>
<span class="p_del">-		pte_unmap(pte);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	smp_wmb(); /* make pte visible before pmd */</span>
<span class="p_del">-	pmd_populate(mm, pmd, pgtable);</span>
<span class="p_del">-	put_huge_zero_page();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __split_huge_page_pmd(struct vm_area_struct *vma, unsigned long address,</span>
<span class="p_del">-		pmd_t *pmd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	spinlock_t *ptl;</span>
<span class="p_del">-	struct page *page = NULL;</span>
<span class="p_del">-	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_del">-	unsigned long haddr = address &amp; HPAGE_PMD_MASK;</span>
<span class="p_del">-	unsigned long mmun_start;	/* For mmu_notifiers */</span>
<span class="p_del">-	unsigned long mmun_end;		/* For mmu_notifiers */</span>
<span class="p_del">-</span>
<span class="p_del">-	BUG_ON(vma-&gt;vm_start &gt; haddr || vma-&gt;vm_end &lt; haddr + HPAGE_PMD_SIZE);</span>
<span class="p_del">-</span>
<span class="p_del">-	mmun_start = haddr;</span>
<span class="p_del">-	mmun_end   = haddr + HPAGE_PMD_SIZE;</span>
<span class="p_del">-again:</span>
<span class="p_del">-	mmu_notifier_invalidate_range_start(mm, mmun_start, mmun_end);</span>
<span class="p_del">-	ptl = pmd_lock(mm, pmd);</span>
<span class="p_del">-	if (unlikely(!pmd_trans_huge(*pmd)))</span>
<span class="p_del">-		goto unlock;</span>
<span class="p_del">-	if (vma_is_dax(vma)) {</span>
<span class="p_del">-		pmdp_huge_clear_flush(vma, haddr, pmd);</span>
<span class="p_del">-	} else if (is_huge_zero_pmd(*pmd)) {</span>
<span class="p_del">-		__split_huge_zero_page_pmd(vma, haddr, pmd);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		page = pmd_page(*pmd);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!page_count(page), page);</span>
<span class="p_del">-		get_page(page);</span>
<span class="p_del">-	}</span>
<span class="p_del">- unlock:</span>
<span class="p_del">-	spin_unlock(ptl);</span>
<span class="p_del">-	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!page)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	split_huge_page(page);</span>
<span class="p_del">-	put_page(page);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We don&#39;t always have down_write of mmap_sem here: a racing</span>
<span class="p_del">-	 * do_huge_pmd_wp_page() might have copied-on-write to another</span>
<span class="p_del">-	 * huge page before our split_huge_page() got the anon_vma lock.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (unlikely(pmd_trans_huge(*pmd)))</span>
<span class="p_del">-		goto again;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
 static void split_huge_pmd_address(struct vm_area_struct *vma,
 				    unsigned long address)
 {
<span class="p_chunk">@@ -3086,7 +2688,7 @@</span> <span class="p_context"> static void split_huge_pmd_address(struct vm_area_struct *vma,</span>
 	 * Caller holds the mmap_sem write mode, so a huge pmd cannot
 	 * materialize from under us.
 	 */
<span class="p_del">-	__split_huge_page_pmd(vma, address, pmd);</span>
<span class="p_add">+	split_huge_pmd(vma, pmd, address);</span>
 }
 
 void vma_adjust_trans_huge(struct vm_area_struct *vma,

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



