
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[V4,4/6] mm: mlock: Introduce VM_LOCKONFAULT and add mlock flags to enable it - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [V4,4/6] mm: mlock: Introduce VM_LOCKONFAULT and add mlock flags to enable it</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=111821">Eric B Munson</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 21, 2015, 7:59 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1437508781-28655-5-git-send-email-emunson@akamai.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6838011/mbox/"
   >mbox</a>
|
   <a href="/patch/6838011/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6838011/">/patch/6838011/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 698EBC05AC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jul 2015 20:01:56 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 138BA206ED
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jul 2015 20:01:54 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 8D3F9206EC
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue, 21 Jul 2015 20:01:51 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932741AbbGUUAj (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 21 Jul 2015 16:00:39 -0400
Received: from a23-79-238-175.deploy.static.akamaitechnologies.com
	([23.79.238.175]:44669
	&quot;EHLO prod-mail-xrelay07.akamai.com&quot; rhost-flags-OK-FAIL-OK-OK)
	by vger.kernel.org with ESMTP id S1754953AbbGUT7p (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 21 Jul 2015 15:59:45 -0400
Received: from prod-mail-xrelay07.akamai.com (localhost.localdomain
	[127.0.0.1]) by postfix.imss70 (Postfix) with ESMTP id 44D34478B9;
	Tue, 21 Jul 2015 20:00:22 +0000 (GMT)
Received: from prod-mail-relay06.akamai.com (prod-mail-relay06.akamai.com
	[172.17.120.126])
	by prod-mail-xrelay07.akamai.com (Postfix) with ESMTP id 21E9A478BA; 
	Tue, 21 Jul 2015 20:00:22 +0000 (GMT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=akamai.com; s=a1;
	t=1437508822; bh=3fVpjMAz53jcBJ9ZtDvKDV7bYS5fo+3MqvxAL+bNH+0=;
	h=From:To:Cc:Subject:Date:In-Reply-To:References:From;
	b=JWvTABNyL9l56d+94gb6faLcLmyfWQvYV8RUdSYNvqSf1pu1xSJcj9L4vPbDat9z6
	I5rn5W5/1ll1WseDh0WtPMyy4CznDBbkfQkgKdRxQ/JtN5q0FqdzcwDsvnESBZJ+02
	719NFKoeIaxST9wzQNzX8XOcKglZOWni5MNABo+o=
Received: from bos-lp6ds.kendall.corp.akamai.com
	(bos-lp6ds.kendall.corp.akamai.com [172.28.12.165])
	by prod-mail-relay06.akamai.com (Postfix) with ESMTP id 5A99C210B;
	Tue, 21 Jul 2015 19:59:42 +0000 (GMT)
From: Eric B Munson &lt;emunson@akamai.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Eric B Munson &lt;emunson@akamai.com&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Jonathan Corbet &lt;corbet@lwn.net&gt;, linux-alpha@vger.kernel.org,
	linux-kernel@vger.kernel.org, linux-mips@linux-mips.org,
	linux-parisc@vger.kernel.org, linuxppc-dev@lists.ozlabs.org,
	sparclinux@vger.kernel.org, linux-xtensa@linux-xtensa.org,
	dri-devel@lists.freedesktop.org, linux-mm@kvack.org,
	linux-arch@vger.kernel.org, linux-api@vger.kernel.org
Subject: [PATCH V4 4/6] mm: mlock: Introduce VM_LOCKONFAULT and add mlock
	flags to enable it
Date: Tue, 21 Jul 2015 15:59:39 -0400
Message-Id: &lt;1437508781-28655-5-git-send-email-emunson@akamai.com&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1437508781-28655-1-git-send-email-emunson@akamai.com&gt;
References: &lt;1437508781-28655-1-git-send-email-emunson@akamai.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.0 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,UNPARSEABLE_RELAY
	autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=111821">Eric B Munson</a> - July 21, 2015, 7:59 p.m.</div>
<pre class="content">
The cost of faulting in all memory to be locked can be very high when
working with large mappings.  If only portions of the mapping will be
used this can incur a high penalty for locking.

For the example of a large file, this is the usage pattern for a large
statical language model (probably applies to other statical or graphical
models as well).  For the security example, any application transacting
in data that cannot be swapped out (credit card data, medical records,
etc).

This patch introduces the ability to request that pages are not
pre-faulted, but are placed on the unevictable LRU when they are finally
faulted in.  This can be done area at a time via the
mlock2(MLOCK_ONFAULT) or the mlockall(MCL_ONFAULT) system calls.  These
calls can be undone via munlock2(MLOCK_ONFAULT) or
munlockall2(MCL_ONFAULT).

Applying the VM_LOCKONFAULT flag to a mapping with pages that are
already present required the addition of a function in gup.c to pin all
pages which are present in an address range.  It borrows heavily from
__mm_populate().

To keep accounting checks out of the page fault path, users are billed
for the entire mapping lock as if MLOCK_LOCKED was used.
<span class="signed-off-by">
Signed-off-by: Eric B Munson &lt;emunson@akamai.com&gt;</span>
Cc: Michal Hocko &lt;mhocko@suse.cz&gt;
Cc: Vlastimil Babka &lt;vbabka@suse.cz&gt;
Cc: Jonathan Corbet &lt;corbet@lwn.net&gt;
Cc: linux-alpha@vger.kernel.org
Cc: linux-kernel@vger.kernel.org
Cc: linux-mips@linux-mips.org
Cc: linux-parisc@vger.kernel.org
Cc: linuxppc-dev@lists.ozlabs.org
Cc: sparclinux@vger.kernel.org
Cc: linux-xtensa@linux-xtensa.org
Cc: dri-devel@lists.freedesktop.org
Cc: linux-mm@kvack.org
Cc: linux-arch@vger.kernel.org
Cc: linux-api@vger.kernel.org
---
Changes from V3:
Do extensive search for VM_LOCKED and ensure that VM_LOCKONFAULT is also handled
 where appropriate

 arch/alpha/include/uapi/asm/mman.h   |  2 +
 arch/mips/include/uapi/asm/mman.h    |  2 +
 arch/parisc/include/uapi/asm/mman.h  |  2 +
 arch/powerpc/include/uapi/asm/mman.h |  2 +
 arch/sparc/include/uapi/asm/mman.h   |  2 +
 arch/tile/include/uapi/asm/mman.h    |  3 ++
 arch/xtensa/include/uapi/asm/mman.h  |  2 +
 drivers/gpu/drm/drm_vm.c             |  8 ++-
 fs/proc/task_mmu.c                   |  3 +-
 include/linux/mm.h                   |  2 +
 include/uapi/asm-generic/mman.h      |  2 +
 kernel/events/uprobes.c              |  2 +-
 kernel/fork.c                        |  2 +-
 mm/debug.c                           |  1 +
 mm/gup.c                             |  3 +-
 mm/huge_memory.c                     |  3 +-
 mm/hugetlb.c                         |  4 +-
 mm/internal.h                        |  5 +-
 mm/ksm.c                             |  2 +-
 mm/madvise.c                         |  4 +-
 mm/memory.c                          |  5 +-
 mm/mlock.c                           | 98 +++++++++++++++++++++++++-----------
 mm/mmap.c                            | 28 +++++++----
 mm/mremap.c                          |  6 +--
 mm/msync.c                           |  2 +-
 mm/rmap.c                            | 12 ++---
 mm/shmem.c                           |  2 +-
 mm/swap.c                            |  3 +-
 mm/vmscan.c                          |  2 +-
 29 files changed, 145 insertions(+), 69 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - July 22, 2015, 10:03 a.m.</div>
<pre class="content">
On 07/21/2015 09:59 PM, Eric B Munson wrote:
<span class="quote">&gt; The cost of faulting in all memory to be locked can be very high when</span>
<span class="quote">&gt; working with large mappings.  If only portions of the mapping will be</span>
<span class="quote">&gt; used this can incur a high penalty for locking.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; For the example of a large file, this is the usage pattern for a large</span>
<span class="quote">&gt; statical language model (probably applies to other statical or graphical</span>
<span class="quote">&gt; models as well).  For the security example, any application transacting</span>
<span class="quote">&gt; in data that cannot be swapped out (credit card data, medical records,</span>
<span class="quote">&gt; etc).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; This patch introduces the ability to request that pages are not</span>
<span class="quote">&gt; pre-faulted, but are placed on the unevictable LRU when they are finally</span>
<span class="quote">&gt; faulted in.  This can be done area at a time via the</span>
<span class="quote">&gt; mlock2(MLOCK_ONFAULT) or the mlockall(MCL_ONFAULT) system calls.  These</span>
<span class="quote">&gt; calls can be undone via munlock2(MLOCK_ONFAULT) or</span>
<span class="quote">&gt; munlockall2(MCL_ONFAULT).</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Applying the VM_LOCKONFAULT flag to a mapping with pages that are</span>
<span class="quote">&gt; already present required the addition of a function in gup.c to pin all</span>
<span class="quote">&gt; pages which are present in an address range.  It borrows heavily from</span>
<span class="quote">&gt; __mm_populate().</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; To keep accounting checks out of the page fault path, users are billed</span>
<span class="quote">&gt; for the entire mapping lock as if MLOCK_LOCKED was used.</span>

Hi,

I think you should include a complete description of which transitions 
for vma states and mlock2/munlock2 flags applied on them are valid and 
what they do. It will also help with the manpages.
You explained some to Jon in the last thread, but I think there should 
be a canonical description in changelog (if not also Documentation, if 
mlock is covered there).

For example the scenario Jon asked, what happens after a 
mlock2(MLOCK_ONFAULT) followed by mlock2(MLOCK_LOCKED), and that the 
answer is &quot;nothing&quot;. Your promised code comment for apply_vma_flags() 
doesn&#39;t suffice IMHO (and I&#39;m not sure it&#39;s there, anyway?).

But the more I think about the scenario and your new VM_LOCKONFAULT vma 
flag, it seems awkward to me. Why should munlocking at all care if the 
vma was mlocked with MLOCK_LOCKED or MLOCK_ONFAULT? In either case the 
result is that all pages currently populated are munlocked. So the flags 
for munlock2 should be unnecessary.

I also think VM_LOCKONFAULT is unnecessary. VM_LOCKED should be enough - 
see how you had to handle the new flag in all places that had to handle 
the old flag? I think the information whether mlock was supposed to 
fault the whole vma is obsolete at the moment mlock returns. VM_LOCKED 
should be enough for both modes, and the flag to mlock2 could just 
control whether the pre-faulting is done.

So what should be IMHO enough:
- munlock can stay without flags
- mlock2 has only one new flag MLOCK_ONFAULT. If specified, pre-faulting 
is not done, just set VM_LOCKED and mlock pages already present.
- same with mmap(MAP_LOCKONFAULT) (need to define what happens when both 
MAP_LOCKED and MAP_LOCKONFAULT are specified).

Now mlockall(MCL_FUTURE) muddles the situation in that it stores the 
information for future VMA&#39;s in current-&gt;mm-&gt;def_flags, and this 
def_flags would need to distinguish VM_LOCKED with population and 
without. But that could be still solvable without introducing a new vma 
flag everywhere.
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=111821">Eric B Munson</a> - July 22, 2015, 6:43 p.m.</div>
<pre class="content">
On Wed, 22 Jul 2015, Vlastimil Babka wrote:
<span class="quote">
&gt; On 07/21/2015 09:59 PM, Eric B Munson wrote:</span>
<span class="quote">&gt; &gt;The cost of faulting in all memory to be locked can be very high when</span>
<span class="quote">&gt; &gt;working with large mappings.  If only portions of the mapping will be</span>
<span class="quote">&gt; &gt;used this can incur a high penalty for locking.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;For the example of a large file, this is the usage pattern for a large</span>
<span class="quote">&gt; &gt;statical language model (probably applies to other statical or graphical</span>
<span class="quote">&gt; &gt;models as well).  For the security example, any application transacting</span>
<span class="quote">&gt; &gt;in data that cannot be swapped out (credit card data, medical records,</span>
<span class="quote">&gt; &gt;etc).</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;This patch introduces the ability to request that pages are not</span>
<span class="quote">&gt; &gt;pre-faulted, but are placed on the unevictable LRU when they are finally</span>
<span class="quote">&gt; &gt;faulted in.  This can be done area at a time via the</span>
<span class="quote">&gt; &gt;mlock2(MLOCK_ONFAULT) or the mlockall(MCL_ONFAULT) system calls.  These</span>
<span class="quote">&gt; &gt;calls can be undone via munlock2(MLOCK_ONFAULT) or</span>
<span class="quote">&gt; &gt;munlockall2(MCL_ONFAULT).</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Applying the VM_LOCKONFAULT flag to a mapping with pages that are</span>
<span class="quote">&gt; &gt;already present required the addition of a function in gup.c to pin all</span>
<span class="quote">&gt; &gt;pages which are present in an address range.  It borrows heavily from</span>
<span class="quote">&gt; &gt;__mm_populate().</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;To keep accounting checks out of the page fault path, users are billed</span>
<span class="quote">&gt; &gt;for the entire mapping lock as if MLOCK_LOCKED was used.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hi,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think you should include a complete description of which</span>
<span class="quote">&gt; transitions for vma states and mlock2/munlock2 flags applied on them</span>
<span class="quote">&gt; are valid and what they do. It will also help with the manpages.</span>
<span class="quote">&gt; You explained some to Jon in the last thread, but I think there</span>
<span class="quote">&gt; should be a canonical description in changelog (if not also</span>
<span class="quote">&gt; Documentation, if mlock is covered there).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For example the scenario Jon asked, what happens after a</span>
<span class="quote">&gt; mlock2(MLOCK_ONFAULT) followed by mlock2(MLOCK_LOCKED), and that the</span>
<span class="quote">&gt; answer is &quot;nothing&quot;. Your promised code comment for</span>
<span class="quote">&gt; apply_vma_flags() doesn&#39;t suffice IMHO (and I&#39;m not sure it&#39;s there,</span>
<span class="quote">&gt; anyway?).</span>

I missed adding that comment to the code, will be there in V5 along with
the description in the changelog.
<span class="quote">
&gt; </span>
<span class="quote">&gt; But the more I think about the scenario and your new VM_LOCKONFAULT</span>
<span class="quote">&gt; vma flag, it seems awkward to me. Why should munlocking at all care</span>
<span class="quote">&gt; if the vma was mlocked with MLOCK_LOCKED or MLOCK_ONFAULT? In either</span>
<span class="quote">&gt; case the result is that all pages currently populated are munlocked.</span>
<span class="quote">&gt; So the flags for munlock2 should be unnecessary.</span>

Say a user has a large area of interleaved MLOCK_LOCK and MLOCK_ONFAULT
mappings and they want to unlock only the ones with MLOCK_LOCK.  With
the current implementation, this is possible in a single system call
that spans the entire region.  With your suggestion, the user would have
to know what regions where locked with MLOCK_LOCK and call munlock() on
each of them.  IMO, the way munlock2() works better mirrors the way
munlock() currently works when called on a large area of interleaved
locked and unlocked areas.
<span class="quote">
&gt; </span>
<span class="quote">&gt; I also think VM_LOCKONFAULT is unnecessary. VM_LOCKED should be</span>
<span class="quote">&gt; enough - see how you had to handle the new flag in all places that</span>
<span class="quote">&gt; had to handle the old flag? I think the information whether mlock</span>
<span class="quote">&gt; was supposed to fault the whole vma is obsolete at the moment mlock</span>
<span class="quote">&gt; returns. VM_LOCKED should be enough for both modes, and the flag to</span>
<span class="quote">&gt; mlock2 could just control whether the pre-faulting is done.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So what should be IMHO enough:</span>
<span class="quote">&gt; - munlock can stay without flags</span>
<span class="quote">&gt; - mlock2 has only one new flag MLOCK_ONFAULT. If specified,</span>
<span class="quote">&gt; pre-faulting is not done, just set VM_LOCKED and mlock pages already</span>
<span class="quote">&gt; present.</span>
<span class="quote">&gt; - same with mmap(MAP_LOCKONFAULT) (need to define what happens when</span>
<span class="quote">&gt; both MAP_LOCKED and MAP_LOCKONFAULT are specified).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Now mlockall(MCL_FUTURE) muddles the situation in that it stores the</span>
<span class="quote">&gt; information for future VMA&#39;s in current-&gt;mm-&gt;def_flags, and this</span>
<span class="quote">&gt; def_flags would need to distinguish VM_LOCKED with population and</span>
<span class="quote">&gt; without. But that could be still solvable without introducing a new</span>
<span class="quote">&gt; vma flag everywhere.</span>

With you right up until that last paragraph.  I have been staring at
this a while and I cannot come up a way to handle the
mlockall(MCL_ONFAULT) without introducing a new vm flag.  It doesn&#39;t
have to be VM_LOCKONFAULT, we could use the model that Michal Hocko
suggested with something like VM_FAULTPOPULATE.  However, we can&#39;t
really use this flag anywhere except the mlock code becuase we have to
be able to distinguish a caller that wants to use MLOCK_LOCK with
whatever control VM_FAULTPOPULATE might grant outside of mlock and a
caller that wants MLOCK_ONFAULT.  That was a long way of saying we need
an extra vma flag regardless.  However, if that flag only controls if
mlock pre-populates it would work and it would do away with most of the
places I had to touch to handle VM_LOCKONFAULT properly.

I picked VM_LOCKONFAULT because it is explicit about what it is for and
there is little risk of someone coming along in 5 years and saying &quot;why
not overload this flag to do this other thing completely unrelated to
mlock?&quot;.  A flag for controling speculative population is more likely to
be overloaded outside of mlock().

If you have a sane way of handling mlockall(MCL_ONFAULT) without a new
VMA flag, I am happy to give it a try, but I haven&#39;t been able to come
up with one that doesn&#39;t have its own gremlins.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - July 23, 2015, 10:03 a.m.</div>
<pre class="content">
On 07/22/2015 08:43 PM, Eric B Munson wrote:
<span class="quote">&gt; On Wed, 22 Jul 2015, Vlastimil Babka wrote:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Hi,</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I think you should include a complete description of which</span>
<span class="quote">&gt;&gt; transitions for vma states and mlock2/munlock2 flags applied on them</span>
<span class="quote">&gt;&gt; are valid and what they do. It will also help with the manpages.</span>
<span class="quote">&gt;&gt; You explained some to Jon in the last thread, but I think there</span>
<span class="quote">&gt;&gt; should be a canonical description in changelog (if not also</span>
<span class="quote">&gt;&gt; Documentation, if mlock is covered there).</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; For example the scenario Jon asked, what happens after a</span>
<span class="quote">&gt;&gt; mlock2(MLOCK_ONFAULT) followed by mlock2(MLOCK_LOCKED), and that the</span>
<span class="quote">&gt;&gt; answer is &quot;nothing&quot;. Your promised code comment for</span>
<span class="quote">&gt;&gt; apply_vma_flags() doesn&#39;t suffice IMHO (and I&#39;m not sure it&#39;s there,</span>
<span class="quote">&gt;&gt; anyway?).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I missed adding that comment to the code, will be there in V5 along with</span>
<span class="quote">&gt; the description in the changelog.</span>

Thanks!
<span class="quote">
&gt;&gt; </span>
<span class="quote">&gt;&gt; But the more I think about the scenario and your new VM_LOCKONFAULT</span>
<span class="quote">&gt;&gt; vma flag, it seems awkward to me. Why should munlocking at all care</span>
<span class="quote">&gt;&gt; if the vma was mlocked with MLOCK_LOCKED or MLOCK_ONFAULT? In either</span>
<span class="quote">&gt;&gt; case the result is that all pages currently populated are munlocked.</span>
<span class="quote">&gt;&gt; So the flags for munlock2 should be unnecessary.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Say a user has a large area of interleaved MLOCK_LOCK and MLOCK_ONFAULT</span>
<span class="quote">&gt; mappings and they want to unlock only the ones with MLOCK_LOCK.  With</span>
<span class="quote">&gt; the current implementation, this is possible in a single system call</span>
<span class="quote">&gt; that spans the entire region.  With your suggestion, the user would have</span>
<span class="quote">&gt; to know what regions where locked with MLOCK_LOCK and call munlock() on</span>
<span class="quote">&gt; each of them.  IMO, the way munlock2() works better mirrors the way</span>
<span class="quote">&gt; munlock() currently works when called on a large area of interleaved</span>
<span class="quote">&gt; locked and unlocked areas.</span>

Um OK, that scenario is possible in theory. But I have a hard time imagining
that somebody would really want to do that. I think much more people would
benefit from a simpler API.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; I also think VM_LOCKONFAULT is unnecessary. VM_LOCKED should be</span>
<span class="quote">&gt;&gt; enough - see how you had to handle the new flag in all places that</span>
<span class="quote">&gt;&gt; had to handle the old flag? I think the information whether mlock</span>
<span class="quote">&gt;&gt; was supposed to fault the whole vma is obsolete at the moment mlock</span>
<span class="quote">&gt;&gt; returns. VM_LOCKED should be enough for both modes, and the flag to</span>
<span class="quote">&gt;&gt; mlock2 could just control whether the pre-faulting is done.</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; So what should be IMHO enough:</span>
<span class="quote">&gt;&gt; - munlock can stay without flags</span>
<span class="quote">&gt;&gt; - mlock2 has only one new flag MLOCK_ONFAULT. If specified,</span>
<span class="quote">&gt;&gt; pre-faulting is not done, just set VM_LOCKED and mlock pages already</span>
<span class="quote">&gt;&gt; present.</span>
<span class="quote">&gt;&gt; - same with mmap(MAP_LOCKONFAULT) (need to define what happens when</span>
<span class="quote">&gt;&gt; both MAP_LOCKED and MAP_LOCKONFAULT are specified).</span>
<span class="quote">&gt;&gt; </span>
<span class="quote">&gt;&gt; Now mlockall(MCL_FUTURE) muddles the situation in that it stores the</span>
<span class="quote">&gt;&gt; information for future VMA&#39;s in current-&gt;mm-&gt;def_flags, and this</span>
<span class="quote">&gt;&gt; def_flags would need to distinguish VM_LOCKED with population and</span>
<span class="quote">&gt;&gt; without. But that could be still solvable without introducing a new</span>
<span class="quote">&gt;&gt; vma flag everywhere.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With you right up until that last paragraph.  I have been staring at</span>
<span class="quote">&gt; this a while and I cannot come up a way to handle the</span>
<span class="quote">&gt; mlockall(MCL_ONFAULT) without introducing a new vm flag.  It doesn&#39;t</span>
<span class="quote">&gt; have to be VM_LOCKONFAULT, we could use the model that Michal Hocko</span>
<span class="quote">&gt; suggested with something like VM_FAULTPOPULATE.  However, we can&#39;t</span>
<span class="quote">&gt; really use this flag anywhere except the mlock code becuase we have to</span>
<span class="quote">&gt; be able to distinguish a caller that wants to use MLOCK_LOCK with</span>
<span class="quote">&gt; whatever control VM_FAULTPOPULATE might grant outside of mlock and a</span>
<span class="quote">&gt; caller that wants MLOCK_ONFAULT.  That was a long way of saying we need</span>
<span class="quote">&gt; an extra vma flag regardless.  However, if that flag only controls if</span>
<span class="quote">&gt; mlock pre-populates it would work and it would do away with most of the</span>
<span class="quote">&gt; places I had to touch to handle VM_LOCKONFAULT properly.</span>

Yes, it would be a good way. Adding a new vma flag is probably cleanest after
all, but the flag would be set *in addition* to VM_LOCKED, *just* to prevent
pre-faulting. The places that check VM_LOCKED for the actual page mlocking (i.e.
try_to_unmap_one) would just keep checking VM_LOCKED. The places where VM_LOCKED
is checked to trigger prepopulation, would skip that if VM_LOCKONFAULT is also
set. Having VM_LOCKONFAULT set without also VM_LOCKED itself would be invalid state.

This should work fine with the simplified API as I proposed so let me reiterate
and try fill in the blanks:

- mlock2 has only one new flag MLOCK_ONFAULT. If specified, VM_LOCKONFAULT is
set in addition to VM_LOCKED and no prefaulting is done
  - old mlock syscall naturally behaves as mlock2 without MLOCK_ONFAULT
  - calling mlock/mlock2 on an already-mlocked area (if that&#39;s permitted
already?) will add/remove VM_LOCKONFAULT as needed. If it&#39;s removing,
prepopulate whole range. Of course adding VM_LOCKONFAULT to a vma that was
already prefaulted doesn&#39;t make any difference, but it&#39;s consistent with the rest.
- munlock removes both VM_LOCKED and VM_LOCKONFAULT
- mmap could treat MAP_LOCKONFAULT as a modifier to MAP_LOCKED to be consistent?
or not? I&#39;m not sure here, either way subtly differs from mlock API anyway, I
just wish MAP_LOCKED never existed...
- mlockall(MCL_CURRENT) sets or clears VM_LOCKONFAULT depending on
MCL_LOCKONFAULT, mlockall(MCL_FUTURE) does the same on mm-&gt;def_flags
- munlockall2 removes both, like munlock. munlockall2(MCL_FUTURE) does that to
def_flags
<span class="quote">
&gt; I picked VM_LOCKONFAULT because it is explicit about what it is for and</span>
<span class="quote">&gt; there is little risk of someone coming along in 5 years and saying &quot;why</span>
<span class="quote">&gt; not overload this flag to do this other thing completely unrelated to</span>
<span class="quote">&gt; mlock?&quot;.  A flag for controling speculative population is more likely to</span>
<span class="quote">&gt; be overloaded outside of mlock().</span>

Sure, let&#39;s make clear the name is related to mlock, but the behavior could
still be additive to MAP_LOCKED.
<span class="quote">
&gt; If you have a sane way of handling mlockall(MCL_ONFAULT) without a new</span>
<span class="quote">&gt; VMA flag, I am happy to give it a try, but I haven&#39;t been able to come</span>
<span class="quote">&gt; up with one that doesn&#39;t have its own gremlins.</span>

Well we could store the MCL_FUTURE | MCL_ONFAULT bit elsewhere in mm_struct than
the def_flags field. The VM_LOCKED field is already evaluated specially from all
the other def_flags. We are nearing the full 32bit space for vma flags. I think
all I&#39;ve proposed above wouldn&#39;t change much if we removed per-vma
VM_LOCKONFAULT flag from the equation. Just that re-mlocking area already
mlocked *withouth* MLOCK_ONFAULT wouldn&#39;t know that it was alread prepopulated,
and would have to re-populate in either case (I&#39;m not sure, maybe it&#39;s already
done by current implementation anyway so it&#39;s not a potential performance
regression).
Only mlockall(MCL_FUTURE | MCL_ONFAULT) should really need the ONFAULT info to
&quot;stick&quot; somewhere in mm_struct, but it doesn&#39;t have to be def_flags?
--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=111821">Eric B Munson</a> - July 23, 2015, 3:21 p.m.</div>
<pre class="content">
On Thu, 23 Jul 2015, Vlastimil Babka wrote:
<span class="quote">
&gt; On 07/22/2015 08:43 PM, Eric B Munson wrote:</span>
<span class="quote">&gt; &gt; On Wed, 22 Jul 2015, Vlastimil Babka wrote:</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; Hi,</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; I think you should include a complete description of which</span>
<span class="quote">&gt; &gt;&gt; transitions for vma states and mlock2/munlock2 flags applied on them</span>
<span class="quote">&gt; &gt;&gt; are valid and what they do. It will also help with the manpages.</span>
<span class="quote">&gt; &gt;&gt; You explained some to Jon in the last thread, but I think there</span>
<span class="quote">&gt; &gt;&gt; should be a canonical description in changelog (if not also</span>
<span class="quote">&gt; &gt;&gt; Documentation, if mlock is covered there).</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; For example the scenario Jon asked, what happens after a</span>
<span class="quote">&gt; &gt;&gt; mlock2(MLOCK_ONFAULT) followed by mlock2(MLOCK_LOCKED), and that the</span>
<span class="quote">&gt; &gt;&gt; answer is &quot;nothing&quot;. Your promised code comment for</span>
<span class="quote">&gt; &gt;&gt; apply_vma_flags() doesn&#39;t suffice IMHO (and I&#39;m not sure it&#39;s there,</span>
<span class="quote">&gt; &gt;&gt; anyway?).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I missed adding that comment to the code, will be there in V5 along with</span>
<span class="quote">&gt; &gt; the description in the changelog.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thanks!</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; But the more I think about the scenario and your new VM_LOCKONFAULT</span>
<span class="quote">&gt; &gt;&gt; vma flag, it seems awkward to me. Why should munlocking at all care</span>
<span class="quote">&gt; &gt;&gt; if the vma was mlocked with MLOCK_LOCKED or MLOCK_ONFAULT? In either</span>
<span class="quote">&gt; &gt;&gt; case the result is that all pages currently populated are munlocked.</span>
<span class="quote">&gt; &gt;&gt; So the flags for munlock2 should be unnecessary.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Say a user has a large area of interleaved MLOCK_LOCK and MLOCK_ONFAULT</span>
<span class="quote">&gt; &gt; mappings and they want to unlock only the ones with MLOCK_LOCK.  With</span>
<span class="quote">&gt; &gt; the current implementation, this is possible in a single system call</span>
<span class="quote">&gt; &gt; that spans the entire region.  With your suggestion, the user would have</span>
<span class="quote">&gt; &gt; to know what regions where locked with MLOCK_LOCK and call munlock() on</span>
<span class="quote">&gt; &gt; each of them.  IMO, the way munlock2() works better mirrors the way</span>
<span class="quote">&gt; &gt; munlock() currently works when called on a large area of interleaved</span>
<span class="quote">&gt; &gt; locked and unlocked areas.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Um OK, that scenario is possible in theory. But I have a hard time imagining</span>
<span class="quote">&gt; that somebody would really want to do that. I think much more people would</span>
<span class="quote">&gt; benefit from a simpler API.</span>

It wasn&#39;t about imagining a scenario, more about keeping parity with
something that currently works (unlocking a large area of interleaved
locked and unlocked regions).  However, there is no reason we can&#39;t add
the new munlock2 later if it is desired.
<span class="quote">
&gt; </span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; I also think VM_LOCKONFAULT is unnecessary. VM_LOCKED should be</span>
<span class="quote">&gt; &gt;&gt; enough - see how you had to handle the new flag in all places that</span>
<span class="quote">&gt; &gt;&gt; had to handle the old flag? I think the information whether mlock</span>
<span class="quote">&gt; &gt;&gt; was supposed to fault the whole vma is obsolete at the moment mlock</span>
<span class="quote">&gt; &gt;&gt; returns. VM_LOCKED should be enough for both modes, and the flag to</span>
<span class="quote">&gt; &gt;&gt; mlock2 could just control whether the pre-faulting is done.</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; So what should be IMHO enough:</span>
<span class="quote">&gt; &gt;&gt; - munlock can stay without flags</span>
<span class="quote">&gt; &gt;&gt; - mlock2 has only one new flag MLOCK_ONFAULT. If specified,</span>
<span class="quote">&gt; &gt;&gt; pre-faulting is not done, just set VM_LOCKED and mlock pages already</span>
<span class="quote">&gt; &gt;&gt; present.</span>
<span class="quote">&gt; &gt;&gt; - same with mmap(MAP_LOCKONFAULT) (need to define what happens when</span>
<span class="quote">&gt; &gt;&gt; both MAP_LOCKED and MAP_LOCKONFAULT are specified).</span>
<span class="quote">&gt; &gt;&gt; </span>
<span class="quote">&gt; &gt;&gt; Now mlockall(MCL_FUTURE) muddles the situation in that it stores the</span>
<span class="quote">&gt; &gt;&gt; information for future VMA&#39;s in current-&gt;mm-&gt;def_flags, and this</span>
<span class="quote">&gt; &gt;&gt; def_flags would need to distinguish VM_LOCKED with population and</span>
<span class="quote">&gt; &gt;&gt; without. But that could be still solvable without introducing a new</span>
<span class="quote">&gt; &gt;&gt; vma flag everywhere.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; With you right up until that last paragraph.  I have been staring at</span>
<span class="quote">&gt; &gt; this a while and I cannot come up a way to handle the</span>
<span class="quote">&gt; &gt; mlockall(MCL_ONFAULT) without introducing a new vm flag.  It doesn&#39;t</span>
<span class="quote">&gt; &gt; have to be VM_LOCKONFAULT, we could use the model that Michal Hocko</span>
<span class="quote">&gt; &gt; suggested with something like VM_FAULTPOPULATE.  However, we can&#39;t</span>
<span class="quote">&gt; &gt; really use this flag anywhere except the mlock code becuase we have to</span>
<span class="quote">&gt; &gt; be able to distinguish a caller that wants to use MLOCK_LOCK with</span>
<span class="quote">&gt; &gt; whatever control VM_FAULTPOPULATE might grant outside of mlock and a</span>
<span class="quote">&gt; &gt; caller that wants MLOCK_ONFAULT.  That was a long way of saying we need</span>
<span class="quote">&gt; &gt; an extra vma flag regardless.  However, if that flag only controls if</span>
<span class="quote">&gt; &gt; mlock pre-populates it would work and it would do away with most of the</span>
<span class="quote">&gt; &gt; places I had to touch to handle VM_LOCKONFAULT properly.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, it would be a good way. Adding a new vma flag is probably cleanest after</span>
<span class="quote">&gt; all, but the flag would be set *in addition* to VM_LOCKED, *just* to prevent</span>
<span class="quote">&gt; pre-faulting. The places that check VM_LOCKED for the actual page mlocking (i.e.</span>
<span class="quote">&gt; try_to_unmap_one) would just keep checking VM_LOCKED. The places where VM_LOCKED</span>
<span class="quote">&gt; is checked to trigger prepopulation, would skip that if VM_LOCKONFAULT is also</span>
<span class="quote">&gt; set. Having VM_LOCKONFAULT set without also VM_LOCKED itself would be invalid state.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This should work fine with the simplified API as I proposed so let me reiterate</span>
<span class="quote">&gt; and try fill in the blanks:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - mlock2 has only one new flag MLOCK_ONFAULT. If specified, VM_LOCKONFAULT is</span>
<span class="quote">&gt; set in addition to VM_LOCKED and no prefaulting is done</span>
<span class="quote">&gt;   - old mlock syscall naturally behaves as mlock2 without MLOCK_ONFAULT</span>
<span class="quote">&gt;   - calling mlock/mlock2 on an already-mlocked area (if that&#39;s permitted</span>
<span class="quote">&gt; already?) will add/remove VM_LOCKONFAULT as needed. If it&#39;s removing,</span>
<span class="quote">&gt; prepopulate whole range. Of course adding VM_LOCKONFAULT to a vma that was</span>
<span class="quote">&gt; already prefaulted doesn&#39;t make any difference, but it&#39;s consistent with the rest.</span>
<span class="quote">&gt; - munlock removes both VM_LOCKED and VM_LOCKONFAULT</span>
<span class="quote">&gt; - mmap could treat MAP_LOCKONFAULT as a modifier to MAP_LOCKED to be consistent?</span>
<span class="quote">&gt; or not? I&#39;m not sure here, either way subtly differs from mlock API anyway, I</span>
<span class="quote">&gt; just wish MAP_LOCKED never existed...</span>
<span class="quote">&gt; - mlockall(MCL_CURRENT) sets or clears VM_LOCKONFAULT depending on</span>
<span class="quote">&gt; MCL_LOCKONFAULT, mlockall(MCL_FUTURE) does the same on mm-&gt;def_flags</span>
<span class="quote">&gt; - munlockall2 removes both, like munlock. munlockall2(MCL_FUTURE) does that to</span>
<span class="quote">&gt; def_flags</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; I picked VM_LOCKONFAULT because it is explicit about what it is for and</span>
<span class="quote">&gt; &gt; there is little risk of someone coming along in 5 years and saying &quot;why</span>
<span class="quote">&gt; &gt; not overload this flag to do this other thing completely unrelated to</span>
<span class="quote">&gt; &gt; mlock?&quot;.  A flag for controling speculative population is more likely to</span>
<span class="quote">&gt; &gt; be overloaded outside of mlock().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure, let&#39;s make clear the name is related to mlock, but the behavior could</span>
<span class="quote">&gt; still be additive to MAP_LOCKED.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; If you have a sane way of handling mlockall(MCL_ONFAULT) without a new</span>
<span class="quote">&gt; &gt; VMA flag, I am happy to give it a try, but I haven&#39;t been able to come</span>
<span class="quote">&gt; &gt; up with one that doesn&#39;t have its own gremlins.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well we could store the MCL_FUTURE | MCL_ONFAULT bit elsewhere in mm_struct than</span>
<span class="quote">&gt; the def_flags field. The VM_LOCKED field is already evaluated specially from all</span>
<span class="quote">&gt; the other def_flags. We are nearing the full 32bit space for vma flags. I think</span>
<span class="quote">&gt; all I&#39;ve proposed above wouldn&#39;t change much if we removed per-vma</span>
<span class="quote">&gt; VM_LOCKONFAULT flag from the equation. Just that re-mlocking area already</span>
<span class="quote">&gt; mlocked *withouth* MLOCK_ONFAULT wouldn&#39;t know that it was alread prepopulated,</span>
<span class="quote">&gt; and would have to re-populate in either case (I&#39;m not sure, maybe it&#39;s already</span>
<span class="quote">&gt; done by current implementation anyway so it&#39;s not a potential performance</span>
<span class="quote">&gt; regression).</span>
<span class="quote">&gt; Only mlockall(MCL_FUTURE | MCL_ONFAULT) should really need the ONFAULT info to</span>
<span class="quote">&gt; &quot;stick&quot; somewhere in mm_struct, but it doesn&#39;t have to be def_flags?</span>

This all sounds fine and should still cover the usecase that started
this adventure.  I will include this change in the V5 spin.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/alpha/include/uapi/asm/mman.h b/arch/alpha/include/uapi/asm/mman.h</span>
<span class="p_header">index ec72436..77ae8db 100644</span>
<span class="p_header">--- a/arch/alpha/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/alpha/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -37,8 +37,10 @@</span> <span class="p_context"></span>
 
 #define MCL_CURRENT	 8192		/* lock all currently mapped pages */
 #define MCL_FUTURE	16384		/* lock all additions to address space */
<span class="p_add">+#define MCL_ONFAULT	32768		/* lock all pages that are faulted in */</span>
 
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MADV_NORMAL	0		/* no further special treatment */
 #define MADV_RANDOM	1		/* expect random page references */
<span class="p_header">diff --git a/arch/mips/include/uapi/asm/mman.h b/arch/mips/include/uapi/asm/mman.h</span>
<span class="p_header">index 67c1cdf..71ed81d 100644</span>
<span class="p_header">--- a/arch/mips/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/mips/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -61,11 +61,13 @@</span> <span class="p_context"></span>
  */
 #define MCL_CURRENT	1		/* lock all current mappings */
 #define MCL_FUTURE	2		/* lock all future mappings */
<span class="p_add">+#define MCL_ONFAULT	4		/* lock all pages that are faulted in */</span>
 
 /*
  * Flags for mlock
  */
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MADV_NORMAL	0		/* no further special treatment */
 #define MADV_RANDOM	1		/* expect random page references */
<span class="p_header">diff --git a/arch/parisc/include/uapi/asm/mman.h b/arch/parisc/include/uapi/asm/mman.h</span>
<span class="p_header">index daab994..c0871ce 100644</span>
<span class="p_header">--- a/arch/parisc/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/parisc/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -31,8 +31,10 @@</span> <span class="p_context"></span>
 
 #define MCL_CURRENT	1		/* lock all current mappings */
 #define MCL_FUTURE	2		/* lock all future mappings */
<span class="p_add">+#define MCL_ONFAULT	4		/* lock all pages that are faulted in */</span>
 
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MADV_NORMAL     0               /* no further special treatment */
 #define MADV_RANDOM     1               /* expect random page references */
<span class="p_header">diff --git a/arch/powerpc/include/uapi/asm/mman.h b/arch/powerpc/include/uapi/asm/mman.h</span>
<span class="p_header">index 189e85f..f93f7eb 100644</span>
<span class="p_header">--- a/arch/powerpc/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/powerpc/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -22,8 +22,10 @@</span> <span class="p_context"></span>
 
 #define MCL_CURRENT     0x2000          /* lock all currently mapped pages */
 #define MCL_FUTURE      0x4000          /* lock all additions to address space */
<span class="p_add">+#define MCL_ONFAULT	0x8000		/* lock all pages that are faulted in */</span>
 
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MAP_POPULATE	0x8000		/* populate (prefault) pagetables */
 #define MAP_NONBLOCK	0x10000		/* do not block on IO */
<span class="p_header">diff --git a/arch/sparc/include/uapi/asm/mman.h b/arch/sparc/include/uapi/asm/mman.h</span>
<span class="p_header">index 13d51be..8cd2ebc 100644</span>
<span class="p_header">--- a/arch/sparc/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/sparc/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -17,8 +17,10 @@</span> <span class="p_context"></span>
 
 #define MCL_CURRENT     0x2000          /* lock all currently mapped pages */
 #define MCL_FUTURE      0x4000          /* lock all additions to address space */
<span class="p_add">+#define MCL_ONFAULT	0x8000		/* lock all pages that are faulted in */</span>
 
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MAP_POPULATE	0x8000		/* populate (prefault) pagetables */
 #define MAP_NONBLOCK	0x10000		/* do not block on IO */
<span class="p_header">diff --git a/arch/tile/include/uapi/asm/mman.h b/arch/tile/include/uapi/asm/mman.h</span>
<span class="p_header">index f69ce48..acdd013 100644</span>
<span class="p_header">--- a/arch/tile/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/tile/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -36,11 +36,14 @@</span> <span class="p_context"></span>
  */
 #define MCL_CURRENT	1		/* lock all current mappings */
 #define MCL_FUTURE	2		/* lock all future mappings */
<span class="p_add">+#define MCL_ONFAULT	4		/* lock all pages that are faulted in */</span>
<span class="p_add">+</span>
 
 /*
  * Flags for mlock
  */
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 
 #endif /* _ASM_TILE_MMAN_H */
<span class="p_header">diff --git a/arch/xtensa/include/uapi/asm/mman.h b/arch/xtensa/include/uapi/asm/mman.h</span>
<span class="p_header">index 11f354f..5725a15 100644</span>
<span class="p_header">--- a/arch/xtensa/include/uapi/asm/mman.h</span>
<span class="p_header">+++ b/arch/xtensa/include/uapi/asm/mman.h</span>
<span class="p_chunk">@@ -74,11 +74,13 @@</span> <span class="p_context"></span>
  */
 #define MCL_CURRENT	1		/* lock all current mappings */
 #define MCL_FUTURE	2		/* lock all future mappings */
<span class="p_add">+#define MCL_ONFAULT	4		/* lock all pages that are faulted in */</span>
 
 /*
  * Flags for mlock
  */
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #define MADV_NORMAL	0		/* no further special treatment */
 #define MADV_RANDOM	1		/* expect random page references */
<span class="p_header">diff --git a/drivers/gpu/drm/drm_vm.c b/drivers/gpu/drm/drm_vm.c</span>
<span class="p_header">index aab49ee..dfbcfc2 100644</span>
<span class="p_header">--- a/drivers/gpu/drm/drm_vm.c</span>
<span class="p_header">+++ b/drivers/gpu/drm/drm_vm.c</span>
<span class="p_chunk">@@ -699,9 +699,15 @@</span> <span class="p_context"> int drm_vma_info(struct seq_file *m, void *data)</span>
 		   (void *)(unsigned long)virt_to_phys(high_memory));
 
 	list_for_each_entry(pt, &amp;dev-&gt;vmalist, head) {
<span class="p_add">+		char lock_flag = &#39;-&#39;;</span>
<span class="p_add">+</span>
 		vma = pt-&gt;vma;
 		if (!vma)
 			continue;
<span class="p_add">+		if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="p_add">+			lock_flag = &#39;l&#39;;</span>
<span class="p_add">+		else if (vma-&gt;vm_flags &amp; VM_LOCKONFAULT)</span>
<span class="p_add">+			lock_flag = &#39;f&#39;;</span>
 		seq_printf(m,
 			   &quot;\n%5d 0x%pK-0x%pK %c%c%c%c%c%c 0x%08lx000&quot;,
 			   pt-&gt;pid,
<span class="p_chunk">@@ -710,7 +716,7 @@</span> <span class="p_context"> int drm_vma_info(struct seq_file *m, void *data)</span>
 			   vma-&gt;vm_flags &amp; VM_WRITE ? &#39;w&#39; : &#39;-&#39;,
 			   vma-&gt;vm_flags &amp; VM_EXEC ? &#39;x&#39; : &#39;-&#39;,
 			   vma-&gt;vm_flags &amp; VM_MAYSHARE ? &#39;s&#39; : &#39;p&#39;,
<span class="p_del">-			   vma-&gt;vm_flags &amp; VM_LOCKED ? &#39;l&#39; : &#39;-&#39;,</span>
<span class="p_add">+			   lock_flag,</span>
 			   vma-&gt;vm_flags &amp; VM_IO ? &#39;i&#39; : &#39;-&#39;,
 			   vma-&gt;vm_pgoff);
 
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index ca1e091..2c435a7 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -579,6 +579,7 @@</span> <span class="p_context"> static void show_smap_vma_flags(struct seq_file *m, struct vm_area_struct *vma)</span>
 #ifdef CONFIG_X86_INTEL_MPX
 		[ilog2(VM_MPX)]		= &quot;mp&quot;,
 #endif
<span class="p_add">+		[ilog2(VM_LOCKONFAULT)]	= &quot;lf&quot;,</span>
 		[ilog2(VM_LOCKED)]	= &quot;lo&quot;,
 		[ilog2(VM_IO)]		= &quot;io&quot;,
 		[ilog2(VM_SEQ_READ)]	= &quot;sr&quot;,
<span class="p_chunk">@@ -654,7 +655,7 @@</span> <span class="p_context"> static int show_smap(struct seq_file *m, void *v, int is_pid)</span>
 		   mss.swap &gt;&gt; 10,
 		   vma_kernel_pagesize(vma) &gt;&gt; 10,
 		   vma_mmu_pagesize(vma) &gt;&gt; 10,
<span class="p_del">-		   (vma-&gt;vm_flags &amp; VM_LOCKED) ?</span>
<span class="p_add">+		   (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) ?</span>
 			(unsigned long)(mss.pss &gt;&gt; (10 + PSS_SHIFT)) : 0);
 
 	show_smap_vma_flags(m, vma);
<span class="p_header">diff --git a/include/linux/mm.h b/include/linux/mm.h</span>
<span class="p_header">index 2e872f9..e78544f 100644</span>
<span class="p_header">--- a/include/linux/mm.h</span>
<span class="p_header">+++ b/include/linux/mm.h</span>
<span class="p_chunk">@@ -127,6 +127,7 @@</span> <span class="p_context"> extern unsigned int kobjsize(const void *objp);</span>
 #define VM_PFNMAP	0x00000400	/* Page-ranges managed without &quot;struct page&quot;, just pure PFN */
 #define VM_DENYWRITE	0x00000800	/* ETXTBSY on write attempts.. */
 
<span class="p_add">+#define VM_LOCKONFAULT	0x00001000	/* Lock the pages covered when they are faulted in */</span>
 #define VM_LOCKED	0x00002000
 #define VM_IO           0x00004000	/* Memory mapped I/O or similar */
 
<span class="p_chunk">@@ -1865,6 +1866,7 @@</span> <span class="p_context"> static inline void mm_populate(unsigned long addr, unsigned long len)</span>
 	/* Ignore errors */
 	(void) __mm_populate(addr, len, 1);
 }
<span class="p_add">+extern int mm_lock_present(unsigned long addr, unsigned long start);</span>
 #else
 static inline void mm_populate(unsigned long addr, unsigned long len) {}
 #endif
<span class="p_header">diff --git a/include/uapi/asm-generic/mman.h b/include/uapi/asm-generic/mman.h</span>
<span class="p_header">index 242436b..555aab0 100644</span>
<span class="p_header">--- a/include/uapi/asm-generic/mman.h</span>
<span class="p_header">+++ b/include/uapi/asm-generic/mman.h</span>
<span class="p_chunk">@@ -17,7 +17,9 @@</span> <span class="p_context"></span>
 
 #define MCL_CURRENT	1		/* lock all current mappings */
 #define MCL_FUTURE	2		/* lock all future mappings */
<span class="p_add">+#define MCL_ONFAULT	4		/* lock all pages that are faulted in */</span>
 
 #define MLOCK_LOCKED	0x01		/* Lock and populate the specified range */
<span class="p_add">+#define MLOCK_ONFAULT	0x02		/* Lock pages in range after they are faulted in, do not prefault */</span>
 
 #endif /* __ASM_GENERIC_MMAN_H */
<span class="p_header">diff --git a/kernel/events/uprobes.c b/kernel/events/uprobes.c</span>
<span class="p_header">index cb346f2..882c9f6 100644</span>
<span class="p_header">--- a/kernel/events/uprobes.c</span>
<span class="p_header">+++ b/kernel/events/uprobes.c</span>
<span class="p_chunk">@@ -201,7 +201,7 @@</span> <span class="p_context"> static int __replace_page(struct vm_area_struct *vma, unsigned long addr,</span>
 		try_to_free_swap(page);
 	pte_unmap_unlock(ptep, ptl);
 
<span class="p_del">-	if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT))</span>
 		munlock_vma_page(page);
 	put_page(page);
 
<span class="p_header">diff --git a/kernel/fork.c b/kernel/fork.c</span>
<span class="p_header">index dbd9b8d..a949228 100644</span>
<span class="p_header">--- a/kernel/fork.c</span>
<span class="p_header">+++ b/kernel/fork.c</span>
<span class="p_chunk">@@ -454,7 +454,7 @@</span> <span class="p_context"> static int dup_mmap(struct mm_struct *mm, struct mm_struct *oldmm)</span>
 		tmp-&gt;vm_mm = mm;
 		if (anon_vma_fork(tmp, mpnt))
 			goto fail_nomem_anon_vma_fork;
<span class="p_del">-		tmp-&gt;vm_flags &amp;= ~VM_LOCKED;</span>
<span class="p_add">+		tmp-&gt;vm_flags &amp;= ~(VM_LOCKED | VM_LOCKONFAULT);</span>
 		tmp-&gt;vm_next = tmp-&gt;vm_prev = NULL;
 		file = tmp-&gt;vm_file;
 		if (file) {
<span class="p_header">diff --git a/mm/debug.c b/mm/debug.c</span>
<span class="p_header">index 76089dd..25176bb 100644</span>
<span class="p_header">--- a/mm/debug.c</span>
<span class="p_header">+++ b/mm/debug.c</span>
<span class="p_chunk">@@ -121,6 +121,7 @@</span> <span class="p_context"> static const struct trace_print_flags vmaflags_names[] = {</span>
 	{VM_GROWSDOWN,			&quot;growsdown&quot;	},
 	{VM_PFNMAP,			&quot;pfnmap&quot;	},
 	{VM_DENYWRITE,			&quot;denywrite&quot;	},
<span class="p_add">+	{VM_LOCKONFAULT,		&quot;lockonfault&quot;	},</span>
 	{VM_LOCKED,			&quot;locked&quot;	},
 	{VM_IO,				&quot;io&quot;		},
 	{VM_SEQ_READ,			&quot;seqread&quot;	},
<span class="p_header">diff --git a/mm/gup.c b/mm/gup.c</span>
<span class="p_header">index 233ef17..097a22a 100644</span>
<span class="p_header">--- a/mm/gup.c</span>
<span class="p_header">+++ b/mm/gup.c</span>
<span class="p_chunk">@@ -92,7 +92,8 @@</span> <span class="p_context"> retry:</span>
 		 */
 		mark_page_accessed(page);
 	}
<span class="p_del">-	if ((flags &amp; FOLL_POPULATE) &amp;&amp; (vma-&gt;vm_flags &amp; VM_LOCKED)) {</span>
<span class="p_add">+	if ((flags &amp; FOLL_POPULATE) &amp;&amp;</span>
<span class="p_add">+	    (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT))) {</span>
 		/*
 		 * The preliminary mapping check is mainly to avoid the
 		 * pointless overhead of lock_page on the ZERO_PAGE
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index c107094..7985e35 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1238,7 +1238,8 @@</span> <span class="p_context"> struct page *follow_trans_huge_pmd(struct vm_area_struct *vma,</span>
 					  pmd, _pmd,  1))
 			update_mmu_cache_pmd(vma, addr, pmd);
 	}
<span class="p_del">-	if ((flags &amp; FOLL_POPULATE) &amp;&amp; (vma-&gt;vm_flags &amp; VM_LOCKED)) {</span>
<span class="p_add">+	if ((flags &amp; FOLL_POPULATE) &amp;&amp;</span>
<span class="p_add">+	    (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT))) {</span>
 		if (page-&gt;mapping &amp;&amp; trylock_page(page)) {
 			lru_add_drain();
 			if (page-&gt;mapping)
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index a8c3087..82caa48 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -3764,8 +3764,8 @@</span> <span class="p_context"> static unsigned long page_table_shareable(struct vm_area_struct *svma,</span>
 	unsigned long s_end = sbase + PUD_SIZE;
 
 	/* Allow segments to share if only one is marked locked */
<span class="p_del">-	unsigned long vm_flags = vma-&gt;vm_flags &amp; ~VM_LOCKED;</span>
<span class="p_del">-	unsigned long svm_flags = svma-&gt;vm_flags &amp; ~VM_LOCKED;</span>
<span class="p_add">+	unsigned long vm_flags = vma-&gt;vm_flags &amp; ~(VM_LOCKED | VM_LOCKONFAULT);</span>
<span class="p_add">+	unsigned long svm_flags = svma-&gt;vm_flags &amp; ~(VM_LOCKED | VM_LOCKONFAULT);</span>
 
 	/*
 	 * match the virtual addresses, permission and the alignment of the
<span class="p_header">diff --git a/mm/internal.h b/mm/internal.h</span>
<span class="p_header">index 36b23f1..53e140e 100644</span>
<span class="p_header">--- a/mm/internal.h</span>
<span class="p_header">+++ b/mm/internal.h</span>
<span class="p_chunk">@@ -246,10 +246,11 @@</span> <span class="p_context"> void __vma_link_list(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 extern long populate_vma_page_range(struct vm_area_struct *vma,
 		unsigned long start, unsigned long end, int *nonblocking);
 extern void munlock_vma_pages_range(struct vm_area_struct *vma,
<span class="p_del">-			unsigned long start, unsigned long end);</span>
<span class="p_add">+			unsigned long start, unsigned long end, vm_flags_t to_drop);</span>
 static inline void munlock_vma_pages_all(struct vm_area_struct *vma)
 {
<span class="p_del">-	munlock_vma_pages_range(vma, vma-&gt;vm_start, vma-&gt;vm_end);</span>
<span class="p_add">+	munlock_vma_pages_range(vma, vma-&gt;vm_start, vma-&gt;vm_end,</span>
<span class="p_add">+				VM_LOCKED | VM_LOCKONFAULT);</span>
 }
 
 /*
<span class="p_header">diff --git a/mm/ksm.c b/mm/ksm.c</span>
<span class="p_header">index 7ee101e..5d91b7d 100644</span>
<span class="p_header">--- a/mm/ksm.c</span>
<span class="p_header">+++ b/mm/ksm.c</span>
<span class="p_chunk">@@ -1058,7 +1058,7 @@</span> <span class="p_context"> static int try_to_merge_one_page(struct vm_area_struct *vma,</span>
 			err = replace_page(vma, page, kpage, orig_pte);
 	}
 
<span class="p_del">-	if ((vma-&gt;vm_flags &amp; VM_LOCKED) &amp;&amp; kpage &amp;&amp; !err) {</span>
<span class="p_add">+	if ((vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) &amp;&amp; kpage &amp;&amp; !err) {</span>
 		munlock_vma_page(page);
 		if (!PageMlocked(kpage)) {
 			unlock_page(page);
<span class="p_header">diff --git a/mm/madvise.c b/mm/madvise.c</span>
<span class="p_header">index 64bb8a2..c9d9296 100644</span>
<span class="p_header">--- a/mm/madvise.c</span>
<span class="p_header">+++ b/mm/madvise.c</span>
<span class="p_chunk">@@ -279,7 +279,7 @@</span> <span class="p_context"> static long madvise_dontneed(struct vm_area_struct *vma,</span>
 			     unsigned long start, unsigned long end)
 {
 	*prev = vma;
<span class="p_del">-	if (vma-&gt;vm_flags &amp; (VM_LOCKED|VM_HUGETLB|VM_PFNMAP))</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; (VM_LOCKED|VM_LOCKONFAULT|VM_HUGETLB|VM_PFNMAP))</span>
 		return -EINVAL;
 
 	zap_page_range(vma, start, end - start, NULL);
<span class="p_chunk">@@ -300,7 +300,7 @@</span> <span class="p_context"> static long madvise_remove(struct vm_area_struct *vma,</span>
 
 	*prev = NULL;	/* tell sys_madvise we drop mmap_sem */
 
<span class="p_del">-	if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_HUGETLB))</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT | VM_HUGETLB))</span>
 		return -EINVAL;
 
 	f = vma-&gt;vm_file;
<span class="p_header">diff --git a/mm/memory.c b/mm/memory.c</span>
<span class="p_header">index 388dcf9..2b19e0b 100644</span>
<span class="p_header">--- a/mm/memory.c</span>
<span class="p_header">+++ b/mm/memory.c</span>
<span class="p_chunk">@@ -2165,7 +2165,7 @@</span> <span class="p_context"> static int wp_page_copy(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 		 * Don&#39;t let another task, with possibly unlocked vma,
 		 * keep the mlocked page.
 		 */
<span class="p_del">-		if (page_copied &amp;&amp; (vma-&gt;vm_flags &amp; VM_LOCKED)) {</span>
<span class="p_add">+		if (page_copied &amp;&amp; (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT))) {</span>
 			lock_page(old_page);	/* LRU manipulation */
 			munlock_vma_page(old_page);
 			unlock_page(old_page);
<span class="p_chunk">@@ -2577,7 +2577,8 @@</span> <span class="p_context"> static int do_swap_page(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 	}
 
 	swap_free(entry);
<span class="p_del">-	if (vm_swap_full() || (vma-&gt;vm_flags &amp; VM_LOCKED) || PageMlocked(page))</span>
<span class="p_add">+	if (vm_swap_full() || (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) ||</span>
<span class="p_add">+	    PageMlocked(page))</span>
 		try_to_free_swap(page);
 	unlock_page(page);
 	if (page != swapcache) {
<span class="p_header">diff --git a/mm/mlock.c b/mm/mlock.c</span>
<span class="p_header">index d6e61d6..8b45be1 100644</span>
<span class="p_header">--- a/mm/mlock.c</span>
<span class="p_header">+++ b/mm/mlock.c</span>
<span class="p_chunk">@@ -406,23 +406,22 @@</span> <span class="p_context"> static unsigned long __munlock_pagevec_fill(struct pagevec *pvec,</span>
  * @vma - vma containing range to be munlock()ed.
  * @start - start address in @vma of the range
  * @end - end of range in @vma.
<span class="p_add">+ * @to_drop - the VMA flags we want to drop from the specified range</span>
  *
<span class="p_del">- *  For mremap(), munmap() and exit().</span>
<span class="p_add">+ *  For mremap(), munmap(), munlock(), and exit().</span>
  *
<span class="p_del">- * Called with @vma VM_LOCKED.</span>
<span class="p_del">- *</span>
<span class="p_del">- * Returns with VM_LOCKED cleared.  Callers must be prepared to</span>
<span class="p_add">+ * Returns with specified flags cleared.  Callers must be prepared to</span>
  * deal with this.
  *
<span class="p_del">- * We don&#39;t save and restore VM_LOCKED here because pages are</span>
<span class="p_add">+ * We don&#39;t save and restore specified flags here because pages are</span>
  * still on lru.  In unmap path, pages might be scanned by reclaim
  * and re-mlocked by try_to_{munlock|unmap} before we unmap and
  * free them.  This will result in freeing mlocked pages.
  */
<span class="p_del">-void munlock_vma_pages_range(struct vm_area_struct *vma,</span>
<span class="p_del">-			     unsigned long start, unsigned long end)</span>
<span class="p_add">+void munlock_vma_pages_range(struct vm_area_struct *vma, unsigned long start,</span>
<span class="p_add">+			     unsigned long end, vm_flags_t to_drop)</span>
 {
<span class="p_del">-	vma-&gt;vm_flags &amp;= ~VM_LOCKED;</span>
<span class="p_add">+	vma-&gt;vm_flags &amp;= ~to_drop;</span>
 
 	while (start &lt; end) {
 		struct page *page = NULL;
<span class="p_chunk">@@ -502,11 +501,12 @@</span> <span class="p_context"> static int mlock_fixup(struct vm_area_struct *vma, struct vm_area_struct **prev,</span>
 	pgoff_t pgoff;
 	int nr_pages;
 	int ret = 0;
<span class="p_del">-	int lock = !!(newflags &amp; VM_LOCKED);</span>
<span class="p_add">+	int lock = !!(newflags &amp; (VM_LOCKED | VM_LOCKONFAULT));</span>
 
 	if (newflags == vma-&gt;vm_flags || (vma-&gt;vm_flags &amp; VM_SPECIAL) ||
 	    is_vm_hugetlb_page(vma) || vma == get_gate_vma(current-&gt;mm))
<span class="p_del">-		goto out;	/* don&#39;t set VM_LOCKED,  don&#39;t count */</span>
<span class="p_add">+		/* don&#39;t set VM_LOCKED or VM_LOCKONFAULT and don&#39;t count */</span>
<span class="p_add">+		goto out;</span>
 
 	pgoff = vma-&gt;vm_pgoff + ((start - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT);
 	*prev = vma_merge(mm, *prev, start, end, newflags, vma-&gt;anon_vma,
<span class="p_chunk">@@ -546,7 +546,11 @@</span> <span class="p_context"> success:</span>
 	if (lock)
 		vma-&gt;vm_flags = newflags;
 	else
<span class="p_del">-		munlock_vma_pages_range(vma, start, end);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * We need to tell which VM_LOCK* flag(s) we are clearing here</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		munlock_vma_pages_range(vma, start, end,</span>
<span class="p_add">+					(vma-&gt;vm_flags &amp; ~(newflags)));</span>
 
 out:
 	*prev = vma;
<span class="p_chunk">@@ -581,10 +585,12 @@</span> <span class="p_context"> static int apply_vma_flags(unsigned long start, size_t len,</span>
 		/* Here we know that  vma-&gt;vm_start &lt;= nstart &lt; vma-&gt;vm_end. */
 
 		newflags = vma-&gt;vm_flags;
<span class="p_del">-		if (add_flags)</span>
<span class="p_add">+		if (add_flags) {</span>
<span class="p_add">+			newflags &amp;= ~(VM_LOCKED | VM_LOCKONFAULT);</span>
 			newflags |= flags;
<span class="p_del">-		else</span>
<span class="p_add">+		} else {</span>
 			newflags &amp;= ~flags;
<span class="p_add">+		}</span>
 
 		tmp = vma-&gt;vm_end;
 		if (tmp &gt; end)
<span class="p_chunk">@@ -637,9 +643,15 @@</span> <span class="p_context"> static int do_mlock(unsigned long start, size_t len, vm_flags_t flags)</span>
 	if (error)
 		return error;
 
<span class="p_del">-	error = __mm_populate(start, len, 0);</span>
<span class="p_del">-	if (error)</span>
<span class="p_del">-		return __mlock_posix_error_return(error);</span>
<span class="p_add">+	if (flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
<span class="p_add">+		if (flags &amp; VM_LOCKED)</span>
<span class="p_add">+			error = __mm_populate(start, len, 0);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			error = mm_lock_present(start, len);</span>
<span class="p_add">+		if (error)</span>
<span class="p_add">+			return __mlock_posix_error_return(error);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	return 0;
 }
 
<span class="p_chunk">@@ -650,10 +662,14 @@</span> <span class="p_context"> SYSCALL_DEFINE2(mlock, unsigned long, start, size_t, len)</span>
 
 SYSCALL_DEFINE3(mlock2, unsigned long, start, size_t, len, int, flags)
 {
<span class="p_del">-	if (!flags || flags &amp; ~MLOCK_LOCKED)</span>
<span class="p_add">+	if (!flags || (flags &amp; ~(MLOCK_LOCKED | MLOCK_ONFAULT)) ||</span>
<span class="p_add">+	    flags == (MLOCK_LOCKED | MLOCK_ONFAULT))</span>
 		return -EINVAL;
 
<span class="p_del">-	return do_mlock(start, len, VM_LOCKED);</span>
<span class="p_add">+	if (flags &amp; MLOCK_LOCKED)</span>
<span class="p_add">+		return do_mlock(start, len, VM_LOCKED);</span>
<span class="p_add">+</span>
<span class="p_add">+	return do_mlock(start, len, VM_LOCKONFAULT);</span>
 }
 
 static int do_munlock(unsigned long start, size_t len, vm_flags_t flags)
<span class="p_chunk">@@ -672,31 +688,46 @@</span> <span class="p_context"> static int do_munlock(unsigned long start, size_t len, vm_flags_t flags)</span>
 
 SYSCALL_DEFINE2(munlock, unsigned long, start, size_t, len)
 {
<span class="p_del">-	return do_munlock(start, len, VM_LOCKED);</span>
<span class="p_add">+	return do_munlock(start, len, VM_LOCKED | VM_LOCKONFAULT);</span>
 }
 
 SYSCALL_DEFINE3(munlock2, unsigned long, start, size_t, len, int, flags)
 {
<span class="p_del">-	if (!flags || flags &amp; ~MLOCK_LOCKED)</span>
<span class="p_add">+	vm_flags_t to_clear = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!flags || flags &amp; ~(MLOCK_LOCKED | MLOCK_ONFAULT))</span>
 		return -EINVAL;
<span class="p_del">-	return do_munlock(start, len, VM_LOCKED);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (flags &amp; MLOCK_LOCKED)</span>
<span class="p_add">+		to_clear |= VM_LOCKED;</span>
<span class="p_add">+	if (flags &amp; MLOCK_ONFAULT)</span>
<span class="p_add">+		to_clear |= VM_LOCKONFAULT;</span>
<span class="p_add">+</span>
<span class="p_add">+	return do_munlock(start, len, to_clear);</span>
 }
 
 static int do_mlockall(int flags)
 {
 	struct vm_area_struct * vma, * prev = NULL;
<span class="p_add">+	vm_flags_t to_add;</span>
 
 	if (flags &amp; MCL_FUTURE)
 		current-&gt;mm-&gt;def_flags |= VM_LOCKED;
 	if (flags == MCL_FUTURE)
 		goto out;
 
<span class="p_add">+	if (flags &amp; MCL_ONFAULT) {</span>
<span class="p_add">+		current-&gt;mm-&gt;def_flags |= VM_LOCKONFAULT;</span>
<span class="p_add">+		to_add = VM_LOCKONFAULT;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		to_add = VM_LOCKED;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	for (vma = current-&gt;mm-&gt;mmap; vma ; vma = prev-&gt;vm_next) {
 		vm_flags_t newflags;
 
<span class="p_del">-		newflags = vma-&gt;vm_flags &amp; ~VM_LOCKED;</span>
<span class="p_del">-		if (flags &amp; MCL_CURRENT)</span>
<span class="p_del">-			newflags |= VM_LOCKED;</span>
<span class="p_add">+		newflags = vma-&gt;vm_flags &amp; ~(VM_LOCKED | VM_LOCKONFAULT);</span>
<span class="p_add">+		newflags |= to_add;</span>
 
 		/* Ignore errors */
 		mlock_fixup(vma, &amp;prev, vma-&gt;vm_start, vma-&gt;vm_end, newflags);
<span class="p_chunk">@@ -711,7 +742,8 @@</span> <span class="p_context"> SYSCALL_DEFINE1(mlockall, int, flags)</span>
 	unsigned long lock_limit;
 	int ret = -EINVAL;
 
<span class="p_del">-	if (!flags || (flags &amp; ~(MCL_CURRENT | MCL_FUTURE)))</span>
<span class="p_add">+	if (!flags || (flags &amp; ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT)) ||</span>
<span class="p_add">+	    (flags &amp; (MCL_FUTURE | MCL_ONFAULT)) == (MCL_FUTURE | MCL_ONFAULT))</span>
 		goto out;
 
 	ret = -EPERM;
<span class="p_chunk">@@ -740,18 +772,24 @@</span> <span class="p_context"> out:</span>
 static int do_munlockall(int flags)
 {
 	struct vm_area_struct * vma, * prev = NULL;
<span class="p_add">+	vm_flags_t to_clear = 0;</span>
 
 	if (flags &amp; MCL_FUTURE)
 		current-&gt;mm-&gt;def_flags &amp;= ~VM_LOCKED;
<span class="p_add">+	if (flags &amp; MCL_ONFAULT)</span>
<span class="p_add">+		current-&gt;mm-&gt;def_flags &amp;= ~VM_LOCKONFAULT;</span>
 	if (flags == MCL_FUTURE)
 		goto out;
 
<span class="p_add">+	if (flags &amp; MCL_CURRENT)</span>
<span class="p_add">+		to_clear |= VM_LOCKED;</span>
<span class="p_add">+	if (flags &amp; MCL_ONFAULT)</span>
<span class="p_add">+		to_clear |= VM_LOCKONFAULT;</span>
<span class="p_add">+</span>
 	for (vma = current-&gt;mm-&gt;mmap; vma ; vma = prev-&gt;vm_next) {
 		vm_flags_t newflags;
 
<span class="p_del">-		newflags = vma-&gt;vm_flags;</span>
<span class="p_del">-		if (flags &amp; MCL_CURRENT)</span>
<span class="p_del">-			newflags &amp;= ~VM_LOCKED;</span>
<span class="p_add">+		newflags = vma-&gt;vm_flags &amp; ~to_clear;</span>
 
 		/* Ignore errors */
 		mlock_fixup(vma, &amp;prev, vma-&gt;vm_start, vma-&gt;vm_end, newflags);
<span class="p_chunk">@@ -766,7 +804,7 @@</span> <span class="p_context"> SYSCALL_DEFINE0(munlockall)</span>
 	int ret;
 
 	down_write(&amp;current-&gt;mm-&gt;mmap_sem);
<span class="p_del">-	ret = do_munlockall(MCL_CURRENT | MCL_FUTURE);</span>
<span class="p_add">+	ret = do_munlockall(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT);</span>
 	up_write(&amp;current-&gt;mm-&gt;mmap_sem);
 	return ret;
 }
<span class="p_chunk">@@ -775,7 +813,7 @@</span> <span class="p_context"> SYSCALL_DEFINE1(munlockall2, int, flags)</span>
 {
 	int ret = -EINVAL;
 
<span class="p_del">-	if (!flags || flags &amp; ~(MCL_CURRENT | MCL_FUTURE))</span>
<span class="p_add">+	if (!flags || flags &amp; ~(MCL_CURRENT | MCL_FUTURE | MCL_ONFAULT))</span>
 		return ret;
 
 	down_write(&amp;current-&gt;mm-&gt;mmap_sem);
<span class="p_header">diff --git a/mm/mmap.c b/mm/mmap.c</span>
<span class="p_header">index aa632ad..de89be4 100644</span>
<span class="p_header">--- a/mm/mmap.c</span>
<span class="p_header">+++ b/mm/mmap.c</span>
<span class="p_chunk">@@ -1232,8 +1232,8 @@</span> <span class="p_context"> static inline int mlock_future_check(struct mm_struct *mm,</span>
 {
 	unsigned long locked, lock_limit;
 
<span class="p_del">-	/*  mlock MCL_FUTURE? */</span>
<span class="p_del">-	if (flags &amp; VM_LOCKED) {</span>
<span class="p_add">+	/*  mlock MCL_FUTURE or MCL_ONFAULT? */</span>
<span class="p_add">+	if (flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 		locked = len &gt;&gt; PAGE_SHIFT;
 		locked += mm-&gt;locked_vm;
 		lock_limit = rlimit(RLIMIT_MEMLOCK);
<span class="p_chunk">@@ -1646,12 +1646,12 @@</span> <span class="p_context"> out:</span>
 	perf_event_mmap(vma);
 
 	vm_stat_account(mm, vm_flags, file, len &gt;&gt; PAGE_SHIFT);
<span class="p_del">-	if (vm_flags &amp; VM_LOCKED) {</span>
<span class="p_add">+	if (vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 		if (!((vm_flags &amp; VM_SPECIAL) || is_vm_hugetlb_page(vma) ||
 					vma == get_gate_vma(current-&gt;mm)))
 			mm-&gt;locked_vm += (len &gt;&gt; PAGE_SHIFT);
 		else
<span class="p_del">-			vma-&gt;vm_flags &amp;= ~VM_LOCKED;</span>
<span class="p_add">+			vma-&gt;vm_flags &amp;= ~(VM_LOCKED | VM_LOCKONFAULT);</span>
 	}
 
 	if (file)
<span class="p_chunk">@@ -2104,7 +2104,7 @@</span> <span class="p_context"> static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, uns</span>
 		return -ENOMEM;
 
 	/* mlock limit tests */
<span class="p_del">-	if (vma-&gt;vm_flags &amp; VM_LOCKED) {</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 		unsigned long locked;
 		unsigned long limit;
 		locked = mm-&gt;locked_vm + grow;
<span class="p_chunk">@@ -2128,7 +2128,7 @@</span> <span class="p_context"> static int acct_stack_growth(struct vm_area_struct *vma, unsigned long size, uns</span>
 		return -ENOMEM;
 
 	/* Ok, everything looks good - let it rip */
<span class="p_del">-	if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT))</span>
 		mm-&gt;locked_vm += grow;
 	vm_stat_account(mm, vma-&gt;vm_flags, vma-&gt;vm_file, grow);
 	return 0;
<span class="p_chunk">@@ -2583,7 +2583,7 @@</span> <span class="p_context"> int do_munmap(struct mm_struct *mm, unsigned long start, size_t len)</span>
 	if (mm-&gt;locked_vm) {
 		struct vm_area_struct *tmp = vma;
 		while (tmp &amp;&amp; tmp-&gt;vm_start &lt; end) {
<span class="p_del">-			if (tmp-&gt;vm_flags &amp; VM_LOCKED) {</span>
<span class="p_add">+			if (tmp-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 				mm-&gt;locked_vm -= vma_pages(tmp);
 				munlock_vma_pages_all(tmp);
 			}
<span class="p_chunk">@@ -2636,6 +2636,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 	unsigned long populate = 0;
 	unsigned long ret = -EINVAL;
 	struct file *file;
<span class="p_add">+	vm_flags_t drop_lock_flag = 0;</span>
 
 	pr_warn_once(&quot;%s (%d) uses deprecated remap_file_pages() syscall. &quot;
 			&quot;See Documentation/vm/remap_file_pages.txt.\n&quot;,
<span class="p_chunk">@@ -2675,10 +2676,15 @@</span> <span class="p_context"> SYSCALL_DEFINE5(remap_file_pages, unsigned long, start, unsigned long, size,</span>
 	flags |= MAP_SHARED | MAP_FIXED | MAP_POPULATE;
 	if (vma-&gt;vm_flags &amp; VM_LOCKED) {
 		flags |= MAP_LOCKED;
<span class="p_del">-		/* drop PG_Mlocked flag for over-mapped range */</span>
<span class="p_del">-		munlock_vma_pages_range(vma, start, start + size);</span>
<span class="p_add">+		drop_lock_flag = VM_LOCKED;</span>
<span class="p_add">+	} else if (vma-&gt;vm_flags &amp; VM_LOCKONFAULT) {</span>
<span class="p_add">+		drop_lock_flag = VM_LOCKONFAULT;</span>
 	}
 
<span class="p_add">+	if (drop_lock_flag)</span>
<span class="p_add">+		/* drop PG_Mlocked flag for over-mapped range */</span>
<span class="p_add">+		munlock_vma_pages_range(vma, start, start + size, VM_LOCKED);</span>
<span class="p_add">+</span>
 	file = get_file(vma-&gt;vm_file);
 	ret = do_mmap_pgoff(vma-&gt;vm_file, start, size,
 			prot, flags, pgoff, &amp;populate);
<span class="p_chunk">@@ -2781,7 +2787,7 @@</span> <span class="p_context"> static unsigned long do_brk(unsigned long addr, unsigned long len)</span>
 out:
 	perf_event_mmap(vma);
 	mm-&gt;total_vm += len &gt;&gt; PAGE_SHIFT;
<span class="p_del">-	if (flags &amp; VM_LOCKED)</span>
<span class="p_add">+	if (flags &amp; (VM_LOCKED | VM_LOCKONFAULT))</span>
 		mm-&gt;locked_vm += (len &gt;&gt; PAGE_SHIFT);
 	vma-&gt;vm_flags |= VM_SOFTDIRTY;
 	return addr;
<span class="p_chunk">@@ -2816,7 +2822,7 @@</span> <span class="p_context"> void exit_mmap(struct mm_struct *mm)</span>
 	if (mm-&gt;locked_vm) {
 		vma = mm-&gt;mmap;
 		while (vma) {
<span class="p_del">-			if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="p_add">+			if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT))</span>
 				munlock_vma_pages_all(vma);
 			vma = vma-&gt;vm_next;
 		}
<span class="p_header">diff --git a/mm/mremap.c b/mm/mremap.c</span>
<span class="p_header">index a7c93ec..44d4c44 100644</span>
<span class="p_header">--- a/mm/mremap.c</span>
<span class="p_header">+++ b/mm/mremap.c</span>
<span class="p_chunk">@@ -335,7 +335,7 @@</span> <span class="p_context"> static unsigned long move_vma(struct vm_area_struct *vma,</span>
 			vma-&gt;vm_next-&gt;vm_flags |= VM_ACCOUNT;
 	}
 
<span class="p_del">-	if (vm_flags &amp; VM_LOCKED) {</span>
<span class="p_add">+	if (vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 		mm-&gt;locked_vm += new_len &gt;&gt; PAGE_SHIFT;
 		*locked = true;
 	}
<span class="p_chunk">@@ -371,7 +371,7 @@</span> <span class="p_context"> static struct vm_area_struct *vma_to_resize(unsigned long addr,</span>
 			return ERR_PTR(-EINVAL);
 	}
 
<span class="p_del">-	if (vma-&gt;vm_flags &amp; VM_LOCKED) {</span>
<span class="p_add">+	if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 		unsigned long locked, lock_limit;
 		locked = mm-&gt;locked_vm &lt;&lt; PAGE_SHIFT;
 		lock_limit = rlimit(RLIMIT_MEMLOCK);
<span class="p_chunk">@@ -548,7 +548,7 @@</span> <span class="p_context"> SYSCALL_DEFINE5(mremap, unsigned long, addr, unsigned long, old_len,</span>
 			}
 
 			vm_stat_account(mm, vma-&gt;vm_flags, vma-&gt;vm_file, pages);
<span class="p_del">-			if (vma-&gt;vm_flags &amp; VM_LOCKED) {</span>
<span class="p_add">+			if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 				mm-&gt;locked_vm += pages;
 				locked = true;
 				new_addr = addr;
<span class="p_header">diff --git a/mm/msync.c b/mm/msync.c</span>
<span class="p_header">index bb04d53..1183183 100644</span>
<span class="p_header">--- a/mm/msync.c</span>
<span class="p_header">+++ b/mm/msync.c</span>
<span class="p_chunk">@@ -73,7 +73,7 @@</span> <span class="p_context"> SYSCALL_DEFINE3(msync, unsigned long, start, size_t, len, int, flags)</span>
 		}
 		/* Here vma-&gt;vm_start &lt;= start &lt; vma-&gt;vm_end. */
 		if ((flags &amp; MS_INVALIDATE) &amp;&amp;
<span class="p_del">-				(vma-&gt;vm_flags &amp; VM_LOCKED)) {</span>
<span class="p_add">+				(vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT))) {</span>
 			error = -EBUSY;
 			goto out_unlock;
 		}
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 171b687..3e91372 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -742,9 +742,9 @@</span> <span class="p_context"> static int page_referenced_one(struct page *page, struct vm_area_struct *vma,</span>
 		if (!pmd)
 			return SWAP_AGAIN;
 
<span class="p_del">-		if (vma-&gt;vm_flags &amp; VM_LOCKED) {</span>
<span class="p_add">+		if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 			spin_unlock(ptl);
<span class="p_del">-			pra-&gt;vm_flags |= VM_LOCKED;</span>
<span class="p_add">+			pra-&gt;vm_flags |= (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT));</span>
 			return SWAP_FAIL; /* To break the loop */
 		}
 
<span class="p_chunk">@@ -763,9 +763,9 @@</span> <span class="p_context"> static int page_referenced_one(struct page *page, struct vm_area_struct *vma,</span>
 		if (!pte)
 			return SWAP_AGAIN;
 
<span class="p_del">-		if (vma-&gt;vm_flags &amp; VM_LOCKED) {</span>
<span class="p_add">+		if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 			pte_unmap_unlock(pte, ptl);
<span class="p_del">-			pra-&gt;vm_flags |= VM_LOCKED;</span>
<span class="p_add">+			pra-&gt;vm_flags |= (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT));</span>
 			return SWAP_FAIL; /* To break the loop */
 		}
 
<span class="p_chunk">@@ -1205,7 +1205,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	 * skipped over this mm) then we should reactivate it.
 	 */
 	if (!(flags &amp; TTU_IGNORE_MLOCK)) {
<span class="p_del">-		if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="p_add">+		if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT))</span>
 			goto out_mlock;
 
 		if (flags &amp; TTU_MUNLOCK)
<span class="p_chunk">@@ -1315,7 +1315,7 @@</span> <span class="p_context"> out_mlock:</span>
 	 * page is actually mlocked.
 	 */
 	if (down_read_trylock(&amp;vma-&gt;vm_mm-&gt;mmap_sem)) {
<span class="p_del">-		if (vma-&gt;vm_flags &amp; VM_LOCKED) {</span>
<span class="p_add">+		if (vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) {</span>
 			mlock_vma_page(page);
 			ret = SWAP_MLOCK;
 		}
<span class="p_header">diff --git a/mm/shmem.c b/mm/shmem.c</span>
<span class="p_header">index 4caf8ed..9ddf2ca 100644</span>
<span class="p_header">--- a/mm/shmem.c</span>
<span class="p_header">+++ b/mm/shmem.c</span>
<span class="p_chunk">@@ -754,7 +754,7 @@</span> <span class="p_context"> static int shmem_writepage(struct page *page, struct writeback_control *wbc)</span>
 	index = page-&gt;index;
 	inode = mapping-&gt;host;
 	info = SHMEM_I(inode);
<span class="p_del">-	if (info-&gt;flags &amp; VM_LOCKED)</span>
<span class="p_add">+	if (info-&gt;flags &amp; (VM_LOCKED | VM_LOCKONFAULT))</span>
 		goto redirty;
 	if (!total_swap_pages)
 		goto redirty;
<span class="p_header">diff --git a/mm/swap.c b/mm/swap.c</span>
<span class="p_header">index a3a0a2f..3580a21 100644</span>
<span class="p_header">--- a/mm/swap.c</span>
<span class="p_header">+++ b/mm/swap.c</span>
<span class="p_chunk">@@ -710,7 +710,8 @@</span> <span class="p_context"> void lru_cache_add_active_or_unevictable(struct page *page,</span>
 {
 	VM_BUG_ON_PAGE(PageLRU(page), page);
 
<span class="p_del">-	if (likely((vma-&gt;vm_flags &amp; (VM_LOCKED | VM_SPECIAL)) != VM_LOCKED)) {</span>
<span class="p_add">+	if (likely((vma-&gt;vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT)) == 0) ||</span>
<span class="p_add">+		   (vma-&gt;vm_flags &amp; VM_SPECIAL)) {</span>
 		SetPageActive(page);
 		lru_cache_add(page);
 		return;
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index e61445d..019d306 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -804,7 +804,7 @@</span> <span class="p_context"> static enum page_references page_check_references(struct page *page,</span>
 	 * Mlock lost the isolation race with us.  Let try_to_unmap()
 	 * move the page to the unevictable list.
 	 */
<span class="p_del">-	if (vm_flags &amp; VM_LOCKED)</span>
<span class="p_add">+	if (vm_flags &amp; (VM_LOCKED | VM_LOCKONFAULT))</span>
 		return PAGEREF_RECLAIM;
 
 	if (referenced_ptes) {

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



