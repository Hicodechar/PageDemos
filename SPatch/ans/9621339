
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v4,05/11] mm: thp: enable thp migration in generic path - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v4,05/11] mm: thp: enable thp migration in generic path</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=168825">Zi Yan</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>March 13, 2017, 3:45 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20170313154507.3647-6-zi.yan@sent.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9621339/mbox/"
   >mbox</a>
|
   <a href="/patch/9621339/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9621339/">/patch/9621339/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	552DD60414 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Mar 2017 15:47:51 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4418D262FF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Mar 2017 15:47:51 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 34D3927F8C; Mon, 13 Mar 2017 15:47:51 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU, FREEMAIL_FROM,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 0C0EC262FF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Mar 2017 15:47:50 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753755AbdCMPrt (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 13 Mar 2017 11:47:49 -0400
Received: from out1-smtp.messagingengine.com ([66.111.4.25]:33679 &quot;EHLO
	out1-smtp.messagingengine.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S1753312AbdCMPqe (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 13 Mar 2017 11:46:34 -0400
Received: from compute3.internal (compute3.nyi.internal [10.202.2.43])
	by mailout.nyi.internal (Postfix) with ESMTP id D9285209E5;
	Mon, 13 Mar 2017 11:46:16 -0400 (EDT)
Received: from frontend1 ([10.202.2.160])
	by compute3.internal (MEProxy); Mon, 13 Mar 2017 11:46:16 -0400
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=sent.com; h=cc
	:date:from:in-reply-to:message-id:references:subject:to
	:x-me-sender:x-me-sender:x-sasl-enc:x-sasl-enc; s=mesmtp; bh=B27
	GpD7Gj7v7F3uxr4Nn7y8mTkM=; b=kTmAlyA9L4U4Oxm/R6PLv4sC5pYAf8eGHNK
	P9cjdhwj+DvOpUkN1QWisTab2lGO7VUheBHDwkzWbGyMdVf8jjdU6PQP9SM3vu3w
	Qwbw1RY3NiCb78m16cmcOxVjiD6T3/XwyOPnBCM/ME3QhAx6pZvoMJEx7irfurMJ
	NTUKOgjU=
DKIM-Signature: v=1; a=rsa-sha1; c=relaxed/relaxed; d=
	messagingengine.com; h=cc:date:from:in-reply-to:message-id
	:references:subject:to:x-me-sender:x-me-sender:x-sasl-enc
	:x-sasl-enc; s=smtpout; bh=B27GpD7Gj7v7F3uxr4Nn7y8mTkM=; b=mEKuA
	r1V5l6Ly0cos38t9nZB4mhAPsRLXDRQplCwaGFi6ab/HgkwzrjBENbh6ZBzAbo1H
	TKqgiWsqEC77JD3orMgzWEtXbH/4nclTpYPHDr5EZJ/8PBNlvVhrP7o5mhzdV0Le
	wQU2271Ru1HC+LmRegzvg3EnaZAVwDXCy5Mu+M=
X-ME-Sender: &lt;xms:yL7GWIgtt2o6h1eKQJBs7AttNcudWjadHAIcjrB33WvwBoL100Q2qA&gt;
X-Sasl-enc: kffQISBzJ7DMA27WXZhM1c6M8CcdCGDQn9jXfYZ9YuQ3 1489419976
Received: from tenansix.rutgers.edu (pool-165-230-225-59.nat.rutgers.edu
	[165.230.225.59])
	by mail.messagingengine.com (Postfix) with ESMTPA id 76E1C7E41F;
	Mon, 13 Mar 2017 11:46:16 -0400 (EDT)
From: Zi Yan &lt;zi.yan@sent.com&gt;
To: linux-kernel@vger.kernel.org, linux-mm@kvack.org
Cc: kirill.shutemov@linux.intel.com, akpm@linux-foundation.org,
	minchan@kernel.org, vbabka@suse.cz, mgorman@techsingularity.net,
	mhocko@kernel.org, n-horiguchi@ah.jp.nec.com,
	khandual@linux.vnet.ibm.com, zi.yan@cs.rutgers.edu, dnellans@nvidia.com
Subject: [PATCH v4 05/11] mm: thp: enable thp migration in generic path
Date: Mon, 13 Mar 2017 11:45:01 -0400
Message-Id: &lt;20170313154507.3647-6-zi.yan@sent.com&gt;
X-Mailer: git-send-email 2.11.0
In-Reply-To: &lt;20170313154507.3647-1-zi.yan@sent.com&gt;
References: &lt;20170313154507.3647-1-zi.yan@sent.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=168825">Zi Yan</a> - March 13, 2017, 3:45 p.m.</div>
<pre class="content">
<span class="from">From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>

This patch adds thp migration&#39;s core code, including conversions
between a PMD entry and a swap entry, setting PMD migration entry,
removing PMD migration entry, and waiting on PMD migration entries.

This patch makes it possible to support thp migration.
If you fail to allocate a destination page as a thp, you just split
the source thp as we do now, and then enter the normal page migration.
If you succeed to allocate destination thp, you enter thp migration.
Subsequent patches actually enable thp migration for each caller of
page migration by allowing its get_new_page() callback to
allocate thps.

ChangeLog v1 -&gt; v2:
- support pte-mapped thp, doubly-mapped thp
<span class="signed-off-by">
Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>

ChangeLog v2 -&gt; v3:
- use page_vma_mapped_walk()

ChangeLog v3 -&gt; v4:
- factor out the code of removing pte pgtable page in zap_huge_pmd()
<span class="signed-off-by">
Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
---
 arch/x86/include/asm/pgtable_64.h |   2 +
 include/linux/swapops.h           |  70 +++++++++++++++++-
 mm/huge_memory.c                  | 147 ++++++++++++++++++++++++++++++++++----
 mm/migrate.c                      |  29 +++++++-
 mm/page_vma_mapped.c              |  13 +++-
 mm/pgtable-generic.c              |   3 +-
 mm/rmap.c                         |   9 +++
 7 files changed, 252 insertions(+), 21 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - March 14, 2017, 9:19 p.m.</div>
<pre class="content">
Hi Naoya,

[auto build test WARNING on mmotm/master]
[also build test WARNING on next-20170310]
[cannot apply to v4.11-rc2]
[if your patch is applied to the wrong git tree, please drop us a note to help improve the system]

url:    https://github.com/0day-ci/linux/commits/Zi-Yan/mm-page-migration-enhancement-for-thp/20170315-042736
base:   git://git.cmpxchg.org/linux-mmotm.git master
config: m68k-sun3_defconfig (attached as .config)
compiler: m68k-linux-gcc (GCC) 4.9.0
reproduce:
        wget https://raw.githubusercontent.com/01org/lkp-tests/master/sbin/make.cross -O ~/bin/make.cross
        chmod +x ~/bin/make.cross
        # save the attached .config to linux build tree
        make.cross ARCH=m68k 

All warnings (new ones prefixed by &gt;&gt;):

   In file included from fs/proc/task_mmu.c:15:0:
   include/linux/swapops.h: In function &#39;remove_migration_pmd&#39;:
   include/linux/swapops.h:209:2: warning: &#39;return&#39; with a value, in function returning void
     return 0;
     ^
   include/linux/swapops.h: In function &#39;swp_entry_to_pmd&#39;:
<span class="quote">&gt;&gt; include/linux/swapops.h:223:2: warning: missing braces around initializer [-Wmissing-braces]</span>
     return (pmd_t){ 0 };
     ^
   include/linux/swapops.h:223:2: warning: (near initialization for &#39;(anonymous).pmd&#39;) [-Wmissing-braces]

vim +223 include/linux/swapops.h

   203	}
   204	
   205	static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,
   206			struct page *new)
   207	{
   208		BUILD_BUG();
<span class="quote"> &gt; 209		return 0;</span>
   210	}
   211	
   212	static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }
   213	
   214	static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)
   215	{
   216		BUILD_BUG();
   217		return swp_entry(0, 0);
   218	}
   219	
   220	static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)
   221	{
   222		BUILD_BUG();
<span class="quote"> &gt; 223		return (pmd_t){ 0 };</span>
   224	}
   225	
   226	static inline int is_pmd_migration_entry(pmd_t pmd)

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=143191">kbuild test robot</a> - March 14, 2017, 9:26 p.m.</div>
<pre class="content">
Hi Naoya,

[auto build test ERROR on mmotm/master]
[also build test ERROR on next-20170310]
[cannot apply to v4.11-rc2]
[if your patch is applied to the wrong git tree, please drop us a note to help improve the system]

url:    https://github.com/0day-ci/linux/commits/Zi-Yan/mm-page-migration-enhancement-for-thp/20170315-042736
base:   git://git.cmpxchg.org/linux-mmotm.git master
config: i386-randconfig-s0-201711 (attached as .config)
compiler: gcc-6 (Debian 6.2.0-3) 6.2.0 20160901
reproduce:
        # save the attached .config to linux build tree
        make ARCH=i386 

All error/warnings (new ones prefixed by &gt;&gt;):

   In file included from fs/proc/task_mmu.c:15:0:
   include/linux/swapops.h: In function &#39;remove_migration_pmd&#39;:
<span class="quote">&gt;&gt; include/linux/swapops.h:209:9: warning: &#39;return&#39; with a value, in function returning void</span>
     return 0;
            ^
   include/linux/swapops.h:205:20: note: declared here
    static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,
                       ^~~~~~~~~~~~~~~~~~~~
--
   In file included from mm/page_vma_mapped.c:5:0:
   include/linux/swapops.h: In function &#39;remove_migration_pmd&#39;:
<span class="quote">&gt;&gt; include/linux/swapops.h:209:9: warning: &#39;return&#39; with a value, in function returning void</span>
     return 0;
            ^
   include/linux/swapops.h:205:20: note: declared here
    static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,
                       ^~~~~~~~~~~~~~~~~~~~
   In file included from include/asm-generic/bug.h:4:0,
                    from arch/x86/include/asm/bug.h:35,
                    from include/linux/bug.h:4,
                    from include/linux/mmdebug.h:4,
                    from include/linux/mm.h:8,
                    from mm/page_vma_mapped.c:1:
   In function &#39;pmd_to_swp_entry.isra.14&#39;,
       inlined from &#39;page_vma_mapped_walk&#39; at mm/page_vma_mapped.c:149:8:
<span class="quote">&gt;&gt; include/linux/compiler.h:537:38: error: call to &#39;__compiletime_assert_216&#39; declared with attribute error: BUILD_BUG failed</span>
     _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
                                         ^
   include/linux/compiler.h:520:4: note: in definition of macro &#39;__compiletime_assert&#39;
       prefix ## suffix();    \
       ^~~~~~
   include/linux/compiler.h:537:2: note: in expansion of macro &#39;_compiletime_assert&#39;
     _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
     ^~~~~~~~~~~~~~~~~~~
   include/linux/bug.h:54:37: note: in expansion of macro &#39;compiletime_assert&#39;
    #define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)
                                        ^~~~~~~~~~~~~~~~~~
   include/linux/bug.h:88:21: note: in expansion of macro &#39;BUILD_BUG_ON_MSG&#39;
    #define BUILD_BUG() BUILD_BUG_ON_MSG(1, &quot;BUILD_BUG failed&quot;)
                        ^~~~~~~~~~~~~~~~
<span class="quote">&gt;&gt; include/linux/swapops.h:216:2: note: in expansion of macro &#39;BUILD_BUG&#39;</span>
     BUILD_BUG();
     ^~~~~~~~~
--
   In file included from mm/rmap.c:53:0:
   include/linux/swapops.h: In function &#39;remove_migration_pmd&#39;:
<span class="quote">&gt;&gt; include/linux/swapops.h:209:9: warning: &#39;return&#39; with a value, in function returning void</span>
     return 0;
            ^
   include/linux/swapops.h:205:20: note: declared here
    static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,
                       ^~~~~~~~~~~~~~~~~~~~
   In file included from include/asm-generic/bug.h:4:0,
                    from arch/x86/include/asm/bug.h:35,
                    from include/linux/bug.h:4,
                    from include/linux/mmdebug.h:4,
                    from include/linux/mm.h:8,
                    from mm/rmap.c:48:
   In function &#39;set_pmd_migration_entry.isra.28&#39;,
       inlined from &#39;try_to_unmap_one&#39; at mm/rmap.c:1317:5:
   include/linux/compiler.h:537:38: error: call to &#39;__compiletime_assert_202&#39; declared with attribute error: BUILD_BUG failed
     _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
                                         ^
   include/linux/compiler.h:520:4: note: in definition of macro &#39;__compiletime_assert&#39;
       prefix ## suffix();    \
       ^~~~~~
   include/linux/compiler.h:537:2: note: in expansion of macro &#39;_compiletime_assert&#39;
     _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
     ^~~~~~~~~~~~~~~~~~~
   include/linux/bug.h:54:37: note: in expansion of macro &#39;compiletime_assert&#39;
    #define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)
                                        ^~~~~~~~~~~~~~~~~~
   include/linux/bug.h:88:21: note: in expansion of macro &#39;BUILD_BUG_ON_MSG&#39;
    #define BUILD_BUG() BUILD_BUG_ON_MSG(1, &quot;BUILD_BUG failed&quot;)
                        ^~~~~~~~~~~~~~~~
   include/linux/swapops.h:202:2: note: in expansion of macro &#39;BUILD_BUG&#39;
     BUILD_BUG();
     ^~~~~~~~~
--
   In file included from mm/migrate.c:18:0:
   include/linux/swapops.h: In function &#39;remove_migration_pmd&#39;:
<span class="quote">&gt;&gt; include/linux/swapops.h:209:9: warning: &#39;return&#39; with a value, in function returning void</span>
     return 0;
            ^
   include/linux/swapops.h:205:20: note: declared here
    static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,
                       ^~~~~~~~~~~~~~~~~~~~
   In file included from include/asm-generic/bug.h:4:0,
                    from arch/x86/include/asm/bug.h:35,
                    from include/linux/bug.h:4,
                    from include/linux/mmdebug.h:4,
                    from include/linux/mm.h:8,
                    from include/linux/migrate.h:4,
                    from mm/migrate.c:15:
   In function &#39;remove_migration_pmd.isra.32&#39;,
       inlined from &#39;remove_migration_pte&#39; at mm/migrate.c:217:4:
   include/linux/compiler.h:537:38: error: call to &#39;__compiletime_assert_208&#39; declared with attribute error: BUILD_BUG failed
     _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
                                         ^
   include/linux/compiler.h:520:4: note: in definition of macro &#39;__compiletime_assert&#39;
       prefix ## suffix();    \
       ^~~~~~
   include/linux/compiler.h:537:2: note: in expansion of macro &#39;_compiletime_assert&#39;
     _compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
     ^~~~~~~~~~~~~~~~~~~
   include/linux/bug.h:54:37: note: in expansion of macro &#39;compiletime_assert&#39;
    #define BUILD_BUG_ON_MSG(cond, msg) compiletime_assert(!(cond), msg)
                                        ^~~~~~~~~~~~~~~~~~
   include/linux/bug.h:88:21: note: in expansion of macro &#39;BUILD_BUG_ON_MSG&#39;
    #define BUILD_BUG() BUILD_BUG_ON_MSG(1, &quot;BUILD_BUG failed&quot;)
                        ^~~~~~~~~~~~~~~~
   include/linux/swapops.h:208:2: note: in expansion of macro &#39;BUILD_BUG&#39;
     BUILD_BUG();
     ^~~~~~~~~

vim +/__compiletime_assert_216 +537 include/linux/compiler.h

9a8ab1c3 Daniel Santos  2013-02-21  531   *
9a8ab1c3 Daniel Santos  2013-02-21  532   * In tradition of POSIX assert, this macro will break the build if the
9a8ab1c3 Daniel Santos  2013-02-21  533   * supplied condition is *false*, emitting the supplied error message if the
9a8ab1c3 Daniel Santos  2013-02-21  534   * compiler has support to do so.
9a8ab1c3 Daniel Santos  2013-02-21  535   */
9a8ab1c3 Daniel Santos  2013-02-21  536  #define compiletime_assert(condition, msg) \
9a8ab1c3 Daniel Santos  2013-02-21 @537  	_compiletime_assert(condition, msg, __compiletime_assert_, __LINE__)
9a8ab1c3 Daniel Santos  2013-02-21  538  
47933ad4 Peter Zijlstra 2013-11-06  539  #define compiletime_assert_atomic_type(t)				\
47933ad4 Peter Zijlstra 2013-11-06  540  	compiletime_assert(__native_word(t),				\

:::::: The code at line 537 was first introduced by commit
:::::: 9a8ab1c39970a4938a72d94e6fd13be88a797590 bug.h, compiler.h: introduce compiletime_assert &amp; BUILD_BUG_ON_MSG

:::::: TO: Daniel Santos &lt;daniel.santos@pobox.com&gt;
:::::: CC: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;

---
0-DAY kernel test infrastructure                Open Source Technology Center
https://lists.01.org/pipermail/kbuild-all                   Intel Corporation
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - March 24, 2017, 2:28 p.m.</div>
<pre class="content">
On Mon, Mar 13, 2017 at 11:45:01AM -0400, Zi Yan wrote:
<span class="quote">&gt; From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch adds thp migration&#39;s core code, including conversions</span>
<span class="quote">&gt; between a PMD entry and a swap entry, setting PMD migration entry,</span>
<span class="quote">&gt; removing PMD migration entry, and waiting on PMD migration entries.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch makes it possible to support thp migration.</span>
<span class="quote">&gt; If you fail to allocate a destination page as a thp, you just split</span>
<span class="quote">&gt; the source thp as we do now, and then enter the normal page migration.</span>
<span class="quote">&gt; If you succeed to allocate destination thp, you enter thp migration.</span>
<span class="quote">&gt; Subsequent patches actually enable thp migration for each caller of</span>
<span class="quote">&gt; page migration by allowing its get_new_page() callback to</span>
<span class="quote">&gt; allocate thps.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt; - support pte-mapped thp, doubly-mapped thp</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt; - use page_vma_mapped_walk()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ChangeLog v3 -&gt; v4:</span>
<span class="quote">&gt; - factor out the code of removing pte pgtable page in zap_huge_pmd()</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>

See few questions below.

It would be nice to split it into few patches. Probably three or four.
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  arch/x86/include/asm/pgtable_64.h |   2 +</span>
<span class="quote">&gt;  include/linux/swapops.h           |  70 +++++++++++++++++-</span>
<span class="quote">&gt;  mm/huge_memory.c                  | 147 ++++++++++++++++++++++++++++++++++----</span>
<span class="quote">&gt;  mm/migrate.c                      |  29 +++++++-</span>
<span class="quote">&gt;  mm/page_vma_mapped.c              |  13 +++-</span>
<span class="quote">&gt;  mm/pgtable-generic.c              |   3 +-</span>
<span class="quote">&gt;  mm/rmap.c                         |   9 +++</span>
<span class="quote">&gt;  7 files changed, 252 insertions(+), 21 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; index a5c4fc62e078..350397fd2129 100644</span>
<span class="quote">&gt; --- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; +++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="quote">&gt; @@ -187,7 +187,9 @@ static inline int pgd_large(pgd_t pgd) { return 0; }</span>
<span class="quote">&gt;  					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \</span>
<span class="quote">&gt;  					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })</span>
<span class="quote">&gt;  #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })</span>
<span class="quote">&gt; +#define __pmd_to_swp_entry(pmd)		((swp_entry_t) { pmd_val((pmd)) })</span>
<span class="quote">&gt;  #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })</span>
<span class="quote">&gt; +#define __swp_entry_to_pmd(x)		((pmd_t) { .pmd = (x).val })</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern int kern_addr_valid(unsigned long addr);</span>
<span class="quote">&gt;  extern void cleanup_highmap(void);</span>
<span class="quote">&gt; diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="quote">&gt; index 5c3a5f3e7eec..6625bea13869 100644</span>
<span class="quote">&gt; --- a/include/linux/swapops.h</span>
<span class="quote">&gt; +++ b/include/linux/swapops.h</span>
<span class="quote">&gt; @@ -103,7 +103,8 @@ static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
<span class="quote">&gt;  #ifdef CONFIG_MIGRATION</span>
<span class="quote">&gt;  static inline swp_entry_t make_migration_entry(struct page *page, int write)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; -	BUG_ON(!PageLocked(page));</span>
<span class="quote">&gt; +	BUG_ON(!PageLocked(compound_head(page)));</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	return swp_entry(write ? SWP_MIGRATION_WRITE : SWP_MIGRATION_READ,</span>
<span class="quote">&gt;  			page_to_pfn(page));</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -126,7 +127,7 @@ static inline struct page *migration_entry_to_page(swp_entry_t entry)</span>
<span class="quote">&gt;  	 * Any use of migration entries may only occur while the</span>
<span class="quote">&gt;  	 * corresponding page is locked</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	BUG_ON(!PageLocked(p));</span>
<span class="quote">&gt; +	BUG_ON(!PageLocked(compound_head(p)));</span>
<span class="quote">&gt;  	return p;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; @@ -163,6 +164,71 @@ static inline int is_write_migration_entry(swp_entry_t entry)</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +struct page_vma_mapped_walk;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +extern void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *new);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	arch_entry = __pmd_to_swp_entry(pmd);</span>
<span class="quote">&gt; +	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	swp_entry_t arch_entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));</span>
<span class="quote">&gt; +	return __swp_entry_to_pmd(arch_entry);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return !pmd_present(pmd) &amp;&amp; is_migration_entry(pmd_to_swp_entry(pmd));</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#else</span>
<span class="quote">&gt; +static inline void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *new)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return swp_entry(0, 0);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	BUILD_BUG();</span>
<span class="quote">&gt; +	return (pmd_t){ 0 };</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_MEMORY_FAILURE</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  extern atomic_long_t num_poisoned_pages __read_mostly;</span>
<span class="quote">&gt; diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="quote">&gt; index e32ccbd8ee3a..a9c2a0ef5b9b 100644</span>
<span class="quote">&gt; --- a/mm/huge_memory.c</span>
<span class="quote">&gt; +++ b/mm/huge_memory.c</span>
<span class="quote">&gt; @@ -1588,6 +1588,26 @@ static inline void zap_deposited_table(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="quote">&gt;  	atomic_long_dec(&amp;mm-&gt;nr_ptes);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +static inline void remove_trans_huge_pgtable(struct page *page,</span>
<span class="quote">&gt; +		struct mmu_gather *tlb, pmd_t *pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	if (PageAnon(page)) {</span>
<span class="quote">&gt; +		pgtable_t pgtable;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="quote">&gt; +							  pmd);</span>
<span class="quote">&gt; +		pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; +		atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; +		add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="quote">&gt; +				   -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +	} else {</span>
<span class="quote">&gt; +		if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; +			zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; +		add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="quote">&gt; +				   -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		 pmd_t *pmd, unsigned long addr)</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt; @@ -1618,23 +1638,27 @@ int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt;  		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	} else {</span>
<span class="quote">&gt; -		struct page *page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; -		page_remove_rmap(page, true);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; -		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; -		if (PageAnon(page)) {</span>
<span class="quote">&gt; -			pgtable_t pgtable;</span>
<span class="quote">&gt; -			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; -			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="quote">&gt; -			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="quote">&gt; -			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +		struct page *page;</span>
<span class="quote">&gt; +		int migration = 0;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="quote">&gt; +			page = pmd_page(orig_pmd);</span>
<span class="quote">&gt; +			page_remove_rmap(page, true);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="quote">&gt; +			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="quote">&gt; +			remove_trans_huge_pgtable(page, tlb, pmd);</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt; -			if (arch_needs_pgtable_deposit())</span>
<span class="quote">&gt; -				zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="quote">&gt; -			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="quote">&gt; +			swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="quote">&gt; +			page = pfn_to_page(swp_offset(entry));</span>
<span class="quote">&gt; +			remove_trans_huge_pgtable(page, tlb, pmd);</span>
<span class="quote">&gt; +			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="quote">&gt; +			migration = 1;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt;  		spin_unlock(ptl);</span>
<span class="quote">&gt; -		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +		if (!migration)</span>
<span class="quote">&gt; +			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return 1;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt; @@ -2652,3 +2676,98 @@ static int __init split_huge_pages_debugfs(void)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  late_initcall(split_huge_pages_debugfs);</span>
<span class="quote">&gt;  #endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="quote">&gt; +		struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt; +	pmd_t pmdval;</span>
<span class="quote">&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="quote">&gt; +		pmd_t pmdswp;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="quote">&gt; +				address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		flush_cache_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +		pmdval = pmdp_huge_clear_flush(vma, address, pvmw-&gt;pmd);</span>
<span class="quote">&gt; +		if (pmd_dirty(pmdval))</span>
<span class="quote">&gt; +			set_page_dirty(page);</span>
<span class="quote">&gt; +		entry = make_migration_entry(page, pmd_write(pmdval));</span>
<span class="quote">&gt; +		pmdswp = swp_entry_to_pmd(entry);</span>
<span class="quote">&gt; +		set_pmd_at(mm, address, pvmw-&gt;pmd, pmdswp);</span>
<span class="quote">&gt; +		page_remove_rmap(page, true);</span>
<span class="quote">&gt; +		put_page(page);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_range_end(mm, address,</span>
<span class="quote">&gt; +				address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt; +	} else { /* pte-mapped thp */</span>
<span class="quote">&gt; +		pte_t pteval;</span>
<span class="quote">&gt; +		struct page *subpage = page - page_to_pfn(page) + pte_pfn(*pvmw-&gt;pte);</span>
<span class="quote">&gt; +		pte_t swp_pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		pteval = ptep_clear_flush(vma, address, pvmw-&gt;pte);</span>
<span class="quote">&gt; +		if (pte_dirty(pteval))</span>
<span class="quote">&gt; +			set_page_dirty(subpage);</span>
<span class="quote">&gt; +		entry = make_migration_entry(subpage, pte_write(pteval));</span>
<span class="quote">&gt; +		swp_pte = swp_entry_to_pte(entry);</span>
<span class="quote">&gt; +		set_pte_at(mm, address, pvmw-&gt;pte, swp_pte);</span>
<span class="quote">&gt; +		page_remove_rmap(subpage, false);</span>
<span class="quote">&gt; +		put_page(subpage);</span>
<span class="quote">&gt; +		mmu_notifier_invalidate_page(mm, address);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="quote">&gt; +	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="quote">&gt; +	unsigned long address = pvmw-&gt;address;</span>
<span class="quote">&gt; +	swp_entry_t entry;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/* PMD-mapped THP  */</span>
<span class="quote">&gt; +	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="quote">&gt; +		unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="quote">&gt; +		unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="quote">&gt; +		pmd_t pmde;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="quote">&gt; +		get_page(new);</span>
<span class="quote">&gt; +		pmde = pmd_mkold(mk_huge_pmd(new, vma-&gt;vm_page_prot));</span>
<span class="quote">&gt; +		if (is_write_migration_entry(entry))</span>
<span class="quote">&gt; +			pmde = maybe_pmd_mkwrite(pmde, vma);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		flush_cache_range(vma, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		page_add_anon_rmap(new, vma, mmun_start, true);</span>
<span class="quote">&gt; +		pmdp_huge_clear_flush_notify(vma, mmun_start, pvmw-&gt;pmd);</span>
<span class="quote">&gt; +		set_pmd_at(mm, mmun_start, pvmw-&gt;pmd, pmde);</span>
<span class="quote">&gt; +		flush_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="quote">&gt; +		if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="quote">&gt; +			mlock_vma_page(new);</span>
<span class="quote">&gt; +		update_mmu_cache_pmd(vma, address, pvmw-&gt;pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	} else { /* pte-mapped thp */</span>
<span class="quote">&gt; +		pte_t pte;</span>
<span class="quote">&gt; +		pte_t *ptep = pvmw-&gt;pte;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		entry = pte_to_swp_entry(*pvmw-&gt;pte);</span>
<span class="quote">&gt; +		get_page(new);</span>
<span class="quote">&gt; +		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="quote">&gt; +		if (pte_swp_soft_dirty(*pvmw-&gt;pte))</span>
<span class="quote">&gt; +			pte = pte_mksoft_dirty(pte);</span>
<span class="quote">&gt; +		if (is_write_migration_entry(entry))</span>
<span class="quote">&gt; +			pte = maybe_mkwrite(pte, vma);</span>
<span class="quote">&gt; +		flush_dcache_page(new);</span>
<span class="quote">&gt; +		set_pte_at(mm, address, ptep, pte);</span>
<span class="quote">&gt; +		if (PageAnon(new))</span>
<span class="quote">&gt; +			page_add_anon_rmap(new, vma, address, false);</span>
<span class="quote">&gt; +		else</span>
<span class="quote">&gt; +			page_add_file_rmap(new, false);</span>
<span class="quote">&gt; +		update_mmu_cache(vma, address, ptep);</span>
<span class="quote">&gt; +	}</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt; index cda4c2778d04..0bbad6dcf95a 100644</span>
<span class="quote">&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt; @@ -211,6 +211,12 @@ static int remove_migration_pte(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  		new = page - pvmw.page-&gt;index +</span>
<span class="quote">&gt;  			linear_page_index(vma, pvmw.address);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +		/* PMD-mapped THP migration entry */</span>
<span class="quote">&gt; +		if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="quote">&gt; +			remove_migration_pmd(&amp;pvmw, new);</span>
<span class="quote">&gt; +			continue;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>

Any reason not to share PTE handling of non-THP with THP?
<span class="quote">
&gt;  		get_page(new);</span>
<span class="quote">&gt;  		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="quote">&gt;  		if (pte_swp_soft_dirty(*pvmw.pte))</span>
<span class="quote">&gt; @@ -324,6 +330,27 @@ void migration_entry_wait_huge(struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	__migration_entry_wait(mm, pte, ptl);</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="quote">&gt; +void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	spinlock_t *ptl;</span>
<span class="quote">&gt; +	struct page *page;</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	ptl = pmd_lock(mm, pmd);</span>
<span class="quote">&gt; +	if (!is_pmd_migration_entry(*pmd))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +	page = migration_entry_to_page(pmd_to_swp_entry(*pmd));</span>
<span class="quote">&gt; +	if (!get_page_unless_zero(page))</span>
<span class="quote">&gt; +		goto unlock;</span>
<span class="quote">&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt; +	wait_on_page_locked(page);</span>
<span class="quote">&gt; +	put_page(page);</span>
<span class="quote">&gt; +	return;</span>
<span class="quote">&gt; +unlock:</span>
<span class="quote">&gt; +	spin_unlock(ptl);</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +#endif</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  #ifdef CONFIG_BLOCK</span>
<span class="quote">&gt;  /* Returns true if all buffers are successfully locked */</span>
<span class="quote">&gt;  static bool buffer_migrate_lock_buffers(struct buffer_head *head,</span>
<span class="quote">&gt; @@ -1082,7 +1109,7 @@ static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
<span class="quote">&gt;  		goto out;</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (unlikely(PageTransHuge(page))) {</span>
<span class="quote">&gt; +	if (unlikely(PageTransHuge(page) &amp;&amp; !PageTransHuge(newpage))) {</span>
<span class="quote">&gt;  		lock_page(page);</span>
<span class="quote">&gt;  		rc = split_huge_page(page);</span>
<span class="quote">&gt;  		unlock_page(page);</span>
<span class="quote">&gt; diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="quote">&gt; index a23001a22c15..0ed3aee62d50 100644</span>
<span class="quote">&gt; --- a/mm/page_vma_mapped.c</span>
<span class="quote">&gt; +++ b/mm/page_vma_mapped.c</span>
<span class="quote">&gt; @@ -137,16 +137,23 @@ bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
<span class="quote">&gt;  	if (!pud_present(*pud))</span>
<span class="quote">&gt;  		return false;</span>
<span class="quote">&gt;  	pvmw-&gt;pmd = pmd_offset(pud, pvmw-&gt;address);</span>
<span class="quote">&gt; -	if (pmd_trans_huge(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt; +	if (pmd_trans_huge(*pvmw-&gt;pmd) || is_pmd_migration_entry(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt;  		pvmw-&gt;ptl = pmd_lock(mm, pvmw-&gt;pmd);</span>
<span class="quote">&gt; -		if (!pmd_present(*pvmw-&gt;pmd))</span>
<span class="quote">&gt; -			return not_found(pvmw);</span>
<span class="quote">&gt;  		if (likely(pmd_trans_huge(*pvmw-&gt;pmd))) {</span>
<span class="quote">&gt;  			if (pvmw-&gt;flags &amp; PVMW_MIGRATION)</span>
<span class="quote">&gt;  				return not_found(pvmw);</span>
<span class="quote">&gt;  			if (pmd_page(*pvmw-&gt;pmd) != page)</span>
<span class="quote">&gt;  				return not_found(pvmw);</span>
<span class="quote">&gt;  			return true;</span>
<span class="quote">&gt; +		} else if (!pmd_present(*pvmw-&gt;pmd)) {</span>
<span class="quote">&gt; +			if (unlikely(is_migration_entry(pmd_to_swp_entry(*pvmw-&gt;pmd)))) {</span>
<span class="quote">&gt; +				swp_entry_t entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +				if (migration_entry_to_page(entry) != page)</span>
<span class="quote">&gt; +					return not_found(pvmw);</span>
<span class="quote">&gt; +				return true;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +			return not_found(pvmw);</span>
<span class="quote">&gt;  		} else {</span>
<span class="quote">&gt;  			/* THP pmd was split under us: handle on pte level */</span>
<span class="quote">&gt;  			spin_unlock(pvmw-&gt;ptl);</span>
<span class="quote">&gt; diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="quote">&gt; index 4ed5908c65b0..9d550a8a0c71 100644</span>
<span class="quote">&gt; --- a/mm/pgtable-generic.c</span>
<span class="quote">&gt; +++ b/mm/pgtable-generic.c</span>
<span class="quote">&gt; @@ -118,7 +118,8 @@ pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;  {</span>
<span class="quote">&gt;  	pmd_t pmd;</span>
<span class="quote">&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt; -	VM_BUG_ON(!pmd_trans_huge(*pmdp) &amp;&amp; !pmd_devmap(*pmdp));</span>
<span class="quote">&gt; +	VM_BUG_ON(pmd_present(*pmdp) &amp;&amp; !pmd_trans_huge(*pmdp) &amp;&amp;</span>
<span class="quote">&gt; +		  !pmd_devmap(*pmdp));</span>

How does this? _flush doesn&#39;t make sense for !present.
<span class="quote">
&gt;  	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt;  	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;  	return pmd;</span>
<span class="quote">&gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt; index 555cc7ebacf6..2c65abbd7a0e 100644</span>
<span class="quote">&gt; --- a/mm/rmap.c</span>
<span class="quote">&gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt; @@ -1298,6 +1298,7 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	int ret = SWAP_AGAIN;</span>
<span class="quote">&gt;  	enum ttu_flags flags = (enum ttu_flags)arg;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  	/* munlock has nothing to gain from examining un-locked vmas */</span>
<span class="quote">&gt;  	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))</span>
<span class="quote">&gt;  		return SWAP_AGAIN;</span>
<span class="quote">&gt; @@ -1308,6 +1309,14 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  	while (page_vma_mapped_walk(&amp;pvmw)) {</span>
<span class="quote">&gt; +		/* THP migration */</span>
<span class="quote">&gt; +		if (flags &amp; TTU_MIGRATION) {</span>
<span class="quote">&gt; +			if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="quote">&gt; +				set_pmd_migration_entry(&amp;pvmw, page);</span>

Again, it would be nice share PTE handling. It should be rather similar,
no?
<span class="quote">
&gt; +				continue;</span>
<span class="quote">&gt; +			}</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * If the page is mlock()d, we cannot swap it out.</span>
<span class="quote">&gt;  		 * If it&#39;s recently referenced (perhaps page_referenced</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.11.0</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; --</span>
<span class="quote">&gt; To unsubscribe, send a message with &#39;unsubscribe linux-mm&#39; in</span>
<span class="quote">&gt; the body to majordomo@kvack.org.  For more info on Linux MM,</span>
<span class="quote">&gt; see: http://www.linux-mm.org/ .</span>
<span class="quote">&gt; Don&#39;t email: &lt;a href=mailto:&quot;dont@kvack.org&quot;&gt; email@kvack.org &lt;/a&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=123671">Zi Yan</a> - March 24, 2017, 3:30 p.m.</div>
<pre class="content">
Hi Kirill,

Kirill A. Shutemov wrote:
<span class="quote">&gt; On Mon, Mar 13, 2017 at 11:45:01AM -0400, Zi Yan wrote:</span>
<span class="quote">&gt;&gt; From: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch adds thp migration&#39;s core code, including conversions</span>
<span class="quote">&gt;&gt; between a PMD entry and a swap entry, setting PMD migration entry,</span>
<span class="quote">&gt;&gt; removing PMD migration entry, and waiting on PMD migration entries.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch makes it possible to support thp migration.</span>
<span class="quote">&gt;&gt; If you fail to allocate a destination page as a thp, you just split</span>
<span class="quote">&gt;&gt; the source thp as we do now, and then enter the normal page migration.</span>
<span class="quote">&gt;&gt; If you succeed to allocate destination thp, you enter thp migration.</span>
<span class="quote">&gt;&gt; Subsequent patches actually enable thp migration for each caller of</span>
<span class="quote">&gt;&gt; page migration by allowing its get_new_page() callback to</span>
<span class="quote">&gt;&gt; allocate thps.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v1 -&gt; v2:</span>
<span class="quote">&gt;&gt; - support pte-mapped thp, doubly-mapped thp</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v2 -&gt; v3:</span>
<span class="quote">&gt;&gt; - use page_vma_mapped_walk()</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ChangeLog v3 -&gt; v4:</span>
<span class="quote">&gt;&gt; - factor out the code of removing pte pgtable page in zap_huge_pmd()</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Zi Yan &lt;zi.yan@cs.rutgers.edu&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; See few questions below.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It would be nice to split it into few patches. Probably three or four.</span>

This patch was two separate ones in v2:
1. introduce remove_pmd_migration_entry(), set_migration_pmd() and other
auxiliary functions,
2. enable THP migration in the migration path.

But the first one of these two patches would be dead code, since no one
else uses it. Michal also suggested merging two patches into one when he
reviewed v2.

If you have any suggestion, I am OK to split this patch and make it
smaller.

&lt;snip&gt;
<span class="quote">
&gt;&gt; diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="quote">&gt;&gt; index cda4c2778d04..0bbad6dcf95a 100644</span>
<span class="quote">&gt;&gt; --- a/mm/migrate.c</span>
<span class="quote">&gt;&gt; +++ b/mm/migrate.c</span>
<span class="quote">&gt;&gt; @@ -211,6 +211,12 @@ static int remove_migration_pte(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  		new = page - pvmw.page-&gt;index +</span>
<span class="quote">&gt;&gt;  			linear_page_index(vma, pvmw.address);</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +		/* PMD-mapped THP migration entry */</span>
<span class="quote">&gt;&gt; +		if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="quote">&gt;&gt; +			remove_migration_pmd(&amp;pvmw, new);</span>
<span class="quote">&gt;&gt; +			continue;</span>
<span class="quote">&gt;&gt; +		}</span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Any reason not to share PTE handling of non-THP with THP?</span>

You mean PTE-mapped THPs? I was mostly reuse Naoya&#39;s patches. But at
first look, it seems PTE-mapped THP handling code is the same as
existing PTE handling code.

This part of code can be changed to:

+		/* PMD-mapped THP migration entry */
+		if (!pvmw.pte &amp;&amp; pvmw.page) {
+                       VM_BUG_ON_PAGE(!PageTransCompound(page), page);
+			remove_migration_pmd(&amp;pvmw, new);
+			continue;
+		}
+
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt;  		get_page(new);</span>
<span class="quote">&gt;&gt;  		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="quote">&gt;&gt;  		if (pte_swp_soft_dirty(*pvmw.pte))</span>

&lt;snip&gt;
<span class="quote">
&gt;&gt; diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="quote">&gt;&gt; index 4ed5908c65b0..9d550a8a0c71 100644</span>
<span class="quote">&gt;&gt; --- a/mm/pgtable-generic.c</span>
<span class="quote">&gt;&gt; +++ b/mm/pgtable-generic.c</span>
<span class="quote">&gt;&gt; @@ -118,7 +118,8 @@ pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
<span class="quote">&gt;&gt;  {</span>
<span class="quote">&gt;&gt;  	pmd_t pmd;</span>
<span class="quote">&gt;&gt;  	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);</span>
<span class="quote">&gt;&gt; -	VM_BUG_ON(!pmd_trans_huge(*pmdp) &amp;&amp; !pmd_devmap(*pmdp));</span>
<span class="quote">&gt;&gt; +	VM_BUG_ON(pmd_present(*pmdp) &amp;&amp; !pmd_trans_huge(*pmdp) &amp;&amp;</span>
<span class="quote">&gt;&gt; +		  !pmd_devmap(*pmdp));</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; How does this? _flush doesn&#39;t make sense for !present.</span>

Right. It should be:

-	VM_BUG_ON(!pmd_trans_huge(*pmdp) &amp;&amp; !pmd_devmap(*pmdp));
+	VM_BUG_ON((pmd_present(*pmdp) &amp;&amp; !pmd_trans_huge(*pmdp) &amp;&amp;
+		  !pmd_devmap(*pmdp)) || !pmd_present(*pmdp));
<span class="quote">

&gt; </span>
<span class="quote">&gt;&gt;  	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);</span>
<span class="quote">&gt;&gt;  	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="quote">&gt;&gt;  	return pmd;</span>
<span class="quote">&gt;&gt; diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="quote">&gt;&gt; index 555cc7ebacf6..2c65abbd7a0e 100644</span>
<span class="quote">&gt;&gt; --- a/mm/rmap.c</span>
<span class="quote">&gt;&gt; +++ b/mm/rmap.c</span>
<span class="quote">&gt;&gt; @@ -1298,6 +1298,7 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  	int ret = SWAP_AGAIN;</span>
<span class="quote">&gt;&gt;  	enum ttu_flags flags = (enum ttu_flags)arg;</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt; +</span>
<span class="quote">&gt;&gt;  	/* munlock has nothing to gain from examining un-locked vmas */</span>
<span class="quote">&gt;&gt;  	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))</span>
<span class="quote">&gt;&gt;  		return SWAP_AGAIN;</span>
<span class="quote">&gt;&gt; @@ -1308,6 +1309,14 @@ static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
<span class="quote">&gt;&gt;  	}</span>
<span class="quote">&gt;&gt;  </span>
<span class="quote">&gt;&gt;  	while (page_vma_mapped_walk(&amp;pvmw)) {</span>
<span class="quote">&gt;&gt; +		/* THP migration */</span>
<span class="quote">&gt;&gt; +		if (flags &amp; TTU_MIGRATION) {</span>
<span class="quote">&gt;&gt; +			if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="quote">&gt;&gt; +				set_pmd_migration_entry(&amp;pvmw, page);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Again, it would be nice share PTE handling. It should be rather similar,</span>
<span class="quote">&gt; no?</span>

At first look, it should work. I will change it. If it works, it will be
included in the next version.

This can also shrink the patch size.

Thanks.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/include/asm/pgtable_64.h b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">index a5c4fc62e078..350397fd2129 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pgtable_64.h</span>
<span class="p_chunk">@@ -187,7 +187,9 @@</span> <span class="p_context"> static inline int pgd_large(pgd_t pgd) { return 0; }</span>
 					 ((type) &lt;&lt; (SWP_TYPE_FIRST_BIT)) \
 					 | ((offset) &lt;&lt; SWP_OFFSET_FIRST_BIT) })
 #define __pte_to_swp_entry(pte)		((swp_entry_t) { pte_val((pte)) })
<span class="p_add">+#define __pmd_to_swp_entry(pmd)		((swp_entry_t) { pmd_val((pmd)) })</span>
 #define __swp_entry_to_pte(x)		((pte_t) { .pte = (x).val })
<span class="p_add">+#define __swp_entry_to_pmd(x)		((pmd_t) { .pmd = (x).val })</span>
 
 extern int kern_addr_valid(unsigned long addr);
 extern void cleanup_highmap(void);
<span class="p_header">diff --git a/include/linux/swapops.h b/include/linux/swapops.h</span>
<span class="p_header">index 5c3a5f3e7eec..6625bea13869 100644</span>
<span class="p_header">--- a/include/linux/swapops.h</span>
<span class="p_header">+++ b/include/linux/swapops.h</span>
<span class="p_chunk">@@ -103,7 +103,8 @@</span> <span class="p_context"> static inline void *swp_to_radix_entry(swp_entry_t entry)</span>
 #ifdef CONFIG_MIGRATION
 static inline swp_entry_t make_migration_entry(struct page *page, int write)
 {
<span class="p_del">-	BUG_ON(!PageLocked(page));</span>
<span class="p_add">+	BUG_ON(!PageLocked(compound_head(page)));</span>
<span class="p_add">+</span>
 	return swp_entry(write ? SWP_MIGRATION_WRITE : SWP_MIGRATION_READ,
 			page_to_pfn(page));
 }
<span class="p_chunk">@@ -126,7 +127,7 @@</span> <span class="p_context"> static inline struct page *migration_entry_to_page(swp_entry_t entry)</span>
 	 * Any use of migration entries may only occur while the
 	 * corresponding page is locked
 	 */
<span class="p_del">-	BUG_ON(!PageLocked(p));</span>
<span class="p_add">+	BUG_ON(!PageLocked(compound_head(p)));</span>
 	return p;
 }
 
<span class="p_chunk">@@ -163,6 +164,71 @@</span> <span class="p_context"> static inline int is_write_migration_entry(swp_entry_t entry)</span>
 
 #endif
 
<span class="p_add">+struct page_vma_mapped_walk;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+extern void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *new);</span>
<span class="p_add">+</span>
<span class="p_add">+extern void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	swp_entry_t arch_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_entry = __pmd_to_swp_entry(pmd);</span>
<span class="p_add">+	return swp_entry(__swp_type(arch_entry), __swp_offset(arch_entry));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	swp_entry_t arch_entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	arch_entry = __swp_entry(swp_type(entry), swp_offset(entry));</span>
<span class="p_add">+	return __swp_entry_to_pmd(arch_entry);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return !pmd_present(pmd) &amp;&amp; is_migration_entry(pmd_to_swp_entry(pmd));</span>
<span class="p_add">+}</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void remove_migration_pmd(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *new)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline void pmd_migration_entry_wait(struct mm_struct *m, pmd_t *p) { }</span>
<span class="p_add">+</span>
<span class="p_add">+static inline swp_entry_t pmd_to_swp_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return swp_entry(0, 0);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline pmd_t swp_entry_to_pmd(swp_entry_t entry)</span>
<span class="p_add">+{</span>
<span class="p_add">+	BUILD_BUG();</span>
<span class="p_add">+	return (pmd_t){ 0 };</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static inline int is_pmd_migration_entry(pmd_t pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_MEMORY_FAILURE
 
 extern atomic_long_t num_poisoned_pages __read_mostly;
<span class="p_header">diff --git a/mm/huge_memory.c b/mm/huge_memory.c</span>
<span class="p_header">index e32ccbd8ee3a..a9c2a0ef5b9b 100644</span>
<span class="p_header">--- a/mm/huge_memory.c</span>
<span class="p_header">+++ b/mm/huge_memory.c</span>
<span class="p_chunk">@@ -1588,6 +1588,26 @@</span> <span class="p_context"> static inline void zap_deposited_table(struct mm_struct *mm, pmd_t *pmd)</span>
 	atomic_long_dec(&amp;mm-&gt;nr_ptes);
 }
 
<span class="p_add">+static inline void remove_trans_huge_pgtable(struct page *page,</span>
<span class="p_add">+		struct mmu_gather *tlb, pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	if (PageAnon(page)) {</span>
<span class="p_add">+		pgtable_t pgtable;</span>
<span class="p_add">+</span>
<span class="p_add">+		pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm,</span>
<span class="p_add">+							  pmd);</span>
<span class="p_add">+		pte_free(tlb-&gt;mm, pgtable);</span>
<span class="p_add">+		atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="p_add">+		add_mm_counter(tlb-&gt;mm, MM_ANONPAGES,</span>
<span class="p_add">+				   -HPAGE_PMD_NR);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		if (arch_needs_pgtable_deposit())</span>
<span class="p_add">+			zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="p_add">+		add_mm_counter(tlb-&gt;mm, MM_FILEPAGES,</span>
<span class="p_add">+				   -HPAGE_PMD_NR);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,
 		 pmd_t *pmd, unsigned long addr)
 {
<span class="p_chunk">@@ -1618,23 +1638,27 @@</span> <span class="p_context"> int zap_huge_pmd(struct mmu_gather *tlb, struct vm_area_struct *vma,</span>
 		spin_unlock(ptl);
 		tlb_remove_page_size(tlb, pmd_page(orig_pmd), HPAGE_PMD_SIZE);
 	} else {
<span class="p_del">-		struct page *page = pmd_page(orig_pmd);</span>
<span class="p_del">-		page_remove_rmap(page, true);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_del">-		VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_del">-		if (PageAnon(page)) {</span>
<span class="p_del">-			pgtable_t pgtable;</span>
<span class="p_del">-			pgtable = pgtable_trans_huge_withdraw(tlb-&gt;mm, pmd);</span>
<span class="p_del">-			pte_free(tlb-&gt;mm, pgtable);</span>
<span class="p_del">-			atomic_long_dec(&amp;tlb-&gt;mm-&gt;nr_ptes);</span>
<span class="p_del">-			add_mm_counter(tlb-&gt;mm, MM_ANONPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+		struct page *page;</span>
<span class="p_add">+		int migration = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!is_pmd_migration_entry(orig_pmd)) {</span>
<span class="p_add">+			page = pmd_page(orig_pmd);</span>
<span class="p_add">+			page_remove_rmap(page, true);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(page_mapcount(page) &lt; 0, page);</span>
<span class="p_add">+			VM_BUG_ON_PAGE(!PageHead(page), page);</span>
<span class="p_add">+			remove_trans_huge_pgtable(page, tlb, pmd);</span>
 		} else {
<span class="p_del">-			if (arch_needs_pgtable_deposit())</span>
<span class="p_del">-				zap_deposited_table(tlb-&gt;mm, pmd);</span>
<span class="p_del">-			add_mm_counter(tlb-&gt;mm, MM_FILEPAGES, -HPAGE_PMD_NR);</span>
<span class="p_add">+			swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+			entry = pmd_to_swp_entry(orig_pmd);</span>
<span class="p_add">+			page = pfn_to_page(swp_offset(entry));</span>
<span class="p_add">+			remove_trans_huge_pgtable(page, tlb, pmd);</span>
<span class="p_add">+			free_swap_and_cache(entry); /* waring in failure? */</span>
<span class="p_add">+			migration = 1;</span>
 		}
 		spin_unlock(ptl);
<span class="p_del">-		tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
<span class="p_add">+		if (!migration)</span>
<span class="p_add">+			tlb_remove_page_size(tlb, page, HPAGE_PMD_SIZE);</span>
 	}
 	return 1;
 }
<span class="p_chunk">@@ -2652,3 +2676,98 @@</span> <span class="p_context"> static int __init split_huge_pages_debugfs(void)</span>
 }
 late_initcall(split_huge_pages_debugfs);
 #endif
<span class="p_add">+</span>
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+void set_pmd_migration_entry(struct page_vma_mapped_walk *pvmw,</span>
<span class="p_add">+		struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long address = pvmw-&gt;address;</span>
<span class="p_add">+	pmd_t pmdval;</span>
<span class="p_add">+	swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="p_add">+		pmd_t pmdswp;</span>
<span class="p_add">+</span>
<span class="p_add">+		mmu_notifier_invalidate_range_start(mm, address,</span>
<span class="p_add">+				address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+		flush_cache_range(vma, address, address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+		pmdval = pmdp_huge_clear_flush(vma, address, pvmw-&gt;pmd);</span>
<span class="p_add">+		if (pmd_dirty(pmdval))</span>
<span class="p_add">+			set_page_dirty(page);</span>
<span class="p_add">+		entry = make_migration_entry(page, pmd_write(pmdval));</span>
<span class="p_add">+		pmdswp = swp_entry_to_pmd(entry);</span>
<span class="p_add">+		set_pmd_at(mm, address, pvmw-&gt;pmd, pmdswp);</span>
<span class="p_add">+		page_remove_rmap(page, true);</span>
<span class="p_add">+		put_page(page);</span>
<span class="p_add">+</span>
<span class="p_add">+		mmu_notifier_invalidate_range_end(mm, address,</span>
<span class="p_add">+				address + HPAGE_PMD_SIZE);</span>
<span class="p_add">+	} else { /* pte-mapped thp */</span>
<span class="p_add">+		pte_t pteval;</span>
<span class="p_add">+		struct page *subpage = page - page_to_pfn(page) + pte_pfn(*pvmw-&gt;pte);</span>
<span class="p_add">+		pte_t swp_pte;</span>
<span class="p_add">+</span>
<span class="p_add">+		pteval = ptep_clear_flush(vma, address, pvmw-&gt;pte);</span>
<span class="p_add">+		if (pte_dirty(pteval))</span>
<span class="p_add">+			set_page_dirty(subpage);</span>
<span class="p_add">+		entry = make_migration_entry(subpage, pte_write(pteval));</span>
<span class="p_add">+		swp_pte = swp_entry_to_pte(entry);</span>
<span class="p_add">+		set_pte_at(mm, address, pvmw-&gt;pte, swp_pte);</span>
<span class="p_add">+		page_remove_rmap(subpage, false);</span>
<span class="p_add">+		put_page(subpage);</span>
<span class="p_add">+		mmu_notifier_invalidate_page(mm, address);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void remove_migration_pmd(struct page_vma_mapped_walk *pvmw, struct page *new)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct vm_area_struct *vma = pvmw-&gt;vma;</span>
<span class="p_add">+	struct mm_struct *mm = vma-&gt;vm_mm;</span>
<span class="p_add">+	unsigned long address = pvmw-&gt;address;</span>
<span class="p_add">+	swp_entry_t entry;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* PMD-mapped THP  */</span>
<span class="p_add">+	if (pvmw-&gt;pmd &amp;&amp; !pvmw-&gt;pte) {</span>
<span class="p_add">+		unsigned long mmun_start = address &amp; HPAGE_PMD_MASK;</span>
<span class="p_add">+		unsigned long mmun_end = mmun_start + HPAGE_PMD_SIZE;</span>
<span class="p_add">+		pmd_t pmde;</span>
<span class="p_add">+</span>
<span class="p_add">+		entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="p_add">+		get_page(new);</span>
<span class="p_add">+		pmde = pmd_mkold(mk_huge_pmd(new, vma-&gt;vm_page_prot));</span>
<span class="p_add">+		if (is_write_migration_entry(entry))</span>
<span class="p_add">+			pmde = maybe_pmd_mkwrite(pmde, vma);</span>
<span class="p_add">+</span>
<span class="p_add">+		flush_cache_range(vma, mmun_start, mmun_end);</span>
<span class="p_add">+		page_add_anon_rmap(new, vma, mmun_start, true);</span>
<span class="p_add">+		pmdp_huge_clear_flush_notify(vma, mmun_start, pvmw-&gt;pmd);</span>
<span class="p_add">+		set_pmd_at(mm, mmun_start, pvmw-&gt;pmd, pmde);</span>
<span class="p_add">+		flush_tlb_range(vma, mmun_start, mmun_end);</span>
<span class="p_add">+		if (vma-&gt;vm_flags &amp; VM_LOCKED)</span>
<span class="p_add">+			mlock_vma_page(new);</span>
<span class="p_add">+		update_mmu_cache_pmd(vma, address, pvmw-&gt;pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+	} else { /* pte-mapped thp */</span>
<span class="p_add">+		pte_t pte;</span>
<span class="p_add">+		pte_t *ptep = pvmw-&gt;pte;</span>
<span class="p_add">+</span>
<span class="p_add">+		entry = pte_to_swp_entry(*pvmw-&gt;pte);</span>
<span class="p_add">+		get_page(new);</span>
<span class="p_add">+		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));</span>
<span class="p_add">+		if (pte_swp_soft_dirty(*pvmw-&gt;pte))</span>
<span class="p_add">+			pte = pte_mksoft_dirty(pte);</span>
<span class="p_add">+		if (is_write_migration_entry(entry))</span>
<span class="p_add">+			pte = maybe_mkwrite(pte, vma);</span>
<span class="p_add">+		flush_dcache_page(new);</span>
<span class="p_add">+		set_pte_at(mm, address, ptep, pte);</span>
<span class="p_add">+		if (PageAnon(new))</span>
<span class="p_add">+			page_add_anon_rmap(new, vma, address, false);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			page_add_file_rmap(new, false);</span>
<span class="p_add">+		update_mmu_cache(vma, address, ptep);</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_header">diff --git a/mm/migrate.c b/mm/migrate.c</span>
<span class="p_header">index cda4c2778d04..0bbad6dcf95a 100644</span>
<span class="p_header">--- a/mm/migrate.c</span>
<span class="p_header">+++ b/mm/migrate.c</span>
<span class="p_chunk">@@ -211,6 +211,12 @@</span> <span class="p_context"> static int remove_migration_pte(struct page *page, struct vm_area_struct *vma,</span>
 		new = page - pvmw.page-&gt;index +
 			linear_page_index(vma, pvmw.address);
 
<span class="p_add">+		/* PMD-mapped THP migration entry */</span>
<span class="p_add">+		if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="p_add">+			remove_migration_pmd(&amp;pvmw, new);</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		get_page(new);
 		pte = pte_mkold(mk_pte(new, READ_ONCE(vma-&gt;vm_page_prot)));
 		if (pte_swp_soft_dirty(*pvmw.pte))
<span class="p_chunk">@@ -324,6 +330,27 @@</span> <span class="p_context"> void migration_entry_wait_huge(struct vm_area_struct *vma,</span>
 	__migration_entry_wait(mm, pte, ptl);
 }
 
<span class="p_add">+#ifdef CONFIG_ARCH_ENABLE_THP_MIGRATION</span>
<span class="p_add">+void pmd_migration_entry_wait(struct mm_struct *mm, pmd_t *pmd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	ptl = pmd_lock(mm, pmd);</span>
<span class="p_add">+	if (!is_pmd_migration_entry(*pmd))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	page = migration_entry_to_page(pmd_to_swp_entry(*pmd));</span>
<span class="p_add">+	if (!get_page_unless_zero(page))</span>
<span class="p_add">+		goto unlock;</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+	wait_on_page_locked(page);</span>
<span class="p_add">+	put_page(page);</span>
<span class="p_add">+	return;</span>
<span class="p_add">+unlock:</span>
<span class="p_add">+	spin_unlock(ptl);</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_BLOCK
 /* Returns true if all buffers are successfully locked */
 static bool buffer_migrate_lock_buffers(struct buffer_head *head,
<span class="p_chunk">@@ -1082,7 +1109,7 @@</span> <span class="p_context"> static ICE_noinline int unmap_and_move(new_page_t get_new_page,</span>
 		goto out;
 	}
 
<span class="p_del">-	if (unlikely(PageTransHuge(page))) {</span>
<span class="p_add">+	if (unlikely(PageTransHuge(page) &amp;&amp; !PageTransHuge(newpage))) {</span>
 		lock_page(page);
 		rc = split_huge_page(page);
 		unlock_page(page);
<span class="p_header">diff --git a/mm/page_vma_mapped.c b/mm/page_vma_mapped.c</span>
<span class="p_header">index a23001a22c15..0ed3aee62d50 100644</span>
<span class="p_header">--- a/mm/page_vma_mapped.c</span>
<span class="p_header">+++ b/mm/page_vma_mapped.c</span>
<span class="p_chunk">@@ -137,16 +137,23 @@</span> <span class="p_context"> bool page_vma_mapped_walk(struct page_vma_mapped_walk *pvmw)</span>
 	if (!pud_present(*pud))
 		return false;
 	pvmw-&gt;pmd = pmd_offset(pud, pvmw-&gt;address);
<span class="p_del">-	if (pmd_trans_huge(*pvmw-&gt;pmd)) {</span>
<span class="p_add">+	if (pmd_trans_huge(*pvmw-&gt;pmd) || is_pmd_migration_entry(*pvmw-&gt;pmd)) {</span>
 		pvmw-&gt;ptl = pmd_lock(mm, pvmw-&gt;pmd);
<span class="p_del">-		if (!pmd_present(*pvmw-&gt;pmd))</span>
<span class="p_del">-			return not_found(pvmw);</span>
 		if (likely(pmd_trans_huge(*pvmw-&gt;pmd))) {
 			if (pvmw-&gt;flags &amp; PVMW_MIGRATION)
 				return not_found(pvmw);
 			if (pmd_page(*pvmw-&gt;pmd) != page)
 				return not_found(pvmw);
 			return true;
<span class="p_add">+		} else if (!pmd_present(*pvmw-&gt;pmd)) {</span>
<span class="p_add">+			if (unlikely(is_migration_entry(pmd_to_swp_entry(*pvmw-&gt;pmd)))) {</span>
<span class="p_add">+				swp_entry_t entry = pmd_to_swp_entry(*pvmw-&gt;pmd);</span>
<span class="p_add">+</span>
<span class="p_add">+				if (migration_entry_to_page(entry) != page)</span>
<span class="p_add">+					return not_found(pvmw);</span>
<span class="p_add">+				return true;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			return not_found(pvmw);</span>
 		} else {
 			/* THP pmd was split under us: handle on pte level */
 			spin_unlock(pvmw-&gt;ptl);
<span class="p_header">diff --git a/mm/pgtable-generic.c b/mm/pgtable-generic.c</span>
<span class="p_header">index 4ed5908c65b0..9d550a8a0c71 100644</span>
<span class="p_header">--- a/mm/pgtable-generic.c</span>
<span class="p_header">+++ b/mm/pgtable-generic.c</span>
<span class="p_chunk">@@ -118,7 +118,8 @@</span> <span class="p_context"> pmd_t pmdp_huge_clear_flush(struct vm_area_struct *vma, unsigned long address,</span>
 {
 	pmd_t pmd;
 	VM_BUG_ON(address &amp; ~HPAGE_PMD_MASK);
<span class="p_del">-	VM_BUG_ON(!pmd_trans_huge(*pmdp) &amp;&amp; !pmd_devmap(*pmdp));</span>
<span class="p_add">+	VM_BUG_ON(pmd_present(*pmdp) &amp;&amp; !pmd_trans_huge(*pmdp) &amp;&amp;</span>
<span class="p_add">+		  !pmd_devmap(*pmdp));</span>
 	pmd = pmdp_huge_get_and_clear(vma-&gt;vm_mm, address, pmdp);
 	flush_pmd_tlb_range(vma, address, address + HPAGE_PMD_SIZE);
 	return pmd;
<span class="p_header">diff --git a/mm/rmap.c b/mm/rmap.c</span>
<span class="p_header">index 555cc7ebacf6..2c65abbd7a0e 100644</span>
<span class="p_header">--- a/mm/rmap.c</span>
<span class="p_header">+++ b/mm/rmap.c</span>
<span class="p_chunk">@@ -1298,6 +1298,7 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	int ret = SWAP_AGAIN;
 	enum ttu_flags flags = (enum ttu_flags)arg;
 
<span class="p_add">+</span>
 	/* munlock has nothing to gain from examining un-locked vmas */
 	if ((flags &amp; TTU_MUNLOCK) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_LOCKED))
 		return SWAP_AGAIN;
<span class="p_chunk">@@ -1308,6 +1309,14 @@</span> <span class="p_context"> static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma,</span>
 	}
 
 	while (page_vma_mapped_walk(&amp;pvmw)) {
<span class="p_add">+		/* THP migration */</span>
<span class="p_add">+		if (flags &amp; TTU_MIGRATION) {</span>
<span class="p_add">+			if (!PageHuge(page) &amp;&amp; PageTransCompound(page)) {</span>
<span class="p_add">+				set_pmd_migration_entry(&amp;pvmw, page);</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+			}</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
 		/*
 		 * If the page is mlock()d, we cannot swap it out.
 		 * If it&#39;s recently referenced (perhaps page_referenced

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



