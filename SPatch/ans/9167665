
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[12/27] mm, vmscan: Make shrink_node decisions more node-centric - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [12/27] mm, vmscan: Make shrink_node decisions more node-centric</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 9, 2016, 6:04 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1465495483-11855-13-git-send-email-mgorman@techsingularity.net&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9167665/mbox/"
   >mbox</a>
|
   <a href="/patch/9167665/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9167665/">/patch/9167665/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	2C79660890 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Jun 2016 18:08:03 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 20ECC2835B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Jun 2016 18:08:03 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 15C942835C; Thu,  9 Jun 2016 18:08:03 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id F18D52835D
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  9 Jun 2016 18:08:01 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933167AbcFISHl (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Thu, 9 Jun 2016 14:07:41 -0400
Received: from outbound-smtp10.blacknight.com ([46.22.139.15]:45881 &quot;EHLO
	outbound-smtp10.blacknight.com&quot; rhost-flags-OK-OK-OK-OK)
	by vger.kernel.org with ESMTP id S932355AbcFISHh (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Thu, 9 Jun 2016 14:07:37 -0400
Received: from mail.blacknight.com (pemlinmail05.blacknight.ie
	[81.17.254.26])
	by outbound-smtp10.blacknight.com (Postfix) with ESMTPS id
	ACFFF1C18B7 for &lt;linux-kernel@vger.kernel.org&gt;;
	Thu,  9 Jun 2016 19:06:56 +0100 (IST)
Received: (qmail 5413 invoked from network); 9 Jun 2016 18:06:56 -0000
Received: from unknown (HELO stampy.163woodhaven.lan)
	(mgorman@techsingularity.net@[37.228.231.136])
	by 81.17.254.9 with ESMTPA; 9 Jun 2016 18:06:56 -0000
From: Mel Gorman &lt;mgorman@techsingularity.net&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;, Linux-MM &lt;linux-mm@kvack.org&gt;
Cc: Rik van Riel &lt;riel@surriel.com&gt;, Vlastimil Babka &lt;vbabka@suse.cz&gt;,
	Johannes Weiner &lt;hannes@cmpxchg.org&gt;,
	LKML &lt;linux-kernel@vger.kernel.org&gt;,
	Mel Gorman &lt;mgorman@techsingularity.net&gt;
Subject: [PATCH 12/27] mm,
	vmscan: Make shrink_node decisions more node-centric
Date: Thu,  9 Jun 2016 19:04:28 +0100
Message-Id: &lt;1465495483-11855-13-git-send-email-mgorman@techsingularity.net&gt;
X-Mailer: git-send-email 2.6.4
In-Reply-To: &lt;1465495483-11855-1-git-send-email-mgorman@techsingularity.net&gt;
References: &lt;1465495483-11855-1-git-send-email-mgorman@techsingularity.net&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - June 9, 2016, 6:04 p.m.</div>
<pre class="content">
Earlier patches focused on having direct reclaim and kswapd use data that
is node-centric for reclaiming but shrink_node() itself still uses too much
zone information. This patch removes unnecessary zone-based information
with the most important decision being whether to continue reclaim or
not. Some memcg APIs are adjusted as a result even though memcg itself
still uses some zone information.
<span class="signed-off-by">
Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
---
 include/linux/memcontrol.h |  9 +++----
 include/linux/mmzone.h     |  4 ++--
 include/linux/swap.h       |  2 +-
 mm/memcontrol.c            | 17 +++++++-------
 mm/page_alloc.c            |  2 +-
 mm/vmscan.c                | 58 ++++++++++++++++++++++++++--------------------
 mm/workingset.c            |  6 ++---
 7 files changed, 54 insertions(+), 44 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=72672">Vlastimil Babka</a> - June 16, 2016, 1:35 p.m.</div>
<pre class="content">
On 06/09/2016 08:04 PM, Mel Gorman wrote:
<span class="quote">&gt; Earlier patches focused on having direct reclaim and kswapd use data that</span>
<span class="quote">&gt; is node-centric for reclaiming but shrink_node() itself still uses too much</span>
<span class="quote">&gt; zone information. This patch removes unnecessary zone-based information</span>
<span class="quote">&gt; with the most important decision being whether to continue reclaim or</span>
<span class="quote">&gt; not. Some memcg APIs are adjusted as a result even though memcg itself</span>
<span class="quote">&gt; still uses some zone information.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>

[...]
<span class="quote">
&gt; @@ -2372,21 +2374,27 @@ static inline bool should_continue_reclaim(struct zone *zone,</span>
<span class="quote">&gt;  	 * inactive lists are large enough, continue reclaiming</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	pages_for_compaction = (2UL &lt;&lt; sc-&gt;order);</span>
<span class="quote">&gt; -	inactive_lru_pages = node_page_state(zone-&gt;zone_pgdat, NR_INACTIVE_FILE);</span>
<span class="quote">&gt; +	inactive_lru_pages = node_page_state(pgdat, NR_INACTIVE_FILE);</span>
<span class="quote">&gt;  	if (get_nr_swap_pages() &gt; 0)</span>
<span class="quote">&gt; -		inactive_lru_pages += node_page_state(zone-&gt;zone_pgdat, NR_INACTIVE_ANON);</span>
<span class="quote">&gt; +		inactive_lru_pages += node_page_state(pgdat, NR_INACTIVE_ANON);</span>
<span class="quote">&gt;  	if (sc-&gt;nr_reclaimed &lt; pages_for_compaction &amp;&amp;</span>
<span class="quote">&gt;  			inactive_lru_pages &gt; pages_for_compaction)</span>
<span class="quote">&gt;  		return true;</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  	/* If compaction would go ahead or the allocation would succeed, stop */</span>
<span class="quote">&gt; -	switch (compaction_suitable(zone, sc-&gt;order, 0, 0)) {</span>
<span class="quote">&gt; -	case COMPACT_PARTIAL:</span>
<span class="quote">&gt; -	case COMPACT_CONTINUE:</span>
<span class="quote">&gt; -		return false;</span>
<span class="quote">&gt; -	default:</span>
<span class="quote">&gt; -		return true;</span>
<span class="quote">&gt; +	for (z = 0; z &lt;= sc-&gt;reclaim_idx; z++) {</span>
<span class="quote">&gt; +		struct zone *zone = &amp;pgdat-&gt;node_zones[z];</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +		switch (compaction_suitable(zone, sc-&gt;order, 0, 0)) {</span>

Using 0 for classzone_idx here was sort of OK when each zone was 
reclaimed separately, as a Normal allocation not passing appropriate 
classzone_idx (and thus subtracting lowmem reserve from free pages) 
means that a false COMPACT_PARTIAL (or COMPACT_CONTINUE) could be 
returned for e.g. DMA zone. It means a premature end of reclaim for this 
single zone, which is relatively small anyway, so no big deal (and we 
might avoid useless over-reclaim, when even reclaiming everything 
wouldn&#39;t get us above the lowmem_reserve).

But in node-centric reclaim, such premature &quot;return false&quot; from a DMA 
zone stops reclaiming the whole node. So I think we should involve the 
real classzone_idx here.
<span class="quote">
&gt; +		case COMPACT_PARTIAL:</span>
<span class="quote">&gt; +		case COMPACT_CONTINUE:</span>
<span class="quote">&gt; +			return false;</span>
<span class="quote">&gt; +		default:</span>
<span class="quote">&gt; +			/* check next zone */</span>
<span class="quote">&gt; +			;</span>
<span class="quote">&gt; +		}</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt; +	return true;</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=138281">Mel Gorman</a> - June 16, 2016, 2:47 p.m.</div>
<pre class="content">
On Thu, Jun 16, 2016 at 03:35:15PM +0200, Vlastimil Babka wrote:
<span class="quote">&gt; On 06/09/2016 08:04 PM, Mel Gorman wrote:</span>
<span class="quote">&gt; &gt;Earlier patches focused on having direct reclaim and kswapd use data that</span>
<span class="quote">&gt; &gt;is node-centric for reclaiming but shrink_node() itself still uses too much</span>
<span class="quote">&gt; &gt;zone information. This patch removes unnecessary zone-based information</span>
<span class="quote">&gt; &gt;with the most important decision being whether to continue reclaim or</span>
<span class="quote">&gt; &gt;not. Some memcg APIs are adjusted as a result even though memcg itself</span>
<span class="quote">&gt; &gt;still uses some zone information.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;Signed-off-by: Mel Gorman &lt;mgorman@techsingularity.net&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; [...]</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;@@ -2372,21 +2374,27 @@ static inline bool should_continue_reclaim(struct zone *zone,</span>
<span class="quote">&gt; &gt; 	 * inactive lists are large enough, continue reclaiming</span>
<span class="quote">&gt; &gt; 	 */</span>
<span class="quote">&gt; &gt; 	pages_for_compaction = (2UL &lt;&lt; sc-&gt;order);</span>
<span class="quote">&gt; &gt;-	inactive_lru_pages = node_page_state(zone-&gt;zone_pgdat, NR_INACTIVE_FILE);</span>
<span class="quote">&gt; &gt;+	inactive_lru_pages = node_page_state(pgdat, NR_INACTIVE_FILE);</span>
<span class="quote">&gt; &gt; 	if (get_nr_swap_pages() &gt; 0)</span>
<span class="quote">&gt; &gt;-		inactive_lru_pages += node_page_state(zone-&gt;zone_pgdat, NR_INACTIVE_ANON);</span>
<span class="quote">&gt; &gt;+		inactive_lru_pages += node_page_state(pgdat, NR_INACTIVE_ANON);</span>
<span class="quote">&gt; &gt; 	if (sc-&gt;nr_reclaimed &lt; pages_for_compaction &amp;&amp;</span>
<span class="quote">&gt; &gt; 			inactive_lru_pages &gt; pages_for_compaction)</span>
<span class="quote">&gt; &gt; 		return true;</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; 	/* If compaction would go ahead or the allocation would succeed, stop */</span>
<span class="quote">&gt; &gt;-	switch (compaction_suitable(zone, sc-&gt;order, 0, 0)) {</span>
<span class="quote">&gt; &gt;-	case COMPACT_PARTIAL:</span>
<span class="quote">&gt; &gt;-	case COMPACT_CONTINUE:</span>
<span class="quote">&gt; &gt;-		return false;</span>
<span class="quote">&gt; &gt;-	default:</span>
<span class="quote">&gt; &gt;-		return true;</span>
<span class="quote">&gt; &gt;+	for (z = 0; z &lt;= sc-&gt;reclaim_idx; z++) {</span>
<span class="quote">&gt; &gt;+		struct zone *zone = &amp;pgdat-&gt;node_zones[z];</span>
<span class="quote">&gt; &gt;+</span>
<span class="quote">&gt; &gt;+		switch (compaction_suitable(zone, sc-&gt;order, 0, 0)) {</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Using 0 for classzone_idx here was sort of OK when each zone was reclaimed</span>
<span class="quote">&gt; separately, as a Normal allocation not passing appropriate classzone_idx</span>
<span class="quote">&gt; (and thus subtracting lowmem reserve from free pages) means that a false</span>
<span class="quote">&gt; COMPACT_PARTIAL (or COMPACT_CONTINUE) could be returned for e.g. DMA zone.</span>
<span class="quote">&gt; It means a premature end of reclaim for this single zone, which is</span>
<span class="quote">&gt; relatively small anyway, so no big deal (and we might avoid useless</span>
<span class="quote">&gt; over-reclaim, when even reclaiming everything wouldn&#39;t get us above the</span>
<span class="quote">&gt; lowmem_reserve).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But in node-centric reclaim, such premature &quot;return false&quot; from a DMA zone</span>
<span class="quote">&gt; stops reclaiming the whole node. So I think we should involve the real</span>
<span class="quote">&gt; classzone_idx here.</span>
<span class="quote">&gt; </span>

Fair point although for compaction, it&#39;ll occur for a marginal corner
case. Premature allowed compaction for ZONE_DMA is unfortunate but
bizarre to think there would be a high-order allocation restricted to
just that zone too.

I&#39;ll pass in sc-&gt;reclaim_idx as it represents the allocating order. That
highlights that direct reclaim was not setting reclaim_idx but that&#39;s
been corrected.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/memcontrol.h b/include/linux/memcontrol.h</span>
<span class="p_header">index 9adb1bebb487..beee5be5d5d5 100644</span>
<span class="p_header">--- a/include/linux/memcontrol.h</span>
<span class="p_header">+++ b/include/linux/memcontrol.h</span>
<span class="p_chunk">@@ -306,7 +306,8 @@</span> <span class="p_context"> void mem_cgroup_uncharge_list(struct list_head *page_list);</span>
 
 void mem_cgroup_migrate(struct page *oldpage, struct page *newpage);
 
<span class="p_del">-struct lruvec *mem_cgroup_zone_lruvec(struct zone *, struct mem_cgroup *);</span>
<span class="p_add">+struct lruvec *mem_cgroup_lruvec(struct pglist_data *, struct zone *zone,</span>
<span class="p_add">+				 struct mem_cgroup *);</span>
 struct lruvec *mem_cgroup_page_lruvec(struct page *, struct pglist_data *);
 
 bool task_in_mem_cgroup(struct task_struct *task, struct mem_cgroup *memcg);
<span class="p_chunk">@@ -573,10 +574,10 @@</span> <span class="p_context"> static inline void mem_cgroup_migrate(struct page *old, struct page *new)</span>
 {
 }
 
<span class="p_del">-static inline struct lruvec *mem_cgroup_zone_lruvec(struct zone *zone,</span>
<span class="p_del">-						    struct mem_cgroup *memcg)</span>
<span class="p_add">+static inline struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
<span class="p_add">+				struct zone *zone, struct mem_cgroup *memcg)</span>
 {
<span class="p_del">-	return zone_lruvec(zone);</span>
<span class="p_add">+	return node_lruvec(pgdat);</span>
 }
 
 static inline struct lruvec *mem_cgroup_page_lruvec(struct page *page,
<span class="p_header">diff --git a/include/linux/mmzone.h b/include/linux/mmzone.h</span>
<span class="p_header">index 15ce454d0d59..16da23c095a0 100644</span>
<span class="p_header">--- a/include/linux/mmzone.h</span>
<span class="p_header">+++ b/include/linux/mmzone.h</span>
<span class="p_chunk">@@ -735,9 +735,9 @@</span> <span class="p_context"> static inline spinlock_t *zone_lru_lock(struct zone *zone)</span>
 	return &amp;zone-&gt;zone_pgdat-&gt;lru_lock;
 }
 
<span class="p_del">-static inline struct lruvec *zone_lruvec(struct zone *zone)</span>
<span class="p_add">+static inline struct lruvec *node_lruvec(struct pglist_data *pgdat)</span>
 {
<span class="p_del">-	return &amp;zone-&gt;zone_pgdat-&gt;lruvec;</span>
<span class="p_add">+	return &amp;pgdat-&gt;lruvec;</span>
 }
 
 static inline unsigned long pgdat_end_pfn(pg_data_t *pgdat)
<span class="p_header">diff --git a/include/linux/swap.h b/include/linux/swap.h</span>
<span class="p_header">index cc5c958c050b..575efae79a66 100644</span>
<span class="p_header">--- a/include/linux/swap.h</span>
<span class="p_header">+++ b/include/linux/swap.h</span>
<span class="p_chunk">@@ -324,7 +324,7 @@</span> <span class="p_context"> extern unsigned long try_to_free_mem_cgroup_pages(struct mem_cgroup *memcg,</span>
 						  unsigned long nr_pages,
 						  gfp_t gfp_mask,
 						  bool may_swap);
<span class="p_del">-extern unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *mem,</span>
<span class="p_add">+extern unsigned long mem_cgroup_shrink_node(struct mem_cgroup *mem,</span>
 						gfp_t gfp_mask, bool noswap,
 						struct zone *zone,
 						unsigned long *nr_scanned);
<span class="p_header">diff --git a/mm/memcontrol.c b/mm/memcontrol.c</span>
<span class="p_header">index 12f796e52e5e..cda2342af10e 100644</span>
<span class="p_header">--- a/mm/memcontrol.c</span>
<span class="p_header">+++ b/mm/memcontrol.c</span>
<span class="p_chunk">@@ -944,22 +944,23 @@</span> <span class="p_context"> static void invalidate_reclaim_iterators(struct mem_cgroup *dead_memcg)</span>
 	     iter = mem_cgroup_iter(NULL, iter, NULL))
 
 /**
<span class="p_del">- * mem_cgroup_zone_lruvec - get the lru list vector for a zone and memcg</span>
<span class="p_add">+ * mem_cgroup_lruvec - get the lru list vector for a node or a memcg zone</span>
<span class="p_add">+ * @node: node of the wanted lruvec</span>
  * @zone: zone of the wanted lruvec
  * @memcg: memcg of the wanted lruvec
  *
<span class="p_del">- * Returns the lru list vector holding pages for the given @zone and</span>
<span class="p_del">- * @mem.  This can be the global zone lruvec, if the memory controller</span>
<span class="p_add">+ * Returns the lru list vector holding pages for a given @node or a given</span>
<span class="p_add">+ * @memcg and @zone. This can be the node lruvec, if the memory controller</span>
  * is disabled.
  */
<span class="p_del">-struct lruvec *mem_cgroup_zone_lruvec(struct zone *zone,</span>
<span class="p_del">-				      struct mem_cgroup *memcg)</span>
<span class="p_add">+struct lruvec *mem_cgroup_lruvec(struct pglist_data *pgdat,</span>
<span class="p_add">+				 struct zone *zone, struct mem_cgroup *memcg)</span>
 {
 	struct mem_cgroup_per_zone *mz;
 	struct lruvec *lruvec;
 
 	if (mem_cgroup_disabled()) {
<span class="p_del">-		lruvec = zone_lruvec(zone);</span>
<span class="p_add">+		lruvec = node_lruvec(pgdat);</span>
 		goto out;
 	}
 
<span class="p_chunk">@@ -1473,8 +1474,8 @@</span> <span class="p_context"> static int mem_cgroup_soft_reclaim(struct mem_cgroup *root_memcg,</span>
 			}
 			continue;
 		}
<span class="p_del">-		total += mem_cgroup_shrink_node_zone(victim, gfp_mask, false,</span>
<span class="p_del">-						     zone, &amp;nr_scanned);</span>
<span class="p_add">+		total += mem_cgroup_shrink_node(victim, gfp_mask, false,</span>
<span class="p_add">+					zone, &amp;nr_scanned);</span>
 		*total_scanned += nr_scanned;
 		if (!soft_limit_excess(root_memcg))
 			break;
<span class="p_header">diff --git a/mm/page_alloc.c b/mm/page_alloc.c</span>
<span class="p_header">index d8cb483d5cad..fc5bad895732 100644</span>
<span class="p_header">--- a/mm/page_alloc.c</span>
<span class="p_header">+++ b/mm/page_alloc.c</span>
<span class="p_chunk">@@ -5905,6 +5905,7 @@</span> <span class="p_context"> static void __paginginit free_area_init_core(struct pglist_data *pgdat)</span>
 	init_waitqueue_head(&amp;pgdat-&gt;kcompactd_wait);
 #endif
 	pgdat_page_ext_init(pgdat);
<span class="p_add">+	lruvec_init(node_lruvec(pgdat));</span>
 
 	for (j = 0; j &lt; MAX_NR_ZONES; j++) {
 		struct zone *zone = pgdat-&gt;node_zones + j;
<span class="p_chunk">@@ -5968,7 +5969,6 @@</span> <span class="p_context"> static void __paginginit free_area_init_core(struct pglist_data *pgdat)</span>
 		/* For bootup, initialized properly in watermark setup */
 		mod_zone_page_state(zone, NR_ALLOC_BATCH, zone-&gt;managed_pages);
 
<span class="p_del">-		lruvec_init(zone_lruvec(zone));</span>
 		if (!size)
 			continue;
 
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index 6663fc75c3bc..93523944abb0 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -2196,10 +2196,11 @@</span> <span class="p_context"> static inline void init_tlb_ubc(void)</span>
 /*
  * This is a basic per-zone page freer.  Used by both kswapd and direct reclaim.
  */
<span class="p_del">-static void shrink_zone_memcg(struct zone *zone, struct mem_cgroup *memcg,</span>
<span class="p_add">+static void shrink_node_memcg(struct pglist_data *pgdat, struct mem_cgroup *memcg,</span>
 			      struct scan_control *sc, unsigned long *lru_pages)
 {
<span class="p_del">-	struct lruvec *lruvec = mem_cgroup_zone_lruvec(zone, memcg);</span>
<span class="p_add">+	struct zone *zone = &amp;pgdat-&gt;node_zones[sc-&gt;reclaim_idx];</span>
<span class="p_add">+	struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, zone, memcg);</span>
 	unsigned long nr[NR_LRU_LISTS];
 	unsigned long targets[NR_LRU_LISTS];
 	unsigned long nr_to_scan;
<span class="p_chunk">@@ -2332,13 +2333,14 @@</span> <span class="p_context"> static bool in_reclaim_compaction(struct scan_control *sc)</span>
  * calls try_to_compact_zone() that it will have enough free pages to succeed.
  * It will give up earlier than that if there is difficulty reclaiming pages.
  */
<span class="p_del">-static inline bool should_continue_reclaim(struct zone *zone,</span>
<span class="p_add">+static inline bool should_continue_reclaim(struct pglist_data *pgdat,</span>
 					unsigned long nr_reclaimed,
 					unsigned long nr_scanned,
 					struct scan_control *sc)
 {
 	unsigned long pages_for_compaction;
 	unsigned long inactive_lru_pages;
<span class="p_add">+	int z;</span>
 
 	/* If not in reclaim/compaction mode, stop */
 	if (!in_reclaim_compaction(sc))
<span class="p_chunk">@@ -2372,21 +2374,27 @@</span> <span class="p_context"> static inline bool should_continue_reclaim(struct zone *zone,</span>
 	 * inactive lists are large enough, continue reclaiming
 	 */
 	pages_for_compaction = (2UL &lt;&lt; sc-&gt;order);
<span class="p_del">-	inactive_lru_pages = node_page_state(zone-&gt;zone_pgdat, NR_INACTIVE_FILE);</span>
<span class="p_add">+	inactive_lru_pages = node_page_state(pgdat, NR_INACTIVE_FILE);</span>
 	if (get_nr_swap_pages() &gt; 0)
<span class="p_del">-		inactive_lru_pages += node_page_state(zone-&gt;zone_pgdat, NR_INACTIVE_ANON);</span>
<span class="p_add">+		inactive_lru_pages += node_page_state(pgdat, NR_INACTIVE_ANON);</span>
 	if (sc-&gt;nr_reclaimed &lt; pages_for_compaction &amp;&amp;
 			inactive_lru_pages &gt; pages_for_compaction)
 		return true;
 
 	/* If compaction would go ahead or the allocation would succeed, stop */
<span class="p_del">-	switch (compaction_suitable(zone, sc-&gt;order, 0, 0)) {</span>
<span class="p_del">-	case COMPACT_PARTIAL:</span>
<span class="p_del">-	case COMPACT_CONTINUE:</span>
<span class="p_del">-		return false;</span>
<span class="p_del">-	default:</span>
<span class="p_del">-		return true;</span>
<span class="p_add">+	for (z = 0; z &lt;= sc-&gt;reclaim_idx; z++) {</span>
<span class="p_add">+		struct zone *zone = &amp;pgdat-&gt;node_zones[z];</span>
<span class="p_add">+</span>
<span class="p_add">+		switch (compaction_suitable(zone, sc-&gt;order, 0, 0)) {</span>
<span class="p_add">+		case COMPACT_PARTIAL:</span>
<span class="p_add">+		case COMPACT_CONTINUE:</span>
<span class="p_add">+			return false;</span>
<span class="p_add">+		default:</span>
<span class="p_add">+			/* check next zone */</span>
<span class="p_add">+			;</span>
<span class="p_add">+		}</span>
 	}
<span class="p_add">+	return true;</span>
 }
 
 static bool shrink_node(pg_data_t *pgdat, struct scan_control *sc,
<span class="p_chunk">@@ -2395,15 +2403,14 @@</span> <span class="p_context"> static bool shrink_node(pg_data_t *pgdat, struct scan_control *sc,</span>
 	struct reclaim_state *reclaim_state = current-&gt;reclaim_state;
 	unsigned long nr_reclaimed, nr_scanned;
 	bool reclaimable = false;
<span class="p_del">-	struct zone *zone = &amp;pgdat-&gt;node_zones[classzone_idx];</span>
 
 	do {
 		struct mem_cgroup *root = sc-&gt;target_mem_cgroup;
 		struct mem_cgroup_reclaim_cookie reclaim = {
<span class="p_del">-			.zone = zone,</span>
<span class="p_add">+			.zone = &amp;pgdat-&gt;node_zones[classzone_idx],</span>
 			.priority = sc-&gt;priority,
 		};
<span class="p_del">-		unsigned long zone_lru_pages = 0;</span>
<span class="p_add">+		unsigned long node_lru_pages = 0;</span>
 		struct mem_cgroup *memcg;
 
 		nr_reclaimed = sc-&gt;nr_reclaimed;
<span class="p_chunk">@@ -2424,11 +2431,11 @@</span> <span class="p_context"> static bool shrink_node(pg_data_t *pgdat, struct scan_control *sc,</span>
 			reclaimed = sc-&gt;nr_reclaimed;
 			scanned = sc-&gt;nr_scanned;
 
<span class="p_del">-			shrink_zone_memcg(zone, memcg, sc, &amp;lru_pages);</span>
<span class="p_del">-			zone_lru_pages += lru_pages;</span>
<span class="p_add">+			shrink_node_memcg(pgdat, memcg, sc, &amp;lru_pages);</span>
<span class="p_add">+			node_lru_pages += lru_pages;</span>
 
 			if (!global_reclaim(sc) &amp;&amp; sc-&gt;reclaim_idx == classzone_idx)
<span class="p_del">-				shrink_slab(sc-&gt;gfp_mask, zone_to_nid(zone),</span>
<span class="p_add">+				shrink_slab(sc-&gt;gfp_mask, pgdat-&gt;node_id,</span>
 					    memcg, sc-&gt;nr_scanned - scanned,
 					    lru_pages);
 
<span class="p_chunk">@@ -2440,7 +2447,7 @@</span> <span class="p_context"> static bool shrink_node(pg_data_t *pgdat, struct scan_control *sc,</span>
 			/*
 			 * Direct reclaim and kswapd have to scan all memory
 			 * cgroups to fulfill the overall scan target for the
<span class="p_del">-			 * zone.</span>
<span class="p_add">+			 * node.</span>
 			 *
 			 * Limit reclaim, on the other hand, only cares about
 			 * nr_to_reclaim pages to be reclaimed and it will
<span class="p_chunk">@@ -2459,9 +2466,9 @@</span> <span class="p_context"> static bool shrink_node(pg_data_t *pgdat, struct scan_control *sc,</span>
 		 * the eligible LRU pages were scanned.
 		 */
 		if (global_reclaim(sc) &amp;&amp; sc-&gt;reclaim_idx == classzone_idx)
<span class="p_del">-			shrink_slab(sc-&gt;gfp_mask, zone_to_nid(zone), NULL,</span>
<span class="p_add">+			shrink_slab(sc-&gt;gfp_mask, pgdat-&gt;node_id, NULL,</span>
 				    sc-&gt;nr_scanned - nr_scanned,
<span class="p_del">-				    zone_lru_pages);</span>
<span class="p_add">+				    node_lru_pages);</span>
 
 		if (reclaim_state) {
 			sc-&gt;nr_reclaimed += reclaim_state-&gt;reclaimed_slab;
<span class="p_chunk">@@ -2476,7 +2483,7 @@</span> <span class="p_context"> static bool shrink_node(pg_data_t *pgdat, struct scan_control *sc,</span>
 		if (sc-&gt;nr_reclaimed - nr_reclaimed)
 			reclaimable = true;
 
<span class="p_del">-	} while (should_continue_reclaim(zone, sc-&gt;nr_reclaimed - nr_reclaimed,</span>
<span class="p_add">+	} while (should_continue_reclaim(pgdat, sc-&gt;nr_reclaimed - nr_reclaimed,</span>
 					 sc-&gt;nr_scanned - nr_scanned, sc));
 
 	return reclaimable;
<span class="p_chunk">@@ -2866,7 +2873,7 @@</span> <span class="p_context"> unsigned long try_to_free_pages(struct zonelist *zonelist, int order,</span>
 
 #ifdef CONFIG_MEMCG
 
<span class="p_del">-unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *memcg,</span>
<span class="p_add">+unsigned long mem_cgroup_shrink_node(struct mem_cgroup *memcg,</span>
 						gfp_t gfp_mask, bool noswap,
 						struct zone *zone,
 						unsigned long *nr_scanned)
<span class="p_chunk">@@ -2876,6 +2883,7 @@</span> <span class="p_context"> unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *memcg,</span>
 		.target_mem_cgroup = memcg,
 		.may_writepage = !laptop_mode,
 		.may_unmap = 1,
<span class="p_add">+		.reclaim_idx = zone_idx(zone),</span>
 		.may_swap = !noswap,
 	};
 	unsigned long lru_pages;
<span class="p_chunk">@@ -2890,11 +2898,11 @@</span> <span class="p_context"> unsigned long mem_cgroup_shrink_node_zone(struct mem_cgroup *memcg,</span>
 	/*
 	 * NOTE: Although we can get the priority field, using it
 	 * here is not a good idea, since it limits the pages we can scan.
<span class="p_del">-	 * if we don&#39;t reclaim here, the shrink_zone from balance_pgdat</span>
<span class="p_add">+	 * if we don&#39;t reclaim here, the shrink_node from balance_pgdat</span>
 	 * will pick up pages from other mem cgroup&#39;s as well. We hack
 	 * the priority and make it zero.
 	 */
<span class="p_del">-	shrink_zone_memcg(zone, memcg, &amp;sc, &amp;lru_pages);</span>
<span class="p_add">+	shrink_node_memcg(zone-&gt;zone_pgdat, memcg, &amp;sc, &amp;lru_pages);</span>
 
 	trace_mm_vmscan_memcg_softlimit_reclaim_end(sc.nr_reclaimed);
 
<span class="p_chunk">@@ -2952,7 +2960,7 @@</span> <span class="p_context"> static void age_active_anon(struct pglist_data *pgdat,</span>
 
 	memcg = mem_cgroup_iter(NULL, NULL, NULL);
 	do {
<span class="p_del">-		struct lruvec *lruvec = mem_cgroup_zone_lruvec(zone, memcg);</span>
<span class="p_add">+		struct lruvec *lruvec = mem_cgroup_lruvec(pgdat, zone, memcg);</span>
 
 		if (inactive_list_is_low(lruvec, false))
 			shrink_active_list(SWAP_CLUSTER_MAX, lruvec,
<span class="p_header">diff --git a/mm/workingset.c b/mm/workingset.c</span>
<span class="p_header">index c0820e06aaff..10ddf707782a 100644</span>
<span class="p_header">--- a/mm/workingset.c</span>
<span class="p_header">+++ b/mm/workingset.c</span>
<span class="p_chunk">@@ -218,7 +218,7 @@</span> <span class="p_context"> void *workingset_eviction(struct address_space *mapping, struct page *page)</span>
 	VM_BUG_ON_PAGE(page_count(page), page);
 	VM_BUG_ON_PAGE(!PageLocked(page), page);
 
<span class="p_del">-	lruvec = mem_cgroup_zone_lruvec(zone, memcg);</span>
<span class="p_add">+	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, zone, memcg);</span>
 	eviction = atomic_long_inc_return(&amp;lruvec-&gt;inactive_age);
 	return pack_shadow(memcgid, zone, eviction);
 }
<span class="p_chunk">@@ -267,7 +267,7 @@</span> <span class="p_context"> bool workingset_refault(void *shadow)</span>
 		rcu_read_unlock();
 		return false;
 	}
<span class="p_del">-	lruvec = mem_cgroup_zone_lruvec(zone, memcg);</span>
<span class="p_add">+	lruvec = mem_cgroup_lruvec(zone-&gt;zone_pgdat, zone, memcg);</span>
 	refault = atomic_long_read(&amp;lruvec-&gt;inactive_age);
 	active_file = lruvec_lru_size(lruvec, LRU_ACTIVE_FILE);
 	rcu_read_unlock();
<span class="p_chunk">@@ -317,7 +317,7 @@</span> <span class="p_context"> void workingset_activation(struct page *page)</span>
 	 */
 	if (!mem_cgroup_disabled() &amp;&amp; !page_memcg(page))
 		goto out;
<span class="p_del">-	lruvec = mem_cgroup_zone_lruvec(page_zone(page), page_memcg(page));</span>
<span class="p_add">+	lruvec = mem_cgroup_lruvec(page_zone(page)-&gt;zone_pgdat, page_zone(page), page_memcg(page));</span>
 	atomic_long_inc(&amp;lruvec-&gt;inactive_age);
 out:
 	unlock_page_memcg(page);

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



