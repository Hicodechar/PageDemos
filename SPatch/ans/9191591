
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v7,2/9] x86/mm: Update physical mapping variable names (x86_64) - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v7,2/9] x86/mm: Update physical mapping variable names (x86_64)</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=38541">Kees Cook</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 22, 2016, 12:46 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1466556426-32664-3-git-send-email-keescook@chromium.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9191591/mbox/"
   >mbox</a>
|
   <a href="/patch/9191591/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9191591/">/patch/9191591/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	2AB866075A for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 22 Jun 2016 00:47:35 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 17D622837F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 22 Jun 2016 00:47:35 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 0C2592838C; Wed, 22 Jun 2016 00:47:35 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-7.0 required=2.0 tests=BAYES_00,DKIM_SIGNED,
	DKIM_VALID, DKIM_VALID_AU,
	RCVD_IN_DNSWL_HI autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 50EE828383
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 22 Jun 2016 00:47:34 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751951AbcFVArT (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 21 Jun 2016 20:47:19 -0400
Received: from mail-pa0-f41.google.com ([209.85.220.41]:36533 &quot;EHLO
	mail-pa0-f41.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751464AbcFVArP (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 21 Jun 2016 20:47:15 -0400
Received: by mail-pa0-f41.google.com with SMTP id wo6so11116132pac.3
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Tue, 21 Jun 2016 17:47:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=chromium.org; s=google;
	h=from:to:cc:subject:date:message-id:in-reply-to:references;
	bh=s9h6rgEaYNKnycKj7B4qTh/T5Na8eEx5yBMR78fym+c=;
	b=fvPOykDuEE2/U9MrA/oklVRTGrmenpBO2KW/nuFFccv8CmEiv1qGL5uCcNpS85KVw3
	ffe2xPVKBZXSla+6Bm/poRtAwBKa/AOKZaHhCukPWZmHNSXIV/alIbv4vCSROBMyD8VO
	iGtYXY3uEn79eIpipcm+b4FREuTeqzFstXSYk=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=1e100.net; s=20130820;
	h=x-gm-message-state:from:to:cc:subject:date:message-id:in-reply-to
	:references;
	bh=s9h6rgEaYNKnycKj7B4qTh/T5Na8eEx5yBMR78fym+c=;
	b=k/OSbS3PjBOx3yU/ivPEIIP+e/Op3qL/dT8w8K31c3ZUNtvy8TluyLCOxENWHp2pQm
	d5ipPwbTCI5jw3ZRFfAoSe2VJf2EZTJeWmm6/bJIPBCYEVfC73kaEPgQ/jlgl7vqE40E
	so/bigcVpSzwUCtWBE88zBSdKQg5Fm0EFBjSlBb78Ei/GJTVILVPP5WKJaJHKIDSovYl
	/62IK1Vpk/BHLFs/SYWGxyTWuD7MJ7uMp6+NJ0TuwwpwCAS+cehG7VJZ/oE8ALVbAlcS
	yyfz2oKBRx6X1HGzDUZdX7sI4nFi7iBZqCL5t/6Hc5VWEZBI9eqCwfdbql12InnjiRjc
	SrhA==
X-Gm-Message-State: ALyK8tJtxPmix55PsjqkNX1+mhh8PnvlgZr7ml/GpdC2xVCei1yU5ktyPxm1u/D/ILzcS0WV
X-Received: by 10.66.151.140 with SMTP id uq12mr18698746pab.10.1466556434780;
	Tue, 21 Jun 2016 17:47:14 -0700 (PDT)
Received: from www.outflux.net
	(173-164-112-133-Oregon.hfc.comcastbusiness.net. [173.164.112.133])
	by smtp.gmail.com with ESMTPSA id
	vy8sm98779977pab.22.2016.06.21.17.47.13
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Tue, 21 Jun 2016 17:47:13 -0700 (PDT)
From: Kees Cook &lt;keescook@chromium.org&gt;
To: Ingo Molnar &lt;mingo@kernel.org&gt;
Cc: Kees Cook &lt;keescook@chromium.org&gt;, Thomas Garnier &lt;thgarnie@google.com&gt;,
	Andy Lutomirski &lt;luto@kernel.org&gt;, x86@kernel.org,
	Borislav Petkov &lt;bp@suse.de&gt;, Baoquan He &lt;bhe@redhat.com&gt;,
	Yinghai Lu &lt;yinghai@kernel.org&gt;, Juergen Gross &lt;jgross@suse.com&gt;,
	Matt Fleming &lt;matt@codeblueprint.co.uk&gt;, Toshi Kani &lt;toshi.kani@hpe.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Dan Williams &lt;dan.j.williams@intel.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;,
	Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Xiao Guangrong &lt;guangrong.xiao@linux.intel.com&gt;,
	Martin Schwidefsky &lt;schwidefsky@de.ibm.com&gt;,
	&quot;Aneesh Kumar K.V&quot; &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Alexander Kuleshov &lt;kuleshovmail@gmail.com&gt;,
	Alexander Popov &lt;alpopov@ptsecurity.com&gt;,
	Dave Young &lt;dyoung@redhat.com&gt;, Joerg Roedel &lt;jroedel@suse.de&gt;,
	Lv Zheng &lt;lv.zheng@intel.com&gt;, Mark Salter &lt;msalter@redhat.com&gt;,
	Dmitry Vyukov &lt;dvyukov@google.com&gt;, Stephen Smalley &lt;sds@tycho.nsa.gov&gt;,
	Boris Ostrovsky &lt;boris.ostrovsky@oracle.com&gt;,
	Christian Borntraeger &lt;borntraeger@de.ibm.com&gt;,
	Jan Beulich &lt;JBeulich@suse.com&gt;, linux-kernel@vger.kernel.org,
	Jonathan Corbet &lt;corbet@lwn.net&gt;, linux-doc@vger.kernel.org,
	kernel-hardening@lists.openwall.com
Subject: [PATCH v7 2/9] x86/mm: Update physical mapping variable names
	(x86_64)
Date: Tue, 21 Jun 2016 17:46:59 -0700
Message-Id: &lt;1466556426-32664-3-git-send-email-keescook@chromium.org&gt;
X-Mailer: git-send-email 2.7.4
In-Reply-To: &lt;1466556426-32664-1-git-send-email-keescook@chromium.org&gt;
References: &lt;1466556426-32664-1-git-send-email-keescook@chromium.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=38541">Kees Cook</a> - June 22, 2016, 12:46 a.m.</div>
<pre class="content">
<span class="from">From: Thomas Garnier &lt;thgarnie@google.com&gt;</span>

Change the variable names on kernel_physical_mapping_init and related
functions to correctly reflect physical and virtual memory addresses.
Also add comments on each function to describe usage and alignment
constraints.
<span class="signed-off-by">
Signed-off-by: Thomas Garnier &lt;thgarnie@google.com&gt;</span>
<span class="signed-off-by">Signed-off-by: Kees Cook &lt;keescook@chromium.org&gt;</span>
---
 arch/x86/mm/init_64.c | 162 ++++++++++++++++++++++++++++++--------------------
 1 file changed, 96 insertions(+), 66 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/mm/init_64.c b/arch/x86/mm/init_64.c</span>
<span class="p_header">index bce2e5d9edd4..6714712bd5da 100644</span>
<span class="p_header">--- a/arch/x86/mm/init_64.c</span>
<span class="p_header">+++ b/arch/x86/mm/init_64.c</span>
<span class="p_chunk">@@ -328,22 +328,30 @@</span> <span class="p_context"> void __init cleanup_highmap(void)</span>
 	}
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Create PTE level page table mapping for physical addresses.</span>
<span class="p_add">+ * It returns the last physical address mapped.</span>
<span class="p_add">+ */</span>
 static unsigned long __meminit
<span class="p_del">-phys_pte_init(pte_t *pte_page, unsigned long addr, unsigned long end,</span>
<span class="p_add">+phys_pte_init(pte_t *pte_page, unsigned long paddr, unsigned long paddr_end,</span>
 	      pgprot_t prot)
 {
<span class="p_del">-	unsigned long pages = 0, next;</span>
<span class="p_del">-	unsigned long last_map_addr = end;</span>
<span class="p_add">+	unsigned long pages = 0, paddr_next;</span>
<span class="p_add">+	unsigned long paddr_last = paddr_end;</span>
<span class="p_add">+	pte_t *pte;</span>
 	int i;
 
<span class="p_del">-	pte_t *pte = pte_page + pte_index(addr);</span>
<span class="p_add">+	pte = pte_page + pte_index(paddr);</span>
<span class="p_add">+	i = pte_index(paddr);</span>
 
<span class="p_del">-	for (i = pte_index(addr); i &lt; PTRS_PER_PTE; i++, addr = next, pte++) {</span>
<span class="p_del">-		next = (addr &amp; PAGE_MASK) + PAGE_SIZE;</span>
<span class="p_del">-		if (addr &gt;= end) {</span>
<span class="p_add">+	for (; i &lt; PTRS_PER_PTE; i++, paddr = paddr_next, pte++) {</span>
<span class="p_add">+		paddr_next = (paddr &amp; PAGE_MASK) + PAGE_SIZE;</span>
<span class="p_add">+		if (paddr &gt;= paddr_end) {</span>
 			if (!after_bootmem &amp;&amp;
<span class="p_del">-			    !e820_any_mapped(addr &amp; PAGE_MASK, next, E820_RAM) &amp;&amp;</span>
<span class="p_del">-			    !e820_any_mapped(addr &amp; PAGE_MASK, next, E820_RESERVED_KERN))</span>
<span class="p_add">+			    !e820_any_mapped(paddr &amp; PAGE_MASK, paddr_next,</span>
<span class="p_add">+					     E820_RAM) &amp;&amp;</span>
<span class="p_add">+			    !e820_any_mapped(paddr &amp; PAGE_MASK, paddr_next,</span>
<span class="p_add">+					     E820_RESERVED_KERN))</span>
 				set_pte(pte, __pte(0));
 			continue;
 		}
<span class="p_chunk">@@ -361,37 +369,44 @@</span> <span class="p_context"> phys_pte_init(pte_t *pte_page, unsigned long addr, unsigned long end,</span>
 		}
 
 		if (0)
<span class="p_del">-			printk(&quot;   pte=%p addr=%lx pte=%016lx\n&quot;,</span>
<span class="p_del">-			       pte, addr, pfn_pte(addr &gt;&gt; PAGE_SHIFT, PAGE_KERNEL).pte);</span>
<span class="p_add">+			pr_info(&quot;   pte=%p addr=%lx pte=%016lx\n&quot;, pte, paddr,</span>
<span class="p_add">+				pfn_pte(paddr &gt;&gt; PAGE_SHIFT, PAGE_KERNEL).pte);</span>
 		pages++;
<span class="p_del">-		set_pte(pte, pfn_pte(addr &gt;&gt; PAGE_SHIFT, prot));</span>
<span class="p_del">-		last_map_addr = (addr &amp; PAGE_MASK) + PAGE_SIZE;</span>
<span class="p_add">+		set_pte(pte, pfn_pte(paddr &gt;&gt; PAGE_SHIFT, prot));</span>
<span class="p_add">+		paddr_last = (paddr &amp; PAGE_MASK) + PAGE_SIZE;</span>
 	}
 
 	update_page_count(PG_LEVEL_4K, pages);
 
<span class="p_del">-	return last_map_addr;</span>
<span class="p_add">+	return paddr_last;</span>
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Create PMD level page table mapping for physical addresses. The virtual</span>
<span class="p_add">+ * and physical address have to be aligned at this level.</span>
<span class="p_add">+ * It returns the last physical address mapped.</span>
<span class="p_add">+ */</span>
 static unsigned long __meminit
<span class="p_del">-phys_pmd_init(pmd_t *pmd_page, unsigned long address, unsigned long end,</span>
<span class="p_add">+phys_pmd_init(pmd_t *pmd_page, unsigned long paddr, unsigned long paddr_end,</span>
 	      unsigned long page_size_mask, pgprot_t prot)
 {
<span class="p_del">-	unsigned long pages = 0, next;</span>
<span class="p_del">-	unsigned long last_map_addr = end;</span>
<span class="p_add">+	unsigned long pages = 0, paddr_next;</span>
<span class="p_add">+	unsigned long paddr_last = paddr_end;</span>
 
<span class="p_del">-	int i = pmd_index(address);</span>
<span class="p_add">+	int i = pmd_index(paddr);</span>
 
<span class="p_del">-	for (; i &lt; PTRS_PER_PMD; i++, address = next) {</span>
<span class="p_del">-		pmd_t *pmd = pmd_page + pmd_index(address);</span>
<span class="p_add">+	for (; i &lt; PTRS_PER_PMD; i++, paddr = paddr_next) {</span>
<span class="p_add">+		pmd_t *pmd = pmd_page + pmd_index(paddr);</span>
 		pte_t *pte;
 		pgprot_t new_prot = prot;
 
<span class="p_del">-		next = (address &amp; PMD_MASK) + PMD_SIZE;</span>
<span class="p_del">-		if (address &gt;= end) {</span>
<span class="p_add">+		paddr_next = (paddr &amp; PMD_MASK) + PMD_SIZE;</span>
<span class="p_add">+		if (paddr &gt;= paddr_end) {</span>
 			if (!after_bootmem &amp;&amp;
<span class="p_del">-			    !e820_any_mapped(address &amp; PMD_MASK, next, E820_RAM) &amp;&amp;</span>
<span class="p_del">-			    !e820_any_mapped(address &amp; PMD_MASK, next, E820_RESERVED_KERN))</span>
<span class="p_add">+			    !e820_any_mapped(paddr &amp; PMD_MASK, paddr_next,</span>
<span class="p_add">+					     E820_RAM) &amp;&amp;</span>
<span class="p_add">+			    !e820_any_mapped(paddr &amp; PMD_MASK, paddr_next,</span>
<span class="p_add">+					     E820_RESERVED_KERN))</span>
 				set_pmd(pmd, __pmd(0));
 			continue;
 		}
<span class="p_chunk">@@ -400,8 +415,8 @@</span> <span class="p_context"> phys_pmd_init(pmd_t *pmd_page, unsigned long address, unsigned long end,</span>
 			if (!pmd_large(*pmd)) {
 				spin_lock(&amp;init_mm.page_table_lock);
 				pte = (pte_t *)pmd_page_vaddr(*pmd);
<span class="p_del">-				last_map_addr = phys_pte_init(pte, address,</span>
<span class="p_del">-								end, prot);</span>
<span class="p_add">+				paddr_last = phys_pte_init(pte, paddr,</span>
<span class="p_add">+							   paddr_end, prot);</span>
 				spin_unlock(&amp;init_mm.page_table_lock);
 				continue;
 			}
<span class="p_chunk">@@ -420,7 +435,7 @@</span> <span class="p_context"> phys_pmd_init(pmd_t *pmd_page, unsigned long address, unsigned long end,</span>
 			if (page_size_mask &amp; (1 &lt;&lt; PG_LEVEL_2M)) {
 				if (!after_bootmem)
 					pages++;
<span class="p_del">-				last_map_addr = next;</span>
<span class="p_add">+				paddr_last = paddr_next;</span>
 				continue;
 			}
 			new_prot = pte_pgprot(pte_clrhuge(*(pte_t *)pmd));
<span class="p_chunk">@@ -430,42 +445,49 @@</span> <span class="p_context"> phys_pmd_init(pmd_t *pmd_page, unsigned long address, unsigned long end,</span>
 			pages++;
 			spin_lock(&amp;init_mm.page_table_lock);
 			set_pte((pte_t *)pmd,
<span class="p_del">-				pfn_pte((address &amp; PMD_MASK) &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+				pfn_pte((paddr &amp; PMD_MASK) &gt;&gt; PAGE_SHIFT,</span>
 					__pgprot(pgprot_val(prot) | _PAGE_PSE)));
 			spin_unlock(&amp;init_mm.page_table_lock);
<span class="p_del">-			last_map_addr = next;</span>
<span class="p_add">+			paddr_last = paddr_next;</span>
 			continue;
 		}
 
 		pte = alloc_low_page();
<span class="p_del">-		last_map_addr = phys_pte_init(pte, address, end, new_prot);</span>
<span class="p_add">+		paddr_last = phys_pte_init(pte, paddr, paddr_end, new_prot);</span>
 
 		spin_lock(&amp;init_mm.page_table_lock);
 		pmd_populate_kernel(&amp;init_mm, pmd, pte);
 		spin_unlock(&amp;init_mm.page_table_lock);
 	}
 	update_page_count(PG_LEVEL_2M, pages);
<span class="p_del">-	return last_map_addr;</span>
<span class="p_add">+	return paddr_last;</span>
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Create PUD level page table mapping for physical addresses. The virtual</span>
<span class="p_add">+ * and physical address have to be aligned at this level.</span>
<span class="p_add">+ * It returns the last physical address mapped.</span>
<span class="p_add">+ */</span>
 static unsigned long __meminit
<span class="p_del">-phys_pud_init(pud_t *pud_page, unsigned long addr, unsigned long end,</span>
<span class="p_del">-			 unsigned long page_size_mask)</span>
<span class="p_add">+phys_pud_init(pud_t *pud_page, unsigned long paddr, unsigned long paddr_end,</span>
<span class="p_add">+	      unsigned long page_size_mask)</span>
 {
<span class="p_del">-	unsigned long pages = 0, next;</span>
<span class="p_del">-	unsigned long last_map_addr = end;</span>
<span class="p_del">-	int i = pud_index(addr);</span>
<span class="p_add">+	unsigned long pages = 0, paddr_next;</span>
<span class="p_add">+	unsigned long paddr_last = paddr_end;</span>
<span class="p_add">+	int i = pud_index(paddr);</span>
 
<span class="p_del">-	for (; i &lt; PTRS_PER_PUD; i++, addr = next) {</span>
<span class="p_del">-		pud_t *pud = pud_page + pud_index(addr);</span>
<span class="p_add">+	for (; i &lt; PTRS_PER_PUD; i++, paddr = paddr_next) {</span>
<span class="p_add">+		pud_t *pud = pud_page + pud_index(paddr);</span>
 		pmd_t *pmd;
 		pgprot_t prot = PAGE_KERNEL;
 
<span class="p_del">-		next = (addr &amp; PUD_MASK) + PUD_SIZE;</span>
<span class="p_del">-		if (addr &gt;= end) {</span>
<span class="p_add">+		paddr_next = (paddr &amp; PUD_MASK) + PUD_SIZE;</span>
<span class="p_add">+		if (paddr &gt;= paddr_end) {</span>
 			if (!after_bootmem &amp;&amp;
<span class="p_del">-			    !e820_any_mapped(addr &amp; PUD_MASK, next, E820_RAM) &amp;&amp;</span>
<span class="p_del">-			    !e820_any_mapped(addr &amp; PUD_MASK, next, E820_RESERVED_KERN))</span>
<span class="p_add">+			    !e820_any_mapped(paddr &amp; PUD_MASK, paddr_next,</span>
<span class="p_add">+					     E820_RAM) &amp;&amp;</span>
<span class="p_add">+			    !e820_any_mapped(paddr &amp; PUD_MASK, paddr_next,</span>
<span class="p_add">+					     E820_RESERVED_KERN))</span>
 				set_pud(pud, __pud(0));
 			continue;
 		}
<span class="p_chunk">@@ -473,8 +495,10 @@</span> <span class="p_context"> phys_pud_init(pud_t *pud_page, unsigned long addr, unsigned long end,</span>
 		if (pud_val(*pud)) {
 			if (!pud_large(*pud)) {
 				pmd = pmd_offset(pud, 0);
<span class="p_del">-				last_map_addr = phys_pmd_init(pmd, addr, end,</span>
<span class="p_del">-							 page_size_mask, prot);</span>
<span class="p_add">+				paddr_last = phys_pmd_init(pmd, paddr,</span>
<span class="p_add">+							   paddr_end,</span>
<span class="p_add">+							   page_size_mask,</span>
<span class="p_add">+							   prot);</span>
 				__flush_tlb_all();
 				continue;
 			}
<span class="p_chunk">@@ -493,7 +517,7 @@</span> <span class="p_context"> phys_pud_init(pud_t *pud_page, unsigned long addr, unsigned long end,</span>
 			if (page_size_mask &amp; (1 &lt;&lt; PG_LEVEL_1G)) {
 				if (!after_bootmem)
 					pages++;
<span class="p_del">-				last_map_addr = next;</span>
<span class="p_add">+				paddr_last = paddr_next;</span>
 				continue;
 			}
 			prot = pte_pgprot(pte_clrhuge(*(pte_t *)pud));
<span class="p_chunk">@@ -503,16 +527,16 @@</span> <span class="p_context"> phys_pud_init(pud_t *pud_page, unsigned long addr, unsigned long end,</span>
 			pages++;
 			spin_lock(&amp;init_mm.page_table_lock);
 			set_pte((pte_t *)pud,
<span class="p_del">-				pfn_pte((addr &amp; PUD_MASK) &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+				pfn_pte((paddr &amp; PUD_MASK) &gt;&gt; PAGE_SHIFT,</span>
 					PAGE_KERNEL_LARGE));
 			spin_unlock(&amp;init_mm.page_table_lock);
<span class="p_del">-			last_map_addr = next;</span>
<span class="p_add">+			paddr_last = paddr_next;</span>
 			continue;
 		}
 
 		pmd = alloc_low_page();
<span class="p_del">-		last_map_addr = phys_pmd_init(pmd, addr, end, page_size_mask,</span>
<span class="p_del">-					      prot);</span>
<span class="p_add">+		paddr_last = phys_pmd_init(pmd, paddr, paddr_end,</span>
<span class="p_add">+					   page_size_mask, prot);</span>
 
 		spin_lock(&amp;init_mm.page_table_lock);
 		pud_populate(&amp;init_mm, pud, pmd);
<span class="p_chunk">@@ -522,38 +546,44 @@</span> <span class="p_context"> phys_pud_init(pud_t *pud_page, unsigned long addr, unsigned long end,</span>
 
 	update_page_count(PG_LEVEL_1G, pages);
 
<span class="p_del">-	return last_map_addr;</span>
<span class="p_add">+	return paddr_last;</span>
 }
 
<span class="p_add">+/*</span>
<span class="p_add">+ * Create page table mapping for the physical memory for specific physical</span>
<span class="p_add">+ * addresses. The virtual and physical addresses have to be aligned on PUD level</span>
<span class="p_add">+ * down. It returns the last physical address mapped.</span>
<span class="p_add">+ */</span>
 unsigned long __meminit
<span class="p_del">-kernel_physical_mapping_init(unsigned long start,</span>
<span class="p_del">-			     unsigned long end,</span>
<span class="p_add">+kernel_physical_mapping_init(unsigned long paddr_start,</span>
<span class="p_add">+			     unsigned long paddr_end,</span>
 			     unsigned long page_size_mask)
 {
 	bool pgd_changed = false;
<span class="p_del">-	unsigned long next, last_map_addr = end;</span>
<span class="p_del">-	unsigned long addr;</span>
<span class="p_add">+	unsigned long vaddr, vaddr_start, vaddr_end, vaddr_next, paddr_last;</span>
 
<span class="p_del">-	start = (unsigned long)__va(start);</span>
<span class="p_del">-	end = (unsigned long)__va(end);</span>
<span class="p_del">-	addr = start;</span>
<span class="p_add">+	paddr_last = paddr_end;</span>
<span class="p_add">+	vaddr = (unsigned long)__va(paddr_start);</span>
<span class="p_add">+	vaddr_end = (unsigned long)__va(paddr_end);</span>
<span class="p_add">+	vaddr_start = vaddr;</span>
 
<span class="p_del">-	for (; start &lt; end; start = next) {</span>
<span class="p_del">-		pgd_t *pgd = pgd_offset_k(start);</span>
<span class="p_add">+	for (; vaddr &lt; vaddr_end; vaddr = vaddr_next) {</span>
<span class="p_add">+		pgd_t *pgd = pgd_offset_k(vaddr);</span>
 		pud_t *pud;
 
<span class="p_del">-		next = (start &amp; PGDIR_MASK) + PGDIR_SIZE;</span>
<span class="p_add">+		vaddr_next = (vaddr &amp; PGDIR_MASK) + PGDIR_SIZE;</span>
 
 		if (pgd_val(*pgd)) {
 			pud = (pud_t *)pgd_page_vaddr(*pgd);
<span class="p_del">-			last_map_addr = phys_pud_init(pud, __pa(start),</span>
<span class="p_del">-						 __pa(end), page_size_mask);</span>
<span class="p_add">+			paddr_last = phys_pud_init(pud, __pa(vaddr),</span>
<span class="p_add">+						   __pa(vaddr_end),</span>
<span class="p_add">+						   page_size_mask);</span>
 			continue;
 		}
 
 		pud = alloc_low_page();
<span class="p_del">-		last_map_addr = phys_pud_init(pud, __pa(start), __pa(end),</span>
<span class="p_del">-						 page_size_mask);</span>
<span class="p_add">+		paddr_last = phys_pud_init(pud, __pa(vaddr), __pa(vaddr_end),</span>
<span class="p_add">+					   page_size_mask);</span>
 
 		spin_lock(&amp;init_mm.page_table_lock);
 		pgd_populate(&amp;init_mm, pgd, pud);
<span class="p_chunk">@@ -562,11 +592,11 @@</span> <span class="p_context"> kernel_physical_mapping_init(unsigned long start,</span>
 	}
 
 	if (pgd_changed)
<span class="p_del">-		sync_global_pgds(addr, end - 1, 0);</span>
<span class="p_add">+		sync_global_pgds(vaddr_start, vaddr_end - 1, 0);</span>
 
 	__flush_tlb_all();
 
<span class="p_del">-	return last_map_addr;</span>
<span class="p_add">+	return paddr_last;</span>
 }
 
 #ifndef CONFIG_NUMA

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



