
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v3,01/10] mm/hugetlb: add cache of descriptors to resv_map for region_add - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v3,01/10] mm/hugetlb: add cache of descriptors to resv_map for region_add</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>July 13, 2015, 4:20 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1436761268-6397-2-git-send-email-mike.kravetz@oracle.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/6773911/mbox/"
   >mbox</a>
|
   <a href="/patch/6773911/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/6773911/">/patch/6773911/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id AABAB9F2E8
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Jul 2015 04:22:09 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 5AA5C2055C
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Jul 2015 04:22:08 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id CE095206B5
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Jul 2015 04:22:06 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1751507AbbGMEV6 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 13 Jul 2015 00:21:58 -0400
Received: from aserp1040.oracle.com ([141.146.126.69]:29912 &quot;EHLO
	aserp1040.oracle.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751292AbbGMEV4 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 13 Jul 2015 00:21:56 -0400
Received: from userv0021.oracle.com (userv0021.oracle.com [156.151.31.71])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with
	ESMTP id t6D4LIYB020535
	(version=TLSv1 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK);
	Mon, 13 Jul 2015 04:21:18 GMT
Received: from userv0122.oracle.com (userv0122.oracle.com [156.151.31.75])
	by userv0021.oracle.com (8.13.8/8.13.8) with ESMTP id t6D4LHD7003592
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL); 
	Mon, 13 Jul 2015 04:21:17 GMT
Received: from abhmp0013.oracle.com (abhmp0013.oracle.com [141.146.116.19])
	by userv0122.oracle.com (8.13.8/8.13.8) with ESMTP id
	t6D4LH6E006074; Mon, 13 Jul 2015 04:21:17 GMT
Received: from monkey.oracle.com (/50.53.81.168)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Sun, 12 Jul 2015 21:21:16 -0700
From: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
To: linux-mm@kvack.org, linux-kernel@vger.kernel.org,
	linux-api@vger.kernel.org
Cc: Dave Hansen &lt;dave.hansen@linux.intel.com&gt;,
	Naoya Horiguchi &lt;n-horiguchi@ah.jp.nec.com&gt;,
	David Rientjes &lt;rientjes@google.com&gt;, Hugh Dickins &lt;hughd@google.com&gt;,
	Davidlohr Bueso &lt;dave@stgolabs.net&gt;,
	Aneesh Kumar &lt;aneesh.kumar@linux.vnet.ibm.com&gt;,
	Hillf Danton &lt;hillf.zj@alibaba-inc.com&gt;,
	Christoph Hellwig &lt;hch@infradead.org&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;, Michal Hocko &lt;mhocko@suse.cz&gt;,
	Mike Kravetz &lt;mike.kravetz@oracle.com&gt;
Subject: [PATCH v3 01/10] mm/hugetlb: add cache of descriptors to resv_map
	for region_add
Date: Sun, 12 Jul 2015 21:20:59 -0700
Message-Id: &lt;1436761268-6397-2-git-send-email-mike.kravetz@oracle.com&gt;
X-Mailer: git-send-email 2.1.0
In-Reply-To: &lt;1436761268-6397-1-git-send-email-mike.kravetz@oracle.com&gt;
References: &lt;1436761268-6397-1-git-send-email-mike.kravetz@oracle.com&gt;
X-Source-IP: userv0021.oracle.com [156.151.31.71]
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-8.3 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - July 13, 2015, 4:20 a.m.</div>
<pre class="content">
fallocate hole punch will want to remove a specific range of
pages.  When pages are removed, their associated entries in
the region/reserve map will also be removed.  This will break
an assumption in the region_chg/region_add calling sequence.
If a new region descriptor must be allocated, it is done as
part of the region_chg processing.  In this way, region_add
can not fail because it does not need to attempt an allocation.

To prepare for fallocate hole punch, create a &quot;cache&quot; of
descriptors that can be used by region_add if necessary.
region_chg will ensure there are sufficient entries in the
cache.  It will be necessary to track the number of in progress
add operations to know a sufficient number of descriptors
reside in the cache.  A new routine region_abort is added to
adjust this in progress count when add operations are aborted.
vma_abort_reservation is also added for callers creating
reservations with vma_needs_reservation/vma_commit_reservation.
<span class="signed-off-by">
Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
---
 include/linux/hugetlb.h |   3 +
 mm/hugetlb.c            | 169 ++++++++++++++++++++++++++++++++++++++++++------
 2 files changed, 153 insertions(+), 19 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - July 17, 2015, 9:02 a.m.</div>
<pre class="content">
On Sun, Jul 12, 2015 at 09:20:59PM -0700, Mike Kravetz wrote:
<span class="quote">&gt; fallocate hole punch will want to remove a specific range of</span>
<span class="quote">&gt; pages.  When pages are removed, their associated entries in</span>
<span class="quote">&gt; the region/reserve map will also be removed.  This will break</span>
<span class="quote">&gt; an assumption in the region_chg/region_add calling sequence.</span>
<span class="quote">&gt; If a new region descriptor must be allocated, it is done as</span>
<span class="quote">&gt; part of the region_chg processing.  In this way, region_add</span>
<span class="quote">&gt; can not fail because it does not need to attempt an allocation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To prepare for fallocate hole punch, create a &quot;cache&quot; of</span>
<span class="quote">&gt; descriptors that can be used by region_add if necessary.</span>
<span class="quote">&gt; region_chg will ensure there are sufficient entries in the</span>
<span class="quote">&gt; cache.  It will be necessary to track the number of in progress</span>
<span class="quote">&gt; add operations to know a sufficient number of descriptors</span>
<span class="quote">&gt; reside in the cache.  A new routine region_abort is added to</span>
<span class="quote">&gt; adjust this in progress count when add operations are aborted.</span>
<span class="quote">&gt; vma_abort_reservation is also added for callers creating</span>
<span class="quote">&gt; reservations with vma_needs_reservation/vma_commit_reservation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  include/linux/hugetlb.h |   3 +</span>
<span class="quote">&gt;  mm/hugetlb.c            | 169 ++++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;  2 files changed, 153 insertions(+), 19 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="quote">&gt; index d891f94..667cf44 100644</span>
<span class="quote">&gt; --- a/include/linux/hugetlb.h</span>
<span class="quote">&gt; +++ b/include/linux/hugetlb.h</span>
<span class="quote">&gt; @@ -35,6 +35,9 @@ struct resv_map {</span>
<span class="quote">&gt;  	struct kref refs;</span>
<span class="quote">&gt;  	spinlock_t lock;</span>
<span class="quote">&gt;  	struct list_head regions;</span>
<span class="quote">&gt; +	long adds_in_progress;</span>
<span class="quote">&gt; +	struct list_head rgn_cache;</span>
<span class="quote">&gt; +	long rgn_cache_count;</span>
<span class="quote">&gt;  };</span>
<span class="quote">&gt;  extern struct resv_map *resv_map_alloc(void);</span>
<span class="quote">&gt;  void resv_map_release(struct kref *ref);</span>
<span class="quote">&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt; index a8c3087..241d16d 100644</span>
<span class="quote">&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt; @@ -240,11 +240,14 @@ struct file_region {</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt;   * Add the huge page range represented by [f, t) to the reserve</span>
<span class="quote">&gt; - * map.  Existing regions will be expanded to accommodate the</span>
<span class="quote">&gt; - * specified range.  We know only existing regions need to be</span>
<span class="quote">&gt; - * expanded, because region_add is only called after region_chg</span>
<span class="quote">&gt; - * with the same range.  If a new file_region structure must</span>
<span class="quote">&gt; - * be allocated, it is done in region_chg.</span>
<span class="quote">&gt; + * map.  In the normal case, existing regions will be expanded</span>
<span class="quote">&gt; + * to accommodate the specified range.  Sufficient regions should</span>
<span class="quote">&gt; + * exist for expansion due to the previous call to region_chg</span>
<span class="quote">&gt; + * with the same range.  However, it is possible that region_del</span>
<span class="quote">&gt; + * could have been called after region_chg and modifed the map</span>
<span class="quote">&gt; + * in such a way that no region exists to be expanded.  In this</span>
<span class="quote">&gt; + * case, pull a region descriptor from the cache associated with</span>
<span class="quote">&gt; + * the map and use that for the new range.</span>
<span class="quote">&gt;   *</span>
<span class="quote">&gt;   * Return the number of new huge pages added to the map.  This</span>
<span class="quote">&gt;   * number is greater than or equal to zero.</span>
<span class="quote">&gt; @@ -261,6 +264,27 @@ static long region_add(struct resv_map *resv, long f, long t)</span>
<span class="quote">&gt;  		if (f &lt;= rg-&gt;to)</span>
<span class="quote">&gt;  			break;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	if (&amp;rg-&gt;link == head || t &lt; rg-&gt;from) {</span>
<span class="quote">&gt; +		/*</span>
<span class="quote">&gt; +		 * No region exists which can be expanded to include the</span>
<span class="quote">&gt; +		 * specified range.  Pull a region descriptor from the</span>
<span class="quote">&gt; +		 * cache, and use it for this range.</span>
<span class="quote">&gt; +		 */</span>

This comment mentions this if-block, not the VM_BUG_ON below, so it had
better be put the above if-line.
<span class="quote">
&gt; +		VM_BUG_ON(!resv-&gt;rgn_cache_count);</span>

resv-&gt;rgn_cache_count &lt;= 0 might be safer.

...
<span class="quote">&gt; @@ -3236,11 +3360,14 @@ retry:</span>
<span class="quote">&gt;  	 * any allocations necessary to record that reservation occur outside</span>
<span class="quote">&gt;  	 * the spinlock.</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt; -	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="quote">&gt; +	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
<span class="quote">&gt;  		if (vma_needs_reservation(h, vma, address) &lt; 0) {</span>
<span class="quote">&gt;  			ret = VM_FAULT_OOM;</span>
<span class="quote">&gt;  			goto backout_unlocked;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +		/* Just decrements count, does not deallocate */</span>
<span class="quote">&gt; +		vma_abort_reservation(h, vma, address);</span>
<span class="quote">&gt; +	}</span>

This is not &quot;abort reservation&quot; operation, but you use &quot;abort reservation&quot;
routine, which might confusing and makes future maintenance hard. I think
this should be done in a simplified variant of vma_commit_reservation()
(maybe just an alias of your vma_abort_reservation()) or fast path in
vma_commit_reservation().

Thanks,
Naoya Horiguchi
<span class="quote">
&gt;  </span>
<span class="quote">&gt;  	ptl = huge_pte_lockptr(h, mm, ptep);</span>
<span class="quote">&gt;  	spin_lock(ptl);</span>
<span class="quote">&gt; @@ -3387,6 +3514,8 @@ int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
<span class="quote">&gt;  			ret = VM_FAULT_OOM;</span>
<span class="quote">&gt;  			goto out_mutex;</span>
<span class="quote">&gt;  		}</span>
<span class="quote">&gt; +		/* Just decrements count, does not deallocate */</span>
<span class="quote">&gt; +		vma_abort_reservation(h, vma, address);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  		if (!(vma-&gt;vm_flags &amp; VM_MAYSHARE))</span>
<span class="quote">&gt;  			pagecache_page = hugetlbfs_pagecache_page(h,</span>
<span class="quote">&gt; @@ -3726,6 +3855,8 @@ int hugetlb_reserve_pages(struct inode *inode,</span>
<span class="quote">&gt;  	}</span>
<span class="quote">&gt;  	return 0;</span>
<span class="quote">&gt;  out_err:</span>
<span class="quote">&gt; +	if (!vma || vma-&gt;vm_flags &amp; VM_MAYSHARE)</span>
<span class="quote">&gt; +		region_abort(resv_map, from, to);</span>
<span class="quote">&gt;  	if (vma &amp;&amp; is_vma_resv_set(vma, HPAGE_RESV_OWNER))</span>
<span class="quote">&gt;  		kref_put(&amp;resv_map-&gt;refs, resv_map_release);</span>
<span class="quote">&gt;  	return ret;</span>
<span class="quote">&gt; -- </span>
<span class="quote">&gt; 2.1.0</span>
<span class="quote">&gt; --</span>
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=124511">Mike Kravetz</a> - July 20, 2015, 5:50 p.m.</div>
<pre class="content">
On 07/17/2015 02:02 AM, Naoya Horiguchi wrote:
<span class="quote">&gt; On Sun, Jul 12, 2015 at 09:20:59PM -0700, Mike Kravetz wrote:</span>
<span class="quote">&gt;&gt; fallocate hole punch will want to remove a specific range of</span>
<span class="quote">&gt;&gt; pages.  When pages are removed, their associated entries in</span>
<span class="quote">&gt;&gt; the region/reserve map will also be removed.  This will break</span>
<span class="quote">&gt;&gt; an assumption in the region_chg/region_add calling sequence.</span>
<span class="quote">&gt;&gt; If a new region descriptor must be allocated, it is done as</span>
<span class="quote">&gt;&gt; part of the region_chg processing.  In this way, region_add</span>
<span class="quote">&gt;&gt; can not fail because it does not need to attempt an allocation.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; To prepare for fallocate hole punch, create a &quot;cache&quot; of</span>
<span class="quote">&gt;&gt; descriptors that can be used by region_add if necessary.</span>
<span class="quote">&gt;&gt; region_chg will ensure there are sufficient entries in the</span>
<span class="quote">&gt;&gt; cache.  It will be necessary to track the number of in progress</span>
<span class="quote">&gt;&gt; add operations to know a sufficient number of descriptors</span>
<span class="quote">&gt;&gt; reside in the cache.  A new routine region_abort is added to</span>
<span class="quote">&gt;&gt; adjust this in progress count when add operations are aborted.</span>
<span class="quote">&gt;&gt; vma_abort_reservation is also added for callers creating</span>
<span class="quote">&gt;&gt; reservations with vma_needs_reservation/vma_commit_reservation.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Signed-off-by: Mike Kravetz &lt;mike.kravetz@oracle.com&gt;</span>
<span class="quote">&gt;&gt; ---</span>
<span class="quote">&gt;&gt;   include/linux/hugetlb.h |   3 +</span>
<span class="quote">&gt;&gt;   mm/hugetlb.c            | 169 ++++++++++++++++++++++++++++++++++++++++++------</span>
<span class="quote">&gt;&gt;   2 files changed, 153 insertions(+), 19 deletions(-)</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="quote">&gt;&gt; index d891f94..667cf44 100644</span>
<span class="quote">&gt;&gt; --- a/include/linux/hugetlb.h</span>
<span class="quote">&gt;&gt; +++ b/include/linux/hugetlb.h</span>
<span class="quote">&gt;&gt; @@ -35,6 +35,9 @@ struct resv_map {</span>
<span class="quote">&gt;&gt;   	struct kref refs;</span>
<span class="quote">&gt;&gt;   	spinlock_t lock;</span>
<span class="quote">&gt;&gt;   	struct list_head regions;</span>
<span class="quote">&gt;&gt; +	long adds_in_progress;</span>
<span class="quote">&gt;&gt; +	struct list_head rgn_cache;</span>
<span class="quote">&gt;&gt; +	long rgn_cache_count;</span>
<span class="quote">&gt;&gt;   };</span>
<span class="quote">&gt;&gt;   extern struct resv_map *resv_map_alloc(void);</span>
<span class="quote">&gt;&gt;   void resv_map_release(struct kref *ref);</span>
<span class="quote">&gt;&gt; diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; index a8c3087..241d16d 100644</span>
<span class="quote">&gt;&gt; --- a/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; +++ b/mm/hugetlb.c</span>
<span class="quote">&gt;&gt; @@ -240,11 +240,14 @@ struct file_region {</span>
<span class="quote">&gt;&gt;   </span>
<span class="quote">&gt;&gt;   /*</span>
<span class="quote">&gt;&gt;    * Add the huge page range represented by [f, t) to the reserve</span>
<span class="quote">&gt;&gt; - * map.  Existing regions will be expanded to accommodate the</span>
<span class="quote">&gt;&gt; - * specified range.  We know only existing regions need to be</span>
<span class="quote">&gt;&gt; - * expanded, because region_add is only called after region_chg</span>
<span class="quote">&gt;&gt; - * with the same range.  If a new file_region structure must</span>
<span class="quote">&gt;&gt; - * be allocated, it is done in region_chg.</span>
<span class="quote">&gt;&gt; + * map.  In the normal case, existing regions will be expanded</span>
<span class="quote">&gt;&gt; + * to accommodate the specified range.  Sufficient regions should</span>
<span class="quote">&gt;&gt; + * exist for expansion due to the previous call to region_chg</span>
<span class="quote">&gt;&gt; + * with the same range.  However, it is possible that region_del</span>
<span class="quote">&gt;&gt; + * could have been called after region_chg and modifed the map</span>
<span class="quote">&gt;&gt; + * in such a way that no region exists to be expanded.  In this</span>
<span class="quote">&gt;&gt; + * case, pull a region descriptor from the cache associated with</span>
<span class="quote">&gt;&gt; + * the map and use that for the new range.</span>
<span class="quote">&gt;&gt;    *</span>
<span class="quote">&gt;&gt;    * Return the number of new huge pages added to the map.  This</span>
<span class="quote">&gt;&gt;    * number is greater than or equal to zero.</span>
<span class="quote">&gt;&gt; @@ -261,6 +264,27 @@ static long region_add(struct resv_map *resv, long f, long t)</span>
<span class="quote">&gt;&gt;   		if (f &lt;= rg-&gt;to)</span>
<span class="quote">&gt;&gt;   			break;</span>
<span class="quote">&gt;&gt;   </span>
<span class="quote">&gt;&gt; +	if (&amp;rg-&gt;link == head || t &lt; rg-&gt;from) {</span>
<span class="quote">&gt;&gt; +		/*</span>
<span class="quote">&gt;&gt; +		 * No region exists which can be expanded to include the</span>
<span class="quote">&gt;&gt; +		 * specified range.  Pull a region descriptor from the</span>
<span class="quote">&gt;&gt; +		 * cache, and use it for this range.</span>
<span class="quote">&gt;&gt; +		 */</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This comment mentions this if-block, not the VM_BUG_ON below, so it had</span>
<span class="quote">&gt; better be put the above if-line.</span>

OK, I will move and make a minor modification to the comment.
<span class="quote">
&gt; </span>
<span class="quote">&gt;&gt; +		VM_BUG_ON(!resv-&gt;rgn_cache_count);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; resv-&gt;rgn_cache_count &lt;= 0 might be safer.</span>

Sure.
<span class="quote">
&gt; ...</span>
<span class="quote">&gt;&gt; @@ -3236,11 +3360,14 @@ retry:</span>
<span class="quote">&gt;&gt;   	 * any allocations necessary to record that reservation occur outside</span>
<span class="quote">&gt;&gt;   	 * the spinlock.</span>
<span class="quote">&gt;&gt;   	 */</span>
<span class="quote">&gt;&gt; -	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="quote">&gt;&gt; +	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
<span class="quote">&gt;&gt;   		if (vma_needs_reservation(h, vma, address) &lt; 0) {</span>
<span class="quote">&gt;&gt;   			ret = VM_FAULT_OOM;</span>
<span class="quote">&gt;&gt;   			goto backout_unlocked;</span>
<span class="quote">&gt;&gt;   		}</span>
<span class="quote">&gt;&gt; +		/* Just decrements count, does not deallocate */</span>
<span class="quote">&gt;&gt; +		vma_abort_reservation(h, vma, address);</span>
<span class="quote">&gt;&gt; +	}</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This is not &quot;abort reservation&quot; operation, but you use &quot;abort reservation&quot;</span>
<span class="quote">&gt; routine, which might confusing and makes future maintenance hard. I think</span>
<span class="quote">&gt; this should be done in a simplified variant of vma_commit_reservation()</span>
<span class="quote">&gt; (maybe just an alias of your vma_abort_reservation()) or fast path in</span>
<span class="quote">&gt; vma_commit_reservation().</span>

I am struggling a bit with the names of these routines.  The
routines in question are:

vma_needs_reservation - This is a wrapper for region_chg(), so the
	return value is the number of regions needed for the page.
	Since there is only one page, the routine effectively
	becomes a boolean.  Hence the name &quot;needs&quot;.

vma_commit_reservation - This is a wrapper for region_add().  It
	must be called after a prior call to vma_needs_reservation
	and after actual allocation of the page.

We need a way to handle the case where vma_needs_reservation has
been called, but the page allocation is not successful.  I chose
the name vma_abort_reservation, but as noted (even in my comments)
it is not an actual abort.

I am not sure if you are suggesting vma_commit_reservation() should
handle this as a special case.  I think a separately named routine which
indicates and end of the reservation/allocation process would be
easier to understand.

What about changing the name vma_abort_reservation() to
vma_end_reservation()?  This would indicate that the reservation/
allocation process is ended.
<span class="quote">
&gt; Thanks,</span>
<span class="quote">&gt; Naoya Horiguchi</span>

Thank you for your reviews.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4407">Naoya Horiguchi</a> - July 21, 2015, 4:16 a.m.</div>
<pre class="content">
On Mon, Jul 20, 2015 at 10:50:12AM -0700, Mike Kravetz wrote:
...
<span class="quote">&gt; &gt; ...</span>
<span class="quote">&gt; &gt;&gt; @@ -3236,11 +3360,14 @@ retry:</span>
<span class="quote">&gt; &gt;&gt;   	 * any allocations necessary to record that reservation occur outside</span>
<span class="quote">&gt; &gt;&gt;   	 * the spinlock.</span>
<span class="quote">&gt; &gt;&gt;   	 */</span>
<span class="quote">&gt; &gt;&gt; -	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="quote">&gt; &gt;&gt; +	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
<span class="quote">&gt; &gt;&gt;   		if (vma_needs_reservation(h, vma, address) &lt; 0) {</span>
<span class="quote">&gt; &gt;&gt;   			ret = VM_FAULT_OOM;</span>
<span class="quote">&gt; &gt;&gt;   			goto backout_unlocked;</span>
<span class="quote">&gt; &gt;&gt;   		}</span>
<span class="quote">&gt; &gt;&gt; +		/* Just decrements count, does not deallocate */</span>
<span class="quote">&gt; &gt;&gt; +		vma_abort_reservation(h, vma, address);</span>
<span class="quote">&gt; &gt;&gt; +	}</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This is not &quot;abort reservation&quot; operation, but you use &quot;abort reservation&quot;</span>
<span class="quote">&gt; &gt; routine, which might confusing and makes future maintenance hard. I think</span>
<span class="quote">&gt; &gt; this should be done in a simplified variant of vma_commit_reservation()</span>
<span class="quote">&gt; &gt; (maybe just an alias of your vma_abort_reservation()) or fast path in</span>
<span class="quote">&gt; &gt; vma_commit_reservation().</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am struggling a bit with the names of these routines.  The</span>
<span class="quote">&gt; routines in question are:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; vma_needs_reservation - This is a wrapper for region_chg(), so the</span>
<span class="quote">&gt; 	return value is the number of regions needed for the page.</span>
<span class="quote">&gt; 	Since there is only one page, the routine effectively</span>
<span class="quote">&gt; 	becomes a boolean.  Hence the name &quot;needs&quot;.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; vma_commit_reservation - This is a wrapper for region_add().  It</span>
<span class="quote">&gt; 	must be called after a prior call to vma_needs_reservation</span>
<span class="quote">&gt; 	and after actual allocation of the page.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We need a way to handle the case where vma_needs_reservation has</span>
<span class="quote">&gt; been called, but the page allocation is not successful.  I chose</span>
<span class="quote">&gt; the name vma_abort_reservation, but as noted (even in my comments)</span>
<span class="quote">&gt; it is not an actual abort.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I am not sure if you are suggesting vma_commit_reservation() should</span>
<span class="quote">&gt; handle this as a special case.  I think a separately named routine which</span>
<span class="quote">&gt; indicates and end of the reservation/allocation process would be</span>
<span class="quote">&gt; easier to understand.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; What about changing the name vma_abort_reservation() to</span>
<span class="quote">&gt; vma_end_reservation()?  This would indicate that the reservation/</span>
<span class="quote">&gt; allocation process is ended.</span>

OK, vma_end_reservation() sounds nice to me.
<span class="quote">
&gt; &gt; Thanks,</span>
<span class="quote">&gt; &gt; Naoya Horiguchi</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Thank you for your reviews.</span>

You&#39;re welcome :)

Naoya Horiguchi--
To unsubscribe from this list: send the line &quot;unsubscribe linux-kernel&quot; in
the body of a message to majordomo@vger.kernel.org
More majordomo info at  http://vger.kernel.org/majordomo-info.html
Please read the FAQ at  http://www.tux.org/lkml/
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/hugetlb.h b/include/linux/hugetlb.h</span>
<span class="p_header">index d891f94..667cf44 100644</span>
<span class="p_header">--- a/include/linux/hugetlb.h</span>
<span class="p_header">+++ b/include/linux/hugetlb.h</span>
<span class="p_chunk">@@ -35,6 +35,9 @@</span> <span class="p_context"> struct resv_map {</span>
 	struct kref refs;
 	spinlock_t lock;
 	struct list_head regions;
<span class="p_add">+	long adds_in_progress;</span>
<span class="p_add">+	struct list_head rgn_cache;</span>
<span class="p_add">+	long rgn_cache_count;</span>
 };
 extern struct resv_map *resv_map_alloc(void);
 void resv_map_release(struct kref *ref);
<span class="p_header">diff --git a/mm/hugetlb.c b/mm/hugetlb.c</span>
<span class="p_header">index a8c3087..241d16d 100644</span>
<span class="p_header">--- a/mm/hugetlb.c</span>
<span class="p_header">+++ b/mm/hugetlb.c</span>
<span class="p_chunk">@@ -240,11 +240,14 @@</span> <span class="p_context"> struct file_region {</span>
 
 /*
  * Add the huge page range represented by [f, t) to the reserve
<span class="p_del">- * map.  Existing regions will be expanded to accommodate the</span>
<span class="p_del">- * specified range.  We know only existing regions need to be</span>
<span class="p_del">- * expanded, because region_add is only called after region_chg</span>
<span class="p_del">- * with the same range.  If a new file_region structure must</span>
<span class="p_del">- * be allocated, it is done in region_chg.</span>
<span class="p_add">+ * map.  In the normal case, existing regions will be expanded</span>
<span class="p_add">+ * to accommodate the specified range.  Sufficient regions should</span>
<span class="p_add">+ * exist for expansion due to the previous call to region_chg</span>
<span class="p_add">+ * with the same range.  However, it is possible that region_del</span>
<span class="p_add">+ * could have been called after region_chg and modifed the map</span>
<span class="p_add">+ * in such a way that no region exists to be expanded.  In this</span>
<span class="p_add">+ * case, pull a region descriptor from the cache associated with</span>
<span class="p_add">+ * the map and use that for the new range.</span>
  *
  * Return the number of new huge pages added to the map.  This
  * number is greater than or equal to zero.
<span class="p_chunk">@@ -261,6 +264,27 @@</span> <span class="p_context"> static long region_add(struct resv_map *resv, long f, long t)</span>
 		if (f &lt;= rg-&gt;to)
 			break;
 
<span class="p_add">+	if (&amp;rg-&gt;link == head || t &lt; rg-&gt;from) {</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * No region exists which can be expanded to include the</span>
<span class="p_add">+		 * specified range.  Pull a region descriptor from the</span>
<span class="p_add">+		 * cache, and use it for this range.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		VM_BUG_ON(!resv-&gt;rgn_cache_count);</span>
<span class="p_add">+</span>
<span class="p_add">+		resv-&gt;rgn_cache_count--;</span>
<span class="p_add">+		nrg = list_first_entry(&amp;resv-&gt;rgn_cache, struct file_region,</span>
<span class="p_add">+					link);</span>
<span class="p_add">+		list_del(&amp;nrg-&gt;link);</span>
<span class="p_add">+</span>
<span class="p_add">+		nrg-&gt;from = f;</span>
<span class="p_add">+		nrg-&gt;to = t;</span>
<span class="p_add">+		list_add(&amp;nrg-&gt;link, rg-&gt;link.prev);</span>
<span class="p_add">+</span>
<span class="p_add">+		add += t - f;</span>
<span class="p_add">+		goto out_locked;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* Round our left edge to the current segment if it encloses us. */
 	if (f &gt; rg-&gt;from)
 		f = rg-&gt;from;
<span class="p_chunk">@@ -294,6 +318,8 @@</span> <span class="p_context"> static long region_add(struct resv_map *resv, long f, long t)</span>
 	add += t - nrg-&gt;to;		/* Added to end of region */
 	nrg-&gt;to = t;
 
<span class="p_add">+out_locked:</span>
<span class="p_add">+	resv-&gt;adds_in_progress--;</span>
 	spin_unlock(&amp;resv-&gt;lock);
 	VM_BUG_ON(add &lt; 0);
 	return add;
<span class="p_chunk">@@ -312,11 +338,16 @@</span> <span class="p_context"> static long region_add(struct resv_map *resv, long f, long t)</span>
  * so that the subsequent region_add call will have all the
  * regions it needs and will not fail.
  *
<span class="p_add">+ * Upon entry, region_chg will also examine the cache of</span>
<span class="p_add">+ * region descriptors associated with the map.  If there</span>
<span class="p_add">+ * not enough descriptors cached, one will be allocated</span>
<span class="p_add">+ * for the in progress add operation.</span>
<span class="p_add">+ *</span>
  * Returns the number of huge pages that need to be added
  * to the existing reservation map for the range [f, t).
  * This number is greater or equal to zero.  -ENOMEM is
<span class="p_del">- * returned if a new file_region structure is needed and can</span>
<span class="p_del">- * not be allocated.</span>
<span class="p_add">+ * returned if a new file_region structure or cache entry</span>
<span class="p_add">+ * is needed and can not be allocated.</span>
  */
 static long region_chg(struct resv_map *resv, long f, long t)
 {
<span class="p_chunk">@@ -326,6 +357,31 @@</span> <span class="p_context"> static long region_chg(struct resv_map *resv, long f, long t)</span>
 
 retry:
 	spin_lock(&amp;resv-&gt;lock);
<span class="p_add">+retry_locked:</span>
<span class="p_add">+	resv-&gt;adds_in_progress++;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Check for sufficient descriptors in the cache to accommodate</span>
<span class="p_add">+	 * the number of in progress add operations.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (resv-&gt;adds_in_progress &gt; resv-&gt;rgn_cache_count) {</span>
<span class="p_add">+		struct file_region *trg;</span>
<span class="p_add">+</span>
<span class="p_add">+		VM_BUG_ON(resv-&gt;adds_in_progress - resv-&gt;rgn_cache_count &gt; 1);</span>
<span class="p_add">+		/* Must drop lock to allocate a new descriptor. */</span>
<span class="p_add">+		resv-&gt;adds_in_progress--;</span>
<span class="p_add">+		spin_unlock(&amp;resv-&gt;lock);</span>
<span class="p_add">+</span>
<span class="p_add">+		trg = kmalloc(sizeof(*trg), GFP_KERNEL);</span>
<span class="p_add">+		if (!trg)</span>
<span class="p_add">+			return -ENOMEM;</span>
<span class="p_add">+</span>
<span class="p_add">+		spin_lock(&amp;resv-&gt;lock);</span>
<span class="p_add">+		list_add(&amp;trg-&gt;link, &amp;resv-&gt;rgn_cache);</span>
<span class="p_add">+		resv-&gt;rgn_cache_count++;</span>
<span class="p_add">+		goto retry_locked;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 	/* Locate the region we are before or in. */
 	list_for_each_entry(rg, head, link)
 		if (f &lt;= rg-&gt;to)
<span class="p_chunk">@@ -336,6 +392,7 @@</span> <span class="p_context"> retry:</span>
 	 * size such that we can guarantee to record the reservation. */
 	if (&amp;rg-&gt;link == head || t &lt; rg-&gt;from) {
 		if (!nrg) {
<span class="p_add">+			resv-&gt;adds_in_progress--;</span>
 			spin_unlock(&amp;resv-&gt;lock);
 			nrg = kmalloc(sizeof(*nrg), GFP_KERNEL);
 			if (!nrg)
<span class="p_chunk">@@ -385,6 +442,25 @@</span> <span class="p_context"> out_nrg:</span>
 }
 
 /*
<span class="p_add">+ * Abort the in progress add operation.  The adds_in_progress field</span>
<span class="p_add">+ * of the resv_map keeps track of the operations in progress between</span>
<span class="p_add">+ * calls to region_chg and region_add.  Operations are sometimes</span>
<span class="p_add">+ * aborted after the call to region_chg.  In such cases, region_abort</span>
<span class="p_add">+ * is called to decrement the adds_in_progress counter.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * NOTE: The range arguments [f, t) are not needed or used in this</span>
<span class="p_add">+ * routine.  They are kept to make reading the calling code easier as</span>
<span class="p_add">+ * arguments will match the associated region_chg call.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static void region_abort(struct resv_map *resv, long f, long t)</span>
<span class="p_add">+{</span>
<span class="p_add">+	spin_lock(&amp;resv-&gt;lock);</span>
<span class="p_add">+	VM_BUG_ON(!resv-&gt;rgn_cache_count);</span>
<span class="p_add">+	resv-&gt;adds_in_progress--;</span>
<span class="p_add">+	spin_unlock(&amp;resv-&gt;lock);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * Truncate the reserve map at index &#39;end&#39;.  Modify/truncate any
  * region which contains end.  Delete any regions past end.
  * Return the number of huge pages removed from the map.
<span class="p_chunk">@@ -544,22 +620,44 @@</span> <span class="p_context"> static void set_vma_private_data(struct vm_area_struct *vma,</span>
 struct resv_map *resv_map_alloc(void)
 {
 	struct resv_map *resv_map = kmalloc(sizeof(*resv_map), GFP_KERNEL);
<span class="p_del">-	if (!resv_map)</span>
<span class="p_add">+	struct file_region *rg = kmalloc(sizeof(*rg), GFP_KERNEL);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!resv_map || !rg) {</span>
<span class="p_add">+		kfree(resv_map);</span>
<span class="p_add">+		kfree(rg);</span>
 		return NULL;
<span class="p_add">+	}</span>
 
 	kref_init(&amp;resv_map-&gt;refs);
 	spin_lock_init(&amp;resv_map-&gt;lock);
 	INIT_LIST_HEAD(&amp;resv_map-&gt;regions);
 
<span class="p_add">+	resv_map-&gt;adds_in_progress = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	INIT_LIST_HEAD(&amp;resv_map-&gt;rgn_cache);</span>
<span class="p_add">+	list_add(&amp;rg-&gt;link, &amp;resv_map-&gt;rgn_cache);</span>
<span class="p_add">+	resv_map-&gt;rgn_cache_count = 1;</span>
<span class="p_add">+</span>
 	return resv_map;
 }
 
 void resv_map_release(struct kref *ref)
 {
 	struct resv_map *resv_map = container_of(ref, struct resv_map, refs);
<span class="p_add">+	struct list_head *head = &amp;resv_map-&gt;rgn_cache;</span>
<span class="p_add">+	struct file_region *rg, *trg;</span>
 
 	/* Clear out any active regions before we release the map. */
 	region_truncate(resv_map, 0);
<span class="p_add">+</span>
<span class="p_add">+	/* ... and any entries left in the cache */</span>
<span class="p_add">+	list_for_each_entry_safe(rg, trg, head, link) {</span>
<span class="p_add">+		list_del(&amp;rg-&gt;link);</span>
<span class="p_add">+		kfree(rg);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	VM_BUG_ON(resv_map-&gt;adds_in_progress);</span>
<span class="p_add">+</span>
 	kfree(resv_map);
 }
 
<span class="p_chunk">@@ -1473,16 +1571,18 @@</span> <span class="p_context"> static void return_unused_surplus_pages(struct hstate *h,</span>
 	}
 }
 
<span class="p_add">+</span>
 /*
<span class="p_del">- * vma_needs_reservation and vma_commit_reservation are used by the huge</span>
<span class="p_del">- * page allocation routines to manage reservations.</span>
<span class="p_add">+ * vma_needs_reservation, vma_commit_reservation and vma_abort_reservation</span>
<span class="p_add">+ * are used by the huge page allocation routines to manage reservations.</span>
  *
  * vma_needs_reservation is called to determine if the huge page at addr
  * within the vma has an associated reservation.  If a reservation is
  * needed, the value 1 is returned.  The caller is then responsible for
  * managing the global reservation and subpool usage counts.  After
  * the huge page has been allocated, vma_commit_reservation is called
<span class="p_del">- * to add the page to the reservation map.</span>
<span class="p_add">+ * to add the page to the reservation map.  If the reservation must be</span>
<span class="p_add">+ * aborted instead of committed, vma_abort_reservation is called.</span>
  *
  * In the normal case, vma_commit_reservation returns the same value
  * as the preceding vma_needs_reservation call.  The only time this
<span class="p_chunk">@@ -1490,9 +1590,14 @@</span> <span class="p_context"> static void return_unused_surplus_pages(struct hstate *h,</span>
  * is the responsibility of the caller to notice the difference and
  * take appropriate action.
  */
<span class="p_add">+enum vma_resv_mode {</span>
<span class="p_add">+	VMA_NEEDS_RESV,</span>
<span class="p_add">+	VMA_COMMIT_RESV,</span>
<span class="p_add">+	VMA_ABORT_RESV,</span>
<span class="p_add">+};</span>
 static long __vma_reservation_common(struct hstate *h,
 				struct vm_area_struct *vma, unsigned long addr,
<span class="p_del">-				bool commit)</span>
<span class="p_add">+				enum vma_resv_mode mode)</span>
 {
 	struct resv_map *resv;
 	pgoff_t idx;
<span class="p_chunk">@@ -1503,10 +1608,20 @@</span> <span class="p_context"> static long __vma_reservation_common(struct hstate *h,</span>
 		return 1;
 
 	idx = vma_hugecache_offset(h, vma, addr);
<span class="p_del">-	if (commit)</span>
<span class="p_del">-		ret = region_add(resv, idx, idx + 1);</span>
<span class="p_del">-	else</span>
<span class="p_add">+	switch (mode) {</span>
<span class="p_add">+	case VMA_NEEDS_RESV:</span>
 		ret = region_chg(resv, idx, idx + 1);
<span class="p_add">+		break;</span>
<span class="p_add">+	case VMA_COMMIT_RESV:</span>
<span class="p_add">+		ret = region_add(resv, idx, idx + 1);</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	case VMA_ABORT_RESV:</span>
<span class="p_add">+		region_abort(resv, idx, idx + 1);</span>
<span class="p_add">+		ret = 0;</span>
<span class="p_add">+		break;</span>
<span class="p_add">+	default:</span>
<span class="p_add">+		BUG();</span>
<span class="p_add">+	}</span>
 
 	if (vma-&gt;vm_flags &amp; VM_MAYSHARE)
 		return ret;
<span class="p_chunk">@@ -1517,13 +1632,19 @@</span> <span class="p_context"> static long __vma_reservation_common(struct hstate *h,</span>
 static long vma_needs_reservation(struct hstate *h,
 			struct vm_area_struct *vma, unsigned long addr)
 {
<span class="p_del">-	return __vma_reservation_common(h, vma, addr, false);</span>
<span class="p_add">+	return __vma_reservation_common(h, vma, addr, VMA_NEEDS_RESV);</span>
 }
 
 static long vma_commit_reservation(struct hstate *h,
 			struct vm_area_struct *vma, unsigned long addr)
 {
<span class="p_del">-	return __vma_reservation_common(h, vma, addr, true);</span>
<span class="p_add">+	return __vma_reservation_common(h, vma, addr, VMA_COMMIT_RESV);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void vma_abort_reservation(struct hstate *h,</span>
<span class="p_add">+			struct vm_area_struct *vma, unsigned long addr)</span>
<span class="p_add">+{</span>
<span class="p_add">+	(void)__vma_reservation_common(h, vma, addr, VMA_ABORT_RESV);</span>
 }
 
 static struct page *alloc_huge_page(struct vm_area_struct *vma,
<span class="p_chunk">@@ -1549,8 +1670,10 @@</span> <span class="p_context"> static struct page *alloc_huge_page(struct vm_area_struct *vma,</span>
 	if (chg &lt; 0)
 		return ERR_PTR(-ENOMEM);
 	if (chg || avoid_reserve)
<span class="p_del">-		if (hugepage_subpool_get_pages(spool, 1) &lt; 0)</span>
<span class="p_add">+		if (hugepage_subpool_get_pages(spool, 1) &lt; 0) {</span>
<span class="p_add">+			vma_abort_reservation(h, vma, addr);</span>
 			return ERR_PTR(-ENOSPC);
<span class="p_add">+		}</span>
 
 	ret = hugetlb_cgroup_charge_cgroup(idx, pages_per_huge_page(h), &amp;h_cg);
 	if (ret)
<span class="p_chunk">@@ -1596,6 +1719,7 @@</span> <span class="p_context"> out_uncharge_cgroup:</span>
 out_subpool_put:
 	if (chg || avoid_reserve)
 		hugepage_subpool_put_pages(spool, 1);
<span class="p_add">+	vma_abort_reservation(h, vma, addr);</span>
 	return ERR_PTR(-ENOSPC);
 }
 
<span class="p_chunk">@@ -3236,11 +3360,14 @@</span> <span class="p_context"> retry:</span>
 	 * any allocations necessary to record that reservation occur outside
 	 * the spinlock.
 	 */
<span class="p_del">-	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED))</span>
<span class="p_add">+	if ((flags &amp; FAULT_FLAG_WRITE) &amp;&amp; !(vma-&gt;vm_flags &amp; VM_SHARED)) {</span>
 		if (vma_needs_reservation(h, vma, address) &lt; 0) {
 			ret = VM_FAULT_OOM;
 			goto backout_unlocked;
 		}
<span class="p_add">+		/* Just decrements count, does not deallocate */</span>
<span class="p_add">+		vma_abort_reservation(h, vma, address);</span>
<span class="p_add">+	}</span>
 
 	ptl = huge_pte_lockptr(h, mm, ptep);
 	spin_lock(ptl);
<span class="p_chunk">@@ -3387,6 +3514,8 @@</span> <span class="p_context"> int hugetlb_fault(struct mm_struct *mm, struct vm_area_struct *vma,</span>
 			ret = VM_FAULT_OOM;
 			goto out_mutex;
 		}
<span class="p_add">+		/* Just decrements count, does not deallocate */</span>
<span class="p_add">+		vma_abort_reservation(h, vma, address);</span>
 
 		if (!(vma-&gt;vm_flags &amp; VM_MAYSHARE))
 			pagecache_page = hugetlbfs_pagecache_page(h,
<span class="p_chunk">@@ -3726,6 +3855,8 @@</span> <span class="p_context"> int hugetlb_reserve_pages(struct inode *inode,</span>
 	}
 	return 0;
 out_err:
<span class="p_add">+	if (!vma || vma-&gt;vm_flags &amp; VM_MAYSHARE)</span>
<span class="p_add">+		region_abort(resv_map, from, to);</span>
 	if (vma &amp;&amp; is_vma_resv_set(vma, HPAGE_RESV_OWNER))
 		kref_put(&amp;resv_map-&gt;refs, resv_map_release);
 	return ret;

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



