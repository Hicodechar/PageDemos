
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/2] mm, oom: do not rely on TIF_MEMDIE for memory reserves access - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/2] mm, oom: do not rely on TIF_MEMDIE for memory reserves access</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Aug. 3, 2017, 1:39 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;201708031039.GDG05288.OQJOHtLVFMSFFO@I-love.SAKURA.ne.jp&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9877991/mbox/"
   >mbox</a>
|
   <a href="/patch/9877991/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9877991/">/patch/9877991/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	E06BF6035F for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Aug 2017 01:40:08 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id C8D492884E
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Aug 2017 01:40:08 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id BD23828861; Thu,  3 Aug 2017 01:40:08 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id EE4612884B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Thu,  3 Aug 2017 01:40:07 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1752046AbdHCBkE (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 2 Aug 2017 21:40:04 -0400
Received: from www262.sakura.ne.jp ([202.181.97.72]:17597 &quot;EHLO
	www262.sakura.ne.jp&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1751345AbdHCBkD (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 2 Aug 2017 21:40:03 -0400
Received: from fsav406.sakura.ne.jp (fsav406.sakura.ne.jp [133.242.250.105])
	by www262.sakura.ne.jp (8.14.5/8.14.5) with ESMTP id
	v731dfEt060062; Thu, 3 Aug 2017 10:39:42 +0900 (JST)
	(envelope-from penguin-kernel@I-love.SAKURA.ne.jp)
Received: from www262.sakura.ne.jp (202.181.97.72) by fsav406.sakura.ne.jp
	(F-Secure/fsigk_smtp/530/fsav406.sakura.ne.jp); 
	Thu, 03 Aug 2017 10:39:41 +0900 (JST)
X-Virus-Status: clean(F-Secure/fsigk_smtp/530/fsav406.sakura.ne.jp)
Received: from AQUA (softbank126227147111.bbtec.net [126.227.147.111])
	(authenticated bits=0)
	by www262.sakura.ne.jp (8.14.5/8.14.5) with ESMTP id v731dfcV060057; 
	Thu, 3 Aug 2017 10:39:41 +0900 (JST)
	(envelope-from penguin-kernel@I-love.SAKURA.ne.jp)
To: mhocko@kernel.org
Cc: akpm@linux-foundation.org, rientjes@google.com, hannes@cmpxchg.org,
	guro@fb.com, linux-mm@kvack.org, linux-kernel@vger.kernel.org
Subject: Re: [PATCH 1/2] mm,
	oom: do not rely on TIF_MEMDIE for memory reserves access
From: Tetsuo Handa &lt;penguin-kernel@I-love.SAKURA.ne.jp&gt;
References: &lt;20170727090357.3205-1-mhocko@kernel.org&gt;
	&lt;20170727090357.3205-2-mhocko@kernel.org&gt;
	&lt;201708020030.ACB04683.JLHMFVOSFFOtOQ@I-love.SAKURA.ne.jp&gt;
	&lt;20170801165242.GA15518@dhcp22.suse.cz&gt;
In-Reply-To: &lt;20170801165242.GA15518@dhcp22.suse.cz&gt;
Message-Id: &lt;201708031039.GDG05288.OQJOHtLVFMSFFO@I-love.SAKURA.ne.jp&gt;
X-Mailer: Winbiff [Version 2.51 PL2]
X-Accept-Language: ja,en,zh
Date: Thu, 3 Aug 2017 10:39:42 +0900
Mime-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=28">Tetsuo Handa</a> - Aug. 3, 2017, 1:39 a.m.</div>
<pre class="content">
Michal Hocko wrote:
<span class="quote">&gt; On Wed 02-08-17 00:30:33, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; @@ -3603,6 +3612,22 @@ gfp_to_alloc_flags(gfp_t gfp_mask)</span>
<span class="quote">&gt; &gt; &gt;  	return alloc_flags;</span>
<span class="quote">&gt; &gt; &gt;  }</span>
<span class="quote">&gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt; +static bool oom_reserves_allowed(struct task_struct *tsk)</span>
<span class="quote">&gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; +	if (!tsk_is_oom_victim(tsk))</span>
<span class="quote">&gt; &gt; &gt; +		return false;</span>
<span class="quote">&gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; +	 * !MMU doesn&#39;t have oom reaper so we shouldn&#39;t risk the memory reserves</span>
<span class="quote">&gt; &gt; &gt; +	 * depletion and shouldn&#39;t give access to memory reserves passed the</span>
<span class="quote">&gt; &gt; &gt; +	 * exit_mm</span>
<span class="quote">&gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; +	if (!IS_ENABLED(CONFIG_MMU) &amp;&amp; !tsk-&gt;mm)</span>
<span class="quote">&gt; &gt; &gt; +		return false;</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Branching based on CONFIG_MMU is ugly. I suggest timeout based next OOM</span>
<span class="quote">&gt; &gt; victim selection if CONFIG_MMU=n.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I suggest we do not argue about nommu without actually optimizing for or</span>
<span class="quote">&gt; fixing nommu which we are not here. I am even not sure memory reserves</span>
<span class="quote">&gt; can ever be depleted for that config.</span>

I don&#39;t think memory reserves can deplete for CONFIG_MMU=n environment.
But the reason the OOM reaper was introduced is not limited to handling
depletion of memory reserves. The OOM reaper was introduced because
OOM victims might get stuck indirectly waiting for other threads doing
memory allocation. You said
<span class="quote">
  &gt; Yes, exit_aio is the only blocking call I know of currently. But I would</span>
<span class="quote">  &gt; like this to be as robust as possible and so I do not want to rely on</span>
<span class="quote">  &gt; the current implementation. This can change in future and I can</span>
<span class="quote">  &gt; guarantee that nobody will think about the oom path when adding</span>
<span class="quote">  &gt; something to the final __mmput path.</span>

at http://lkml.kernel.org/r/20170726054533.GA960@dhcp22.suse.cz , but
how can you guarantee that nobody will think about the oom path
when adding something to the final __mmput() path without thinking
about possibility of getting stuck waiting for memory allocation in
CONFIG_MMU=n environment? As long as possibility of getting stuck remains,
you should not assume that something you don&#39;t want will not happen.
It&#39;s time to make CONFIG_MMU=n kernels treatable like CONFIG_MMU=y kernels.

If it is technically impossible (or is not worthwhile) to implement
the OOM reaper for CONFIG_MMU=n kernels, I&#39;m fine with timeout based
approach like shown below. Then, we no longer need to use branching
based on CONFIG_MMU.

 include/linux/mm_types.h |  3 +++
 mm/oom_kill.c            | 20 +++++++++++++++-----
 2 files changed, 18 insertions(+), 5 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - Aug. 3, 2017, 7:06 a.m.</div>
<pre class="content">
On Thu 03-08-17 10:39:42, Tetsuo Handa wrote:
<span class="quote">&gt; Michal Hocko wrote:</span>
<span class="quote">&gt; &gt; On Wed 02-08-17 00:30:33, Tetsuo Handa wrote:</span>
<span class="quote">&gt; &gt; &gt; &gt; @@ -3603,6 +3612,22 @@ gfp_to_alloc_flags(gfp_t gfp_mask)</span>
<span class="quote">&gt; &gt; &gt; &gt;  	return alloc_flags;</span>
<span class="quote">&gt; &gt; &gt; &gt;  }</span>
<span class="quote">&gt; &gt; &gt; &gt;  </span>
<span class="quote">&gt; &gt; &gt; &gt; +static bool oom_reserves_allowed(struct task_struct *tsk)</span>
<span class="quote">&gt; &gt; &gt; &gt; +{</span>
<span class="quote">&gt; &gt; &gt; &gt; +	if (!tsk_is_oom_victim(tsk))</span>
<span class="quote">&gt; &gt; &gt; &gt; +		return false;</span>
<span class="quote">&gt; &gt; &gt; &gt; +</span>
<span class="quote">&gt; &gt; &gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * !MMU doesn&#39;t have oom reaper so we shouldn&#39;t risk the memory reserves</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * depletion and shouldn&#39;t give access to memory reserves passed the</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 * exit_mm</span>
<span class="quote">&gt; &gt; &gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; &gt; &gt; +	if (!IS_ENABLED(CONFIG_MMU) &amp;&amp; !tsk-&gt;mm)</span>
<span class="quote">&gt; &gt; &gt; &gt; +		return false;</span>
<span class="quote">&gt; &gt; &gt; </span>
<span class="quote">&gt; &gt; &gt; Branching based on CONFIG_MMU is ugly. I suggest timeout based next OOM</span>
<span class="quote">&gt; &gt; &gt; victim selection if CONFIG_MMU=n.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I suggest we do not argue about nommu without actually optimizing for or</span>
<span class="quote">&gt; &gt; fixing nommu which we are not here. I am even not sure memory reserves</span>
<span class="quote">&gt; &gt; can ever be depleted for that config.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I don&#39;t think memory reserves can deplete for CONFIG_MMU=n environment.</span>
<span class="quote">&gt; But the reason the OOM reaper was introduced is not limited to handling</span>
<span class="quote">&gt; depletion of memory reserves. The OOM reaper was introduced because</span>
<span class="quote">&gt; OOM victims might get stuck indirectly waiting for other threads doing</span>
<span class="quote">&gt; memory allocation. You said</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;   &gt; Yes, exit_aio is the only blocking call I know of currently. But I would</span>
<span class="quote">&gt;   &gt; like this to be as robust as possible and so I do not want to rely on</span>
<span class="quote">&gt;   &gt; the current implementation. This can change in future and I can</span>
<span class="quote">&gt;   &gt; guarantee that nobody will think about the oom path when adding</span>
<span class="quote">&gt;   &gt; something to the final __mmput path.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; at http://lkml.kernel.org/r/20170726054533.GA960@dhcp22.suse.cz , but</span>
<span class="quote">&gt; how can you guarantee that nobody will think about the oom path</span>
<span class="quote">&gt; when adding something to the final __mmput() path without thinking</span>
<span class="quote">&gt; about possibility of getting stuck waiting for memory allocation in</span>
<span class="quote">&gt; CONFIG_MMU=n environment?</span>

Look, I really appreciate your sentiment for for nommu platform but with
an absolute lack of _any_ oom reports on that platform that I am aware
of nor any reports about lockups during oom I am less than thrilled to
add a code to fix a problem which even might not exist. Nommu is usually
very special with a very specific workload running (e.g. no overcommit)
so I strongly suspect that any OOM theories are highly academic.

All I do care about is to not regress nommu as much as possible. So can
we get back to the proposed patch and updates I have done to address
your review feedback please?
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/include/linux/mm_types.h b/include/linux/mm_types.h</span>
<span class="p_header">index 7f384bb..374a2ae 100644</span>
<span class="p_header">--- a/include/linux/mm_types.h</span>
<span class="p_header">+++ b/include/linux/mm_types.h</span>
<span class="p_chunk">@@ -504,6 +504,9 @@</span> <span class="p_context"> struct mm_struct {</span>
 	atomic_long_t hugetlb_usage;
 #endif
 	struct work_struct async_put_work;
<span class="p_add">+#ifndef CONFIG_MMU</span>
<span class="p_add">+	unsigned long oom_victim_wait_timer;</span>
<span class="p_add">+#endif</span>
 } __randomize_layout;
 
 extern struct mm_struct init_mm;
<span class="p_header">diff --git a/mm/oom_kill.c b/mm/oom_kill.c</span>
<span class="p_header">index 9e8b4f0..dd6239d 100644</span>
<span class="p_header">--- a/mm/oom_kill.c</span>
<span class="p_header">+++ b/mm/oom_kill.c</span>
<span class="p_chunk">@@ -53,6 +53,17 @@</span> <span class="p_context"></span>
 
 DEFINE_MUTEX(oom_lock);
 
<span class="p_add">+static bool should_ignore_this_mm(struct mm_struct *mm)</span>
<span class="p_add">+{</span>
<span class="p_add">+#ifndef CONFIG_MMU</span>
<span class="p_add">+	if (!mm-&gt;oom_victim_wait_timer)</span>
<span class="p_add">+		mm-&gt;oom_victim_wait_timer = jiffies;</span>
<span class="p_add">+	else if (time_after(jiffies, mm-&gt;oom_victim_wait_timer + HZ))</span>
<span class="p_add">+		return true;</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	return test_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags);</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #ifdef CONFIG_NUMA
 /**
  * has_intersects_mems_allowed() - check task eligiblity for kill
<span class="p_chunk">@@ -188,9 +199,8 @@</span> <span class="p_context"> unsigned long oom_badness(struct task_struct *p, struct mem_cgroup *memcg,</span>
 	 * the middle of vfork
 	 */
 	adj = (long)p-&gt;signal-&gt;oom_score_adj;
<span class="p_del">-	if (adj == OOM_SCORE_ADJ_MIN ||</span>
<span class="p_del">-			test_bit(MMF_OOM_SKIP, &amp;p-&gt;mm-&gt;flags) ||</span>
<span class="p_del">-			in_vfork(p)) {</span>
<span class="p_add">+	if (adj == OOM_SCORE_ADJ_MIN || should_ignore_this_mm(p-&gt;mm) ||</span>
<span class="p_add">+	    in_vfork(p)) {</span>
 		task_unlock(p);
 		return 0;
 	}
<span class="p_chunk">@@ -303,7 +313,7 @@</span> <span class="p_context"> static int oom_evaluate_task(struct task_struct *task, void *arg)</span>
 	 * any memory is quite low.
 	 */
 	if (!is_sysrq_oom(oc) &amp;&amp; tsk_is_oom_victim(task)) {
<span class="p_del">-		if (test_bit(MMF_OOM_SKIP, &amp;task-&gt;signal-&gt;oom_mm-&gt;flags))</span>
<span class="p_add">+		if (should_ignore_this_mm(task-&gt;signal-&gt;oom_mm))</span>
 			goto next;
 		goto abort;
 	}
<span class="p_chunk">@@ -783,7 +793,7 @@</span> <span class="p_context"> static bool task_will_free_mem(struct task_struct *task)</span>
 	 * This task has already been drained by the oom reaper so there are
 	 * only small chances it will free some more
 	 */
<span class="p_del">-	if (test_bit(MMF_OOM_SKIP, &amp;mm-&gt;flags))</span>
<span class="p_add">+	if (should_ignore_this_mm(mm))</span>
 		return false;
 
 	if (atomic_read(&amp;mm-&gt;mm_users) &lt;= 1)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



