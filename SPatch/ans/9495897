
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/2] arm64: dma_mapping: allow PCI host driver to limit DMA mask - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/2] arm64: dma_mapping: allow PCI host driver to limit DMA mask</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 3, 2017, 11:13 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;5224989.KFLmAz9Gqk@wuerfel&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9495897/mbox/"
   >mbox</a>
|
   <a href="/patch/9495897/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9495897/">/patch/9495897/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	BDE1F606A7 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Jan 2017 23:14:20 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B0CE227165
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Jan 2017 23:14:20 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id A598027C7A; Tue,  3 Jan 2017 23:14:20 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.4 required=2.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RCVD_IN_SORBS_SPAM autolearn=unavailable version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 1962E27165
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Tue,  3 Jan 2017 23:14:20 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1761828AbdACXOH (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Tue, 3 Jan 2017 18:14:07 -0500
Received: from mout.kundenserver.de ([212.227.17.24]:57542 &quot;EHLO
	mout.kundenserver.de&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1759986AbdACXN4 (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Tue, 3 Jan 2017 18:13:56 -0500
Received: from wuerfel.localnet ([78.43.21.235]) by mrelayeu.kundenserver.de
	(mreue102 [212.227.15.145]) with ESMTPSA (Nemesis) id
	0M2dm9-1cfXQ33ax3-00sOhG; Wed, 04 Jan 2017 00:13:18 +0100
From: Arnd Bergmann &lt;arnd@arndb.de&gt;
To: linux-arm-kernel@lists.infradead.org
Cc: Will Deacon &lt;will.deacon@arm.com&gt;,
	Nikita Yushchenko &lt;nikita.yoush@cogentembedded.com&gt;,
	Catalin Marinas &lt;catalin.marinas@arm.com&gt;,
	linux-kernel@vger.kernel.org, linux-renesas-soc@vger.kernel.org,
	Simon Horman &lt;horms@verge.net.au&gt;, linux-pci@vger.kernel.org,
	Bjorn Helgaas &lt;bhelgaas@google.com&gt;, artemi.ivanov@cogentembedded.com
Subject: Re: [PATCH 1/2] arm64: dma_mapping: allow PCI host driver to limit
	DMA mask
Date: Wed, 04 Jan 2017 00:13:16 +0100
Message-ID: &lt;5224989.KFLmAz9Gqk@wuerfel&gt;
User-Agent: KMail/5.1.3 (Linux/4.4.0-34-generic; KDE/5.18.0; x86_64; ; )
In-Reply-To: &lt;20170103184444.GP6986@arm.com&gt;
References: &lt;1483044304-2085-1-git-send-email-nikita.yoush@cogentembedded.com&gt;
	&lt;20170103184444.GP6986@arm.com&gt;
MIME-Version: 1.0
Content-Transfer-Encoding: 7Bit
Content-Type: text/plain; charset=&quot;us-ascii&quot;
X-Provags-ID: V03:K0:PAW0V86kHa7epbuwVxmxSvYM66TOsIIejpajafMwos0Wk+P4xLw
	p5hbiIQb3kPigMRNFJtK5WNUJJLpFWTxcE95NVhXNh46Od7pgZ3a2Eh4bjQMppwjWoCGCaA
	rqtWjOPzFzuFxMyKlqclaSqZUJZKcA7uM9KIzewJbf8Ah1GfruTL8f3XGmyY7VC4FYIHoWb
	1biR4iyTI3HW0QkVpqmBQ==
X-UI-Out-Filterresults: notjunk:1; V01:K0:uusVk9TTGxY=:nczdbW2VDsvNSERs4VQzV6
	XfEC3egOZJ399PPKxo8etNDA8BFBc9wY8ixlPfuwlSOM807uZTF00gVhgNfryU8aKi3qhKdlE
	gXEvG9U/az30z1TR8IMjm7Je8kqjDVibm1DEPTeqiYuKSKzmymsaR647vA5AjMq8M24Y3FFPG
	VEX+5F5JOuN6NOzM7diJ4GkxN28yhnHf3imDIxju9dGEVl4swYZyQI0LCNHeHCB7aEv22W0s0
	6HxTCKoE52tUh2hPf7eKIh1FktyoeBkbmjLyX41UupY31S8fJbPGr4vxRPwEYY/+6N1+sMDGL
	22ak3tm5SdsNbAanVPWPGY8CHXby3vEIPwsXMNfQaZU2rwGDpkYO+JLI/tp9/jMp9Yw5zoQT7
	mQh1K1ebPrJzCiXL8AOJxghaTwbKzHQSs7Esb9qJMreeykwKfnxKyqFQvWQNo6Ay+GwHwh9dS
	UV6zV1TwaOgPEfbAWYqljBONznqNputVM/KMdiIX0s9ahyMVuWEizqaOKqA5X4ogigj6hK/bO
	cCHVjkwMABLiAni7+8oXogeGzK3LV2GVqYsxfnQmj7d3Xcbk41z9Vx7ij0MlnlUXH0fDXqVWV
	CICG4MEAdaj88i0eN6aZqnv3aw9V/gN7Fm41xAtFAgQ9ZFa1OBEVbGs+wTW4qgFhiB31q5jjq
	LHNK8qdYyxYQs48VYBsWHkd2DQLzIymk8zEPX/wYUOwxHvR8ZfcGvlArxe9E11BNvCNv0AWWo
	hljhTMY50Ll+580J
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 3, 2017, 11:13 p.m.</div>
<pre class="content">
On Tuesday, January 3, 2017 6:44:44 PM CET Will Deacon wrote:
<span class="quote">&gt; &gt; @@ -347,6 +348,16 @@ static int __swiotlb_get_sgtable(struct device *dev, struct sg_table *sgt,</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  static int __swiotlb_dma_supported(struct device *hwdev, u64 mask)</span>
<span class="quote">&gt; &gt;  {</span>
<span class="quote">&gt; &gt; +#ifdef CONFIG_PCI</span>
<span class="quote">&gt; &gt; +     if (dev_is_pci(hwdev)) {</span>
<span class="quote">&gt; &gt; +             struct pci_dev *pdev = to_pci_dev(hwdev);</span>
<span class="quote">&gt; &gt; +             struct pci_host_bridge *br = pci_find_host_bridge(pdev-&gt;bus);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +             if (br-&gt;dev.dma_mask &amp;&amp; (*br-&gt;dev.dma_mask) &amp;&amp;</span>
<span class="quote">&gt; &gt; +                             (mask &amp; (*br-&gt;dev.dma_mask)) != mask)</span>
<span class="quote">&gt; &gt; +                     return 0;</span>
<span class="quote">&gt; &gt; +     }</span>
<span class="quote">&gt; &gt; +#endif</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Hmm, but this makes it look like the problem is both arm64 and swiotlb</span>
<span class="quote">&gt; specific, when in reality it&#39;s not. Perhaps another hack you could try</span>
<span class="quote">&gt; would be to register a PCI bus notifier in the host bridge looking for</span>
<span class="quote">&gt; BUS_NOTIFY_BIND_DRIVER, then you could proxy the DMA ops for each child</span>
<span class="quote">&gt; device before the driver has probed, but adding a dma_set_mask callback</span>
<span class="quote">&gt; to limit the mask to what you need?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I agree that it would be better if dma_set_mask handled all of this</span>
<span class="quote">&gt; transparently, but it&#39;s all based on the underlying ops rather than the</span>
<span class="quote">&gt; bus type.</span>

This is what I prototyped a long time ago when this first came up.
I still think this needs to be solved properly for all of arm64, not
with a PCI specific hack, and in particular not using notifiers.

	Arnd

commit 9a57d58d116800a535510053136c6dd7a9c26e25
Author: Arnd Bergmann &lt;arnd@arndb.de&gt;
Date:   Tue Nov 17 14:06:55 2015 +0100

    [EXPERIMENTAL] ARM64: check implement dma_set_mask
    
    Needs work for coherent mask
<span class="signed-off-by">    
    Signed-off-by: Arnd Bergmann &lt;arnd@arndb.de&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 4, 2017, 6:24 a.m.</div>
<pre class="content">
<span class="quote">&gt; commit 9a57d58d116800a535510053136c6dd7a9c26e25</span>
<span class="quote">&gt; Author: Arnd Bergmann &lt;arnd@arndb.de&gt;</span>
<span class="quote">&gt; Date:   Tue Nov 17 14:06:55 2015 +0100</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;     [EXPERIMENTAL] ARM64: check implement dma_set_mask</span>
<span class="quote">&gt;     </span>
<span class="quote">&gt;     Needs work for coherent mask</span>
<span class="quote">&gt;     </span>
<span class="quote">&gt;     Signed-off-by: Arnd Bergmann &lt;arnd@arndb.de&gt;</span>

Unfortunately this is far incomplete
<span class="quote">
&gt; @@ -957,6 +983,18 @@ void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,</span>
<span class="quote">&gt;  	if (!dev-&gt;archdata.dma_ops)</span>
<span class="quote">&gt;  		dev-&gt;archdata.dma_ops = &amp;swiotlb_dma_ops;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * we don&#39;t yet support buses that have a non-zero mapping.</span>
<span class="quote">&gt; +	 *  Let&#39;s hope we won&#39;t need it</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	WARN_ON(dma_base != 0);</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +	/*</span>
<span class="quote">&gt; +	 * Whatever the parent bus can set. A device must not set</span>
<span class="quote">&gt; +	 * a DMA mask larger than this.</span>
<span class="quote">&gt; +	 */</span>
<span class="quote">&gt; +	dev-&gt;archdata.parent_dma_mask = size;</span>
<span class="quote">&gt; +</span>

... because size/mask passed here for PCI devices are meaningless.

For OF platforms, this is called via of_dma_configure(), that checks
dma-ranges of node that is *parent* for host bridge. Host bridge
currently does not control this at all.

In current device trees no dma-ranges is defined for nodes that are
parents to pci host bridges. This will make of_dma_configure() to fall
back to 32-bit size for all devices on all current platforms.  Thus
applying this patch will immediately break 64-bit dma masks on all
hardware that supports it.


Also related: dma-ranges property used by several pci host bridges is
*not* compatible with &quot;legacy&quot; dma-ranges parsed by of_get_dma_range() -
former uses additional flags word at beginning.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 4, 2017, 1:29 p.m.</div>
<pre class="content">
On Wednesday, January 4, 2017 9:24:09 AM CET Nikita Yushchenko wrote:
<span class="quote">&gt; &gt; commit 9a57d58d116800a535510053136c6dd7a9c26e25</span>
<span class="quote">&gt; &gt; Author: Arnd Bergmann &lt;arnd@arndb.de&gt;</span>
<span class="quote">&gt; &gt; Date:   Tue Nov 17 14:06:55 2015 +0100</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt;     [EXPERIMENTAL] ARM64: check implement dma_set_mask</span>
<span class="quote">&gt; &gt;     </span>
<span class="quote">&gt; &gt;     Needs work for coherent mask</span>
<span class="quote">&gt; &gt;     </span>
<span class="quote">&gt; &gt;     Signed-off-by: Arnd Bergmann &lt;arnd@arndb.de&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Unfortunately this is far incomplete</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; @@ -957,6 +983,18 @@ void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,</span>
<span class="quote">&gt; &gt;  	if (!dev-&gt;archdata.dma_ops)</span>
<span class="quote">&gt; &gt;  		dev-&gt;archdata.dma_ops = &amp;swiotlb_dma_ops;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * we don&#39;t yet support buses that have a non-zero mapping.</span>
<span class="quote">&gt; &gt; +	 *  Let&#39;s hope we won&#39;t need it</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	WARN_ON(dma_base != 0);</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; &gt; +	/*</span>
<span class="quote">&gt; &gt; +	 * Whatever the parent bus can set. A device must not set</span>
<span class="quote">&gt; &gt; +	 * a DMA mask larger than this.</span>
<span class="quote">&gt; &gt; +	 */</span>
<span class="quote">&gt; &gt; +	dev-&gt;archdata.parent_dma_mask = size;</span>
<span class="quote">&gt; &gt; +</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ... because size/mask passed here for PCI devices are meaningless.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For OF platforms, this is called via of_dma_configure(), that checks</span>
<span class="quote">&gt; dma-ranges of node that is *parent* for host bridge. Host bridge</span>
<span class="quote">&gt; currently does not control this at all.</span>

We need to think about this a bit. Is it actually the PCI host
bridge that limits the ranges here, or the bus that it is connected
to. In the latter case, the caller needs to be adapted to handle
both.
<span class="quote">
&gt; In current device trees no dma-ranges is defined for nodes that are</span>
<span class="quote">&gt; parents to pci host bridges. This will make of_dma_configure() to fall</span>
<span class="quote">&gt; back to 32-bit size for all devices on all current platforms.  Thus</span>
<span class="quote">&gt; applying this patch will immediately break 64-bit dma masks on all</span>
<span class="quote">&gt; hardware that supports it.</span>

No, it won&#39;t break it, it will just fall back to swiotlb for all the
ones that are lacking the dma-ranges property. I think this is correct
behavior.
<span class="quote">
&gt; Also related: dma-ranges property used by several pci host bridges is</span>
<span class="quote">&gt; *not* compatible with &quot;legacy&quot; dma-ranges parsed by of_get_dma_range() -</span>
<span class="quote">&gt; former uses additional flags word at beginning.</span>

Can you elaborate? Do we have PCI host bridges that use wrongly formatted
dma-ranges properties?

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 4, 2017, 2:30 p.m.</div>
<pre class="content">
<span class="quote">&gt;&gt; For OF platforms, this is called via of_dma_configure(), that checks</span>
<span class="quote">&gt;&gt; dma-ranges of node that is *parent* for host bridge. Host bridge</span>
<span class="quote">&gt;&gt; currently does not control this at all.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We need to think about this a bit. Is it actually the PCI host</span>
<span class="quote">&gt; bridge that limits the ranges here, or the bus that it is connected</span>
<span class="quote">&gt; to. In the latter case, the caller needs to be adapted to handle</span>
<span class="quote">&gt; both.</span>

In r-car case, I&#39;m not sure what is the source of limitation at physical
level.

pcie-rcar driver configures ranges for PCIe inbound transactions based
on dma-ranges property in it&#39;s device tree node. In the current device
tree for this platform, that only contains one range and it is in lower
memory.

NVMe driver tries i/o to kmalloc()ed area. That returns 0x5xxxxxxxx
addresses here. As a quick experiment, I tried to add second range to
pcie-rcar&#39;s dma-ranges to cover 0x5xxxxxxxx area - but that did not make
DMA to high addresses working.

My current understanding is that host bridge hardware module can&#39;t
handle inbound transactions to PCI addresses above 4G - and this
limitations comes from host bridge itself.

I&#39;ve read somewhere in the lists that pcie-rcar hardware is &quot;32-bit&quot; -
but I don&#39;t remember where, and don&#39;t know lowlevel details. Maybe
somebody from linux-renesas can elaborate?
<span class="quote">

&gt;&gt; In current device trees no dma-ranges is defined for nodes that are</span>
<span class="quote">&gt;&gt; parents to pci host bridges. This will make of_dma_configure() to fall</span>
<span class="quote">&gt;&gt; back to 32-bit size for all devices on all current platforms.  Thus</span>
<span class="quote">&gt;&gt; applying this patch will immediately break 64-bit dma masks on all</span>
<span class="quote">&gt;&gt; hardware that supports it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; No, it won&#39;t break it, it will just fall back to swiotlb for all the</span>
<span class="quote">&gt; ones that are lacking the dma-ranges property. I think this is correct</span>
<span class="quote">&gt; behavior.</span>

I&#39;d say - for all ones that have parents without dma-ranges property.

As of 4.10-rc2, I see only two definitions of wide parent dma-ranges
under arch/arm64/boot/dts/ - in amd/amd-seattle-soc.dtsi and
apm/apm-storm.dtsi

Are these the only arm64 platforms that can to DMA to high addresses?
I&#39;m not arm64 expert but I&#39;d be surprised if that&#39;s the case.
<span class="quote">

&gt;&gt; Also related: dma-ranges property used by several pci host bridges is</span>
<span class="quote">&gt;&gt; *not* compatible with &quot;legacy&quot; dma-ranges parsed by of_get_dma_range() -</span>
<span class="quote">&gt;&gt; former uses additional flags word at beginning.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can you elaborate? Do we have PCI host bridges that use wrongly formatted</span>
<span class="quote">&gt; dma-ranges properties?</span>

of_dma_get_range() expects &lt;dma_addr cpu_addr size&gt; format.

pcie-rcar.c, pci-rcar-gen2.c, pci-xgene.c and pcie-iproc.c from
drivers/pci/host/ all parse dma-ranges using of_pci_range_parser that
uses &lt;flags pci-addr cpu-addr size&gt; format - i.e. something different
from what of_dma_get_range() uses.


Nikita
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 4, 2017, 2:46 p.m.</div>
<pre class="content">
On Wednesday, January 4, 2017 5:30:19 PM CET Nikita Yushchenko wrote:
<span class="quote">&gt; &gt;&gt; For OF platforms, this is called via of_dma_configure(), that checks</span>
<span class="quote">&gt; &gt;&gt; dma-ranges of node that is *parent* for host bridge. Host bridge</span>
<span class="quote">&gt; &gt;&gt; currently does not control this at all.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We need to think about this a bit. Is it actually the PCI host</span>
<span class="quote">&gt; &gt; bridge that limits the ranges here, or the bus that it is connected</span>
<span class="quote">&gt; &gt; to. In the latter case, the caller needs to be adapted to handle</span>
<span class="quote">&gt; &gt; both.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; In r-car case, I&#39;m not sure what is the source of limitation at physical</span>
<span class="quote">&gt; level.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; pcie-rcar driver configures ranges for PCIe inbound transactions based</span>
<span class="quote">&gt; on dma-ranges property in it&#39;s device tree node. In the current device</span>
<span class="quote">&gt; tree for this platform, that only contains one range and it is in lower</span>
<span class="quote">&gt; memory.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NVMe driver tries i/o to kmalloc()ed area. That returns 0x5xxxxxxxx</span>
<span class="quote">&gt; addresses here. As a quick experiment, I tried to add second range to</span>
<span class="quote">&gt; pcie-rcar&#39;s dma-ranges to cover 0x5xxxxxxxx area - but that did not make</span>
<span class="quote">&gt; DMA to high addresses working.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; My current understanding is that host bridge hardware module can&#39;t</span>
<span class="quote">&gt; handle inbound transactions to PCI addresses above 4G - and this</span>
<span class="quote">&gt; limitations comes from host bridge itself.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve read somewhere in the lists that pcie-rcar hardware is &quot;32-bit&quot; -</span>
<span class="quote">&gt; but I don&#39;t remember where, and don&#39;t know lowlevel details. Maybe</span>
<span class="quote">&gt; somebody from linux-renesas can elaborate?</span>

Just a guess, but if the inbound translation windows in the host
bridge are wider than 32-bit, the reason for setting up a single
32-bit window is probably because that is what the parent bus supports.
<span class="quote">
&gt; &gt;&gt; In current device trees no dma-ranges is defined for nodes that are</span>
<span class="quote">&gt; &gt;&gt; parents to pci host bridges. This will make of_dma_configure() to fall</span>
<span class="quote">&gt; &gt;&gt; back to 32-bit size for all devices on all current platforms.  Thus</span>
<span class="quote">&gt; &gt;&gt; applying this patch will immediately break 64-bit dma masks on all</span>
<span class="quote">&gt; &gt;&gt; hardware that supports it.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; No, it won&#39;t break it, it will just fall back to swiotlb for all the</span>
<span class="quote">&gt; &gt; ones that are lacking the dma-ranges property. I think this is correct</span>
<span class="quote">&gt; &gt; behavior.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;d say - for all ones that have parents without dma-ranges property.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As of 4.10-rc2, I see only two definitions of wide parent dma-ranges</span>
<span class="quote">&gt; under arch/arm64/boot/dts/ - in amd/amd-seattle-soc.dtsi and</span>
<span class="quote">&gt; apm/apm-storm.dtsi</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Are these the only arm64 platforms that can to DMA to high addresses?</span>
<span class="quote">&gt; I&#39;m not arm64 expert but I&#39;d be surprised if that&#39;s the case.</span>

It&#39;s likely that a few others also do high DMA, but a lot of arm64
chips are actually derived from earlier 32-bit chips and don&#39;t
even support any RAM above 4GB, as well as having a lot of 32-bit
DMA masters.
<span class="quote">
&gt; &gt;&gt; Also related: dma-ranges property used by several pci host bridges is</span>
<span class="quote">&gt; &gt;&gt; *not* compatible with &quot;legacy&quot; dma-ranges parsed by of_get_dma_range() -</span>
<span class="quote">&gt; &gt;&gt; former uses additional flags word at beginning.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Can you elaborate? Do we have PCI host bridges that use wrongly formatted</span>
<span class="quote">&gt; &gt; dma-ranges properties?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; of_dma_get_range() expects &lt;dma_addr cpu_addr size&gt; format.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; pcie-rcar.c, pci-rcar-gen2.c, pci-xgene.c and pcie-iproc.c from</span>
<span class="quote">&gt; drivers/pci/host/ all parse dma-ranges using of_pci_range_parser that</span>
<span class="quote">&gt; uses &lt;flags pci-addr cpu-addr size&gt; format - i.e. something different</span>
<span class="quote">&gt; from what of_dma_get_range() uses.</span>

The &quot;dma_addr&quot; here is expressed in terms of #address-cells of the
bus it is in, and that is &quot;3&quot; in case of PCI, where the first 32-bit
word is a bit pattern containing various things, and the other two
cells are a 64-bit address. I think this is correct, but we may
need to add some special handling for parsing PCI host bridges
in of_dma_get_range, to ensure we actually look at translations for
the memory space.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 4, 2017, 3:29 p.m.</div>
<pre class="content">
<span class="quote">&gt;&gt;&gt;&gt; For OF platforms, this is called via of_dma_configure(), that checks</span>
<span class="quote">&gt;&gt;&gt;&gt; dma-ranges of node that is *parent* for host bridge. Host bridge</span>
<span class="quote">&gt;&gt;&gt;&gt; currently does not control this at all.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; We need to think about this a bit. Is it actually the PCI host</span>
<span class="quote">&gt;&gt;&gt; bridge that limits the ranges here, or the bus that it is connected</span>
<span class="quote">&gt;&gt;&gt; to. In the latter case, the caller needs to be adapted to handle</span>
<span class="quote">&gt;&gt;&gt; both.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; In r-car case, I&#39;m not sure what is the source of limitation at physical</span>
<span class="quote">&gt;&gt; level.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; pcie-rcar driver configures ranges for PCIe inbound transactions based</span>
<span class="quote">&gt;&gt; on dma-ranges property in it&#39;s device tree node. In the current device</span>
<span class="quote">&gt;&gt; tree for this platform, that only contains one range and it is in lower</span>
<span class="quote">&gt;&gt; memory.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; NVMe driver tries i/o to kmalloc()ed area. That returns 0x5xxxxxxxx</span>
<span class="quote">&gt;&gt; addresses here. As a quick experiment, I tried to add second range to</span>
<span class="quote">&gt;&gt; pcie-rcar&#39;s dma-ranges to cover 0x5xxxxxxxx area - but that did not make</span>
<span class="quote">&gt;&gt; DMA to high addresses working.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; My current understanding is that host bridge hardware module can&#39;t</span>
<span class="quote">&gt;&gt; handle inbound transactions to PCI addresses above 4G - and this</span>
<span class="quote">&gt;&gt; limitations comes from host bridge itself.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I&#39;ve read somewhere in the lists that pcie-rcar hardware is &quot;32-bit&quot; -</span>
<span class="quote">&gt;&gt; but I don&#39;t remember where, and don&#39;t know lowlevel details. Maybe</span>
<span class="quote">&gt;&gt; somebody from linux-renesas can elaborate?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just a guess, but if the inbound translation windows in the host</span>
<span class="quote">&gt; bridge are wider than 32-bit, the reason for setting up a single</span>
<span class="quote">&gt; 32-bit window is probably because that is what the parent bus supports.</span>

Well anyway applying patch similar to your&#39;s will fix pcie-rcar + nvme
case - thus I don&#39;t object :)   But it can break other cases ...

But why do you hook at set_dma_mask() and overwrite mask inside, instead
of hooking at dma_supported() and rejecting unsupported mask?

I think later is better, because it lets drivers to handle unsupported
high-dma case, like documented in DMA-API_HOWTO.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=176">Arnd Bergmann</a> - Jan. 6, 2017, 11:10 a.m.</div>
<pre class="content">
On Wednesday, January 4, 2017 6:29:39 PM CET Nikita Yushchenko wrote:
<span class="quote">
&gt; &gt; Just a guess, but if the inbound translation windows in the host</span>
<span class="quote">&gt; &gt; bridge are wider than 32-bit, the reason for setting up a single</span>
<span class="quote">&gt; &gt; 32-bit window is probably because that is what the parent bus supports.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Well anyway applying patch similar to your&#39;s will fix pcie-rcar + nvme</span>
<span class="quote">&gt; case - thus I don&#39;t object :)   But it can break other cases ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But why do you hook at set_dma_mask() and overwrite mask inside, instead</span>
<span class="quote">&gt; of hooking at dma_supported() and rejecting unsupported mask?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think later is better, because it lets drivers to handle unsupported</span>
<span class="quote">&gt; high-dma case, like documented in DMA-API_HOWTO.</span>

I think the behavior I put in there is required for swiotlb to make
sense, otherwise you would rely on the driver to handle dma_set_mask()
failure gracefully with its own bounce buffers (as network and
scsi drivers do but others don&#39;t).

Having swiotlb or iommu enabled should result in dma_set_mask() always
succeeding unless the mask is too small to cover the swiotlb
bounce buffer area or the iommu virtual address space. This behavior
is particularly important in case the bus address space is narrower
than 32-bit, as we have to guarantee that the fallback to 32-bit
DMA always succeeds. There are also a lot of drivers that try to
set a 64-bit mask but don&#39;t implement bounce buffers for streaming
mappings if that fails, and swiotlb is what we use to make those
drivers work.

And yes, the API is a horrible mess.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 6, 2017, 1:47 p.m.</div>
<pre class="content">
<span class="quote">&gt;&gt;&gt; Just a guess, but if the inbound translation windows in the host</span>
<span class="quote">&gt;&gt;&gt; bridge are wider than 32-bit, the reason for setting up a single</span>
<span class="quote">&gt;&gt;&gt; 32-bit window is probably because that is what the parent bus supports.</span>

I&#39;ve re-checked rcar-pcie hardware documentation.

It indeed mentions that AXI bus it sits on is 32-bit.
<span class="quote">

&gt;&gt; Well anyway applying patch similar to your&#39;s will fix pcie-rcar + nvme</span>
<span class="quote">&gt;&gt; case - thus I don&#39;t object :)   But it can break other cases ...</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; But why do you hook at set_dma_mask() and overwrite mask inside, instead</span>
<span class="quote">&gt;&gt; of hooking at dma_supported() and rejecting unsupported mask?</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; I think later is better, because it lets drivers to handle unsupported</span>
<span class="quote">&gt;&gt; high-dma case, like documented in DMA-API_HOWTO.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think the behavior I put in there is required for swiotlb to make</span>
<span class="quote">&gt; sense, otherwise you would rely on the driver to handle dma_set_mask()</span>
<span class="quote">&gt; failure gracefully with its own bounce buffers (as network and</span>
<span class="quote">&gt; scsi drivers do but others don&#39;t).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Having swiotlb or iommu enabled should result in dma_set_mask() always</span>
<span class="quote">&gt; succeeding unless the mask is too small to cover the swiotlb</span>
<span class="quote">&gt; bounce buffer area or the iommu virtual address space. This behavior</span>
<span class="quote">&gt; is particularly important in case the bus address space is narrower</span>
<span class="quote">&gt; than 32-bit, as we have to guarantee that the fallback to 32-bit</span>
<span class="quote">&gt; DMA always succeeds. There are also a lot of drivers that try to</span>
<span class="quote">&gt; set a 64-bit mask but don&#39;t implement bounce buffers for streaming</span>
<span class="quote">&gt; mappings if that fails, and swiotlb is what we use to make those</span>
<span class="quote">&gt; drivers work.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; And yes, the API is a horrible mess.</span>

With my patch applied and thus 32bit dma_mask set for NVMe device, I do
see high addresses passed to dma_map_*() routines and handled by
swiotlb. Thus your statement that behavior &quot;succeed 64bit dma_set_mask()
operation but silently replace mask behind the scene&quot; is required for
swiotlb to be used, does not match reality.

It can be interpreted as a breakage elsewhere, but it&#39;s hard to point
particular &quot;root cause&quot;. The entire infrastructure to allocate and use
DMA memory is messy.

Still current code does not work, thus fix is needed.

Perhaps need to introduce some generic API to &quot;allocate memory best
suited for DMA to particular device&quot;, and fix allocation points (in
drivers, filesystems, etc) to use it. Such an API could try to allocate
area that can be DMAed by hardware, and fallback to other memory that
can be used via swiotlb or other bounce buffer implementation.

But for now, have to stay with dma masks. Will follow-up with a patch
based on your but with coherent mask handling added.

Nikita
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=59301">Arnd Bergmann</a> - Jan. 9, 2017, 2:05 p.m.</div>
<pre class="content">
On Friday, January 6, 2017 4:47:59 PM CET Nikita Yushchenko wrote:
<span class="quote">&gt; &gt;&gt;&gt; Just a guess, but if the inbound translation windows in the host</span>
<span class="quote">&gt; &gt;&gt;&gt; bridge are wider than 32-bit, the reason for setting up a single</span>
<span class="quote">&gt; &gt;&gt;&gt; 32-bit window is probably because that is what the parent bus supports.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I&#39;ve re-checked rcar-pcie hardware documentation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It indeed mentions that AXI bus it sits on is 32-bit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt;&gt; Well anyway applying patch similar to your&#39;s will fix pcie-rcar + nvme</span>
<span class="quote">&gt; &gt;&gt; case - thus I don&#39;t object :)   But it can break other cases ...</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; But why do you hook at set_dma_mask() and overwrite mask inside, instead</span>
<span class="quote">&gt; &gt;&gt; of hooking at dma_supported() and rejecting unsupported mask?</span>
<span class="quote">&gt; &gt;&gt;</span>
<span class="quote">&gt; &gt;&gt; I think later is better, because it lets drivers to handle unsupported</span>
<span class="quote">&gt; &gt;&gt; high-dma case, like documented in DMA-API_HOWTO.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think the behavior I put in there is required for swiotlb to make</span>
<span class="quote">&gt; &gt; sense, otherwise you would rely on the driver to handle dma_set_mask()</span>
<span class="quote">&gt; &gt; failure gracefully with its own bounce buffers (as network and</span>
<span class="quote">&gt; &gt; scsi drivers do but others don&#39;t).</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Having swiotlb or iommu enabled should result in dma_set_mask() always</span>
<span class="quote">&gt; &gt; succeeding unless the mask is too small to cover the swiotlb</span>
<span class="quote">&gt; &gt; bounce buffer area or the iommu virtual address space. This behavior</span>
<span class="quote">&gt; &gt; is particularly important in case the bus address space is narrower</span>
<span class="quote">&gt; &gt; than 32-bit, as we have to guarantee that the fallback to 32-bit</span>
<span class="quote">&gt; &gt; DMA always succeeds. There are also a lot of drivers that try to</span>
<span class="quote">&gt; &gt; set a 64-bit mask but don&#39;t implement bounce buffers for streaming</span>
<span class="quote">&gt; &gt; mappings if that fails, and swiotlb is what we use to make those</span>
<span class="quote">&gt; &gt; drivers work.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; And yes, the API is a horrible mess.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; With my patch applied and thus 32bit dma_mask set for NVMe device, I do</span>
<span class="quote">&gt; see high addresses passed to dma_map_*() routines and handled by</span>
<span class="quote">&gt; swiotlb. Thus your statement that behavior &quot;succeed 64bit dma_set_mask()</span>
<span class="quote">&gt; operation but silently replace mask behind the scene&quot; is required for</span>
<span class="quote">&gt; swiotlb to be used, does not match reality.</span>

See my point about drivers that don&#39;t implement bounce buffering.
Apparently NVMe is one of them, unlike the SATA/SCSI/MMC storage
drivers that do their own thing.

The problem again is the inconsistency of the API.
<span class="quote">
&gt; It can be interpreted as a breakage elsewhere, but it&#39;s hard to point</span>
<span class="quote">&gt; particular &quot;root cause&quot;. The entire infrastructure to allocate and use</span>
<span class="quote">&gt; DMA memory is messy.</span>

Absolutely.

What I think happened here in chronological order is:

- In the old days, 64-bit architectures tended to use an IOMMU 
  all the time to work around 32-bit limitations on DMA masters
- Some architectures had no IOMMU that fully solved this and the
  dma-mapping API required drivers to set the right mask and check
  the return code. If this failed, the driver needed to use its
  own bounce buffers as network and scsi do. See also the
  grossly misnamed &quot;PCI_DMA_BUS_IS_PHYS&quot; macro.
- As we never had support for bounce buffers in all drivers, and
  early 64-bit Intel machines had no IOMMU, the swiotlb code was
  introduced as a workaround, so we can use the IOMMU case without
  driver specific bounce buffers everywhere
- As most of the important 64-bit architectures (x86, arm64, powerpc)
  now always have either IOMMU or swiotlb enabled, drivers like
  NVMe started relying on it, and no longer handle a dma_set_mask
  failure properly.

We may need to audit how drivers typically handle dma_set_mask()
failure. The NVMe driver in its current state will probably cause
silent data corruption when used on a 64-bit architecture that has
a 32-bit bus but neither swiotlb nor iommu enabled at runtime.

I would argue that the driver should be fixed to either refuse
working in that configuration to avoid data corruption, or that
it should implement bounce buffering like SCSI does. If we make it
simply not work, then your suggestion of making dma_set_mask()
fail will break your system in a different way.
<span class="quote">
&gt; Still current code does not work, thus fix is needed.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Perhaps need to introduce some generic API to &quot;allocate memory best</span>
<span class="quote">&gt; suited for DMA to particular device&quot;, and fix allocation points (in</span>
<span class="quote">&gt; drivers, filesystems, etc) to use it. Such an API could try to allocate</span>
<span class="quote">&gt; area that can be DMAed by hardware, and fallback to other memory that</span>
<span class="quote">&gt; can be used via swiotlb or other bounce buffer implementation.</span>

The DMA mapping API is meant to do this, but we can definitely improve
it or clarify some of the rules.
<span class="quote"> 
&gt; But for now, have to stay with dma masks. Will follow-up with a patch</span>
<span class="quote">&gt; based on your but with coherent mask handling added.</span>

Ok.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 9, 2017, 8:34 p.m.</div>
<pre class="content">
[CCing NVMe maintainers since we are discussion issues in that driver]
<span class="quote">
&gt;&gt; With my patch applied and thus 32bit dma_mask set for NVMe device, I do</span>
<span class="quote">&gt;&gt; see high addresses passed to dma_map_*() routines and handled by</span>
<span class="quote">&gt;&gt; swiotlb. Thus your statement that behavior &quot;succeed 64bit dma_set_mask()</span>
<span class="quote">&gt;&gt; operation but silently replace mask behind the scene&quot; is required for</span>
<span class="quote">&gt;&gt; swiotlb to be used, does not match reality.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; See my point about drivers that don&#39;t implement bounce buffering.</span>
<span class="quote">&gt; Apparently NVMe is one of them, unlike the SATA/SCSI/MMC storage</span>
<span class="quote">&gt; drivers that do their own thing.</span>

I believe the bounce buffering code you refer to is not in SATA/SCSI/MMC
but in block layer, in particular it should be controlled by
blk_queue_bounce_limit().  [Yes there is CONFIG_MMC_BLOCK_BOUNCE but it
is something completely different, namely it is for request merging for
hw not supporting scatter-gather].  And NVMe also uses block layer and
thus should get same support.

But blk_queue_bounce_limit() is somewhat broken, it has very strange
code under #if BITS_PER_LONG == 64 that makes setting max_addr to
0xffffffff not working if max_low_pfn is above 4G.

Maybe fixing that, together with making NVMe use this API, could stop it
from issuing dma_map()s of addresses beyond mask.
<span class="quote">

&gt; What I think happened here in chronological order is:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - In the old days, 64-bit architectures tended to use an IOMMU </span>
<span class="quote">&gt;   all the time to work around 32-bit limitations on DMA masters</span>
<span class="quote">&gt; - Some architectures had no IOMMU that fully solved this and the</span>
<span class="quote">&gt;   dma-mapping API required drivers to set the right mask and check</span>
<span class="quote">&gt;   the return code. If this failed, the driver needed to use its</span>
<span class="quote">&gt;   own bounce buffers as network and scsi do. See also the</span>
<span class="quote">&gt;   grossly misnamed &quot;PCI_DMA_BUS_IS_PHYS&quot; macro.</span>
<span class="quote">&gt; - As we never had support for bounce buffers in all drivers, and</span>
<span class="quote">&gt;   early 64-bit Intel machines had no IOMMU, the swiotlb code was</span>
<span class="quote">&gt;   introduced as a workaround, so we can use the IOMMU case without</span>
<span class="quote">&gt;   driver specific bounce buffers everywhere</span>
<span class="quote">&gt; - As most of the important 64-bit architectures (x86, arm64, powerpc)</span>
<span class="quote">&gt;   now always have either IOMMU or swiotlb enabled, drivers like</span>
<span class="quote">&gt;   NVMe started relying on it, and no longer handle a dma_set_mask</span>
<span class="quote">&gt;   failure properly.</span>

... and architectures started to add to this breakage, not handling
dma_set_mask() as documented.


As for PCI_DMA_BUS_IS_PHYS - ironically, what all current usages of this
macro in the kernel do is - *disable* bounce buffers in block layer if
PCI_DMA_BUS_IS_PHYS is zero.  Defining it to zero (as arm64 currently
does) on system with memory above 4G makes all block drivers to depend
on swiotlb (or iommu). Affected drivers are SCSI and IDE.
<span class="quote">
&gt; We may need to audit how drivers typically handle dma_set_mask()</span>
<span class="quote">&gt; failure. The NVMe driver in its current state will probably cause</span>
<span class="quote">&gt; silent data corruption when used on a 64-bit architecture that has</span>
<span class="quote">&gt; a 32-bit bus but neither swiotlb nor iommu enabled at runtime.</span>

With current code NVME causes system memory breakage even if swiotlb is
there - because it&#39;s dma_set_mask_and_coherent(DMA_BIT_MASK(64)) call
has effect of silent disable of swiotlb.
<span class="quote">

&gt; I would argue that the driver should be fixed to either refuse</span>
<span class="quote">&gt; working in that configuration to avoid data corruption, or that</span>
<span class="quote">&gt; it should implement bounce buffering like SCSI does.</span>

Difference from &quot;SCSI&quot; (actually - from block drivers that work) is in
that dma_set_mask_and_coherent(DMA_BIT_MASK(64)) call: driver that does
not do it works, driver that does it fails.

Per documentation, driver *should* do it if it&#39;s hardware supports
64-bit dma, and platform *should* either fail this call, or ensure that
64-bit addresses can be dma_map()ed successfully.

So what we have on arm64 is - drivers that follow documented procedure
fail, drivers that don&#39;t follow it work, That&#39;s nonsense.
<span class="quote">
&gt; If we make it</span>
<span class="quote">&gt; simply not work, then your suggestion of making dma_set_mask()</span>
<span class="quote">&gt; fail will break your system in a different way.</span>

Proper fix should fix *both* architecture and NVMe.

- architecture should stop breaking 64-bit DMA when driver attempts to
set 64-bit dma mask,

- NVMe should issue proper blk_queue_bounce_limit() call based on what
is actually set mask,

- and blk_queue_bounce_limit() should also be fixed to actually set
0xffffffff limit, instead of replacing it with (max_low_pfn &lt;&lt;
PAGE_SHIFT) as it does now.
<span class="quote">

&gt;&gt; Still current code does not work, thus fix is needed.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Perhaps need to introduce some generic API to &quot;allocate memory best</span>
<span class="quote">&gt;&gt; suited for DMA to particular device&quot;, and fix allocation points (in</span>
<span class="quote">&gt;&gt; drivers, filesystems, etc) to use it. Such an API could try to allocate</span>
<span class="quote">&gt;&gt; area that can be DMAed by hardware, and fallback to other memory that</span>
<span class="quote">&gt;&gt; can be used via swiotlb or other bounce buffer implementation.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The DMA mapping API is meant to do this, but we can definitely improve</span>
<span class="quote">&gt; it or clarify some of the rules.</span>

DMA mapping API can&#39;t help here, it&#39;s about mapping, not about allocation.

What I mean is some API to allocate memory for use with streaming DMA in
such way that bounce buffers won&#39;t be needed. There are many cases when
at buffer allocation time, it is already known that buffer will be used
for DMA with particular device. Bounce buffers will still be needed
cases when no such information is available at allocation time, or when
there is no directly-DMAable memory available at allocation time.
<span class="quote">
&gt;&gt; But for now, have to stay with dma masks. Will follow-up with a patch</span>
<span class="quote">&gt;&gt; based on your but with coherent mask handling added.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ok.</span>

Already posted. Can we have that merged? At least it will make things to
stop breaking memory and start working.

Nikita
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - Jan. 9, 2017, 8:57 p.m.</div>
<pre class="content">
On Mon, Jan 09, 2017 at 11:34:55PM +0300, Nikita Yushchenko wrote:
<span class="quote">&gt; I believe the bounce buffering code you refer to is not in SATA/SCSI/MMC</span>
<span class="quote">&gt; but in block layer, in particular it should be controlled by</span>
<span class="quote">&gt; blk_queue_bounce_limit().  [Yes there is CONFIG_MMC_BLOCK_BOUNCE but it</span>
<span class="quote">&gt; is something completely different, namely it is for request merging for</span>
<span class="quote">&gt; hw not supporting scatter-gather].  And NVMe also uses block layer and</span>
<span class="quote">&gt; thus should get same support.</span>

NVMe shouldn&#39;t have to call blk_queue_bounce_limit - 
blk_queue_bounce_limit is to set the DMA addressing limit of the device.
NVMe devices must support unlimited 64-bit addressing and thus calling
blk_queue_bounce_limit from NVMe does not make sense.

That being said currently the default for a queue without a call
to blk_queue_make_request which does the wrong thing on highmem
setups, so we should fix it.  In fact BLK_BOUNCE_HIGH as-is doesn&#39;t
really make much sense these days as no driver should ever dereference
pages passed to it directly.
<span class="quote">
&gt; Maybe fixing that, together with making NVMe use this API, could stop it</span>
<span class="quote">&gt; from issuing dma_map()s of addresses beyond mask.</span>

NVMe should never bounce, the fact that it currently possibly does
for highmem pages is a bug.
<span class="quote">
&gt; As for PCI_DMA_BUS_IS_PHYS - ironically, what all current usages of this</span>
<span class="quote">&gt; macro in the kernel do is - *disable* bounce buffers in block layer if</span>
<span class="quote">&gt; PCI_DMA_BUS_IS_PHYS is zero.</span>

That&#39;s not ironic but the whole point of the macro (horrible name and
the fact that it should be a dma_ops setting aside).
<span class="quote">
&gt; - architecture should stop breaking 64-bit DMA when driver attempts to</span>
<span class="quote">&gt; set 64-bit dma mask,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; - NVMe should issue proper blk_queue_bounce_limit() call based on what</span>
<span class="quote">&gt; is actually set mask,</span>

Or even better remove the call to dma_set_mask_and_coherent with
DMA_BIT_MASK(32).  NVMe is designed around having proper 64-bit DMA
addressing, there is not point in trying to pretent it works without that
<span class="quote">
&gt; - and blk_queue_bounce_limit() should also be fixed to actually set</span>
<span class="quote">&gt; 0xffffffff limit, instead of replacing it with (max_low_pfn &lt;&lt;</span>
<span class="quote">&gt; PAGE_SHIFT) as it does now.</span>

We need to kill off BLK_BOUNCE_HIGH, it just doesn&#39;t make sense to
mix the highmem aspect with the addressing limits.  In fact the whole
block bouncing scheme doesn&#39;t make much sense at all these days, we
should rely on swiotlb instead.
<span class="quote">
&gt; What I mean is some API to allocate memory for use with streaming DMA in</span>
<span class="quote">&gt; such way that bounce buffers won&#39;t be needed. There are many cases when</span>
<span class="quote">&gt; at buffer allocation time, it is already known that buffer will be used</span>
<span class="quote">&gt; for DMA with particular device. Bounce buffers will still be needed</span>
<span class="quote">&gt; cases when no such information is available at allocation time, or when</span>
<span class="quote">&gt; there is no directly-DMAable memory available at allocation time.</span>

For block I/O that is never the case.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 10, 2017, 6:47 a.m.</div>
<pre class="content">
<span class="quote">&gt;&gt; I believe the bounce buffering code you refer to is not in SATA/SCSI/MMC</span>
<span class="quote">&gt;&gt; but in block layer, in particular it should be controlled by</span>
<span class="quote">&gt;&gt; blk_queue_bounce_limit().  [Yes there is CONFIG_MMC_BLOCK_BOUNCE but it</span>
<span class="quote">&gt;&gt; is something completely different, namely it is for request merging for</span>
<span class="quote">&gt;&gt; hw not supporting scatter-gather].  And NVMe also uses block layer and</span>
<span class="quote">&gt;&gt; thus should get same support.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NVMe shouldn&#39;t have to call blk_queue_bounce_limit - </span>
<span class="quote">&gt; blk_queue_bounce_limit is to set the DMA addressing limit of the device.</span>
<span class="quote">&gt; NVMe devices must support unlimited 64-bit addressing and thus calling</span>
<span class="quote">&gt; blk_queue_bounce_limit from NVMe does not make sense.</span>

I&#39;m now working with HW that:
- is now way &quot;low end&quot; or &quot;obsolete&quot;, it has 4G of RAM and 8 CPU cores,
and is being manufactured and developed,
- has 75% of it&#39;s RAM located beyond first 4G of address space,
- can&#39;t physically handle incoming PCIe transactions addressed to memory
beyond 4G.

Swiotlb is used there, sure (once a bug in arm64 arch is patched). But
that setup still has at least two issues.

(1) it constantly runs of swiotlb space, logs are full of warnings
despite of rate limiting,

(2) it runs far suboptimal due to bounce-buffering almost all i/o,
despite of lots of free memory in area where direct DMA is possible.

I&#39;m looking for proper way to address these. Shooting HW designer as you
suggested elsewhere doesn&#39;t look like a practical solution. Any better
ideas?


Per my current understanding, blk-level bounce buffering will at least
help with (1) - if done properly it will allocate bounce buffers within
entire memory below 4G, not within dedicated swiotlb space (that is
small and enlarging it makes memory permanently unavailable for other
use).  This looks simple and safe (in sense of not anyhow breaking
unrelated use cases).

Addressing (2) looks much more difficult because different memory
allocation policy is required for that.
<span class="quote">

&gt; That being said currently the default for a queue without a call</span>
<span class="quote">&gt; to blk_queue_make_request which does the wrong thing on highmem</span>
<span class="quote">&gt; setups, so we should fix it.  In fact BLK_BOUNCE_HIGH as-is doesn&#39;t</span>
<span class="quote">&gt; really make much sense these days as no driver should ever dereference</span>
<span class="quote">&gt; pages passed to it directly.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Maybe fixing that, together with making NVMe use this API, could stop it</span>
<span class="quote">&gt;&gt; from issuing dma_map()s of addresses beyond mask.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; NVMe should never bounce, the fact that it currently possibly does</span>
<span class="quote">&gt; for highmem pages is a bug.</span>

The entire topic is absolutely not related to highmem (i.e. memory not
directly addressable by 32-bit kernel).

What we are discussing is hw-originated restriction on where DMA is
possible.
<span class="quote">

&gt; Or even better remove the call to dma_set_mask_and_coherent with</span>
<span class="quote">&gt; DMA_BIT_MASK(32).  NVMe is designed around having proper 64-bit DMA</span>
<span class="quote">&gt; addressing, there is not point in trying to pretent it works without that</span>

Are you claiming that NVMe driver in mainline is intentionally designed
to not work on HW that can&#39;t do DMA to entire 64-bit space?

Such setups do exist and there is interest to make them working.
<span class="quote">

&gt; We need to kill off BLK_BOUNCE_HIGH, it just doesn&#39;t make sense to</span>
<span class="quote">&gt; mix the highmem aspect with the addressing limits.  In fact the whole</span>
<span class="quote">&gt; block bouncing scheme doesn&#39;t make much sense at all these days, we</span>
<span class="quote">&gt; should rely on swiotlb instead.</span>

I agree that centralized bounce buffering is better than
subsystem-implemented bounce buffering.

I still claim that even better - especially from performance point of
view - is some memory allocation policy that is aware of HW limitations
and avoids bounce buffering at least when it is possible.
<span class="quote">

&gt;&gt; What I mean is some API to allocate memory for use with streaming DMA in</span>
<span class="quote">&gt;&gt; such way that bounce buffers won&#39;t be needed. There are many cases when</span>
<span class="quote">&gt;&gt; at buffer allocation time, it is already known that buffer will be used</span>
<span class="quote">&gt;&gt; for DMA with particular device. Bounce buffers will still be needed</span>
<span class="quote">&gt;&gt; cases when no such information is available at allocation time, or when</span>
<span class="quote">&gt;&gt; there is no directly-DMAable memory available at allocation time.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For block I/O that is never the case.</span>

Quite a few pages used for block I/O are allocated by filemap code - and
at allocation point it is known what inode page is being allocated for.
If this inode is from filesystem located on a known device with known
DMA limitations, this knowledge can be used to allocate page that can be
DMAed directly.

Sure there are lots of cases when at allocation time there is no idea
what device will run DMA on page being allocated, or perhaps page is
going to be shared, or whatever. Such cases unavoidably require bounce
buffers if page ends to be used with device with DMA limitations. But
still there are cases when better allocation can remove need for bounce
buffers - without any hurt for other cases.


Nikita
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - Jan. 10, 2017, 7:07 a.m.</div>
<pre class="content">
On Tue, Jan 10, 2017 at 09:47:21AM +0300, Nikita Yushchenko wrote:
<span class="quote">&gt; I&#39;m now working with HW that:</span>
<span class="quote">&gt; - is now way &quot;low end&quot; or &quot;obsolete&quot;, it has 4G of RAM and 8 CPU cores,</span>
<span class="quote">&gt; and is being manufactured and developed,</span>
<span class="quote">&gt; - has 75% of it&#39;s RAM located beyond first 4G of address space,</span>
<span class="quote">&gt; - can&#39;t physically handle incoming PCIe transactions addressed to memory</span>
<span class="quote">&gt; beyond 4G.</span>

It might not be low end or obselete, but it&#39;s absolutely braindead.
Your I/O performance will suffer badly for the life of the platform
because someone tries to save 2 cents, and there is not much we can do
about it.
<span class="quote">
&gt; (1) it constantly runs of swiotlb space, logs are full of warnings</span>
<span class="quote">&gt; despite of rate limiting,</span>
<span class="quote">
&gt; Per my current understanding, blk-level bounce buffering will at least</span>
<span class="quote">&gt; help with (1) - if done properly it will allocate bounce buffers within</span>
<span class="quote">&gt; entire memory below 4G, not within dedicated swiotlb space (that is</span>
<span class="quote">&gt; small and enlarging it makes memory permanently unavailable for other</span>
<span class="quote">&gt; use).  This looks simple and safe (in sense of not anyhow breaking</span>
<span class="quote">&gt; unrelated use cases).</span>

Yes.  Although there is absolutely no reason why swiotlb could not
do the same.
<span class="quote">
&gt; (2) it runs far suboptimal due to bounce-buffering almost all i/o,</span>
<span class="quote">&gt; despite of lots of free memory in area where direct DMA is possible.</span>
<span class="quote">
&gt; Addressing (2) looks much more difficult because different memory</span>
<span class="quote">&gt; allocation policy is required for that.</span>

It&#39;s basically not possible.  Every piece of memory in a Linux
kernel is a possible source of I/O, and depending on the workload
type it might even be a the prime source of I/O.
<span class="quote">
&gt; &gt; NVMe should never bounce, the fact that it currently possibly does</span>
<span class="quote">&gt; &gt; for highmem pages is a bug.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The entire topic is absolutely not related to highmem (i.e. memory not</span>
<span class="quote">&gt; directly addressable by 32-bit kernel).</span>

I did not say this affects you, but thanks to your mail I noticed that
NVMe has a suboptimal setting there.  Also note that highmem does not
have to imply a 32-bit kernel, just physical memory that is not in the
kernel mapping.
<span class="quote">
&gt; What we are discussing is hw-originated restriction on where DMA is</span>
<span class="quote">&gt; possible.</span>

Yes, where hw means the SOC, and not the actual I/O device, which is an
important distinction.
<span class="quote">
&gt; &gt; Or even better remove the call to dma_set_mask_and_coherent with</span>
<span class="quote">&gt; &gt; DMA_BIT_MASK(32).  NVMe is designed around having proper 64-bit DMA</span>
<span class="quote">&gt; &gt; addressing, there is not point in trying to pretent it works without that</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Are you claiming that NVMe driver in mainline is intentionally designed</span>
<span class="quote">&gt; to not work on HW that can&#39;t do DMA to entire 64-bit space?</span>

It is not intenteded to handle the case where the SOC / chipset
can&#39;t handle DMA to all physical memoery, yes.
<span class="quote">
&gt; Such setups do exist and there is interest to make them working.</span>

Sure, but it&#39;s not the job of the NVMe driver to work around such a broken
system.  It&#39;s something your architecture code needs to do, maybe with
a bit of core kernel support.
<span class="quote">
&gt; Quite a few pages used for block I/O are allocated by filemap code - and</span>
<span class="quote">&gt; at allocation point it is known what inode page is being allocated for.</span>
<span class="quote">&gt; If this inode is from filesystem located on a known device with known</span>
<span class="quote">&gt; DMA limitations, this knowledge can be used to allocate page that can be</span>
<span class="quote">&gt; DMAed directly.</span>

But in other cases we might never DMA to it.  Or we rarely DMA to it, say
for a machine running databses or qemu and using lots of direct I/O. Or
a storage target using it&#39;s local alloc_pages buffers.
<span class="quote">
&gt; Sure there are lots of cases when at allocation time there is no idea</span>
<span class="quote">&gt; what device will run DMA on page being allocated, or perhaps page is</span>
<span class="quote">&gt; going to be shared, or whatever. Such cases unavoidably require bounce</span>
<span class="quote">&gt; buffers if page ends to be used with device with DMA limitations. But</span>
<span class="quote">&gt; still there are cases when better allocation can remove need for bounce</span>
<span class="quote">&gt; buffers - without any hurt for other cases.</span>

It takes your max 1GB DMA addressable memoery away from other uses,
and introduce the crazy highmem VM tuning issues we had with big
32-bit x86 systems in the past.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=170319">Nikita Yushchenko</a> - Jan. 10, 2017, 7:31 a.m.</div>
<pre class="content">
Christoph, thanks for clear input.

Arnd, I think that given this discussion, best short-term solution is
indeed the patch I&#39;ve submitted yesterday. That is, your version +
coherent mask support.  With that, set_dma_mask(DMA_BIT_MASK(64)) will
succeed and hardware with work with swiotlb.

Possible next step is to teach swiotlb to dynamically allocate bounce
buffers within entire arm64&#39;s ZONE_DMA.

Also there is some hope that R-Car *can* iommu-translate addresses that
PCIe module issues to system bus.  Although previous attempts to make
that working failed. Additional research is needed here.

Nikita
<span class="quote">
&gt; On Tue, Jan 10, 2017 at 09:47:21AM +0300, Nikita Yushchenko wrote:</span>
<span class="quote">&gt;&gt; I&#39;m now working with HW that:</span>
<span class="quote">&gt;&gt; - is now way &quot;low end&quot; or &quot;obsolete&quot;, it has 4G of RAM and 8 CPU cores,</span>
<span class="quote">&gt;&gt; and is being manufactured and developed,</span>
<span class="quote">&gt;&gt; - has 75% of it&#39;s RAM located beyond first 4G of address space,</span>
<span class="quote">&gt;&gt; - can&#39;t physically handle incoming PCIe transactions addressed to memory</span>
<span class="quote">&gt;&gt; beyond 4G.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It might not be low end or obselete, but it&#39;s absolutely braindead.</span>
<span class="quote">&gt; Your I/O performance will suffer badly for the life of the platform</span>
<span class="quote">&gt; because someone tries to save 2 cents, and there is not much we can do</span>
<span class="quote">&gt; about it.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; (1) it constantly runs of swiotlb space, logs are full of warnings</span>
<span class="quote">&gt;&gt; despite of rate limiting,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Per my current understanding, blk-level bounce buffering will at least</span>
<span class="quote">&gt;&gt; help with (1) - if done properly it will allocate bounce buffers within</span>
<span class="quote">&gt;&gt; entire memory below 4G, not within dedicated swiotlb space (that is</span>
<span class="quote">&gt;&gt; small and enlarging it makes memory permanently unavailable for other</span>
<span class="quote">&gt;&gt; use).  This looks simple and safe (in sense of not anyhow breaking</span>
<span class="quote">&gt;&gt; unrelated use cases).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes.  Although there is absolutely no reason why swiotlb could not</span>
<span class="quote">&gt; do the same.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; (2) it runs far suboptimal due to bounce-buffering almost all i/o,</span>
<span class="quote">&gt;&gt; despite of lots of free memory in area where direct DMA is possible.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Addressing (2) looks much more difficult because different memory</span>
<span class="quote">&gt;&gt; allocation policy is required for that.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It&#39;s basically not possible.  Every piece of memory in a Linux</span>
<span class="quote">&gt; kernel is a possible source of I/O, and depending on the workload</span>
<span class="quote">&gt; type it might even be a the prime source of I/O.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; NVMe should never bounce, the fact that it currently possibly does</span>
<span class="quote">&gt;&gt;&gt; for highmem pages is a bug.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The entire topic is absolutely not related to highmem (i.e. memory not</span>
<span class="quote">&gt;&gt; directly addressable by 32-bit kernel).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I did not say this affects you, but thanks to your mail I noticed that</span>
<span class="quote">&gt; NVMe has a suboptimal setting there.  Also note that highmem does not</span>
<span class="quote">&gt; have to imply a 32-bit kernel, just physical memory that is not in the</span>
<span class="quote">&gt; kernel mapping.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; What we are discussing is hw-originated restriction on where DMA is</span>
<span class="quote">&gt;&gt; possible.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Yes, where hw means the SOC, and not the actual I/O device, which is an</span>
<span class="quote">&gt; important distinction.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt;&gt; Or even better remove the call to dma_set_mask_and_coherent with</span>
<span class="quote">&gt;&gt;&gt; DMA_BIT_MASK(32).  NVMe is designed around having proper 64-bit DMA</span>
<span class="quote">&gt;&gt;&gt; addressing, there is not point in trying to pretent it works without that</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Are you claiming that NVMe driver in mainline is intentionally designed</span>
<span class="quote">&gt;&gt; to not work on HW that can&#39;t do DMA to entire 64-bit space?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It is not intenteded to handle the case where the SOC / chipset</span>
<span class="quote">&gt; can&#39;t handle DMA to all physical memoery, yes.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Such setups do exist and there is interest to make them working.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Sure, but it&#39;s not the job of the NVMe driver to work around such a broken</span>
<span class="quote">&gt; system.  It&#39;s something your architecture code needs to do, maybe with</span>
<span class="quote">&gt; a bit of core kernel support.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Quite a few pages used for block I/O are allocated by filemap code - and</span>
<span class="quote">&gt;&gt; at allocation point it is known what inode page is being allocated for.</span>
<span class="quote">&gt;&gt; If this inode is from filesystem located on a known device with known</span>
<span class="quote">&gt;&gt; DMA limitations, this knowledge can be used to allocate page that can be</span>
<span class="quote">&gt;&gt; DMAed directly.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But in other cases we might never DMA to it.  Or we rarely DMA to it, say</span>
<span class="quote">&gt; for a machine running databses or qemu and using lots of direct I/O. Or</span>
<span class="quote">&gt; a storage target using it&#39;s local alloc_pages buffers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;&gt; Sure there are lots of cases when at allocation time there is no idea</span>
<span class="quote">&gt;&gt; what device will run DMA on page being allocated, or perhaps page is</span>
<span class="quote">&gt;&gt; going to be shared, or whatever. Such cases unavoidably require bounce</span>
<span class="quote">&gt;&gt; buffers if page ends to be used with device with DMA limitations. But</span>
<span class="quote">&gt;&gt; still there are cases when better allocation can remove need for bounce</span>
<span class="quote">&gt;&gt; buffers - without any hurt for other cases.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It takes your max 1GB DMA addressable memoery away from other uses,</span>
<span class="quote">&gt; and introduce the crazy highmem VM tuning issues we had with big</span>
<span class="quote">&gt; 32-bit x86 systems in the past.</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=59301">Arnd Bergmann</a> - Jan. 10, 2017, 10:47 a.m.</div>
<pre class="content">
On Monday, January 9, 2017 9:57:46 PM CET Christoph Hellwig wrote:
<span class="quote">&gt; &gt; - architecture should stop breaking 64-bit DMA when driver attempts to</span>
<span class="quote">&gt; &gt; set 64-bit dma mask,</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; - NVMe should issue proper blk_queue_bounce_limit() call based on what</span>
<span class="quote">&gt; &gt; is actually set mask,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or even better remove the call to dma_set_mask_and_coherent with</span>
<span class="quote">&gt; DMA_BIT_MASK(32).  NVMe is designed around having proper 64-bit DMA</span>
<span class="quote">&gt; addressing, there is not point in trying to pretent it works without that</span>

Agreed, let&#39;s just fail the probe() if DMA_BIT_MASK(64) fails, and
have swiotlb work around machines that for some reason need bounce
buffers.
<span class="quote">
&gt; &gt; - and blk_queue_bounce_limit() should also be fixed to actually set</span>
<span class="quote">&gt; &gt; 0xffffffff limit, instead of replacing it with (max_low_pfn &lt;&lt;</span>
<span class="quote">&gt; &gt; PAGE_SHIFT) as it does now.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We need to kill off BLK_BOUNCE_HIGH, it just doesn&#39;t make sense to</span>
<span class="quote">&gt; mix the highmem aspect with the addressing limits.  In fact the whole</span>
<span class="quote">&gt; block bouncing scheme doesn&#39;t make much sense at all these days, we</span>
<span class="quote">&gt; should rely on swiotlb instead.</span>

If we do this, we should probably have another look at the respective
NETIF_F_HIGHDMA support in the network stack, which does the same thing
and mixes up highmem on 32-bit architectures with the DMA address limit.
(side note: there are actually cases in which you have a 31-bit DMA
mask but 3 GB of lowmem using CONFIG_VMSPLIT_1G, so BLK_BOUNCE_HIGH
and !NETIF_F_HIGHDMA are both missing the limit, causing data corruption
without swiotlb).

Before we rely too much on swiotlb, we may also need to consider which
architectures today rely on bouncing in blk and network.

I see that we have CONFIG_ARCH_PHYS_ADDR_T_64BIT on a couple of
32-bit architectures without swiotlb (arc, arm, some mips32), and
there are several 64-bit architectures that do not have swiotlb
(alpha, parisc, s390, sparc). I believe that alpha, s390 and sparc
always use some form of IOMMU, but the other four apparently don&#39;t,
so we would need to add swiotlb support there to remove all the
bounce buffering in network and block layers.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=59301">Arnd Bergmann</a> - Jan. 10, 2017, 10:54 a.m.</div>
<pre class="content">
On Tuesday, January 10, 2017 8:07:20 AM CET Christoph Hellwig wrote:
<span class="quote">&gt; On Tue, Jan 10, 2017 at 09:47:21AM +0300, Nikita Yushchenko wrote:</span>
<span class="quote">&gt; &gt; I&#39;m now working with HW that:</span>
<span class="quote">&gt; &gt; - is now way &quot;low end&quot; or &quot;obsolete&quot;, it has 4G of RAM and 8 CPU cores,</span>
<span class="quote">&gt; &gt; and is being manufactured and developed,</span>
<span class="quote">&gt; &gt; - has 75% of it&#39;s RAM located beyond first 4G of address space,</span>
<span class="quote">&gt; &gt; - can&#39;t physically handle incoming PCIe transactions addressed to memory</span>
<span class="quote">&gt; &gt; beyond 4G.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; It might not be low end or obselete, but it&#39;s absolutely braindead.</span>
<span class="quote">&gt; Your I/O performance will suffer badly for the life of the platform</span>
<span class="quote">&gt; because someone tries to save 2 cents, and there is not much we can do</span>
<span class="quote">&gt; about it.</span>

Unfortunately it is a common problem for arm64 chips that were designed
by taking a 32-bit SoC and replacing the CPU core. The swiotlb is the
right workaround for this, and I think we all agree that we should
just make it work correctly.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=59301">Arnd Bergmann</a> - Jan. 10, 2017, 11:01 a.m.</div>
<pre class="content">
On Tuesday, January 10, 2017 10:31:47 AM CET Nikita Yushchenko wrote:
<span class="quote">&gt; Christoph, thanks for clear input.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Arnd, I think that given this discussion, best short-term solution is</span>
<span class="quote">&gt; indeed the patch I&#39;ve submitted yesterday. That is, your version +</span>
<span class="quote">&gt; coherent mask support.  With that, set_dma_mask(DMA_BIT_MASK(64)) will</span>
<span class="quote">&gt; succeed and hardware with work with swiotlb.</span>

Ok, good.
<span class="quote">
&gt; Possible next step is to teach swiotlb to dynamically allocate bounce</span>
<span class="quote">&gt; buffers within entire arm64&#39;s ZONE_DMA.</span>

That seems reasonable, yes. We probably have to do both, as there are
cases where a device has dma_mask smaller than ZONE_DMA but the swiotlb
bounce area is low enough to work anyway.

Another workaround me might need is to limit amount of concurrent DMA
in the NVMe driver based on some platform quirk. The way that NVMe works,
it can have very large amounts of data that is concurrently mapped into
the device. SWIOTLB is one case where this currently fails, but another
example would be old PowerPC servers that have a 256MB window of virtual
I/O addresses per VM guest in their IOMMU. Those will likely fail the same
way that your does.
<span class="quote">
&gt; Also there is some hope that R-Car *can* iommu-translate addresses that</span>
<span class="quote">&gt; PCIe module issues to system bus.  Although previous attempts to make</span>
<span class="quote">&gt; that working failed. Additional research is needed here.</span>

Does this IOMMU support remapping data within a virtual machine? I believe
there are some that only do one of the two -- either you can have guest
machines with DMA access to their low memory, or you can remap data on
the fly in the host.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - Jan. 10, 2017, 2:44 p.m.</div>
<pre class="content">
On Tue, Jan 10, 2017 at 11:47:42AM +0100, Arnd Bergmann wrote:
<span class="quote">&gt; I see that we have CONFIG_ARCH_PHYS_ADDR_T_64BIT on a couple of</span>
<span class="quote">&gt; 32-bit architectures without swiotlb (arc, arm, some mips32), and</span>
<span class="quote">&gt; there are several 64-bit architectures that do not have swiotlb</span>
<span class="quote">&gt; (alpha, parisc, s390, sparc). I believe that alpha, s390 and sparc</span>
<span class="quote">&gt; always use some form of IOMMU, but the other four apparently don&#39;t,</span>
<span class="quote">&gt; so we would need to add swiotlb support there to remove all the</span>
<span class="quote">&gt; bounce buffering in network and block layers.</span>

mips has lots of weird swiotlb wire-up in it&#39;s board code (the swiotlb
arch glue really needs some major cleanup..), as does arm.  Not
sure about the others.

Getting rid of highmem bouncing in the block layer will take some time
as various PIO-only drivers rely on it at the moment.  These should
all be convertable to kmap that data, but it needs a careful audit
first.  For 4.11 I&#39;ll plan to switch away from bouncing highmem by
default at least, though and maybe also convert a few PIO drivers.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - Jan. 10, 2017, 2:48 p.m.</div>
<pre class="content">
On Tue, Jan 10, 2017 at 12:01:05PM +0100, Arnd Bergmann wrote:
<span class="quote">&gt; Another workaround me might need is to limit amount of concurrent DMA</span>
<span class="quote">&gt; in the NVMe driver based on some platform quirk. The way that NVMe works,</span>
<span class="quote">&gt; it can have very large amounts of data that is concurrently mapped into</span>
<span class="quote">&gt; the device.</span>

That&#39;s not really just NVMe - other storage and network controllers also
can DMA map giant amounts of memory.  There are a couple aspects to it:

 - dma coherent memoery - right now NVMe doesn&#39;t use too much of it,
   but upcoming low-end NVMe controllers will soon start to require
   fairl large amounts of it for the host memory buffer feature that
   allows for DRAM-less controller designs.  As an interesting quirk
   that is memory only used by the PCIe devices, and never accessed
   by the Linux host at all.

 - size vs number of the dynamic mapping.  We probably want the dma_ops
   specify a maximum mapping size for a given device.  As long as we
   can make progress with a few mappings swiotlb / the iommu can just
   fail mapping and the driver will propagate that to the block layer
   that throttles I/O.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=59301">Arnd Bergmann</a> - Jan. 10, 2017, 3 p.m.</div>
<pre class="content">
On Tuesday, January 10, 2017 3:44:53 PM CET Christoph Hellwig wrote:
<span class="quote">&gt; On Tue, Jan 10, 2017 at 11:47:42AM +0100, Arnd Bergmann wrote:</span>
<span class="quote">&gt; &gt; I see that we have CONFIG_ARCH_PHYS_ADDR_T_64BIT on a couple of</span>
<span class="quote">&gt; &gt; 32-bit architectures without swiotlb (arc, arm, some mips32), and</span>
<span class="quote">&gt; &gt; there are several 64-bit architectures that do not have swiotlb</span>
<span class="quote">&gt; &gt; (alpha, parisc, s390, sparc). I believe that alpha, s390 and sparc</span>
<span class="quote">&gt; &gt; always use some form of IOMMU, but the other four apparently don&#39;t,</span>
<span class="quote">&gt; &gt; so we would need to add swiotlb support there to remove all the</span>
<span class="quote">&gt; &gt; bounce buffering in network and block layers.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; mips has lots of weird swiotlb wire-up in it&#39;s board code (the swiotlb</span>
<span class="quote">&gt; arch glue really needs some major cleanup..),</span>

My reading of the MIPS code was that only the 64-bit platforms use it,
but there are a number of 32-bit platforms that have 64-bit physical
addresses and don&#39;t.
<span class="quote">
&gt; as does arm.  Not sure about the others.</span>

32-bit ARM doesn&#39;t actually use SWIOTLB at all, despite selecting it
in Kconfig. I think Xen uses it for its own purposes, but nothing
else does.

Most ARM platforms can&#39;t actually have RAM beyond 4GB, and the ones
that do have it tend to also come with an IOMMU, but I remember
at least BCM53xx actually needing swiotlb on some chip revisions
that are widely used and that cannot DMA to the second memory bank
from PCI (!).

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=59301">Arnd Bergmann</a> - Jan. 10, 2017, 3:02 p.m.</div>
<pre class="content">
On Tuesday, January 10, 2017 3:48:39 PM CET Christoph Hellwig wrote:
<span class="quote">&gt; On Tue, Jan 10, 2017 at 12:01:05PM +0100, Arnd Bergmann wrote:</span>
<span class="quote">&gt; &gt; Another workaround me might need is to limit amount of concurrent DMA</span>
<span class="quote">&gt; &gt; in the NVMe driver based on some platform quirk. The way that NVMe works,</span>
<span class="quote">&gt; &gt; it can have very large amounts of data that is concurrently mapped into</span>
<span class="quote">&gt; &gt; the device.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; That&#39;s not really just NVMe - other storage and network controllers also</span>
<span class="quote">&gt; can DMA map giant amounts of memory.  There are a couple aspects to it:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt;  - dma coherent memoery - right now NVMe doesn&#39;t use too much of it,</span>
<span class="quote">&gt;    but upcoming low-end NVMe controllers will soon start to require</span>
<span class="quote">&gt;    fairl large amounts of it for the host memory buffer feature that</span>
<span class="quote">&gt;    allows for DRAM-less controller designs.  As an interesting quirk</span>
<span class="quote">&gt;    that is memory only used by the PCIe devices, and never accessed</span>
<span class="quote">&gt;    by the Linux host at all.</span>

Right, that is going to become interesting, as some platforms are
very limited with their coherent allocations.
<span class="quote">
&gt;  - size vs number of the dynamic mapping.  We probably want the dma_ops</span>
<span class="quote">&gt;    specify a maximum mapping size for a given device.  As long as we</span>
<span class="quote">&gt;    can make progress with a few mappings swiotlb / the iommu can just</span>
<span class="quote">&gt;    fail mapping and the driver will propagate that to the block layer</span>
<span class="quote">&gt;    that throttles I/O.</span>

Good idea.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=160021">Sagi Grimberg</a> - Jan. 12, 2017, 10:09 a.m.</div>
<pre class="content">
<span class="quote">&gt;&gt; Another workaround me might need is to limit amount of concurrent DMA</span>
<span class="quote">&gt;&gt; in the NVMe driver based on some platform quirk. The way that NVMe works,</span>
<span class="quote">&gt;&gt; it can have very large amounts of data that is concurrently mapped into</span>
<span class="quote">&gt;&gt; the device.</span>
<span class="quote">&gt;</span>
<span class="quote">&gt; That&#39;s not really just NVMe - other storage and network controllers also</span>
<span class="quote">&gt; can DMA map giant amounts of memory.  There are a couple aspects to it:</span>
<span class="quote">&gt;</span>
<span class="quote">&gt;  - dma coherent memoery - right now NVMe doesn&#39;t use too much of it,</span>
<span class="quote">&gt;    but upcoming low-end NVMe controllers will soon start to require</span>
<span class="quote">&gt;    fairl large amounts of it for the host memory buffer feature that</span>
<span class="quote">&gt;    allows for DRAM-less controller designs.  As an interesting quirk</span>
<span class="quote">&gt;    that is memory only used by the PCIe devices, and never accessed</span>
<span class="quote">&gt;    by the Linux host at all.</span>

Would it make sense to convert the nvme driver to use normal allocations
and use the DMA streaming APIs (dma_sync_single_for_[cpu|device]) for
both queues and future HMB?
<span class="quote">
&gt;  - size vs number of the dynamic mapping.  We probably want the dma_ops</span>
<span class="quote">&gt;    specify a maximum mapping size for a given device.  As long as we</span>
<span class="quote">&gt;    can make progress with a few mappings swiotlb / the iommu can just</span>
<span class="quote">&gt;    fail mapping and the driver will propagate that to the block layer</span>
<span class="quote">&gt;    that throttles I/O.</span>

Isn&#39;t max mapping size per device too restrictive? it is possible that
not all devices posses active mappings concurrently.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=59301">Arnd Bergmann</a> - Jan. 12, 2017, 11:56 a.m.</div>
<pre class="content">
On Thursday, January 12, 2017 12:09:11 PM CET Sagi Grimberg wrote:
<span class="quote">&gt; &gt;&gt; Another workaround me might need is to limit amount of concurrent DMA</span>
<span class="quote">&gt; &gt;&gt; in the NVMe driver based on some platform quirk. The way that NVMe works,</span>
<span class="quote">&gt; &gt;&gt; it can have very large amounts of data that is concurrently mapped into</span>
<span class="quote">&gt; &gt;&gt; the device.</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt; That&#39;s not really just NVMe - other storage and network controllers also</span>
<span class="quote">&gt; &gt; can DMA map giant amounts of memory.  There are a couple aspects to it:</span>
<span class="quote">&gt; &gt;</span>
<span class="quote">&gt; &gt;  - dma coherent memoery - right now NVMe doesn&#39;t use too much of it,</span>
<span class="quote">&gt; &gt;    but upcoming low-end NVMe controllers will soon start to require</span>
<span class="quote">&gt; &gt;    fairl large amounts of it for the host memory buffer feature that</span>
<span class="quote">&gt; &gt;    allows for DRAM-less controller designs.  As an interesting quirk</span>
<span class="quote">&gt; &gt;    that is memory only used by the PCIe devices, and never accessed</span>
<span class="quote">&gt; &gt;    by the Linux host at all.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Would it make sense to convert the nvme driver to use normal allocations</span>
<span class="quote">&gt; and use the DMA streaming APIs (dma_sync_single_for_[cpu|device]) for</span>
<span class="quote">&gt; both queues and future HMB?</span>

That is an interesting question: We actually have the
&quot;DMA_ATTR_NO_KERNEL_MAPPING&quot; for this case, and ARM implements
it in the coherent interface, so that might be a good fit.

Implementing it in the streaming API makes no sense since we
already have a kernel mapping here, but using a normal allocation
(possibly with DMA_ATTR_NON_CONSISTENT or DMA_ATTR_SKIP_CPU_SYNC,
need to check) might help on other architectures that have
limited amounts of coherent memory and no CMA.

Another benefit of the coherent API for this kind of buffer is
that we can use CMA where available to get a large consecutive
chunk of RAM on architectures without an IOMMU when normal
memory is no longer available because of fragmentation.

	Arnd
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=99">Christoph Hellwig</a> - Jan. 12, 2017, 1:07 p.m.</div>
<pre class="content">
On Thu, Jan 12, 2017 at 12:56:07PM +0100, Arnd Bergmann wrote:
<span class="quote">&gt; That is an interesting question: We actually have the</span>
<span class="quote">&gt; &quot;DMA_ATTR_NO_KERNEL_MAPPING&quot; for this case, and ARM implements</span>
<span class="quote">&gt; it in the coherent interface, so that might be a good fit.</span>

Yes. my WIP HMB patch uses DMA_ATTR_NO_KERNEL_MAPPING, although I&#39;m
workin on x86 at the moment where it&#39;s a no-op.
<span class="quote">
&gt; Implementing it in the streaming API makes no sense since we</span>
<span class="quote">&gt; already have a kernel mapping here, but using a normal allocation</span>
<span class="quote">&gt; (possibly with DMA_ATTR_NON_CONSISTENT or DMA_ATTR_SKIP_CPU_SYNC,</span>
<span class="quote">&gt; need to check) might help on other architectures that have</span>
<span class="quote">&gt; limited amounts of coherent memory and no CMA.</span>

Though about that - but in the end DMA_ATTR_NO_KERNEL_MAPPING implies
those, so instead of using lots of flags in driver I&#39;d rather fix
up more dma_ops implementations to take advantage of
DMA_ATTR_NO_KERNEL_MAPPING.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm64/include/asm/device.h b/arch/arm64/include/asm/device.h</span>
<span class="p_header">index 243ef256b8c9..a57e7bb10e71 100644</span>
<span class="p_header">--- a/arch/arm64/include/asm/device.h</span>
<span class="p_header">+++ b/arch/arm64/include/asm/device.h</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"> struct dev_archdata {</span>
 	void *iommu;			/* private IOMMU data */
 #endif
 	bool dma_coherent;
<span class="p_add">+	u64 parent_dma_mask;</span>
 };
 
 struct pdev_archdata {
<span class="p_header">diff --git a/arch/arm64/mm/dma-mapping.c b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">index 290a84f3351f..aa65875c611b 100644</span>
<span class="p_header">--- a/arch/arm64/mm/dma-mapping.c</span>
<span class="p_header">+++ b/arch/arm64/mm/dma-mapping.c</span>
<span class="p_chunk">@@ -352,6 +352,31 @@</span> <span class="p_context"> static int __swiotlb_dma_supported(struct device *hwdev, u64 mask)</span>
 	return 1;
 }
 
<span class="p_add">+static int __swiotlb_set_dma_mask(struct device *dev, u64 mask)</span>
<span class="p_add">+{</span>
<span class="p_add">+	/* device is not DMA capable */</span>
<span class="p_add">+	if (!dev-&gt;dma_mask)</span>
<span class="p_add">+		return -EIO;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* mask is below swiotlb bounce buffer, so fail */</span>
<span class="p_add">+	if (!swiotlb_dma_supported(dev, mask))</span>
<span class="p_add">+		return -EIO;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * because of the swiotlb, we can return success for</span>
<span class="p_add">+	 * larger masks, but need to ensure that bounce buffers</span>
<span class="p_add">+	 * are used above parent_dma_mask, so set that as</span>
<span class="p_add">+	 * the effective mask.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (mask &gt; dev-&gt;archdata.parent_dma_mask)</span>
<span class="p_add">+		mask = dev-&gt;archdata.parent_dma_mask;</span>
<span class="p_add">+</span>
<span class="p_add">+</span>
<span class="p_add">+	*dev-&gt;dma_mask = mask;</span>
<span class="p_add">+</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 static struct dma_map_ops swiotlb_dma_ops = {
 	.alloc = __dma_alloc,
 	.free = __dma_free,
<span class="p_chunk">@@ -367,6 +392,7 @@</span> <span class="p_context"> static struct dma_map_ops swiotlb_dma_ops = {</span>
 	.sync_sg_for_device = __swiotlb_sync_sg_for_device,
 	.dma_supported = __swiotlb_dma_supported,
 	.mapping_error = swiotlb_dma_mapping_error,
<span class="p_add">+	.set_dma_mask = __swiotlb_set_dma_mask,</span>
 };
 
 static int __init atomic_pool_init(void)
<span class="p_chunk">@@ -957,6 +983,18 @@</span> <span class="p_context"> void arch_setup_dma_ops(struct device *dev, u64 dma_base, u64 size,</span>
 	if (!dev-&gt;archdata.dma_ops)
 		dev-&gt;archdata.dma_ops = &amp;swiotlb_dma_ops;
 
<span class="p_add">+	/*</span>
<span class="p_add">+	 * we don&#39;t yet support buses that have a non-zero mapping.</span>
<span class="p_add">+	 *  Let&#39;s hope we won&#39;t need it</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	WARN_ON(dma_base != 0);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Whatever the parent bus can set. A device must not set</span>
<span class="p_add">+	 * a DMA mask larger than this.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	dev-&gt;archdata.parent_dma_mask = size;</span>
<span class="p_add">+</span>
 	dev-&gt;archdata.dma_coherent = coherent;
 	__iommu_setup_dma_ops(dev, dma_base, size, iommu);
 }

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



