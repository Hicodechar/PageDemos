
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[tip:locking/core] x86, locking/spinlocks: Remove ticket (spin)lock implementation - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [tip:locking/core] x86, locking/spinlocks: Remove ticket (spin)lock implementation</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Sept. 30, 2016, noon</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;tip-cfd8983f03c7b2f977faab8dfc4ec5f6dbf9c1f3@git.kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9358069/mbox/"
   >mbox</a>
|
   <a href="/patch/9358069/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9358069/">/patch/9358069/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	591976075E for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 30 Sep 2016 12:01:14 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4832D29FD1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 30 Sep 2016 12:01:14 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 3C50C29FD8; Fri, 30 Sep 2016 12:01:14 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 213C429FD1
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Fri, 30 Sep 2016 12:01:11 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S932976AbcI3MBD (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Fri, 30 Sep 2016 08:01:03 -0400
Received: from terminus.zytor.com ([198.137.202.10]:44896 &quot;EHLO
	terminus.zytor.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S932177AbcI3MAz (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Fri, 30 Sep 2016 08:00:55 -0400
Received: from terminus.zytor.com (localhost [127.0.0.1])
	by terminus.zytor.com (8.15.2/8.15.2) with ESMTP id u8UC08AZ025595;
	Fri, 30 Sep 2016 05:00:08 -0700
Received: (from tipbot@localhost)
	by terminus.zytor.com (8.15.2/8.15.2/Submit) id u8UC07EE025592;
	Fri, 30 Sep 2016 05:00:07 -0700
Date: Fri, 30 Sep 2016 05:00:07 -0700
X-Authentication-Warning: terminus.zytor.com: tipbot set sender to
	tipbot@zytor.com using -f
From: tip-bot for Peter Zijlstra &lt;tipbot@zytor.com&gt;
Message-ID: &lt;tip-cfd8983f03c7b2f977faab8dfc4ec5f6dbf9c1f3@git.kernel.org&gt;
Cc: mingo@kernel.org, hpa@zytor.com, akpm@linux-foundation.org,
	peterz@infradead.org, tglx@linutronix.de,
	linux-kernel@vger.kernel.org, paulmck@linux.vnet.ibm.com,
	waiman.long@hpe.com, torvalds@linux-foundation.org
Reply-To: hpa@zytor.com, peterz@infradead.org, akpm@linux-foundation.org,
	mingo@kernel.org, waiman.long@hpe.com,
	torvalds@linux-foundation.org, linux-kernel@vger.kernel.org,
	tglx@linutronix.de, paulmck@linux.vnet.ibm.com
In-Reply-To: &lt;20160518184302.GO3193@twins.programming.kicks-ass.net&gt;
References: &lt;20160518184302.GO3193@twins.programming.kicks-ass.net&gt;
To: linux-tip-commits@vger.kernel.org
Subject: [tip:locking/core] x86, locking/spinlocks: Remove ticket (spin)lock
	implementation
Git-Commit-ID: cfd8983f03c7b2f977faab8dfc4ec5f6dbf9c1f3
X-Mailer: tip-git-log-daemon
Robot-ID: &lt;tip-bot.git.kernel.org&gt;
Robot-Unsubscribe: Contact &lt;mailto:hpa@kernel.org&gt; to get blacklisted from
	these emails
MIME-Version: 1.0
Content-Transfer-Encoding: 8bit
Content-Type: text/plain; charset=UTF-8
Content-Disposition: inline
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=60001">tip-bot for Jacob Shin</a> - Sept. 30, 2016, noon</div>
<pre class="content">
Commit-ID:  cfd8983f03c7b2f977faab8dfc4ec5f6dbf9c1f3
Gitweb:     http://git.kernel.org/tip/cfd8983f03c7b2f977faab8dfc4ec5f6dbf9c1f3
Author:     Peter Zijlstra &lt;peterz@infradead.org&gt;
AuthorDate: Wed, 18 May 2016 20:43:02 +0200
Committer:  Ingo Molnar &lt;mingo@kernel.org&gt;
CommitDate: Fri, 30 Sep 2016 10:56:00 +0200

x86, locking/spinlocks: Remove ticket (spin)lock implementation

We&#39;ve unconditionally used the queued spinlock for many releases now.

Its time to remove the old ticket lock code.
<span class="signed-off-by">
Signed-off-by: Peter Zijlstra (Intel) &lt;peterz@infradead.org&gt;</span>
Cc: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: Paul E. McKenney &lt;paulmck@linux.vnet.ibm.com&gt;
Cc: Peter Zijlstra &lt;peterz@infradead.org&gt;
Cc: Thomas Gleixner &lt;tglx@linutronix.de&gt;
Cc: Waiman Long &lt;waiman.long@hpe.com&gt;
Cc: Waiman.Long@hpe.com
Cc: david.vrabel@citrix.com
Cc: dhowells@redhat.com
Cc: pbonzini@redhat.com
Cc: xen-devel@lists.xenproject.org
Link: http://lkml.kernel.org/r/20160518184302.GO3193@twins.programming.kicks-ass.net
<span class="signed-off-by">Signed-off-by: Ingo Molnar &lt;mingo@kernel.org&gt;</span>
---
 arch/x86/Kconfig                      |   3 +-
 arch/x86/include/asm/paravirt.h       |  18 ---
 arch/x86/include/asm/paravirt_types.h |   7 -
 arch/x86/include/asm/spinlock.h       | 174 -----------------------
 arch/x86/include/asm/spinlock_types.h |  13 --
 arch/x86/kernel/kvm.c                 | 245 ---------------------------------
 arch/x86/kernel/paravirt-spinlocks.c  |   7 -
 arch/x86/kernel/paravirt_patch_32.c   |   4 +-
 arch/x86/kernel/paravirt_patch_64.c   |   4 +-
 arch/x86/xen/spinlock.c               | 250 +---------------------------------
 10 files changed, 6 insertions(+), 719 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/Kconfig b/arch/x86/Kconfig</span>
<span class="p_header">index 2a1f0ce..0cc8811 100644</span>
<span class="p_header">--- a/arch/x86/Kconfig</span>
<span class="p_header">+++ b/arch/x86/Kconfig</span>
<span class="p_chunk">@@ -705,7 +705,6 @@</span> <span class="p_context"> config PARAVIRT_DEBUG</span>
 config PARAVIRT_SPINLOCKS
 	bool &quot;Paravirtualization layer for spinlocks&quot;
 	depends on PARAVIRT &amp;&amp; SMP
<span class="p_del">-	select UNINLINE_SPIN_UNLOCK if !QUEUED_SPINLOCKS</span>
 	---help---
 	  Paravirtualized spinlocks allow a pvops backend to replace the
 	  spinlock implementation with something virtualization-friendly
<span class="p_chunk">@@ -718,7 +717,7 @@</span> <span class="p_context"> config PARAVIRT_SPINLOCKS</span>
 
 config QUEUED_LOCK_STAT
 	bool &quot;Paravirt queued spinlock statistics&quot;
<span class="p_del">-	depends on PARAVIRT_SPINLOCKS &amp;&amp; DEBUG_FS &amp;&amp; QUEUED_SPINLOCKS</span>
<span class="p_add">+	depends on PARAVIRT_SPINLOCKS &amp;&amp; DEBUG_FS</span>
 	---help---
 	  Enable the collection of statistical data on the slowpath
 	  behavior of paravirtualized queued spinlocks and report
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">index 2970d22..4cd8db0 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt.h</span>
<span class="p_chunk">@@ -661,8 +661,6 @@</span> <span class="p_context"> static inline void __set_fixmap(unsigned /* enum fixed_addresses */ idx,</span>
 
 #if defined(CONFIG_SMP) &amp;&amp; defined(CONFIG_PARAVIRT_SPINLOCKS)
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_del">-</span>
 static __always_inline void pv_queued_spin_lock_slowpath(struct qspinlock *lock,
 							u32 val)
 {
<span class="p_chunk">@@ -684,22 +682,6 @@</span> <span class="p_context"> static __always_inline void pv_kick(int cpu)</span>
 	PVOP_VCALL1(pv_lock_ops.kick, cpu);
 }
 
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void __ticket_lock_spinning(struct arch_spinlock *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-	PVOP_VCALLEE2(pv_lock_ops.lock_spinning, lock, ticket);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void __ticket_unlock_kick(struct arch_spinlock *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-	PVOP_VCALL2(pv_lock_ops.unlock_kick, lock, ticket);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
 #endif /* SMP &amp;&amp; PARAVIRT_SPINLOCKS */
 
 #ifdef CONFIG_X86_32
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">index 7fa9e77..60aac60 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_chunk">@@ -301,23 +301,16 @@</span> <span class="p_context"> struct pv_mmu_ops {</span>
 struct arch_spinlock;
 #ifdef CONFIG_SMP
 #include &lt;asm/spinlock_types.h&gt;
<span class="p_del">-#else</span>
<span class="p_del">-typedef u16 __ticket_t;</span>
 #endif
 
 struct qspinlock;
 
 struct pv_lock_ops {
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 	void (*queued_spin_lock_slowpath)(struct qspinlock *lock, u32 val);
 	struct paravirt_callee_save queued_spin_unlock;
 
 	void (*wait)(u8 *ptr, u8 val);
 	void (*kick)(int cpu);
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-	struct paravirt_callee_save lock_spinning;</span>
<span class="p_del">-	void (*unlock_kick)(struct arch_spinlock *lock, __ticket_t ticket);</span>
<span class="p_del">-#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
 };
 
 /* This contains all the paravirt structures: we get a convenient
<span class="p_header">diff --git a/arch/x86/include/asm/spinlock.h b/arch/x86/include/asm/spinlock.h</span>
<span class="p_header">index be0a059..921bea7 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/spinlock.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/spinlock.h</span>
<span class="p_chunk">@@ -20,187 +20,13 @@</span> <span class="p_context"></span>
  * (the type definitions are in asm/spinlock_types.h)
  */
 
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-# define LOCK_PTR_REG &quot;a&quot;</span>
<span class="p_del">-#else</span>
<span class="p_del">-# define LOCK_PTR_REG &quot;D&quot;</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-#if defined(CONFIG_X86_32) &amp;&amp; (defined(CONFIG_X86_PPRO_FENCE))</span>
<span class="p_del">-/*</span>
<span class="p_del">- * On PPro SMP, we use a locked operation to unlock</span>
<span class="p_del">- * (PPro errata 66, 92)</span>
<span class="p_del">- */</span>
<span class="p_del">-# define UNLOCK_LOCK_PREFIX LOCK_PREFIX</span>
<span class="p_del">-#else</span>
<span class="p_del">-# define UNLOCK_LOCK_PREFIX</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 /* How long a lock should spin before we consider blocking */
 #define SPIN_THRESHOLD	(1 &lt;&lt; 15)
 
 extern struct static_key paravirt_ticketlocks_enabled;
 static __always_inline bool static_key_false(struct static_key *key);
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 #include &lt;asm/qspinlock.h&gt;
<span class="p_del">-#else</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_PARAVIRT_SPINLOCKS</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void __ticket_enter_slowpath(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	set_bit(0, (volatile unsigned long *)&amp;lock-&gt;tickets.head);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#else  /* !CONFIG_PARAVIRT_SPINLOCKS */</span>
<span class="p_del">-static __always_inline void __ticket_lock_spinning(arch_spinlock_t *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-static inline void __ticket_unlock_kick(arch_spinlock_t *lock,</span>
<span class="p_del">-							__ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* CONFIG_PARAVIRT_SPINLOCKS */</span>
<span class="p_del">-static inline int  __tickets_equal(__ticket_t one, __ticket_t two)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return !((one ^ two) &amp; ~TICKET_SLOWPATH_FLAG);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void __ticket_check_and_clear_slowpath(arch_spinlock_t *lock,</span>
<span class="p_del">-							__ticket_t head)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (head &amp; TICKET_SLOWPATH_FLAG) {</span>
<span class="p_del">-		arch_spinlock_t old, new;</span>
<span class="p_del">-</span>
<span class="p_del">-		old.tickets.head = head;</span>
<span class="p_del">-		new.tickets.head = head &amp; ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-		old.tickets.tail = new.tickets.head + TICKET_LOCK_INC;</span>
<span class="p_del">-		new.tickets.tail = old.tickets.tail;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* try to clear slowpath flag when there are no contenders */</span>
<span class="p_del">-		cmpxchg(&amp;lock-&gt;head_tail, old.head_tail, new.head_tail);</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline int arch_spin_value_unlocked(arch_spinlock_t lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return __tickets_equal(lock.tickets.head, lock.tickets.tail);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Ticket locks are conceptually two parts, one indicating the current head of</span>
<span class="p_del">- * the queue, and the other indicating the current tail. The lock is acquired</span>
<span class="p_del">- * by atomically noting the tail and incrementing it by one (thus adding</span>
<span class="p_del">- * ourself to the queue and noting our position), then waiting until the head</span>
<span class="p_del">- * becomes equal to the the initial value of the tail.</span>
<span class="p_del">- *</span>
<span class="p_del">- * We use an xadd covering *both* parts of the lock, to increment the tail and</span>
<span class="p_del">- * also load the position of the head, which takes care of memory ordering</span>
<span class="p_del">- * issues and should be optimal for the uncontended case. Note the tail must be</span>
<span class="p_del">- * in the high part, because a wide xadd increment of the low part would carry</span>
<span class="p_del">- * up and contaminate the high part.</span>
<span class="p_del">- */</span>
<span class="p_del">-static __always_inline void arch_spin_lock(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	register struct __raw_tickets inc = { .tail = TICKET_LOCK_INC };</span>
<span class="p_del">-</span>
<span class="p_del">-	inc = xadd(&amp;lock-&gt;tickets, inc);</span>
<span class="p_del">-	if (likely(inc.head == inc.tail))</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-</span>
<span class="p_del">-	for (;;) {</span>
<span class="p_del">-		unsigned count = SPIN_THRESHOLD;</span>
<span class="p_del">-</span>
<span class="p_del">-		do {</span>
<span class="p_del">-			inc.head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-			if (__tickets_equal(inc.head, inc.tail))</span>
<span class="p_del">-				goto clear_slowpath;</span>
<span class="p_del">-			cpu_relax();</span>
<span class="p_del">-		} while (--count);</span>
<span class="p_del">-		__ticket_lock_spinning(lock, inc.tail);</span>
<span class="p_del">-	}</span>
<span class="p_del">-clear_slowpath:</span>
<span class="p_del">-	__ticket_check_and_clear_slowpath(lock, inc.head);</span>
<span class="p_del">-out:</span>
<span class="p_del">-	barrier();	/* make sure nothing creeps before the lock is taken */</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline int arch_spin_trylock(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	arch_spinlock_t old, new;</span>
<span class="p_del">-</span>
<span class="p_del">-	old.tickets = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-	if (!__tickets_equal(old.tickets.head, old.tickets.tail))</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	new.head_tail = old.head_tail + (TICKET_LOCK_INC &lt;&lt; TICKET_SHIFT);</span>
<span class="p_del">-	new.head_tail &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* cmpxchg is a full barrier, so nothing can move before it */</span>
<span class="p_del">-	return cmpxchg(&amp;lock-&gt;head_tail, old.head_tail, new.head_tail) == old.head_tail;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void arch_spin_unlock(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	if (TICKET_SLOWPATH_FLAG &amp;&amp;</span>
<span class="p_del">-		static_key_false(&amp;paravirt_ticketlocks_enabled)) {</span>
<span class="p_del">-		__ticket_t head;</span>
<span class="p_del">-</span>
<span class="p_del">-		BUILD_BUG_ON(((__ticket_t)NR_CPUS) != NR_CPUS);</span>
<span class="p_del">-</span>
<span class="p_del">-		head = xadd(&amp;lock-&gt;tickets.head, TICKET_LOCK_INC);</span>
<span class="p_del">-</span>
<span class="p_del">-		if (unlikely(head &amp; TICKET_SLOWPATH_FLAG)) {</span>
<span class="p_del">-			head &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-			__ticket_unlock_kick(lock, (head + TICKET_LOCK_INC));</span>
<span class="p_del">-		}</span>
<span class="p_del">-	} else</span>
<span class="p_del">-		__add(&amp;lock-&gt;tickets.head, TICKET_LOCK_INC, UNLOCK_LOCK_PREFIX);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int arch_spin_is_locked(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-</span>
<span class="p_del">-	return !__tickets_equal(tmp.tail, tmp.head);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline int arch_spin_is_contended(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-</span>
<span class="p_del">-	tmp.head &amp;= ~TICKET_SLOWPATH_FLAG;</span>
<span class="p_del">-	return (__ticket_t)(tmp.tail - tmp.head) &gt; TICKET_LOCK_INC;</span>
<span class="p_del">-}</span>
<span class="p_del">-#define arch_spin_is_contended	arch_spin_is_contended</span>
<span class="p_del">-</span>
<span class="p_del">-static __always_inline void arch_spin_lock_flags(arch_spinlock_t *lock,</span>
<span class="p_del">-						  unsigned long flags)</span>
<span class="p_del">-{</span>
<span class="p_del">-	arch_spin_lock(lock);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void arch_spin_unlock_wait(arch_spinlock_t *lock)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__ticket_t head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-</span>
<span class="p_del">-	for (;;) {</span>
<span class="p_del">-		struct __raw_tickets tmp = READ_ONCE(lock-&gt;tickets);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * We need to check &quot;unlocked&quot; in a loop, tmp.head == head</span>
<span class="p_del">-		 * can be false positive because of overflow.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		if (__tickets_equal(tmp.head, tmp.tail) ||</span>
<span class="p_del">-				!__tickets_equal(tmp.head, head))</span>
<span class="p_del">-			break;</span>
<span class="p_del">-</span>
<span class="p_del">-		cpu_relax();</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
 
 /*
  * Read-write spinlocks, allowing multiple readers
<span class="p_header">diff --git a/arch/x86/include/asm/spinlock_types.h b/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_header">index 65c3e37..25311eb 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/spinlock_types.h</span>
<span class="p_chunk">@@ -23,20 +23,7 @@</span> <span class="p_context"> typedef u32 __ticketpair_t;</span>
 
 #define TICKET_SHIFT	(sizeof(__ticket_t) * 8)
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 #include &lt;asm-generic/qspinlock_types.h&gt;
<span class="p_del">-#else</span>
<span class="p_del">-typedef struct arch_spinlock {</span>
<span class="p_del">-	union {</span>
<span class="p_del">-		__ticketpair_t head_tail;</span>
<span class="p_del">-		struct __raw_tickets {</span>
<span class="p_del">-			__ticket_t head, tail;</span>
<span class="p_del">-		} tickets;</span>
<span class="p_del">-	};</span>
<span class="p_del">-} arch_spinlock_t;</span>
<span class="p_del">-</span>
<span class="p_del">-#define __ARCH_SPIN_LOCK_UNLOCKED	{ { 0 } }</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
 
 #include &lt;asm-generic/qrwlock_types.h&gt;
 
<span class="p_header">diff --git a/arch/x86/kernel/kvm.c b/arch/x86/kernel/kvm.c</span>
<span class="p_header">index 1726c4c..865058d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvm.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvm.c</span>
<span class="p_chunk">@@ -575,9 +575,6 @@</span> <span class="p_context"> static void kvm_kick_cpu(int cpu)</span>
 	kvm_hypercall2(KVM_HC_KICK_CPU, flags, apicid);
 }
 
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_del">-</span>
 #include &lt;asm/qspinlock.h&gt;
 
 static void kvm_wait(u8 *ptr, u8 val)
<span class="p_chunk">@@ -606,243 +603,6 @@</span> <span class="p_context"> out:</span>
 	local_irq_restore(flags);
 }
 
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
<span class="p_del">-enum kvm_contention_stat {</span>
<span class="p_del">-	TAKEN_SLOW,</span>
<span class="p_del">-	TAKEN_SLOW_PICKUP,</span>
<span class="p_del">-	RELEASED_SLOW,</span>
<span class="p_del">-	RELEASED_SLOW_KICKED,</span>
<span class="p_del">-	NR_CONTENTION_STATS</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_KVM_DEBUG_FS</span>
<span class="p_del">-#define HISTO_BUCKETS	30</span>
<span class="p_del">-</span>
<span class="p_del">-static struct kvm_spinlock_stats</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 contention_stats[NR_CONTENTION_STATS];</span>
<span class="p_del">-	u32 histo_spin_blocked[HISTO_BUCKETS+1];</span>
<span class="p_del">-	u64 time_blocked;</span>
<span class="p_del">-} spinlock_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static u8 zero_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void check_zero(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u8 ret;</span>
<span class="p_del">-	u8 old;</span>
<span class="p_del">-</span>
<span class="p_del">-	old = READ_ONCE(zero_stats);</span>
<span class="p_del">-	if (unlikely(old)) {</span>
<span class="p_del">-		ret = cmpxchg(&amp;zero_stats, old, 0);</span>
<span class="p_del">-		/* This ensures only one fellow resets the stat */</span>
<span class="p_del">-		if (ret == old)</span>
<span class="p_del">-			memset(&amp;spinlock_stats, 0, sizeof(spinlock_stats));</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void add_stats(enum kvm_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-	spinlock_stats.contention_stats[var] += val;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return sched_clock();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __spin_time_accum(u64 delta, u32 *array)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned index;</span>
<span class="p_del">-</span>
<span class="p_del">-	index = ilog2(delta);</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (index &lt; HISTO_BUCKETS)</span>
<span class="p_del">-		array[index]++;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		array[HISTO_BUCKETS]++;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 delta;</span>
<span class="p_del">-</span>
<span class="p_del">-	delta = sched_clock() - start;</span>
<span class="p_del">-	__spin_time_accum(delta, spinlock_stats.histo_spin_blocked);</span>
<span class="p_del">-	spinlock_stats.time_blocked += delta;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static struct dentry *d_spin_debug;</span>
<span class="p_del">-static struct dentry *d_kvm_debug;</span>
<span class="p_del">-</span>
<span class="p_del">-static struct dentry *kvm_init_debugfs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	d_kvm_debug = debugfs_create_dir(&quot;kvm-guest&quot;, NULL);</span>
<span class="p_del">-	if (!d_kvm_debug)</span>
<span class="p_del">-		printk(KERN_WARNING &quot;Could not create &#39;kvm&#39; debugfs directory\n&quot;);</span>
<span class="p_del">-</span>
<span class="p_del">-	return d_kvm_debug;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init kvm_spinlock_debugfs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct dentry *d_kvm;</span>
<span class="p_del">-</span>
<span class="p_del">-	d_kvm = kvm_init_debugfs();</span>
<span class="p_del">-	if (d_kvm == NULL)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-	d_spin_debug = debugfs_create_dir(&quot;spinlocks&quot;, d_kvm);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u8(&quot;zero_stats&quot;, 0644, d_spin_debug, &amp;zero_stats);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[TAKEN_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow_pickup&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_PICKUP]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[RELEASED_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow_kicked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		   &amp;spinlock_stats.contention_stats[RELEASED_SLOW_KICKED]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u64(&quot;time_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.time_blocked);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32_array(&quot;histo_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-		     spinlock_stats.histo_spin_blocked, HISTO_BUCKETS + 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-fs_initcall(kvm_spinlock_debugfs);</span>
<span class="p_del">-#else  /* !CONFIG_KVM_DEBUG_FS */</span>
<span class="p_del">-static inline void add_stats(enum kvm_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif  /* CONFIG_KVM_DEBUG_FS */</span>
<span class="p_del">-</span>
<span class="p_del">-struct kvm_lock_waiting {</span>
<span class="p_del">-	struct arch_spinlock *lock;</span>
<span class="p_del">-	__ticket_t want;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-/* cpus &#39;waiting&#39; on a spinlock to become available */</span>
<span class="p_del">-static cpumask_t waiting_cpus;</span>
<span class="p_del">-</span>
<span class="p_del">-/* Track spinlock on which a cpu is waiting */</span>
<span class="p_del">-static DEFINE_PER_CPU(struct kvm_lock_waiting, klock_waiting);</span>
<span class="p_del">-</span>
<span class="p_del">-__visible void kvm_lock_spinning(struct arch_spinlock *lock, __ticket_t want)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct kvm_lock_waiting *w;</span>
<span class="p_del">-	int cpu;</span>
<span class="p_del">-	u64 start;</span>
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-	__ticket_t head;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (in_nmi())</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	w = this_cpu_ptr(&amp;klock_waiting);</span>
<span class="p_del">-	cpu = smp_processor_id();</span>
<span class="p_del">-	start = spin_time_start();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Make sure an interrupt handler can&#39;t upset things in a</span>
<span class="p_del">-	 * partially setup state.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The ordering protocol on this is that the &quot;lock&quot; pointer</span>
<span class="p_del">-	 * may only be set non-NULL if the &quot;want&quot; ticket is correct.</span>
<span class="p_del">-	 * If we&#39;re updating &quot;want&quot;, we must first clear &quot;lock&quot;.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;want = want;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;lock = lock;</span>
<span class="p_del">-</span>
<span class="p_del">-	add_stats(TAKEN_SLOW, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * This uses set_bit, which is atomic but we should not rely on its</span>
<span class="p_del">-	 * reordering gurantees. So barrier is needed after this call.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	cpumask_set_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-</span>
<span class="p_del">-	barrier();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Mark entry to slowpath before doing the pickup test to make</span>
<span class="p_del">-	 * sure we don&#39;t deadlock with an unlocker.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	__ticket_enter_slowpath(lock);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* make sure enter_slowpath, which is atomic does not cross the read */</span>
<span class="p_del">-	smp_mb__after_atomic();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * check again make sure it didn&#39;t become free while</span>
<span class="p_del">-	 * we weren&#39;t looking.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-	if (__tickets_equal(head, want)) {</span>
<span class="p_del">-		add_stats(TAKEN_SLOW_PICKUP, 1);</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * halt until it&#39;s our turn and kicked. Note that we do safe halt</span>
<span class="p_del">-	 * for irq enabled case to avoid hang when lock info is overwritten</span>
<span class="p_del">-	 * in irq spinlock slowpath and no spurious interrupt occur to save us.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (arch_irqs_disabled_flags(flags))</span>
<span class="p_del">-		halt();</span>
<span class="p_del">-	else</span>
<span class="p_del">-		safe_halt();</span>
<span class="p_del">-</span>
<span class="p_del">-out:</span>
<span class="p_del">-	cpumask_clear_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-	spin_time_accum_blocked(start);</span>
<span class="p_del">-}</span>
<span class="p_del">-PV_CALLEE_SAVE_REGS_THUNK(kvm_lock_spinning);</span>
<span class="p_del">-</span>
<span class="p_del">-/* Kick vcpu waiting on @lock-&gt;head to reach value @ticket */</span>
<span class="p_del">-static void kvm_unlock_kick(struct arch_spinlock *lock, __ticket_t ticket)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	add_stats(RELEASED_SLOW, 1);</span>
<span class="p_del">-	for_each_cpu(cpu, &amp;waiting_cpus) {</span>
<span class="p_del">-		const struct kvm_lock_waiting *w = &amp;per_cpu(klock_waiting, cpu);</span>
<span class="p_del">-		if (READ_ONCE(w-&gt;lock) == lock &amp;&amp;</span>
<span class="p_del">-		    READ_ONCE(w-&gt;want) == ticket) {</span>
<span class="p_del">-			add_stats(RELEASED_SLOW_KICKED, 1);</span>
<span class="p_del">-			kvm_kick_cpu(cpu);</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
 /*
  * Setup pv_lock_ops to exploit KVM_FEATURE_PV_UNHALT if present.
  */
<span class="p_chunk">@@ -854,16 +614,11 @@</span> <span class="p_context"> void __init kvm_spinlock_init(void)</span>
 	if (!kvm_para_has_feature(KVM_FEATURE_PV_UNHALT))
 		return;
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 	__pv_init_lock_hash();
 	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
 	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
 	pv_lock_ops.wait = kvm_wait;
 	pv_lock_ops.kick = kvm_kick_cpu;
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(kvm_lock_spinning);</span>
<span class="p_del">-	pv_lock_ops.unlock_kick = kvm_unlock_kick;</span>
<span class="p_del">-#endif</span>
 }
 
 static __init int kvm_spinlock_init_jump(void)
<span class="p_header">diff --git a/arch/x86/kernel/paravirt-spinlocks.c b/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_header">index 1939a02..2c55a00 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt-spinlocks.c</span>
<span class="p_chunk">@@ -8,7 +8,6 @@</span> <span class="p_context"></span>
 
 #include &lt;asm/paravirt.h&gt;
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 __visible void __native_queued_spin_unlock(struct qspinlock *lock)
 {
 	native_queued_spin_unlock(lock);
<span class="p_chunk">@@ -21,19 +20,13 @@</span> <span class="p_context"> bool pv_is_native_spin_unlock(void)</span>
 	return pv_lock_ops.queued_spin_unlock.func ==
 		__raw_callee_save___native_queued_spin_unlock;
 }
<span class="p_del">-#endif</span>
 
 struct pv_lock_ops pv_lock_ops = {
 #ifdef CONFIG_SMP
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
 	.queued_spin_lock_slowpath = native_queued_spin_lock_slowpath,
 	.queued_spin_unlock = PV_CALLEE_SAVE(__native_queued_spin_unlock),
 	.wait = paravirt_nop,
 	.kick = paravirt_nop,
<span class="p_del">-#else /* !CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-	.lock_spinning = __PV_IS_CALLEE_SAVE(paravirt_nop),</span>
<span class="p_del">-	.unlock_kick = paravirt_nop,</span>
<span class="p_del">-#endif /* !CONFIG_QUEUED_SPINLOCKS */</span>
 #endif /* SMP */
 };
 EXPORT_SYMBOL(pv_lock_ops);
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_32.c b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">index 158dc06..920c6ae 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_chunk">@@ -10,7 +10,7 @@</span> <span class="p_context"> DEF_NATIVE(pv_mmu_ops, write_cr3, &quot;mov %eax, %cr3&quot;);</span>
 DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;mov %cr3, %eax&quot;);
 DEF_NATIVE(pv_cpu_ops, clts, &quot;clts&quot;);
 
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%eax)&quot;);
 #endif
 
<span class="p_chunk">@@ -49,7 +49,7 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_mmu_ops, read_cr3);
 		PATCH_SITE(pv_mmu_ops, write_cr3);
 		PATCH_SITE(pv_cpu_ops, clts);
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):
 			if (pv_is_native_spin_unlock()) {
 				start = start_pv_lock_ops_queued_spin_unlock;
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_64.c b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">index e70087a..bb3840c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_chunk">@@ -19,7 +19,7 @@</span> <span class="p_context"> DEF_NATIVE(pv_cpu_ops, swapgs, &quot;swapgs&quot;);</span>
 DEF_NATIVE(, mov32, &quot;mov %edi, %eax&quot;);
 DEF_NATIVE(, mov64, &quot;mov %rdi, %rax&quot;);
 
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 DEF_NATIVE(pv_lock_ops, queued_spin_unlock, &quot;movb $0, (%rdi)&quot;);
 #endif
 
<span class="p_chunk">@@ -61,7 +61,7 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_cpu_ops, clts);
 		PATCH_SITE(pv_mmu_ops, flush_tlb_single);
 		PATCH_SITE(pv_cpu_ops, wbinvd);
<span class="p_del">-#if defined(CONFIG_PARAVIRT_SPINLOCKS) &amp;&amp; defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_add">+#if defined(CONFIG_PARAVIRT_SPINLOCKS)</span>
 		case PARAVIRT_PATCH(pv_lock_ops.queued_spin_unlock):
 			if (pv_is_native_spin_unlock()) {
 				start = start_pv_lock_ops_queued_spin_unlock;
<span class="p_header">diff --git a/arch/x86/xen/spinlock.c b/arch/x86/xen/spinlock.c</span>
<span class="p_header">index f42e78d..3d6e006 100644</span>
<span class="p_header">--- a/arch/x86/xen/spinlock.c</span>
<span class="p_header">+++ b/arch/x86/xen/spinlock.c</span>
<span class="p_chunk">@@ -21,8 +21,6 @@</span> <span class="p_context"> static DEFINE_PER_CPU(int, lock_kicker_irq) = -1;</span>
 static DEFINE_PER_CPU(char *, irq_name);
 static bool xen_pvspin = true;
 
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_del">-</span>
 #include &lt;asm/qspinlock.h&gt;
 
 static void xen_qlock_kick(int cpu)
<span class="p_chunk">@@ -71,207 +69,6 @@</span> <span class="p_context"> static void xen_qlock_wait(u8 *byte, u8 val)</span>
 	xen_poll_irq(irq);
 }
 
<span class="p_del">-#else /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
<span class="p_del">-enum xen_contention_stat {</span>
<span class="p_del">-	TAKEN_SLOW,</span>
<span class="p_del">-	TAKEN_SLOW_PICKUP,</span>
<span class="p_del">-	TAKEN_SLOW_SPURIOUS,</span>
<span class="p_del">-	RELEASED_SLOW,</span>
<span class="p_del">-	RELEASED_SLOW_KICKED,</span>
<span class="p_del">-	NR_CONTENTION_STATS</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_XEN_DEBUG_FS</span>
<span class="p_del">-#define HISTO_BUCKETS	30</span>
<span class="p_del">-static struct xen_spinlock_stats</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 contention_stats[NR_CONTENTION_STATS];</span>
<span class="p_del">-	u32 histo_spin_blocked[HISTO_BUCKETS+1];</span>
<span class="p_del">-	u64 time_blocked;</span>
<span class="p_del">-} spinlock_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static u8 zero_stats;</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void check_zero(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u8 ret;</span>
<span class="p_del">-	u8 old = READ_ONCE(zero_stats);</span>
<span class="p_del">-	if (unlikely(old)) {</span>
<span class="p_del">-		ret = cmpxchg(&amp;zero_stats, old, 0);</span>
<span class="p_del">-		/* This ensures only one fellow resets the stat */</span>
<span class="p_del">-		if (ret == old)</span>
<span class="p_del">-			memset(&amp;spinlock_stats, 0, sizeof(spinlock_stats));</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void add_stats(enum xen_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-	spinlock_stats.contention_stats[var] += val;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return xen_clocksource_read();</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __spin_time_accum(u64 delta, u32 *array)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned index = ilog2(delta);</span>
<span class="p_del">-</span>
<span class="p_del">-	check_zero();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (index &lt; HISTO_BUCKETS)</span>
<span class="p_del">-		array[index]++;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		array[HISTO_BUCKETS]++;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-	u32 delta = xen_clocksource_read() - start;</span>
<span class="p_del">-</span>
<span class="p_del">-	__spin_time_accum(delta, spinlock_stats.histo_spin_blocked);</span>
<span class="p_del">-	spinlock_stats.time_blocked += delta;</span>
<span class="p_del">-}</span>
<span class="p_del">-#else  /* !CONFIG_XEN_DEBUG_FS */</span>
<span class="p_del">-static inline void add_stats(enum xen_contention_stat var, u32 val)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline u64 spin_time_start(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static inline void spin_time_accum_blocked(u64 start)</span>
<span class="p_del">-{</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif  /* CONFIG_XEN_DEBUG_FS */</span>
<span class="p_del">-</span>
<span class="p_del">-struct xen_lock_waiting {</span>
<span class="p_del">-	struct arch_spinlock *lock;</span>
<span class="p_del">-	__ticket_t want;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-static DEFINE_PER_CPU(struct xen_lock_waiting, lock_waiting);</span>
<span class="p_del">-static cpumask_t waiting_cpus;</span>
<span class="p_del">-</span>
<span class="p_del">-__visible void xen_lock_spinning(struct arch_spinlock *lock, __ticket_t want)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int irq = __this_cpu_read(lock_kicker_irq);</span>
<span class="p_del">-	struct xen_lock_waiting *w = this_cpu_ptr(&amp;lock_waiting);</span>
<span class="p_del">-	int cpu = smp_processor_id();</span>
<span class="p_del">-	u64 start;</span>
<span class="p_del">-	__ticket_t head;</span>
<span class="p_del">-	unsigned long flags;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* If kicker interrupts not initialized yet, just spin */</span>
<span class="p_del">-	if (irq == -1)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	start = spin_time_start();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Make sure an interrupt handler can&#39;t upset things in a</span>
<span class="p_del">-	 * partially setup state.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We don&#39;t really care if we&#39;re overwriting some other</span>
<span class="p_del">-	 * (lock,want) pair, as that would mean that we&#39;re currently</span>
<span class="p_del">-	 * in an interrupt context, and the outer context had</span>
<span class="p_del">-	 * interrupts enabled.  That has already kicked the VCPU out</span>
<span class="p_del">-	 * of xen_poll_irq(), so it will just return spuriously and</span>
<span class="p_del">-	 * retry with newly setup (lock,want).</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 * The ordering protocol on this is that the &quot;lock&quot; pointer</span>
<span class="p_del">-	 * may only be set non-NULL if the &quot;want&quot; ticket is correct.</span>
<span class="p_del">-	 * If we&#39;re updating &quot;want&quot;, we must first clear &quot;lock&quot;.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;want = want;</span>
<span class="p_del">-	smp_wmb();</span>
<span class="p_del">-	w-&gt;lock = lock;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* This uses set_bit, which atomic and therefore a barrier */</span>
<span class="p_del">-	cpumask_set_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-	add_stats(TAKEN_SLOW, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* clear pending */</span>
<span class="p_del">-	xen_clear_irq_pending(irq);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Only check lock once pending cleared */</span>
<span class="p_del">-	barrier();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Mark entry to slowpath before doing the pickup test to make</span>
<span class="p_del">-	 * sure we don&#39;t deadlock with an unlocker.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	__ticket_enter_slowpath(lock);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* make sure enter_slowpath, which is atomic does not cross the read */</span>
<span class="p_del">-	smp_mb__after_atomic();</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * check again make sure it didn&#39;t become free while</span>
<span class="p_del">-	 * we weren&#39;t looking</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	head = READ_ONCE(lock-&gt;tickets.head);</span>
<span class="p_del">-	if (__tickets_equal(head, want)) {</span>
<span class="p_del">-		add_stats(TAKEN_SLOW_PICKUP, 1);</span>
<span class="p_del">-		goto out;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Allow interrupts while blocked */</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * If an interrupt happens here, it will leave the wakeup irq</span>
<span class="p_del">-	 * pending, which will cause xen_poll_irq() to return</span>
<span class="p_del">-	 * immediately.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Block until irq becomes pending (or perhaps a spurious wakeup) */</span>
<span class="p_del">-	xen_poll_irq(irq);</span>
<span class="p_del">-	add_stats(TAKEN_SLOW_SPURIOUS, !xen_test_irq_pending(irq));</span>
<span class="p_del">-</span>
<span class="p_del">-	local_irq_save(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	kstat_incr_irq_this_cpu(irq);</span>
<span class="p_del">-out:</span>
<span class="p_del">-	cpumask_clear_cpu(cpu, &amp;waiting_cpus);</span>
<span class="p_del">-	w-&gt;lock = NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-	local_irq_restore(flags);</span>
<span class="p_del">-</span>
<span class="p_del">-	spin_time_accum_blocked(start);</span>
<span class="p_del">-}</span>
<span class="p_del">-PV_CALLEE_SAVE_REGS_THUNK(xen_lock_spinning);</span>
<span class="p_del">-</span>
<span class="p_del">-static void xen_unlock_kick(struct arch_spinlock *lock, __ticket_t next)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int cpu;</span>
<span class="p_del">-</span>
<span class="p_del">-	add_stats(RELEASED_SLOW, 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	for_each_cpu(cpu, &amp;waiting_cpus) {</span>
<span class="p_del">-		const struct xen_lock_waiting *w = &amp;per_cpu(lock_waiting, cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-		/* Make sure we read lock before want */</span>
<span class="p_del">-		if (READ_ONCE(w-&gt;lock) == lock &amp;&amp;</span>
<span class="p_del">-		    READ_ONCE(w-&gt;want) == next) {</span>
<span class="p_del">-			add_stats(RELEASED_SLOW_KICKED, 1);</span>
<span class="p_del">-			xen_send_IPI_one(cpu, XEN_SPIN_UNLOCK_VECTOR);</span>
<span class="p_del">-			break;</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif /* CONFIG_QUEUED_SPINLOCKS */</span>
<span class="p_del">-</span>
 static irqreturn_t dummy_handler(int irq, void *dev_id)
 {
 	BUG();
<span class="p_chunk">@@ -334,16 +131,12 @@</span> <span class="p_context"> void __init xen_init_spinlocks(void)</span>
 		return;
 	}
 	printk(KERN_DEBUG &quot;xen: PV spinlocks enabled\n&quot;);
<span class="p_del">-#ifdef CONFIG_QUEUED_SPINLOCKS</span>
<span class="p_add">+</span>
 	__pv_init_lock_hash();
 	pv_lock_ops.queued_spin_lock_slowpath = __pv_queued_spin_lock_slowpath;
 	pv_lock_ops.queued_spin_unlock = PV_CALLEE_SAVE(__pv_queued_spin_unlock);
 	pv_lock_ops.wait = xen_qlock_wait;
 	pv_lock_ops.kick = xen_qlock_kick;
<span class="p_del">-#else</span>
<span class="p_del">-	pv_lock_ops.lock_spinning = PV_CALLEE_SAVE(xen_lock_spinning);</span>
<span class="p_del">-	pv_lock_ops.unlock_kick = xen_unlock_kick;</span>
<span class="p_del">-#endif</span>
 }
 
 /*
<span class="p_chunk">@@ -372,44 +165,3 @@</span> <span class="p_context"> static __init int xen_parse_nopvspin(char *arg)</span>
 }
 early_param(&quot;xen_nopvspin&quot;, xen_parse_nopvspin);
 
<span class="p_del">-#if defined(CONFIG_XEN_DEBUG_FS) &amp;&amp; !defined(CONFIG_QUEUED_SPINLOCKS)</span>
<span class="p_del">-</span>
<span class="p_del">-static struct dentry *d_spin_debug;</span>
<span class="p_del">-</span>
<span class="p_del">-static int __init xen_spinlock_debugfs(void)</span>
<span class="p_del">-{</span>
<span class="p_del">-	struct dentry *d_xen = xen_init_debugfs();</span>
<span class="p_del">-</span>
<span class="p_del">-	if (d_xen == NULL)</span>
<span class="p_del">-		return -ENOMEM;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!xen_pvspin)</span>
<span class="p_del">-		return 0;</span>
<span class="p_del">-</span>
<span class="p_del">-	d_spin_debug = debugfs_create_dir(&quot;spinlocks&quot;, d_xen);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u8(&quot;zero_stats&quot;, 0644, d_spin_debug, &amp;zero_stats);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow_pickup&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_PICKUP]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;taken_slow_spurious&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[TAKEN_SLOW_SPURIOUS]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[RELEASED_SLOW]);</span>
<span class="p_del">-	debugfs_create_u32(&quot;released_slow_kicked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.contention_stats[RELEASED_SLOW_KICKED]);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u64(&quot;time_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-			   &amp;spinlock_stats.time_blocked);</span>
<span class="p_del">-</span>
<span class="p_del">-	debugfs_create_u32_array(&quot;histo_blocked&quot;, 0444, d_spin_debug,</span>
<span class="p_del">-				spinlock_stats.histo_spin_blocked, HISTO_BUCKETS + 1);</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-fs_initcall(xen_spinlock_debugfs);</span>
<span class="p_del">-</span>
<span class="p_del">-#endif	/* CONFIG_XEN_DEBUG_FS */</span>

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



