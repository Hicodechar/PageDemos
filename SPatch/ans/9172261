
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[v1,3/3] mm: per-process reclaim - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [v1,3/3] mm: per-process reclaim</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>June 13, 2016, 7:50 a.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1465804259-29345-4-git-send-email-minchan@kernel.org&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/9172261/mbox/"
   >mbox</a>
|
   <a href="/patch/9172261/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/9172261/">/patch/9172261/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	31CBA604DB for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Jun 2016 07:51:58 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 22DC322230
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Jun 2016 07:51:58 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id 17C1327AC2; Mon, 13 Jun 2016 07:51:58 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 4202622230
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 13 Jun 2016 07:51:57 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S964940AbcFMHvw (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 13 Jun 2016 03:51:52 -0400
Received: from LGEAMRELO13.lge.com ([156.147.23.53]:37233 &quot;EHLO
	lgeamrelo13.lge.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S933596AbcFMHvG (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 13 Jun 2016 03:51:06 -0400
Received: from unknown (HELO lgeamrelo02.lge.com) (156.147.1.126)
	by 156.147.23.53 with ESMTP; 13 Jun 2016 16:51:04 +0900
X-Original-SENDERIP: 156.147.1.126
X-Original-MAILFROM: minchan@kernel.org
Received: from unknown (HELO localhost.localdomain) (10.177.223.161)
	by 156.147.1.126 with ESMTP; 13 Jun 2016 16:51:03 +0900
X-Original-SENDERIP: 10.177.223.161
X-Original-MAILFROM: minchan@kernel.org
From: Minchan Kim &lt;minchan@kernel.org&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, linux-mm@kvack.org,
	Rik van Riel &lt;riel@redhat.com&gt;, Minchan Kim &lt;minchan@kernel.org&gt;,
	Sangwoo Park &lt;sangwoo2.park@lge.com&gt;
Subject: [PATCH v1 3/3] mm: per-process reclaim
Date: Mon, 13 Jun 2016 16:50:58 +0900
Message-Id: &lt;1465804259-29345-4-git-send-email-minchan@kernel.org&gt;
X-Mailer: git-send-email 1.9.1
In-Reply-To: &lt;1465804259-29345-1-git-send-email-minchan@kernel.org&gt;
References: &lt;1465804259-29345-1-git-send-email-minchan@kernel.org&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - June 13, 2016, 7:50 a.m.</div>
<pre class="content">
These day, there are many platforms available in the embedded market
and sometime, they has more hints about workingset than kernel so
they want to involve memory management more heavily like android&#39;s
lowmemory killer and ashmem or user-daemon with lowmemory notifier.

This patch adds add new method for userspace to manage memory
efficiently via knob &quot;/proc/&lt;pid&gt;/reclaim&quot; so platform can reclaim
any process anytime.

One of useful usecase is to avoid process killing for getting free
memory in android, which was really terrible experience because I
lost my best score of game I had ever after I switch the phone call
while I enjoyed the game as well as slow start-up by cold launching.

Our product have used it in real procuct.

Quote from Sangwoo Park &lt;angwoo2.park@lge.com&gt;
Thanks for the data, Sangwoo!
&quot;
- Test scenaro
  - platform: android
  - target: MSM8952, 2G DDR, 16G eMMC
  - scenario
    retry app launch and Back Home with 16 apps and 16 turns
    (total app launch count is 256)
  - result:
			  resume count   |  cold launching count
-----------------------------------------------------------------
 vanilla           |           85        |          171
 perproc reclaim   |           184       |           72
&quot;

Higher resume count is better because cold launching needs loading
lots of resource data which takes above 15 ~ 20 seconds for some
games while successful resume just takes 1~5 second.

As perproc reclaim way with new management policy, we could reduce
cold launching a lot(i.e., 171-72) so that it reduces app startup
a lot.

Another useful function from this feature is to make swapout easily
which is useful for testing swapout stress and workloads.

Interface:

Reclaim file-backed pages only.
	echo 1 &gt; /proc/&lt;pid&gt;/reclaim
Reclaim anonymous pages only.
	echo 2 &gt; /proc/&lt;pid&gt;/reclaim
Reclaim all pages
	echo 3 &gt; /proc/&lt;pid&gt;/reclaim

bit 1 : file, bit 2 : anon, bit 1 &amp; 2 : all

Note:
If a page is shared by other processes(i.e., page_mapcount(page) &gt; 1),
it couldn&#39;t be reclaimed.

Cc: Sangwoo Park &lt;sangwoo2.park@lge.com&gt;
<span class="signed-off-by">Signed-off-by: Minchan Kim &lt;minchan@kernel.org&gt;</span>
---
 Documentation/filesystems/proc.txt |  15 ++++
 fs/proc/base.c                     |   1 +
 fs/proc/internal.h                 |   1 +
 fs/proc/task_mmu.c                 | 149 +++++++++++++++++++++++++++++++++++++
 include/linux/rmap.h               |   4 +
 mm/vmscan.c                        |  40 ++++++++++
 6 files changed, 210 insertions(+)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - June 13, 2016, 3:06 p.m.</div>
<pre class="content">
Hi Minchan,

On Mon, Jun 13, 2016 at 04:50:58PM +0900, Minchan Kim wrote:
<span class="quote">&gt; These day, there are many platforms available in the embedded market</span>
<span class="quote">&gt; and sometime, they has more hints about workingset than kernel so</span>
<span class="quote">&gt; they want to involve memory management more heavily like android&#39;s</span>
<span class="quote">&gt; lowmemory killer and ashmem or user-daemon with lowmemory notifier.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch adds add new method for userspace to manage memory</span>
<span class="quote">&gt; efficiently via knob &quot;/proc/&lt;pid&gt;/reclaim&quot; so platform can reclaim</span>
<span class="quote">&gt; any process anytime.</span>

Cgroups are our canonical way to control system resources on a per
process or group-of-processes level. I don&#39;t like the idea of adding
ad-hoc interfaces for single-use cases like this.

For this particular case, you can already stick each app into its own
cgroup and use memory.force_empty to target-reclaim them.

Or better yet, set the soft limits / memory.low to guide physical
memory pressure, once it actually occurs, toward the least-important
apps? We usually prefer doing work on-demand rather than proactively.

The one-cgroup-per-app model would give Android much more control and
would also remove a *lot* of overhead during task switches, see this:
https://lkml.org/lkml/2014/12/19/358
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=181">Rik van Riel</a> - June 13, 2016, 5:06 p.m.</div>
<pre class="content">
On Mon, 2016-06-13 at 16:50 +0900, Minchan Kim wrote:
<span class="quote">&gt; These day, there are many platforms available in the embedded market</span>
<span class="quote">&gt; and sometime, they has more hints about workingset than kernel so</span>
<span class="quote">&gt; they want to involve memory management more heavily like android&#39;s</span>
<span class="quote">&gt; lowmemory killer and ashmem or user-daemon with lowmemory notifier.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; This patch adds add new method for userspace to manage memory</span>
<span class="quote">&gt; efficiently via knob &quot;/proc/&lt;pid&gt;/reclaim&quot; so platform can reclaim</span>
<span class="quote">&gt; any process anytime.</span>
<span class="quote">&gt; </span>

Could it make sense to invoke this automatically,
perhaps from the Android low memory killer code?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - June 15, 2016, 12:40 a.m.</div>
<pre class="content">
Hi Johannes,

On Mon, Jun 13, 2016 at 11:06:53AM -0400, Johannes Weiner wrote:
<span class="quote">&gt; Hi Minchan,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Mon, Jun 13, 2016 at 04:50:58PM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; These day, there are many platforms available in the embedded market</span>
<span class="quote">&gt; &gt; and sometime, they has more hints about workingset than kernel so</span>
<span class="quote">&gt; &gt; they want to involve memory management more heavily like android&#39;s</span>
<span class="quote">&gt; &gt; lowmemory killer and ashmem or user-daemon with lowmemory notifier.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This patch adds add new method for userspace to manage memory</span>
<span class="quote">&gt; &gt; efficiently via knob &quot;/proc/&lt;pid&gt;/reclaim&quot; so platform can reclaim</span>
<span class="quote">&gt; &gt; any process anytime.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cgroups are our canonical way to control system resources on a per</span>
<span class="quote">&gt; process or group-of-processes level. I don&#39;t like the idea of adding</span>
<span class="quote">&gt; ad-hoc interfaces for single-use cases like this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For this particular case, you can already stick each app into its own</span>
<span class="quote">&gt; cgroup and use memory.force_empty to target-reclaim them.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or better yet, set the soft limits / memory.low to guide physical</span>
<span class="quote">&gt; memory pressure, once it actually occurs, toward the least-important</span>
<span class="quote">&gt; apps? We usually prefer doing work on-demand rather than proactively.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The one-cgroup-per-app model would give Android much more control and</span>
<span class="quote">&gt; would also remove a *lot* of overhead during task switches, see this:</span>
<span class="quote">&gt; https://lkml.org/lkml/2014/12/19/358</span>

I didn&#39;t notice that. Thanks for the pointing.
I read the thread you pointed out and read memcg code.

Firstly, I thought one-cgroup-per-app model is abuse of memcg but now
I feel your suggestion does make sense that it&#39;s right direction for
control memory from the userspace. Just a concern is that not sure
how hard we can map memory management model from global memory pressure
to per-app pressure model smoothly.

A question is it seems cgroup2 doesn&#39;t have per-cgroup swappiness.
Why?

I think we need it in one-cgroup-per-app model.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - June 15, 2016, 1:01 a.m.</div>
<pre class="content">
On Mon, Jun 13, 2016 at 01:06:35PM -0400, Rik van Riel wrote:
<span class="quote">&gt; On Mon, 2016-06-13 at 16:50 +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; These day, there are many platforms available in the embedded market</span>
<span class="quote">&gt; &gt; and sometime, they has more hints about workingset than kernel so</span>
<span class="quote">&gt; &gt; they want to involve memory management more heavily like android&#39;s</span>
<span class="quote">&gt; &gt; lowmemory killer and ashmem or user-daemon with lowmemory notifier.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; This patch adds add new method for userspace to manage memory</span>
<span class="quote">&gt; &gt; efficiently via knob &quot;/proc/&lt;pid&gt;/reclaim&quot; so platform can reclaim</span>
<span class="quote">&gt; &gt; any process anytime.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Could it make sense to invoke this automatically,</span>
<span class="quote">&gt; perhaps from the Android low memory killer code?</span>

It&#39;s doable. In fact, It was first internal implementation of our
product. However, I wanted to use it on platforms which don&#39;t have
lowmemory killer. :)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=137061">Michal Hocko</a> - June 16, 2016, 11:07 a.m.</div>
<pre class="content">
On Wed 15-06-16 09:40:27, Minchan Kim wrote:
[...]
<span class="quote">&gt; A question is it seems cgroup2 doesn&#39;t have per-cgroup swappiness.</span>
<span class="quote">&gt; Why?</span>

There was no strong use case for it AFAICT.
<span class="quote"> 
&gt; I think we need it in one-cgroup-per-app model.</span>

I wouldn&#39;t be opposed if it is really needed.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=45">Johannes Weiner</a> - June 16, 2016, 2:41 p.m.</div>
<pre class="content">
On Wed, Jun 15, 2016 at 09:40:27AM +0900, Minchan Kim wrote:
<span class="quote">&gt; A question is it seems cgroup2 doesn&#39;t have per-cgroup swappiness.</span>
<span class="quote">&gt; Why?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; I think we need it in one-cgroup-per-app model.</span>

Can you explain why you think that?

As we have talked about this recently in the LRU balancing thread,
swappiness is the cost factor between file IO and swapping, so the
only situation I can imagine you&#39;d need a memcg swappiness setting is
when you have different cgroups use different storage devices that do
not have comparable speeds.

So I&#39;m not sure I understand the relationship to an app-group model.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=36811">Minchan Kim</a> - June 17, 2016, 6:43 a.m.</div>
<pre class="content">
Hi Hannes,

On Thu, Jun 16, 2016 at 10:41:02AM -0400, Johannes Weiner wrote:
<span class="quote">&gt; On Wed, Jun 15, 2016 at 09:40:27AM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt; &gt; A question is it seems cgroup2 doesn&#39;t have per-cgroup swappiness.</span>
<span class="quote">&gt; &gt; Why?</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; I think we need it in one-cgroup-per-app model.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Can you explain why you think that?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; As we have talked about this recently in the LRU balancing thread,</span>
<span class="quote">&gt; swappiness is the cost factor between file IO and swapping, so the</span>
<span class="quote">&gt; only situation I can imagine you&#39;d need a memcg swappiness setting is</span>
<span class="quote">&gt; when you have different cgroups use different storage devices that do</span>
<span class="quote">&gt; not have comparable speeds.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; So I&#39;m not sure I understand the relationship to an app-group model.</span>

Sorry for lacking the inforamtion. I should have written more clear.
In fact, what we need is *per-memcg-swap-device*.

What I want is to avoid kill background application although memory
is overflow because cold launcing of app takes a very long time
compared to resume(ie, just switching). I also want to keep a mount
of free pages in the memory so that new application startup cannot
be stuck by reclaim activities.

To get free memory, I want to reclaim less important app rather than
killing. In this time, we can support two swap devices.

A one is zram, other is slow storage but much bigger than zram size.
Then, we can use storage swap to reclaim pages for not-important app
while we can use zram swap for for important app(e.g., forground app,
system services, daemon and so on).

IOW, we want to support mutiple swap device with one-cgroup-per-app
and the storage speed is totally different.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=4745">Balbir Singh</a> - June 17, 2016, 7:24 a.m.</div>
<pre class="content">
On 14/06/16 01:06, Johannes Weiner wrote:
<span class="quote">&gt; Hi Minchan,</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; On Mon, Jun 13, 2016 at 04:50:58PM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt; These day, there are many platforms available in the embedded market</span>
<span class="quote">&gt;&gt; and sometime, they has more hints about workingset than kernel so</span>
<span class="quote">&gt;&gt; they want to involve memory management more heavily like android&#39;s</span>
<span class="quote">&gt;&gt; lowmemory killer and ashmem or user-daemon with lowmemory notifier.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; This patch adds add new method for userspace to manage memory</span>
<span class="quote">&gt;&gt; efficiently via knob &quot;/proc/&lt;pid&gt;/reclaim&quot; so platform can reclaim</span>
<span class="quote">&gt;&gt; any process anytime.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Cgroups are our canonical way to control system resources on a per</span>
<span class="quote">&gt; process or group-of-processes level. I don&#39;t like the idea of adding</span>
<span class="quote">&gt; ad-hoc interfaces for single-use cases like this.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; For this particular case, you can already stick each app into its own</span>
<span class="quote">&gt; cgroup and use memory.force_empty to target-reclaim them.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Or better yet, set the soft limits / memory.low to guide physical</span>
<span class="quote">&gt; memory pressure, once it actually occurs, toward the least-important</span>
<span class="quote">&gt; apps? We usually prefer doing work on-demand rather than proactively.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; The one-cgroup-per-app model would give Android much more control and</span>
<span class="quote">&gt; would also remove a *lot* of overhead during task switches, see this:</span>
<span class="quote">&gt; https://lkml.org/lkml/2014/12/19/358</span>

Yes, I&#39;d agree. cgroups can group many tasks, but the group size can be
1 as well. Could you try the same test with the recommended approach and
see if it works as desired? 

Balbir Singh
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=119931">Vinayak Menon</a> - June 17, 2016, 7:57 a.m.</div>
<pre class="content">
On 6/17/2016 12:54 PM, Balbir Singh wrote:
<span class="quote">&gt;</span>
<span class="quote">&gt; On 14/06/16 01:06, Johannes Weiner wrote:</span>
<span class="quote">&gt;&gt; Hi Minchan,</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; On Mon, Jun 13, 2016 at 04:50:58PM +0900, Minchan Kim wrote:</span>
<span class="quote">&gt;&gt;&gt; These day, there are many platforms available in the embedded market</span>
<span class="quote">&gt;&gt;&gt; and sometime, they has more hints about workingset than kernel so</span>
<span class="quote">&gt;&gt;&gt; they want to involve memory management more heavily like android&#39;s</span>
<span class="quote">&gt;&gt;&gt; lowmemory killer and ashmem or user-daemon with lowmemory notifier.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; This patch adds add new method for userspace to manage memory</span>
<span class="quote">&gt;&gt;&gt; efficiently via knob &quot;/proc/&lt;pid&gt;/reclaim&quot; so platform can reclaim</span>
<span class="quote">&gt;&gt;&gt; any process anytime.</span>
<span class="quote">&gt;&gt; Cgroups are our canonical way to control system resources on a per</span>
<span class="quote">&gt;&gt; process or group-of-processes level. I don&#39;t like the idea of adding</span>
<span class="quote">&gt;&gt; ad-hoc interfaces for single-use cases like this.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; For this particular case, you can already stick each app into its own</span>
<span class="quote">&gt;&gt; cgroup and use memory.force_empty to target-reclaim them.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Or better yet, set the soft limits / memory.low to guide physical</span>
<span class="quote">&gt;&gt; memory pressure, once it actually occurs, toward the least-important</span>
<span class="quote">&gt;&gt; apps? We usually prefer doing work on-demand rather than proactively.</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; The one-cgroup-per-app model would give Android much more control and</span>
<span class="quote">&gt;&gt; would also remove a *lot* of overhead during task switches, see this:</span>
<span class="quote">&gt;&gt; https://lkml.org/lkml/2014/12/19/358</span>
<span class="quote">&gt; Yes, I&#39;d agree. cgroups can group many tasks, but the group size can be</span>
<span class="quote">&gt; 1 as well. Could you try the same test with the recommended approach and</span>
<span class="quote">&gt; see if it works as desired? </span>
<span class="quote">&gt;</span>
With cgroup v2, IIUC there can be only a single hierarchy where all controllers exist, and
a process can be part of only one cgroup. If that is true, with per task cgroup, a task can
be present only in its own cgroup. That being the case would it be feasible to have other
parallel controllers like CPU which would not be able to work efficiently with per task cgroup ?
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/Documentation/filesystems/proc.txt b/Documentation/filesystems/proc.txt</span>
<span class="p_header">index 50fcf48f4d58..3b6adf370f3c 100644</span>
<span class="p_header">--- a/Documentation/filesystems/proc.txt</span>
<span class="p_header">+++ b/Documentation/filesystems/proc.txt</span>
<span class="p_chunk">@@ -138,6 +138,7 @@</span> <span class="p_context"> Table 1-1: Process specific entries in /proc</span>
  maps		Memory maps to executables and library files	(2.4)
  mem		Memory held by this process
  root		Link to the root directory of this process
<span class="p_add">+ reclaim	Reclaim pages in this process</span>
  stat		Process status
  statm		Process memory status information
  status		Process status in human readable form
<span class="p_chunk">@@ -536,6 +537,20 @@</span> <span class="p_context"> To reset the peak resident set size (&quot;high water mark&quot;) to the process&#39;s</span>
 
 Any other value written to /proc/PID/clear_refs will have no effect.
 
<span class="p_add">+The file /proc/PID/reclaim is used to reclaim pages in this process.</span>
<span class="p_add">+bit 1: file, bit 2: anon, bit 3: all</span>
<span class="p_add">+</span>
<span class="p_add">+To reclaim file-backed pages,</span>
<span class="p_add">+    &gt; echo 1 &gt; /proc/PID/reclaim</span>
<span class="p_add">+</span>
<span class="p_add">+To reclaim anonymous pages,</span>
<span class="p_add">+    &gt; echo 2 &gt; /proc/PID/reclaim</span>
<span class="p_add">+</span>
<span class="p_add">+To reclaim all pages,</span>
<span class="p_add">+    &gt; echo 3 &gt; /proc/PID/reclaim</span>
<span class="p_add">+</span>
<span class="p_add">+If a page is shared by several processes, it cannot be reclaimed.</span>
<span class="p_add">+</span>
 The /proc/pid/pagemap gives the PFN, which can be used to find the pageflags
 using /proc/kpageflags and number of times a page is mapped using
 /proc/kpagecount. For detailed explanation, see Documentation/vm/pagemap.txt.
<span class="p_header">diff --git a/fs/proc/base.c b/fs/proc/base.c</span>
<span class="p_header">index 93e7754fd5b2..b957d929516d 100644</span>
<span class="p_header">--- a/fs/proc/base.c</span>
<span class="p_header">+++ b/fs/proc/base.c</span>
<span class="p_chunk">@@ -2848,6 +2848,7 @@</span> <span class="p_context"> static const struct pid_entry tgid_base_stuff[] = {</span>
 	REG(&quot;mounts&quot;,     S_IRUGO, proc_mounts_operations),
 	REG(&quot;mountinfo&quot;,  S_IRUGO, proc_mountinfo_operations),
 	REG(&quot;mountstats&quot;, S_IRUSR, proc_mountstats_operations),
<span class="p_add">+	REG(&quot;reclaim&quot;, S_IWUSR, proc_reclaim_operations),</span>
 #ifdef CONFIG_PROC_PAGE_MONITOR
 	REG(&quot;clear_refs&quot;, S_IWUSR, proc_clear_refs_operations),
 	REG(&quot;smaps&quot;,      S_IRUGO, proc_pid_smaps_operations),
<span class="p_header">diff --git a/fs/proc/internal.h b/fs/proc/internal.h</span>
<span class="p_header">index aa2781095bd1..ef2b01533c97 100644</span>
<span class="p_header">--- a/fs/proc/internal.h</span>
<span class="p_header">+++ b/fs/proc/internal.h</span>
<span class="p_chunk">@@ -209,6 +209,7 @@</span> <span class="p_context"> struct pde_opener {</span>
 extern const struct inode_operations proc_link_inode_operations;
 
 extern const struct inode_operations proc_pid_link_inode_operations;
<span class="p_add">+extern const struct file_operations proc_reclaim_operations;</span>
 
 extern void proc_init_inodecache(void);
 extern struct inode *proc_get_inode(struct super_block *, struct proc_dir_entry *);
<span class="p_header">diff --git a/fs/proc/task_mmu.c b/fs/proc/task_mmu.c</span>
<span class="p_header">index 187d84ef9de9..31e4657f8fe9 100644</span>
<span class="p_header">--- a/fs/proc/task_mmu.c</span>
<span class="p_header">+++ b/fs/proc/task_mmu.c</span>
<span class="p_chunk">@@ -11,6 +11,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/mempolicy.h&gt;
 #include &lt;linux/rmap.h&gt;
 #include &lt;linux/swap.h&gt;
<span class="p_add">+#include &lt;linux/mm_inline.h&gt;</span>
 #include &lt;linux/swapops.h&gt;
 #include &lt;linux/mmu_notifier.h&gt;
 #include &lt;linux/page_idle.h&gt;
<span class="p_chunk">@@ -1465,6 +1466,154 @@</span> <span class="p_context"> const struct file_operations proc_pagemap_operations = {</span>
 };
 #endif /* CONFIG_PROC_PAGE_MONITOR */
 
<span class="p_add">+static int reclaim_pte_range(pmd_t *pmd, unsigned long addr,</span>
<span class="p_add">+				unsigned long end, struct mm_walk *walk)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct mm_struct *mm = walk-&gt;mm;</span>
<span class="p_add">+	struct vm_area_struct *vma = walk-&gt;private;</span>
<span class="p_add">+	pte_t *orig_pte, *pte, ptent;</span>
<span class="p_add">+	spinlock_t *ptl;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+	LIST_HEAD(page_list);</span>
<span class="p_add">+	int isolated = 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	split_huge_pmd(vma, pmd, addr);</span>
<span class="p_add">+	if (pmd_trans_unstable(pmd))</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+</span>
<span class="p_add">+	orig_pte = pte = pte_offset_map_lock(mm, pmd, addr, &amp;ptl);</span>
<span class="p_add">+	for (; addr != end; pte++, addr += PAGE_SIZE) {</span>
<span class="p_add">+		ptent = *pte;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (!pte_present(ptent))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		page = vm_normal_page(vma, addr, ptent);</span>
<span class="p_add">+		if (!page)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (page_mapcount(page) != 1)</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (PageTransCompound(page)) {</span>
<span class="p_add">+			get_page(page);</span>
<span class="p_add">+			if (!trylock_page(page)) {</span>
<span class="p_add">+				put_page(page);</span>
<span class="p_add">+				goto out;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			pte_unmap_unlock(orig_pte, ptl);</span>
<span class="p_add">+</span>
<span class="p_add">+			if (split_huge_page(page)) {</span>
<span class="p_add">+				unlock_page(page);</span>
<span class="p_add">+				put_page(page);</span>
<span class="p_add">+				orig_pte = pte_offset_map_lock(mm, pmd,</span>
<span class="p_add">+								addr, &amp;ptl);</span>
<span class="p_add">+				goto out;</span>
<span class="p_add">+			}</span>
<span class="p_add">+			put_page(page);</span>
<span class="p_add">+			unlock_page(page);</span>
<span class="p_add">+			pte = pte_offset_map_lock(mm, pmd, addr, &amp;ptl);</span>
<span class="p_add">+			pte--;</span>
<span class="p_add">+			addr -= PAGE_SIZE;</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+		}</span>
<span class="p_add">+</span>
<span class="p_add">+		VM_BUG_ON_PAGE(PageTransCompound(page), page);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (isolate_lru_page(page))</span>
<span class="p_add">+			continue;</span>
<span class="p_add">+</span>
<span class="p_add">+		list_add(&amp;page-&gt;lru, &amp;page_list);</span>
<span class="p_add">+		inc_zone_page_state(page, NR_ISOLATED_ANON +</span>
<span class="p_add">+					page_is_file_cache(page));</span>
<span class="p_add">+		isolated++;</span>
<span class="p_add">+		if (isolated &gt;= SWAP_CLUSTER_MAX) {</span>
<span class="p_add">+			pte_unmap_unlock(orig_pte, ptl);</span>
<span class="p_add">+			reclaim_pages_from_list(&amp;page_list);</span>
<span class="p_add">+			isolated = 0;</span>
<span class="p_add">+			cond_resched();</span>
<span class="p_add">+			orig_pte = pte_offset_map_lock(mm, pmd, addr, &amp;ptl);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+out:</span>
<span class="p_add">+	pte_unmap_unlock(orig_pte, ptl);</span>
<span class="p_add">+	reclaim_pages_from_list(&amp;page_list);</span>
<span class="p_add">+</span>
<span class="p_add">+	cond_resched();</span>
<span class="p_add">+	return 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+enum reclaim_type {</span>
<span class="p_add">+	RECLAIM_FILE = 1,</span>
<span class="p_add">+	RECLAIM_ANON,</span>
<span class="p_add">+	RECLAIM_ALL,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static ssize_t reclaim_write(struct file *file, const char __user *buf,</span>
<span class="p_add">+				size_t count, loff_t *ppos)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct task_struct *task;</span>
<span class="p_add">+	char buffer[PROC_NUMBUF];</span>
<span class="p_add">+	struct mm_struct *mm;</span>
<span class="p_add">+	struct vm_area_struct *vma;</span>
<span class="p_add">+	int itype;</span>
<span class="p_add">+	int rv;</span>
<span class="p_add">+	enum reclaim_type type;</span>
<span class="p_add">+</span>
<span class="p_add">+	memset(buffer, 0, sizeof(buffer));</span>
<span class="p_add">+	if (count &gt; sizeof(buffer) - 1)</span>
<span class="p_add">+		count = sizeof(buffer) - 1;</span>
<span class="p_add">+	if (copy_from_user(buffer, buf, count))</span>
<span class="p_add">+		return -EFAULT;</span>
<span class="p_add">+	rv = kstrtoint(strstrip(buffer), 10, &amp;itype);</span>
<span class="p_add">+	if (rv &lt; 0)</span>
<span class="p_add">+		return rv;</span>
<span class="p_add">+	type = (enum reclaim_type)itype;</span>
<span class="p_add">+	if (type &lt; RECLAIM_FILE || type &gt; RECLAIM_ALL)</span>
<span class="p_add">+		return -EINVAL;</span>
<span class="p_add">+</span>
<span class="p_add">+	task = get_proc_task(file-&gt;f_path.dentry-&gt;d_inode);</span>
<span class="p_add">+	if (!task)</span>
<span class="p_add">+		return -ESRCH;</span>
<span class="p_add">+</span>
<span class="p_add">+	mm = get_task_mm(task);</span>
<span class="p_add">+	if (mm) {</span>
<span class="p_add">+		struct mm_walk reclaim_walk = {</span>
<span class="p_add">+			.pmd_entry = reclaim_pte_range,</span>
<span class="p_add">+			.mm = mm,</span>
<span class="p_add">+		};</span>
<span class="p_add">+</span>
<span class="p_add">+		down_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		for (vma = mm-&gt;mmap; vma; vma = vma-&gt;vm_next) {</span>
<span class="p_add">+			reclaim_walk.private = vma;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (is_vm_hugetlb_page(vma))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (!vma_is_anonymous(vma) &amp;&amp; !(type &amp; RECLAIM_FILE))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			if (vma_is_anonymous(vma) &amp;&amp; !(type &amp; RECLAIM_ANON))</span>
<span class="p_add">+				continue;</span>
<span class="p_add">+</span>
<span class="p_add">+			walk_page_range(vma-&gt;vm_start, vma-&gt;vm_end,</span>
<span class="p_add">+					&amp;reclaim_walk);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		flush_tlb_mm(mm);</span>
<span class="p_add">+		up_read(&amp;mm-&gt;mmap_sem);</span>
<span class="p_add">+		mmput(mm);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	put_task_struct(task);</span>
<span class="p_add">+</span>
<span class="p_add">+	return count;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+const struct file_operations proc_reclaim_operations = {</span>
<span class="p_add">+	.write		= reclaim_write,</span>
<span class="p_add">+	.llseek		= noop_llseek,</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
 #ifdef CONFIG_NUMA
 
 struct numa_maps {
<span class="p_header">diff --git a/include/linux/rmap.h b/include/linux/rmap.h</span>
<span class="p_header">index 5704f101b52e..e90a21b78da3 100644</span>
<span class="p_header">--- a/include/linux/rmap.h</span>
<span class="p_header">+++ b/include/linux/rmap.h</span>
<span class="p_chunk">@@ -10,6 +10,10 @@</span> <span class="p_context"></span>
 #include &lt;linux/rwsem.h&gt;
 #include &lt;linux/memcontrol.h&gt;
 
<span class="p_add">+extern int isolate_lru_page(struct page *page);</span>
<span class="p_add">+extern void putback_lru_page(struct page *page);</span>
<span class="p_add">+extern unsigned long reclaim_pages_from_list(struct list_head *page_list);</span>
<span class="p_add">+</span>
 /*
  * The anon_vma heads a list of private &quot;related&quot; vmas, to scan if
  * an anonymous page pointing to this anon_vma needs to be unmapped:
<span class="p_header">diff --git a/mm/vmscan.c b/mm/vmscan.c</span>
<span class="p_header">index d20c9e863d35..442866f77251 100644</span>
<span class="p_header">--- a/mm/vmscan.c</span>
<span class="p_header">+++ b/mm/vmscan.c</span>
<span class="p_chunk">@@ -1212,6 +1212,13 @@</span> <span class="p_context"> static unsigned long shrink_page_list(struct list_head *page_list,</span>
 		 * appear not as the counts should be low
 		 */
 		list_add(&amp;page-&gt;lru, &amp;free_pages);
<span class="p_add">+		/*</span>
<span class="p_add">+		 * If pagelist are from multiple zones, we should decrease</span>
<span class="p_add">+		 * NR_ISOLATED_ANON + x on freed pages in here.</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		if (!zone)</span>
<span class="p_add">+			dec_zone_page_state(page, NR_ISOLATED_ANON +</span>
<span class="p_add">+					page_is_file_cache(page));</span>
 		continue;
 
 cull_mlocked:
<span class="p_chunk">@@ -1280,6 +1287,39 @@</span> <span class="p_context"> unsigned long reclaim_clean_pages_from_list(struct zone *zone,</span>
 	return ret;
 }
 
<span class="p_add">+unsigned long reclaim_pages_from_list(struct list_head *page_list)</span>
<span class="p_add">+{</span>
<span class="p_add">+	struct scan_control sc = {</span>
<span class="p_add">+		.gfp_mask = GFP_KERNEL,</span>
<span class="p_add">+		.priority = DEF_PRIORITY,</span>
<span class="p_add">+		.may_writepage = 1,</span>
<span class="p_add">+		.may_unmap = 1,</span>
<span class="p_add">+		.may_swap = 1,</span>
<span class="p_add">+		.force_reclaim = 1,</span>
<span class="p_add">+	};</span>
<span class="p_add">+</span>
<span class="p_add">+	unsigned long nr_reclaimed, dummy1, dummy2, dummy3, dummy4, dummy5;</span>
<span class="p_add">+	struct page *page;</span>
<span class="p_add">+</span>
<span class="p_add">+	list_for_each_entry(page, page_list, lru)</span>
<span class="p_add">+		ClearPageActive(page);</span>
<span class="p_add">+</span>
<span class="p_add">+	nr_reclaimed = shrink_page_list(page_list, &amp;sc,</span>
<span class="p_add">+					TTU_UNMAP|TTU_IGNORE_ACCESS,</span>
<span class="p_add">+					&amp;dummy1, &amp;dummy2, &amp;dummy3,</span>
<span class="p_add">+					&amp;dummy4, &amp;dummy5);</span>
<span class="p_add">+</span>
<span class="p_add">+	while (!list_empty(page_list)) {</span>
<span class="p_add">+		page = lru_to_page(page_list);</span>
<span class="p_add">+		list_del(&amp;page-&gt;lru);</span>
<span class="p_add">+		dec_zone_page_state(page, NR_ISOLATED_ANON +</span>
<span class="p_add">+				page_is_file_cache(page));</span>
<span class="p_add">+		putback_lru_page(page);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return nr_reclaimed;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * Attempt to remove the specified page from its LRU.  Only take this page
  * if it is of the appropriate PageActive status.  Pages which are being

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



