
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[1/1] mm: thp: kvm: fix memory corruption in KVM with THP enabled - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [1/1] mm: thp: kvm: fix memory corruption in KVM with THP enabled</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>April 27, 2016, 12:04 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;1461758686-27157-1-git-send-email-aarcange@redhat.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8956211/mbox/"
   >mbox</a>
|
   <a href="/patch/8956211/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8956211/">/patch/8956211/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork2.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork2.web.kernel.org (Postfix) with ESMTP id 2DF4EBF29F
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 27 Apr 2016 12:04:56 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 6690D201EF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 27 Apr 2016 12:04:55 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 47F672025B
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 27 Apr 2016 12:04:54 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S1753471AbcD0MEu (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 27 Apr 2016 08:04:50 -0400
Received: from mx1.redhat.com ([209.132.183.28]:60564 &quot;EHLO mx1.redhat.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S1753116AbcD0MEt (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 27 Apr 2016 08:04:49 -0400
Received: from int-mx11.intmail.prod.int.phx2.redhat.com
	(int-mx11.intmail.prod.int.phx2.redhat.com [10.5.11.24])
	(using TLSv1.2 with cipher ECDHE-RSA-AES256-GCM-SHA384 (256/256
	bits)) (No client certificate requested)
	by mx1.redhat.com (Postfix) with ESMTPS id 89FF1821C7;
	Wed, 27 Apr 2016 12:04:48 +0000 (UTC)
Received: from mail.random (ovpn-116-29.ams2.redhat.com [10.36.116.29])
	by int-mx11.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with
	ESMTP id u3RC4k47016455
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256
	verify=NO); Wed, 27 Apr 2016 08:04:47 -0400
From: Andrea Arcangeli &lt;aarcange@redhat.com&gt;
To: Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	linux-kernel@vger.kernel.org, linux-mm@kvack.org, qemu-devel@nongnu.org
Cc: &quot;Dr. David Alan Gilbert&quot; &lt;dgilbert@redhat.com&gt;,
	&quot;Kirill A. Shutemov&quot; &lt;kirill@shutemov.name&gt;,
	&quot;Li, Liang Z&quot; &lt;liang.z.li@intel.com&gt;, Amit Shah &lt;amit.shah@redhat.com&gt;,
	Paolo Bonzini &lt;pbonzini@redhat.com&gt;
Subject: [PATCH 1/1] mm: thp: kvm: fix memory corruption in KVM with THP
	enabled
Date: Wed, 27 Apr 2016 14:04:46 +0200
Message-Id: &lt;1461758686-27157-1-git-send-email-aarcange@redhat.com&gt;
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.24
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-7.9 required=5.0 tests=BAYES_00, RCVD_IN_DNSWL_HI, 
	RP_MATCHES_RCVD,
	UNPARSEABLE_RELAY autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - April 27, 2016, 12:04 p.m.</div>
<pre class="content">
After the THP refcounting change, obtaining a compound pages from
get_user_pages() no longer allows us to assume the entire compound
page is immediately mappable from a secondary MMU.

A secondary MMU doesn&#39;t want to call get_user_pages() more than once
for each compound page, in order to know if it can map the whole
compound page. So a secondary MMU needs to know from a single
get_user_pages() invocation when it can map immediately the entire
compound page to avoid a flood of unnecessary secondary MMU faults and
spurious atomic_inc()/atomic_dec() (pages don&#39;t have to be pinned by
MMU notifier users).

Ideally instead of the page-&gt;_mapcount &lt; 1 check, get_user_pages()
should return the granularity of the &quot;page&quot; mapping in the &quot;mm&quot; passed
to get_user_pages(). However it&#39;s non trivial change to pass the &quot;pmd&quot;
status belonging to the &quot;mm&quot; walked by get_user_pages up the stack (up
to the caller of get_user_pages). So the fix just checks if there is
not a single pte mapping on the page returned by get_user_pages, and
in turn if the caller can assume that the whole compound page is
mapped in the current &quot;mm&quot; (in a pmd_trans_huge()). In such case the
entire compound page is safe to map into the secondary MMU without
additional get_user_pages() calls on the surrounding tail/head
pages. In addition of being faster, not having to run other
get_user_pages() calls also reduces the memory footprint of the
secondary MMU fault in case the pmd split happened as result of memory
pressure.

Without this fix after a MADV_DONTNEED (like invoked by QEMU during
postcopy live migration or balloning) or after generic swapping (with
a failure in split_huge_page() that would only result in pmd splitting
and not a physical page split), KVM would map the whole compound page
into the shadow pagetables, despite regular faults or userfaults (like
UFFDIO_COPY) may map regular pages into the primary MMU as result of
the pte faults, leading to the guest mode and userland mode going out
of sync and not working on the same memory at all times.
<span class="signed-off-by">
Signed-off-by: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
---
 arch/arm/kvm/mmu.c         |  2 +-
 arch/x86/kvm/mmu.c         |  4 ++--
 include/linux/page-flags.h | 22 ++++++++++++++++++++++
 3 files changed, 25 insertions(+), 3 deletions(-)
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - April 27, 2016, 1:50 p.m.</div>
<pre class="content">
On Wed, Apr 27, 2016 at 02:04:46PM +0200, Andrea Arcangeli wrote:
<span class="quote">&gt; After the THP refcounting change, obtaining a compound pages from</span>
<span class="quote">&gt; get_user_pages() no longer allows us to assume the entire compound</span>
<span class="quote">&gt; page is immediately mappable from a secondary MMU.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; A secondary MMU doesn&#39;t want to call get_user_pages() more than once</span>
<span class="quote">&gt; for each compound page, in order to know if it can map the whole</span>
<span class="quote">&gt; compound page. So a secondary MMU needs to know from a single</span>
<span class="quote">&gt; get_user_pages() invocation when it can map immediately the entire</span>
<span class="quote">&gt; compound page to avoid a flood of unnecessary secondary MMU faults and</span>
<span class="quote">&gt; spurious atomic_inc()/atomic_dec() (pages don&#39;t have to be pinned by</span>
<span class="quote">&gt; MMU notifier users).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Ideally instead of the page-&gt;_mapcount &lt; 1 check, get_user_pages()</span>
<span class="quote">&gt; should return the granularity of the &quot;page&quot; mapping in the &quot;mm&quot; passed</span>
<span class="quote">&gt; to get_user_pages(). However it&#39;s non trivial change to pass the &quot;pmd&quot;</span>
<span class="quote">&gt; status belonging to the &quot;mm&quot; walked by get_user_pages up the stack (up</span>
<span class="quote">&gt; to the caller of get_user_pages). So the fix just checks if there is</span>
<span class="quote">&gt; not a single pte mapping on the page returned by get_user_pages, and</span>
<span class="quote">&gt; in turn if the caller can assume that the whole compound page is</span>
<span class="quote">&gt; mapped in the current &quot;mm&quot; (in a pmd_trans_huge()). In such case the</span>
<span class="quote">&gt; entire compound page is safe to map into the secondary MMU without</span>
<span class="quote">&gt; additional get_user_pages() calls on the surrounding tail/head</span>
<span class="quote">&gt; pages. In addition of being faster, not having to run other</span>
<span class="quote">&gt; get_user_pages() calls also reduces the memory footprint of the</span>
<span class="quote">&gt; secondary MMU fault in case the pmd split happened as result of memory</span>
<span class="quote">&gt; pressure.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Without this fix after a MADV_DONTNEED (like invoked by QEMU during</span>
<span class="quote">&gt; postcopy live migration or balloning) or after generic swapping (with</span>
<span class="quote">&gt; a failure in split_huge_page() that would only result in pmd splitting</span>
<span class="quote">&gt; and not a physical page split), KVM would map the whole compound page</span>
<span class="quote">&gt; into the shadow pagetables, despite regular faults or userfaults (like</span>
<span class="quote">&gt; UFFDIO_COPY) may map regular pages into the primary MMU as result of</span>
<span class="quote">&gt; the pte faults, leading to the guest mode and userland mode going out</span>
<span class="quote">&gt; of sync and not working on the same memory at all times.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Andrea Arcangeli &lt;aarcange@redhat.com&gt;</span>
<span class="quote">&gt; ---</span>
<span class="quote">&gt;  arch/arm/kvm/mmu.c         |  2 +-</span>
<span class="quote">&gt;  arch/x86/kvm/mmu.c         |  4 ++--</span>
<span class="quote">&gt;  include/linux/page-flags.h | 22 ++++++++++++++++++++++</span>
<span class="quote">&gt;  3 files changed, 25 insertions(+), 3 deletions(-)</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; index 58dbd5c..d6d4191 100644</span>
<span class="quote">&gt; --- a/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/arm/kvm/mmu.c</span>
<span class="quote">&gt; @@ -1004,7 +1004,7 @@ static bool transparent_hugepage_adjust(kvm_pfn_t *pfnp, phys_addr_t *ipap)</span>
<span class="quote">&gt;  	kvm_pfn_t pfn = *pfnp;</span>
<span class="quote">&gt;  	gfn_t gfn = *ipap &gt;&gt; PAGE_SHIFT;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; -	if (PageTransCompound(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; +	if (PageTransCompoundMap(pfn_to_page(pfn))) {</span>
<span class="quote">&gt;  		unsigned long mask;</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt;  		 * The address we faulted on is backed by a transparent huge</span>
<span class="quote">&gt; diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c</span>
<span class="quote">&gt; index 1ff4dbb..b6f50e8 100644</span>
<span class="quote">&gt; --- a/arch/x86/kvm/mmu.c</span>
<span class="quote">&gt; +++ b/arch/x86/kvm/mmu.c</span>
<span class="quote">&gt; @@ -2823,7 +2823,7 @@ static void transparent_hugepage_adjust(struct kvm_vcpu *vcpu,</span>
<span class="quote">&gt;  	 */</span>
<span class="quote">&gt;  	if (!is_error_noslot_pfn(pfn) &amp;&amp; !kvm_is_reserved_pfn(pfn) &amp;&amp;</span>
<span class="quote">&gt;  	    level == PT_PAGE_TABLE_LEVEL &amp;&amp;</span>
<span class="quote">&gt; -	    PageTransCompound(pfn_to_page(pfn)) &amp;&amp;</span>
<span class="quote">&gt; +	    PageTransCompoundMap(pfn_to_page(pfn)) &amp;&amp;</span>
<span class="quote">&gt;  	    !mmu_gfn_lpage_is_disallowed(vcpu, gfn, PT_DIRECTORY_LEVEL)) {</span>
<span class="quote">&gt;  		unsigned long mask;</span>
<span class="quote">&gt;  		/*</span>
<span class="quote">&gt; @@ -4785,7 +4785,7 @@ restart:</span>
<span class="quote">&gt;  		 */</span>
<span class="quote">&gt;  		if (sp-&gt;role.direct &amp;&amp;</span>
<span class="quote">&gt;  			!kvm_is_reserved_pfn(pfn) &amp;&amp;</span>
<span class="quote">&gt; -			PageTransCompound(pfn_to_page(pfn))) {</span>
<span class="quote">&gt; +			PageTransCompoundMap(pfn_to_page(pfn))) {</span>
<span class="quote">&gt;  			drop_spte(kvm, sptep);</span>
<span class="quote">&gt;  			need_tlb_flush = 1;</span>
<span class="quote">&gt;  			goto restart;</span>
<span class="quote">&gt; diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h</span>
<span class="quote">&gt; index f4ed4f1b..6b052aa 100644</span>
<span class="quote">&gt; --- a/include/linux/page-flags.h</span>
<span class="quote">&gt; +++ b/include/linux/page-flags.h</span>
<span class="quote">&gt; @@ -517,6 +517,27 @@ static inline int PageTransCompound(struct page *page)</span>
<span class="quote">&gt;  }</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  /*</span>
<span class="quote">&gt; + * PageTransCompoundMap is the same as PageTransCompound, but it also</span>
<span class="quote">&gt; + * guarantees the primary MMU has the entire compound page mapped</span>
<span class="quote">&gt; + * through pmd_trans_huge, which in turn guarantees the secondary MMUs</span>
<span class="quote">&gt; + * can also map the entire compound page. This allows the secondary</span>
<span class="quote">&gt; + * MMUs to call get_user_pages() only once for each compound page and</span>
<span class="quote">&gt; + * to immediately map the entire compound page with a single secondary</span>
<span class="quote">&gt; + * MMU fault. If there will be a pmd split later, the secondary MMUs</span>
<span class="quote">&gt; + * will get an update through the MMU notifier invalidation through</span>
<span class="quote">&gt; + * split_huge_pmd().</span>
<span class="quote">&gt; + *</span>
<span class="quote">&gt; + * Unlike PageTransCompound, this is safe to be called only while</span>
<span class="quote">&gt; + * split_huge_pmd() cannot run from under us, like if protected by the</span>
<span class="quote">&gt; + * MMU notifier, otherwise it may result in page-&gt;_mapcount &lt; 0 false</span>
<span class="quote">&gt; + * positives.</span>
<span class="quote">&gt; + */</span>

I know nothing about kvm. How do you protect against pmd splitting between
get_user_pages() and the check?

And the helper looks highly kvm-specific, doesn&#39;t it?
<span class="quote">
&gt; +static inline int PageTransCompoundMap(struct page *page)</span>
<span class="quote">&gt; +{</span>
<span class="quote">&gt; +	return PageTransCompound(page) &amp;&amp; atomic_read(&amp;page-&gt;_mapcount) &lt; 0;</span>
<span class="quote">&gt; +}</span>
<span class="quote">&gt; +</span>
<span class="quote">&gt; +/*</span>
<span class="quote">&gt;   * PageTransTail returns true for both transparent huge pages</span>
<span class="quote">&gt;   * and hugetlbfs pages, so it should only be called when it&#39;s known</span>
<span class="quote">&gt;   * that hugetlbfs pages aren&#39;t involved.</span>
<span class="quote">&gt; @@ -559,6 +580,7 @@ static inline int TestClearPageDoubleMap(struct page *page)</span>
<span class="quote">&gt;  #else</span>
<span class="quote">&gt;  TESTPAGEFLAG_FALSE(TransHuge)</span>
<span class="quote">&gt;  TESTPAGEFLAG_FALSE(TransCompound)</span>
<span class="quote">&gt; +TESTPAGEFLAG_FALSE(TransCompoundMap)</span>
<span class="quote">&gt;  TESTPAGEFLAG_FALSE(TransTail)</span>
<span class="quote">&gt;  TESTPAGEFLAG_FALSE(DoubleMap)</span>
<span class="quote">&gt;  	TESTSETFLAG_FALSE(DoubleMap)</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - April 27, 2016, 2:59 p.m.</div>
<pre class="content">
On Wed, Apr 27, 2016 at 04:50:30PM +0300, Kirill A. Shutemov wrote:
<span class="quote">&gt; I know nothing about kvm. How do you protect against pmd splitting between</span>
<span class="quote">&gt; get_user_pages() and the check?</span>

get_user_pages_fast() runs fully lockless and unpins the page right
away (we need a get_user_pages_fast without the FOLL_GET in fact to
avoid a totally useless atomic_inc/dec!).

Then we take a lock that is also taken by
mmu_notifier_invalidate_range_start. This way __split_huge_pmd will
block in mmu_notifier_invalidate_range_start if it tries to run again
(every other mmu notifier like mmu_notifier_invalidate_page will also
block).

Then after we serialized against __split_huge_pmd through the MMU
notifier KVM internal locking, we are able to tell if any mmu_notifier
invalidate happened in the region just before get_user_pages_fast()
was invoked, until we call PageCompoundTransMap and we actually map
the shadow pagetable into the compound page with hugepage
granularity (to allow real 2MB TLBs if guest also uses trans_huge_pmd
in the guest pagetables).

After the shadow pagetable is mapped, we drop the internal MMU
notifier lock and __split_huge_pmd mmu_notifier_invalidate_range_start
can continue and drop the shadow pagetable that we just mapped in the
above paragraph just before dropping the mmu notifier internal lock.

To be able to tell if any invalidate happened while
get_user_pages_fast was running and until we grab the lock again and
we start mapping the shadow pagtable we use:

	mmu_seq = vcpu-&gt;kvm-&gt;mmu_notifier_seq;
	smp_rmb();

	if (try_async_pf(vcpu, prefault, gfn, v, &amp;pfn, write, &amp;map_writable))
	    ^^^^^^^^^^^^ this is get_user_pages and does put_page on the page
	    		 and just returns the &amp;pfn
	    		 this is why we need a get_user_pages_fast that won&#39;t
			 attempt to touch the page-&gt;_count at all! we can avoid
			 2 atomic ops for each secondary MMU fault that way
		return 0;

	spin_lock(&amp;vcpu-&gt;kvm-&gt;mmu_lock);
	if (mmu_notifier_retry(vcpu-&gt;kvm, mmu_seq))
		goto out_unlock;
	... here we check PageTransCompoundMap(pfn_to_page(pfn)) and
	map a 4k or 2MB shadow pagetable on &quot;pfn&quot; ...


Note mmu_notifier_retry does the other side of the smp_rmb():

	smp_rmb();
	if (kvm-&gt;mmu_notifier_seq != mmu_seq)
		return 1;
	return 0;
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - April 27, 2016, 3:18 p.m.</div>
<pre class="content">
On Wed, Apr 27, 2016 at 04:59:57PM +0200, Andrea Arcangeli wrote:
<span class="quote">&gt; On Wed, Apr 27, 2016 at 04:50:30PM +0300, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; I know nothing about kvm. How do you protect against pmd splitting between</span>
<span class="quote">&gt; &gt; get_user_pages() and the check?</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; get_user_pages_fast() runs fully lockless and unpins the page right</span>
<span class="quote">&gt; away (we need a get_user_pages_fast without the FOLL_GET in fact to</span>
<span class="quote">&gt; avoid a totally useless atomic_inc/dec!).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Then we take a lock that is also taken by</span>
<span class="quote">&gt; mmu_notifier_invalidate_range_start. This way __split_huge_pmd will</span>
<span class="quote">&gt; block in mmu_notifier_invalidate_range_start if it tries to run again</span>
<span class="quote">&gt; (every other mmu notifier like mmu_notifier_invalidate_page will also</span>
<span class="quote">&gt; block).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Then after we serialized against __split_huge_pmd through the MMU</span>
<span class="quote">&gt; notifier KVM internal locking, we are able to tell if any mmu_notifier</span>
<span class="quote">&gt; invalidate happened in the region just before get_user_pages_fast()</span>
<span class="quote">&gt; was invoked, until we call PageCompoundTransMap and we actually map</span>
<span class="quote">&gt; the shadow pagetable into the compound page with hugepage</span>
<span class="quote">&gt; granularity (to allow real 2MB TLBs if guest also uses trans_huge_pmd</span>
<span class="quote">&gt; in the guest pagetables).</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; After the shadow pagetable is mapped, we drop the internal MMU</span>
<span class="quote">&gt; notifier lock and __split_huge_pmd mmu_notifier_invalidate_range_start</span>
<span class="quote">&gt; can continue and drop the shadow pagetable that we just mapped in the</span>
<span class="quote">&gt; above paragraph just before dropping the mmu notifier internal lock.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; To be able to tell if any invalidate happened while</span>
<span class="quote">&gt; get_user_pages_fast was running and until we grab the lock again and</span>
<span class="quote">&gt; we start mapping the shadow pagtable we use:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	mmu_seq = vcpu-&gt;kvm-&gt;mmu_notifier_seq;</span>
<span class="quote">&gt; 	smp_rmb();</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	if (try_async_pf(vcpu, prefault, gfn, v, &amp;pfn, write, &amp;map_writable))</span>
<span class="quote">&gt; 	    ^^^^^^^^^^^^ this is get_user_pages and does put_page on the page</span>
<span class="quote">&gt; 	    		 and just returns the &amp;pfn</span>
<span class="quote">&gt; 	    		 this is why we need a get_user_pages_fast that won&#39;t</span>
<span class="quote">&gt; 			 attempt to touch the page-&gt;_count at all! we can avoid</span>
<span class="quote">&gt; 			 2 atomic ops for each secondary MMU fault that way</span>
<span class="quote">&gt; 		return 0;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	spin_lock(&amp;vcpu-&gt;kvm-&gt;mmu_lock);</span>
<span class="quote">&gt; 	if (mmu_notifier_retry(vcpu-&gt;kvm, mmu_seq))</span>
<span class="quote">&gt; 		goto out_unlock;</span>
<span class="quote">&gt; 	... here we check PageTransCompoundMap(pfn_to_page(pfn)) and</span>
<span class="quote">&gt; 	map a 4k or 2MB shadow pagetable on &quot;pfn&quot; ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Note mmu_notifier_retry does the other side of the smp_rmb():</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; 	smp_rmb();</span>
<span class="quote">&gt; 	if (kvm-&gt;mmu_notifier_seq != mmu_seq)</span>
<span class="quote">&gt; 		return 1;</span>
<span class="quote">&gt; 	return 0;</span>

Okay, I see.

But do we really want to make PageTransCompoundMap() visiable beyond KVM
code? It looks like too KVM-specific.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - April 27, 2016, 3:57 p.m.</div>
<pre class="content">
On Wed, Apr 27, 2016 at 06:18:34PM +0300, Kirill A. Shutemov wrote:
<span class="quote">&gt; Okay, I see.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; But do we really want to make PageTransCompoundMap() visiable beyond KVM</span>
<span class="quote">&gt; code? It looks like too KVM-specific.</span>

Any other secondary MMU notifier manager (KVM is just one of the many
MMU notifier users) will need the same information if it doesn&#39;t want
to run a flood of get_user_pages_fast and it can support multiple
granularity in the secondary MMU mappings, so I think it is justified
to be exposed not just to KVM.

The other option would be to move transparent_hugepage_adjust to
mm/huge_memory.c but that currently has all kind of KVM data
structures in it, so it&#39;s definitely not a cut-and-paste work, so I
couldn&#39;t do a fix as cleaner as this one for 4.6.
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=965">Andrea Arcangeli</a> - April 27, 2016, 4:03 p.m.</div>
<pre class="content">
On Wed, Apr 27, 2016 at 05:57:30PM +0200, Andrea Arcangeli wrote:
<span class="quote">&gt; couldn&#39;t do a fix as cleaner as this one for 4.6.</span>

ehm &quot;cleaner then&quot;

If you&#39;ve suggestions for a better name than PageTransCompoundMap I
can respin a new patch though, I considered &quot;CanMap&quot; but I opted for
the short version.

Also I&#39;m not really sure moving transparent_hugepage_adjust will make
much sense. I mentioned it because Andres in another thread said it
was suggested but the real common code knowledge is about
PageTransCompoundMap only, all sort of !mmu_gfn_lpage_is_disallowed
for dirty logging at 4k shadow granularity is KVM internal.
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/arm/kvm/mmu.c b/arch/arm/kvm/mmu.c</span>
<span class="p_header">index 58dbd5c..d6d4191 100644</span>
<span class="p_header">--- a/arch/arm/kvm/mmu.c</span>
<span class="p_header">+++ b/arch/arm/kvm/mmu.c</span>
<span class="p_chunk">@@ -1004,7 +1004,7 @@</span> <span class="p_context"> static bool transparent_hugepage_adjust(kvm_pfn_t *pfnp, phys_addr_t *ipap)</span>
 	kvm_pfn_t pfn = *pfnp;
 	gfn_t gfn = *ipap &gt;&gt; PAGE_SHIFT;
 
<span class="p_del">-	if (PageTransCompound(pfn_to_page(pfn))) {</span>
<span class="p_add">+	if (PageTransCompoundMap(pfn_to_page(pfn))) {</span>
 		unsigned long mask;
 		/*
 		 * The address we faulted on is backed by a transparent huge
<span class="p_header">diff --git a/arch/x86/kvm/mmu.c b/arch/x86/kvm/mmu.c</span>
<span class="p_header">index 1ff4dbb..b6f50e8 100644</span>
<span class="p_header">--- a/arch/x86/kvm/mmu.c</span>
<span class="p_header">+++ b/arch/x86/kvm/mmu.c</span>
<span class="p_chunk">@@ -2823,7 +2823,7 @@</span> <span class="p_context"> static void transparent_hugepage_adjust(struct kvm_vcpu *vcpu,</span>
 	 */
 	if (!is_error_noslot_pfn(pfn) &amp;&amp; !kvm_is_reserved_pfn(pfn) &amp;&amp;
 	    level == PT_PAGE_TABLE_LEVEL &amp;&amp;
<span class="p_del">-	    PageTransCompound(pfn_to_page(pfn)) &amp;&amp;</span>
<span class="p_add">+	    PageTransCompoundMap(pfn_to_page(pfn)) &amp;&amp;</span>
 	    !mmu_gfn_lpage_is_disallowed(vcpu, gfn, PT_DIRECTORY_LEVEL)) {
 		unsigned long mask;
 		/*
<span class="p_chunk">@@ -4785,7 +4785,7 @@</span> <span class="p_context"> restart:</span>
 		 */
 		if (sp-&gt;role.direct &amp;&amp;
 			!kvm_is_reserved_pfn(pfn) &amp;&amp;
<span class="p_del">-			PageTransCompound(pfn_to_page(pfn))) {</span>
<span class="p_add">+			PageTransCompoundMap(pfn_to_page(pfn))) {</span>
 			drop_spte(kvm, sptep);
 			need_tlb_flush = 1;
 			goto restart;
<span class="p_header">diff --git a/include/linux/page-flags.h b/include/linux/page-flags.h</span>
<span class="p_header">index f4ed4f1b..6b052aa 100644</span>
<span class="p_header">--- a/include/linux/page-flags.h</span>
<span class="p_header">+++ b/include/linux/page-flags.h</span>
<span class="p_chunk">@@ -517,6 +517,27 @@</span> <span class="p_context"> static inline int PageTransCompound(struct page *page)</span>
 }
 
 /*
<span class="p_add">+ * PageTransCompoundMap is the same as PageTransCompound, but it also</span>
<span class="p_add">+ * guarantees the primary MMU has the entire compound page mapped</span>
<span class="p_add">+ * through pmd_trans_huge, which in turn guarantees the secondary MMUs</span>
<span class="p_add">+ * can also map the entire compound page. This allows the secondary</span>
<span class="p_add">+ * MMUs to call get_user_pages() only once for each compound page and</span>
<span class="p_add">+ * to immediately map the entire compound page with a single secondary</span>
<span class="p_add">+ * MMU fault. If there will be a pmd split later, the secondary MMUs</span>
<span class="p_add">+ * will get an update through the MMU notifier invalidation through</span>
<span class="p_add">+ * split_huge_pmd().</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Unlike PageTransCompound, this is safe to be called only while</span>
<span class="p_add">+ * split_huge_pmd() cannot run from under us, like if protected by the</span>
<span class="p_add">+ * MMU notifier, otherwise it may result in page-&gt;_mapcount &lt; 0 false</span>
<span class="p_add">+ * positives.</span>
<span class="p_add">+ */</span>
<span class="p_add">+static inline int PageTransCompoundMap(struct page *page)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return PageTransCompound(page) &amp;&amp; atomic_read(&amp;page-&gt;_mapcount) &lt; 0;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+/*</span>
  * PageTransTail returns true for both transparent huge pages
  * and hugetlbfs pages, so it should only be called when it&#39;s known
  * that hugetlbfs pages aren&#39;t involved.
<span class="p_chunk">@@ -559,6 +580,7 @@</span> <span class="p_context"> static inline int TestClearPageDoubleMap(struct page *page)</span>
 #else
 TESTPAGEFLAG_FALSE(TransHuge)
 TESTPAGEFLAG_FALSE(TransCompound)
<span class="p_add">+TESTPAGEFLAG_FALSE(TransCompoundMap)</span>
 TESTPAGEFLAG_FALSE(TransTail)
 TESTPAGEFLAG_FALSE(DoubleMap)
 	TESTSETFLAG_FALSE(DoubleMap)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



