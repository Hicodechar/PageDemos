
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[GIT,PULL] x86/asm changes for v4.5 - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [GIT,PULL] x86/asm changes for v4.5</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 11, 2016, 2:53 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20160111145347.GA24804@gmail.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/8005191/mbox/"
   >mbox</a>
|
   <a href="/patch/8005191/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/8005191/">/patch/8005191/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
X-Original-To: patchwork-LKML@patchwork.kernel.org
Delivered-To: patchwork-parsemail@patchwork1.web.kernel.org
Received: from mail.kernel.org (mail.kernel.org [198.145.29.136])
	by patchwork1.web.kernel.org (Postfix) with ESMTP id A64169F2F4
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 11 Jan 2016 14:54:05 +0000 (UTC)
Received: from mail.kernel.org (localhost [127.0.0.1])
	by mail.kernel.org (Postfix) with ESMTP id 80F95200DF
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 11 Jan 2016 14:54:03 +0000 (UTC)
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.kernel.org (Postfix) with ESMTP id 1FDE6201FE
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Mon, 11 Jan 2016 14:54:01 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S933253AbcAKOx4 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Mon, 11 Jan 2016 09:53:56 -0500
Received: from mail-wm0-f67.google.com ([74.125.82.67]:33194 &quot;EHLO
	mail-wm0-f67.google.com&quot; rhost-flags-OK-OK-OK-OK) by vger.kernel.org
	with ESMTP id S1757612AbcAKOxx (ORCPT
	&lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Mon, 11 Jan 2016 09:53:53 -0500
Received: by mail-wm0-f67.google.com with SMTP id u188so26598478wmu.0
	for &lt;linux-kernel@vger.kernel.org&gt;;
	Mon, 11 Jan 2016 06:53:52 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=gmail.com; s=20120113;
	h=sender:date:from:to:cc:subject:message-id:mime-version:content-type
	:content-disposition:user-agent;
	bh=mqPEX+5CGdo0PfdynUk2j13Kptsom2UK/4by/7xxezI=;
	b=IUGNhciaFdy3ozsRD67TW2tEd67PJHfxsG/TOws3lO4pVvuCcwxgcEtziO72HgxADG
	TAxR1Udc21wP73m7h8hzPlndVYlgD/Rb/N5Mp6zMGk6v38dfCOvuDIs90Q3DB/57Ikuo
	XNTkRUfG8z8izHNSNkkfn/P8fIwsc3Zi+jrKxOxeQKTrmpdRW4008MvKoAnaAktZfHGo
	Zum8Bp/PcE+2HBtZglVM3FC7ma2FZYrU6ex79jtP7rNRbmEBuJc+x9daxHrfQOZSSvXX
	e0msXiXJyAqO626im+f44R5Pk0PLW3DpJi/VWRZ1M7wNPU0Fl553Ps7mVp1QGahqOn2j
	OSKQ==
X-Received: by 10.28.4.139 with SMTP id 133mr13532853wme.21.1452524031329;
	Mon, 11 Jan 2016 06:53:51 -0800 (PST)
Received: from gmail.com (54033495.catv.pool.telekom.hu. [84.3.52.149])
	by smtp.gmail.com with ESMTPSA id
	m128sm13105708wma.24.2016.01.11.06.53.49
	(version=TLS1_2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
	Mon, 11 Jan 2016 06:53:50 -0800 (PST)
Date: Mon, 11 Jan 2016 15:53:47 +0100
From: Ingo Molnar &lt;mingo@kernel.org&gt;
To: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;
Cc: linux-kernel@vger.kernel.org, Thomas Gleixner &lt;tglx@linutronix.de&gt;,
	&quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Andrew Morton &lt;akpm@linux-foundation.org&gt;,
	Peter Zijlstra &lt;a.p.zijlstra@chello.nl&gt;
Subject: [GIT PULL] x86/asm changes for v4.5
Message-ID: &lt;20160111145347.GA24804@gmail.com&gt;
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Disposition: inline
User-Agent: Mutt/1.5.23 (2014-03-12)
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Spam-Status: No, score=-6.8 required=5.0 tests=BAYES_00,DKIM_SIGNED,
	RCVD_IN_DNSWL_HI,RP_MATCHES_RCVD,T_DKIM_INVALID,UNPARSEABLE_RELAY
	autolearn=unavailable version=3.3.1
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on mail.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=35552">Ingo Molnar</a> - Jan. 11, 2016, 2:53 p.m.</div>
<pre class="content">
Linus,

Please pull the latest x86-asm-for-linus git tree from:

   git://git.kernel.org/pub/scm/linux/kernel/git/tip/tip.git x86-asm-for-linus

   # HEAD: 8705d603edd49f1cff165cd3b7998f4c7f098d27 x86/vsdo: Fix build on PARAVIRT_CLOCK=y, KVM_GUEST=n

The main changes in this cycle were:

 - vDSO and asm entry improvements (Andy Lutomirski)

 - Xen paravirt entry enhancements (Boris Ostrovsky)

 - asm entry labels enhancement (Borislav Petkov)

 - and other misc changes (Thomas Gleixner, me)

  out-of-topic modifications in x86-asm-for-linus:
  --------------------------------------------------
  include/linux/context_tracking_state.h# ed11a7f1b3bd: context_tracking: Switch to 
  kernel/context_tracking.c          # ed11a7f1b3bd: context_tracking: Switch to 

 Thanks,

	Ingo

------------------&gt;
Andy Lutomirski (10):
      context_tracking: Switch to new static_branch API
      x86/asm: Error out if asm/jump_label.h is included inappropriately
      x86/asm: Add asm macros for static keys/jump labels
      x86/entry/64: Bypass enter_from_user_mode on non-context-tracking boots
      x86/kvm: On KVM re-enable (e.g. after suspend), update clocks
      x86, vdso, pvclock: Simplify and speed up the vdso pvclock reader
      x86/vdso: Get pvclock data from the vvar VMA instead of the fixmap
      x86/vdso: Remove pvclock fixmap machinery
      x86/vdso: Enable vdso pvclock access on all vdso variants
      x86/vsdo: Fix build on PARAVIRT_CLOCK=y, KVM_GUEST=n

Boris Ostrovsky (3):
      x86/xen: Avoid fast syscall path for Xen PV guests
      x86/paravirt: Remove the unused irq_enable_sysexit pv op
      x86/entry, x86/paravirt: Remove the unused usergs_sysret32 PV op

Borislav Petkov (1):
      x86/entry/64_compat: Make labels local

Ingo Molnar (1):
      x86/platform/uv: Include clocksource.h for clocksource_touch_watchdog()

Thomas Gleixner (1):
      Revert &quot;x86/kvm: On KVM re-enable (e.g. after suspend), update clocks&quot;


 arch/x86/entry/calling.h               |  15 ++++
 arch/x86/entry/entry_32.S              |  13 ++-
 arch/x86/entry/entry_64.S              |   8 +-
 arch/x86/entry/entry_64_compat.S       |  30 +++----
 arch/x86/entry/vdso/vclock_gettime.c   | 151 ++++++++++++++++-----------------
 arch/x86/entry/vdso/vdso-layout.lds.S  |   3 +-
 arch/x86/entry/vdso/vdso2c.c           |   3 +
 arch/x86/entry/vdso/vma.c              |  14 +++
 arch/x86/include/asm/cpufeature.h      |   1 +
 arch/x86/include/asm/fixmap.h          |   5 --
 arch/x86/include/asm/jump_label.h      |  63 ++++++++++++--
 arch/x86/include/asm/paravirt.h        |  12 ---
 arch/x86/include/asm/paravirt_types.h  |  17 ----
 arch/x86/include/asm/pvclock.h         |  14 +--
 arch/x86/include/asm/vdso.h            |   1 +
 arch/x86/kernel/asm-offsets.c          |   3 -
 arch/x86/kernel/asm-offsets_64.c       |   1 -
 arch/x86/kernel/kvmclock.c             |  11 ++-
 arch/x86/kernel/paravirt.c             |  12 ---
 arch/x86/kernel/paravirt_patch_32.c    |   2 -
 arch/x86/kernel/paravirt_patch_64.c    |   3 -
 arch/x86/kernel/pvclock.c              |  24 ------
 arch/x86/platform/uv/uv_nmi.c          |   1 +
 arch/x86/xen/enlighten.c               |   7 +-
 arch/x86/xen/xen-asm_32.S              |  14 ---
 arch/x86/xen/xen-asm_64.S              |  19 -----
 arch/x86/xen/xen-ops.h                 |   3 -
 include/linux/context_tracking_state.h |   4 +-
 kernel/context_tracking.c              |   4 +-
 29 files changed, 207 insertions(+), 251 deletions(-)
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/entry/calling.h b/arch/x86/entry/calling.h</span>
<span class="p_header">index 3c71dd947c7b..e32206e09868 100644</span>
<span class="p_header">--- a/arch/x86/entry/calling.h</span>
<span class="p_header">+++ b/arch/x86/entry/calling.h</span>
<span class="p_chunk">@@ -1,3 +1,5 @@</span> <span class="p_context"></span>
<span class="p_add">+#include &lt;linux/jump_label.h&gt;</span>
<span class="p_add">+</span>
 /*
 
  x86 function call convention, 64-bit:
<span class="p_chunk">@@ -232,3 +234,16 @@</span> <span class="p_context"> For 32-bit we have the following conventions - kernel is built with</span>
 
 #endif /* CONFIG_X86_64 */
 
<span class="p_add">+/*</span>
<span class="p_add">+ * This does &#39;call enter_from_user_mode&#39; unless we can avoid it based on</span>
<span class="p_add">+ * kernel config or using the static jump infrastructure.</span>
<span class="p_add">+ */</span>
<span class="p_add">+.macro CALL_enter_from_user_mode</span>
<span class="p_add">+#ifdef CONFIG_CONTEXT_TRACKING</span>
<span class="p_add">+#ifdef HAVE_JUMP_LABEL</span>
<span class="p_add">+	STATIC_JUMP_IF_FALSE .Lafter_call_\@, context_tracking_enabled, def=0</span>
<span class="p_add">+#endif</span>
<span class="p_add">+	call enter_from_user_mode</span>
<span class="p_add">+.Lafter_call_\@:</span>
<span class="p_add">+#endif</span>
<span class="p_add">+.endm</span>
<span class="p_header">diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S</span>
<span class="p_header">index 3eb572ed3d7a..9870c972d345 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_32.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_32.S</span>
<span class="p_chunk">@@ -308,8 +308,9 @@</span> <span class="p_context"> ENTRY(entry_SYSENTER_32)</span>
 
 	movl	%esp, %eax
 	call	do_fast_syscall_32
<span class="p_del">-	testl	%eax, %eax</span>
<span class="p_del">-	jz	.Lsyscall_32_done</span>
<span class="p_add">+	/* XEN PV guests always use IRET path */</span>
<span class="p_add">+	ALTERNATIVE &quot;testl %eax, %eax; jz .Lsyscall_32_done&quot;, \</span>
<span class="p_add">+		    &quot;jmp .Lsyscall_32_done&quot;, X86_FEATURE_XENPV</span>
 
 /* Opportunistic SYSEXIT */
 	TRACE_IRQS_ON			/* User mode traces as IRQs on. */
<span class="p_chunk">@@ -328,7 +329,8 @@</span> <span class="p_context"> ENTRY(entry_SYSENTER_32)</span>
 	 * Return back to the vDSO, which will pop ecx and edx.
 	 * Don&#39;t bother with DS and ES (they already contain __USER_DS).
 	 */
<span class="p_del">-	ENABLE_INTERRUPTS_SYSEXIT</span>
<span class="p_add">+	sti</span>
<span class="p_add">+	sysexit</span>
 
 .pushsection .fixup, &quot;ax&quot;
 2:	movl	$0, PT_FS(%esp)
<span class="p_chunk">@@ -551,11 +553,6 @@</span> <span class="p_context"> ENTRY(native_iret)</span>
 	iret
 	_ASM_EXTABLE(native_iret, iret_exc)
 END(native_iret)
<span class="p_del">-</span>
<span class="p_del">-ENTRY(native_irq_enable_sysexit)</span>
<span class="p_del">-	sti</span>
<span class="p_del">-	sysexit</span>
<span class="p_del">-END(native_irq_enable_sysexit)</span>
 #endif
 
 ENTRY(overflow)
<span class="p_header">diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S</span>
<span class="p_header">index a55697d19824..9d34d3cfceb6 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64.S</span>
<span class="p_chunk">@@ -520,9 +520,7 @@</span> <span class="p_context"> END(irq_entries_start)</span>
 	 */
 	TRACE_IRQS_OFF
 
<span class="p_del">-#ifdef CONFIG_CONTEXT_TRACKING</span>
<span class="p_del">-	call enter_from_user_mode</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	CALL_enter_from_user_mode</span>
 
 1:
 	/*
<span class="p_chunk">@@ -1066,9 +1064,7 @@</span> <span class="p_context"> ENTRY(error_entry)</span>
 	 * (which can take locks).
 	 */
 	TRACE_IRQS_OFF
<span class="p_del">-#ifdef CONFIG_CONTEXT_TRACKING</span>
<span class="p_del">-	call enter_from_user_mode</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	CALL_enter_from_user_mode</span>
 	ret
 
 .Lerror_entry_done:
<span class="p_header">diff --git a/arch/x86/entry/entry_64_compat.S b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">index c3201830a85e..8d802a109fac 100644</span>
<span class="p_header">--- a/arch/x86/entry/entry_64_compat.S</span>
<span class="p_header">+++ b/arch/x86/entry/entry_64_compat.S</span>
<span class="p_chunk">@@ -18,13 +18,6 @@</span> <span class="p_context"></span>
 
 	.section .entry.text, &quot;ax&quot;
 
<span class="p_del">-#ifdef CONFIG_PARAVIRT</span>
<span class="p_del">-ENTRY(native_usergs_sysret32)</span>
<span class="p_del">-	swapgs</span>
<span class="p_del">-	sysretl</span>
<span class="p_del">-ENDPROC(native_usergs_sysret32)</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 /*
  * 32-bit SYSENTER instruction entry.
  *
<span class="p_chunk">@@ -103,15 +96,15 @@</span> <span class="p_context"> ENTRY(entry_SYSENTER_compat)</span>
 	 * This needs to happen before enabling interrupts so that
 	 * we don&#39;t get preempted with NT set.
 	 *
<span class="p_del">-	 * NB.: sysenter_fix_flags is a label with the code under it moved</span>
<span class="p_add">+	 * NB.: .Lsysenter_fix_flags is a label with the code under it moved</span>
 	 * out-of-line as an optimization: NT is unlikely to be set in the
 	 * majority of the cases and instead of polluting the I$ unnecessarily,
 	 * we&#39;re keeping that code behind a branch which will predict as
 	 * not-taken and therefore its instructions won&#39;t be fetched.
 	 */
 	testl	$X86_EFLAGS_NT, EFLAGS(%rsp)
<span class="p_del">-	jnz	sysenter_fix_flags</span>
<span class="p_del">-sysenter_flags_fixed:</span>
<span class="p_add">+	jnz	.Lsysenter_fix_flags</span>
<span class="p_add">+.Lsysenter_flags_fixed:</span>
 
 	/*
 	 * User mode is traced as though IRQs are on, and SYSENTER
<span class="p_chunk">@@ -121,14 +114,15 @@</span> <span class="p_context"> ENTRY(entry_SYSENTER_compat)</span>
 
 	movq	%rsp, %rdi
 	call	do_fast_syscall_32
<span class="p_del">-	testl	%eax, %eax</span>
<span class="p_del">-	jz	.Lsyscall_32_done</span>
<span class="p_add">+	/* XEN PV guests always use IRET path */</span>
<span class="p_add">+	ALTERNATIVE &quot;testl %eax, %eax; jz .Lsyscall_32_done&quot;, \</span>
<span class="p_add">+		    &quot;jmp .Lsyscall_32_done&quot;, X86_FEATURE_XENPV</span>
 	jmp	sysret32_from_system_call
 
<span class="p_del">-sysenter_fix_flags:</span>
<span class="p_add">+.Lsysenter_fix_flags:</span>
 	pushq	$X86_EFLAGS_FIXED
 	popfq
<span class="p_del">-	jmp	sysenter_flags_fixed</span>
<span class="p_add">+	jmp	.Lsysenter_flags_fixed</span>
 ENDPROC(entry_SYSENTER_compat)
 
 /*
<span class="p_chunk">@@ -200,8 +194,9 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_compat)</span>
 
 	movq	%rsp, %rdi
 	call	do_fast_syscall_32
<span class="p_del">-	testl	%eax, %eax</span>
<span class="p_del">-	jz	.Lsyscall_32_done</span>
<span class="p_add">+	/* XEN PV guests always use IRET path */</span>
<span class="p_add">+	ALTERNATIVE &quot;testl %eax, %eax; jz .Lsyscall_32_done&quot;, \</span>
<span class="p_add">+		    &quot;jmp .Lsyscall_32_done&quot;, X86_FEATURE_XENPV</span>
 
 	/* Opportunistic SYSRET */
 sysret32_from_system_call:
<span class="p_chunk">@@ -236,7 +231,8 @@</span> <span class="p_context"> ENTRY(entry_SYSCALL_compat)</span>
 	xorq	%r9, %r9
 	xorq	%r10, %r10
 	movq	RSP-ORIG_RAX(%rsp), %rsp
<span class="p_del">-        USERGS_SYSRET32</span>
<span class="p_add">+	swapgs</span>
<span class="p_add">+	sysretl</span>
 END(entry_SYSCALL_compat)
 
 /*
<span class="p_header">diff --git a/arch/x86/entry/vdso/vclock_gettime.c b/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_header">index ca94fa649251..8602f06c759f 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vclock_gettime.c</span>
<span class="p_chunk">@@ -17,8 +17,10 @@</span> <span class="p_context"></span>
 #include &lt;asm/vvar.h&gt;
 #include &lt;asm/unistd.h&gt;
 #include &lt;asm/msr.h&gt;
<span class="p_add">+#include &lt;asm/pvclock.h&gt;</span>
 #include &lt;linux/math64.h&gt;
 #include &lt;linux/time.h&gt;
<span class="p_add">+#include &lt;linux/kernel.h&gt;</span>
 
 #define gtod (&amp;VVAR(vsyscall_gtod_data))
 
<span class="p_chunk">@@ -36,12 +38,12 @@</span> <span class="p_context"> static notrace cycle_t vread_hpet(void)</span>
 }
 #endif
 
<span class="p_del">-#ifndef BUILD_VDSO32</span>
<span class="p_add">+#ifdef CONFIG_PARAVIRT_CLOCK</span>
<span class="p_add">+extern u8 pvclock_page</span>
<span class="p_add">+	__attribute__((visibility(&quot;hidden&quot;)));</span>
<span class="p_add">+#endif</span>
 
<span class="p_del">-#include &lt;linux/kernel.h&gt;</span>
<span class="p_del">-#include &lt;asm/vsyscall.h&gt;</span>
<span class="p_del">-#include &lt;asm/fixmap.h&gt;</span>
<span class="p_del">-#include &lt;asm/pvclock.h&gt;</span>
<span class="p_add">+#ifndef BUILD_VDSO32</span>
 
 notrace static long vdso_fallback_gettime(long clock, struct timespec *ts)
 {
<span class="p_chunk">@@ -60,75 +62,6 @@</span> <span class="p_context"> notrace static long vdso_fallback_gtod(struct timeval *tv, struct timezone *tz)</span>
 	return ret;
 }
 
<span class="p_del">-#ifdef CONFIG_PARAVIRT_CLOCK</span>
<span class="p_del">-</span>
<span class="p_del">-static notrace const struct pvclock_vsyscall_time_info *get_pvti(int cpu)</span>
<span class="p_del">-{</span>
<span class="p_del">-	const struct pvclock_vsyscall_time_info *pvti_base;</span>
<span class="p_del">-	int idx = cpu / (PAGE_SIZE/PVTI_SIZE);</span>
<span class="p_del">-	int offset = cpu % (PAGE_SIZE/PVTI_SIZE);</span>
<span class="p_del">-</span>
<span class="p_del">-	BUG_ON(PVCLOCK_FIXMAP_BEGIN + idx &gt; PVCLOCK_FIXMAP_END);</span>
<span class="p_del">-</span>
<span class="p_del">-	pvti_base = (struct pvclock_vsyscall_time_info *)</span>
<span class="p_del">-		    __fix_to_virt(PVCLOCK_FIXMAP_BEGIN+idx);</span>
<span class="p_del">-</span>
<span class="p_del">-	return &amp;pvti_base[offset];</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static notrace cycle_t vread_pvclock(int *mode)</span>
<span class="p_del">-{</span>
<span class="p_del">-	const struct pvclock_vsyscall_time_info *pvti;</span>
<span class="p_del">-	cycle_t ret;</span>
<span class="p_del">-	u64 last;</span>
<span class="p_del">-	u32 version;</span>
<span class="p_del">-	u8 flags;</span>
<span class="p_del">-	unsigned cpu, cpu1;</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Note: hypervisor must guarantee that:</span>
<span class="p_del">-	 * 1. cpu ID number maps 1:1 to per-CPU pvclock time info.</span>
<span class="p_del">-	 * 2. that per-CPU pvclock time info is updated if the</span>
<span class="p_del">-	 *    underlying CPU changes.</span>
<span class="p_del">-	 * 3. that version is increased whenever underlying CPU</span>
<span class="p_del">-	 *    changes.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	do {</span>
<span class="p_del">-		cpu = __getcpu() &amp; VGETCPU_CPU_MASK;</span>
<span class="p_del">-		/* TODO: We can put vcpu id into higher bits of pvti.version.</span>
<span class="p_del">-		 * This will save a couple of cycles by getting rid of</span>
<span class="p_del">-		 * __getcpu() calls (Gleb).</span>
<span class="p_del">-		 */</span>
<span class="p_del">-</span>
<span class="p_del">-		pvti = get_pvti(cpu);</span>
<span class="p_del">-</span>
<span class="p_del">-		version = __pvclock_read_cycles(&amp;pvti-&gt;pvti, &amp;ret, &amp;flags);</span>
<span class="p_del">-</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Test we&#39;re still on the cpu as well as the version.</span>
<span class="p_del">-		 * We could have been migrated just after the first</span>
<span class="p_del">-		 * vgetcpu but before fetching the version, so we</span>
<span class="p_del">-		 * wouldn&#39;t notice a version change.</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		cpu1 = __getcpu() &amp; VGETCPU_CPU_MASK;</span>
<span class="p_del">-	} while (unlikely(cpu != cpu1 ||</span>
<span class="p_del">-			  (pvti-&gt;pvti.version &amp; 1) ||</span>
<span class="p_del">-			  pvti-&gt;pvti.version != version));</span>
<span class="p_del">-</span>
<span class="p_del">-	if (unlikely(!(flags &amp; PVCLOCK_TSC_STABLE_BIT)))</span>
<span class="p_del">-		*mode = VCLOCK_NONE;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* refer to tsc.c read_tsc() comment for rationale */</span>
<span class="p_del">-	last = gtod-&gt;cycle_last;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (likely(ret &gt;= last))</span>
<span class="p_del">-		return ret;</span>
<span class="p_del">-</span>
<span class="p_del">-	return last;</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif</span>
 
 #else
 
<span class="p_chunk">@@ -162,15 +95,77 @@</span> <span class="p_context"> notrace static long vdso_fallback_gtod(struct timeval *tv, struct timezone *tz)</span>
 	return ret;
 }
 
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 #ifdef CONFIG_PARAVIRT_CLOCK
<span class="p_add">+static notrace const struct pvclock_vsyscall_time_info *get_pvti0(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return (const struct pvclock_vsyscall_time_info *)&amp;pvclock_page;</span>
<span class="p_add">+}</span>
 
 static notrace cycle_t vread_pvclock(int *mode)
 {
<span class="p_del">-	*mode = VCLOCK_NONE;</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif</span>
<span class="p_add">+	const struct pvclock_vcpu_time_info *pvti = &amp;get_pvti0()-&gt;pvti;</span>
<span class="p_add">+	cycle_t ret;</span>
<span class="p_add">+	u64 tsc, pvti_tsc;</span>
<span class="p_add">+	u64 last, delta, pvti_system_time;</span>
<span class="p_add">+	u32 version, pvti_tsc_to_system_mul, pvti_tsc_shift;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Note: The kernel and hypervisor must guarantee that cpu ID</span>
<span class="p_add">+	 * number maps 1:1 to per-CPU pvclock time info.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * Because the hypervisor is entirely unaware of guest userspace</span>
<span class="p_add">+	 * preemption, it cannot guarantee that per-CPU pvclock time</span>
<span class="p_add">+	 * info is updated if the underlying CPU changes or that that</span>
<span class="p_add">+	 * version is increased whenever underlying CPU changes.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * On KVM, we are guaranteed that pvti updates for any vCPU are</span>
<span class="p_add">+	 * atomic as seen by *all* vCPUs.  This is an even stronger</span>
<span class="p_add">+	 * guarantee than we get with a normal seqlock.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 * On Xen, we don&#39;t appear to have that guarantee, but Xen still</span>
<span class="p_add">+	 * supplies a valid seqlock using the version field.</span>
<span class="p_add">+</span>
<span class="p_add">+	 * We only do pvclock vdso timing at all if</span>
<span class="p_add">+	 * PVCLOCK_TSC_STABLE_BIT is set, and we interpret that bit to</span>
<span class="p_add">+	 * mean that all vCPUs have matching pvti and that the TSC is</span>
<span class="p_add">+	 * synced, so we can just look at vCPU 0&#39;s pvti.</span>
<span class="p_add">+	 */</span>
 
<span class="p_add">+	if (unlikely(!(pvti-&gt;flags &amp; PVCLOCK_TSC_STABLE_BIT))) {</span>
<span class="p_add">+		*mode = VCLOCK_NONE;</span>
<span class="p_add">+		return 0;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	do {</span>
<span class="p_add">+		version = pvti-&gt;version;</span>
<span class="p_add">+</span>
<span class="p_add">+		smp_rmb();</span>
<span class="p_add">+</span>
<span class="p_add">+		tsc = rdtsc_ordered();</span>
<span class="p_add">+		pvti_tsc_to_system_mul = pvti-&gt;tsc_to_system_mul;</span>
<span class="p_add">+		pvti_tsc_shift = pvti-&gt;tsc_shift;</span>
<span class="p_add">+		pvti_system_time = pvti-&gt;system_time;</span>
<span class="p_add">+		pvti_tsc = pvti-&gt;tsc_timestamp;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* Make sure that the version double-check is last. */</span>
<span class="p_add">+		smp_rmb();</span>
<span class="p_add">+	} while (unlikely((version &amp; 1) || version != pvti-&gt;version));</span>
<span class="p_add">+</span>
<span class="p_add">+	delta = tsc - pvti_tsc;</span>
<span class="p_add">+	ret = pvti_system_time +</span>
<span class="p_add">+		pvclock_scale_delta(delta, pvti_tsc_to_system_mul,</span>
<span class="p_add">+				    pvti_tsc_shift);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* refer to vread_tsc() comment for rationale */</span>
<span class="p_add">+	last = gtod-&gt;cycle_last;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (likely(ret &gt;= last))</span>
<span class="p_add">+		return ret;</span>
<span class="p_add">+</span>
<span class="p_add">+	return last;</span>
<span class="p_add">+}</span>
 #endif
 
 notrace static cycle_t vread_tsc(void)
<span class="p_header">diff --git a/arch/x86/entry/vdso/vdso-layout.lds.S b/arch/x86/entry/vdso/vdso-layout.lds.S</span>
<span class="p_header">index de2c921025f5..4158acc17df0 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vdso-layout.lds.S</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vdso-layout.lds.S</span>
<span class="p_chunk">@@ -25,7 +25,7 @@</span> <span class="p_context"> SECTIONS</span>
 	 * segment.
 	 */
 
<span class="p_del">-	vvar_start = . - 2 * PAGE_SIZE;</span>
<span class="p_add">+	vvar_start = . - 3 * PAGE_SIZE;</span>
 	vvar_page = vvar_start;
 
 	/* Place all vvars at the offsets in asm/vvar.h. */
<span class="p_chunk">@@ -36,6 +36,7 @@</span> <span class="p_context"> SECTIONS</span>
 #undef EMIT_VVAR
 
 	hpet_page = vvar_start + PAGE_SIZE;
<span class="p_add">+	pvclock_page = vvar_start + 2 * PAGE_SIZE;</span>
 
 	. = SIZEOF_HEADERS;
 
<span class="p_header">diff --git a/arch/x86/entry/vdso/vdso2c.c b/arch/x86/entry/vdso/vdso2c.c</span>
<span class="p_header">index 785d9922b106..491020b2826d 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vdso2c.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vdso2c.c</span>
<span class="p_chunk">@@ -73,6 +73,7 @@</span> <span class="p_context"> enum {</span>
 	sym_vvar_start,
 	sym_vvar_page,
 	sym_hpet_page,
<span class="p_add">+	sym_pvclock_page,</span>
 	sym_VDSO_FAKE_SECTION_TABLE_START,
 	sym_VDSO_FAKE_SECTION_TABLE_END,
 };
<span class="p_chunk">@@ -80,6 +81,7 @@</span> <span class="p_context"> enum {</span>
 const int special_pages[] = {
 	sym_vvar_page,
 	sym_hpet_page,
<span class="p_add">+	sym_pvclock_page,</span>
 };
 
 struct vdso_sym {
<span class="p_chunk">@@ -91,6 +93,7 @@</span> <span class="p_context"> struct vdso_sym required_syms[] = {</span>
 	[sym_vvar_start] = {&quot;vvar_start&quot;, true},
 	[sym_vvar_page] = {&quot;vvar_page&quot;, true},
 	[sym_hpet_page] = {&quot;hpet_page&quot;, true},
<span class="p_add">+	[sym_pvclock_page] = {&quot;pvclock_page&quot;, true},</span>
 	[sym_VDSO_FAKE_SECTION_TABLE_START] = {
 		&quot;VDSO_FAKE_SECTION_TABLE_START&quot;, false
 	},
<span class="p_header">diff --git a/arch/x86/entry/vdso/vma.c b/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">index 64df47148160..b8f69e264ac4 100644</span>
<span class="p_header">--- a/arch/x86/entry/vdso/vma.c</span>
<span class="p_header">+++ b/arch/x86/entry/vdso/vma.c</span>
<span class="p_chunk">@@ -12,6 +12,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/random.h&gt;
 #include &lt;linux/elf.h&gt;
 #include &lt;linux/cpu.h&gt;
<span class="p_add">+#include &lt;asm/pvclock.h&gt;</span>
 #include &lt;asm/vgtod.h&gt;
 #include &lt;asm/proto.h&gt;
 #include &lt;asm/vdso.h&gt;
<span class="p_chunk">@@ -100,6 +101,7 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, bool calculate_addr)</span>
 		.name = &quot;[vvar]&quot;,
 		.pages = no_pages,
 	};
<span class="p_add">+	struct pvclock_vsyscall_time_info *pvti;</span>
 
 	if (calculate_addr) {
 		addr = vdso_addr(current-&gt;mm-&gt;start_stack,
<span class="p_chunk">@@ -169,6 +171,18 @@</span> <span class="p_context"> static int map_vdso(const struct vdso_image *image, bool calculate_addr)</span>
 	}
 #endif
 
<span class="p_add">+	pvti = pvclock_pvti_cpu0_va();</span>
<span class="p_add">+	if (pvti &amp;&amp; image-&gt;sym_pvclock_page) {</span>
<span class="p_add">+		ret = remap_pfn_range(vma,</span>
<span class="p_add">+				      text_start + image-&gt;sym_pvclock_page,</span>
<span class="p_add">+				      __pa(pvti) &gt;&gt; PAGE_SHIFT,</span>
<span class="p_add">+				      PAGE_SIZE,</span>
<span class="p_add">+				      PAGE_READONLY);</span>
<span class="p_add">+</span>
<span class="p_add">+		if (ret)</span>
<span class="p_add">+			goto up_fail;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
 up_fail:
 	if (ret)
 		current-&gt;mm-&gt;context.vdso = NULL;
<span class="p_header">diff --git a/arch/x86/include/asm/cpufeature.h b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">index e4f8010f22e0..f7ba9fbf12ee 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/cpufeature.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/cpufeature.h</span>
<span class="p_chunk">@@ -216,6 +216,7 @@</span> <span class="p_context"></span>
 #define X86_FEATURE_PAUSEFILTER ( 8*32+13) /* AMD filtered pause intercept */
 #define X86_FEATURE_PFTHRESHOLD ( 8*32+14) /* AMD pause filter threshold */
 #define X86_FEATURE_VMMCALL     ( 8*32+15) /* Prefer vmmcall to vmcall */
<span class="p_add">+#define X86_FEATURE_XENPV       ( 8*32+16) /* &quot;&quot; Xen paravirtual guest */</span>
 
 
 /* Intel-defined CPU features, CPUID level 0x00000007:0 (ebx), word 9 */
<span class="p_header">diff --git a/arch/x86/include/asm/fixmap.h b/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">index f80d70009ff8..6d7d0e52ed5a 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/fixmap.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/fixmap.h</span>
<span class="p_chunk">@@ -19,7 +19,6 @@</span> <span class="p_context"></span>
 #include &lt;asm/acpi.h&gt;
 #include &lt;asm/apicdef.h&gt;
 #include &lt;asm/page.h&gt;
<span class="p_del">-#include &lt;asm/pvclock.h&gt;</span>
 #ifdef CONFIG_X86_32
 #include &lt;linux/threads.h&gt;
 #include &lt;asm/kmap_types.h&gt;
<span class="p_chunk">@@ -72,10 +71,6 @@</span> <span class="p_context"> enum fixed_addresses {</span>
 #ifdef CONFIG_X86_VSYSCALL_EMULATION
 	VSYSCALL_PAGE = (FIXADDR_TOP - VSYSCALL_ADDR) &gt;&gt; PAGE_SHIFT,
 #endif
<span class="p_del">-#ifdef CONFIG_PARAVIRT_CLOCK</span>
<span class="p_del">-	PVCLOCK_FIXMAP_BEGIN,</span>
<span class="p_del">-	PVCLOCK_FIXMAP_END = PVCLOCK_FIXMAP_BEGIN+PVCLOCK_VSYSCALL_NR_PAGES-1,</span>
<span class="p_del">-#endif</span>
 #endif
 	FIX_DBGP_BASE,
 	FIX_EARLYCON_MEM_BASE,
<span class="p_header">diff --git a/arch/x86/include/asm/jump_label.h b/arch/x86/include/asm/jump_label.h</span>
<span class="p_header">index 5daeca3d0f9e..adc54c12cbd1 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/jump_label.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/jump_label.h</span>
<span class="p_chunk">@@ -1,12 +1,18 @@</span> <span class="p_context"></span>
 #ifndef _ASM_X86_JUMP_LABEL_H
 #define _ASM_X86_JUMP_LABEL_H
 
<span class="p_del">-#ifndef __ASSEMBLY__</span>
<span class="p_del">-</span>
<span class="p_del">-#include &lt;linux/stringify.h&gt;</span>
<span class="p_del">-#include &lt;linux/types.h&gt;</span>
<span class="p_del">-#include &lt;asm/nops.h&gt;</span>
<span class="p_del">-#include &lt;asm/asm.h&gt;</span>
<span class="p_add">+#ifndef HAVE_JUMP_LABEL</span>
<span class="p_add">+/*</span>
<span class="p_add">+ * For better or for worse, if jump labels (the gcc extension) are missing,</span>
<span class="p_add">+ * then the entire static branch patching infrastructure is compiled out.</span>
<span class="p_add">+ * If that happens, the code in here will malfunction.  Raise a compiler</span>
<span class="p_add">+ * error instead.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * In theory, jump labels and the static branch patching infrastructure</span>
<span class="p_add">+ * could be decoupled to fix this.</span>
<span class="p_add">+ */</span>
<span class="p_add">+#error asm/jump_label.h included on a non-jump-label kernel</span>
<span class="p_add">+#endif</span>
 
 #define JUMP_LABEL_NOP_SIZE 5
 
<span class="p_chunk">@@ -16,6 +22,14 @@</span> <span class="p_context"></span>
 # define STATIC_KEY_INIT_NOP GENERIC_NOP5_ATOMIC
 #endif
 
<span class="p_add">+#include &lt;asm/asm.h&gt;</span>
<span class="p_add">+#include &lt;asm/nops.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#ifndef __ASSEMBLY__</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/stringify.h&gt;</span>
<span class="p_add">+#include &lt;linux/types.h&gt;</span>
<span class="p_add">+</span>
 static __always_inline bool arch_static_branch(struct static_key *key, bool branch)
 {
 	asm_volatile_goto(&quot;1:&quot;
<span class="p_chunk">@@ -59,5 +73,40 @@</span> <span class="p_context"> struct jump_entry {</span>
 	jump_label_t key;
 };
 
<span class="p_del">-#endif  /* __ASSEMBLY__ */</span>
<span class="p_add">+#else	/* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
<span class="p_add">+.macro STATIC_JUMP_IF_TRUE target, key, def</span>
<span class="p_add">+.Lstatic_jump_\@:</span>
<span class="p_add">+	.if \def</span>
<span class="p_add">+	/* Equivalent to &quot;jmp.d32 \target&quot; */</span>
<span class="p_add">+	.byte		0xe9</span>
<span class="p_add">+	.long		\target - .Lstatic_jump_after_\@</span>
<span class="p_add">+.Lstatic_jump_after_\@:</span>
<span class="p_add">+	.else</span>
<span class="p_add">+	.byte		STATIC_KEY_INIT_NOP</span>
<span class="p_add">+	.endif</span>
<span class="p_add">+	.pushsection __jump_table, &quot;aw&quot;</span>
<span class="p_add">+	_ASM_ALIGN</span>
<span class="p_add">+	_ASM_PTR	.Lstatic_jump_\@, \target, \key</span>
<span class="p_add">+	.popsection</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+.macro STATIC_JUMP_IF_FALSE target, key, def</span>
<span class="p_add">+.Lstatic_jump_\@:</span>
<span class="p_add">+	.if \def</span>
<span class="p_add">+	.byte		STATIC_KEY_INIT_NOP</span>
<span class="p_add">+	.else</span>
<span class="p_add">+	/* Equivalent to &quot;jmp.d32 \target&quot; */</span>
<span class="p_add">+	.byte		0xe9</span>
<span class="p_add">+	.long		\target - .Lstatic_jump_after_\@</span>
<span class="p_add">+.Lstatic_jump_after_\@:</span>
<span class="p_add">+	.endif</span>
<span class="p_add">+	.pushsection __jump_table, &quot;aw&quot;</span>
<span class="p_add">+	_ASM_ALIGN</span>
<span class="p_add">+	_ASM_PTR	.Lstatic_jump_\@, \target, \key + 1</span>
<span class="p_add">+	.popsection</span>
<span class="p_add">+.endm</span>
<span class="p_add">+</span>
<span class="p_add">+#endif	/* __ASSEMBLY__ */</span>
<span class="p_add">+</span>
 #endif
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt.h b/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">index 10d0596433f8..1b71c3aeae86 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt.h</span>
<span class="p_chunk">@@ -922,23 +922,11 @@</span> <span class="p_context"> extern void default_banner(void);</span>
 		  call PARA_INDIRECT(pv_irq_ops+PV_IRQ_irq_enable);	\
 		  PV_RESTORE_REGS(clobbers | CLBR_CALLEE_SAVE);)
 
<span class="p_del">-#define USERGS_SYSRET32							\</span>
<span class="p_del">-	PARA_SITE(PARA_PATCH(pv_cpu_ops, PV_CPU_usergs_sysret32),	\</span>
<span class="p_del">-		  CLBR_NONE,						\</span>
<span class="p_del">-		  jmp PARA_INDIRECT(pv_cpu_ops+PV_CPU_usergs_sysret32))</span>
<span class="p_del">-</span>
 #ifdef CONFIG_X86_32
 #define GET_CR0_INTO_EAX				\
 	push %ecx; push %edx;				\
 	call PARA_INDIRECT(pv_cpu_ops+PV_CPU_read_cr0);	\
 	pop %edx; pop %ecx
<span class="p_del">-</span>
<span class="p_del">-#define ENABLE_INTERRUPTS_SYSEXIT					\</span>
<span class="p_del">-	PARA_SITE(PARA_PATCH(pv_cpu_ops, PV_CPU_irq_enable_sysexit),	\</span>
<span class="p_del">-		  CLBR_NONE,						\</span>
<span class="p_del">-		  jmp PARA_INDIRECT(pv_cpu_ops+PV_CPU_irq_enable_sysexit))</span>
<span class="p_del">-</span>
<span class="p_del">-</span>
 #else	/* !CONFIG_X86_32 */
 
 /*
<span class="p_header">diff --git a/arch/x86/include/asm/paravirt_types.h b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">index 31247b5bff7c..702c8bdf7b66 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/paravirt_types.h</span>
<span class="p_chunk">@@ -157,15 +157,6 @@</span> <span class="p_context"> struct pv_cpu_ops {</span>
 
 	u64 (*read_pmc)(int counter);
 
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Atomically enable interrupts and return to userspace.  This</span>
<span class="p_del">-	 * is only used in 32-bit kernels.  64-bit kernels use</span>
<span class="p_del">-	 * usergs_sysret32 instead.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	void (*irq_enable_sysexit)(void);</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
 	/*
 	 * Switch to usermode gs and return to 64-bit usermode using
 	 * sysret.  Only used in 64-bit kernels to return to 64-bit
<span class="p_chunk">@@ -174,14 +165,6 @@</span> <span class="p_context"> struct pv_cpu_ops {</span>
 	 */
 	void (*usergs_sysret64)(void);
 
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Switch to usermode gs and return to 32-bit usermode using</span>
<span class="p_del">-	 * sysret.  Used to return to 32-on-64 compat processes.</span>
<span class="p_del">-	 * Other usermode register state, including %esp, must already</span>
<span class="p_del">-	 * be restored.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	void (*usergs_sysret32)(void);</span>
<span class="p_del">-</span>
 	/* Normal iret.  Jump to this with the standard iret stack
 	   frame set up. */
 	void (*iret)(void);
<span class="p_header">diff --git a/arch/x86/include/asm/pvclock.h b/arch/x86/include/asm/pvclock.h</span>
<span class="p_header">index 7a6bed5c08bc..fdcc04020636 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/pvclock.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/pvclock.h</span>
<span class="p_chunk">@@ -4,6 +4,15 @@</span> <span class="p_context"></span>
 #include &lt;linux/clocksource.h&gt;
 #include &lt;asm/pvclock-abi.h&gt;
 
<span class="p_add">+#ifdef CONFIG_KVM_GUEST</span>
<span class="p_add">+extern struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void);</span>
<span class="p_add">+#else</span>
<span class="p_add">+static inline struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return NULL;</span>
<span class="p_add">+}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
 /* some helper functions for xen and kvm pv clock sources */
 cycle_t pvclock_clocksource_read(struct pvclock_vcpu_time_info *src);
 u8 pvclock_read_flags(struct pvclock_vcpu_time_info *src);
<span class="p_chunk">@@ -91,10 +100,5 @@</span> <span class="p_context"> struct pvclock_vsyscall_time_info {</span>
 } __attribute__((__aligned__(SMP_CACHE_BYTES)));
 
 #define PVTI_SIZE sizeof(struct pvclock_vsyscall_time_info)
<span class="p_del">-#define PVCLOCK_VSYSCALL_NR_PAGES (((NR_CPUS-1)/(PAGE_SIZE/PVTI_SIZE))+1)</span>
<span class="p_del">-</span>
<span class="p_del">-int __init pvclock_init_vsyscall(struct pvclock_vsyscall_time_info *i,</span>
<span class="p_del">-				 int size);</span>
<span class="p_del">-struct pvclock_vcpu_time_info *pvclock_get_vsyscall_time_info(int cpu);</span>
 
 #endif /* _ASM_X86_PVCLOCK_H */
<span class="p_header">diff --git a/arch/x86/include/asm/vdso.h b/arch/x86/include/asm/vdso.h</span>
<span class="p_header">index 756de9190aec..deabaf9759b6 100644</span>
<span class="p_header">--- a/arch/x86/include/asm/vdso.h</span>
<span class="p_header">+++ b/arch/x86/include/asm/vdso.h</span>
<span class="p_chunk">@@ -22,6 +22,7 @@</span> <span class="p_context"> struct vdso_image {</span>
 
 	long sym_vvar_page;
 	long sym_hpet_page;
<span class="p_add">+	long sym_pvclock_page;</span>
 	long sym_VDSO32_NOTE_MASK;
 	long sym___kernel_sigreturn;
 	long sym___kernel_rt_sigreturn;
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets.c b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">index 439df975bc7a..84a7524b202c 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets.c</span>
<span class="p_chunk">@@ -65,9 +65,6 @@</span> <span class="p_context"> void common(void) {</span>
 	OFFSET(PV_IRQ_irq_disable, pv_irq_ops, irq_disable);
 	OFFSET(PV_IRQ_irq_enable, pv_irq_ops, irq_enable);
 	OFFSET(PV_CPU_iret, pv_cpu_ops, iret);
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-	OFFSET(PV_CPU_irq_enable_sysexit, pv_cpu_ops, irq_enable_sysexit);</span>
<span class="p_del">-#endif</span>
 	OFFSET(PV_CPU_read_cr0, pv_cpu_ops, read_cr0);
 	OFFSET(PV_MMU_read_cr2, pv_mmu_ops, read_cr2);
 #endif
<span class="p_header">diff --git a/arch/x86/kernel/asm-offsets_64.c b/arch/x86/kernel/asm-offsets_64.c</span>
<span class="p_header">index d8f42f902a0f..f2edafb5f24e 100644</span>
<span class="p_header">--- a/arch/x86/kernel/asm-offsets_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/asm-offsets_64.c</span>
<span class="p_chunk">@@ -23,7 +23,6 @@</span> <span class="p_context"> int main(void)</span>
 {
 #ifdef CONFIG_PARAVIRT
 	OFFSET(PV_IRQ_adjust_exception_frame, pv_irq_ops, adjust_exception_frame);
<span class="p_del">-	OFFSET(PV_CPU_usergs_sysret32, pv_cpu_ops, usergs_sysret32);</span>
 	OFFSET(PV_CPU_usergs_sysret64, pv_cpu_ops, usergs_sysret64);
 	OFFSET(PV_CPU_swapgs, pv_cpu_ops, swapgs);
 	BLANK();
<span class="p_header">diff --git a/arch/x86/kernel/kvmclock.c b/arch/x86/kernel/kvmclock.c</span>
<span class="p_header">index 2bd81e302427..72cef58693c7 100644</span>
<span class="p_header">--- a/arch/x86/kernel/kvmclock.c</span>
<span class="p_header">+++ b/arch/x86/kernel/kvmclock.c</span>
<span class="p_chunk">@@ -45,6 +45,11 @@</span> <span class="p_context"> early_param(&quot;no-kvmclock&quot;, parse_no_kvmclock);</span>
 static struct pvclock_vsyscall_time_info *hv_clock;
 static struct pvclock_wall_clock wall_clock;
 
<span class="p_add">+struct pvclock_vsyscall_time_info *pvclock_pvti_cpu0_va(void)</span>
<span class="p_add">+{</span>
<span class="p_add">+	return hv_clock;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
 /*
  * The wallclock is the time of day when we booted. Since then, some time may
  * have elapsed since the hypervisor wrote the data. So we try to account for
<span class="p_chunk">@@ -305,7 +310,6 @@</span> <span class="p_context"> int __init kvm_setup_vsyscall_timeinfo(void)</span>
 {
 #ifdef CONFIG_X86_64
 	int cpu;
<span class="p_del">-	int ret;</span>
 	u8 flags;
 	struct pvclock_vcpu_time_info *vcpu_time;
 	unsigned int size;
<span class="p_chunk">@@ -325,11 +329,6 @@</span> <span class="p_context"> int __init kvm_setup_vsyscall_timeinfo(void)</span>
 		return 1;
 	}
 
<span class="p_del">-	if ((ret = pvclock_init_vsyscall(hv_clock, size))) {</span>
<span class="p_del">-		put_cpu();</span>
<span class="p_del">-		return ret;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
 	put_cpu();
 
 	kvm_clock.archdata.vclock_mode = VCLOCK_PVCLOCK;
<span class="p_header">diff --git a/arch/x86/kernel/paravirt.c b/arch/x86/kernel/paravirt.c</span>
<span class="p_header">index c2130aef3f9d..8c19b4d5e719 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt.c</span>
<span class="p_chunk">@@ -162,10 +162,6 @@</span> <span class="p_context"> unsigned paravirt_patch_default(u8 type, u16 clobbers, void *insnbuf,</span>
 		ret = paravirt_patch_ident_64(insnbuf, len);
 
 	else if (type == PARAVIRT_PATCH(pv_cpu_ops.iret) ||
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-		 type == PARAVIRT_PATCH(pv_cpu_ops.irq_enable_sysexit) ||</span>
<span class="p_del">-#endif</span>
<span class="p_del">-		 type == PARAVIRT_PATCH(pv_cpu_ops.usergs_sysret32) ||</span>
 		 type == PARAVIRT_PATCH(pv_cpu_ops.usergs_sysret64))
 		/* If operation requires a jmp, then jmp */
 		ret = paravirt_patch_jmp(insnbuf, opfunc, addr, len);
<span class="p_chunk">@@ -220,8 +216,6 @@</span> <span class="p_context"> static u64 native_steal_clock(int cpu)</span>
 
 /* These are in entry.S */
 extern void native_iret(void);
<span class="p_del">-extern void native_irq_enable_sysexit(void);</span>
<span class="p_del">-extern void native_usergs_sysret32(void);</span>
 extern void native_usergs_sysret64(void);
 
 static struct resource reserve_ioports = {
<span class="p_chunk">@@ -379,13 +373,7 @@</span> <span class="p_context"> __visible struct pv_cpu_ops pv_cpu_ops = {</span>
 
 	.load_sp0 = native_load_sp0,
 
<span class="p_del">-#if defined(CONFIG_X86_32)</span>
<span class="p_del">-	.irq_enable_sysexit = native_irq_enable_sysexit,</span>
<span class="p_del">-#endif</span>
 #ifdef CONFIG_X86_64
<span class="p_del">-#ifdef CONFIG_IA32_EMULATION</span>
<span class="p_del">-	.usergs_sysret32 = native_usergs_sysret32,</span>
<span class="p_del">-#endif</span>
 	.usergs_sysret64 = native_usergs_sysret64,
 #endif
 	.iret = native_iret,
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_32.c b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">index c89f50a76e97..158dc0650d5d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_32.c</span>
<span class="p_chunk">@@ -5,7 +5,6 @@</span> <span class="p_context"> DEF_NATIVE(pv_irq_ops, irq_enable, &quot;sti&quot;);</span>
 DEF_NATIVE(pv_irq_ops, restore_fl, &quot;push %eax; popf&quot;);
 DEF_NATIVE(pv_irq_ops, save_fl, &quot;pushf; pop %eax&quot;);
 DEF_NATIVE(pv_cpu_ops, iret, &quot;iret&quot;);
<span class="p_del">-DEF_NATIVE(pv_cpu_ops, irq_enable_sysexit, &quot;sti; sysexit&quot;);</span>
 DEF_NATIVE(pv_mmu_ops, read_cr2, &quot;mov %cr2, %eax&quot;);
 DEF_NATIVE(pv_mmu_ops, write_cr3, &quot;mov %eax, %cr3&quot;);
 DEF_NATIVE(pv_mmu_ops, read_cr3, &quot;mov %cr3, %eax&quot;);
<span class="p_chunk">@@ -46,7 +45,6 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_irq_ops, restore_fl);
 		PATCH_SITE(pv_irq_ops, save_fl);
 		PATCH_SITE(pv_cpu_ops, iret);
<span class="p_del">-		PATCH_SITE(pv_cpu_ops, irq_enable_sysexit);</span>
 		PATCH_SITE(pv_mmu_ops, read_cr2);
 		PATCH_SITE(pv_mmu_ops, read_cr3);
 		PATCH_SITE(pv_mmu_ops, write_cr3);
<span class="p_header">diff --git a/arch/x86/kernel/paravirt_patch_64.c b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">index 8aa05583bc42..e70087a04cc8 100644</span>
<span class="p_header">--- a/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_header">+++ b/arch/x86/kernel/paravirt_patch_64.c</span>
<span class="p_chunk">@@ -13,9 +13,7 @@</span> <span class="p_context"> DEF_NATIVE(pv_mmu_ops, flush_tlb_single, &quot;invlpg (%rdi)&quot;);</span>
 DEF_NATIVE(pv_cpu_ops, clts, &quot;clts&quot;);
 DEF_NATIVE(pv_cpu_ops, wbinvd, &quot;wbinvd&quot;);
 
<span class="p_del">-DEF_NATIVE(pv_cpu_ops, irq_enable_sysexit, &quot;swapgs; sti; sysexit&quot;);</span>
 DEF_NATIVE(pv_cpu_ops, usergs_sysret64, &quot;swapgs; sysretq&quot;);
<span class="p_del">-DEF_NATIVE(pv_cpu_ops, usergs_sysret32, &quot;swapgs; sysretl&quot;);</span>
 DEF_NATIVE(pv_cpu_ops, swapgs, &quot;swapgs&quot;);
 
 DEF_NATIVE(, mov32, &quot;mov %edi, %eax&quot;);
<span class="p_chunk">@@ -55,7 +53,6 @@</span> <span class="p_context"> unsigned native_patch(u8 type, u16 clobbers, void *ibuf,</span>
 		PATCH_SITE(pv_irq_ops, save_fl);
 		PATCH_SITE(pv_irq_ops, irq_enable);
 		PATCH_SITE(pv_irq_ops, irq_disable);
<span class="p_del">-		PATCH_SITE(pv_cpu_ops, usergs_sysret32);</span>
 		PATCH_SITE(pv_cpu_ops, usergs_sysret64);
 		PATCH_SITE(pv_cpu_ops, swapgs);
 		PATCH_SITE(pv_mmu_ops, read_cr2);
<span class="p_header">diff --git a/arch/x86/kernel/pvclock.c b/arch/x86/kernel/pvclock.c</span>
<span class="p_header">index 2f355d229a58..99bfc025111d 100644</span>
<span class="p_header">--- a/arch/x86/kernel/pvclock.c</span>
<span class="p_header">+++ b/arch/x86/kernel/pvclock.c</span>
<span class="p_chunk">@@ -140,27 +140,3 @@</span> <span class="p_context"> void pvclock_read_wallclock(struct pvclock_wall_clock *wall_clock,</span>
 
 	set_normalized_timespec(ts, now.tv_sec, now.tv_nsec);
 }
<span class="p_del">-</span>
<span class="p_del">-#ifdef CONFIG_X86_64</span>
<span class="p_del">-/*</span>
<span class="p_del">- * Initialize the generic pvclock vsyscall state.  This will allocate</span>
<span class="p_del">- * a/some page(s) for the per-vcpu pvclock information, set up a</span>
<span class="p_del">- * fixmap mapping for the page(s)</span>
<span class="p_del">- */</span>
<span class="p_del">-</span>
<span class="p_del">-int __init pvclock_init_vsyscall(struct pvclock_vsyscall_time_info *i,</span>
<span class="p_del">-				 int size)</span>
<span class="p_del">-{</span>
<span class="p_del">-	int idx;</span>
<span class="p_del">-</span>
<span class="p_del">-	WARN_ON (size != PVCLOCK_VSYSCALL_NR_PAGES*PAGE_SIZE);</span>
<span class="p_del">-</span>
<span class="p_del">-	for (idx = 0; idx &lt;= (PVCLOCK_FIXMAP_END-PVCLOCK_FIXMAP_BEGIN); idx++) {</span>
<span class="p_del">-		__set_fixmap(PVCLOCK_FIXMAP_BEGIN + idx,</span>
<span class="p_del">-			     __pa(i) + (idx*PAGE_SIZE),</span>
<span class="p_del">-			     PAGE_KERNEL_VVAR);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return 0;</span>
<span class="p_del">-}</span>
<span class="p_del">-#endif</span>
<span class="p_header">diff --git a/arch/x86/platform/uv/uv_nmi.c b/arch/x86/platform/uv/uv_nmi.c</span>
<span class="p_header">index 327f21c3bde1..8dd80050d705 100644</span>
<span class="p_header">--- a/arch/x86/platform/uv/uv_nmi.c</span>
<span class="p_header">+++ b/arch/x86/platform/uv/uv_nmi.c</span>
<span class="p_chunk">@@ -28,6 +28,7 @@</span> <span class="p_context"></span>
 #include &lt;linux/nmi.h&gt;
 #include &lt;linux/sched.h&gt;
 #include &lt;linux/slab.h&gt;
<span class="p_add">+#include &lt;linux/clocksource.h&gt;</span>
 
 #include &lt;asm/apic.h&gt;
 #include &lt;asm/current.h&gt;
<span class="p_header">diff --git a/arch/x86/xen/enlighten.c b/arch/x86/xen/enlighten.c</span>
<span class="p_header">index 5774800ff583..a068e36382b7 100644</span>
<span class="p_header">--- a/arch/x86/xen/enlighten.c</span>
<span class="p_header">+++ b/arch/x86/xen/enlighten.c</span>
<span class="p_chunk">@@ -1229,10 +1229,7 @@</span> <span class="p_context"> static const struct pv_cpu_ops xen_cpu_ops __initconst = {</span>
 
 	.iret = xen_iret,
 #ifdef CONFIG_X86_64
<span class="p_del">-	.usergs_sysret32 = xen_sysret32,</span>
 	.usergs_sysret64 = xen_sysret64,
<span class="p_del">-#else</span>
<span class="p_del">-	.irq_enable_sysexit = xen_sysexit,</span>
 #endif
 
 	.load_tr_desc = paravirt_nop,
<span class="p_chunk">@@ -1886,8 +1883,10 @@</span> <span class="p_context"> EXPORT_SYMBOL_GPL(xen_hvm_need_lapic);</span>
 
 static void xen_set_cpu_features(struct cpuinfo_x86 *c)
 {
<span class="p_del">-	if (xen_pv_domain())</span>
<span class="p_add">+	if (xen_pv_domain()) {</span>
 		clear_cpu_bug(c, X86_BUG_SYSRET_SS_ATTRS);
<span class="p_add">+		set_cpu_cap(c, X86_FEATURE_XENPV);</span>
<span class="p_add">+	}</span>
 }
 
 const struct hypervisor_x86 x86_hyper_xen = {
<span class="p_header">diff --git a/arch/x86/xen/xen-asm_32.S b/arch/x86/xen/xen-asm_32.S</span>
<span class="p_header">index fd92a64d748e..feb6d40a0860 100644</span>
<span class="p_header">--- a/arch/x86/xen/xen-asm_32.S</span>
<span class="p_header">+++ b/arch/x86/xen/xen-asm_32.S</span>
<span class="p_chunk">@@ -35,20 +35,6 @@</span> <span class="p_context"></span>
 	ret
 
 /*
<span class="p_del">- * We can&#39;t use sysexit directly, because we&#39;re not running in ring0.</span>
<span class="p_del">- * But we can easily fake it up using iret.  Assuming xen_sysexit is</span>
<span class="p_del">- * jumped to with a standard stack frame, we can just strip it back to</span>
<span class="p_del">- * a standard iret frame and use iret.</span>
<span class="p_del">- */</span>
<span class="p_del">-ENTRY(xen_sysexit)</span>
<span class="p_del">-	movl PT_EAX(%esp), %eax			/* Shouldn&#39;t be necessary? */</span>
<span class="p_del">-	orl $X86_EFLAGS_IF, PT_EFLAGS(%esp)</span>
<span class="p_del">-	lea PT_EIP(%esp), %esp</span>
<span class="p_del">-</span>
<span class="p_del">-	jmp xen_iret</span>
<span class="p_del">-ENDPROC(xen_sysexit)</span>
<span class="p_del">-</span>
<span class="p_del">-/*</span>
  * This is run where a normal iret would be run, with the same stack setup:
  *	8: eflags
  *	4: cs
<span class="p_header">diff --git a/arch/x86/xen/xen-asm_64.S b/arch/x86/xen/xen-asm_64.S</span>
<span class="p_header">index f22667abf7b9..cc8acc410ddb 100644</span>
<span class="p_header">--- a/arch/x86/xen/xen-asm_64.S</span>
<span class="p_header">+++ b/arch/x86/xen/xen-asm_64.S</span>
<span class="p_chunk">@@ -68,25 +68,6 @@</span> <span class="p_context"> ENTRY(xen_sysret64)</span>
 ENDPATCH(xen_sysret64)
 RELOC(xen_sysret64, 1b+1)
 
<span class="p_del">-ENTRY(xen_sysret32)</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * We&#39;re already on the usermode stack at this point, but</span>
<span class="p_del">-	 * still with the kernel gs, so we can easily switch back</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	movq %rsp, PER_CPU_VAR(rsp_scratch)</span>
<span class="p_del">-	movq PER_CPU_VAR(cpu_current_top_of_stack), %rsp</span>
<span class="p_del">-</span>
<span class="p_del">-	pushq $__USER32_DS</span>
<span class="p_del">-	pushq PER_CPU_VAR(rsp_scratch)</span>
<span class="p_del">-	pushq %r11</span>
<span class="p_del">-	pushq $__USER32_CS</span>
<span class="p_del">-	pushq %rcx</span>
<span class="p_del">-</span>
<span class="p_del">-	pushq $0</span>
<span class="p_del">-1:	jmp hypercall_iret</span>
<span class="p_del">-ENDPATCH(xen_sysret32)</span>
<span class="p_del">-RELOC(xen_sysret32, 1b+1)</span>
<span class="p_del">-</span>
 /*
  * Xen handles syscall callbacks much like ordinary exceptions, which
  * means we have:
<span class="p_header">diff --git a/arch/x86/xen/xen-ops.h b/arch/x86/xen/xen-ops.h</span>
<span class="p_header">index 1399423f3418..4140b070f2e9 100644</span>
<span class="p_header">--- a/arch/x86/xen/xen-ops.h</span>
<span class="p_header">+++ b/arch/x86/xen/xen-ops.h</span>
<span class="p_chunk">@@ -139,9 +139,6 @@</span> <span class="p_context"> DECL_ASM(void, xen_restore_fl_direct, unsigned long);</span>
 
 /* These are not functions, and cannot be called normally */
 __visible void xen_iret(void);
<span class="p_del">-#ifdef CONFIG_X86_32</span>
<span class="p_del">-__visible void xen_sysexit(void);</span>
<span class="p_del">-#endif</span>
 __visible void xen_sysret32(void);
 __visible void xen_sysret64(void);
 __visible void xen_adjust_exception_frame(void);
<span class="p_header">diff --git a/include/linux/context_tracking_state.h b/include/linux/context_tracking_state.h</span>
<span class="p_header">index ee956c528fab..1d34fe68f48a 100644</span>
<span class="p_header">--- a/include/linux/context_tracking_state.h</span>
<span class="p_header">+++ b/include/linux/context_tracking_state.h</span>
<span class="p_chunk">@@ -22,12 +22,12 @@</span> <span class="p_context"> struct context_tracking {</span>
 };
 
 #ifdef CONFIG_CONTEXT_TRACKING
<span class="p_del">-extern struct static_key context_tracking_enabled;</span>
<span class="p_add">+extern struct static_key_false context_tracking_enabled;</span>
 DECLARE_PER_CPU(struct context_tracking, context_tracking);
 
 static inline bool context_tracking_is_enabled(void)
 {
<span class="p_del">-	return static_key_false(&amp;context_tracking_enabled);</span>
<span class="p_add">+	return static_branch_unlikely(&amp;context_tracking_enabled);</span>
 }
 
 static inline bool context_tracking_cpu_is_enabled(void)
<span class="p_header">diff --git a/kernel/context_tracking.c b/kernel/context_tracking.c</span>
<span class="p_header">index d8560ee3bab7..9ad37b9e44a7 100644</span>
<span class="p_header">--- a/kernel/context_tracking.c</span>
<span class="p_header">+++ b/kernel/context_tracking.c</span>
<span class="p_chunk">@@ -24,7 +24,7 @@</span> <span class="p_context"></span>
 #define CREATE_TRACE_POINTS
 #include &lt;trace/events/context_tracking.h&gt;
 
<span class="p_del">-struct static_key context_tracking_enabled = STATIC_KEY_INIT_FALSE;</span>
<span class="p_add">+DEFINE_STATIC_KEY_FALSE(context_tracking_enabled);</span>
 EXPORT_SYMBOL_GPL(context_tracking_enabled);
 
 DEFINE_PER_CPU(struct context_tracking, context_tracking);
<span class="p_chunk">@@ -191,7 +191,7 @@</span> <span class="p_context"> void __init context_tracking_cpu_set(int cpu)</span>
 
 	if (!per_cpu(context_tracking.active, cpu)) {
 		per_cpu(context_tracking.active, cpu) = true;
<span class="p_del">-		static_key_slow_inc(&amp;context_tracking_enabled);</span>
<span class="p_add">+		static_branch_inc(&amp;context_tracking_enabled);</span>
 	}
 
 	if (initialized)

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



