
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
 "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
 <head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
  <title>[PATCHv3,1/3] x86/mm/encrypt: Move page table helpers into separate translation unit - Patchwork</title>
  <link rel="stylesheet" type="text/css" href="/static/css/style.css"/>
  <script type="text/javascript" src="/static/js/common.js"></script>
  <script type="text/javascript" src="/static/js/jquery-1.10.1.min.js"></script>

 </head>
 <body>
  <div id="title">
  <h1 style="float: left;">
     <a
      href="/">Patchwork</a>
    [PATCHv3,1/3] x86/mm/encrypt: Move page table helpers into separate translation unit</h1>
  <div id="auth">

     <a href="/user/login/">login</a>
     <br/>
     <a href="/register/">register</a>
     <br/>
     <a href="/mail/">mail settings</a>

   </div>
   <div style="clear: both;"></div>
  </div>
  <div id="nav">
   <div id="navleft">
   
    <strong>Project</strong>: LKML
     :
     <a href="/project/LKML/list/"
      >patches</a>
     :
     <a href="/project/LKML/"
      >project info</a>
    
     :
     <a href="/"
     >other projects</a>
     
    
   </div>
   <div id="navright">
    <a href="/help/about/">about</a>
   </div>
   <div style="clear: both"></div>
  </div>

  <div id="content">

<script language="JavaScript" type="text/javascript">
function toggle_headers(link_id, headers_id)
{
    var link = document.getElementById(link_id)
    var headers = document.getElementById(headers_id)

    var hidden = headers.style['display'] == 'none';

    if (hidden) {
        link.innerHTML = 'hide';
        headers.style['display'] = 'block';
    } else {
        link.innerHTML = 'show';
        headers.style['display'] = 'none';
    }

}
</script>

<table class="patchmeta">
 <tr>
  <th>Submitter</th>
  <td><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a></td>
 </tr>
 <tr>
  <th>Date</th>
  <td>Jan. 24, 2018, 4:36 p.m.</td>
 </tr>
 <tr>
  <th>Message ID</th>
  <td>&lt;20180124163623.61765-2-kirill.shutemov@linux.intel.com&gt;</td>
 </tr>
 <tr>
  <th>Download</th>
  <td>
   <a href="/patch/10182841/mbox/"
   >mbox</a>
|
   <a href="/patch/10182841/raw/"
   >patch</a>

   </td>
 </tr>
 <tr>
  <th>Permalink</th>
  <td><a href="/patch/10182841/">/patch/10182841/</a>
 </tr>
  <tr>
   <th>State</th>
   <td>New</td>
  </tr>


 <tr>
  <th>Headers</th>
  <td><a id="togglepatchheaders"
   href="javascript:toggle_headers('togglepatchheaders', 'patchheaders')"
   >show</a>
   <div id="patchheaders" class="patchheaders" style="display:none;">
    <pre>Return-Path: &lt;linux-kernel-owner@kernel.org&gt;
Received: from mail.wl.linuxfoundation.org (pdx-wl-mail.web.codeaurora.org
	[172.30.200.125])
	by pdx-korg-patchwork.web.codeaurora.org (Postfix) with ESMTP id
	1854C602B7 for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 Jan 2018 16:38:00 +0000 (UTC)
Received: from mail.wl.linuxfoundation.org (localhost [127.0.0.1])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id 07658288CA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 Jan 2018 16:38:00 +0000 (UTC)
Received: by mail.wl.linuxfoundation.org (Postfix, from userid 486)
	id EFF1F2896A; Wed, 24 Jan 2018 16:37:59 +0000 (UTC)
X-Spam-Checker-Version: SpamAssassin 3.3.1 (2010-03-16) on
	pdx-wl-mail.web.codeaurora.org
X-Spam-Level: 
X-Spam-Status: No, score=-6.9 required=2.0 tests=BAYES_00,RCVD_IN_DNSWL_HI
	autolearn=ham version=3.3.1
Received: from vger.kernel.org (vger.kernel.org [209.132.180.67])
	by mail.wl.linuxfoundation.org (Postfix) with ESMTP id B6742288CA
	for &lt;patchwork-LKML@patchwork.kernel.org&gt;;
	Wed, 24 Jan 2018 16:37:57 +0000 (UTC)
Received: (majordomo@vger.kernel.org) by vger.kernel.org via listexpand
	id S964961AbeAXQh4 (ORCPT
	&lt;rfc822;patchwork-LKML@patchwork.kernel.org&gt;);
	Wed, 24 Jan 2018 11:37:56 -0500
Received: from mga14.intel.com ([192.55.52.115]:59475 &quot;EHLO mga14.intel.com&quot;
	rhost-flags-OK-OK-OK-OK) by vger.kernel.org with ESMTP
	id S934225AbeAXQgj (ORCPT &lt;rfc822;linux-kernel@vger.kernel.org&gt;);
	Wed, 24 Jan 2018 11:36:39 -0500
X-Amp-Result: SKIPPED(no attachment in message)
X-Amp-File-Uploaded: False
Received: from orsmga008.jf.intel.com ([10.7.209.65])
	by fmsmga103.fm.intel.com with ESMTP/TLS/DHE-RSA-AES256-GCM-SHA384;
	24 Jan 2018 08:36:35 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i=&quot;5.46,408,1511856000&quot;; d=&quot;scan&#39;208&quot;;a=&quot;12730053&quot;
Received: from black.fi.intel.com ([10.237.72.28])
	by orsmga008.jf.intel.com with ESMTP; 24 Jan 2018 08:36:32 -0800
Received: by black.fi.intel.com (Postfix, from userid 1000)
	id 6FE7B7C; Wed, 24 Jan 2018 18:36:31 +0200 (EET)
From: &quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
To: Ingo Molnar &lt;mingo@redhat.com&gt;, x86@kernel.org,
	Thomas Gleixner &lt;tglx@linutronix.de&gt;, &quot;H. Peter Anvin&quot; &lt;hpa@zytor.com&gt;,
	Tom Lendacky &lt;thomas.lendacky@amd.com&gt;
Cc: Linus Torvalds &lt;torvalds@linux-foundation.org&gt;,
	Borislav Petkov &lt;bp@suse.de&gt;, linux-mm@kvack.org,
	linux-kernel@vger.kernel.org,
	&quot;Kirill A. Shutemov&quot; &lt;kirill.shutemov@linux.intel.com&gt;
Subject: [PATCHv3 1/3] x86/mm/encrypt: Move page table helpers into separate
	translation unit
Date: Wed, 24 Jan 2018 19:36:21 +0300
Message-Id: &lt;20180124163623.61765-2-kirill.shutemov@linux.intel.com&gt;
X-Mailer: git-send-email 2.15.1
In-Reply-To: &lt;20180124163623.61765-1-kirill.shutemov@linux.intel.com&gt;
References: &lt;20180124163623.61765-1-kirill.shutemov@linux.intel.com&gt;
Sender: linux-kernel-owner@vger.kernel.org
Precedence: bulk
List-ID: &lt;linux-kernel.vger.kernel.org&gt;
X-Mailing-List: linux-kernel@vger.kernel.org
X-Virus-Scanned: ClamAV using ClamSMTP
</pre>
   </div>
  </td>
 </tr>
</table>

<div class="patchforms">





 <div style="clear: both;">
 </div>
</div>



<h2>Comments</h2>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=40781">Kirill A. Shutemov</a> - Jan. 24, 2018, 4:36 p.m.</div>
<pre class="content">
There are bunch of functions in mem_encrypt.c that operate on the
identity mapping, which means they want virtual addresses to be equal to
physical one, without PAGE_OFFSET shift.

We also need to avoid paravirtualizaion call there.

Getting this done is tricky. We cannot use usual page table helpers.
It forces us to open-code a lot of things. It makes code ugly and hard
to modify.

We can get it work with the page table helpers, but it requires few
preprocessor tricks. These tricks may have side effects for the rest of
the file.

Let&#39;s isolate such functions into own translation unit.
<span class="signed-off-by">
Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
---
 arch/x86/mm/Makefile               |  14 +-
 arch/x86/mm/mem_encrypt.c          | 578 +----------------------------------
 arch/x86/mm/mem_encrypt_identity.c | 596 +++++++++++++++++++++++++++++++++++++
 arch/x86/mm/mm_internal.h          |   1 +
 4 files changed, 607 insertions(+), 582 deletions(-)
 create mode 100644 arch/x86/mm/mem_encrypt_identity.c
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80801">Tom Lendacky</a> - Jan. 30, 2018, 10:26 p.m.</div>
<pre class="content">
On 1/24/2018 10:36 AM, Kirill A. Shutemov wrote:
<span class="quote">&gt; There are bunch of functions in mem_encrypt.c that operate on the</span>
<span class="quote">&gt; identity mapping, which means they want virtual addresses to be equal to</span>
<span class="quote">&gt; physical one, without PAGE_OFFSET shift.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We also need to avoid paravirtualizaion call there.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Getting this done is tricky. We cannot use usual page table helpers.</span>
<span class="quote">&gt; It forces us to open-code a lot of things. It makes code ugly and hard</span>
<span class="quote">&gt; to modify.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; We can get it work with the page table helpers, but it requires few</span>
<span class="quote">&gt; preprocessor tricks. These tricks may have side effects for the rest of</span>
<span class="quote">&gt; the file.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Let&#39;s isolate such functions into own translation unit.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>

Just one minor comment at the end.  With that change:
<span class="reviewed-by">
Reviewed-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="quote">
&gt; ---</span>
<span class="quote">&gt;  arch/x86/mm/Makefile               |  14 +-</span>
<span class="quote">&gt;  arch/x86/mm/mem_encrypt.c          | 578 +----------------------------------</span>
<span class="quote">&gt;  arch/x86/mm/mem_encrypt_identity.c | 596 +++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;  arch/x86/mm/mm_internal.h          |   1 +</span>
<span class="quote">&gt;  4 files changed, 607 insertions(+), 582 deletions(-)</span>
<span class="quote">&gt;  create mode 100644 arch/x86/mm/mem_encrypt_identity.c</span>
<span class="quote">&gt; </span>

...
<span class="quote">
&gt; diff --git a/arch/x86/mm/mm_internal.h b/arch/x86/mm/mm_internal.h</span>
<span class="quote">&gt; index 4e1f6e1b8159..7b4fc4386d90 100644</span>
<span class="quote">&gt; --- a/arch/x86/mm/mm_internal.h</span>
<span class="quote">&gt; +++ b/arch/x86/mm/mm_internal.h</span>
<span class="quote">&gt; @@ -19,4 +19,5 @@ extern int after_bootmem;</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt;  void update_cache_mode_entry(unsigned entry, enum page_cache_mode cache);</span>
<span class="quote">&gt;  </span>
<span class="quote">&gt; +extern bool sev_enabled __section(.data);</span>

Lets move this into arch/x86/include/asm/mem_encrypt.h and then add
#include &lt;linux/mem_encrypt.h&gt; to mem_encrypt_identity.c.

Thanks,
Tom
<span class="quote">
&gt;  #endif	/* __X86_MM_INTERNAL_H */</span>
<span class="quote">&gt;</span>
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=874">Kirill A. Shutemov</a> - Jan. 30, 2018, 10:28 p.m.</div>
<pre class="content">
On Tue, Jan 30, 2018 at 04:26:03PM -0600, Tom Lendacky wrote:
<span class="quote">&gt; On 1/24/2018 10:36 AM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt; &gt; There are bunch of functions in mem_encrypt.c that operate on the</span>
<span class="quote">&gt; &gt; identity mapping, which means they want virtual addresses to be equal to</span>
<span class="quote">&gt; &gt; physical one, without PAGE_OFFSET shift.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We also need to avoid paravirtualizaion call there.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Getting this done is tricky. We cannot use usual page table helpers.</span>
<span class="quote">&gt; &gt; It forces us to open-code a lot of things. It makes code ugly and hard</span>
<span class="quote">&gt; &gt; to modify.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; We can get it work with the page table helpers, but it requires few</span>
<span class="quote">&gt; &gt; preprocessor tricks. These tricks may have side effects for the rest of</span>
<span class="quote">&gt; &gt; the file.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Let&#39;s isolate such functions into own translation unit.</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; &gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Just one minor comment at the end.  With that change:</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Reviewed-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; ---</span>
<span class="quote">&gt; &gt;  arch/x86/mm/Makefile               |  14 +-</span>
<span class="quote">&gt; &gt;  arch/x86/mm/mem_encrypt.c          | 578 +----------------------------------</span>
<span class="quote">&gt; &gt;  arch/x86/mm/mem_encrypt_identity.c | 596 +++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt; &gt;  arch/x86/mm/mm_internal.h          |   1 +</span>
<span class="quote">&gt; &gt;  4 files changed, 607 insertions(+), 582 deletions(-)</span>
<span class="quote">&gt; &gt;  create mode 100644 arch/x86/mm/mem_encrypt_identity.c</span>
<span class="quote">&gt; &gt; </span>
<span class="quote">&gt; </span>
<span class="quote">&gt; ...</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; &gt; diff --git a/arch/x86/mm/mm_internal.h b/arch/x86/mm/mm_internal.h</span>
<span class="quote">&gt; &gt; index 4e1f6e1b8159..7b4fc4386d90 100644</span>
<span class="quote">&gt; &gt; --- a/arch/x86/mm/mm_internal.h</span>
<span class="quote">&gt; &gt; +++ b/arch/x86/mm/mm_internal.h</span>
<span class="quote">&gt; &gt; @@ -19,4 +19,5 @@ extern int after_bootmem;</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt;  void update_cache_mode_entry(unsigned entry, enum page_cache_mode cache);</span>
<span class="quote">&gt; &gt;  </span>
<span class="quote">&gt; &gt; +extern bool sev_enabled __section(.data);</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Lets move this into arch/x86/include/asm/mem_encrypt.h and then add</span>
<span class="quote">&gt; #include &lt;linux/mem_encrypt.h&gt; to mem_encrypt_identity.c.</span>

Why? Will we need it beyond arch/x86/mm/ in the future?
</pre>
</div>

<div class="comment">
<div class="meta"><a href="/project/LKML/list/?submitter=80801">Tom Lendacky</a> - Jan. 30, 2018, 10:40 p.m.</div>
<pre class="content">
On 1/30/2018 4:28 PM, Kirill A. Shutemov wrote:
<span class="quote">&gt; On Tue, Jan 30, 2018 at 04:26:03PM -0600, Tom Lendacky wrote:</span>
<span class="quote">&gt;&gt; On 1/24/2018 10:36 AM, Kirill A. Shutemov wrote:</span>
<span class="quote">&gt;&gt;&gt; There are bunch of functions in mem_encrypt.c that operate on the</span>
<span class="quote">&gt;&gt;&gt; identity mapping, which means they want virtual addresses to be equal to</span>
<span class="quote">&gt;&gt;&gt; physical one, without PAGE_OFFSET shift.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; We also need to avoid paravirtualizaion call there.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Getting this done is tricky. We cannot use usual page table helpers.</span>
<span class="quote">&gt;&gt;&gt; It forces us to open-code a lot of things. It makes code ugly and hard</span>
<span class="quote">&gt;&gt;&gt; to modify.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; We can get it work with the page table helpers, but it requires few</span>
<span class="quote">&gt;&gt;&gt; preprocessor tricks. These tricks may have side effects for the rest of</span>
<span class="quote">&gt;&gt;&gt; the file.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Let&#39;s isolate such functions into own translation unit.</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; Signed-off-by: Kirill A. Shutemov &lt;kirill.shutemov@linux.intel.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Just one minor comment at the end.  With that change:</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Reviewed-by: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; ---</span>
<span class="quote">&gt;&gt;&gt;  arch/x86/mm/Makefile               |  14 +-</span>
<span class="quote">&gt;&gt;&gt;  arch/x86/mm/mem_encrypt.c          | 578 +----------------------------------</span>
<span class="quote">&gt;&gt;&gt;  arch/x86/mm/mem_encrypt_identity.c | 596 +++++++++++++++++++++++++++++++++++++</span>
<span class="quote">&gt;&gt;&gt;  arch/x86/mm/mm_internal.h          |   1 +</span>
<span class="quote">&gt;&gt;&gt;  4 files changed, 607 insertions(+), 582 deletions(-)</span>
<span class="quote">&gt;&gt;&gt;  create mode 100644 arch/x86/mm/mem_encrypt_identity.c</span>
<span class="quote">&gt;&gt;&gt;</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; ...</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt;&gt; diff --git a/arch/x86/mm/mm_internal.h b/arch/x86/mm/mm_internal.h</span>
<span class="quote">&gt;&gt;&gt; index 4e1f6e1b8159..7b4fc4386d90 100644</span>
<span class="quote">&gt;&gt;&gt; --- a/arch/x86/mm/mm_internal.h</span>
<span class="quote">&gt;&gt;&gt; +++ b/arch/x86/mm/mm_internal.h</span>
<span class="quote">&gt;&gt;&gt; @@ -19,4 +19,5 @@ extern int after_bootmem;</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt;  void update_cache_mode_entry(unsigned entry, enum page_cache_mode cache);</span>
<span class="quote">&gt;&gt;&gt;  </span>
<span class="quote">&gt;&gt;&gt; +extern bool sev_enabled __section(.data);</span>
<span class="quote">&gt;&gt;</span>
<span class="quote">&gt;&gt; Lets move this into arch/x86/include/asm/mem_encrypt.h and then add</span>
<span class="quote">&gt;&gt; #include &lt;linux/mem_encrypt.h&gt; to mem_encrypt_identity.c.</span>
<span class="quote">&gt; </span>
<span class="quote">&gt; Why? Will we need it beyond arch/x86/mm/ in the future?</span>

I just think it would be best to keep all the memory encryption stuff
together.

Thanks,
Tom
<span class="quote">
&gt;</span>
</pre>
</div>



<h2>Patch</h2>
<div class="patch">
<pre class="content">
<span class="p_header">diff --git a/arch/x86/mm/Makefile b/arch/x86/mm/Makefile</span>
<span class="p_header">index 27e9e90a8d35..03c6c8561623 100644</span>
<span class="p_header">--- a/arch/x86/mm/Makefile</span>
<span class="p_header">+++ b/arch/x86/mm/Makefile</span>
<span class="p_chunk">@@ -1,12 +1,15 @@</span> <span class="p_context"></span>
 # SPDX-License-Identifier: GPL-2.0
<span class="p_del">-# Kernel does not boot with instrumentation of tlb.c and mem_encrypt.c</span>
<span class="p_del">-KCOV_INSTRUMENT_tlb.o		:= n</span>
<span class="p_del">-KCOV_INSTRUMENT_mem_encrypt.o	:= n</span>
<span class="p_add">+# Kernel does not boot with instrumentation of tlb.c and mem_encrypt*.c</span>
<span class="p_add">+KCOV_INSTRUMENT_tlb.o			:= n</span>
<span class="p_add">+KCOV_INSTRUMENT_mem_encrypt.o		:= n</span>
<span class="p_add">+KCOV_INSTRUMENT_mem_encrypt_identity.o	:= n</span>
 
<span class="p_del">-KASAN_SANITIZE_mem_encrypt.o	:= n</span>
<span class="p_add">+KASAN_SANITIZE_mem_encrypt.o		:= n</span>
<span class="p_add">+KASAN_SANITIZE_mem_encrypt_identity.o	:= n</span>
 
 ifdef CONFIG_FUNCTION_TRACER
<span class="p_del">-CFLAGS_REMOVE_mem_encrypt.o	= -pg</span>
<span class="p_add">+CFLAGS_REMOVE_mem_encrypt.o		= -pg</span>
<span class="p_add">+CFLAGS_REMOVE_mem_encrypt_identity.o	= -pg</span>
 endif
 
 obj-y	:=  init.o init_$(BITS).o fault.o ioremap.o extable.o pageattr.o mmap.o \
<span class="p_chunk">@@ -47,4 +50,5 @@</span> <span class="p_context"> obj-$(CONFIG_RANDOMIZE_MEMORY)			+= kaslr.o</span>
 obj-$(CONFIG_PAGE_TABLE_ISOLATION)		+= pti.o
 
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt.o
<span class="p_add">+obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_identity.o</span>
 obj-$(CONFIG_AMD_MEM_ENCRYPT)	+= mem_encrypt_boot.o
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt.c b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">index e1d61e8500f9..86f736b331ac 100644</span>
<span class="p_header">--- a/arch/x86/mm/mem_encrypt.c</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt.c</span>
<span class="p_chunk">@@ -25,17 +25,12 @@</span> <span class="p_context"></span>
 #include &lt;asm/bootparam.h&gt;
 #include &lt;asm/set_memory.h&gt;
 #include &lt;asm/cacheflush.h&gt;
<span class="p_del">-#include &lt;asm/sections.h&gt;</span>
 #include &lt;asm/processor-flags.h&gt;
 #include &lt;asm/msr.h&gt;
 #include &lt;asm/cmdline.h&gt;
 
 #include &quot;mm_internal.h&quot;
 
<span class="p_del">-static char sme_cmdline_arg[] __initdata = &quot;mem_encrypt&quot;;</span>
<span class="p_del">-static char sme_cmdline_on[]  __initdata = &quot;on&quot;;</span>
<span class="p_del">-static char sme_cmdline_off[] __initdata = &quot;off&quot;;</span>
<span class="p_del">-</span>
 /*
  * Since SME related variables are set early in the boot process they must
  * reside in the .data section so as not to be zeroed out when the .bss
<span class="p_chunk">@@ -46,7 +41,7 @@</span> <span class="p_context"> EXPORT_SYMBOL(sme_me_mask);</span>
 DEFINE_STATIC_KEY_FALSE(sev_enable_key);
 EXPORT_SYMBOL_GPL(sev_enable_key);
 
<span class="p_del">-static bool sev_enabled __section(.data);</span>
<span class="p_add">+bool sev_enabled __section(.data);</span>
 
 /* Buffer used for early in-place encryption by BSP, no locking needed */
 static char sme_early_buffer[PAGE_SIZE] __aligned(PAGE_SIZE);
<span class="p_chunk">@@ -463,574 +458,3 @@</span> <span class="p_context"> void swiotlb_set_mem_attributes(void *vaddr, unsigned long size)</span>
 	/* Make the SWIOTLB buffer area decrypted */
 	set_memory_decrypted((unsigned long)vaddr, size &gt;&gt; PAGE_SHIFT);
 }
<span class="p_del">-</span>
<span class="p_del">-struct sme_populate_pgd_data {</span>
<span class="p_del">-	void	*pgtable_area;</span>
<span class="p_del">-	pgd_t	*pgd;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmdval_t pmd_flags;</span>
<span class="p_del">-	pteval_t pte_flags;</span>
<span class="p_del">-	unsigned long paddr;</span>
<span class="p_del">-</span>
<span class="p_del">-	unsigned long vaddr;</span>
<span class="p_del">-	unsigned long vaddr_end;</span>
<span class="p_del">-};</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_clear_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long pgd_start, pgd_end, pgd_size;</span>
<span class="p_del">-	pgd_t *pgd_p;</span>
<span class="p_del">-</span>
<span class="p_del">-	pgd_start = ppd-&gt;vaddr &amp; PGDIR_MASK;</span>
<span class="p_del">-	pgd_end = ppd-&gt;vaddr_end &amp; PGDIR_MASK;</span>
<span class="p_del">-</span>
<span class="p_del">-	pgd_size = (((pgd_end - pgd_start) / PGDIR_SIZE) + 1) * sizeof(pgd_t);</span>
<span class="p_del">-</span>
<span class="p_del">-	pgd_p = ppd-&gt;pgd + pgd_index(ppd-&gt;vaddr);</span>
<span class="p_del">-</span>
<span class="p_del">-	memset(pgd_p, 0, pgd_size);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-#define PGD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define P4D_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define PUD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-#define PMD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_del">-</span>
<span class="p_del">-#define PMD_FLAGS_LARGE		(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PMD_FLAGS_DEC		PMD_FLAGS_LARGE</span>
<span class="p_del">-#define PMD_FLAGS_DEC_WP	((PMD_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_del">-				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_del">-</span>
<span class="p_del">-#define PMD_FLAGS_ENC		(PMD_FLAGS_LARGE | _PAGE_ENC)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PTE_FLAGS		(__PAGE_KERNEL_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_del">-</span>
<span class="p_del">-#define PTE_FLAGS_DEC		PTE_FLAGS</span>
<span class="p_del">-#define PTE_FLAGS_DEC_WP	((PTE_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_del">-				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_del">-</span>
<span class="p_del">-#define PTE_FLAGS_ENC		(PTE_FLAGS | _PAGE_ENC)</span>
<span class="p_del">-</span>
<span class="p_del">-static pmd_t __init *sme_prepare_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pgd_t *pgd_p;</span>
<span class="p_del">-	p4d_t *p4d_p;</span>
<span class="p_del">-	pud_t *pud_p;</span>
<span class="p_del">-	pmd_t *pmd_p;</span>
<span class="p_del">-</span>
<span class="p_del">-	pgd_p = ppd-&gt;pgd + pgd_index(ppd-&gt;vaddr);</span>
<span class="p_del">-	if (native_pgd_val(*pgd_p)) {</span>
<span class="p_del">-		if (IS_ENABLED(CONFIG_X86_5LEVEL))</span>
<span class="p_del">-			p4d_p = (p4d_t *)(native_pgd_val(*pgd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_del">-		else</span>
<span class="p_del">-			pud_p = (pud_t *)(native_pgd_val(*pgd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		pgd_t pgd;</span>
<span class="p_del">-</span>
<span class="p_del">-		if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_del">-			p4d_p = ppd-&gt;pgtable_area;</span>
<span class="p_del">-			memset(p4d_p, 0, sizeof(*p4d_p) * PTRS_PER_P4D);</span>
<span class="p_del">-			ppd-&gt;pgtable_area += sizeof(*p4d_p) * PTRS_PER_P4D;</span>
<span class="p_del">-</span>
<span class="p_del">-			pgd = native_make_pgd((pgdval_t)p4d_p + PGD_FLAGS);</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			pud_p = ppd-&gt;pgtable_area;</span>
<span class="p_del">-			memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);</span>
<span class="p_del">-			ppd-&gt;pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
<span class="p_del">-</span>
<span class="p_del">-			pgd = native_make_pgd((pgdval_t)pud_p + PGD_FLAGS);</span>
<span class="p_del">-		}</span>
<span class="p_del">-		native_set_pgd(pgd_p, pgd);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_del">-		p4d_p += p4d_index(ppd-&gt;vaddr);</span>
<span class="p_del">-		if (native_p4d_val(*p4d_p)) {</span>
<span class="p_del">-			pud_p = (pud_t *)(native_p4d_val(*p4d_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_del">-		} else {</span>
<span class="p_del">-			p4d_t p4d;</span>
<span class="p_del">-</span>
<span class="p_del">-			pud_p = ppd-&gt;pgtable_area;</span>
<span class="p_del">-			memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);</span>
<span class="p_del">-			ppd-&gt;pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
<span class="p_del">-</span>
<span class="p_del">-			p4d = native_make_p4d((pudval_t)pud_p + P4D_FLAGS);</span>
<span class="p_del">-			native_set_p4d(p4d_p, p4d);</span>
<span class="p_del">-		}</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	pud_p += pud_index(ppd-&gt;vaddr);</span>
<span class="p_del">-	if (native_pud_val(*pud_p)) {</span>
<span class="p_del">-		if (native_pud_val(*pud_p) &amp; _PAGE_PSE)</span>
<span class="p_del">-			return NULL;</span>
<span class="p_del">-</span>
<span class="p_del">-		pmd_p = (pmd_t *)(native_pud_val(*pud_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		pud_t pud;</span>
<span class="p_del">-</span>
<span class="p_del">-		pmd_p = ppd-&gt;pgtable_area;</span>
<span class="p_del">-		memset(pmd_p, 0, sizeof(*pmd_p) * PTRS_PER_PMD);</span>
<span class="p_del">-		ppd-&gt;pgtable_area += sizeof(*pmd_p) * PTRS_PER_PMD;</span>
<span class="p_del">-</span>
<span class="p_del">-		pud = native_make_pud((pmdval_t)pmd_p + PUD_FLAGS);</span>
<span class="p_del">-		native_set_pud(pud_p, pud);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	return pmd_p;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_populate_pgd_large(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pmd_t *pmd_p;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmd_p = sme_prepare_pgd(ppd);</span>
<span class="p_del">-	if (!pmd_p)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmd_p += pmd_index(ppd-&gt;vaddr);</span>
<span class="p_del">-	if (!native_pmd_val(*pmd_p) || !(native_pmd_val(*pmd_p) &amp; _PAGE_PSE))</span>
<span class="p_del">-		native_set_pmd(pmd_p, native_make_pmd(ppd-&gt;paddr | ppd-&gt;pmd_flags));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_populate_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	pmd_t *pmd_p;</span>
<span class="p_del">-	pte_t *pte_p;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmd_p = sme_prepare_pgd(ppd);</span>
<span class="p_del">-	if (!pmd_p)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	pmd_p += pmd_index(ppd-&gt;vaddr);</span>
<span class="p_del">-	if (native_pmd_val(*pmd_p)) {</span>
<span class="p_del">-		if (native_pmd_val(*pmd_p) &amp; _PAGE_PSE)</span>
<span class="p_del">-			return;</span>
<span class="p_del">-</span>
<span class="p_del">-		pte_p = (pte_t *)(native_pmd_val(*pmd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		pmd_t pmd;</span>
<span class="p_del">-</span>
<span class="p_del">-		pte_p = ppd-&gt;pgtable_area;</span>
<span class="p_del">-		memset(pte_p, 0, sizeof(*pte_p) * PTRS_PER_PTE);</span>
<span class="p_del">-		ppd-&gt;pgtable_area += sizeof(*pte_p) * PTRS_PER_PTE;</span>
<span class="p_del">-</span>
<span class="p_del">-		pmd = native_make_pmd((pteval_t)pte_p + PMD_FLAGS);</span>
<span class="p_del">-		native_set_pmd(pmd_p, pmd);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	pte_p += pte_index(ppd-&gt;vaddr);</span>
<span class="p_del">-	if (!native_pte_val(*pte_p))</span>
<span class="p_del">-		native_set_pte(pte_p, native_make_pte(ppd-&gt;paddr | ppd-&gt;pte_flags));</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init __sme_map_range_pmd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	while (ppd-&gt;vaddr &lt; ppd-&gt;vaddr_end) {</span>
<span class="p_del">-		sme_populate_pgd_large(ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-		ppd-&gt;vaddr += PMD_PAGE_SIZE;</span>
<span class="p_del">-		ppd-&gt;paddr += PMD_PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init __sme_map_range_pte(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	while (ppd-&gt;vaddr &lt; ppd-&gt;vaddr_end) {</span>
<span class="p_del">-		sme_populate_pgd(ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-		ppd-&gt;vaddr += PAGE_SIZE;</span>
<span class="p_del">-		ppd-&gt;paddr += PAGE_SIZE;</span>
<span class="p_del">-	}</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init __sme_map_range(struct sme_populate_pgd_data *ppd,</span>
<span class="p_del">-				   pmdval_t pmd_flags, pteval_t pte_flags)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long vaddr_end;</span>
<span class="p_del">-</span>
<span class="p_del">-	ppd-&gt;pmd_flags = pmd_flags;</span>
<span class="p_del">-	ppd-&gt;pte_flags = pte_flags;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Save original end value since we modify the struct value */</span>
<span class="p_del">-	vaddr_end = ppd-&gt;vaddr_end;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* If start is not 2MB aligned, create PTE entries */</span>
<span class="p_del">-	ppd-&gt;vaddr_end = ALIGN(ppd-&gt;vaddr, PMD_PAGE_SIZE);</span>
<span class="p_del">-	__sme_map_range_pte(ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Create PMD entries */</span>
<span class="p_del">-	ppd-&gt;vaddr_end = vaddr_end &amp; PMD_PAGE_MASK;</span>
<span class="p_del">-	__sme_map_range_pmd(ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* If end is not 2MB aligned, create PTE entries */</span>
<span class="p_del">-	ppd-&gt;vaddr_end = vaddr_end;</span>
<span class="p_del">-	__sme_map_range_pte(ppd);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_map_range_encrypted(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__sme_map_range(ppd, PMD_FLAGS_ENC, PTE_FLAGS_ENC);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_map_range_decrypted(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__sme_map_range(ppd, PMD_FLAGS_DEC, PTE_FLAGS_DEC);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static void __init sme_map_range_decrypted_wp(struct sme_populate_pgd_data *ppd)</span>
<span class="p_del">-{</span>
<span class="p_del">-	__sme_map_range(ppd, PMD_FLAGS_DEC_WP, PTE_FLAGS_DEC_WP);</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-static unsigned long __init sme_pgtable_calc(unsigned long len)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long p4d_size, pud_size, pmd_size, pte_size;</span>
<span class="p_del">-	unsigned long total;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Perform a relatively simplistic calculation of the pagetable</span>
<span class="p_del">-	 * entries that are needed. Those mappings will be covered mostly</span>
<span class="p_del">-	 * by 2MB PMD entries so we can conservatively calculate the required</span>
<span class="p_del">-	 * number of P4D, PUD and PMD structures needed to perform the</span>
<span class="p_del">-	 * mappings.  For mappings that are not 2MB aligned, PTE mappings</span>
<span class="p_del">-	 * would be needed for the start and end portion of the address range</span>
<span class="p_del">-	 * that fall outside of the 2MB alignment.  This results in, at most,</span>
<span class="p_del">-	 * two extra pages to hold PTE entries for each range that is mapped.</span>
<span class="p_del">-	 * Incrementing the count for each covers the case where the addresses</span>
<span class="p_del">-	 * cross entries.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_del">-		p4d_size = (ALIGN(len, PGDIR_SIZE) / PGDIR_SIZE) + 1;</span>
<span class="p_del">-		p4d_size *= sizeof(p4d_t) * PTRS_PER_P4D;</span>
<span class="p_del">-		pud_size = (ALIGN(len, P4D_SIZE) / P4D_SIZE) + 1;</span>
<span class="p_del">-		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		p4d_size = 0;</span>
<span class="p_del">-		pud_size = (ALIGN(len, PGDIR_SIZE) / PGDIR_SIZE) + 1;</span>
<span class="p_del">-		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	pmd_size = (ALIGN(len, PUD_SIZE) / PUD_SIZE) + 1;</span>
<span class="p_del">-	pmd_size *= sizeof(pmd_t) * PTRS_PER_PMD;</span>
<span class="p_del">-	pte_size = 2 * sizeof(pte_t) * PTRS_PER_PTE;</span>
<span class="p_del">-</span>
<span class="p_del">-	total = p4d_size + pud_size + pmd_size + pte_size;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Now calculate the added pagetable structures needed to populate</span>
<span class="p_del">-	 * the new pagetables.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_del">-		p4d_size = ALIGN(total, PGDIR_SIZE) / PGDIR_SIZE;</span>
<span class="p_del">-		p4d_size *= sizeof(p4d_t) * PTRS_PER_P4D;</span>
<span class="p_del">-		pud_size = ALIGN(total, P4D_SIZE) / P4D_SIZE;</span>
<span class="p_del">-		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		p4d_size = 0;</span>
<span class="p_del">-		pud_size = ALIGN(total, PGDIR_SIZE) / PGDIR_SIZE;</span>
<span class="p_del">-		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_del">-	}</span>
<span class="p_del">-	pmd_size = ALIGN(total, PUD_SIZE) / PUD_SIZE;</span>
<span class="p_del">-	pmd_size *= sizeof(pmd_t) * PTRS_PER_PMD;</span>
<span class="p_del">-</span>
<span class="p_del">-	total += p4d_size + pud_size + pmd_size;</span>
<span class="p_del">-</span>
<span class="p_del">-	return total;</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __init __nostackprotector sme_encrypt_kernel(struct boot_params *bp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	unsigned long workarea_start, workarea_end, workarea_len;</span>
<span class="p_del">-	unsigned long execute_start, execute_end, execute_len;</span>
<span class="p_del">-	unsigned long kernel_start, kernel_end, kernel_len;</span>
<span class="p_del">-	unsigned long initrd_start, initrd_end, initrd_len;</span>
<span class="p_del">-	struct sme_populate_pgd_data ppd;</span>
<span class="p_del">-	unsigned long pgtable_area_len;</span>
<span class="p_del">-	unsigned long decrypted_base;</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!sme_active())</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Prepare for encrypting the kernel and initrd by building new</span>
<span class="p_del">-	 * pagetables with the necessary attributes needed to encrypt the</span>
<span class="p_del">-	 * kernel in place.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *   One range of virtual addresses will map the memory occupied</span>
<span class="p_del">-	 *   by the kernel and initrd as encrypted.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *   Another range of virtual addresses will map the memory occupied</span>
<span class="p_del">-	 *   by the kernel and initrd as decrypted and write-protected.</span>
<span class="p_del">-	 *</span>
<span class="p_del">-	 *     The use of write-protect attribute will prevent any of the</span>
<span class="p_del">-	 *     memory from being cached.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Physical addresses gives us the identity mapped virtual addresses */</span>
<span class="p_del">-	kernel_start = __pa_symbol(_text);</span>
<span class="p_del">-	kernel_end = ALIGN(__pa_symbol(_end), PMD_PAGE_SIZE);</span>
<span class="p_del">-	kernel_len = kernel_end - kernel_start;</span>
<span class="p_del">-</span>
<span class="p_del">-	initrd_start = 0;</span>
<span class="p_del">-	initrd_end = 0;</span>
<span class="p_del">-	initrd_len = 0;</span>
<span class="p_del">-#ifdef CONFIG_BLK_DEV_INITRD</span>
<span class="p_del">-	initrd_len = (unsigned long)bp-&gt;hdr.ramdisk_size |</span>
<span class="p_del">-		     ((unsigned long)bp-&gt;ext_ramdisk_size &lt;&lt; 32);</span>
<span class="p_del">-	if (initrd_len) {</span>
<span class="p_del">-		initrd_start = (unsigned long)bp-&gt;hdr.ramdisk_image |</span>
<span class="p_del">-			       ((unsigned long)bp-&gt;ext_ramdisk_image &lt;&lt; 32);</span>
<span class="p_del">-		initrd_end = PAGE_ALIGN(initrd_start + initrd_len);</span>
<span class="p_del">-		initrd_len = initrd_end - initrd_start;</span>
<span class="p_del">-	}</span>
<span class="p_del">-#endif</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Set the encryption workarea to be immediately after the kernel */</span>
<span class="p_del">-	workarea_start = kernel_end;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Calculate required number of workarea bytes needed:</span>
<span class="p_del">-	 *   executable encryption area size:</span>
<span class="p_del">-	 *     stack page (PAGE_SIZE)</span>
<span class="p_del">-	 *     encryption routine page (PAGE_SIZE)</span>
<span class="p_del">-	 *     intermediate copy buffer (PMD_PAGE_SIZE)</span>
<span class="p_del">-	 *   pagetable structures for the encryption of the kernel</span>
<span class="p_del">-	 *   pagetable structures for workarea (in case not currently mapped)</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	execute_start = workarea_start;</span>
<span class="p_del">-	execute_end = execute_start + (PAGE_SIZE * 2) + PMD_PAGE_SIZE;</span>
<span class="p_del">-	execute_len = execute_end - execute_start;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * One PGD for both encrypted and decrypted mappings and a set of</span>
<span class="p_del">-	 * PUDs and PMDs for each of the encrypted and decrypted mappings.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	pgtable_area_len = sizeof(pgd_t) * PTRS_PER_PGD;</span>
<span class="p_del">-	pgtable_area_len += sme_pgtable_calc(execute_end - kernel_start) * 2;</span>
<span class="p_del">-	if (initrd_len)</span>
<span class="p_del">-		pgtable_area_len += sme_pgtable_calc(initrd_len) * 2;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* PUDs and PMDs needed in the current pagetables for the workarea */</span>
<span class="p_del">-	pgtable_area_len += sme_pgtable_calc(execute_len + pgtable_area_len);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * The total workarea includes the executable encryption area and</span>
<span class="p_del">-	 * the pagetable area. The start of the workarea is already 2MB</span>
<span class="p_del">-	 * aligned, align the end of the workarea on a 2MB boundary so that</span>
<span class="p_del">-	 * we don&#39;t try to create/allocate PTE entries from the workarea</span>
<span class="p_del">-	 * before it is mapped.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	workarea_len = execute_len + pgtable_area_len;</span>
<span class="p_del">-	workarea_end = ALIGN(workarea_start + workarea_len, PMD_PAGE_SIZE);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Set the address to the start of where newly created pagetable</span>
<span class="p_del">-	 * structures (PGDs, PUDs and PMDs) will be allocated. New pagetable</span>
<span class="p_del">-	 * structures are created when the workarea is added to the current</span>
<span class="p_del">-	 * pagetables and when the new encrypted and decrypted kernel</span>
<span class="p_del">-	 * mappings are populated.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	ppd.pgtable_area = (void *)execute_end;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Make sure the current pagetable structure has entries for</span>
<span class="p_del">-	 * addressing the workarea.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	ppd.pgd = (pgd_t *)native_read_cr3_pa();</span>
<span class="p_del">-	ppd.paddr = workarea_start;</span>
<span class="p_del">-	ppd.vaddr = workarea_start;</span>
<span class="p_del">-	ppd.vaddr_end = workarea_end;</span>
<span class="p_del">-	sme_map_range_decrypted(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Flush the TLB - no globals so cr3 is enough */</span>
<span class="p_del">-	native_write_cr3(__native_read_cr3());</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * A new pagetable structure is being built to allow for the kernel</span>
<span class="p_del">-	 * and initrd to be encrypted. It starts with an empty PGD that will</span>
<span class="p_del">-	 * then be populated with new PUDs and PMDs as the encrypted and</span>
<span class="p_del">-	 * decrypted kernel mappings are created.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	ppd.pgd = ppd.pgtable_area;</span>
<span class="p_del">-	memset(ppd.pgd, 0, sizeof(pgd_t) * PTRS_PER_PGD);</span>
<span class="p_del">-	ppd.pgtable_area += sizeof(pgd_t) * PTRS_PER_PGD;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * A different PGD index/entry must be used to get different</span>
<span class="p_del">-	 * pagetable entries for the decrypted mapping. Choose the next</span>
<span class="p_del">-	 * PGD index and convert it to a virtual address to be used as</span>
<span class="p_del">-	 * the base of the mapping.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	decrypted_base = (pgd_index(workarea_end) + 1) &amp; (PTRS_PER_PGD - 1);</span>
<span class="p_del">-	if (initrd_len) {</span>
<span class="p_del">-		unsigned long check_base;</span>
<span class="p_del">-</span>
<span class="p_del">-		check_base = (pgd_index(initrd_end) + 1) &amp; (PTRS_PER_PGD - 1);</span>
<span class="p_del">-		decrypted_base = max(decrypted_base, check_base);</span>
<span class="p_del">-	}</span>
<span class="p_del">-	decrypted_base &lt;&lt;= PGDIR_SHIFT;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Add encrypted kernel (identity) mappings */</span>
<span class="p_del">-	ppd.paddr = kernel_start;</span>
<span class="p_del">-	ppd.vaddr = kernel_start;</span>
<span class="p_del">-	ppd.vaddr_end = kernel_end;</span>
<span class="p_del">-	sme_map_range_encrypted(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Add decrypted, write-protected kernel (non-identity) mappings */</span>
<span class="p_del">-	ppd.paddr = kernel_start;</span>
<span class="p_del">-	ppd.vaddr = kernel_start + decrypted_base;</span>
<span class="p_del">-	ppd.vaddr_end = kernel_end + decrypted_base;</span>
<span class="p_del">-	sme_map_range_decrypted_wp(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (initrd_len) {</span>
<span class="p_del">-		/* Add encrypted initrd (identity) mappings */</span>
<span class="p_del">-		ppd.paddr = initrd_start;</span>
<span class="p_del">-		ppd.vaddr = initrd_start;</span>
<span class="p_del">-		ppd.vaddr_end = initrd_end;</span>
<span class="p_del">-		sme_map_range_encrypted(&amp;ppd);</span>
<span class="p_del">-		/*</span>
<span class="p_del">-		 * Add decrypted, write-protected initrd (non-identity) mappings</span>
<span class="p_del">-		 */</span>
<span class="p_del">-		ppd.paddr = initrd_start;</span>
<span class="p_del">-		ppd.vaddr = initrd_start + decrypted_base;</span>
<span class="p_del">-		ppd.vaddr_end = initrd_end + decrypted_base;</span>
<span class="p_del">-		sme_map_range_decrypted_wp(&amp;ppd);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Add decrypted workarea mappings to both kernel mappings */</span>
<span class="p_del">-	ppd.paddr = workarea_start;</span>
<span class="p_del">-	ppd.vaddr = workarea_start;</span>
<span class="p_del">-	ppd.vaddr_end = workarea_end;</span>
<span class="p_del">-	sme_map_range_decrypted(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	ppd.paddr = workarea_start;</span>
<span class="p_del">-	ppd.vaddr = workarea_start + decrypted_base;</span>
<span class="p_del">-	ppd.vaddr_end = workarea_end + decrypted_base;</span>
<span class="p_del">-	sme_map_range_decrypted(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Perform the encryption */</span>
<span class="p_del">-	sme_encrypt_execute(kernel_start, kernel_start + decrypted_base,</span>
<span class="p_del">-			    kernel_len, workarea_start, (unsigned long)ppd.pgd);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (initrd_len)</span>
<span class="p_del">-		sme_encrypt_execute(initrd_start, initrd_start + decrypted_base,</span>
<span class="p_del">-				    initrd_len, workarea_start,</span>
<span class="p_del">-				    (unsigned long)ppd.pgd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * At this point we are running encrypted.  Remove the mappings for</span>
<span class="p_del">-	 * the decrypted areas - all that is needed for this is to remove</span>
<span class="p_del">-	 * the PGD entry/entries.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	ppd.vaddr = kernel_start + decrypted_base;</span>
<span class="p_del">-	ppd.vaddr_end = kernel_end + decrypted_base;</span>
<span class="p_del">-	sme_clear_pgd(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	if (initrd_len) {</span>
<span class="p_del">-		ppd.vaddr = initrd_start + decrypted_base;</span>
<span class="p_del">-		ppd.vaddr_end = initrd_end + decrypted_base;</span>
<span class="p_del">-		sme_clear_pgd(&amp;ppd);</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	ppd.vaddr = workarea_start + decrypted_base;</span>
<span class="p_del">-	ppd.vaddr_end = workarea_end + decrypted_base;</span>
<span class="p_del">-	sme_clear_pgd(&amp;ppd);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Flush the TLB - no globals so cr3 is enough */</span>
<span class="p_del">-	native_write_cr3(__native_read_cr3());</span>
<span class="p_del">-}</span>
<span class="p_del">-</span>
<span class="p_del">-void __init __nostackprotector sme_enable(struct boot_params *bp)</span>
<span class="p_del">-{</span>
<span class="p_del">-	const char *cmdline_ptr, *cmdline_arg, *cmdline_on, *cmdline_off;</span>
<span class="p_del">-	unsigned int eax, ebx, ecx, edx;</span>
<span class="p_del">-	unsigned long feature_mask;</span>
<span class="p_del">-	bool active_by_default;</span>
<span class="p_del">-	unsigned long me_mask;</span>
<span class="p_del">-	char buffer[16];</span>
<span class="p_del">-	u64 msr;</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Check for the SME/SEV support leaf */</span>
<span class="p_del">-	eax = 0x80000000;</span>
<span class="p_del">-	ecx = 0;</span>
<span class="p_del">-	native_cpuid(&amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_del">-	if (eax &lt; 0x8000001f)</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-#define AMD_SME_BIT	BIT(0)</span>
<span class="p_del">-#define AMD_SEV_BIT	BIT(1)</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Set the feature mask (SME or SEV) based on whether we are</span>
<span class="p_del">-	 * running under a hypervisor.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	eax = 1;</span>
<span class="p_del">-	ecx = 0;</span>
<span class="p_del">-	native_cpuid(&amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_del">-	feature_mask = (ecx &amp; BIT(31)) ? AMD_SEV_BIT : AMD_SME_BIT;</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Check for the SME/SEV feature:</span>
<span class="p_del">-	 *   CPUID Fn8000_001F[EAX]</span>
<span class="p_del">-	 *   - Bit 0 - Secure Memory Encryption support</span>
<span class="p_del">-	 *   - Bit 1 - Secure Encrypted Virtualization support</span>
<span class="p_del">-	 *   CPUID Fn8000_001F[EBX]</span>
<span class="p_del">-	 *   - Bits 5:0 - Pagetable bit position used to indicate encryption</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	eax = 0x8000001f;</span>
<span class="p_del">-	ecx = 0;</span>
<span class="p_del">-	native_cpuid(&amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_del">-	if (!(eax &amp; feature_mask))</span>
<span class="p_del">-		return;</span>
<span class="p_del">-</span>
<span class="p_del">-	me_mask = 1UL &lt;&lt; (ebx &amp; 0x3f);</span>
<span class="p_del">-</span>
<span class="p_del">-	/* Check if memory encryption is enabled */</span>
<span class="p_del">-	if (feature_mask == AMD_SME_BIT) {</span>
<span class="p_del">-		/* For SME, check the SYSCFG MSR */</span>
<span class="p_del">-		msr = __rdmsr(MSR_K8_SYSCFG);</span>
<span class="p_del">-		if (!(msr &amp; MSR_K8_SYSCFG_MEM_ENCRYPT))</span>
<span class="p_del">-			return;</span>
<span class="p_del">-	} else {</span>
<span class="p_del">-		/* For SEV, check the SEV MSR */</span>
<span class="p_del">-		msr = __rdmsr(MSR_AMD64_SEV);</span>
<span class="p_del">-		if (!(msr &amp; MSR_AMD64_SEV_ENABLED))</span>
<span class="p_del">-			return;</span>
<span class="p_del">-</span>
<span class="p_del">-		/* SEV state cannot be controlled by a command line option */</span>
<span class="p_del">-		sme_me_mask = me_mask;</span>
<span class="p_del">-		sev_enabled = true;</span>
<span class="p_del">-		return;</span>
<span class="p_del">-	}</span>
<span class="p_del">-</span>
<span class="p_del">-	/*</span>
<span class="p_del">-	 * Fixups have not been applied to phys_base yet and we&#39;re running</span>
<span class="p_del">-	 * identity mapped, so we must obtain the address to the SME command</span>
<span class="p_del">-	 * line argument data using rip-relative addressing.</span>
<span class="p_del">-	 */</span>
<span class="p_del">-	asm (&quot;lea sme_cmdline_arg(%%rip), %0&quot;</span>
<span class="p_del">-	     : &quot;=r&quot; (cmdline_arg)</span>
<span class="p_del">-	     : &quot;p&quot; (sme_cmdline_arg));</span>
<span class="p_del">-	asm (&quot;lea sme_cmdline_on(%%rip), %0&quot;</span>
<span class="p_del">-	     : &quot;=r&quot; (cmdline_on)</span>
<span class="p_del">-	     : &quot;p&quot; (sme_cmdline_on));</span>
<span class="p_del">-	asm (&quot;lea sme_cmdline_off(%%rip), %0&quot;</span>
<span class="p_del">-	     : &quot;=r&quot; (cmdline_off)</span>
<span class="p_del">-	     : &quot;p&quot; (sme_cmdline_off));</span>
<span class="p_del">-</span>
<span class="p_del">-	if (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT_ACTIVE_BY_DEFAULT))</span>
<span class="p_del">-		active_by_default = true;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		active_by_default = false;</span>
<span class="p_del">-</span>
<span class="p_del">-	cmdline_ptr = (const char *)((u64)bp-&gt;hdr.cmd_line_ptr |</span>
<span class="p_del">-				     ((u64)bp-&gt;ext_cmd_line_ptr &lt;&lt; 32));</span>
<span class="p_del">-</span>
<span class="p_del">-	cmdline_find_option(cmdline_ptr, cmdline_arg, buffer, sizeof(buffer));</span>
<span class="p_del">-</span>
<span class="p_del">-	if (!strncmp(buffer, cmdline_on, sizeof(buffer)))</span>
<span class="p_del">-		sme_me_mask = me_mask;</span>
<span class="p_del">-	else if (!strncmp(buffer, cmdline_off, sizeof(buffer)))</span>
<span class="p_del">-		sme_me_mask = 0;</span>
<span class="p_del">-	else</span>
<span class="p_del">-		sme_me_mask = active_by_default ? me_mask : 0;</span>
<span class="p_del">-}</span>
<span class="p_header">diff --git a/arch/x86/mm/mem_encrypt_identity.c b/arch/x86/mm/mem_encrypt_identity.c</span>
new file mode 100644
<span class="p_header">index 000000000000..c23d55cb25c4</span>
<span class="p_header">--- /dev/null</span>
<span class="p_header">+++ b/arch/x86/mm/mem_encrypt_identity.c</span>
<span class="p_chunk">@@ -0,0 +1,596 @@</span> <span class="p_context"></span>
<span class="p_add">+/*</span>
<span class="p_add">+ * AMD Memory Encryption Support</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Copyright (C) 2016 Advanced Micro Devices, Inc.</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * Author: Tom Lendacky &lt;thomas.lendacky@amd.com&gt;</span>
<span class="p_add">+ *</span>
<span class="p_add">+ * This program is free software; you can redistribute it and/or modify</span>
<span class="p_add">+ * it under the terms of the GNU General Public License version 2 as</span>
<span class="p_add">+ * published by the Free Software Foundation.</span>
<span class="p_add">+ */</span>
<span class="p_add">+</span>
<span class="p_add">+#define DISABLE_BRANCH_PROFILING</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;linux/mm.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &lt;asm/setup.h&gt;</span>
<span class="p_add">+#include &lt;asm/sections.h&gt;</span>
<span class="p_add">+#include &lt;asm/cmdline.h&gt;</span>
<span class="p_add">+</span>
<span class="p_add">+#include &quot;mm_internal.h&quot;</span>
<span class="p_add">+</span>
<span class="p_add">+#define PGD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define P4D_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define PUD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+#define PMD_FLAGS		_KERNPG_TABLE_NOENC</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_FLAGS_LARGE		(__PAGE_KERNEL_LARGE_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_FLAGS_DEC		PMD_FLAGS_LARGE</span>
<span class="p_add">+#define PMD_FLAGS_DEC_WP	((PMD_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_add">+				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_add">+</span>
<span class="p_add">+#define PMD_FLAGS_ENC		(PMD_FLAGS_LARGE | _PAGE_ENC)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTE_FLAGS		(__PAGE_KERNEL_EXEC &amp; ~_PAGE_GLOBAL)</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTE_FLAGS_DEC		PTE_FLAGS</span>
<span class="p_add">+#define PTE_FLAGS_DEC_WP	((PTE_FLAGS_DEC &amp; ~_PAGE_CACHE_MASK) | \</span>
<span class="p_add">+				 (_PAGE_PAT | _PAGE_PWT))</span>
<span class="p_add">+</span>
<span class="p_add">+#define PTE_FLAGS_ENC		(PTE_FLAGS | _PAGE_ENC)</span>
<span class="p_add">+</span>
<span class="p_add">+struct sme_populate_pgd_data {</span>
<span class="p_add">+	void    *pgtable_area;</span>
<span class="p_add">+	pgd_t   *pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmdval_t pmd_flags;</span>
<span class="p_add">+	pteval_t pte_flags;</span>
<span class="p_add">+	unsigned long paddr;</span>
<span class="p_add">+</span>
<span class="p_add">+	unsigned long vaddr;</span>
<span class="p_add">+	unsigned long vaddr_end;</span>
<span class="p_add">+};</span>
<span class="p_add">+</span>
<span class="p_add">+static char sme_cmdline_arg[] __initdata = &quot;mem_encrypt&quot;;</span>
<span class="p_add">+static char sme_cmdline_on[]  __initdata = &quot;on&quot;;</span>
<span class="p_add">+static char sme_cmdline_off[] __initdata = &quot;off&quot;;</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_clear_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long pgd_start, pgd_end, pgd_size;</span>
<span class="p_add">+	pgd_t *pgd_p;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd_start = ppd-&gt;vaddr &amp; PGDIR_MASK;</span>
<span class="p_add">+	pgd_end = ppd-&gt;vaddr_end &amp; PGDIR_MASK;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd_size = (((pgd_end - pgd_start) / PGDIR_SIZE) + 1) * sizeof(pgd_t);</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd_p = ppd-&gt;pgd + pgd_index(ppd-&gt;vaddr);</span>
<span class="p_add">+</span>
<span class="p_add">+	memset(pgd_p, 0, pgd_size);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static pmd_t __init *sme_prepare_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pgd_t *pgd_p;</span>
<span class="p_add">+	p4d_t *p4d_p;</span>
<span class="p_add">+	pud_t *pud_p;</span>
<span class="p_add">+	pmd_t *pmd_p;</span>
<span class="p_add">+</span>
<span class="p_add">+	pgd_p = ppd-&gt;pgd + pgd_index(ppd-&gt;vaddr);</span>
<span class="p_add">+	if (native_pgd_val(*pgd_p)) {</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_X86_5LEVEL))</span>
<span class="p_add">+			p4d_p = (p4d_t *)(native_pgd_val(*pgd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_add">+		else</span>
<span class="p_add">+			pud_p = (pud_t *)(native_pgd_val(*pgd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pgd_t pgd;</span>
<span class="p_add">+</span>
<span class="p_add">+		if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_add">+			p4d_p = ppd-&gt;pgtable_area;</span>
<span class="p_add">+			memset(p4d_p, 0, sizeof(*p4d_p) * PTRS_PER_P4D);</span>
<span class="p_add">+			ppd-&gt;pgtable_area += sizeof(*p4d_p) * PTRS_PER_P4D;</span>
<span class="p_add">+</span>
<span class="p_add">+			pgd = native_make_pgd((pgdval_t)p4d_p + PGD_FLAGS);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			pud_p = ppd-&gt;pgtable_area;</span>
<span class="p_add">+			memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);</span>
<span class="p_add">+			ppd-&gt;pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
<span class="p_add">+</span>
<span class="p_add">+			pgd = native_make_pgd((pgdval_t)pud_p + PGD_FLAGS);</span>
<span class="p_add">+		}</span>
<span class="p_add">+		native_set_pgd(pgd_p, pgd);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_add">+		p4d_p += p4d_index(ppd-&gt;vaddr);</span>
<span class="p_add">+		if (native_p4d_val(*p4d_p)) {</span>
<span class="p_add">+			pud_p = (pud_t *)(native_p4d_val(*p4d_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_add">+		} else {</span>
<span class="p_add">+			p4d_t p4d;</span>
<span class="p_add">+</span>
<span class="p_add">+			pud_p = ppd-&gt;pgtable_area;</span>
<span class="p_add">+			memset(pud_p, 0, sizeof(*pud_p) * PTRS_PER_PUD);</span>
<span class="p_add">+			ppd-&gt;pgtable_area += sizeof(*pud_p) * PTRS_PER_PUD;</span>
<span class="p_add">+</span>
<span class="p_add">+			p4d = native_make_p4d((pudval_t)pud_p + P4D_FLAGS);</span>
<span class="p_add">+			native_set_p4d(p4d_p, p4d);</span>
<span class="p_add">+		}</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pud_p += pud_index(ppd-&gt;vaddr);</span>
<span class="p_add">+	if (native_pud_val(*pud_p)) {</span>
<span class="p_add">+		if (native_pud_val(*pud_p) &amp; _PAGE_PSE)</span>
<span class="p_add">+			return NULL;</span>
<span class="p_add">+</span>
<span class="p_add">+		pmd_p = (pmd_t *)(native_pud_val(*pud_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pud_t pud;</span>
<span class="p_add">+</span>
<span class="p_add">+		pmd_p = ppd-&gt;pgtable_area;</span>
<span class="p_add">+		memset(pmd_p, 0, sizeof(*pmd_p) * PTRS_PER_PMD);</span>
<span class="p_add">+		ppd-&gt;pgtable_area += sizeof(*pmd_p) * PTRS_PER_PMD;</span>
<span class="p_add">+</span>
<span class="p_add">+		pud = native_make_pud((pmdval_t)pmd_p + PUD_FLAGS);</span>
<span class="p_add">+		native_set_pud(pud_p, pud);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	return pmd_p;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_populate_pgd_large(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd_p;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p = sme_prepare_pgd(ppd);</span>
<span class="p_add">+	if (!pmd_p)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p += pmd_index(ppd-&gt;vaddr);</span>
<span class="p_add">+	if (!native_pmd_val(*pmd_p) || !(native_pmd_val(*pmd_p) &amp; _PAGE_PSE))</span>
<span class="p_add">+		native_set_pmd(pmd_p, native_make_pmd(ppd-&gt;paddr | ppd-&gt;pmd_flags));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_populate_pgd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	pmd_t *pmd_p;</span>
<span class="p_add">+	pte_t *pte_p;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p = sme_prepare_pgd(ppd);</span>
<span class="p_add">+	if (!pmd_p)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	pmd_p += pmd_index(ppd-&gt;vaddr);</span>
<span class="p_add">+	if (native_pmd_val(*pmd_p)) {</span>
<span class="p_add">+		if (native_pmd_val(*pmd_p) &amp; _PAGE_PSE)</span>
<span class="p_add">+			return;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte_p = (pte_t *)(native_pmd_val(*pmd_p) &amp; ~PTE_FLAGS_MASK);</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		pmd_t pmd;</span>
<span class="p_add">+</span>
<span class="p_add">+		pte_p = ppd-&gt;pgtable_area;</span>
<span class="p_add">+		memset(pte_p, 0, sizeof(*pte_p) * PTRS_PER_PTE);</span>
<span class="p_add">+		ppd-&gt;pgtable_area += sizeof(*pte_p) * PTRS_PER_PTE;</span>
<span class="p_add">+</span>
<span class="p_add">+		pmd = native_make_pmd((pteval_t)pte_p + PMD_FLAGS);</span>
<span class="p_add">+		native_set_pmd(pmd_p, pmd);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	pte_p += pte_index(ppd-&gt;vaddr);</span>
<span class="p_add">+	if (!native_pte_val(*pte_p))</span>
<span class="p_add">+		native_set_pte(pte_p, native_make_pte(ppd-&gt;paddr | ppd-&gt;pte_flags));</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init __sme_map_range_pmd(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (ppd-&gt;vaddr &lt; ppd-&gt;vaddr_end) {</span>
<span class="p_add">+		sme_populate_pgd_large(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+		ppd-&gt;vaddr += PMD_PAGE_SIZE;</span>
<span class="p_add">+		ppd-&gt;paddr += PMD_PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init __sme_map_range_pte(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	while (ppd-&gt;vaddr &lt; ppd-&gt;vaddr_end) {</span>
<span class="p_add">+		sme_populate_pgd(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+		ppd-&gt;vaddr += PAGE_SIZE;</span>
<span class="p_add">+		ppd-&gt;paddr += PAGE_SIZE;</span>
<span class="p_add">+	}</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init __sme_map_range(struct sme_populate_pgd_data *ppd,</span>
<span class="p_add">+				   pmdval_t pmd_flags, pteval_t pte_flags)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long vaddr_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	ppd-&gt;pmd_flags = pmd_flags;</span>
<span class="p_add">+	ppd-&gt;pte_flags = pte_flags;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Save original end value since we modify the struct value */</span>
<span class="p_add">+	vaddr_end = ppd-&gt;vaddr_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If start is not 2MB aligned, create PTE entries */</span>
<span class="p_add">+	ppd-&gt;vaddr_end = ALIGN(ppd-&gt;vaddr, PMD_PAGE_SIZE);</span>
<span class="p_add">+	__sme_map_range_pte(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Create PMD entries */</span>
<span class="p_add">+	ppd-&gt;vaddr_end = vaddr_end &amp; PMD_PAGE_MASK;</span>
<span class="p_add">+	__sme_map_range_pmd(ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* If end is not 2MB aligned, create PTE entries */</span>
<span class="p_add">+	ppd-&gt;vaddr_end = vaddr_end;</span>
<span class="p_add">+	__sme_map_range_pte(ppd);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_map_range_encrypted(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__sme_map_range(ppd, PMD_FLAGS_ENC, PTE_FLAGS_ENC);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_map_range_decrypted(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__sme_map_range(ppd, PMD_FLAGS_DEC, PTE_FLAGS_DEC);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static void __init sme_map_range_decrypted_wp(struct sme_populate_pgd_data *ppd)</span>
<span class="p_add">+{</span>
<span class="p_add">+	__sme_map_range(ppd, PMD_FLAGS_DEC_WP, PTE_FLAGS_DEC_WP);</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+static unsigned long __init sme_pgtable_calc(unsigned long len)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long p4d_size, pud_size, pmd_size, pte_size;</span>
<span class="p_add">+	unsigned long total;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Perform a relatively simplistic calculation of the pagetable</span>
<span class="p_add">+	 * entries that are needed. Those mappings will be covered mostly</span>
<span class="p_add">+	 * by 2MB PMD entries so we can conservatively calculate the required</span>
<span class="p_add">+	 * number of P4D, PUD and PMD structures needed to perform the</span>
<span class="p_add">+	 * mappings.  For mappings that are not 2MB aligned, PTE mappings</span>
<span class="p_add">+	 * would be needed for the start and end portion of the address range</span>
<span class="p_add">+	 * that fall outside of the 2MB alignment.  This results in, at most,</span>
<span class="p_add">+	 * two extra pages to hold PTE entries for each range that is mapped.</span>
<span class="p_add">+	 * Incrementing the count for each covers the case where the addresses</span>
<span class="p_add">+	 * cross entries.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_add">+		p4d_size = (ALIGN(len, PGDIR_SIZE) / PGDIR_SIZE) + 1;</span>
<span class="p_add">+		p4d_size *= sizeof(p4d_t) * PTRS_PER_P4D;</span>
<span class="p_add">+		pud_size = (ALIGN(len, P4D_SIZE) / P4D_SIZE) + 1;</span>
<span class="p_add">+		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		p4d_size = 0;</span>
<span class="p_add">+		pud_size = (ALIGN(len, PGDIR_SIZE) / PGDIR_SIZE) + 1;</span>
<span class="p_add">+		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	pmd_size = (ALIGN(len, PUD_SIZE) / PUD_SIZE) + 1;</span>
<span class="p_add">+	pmd_size *= sizeof(pmd_t) * PTRS_PER_PMD;</span>
<span class="p_add">+	pte_size = 2 * sizeof(pte_t) * PTRS_PER_PTE;</span>
<span class="p_add">+</span>
<span class="p_add">+	total = p4d_size + pud_size + pmd_size + pte_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Now calculate the added pagetable structures needed to populate</span>
<span class="p_add">+	 * the new pagetables.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_X86_5LEVEL)) {</span>
<span class="p_add">+		p4d_size = ALIGN(total, PGDIR_SIZE) / PGDIR_SIZE;</span>
<span class="p_add">+		p4d_size *= sizeof(p4d_t) * PTRS_PER_P4D;</span>
<span class="p_add">+		pud_size = ALIGN(total, P4D_SIZE) / P4D_SIZE;</span>
<span class="p_add">+		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		p4d_size = 0;</span>
<span class="p_add">+		pud_size = ALIGN(total, PGDIR_SIZE) / PGDIR_SIZE;</span>
<span class="p_add">+		pud_size *= sizeof(pud_t) * PTRS_PER_PUD;</span>
<span class="p_add">+	}</span>
<span class="p_add">+	pmd_size = ALIGN(total, PUD_SIZE) / PUD_SIZE;</span>
<span class="p_add">+	pmd_size *= sizeof(pmd_t) * PTRS_PER_PMD;</span>
<span class="p_add">+</span>
<span class="p_add">+	total += p4d_size + pud_size + pmd_size;</span>
<span class="p_add">+</span>
<span class="p_add">+	return total;</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init __nostackprotector sme_encrypt_kernel(struct boot_params *bp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	unsigned long workarea_start, workarea_end, workarea_len;</span>
<span class="p_add">+	unsigned long execute_start, execute_end, execute_len;</span>
<span class="p_add">+	unsigned long kernel_start, kernel_end, kernel_len;</span>
<span class="p_add">+	unsigned long initrd_start, initrd_end, initrd_len;</span>
<span class="p_add">+	struct sme_populate_pgd_data ppd;</span>
<span class="p_add">+	unsigned long pgtable_area_len;</span>
<span class="p_add">+	unsigned long decrypted_base;</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!sme_active())</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Prepare for encrypting the kernel and initrd by building new</span>
<span class="p_add">+	 * pagetables with the necessary attributes needed to encrypt the</span>
<span class="p_add">+	 * kernel in place.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *   One range of virtual addresses will map the memory occupied</span>
<span class="p_add">+	 *   by the kernel and initrd as encrypted.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *   Another range of virtual addresses will map the memory occupied</span>
<span class="p_add">+	 *   by the kernel and initrd as decrypted and write-protected.</span>
<span class="p_add">+	 *</span>
<span class="p_add">+	 *     The use of write-protect attribute will prevent any of the</span>
<span class="p_add">+	 *     memory from being cached.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Physical addresses gives us the identity mapped virtual addresses */</span>
<span class="p_add">+	kernel_start = __pa_symbol(_text);</span>
<span class="p_add">+	kernel_end = ALIGN(__pa_symbol(_end), PMD_PAGE_SIZE);</span>
<span class="p_add">+	kernel_len = kernel_end - kernel_start;</span>
<span class="p_add">+</span>
<span class="p_add">+	initrd_start = 0;</span>
<span class="p_add">+	initrd_end = 0;</span>
<span class="p_add">+	initrd_len = 0;</span>
<span class="p_add">+#ifdef CONFIG_BLK_DEV_INITRD</span>
<span class="p_add">+	initrd_len = (unsigned long)bp-&gt;hdr.ramdisk_size |</span>
<span class="p_add">+		     ((unsigned long)bp-&gt;ext_ramdisk_size &lt;&lt; 32);</span>
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		initrd_start = (unsigned long)bp-&gt;hdr.ramdisk_image |</span>
<span class="p_add">+			       ((unsigned long)bp-&gt;ext_ramdisk_image &lt;&lt; 32);</span>
<span class="p_add">+		initrd_end = PAGE_ALIGN(initrd_start + initrd_len);</span>
<span class="p_add">+		initrd_len = initrd_end - initrd_start;</span>
<span class="p_add">+	}</span>
<span class="p_add">+#endif</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Set the encryption workarea to be immediately after the kernel */</span>
<span class="p_add">+	workarea_start = kernel_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Calculate required number of workarea bytes needed:</span>
<span class="p_add">+	 *   executable encryption area size:</span>
<span class="p_add">+	 *     stack page (PAGE_SIZE)</span>
<span class="p_add">+	 *     encryption routine page (PAGE_SIZE)</span>
<span class="p_add">+	 *     intermediate copy buffer (PMD_PAGE_SIZE)</span>
<span class="p_add">+	 *   pagetable structures for the encryption of the kernel</span>
<span class="p_add">+	 *   pagetable structures for workarea (in case not currently mapped)</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	execute_start = workarea_start;</span>
<span class="p_add">+	execute_end = execute_start + (PAGE_SIZE * 2) + PMD_PAGE_SIZE;</span>
<span class="p_add">+	execute_len = execute_end - execute_start;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * One PGD for both encrypted and decrypted mappings and a set of</span>
<span class="p_add">+	 * PUDs and PMDs for each of the encrypted and decrypted mappings.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	pgtable_area_len = sizeof(pgd_t) * PTRS_PER_PGD;</span>
<span class="p_add">+	pgtable_area_len += sme_pgtable_calc(execute_end - kernel_start) * 2;</span>
<span class="p_add">+	if (initrd_len)</span>
<span class="p_add">+		pgtable_area_len += sme_pgtable_calc(initrd_len) * 2;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* PUDs and PMDs needed in the current pagetables for the workarea */</span>
<span class="p_add">+	pgtable_area_len += sme_pgtable_calc(execute_len + pgtable_area_len);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * The total workarea includes the executable encryption area and</span>
<span class="p_add">+	 * the pagetable area. The start of the workarea is already 2MB</span>
<span class="p_add">+	 * aligned, align the end of the workarea on a 2MB boundary so that</span>
<span class="p_add">+	 * we don&#39;t try to create/allocate PTE entries from the workarea</span>
<span class="p_add">+	 * before it is mapped.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	workarea_len = execute_len + pgtable_area_len;</span>
<span class="p_add">+	workarea_end = ALIGN(workarea_start + workarea_len, PMD_PAGE_SIZE);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Set the address to the start of where newly created pagetable</span>
<span class="p_add">+	 * structures (PGDs, PUDs and PMDs) will be allocated. New pagetable</span>
<span class="p_add">+	 * structures are created when the workarea is added to the current</span>
<span class="p_add">+	 * pagetables and when the new encrypted and decrypted kernel</span>
<span class="p_add">+	 * mappings are populated.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ppd.pgtable_area = (void *)execute_end;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Make sure the current pagetable structure has entries for</span>
<span class="p_add">+	 * addressing the workarea.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ppd.pgd = (pgd_t *)native_read_cr3_pa();</span>
<span class="p_add">+	ppd.paddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end;</span>
<span class="p_add">+	sme_map_range_decrypted(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Flush the TLB - no globals so cr3 is enough */</span>
<span class="p_add">+	native_write_cr3(__native_read_cr3());</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * A new pagetable structure is being built to allow for the kernel</span>
<span class="p_add">+	 * and initrd to be encrypted. It starts with an empty PGD that will</span>
<span class="p_add">+	 * then be populated with new PUDs and PMDs as the encrypted and</span>
<span class="p_add">+	 * decrypted kernel mappings are created.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ppd.pgd = ppd.pgtable_area;</span>
<span class="p_add">+	memset(ppd.pgd, 0, sizeof(pgd_t) * PTRS_PER_PGD);</span>
<span class="p_add">+	ppd.pgtable_area += sizeof(pgd_t) * PTRS_PER_PGD;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * A different PGD index/entry must be used to get different</span>
<span class="p_add">+	 * pagetable entries for the decrypted mapping. Choose the next</span>
<span class="p_add">+	 * PGD index and convert it to a virtual address to be used as</span>
<span class="p_add">+	 * the base of the mapping.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	decrypted_base = (pgd_index(workarea_end) + 1) &amp; (PTRS_PER_PGD - 1);</span>
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		unsigned long check_base;</span>
<span class="p_add">+</span>
<span class="p_add">+		check_base = (pgd_index(initrd_end) + 1) &amp; (PTRS_PER_PGD - 1);</span>
<span class="p_add">+		decrypted_base = max(decrypted_base, check_base);</span>
<span class="p_add">+	}</span>
<span class="p_add">+	decrypted_base &lt;&lt;= PGDIR_SHIFT;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Add encrypted kernel (identity) mappings */</span>
<span class="p_add">+	ppd.paddr = kernel_start;</span>
<span class="p_add">+	ppd.vaddr = kernel_start;</span>
<span class="p_add">+	ppd.vaddr_end = kernel_end;</span>
<span class="p_add">+	sme_map_range_encrypted(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Add decrypted, write-protected kernel (non-identity) mappings */</span>
<span class="p_add">+	ppd.paddr = kernel_start;</span>
<span class="p_add">+	ppd.vaddr = kernel_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = kernel_end + decrypted_base;</span>
<span class="p_add">+	sme_map_range_decrypted_wp(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		/* Add encrypted initrd (identity) mappings */</span>
<span class="p_add">+		ppd.paddr = initrd_start;</span>
<span class="p_add">+		ppd.vaddr = initrd_start;</span>
<span class="p_add">+		ppd.vaddr_end = initrd_end;</span>
<span class="p_add">+		sme_map_range_encrypted(&amp;ppd);</span>
<span class="p_add">+		/*</span>
<span class="p_add">+		 * Add decrypted, write-protected initrd (non-identity) mappings</span>
<span class="p_add">+		 */</span>
<span class="p_add">+		ppd.paddr = initrd_start;</span>
<span class="p_add">+		ppd.vaddr = initrd_start + decrypted_base;</span>
<span class="p_add">+		ppd.vaddr_end = initrd_end + decrypted_base;</span>
<span class="p_add">+		sme_map_range_decrypted_wp(&amp;ppd);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Add decrypted workarea mappings to both kernel mappings */</span>
<span class="p_add">+	ppd.paddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end;</span>
<span class="p_add">+	sme_map_range_decrypted(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	ppd.paddr = workarea_start;</span>
<span class="p_add">+	ppd.vaddr = workarea_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end + decrypted_base;</span>
<span class="p_add">+	sme_map_range_decrypted(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Perform the encryption */</span>
<span class="p_add">+	sme_encrypt_execute(kernel_start, kernel_start + decrypted_base,</span>
<span class="p_add">+			    kernel_len, workarea_start, (unsigned long)ppd.pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (initrd_len)</span>
<span class="p_add">+		sme_encrypt_execute(initrd_start, initrd_start + decrypted_base,</span>
<span class="p_add">+				    initrd_len, workarea_start,</span>
<span class="p_add">+				    (unsigned long)ppd.pgd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * At this point we are running encrypted.  Remove the mappings for</span>
<span class="p_add">+	 * the decrypted areas - all that is needed for this is to remove</span>
<span class="p_add">+	 * the PGD entry/entries.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	ppd.vaddr = kernel_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = kernel_end + decrypted_base;</span>
<span class="p_add">+	sme_clear_pgd(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	if (initrd_len) {</span>
<span class="p_add">+		ppd.vaddr = initrd_start + decrypted_base;</span>
<span class="p_add">+		ppd.vaddr_end = initrd_end + decrypted_base;</span>
<span class="p_add">+		sme_clear_pgd(&amp;ppd);</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	ppd.vaddr = workarea_start + decrypted_base;</span>
<span class="p_add">+	ppd.vaddr_end = workarea_end + decrypted_base;</span>
<span class="p_add">+	sme_clear_pgd(&amp;ppd);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Flush the TLB - no globals so cr3 is enough */</span>
<span class="p_add">+	native_write_cr3(__native_read_cr3());</span>
<span class="p_add">+}</span>
<span class="p_add">+</span>
<span class="p_add">+void __init __nostackprotector sme_enable(struct boot_params *bp)</span>
<span class="p_add">+{</span>
<span class="p_add">+	const char *cmdline_ptr, *cmdline_arg, *cmdline_on, *cmdline_off;</span>
<span class="p_add">+	unsigned int eax, ebx, ecx, edx;</span>
<span class="p_add">+	unsigned long feature_mask;</span>
<span class="p_add">+	bool active_by_default;</span>
<span class="p_add">+	unsigned long me_mask;</span>
<span class="p_add">+	char buffer[16];</span>
<span class="p_add">+	u64 msr;</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check for the SME/SEV support leaf */</span>
<span class="p_add">+	eax = 0x80000000;</span>
<span class="p_add">+	ecx = 0;</span>
<span class="p_add">+	native_cpuid(&amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_add">+	if (eax &lt; 0x8000001f)</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+#define AMD_SME_BIT	BIT(0)</span>
<span class="p_add">+#define AMD_SEV_BIT	BIT(1)</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Set the feature mask (SME or SEV) based on whether we are</span>
<span class="p_add">+	 * running under a hypervisor.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	eax = 1;</span>
<span class="p_add">+	ecx = 0;</span>
<span class="p_add">+	native_cpuid(&amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_add">+	feature_mask = (ecx &amp; BIT(31)) ? AMD_SEV_BIT : AMD_SME_BIT;</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Check for the SME/SEV feature:</span>
<span class="p_add">+	 *   CPUID Fn8000_001F[EAX]</span>
<span class="p_add">+	 *   - Bit 0 - Secure Memory Encryption support</span>
<span class="p_add">+	 *   - Bit 1 - Secure Encrypted Virtualization support</span>
<span class="p_add">+	 *   CPUID Fn8000_001F[EBX]</span>
<span class="p_add">+	 *   - Bits 5:0 - Pagetable bit position used to indicate encryption</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	eax = 0x8000001f;</span>
<span class="p_add">+	ecx = 0;</span>
<span class="p_add">+	native_cpuid(&amp;eax, &amp;ebx, &amp;ecx, &amp;edx);</span>
<span class="p_add">+	if (!(eax &amp; feature_mask))</span>
<span class="p_add">+		return;</span>
<span class="p_add">+</span>
<span class="p_add">+	me_mask = 1UL &lt;&lt; (ebx &amp; 0x3f);</span>
<span class="p_add">+</span>
<span class="p_add">+	/* Check if memory encryption is enabled */</span>
<span class="p_add">+	if (feature_mask == AMD_SME_BIT) {</span>
<span class="p_add">+		/* For SME, check the SYSCFG MSR */</span>
<span class="p_add">+		msr = __rdmsr(MSR_K8_SYSCFG);</span>
<span class="p_add">+		if (!(msr &amp; MSR_K8_SYSCFG_MEM_ENCRYPT))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+	} else {</span>
<span class="p_add">+		/* For SEV, check the SEV MSR */</span>
<span class="p_add">+		msr = __rdmsr(MSR_AMD64_SEV);</span>
<span class="p_add">+		if (!(msr &amp; MSR_AMD64_SEV_ENABLED))</span>
<span class="p_add">+			return;</span>
<span class="p_add">+</span>
<span class="p_add">+		/* SEV state cannot be controlled by a command line option */</span>
<span class="p_add">+		sme_me_mask = me_mask;</span>
<span class="p_add">+		sev_enabled = true;</span>
<span class="p_add">+		return;</span>
<span class="p_add">+	}</span>
<span class="p_add">+</span>
<span class="p_add">+	/*</span>
<span class="p_add">+	 * Fixups have not been applied to phys_base yet and we&#39;re running</span>
<span class="p_add">+	 * identity mapped, so we must obtain the address to the SME command</span>
<span class="p_add">+	 * line argument data using rip-relative addressing.</span>
<span class="p_add">+	 */</span>
<span class="p_add">+	asm (&quot;lea sme_cmdline_arg(%%rip), %0&quot;</span>
<span class="p_add">+	     : &quot;=r&quot; (cmdline_arg)</span>
<span class="p_add">+	     : &quot;p&quot; (sme_cmdline_arg));</span>
<span class="p_add">+	asm (&quot;lea sme_cmdline_on(%%rip), %0&quot;</span>
<span class="p_add">+	     : &quot;=r&quot; (cmdline_on)</span>
<span class="p_add">+	     : &quot;p&quot; (sme_cmdline_on));</span>
<span class="p_add">+	asm (&quot;lea sme_cmdline_off(%%rip), %0&quot;</span>
<span class="p_add">+	     : &quot;=r&quot; (cmdline_off)</span>
<span class="p_add">+	     : &quot;p&quot; (sme_cmdline_off));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (IS_ENABLED(CONFIG_AMD_MEM_ENCRYPT_ACTIVE_BY_DEFAULT))</span>
<span class="p_add">+		active_by_default = true;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		active_by_default = false;</span>
<span class="p_add">+</span>
<span class="p_add">+	cmdline_ptr = (const char *)((u64)bp-&gt;hdr.cmd_line_ptr |</span>
<span class="p_add">+				     ((u64)bp-&gt;ext_cmd_line_ptr &lt;&lt; 32));</span>
<span class="p_add">+</span>
<span class="p_add">+	cmdline_find_option(cmdline_ptr, cmdline_arg, buffer, sizeof(buffer));</span>
<span class="p_add">+</span>
<span class="p_add">+	if (!strncmp(buffer, cmdline_on, sizeof(buffer)))</span>
<span class="p_add">+		sme_me_mask = me_mask;</span>
<span class="p_add">+	else if (!strncmp(buffer, cmdline_off, sizeof(buffer)))</span>
<span class="p_add">+		sme_me_mask = 0;</span>
<span class="p_add">+	else</span>
<span class="p_add">+		sme_me_mask = active_by_default ? me_mask : 0;</span>
<span class="p_add">+}</span>
<span class="p_header">diff --git a/arch/x86/mm/mm_internal.h b/arch/x86/mm/mm_internal.h</span>
<span class="p_header">index 4e1f6e1b8159..7b4fc4386d90 100644</span>
<span class="p_header">--- a/arch/x86/mm/mm_internal.h</span>
<span class="p_header">+++ b/arch/x86/mm/mm_internal.h</span>
<span class="p_chunk">@@ -19,4 +19,5 @@</span> <span class="p_context"> extern int after_bootmem;</span>
 
 void update_cache_mode_entry(unsigned entry, enum page_cache_mode cache);
 
<span class="p_add">+extern bool sev_enabled __section(.data);</span>
 #endif	/* __X86_MM_INTERNAL_H */

</pre>
</div>




  </div>
  <div id="footer">
   <a href="http://jk.ozlabs.org/projects/patchwork/">patchwork</a>
   patch tracking system
  </div>
 </body>
</html>



